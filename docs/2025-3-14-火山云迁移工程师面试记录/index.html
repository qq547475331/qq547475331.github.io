<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  项目做的哪些工作？
  #


  上云的系统的架构？
  #


  pass平台是自研的平台吗？
  #


  应用上云的工作？
  #


  自己搭的k8s？
  #


  k8s怎么管理容器的？
  #

Kubernetes (K8s) 通过 容器编排 来管理容器，主要包括以下几个关键机制：

  1. 编排与调度
  #

K8s 通过 调度器（Scheduler） 将 Pod 调度到合适的节点上，依据资源利用率、节点健康状况等进行智能调度。

  2. 资源管理
  #


Cgroup：限制和管理容器的 CPU、内存等资源使用。
Namespace：提供资源隔离，如网络、存储、进程等。


  3. 生命周期管理
  #


Pod 及其控制器（Deployment、StatefulSet、DaemonSet 等） 负责管理容器的启动、更新、扩缩容。
Liveness & Readiness Probe：健康检查，确保容器正常运行。


  4. 网络管理
  #


CNI 插件（如 Flannel、Calico）：提供 Pod 之间的通信。
Service 解决 Pod 动态 IP 问题，实现服务发现和负载均衡。


  5. 存储管理
  #


PV & PVC：管理持久化存储，支持 NFS、Ceph、Local Volume 等后端存储。


  6. 安全与访问控制
  #


RBAC：基于角色的访问控制，限制权限。
NetworkPolicy：控制 Pod 之间的流量访问策略。

通过这些机制，Kubernetes 能够高效、自动化地管理容器的生命周期、资源分配、网络通信和安全策略，实现弹性伸缩和高可用性。"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://qq547475331.github.io/docs/2025-3-14-%E7%81%AB%E5%B1%B1%E4%BA%91%E8%BF%81%E7%A7%BB%E5%B7%A5%E7%A8%8B%E5%B8%88%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/"><meta property="og:site_name" content="Guichen's Blog"><meta property="og:title" content="2025-3-14 火山云迁移工程师面试记录"><meta property="og:description" content="项目做的哪些工作？ # 上云的系统的架构？ # pass平台是自研的平台吗？ # 应用上云的工作？ # 自己搭的k8s？ # k8s怎么管理容器的？ # Kubernetes (K8s) 通过 容器编排 来管理容器，主要包括以下几个关键机制：
1. 编排与调度 # K8s 通过 调度器（Scheduler） 将 Pod 调度到合适的节点上，依据资源利用率、节点健康状况等进行智能调度。
2. 资源管理 # Cgroup：限制和管理容器的 CPU、内存等资源使用。 Namespace：提供资源隔离，如网络、存储、进程等。 3. 生命周期管理 # Pod 及其控制器（Deployment、StatefulSet、DaemonSet 等） 负责管理容器的启动、更新、扩缩容。 Liveness & Readiness Probe：健康检查，确保容器正常运行。 4. 网络管理 # CNI 插件（如 Flannel、Calico）：提供 Pod 之间的通信。 Service 解决 Pod 动态 IP 问题，实现服务发现和负载均衡。 5. 存储管理 # PV & PVC：管理持久化存储，支持 NFS、Ceph、Local Volume 等后端存储。 6. 安全与访问控制 # RBAC：基于角色的访问控制，限制权限。 NetworkPolicy：控制 Pod 之间的流量访问策略。 通过这些机制，Kubernetes 能够高效、自动化地管理容器的生命周期、资源分配、网络通信和安全策略，实现弹性伸缩和高可用性。"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><title>2025-3-14 火山云迁移工程师面试记录 | Guichen's Blog</title>
<link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://qq547475331.github.io/docs/2025-3-14-%E7%81%AB%E5%B1%B1%E4%BA%91%E8%BF%81%E7%A7%BB%E5%B7%A5%E7%A8%8B%E5%B8%88%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/><link rel=stylesheet href=/book.min.6c8b9d2a1fc95075ed7da46ca81060b39add8fff6741ac51259f768929281e2c.css integrity="sha256-bIudKh/JUHXtfaRsqBBgs5rdj/9nQaxRJZ92iSkoHiw=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.4c8d54c6a3dc43d4b12b0b0a8953faeef995cda46ccfd5c21b6e81efdba2893f.js integrity="sha256-TI1UxqPcQ9SxKwsKiVP67vmVzaRsz9XCG26B79uiiT8=" crossorigin=anonymous></script></head><script src=https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.2.3/mermaid.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){mermaid.initialize({startOnLoad:!0});let e=document.querySelectorAll("pre > code.language-mermaid");e.forEach(e=>{let t=document.createElement("div");t.classList.add("mermaid"),t.innerHTML=e.innerText,e.parentNode.replaceWith(t)}),mermaid.init(void 0,document.querySelectorAll(".mermaid"))})</script><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>Guichen's Blog</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><a href=/docs/2025-3-8-k8s%E5%88%A0%E9%99%A4pod-deployment%E7%9A%84%E6%B5%81%E7%A8%8B%E5%9B%BE%E8%AF%A6%E8%A7%A3/>2025-3-8 k8s删除pod deployment的流程图详解</a></li><li><a href=/docs/2025-3-8-k8s%E5%88%9B%E5%BB%BApod-deployment%E6%B5%81%E7%A8%8B%E5%9B%BE%E8%AF%A6%E8%A7%A3/>2025-3-8 k8s创建pod 流程图详解</a></li><li><a href=/docs/2025-3-18-5w-pod%E5%8E%8B%E6%B5%8B%E5%A4%8D%E7%9B%98/>2025-3-18 5w pod压测复盘</a></li><li><a href=/docs/2025-3-14-%E7%81%AB%E5%B1%B1%E4%BA%91%E8%BF%81%E7%A7%BB%E5%B7%A5%E7%A8%8B%E5%B8%88%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/ class=active>2025-3-14 火山云迁移工程师面试记录</a></li><li><a href=/docs/2025-3-13-istio%E6%B5%81%E9%87%8F%E5%88%86%E6%9E%90/>2025-3-13 istio流量分析</a></li><li><a href=/docs/2025-3-13-calico%E4%B8%89%E7%A7%8D%E6%A8%A1%E5%BC%8F%E4%B8%8B%E6%B5%81%E9%87%8F%E4%BC%A0%E8%BE%93%E8%B7%AF%E5%BE%84%E5%88%86%E6%9E%90/>2025-3-13 calico三种模式下流量传输</a></li><li><a href=/docs/2025-3-12-%E5%A1%94%E8%B5%9E%E9%9D%A2%E8%AF%95/>2025-3-12 塔赞面试</a></li><li><a href=/docs/2025-3-12-%E8%BF%BD%E8%A7%85%E9%9D%A2%E8%AF%95/>2025-3-12 追觅面试</a></li><li><a href=/docs/2025-2-7-%E8%AE%A1%E5%88%922/>2025-2-7 美国码农计划</a></li><li><a href=/docs/2025-2-7-%E8%AE%A1%E5%88%92/>2025-2-7 美国码农薪酬</a></li><li><a href=/docs/2025-2-7-k8s%E7%BB%84%E4%BB%B6/>2025-2-7 k8s组件</a></li><li><a href=/docs/2025-2-28-prometheus/>2025-2-28 prometheus 面试题</a></li><li><a href=/docs/2025-2-26-%E9%9D%A2%E8%AF%950225/>2025-2-25 面试0225</a></li><li><a href=/docs/2025-2-24-%E9%AB%98%E7%BA%A7%E8%BF%90%E7%BB%B4%E9%9D%A2%E8%AF%95%E9%A2%98_ai_linux%E9%83%A8%E5%88%86/>2025-2-24 高级运维面试题-linux部分</a></li><li><a href=/docs/2025-2-24-%E4%B8%AD%E7%BA%A7%E8%BF%90%E7%BB%B4%E9%9D%A2%E8%AF%95%E9%A2%98_%E9%A2%98%E7%9B%AE/>2025-2-24 中级运维面试题</a></li><li><a href=/docs/2025-2-24-%E9%9D%A2%E8%AF%950224/>2025-2-24 0224面试</a></li><li><a href=/docs/2025-2-20-%E9%9D%A2%E8%AF%950220/>2025-2-20 面试0220</a></li><li><a href=/docs/2025-2-19-%E9%9D%A2%E8%AF%950219/>2025-2-19 面试0219</a></li><li><a href=/docs/2025-2-18-%E9%9D%A2%E8%AF%95/>2025-2-18 面试2025-0218</a></li><li><a href=/docs/2025-2-26-k8s%E7%9B%B8%E5%85%B3/>2025-2-16 k8s题目</a></li><li><a href=/docs/2025-2-12-%E9%9D%A2%E8%AF%950212/>2025-2-12 面试0212</a></li><li><a href=/docs/2025-2-11-%E9%9D%A2%E8%AF%950211/>2025-2-11 面试2025-02-11</a></li><li><a href=/docs/2024-3-4-k8s%E6%B5%81%E9%87%8F%E9%93%BE%E8%B7%AF%E5%89%96%E6%9E%90/>2025-1-16 k8s流量链路剖析</a></li><li><a href=/docs/2025-1-16-k8s%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E6%8C%87%E5%8D%97/>2025-1-16 k8s常见故障指南</a></li><li><a href=/docs/2024-3-4-k8s-csi%E5%89%96%E6%9E%90/>2025-1-16 CSI剖析演进</a></li><li><a href=/docs/2025-1-1-%E8%A6%81%E4%B8%8D%E8%A6%81%E5%88%9B%E4%B8%9A/>2025-1-1 要不要创业</a></li><li><a href=/docs/2025-1-1-%E6%97%A9%E6%9C%9F%E6%A8%A1%E5%BC%8F/>2025-1-1 早期模式</a></li><li><a href=/docs/2025-1-1-%E5%A4%A7%E5%A0%B0%E6%B2%B3-%E6%88%91%E7%9A%84%E4%BF%9D%E5%A7%86/>2025-1-1 大堰河-我的保姆</a></li><li><a href=/docs/2025-1-1-%E5%88%9D%E5%88%9B%E5%85%AC%E5%8F%B8/>2025-1-1 初创公司</a></li><li><a href=/docs/2025-1-1-%E5%88%9B%E4%B8%9A%E8%80%85%E4%BA%A4%E6%B5%81/>2025-1-1 创业者交流</a></li><li><a href=/docs/2025-1-1-%E5%88%9B%E4%B8%9A%E7%82%B9%E5%AD%90/>2025-1-1 创业点子</a></li><li><a href=/docs/2025-1-1-sealos%E8%8E%B7%E6%8A%95/>2025-1-1 sealos获投</a></li><li><a href=/docs/2024-8-1-linux%E8%BF%90%E7%BB%B4%E5%B7%A5%E7%A8%8B%E5%B8%8850%E4%B8%AA%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/>2024-8-1 linux面试题</a></li><li><a href=/docs/2024-8-1-%E5%B8%B8%E8%A7%81linux%E8%BF%90%E7%BB%B4%E9%9D%A2%E8%AF%95%E9%A2%98%E6%89%BE%E5%B7%A5%E4%BD%9C%E7%9A%84%E5%BF%85%E7%9C%8B/>2024-8-1 linux运维面试题</a></li><li><a href=/docs/2024-8-1-kubernetes%E9%9D%A2%E8%AF%95%E9%A2%98/>2024-8-1 k8s面试题</a></li><li><a href=/docs/2024-5-14-%E5%8D%95master%E5%8D%95etcd%E6%94%B9%E9%80%A0/>2024-5-1 单master单etcd改造为3master3etcd</a></li><li><a href=/docs/2024-4-17-%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93/>2024-4-17 面试总结</a></li><li><a href=/docs/2024-3-8-%E9%9D%A2%E8%AF%950308/>2024-3-8 面试</a></li><li><a href=/docs/2024-3-4-cni%E5%89%96%E6%9E%90%E6%BC%94%E8%BF%9B/>2024-3-4 CNI剖析演进</a></li><li><a href=/docs/2024-3-19-%E4%B8%A4%E5%BC%A0%E5%9B%BE%E5%85%A8%E9%9D%A2%E7%90%86%E8%A7%A3k8s%E5%8E%9F%E7%90%86/>2024-3-19 两张图全面理解k8s原理</a></li><li><a href=/docs/2025-3-14-vivo%E9%9D%A2%E8%AF%95/>2024-3-14 vivo面试</a></li><li><a href=/docs/2024-2-26-%E9%9D%A2%E8%AF%95/>2024-2-26 面试</a></li><li><a href=/docs/2024-2-22-k8s%E9%9D%A2%E8%AF%95%E5%AE%9D%E5%85%B8/>2024-2-22 k8s面试宝典</a></li><li><a href=/docs/2024-2-22-k8s%E6%9E%B6%E6%9E%84%E5%B8%88%E9%9D%A2%E8%AF%95%E5%A4%A7%E5%85%A8/>2024-2-22 k8s架构师面试大全</a></li><li><a href=/docs/2024-12-10-docker-registrry/>2024-12-10 docker registrry</a></li><li><a href=/docs/2024-12-09-openstack-ssh%E8%BF%9E%E6%8E%A5/>2024-12-09 openstack ssh连接</a></li><li><a href=/docs/2024-12-08-mutilpass%E9%83%A8%E7%BD%B2openstack/>2024-12-09 mutilpass部署openstack devstack形式</a></li><li><a href=/docs/2024-12-09-helmchart-%E9%83%A8%E7%BD%B2flask%E5%BA%94%E7%94%A8/>2024-12-09 helmchart 部署flask应用</a></li><li><a href=/docs/2024-12-09-docker-daemon.json/>2024-12-09 docker daemon.json</a></li><li><a href=/docs/2024-12-08-%E5%9D%97%E5%AD%98%E5%82%A8%E5%92%8C%E5%AF%B9%E8%B1%A1%E5%82%A8%E5%AD%98%E5%8C%BA%E5%88%AB/>2024-12-08 块存储和对象储存区别</a></li><li><a href=/docs/2024-12-08-openstack%E9%9C%80%E8%A6%81%E5%87%A0%E5%8F%B0%E8%99%9A%E6%8B%9F%E6%9C%BA/>2024-12-08 openstack需要几台虚拟机</a></li><li><a href=/docs/2024-12-08-openstack%E5%92%8Ckubernetes%E5%8C%BA%E5%88%AB/>2024-12-08 openstack和kubernetes区别</a></li><li><a href=/docs/2024-12-08-nano%E6%93%8D%E4%BD%9C/>2024-12-08 nano操作</a></li><li><a href=/docs/2024-12-08-mutilpass%E6%93%8D%E4%BD%9C/>2024-12-08 mutilpass操作</a></li><li><a href=/docs/2024-12-08-devstack/>2024-12-08 devstack</a></li><li><a href=/docs/2024-12-07-microk8s/>2024-12-07 microk8s</a></li><li><a href=/docs/2024-12-05-kubeasz%E9%83%A8%E7%BD%B2k8s/>2024-12-05 kubeasz部署k8s</a></li><li><a href=/docs/2024-10-20-%E5%88%9B%E5%BB%BA%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/>2024-10-20 使用 Keepalived 和 HAproxy 创建高可用 Kubernetes 集群</a></li><li><a href=/docs/%E9%A1%B6%E7%BA%A7devops%E5%B7%A5%E5%85%B7%E5%A4%A7%E7%9B%98%E7%82%B9-ding-ji-devops-gong-ju-da-pan-dian/>2024-08-02 顶级devops工具大盘点</a></li><li><a href=/docs/%E6%B8%85%E7%90%86docker%E9%95%9C%E5%83%8F-qing-li-docker-jing-xiang/>2024-08-02 清理docker镜像</a></li><li><a href=/docs/%E6%9E%84%E5%BB%BA%E5%AE%B9%E5%99%A8%E9%95%9C%E5%83%8F%E5%88%A9%E5%99%A8buildkit-gou-jian-rong-qi-jing-xiang-li-qi-buildkit/>2024-08-02 构建容器镜像利器buildkit</a></li><li><a href=/docs/%E6%98%AF%E6%8A%80%E6%9C%AF%E5%A4%A7%E7%A5%9E%E8%BF%98%E6%98%AF%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84%E9%83%A8%E7%9A%84%E7%A5%B8%E5%AE%B3-shi-ji-shu-da-shen-hai-shi-ji-chu-jia-gou-bu-de-huo-hai/>2024-08-02 是技术大神还是基础架构部的祸害</a></li><li><a href=/docs/%E6%90%AD%E4%B8%AA%E6%97%A5%E5%BF%97%E6%89%8B%E6%9C%BA%E7%B3%BB%E7%BB%9F%E4%B8%8D%E9%A6%99%E5%90%97-da-ge-ri-zhi-shou-ji-xi-tong-bu-xiang-ma/>2024-08-02 搭个日志手机系统不香吗</a></li><li><a href=/docs/%E6%88%91%E5%8F%AA%E6%83%B3%E5%81%9A%E6%8A%80%E6%9C%AF-%E8%B5%B0%E6%8A%80%E6%9C%AF%E8%B7%AF%E7%BA%BF-wo-zhi-xiang-zuo-ji-shu-zou-ji-shu-lu-xian/>2024-08-02 我只想做技术 走技术路线</a></li><li><a href=/docs/%E5%B8%B8%E8%A7%81linux%E8%BF%90%E7%BB%B4%E9%9D%A2%E8%AF%95%E9%A2%98-chang-jian-linux-yun-wei-mian-shi-ti/>2024-08-02 常见linux运维面试题</a></li><li><a href=/docs/%E5%A4%A7%E5%8E%82%E6%80%BB%E7%BB%93nginx%E9%AB%98%E5%B9%B6%E5%8F%91%E4%BC%98%E5%8C%96%E7%AC%94%E8%AE%B0-da-chang-zong-jie-nginx-gao-bing-fa-you-hua-bi-ji/>2024-08-02 大厂总结nginx高并发优化笔记</a></li><li><a href=/docs/%E5%8F%B2%E4%B8%8A%E6%9C%80%E7%89%9Bjenkins-pipeline%E6%B5%81%E6%B0%B4%E7%BA%BF%E8%AF%A6%E8%A7%A3-shi-shang-zui-niu-jenkinspipeline-liu-shui-xian-xiang-jie/>2024-08-02 史上最牛jenkins pipeline流水线详解</a></li><li><a href=/docs/teg%E4%B8%8Eistio%E9%9B%86%E6%88%90-teg-yu-istio-ji-cheng/>2024-08-02 TEG与istio集成</a></li><li><a href=/docs/prometheus-stack-prometheus-stack/>2024-08-02 prometheus-stack</a></li><li><a href=/docs/pixie-pixie/>2024-08-02 pixie</a></li><li><a href=/docs/nginx%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E6%83%8A%E7%BE%A4%E6%95%88%E5%BA%94-nginx-ru-he-jie-jue-jing-qun-xiao-ying/>2024-08-02 nginx如何解决惊群效应</a></li><li><a href=/docs/netctl%E6%A3%80%E6%B5%8B%E9%9B%86%E7%BE%A4pod%E9%97%B4%E8%BF%9E%E9%80%9A%E6%80%A7-netctl-jian-ce-ji-qun-pod-jian-lian-tong-xing/>2024-08-02 netctl检测集群pod间连通性</a></li><li><a href=/docs/linux%E8%BF%90%E7%BB%B4%E5%B7%A5%E7%A8%8B%E5%B8%8850%E4%B8%AA%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98-linux-yun-wei-gong-cheng-shi-50-ge-chang-jian-mian-shi-ti/>2024-08-02 linux运维工程师50个常见面试题</a></li><li><a href=/docs/linux%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-%E4%B8%83%E4%B8%AA%E5%AE%9E%E6%88%98%E7%BB%8F%E9%AA%8C-linux-xi-tong-xing-neng-you-hua-qi-ge-shi-zhan-jing-yan/>2024-08-02 linux系统性能优化 七个实战经验</a></li><li><a href=/docs/linux-awk%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E5%99%A8-8%E4%B8%AA%E6%A1%88%E4%BE%8B-linuxawk-wen-ben-chu-li-qi-8-ge-an-li/>2024-08-02 linux awk文本处理器 8个案例</a></li><li><a href=/docs/kubewharf-kubewharf/>2024-08-02 kubewharf</a></li><li><a href=/docs/kruise%E5%8E%9F%E5%9C%B0%E5%8D%87%E7%BA%A7%E8%A7%A3%E6%9E%90-kruise-yuan-de-sheng-ji-jie-xi/>2024-08-02 kruise原地升级解析</a></li><li><a href=/docs/k8s%E9%9D%A2%E8%AF%95%E9%A2%98-k8s-mian-shi-ti/>2024-08-02 K8S面试题</a></li><li><a href=/docs/k8s%E8%83%8C%E5%90%8Eservice%E6%98%AF%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C%E7%9A%84-k8s-bei-hou-service-shi-ru-he-gong-zuo-de/>2024-08-02 k8s背后service是如何工作的</a></li><li><a href=/docs/k8s%E7%9A%84%E6%9C%80%E5%90%8E%E4%B8%80%E5%9D%97%E6%8B%BC%E5%9B%BE-dbpaas-k8s-de-zui-hou-yi-kuai-pin-tu-dbpaas/>2024-08-02 K8S的最后一块拼图</a></li><li><a href=/docs/istio%E9%83%A8%E7%BD%B2-istio-bu-shu/>2024-08-02 istio部署</a></li><li><a href=/docs/istio-ingress-gateway-istio-ingress-gateway/>2024-08-02 istio-ingress-gateway</a></li><li><a href=/docs/godel-scheduler-godel-scheduler/>2024-08-02 godel-scheduler</a></li><li><a href=/docs/dockerfile%E5%AE%9A%E5%88%B6%E4%B8%93%E5%B1%9E%E9%95%9C%E5%83%8F-dockerfile-ding-zhi-zhuan-shu-jing-xiang/>2024-08-02 dockerfile定制专属镜像</a></li><li><a href=/docs/33%E6%AC%BEgitops%E4%B8%8Edevops%E4%B8%BB%E6%B5%81%E7%B3%BB%E7%BB%9F-33-kuan-gitops-yu-devops-zhu-liu-xi-tong/>2024-08-02 33款gitops与devops主流系统</a></li><li><a href=/docs/openkruise%E8%AF%A6%E7%BB%86%E8%A7%A3%E9%87%8A%E4%BB%A5%E5%8F%8A%E5%8E%9F%E5%9C%B0%E5%8D%87%E7%BA%A7%E5%8F%8A%E5%85%A8%E9%93%BE%E8%B7%AF%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83%E6%96%B9%E6%A1%88-openkruise-xiang-xi-jie-shi-yi-ji-yuan-de-sheng-ji-ji-quan-lian-lu-hui-du-fa-bu-fang-an/>2024-07-22 OpenKruise详细解释以及原地升级及全链路灰度发布方案</a></li><li><a href=/docs/k8s%E4%B9%8Bingress-nginx%E5%8E%9F%E7%90%86%E5%8F%8A%E9%85%8D%E7%BD%AE-k8s-zhi-ingress-nginx-yuan-li-ji-pei-zhi/>2024-07-05 K8S之ingress-nginx原理及配置</a></li><li><a href=/docs/%E4%BD%BF%E7%94%A8cloudflarecf%E6%90%AD%E5%BB%BAdockerhub%E4%BB%A3%E7%90%86-shi-yong-cloudflarecf-da-jian-dockerhub-dai-li/>2024-06-28 使用cloudflare(CF)搭建dockerhub代理</a></li><li><a href=/docs/%E5%A6%82%E4%BD%95%E4%B8%BAk8s%E4%BF%9D%E9%A9%BE%E6%8A%A4%E8%88%AA-ru-he-wei-k8s-bao-jia-hu-hang/>2024-04-16 如何为K8S保驾护航</a></li><li><a href=/docs/k8s%E5%A6%82%E4%BD%95%E8%8E%B7%E5%BE%97-ip-k8s-ru-he-huo-de-ip/>2024-04-16 K8S如何获得 IP</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Bstateful_setgo%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-statefulsetgo-yuan-ma-jie-du/>2024-04-10 K8S控制器之stateful_set.go源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Bstateful_set_status_updatego%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-statefulsetstatusupdatego-yuan-ma-jie-du/>2024-04-10 K8S控制器之stateful_set_status_update.go源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Bstateful_set_controlgo%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-statefulsetcontrolgo-yuan-ma-jie-du/>2024-04-10 K8S控制器之stateful_set_control.go源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Bstateful_pod_controlgo%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-statefulpodcontrolgo-yuan-ma-jie-du/>2024-04-10 K8S控制器之stateful_pod_control.go源码解读</a></li><li><a href=/docs/k8s%E8%B0%83%E5%BA%A6%E5%99%A8-extendergo-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-diao-du-qi-extendergo-yuan-ma-jie-du/>2024-04-09 K8S调度器 extender.go 源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Bsyncgo-%E5%90%8C%E6%AD%A5-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-syncgo-tong-bu-yuan-ma-jie-du/>2024-04-09 K8S控制器之sync.go 同步 源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Brollbackgo-%E5%9B%9E%E6%BB%9A-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-rollbackgo-hui-gun-yuan-ma-jie-du/>2024-04-09 K8S控制器之rollback.go 回滚 源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Brecreatego-%E9%87%8D%E5%BB%BA-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-recreatego-zhong-jian-yuan-ma-jie-du/>2024-04-09 K8S控制器之recreate.go 重建 源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8B-schedulergo-%E8%B0%83%E5%BA%A6%E5%99%A8-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-schedulergo-diao-du-qi-yuan-ma-jie-du/>2024-04-09 K8S控制器之 scheduler.go 调度器 源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8B-rollinggo-%E6%BB%9A%E5%8A%A8%E6%9B%B4%E6%96%B0-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-rollinggo-gun-dong-geng-xin-yuan-ma-jie-du/>2024-04-09 K8S控制器之 rolling.go 滚动更新 源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8B-progressgo-%E8%BF%9B%E5%BA%A6-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-progressgo-jin-du-yuan-ma-jie-du/>2024-04-09 K8S控制器之 progress.go 进度 源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8B-deployment_controllergo%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-deploymentcontrollergo-yuan-ma-jie-du/>2024-04-09 K8S控制器之 deployment_controller.go源码解读</a></li><li><a href=/docs/k8s-%E8%B0%83%E5%BA%A6%E5%99%A8-scheduler_onego-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-diao-du-qi-scheduleronego-yuan-ma-jie-du/>2024-04-09 K8S 调度器 scheduler_one.go 源码解读</a></li><li><a href=/docs/%E5%BD%BB%E6%82%9F%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C-che-wu-rong-qi-wang-luo/>2024-04-07 彻悟容器网络</a></li><li><a href=/docs/%E9%9D%A2%E8%AF%95%E7%94%A8-golang-%E6%89%8B%E6%92%B8-lru-mian-shi-yong-golang-shou-lu-lru/>2024-04-03 面试用 Golang 手撸 LRU</a></li><li><a href=/docs/%E8%87%AA%E5%8A%A8%E5%B1%8F%E8%94%BDip%E6%94%BB%E5%87%BB-zi-dong-ping-bi-ip-gong-ji/>2024-04-03 自动屏蔽IP攻击</a></li><li><a href=/docs/%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85kubephere-li-xian-an-zhuang-kubephere/>2024-04-03 离线安装kubephere</a></li><li><a href=/docs/%E7%A3%81%E7%9B%98%E6%95%B0%E6%8D%AE%E6%81%A2%E5%A4%8D-ci-pan-shu-ju-hui-fu/>2024-04-03 磁盘数据恢复</a></li><li><a href=/docs/%E6%B8%85%E7%90%86%E6%AE%8B%E7%95%99%E7%9A%84calico%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6-qing-li-can-liu-de-calico-wang-luo/>2024-04-03 清理残留的calico网络插件</a></li><li><a href=/docs/%E6%B5%81%E9%87%8F%E4%BD%95%E5%A4%84%E6%9D%A5%E4%BD%95%E5%A4%84%E5%8E%BB-liu-liang-he-chu-lai-he-chu-qu/>2024-04-03 流量何处来何处去</a></li><li><a href=/docs/%E6%9E%81%E5%A4%A7%E6%8F%90%E9%AB%98%E5%B7%A5%E4%BD%9C%E6%95%88%E7%8E%87%E7%9A%84-linux-%E5%91%BD%E4%BB%A4-ji-da-ti-gao-gong-zuo-xiao-lv-de-linux-ming-ling/>2024-04-03 极大提高工作效率的 Linux 命令</a></li><li><a href=/docs/%E6%96%87%E5%AD%A6%E7%9A%84%E6%95%85%E4%B9%A1-wen-xue-de-gu-xiang/>2024-04-03 文学的故乡</a></li><li><a href=/docs/%E6%90%9E%E6%87%82k8s%E9%89%B4%E6%9D%83-gao-dong-k8s-jian-quan/>2024-04-03 搞懂K8S鉴权</a></li><li><a href=/docs/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E5%8E%9F%E7%90%86-rong-qi-wang-luo-yuan-li/>2024-04-03 容器网络原理</a></li><li><a href=/docs/%E5%AE%B9%E5%99%A8%E7%9A%84%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E4%B8%80-overlayfs-%E5%8E%9F%E7%90%86-rong-qi-de-wen-jian-xi-tong--yi-overlayfs-yuan-li/>2024-04-03 容器的文件系统 OverlayFS 原理</a></li><li><a href=/docs/%E5%AE%B9%E5%99%A8%E5%8E%9F%E7%90%86-rong-qi-yuan-li/>2024-04-03 容器原理</a></li><li><a href=/docs/%E5%AE%B9%E5%99%A8%E5%86%85%E7%9A%84-1-%E5%8F%B7%E8%BF%9B%E7%A8%8B-rong-qi-nei-de-1-hao-jin-cheng/>2024-04-03 容器内的 1 号进程</a></li><li><a href=/docs/%E5%AE%B9%E5%99%A8%E4%B8%AD%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90%E4%BB%A5%E5%8F%8A%E4%B8%8D%E5%90%8Cdnspolicy%E5%AF%B9%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90%E7%9A%84%E5%BD%B1%E5%93%8D-rong-qi-zhong-yu-ming-jie-xi-yi-ji-bu-tong-dnspolicy-dui-yu-ming-jie-xi-de-ying-xiang/>2024-04-03 容器中域名解析以及不同dnspolicy对域名解析的影响</a></li><li><a href=/docs/%E5%A6%82%E4%BD%95%E8%B0%83%E8%AF%95-crash-%E5%AE%B9%E5%99%A8%E7%9A%84%E7%BD%91%E7%BB%9C-ru-he-diao-shi-crash-rong-qi-de-wang-luo/>2024-04-03 如何调试 crash 容器的网络</a></li><li><a href=/docs/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8tekton%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BAcicd%E5%B9%B3%E5%8F%B0-ru-he-shi-yong-tekton-kuai-su-da-jian-cicd-ping-tai/>2024-04-03 如何使用tekton快速搭建CI/CD平台</a></li><li><a href=/docs/%E5%A4%A7%E8%A7%84%E6%A8%A1%E5%B9%B6%E5%8F%91%E4%B8%8B%E5%A6%82%E4%BD%95%E5%8A%A0%E5%BF%AB-pod-%E5%90%AF%E5%8A%A8%E9%80%9F%E5%BA%A6-da-gui-mo-bing-fa-xia-ru-he-jia-kuai-pod-qi-dong-su-du/>2024-04-03 大规模并发下如何加快 Pod 启动速度</a></li><li><a href=/docs/%E4%BD%BF%E7%94%A8kubernees-leases-%E8%BD%BB%E6%9D%BE%E5%AE%9E%E7%8E%B0leader-election-shi-yong-kuberneesleases-qing-song-shi-xian-leaderelection/>2024-04-03 使用kubernees leases 轻松实现leader election</a></li><li><a href=/docs/%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2k8s%E5%8A%A0%E8%8A%82%E7%82%B9%E6%93%8D%E4%BD%9C-er-jin-zhi-bu-shu-k8s-jia-jie-dian-cao-zuo/>2024-04-03 二进制部署K8S加节点操作</a></li><li><a href=/docs/%E4%B8%A4%E5%BC%A0%E5%9B%BE%E5%85%A8%E9%9D%A2%E7%90%86%E8%A7%A3k8s%E5%8E%9F%E7%90%86-liang-zhang-tu-quan-mian-li-jie-k8s-yuan-li/>2024-04-03 两张图全面理解K8S原理</a></li><li><a href=/docs/ssl%E8%AF%81%E4%B9%A6%E8%87%AA%E7%AD%BE%E5%8F%91-ssl-zheng-shu-zi-qian-fa/>2024-04-03 ssl证书自签发</a></li><li><a href=/docs/prometheus%E4%BC%81%E4%B8%9A%E7%BA%A7%E7%9B%91%E6%8E%A7%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93-prometheus-qi-ye-ji-jian-kong-shi-yong-zong-jie/>2024-04-03 prometheus企业级监控使用总结</a></li><li><a href=/docs/metallb-l2-%E5%8E%9F%E7%90%86-metallbl2-yuan-li/>2024-04-03 MetalLB L2 原理</a></li><li><a href=/docs/linux-%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E5%A4%A7%E5%85%A8-linux-xing-neng-you-hua-da-quan/>2024-04-03 Linux 性能优化大全</a></li><li><a href=/docs/kubernetes-%E8%AF%81%E4%B9%A6%E8%AF%A6%E8%A7%A3%E9%89%B4%E6%9D%83-kubernetes-zheng-shu-xiang-jie--jian-quan-/>2024-04-03 Kubernetes 证书详解(鉴权)</a></li><li><a href=/docs/kubernetes-%E8%AF%81%E4%B9%A6%E8%AF%A6%E8%A7%A3%E8%AE%A4%E8%AF%81-kubernetes-zheng-shu-xiang-jie--ren-zheng-/>2024-04-03 Kubernetes 证书详解(认证)</a></li><li><a href=/docs/kubernetes-%E6%BA%90%E7%A0%81%E7%BB%93%E6%9E%84-kubernetes-yuan-ma-jie-gou/>2024-04-03 Kubernetes 源码结构</a></li><li><a href=/docs/kubernetes-api-kubernetesapi/>2024-04-03 Kubernetes API</a></li><li><a href=/docs/kubekey%E6%B7%BB%E5%8A%A0%E6%96%B0%E8%8A%82%E7%82%B9-kubekey-tian-jia-xin-jie-dian/>2024-04-03 kubekey添加新节点</a></li><li><a href=/docs/k8s%E9%9D%A2%E8%AF%95%E5%AE%9D%E5%85%B8-k8s-mian-shi-bao-dian/>2024-04-03 K8S面试宝典</a></li><li><a href=/docs/k8s%E9%9D%A2%E8%AF%95%E5%A4%A7%E5%85%A8-k8s-mian-shi-da-quan/>2024-04-03 K8S面试大全</a></li><li><a href=/docs/k8s%E8%BF%90%E7%BB%B4%E4%B9%8B%E6%B8%85%E7%90%86%E7%A3%81%E7%9B%98-k8s-yun-wei-zhi-qing-li-ci-pan/>2024-04-03 k8s运维之清理磁盘</a></li><li><a href=/docs/k8s%E8%B0%83%E8%AF%95pod-k8s-diao-shi-pod/>2024-04-03 K8S调试POD</a></li><li><a href=/docs/k8s%E7%9A%84pod%E7%B1%BB%E5%9E%8B-k8s-de-pod-lei-xing/>2024-04-03 K8S的POD类型</a></li><li><a href=/docs/k8s%E5%BA%94%E7%94%A8%E7%9A%84%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5-k8s-ying-yong-de-zui-jia-shi-jian/>2024-04-03 k8s应用的最佳实践</a></li><li><a href=/docs/k8s%E5%91%BD%E4%BB%A4%E6%8C%87%E5%8D%97-k8s-ming-ling-zhi-nan/>2024-04-03 K8S命令指南</a></li><li><a href=/docs/k8s%E5%8E%9F%E5%9C%B0%E5%8D%87%E7%BA%A7-k8s-yuan-de-sheng-ji/>2024-04-03 K8S原地升级</a></li><li><a href=/docs/k8s-%E6%8E%A2%E9%92%88%E5%8E%9F%E7%90%86-k8s-tan-zhen-yuan-li/>2024-04-03 K8S 探针原理</a></li><li><a href=/docs/k8s-%E5%BC%80%E5%8F%91%E5%8F%AF%E4%B8%8D%E6%AD%A2-crud-k8s-kai-fa-ke-bu-zhi-crud/>2024-04-03 K8S 开发可不止 CRUD</a></li><li><a href=/docs/k8s-gpt-k8sgpt/>2024-04-03 K8S GPT</a></li><li><a href=/docs/k8s-csi-openebs%E5%8E%9F%E7%90%86-k8scsiopenebs-yuan-li/>2024-04-03 K8S csi openebs原理</a></li><li><a href=/docs/helm-chart%E5%92%8Crepo-helmchart-he-repo/>2024-04-03 helm chart和repo</a></li><li><a href=/docs/flanel%E7%BD%91%E7%BB%9C-flanel-wang-luo/>2024-04-03 flanel网络</a></li><li><a href=/docs/etcd%E7%A8%B3%E5%AE%9A%E6%80%A7%E5%8F%8A%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E5%AE%9E%E8%B7%B5-etcd-wen-ding-xing-ji-xing-neng-you-hua-shi-jian/>2024-04-03 ETCD稳定性及性能优化实践</a></li><li><a href=/docs/etcd%E5%A4%87%E4%BB%BD-etcd-bei-fen/>2024-04-03 ETCD备份</a></li><li><a href=/docs/docker%E9%87%8D%E8%A6%81%E7%9A%84%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86%E7%82%B9-docker-zhong-yao-de-wang-luo-zhi-shi-dian/>2024-04-03 Docker重要的网络知识点</a></li><li><a href=/docs/dockerfile%E7%9A%84copy%E5%92%8Cadd%E7%9A%84%E5%8C%BA%E5%88%AB-dockerfile-de-copy-he-add-de-qu-bie/>2024-04-03 dockerfile的copy和add的区别</a></li><li><a href=/docs/coredns%E4%B9%8B%E5%85%89-coredns-zhi-guang/>2024-04-03 COREDNS之光</a></li><li><a href=/docs/containerd-%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C-containerd-ji-ben-cao-zuo/>2024-04-03 Containerd 基本操作</a></li><li><a href=/docs/cni%E6%8F%92%E4%BB%B6%E9%80%89%E5%9E%8B-cni-cha-jian-xuan-xing/>2024-04-03 CNI插件选型</a></li><li><a href=/docs/client-go-%E6%9E%B6%E6%9E%84-client-go-jia-gou/>2024-04-03 Client-go 架构</a></li><li><a href=/docs/client-go-%E5%9B%9B%E7%A7%8D%E5%AE%A2%E6%88%B7%E7%AB%AF-client-go-si-zhong-ke-hu-duan/>2024-04-03 Client-go 四种客户端</a></li><li><a href=/docs/cicd%E6%80%9D%E8%80%83-cicd-si-kao/>2024-04-03 CICD思考</a></li><li><a href=/docs/calico%E7%BD%91%E7%BB%9C%E8%87%AA%E5%AE%9A%E4%B9%89-calico-wang-luo-zi-ding-yi/>2024-04-03 Calico网络自定义</a></li><li><a href=/docs/acme%E8%87%AA%E5%8A%A8%E6%9B%B4%E6%96%B0%E8%AF%81%E4%B9%A6-acme-zi-dong-geng-xin-zheng-shu/>2024-04-03 acme自动更新证书</a></li><li><a href=/docs/16%E4%B8%AA%E6%A6%82%E5%BF%B5%E5%B8%A6%E4%BD%A0%E5%85%A5%E9%97%A8-kubernetes-16-ge-gai-nian-dai-ni-ru-men-kubernetes/>2024-04-03 16个概念带你入门 Kubernetes</a></li><li><a href=/docs/%E9%9D%A2%E8%AF%950308-mian-shi-0308/>2024-04-03 面试0308</a></li><li><a href=/docs/600%E6%9D%A1%E6%9C%80%E5%BC%BAlinux%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93-600-tiao-zui-qiang-linux-ming-ling-zong-jie/>2024-04-03 600条最强linux命令总结</a></li><li><a href=/docs/16%E5%BC%A0%E7%A1%AC%E6%A0%B8%E5%9B%BE%E8%A7%A3k8s%E7%BD%91%E7%BB%9C-16-zhang-ying-he-tu-jie-k8s-wang-luo/>2024-04-03 16张硬核图解k8s网络</a></li><li><a href=/docs/k8s%E4%B9%8Bkubelet%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-zhi-kubelet-yuan-ma-jie-du/>2024-03-28 k8s之kubelet源码解读</a></li><li><a href=/docs/k8s-%E6%B5%81%E9%87%8F%E9%93%BE%E8%B7%AF%E5%89%96%E6%9E%90-k8s-liu-liang-lian-lu-pou-xi/>2024-03-04 K8S 流量链路剖析</a></li><li><a href=/docs/k8s-csi%E5%89%96%E6%9E%90%E6%BC%94%E8%BF%9B-k8scsi-pou-xi-yan-jin/>2024-03-04 K8S CSI剖析演进</a></li><li><a href=/docs/k8s-cni%E5%89%96%E6%9E%90%E6%BC%94%E8%BF%9B-k8scni-pou-xi-yan-jin/>2024-03-04 K8S CNI剖析演进</a></li><li><a href=/docs/%E4%BD%BF%E7%94%A8-openfunction-%E5%9C%A8%E4%BB%BB%E4%BD%95%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD%E4%B8%8A%E8%BF%90%E8%A1%8C%E6%97%A0%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%B7%A5%E4%BD%9C%E8%B4%9F%E8%BD%BD-shi-yong-openfunction-zai-ren-he-ji-chu-she-shi-shang-yun-xing-wu-fu-wu-qi-gong-zuo-fu-zai/>2024-01-21 使用 OpenFunction 在任何基础设施上运行无服务器工作负载</a></li><li><a href=/docs/%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85%E9%9B%86%E7%BE%A4-li-xian-an-zhuang-ji-qun/>2023-09-28 离线安装集群</a></li><li><a href=/docs/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%AF%B4%E6%98%8E-cao-zuo-xi-tong-shuo-ming/>2023-09-28 操作系统说明</a></li><li><a href=/docs/%E5%BF%AB%E9%80%9F%E6%8C%87%E5%8D%97-kuai-su-zhi-nan/>2023-09-28 快速指南</a></li><li><a href=/docs/%E5%BC%80%E5%A7%8B%E4%BD%BF%E7%94%A8-cilium-kai-shi-shi-yong-cilium/>2023-09-28 开始使用 cilium</a></li><li><a href=/docs/%E5%A4%9A%E6%9E%B6%E6%9E%84%E6%94%AF%E6%8C%81-duo-jia-gou-zhi-chi/>2023-09-28 多架构支持</a></li><li><a href=/docs/%E5%85%AC%E6%9C%89%E4%BA%91%E4%B8%8A%E9%83%A8%E7%BD%B2-kubeasz-gong-you-yun-shang-bu-shu-kubeasz/>2023-09-28 公有云上部署</a></li><li><a href=/docs/%E4%B8%AA%E6%80%A7%E5%8C%96%E9%9B%86%E7%BE%A4%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE-ge-xing-hua-ji-qun-can-shu-pei-zhi/>2023-09-28 个性化集群参数配置</a></li><li><a href=/docs/network-check-network-check/>2023-09-28 network-check</a></li><li><a href=/docs/kube-router-%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6-kube-router-wang-luo-zu-jian/>2023-09-28 kube-router 网络组件</a></li><li><a href=/docs/ezctl-%E5%91%BD%E4%BB%A4%E8%A1%8C%E4%BB%8B%E7%BB%8D-ezctl-ming-ling-xing-jie-shao/>2023-09-28 ezctl 命令行介绍</a></li><li><a href=/docs/ex-lb-%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E9%83%A8%E7%BD%B2-ex-lb-fu-zai-jun-heng-bu-shu/>2023-09-28 EX-LB 负载均衡部署</a></li><li><a href=/docs/calico-%E9%85%8D%E7%BD%AE-bgp-route-reflectors-calico-pei-zhi-bgproutereflectors/>2023-09-28 calico 配置 BGP Route Reflectors</a></li><li><a href=/docs/07-%E5%AE%89%E8%A3%85%E9%9B%86%E7%BE%A4%E4%B8%BB%E8%A6%81%E6%8F%92%E4%BB%B6-07--an-zhuang-ji-qun-zhu-yao-cha-jian/>2023-09-28 15:26:42.651 07-安装集群主要插件</a></li><li><a href=/docs/08-k8s-%E9%9B%86%E7%BE%A4%E5%AD%98%E5%82%A8--k8s-ji-qun-cun-chu/>2023-09-28 08-K8S 集群存储</a></li><li><a href=/docs/06-%E5%AE%89%E8%A3%85%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6-06--an-zhuang-wang-luo-zu-jian/>2023-09-28 06-安装网络组件</a></li><li><a href=/docs/06-%E5%AE%89%E8%A3%85kube-ovn%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6-06--an-zhuang-kube-ovn-wang-luo-zu-jian/>2023-09-28 06-安装kube-ovn网络组件</a></li><li><a href=/docs/06-%E5%AE%89%E8%A3%85flannel%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6-06--an-zhuang-flannel-wang-luo-zu-jian/>2023-09-28 06-安装flannel网络组件</a></li><li><a href=/docs/06-%E5%AE%89%E8%A3%85cilium%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6-06--an-zhuang-cilium-wang-luo-zu-jian/>2023-09-28 06-安装cilium网络组件</a></li><li><a href=/docs/06-%E5%AE%89%E8%A3%85calico%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6-06--an-zhuang-calico-wang-luo-zu-jian/>2023-09-28 06-安装calico网络组件</a></li><li><a href=/docs/02-%E5%AE%89%E8%A3%85etcd%E9%9B%86%E7%BE%A4-02--an-zhuang-etcd-ji-qun/>2023-09-28 02-安装etcd集群</a></li><li><a href=/docs/00-%E9%9B%86%E7%BE%A4%E8%A7%84%E5%88%92%E5%92%8C%E5%9F%BA%E7%A1%80%E5%8F%82%E6%95%B0%E8%AE%BE%E5%AE%9A-00--ji-qun-gui-hua-he-ji-chu-can-shu-she-ding/>2023-09-28 00-集群规划和基础参数设定</a></li><li><a href=/docs/05-%E5%AE%89%E8%A3%85kube_node%E8%8A%82%E7%82%B9-05--an-zhuang-kubenode-jie-dian/>2023-09-28 05-安装kube_node节点</a></li><li><a href=/docs/04-%E5%AE%89%E8%A3%85kube_master%E8%8A%82%E7%82%B9-04--an-zhuang-kubemaster-jie-dian/>2023-09-28 04-安装kube_master节点</a></li><li><a href=/docs/03-%E5%AE%89%E8%A3%85%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6-03--an-zhuang-rong-qi-yun-xing-shi/>2023-09-28 03-安装容器运行时</a></li><li><a href=/docs/01-%E5%88%9B%E5%BB%BA%E8%AF%81%E4%B9%A6%E5%92%8C%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87-01--chuang-jian-zheng-shu-he-huan-jing-zhun-bei/>2023-09-28 01-创建证书和环境准备</a></li><li><a href=/docs/%E6%9C%89%E8%BF%993%E4%B8%AA%E8%BF%B9%E8%B1%A1%E4%BD%A0%E5%B0%B1%E8%AF%A5%E7%A6%BB%E8%81%8C%E4%BA%86-you-zhe-3-ge-ji-xiang--ni-jiu-gai-li-zhi-le/>2023-09-21 思考</a></li><li><a href=/docs/%E4%BD%BF%E7%94%A8-keepalived-%E5%92%8C-haproxy-%E5%88%9B%E5%BB%BA%E9%AB%98%E5%8F%AF%E7%94%A8-kubernetes-%E9%9B%86%E7%BE%A4-shi-yong-keepalived-he-haproxy-chuang-jian-gao-ke-yong-kubernetes-ji-qun/>2023-04-12 使用 Keepalived 和 HAproxy 创建高可用 Kubernetes 集群</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h3>2025-3-14 火山云迁移工程师面试记录</h3><label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><ul><li><a href=#1-编排与调度>1. <strong>编排与调度</strong></a></li><li><a href=#2-资源管理>2. <strong>资源管理</strong></a></li><li><a href=#3-生命周期管理>3. <strong>生命周期管理</strong></a></li><li><a href=#4-网络管理>4. <strong>网络管理</strong></a></li><li><a href=#5-存储管理>5. <strong>存储管理</strong></a></li><li><a href=#6-安全与访问控制>6. <strong>安全与访问控制</strong></a></li></ul></li></ul><ul><li><ul><li><a href=#1-namespace命名空间隔离><strong>1. Namespace（命名空间隔离）</strong></a></li><li><a href=#2-cgroups控制组><strong>2. Cgroups（控制组）</strong></a></li><li><a href=#3-文件系统隔离><strong>3. 文件系统隔离</strong></a></li><li><a href=#总结><strong>总结</strong></a></li></ul></li></ul><ul><li><ul><li><a href=#1-liveness-probe存活探针><strong>1. Liveness Probe（存活探针）</strong></a></li><li><a href=#2-readiness-probe就绪探针><strong>2. Readiness Probe（就绪探针）</strong></a></li><li><a href=#3-startup-probe启动探针><strong>3. Startup Probe（启动探针）</strong></a></li><li><a href=#探针的检测方式><strong>探针的检测方式</strong></a></li><li><a href=#总结-1><strong>总结</strong></a></li></ul></li></ul><ul><li><ul><li><a href=#1-存活检测liveness-probe><strong>1. 存活检测（Liveness Probe）</strong></a></li><li><a href=#2-就绪检测readiness-probe><strong>2. 就绪检测（Readiness Probe）</strong></a></li><li><a href=#存活检测与就绪检测的差异><strong>存活检测与就绪检测的差异</strong></a></li><li><a href=#如何选择使用><strong>如何选择使用</strong></a></li></ul></li></ul><ul><li><ul><li><a href=#liveness-probe存活检测><strong>Liveness Probe（存活检测）</strong></a></li><li><a href=#readiness-probe就绪检测><strong>Readiness Probe（就绪检测）</strong></a></li><li><a href=#liveness-probe-与-readiness-probe-的对比><strong>Liveness Probe 与 Readiness Probe 的对比</strong></a></li><li><a href=#总结-2><strong>总结</strong></a></li></ul></li></ul><ul><li><ul><li><a href=#1-容器可能会接收到流量导致错误响应>1. <strong>容器可能会接收到流量，导致错误响应</strong></a></li><li><a href=#2-滚动更新时流量可能会被发送到未准备好的容器>2. <strong>滚动更新时，流量可能会被发送到未准备好的容器</strong></a></li><li><a href=#3-错误的容器可能被认为是健康的>3. <strong>错误的容器可能被认为是健康的</strong></a></li><li><a href=#4-无法区分容器启动状态和运行状态>4. <strong>无法区分容器启动状态和运行状态</strong></a></li><li><a href=#5-无法有效控制流量分配>5. <strong>无法有效控制流量分配</strong></a></li><li><a href=#总结-3>总结</a></li></ul></li></ul><ul><li><ul><li><a href=#如何实现就绪检测>如何实现就绪检测？</a></li><li><a href=#配置方式>配置方式</a></li><li><a href=#如何保证容器是就绪的>如何保证容器是就绪的？</a></li><li><a href=#小结>小结</a></li></ul></li></ul><ul><li><ul><li><a href=#就绪检测判断的内容>就绪检测判断的内容：</a></li><li><a href=#如何实现就绪检测-1>如何实现就绪检测？</a></li><li><a href=#判断什么东西>判断什么东西？</a></li><li><a href=#就绪检测如何工作>就绪检测如何工作？</a></li><li><a href=#适用场景>适用场景：</a></li><li><a href=#小结-1>小结：</a></li></ul></li></ul><ul><li><ul><li><a href=#存活检测与就绪检测的区别>存活检测与就绪检测的区别：</a></li><li><a href=#即使端口通了应用可能仍未就绪的原因>即使端口通了，应用可能仍未就绪的原因：</a></li><li><a href=#举例说明>举例说明：</a></li><li><a href=#可能导致容器未就绪的其他因素>可能导致容器未就绪的其他因素：</a></li><li><a href=#小结-2>小结：</a></li></ul></li></ul><ul><li><ul><li><a href=#hpa-的工作原理><strong>HPA 的工作原理</strong>：</a></li><li><a href=#hpa-如何工作><strong>HPA 如何工作</strong>：</a></li><li><a href=#配置-hpa><strong>配置 HPA</strong>：</a></li><li><a href=#优点-2><strong>优点</strong>：</a></li><li><a href=#适用场景-1><strong>适用场景</strong>：</a></li><li><a href=#缺点-2><strong>缺点</strong>：</a></li><li><a href=#总结-4><strong>总结</strong>：</a></li></ul></li></ul><ul><li><ul><li><a href=#1-cpu-使用率>1. <strong>CPU 使用率</strong>：</a></li><li><a href=#2-内存使用率>2. <strong>内存使用率</strong>：</a></li><li><a href=#3-自定义指标>3. <strong>自定义指标</strong>：</a></li><li><a href=#4-目标指标配置>4. <strong>目标指标配置</strong>：</a></li><li><a href=#如何收集自定义指标><strong>如何收集自定义指标</strong>：</a></li><li><a href=#总结-5><strong>总结</strong>：</a></li></ul></li></ul><ul><li><ul><li><a href=#1-内存泄漏问题>1. <strong>内存泄漏问题</strong>：</a></li><li><a href=#2-内存使用高峰>2. <strong>内存使用高峰</strong>：</a></li><li><a href=#3-内存配置不当>3. <strong>内存配置不当</strong>：</a></li><li><a href=#4-java-应用的内存管理>4. <strong>Java 应用的内存管理</strong>：</a></li><li><a href=#5-内存与-cpu-之间的关系>5. <strong>内存与 CPU 之间的关系</strong>：</a></li><li><a href=#6-延迟问题>6. <strong>延迟问题</strong>：</a></li><li><a href=#总结-6>总结：</a></li><li><a href=#requests-和-limits-的作用><code>requests</code> 和 <code>limits</code> 的作用：</a></li><li><a href=#解释>解释：</a></li><li><a href=#举个例子>举个例子：</a></li><li><a href=#总结-7>总结：</a></li></ul></li></ul><ul><li><ul><li><a href=#可能导致的问题>可能导致的问题：</a></li><li><a href=#解决方案>解决方案：</a></li><li><a href=#总结-8>总结：</a></li></ul></li></ul><ul><li><ul><li><a href=#1-cpu-使用率cpu-utilization>1. <strong>CPU 使用率</strong>（CPU Utilization）</a></li><li><a href=#2-内存使用率memory-utilization>2. <strong>内存使用率</strong>（Memory Utilization）</a></li><li><a href=#3-自定义指标custom-metrics>3. <strong>自定义指标</strong>（Custom Metrics）</a></li><li><a href=#4-请求数和响应时间等基于业务的指标>4. <strong>请求数和响应时间等（基于业务的指标）</strong></a></li><li><a href=#5-网络流量network-traffic>5. <strong>网络流量</strong>（Network Traffic）</a></li><li><a href=#6-多个指标组合multiple-metrics>6. <strong>多个指标组合</strong>（Multiple Metrics）</a></li><li><a href=#hpa-的工作原理-1>HPA 的工作原理：</a></li><li><a href=#适用性-2>适用性：</a></li><li><a href=#总结-9>总结：</a></li></ul></li></ul><ul><li><ul><li><a href=#1-service---clusterip集群内部暴露>1. <strong>Service - ClusterIP（集群内部暴露）</strong></a></li><li><a href=#2-service---nodeport集群外部暴露>2. <strong>Service - NodePort（集群外部暴露）</strong></a></li><li><a href=#3-service---loadbalancer外部负载均衡器>3. <strong>Service - LoadBalancer（外部负载均衡器）</strong></a></li><li><a href=#4-ingress入口控制器>4. <strong>Ingress（入口控制器）</strong></a></li><li><a href=#5-externalname>5. <strong>ExternalName</strong></a></li><li><a href=#总结-10>总结：</a></li></ul></li></ul><ul><li><ul><li><a href=#ingress-的优点><strong>Ingress 的优点：</strong></a></li><li><a href=#ingress-的缺点><strong>Ingress 的缺点：</strong></a></li><li><a href=#nodeport-的优点><strong>NodePort 的优点：</strong></a></li><li><a href=#nodeport-的缺点><strong>NodePort 的缺点：</strong></a></li><li><a href=#总结对比><strong>总结对比：</strong></a></li><li><a href=#适用场景-2><strong>适用场景</strong>：</a></li></ul></li></ul><ul><li><ul><li><a href=#1-ingress应用层layer-7><strong>1. Ingress：应用层（Layer 7）</strong></a></li><li><a href=#2-nodeport传输层layer-4><strong>2. NodePort：传输层（Layer 4）</strong></a></li><li><a href=#总结-11><strong>总结：</strong></a></li></ul></li></ul><ul><li><ul><li><a href=#1-明确沟通理解需求>1. <strong>明确沟通，理解需求</strong></a></li><li><a href=#2-评估影响和可行性>2. <strong>评估影响和可行性</strong></a></li><li><a href=#3-与上级或同事协商>3. <strong>与上级或同事协商</strong></a></li><li><a href=#4-明确边界调整合同或工作协议>4. <strong>明确边界，调整合同或工作协议</strong></a></li><li><a href=#5-灵活应对保持专业>5. <strong>灵活应对，保持专业</strong></a></li></ul></li></ul><ul><li><ul><li><a href=#1-了解需求的细节>1. <strong>了解需求的细节</strong></a></li><li><a href=#2-评估影响与可行性>2. <strong>评估影响与可行性</strong></a></li><li><a href=#3-与团队讨论并调整计划>3. <strong>与团队讨论并调整计划</strong></a></li><li><a href=#4-与甲方沟通调整计划与交付>4. <strong>与甲方沟通调整计划与交付</strong></a></li><li><a href=#5-签订变更协议或文档>5. <strong>签订变更协议或文档</strong></a></li><li><a href=#6-持续监控与管理>6. <strong>持续监控与管理</strong></a></li><li><a href=#7-总结与教训>7. <strong>总结与教训</strong></a></li><li><a href=#总结-12>总结</a></li></ul></li></ul><ul><li><ul><li><a href=#1-备份-etcd-数据>1. <strong>备份 Etcd 数据</strong></a></li><li><a href=#2-证书管理与生成>2. <strong>证书管理与生成</strong></a></li><li><a href=#3-集群扩展操作>3. <strong>集群扩展操作</strong></a></li><li><a href=#4-验证和监控>4. <strong>验证和监控</strong></a></li><li><a href=#5-回退策略>5. <strong>回退策略</strong></a></li><li><a href=#总结-13>总结</a></li></ul></li></ul><ul><li><ul><li><a href=#1-标记节点为不可调度-cordon>1. <strong>标记节点为不可调度 (Cordon)</strong></a></li><li><a href=#2-迁移-pods-drain>2. <strong>迁移 Pods (Drain)</strong></a></li><li><a href=#3-删除节点>3. <strong>删除节点</strong></a></li><li><a href=#4-恢复节点>4. <strong>恢复节点</strong></a></li><li><a href=#5-验证节点状态>5. <strong>验证节点状态</strong></a></li><li><a href=#6-其他注意事项>6. <strong>其他注意事项</strong></a></li><li><a href=#7-批量下线多个节点>7. <strong>批量下线多个节点</strong></a></li></ul></li></ul><ul><li><ul><li><a href=#驱逐节点上的所有-pod>驱逐节点上的所有 Pod</a></li><li><a href=#示例-3>示例</a></li><li><a href=#重要提示>重要提示</a></li><li><a href=#恢复节点>恢复节点</a></li></ul></li></ul><ul><li><ul><li><a href=#1-迁云准备工作>1. <strong>迁云准备工作</strong></a></li><li><a href=#2-迁云实施>2. <strong>迁云实施</strong></a></li><li><a href=#3-迁云后的优化>3. <strong>迁云后的优化</strong></a></li><li><a href=#4-如何查看迁云工作进展>4. <strong>如何查看迁云工作进展</strong></a></li><li><a href=#5-常见的迁云工具和方法>5. <strong>常见的迁云工具和方法</strong></a></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=项目做的哪些工作>项目做的哪些工作？
<a class=anchor href=#%e9%a1%b9%e7%9b%ae%e5%81%9a%e7%9a%84%e5%93%aa%e4%ba%9b%e5%b7%a5%e4%bd%9c>#</a></h1><h1 id=上云的系统的架构>上云的系统的架构？
<a class=anchor href=#%e4%b8%8a%e4%ba%91%e7%9a%84%e7%b3%bb%e7%bb%9f%e7%9a%84%e6%9e%b6%e6%9e%84>#</a></h1><h1 id=pass平台是自研的平台吗>pass平台是自研的平台吗？
<a class=anchor href=#pass%e5%b9%b3%e5%8f%b0%e6%98%af%e8%87%aa%e7%a0%94%e7%9a%84%e5%b9%b3%e5%8f%b0%e5%90%97>#</a></h1><h1 id=应用上云的工作>应用上云的工作？
<a class=anchor href=#%e5%ba%94%e7%94%a8%e4%b8%8a%e4%ba%91%e7%9a%84%e5%b7%a5%e4%bd%9c>#</a></h1><h1 id=自己搭的k8s>自己搭的k8s？
<a class=anchor href=#%e8%87%aa%e5%b7%b1%e6%90%ad%e7%9a%84k8s>#</a></h1><h1 id=k8s怎么管理容器的>k8s怎么管理容器的？
<a class=anchor href=#k8s%e6%80%8e%e4%b9%88%e7%ae%a1%e7%90%86%e5%ae%b9%e5%99%a8%e7%9a%84>#</a></h1><p>Kubernetes (K8s) 通过 <strong>容器编排</strong> 来管理容器，主要包括以下几个关键机制：</p><h3 id=1-编排与调度>1. <strong>编排与调度</strong>
<a class=anchor href=#1-%e7%bc%96%e6%8e%92%e4%b8%8e%e8%b0%83%e5%ba%a6>#</a></h3><p>K8s 通过 <strong>调度器（Scheduler）</strong> 将 Pod 调度到合适的节点上，依据资源利用率、节点健康状况等进行智能调度。</p><h3 id=2-资源管理>2. <strong>资源管理</strong>
<a class=anchor href=#2-%e8%b5%84%e6%ba%90%e7%ae%a1%e7%90%86>#</a></h3><ul><li><strong>Cgroup</strong>：限制和管理容器的 CPU、内存等资源使用。</li><li><strong>Namespace</strong>：提供资源隔离，如网络、存储、进程等。</li></ul><h3 id=3-生命周期管理>3. <strong>生命周期管理</strong>
<a class=anchor href=#3-%e7%94%9f%e5%91%bd%e5%91%a8%e6%9c%9f%e7%ae%a1%e7%90%86>#</a></h3><ul><li><strong>Pod 及其控制器（Deployment、StatefulSet、DaemonSet 等）</strong> 负责管理容器的启动、更新、扩缩容。</li><li><strong>Liveness & Readiness Probe</strong>：健康检查，确保容器正常运行。</li></ul><h3 id=4-网络管理>4. <strong>网络管理</strong>
<a class=anchor href=#4-%e7%bd%91%e7%bb%9c%e7%ae%a1%e7%90%86>#</a></h3><ul><li><strong>CNI 插件</strong>（如 Flannel、Calico）：提供 Pod 之间的通信。</li><li><strong>Service</strong> 解决 Pod 动态 IP 问题，实现服务发现和负载均衡。</li></ul><h3 id=5-存储管理>5. <strong>存储管理</strong>
<a class=anchor href=#5-%e5%ad%98%e5%82%a8%e7%ae%a1%e7%90%86>#</a></h3><ul><li><strong>PV & PVC</strong>：管理持久化存储，支持 NFS、Ceph、Local Volume 等后端存储。</li></ul><h3 id=6-安全与访问控制>6. <strong>安全与访问控制</strong>
<a class=anchor href=#6-%e5%ae%89%e5%85%a8%e4%b8%8e%e8%ae%bf%e9%97%ae%e6%8e%a7%e5%88%b6>#</a></h3><ul><li><strong>RBAC</strong>：基于角色的访问控制，限制权限。</li><li><strong>NetworkPolicy</strong>：控制 Pod 之间的流量访问策略。</li></ul><p>通过这些机制，Kubernetes 能够高效、自动化地管理容器的生命周期、资源分配、网络通信和安全策略，实现弹性伸缩和高可用性。</p><h1 id=容器之间是怎么做到资源隔离的>容器之间是怎么做到资源隔离的？
<a class=anchor href=#%e5%ae%b9%e5%99%a8%e4%b9%8b%e9%97%b4%e6%98%af%e6%80%8e%e4%b9%88%e5%81%9a%e5%88%b0%e8%b5%84%e6%ba%90%e9%9a%94%e7%a6%bb%e7%9a%84>#</a></h1><p>容器之间的资源隔离主要依赖于 <strong>Linux 内核机制</strong>，主要涉及 <strong>Namespace</strong> 和 <strong>Cgroups</strong> 两大核心技术。</p><hr><h3 id=1-namespace命名空间隔离><strong>1. Namespace（命名空间隔离）</strong>
<a class=anchor href=#1-namespace%e5%91%bd%e5%90%8d%e7%a9%ba%e9%97%b4%e9%9a%94%e7%a6%bb>#</a></h3><p>Namespace 提供了进程级的资源隔离，使每个容器看起来像是在自己的独立系统中运行，关键的 Namespace 类型包括：</p><ul><li><strong>PID Namespace</strong>：隔离进程 ID，容器内的进程只能看到自己的进程，无法访问宿主机或其他容器的进程。</li><li><strong>NET Namespace</strong>：隔离网络环境，每个容器可以有独立的 IP、端口、路由等，默认情况下，容器之间的网络互不相通。</li><li><strong>IPC Namespace</strong>：隔离进程间通信（如信号量、消息队列、共享内存），避免跨容器干扰。</li><li><strong>UTS Namespace</strong>：隔离主机名，容器可以有自己的 <code>hostname</code> 和 <code>domainname</code>。</li><li><strong>Mount Namespace</strong>：隔离文件系统，使容器只能访问挂载的文件系统，无法访问宿主机的文件。</li><li><strong>User Namespace</strong>：隔离用户权限，使容器内的 root 用户映射为宿主机的非 root 用户，提高安全性。</li></ul><hr><h3 id=2-cgroups控制组><strong>2. Cgroups（控制组）</strong>
<a class=anchor href=#2-cgroups%e6%8e%a7%e5%88%b6%e7%bb%84>#</a></h3><p>Cgroups（Control Groups）用于 <strong>限制和管理容器的资源使用</strong>，防止某个容器独占 CPU、内存等资源，关键点包括：</p><ul><li><strong>CPU 限制</strong>：通过 <code>cpu.shares</code>、<code>cpu.cfs_quota_us</code> 等参数，限制容器可用的 CPU 时间。</li><li><strong>内存隔离</strong>：通过 <code>memory.limit_in_bytes</code> 控制容器最大可用内存，并提供 OOM 保护。</li><li><strong>I/O 限制</strong>：通过 <code>blkio.throttle.*</code> 限制容器的磁盘读写速度，防止 I/O 资源争夺。</li><li><strong>网络带宽限制</strong>：通过 <code>tc</code>（Traffic Control）限制容器的网络流量。</li></ul><hr><h3 id=3-文件系统隔离><strong>3. 文件系统隔离</strong>
<a class=anchor href=#3-%e6%96%87%e4%bb%b6%e7%b3%bb%e7%bb%9f%e9%9a%94%e7%a6%bb>#</a></h3><ul><li><strong>UnionFS（如 OverlayFS）</strong>：容器的文件系统是分层的，多个容器共享基础镜像，修改时创建独立的可写层，互不影响。</li><li><strong>chroot & Mount Namespace</strong>：容器的根文件系统 <code>/</code> 是独立的，无法访问宿主机文件（除非主动挂载）。</li></ul><hr><h3 id=总结><strong>总结</strong>
<a class=anchor href=#%e6%80%bb%e7%bb%93>#</a></h3><ul><li><strong>Namespace 负责隔离</strong>：进程、网络、文件系统、用户权限等。</li><li><strong>Cgroups 负责限制</strong>：CPU、内存、I/O、网络等资源。</li><li><strong>文件系统隔离</strong>：保证容器对文件的独立访问。</li></ul><p>这些机制确保了容器之间的资源独立性，使其类似于轻量级的虚拟机，但具备更高的效率和灵活性。</p><h1 id=k8s中如何实现健康检查>k8s中如何实现健康检查？
<a class=anchor href=#k8s%e4%b8%ad%e5%a6%82%e4%bd%95%e5%ae%9e%e7%8e%b0%e5%81%a5%e5%ba%b7%e6%a3%80%e6%9f%a5>#</a></h1><p>在 Kubernetes（K8s）中，健康检查（Health Check）主要通过 <strong>探针（Probes）</strong> 实现，确保 Pod 内部的容器正常运行。K8s 提供了三种探针：</p><h3 id=1-liveness-probe存活探针><strong>1. Liveness Probe（存活探针）</strong>
<a class=anchor href=#1-liveness-probe%e5%ad%98%e6%b4%bb%e6%8e%a2%e9%92%88>#</a></h3><p>用于检测容器是否<strong>存活</strong>，如果探测失败，K8s 会<strong>重启容器</strong>。常用于检测应用程序是否<strong>发生死锁</strong>或进入不可恢复的状态。</p><p><strong>示例：</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>livenessProbe</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>httpGet</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>path</span>: <span style=color:#ae81ff>/healthz</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>port</span>: <span style=color:#ae81ff>8080</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>initialDelaySeconds</span>: <span style=color:#ae81ff>3</span>  <span style=color:#75715e># 启动后等待 3 秒再执行检查</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>periodSeconds</span>: <span style=color:#ae81ff>10</span>       <span style=color:#75715e># 每 10 秒执行一次</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>failureThreshold</span>: <span style=color:#ae81ff>3</span>     <span style=color:#75715e># 连续 3 次失败才会触发重启</span>
</span></span></code></pre></div><hr><h3 id=2-readiness-probe就绪探针><strong>2. Readiness Probe（就绪探针）</strong>
<a class=anchor href=#2-readiness-probe%e5%b0%b1%e7%bb%aa%e6%8e%a2%e9%92%88>#</a></h3><p>用于检测容器是否<strong>就绪</strong>，只有探测成功的 Pod 才会加入到 Service 的<strong>负载均衡</strong>中。适用于应用<strong>启动时间长</strong>，或者依赖外部服务的情况。</p><p><strong>示例：</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>readinessProbe</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>exec</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>command</span>: [<span style=color:#e6db74>&#34;cat&#34;</span>, <span style=color:#e6db74>&#34;/tmp/healthy&#34;</span>]
</span></span><span style=display:flex><span>  <span style=color:#f92672>initialDelaySeconds</span>: <span style=color:#ae81ff>5</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>periodSeconds</span>: <span style=color:#ae81ff>5</span>
</span></span></code></pre></div><blockquote><p>这个示例中，K8s 会执行 <code>cat /tmp/healthy</code>，如果文件存在则视为健康，否则 Pod 不会接收流量。</p></blockquote><hr><h3 id=3-startup-probe启动探针><strong>3. Startup Probe（启动探针）</strong>
<a class=anchor href=#3-startup-probe%e5%90%af%e5%8a%a8%e6%8e%a2%e9%92%88>#</a></h3><p>用于检测容器是否<strong>成功启动</strong>，特别适用于<strong>启动时间很长</strong>的应用。只有<strong>启动探测成功</strong>后，K8s 才会执行 <code>livenessProbe</code> 和 <code>readinessProbe</code>。</p><p><strong>示例：</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>startupProbe</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>tcpSocket</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>port</span>: <span style=color:#ae81ff>8080</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>initialDelaySeconds</span>: <span style=color:#ae81ff>10</span>  <span style=color:#75715e># 启动后等待 10 秒再执行检查</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>periodSeconds</span>: <span style=color:#ae81ff>5</span>         <span style=color:#75715e># 每 5 秒执行一次</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>failureThreshold</span>: <span style=color:#ae81ff>30</span>      <span style=color:#75715e># 允许探测失败 30 次，避免启动慢的应用被误杀</span>
</span></span></code></pre></div><hr><h3 id=探针的检测方式><strong>探针的检测方式</strong>
<a class=anchor href=#%e6%8e%a2%e9%92%88%e7%9a%84%e6%a3%80%e6%b5%8b%e6%96%b9%e5%bc%8f>#</a></h3><p>探针可以使用三种方式来进行健康检查：</p><ol><li><p>HTTP 请求（httpGet）：</p><ul><li>适用于提供 HTTP 健康检查接口的应用（如 <code>GET /healthz</code>）。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>httpGet</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>path</span>: <span style=color:#ae81ff>/healthz</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>port</span>: <span style=color:#ae81ff>8080</span>
</span></span></code></pre></div></li><li><p>命令执行（exec）：</p><ul><li>适用于执行 Shell 命令来判断应用状态。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>exec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>command</span>: [<span style=color:#e6db74>&#34;cat&#34;</span>, <span style=color:#e6db74>&#34;/tmp/healthy&#34;</span>]
</span></span></code></pre></div></li><li><p>TCP 端口（tcpSocket）：</p><ul><li>适用于检测端口是否监听，常用于数据库或 TCP 服务。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>tcpSocket</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>port</span>: <span style=color:#ae81ff>3306</span>
</span></span></code></pre></div></li></ol><hr><h3 id=总结-1><strong>总结</strong>
<a class=anchor href=#%e6%80%bb%e7%bb%93-1>#</a></h3><table><thead><tr><th>探针类型</th><th>作用</th></tr></thead><tbody><tr><td>Liveness Probe</td><td>检测容器是否存活，失败则<strong>重启容器</strong></td></tr><tr><td>Readiness Probe</td><td>检测容器是否就绪，失败则<strong>不接受流量</strong></td></tr><tr><td>Startup Probe</td><td>检测容器是否成功启动，失败则<strong>杀死容器</strong></td></tr></tbody></table><p>健康检查可以提高应用的可用性，避免将不健康的 Pod 加入集群，同时减少因应用异常导致的长时间不可用问题。</p><h1 id=什么时候用存活检测什么时候用就绪检测>什么时候用存活检测，什么时候用就绪检测？
<a class=anchor href=#%e4%bb%80%e4%b9%88%e6%97%b6%e5%80%99%e7%94%a8%e5%ad%98%e6%b4%bb%e6%a3%80%e6%b5%8b%e4%bb%80%e4%b9%88%e6%97%b6%e5%80%99%e7%94%a8%e5%b0%b1%e7%bb%aa%e6%a3%80%e6%b5%8b>#</a></h1><p>在 Kubernetes 中，<strong>存活检测（Liveness Probe）</strong> 和 <strong>就绪检测（Readiness Probe）</strong> 都是用于监控容器健康状况的，但它们的用途和作用有所不同。以下是它们的使用场景：</p><h3 id=1-存活检测liveness-probe><strong>1. 存活检测（Liveness Probe）</strong>
<a class=anchor href=#1-%e5%ad%98%e6%b4%bb%e6%a3%80%e6%b5%8bliveness-probe>#</a></h3><p><strong>用途：</strong> 用于判断容器是否存活。如果存活检查失败，K8s 会<strong>重启容器</strong>，确保容器能够恢复到正常工作状态。</p><p><strong>使用场景：</strong></p><ul><li><strong>应用死锁或无法恢复的错误</strong>：如果应用程序进入了死锁或无法恢复的状态，存活检测可以及时发现问题并自动重启容器。</li><li><strong>长时间未响应的应用</strong>：如果应用出现长时间无响应的状态，例如处理请求时阻塞，可以通过存活检测将其标记为不可用，并让 Kubernetes 自动重启。</li><li><strong>自动恢复</strong>：存活检测可以帮助在容器崩溃或进入错误状态时自动进行恢复，无需人工干预。</li></ul><p><strong>示例：</strong></p><ul><li>数据库服务出现死锁</li><li>应用崩溃，未能自恢复</li></ul><h3 id=2-就绪检测readiness-probe><strong>2. 就绪检测（Readiness Probe）</strong>
<a class=anchor href=#2-%e5%b0%b1%e7%bb%aa%e6%a3%80%e6%b5%8breadiness-probe>#</a></h3><p><strong>用途：</strong> 用于判断容器是否<strong>准备好接收流量</strong>。如果就绪检查失败，K8s 会<strong>从 Service 的负载均衡中移除该 Pod</strong>，直到容器就绪后才会再次加入。</p><p><strong>使用场景：</strong></p><ul><li><strong>应用启动时间长</strong>：如果应用需要较长的启动时间，避免在启动期间接收流量，可以使用就绪检测，确保容器完全启动后再将流量引导到容器。</li><li><strong>依赖外部服务</strong>：当容器依赖其他服务（如数据库、外部 API 等），并且在没有这些服务可用时容器无法正常工作，可以使用就绪探针来避免流量进入不可用的容器。</li><li><strong>滚动更新期间</strong>：在应用滚动更新时，使用就绪探针可以确保只有那些准备好接受流量的容器才会接收到流量，避免请求被发送到尚未完全启动的容器。</li></ul><p><strong>示例：</strong></p><ul><li>启动时需要等待数据库或缓存服务连接</li><li>长时间初始化的应用，比如一些需要加载大量数据的服务</li></ul><h3 id=存活检测与就绪检测的差异><strong>存活检测与就绪检测的差异</strong>
<a class=anchor href=#%e5%ad%98%e6%b4%bb%e6%a3%80%e6%b5%8b%e4%b8%8e%e5%b0%b1%e7%bb%aa%e6%a3%80%e6%b5%8b%e7%9a%84%e5%b7%ae%e5%bc%82>#</a></h3><table><thead><tr><th>特性</th><th>存活检测（Liveness Probe）</th><th>就绪检测（Readiness Probe）</th></tr></thead><tbody><tr><td><strong>触发条件</strong></td><td>容器是否处于“存活”状态</td><td>容器是否准备好接收流量</td></tr><tr><td><strong>触发操作</strong></td><td>如果失败，K8s 会重启容器</td><td>如果失败，K8s 会将容器从 Service 的负载均衡中移除</td></tr><tr><td><strong>使用场景</strong></td><td>应用出现死锁、崩溃或无法恢复的情况</td><td>应用启动、依赖外部服务或长时间初始化的应用</td></tr><tr><td><strong>常见的恢复行为</strong></td><td>重启容器</td><td>停止将流量发送到该容器，直到就绪检查通过</td></tr></tbody></table><h3 id=如何选择使用><strong>如何选择使用</strong>
<a class=anchor href=#%e5%a6%82%e4%bd%95%e9%80%89%e6%8b%a9%e4%bd%bf%e7%94%a8>#</a></h3><ul><li><strong>使用存活检测</strong>：如果容器在启动后，应用可能会进入死锁状态或无法正常工作，需要通过重启恢复时，使用存活检测。</li><li><strong>使用就绪检测</strong>：如果容器在启动时需要等待某些条件（如外部服务）或需要更长时间来初始化，但不希望在此期间接收流量，使用就绪检测。</li></ul><p>通常，<strong>存活检测</strong>和<strong>就绪检测</strong>可以一起使用，以确保容器在没有异常时正常工作，同时避免在启动时接收流量。</p><h1 id=优缺点适用性>优缺点？适用性？
<a class=anchor href=#%e4%bc%98%e7%bc%ba%e7%82%b9%e9%80%82%e7%94%a8%e6%80%a7>#</a></h1><h3 id=liveness-probe存活检测><strong>Liveness Probe（存活检测）</strong>
<a class=anchor href=#liveness-probe%e5%ad%98%e6%b4%bb%e6%a3%80%e6%b5%8b>#</a></h3><h4 id=优点><strong>优点</strong>
<a class=anchor href=#%e4%bc%98%e7%82%b9>#</a></h4><ol><li><strong>自动恢复</strong>：当容器无法恢复或进入死锁状态时，Kubernetes 会自动重启容器，减少人工干预，保证应用持续可用。</li><li><strong>减少系统故障时间</strong>：通过检测到容器状态异常并快速重启，减少了容器出现问题的停机时间。</li><li><strong>提高容器的健壮性</strong>：即使容器在运行时出现问题，存活检测也能确保容器在不健康时被及时重启，从而提高系统的稳定性。</li></ol><h4 id=缺点><strong>缺点</strong>
<a class=anchor href=#%e7%bc%ba%e7%82%b9>#</a></h4><ol><li><strong>重启容器可能会导致短暂的服务中断</strong>：在容器被重启期间，应用可能会短时间内无法提供服务，影响可用性。</li><li><strong>重启触发条件不当可能影响系统性能</strong>：如果存活检测的触发条件设置过于敏感，可能会导致容器频繁重启，浪费资源并影响系统性能。</li></ol><h4 id=适用性><strong>适用性</strong>
<a class=anchor href=#%e9%80%82%e7%94%a8%e6%80%a7>#</a></h4><ul><li><strong>死锁和崩溃场景</strong>：适用于那些在运行过程中可能会进入死锁或无法恢复的状态的应用。</li><li><strong>自动恢复场景</strong>：适用于容器出现无法自恢复的问题时，通过自动重启恢复服务。</li><li><strong>长时间无响应或崩溃的应用</strong>：如数据库服务等长时间无响应，存活探针可帮助检测并重启。</li></ul><h3 id=readiness-probe就绪检测><strong>Readiness Probe（就绪检测）</strong>
<a class=anchor href=#readiness-probe%e5%b0%b1%e7%bb%aa%e6%a3%80%e6%b5%8b>#</a></h3><h4 id=优点-1><strong>优点</strong>
<a class=anchor href=#%e4%bc%98%e7%82%b9-1>#</a></h4><ol><li><strong>优化流量管理</strong>：确保容器在完全准备好之前不接收流量，避免请求失败或处理不当。</li><li><strong>优雅的滚动更新</strong>：在进行滚动更新时，可以确保只有准备好处理流量的容器才会接收到流量，避免了不完整的服务版本对用户的影响。</li><li><strong>提升容器启动过程的稳定性</strong>：对于启动时间较长的容器，容器准备好后才会接受流量，避免不必要的流量冲击。</li></ol><h4 id=缺点-1><strong>缺点</strong>
<a class=anchor href=#%e7%bc%ba%e7%82%b9-1>#</a></h4><ol><li><strong>延迟接收流量</strong>：在容器启动过程中，可能会延迟接收流量，如果就绪探针的配置过于严格，可能会影响服务的启动速度。</li><li><strong>错误的就绪状态</strong>：如果就绪检测配置不当，可能导致容器提前被移出流量池，导致系统的部分功能不可用。</li></ol><h4 id=适用性-1><strong>适用性</strong>
<a class=anchor href=#%e9%80%82%e7%94%a8%e6%80%a7-1>#</a></h4><ul><li><strong>长时间启动的服务</strong>：适用于启动时间较长的服务，例如依赖数据库连接或需要加载大量数据的应用。</li><li><strong>依赖外部服务</strong>：适用于需要等待外部服务（如数据库、缓存等）连接后才能正常工作的应用。</li><li><strong>滚动更新时的流量控制</strong>：适用于滚动更新期间，确保新容器容器完全启动且健康后才接受流量。</li></ul><h3 id=liveness-probe-与-readiness-probe-的对比><strong>Liveness Probe 与 Readiness Probe 的对比</strong>
<a class=anchor href=#liveness-probe-%e4%b8%8e-readiness-probe-%e7%9a%84%e5%af%b9%e6%af%94>#</a></h3><table><thead><tr><th>特性</th><th>存活检测（Liveness Probe）</th><th>就绪检测（Readiness Probe）</th></tr></thead><tbody><tr><td><strong>功能</strong></td><td>检查容器是否处于存活状态，是否需要重启</td><td>检查容器是否已准备好接受流量</td></tr><tr><td><strong>触发操作</strong></td><td>失败时，K8s 会重启容器</td><td>失败时，K8s 会将容器从服务负载均衡中移除，直到健康为止</td></tr><tr><td><strong>应用场景</strong></td><td>容器死锁或崩溃后需要自动重启</td><td>容器启动过程中、需要依赖外部服务或等待初始化完成时避免接受流量</td></tr><tr><td><strong>优点</strong></td><td>自动恢复，减少手动干预，保持容器稳定运行</td><td>优化流量管理，确保只将流量发送到健康的容器，避免不必要的流量冲击</td></tr><tr><td><strong>缺点</strong></td><td>容器重启时会暂时不可用，可能影响用户体验</td><td>容器启动时可能延迟接收流量，导致启动过程被影响</td></tr></tbody></table><h3 id=总结-2><strong>总结</strong>
<a class=anchor href=#%e6%80%bb%e7%bb%93-2>#</a></h3><ul><li><strong>存活检测（Liveness Probe）</strong> 主要用于检测容器是否死掉或进入无法恢复的状态，适用于那些可能在运行时崩溃或死锁的容器。它能够帮助自动恢复，确保容器在失败后能够迅速重新启动。</li><li><strong>就绪检测（Readiness Probe）</strong> 主要用于检测容器是否准备好接收流量，适用于那些启动过程中需要等待外部依赖或有较长启动时间的容器。它能确保只有在容器完全启动并且准备好后，才开始接收流量。</li></ul><p>这两者可以结合使用：容器启动时先进行就绪检查，确保完全准备好后接收流量；一旦运行过程中出现问题，通过存活检查进行容器的重启。</p><h1 id=只会用存活不用就绪会出现什么问题>只会用存活不用就绪会出现什么问题？
<a class=anchor href=#%e5%8f%aa%e4%bc%9a%e7%94%a8%e5%ad%98%e6%b4%bb%e4%b8%8d%e7%94%a8%e5%b0%b1%e7%bb%aa%e4%bc%9a%e5%87%ba%e7%8e%b0%e4%bb%80%e4%b9%88%e9%97%ae%e9%a2%98>#</a></h1><p>如果只使用 <strong>存活检测（Liveness Probe）</strong> 而不使用 <strong>就绪检测（Readiness Probe）</strong>，可能会导致以下问题：</p><h3 id=1-容器可能会接收到流量导致错误响应>1. <strong>容器可能会接收到流量，导致错误响应</strong>
<a class=anchor href=#1-%e5%ae%b9%e5%99%a8%e5%8f%af%e8%83%bd%e4%bc%9a%e6%8e%a5%e6%94%b6%e5%88%b0%e6%b5%81%e9%87%8f%e5%af%bc%e8%87%b4%e9%94%99%e8%af%af%e5%93%8d%e5%ba%94>#</a></h3><ul><li><strong>问题</strong>：如果容器在启动阶段还没有完全准备好处理流量（比如等待外部服务连接，或者正在加载大量数据），那么此时它可能无法正常响应请求。如果只配置了存活检测，容器会被认为是“活的”，但是由于未准备好处理流量，仍然会接收到来自负载均衡器的请求，导致错误的响应或超时。</li><li><strong>后果</strong>：应用可能会因为无法处理流量而出现错误或者返回异常响应，影响用户体验。</li></ul><h3 id=2-滚动更新时流量可能会被发送到未准备好的容器>2. <strong>滚动更新时，流量可能会被发送到未准备好的容器</strong>
<a class=anchor href=#2-%e6%bb%9a%e5%8a%a8%e6%9b%b4%e6%96%b0%e6%97%b6%e6%b5%81%e9%87%8f%e5%8f%af%e8%83%bd%e4%bc%9a%e8%a2%ab%e5%8f%91%e9%80%81%e5%88%b0%e6%9c%aa%e5%87%86%e5%a4%87%e5%a5%bd%e7%9a%84%e5%ae%b9%e5%99%a8>#</a></h3><ul><li><strong>问题</strong>：在进行滚动更新时，新的容器可能需要一些时间来启动并初始化，例如连接数据库、缓存或其他外部服务。如果没有就绪检测，新容器可能会在还未完全启动时就接收到了流量，导致请求失败。</li><li><strong>后果</strong>：用户可能会遭遇服务中断或错误，因为新容器尚未准备好处理请求。</li></ul><h3 id=3-错误的容器可能被认为是健康的>3. <strong>错误的容器可能被认为是健康的</strong>
<a class=anchor href=#3-%e9%94%99%e8%af%af%e7%9a%84%e5%ae%b9%e5%99%a8%e5%8f%af%e8%83%bd%e8%a2%ab%e8%ae%a4%e4%b8%ba%e6%98%af%e5%81%a5%e5%ba%b7%e7%9a%84>#</a></h3><ul><li><strong>问题</strong>：存活检测的目的是检查容器是否健康，一旦容器不健康，Kubernetes 会重启它。但是，如果容器本身的功能部分失效（如缺少必要的依赖、服务没有完全启动等），而存活检测没有发现这些问题，容器可能不会被重启。这时，容器可能仍然“活着”，但并不能处理请求。</li><li><strong>后果</strong>：这些容器可能在没有被重启的情况下继续存在，导致系统稳定性降低。</li></ul><h3 id=4-无法区分容器启动状态和运行状态>4. <strong>无法区分容器启动状态和运行状态</strong>
<a class=anchor href=#4-%e6%97%a0%e6%b3%95%e5%8c%ba%e5%88%86%e5%ae%b9%e5%99%a8%e5%90%af%e5%8a%a8%e7%8a%b6%e6%80%81%e5%92%8c%e8%bf%90%e8%a1%8c%e7%8a%b6%e6%80%81>#</a></h3><ul><li><strong>问题</strong>：只使用存活检测无法明确容器是否准备好接收流量。存活检测只关心容器是否死掉，无法确保容器是否准备好处理流量。</li><li><strong>后果</strong>：容器在未准备好时就会开始接收请求，可能导致用户体验变差。</li></ul><h3 id=5-无法有效控制流量分配>5. <strong>无法有效控制流量分配</strong>
<a class=anchor href=#5-%e6%97%a0%e6%b3%95%e6%9c%89%e6%95%88%e6%8e%a7%e5%88%b6%e6%b5%81%e9%87%8f%e5%88%86%e9%85%8d>#</a></h3><ul><li><strong>问题</strong>：当容器处于启动阶段或正在恢复的过程中，容器的资源和服务可能尚未完全准备好。没有就绪检测就无法精确地控制流量的分配。</li><li><strong>后果</strong>：流量可能会分配给不健康或未准备好的容器，从而导致请求失败，影响系统的可用性。</li></ul><h3 id=总结-3>总结
<a class=anchor href=#%e6%80%bb%e7%bb%93-3>#</a></h3><p>如果只使用存活检测而不使用就绪检测，最大的风险就是容器可能在还没有完全准备好时就开始接收流量，导致错误响应、请求失败、系统不稳定等问题。为了避免这种情况，应该同时使用存活检测和就绪检测，确保容器在完全准备好后才开始接收流量，并在出现问题时能够自动恢复。</p><h1 id=就绪怎么保证他是就绪的怎么实现>就绪怎么保证他是就绪的？怎么实现？
<a class=anchor href=#%e5%b0%b1%e7%bb%aa%e6%80%8e%e4%b9%88%e4%bf%9d%e8%af%81%e4%bb%96%e6%98%af%e5%b0%b1%e7%bb%aa%e7%9a%84%e6%80%8e%e4%b9%88%e5%ae%9e%e7%8e%b0>#</a></h1><p>Kubernetes 的 <strong>就绪检测（Readiness Probe）</strong> 用于判断容器是否已准备好接收流量。就绪探针确保容器在能够正常提供服务之前不会接收到流量，这样可以防止请求发送到一个未完全启动或正在初始化的容器，避免因容器未准备好而导致的错误响应或失败。</p><h3 id=如何实现就绪检测>如何实现就绪检测？
<a class=anchor href=#%e5%a6%82%e4%bd%95%e5%ae%9e%e7%8e%b0%e5%b0%b1%e7%bb%aa%e6%a3%80%e6%b5%8b>#</a></h3><p>在 Kubernetes 中，可以通过配置 <strong>就绪探针</strong> 来实现这一功能。就绪探针有三种方式来检查容器的就绪状态：</p><ol><li><strong>HTTP GET 请求</strong></li><li><strong>TCP Socket 检查</strong></li><li><strong>执行命令检查</strong></li></ol><h3 id=配置方式>配置方式
<a class=anchor href=#%e9%85%8d%e7%bd%ae%e6%96%b9%e5%bc%8f>#</a></h3><p>在 Kubernetes 中，你可以通过在 Pod 的 <code>spec.containers</code> 部分配置 <code>readinessProbe</code> 来实现就绪检测。下面是每种方式的详细实现和配置示例：</p><h4 id=1-http-get-请求>1. <strong>HTTP GET 请求</strong>
<a class=anchor href=#1-http-get-%e8%af%b7%e6%b1%82>#</a></h4><p>Kubernetes 可以通过发送 HTTP 请求到容器内部的某个端口（通常是容器的健康检查端点）来检查容器的就绪状态。若请求成功（返回状态码 200-399），则认为容器已准备好接收流量。</p><p><strong>示例：</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Pod</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>example-pod</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>containers</span>:
</span></span><span style=display:flex><span>  - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>example-container</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>image</span>: <span style=color:#ae81ff>example-image</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>readinessProbe</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>httpGet</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>path</span>: <span style=color:#ae81ff>/healthz</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>port</span>: <span style=color:#ae81ff>8080</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>initialDelaySeconds</span>: <span style=color:#ae81ff>5</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>periodSeconds</span>: <span style=color:#ae81ff>10</span>
</span></span></code></pre></div><ul><li><code>path</code>: 健康检查的 HTTP 路径。</li><li><code>port</code>: 用于检查的端口。</li><li><code>initialDelaySeconds</code>: 容器启动后的延迟时间，在此时间内就绪探针不会被触发。</li><li><code>periodSeconds</code>: 探针执行的时间间隔。</li></ul><h4 id=2-tcp-socket-检查>2. <strong>TCP Socket 检查</strong>
<a class=anchor href=#2-tcp-socket-%e6%a3%80%e6%9f%a5>#</a></h4><p>如果容器内的应用没有 HTTP 服务，或者你只需要检查某个端口是否开放，可以使用 TCP Socket 检查。Kubernetes 会尝试与容器的某个端口建立连接，如果能够成功连接，则认为容器是就绪的。</p><p><strong>示例：</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Pod</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>example-pod</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>containers</span>:
</span></span><span style=display:flex><span>  - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>example-container</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>image</span>: <span style=color:#ae81ff>example-image</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>readinessProbe</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>tcpSocket</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>port</span>: <span style=color:#ae81ff>8080</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>initialDelaySeconds</span>: <span style=color:#ae81ff>5</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>periodSeconds</span>: <span style=color:#ae81ff>10</span>
</span></span></code></pre></div><ul><li><code>tcpSocket</code>: 选择容器的端口进行连接检查。</li></ul><h4 id=3-执行命令检查>3. <strong>执行命令检查</strong>
<a class=anchor href=#3-%e6%89%a7%e8%a1%8c%e5%91%bd%e4%bb%a4%e6%a3%80%e6%9f%a5>#</a></h4><p>你还可以通过在容器内部执行命令来检查容器的就绪状态。命令返回的退出代码决定了容器是否就绪，<code>0</code> 表示成功，其他值表示失败。</p><p><strong>示例：</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Pod</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>example-pod</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>containers</span>:
</span></span><span style=display:flex><span>  - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>example-container</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>image</span>: <span style=color:#ae81ff>example-image</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>readinessProbe</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>exec</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>command</span>:
</span></span><span style=display:flex><span>        - <span style=color:#e6db74>&#34;cat&#34;</span>
</span></span><span style=display:flex><span>        - <span style=color:#e6db74>&#34;/tmp/readyfile&#34;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>initialDelaySeconds</span>: <span style=color:#ae81ff>5</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>periodSeconds</span>: <span style=color:#ae81ff>10</span>
</span></span></code></pre></div><ul><li><code>exec.command</code>: 容器内执行的命令。</li></ul><h3 id=如何保证容器是就绪的>如何保证容器是就绪的？
<a class=anchor href=#%e5%a6%82%e4%bd%95%e4%bf%9d%e8%af%81%e5%ae%b9%e5%99%a8%e6%98%af%e5%b0%b1%e7%bb%aa%e7%9a%84>#</a></h3><p>容器的就绪检测是通过健康检查端点（如 HTTP、TCP 或命令）来确定容器是否能够处理流量的。如果容器在检查过程中无法成功响应，则 Kubernetes 会认为容器不可用，不会向其分配流量。就绪检测的实现本质上取决于应用程序的状态：确保容器内部的服务或进程在就绪时能够响应探针。</p><ol><li><strong>应用程序支持健康检查端点</strong>：确保容器内的应用程序具有健康检查端点，如 <code>/healthz</code> 或类似的服务端口。这个端点应该在应用准备好时返回一个 2xx 的 HTTP 状态码，或者是 TCP 连接正常，或者命令执行成功。</li><li><strong>适当配置探针的 <code>initialDelaySeconds</code> 和 <code>periodSeconds</code></strong>：<code>initialDelaySeconds</code> 用来指定容器启动后的初始延迟时间，<code>periodSeconds</code> 用来指定健康检查的周期。要确保这些配置值合理，以便容器在完全启动后才能接收流量。</li><li><strong>确保容器能够正确处理探针</strong>：就绪检测探针本质上是用来检查容器是否有能力处理流量。因此，确保应用程序在完全启动后能正确响应健康检查请求，比如数据库连接完成、必要的外部服务已经就绪等。</li><li><strong>合理配置容器启动顺序</strong>：在多容器 Pod 中，合理配置容器启动顺序，确保应用依赖的服务先启动。</li></ol><h3 id=小结>小结
<a class=anchor href=#%e5%b0%8f%e7%bb%93>#</a></h3><p>就绪检测确保容器在能够响应流量之前，不会被负载均衡器或其他流量调度机制所调用。这可以有效避免请求被发送到尚未完全初始化或启动的容器，导致错误响应或请求失败。通过合理配置就绪探针，可以确保容器的健康状态被准确监测，提升服务的稳定性和可靠性。</p><h1 id=就绪检测怎么实现判断什么东西>就绪检测怎么实现？判断什么东西？
<a class=anchor href=#%e5%b0%b1%e7%bb%aa%e6%a3%80%e6%b5%8b%e6%80%8e%e4%b9%88%e5%ae%9e%e7%8e%b0%e5%88%a4%e6%96%ad%e4%bb%80%e4%b9%88%e4%b8%9c%e8%a5%bf>#</a></h1><p><strong>就绪检测（Readiness Probe）</strong> 是 Kubernetes 中用于确定一个容器是否准备好接收流量的一种机制。它帮助 Kubernetes 确保容器在启动完成并准备好处理请求后才会开始接收流量。就绪检测通常用来判断应用程序是否已经完全初始化，可以提供服务。</p><h3 id=就绪检测判断的内容>就绪检测判断的内容：
<a class=anchor href=#%e5%b0%b1%e7%bb%aa%e6%a3%80%e6%b5%8b%e5%88%a4%e6%96%ad%e7%9a%84%e5%86%85%e5%ae%b9>#</a></h3><ol><li><strong>应用是否初始化完毕</strong>：<ul><li>对于大多数应用，启动过程可能需要一些时间，如加载配置文件、建立数据库连接或其他服务依赖。就绪检测可以确保应用在完全准备好后才开始接收请求。</li></ul></li><li><strong>外部依赖是否可用</strong>：<ul><li>有些容器可能依赖其他服务（如数据库、消息队列等）。就绪检测可以用来确认这些外部依赖是否可用，确保容器能够正常与其他服务交互。</li></ul></li><li><strong>容器是否健康</strong>：<ul><li>如果容器内的服务或进程出现异常，容器可能无法处理请求。通过就绪检测，可以及时发现并避免流量发送到不健康的容器上。</li></ul></li></ol><h3 id=如何实现就绪检测-1>如何实现就绪检测？
<a class=anchor href=#%e5%a6%82%e4%bd%95%e5%ae%9e%e7%8e%b0%e5%b0%b1%e7%bb%aa%e6%a3%80%e6%b5%8b-1>#</a></h3><p>Kubernetes 提供了几种方式来实现就绪检测，具体可以根据应用程序的需求来选择。</p><h4 id=1-http-get-请求-1>1. <strong>HTTP GET 请求</strong>：
<a class=anchor href=#1-http-get-%e8%af%b7%e6%b1%82-1>#</a></h4><p>如果容器内运行的是 Web 服务，可以配置 Kubernetes 通过 HTTP 请求访问容器内的某个 URL 来判断容器是否就绪。容器需要提供一个健康检查的 HTTP 端点（如 <code>/healthz</code> 或 <code>/readiness</code>），当该端点返回 2xx 状态码时，表示容器就绪。</p><p><strong>示例：</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>readinessProbe</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>httpGet</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>path</span>: <span style=color:#ae81ff>/readiness</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>port</span>: <span style=color:#ae81ff>8080</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>initialDelaySeconds</span>: <span style=color:#ae81ff>5</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>periodSeconds</span>: <span style=color:#ae81ff>10</span>
</span></span></code></pre></div><ul><li><code>path</code>：健康检查的 URL 路径，容器内提供的健康检查端点。</li><li><code>port</code>：健康检查访问的端口。</li><li><code>initialDelaySeconds</code>：容器启动后等待多长时间再开始进行就绪检查。</li><li><code>periodSeconds</code>：检查的周期，即多久进行一次检测。</li></ul><h4 id=2-tcp-socket-检查-1>2. <strong>TCP Socket 检查</strong>：
<a class=anchor href=#2-tcp-socket-%e6%a3%80%e6%9f%a5-1>#</a></h4><p>如果容器内没有 HTTP 服务，或者你只需要检查某个端口是否可用，可以使用 TCP Socket 检查。Kubernetes 将尝试与容器的某个端口建立连接。如果连接成功，则认为容器就绪。</p><p><strong>示例：</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>readinessProbe</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>tcpSocket</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>port</span>: <span style=color:#ae81ff>8080</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>initialDelaySeconds</span>: <span style=color:#ae81ff>5</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>periodSeconds</span>: <span style=color:#ae81ff>10</span>
</span></span></code></pre></div><ul><li><code>tcpSocket</code>：指定容器内的端口进行连接检查。</li></ul><h4 id=3-执行命令检查-1>3. <strong>执行命令检查</strong>：
<a class=anchor href=#3-%e6%89%a7%e8%a1%8c%e5%91%bd%e4%bb%a4%e6%a3%80%e6%9f%a5-1>#</a></h4><p>如果容器内没有 HTTP 或 TCP 服务，或者需要更复杂的就绪判断，可以通过执行命令来检查容器的状态。命令成功返回（退出码为 0）表示容器就绪。</p><p><strong>示例：</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>readinessProbe</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>exec</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>command</span>:
</span></span><span style=display:flex><span>    - <span style=color:#e6db74>&#34;cat&#34;</span>
</span></span><span style=display:flex><span>    - <span style=color:#e6db74>&#34;/tmp/readyfile&#34;</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>initialDelaySeconds</span>: <span style=color:#ae81ff>5</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>periodSeconds</span>: <span style=color:#ae81ff>10</span>
</span></span></code></pre></div><ul><li><code>exec.command</code>：容器内要执行的命令。如果命令成功执行并返回 0，则认为容器就绪。</li></ul><h3 id=判断什么东西>判断什么东西？
<a class=anchor href=#%e5%88%a4%e6%96%ad%e4%bb%80%e4%b9%88%e4%b8%9c%e8%a5%bf>#</a></h3><ol><li><strong>应用启动是否完成</strong>：<ul><li>应用程序的启动通常需要一些时间，可能包括加载配置文件、初始化数据库连接等操作。就绪探针用来判断这些操作是否完成，确保应用程序已完全准备好提供服务。</li></ul></li><li><strong>服务依赖是否准备好</strong>：<ul><li>如果容器依赖于其他服务（如数据库、缓存服务等），就绪检测可以确保这些外部依赖已准备就绪。例如，检查数据库是否已连接，或外部 API 是否可用。</li></ul></li><li><strong>容器内的服务是否正常运行</strong>：<ul><li>容器内的服务是否正在正常运行是就绪检测的一个关键点。如果容器内部的应用程序无法响应请求（如崩溃、挂起等），就绪检测可以及时发现这一点，防止流量被发送到不健康的容器。</li></ul></li></ol><h3 id=就绪检测如何工作>就绪检测如何工作？
<a class=anchor href=#%e5%b0%b1%e7%bb%aa%e6%a3%80%e6%b5%8b%e5%a6%82%e4%bd%95%e5%b7%a5%e4%bd%9c>#</a></h3><ol><li>Kubernetes 会定期根据配置的 <code>readinessProbe</code> 来执行检查（HTTP GET、TCP Socket 或命令）。</li><li>如果探针的检查结果为成功（即返回 2xx 状态码、端口开放或命令成功执行），则认为容器就绪，Kubernetes 会将流量路由到该容器。</li><li>如果检查失败，Kubernetes 会将该容器标记为不就绪，不会将流量发送到该容器，直到就绪检测恢复为成功。</li></ol><h3 id=适用场景>适用场景：
<a class=anchor href=#%e9%80%82%e7%94%a8%e5%9c%ba%e6%99%af>#</a></h3><ul><li><strong>容器需要一些时间来启动并初始化</strong>：对于需要初始化外部依赖或执行一些复杂操作的应用，使用就绪探针可以确保容器只在完全准备好时才接受流量。</li><li><strong>容器内应用的状态有多种变化</strong>：例如，某些服务可能会因为负载过高或维护而临时不可用，使用就绪探针可以确保只在服务恢复时才接收流量。</li><li><strong>多容器 Pod 中的容器依赖</strong>：在多个容器的 Pod 中，如果某些容器依赖于其他容器（例如共享存储、数据库服务），就绪探针可以确保容器在其他依赖容器就绪后才开始工作。</li></ul><h3 id=小结-1>小结：
<a class=anchor href=#%e5%b0%8f%e7%bb%93-1>#</a></h3><p>就绪检测主要是用来判断容器是否完全准备好接收流量，避免请求被发送到未准备好的容器中。通过 HTTP 请求、TCP Socket 或命令执行的方式，Kubernetes 能有效地确保容器在具备处理能力时才接收请求，提高系统的稳定性和可靠性。</p><h1 id=一个应用存活检测检测他的端口是不是通的在端口通的情况下为什么这个应用还没有就绪和还有哪些因素导致他没有就绪>一个应用，存活检测，检测他的端口是不是通的，在端口通的情况下，为什么这个应用还没有就绪，和还有哪些因素导致他没有就绪？
<a class=anchor href=#%e4%b8%80%e4%b8%aa%e5%ba%94%e7%94%a8%e5%ad%98%e6%b4%bb%e6%a3%80%e6%b5%8b%e6%a3%80%e6%b5%8b%e4%bb%96%e7%9a%84%e7%ab%af%e5%8f%a3%e6%98%af%e4%b8%8d%e6%98%af%e9%80%9a%e7%9a%84%e5%9c%a8%e7%ab%af%e5%8f%a3%e9%80%9a%e7%9a%84%e6%83%85%e5%86%b5%e4%b8%8b%e4%b8%ba%e4%bb%80%e4%b9%88%e8%bf%99%e4%b8%aa%e5%ba%94%e7%94%a8%e8%bf%98%e6%b2%a1%e6%9c%89%e5%b0%b1%e7%bb%aa%e5%92%8c%e8%bf%98%e6%9c%89%e5%93%aa%e4%ba%9b%e5%9b%a0%e7%b4%a0%e5%af%bc%e8%87%b4%e4%bb%96%e6%b2%a1%e6%9c%89%e5%b0%b1%e7%bb%aa>#</a></h1><p>一个应用即使在 <strong>存活检测</strong> 中通过了端口是否开放的检查，依然可能没有就绪。这是因为存活检测和就绪检测的关注点不同，存活检测只是确保容器存活并能够响应基本的请求，而就绪检测则关注容器是否真正准备好处理业务流量。</p><h3 id=存活检测与就绪检测的区别>存活检测与就绪检测的区别：
<a class=anchor href=#%e5%ad%98%e6%b4%bb%e6%a3%80%e6%b5%8b%e4%b8%8e%e5%b0%b1%e7%bb%aa%e6%a3%80%e6%b5%8b%e7%9a%84%e5%8c%ba%e5%88%ab>#</a></h3><ul><li><strong>存活检测（Liveness Probe）</strong>：检查容器是否活着并能正常响应请求。如果存活探针失败，Kubernetes 会重启容器。</li><li><strong>就绪检测（Readiness Probe）</strong>：检查容器是否准备好接收流量。如果就绪探针失败，Kubernetes 会停止将流量发送到该容器，直到它恢复就绪状态。</li></ul><h3 id=即使端口通了应用可能仍未就绪的原因>即使端口通了，应用可能仍未就绪的原因：
<a class=anchor href=#%e5%8d%b3%e4%bd%bf%e7%ab%af%e5%8f%a3%e9%80%9a%e4%ba%86%e5%ba%94%e7%94%a8%e5%8f%af%e8%83%bd%e4%bb%8d%e6%9c%aa%e5%b0%b1%e7%bb%aa%e7%9a%84%e5%8e%9f%e5%9b%a0>#</a></h3><ol><li><strong>应用启动未完成</strong>：<ul><li>即使端口开放，应用程序的内部逻辑（如加载配置文件、初始化数据库连接、加载缓存等）可能还没有完全完成。此时应用虽然能响应网络请求，但并不具备处理业务流量的能力。</li></ul></li><li><strong>依赖服务未就绪</strong>：<ul><li>应用可能依赖外部服务（如数据库、消息队列、其他微服务等），这些服务在容器启动时还未准备好。此时虽然应用的端口已开放，但它可能无法提供实际的业务功能，导致其不就绪。</li></ul></li><li><strong>容器内的进程或线程未完全启动</strong>：<ul><li>某些应用可能需要多个进程或线程才能启动并提供服务。如果这些进程或线程尚未启动完毕，容器虽然已可以访问端口，但应用尚未准备好提供正常的服务。</li></ul></li><li><strong>资源不足（如内存、CPU 等）</strong>：<ul><li>容器的资源（如内存、CPU）可能不足，导致应用的某些功能无法正常启动或运行。虽然容器的端口已经开放，但如果资源不足，应用可能无法完成所有初始化任务，因此不能视为就绪。</li></ul></li><li><strong>应用内部异常或错误</strong>：<ul><li>应用内部可能发生了一些异常，虽然服务端口开放，外部请求可以进入容器，但应用的核心功能仍然未准备好。比如，应用可能在启动过程中出现了错误，导致某些重要的功能尚未启动，不能正确响应业务请求。</li></ul></li><li><strong>缓慢的外部依赖初始化</strong>：<ul><li>即使端口开放，某些外部依赖（例如外部数据库、第三方 API 等）的初始化可能较慢。就绪探针通常会检查这些外部依赖的可用性，确保所有依赖服务都能正常工作后，容器才会被标记为就绪。</li></ul></li></ol><h3 id=举例说明>举例说明：
<a class=anchor href=#%e4%b8%be%e4%be%8b%e8%af%b4%e6%98%8e>#</a></h3><p>假设你的应用是一个 Web 应用，它绑定了某个端口 8080 并返回 200 OK，但应用的数据库连接尚未建立。如果你只使用存活检测检查端口是否通，容器会被认为是活的，存活探针会返回成功。但如果你使用就绪探针来检测数据库连接是否可用，容器将被认为不就绪，因为数据库尚未连接成功。</p><h3 id=可能导致容器未就绪的其他因素>可能导致容器未就绪的其他因素：
<a class=anchor href=#%e5%8f%af%e8%83%bd%e5%af%bc%e8%87%b4%e5%ae%b9%e5%99%a8%e6%9c%aa%e5%b0%b1%e7%bb%aa%e7%9a%84%e5%85%b6%e4%bb%96%e5%9b%a0%e7%b4%a0>#</a></h3><ul><li><strong>网络延迟或配置错误</strong>：<ul><li>容器内外部网络配置可能会导致应用的依赖服务无法正常连接，导致就绪探针失败。</li></ul></li><li><strong>文件或资源未加载</strong>：<ul><li>如果应用依赖某些文件、资源或配置文件，且这些文件尚未加载或有错误，应用可能无法正确处理请求，导致不就绪。</li></ul></li><li><strong>初始化任务需要时间</strong>：<ul><li>某些应用需要执行长时间的初始化任务，如数据迁移、缓存预热等。尽管端口开放，但这些任务未完成时，应用也不应接收流量。</li></ul></li></ul><h3 id=小结-2>小结：
<a class=anchor href=#%e5%b0%8f%e7%bb%93-2>#</a></h3><p>虽然容器的端口开放，可以通过存活检测确保容器是否存活，但这并不意味着容器已经准备好处理请求。<strong>就绪探针</strong> 通过检测容器是否能够提供业务功能（如外部依赖是否可用、应用是否初始化完成等），确保只有当应用真正准备好时，才会将流量路由到容器。</p><h1 id=了解hpa吗>了解HPA吗？
<a class=anchor href=#%e4%ba%86%e8%a7%a3hpa%e5%90%97>#</a></h1><p>是的，<strong>HPA</strong>（Horizontal Pod Autoscaler）是 Kubernetes 中的一个重要组件，用于自动调整 Pod 的副本数量，以应对应用的负载变化。</p><h3 id=hpa-的工作原理><strong>HPA 的工作原理</strong>：
<a class=anchor href=#hpa-%e7%9a%84%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86>#</a></h3><p>HPA 会根据设定的指标（比如 CPU 使用率、内存使用率或自定义的指标）自动调整 Pod 的副本数。通过 HPA，Kubernetes 可以根据工作负载的需要动态地增加或减少 Pod 数量，从而实现负载均衡和资源优化。</p><h3 id=hpa-如何工作><strong>HPA 如何工作</strong>：
<a class=anchor href=#hpa-%e5%a6%82%e4%bd%95%e5%b7%a5%e4%bd%9c>#</a></h3><ol><li><strong>监控指标</strong>： HPA 通过监控各个 Pod 的资源使用情况（如 CPU、内存等），并与设定的目标指标进行比较。如果当前指标超过目标阈值，则 HPA 会增加副本数，反之，如果指标低于目标阈值，则会减少副本数。</li><li><strong>控制器的作用</strong>： HPA 会在每个周期（通常为 30 秒）评估当前指标，并根据这些评估来决定是否需要调整副本数量。HPA 通过与 Kubernetes API 的 <strong>ReplicaSet</strong> 或 <strong>Deployment</strong> 控制器交互来增加或减少 Pod 的副本数。</li><li><strong>指标源</strong>：<ul><li><strong>默认指标</strong>：HPA 支持基于 <strong>CPU 使用率</strong> 和 <strong>内存使用率</strong> 的自动扩缩容。</li><li><strong>自定义指标</strong>：除了默认的 CPU 和内存，HPA 还可以结合其他指标（如 HTTP 请求数、队列长度等）来进行自动扩展。使用自定义指标通常需要结合 <strong>Prometheus</strong> 等监控系统。</li></ul></li><li><strong>目标值</strong>： 在 HPA 中，你可以设置一个目标值（如 CPU 使用率的 50%）。HPA 会根据当前的资源使用情况和目标值来调整 Pod 的副本数量。</li></ol><h3 id=配置-hpa><strong>配置 HPA</strong>：
<a class=anchor href=#%e9%85%8d%e7%bd%ae-hpa>#</a></h3><p>创建 HPA 时，需要指定以下参数：</p><ul><li><strong>目标指标</strong>（如 CPU 使用率、内存使用率或自定义指标）。</li><li><strong>最小副本数</strong>：当负载很低时，Pod 的副本数最少。</li><li><strong>最大副本数</strong>：当负载很高时，Pod 的副本数最多。</li></ul><h4 id=示例-hpa-配置>示例 HPA 配置：
<a class=anchor href=#%e7%a4%ba%e4%be%8b-hpa-%e9%85%8d%e7%bd%ae>#</a></h4><p>假设我们有一个基于 CPU 使用率的 HPA 配置，自动调整副本数。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>autoscaling/v2</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>HorizontalPodAutoscaler</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>example-hpa</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>default</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>scaleTargetRef</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>apps/v1</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Deployment</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>name</span>: <span style=color:#ae81ff>example-deployment</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>minReplicas</span>: <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>maxReplicas</span>: <span style=color:#ae81ff>10</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>metrics</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>type</span>: <span style=color:#ae81ff>Resource</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>resource</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>name</span>: <span style=color:#ae81ff>cpu</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>target</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>type</span>: <span style=color:#ae81ff>Utilization</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>averageUtilization</span>: <span style=color:#ae81ff>50</span>  <span style=color:#75715e># 目标 CPU 使用率 50%</span>
</span></span></code></pre></div><p>在这个示例中：</p><ul><li>HPA 会监控 <code>example-deployment</code> 的 CPU 使用率。</li><li>当 CPU 使用率超过 50% 时，它会增加 Pod 副本数。</li><li>如果 CPU 使用率降至低于 50%，则减少副本数。</li><li>副本数最小为 2，最大为 10。</li></ul><h3 id=优点-2><strong>优点</strong>：
<a class=anchor href=#%e4%bc%98%e7%82%b9-2>#</a></h3><ol><li><strong>自动化扩缩容</strong>：通过自动增加或减少 Pod 副本数，可以确保应用在负载变化时能够持续稳定运行。</li><li><strong>节省资源</strong>：当负载低时，HPA 会减少 Pod 副本数，从而节省资源，避免不必要的开销。</li><li><strong>应对流量波动</strong>：可以根据流量的波动自动调整资源，适应不同的工作负载需求。</li></ol><h3 id=适用场景-1><strong>适用场景</strong>：
<a class=anchor href=#%e9%80%82%e7%94%a8%e5%9c%ba%e6%99%af-1>#</a></h3><ul><li><strong>负载不均衡</strong>：当应用的负载波动较大，HPA 可以帮助自动扩展或缩减 Pod 数量，以平衡负载。</li><li><strong>大流量网站</strong>：例如电商网站，可能在高峰期有大量用户请求，HPA 可以自动增加 Pod 数量应对高负载。</li><li><strong>按需缩容</strong>：当负载较低时，HPA 会减少 Pod 数量，避免资源浪费。</li></ul><h3 id=缺点-2><strong>缺点</strong>：
<a class=anchor href=#%e7%bc%ba%e7%82%b9-2>#</a></h3><ol><li><strong>延迟响应</strong>：HPA 需要一定的时间来评估负载并做出反应，因此可能存在轻微的延迟，特别是在负载变化迅速的情况下。</li><li><strong>单一指标依赖</strong>：默认的 HPA 主要依赖 CPU 或内存作为扩容的指标，可能无法适应一些复杂的负载需求。虽然可以使用自定义指标，但配置较为复杂。</li><li><strong>资源限制</strong>：HPA 可能因为资源限制（如 API 请求限制、监控指标频率等）而无法进行及时的扩容或缩容。</li></ol><h3 id=总结-4><strong>总结</strong>：
<a class=anchor href=#%e6%80%bb%e7%bb%93-4>#</a></h3><p>HPA 是 Kubernetes 中非常有用的特性，可以自动根据负载调节容器的副本数，从而确保应用在不同的负载条件下都能正常运行。通过合理配置，能够实现资源的动态分配，有效提高系统的可靠性和效率。</p><h1 id=java应用-hpa配置配置哪些指标作为扩缩容指标>java应用 HPA配置，配置哪些指标作为扩缩容指标？
<a class=anchor href=#java%e5%ba%94%e7%94%a8-hpa%e9%85%8d%e7%bd%ae%e9%85%8d%e7%bd%ae%e5%93%aa%e4%ba%9b%e6%8c%87%e6%a0%87%e4%bd%9c%e4%b8%ba%e6%89%a9%e7%bc%a9%e5%ae%b9%e6%8c%87%e6%a0%87>#</a></h1><p>在 Java 应用中，使用 Kubernetes 的 Horizontal Pod Autoscaler（HPA）进行扩缩容时，通常会根据不同的指标来调整 Pod 的副本数。对于 Java 应用，常见的扩缩容指标包括 <strong>CPU 使用率</strong>、<strong>内存使用率</strong>，以及<strong>自定义指标</strong>。以下是具体的配置和常见的扩缩容指标：</p><h3 id=1-cpu-使用率>1. <strong>CPU 使用率</strong>：
<a class=anchor href=#1-cpu-%e4%bd%bf%e7%94%a8%e7%8e%87>#</a></h3><p>这是 HPA 中最常用的默认指标之一。Java 应用通常会占用一定的 CPU 资源，HPA 会根据 CPU 的使用情况来自动增加或减少 Pod 副本数。</p><ul><li><strong>适用场景</strong>：当应用的负载增加时，CPU 使用率通常会增高，HPA 可以通过增加 Pod 数量来分摊负载。</li><li><strong>如何配置</strong>： 在 <code>HorizontalPodAutoscaler</code> 的配置中，指定 <code>cpu</code> 为扩容指标，并设置 <code>targetUtilization</code>（目标使用率）。</li></ul><h4 id=示例>示例：
<a class=anchor href=#%e7%a4%ba%e4%be%8b>#</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>autoscaling/v2</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>HorizontalPodAutoscaler</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>java-app-hpa</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>default</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>scaleTargetRef</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>apps/v1</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Deployment</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>name</span>: <span style=color:#ae81ff>java-app-deployment</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>minReplicas</span>: <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>maxReplicas</span>: <span style=color:#ae81ff>10</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>metrics</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>type</span>: <span style=color:#ae81ff>Resource</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>resource</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>name</span>: <span style=color:#ae81ff>cpu</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>target</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>type</span>: <span style=color:#ae81ff>Utilization</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>averageUtilization</span>: <span style=color:#ae81ff>50</span>  <span style=color:#75715e># 目标 CPU 使用率 50%</span>
</span></span></code></pre></div><p>在这个示例中：</p><ul><li>HPA 将监控 <code>java-app-deployment</code> 的 CPU 使用率。</li><li>当 CPU 使用率超过 50% 时，HPA 会增加 Pod 副本数，反之则减少副本数。</li></ul><h3 id=2-内存使用率>2. <strong>内存使用率</strong>：
<a class=anchor href=#2-%e5%86%85%e5%ad%98%e4%bd%bf%e7%94%a8%e7%8e%87>#</a></h3><p>内存使用率是另一个常用的扩缩容指标。对于 Java 应用，如果内存占用过高（比如频繁发生 GC 或内存泄漏等问题），可以使用内存使用率作为扩容或缩容的依据。</p><ul><li><strong>适用场景</strong>：当应用的内存占用增加，可能会影响性能（例如，频繁的垃圾回收），此时可以通过增加 Pod 数量来分担内存负载。</li><li><strong>如何配置</strong>： 类似于 CPU，内存也是可以作为资源指标进行扩缩容。</li></ul><h4 id=示例-1>示例：
<a class=anchor href=#%e7%a4%ba%e4%be%8b-1>#</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>autoscaling/v2</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>HorizontalPodAutoscaler</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>java-app-hpa</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>default</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>scaleTargetRef</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>apps/v1</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Deployment</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>name</span>: <span style=color:#ae81ff>java-app-deployment</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>minReplicas</span>: <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>maxReplicas</span>: <span style=color:#ae81ff>10</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>metrics</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>type</span>: <span style=color:#ae81ff>Resource</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>resource</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>name</span>: <span style=color:#ae81ff>memory</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>target</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>type</span>: <span style=color:#ae81ff>Utilization</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>averageUtilization</span>: <span style=color:#ae81ff>80</span>  <span style=color:#75715e># 目标内存使用率 80%</span>
</span></span></code></pre></div><p>这个配置将使 HPA 根据 Java 应用的内存使用情况来扩展或缩减 Pod 副本数。</p><h3 id=3-自定义指标>3. <strong>自定义指标</strong>：
<a class=anchor href=#3-%e8%87%aa%e5%ae%9a%e4%b9%89%e6%8c%87%e6%a0%87>#</a></h3><p>除了 CPU 和内存，Java 应用的扩缩容还可以根据自定义指标来进行。例如：</p><ul><li><strong>请求数</strong>（HTTP 请求数、API 请求数等）。</li><li><strong>响应时间</strong>（请求的响应时间）。</li><li><strong>队列长度</strong>（消息队列或任务队列的长度）。</li><li><strong>线程池使用情况</strong>（线程池的活跃线程数）。</li></ul><p>在实际应用中，可能需要通过 <strong>Prometheus</strong> 或其他监控系统收集这些自定义指标，并结合 HPA 进行扩缩容。常见做法是通过 Prometheus adapter 来提供自定义指标。</p><h4 id=示例-2>示例：
<a class=anchor href=#%e7%a4%ba%e4%be%8b-2>#</a></h4><p>假设我们想根据每秒请求数 (<code>http_requests_per_second</code>) 来扩展 Java 应用的 Pod 数量，配置可以如下：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>autoscaling/v2</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>HorizontalPodAutoscaler</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>java-app-hpa</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>default</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>scaleTargetRef</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>apps/v1</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Deployment</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>name</span>: <span style=color:#ae81ff>java-app-deployment</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>minReplicas</span>: <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>maxReplicas</span>: <span style=color:#ae81ff>10</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>metrics</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>type</span>: <span style=color:#ae81ff>External</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>external</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>metric</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>name</span>: <span style=color:#ae81ff>http_requests_per_second</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>target</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>type</span>: <span style=color:#ae81ff>AverageValue</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>averageValue</span>: <span style=color:#ae81ff>1000</span>  <span style=color:#75715e># 每秒 1000 个请求</span>
</span></span></code></pre></div><h3 id=4-目标指标配置>4. <strong>目标指标配置</strong>：
<a class=anchor href=#4-%e7%9b%ae%e6%a0%87%e6%8c%87%e6%a0%87%e9%85%8d%e7%bd%ae>#</a></h3><ul><li><strong>平均使用率</strong>（AverageUtilization）：表示目标资源的平均使用率，通常用于 CPU 和内存的自动扩缩容。</li><li><strong>平均值</strong>（AverageValue）：用于自定义指标，表示目标指标的平均值。</li></ul><h3 id=如何收集自定义指标><strong>如何收集自定义指标</strong>：
<a class=anchor href=#%e5%a6%82%e4%bd%95%e6%94%b6%e9%9b%86%e8%87%aa%e5%ae%9a%e4%b9%89%e6%8c%87%e6%a0%87>#</a></h3><ol><li>使用 <strong>Prometheus</strong> 作为监控工具，结合 Prometheus Adapter 将自定义指标暴露给 HPA。</li><li>使用 <strong>Kubernetes Metrics Server</strong> 收集系统级别的指标（如 CPU、内存）。</li><li>可以通过 <strong>custom-metrics-apiserver</strong> 扩展 Kubernetes 以支持更多自定义指标。</li></ol><h3 id=总结-5><strong>总结</strong>：
<a class=anchor href=#%e6%80%bb%e7%bb%93-5>#</a></h3><p>对于 Java 应用的 HPA 配置，可以使用以下几种主要指标：</p><ul><li><strong>CPU 使用率</strong>（默认指标）。</li><li><strong>内存使用率</strong>（适用于内存占用较高的应用）。</li><li><strong>自定义指标</strong>（如 HTTP 请求数、响应时间、队列长度、线程池等）。</li></ul><p>根据应用的具体需求和负载特征，选择适合的指标来进行扩缩容配置，以确保 Java 应用能够根据实际负载自动调整资源，保持系统的稳定性和性能。</p><h1 id=使用内存有没有什么问题>使用内存有没有什么问题？
<a class=anchor href=#%e4%bd%bf%e7%94%a8%e5%86%85%e5%ad%98%e6%9c%89%e6%b2%a1%e6%9c%89%e4%bb%80%e4%b9%88%e9%97%ae%e9%a2%98>#</a></h1><p>使用内存作为 Kubernetes Horizontal Pod Autoscaler (HPA) 的扩缩容指标时，虽然它是一个有用的指标，但也存在一些潜在的问题和挑战。以下是使用内存作为扩缩容指标时可能遇到的一些问题：</p><h3 id=1-内存泄漏问题>1. <strong>内存泄漏问题</strong>：
<a class=anchor href=#1-%e5%86%85%e5%ad%98%e6%b3%84%e6%bc%8f%e9%97%ae%e9%a2%98>#</a></h3><p>Java 应用中，内存泄漏可能导致内存持续增长，即使应用负载并未增加。这种情况下，HPA 会看到内存使用率持续增加，并可能触发扩容，但实际上应用并未真正需要更多资源。</p><ul><li><strong>问题</strong>：HPA 会基于内存的使用情况扩容，但内存的增加并非因为负载增加，而是可能由于内存泄漏等问题。这可能导致应用不必要地增加 Pod 数量，从而浪费资源。</li><li><strong>解决方案</strong>：监控应用的健康状态，确保内存增长是由于负载增加而非内存泄漏。可以结合 <strong>存活探针</strong> 来检测是否存在内存泄漏问题。</li></ul><h3 id=2-内存使用高峰>2. <strong>内存使用高峰</strong>：
<a class=anchor href=#2-%e5%86%85%e5%ad%98%e4%bd%bf%e7%94%a8%e9%ab%98%e5%b3%b0>#</a></h3><p>Java 应用的内存使用高峰通常出现在垃圾回收（GC）期间，尤其是在大内存分配和处理大量数据时。GC 会暂时消耗大量内存，但并不意味着需要扩容。</p><ul><li><strong>问题</strong>：HPA 可能会错误地扩容，因为它可能会检测到 GC 导致的内存峰值。随着内存占用达到阈值，HPA 可能会增加 Pod 数量，导致不必要的扩容。</li><li><strong>解决方案</strong>：可以调整内存阈值，使其适应 Java 应用的内存波动。此外，可以使用 <strong><code>targetAverageUtilization</code></strong> 来指定合理的内存使用率目标，并考虑添加 <strong>GC 时间</strong> 或 <strong>线程池监控</strong> 作为指标来减小影响。</li></ul><h3 id=3-内存配置不当>3. <strong>内存配置不当</strong>：
<a class=anchor href=#3-%e5%86%85%e5%ad%98%e9%85%8d%e7%bd%ae%e4%b8%8d%e5%bd%93>#</a></h3><p>Java 应用在 Kubernetes 中运行时，容器的内存配置非常重要。如果 <code>requests.memory</code> 和 <code>limits.memory</code> 配置不当，可能导致 HPA 在应用的内存需求发生波动时进行频繁的扩缩容。</p><ul><li><strong>问题</strong>：如果内存 <code>requests</code> 设置得过低，可能导致容器在负载增加时因为内存不足而被 OOM (Out Of Memory) 杀死；而如果设置过高，可能会导致资源浪费。</li><li><strong>解决方案</strong>：合理配置容器的内存 <code>requests</code> 和 <code>limits</code>，确保它们既不浪费资源，也能满足负载需求。对内存需求进行基准测试，确保配置合理。</li></ul><h3 id=4-java-应用的内存管理>4. <strong>Java 应用的内存管理</strong>：
<a class=anchor href=#4-java-%e5%ba%94%e7%94%a8%e7%9a%84%e5%86%85%e5%ad%98%e7%ae%a1%e7%90%86>#</a></h3><p>Java 应用的内存管理是由 Java 虚拟机（JVM）负责的，可能会存在一定的延迟，特别是在垃圾回收发生时，内存的使用率可能会突然飙升。</p><ul><li><p><strong>问题</strong>：内存的使用率可能会出现突发波动（例如，GC 后使用的内存急剧增加），这可能导致 HPA 根据瞬时的内存波动做出扩容决策，从而引发不必要的扩容或缩容。</p></li><li><p>解决方案</p><p>：可以考虑为 Java 应用配置合理的</p><p>JVM 参数</p><p>，例如：</p><ul><li><code>-XX:MaxRAMPercentage</code>（控制 JVM 最大可用内存比例）。</li><li><code>-XX:InitialRAMPercentage</code>（控制 JVM 启动时的内存分配）。 这些配置可以帮助避免频繁的内存波动。</li></ul></li></ul><h3 id=5-内存与-cpu-之间的关系>5. <strong>内存与 CPU 之间的关系</strong>：
<a class=anchor href=#5-%e5%86%85%e5%ad%98%e4%b8%8e-cpu-%e4%b9%8b%e9%97%b4%e7%9a%84%e5%85%b3%e7%b3%bb>#</a></h3><p>内存和 CPU 通常是关联的，尤其是 Java 应用可能需要更多的 CPU 资源来处理大量的内存操作（如 GC）。但是，HPA 默认只考虑内存或 CPU，而没有考虑它们的关联。</p><ul><li><strong>问题</strong>：如果只用内存作为扩容指标，而忽略了 CPU 使用情况，可能会导致扩容时内存充足但 CPU 资源不足，或者 CPU 充足但内存过多，从而导致性能不佳。</li><li><strong>解决方案</strong>：可以同时监控 <strong>CPU</strong> 和 <strong>内存</strong>，并根据多种指标综合判断是否需要扩容。可以通过 HPA 配置同时使用 CPU 和内存作为扩缩容指标。</li></ul><h3 id=6-延迟问题>6. <strong>延迟问题</strong>：
<a class=anchor href=#6-%e5%bb%b6%e8%bf%9f%e9%97%ae%e9%a2%98>#</a></h3><p>内存使用率的变化可能具有一定的滞后性，特别是对大型 Java 应用而言，内存的占用和释放会随着负载的增加或减少而变化，但可能需要一段时间才能反应在 HPA 中。</p><ul><li><strong>问题</strong>：HPA 可能会有延迟，导致它未能及时响应负载的变化，导致过度扩容或缩容。</li><li><strong>解决方案</strong>：可以通过调整 <strong>HPA 的 <code>stabilizationWindowSeconds</code></strong> 配置来控制扩容和缩容的频率和延迟，从而避免频繁的波动。</li></ul><h3 id=总结-6>总结：
<a class=anchor href=#%e6%80%bb%e7%bb%93-6>#</a></h3><p>尽管内存是扩缩容的一个重要指标，但它也有一些潜在问题：</p><ul><li><strong>内存泄漏</strong>：可能会导致不必要的扩容。</li><li><strong>GC 高峰</strong>：可能会导致内存波动，从而引发不必要的扩容。</li><li><strong>JVM 配置</strong>：不合理的 JVM 参数会影响内存使用和扩容决策。</li><li><strong>内存与 CPU 的关系</strong>：内存和 CPU 的使用是紧密相关的，仅使用内存作为唯一指标可能会导致不准确的扩缩容决策。</li><li><strong>延迟</strong>：内存的使用波动可能不会立即反应在 HPA 中，导致反应不及时。</li></ul><p>因此，最好在使用内存作为扩缩容指标时，结合 CPU 使用率、自定义指标以及合理的 JVM 配置来综合考虑，以确保系统的稳定性和性能。</p><p>当容器的内存 <code>requests</code> 设置得过低时，Kubernetes 会认为容器只需要较少的内存资源，因此在调度时会为容器分配相对较少的内存。</p><h3 id=requests-和-limits-的作用><code>requests</code> 和 <code>limits</code> 的作用：
<a class=anchor href=#requests-%e5%92%8c-limits-%e7%9a%84%e4%bd%9c%e7%94%a8>#</a></h3><ul><li><strong>requests</strong>：容器启动时所需的最小内存。Kubernetes 会根据这个值来决定容器调度在哪个节点上。<code>requests</code> 通常是容器启动时需要的内存量，Kubernetes 会为容器分配至少这个数量的内存。</li><li><strong>limits</strong>：容器在运行时能够使用的最大内存。容器无法使用超过此限制的内存量。</li></ul><h3 id=解释>解释：
<a class=anchor href=#%e8%a7%a3%e9%87%8a>#</a></h3><p>如果你将容器的内存 <code>requests</code> 设置得过低，Kubernetes 会在调度时分配比实际所需更少的内存。假如容器的负载增加，应用需要更多内存时，它可能会超过分配给它的内存资源（<code>requests</code>），并且这个容器将面临以下情况：</p><ul><li><strong>内存不足</strong>：容器超出了它被分配的内存上限，并且系统无法再提供额外的内存资源。此时，容器可能会因为内存不足而崩溃。</li><li><strong>OOM 杀死</strong>：OOM（Out Of Memory）是指“内存溢出”，如果容器超出了它的内存限制，操作系统会杀死该容器，以释放资源。容器的进程会被强制终止。</li></ul><h3 id=举个例子>举个例子：
<a class=anchor href=#%e4%b8%be%e4%b8%aa%e4%be%8b%e5%ad%90>#</a></h3><p>假设你设置了容器的内存 <code>requests</code> 为 <strong>256Mi</strong>，但是应用在负载较高时需要 <strong>512Mi</strong> 的内存。如果没有设置足够的内存 <code>requests</code>，Kubernetes 在调度时会只为容器分配 256Mi 内存。随着应用负载增加，容器尝试使用更多内存（例如达到 512Mi），这时容器会被操作系统检测到内存使用超过了容器的配置上限，最终被 <strong>OOM Killer</strong> 杀死，以防止系统崩溃。</p><h3 id=总结-7>总结：
<a class=anchor href=#%e6%80%bb%e7%bb%93-7>#</a></h3><ul><li><strong>设置内存 <code>requests</code> 过低</strong> 可能导致容器在需要更多内存时没有足够的资源，从而触发 OOM（Out of Memory）错误，导致容器崩溃。</li><li><strong>解决方法</strong>：合理配置内存 <code>requests</code> 和 <code>limits</code>，确保容器在负载增加时有足够的内存资源，避免发生 OOM 杀死。</li></ul><h1 id=java本身有堆内存一启动就超过了怎么办4g机器-堆内存3g一启动就超了>java本身有堆内存，一启动就超过了怎么办？4g机器 堆内存3g，一启动就超了。
<a class=anchor href=#java%e6%9c%ac%e8%ba%ab%e6%9c%89%e5%a0%86%e5%86%85%e5%ad%98%e4%b8%80%e5%90%af%e5%8a%a8%e5%b0%b1%e8%b6%85%e8%bf%87%e4%ba%86%e6%80%8e%e4%b9%88%e5%8a%9e4g%e6%9c%ba%e5%99%a8-%e5%a0%86%e5%86%85%e5%ad%983g%e4%b8%80%e5%90%af%e5%8a%a8%e5%b0%b1%e8%b6%85%e4%ba%86>#</a></h1><p>在 Java 应用中，堆内存的大小是由 JVM 的参数配置的，通常通过 <code>-Xms</code> 和 <code>-Xmx</code> 来设置：</p><ul><li><code>-Xms</code>：JVM 初始堆内存大小。</li><li><code>-Xmx</code>：JVM 最大堆内存大小。</li></ul><p>如果你配置了堆内存 <code>-Xmx</code> 为 3GB，并且机器总内存只有 4GB，那么 Java 应用在启动时可能会由于堆内存设置过大，导致系统资源不够用。实际上，JVM 需要更多的内存来运行应用，不仅仅是堆内存，还包括非堆内存、线程栈、JVM 本身的内存需求等。</p><h3 id=可能导致的问题>可能导致的问题：
<a class=anchor href=#%e5%8f%af%e8%83%bd%e5%af%bc%e8%87%b4%e7%9a%84%e9%97%ae%e9%a2%98>#</a></h3><ol><li><strong>内存溢出（OutOfMemoryError）</strong>：<ul><li>如果堆内存分配过大，尤其是在物理内存有限的情况下，JVM 会尝试为堆分配过多内存，可能会导致机器无法满足这些内存需求，进而导致 JVM 崩溃并抛出 <code>OutOfMemoryError</code> 错误。</li></ul></li><li><strong>OOM 杀死（OutOf Memory Killer）</strong>：<ul><li>如果 JVM 请求的内存超出了机器的物理内存，操作系统的 OOM Killer 可能会终止 Java 进程来防止系统崩溃。这在内存不足的机器上较为常见。</li></ul></li><li><strong>系统性能下降</strong>：<ul><li>即使应用没有被 OOM Killer 杀死，过大的堆内存也会导致机器其他进程的内存不足，可能会引起整个系统的性能下降。</li></ul></li></ol><h3 id=解决方案>解决方案：
<a class=anchor href=#%e8%a7%a3%e5%86%b3%e6%96%b9%e6%a1%88>#</a></h3><h4 id=1-调整-java-堆内存大小>1. <strong>调整 Java 堆内存大小</strong>
<a class=anchor href=#1-%e8%b0%83%e6%95%b4-java-%e5%a0%86%e5%86%85%e5%ad%98%e5%a4%a7%e5%b0%8f>#</a></h4><ul><li><p>你可以根据机器的物理内存来合理设置 JVM 的堆内存大小。比如，在 4GB 内存的机器上，给 Java 堆内存设置 2GB 到 2.5GB 是比较合理的配置。</p></li><li><p>修改启动命令为：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>java -Xms2g -Xmx2g -jar yourapp.jar
</span></span></code></pre></div><p>这样设置会将堆内存限制在 2GB，避免占用过多的内存。</p></li></ul><h4 id=2-考虑系统的其他内存需求>2. <strong>考虑系统的其他内存需求</strong>
<a class=anchor href=#2-%e8%80%83%e8%99%91%e7%b3%bb%e7%bb%9f%e7%9a%84%e5%85%b6%e4%bb%96%e5%86%85%e5%ad%98%e9%9c%80%e6%b1%82>#</a></h4><ul><li>除了 Java 堆内存外，还需要考虑 JVM 的其他内存需求，比如：<ul><li>Metaspace：JVM 中类的元数据存储区域。</li><li>栈内存：每个线程都会分配一定的内存作为栈空间。</li></ul></li><li>如果你设置了堆内存过大，可能会忽略这些额外的内存需求，导致其他系统进程的内存不足。</li></ul><h4 id=3-使用更合理的容器资源限制在-kubernetes-中>3. <strong>使用更合理的容器资源限制（在 Kubernetes 中）</strong>
<a class=anchor href=#3-%e4%bd%bf%e7%94%a8%e6%9b%b4%e5%90%88%e7%90%86%e7%9a%84%e5%ae%b9%e5%99%a8%e8%b5%84%e6%ba%90%e9%99%90%e5%88%b6%e5%9c%a8-kubernetes-%e4%b8%ad>#</a></h4><ul><li><p>在 Kubernetes 中，你可以为 Java 应用配置合理的资源请求 (<code>requests</code>) 和限制 (<code>limits</code>) 来避免内存分配过高。</p></li><li><p>比如，配置</p><pre tabindex=0><code>requests
</code></pre><p>和</p><pre tabindex=0><code>limits
</code></pre><p>：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>resources</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>requests</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>memory</span>: <span style=color:#e6db74>&#34;2Gi&#34;</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>limits</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>memory</span>: <span style=color:#e6db74>&#34;3Gi&#34;</span>
</span></span></code></pre></div><p>这样，Kubernetes 会根据你指定的内存资源来管理容器，避免过度占用主机内存。</p></li></ul><h4 id=4-监控内存使用>4. <strong>监控内存使用</strong>
<a class=anchor href=#4-%e7%9b%91%e6%8e%a7%e5%86%85%e5%ad%98%e4%bd%bf%e7%94%a8>#</a></h4><ul><li>在生产环境中，你可以使用 <strong>Prometheus</strong> 和 <strong>Grafana</strong> 等监控工具来实时监控 Java 应用的内存使用情况，并根据监控数据及时调整资源配置。</li></ul><h4 id=5-垃圾回收调优>5. <strong>垃圾回收调优</strong>
<a class=anchor href=#5-%e5%9e%83%e5%9c%be%e5%9b%9e%e6%94%b6%e8%b0%83%e4%bc%98>#</a></h4><ul><li><p>Java 的垃圾回收机制（GC）也会影响内存使用。你可以通过调整垃圾回收策略来优化内存的使用，减少内存峰值。例如，使用 G1 垃圾回收器并调整相关参数：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>java -Xms2g -Xmx2g -XX:+UseG1GC -XX:MaxGCPauseMillis<span style=color:#f92672>=</span><span style=color:#ae81ff>200</span> -XX:G1HeapRegionSize<span style=color:#f92672>=</span>32M -jar yourapp.jar
</span></span></code></pre></div><p>这样可以更好地管理堆内存，避免内存过度增长。</p></li></ul><h3 id=总结-8>总结：
<a class=anchor href=#%e6%80%bb%e7%bb%93-8>#</a></h3><ul><li>如果在 4GB 的机器上配置了 3GB 的堆内存，建议降低堆内存的最大值，以避免 OOM 和性能问题。合理的堆内存大小设置应该考虑到机器的物理内存和 JVM 其他内存需求。</li><li>调整 JVM 启动参数并确保堆内存和其他内存需求平衡，同时在 Kubernetes 中适当设置资源请求和限制，有助于避免内存不足的问题。</li></ul><h1 id=hpa指标依赖什么做扩缩容>hpa指标依赖什么做扩缩容？
<a class=anchor href=#hpa%e6%8c%87%e6%a0%87%e4%be%9d%e8%b5%96%e4%bb%80%e4%b9%88%e5%81%9a%e6%89%a9%e7%bc%a9%e5%ae%b9>#</a></h1><p>在 Kubernetes 中，<strong>Horizontal Pod Autoscaler (HPA)</strong> 主要通过以下几个指标来自动扩缩容：</p><h3 id=1-cpu-使用率cpu-utilization>1. <strong>CPU 使用率</strong>（CPU Utilization）
<a class=anchor href=#1-cpu-%e4%bd%bf%e7%94%a8%e7%8e%87cpu-utilization>#</a></h3><ul><li><strong>CPU 使用率</strong>是 HPA 最常用的扩缩容指标。它基于 Pod 的实际 CPU 使用量与 Pod 的 CPU 请求 (<code>requests</code>) 之间的比例来进行判断。</li><li>当 CPU 使用率超过预设的阈值时，HPA 会启动扩容操作；当 CPU 使用率低于阈值时，HPA 会启动缩容操作。</li><li>例如，假设设置了 CPU 使用率目标为 80%，当 Pod 的 CPU 使用率超过这个阈值时，HPA 会自动增加 Pod 数量，以平衡负载。</li></ul><p><strong>配置示例</strong>：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>autoscaling/v2</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>HorizontalPodAutoscaler</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-app-hpa</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>scaleTargetRef</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>apps/v1</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Deployment</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-app</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>minReplicas</span>: <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>maxReplicas</span>: <span style=color:#ae81ff>10</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>metrics</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>type</span>: <span style=color:#ae81ff>Resource</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>resource</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>name</span>: <span style=color:#ae81ff>cpu</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>target</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>type</span>: <span style=color:#ae81ff>Utilization</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>averageUtilization</span>: <span style=color:#ae81ff>80</span>
</span></span></code></pre></div><h3 id=2-内存使用率memory-utilization>2. <strong>内存使用率</strong>（Memory Utilization）
<a class=anchor href=#2-%e5%86%85%e5%ad%98%e4%bd%bf%e7%94%a8%e7%8e%87memory-utilization>#</a></h3><ul><li><strong>内存使用率</strong>是另一个常见的指标，HPA 会根据 Pod 使用的内存与请求的内存 (<code>requests</code>) 的比例来做扩缩容。</li><li>当内存使用超过阈值时，HPA 会增加 Pod 数量；当内存使用低于阈值时，HPA 会减少 Pod 数量。</li></ul><p><strong>配置示例</strong>：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>autoscaling/v2</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>HorizontalPodAutoscaler</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-app-hpa</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>scaleTargetRef</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>apps/v1</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Deployment</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-app</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>minReplicas</span>: <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>maxReplicas</span>: <span style=color:#ae81ff>10</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>metrics</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>type</span>: <span style=color:#ae81ff>Resource</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>resource</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>name</span>: <span style=color:#ae81ff>memory</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>target</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>type</span>: <span style=color:#ae81ff>Utilization</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>averageUtilization</span>: <span style=color:#ae81ff>80</span>
</span></span></code></pre></div><h3 id=3-自定义指标custom-metrics>3. <strong>自定义指标</strong>（Custom Metrics）
<a class=anchor href=#3-%e8%87%aa%e5%ae%9a%e4%b9%89%e6%8c%87%e6%a0%87custom-metrics>#</a></h3><ul><li>除了 CPU 和内存，HPA 还支持使用 <strong>自定义指标</strong>（例如，业务相关的指标）来触发扩缩容。这些自定义指标可以是来自 Prometheus、Stackdriver 或其他监控系统的数据。</li><li>自定义指标可以是业务相关的，如 HTTP 请求数、数据库连接数等，适用于需要根据业务负载来进行扩缩容的场景。</li></ul><p><strong>配置示例</strong>：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>autoscaling/v2</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>HorizontalPodAutoscaler</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-app-hpa</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>scaleTargetRef</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>apps/v1</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Deployment</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-app</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>minReplicas</span>: <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>maxReplicas</span>: <span style=color:#ae81ff>10</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>metrics</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>type</span>: <span style=color:#ae81ff>External</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>external</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>metric</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>name</span>: <span style=color:#ae81ff>http_requests_per_second</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>target</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>type</span>: <span style=color:#ae81ff>Value</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>value</span>: <span style=color:#e6db74>&#34;1000&#34;</span>
</span></span></code></pre></div><h3 id=4-请求数和响应时间等基于业务的指标>4. <strong>请求数和响应时间等（基于业务的指标）</strong>
<a class=anchor href=#4-%e8%af%b7%e6%b1%82%e6%95%b0%e5%92%8c%e5%93%8d%e5%ba%94%e6%97%b6%e9%97%b4%e7%ad%89%e5%9f%ba%e4%ba%8e%e4%b8%9a%e5%8a%a1%e7%9a%84%e6%8c%87%e6%a0%87>#</a></h3><ul><li>你还可以基于 <strong>请求数</strong>（例如，API 的请求量）或者 <strong>响应时间</strong> 等业务指标来设置 HPA 的扩缩容策略。这对于具有复杂流量模式和业务逻辑的应用非常有用。</li></ul><h3 id=5-网络流量network-traffic>5. <strong>网络流量</strong>（Network Traffic）
<a class=anchor href=#5-%e7%bd%91%e7%bb%9c%e6%b5%81%e9%87%8fnetwork-traffic>#</a></h3><ul><li>HPA 也支持基于网络流量的扩缩容。如果应用的负载会导致网络流量的变化，可以通过网络流量来触发扩容操作。</li></ul><h3 id=6-多个指标组合multiple-metrics>6. <strong>多个指标组合</strong>（Multiple Metrics）
<a class=anchor href=#6-%e5%a4%9a%e4%b8%aa%e6%8c%87%e6%a0%87%e7%bb%84%e5%90%88multiple-metrics>#</a></h3><ul><li>HPA 也支持使用多个指标来组合触发扩缩容。你可以配置 HPA 使用多个资源指标或自定义指标进行扩缩容，Kubernetes 会根据这些指标的值来决定是否需要扩容或缩容。</li></ul><h3 id=hpa-的工作原理-1>HPA 的工作原理：
<a class=anchor href=#hpa-%e7%9a%84%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86-1>#</a></h3><ol><li><strong>自动监控</strong>：HPA 监控 Pod 的各个指标（如 CPU、内存、请求数等），并与设定的目标值进行比较。</li><li><strong>扩容和缩容</strong>：当某个指标超过或低于目标值时，HPA 会自动调整 Pod 的副本数，以保证应用的性能和稳定性。</li><li><strong>调整频率</strong>：HPA 会根据指定的时间间隔周期性地检查指标值，并根据需求进行扩缩容。</li></ol><h3 id=适用性-2>适用性：
<a class=anchor href=#%e9%80%82%e7%94%a8%e6%80%a7-2>#</a></h3><ul><li><strong>CPU 和内存</strong>：适用于大多数应用，尤其是需要处理大量请求的应用，或者资源需求波动较大的应用。</li><li><strong>自定义指标</strong>：适用于有特定业务需求的应用，能够根据实际的业务负载进行扩缩容。</li><li><strong>组合指标</strong>：适用于复杂的应用，能够根据多个因素同时触发扩缩容策略。</li></ul><h3 id=总结-9>总结：
<a class=anchor href=#%e6%80%bb%e7%bb%93-9>#</a></h3><p>HPA 的扩缩容依赖于多个指标，如 CPU 使用率、内存使用率、请求数、响应时间等。通过灵活配置这些指标，HPA 可以帮助自动化管理容器的负载，确保应用的性能和稳定性。</p><h1 id=集群内有个服务对外暴露方式>集群内有个服务，对外暴露方式？
<a class=anchor href=#%e9%9b%86%e7%be%a4%e5%86%85%e6%9c%89%e4%b8%aa%e6%9c%8d%e5%8a%a1%e5%af%b9%e5%a4%96%e6%9a%b4%e9%9c%b2%e6%96%b9%e5%bc%8f>#</a></h1><p>在 Kubernetes 集群中，可以通过以下几种方式将服务暴露到外部，以便外部用户或系统可以访问该服务：</p><h3 id=1-service---clusterip集群内部暴露>1. <strong>Service - ClusterIP（集群内部暴露）</strong>
<a class=anchor href=#1-service---clusterip%e9%9b%86%e7%be%a4%e5%86%85%e9%83%a8%e6%9a%b4%e9%9c%b2>#</a></h3><ul><li><strong>ClusterIP</strong> 是 Kubernetes 默认的服务类型，仅在集群内部可访问。外部无法直接访问该服务。</li><li><strong>适用场景</strong>：如果只需要在集群内部的不同服务之间进行通信，使用 <code>ClusterIP</code> 即可。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Service</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-service</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>selector</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>app</span>: <span style=color:#ae81ff>my-app</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>ports</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>protocol</span>: <span style=color:#ae81ff>TCP</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>port</span>: <span style=color:#ae81ff>80</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>targetPort</span>: <span style=color:#ae81ff>8080</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>type</span>: <span style=color:#ae81ff>ClusterIP</span>
</span></span></code></pre></div><h3 id=2-service---nodeport集群外部暴露>2. <strong>Service - NodePort（集群外部暴露）</strong>
<a class=anchor href=#2-service---nodeport%e9%9b%86%e7%be%a4%e5%a4%96%e9%83%a8%e6%9a%b4%e9%9c%b2>#</a></h3><ul><li><strong>NodePort</strong> 是通过在每个节点上打开一个端口，使外部能够访问服务。Kubernetes 会在每个节点上分配一个端口，通过这个端口可以访问到集群内的服务。</li><li>适用场景：适用于测试环境或直接暴露给外部流量的小规模应用。</li></ul><p><strong>优点</strong>：</p><ul><li>通过固定的端口号可以轻松访问集群内的服务。</li></ul><p><strong>缺点</strong>：</p><ul><li>端口号暴露给外部，安全性较差，难以管理。</li><li>只能暴露单一端口，不适用于复杂的负载均衡。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Service</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-service</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>selector</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>app</span>: <span style=color:#ae81ff>my-app</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>ports</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>protocol</span>: <span style=color:#ae81ff>TCP</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>port</span>: <span style=color:#ae81ff>80</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>targetPort</span>: <span style=color:#ae81ff>8080</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>nodePort</span>: <span style=color:#ae81ff>30007</span>  <span style=color:#75715e># 外部访问的端口</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>type</span>: <span style=color:#ae81ff>NodePort</span>
</span></span></code></pre></div><p>外部可以通过任一节点的 IP 和该端口（<code>30007</code>）访问服务。例如：<code>http://&lt;NodeIP>:30007</code></p><h3 id=3-service---loadbalancer外部负载均衡器>3. <strong>Service - LoadBalancer（外部负载均衡器）</strong>
<a class=anchor href=#3-service---loadbalancer%e5%a4%96%e9%83%a8%e8%b4%9f%e8%bd%bd%e5%9d%87%e8%a1%a1%e5%99%a8>#</a></h3><ul><li><strong>LoadBalancer</strong> 类型的服务会请求云服务提供商创建一个外部负载均衡器，并为其分配一个公共 IP 地址。流量通过该负载均衡器转发到 Kubernetes 集群内部的服务。</li><li><strong>适用场景</strong>：适用于需要高可用和流量分发的生产环境，尤其是云环境（如 AWS、Azure、GCP）。</li></ul><p><strong>优点</strong>：</p><ul><li>自动集成负载均衡器，流量分发到各个节点。</li><li>更加稳定和可靠，适用于大规模、高流量的应用。</li></ul><p><strong>缺点</strong>：</p><ul><li>通常会产生额外的费用。</li><li>仅支持云平台的负载均衡器。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Service</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-service</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>selector</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>app</span>: <span style=color:#ae81ff>my-app</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>ports</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>protocol</span>: <span style=color:#ae81ff>TCP</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>port</span>: <span style=color:#ae81ff>80</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>targetPort</span>: <span style=color:#ae81ff>8080</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>type</span>: <span style=color:#ae81ff>LoadBalancer</span>
</span></span></code></pre></div><p>在云平台上，Kubernetes 会自动创建负载均衡器，并为其分配一个公网 IP，外部可以通过该 IP 访问服务。</p><h3 id=4-ingress入口控制器>4. <strong>Ingress（入口控制器）</strong>
<a class=anchor href=#4-ingress%e5%85%a5%e5%8f%a3%e6%8e%a7%e5%88%b6%e5%99%a8>#</a></h3><ul><li><strong>Ingress</strong> 是一种更加灵活的方式，允许通过 HTTP 或 HTTPS 协议暴露集群内的服务。Ingress 控制器将外部请求路由到指定的 Kubernetes 服务。</li><li>Ingress 可以配置更多的路由规则，支持 URL 路径、域名和负载均衡等功能。</li><li><strong>适用场景</strong>：适用于多个服务的统一入口管理，尤其是 Web 应用和微服务架构。</li></ul><p><strong>优点</strong>：</p><ul><li>可以根据 URL 路径或域名来路由流量。</li><li>支持 TLS 加密，安全性高。</li><li>配合反向代理（如 Nginx）可实现灵活的流量控制和路由。</li></ul><p><strong>缺点</strong>：</p><ul><li>需要配置 Ingress 控制器，稍微复杂。</li></ul><p><strong>配置示例</strong>：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>networking.k8s.io/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Ingress</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-ingress</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>rules</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>host</span>: <span style=color:#ae81ff>my-app.example.com</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>http</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>paths</span>:
</span></span><span style=display:flex><span>          - <span style=color:#f92672>path</span>: <span style=color:#ae81ff>/myapp</span>
</span></span><span style=display:flex><span>            <span style=color:#f92672>pathType</span>: <span style=color:#ae81ff>Prefix</span>
</span></span><span style=display:flex><span>            <span style=color:#f92672>backend</span>:
</span></span><span style=display:flex><span>              <span style=color:#f92672>service</span>:
</span></span><span style=display:flex><span>                <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-service</span>
</span></span><span style=display:flex><span>                <span style=color:#f92672>port</span>:
</span></span><span style=display:flex><span>                  <span style=color:#f92672>number</span>: <span style=color:#ae81ff>80</span>
</span></span></code></pre></div><p>外部请求 <code>http://my-app.example.com/myapp</code> 将被路由到 <code>my-service</code> 服务。</p><h3 id=5-externalname>5. <strong>ExternalName</strong>
<a class=anchor href=#5-externalname>#</a></h3><ul><li><strong>ExternalName</strong> 是一种特殊类型的服务，它将 Kubernetes 服务名称映射到外部 DNS 名称。与其他类型的服务不同，<code>ExternalName</code> 服务不创建实际的服务端口，而是直接指向外部服务。</li><li><strong>适用场景</strong>：用于访问集群外部的服务。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Service</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-external-service</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>type</span>: <span style=color:#ae81ff>ExternalName</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>externalName</span>: <span style=color:#ae81ff>my-service.example.com</span>
</span></span></code></pre></div><h3 id=总结-10>总结：
<a class=anchor href=#%e6%80%bb%e7%bb%93-10>#</a></h3><ul><li><strong>NodePort</strong> 和 <strong>LoadBalancer</strong> 是暴露外部流量的常见方式。</li><li><strong>Ingress</strong> 提供了一种更灵活的流量管理方式，适用于复杂的路由需求。</li><li><strong>ExternalName</strong> 用于访问集群外部的服务。</li></ul><p>选择合适的暴露方式取决于您的具体场景和需求，例如，生产环境下通常使用 <strong>LoadBalancer</strong> 或 <strong>Ingress</strong>，而开发环境中可能选择 <strong>NodePort</strong> 或 <strong>ClusterIP</strong>。</p><h1 id=ingress相对于nodeport有什么优缺点>ingress相对于nodeport有什么优缺点？
<a class=anchor href=#ingress%e7%9b%b8%e5%af%b9%e4%ba%8enodeport%e6%9c%89%e4%bb%80%e4%b9%88%e4%bc%98%e7%bc%ba%e7%82%b9>#</a></h1><p><strong>Ingress</strong> 和 <strong>NodePort</strong> 都是将 Kubernetes 服务暴露到外部网络的常用方式，但它们各自有不同的特点和适用场景。下面是 <strong>Ingress</strong> 相对于 <strong>NodePort</strong> 的优缺点比较：</p><h3 id=ingress-的优点><strong>Ingress 的优点：</strong>
<a class=anchor href=#ingress-%e7%9a%84%e4%bc%98%e7%82%b9>#</a></h3><ol><li><strong>灵活的路由规则：</strong><ul><li>Ingress 可以根据 URL 路径、域名等灵活地将流量路由到不同的服务。例如，您可以设置规则让 <code>example.com/app1</code> 指向一个服务，<code>example.com/app2</code> 指向另一个服务。</li><li>适合多服务统一入口管理，避免每个服务都暴露一个端口。</li></ul></li><li><strong>支持 HTTPS 和 TLS 加密：</strong><ul><li>Ingress 控制器可以轻松配置 SSL/TLS，加密流量，并且能提供证书管理（如通过 Let&rsquo;s Encrypt 自动生成证书）。</li><li>这样可以统一管理 SSL 证书，增强安全性。</li></ul></li><li><strong>集中管理入口流量：</strong><ul><li>通过 Ingress，所有外部流量可以通过一个统一的入口进行管理，避免了暴露多个端口，提高了管理效率。</li><li>可以结合负载均衡、URL 路由等功能，提升流量的控制和分发能力。</li></ul></li><li><strong>支持负载均衡：</strong><ul><li>Ingress 控制器通常会提供负载均衡功能，能够自动地将流量均匀分配到多个后端服务实例，增强了高可用性。</li></ul></li><li><strong>多域名支持：</strong><ul><li>Ingress 可以通过不同的域名和路径来路由流量，例如不同的域名指向不同的服务，支持复杂的微服务架构。</li></ul></li><li><strong>不需要公开每个服务的端口：</strong><ul><li>相比于 NodePort 需要为每个服务暴露一个端口，Ingress 仅需暴露一个入口点，简化了外部访问配置。</li></ul></li></ol><h3 id=ingress-的缺点><strong>Ingress 的缺点：</strong>
<a class=anchor href=#ingress-%e7%9a%84%e7%bc%ba%e7%82%b9>#</a></h3><ol><li><strong>需要额外的 Ingress 控制器：</strong><ul><li>Ingress 依赖于 Ingress 控制器（如 Nginx Ingress Controller 或 Traefik），需要额外的部署和配置工作。NodePort 不需要额外的组件。</li></ul></li><li><strong>配置较复杂：</strong><ul><li>相比 NodePort，Ingress 配置较为复杂，需要为不同的路由规则、TLS 配置等进行细致设置。对于小型项目或简单需求，可能会觉得过于复杂。</li></ul></li><li><strong>依赖于反向代理：</strong><ul><li>Ingress 本质上依赖于反向代理（例如 Nginx 或 HAProxy）来实现流量的路由和负载均衡，因此可能需要额外的资源和管理开销。</li></ul></li><li><strong>性能开销：</strong><ul><li>由于 Ingress 控制器通常需要处理更多的流量路由和负载均衡等操作，相比 NodePort 的简单端口映射，可能引入一些性能开销，尤其是高并发情况下。</li></ul></li></ol><hr><h3 id=nodeport-的优点><strong>NodePort 的优点：</strong>
<a class=anchor href=#nodeport-%e7%9a%84%e4%bc%98%e7%82%b9>#</a></h3><ol><li><strong>简单易用：</strong><ul><li>NodePort 非常简单，只需为每个服务分配一个端口，外部流量通过该端口访问集群内的服务。</li><li>没有额外的配置要求，适用于小型应用或简单的暴露需求。</li></ul></li><li><strong>无需额外控制器：</strong><ul><li>NodePort 不需要 Ingress 控制器或其他额外组件，配置非常直观，适用于简单场景。</li></ul></li><li><strong>直接暴露端口：</strong><ul><li>外部可以直接通过任意节点的 IP 和暴露的端口访问服务，适合一些小规模或开发测试环境。</li></ul></li></ol><h3 id=nodeport-的缺点><strong>NodePort 的缺点：</strong>
<a class=anchor href=#nodeport-%e7%9a%84%e7%bc%ba%e7%82%b9>#</a></h3><ol><li><strong>暴露多个端口：</strong><ul><li>每个服务都需要暴露一个端口，这会导致集群节点暴露大量端口，管理起来会非常麻烦，尤其是服务数量较多时。</li><li>每个服务都需要一个单独的端口，外部访问时需要记住多个端口号。</li></ul></li><li><strong>不支持复杂的路由：</strong><ul><li>NodePort 只提供端口映射，无法根据 URL 路径或域名来分发流量，不能像 Ingress 那样实现灵活的流量控制和负载均衡。</li></ul></li><li><strong>无法自动管理 HTTPS：</strong><ul><li>NodePort 不支持 HTTPS 或 TLS 加密，必须在服务端自行配置证书。这对于需要安全访问的应用来说会增加额外的复杂度。</li></ul></li><li><strong>暴露每个服务的端口：</strong><ul><li>如果集群中的服务较多，暴露多个端口可能导致安全问题。所有的端口都会被公开到外部，可能增加潜在的攻击面。</li></ul></li><li><strong>不适用于负载均衡：</strong><ul><li>NodePort 没有内建的负载均衡机制，流量需要手动分发到不同节点的端口，通常不如 Ingress 提供的负载均衡功能灵活和高效。</li></ul></li></ol><hr><h3 id=总结对比><strong>总结对比：</strong>
<a class=anchor href=#%e6%80%bb%e7%bb%93%e5%af%b9%e6%af%94>#</a></h3><table><thead><tr><th>特性</th><th>Ingress</th><th>NodePort</th></tr></thead><tbody><tr><td>配置复杂度</td><td>高</td><td>低</td></tr><tr><td>支持路由规则</td><td>是（URL、域名、路径等）</td><td>否</td></tr><tr><td>支持 HTTPS/TLS</td><td>是（通过 Ingress 控制器）</td><td>否</td></tr><tr><td>支持负载均衡</td><td>是</td><td>否</td></tr><tr><td>使用场景</td><td>适合需要灵活路由、高可用、统一入口管理的生产环境</td><td>适合小规模、开发和测试环境</td></tr><tr><td>额外依赖</td><td>需要 Ingress 控制器</td><td>无</td></tr><tr><td>安全性</td><td>较高（支持 TLS）</td><td>较低（暴露多个端口）</td></tr></tbody></table><h3 id=适用场景-2><strong>适用场景</strong>：
<a class=anchor href=#%e9%80%82%e7%94%a8%e5%9c%ba%e6%99%af-2>#</a></h3><ul><li><strong>Ingress</strong> 更适用于大规模生产环境，尤其是需要路由、负载均衡和统一的外部访问入口时。它能帮助更好地管理多个服务的流量并提供安全性。</li><li><strong>NodePort</strong> 适用于小型应用、测试环境或简单的场景，当你只需要将服务暴露到外部且无需复杂的路由时，NodePort 是一个简单直接的选择。</li></ul><h1 id=ingress和nodeport分别属于几层的转发>ingress和nodeport分别属于几层的转发？
<a class=anchor href=#ingress%e5%92%8cnodeport%e5%88%86%e5%88%ab%e5%b1%9e%e4%ba%8e%e5%87%a0%e5%b1%82%e7%9a%84%e8%bd%ac%e5%8f%91>#</a></h1><p>在 Kubernetes 中，<strong>Ingress</strong> 和 <strong>NodePort</strong> 分别涉及不同的网络层次。具体来说：</p><h3 id=1-ingress应用层layer-7><strong>1. Ingress：应用层（Layer 7）</strong>
<a class=anchor href=#1-ingress%e5%ba%94%e7%94%a8%e5%b1%82layer-7>#</a></h3><p>Ingress 是在 <strong>第七层</strong>（<strong>应用层</strong>）进行流量转发的。它依赖于 <strong>Ingress 控制器</strong>（如 Nginx、Traefik 等）来解析和管理 HTTP/HTTPS 请求。Ingress 控制器根据定义的路由规则，将请求转发到对应的 Kubernetes 服务。Ingress 主要通过 URL 路径、域名、头信息等信息来做流量的路由和负载均衡，属于 <strong>HTTP 层面的反向代理和负载均衡</strong>。</p><ul><li><strong>路由规则</strong>：根据请求的 URL 路径、域名或其他 HTTP 层的请求信息来决定将流量转发到哪个后端服务。</li><li><strong>支持 HTTPS/TLS</strong>：Ingress 可以配置 SSL/TLS，处理加密流量。</li><li><strong>适用场景</strong>：通常用于管理多个服务的外部访问入口。</li></ul><h3 id=2-nodeport传输层layer-4><strong>2. NodePort：传输层（Layer 4）</strong>
<a class=anchor href=#2-nodeport%e4%bc%a0%e8%be%93%e5%b1%82layer-4>#</a></h3><p>NodePort 是在 <strong>第四层</strong>（<strong>传输层</strong>）进行流量转发的。它通过暴露集群中节点上的某个端口（一个 TCP 或 UDP 端口）来允许外部流量访问集群内的服务。Kubernetes 会将外部流量转发到对应的服务后端（Pod）。NodePort 不关心流量的内容（如 HTTP 请求的 URL），只是在网络层面转发流量，主要基于 IP 地址和端口号。</p><ul><li><strong>端口映射</strong>：外部请求会通过 Kubernetes 集群中任一节点的某个端口访问服务，这个端口会映射到集群内服务的目标端口。</li><li><strong>没有路由规则</strong>：NodePort 无法像 Ingress 那样进行基于 URL 或域名的路由，只能通过端口来区分不同服务。</li><li><strong>适用场景</strong>：适合简单场景，或者需要将服务直接暴露给外部网络时。</li></ul><h3 id=总结-11><strong>总结：</strong>
<a class=anchor href=#%e6%80%bb%e7%bb%93-11>#</a></h3><ul><li><strong>Ingress</strong>：位于 <strong>第七层（应用层）</strong>，主要用于基于 HTTP/HTTPS 的流量路由，进行更复杂的负载均衡、TLS 加密、域名和路径的分发等。</li><li><strong>NodePort</strong>：位于 <strong>第四层（传输层）</strong>，基于 IP 和端口进行流量转发，通常用于简单的外部访问，且不涉及应用层路由和加密功能。</li></ul><h1 id=除了应用运维还有什么对客经验>除了应用运维还有什么对客经验？
<a class=anchor href=#%e9%99%a4%e4%ba%86%e5%ba%94%e7%94%a8%e8%bf%90%e7%bb%b4%e8%bf%98%e6%9c%89%e4%bb%80%e4%b9%88%e5%af%b9%e5%ae%a2%e7%bb%8f%e9%aa%8c>#</a></h1><p>和甲方对方案，协调资源，协助甲方解决平台使用中遇到的问题</p><h1 id=甲方安排你现在工作范围外你会怎么做>甲方安排你现在工作范围外，你会怎么做？
<a class=anchor href=#%e7%94%b2%e6%96%b9%e5%ae%89%e6%8e%92%e4%bd%a0%e7%8e%b0%e5%9c%a8%e5%b7%a5%e4%bd%9c%e8%8c%83%e5%9b%b4%e5%a4%96%e4%bd%a0%e4%bc%9a%e6%80%8e%e4%b9%88%e5%81%9a>#</a></h1><p>如果甲方安排的工作超出了我的当前工作范围，我会采取以下步骤来处理这种情况：</p><h3 id=1-明确沟通理解需求>1. <strong>明确沟通，理解需求</strong>
<a class=anchor href=#1-%e6%98%8e%e7%a1%ae%e6%b2%9f%e9%80%9a%e7%90%86%e8%a7%a3%e9%9c%80%e6%b1%82>#</a></h3><p>首先，我会与甲方或相关负责人进行沟通，明确他们的需求和期望。可能是他们的需求超出了我目前的工作范围，或者可能是他们误解了工作职责的边界。通过沟通，我可以更清楚地了解他们的期望，并判断这项工作是否属于我的职责范畴。</p><h3 id=2-评估影响和可行性>2. <strong>评估影响和可行性</strong>
<a class=anchor href=#2-%e8%af%84%e4%bc%b0%e5%bd%b1%e5%93%8d%e5%92%8c%e5%8f%af%e8%a1%8c%e6%80%a7>#</a></h3><p>如果这项工作确实超出了我的工作范围，我会评估一下它的紧急性、重要性以及是否能带来业务价值。然后，我会评估我现有的工作安排是否能够调整，是否能合理分配资源来处理这项任务。如果工作量过大或者当前无法立即开始，我会考虑是否需要推迟或调整其他任务。</p><h3 id=3-与上级或同事协商>3. <strong>与上级或同事协商</strong>
<a class=anchor href=#3-%e4%b8%8e%e4%b8%8a%e7%ba%a7%e6%88%96%e5%90%8c%e4%ba%8b%e5%8d%8f%e5%95%86>#</a></h3><p>如果我认为这项工作超出了我的职责范围，我会和我的上级或者相关负责人进行沟通，向他们说明当前工作范围的界限，并请求他们的帮助或意见。我可能会提议是否可以将任务分配给其他相关人员，或者通过调整项目进度来解决这个问题。</p><h3 id=4-明确边界调整合同或工作协议>4. <strong>明确边界，调整合同或工作协议</strong>
<a class=anchor href=#4-%e6%98%8e%e7%a1%ae%e8%be%b9%e7%95%8c%e8%b0%83%e6%95%b4%e5%90%88%e5%90%8c%e6%88%96%e5%b7%a5%e4%bd%9c%e5%8d%8f%e8%ae%ae>#</a></h3><p>如果这类情况是持续的，我会和甲方或我的领导讨论工作合同或协议的调整，明确哪些任务属于我的职责范畴，哪些任务需要额外的资源或者费用。确保工作范围的明确和合理分配，避免出现长期超出职责范围的情况。</p><h3 id=5-灵活应对保持专业>5. <strong>灵活应对，保持专业</strong>
<a class=anchor href=#5-%e7%81%b5%e6%b4%bb%e5%ba%94%e5%af%b9%e4%bf%9d%e6%8c%81%e4%b8%93%e4%b8%9a>#</a></h3><p>在整个过程中，我会保持专业、灵活和积极的态度，尽量寻找对双方都能接受的解决方案。如果甲方的需求有一定合理性，并且调整工作范围后不会影响现有工作质量和进度，我会考虑接手，但需要提前明确时间和资源分配。</p><p>总的来说，关键是保持良好的沟通，合理评估，确保工作安排的清晰和高效。</p><h1 id=作为项目负责人甲方提到的需求超过你的范围但是还是在你的项目内的怎么做>作为项目负责人，甲方提到的需求超过你的范围，但是还是在你的项目内的，怎么做？
<a class=anchor href=#%e4%bd%9c%e4%b8%ba%e9%a1%b9%e7%9b%ae%e8%b4%9f%e8%b4%a3%e4%ba%ba%e7%94%b2%e6%96%b9%e6%8f%90%e5%88%b0%e7%9a%84%e9%9c%80%e6%b1%82%e8%b6%85%e8%bf%87%e4%bd%a0%e7%9a%84%e8%8c%83%e5%9b%b4%e4%bd%86%e6%98%af%e8%bf%98%e6%98%af%e5%9c%a8%e4%bd%a0%e7%9a%84%e9%a1%b9%e7%9b%ae%e5%86%85%e7%9a%84%e6%80%8e%e4%b9%88%e5%81%9a>#</a></h1><p>作为项目负责人，面对甲方提出的需求超出我的责任范围，但仍在项目内的情况，我会采取以下步骤来妥善处理：</p><h3 id=1-了解需求的细节>1. <strong>了解需求的细节</strong>
<a class=anchor href=#1-%e4%ba%86%e8%a7%a3%e9%9c%80%e6%b1%82%e7%9a%84%e7%bb%86%e8%8a%82>#</a></h3><p>首先，我需要与甲方详细沟通，明确他们的具体需求。了解需求的背景、目标和期望，确保我完全理解他们的需求，并且能够判断它对项目的影响。必要时，可以要求甲方提供更多背景信息和期望结果。</p><h3 id=2-评估影响与可行性>2. <strong>评估影响与可行性</strong>
<a class=anchor href=#2-%e8%af%84%e4%bc%b0%e5%bd%b1%e5%93%8d%e4%b8%8e%e5%8f%af%e8%a1%8c%e6%80%a7>#</a></h3><p>一旦明确了需求，我会评估这项需求对项目的整体影响。考虑以下几个方面：</p><ul><li><strong>时间</strong>：这项需求会影响到项目的进度吗？是否会导致延期？</li><li><strong>资源</strong>：需要额外的资源（人力、资金、技术等）吗？是否能够合理调配现有资源来完成？</li><li><strong>质量</strong>：这项需求是否会影响项目交付的质量？是否需要额外的测试、优化等？</li><li><strong>风险</strong>：是否有可能带来额外的风险？如何控制这些风险？</li></ul><p>如果评估后认为这项需求合理且可行，我会继续处理。如果无法评估其影响，可能需要与甲方进一步探讨或寻求外部专家的帮助。</p><h3 id=3-与团队讨论并调整计划>3. <strong>与团队讨论并调整计划</strong>
<a class=anchor href=#3-%e4%b8%8e%e5%9b%a2%e9%98%9f%e8%ae%a8%e8%ae%ba%e5%b9%b6%e8%b0%83%e6%95%b4%e8%ae%a1%e5%88%92>#</a></h3><p>将需求的影响和评估结果与项目团队分享，讨论是否能够满足这个需求，并协调团队成员的工作负载。根据评估结果，重新调整项目计划和资源分配，确保需求可以在不影响项目主要目标的情况下完成。</p><h3 id=4-与甲方沟通调整计划与交付>4. <strong>与甲方沟通调整计划与交付</strong>
<a class=anchor href=#4-%e4%b8%8e%e7%94%b2%e6%96%b9%e6%b2%9f%e9%80%9a%e8%b0%83%e6%95%b4%e8%ae%a1%e5%88%92%e4%b8%8e%e4%ba%a4%e4%bb%98>#</a></h3><p>如果需求需要超出原定的工作范围，且会对项目的时间表或预算造成影响，我会与甲方进行沟通，说明可能需要的调整：</p><ul><li><strong>调整时间表</strong>：根据需求的复杂性，可能需要延长交付时间。讨论一个可行的时间表，并明确各阶段的交付。</li><li><strong>调整预算</strong>：如果需求会导致额外的成本，我会与甲方沟通预算调整，并获得他们的确认。</li><li><strong>调整优先级</strong>：如果这个需求优先级较高，我会建议将它放到前面处理，其他低优先级任务则可以推迟。</li></ul><h3 id=5-签订变更协议或文档>5. <strong>签订变更协议或文档</strong>
<a class=anchor href=#5-%e7%ad%be%e8%ae%a2%e5%8f%98%e6%9b%b4%e5%8d%8f%e8%ae%ae%e6%88%96%e6%96%87%e6%a1%a3>#</a></h3><p>如果需求导致项目的范围、进度或预算发生变化，我会与甲方达成一致并签订变更协议。变更协议中应包括：</p><ul><li>变更的详细内容</li><li>新的交付时间和里程碑</li><li>变更所涉及的资源和预算</li><li>变更后可能产生的风险与应对措施</li></ul><h3 id=6-持续监控与管理>6. <strong>持续监控与管理</strong>
<a class=anchor href=#6-%e6%8c%81%e7%bb%ad%e7%9b%91%e6%8e%a7%e4%b8%8e%e7%ae%a1%e7%90%86>#</a></h3><p>在需求变更后的实施阶段，我会持续跟踪进展，确保新需求的完成不会影响原有目标的交付。定期与甲方沟通，报告项目进度和可能遇到的问题。</p><h3 id=7-总结与教训>7. <strong>总结与教训</strong>
<a class=anchor href=#7-%e6%80%bb%e7%bb%93%e4%b8%8e%e6%95%99%e8%ae%ad>#</a></h3><p>项目完成后，会对需求变更进行总结，评估变更的实施效果以及对项目的影响。这些总结会帮助我们在未来的项目中更好地应对类似的情况。</p><h3 id=总结-12>总结
<a class=anchor href=#%e6%80%bb%e7%bb%93-12>#</a></h3><p>作为项目负责人，面对甲方提出的超出原始范围的需求，最关键的是保持开放和透明的沟通，合理评估需求的可行性，调整项目计划，并确保项目质量和交付时间不受影响。同时，确保通过变更协议等正式文档明确所有变更内容，并与甲方达成一致，减少后续争议。</p><h1 id=遇到的比较复杂和深入的问题难点是什么>遇到的比较复杂和深入的问题？难点是什么？
<a class=anchor href=#%e9%81%87%e5%88%b0%e7%9a%84%e6%af%94%e8%be%83%e5%a4%8d%e6%9d%82%e5%92%8c%e6%b7%b1%e5%85%a5%e7%9a%84%e9%97%ae%e9%a2%98%e9%9a%be%e7%82%b9%e6%98%af%e4%bb%80%e4%b9%88>#</a></h1><p>在进行单 Master 单 Etcd 节点扩展到 3 Master 3 Etcd 节点的改造时，确实会面临多个挑战，特别是在证书的管理和备份方面。以下是一些关键步骤及其潜在难点，以及如何确保改造的顺利进行，特别是在备份 Etcd 数据方面。</p><h3 id=1-备份-etcd-数据>1. <strong>备份 Etcd 数据</strong>
<a class=anchor href=#1-%e5%a4%87%e4%bb%bd-etcd-%e6%95%b0%e6%8d%ae>#</a></h3><ul><li><p><strong>重要性</strong>：Etcd 是 Kubernetes 的核心组件之一，存储了集群的所有状态和配置数据。在扩展操作之前，必须先确保 Etcd 数据的安全备份。</p></li><li><p>备份步骤：</p><ol><li><p>使用</p><pre tabindex=0><code>etcdctl
</code></pre><p>工具进行备份：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ETCDCTL_API<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span> etcdctl snapshot save /path/to/backup.db --endpoints<span style=color:#f92672>=</span>&lt;etcd_endpoint&gt; --cacert<span style=color:#f92672>=</span>&lt;path_to_ca.crt&gt; --cert<span style=color:#f92672>=</span>&lt;path_to_cert.crt&gt; --key<span style=color:#f92672>=</span>&lt;path_to_cert.key&gt;
</span></span></code></pre></div></li><li><p>确保备份文件保存到安全的位置，最好是多副本保存。</p></li><li><p>可以将备份文件压缩并使用外部存储进行备份，以防止数据丢失。</p></li></ol></li><li><p>恢复备份：</p><ul><li><p>如果在扩展过程中遇到问题，可以使用以下命令恢复 Etcd 数据：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ETCDCTL_API<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span> etcdctl snapshot restore /path/to/backup.db --data-dir<span style=color:#f92672>=</span>/path/to/new_etcd_data_dir --name<span style=color:#f92672>=</span>&lt;new_etcd_name&gt; --initial-cluster<span style=color:#f92672>=</span>&lt;etcd_cluster&gt; --initial-cluster-token<span style=color:#f92672>=</span>&lt;cluster_token&gt; --initial-cluster-state<span style=color:#f92672>=</span>existing
</span></span></code></pre></div></li></ul></li></ul><h3 id=2-证书管理与生成>2. <strong>证书管理与生成</strong>
<a class=anchor href=#2-%e8%af%81%e4%b9%a6%e7%ae%a1%e7%90%86%e4%b8%8e%e7%94%9f%e6%88%90>#</a></h3><p>证书管理是 Kubernetes 集群安全性和稳定性的关键。在扩展 Master 和 Etcd 节点时，必须重新生成并分发新的证书，确保各个组件的通信安全。</p><h4 id=证书的重新生成><strong>证书的重新生成</strong>
<a class=anchor href=#%e8%af%81%e4%b9%a6%e7%9a%84%e9%87%8d%e6%96%b0%e7%94%9f%e6%88%90>#</a></h4><ul><li><p>生成新证书：</p><ul><li><p>使用 CA 证书和</p><pre tabindex=0><code>.cnf
</code></pre><p>文件来生成新的证书，具体步骤如下：</p><ol><li><p>为 Master 节点生成证书：</p><ul><li><p>使用 <code>master_ssl.cnf</code> 文件生成包含所有 Master 节点 IP 和硬负载 VIP 的证书。</p></li><li><p>生成过程示例：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>openssl req -new -nodes -out master.csr -keyout master.key -config master_ssl.cnf
</span></span><span style=display:flex><span>openssl x509 -req -in master.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out master.crt -days <span style=color:#ae81ff>365</span>
</span></span></code></pre></div></li></ul></li><li><p>为 Etcd 节点生成证书：</p><ul><li><p>使用 <code>etcd_ssl.cnf</code> 文件生成包含 Etcd 节点 IP 的证书。</p></li><li><p>生成过程示例：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>openssl req -new -nodes -out etcd.csr -keyout etcd.key -config etcd_ssl.cnf
</span></span><span style=display:flex><span>openssl x509 -req -in etcd.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out etcd.crt -days <span style=color:#ae81ff>365</span>
</span></span></code></pre></div></li></ul></li><li><p>为 Worker 节点生成证书：</p><ul><li><p>根据</p><pre tabindex=0><code>master_ssl.cnf
</code></pre><p>生成 Worker 节点的客户端证书：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>openssl req -new -nodes -out worker-client.csr -keyout worker-client.key -config master_ssl.cnf
</span></span><span style=display:flex><span>openssl x509 -req -in worker-client.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out worker-client.crt -days <span style=color:#ae81ff>365</span>
</span></span></code></pre></div></li></ul></li></ol></li></ul></li></ul><h4 id=证书替换和拷贝><strong>证书替换和拷贝</strong>
<a class=anchor href=#%e8%af%81%e4%b9%a6%e6%9b%bf%e6%8d%a2%e5%92%8c%e6%8b%b7%e8%b4%9d>#</a></h4><ul><li>将新生成的证书分别拷贝到 Etcd、Master 和 Worker 节点。<ul><li><strong>Etcd 节点</strong>：替换 Etcd 证书后，重启 Etcd 服务。</li><li><strong>Master 节点</strong>：替换 Master 节点的证书后，重启 <code>kube-apiserver</code>、<code>kube-scheduler</code>、<code>kube-controller-manager</code>、<code>kubelet</code> 和 <code>kube-proxy</code> 服务。</li><li><strong>Worker 节点</strong>：替换 Worker 节点的证书后，重启 <code>kubelet</code> 和 <code>kube-proxy</code> 服务。</li></ul></li></ul><h3 id=3-集群扩展操作>3. <strong>集群扩展操作</strong>
<a class=anchor href=#3-%e9%9b%86%e7%be%a4%e6%89%a9%e5%b1%95%e6%93%8d%e4%bd%9c>#</a></h3><p>在证书替换后，你将进行集群的扩展，具体步骤如下：</p><h4 id=扩展-master-节点><strong>扩展 Master 节点</strong>
<a class=anchor href=#%e6%89%a9%e5%b1%95-master-%e8%8a%82%e7%82%b9>#</a></h4><ul><li>配置并启动新的 Master 节点，确保新节点能够加入到现有的集群中。</li><li>配置 <code>kube-apiserver</code>、<code>kube-scheduler</code>、<code>kube-controller-manager</code> 等组件，并确保它们能够与新的 Etcd 节点进行通信。</li></ul><h4 id=扩展-etcd-节点><strong>扩展 Etcd 节点</strong>
<a class=anchor href=#%e6%89%a9%e5%b1%95-etcd-%e8%8a%82%e7%82%b9>#</a></h4><ul><li>新增的 Etcd 节点需要同步集群数据。</li><li>确保 <code>etcdctl</code> 通过新的证书和配置能够连接到现有的 Etcd 集群，并进行数据同步。</li></ul><h3 id=4-验证和监控>4. <strong>验证和监控</strong>
<a class=anchor href=#4-%e9%aa%8c%e8%af%81%e5%92%8c%e7%9b%91%e6%8e%a7>#</a></h3><p>完成扩展后，验证集群的健康状态，确保 Master 节点、Etcd 节点和 Worker 节点服务正常：</p><ul><li>使用 <code>kubectl get nodes</code> 和 <code>kubectl get pods --all-namespaces</code> 检查集群状态。</li><li>确保各个组件如 <code>kube-apiserver</code>、<code>kube-scheduler</code>、<code>kube-controller-manager</code>、<code>etcd</code> 服务正常运行。</li></ul><h3 id=5-回退策略>5. <strong>回退策略</strong>
<a class=anchor href=#5-%e5%9b%9e%e9%80%80%e7%ad%96%e7%95%a5>#</a></h3><p>如果扩展过程中出现问题，可以按照以下步骤回退：</p><ol><li><strong>删除新增的 Master 节点和 Etcd 节点</strong>。</li><li><strong>恢复备份证书</strong>：将老的证书从备份恢复到 Master 节点和 Etcd 节点。</li><li><strong>重启相关服务</strong>：重启 Master 和 Worker 节点上的 Kubernetes 服务，确保恢复正常。</li></ol><h3 id=总结-13>总结
<a class=anchor href=#%e6%80%bb%e7%bb%93-13>#</a></h3><p>将单 Master 单 Etcd 节点扩展到 3 Master 3 Etcd 节点的过程，涉及证书管理、服务重启、集群扩展等多个方面。特别是在 Etcd 数据备份和恢复、证书生成与替换等环节需要谨慎操作，以确保集群的稳定性和数据安全。在执行过程中，建议在每个步骤前进行充分的测试，确保新节点能够正常加入并与现有节点协同工作。同时，也要为可能的回退情况做好充分准备。</p><h1 id=下线一个或者多个节点>下线一个或者多个节点？
<a class=anchor href=#%e4%b8%8b%e7%ba%bf%e4%b8%80%e4%b8%aa%e6%88%96%e8%80%85%e5%a4%9a%e4%b8%aa%e8%8a%82%e7%82%b9>#</a></h1><p>下线一个或多个 Kubernetes 节点的操作通常包括从集群中删除节点并确保集群继续稳定运行。具体操作步骤如下：</p><h3 id=1-标记节点为不可调度-cordon>1. <strong>标记节点为不可调度 (Cordon)</strong>
<a class=anchor href=#1-%e6%a0%87%e8%ae%b0%e8%8a%82%e7%82%b9%e4%b8%ba%e4%b8%8d%e5%8f%af%e8%b0%83%e5%ba%a6-cordon>#</a></h3><p>在下线节点之前，首先应该标记节点为不可调度，这样可以确保不会有新的 Pod 被调度到该节点上。使用以下命令将节点标记为不可调度：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl cordon &lt;node-name&gt;
</span></span></code></pre></div><p>这会使该节点无法再调度新的 Pod，但是当前运行在该节点上的 Pod 不会立即停止。</p><h3 id=2-迁移-pods-drain>2. <strong>迁移 Pods (Drain)</strong>
<a class=anchor href=#2-%e8%bf%81%e7%a7%bb-pods-drain>#</a></h3><p>将节点上的现有 Pod 安全地迁移到其他节点上。使用 <code>kubectl drain</code> 命令可以优雅地迁移 Pod，并删除该节点上的所有 Pod。确保该节点上的 Pod 可以正常迁移到其他节点（比如通过设置 Pod 的 <code>replica</code> 数量）。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl drain &lt;node-name&gt; --ignore-daemonsets --delete-local-data
</span></span></code></pre></div><ul><li><code>--ignore-daemonsets</code>：表示不影响 DaemonSet 中的 Pod，因为 DaemonSet Pod 通常会重新调度到其他节点。</li><li><code>--delete-local-data</code>：如果节点上有本地数据并且不需要保留，可以使用此选项来删除它。</li></ul><p>注意：<code>drain</code> 命令会将所有 Pod 移除，包括那些没有被正常调度的 Pod（比如一些没有副本或 Pod 模板设置的应用），在执行此操作之前需要确保可以安全删除这些 Pod。</p><h3 id=3-删除节点>3. <strong>删除节点</strong>
<a class=anchor href=#3-%e5%88%a0%e9%99%a4%e8%8a%82%e7%82%b9>#</a></h3><p>如果节点上的所有 Pod 都已成功迁移，并且你确认不再需要该节点，可以从 Kubernetes 集群中完全删除该节点：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl delete node &lt;node-name&gt;
</span></span></code></pre></div><p>这将从集群的节点列表中移除该节点。</p><h3 id=4-恢复节点>4. <strong>恢复节点</strong>
<a class=anchor href=#4-%e6%81%a2%e5%a4%8d%e8%8a%82%e7%82%b9>#</a></h3><p>如果你要恢复之前下线的节点，可以通过以下命令将其重新加入到集群中：</p><ol><li>在节点上运行 <code>kubeadm join</code> 命令（如果是用 <code>kubeadm</code> 部署的集群）。确保该节点能够访问到 API server。</li><li>如果节点之前被 <code>cordon</code> 或 <code>drain</code>，需要执行 <code>kubectl uncordon</code> 来允许该节点重新接收调度。</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl uncordon &lt;node-name&gt;
</span></span></code></pre></div><h3 id=5-验证节点状态>5. <strong>验证节点状态</strong>
<a class=anchor href=#5-%e9%aa%8c%e8%af%81%e8%8a%82%e7%82%b9%e7%8a%b6%e6%80%81>#</a></h3><p>在完成下线操作后，使用以下命令验证节点的状态，确保集群状态稳定：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get nodes
</span></span></code></pre></div><p>检查 <code>Ready</code> 状态的节点数量和所有 Pod 是否已正确迁移。</p><h3 id=6-其他注意事项>6. <strong>其他注意事项</strong>
<a class=anchor href=#6-%e5%85%b6%e4%bb%96%e6%b3%a8%e6%84%8f%e4%ba%8b%e9%a1%b9>#</a></h3><ul><li><strong>负载均衡</strong>：如果节点涉及负载均衡（例如使用 <code>Ingress</code> 或其他负载均衡器），需要确保流量被正确地转发到剩余的节点。</li><li><strong>资源回收</strong>：下线节点后，如果该节点拥有大量资源（如本地存储），需要清理节点上的数据以释放资源。</li><li><strong>监控</strong>：监控集群资源使用情况，确保下线节点后，剩余的节点可以处理当前负载。</li></ul><h3 id=7-批量下线多个节点>7. <strong>批量下线多个节点</strong>
<a class=anchor href=#7-%e6%89%b9%e9%87%8f%e4%b8%8b%e7%ba%bf%e5%a4%9a%e4%b8%aa%e8%8a%82%e7%82%b9>#</a></h3><p>如果要下线多个节点，可以通过循环执行上面的步骤或使用脚本批量操作。例如，使用下面的脚本来标记多个节点为不可调度并迁移它们：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#66d9ef>for</span> node in <span style=color:#66d9ef>$(</span>kubectl get nodes --selector<span style=color:#f92672>=</span>&lt;label-selector&gt; -o name<span style=color:#66d9ef>)</span>; <span style=color:#66d9ef>do</span>
</span></span><span style=display:flex><span>  kubectl cordon $node
</span></span><span style=display:flex><span>  kubectl drain $node --ignore-daemonsets --delete-local-data
</span></span><span style=display:flex><span><span style=color:#66d9ef>done</span>
</span></span></code></pre></div><p>这将自动对符合选择器条件的所有节点执行 <code>cordon</code> 和 <code>drain</code> 操作。</p><p>通过上述步骤，你可以优雅地下线一个或多个节点，确保集群的稳定性和服务不中断。在进行节点下线前，务必评估集群负载和资源情况，确保节点下线后其他节点能够承载相应的负载。</p><h1 id=驱逐节点上的pod命令>驱逐节点上的pod命令？
<a class=anchor href=#%e9%a9%b1%e9%80%90%e8%8a%82%e7%82%b9%e4%b8%8a%e7%9a%84pod%e5%91%bd%e4%bb%a4>#</a></h1><p>驱逐节点上的 Pod，可以使用 <code>kubectl drain</code> 命令。该命令将会将节点上的所有 Pod 移除，并确保节点上不会有新的 Pod 被调度到这个节点。</p><h3 id=驱逐节点上的所有-pod>驱逐节点上的所有 Pod
<a class=anchor href=#%e9%a9%b1%e9%80%90%e8%8a%82%e7%82%b9%e4%b8%8a%e7%9a%84%e6%89%80%e6%9c%89-pod>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl drain &lt;node-name&gt; --ignore-daemonsets --delete-local-data
</span></span></code></pre></div><ul><li><code>&lt;node-name></code>：你想要驱逐 Pod 的节点名称。</li><li><code>--ignore-daemonsets</code>：表示不驱逐 DaemonSet 管理的 Pod。DaemonSet 通常会在所有节点上保持一个 Pod 实例，因此不应该驱逐这些 Pod。</li><li><code>--delete-local-data</code>：如果节点上有本地数据（例如使用 <code>emptyDir</code> 存储的临时数据），这个选项会删除这些数据。<strong>小心使用</strong>，因为这会删除节点上的本地存储数据。</li></ul><h3 id=示例-3>示例
<a class=anchor href=#%e7%a4%ba%e4%be%8b-3>#</a></h3><p>如果你想驱逐名为 <code>node1</code> 节点上的所有 Pod，命令如下：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl drain node1 --ignore-daemonsets --delete-local-data
</span></span></code></pre></div><h3 id=重要提示>重要提示
<a class=anchor href=#%e9%87%8d%e8%a6%81%e6%8f%90%e7%a4%ba>#</a></h3><ol><li><strong>确保 Pod 可迁移</strong>：在执行驱逐操作之前，确保节点上的 Pod 能够迁移到其他节点，这通常需要集群有足够的资源和调度策略。</li><li><strong>排除 DaemonSet 的影响</strong>：<code>--ignore-daemonsets</code> 选项会确保 DaemonSet 管理的 Pod 不会被驱逐，因为这些 Pod 通常会重新调度到该节点上。</li></ol><h3 id=恢复节点>恢复节点
<a class=anchor href=#%e6%81%a2%e5%a4%8d%e8%8a%82%e7%82%b9>#</a></h3><p>如果你不再需要驱逐操作，可以使用 <code>kubectl uncordon</code> 将节点恢复为可调度状态：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl uncordon &lt;node-name&gt;
</span></span></code></pre></div><p>这将允许新的 Pod 被调度到该节点。</p><h1 id=迁云的工作怎么去看>迁云的工作？怎么去看？
<a class=anchor href=#%e8%bf%81%e4%ba%91%e7%9a%84%e5%b7%a5%e4%bd%9c%e6%80%8e%e4%b9%88%e5%8e%bb%e7%9c%8b>#</a></h1><p>迁云（Cloud Migration）是将企业的 IT 系统、应用程序、数据等从传统的本地数据中心或其他云平台迁移到云平台的过程。迁云的工作涉及多个方面，通常包括计划、执行和后期优化等多个阶段。以下是迁云的主要工作内容及如何查看这些工作：</p><h3 id=1-迁云准备工作>1. <strong>迁云准备工作</strong>
<a class=anchor href=#1-%e8%bf%81%e4%ba%91%e5%87%86%e5%a4%87%e5%b7%a5%e4%bd%9c>#</a></h3><ul><li><strong>评估现有环境</strong>：首先，评估现有的 IT 基础设施、应用、数据存储等，确定迁移的范围、优先级和挑战。这包括了解目前的技术架构、性能需求、存储容量、网络带宽等。</li><li><strong>选择云平台</strong>：选择适合的云平台（如 AWS、Azure、Google Cloud、阿里云等），根据业务需求、成本、安全性等考虑因素，决定是选择公有云、私有云还是混合云。</li><li><strong>确定迁移策略</strong>：常见的迁移策略有：<ul><li><strong>重构（Replatforming）</strong>：修改部分应用或架构，适配云平台。</li><li><strong>重建（Refactoring）</strong>：对应用进行大规模的重构，充分利用云平台的服务（如微服务架构）。</li><li><strong>替代（Rehosting）</strong>：将现有应用“搬迁”到云平台，不进行大规模更改（也称为“lift and shift”）。</li></ul></li><li><strong>制定迁移计划</strong>：包含迁移的时间表、资源需求、预算、团队分工等，确保各项准备工作都能按时完成。</li></ul><h3 id=2-迁云实施>2. <strong>迁云实施</strong>
<a class=anchor href=#2-%e8%bf%81%e4%ba%91%e5%ae%9e%e6%96%bd>#</a></h3><ul><li><strong>数据迁移</strong>：将现有的数据库、文件、存储等迁移到云平台。通常需要选择合适的数据迁移工具（如 AWS Database Migration Service，Azure Data Box 等）。</li><li><strong>应用迁移</strong>：将现有应用部署到云平台。可以通过容器化、虚拟机、或重构应用架构等方式来进行迁移。</li><li><strong>网络配置</strong>：配置云平台中的虚拟网络、VPN、子网等，确保网络连接性与安全性。</li><li><strong>安全性和合规性</strong>：确保迁移后的环境满足相应的安全和合规要求，配置云平台的身份与访问管理（IAM）、加密等安全措施。</li><li><strong>自动化与监控</strong>：在云平台上设置自动化和监控工具，确保应用和基础设施能够高效稳定运行。</li></ul><h3 id=3-迁云后的优化>3. <strong>迁云后的优化</strong>
<a class=anchor href=#3-%e8%bf%81%e4%ba%91%e5%90%8e%e7%9a%84%e4%bc%98%e5%8c%96>#</a></h3><ul><li><strong>性能优化</strong>：迁移后的系统可能会面临性能瓶颈，需根据云平台的特性进行性能调优，如选择合适的实例类型、优化存储和数据库性能等。</li><li><strong>成本优化</strong>：通过云平台的成本管理工具进行优化，监控资源使用情况，避免浪费。</li><li><strong>备份和恢复</strong>：为数据和应用设置灾备与备份策略，确保在云环境中的数据安全。</li><li><strong>持续监控</strong>：迁移后，要持续监控应用和系统的健康状态，及时发现并解决潜在问题。</li></ul><h3 id=4-如何查看迁云工作进展>4. <strong>如何查看迁云工作进展</strong>
<a class=anchor href=#4-%e5%a6%82%e4%bd%95%e6%9f%a5%e7%9c%8b%e8%bf%81%e4%ba%91%e5%b7%a5%e4%bd%9c%e8%bf%9b%e5%b1%95>#</a></h3><ul><li><strong>项目管理工具</strong>：使用项目管理工具（如 Jira、Trello 等）查看迁云项目的进度、任务分配、里程碑等。</li><li><strong>迁移工具报告</strong>：云平台通常提供迁移工具的状态报告，如迁移进度、错误日志、已完成的步骤等。</li><li><strong>监控系统</strong>：通过云平台的监控工具（如 AWS CloudWatch，Azure Monitor 等）查看迁移后的应用和基础设施的健康状况、性能指标等。</li><li><strong>团队会议和沟通</strong>：定期与项目团队开会，沟通迁云进度、面临的挑战和解决方案。</li><li><strong>文档和知识库</strong>：查看项目相关的文档和知识库，确保团队成员之间的信息传递和共享。</li></ul><h3 id=5-常见的迁云工具和方法>5. <strong>常见的迁云工具和方法</strong>
<a class=anchor href=#5-%e5%b8%b8%e8%a7%81%e7%9a%84%e8%bf%81%e4%ba%91%e5%b7%a5%e5%85%b7%e5%92%8c%e6%96%b9%e6%b3%95>#</a></h3><ul><li><strong>Lift-and-Shift</strong>：不对应用进行修改，直接迁移到云。适用于已经标准化和虚拟化的应用。</li><li><strong>CloudEndure</strong>：提供零停机的迁移方案，支持 AWS、Azure 和 Google Cloud。</li><li><strong>AWS Migration Hub</strong>：用于监控迁移进度和迁移流程。</li><li><strong>Azure Migrate</strong>：Azure 提供的迁移工具，支持多种迁移方式。</li><li><strong>Google Cloud Migrate</strong>：Google 提供的迁移工具，支持多云环境的数据迁移。</li></ul><p>通过以上步骤和工具，你可以系统地管理迁云工作，确保顺利完成从传统环境到云平台的迁移过程。</p><p>做售中。</p><p>原厂加生态。</p><p>以做事为主。</p><p>出差有问题吗？</p></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><ul><li><a href=#1-编排与调度>1. <strong>编排与调度</strong></a></li><li><a href=#2-资源管理>2. <strong>资源管理</strong></a></li><li><a href=#3-生命周期管理>3. <strong>生命周期管理</strong></a></li><li><a href=#4-网络管理>4. <strong>网络管理</strong></a></li><li><a href=#5-存储管理>5. <strong>存储管理</strong></a></li><li><a href=#6-安全与访问控制>6. <strong>安全与访问控制</strong></a></li></ul></li></ul><ul><li><ul><li><a href=#1-namespace命名空间隔离><strong>1. Namespace（命名空间隔离）</strong></a></li><li><a href=#2-cgroups控制组><strong>2. Cgroups（控制组）</strong></a></li><li><a href=#3-文件系统隔离><strong>3. 文件系统隔离</strong></a></li><li><a href=#总结><strong>总结</strong></a></li></ul></li></ul><ul><li><ul><li><a href=#1-liveness-probe存活探针><strong>1. Liveness Probe（存活探针）</strong></a></li><li><a href=#2-readiness-probe就绪探针><strong>2. Readiness Probe（就绪探针）</strong></a></li><li><a href=#3-startup-probe启动探针><strong>3. Startup Probe（启动探针）</strong></a></li><li><a href=#探针的检测方式><strong>探针的检测方式</strong></a></li><li><a href=#总结-1><strong>总结</strong></a></li></ul></li></ul><ul><li><ul><li><a href=#1-存活检测liveness-probe><strong>1. 存活检测（Liveness Probe）</strong></a></li><li><a href=#2-就绪检测readiness-probe><strong>2. 就绪检测（Readiness Probe）</strong></a></li><li><a href=#存活检测与就绪检测的差异><strong>存活检测与就绪检测的差异</strong></a></li><li><a href=#如何选择使用><strong>如何选择使用</strong></a></li></ul></li></ul><ul><li><ul><li><a href=#liveness-probe存活检测><strong>Liveness Probe（存活检测）</strong></a></li><li><a href=#readiness-probe就绪检测><strong>Readiness Probe（就绪检测）</strong></a></li><li><a href=#liveness-probe-与-readiness-probe-的对比><strong>Liveness Probe 与 Readiness Probe 的对比</strong></a></li><li><a href=#总结-2><strong>总结</strong></a></li></ul></li></ul><ul><li><ul><li><a href=#1-容器可能会接收到流量导致错误响应>1. <strong>容器可能会接收到流量，导致错误响应</strong></a></li><li><a href=#2-滚动更新时流量可能会被发送到未准备好的容器>2. <strong>滚动更新时，流量可能会被发送到未准备好的容器</strong></a></li><li><a href=#3-错误的容器可能被认为是健康的>3. <strong>错误的容器可能被认为是健康的</strong></a></li><li><a href=#4-无法区分容器启动状态和运行状态>4. <strong>无法区分容器启动状态和运行状态</strong></a></li><li><a href=#5-无法有效控制流量分配>5. <strong>无法有效控制流量分配</strong></a></li><li><a href=#总结-3>总结</a></li></ul></li></ul><ul><li><ul><li><a href=#如何实现就绪检测>如何实现就绪检测？</a></li><li><a href=#配置方式>配置方式</a></li><li><a href=#如何保证容器是就绪的>如何保证容器是就绪的？</a></li><li><a href=#小结>小结</a></li></ul></li></ul><ul><li><ul><li><a href=#就绪检测判断的内容>就绪检测判断的内容：</a></li><li><a href=#如何实现就绪检测-1>如何实现就绪检测？</a></li><li><a href=#判断什么东西>判断什么东西？</a></li><li><a href=#就绪检测如何工作>就绪检测如何工作？</a></li><li><a href=#适用场景>适用场景：</a></li><li><a href=#小结-1>小结：</a></li></ul></li></ul><ul><li><ul><li><a href=#存活检测与就绪检测的区别>存活检测与就绪检测的区别：</a></li><li><a href=#即使端口通了应用可能仍未就绪的原因>即使端口通了，应用可能仍未就绪的原因：</a></li><li><a href=#举例说明>举例说明：</a></li><li><a href=#可能导致容器未就绪的其他因素>可能导致容器未就绪的其他因素：</a></li><li><a href=#小结-2>小结：</a></li></ul></li></ul><ul><li><ul><li><a href=#hpa-的工作原理><strong>HPA 的工作原理</strong>：</a></li><li><a href=#hpa-如何工作><strong>HPA 如何工作</strong>：</a></li><li><a href=#配置-hpa><strong>配置 HPA</strong>：</a></li><li><a href=#优点-2><strong>优点</strong>：</a></li><li><a href=#适用场景-1><strong>适用场景</strong>：</a></li><li><a href=#缺点-2><strong>缺点</strong>：</a></li><li><a href=#总结-4><strong>总结</strong>：</a></li></ul></li></ul><ul><li><ul><li><a href=#1-cpu-使用率>1. <strong>CPU 使用率</strong>：</a></li><li><a href=#2-内存使用率>2. <strong>内存使用率</strong>：</a></li><li><a href=#3-自定义指标>3. <strong>自定义指标</strong>：</a></li><li><a href=#4-目标指标配置>4. <strong>目标指标配置</strong>：</a></li><li><a href=#如何收集自定义指标><strong>如何收集自定义指标</strong>：</a></li><li><a href=#总结-5><strong>总结</strong>：</a></li></ul></li></ul><ul><li><ul><li><a href=#1-内存泄漏问题>1. <strong>内存泄漏问题</strong>：</a></li><li><a href=#2-内存使用高峰>2. <strong>内存使用高峰</strong>：</a></li><li><a href=#3-内存配置不当>3. <strong>内存配置不当</strong>：</a></li><li><a href=#4-java-应用的内存管理>4. <strong>Java 应用的内存管理</strong>：</a></li><li><a href=#5-内存与-cpu-之间的关系>5. <strong>内存与 CPU 之间的关系</strong>：</a></li><li><a href=#6-延迟问题>6. <strong>延迟问题</strong>：</a></li><li><a href=#总结-6>总结：</a></li><li><a href=#requests-和-limits-的作用><code>requests</code> 和 <code>limits</code> 的作用：</a></li><li><a href=#解释>解释：</a></li><li><a href=#举个例子>举个例子：</a></li><li><a href=#总结-7>总结：</a></li></ul></li></ul><ul><li><ul><li><a href=#可能导致的问题>可能导致的问题：</a></li><li><a href=#解决方案>解决方案：</a></li><li><a href=#总结-8>总结：</a></li></ul></li></ul><ul><li><ul><li><a href=#1-cpu-使用率cpu-utilization>1. <strong>CPU 使用率</strong>（CPU Utilization）</a></li><li><a href=#2-内存使用率memory-utilization>2. <strong>内存使用率</strong>（Memory Utilization）</a></li><li><a href=#3-自定义指标custom-metrics>3. <strong>自定义指标</strong>（Custom Metrics）</a></li><li><a href=#4-请求数和响应时间等基于业务的指标>4. <strong>请求数和响应时间等（基于业务的指标）</strong></a></li><li><a href=#5-网络流量network-traffic>5. <strong>网络流量</strong>（Network Traffic）</a></li><li><a href=#6-多个指标组合multiple-metrics>6. <strong>多个指标组合</strong>（Multiple Metrics）</a></li><li><a href=#hpa-的工作原理-1>HPA 的工作原理：</a></li><li><a href=#适用性-2>适用性：</a></li><li><a href=#总结-9>总结：</a></li></ul></li></ul><ul><li><ul><li><a href=#1-service---clusterip集群内部暴露>1. <strong>Service - ClusterIP（集群内部暴露）</strong></a></li><li><a href=#2-service---nodeport集群外部暴露>2. <strong>Service - NodePort（集群外部暴露）</strong></a></li><li><a href=#3-service---loadbalancer外部负载均衡器>3. <strong>Service - LoadBalancer（外部负载均衡器）</strong></a></li><li><a href=#4-ingress入口控制器>4. <strong>Ingress（入口控制器）</strong></a></li><li><a href=#5-externalname>5. <strong>ExternalName</strong></a></li><li><a href=#总结-10>总结：</a></li></ul></li></ul><ul><li><ul><li><a href=#ingress-的优点><strong>Ingress 的优点：</strong></a></li><li><a href=#ingress-的缺点><strong>Ingress 的缺点：</strong></a></li><li><a href=#nodeport-的优点><strong>NodePort 的优点：</strong></a></li><li><a href=#nodeport-的缺点><strong>NodePort 的缺点：</strong></a></li><li><a href=#总结对比><strong>总结对比：</strong></a></li><li><a href=#适用场景-2><strong>适用场景</strong>：</a></li></ul></li></ul><ul><li><ul><li><a href=#1-ingress应用层layer-7><strong>1. Ingress：应用层（Layer 7）</strong></a></li><li><a href=#2-nodeport传输层layer-4><strong>2. NodePort：传输层（Layer 4）</strong></a></li><li><a href=#总结-11><strong>总结：</strong></a></li></ul></li></ul><ul><li><ul><li><a href=#1-明确沟通理解需求>1. <strong>明确沟通，理解需求</strong></a></li><li><a href=#2-评估影响和可行性>2. <strong>评估影响和可行性</strong></a></li><li><a href=#3-与上级或同事协商>3. <strong>与上级或同事协商</strong></a></li><li><a href=#4-明确边界调整合同或工作协议>4. <strong>明确边界，调整合同或工作协议</strong></a></li><li><a href=#5-灵活应对保持专业>5. <strong>灵活应对，保持专业</strong></a></li></ul></li></ul><ul><li><ul><li><a href=#1-了解需求的细节>1. <strong>了解需求的细节</strong></a></li><li><a href=#2-评估影响与可行性>2. <strong>评估影响与可行性</strong></a></li><li><a href=#3-与团队讨论并调整计划>3. <strong>与团队讨论并调整计划</strong></a></li><li><a href=#4-与甲方沟通调整计划与交付>4. <strong>与甲方沟通调整计划与交付</strong></a></li><li><a href=#5-签订变更协议或文档>5. <strong>签订变更协议或文档</strong></a></li><li><a href=#6-持续监控与管理>6. <strong>持续监控与管理</strong></a></li><li><a href=#7-总结与教训>7. <strong>总结与教训</strong></a></li><li><a href=#总结-12>总结</a></li></ul></li></ul><ul><li><ul><li><a href=#1-备份-etcd-数据>1. <strong>备份 Etcd 数据</strong></a></li><li><a href=#2-证书管理与生成>2. <strong>证书管理与生成</strong></a></li><li><a href=#3-集群扩展操作>3. <strong>集群扩展操作</strong></a></li><li><a href=#4-验证和监控>4. <strong>验证和监控</strong></a></li><li><a href=#5-回退策略>5. <strong>回退策略</strong></a></li><li><a href=#总结-13>总结</a></li></ul></li></ul><ul><li><ul><li><a href=#1-标记节点为不可调度-cordon>1. <strong>标记节点为不可调度 (Cordon)</strong></a></li><li><a href=#2-迁移-pods-drain>2. <strong>迁移 Pods (Drain)</strong></a></li><li><a href=#3-删除节点>3. <strong>删除节点</strong></a></li><li><a href=#4-恢复节点>4. <strong>恢复节点</strong></a></li><li><a href=#5-验证节点状态>5. <strong>验证节点状态</strong></a></li><li><a href=#6-其他注意事项>6. <strong>其他注意事项</strong></a></li><li><a href=#7-批量下线多个节点>7. <strong>批量下线多个节点</strong></a></li></ul></li></ul><ul><li><ul><li><a href=#驱逐节点上的所有-pod>驱逐节点上的所有 Pod</a></li><li><a href=#示例-3>示例</a></li><li><a href=#重要提示>重要提示</a></li><li><a href=#恢复节点>恢复节点</a></li></ul></li></ul><ul><li><ul><li><a href=#1-迁云准备工作>1. <strong>迁云准备工作</strong></a></li><li><a href=#2-迁云实施>2. <strong>迁云实施</strong></a></li><li><a href=#3-迁云后的优化>3. <strong>迁云后的优化</strong></a></li><li><a href=#4-如何查看迁云工作进展>4. <strong>如何查看迁云工作进展</strong></a></li><li><a href=#5-常见的迁云工具和方法>5. <strong>常见的迁云工具和方法</strong></a></li></ul></li></ul></nav></div></aside></main></body></html>