<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  你们的k8s集群的高可用架构怎么设计的？
  #

5个master节点 剩下是node节点，联邦，2个控制面集群和38运行面集群 组成k8s联邦，控制面k8s集群只跑paas平台的应用，运行面集群跑业务应用，AD BC 分区，多AZ夸机房容灾
Kubernetes 集群的高可用架构设计对于确保系统的稳定性、可扩展性和容灾能力非常重要。根据你描述的设计架构，以下是详细的分析和展开：

  1. 5个 Master 节点（控制面）
  #


多节点设计：高可用的 Kubernetes 集群需要至少三个 master 节点来提供冗余，以避免单点故障。你选择了 5 个 master 节点，这样可以进一步增强集群的高可用性，避免任何 2 个节点宕机时集群服务的不可用。
控制面职责：Kubernetes master 节点负责调度、集群状态管理、API 服务器、控制器管理器等核心功能。将 master 节点分布在不同的物理或虚拟机上，并使用负载均衡器进行流量分配，可以有效避免 master 节点单点故障。
负载均衡：使用负载均衡器（如 Nginx、HAProxy 或云服务商的负载均衡）对外暴露 Kubernetes API Server，使得所有的客户端（如 kubectl、应用）能够在多个 master 节点之间负载均衡，避免单一 master 节点的宕机导致不可用。


  2. Node 节点（工作节点）
  #


运行业务应用：Kubernetes 的 node 节点负责运行容器化的业务应用。在该架构下，节点数目通常根据工作负载的需求进行扩展。每个 node 节点运行 kubelet 和 kube-proxy，这些组件管理节点的健康和服务发现。
自动扩容：可以通过 Kubernetes 集群的 HPA（Horizontal Pod Autoscaler）来根据业务需求动态地增加或减少 node 节点的数量，从而应对负载的波动。


  3. 联邦架构（Federation）
  #


控制面集群和运行面集群的分离：在联邦架构中，控制面集群负责管理和控制集群的整体状态，但不运行具体的业务应用。而运行面集群专注于运行业务应用，负载由控制面集群统一调度。控制面集群通常只运行 PaaS 平台相关的应用，而业务应用则运行在运行面集群中。
联邦调度：Kubernetes 联邦可以跨多个集群进行调度，支持多个集群中的服务发现、跨集群负载均衡等功能。使用 Kubernetes 联邦可以实现跨地域、跨数据中心的服务扩展，从而保证高可用性。
跨区域容灾：联邦架构中的多个运行面集群可以部署在不同的可用区域（Availability Zone，AZ）和不同的数据中心。通过合理的设计，可以在一个区域发生故障时，自动将流量切换到另一个区域，从而实现跨区域容灾。


  4. AD BC 分区
  #


AD 和 BC 是不同的区域分区：这里的 AD 和 BC 很可能指的是不同的应用区域分区或者不同的业务功能模块分区。AD 和 BC 代表了两类不同的业务场景，每个分区可能有独立的业务需求和安全隔离要求。
分区管理：通过为不同的业务模块使用不同的 Kubernetes Namespace，可以在同一集群中实现一定程度的资源隔离、权限管理和网络隔离。


  5. 多 AZ 跨机房容灾设计
  #


跨机房部署：通过将 Kubernetes 集群部署在多个数据中心或多个可用区（AZ）之间，实现高可用和灾难恢复。例如，使用多个 master 节点跨机房部署，并利用云服务提供的负载均衡和高可用性网络设计，保证 master 节点能够跨机房访问且高可用。
Pod 跨 AZ 部署：在 Kubernetes 集群中，Pod 可以跨多个 AZ 部署，通过配置 topologySpreadConstraints 或使用适当的亲和性和反亲和性策略，确保应用在多个 AZ 中有冗余备份，避免某个 AZ 故障导致整个服务不可用。
Persistent Volume 跨 AZ 存储：使用跨 AZ 的持久化存储（如云提供商的跨区域存储卷）确保在一个 AZ 故障时，应用的持久化数据能够快速恢复。某些云平台如 AWS EBS 支持跨可用区的数据复制。


  6. 健康检查和负载均衡
  #


健康检查：通过存活检测和就绪检测确保每个组件（如 kube-apiserver、etcd、kubelet 等）都处于正常状态。如果某个组件失败，自动进行自愈或重新调度。
Pod 调度和负载均衡：Kubernetes 调度器会根据集群的负载和容器的资源需求，将 Pod 分配到合适的 Node 节点上，同时利用 Service 和 Ingress 对外暴露应用，确保流量能够在不同的 Pod 和节点之间负载均衡。


  7. 灾难恢复与容错
  #


Etcd 高可用：Etcd 是 Kubernetes 集群的关键组件，存储了集群的所有配置信息和状态。通过将 Etcd 部署为高可用集群（通常是奇数个节点，如 3 个或 5 个节点），确保在任意节点故障时，集群依然能够正常工作。
多区域冗余：通过跨区域部署 Etcd 节点和 Master 节点，确保在单个区域或数据中心发生故障时，其他区域可以继续提供服务。即使在大规模灾难中，也能最大程度地保持系统的可用性。


  8. 备份与恢复
  #


定期备份：定期备份 Kubernetes 集群的核心组件（如 etcd、kube-apiserver 的配置等），以应对数据丢失或灾难恢复需求。
恢复方案：测试并制定恢复方案，在发生故障时能够迅速将集群恢复到正常状态。例如，可以使用 Velero 或其他备份工具进行集群备份和恢复。


  总结
  #

通过这样的高可用架构设计，Kubernetes 集群能够在面对节点故障、区域故障等情况下保持高度的可靠性和可扩展性。联邦架构、跨机房容灾、多 AZ 部署、Etcd 高可用等设计确保了应用在不同的业务需求下能够得到良好的支持，同时具备了强大的容错能力和扩展能力。"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://qq547475331.github.io/docs/2025-3-14-vivo%E9%9D%A2%E8%AF%95/"><meta property="og:site_name" content="Guichen's Blog"><meta property="og:title" content="2025-3-14 vivo面试"><meta property="og:description" content="你们的k8s集群的高可用架构怎么设计的？ # 5个master节点 剩下是node节点，联邦，2个控制面集群和38运行面集群 组成k8s联邦，控制面k8s集群只跑paas平台的应用，运行面集群跑业务应用，AD BC 分区，多AZ夸机房容灾
Kubernetes 集群的高可用架构设计对于确保系统的稳定性、可扩展性和容灾能力非常重要。根据你描述的设计架构，以下是详细的分析和展开：
1. 5个 Master 节点（控制面） # 多节点设计：高可用的 Kubernetes 集群需要至少三个 master 节点来提供冗余，以避免单点故障。你选择了 5 个 master 节点，这样可以进一步增强集群的高可用性，避免任何 2 个节点宕机时集群服务的不可用。 控制面职责：Kubernetes master 节点负责调度、集群状态管理、API 服务器、控制器管理器等核心功能。将 master 节点分布在不同的物理或虚拟机上，并使用负载均衡器进行流量分配，可以有效避免 master 节点单点故障。 负载均衡：使用负载均衡器（如 Nginx、HAProxy 或云服务商的负载均衡）对外暴露 Kubernetes API Server，使得所有的客户端（如 kubectl、应用）能够在多个 master 节点之间负载均衡，避免单一 master 节点的宕机导致不可用。 2. Node 节点（工作节点） # 运行业务应用：Kubernetes 的 node 节点负责运行容器化的业务应用。在该架构下，节点数目通常根据工作负载的需求进行扩展。每个 node 节点运行 kubelet 和 kube-proxy，这些组件管理节点的健康和服务发现。 自动扩容：可以通过 Kubernetes 集群的 HPA（Horizontal Pod Autoscaler）来根据业务需求动态地增加或减少 node 节点的数量，从而应对负载的波动。 3. 联邦架构（Federation） # 控制面集群和运行面集群的分离：在联邦架构中，控制面集群负责管理和控制集群的整体状态，但不运行具体的业务应用。而运行面集群专注于运行业务应用，负载由控制面集群统一调度。控制面集群通常只运行 PaaS 平台相关的应用，而业务应用则运行在运行面集群中。 联邦调度：Kubernetes 联邦可以跨多个集群进行调度，支持多个集群中的服务发现、跨集群负载均衡等功能。使用 Kubernetes 联邦可以实现跨地域、跨数据中心的服务扩展，从而保证高可用性。 跨区域容灾：联邦架构中的多个运行面集群可以部署在不同的可用区域（Availability Zone，AZ）和不同的数据中心。通过合理的设计，可以在一个区域发生故障时，自动将流量切换到另一个区域，从而实现跨区域容灾。 4. AD BC 分区 # AD 和 BC 是不同的区域分区：这里的 AD 和 BC 很可能指的是不同的应用区域分区或者不同的业务功能模块分区。AD 和 BC 代表了两类不同的业务场景，每个分区可能有独立的业务需求和安全隔离要求。 分区管理：通过为不同的业务模块使用不同的 Kubernetes Namespace，可以在同一集群中实现一定程度的资源隔离、权限管理和网络隔离。 5. 多 AZ 跨机房容灾设计 # 跨机房部署：通过将 Kubernetes 集群部署在多个数据中心或多个可用区（AZ）之间，实现高可用和灾难恢复。例如，使用多个 master 节点跨机房部署，并利用云服务提供的负载均衡和高可用性网络设计，保证 master 节点能够跨机房访问且高可用。 Pod 跨 AZ 部署：在 Kubernetes 集群中，Pod 可以跨多个 AZ 部署，通过配置 topologySpreadConstraints 或使用适当的亲和性和反亲和性策略，确保应用在多个 AZ 中有冗余备份，避免某个 AZ 故障导致整个服务不可用。 Persistent Volume 跨 AZ 存储：使用跨 AZ 的持久化存储（如云提供商的跨区域存储卷）确保在一个 AZ 故障时，应用的持久化数据能够快速恢复。某些云平台如 AWS EBS 支持跨可用区的数据复制。 6. 健康检查和负载均衡 # 健康检查：通过存活检测和就绪检测确保每个组件（如 kube-apiserver、etcd、kubelet 等）都处于正常状态。如果某个组件失败，自动进行自愈或重新调度。 Pod 调度和负载均衡：Kubernetes 调度器会根据集群的负载和容器的资源需求，将 Pod 分配到合适的 Node 节点上，同时利用 Service 和 Ingress 对外暴露应用，确保流量能够在不同的 Pod 和节点之间负载均衡。 7. 灾难恢复与容错 # Etcd 高可用：Etcd 是 Kubernetes 集群的关键组件，存储了集群的所有配置信息和状态。通过将 Etcd 部署为高可用集群（通常是奇数个节点，如 3 个或 5 个节点），确保在任意节点故障时，集群依然能够正常工作。 多区域冗余：通过跨区域部署 Etcd 节点和 Master 节点，确保在单个区域或数据中心发生故障时，其他区域可以继续提供服务。即使在大规模灾难中，也能最大程度地保持系统的可用性。 8. 备份与恢复 # 定期备份：定期备份 Kubernetes 集群的核心组件（如 etcd、kube-apiserver 的配置等），以应对数据丢失或灾难恢复需求。 恢复方案：测试并制定恢复方案，在发生故障时能够迅速将集群恢复到正常状态。例如，可以使用 Velero 或其他备份工具进行集群备份和恢复。 总结 # 通过这样的高可用架构设计，Kubernetes 集群能够在面对节点故障、区域故障等情况下保持高度的可靠性和可扩展性。联邦架构、跨机房容灾、多 AZ 部署、Etcd 高可用等设计确保了应用在不同的业务需求下能够得到良好的支持，同时具备了强大的容错能力和扩展能力。"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><title>2025-3-14 vivo面试 | Guichen's Blog</title>
<link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://qq547475331.github.io/docs/2025-3-14-vivo%E9%9D%A2%E8%AF%95/><link rel=stylesheet href=/book.min.6c8b9d2a1fc95075ed7da46ca81060b39add8fff6741ac51259f768929281e2c.css integrity="sha256-bIudKh/JUHXtfaRsqBBgs5rdj/9nQaxRJZ92iSkoHiw=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.f95c6b9a60666b7048cee04630cfc1e8d9370fb96c18c5c4d047db5e7a5d2e18.js integrity="sha256-+VxrmmBma3BIzuBGMM/B6Nk3D7lsGMXE0EfbXnpdLhg=" crossorigin=anonymous></script></head><script src=https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.2.3/mermaid.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){mermaid.initialize({startOnLoad:!0});let e=document.querySelectorAll("pre > code.language-mermaid");e.forEach(e=>{let t=document.createElement("div");t.classList.add("mermaid"),t.innerHTML=e.innerText,e.parentNode.replaceWith(t)}),mermaid.init(void 0,document.querySelectorAll(".mermaid"))})</script><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>Guichen's Blog</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><a href=/docs/2025-4-16-%E8%87%AA%E7%A0%94k8s%E5%B9%B3%E5%8F%B0/>2025-4-16 自研k8s平台</a></li><li><a href=/docs/2025-4-16-sleep%E7%9D%A1%E7%9C%A0%E5%BA%94%E7%94%A8/>2025-4-16 sleep睡眠应用</a></li><li><a href=/docs/2025-4-16-paas%E8%AE%BE%E8%AE%A1/>2025-4-16 paas开发记录</a></li><li><a href=/docs/2025-4-16-cursoe-free-vip/>2025-4-16 Cursor Free VIP</a></li><li><a href=/docs/2025-4-16-boss%E7%9B%B4%E8%81%98%E8%87%AA%E5%8A%A8%E6%8A%95%E9%80%92/>2025-4-16 BOSS直聘自动投递</a></li><li><a href=/docs/2025-3-30-metallb/>2025-3-30 metallb</a></li><li><a href=/docs/2025-3-24-%E8%87%AA%E6%88%91%E4%BB%8B%E7%BB%8D/>2025-3-24 自我介绍</a></li><li><a href=/docs/2025-3-20-victoriametrics-%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84/>2025-3-20 victoriametrics高可用架构</a></li><li><a href=/docs/2025-3-20-victoriametrics%E6%9E%B6%E6%9E%84/>2025-3-20 victoriametrics 架构</a></li><li><a href=/docs/2025-3-20-victoriametrics%E5%92%8Cthanos%E5%AF%B9%E6%AF%94/>2025-3-20 VictoriaMetrics 和 Thanos 对比</a></li><li><a href=/docs/2025-3-20-thanos%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84/>2025-3-20 thanos高可用架构</a></li><li><a href=/docs/2025-3-20-thanos%E6%9E%B6%E6%9E%84/>2025-3-20 thanos架构</a></li><li><a href=/docs/2025-3-18-5w-pod%E5%8E%8B%E6%B5%8B%E5%A4%8D%E7%9B%98/>2025-3-18 5w pod压测复盘</a></li><li><a href=/docs/2025-3-14-%E7%81%AB%E5%B1%B1%E4%BA%91%E8%BF%81%E7%A7%BB%E5%B7%A5%E7%A8%8B%E5%B8%88%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/>2025-3-14 火山云迁移工程师面试记录</a></li><li><a href=/docs/2025-3-14-vivo%E9%9D%A2%E8%AF%95/ class=active>2025-3-14 vivo面试</a></li><li><a href=/docs/2025-3-13-istio%E6%B5%81%E9%87%8F%E5%88%86%E6%9E%90/>2025-3-13 istio流量分析</a></li><li><a href=/docs/2025-3-13-calico%E4%B8%89%E7%A7%8D%E6%A8%A1%E5%BC%8F%E4%B8%8B%E6%B5%81%E9%87%8F%E4%BC%A0%E8%BE%93%E8%B7%AF%E5%BE%84%E5%88%86%E6%9E%90/>2025-3-13 calico三种模式下流量传输</a></li><li><a href=/docs/2025-3-12-%E5%A1%94%E8%B5%9E%E9%9D%A2%E8%AF%95/>2025-3-12 塔赞面试</a></li><li><a href=/docs/2025-3-12-%E8%BF%BD%E8%A7%85%E9%9D%A2%E8%AF%95/>2025-3-12 追觅面试</a></li><li><a href=/docs/2025-3-8-k8s%E5%88%A0%E9%99%A4pod-deployment%E7%9A%84%E6%B5%81%E7%A8%8B%E5%9B%BE%E8%AF%A6%E8%A7%A3/>2025-3-08 k8s删除pod或deployment的流程图详解</a></li><li><a href=/docs/2025-3-8-k8s%E5%88%9B%E5%BB%BApod-deployment%E6%B5%81%E7%A8%8B%E5%9B%BE%E8%AF%A6%E8%A7%A3/>2025-3-08 k8s创建pod流程图详解</a></li><li><a href=/docs/2025-2-28-prometheus%E9%A2%98%E7%9B%AE/>2025-2-28 prometheus面试题</a></li><li><a href=/docs/2025-2-26-%E9%9D%A2%E8%AF%950225/>2025-2-25 面试0225</a></li><li><a href=/docs/2025-2-24-%E9%AB%98%E7%BA%A7%E8%BF%90%E7%BB%B4%E9%9D%A2%E8%AF%95%E9%A2%98_ai_linux%E9%83%A8%E5%88%86/>2025-2-24 高级运维面试题-linux部分</a></li><li><a href=/docs/2025-2-24-%E4%B8%AD%E7%BA%A7%E8%BF%90%E7%BB%B4%E9%9D%A2%E8%AF%95%E9%A2%98_%E9%A2%98%E7%9B%AE/>2025-2-24 中级运维面试题</a></li><li><a href=/docs/2025-2-24-%E9%9D%A2%E8%AF%950224/>2025-2-24 0224面试</a></li><li><a href=/docs/2025-2-20-%E9%9D%A2%E8%AF%950220/>2025-2-20 面试0220</a></li><li><a href=/docs/2025-2-19-%E9%9D%A2%E8%AF%950219/>2025-2-19 面试0219</a></li><li><a href=/docs/2025-2-18-%E9%9D%A2%E8%AF%95/>2025-2-18 面试2025-0218</a></li><li><a href=/docs/2025-2-26-k8s%E7%9B%B8%E5%85%B3/>2025-2-16 k8s题目</a></li><li><a href=/docs/2025-2-12-%E9%9D%A2%E8%AF%950212/>2025-2-12 面试0212</a></li><li><a href=/docs/2025-2-11-%E9%9D%A2%E8%AF%950211/>2025-2-11 面试2025-02-11</a></li><li><a href=/docs/2025-2-7-%E8%AE%A1%E5%88%922/>2025-2-07 美国码农计划</a></li><li><a href=/docs/2025-2-7-%E8%AE%A1%E5%88%92/>2025-2-07 美国码农薪酬</a></li><li><a href=/docs/2025-2-7-k8s%E7%BB%84%E4%BB%B6/>2025-2-07 k8s组件</a></li><li><a href=/docs/2025-1-16-k8s%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E6%8C%87%E5%8D%97/>2025-1-16 k8s常见故障指南</a></li><li><a href=/docs/2025-1-1-%E8%A6%81%E4%B8%8D%E8%A6%81%E5%88%9B%E4%B8%9A/>2025-1-1 要不要创业</a></li><li><a href=/docs/2025-1-1-%E6%97%A9%E6%9C%9F%E6%A8%A1%E5%BC%8F/>2025-1-1 早期模式</a></li><li><a href=/docs/2025-1-1-%E5%A4%A7%E5%A0%B0%E6%B2%B3-%E6%88%91%E7%9A%84%E4%BF%9D%E5%A7%86/>2025-1-1 大堰河-我的保姆</a></li><li><a href=/docs/2025-1-1-%E5%88%9D%E5%88%9B%E5%85%AC%E5%8F%B8/>2025-1-1 初创公司</a></li><li><a href=/docs/2025-1-1-%E5%88%9B%E4%B8%9A%E8%80%85%E4%BA%A4%E6%B5%81/>2025-1-1 创业者交流</a></li><li><a href=/docs/2025-1-1-%E5%88%9B%E4%B8%9A%E7%82%B9%E5%AD%90/>2025-1-1 创业点子</a></li><li><a href=/docs/2025-1-1-sealos%E8%8E%B7%E6%8A%95/>2025-1-1 sealos获投</a></li><li><a href=/docs/2024-12-10-docker-registrry/>2024-12-10 docker registrry</a></li><li><a href=/docs/2024-12-09-openstack-ssh%E8%BF%9E%E6%8E%A5/>2024-12-09 openstack ssh连接</a></li><li><a href=/docs/2024-12-08-mutilpass%E9%83%A8%E7%BD%B2openstack/>2024-12-09 mutilpass部署openstack devstack形式</a></li><li><a href=/docs/2024-12-09-helmchart-%E9%83%A8%E7%BD%B2flask%E5%BA%94%E7%94%A8/>2024-12-09 helmchart 部署flask应用</a></li><li><a href=/docs/2024-12-09-docker-daemon.json/>2024-12-09 docker daemon.json</a></li><li><a href=/docs/2024-12-08-%E5%9D%97%E5%AD%98%E5%82%A8%E5%92%8C%E5%AF%B9%E8%B1%A1%E5%82%A8%E5%AD%98%E5%8C%BA%E5%88%AB/>2024-12-08 块存储和对象储存区别</a></li><li><a href=/docs/2024-12-08-openstack%E9%9C%80%E8%A6%81%E5%87%A0%E5%8F%B0%E8%99%9A%E6%8B%9F%E6%9C%BA/>2024-12-08 openstack需要几台虚拟机</a></li><li><a href=/docs/2024-12-08-openstack%E5%92%8Ckubernetes%E5%8C%BA%E5%88%AB/>2024-12-08 openstack和kubernetes区别</a></li><li><a href=/docs/2024-12-08-nano%E6%93%8D%E4%BD%9C/>2024-12-08 nano操作</a></li><li><a href=/docs/2024-12-08-mutilpass%E6%93%8D%E4%BD%9C/>2024-12-08 mutilpass操作</a></li><li><a href=/docs/2024-12-08-devstack/>2024-12-08 devstack</a></li><li><a href=/docs/2024-12-07-microk8s/>2024-12-07 microk8s</a></li><li><a href=/docs/2024-12-05-kubeasz%E9%83%A8%E7%BD%B2k8s/>2024-12-05 kubeasz部署k8s</a></li><li><a href=/docs/2024-10-20-%E5%88%9B%E5%BB%BA%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/>2024-10-20 使用 Keepalived 和 HAproxy 创建高可用 Kubernetes 集群</a></li><li><a href=/docs/%E9%A1%B6%E7%BA%A7devops%E5%B7%A5%E5%85%B7%E5%A4%A7%E7%9B%98%E7%82%B9-ding-ji-devops-gong-ju-da-pan-dian/>2024-08-02 顶级devops工具大盘点</a></li><li><a href=/docs/%E6%B8%85%E7%90%86docker%E9%95%9C%E5%83%8F-qing-li-docker-jing-xiang/>2024-08-02 清理docker镜像</a></li><li><a href=/docs/%E6%9E%84%E5%BB%BA%E5%AE%B9%E5%99%A8%E9%95%9C%E5%83%8F%E5%88%A9%E5%99%A8buildkit-gou-jian-rong-qi-jing-xiang-li-qi-buildkit/>2024-08-02 构建容器镜像利器buildkit</a></li><li><a href=/docs/%E6%98%AF%E6%8A%80%E6%9C%AF%E5%A4%A7%E7%A5%9E%E8%BF%98%E6%98%AF%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84%E9%83%A8%E7%9A%84%E7%A5%B8%E5%AE%B3-shi-ji-shu-da-shen-hai-shi-ji-chu-jia-gou-bu-de-huo-hai/>2024-08-02 是技术大神还是基础架构部的祸害</a></li><li><a href=/docs/%E6%90%AD%E4%B8%AA%E6%97%A5%E5%BF%97%E6%89%8B%E6%9C%BA%E7%B3%BB%E7%BB%9F%E4%B8%8D%E9%A6%99%E5%90%97-da-ge-ri-zhi-shou-ji-xi-tong-bu-xiang-ma/>2024-08-02 搭个日志手机系统不香吗</a></li><li><a href=/docs/%E6%88%91%E5%8F%AA%E6%83%B3%E5%81%9A%E6%8A%80%E6%9C%AF-%E8%B5%B0%E6%8A%80%E6%9C%AF%E8%B7%AF%E7%BA%BF-wo-zhi-xiang-zuo-ji-shu-zou-ji-shu-lu-xian/>2024-08-02 我只想做技术 走技术路线</a></li><li><a href=/docs/%E5%B8%B8%E8%A7%81linux%E8%BF%90%E7%BB%B4%E9%9D%A2%E8%AF%95%E9%A2%98-chang-jian-linux-yun-wei-mian-shi-ti/>2024-08-02 常见linux运维面试题</a></li><li><a href=/docs/%E5%A4%A7%E5%8E%82%E6%80%BB%E7%BB%93nginx%E9%AB%98%E5%B9%B6%E5%8F%91%E4%BC%98%E5%8C%96%E7%AC%94%E8%AE%B0-da-chang-zong-jie-nginx-gao-bing-fa-you-hua-bi-ji/>2024-08-02 大厂总结nginx高并发优化笔记</a></li><li><a href=/docs/%E5%8F%B2%E4%B8%8A%E6%9C%80%E7%89%9Bjenkins-pipeline%E6%B5%81%E6%B0%B4%E7%BA%BF%E8%AF%A6%E8%A7%A3-shi-shang-zui-niu-jenkinspipeline-liu-shui-xian-xiang-jie/>2024-08-02 史上最牛jenkins pipeline流水线详解</a></li><li><a href=/docs/teg%E4%B8%8Eistio%E9%9B%86%E6%88%90-teg-yu-istio-ji-cheng/>2024-08-02 TEG与istio集成</a></li><li><a href=/docs/prometheus-stack-prometheus-stack/>2024-08-02 prometheus-stack</a></li><li><a href=/docs/pixie-pixie/>2024-08-02 pixie</a></li><li><a href=/docs/nginx%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E6%83%8A%E7%BE%A4%E6%95%88%E5%BA%94-nginx-ru-he-jie-jue-jing-qun-xiao-ying/>2024-08-02 nginx如何解决惊群效应</a></li><li><a href=/docs/netctl%E6%A3%80%E6%B5%8B%E9%9B%86%E7%BE%A4pod%E9%97%B4%E8%BF%9E%E9%80%9A%E6%80%A7-netctl-jian-ce-ji-qun-pod-jian-lian-tong-xing/>2024-08-02 netctl检测集群pod间连通性</a></li><li><a href=/docs/linux%E8%BF%90%E7%BB%B4%E5%B7%A5%E7%A8%8B%E5%B8%8850%E4%B8%AA%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98-linux-yun-wei-gong-cheng-shi-50-ge-chang-jian-mian-shi-ti/>2024-08-02 linux运维工程师50个常见面试题</a></li><li><a href=/docs/linux%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-%E4%B8%83%E4%B8%AA%E5%AE%9E%E6%88%98%E7%BB%8F%E9%AA%8C-linux-xi-tong-xing-neng-you-hua-qi-ge-shi-zhan-jing-yan/>2024-08-02 linux系统性能优化 七个实战经验</a></li><li><a href=/docs/linux-awk%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E5%99%A8-8%E4%B8%AA%E6%A1%88%E4%BE%8B-linuxawk-wen-ben-chu-li-qi-8-ge-an-li/>2024-08-02 linux awk文本处理器 8个案例</a></li><li><a href=/docs/kubewharf-kubewharf/>2024-08-02 kubewharf</a></li><li><a href=/docs/kruise%E5%8E%9F%E5%9C%B0%E5%8D%87%E7%BA%A7%E8%A7%A3%E6%9E%90-kruise-yuan-de-sheng-ji-jie-xi/>2024-08-02 kruise原地升级解析</a></li><li><a href=/docs/k8s%E9%9D%A2%E8%AF%95%E9%A2%98-k8s-mian-shi-ti/>2024-08-02 K8S面试题</a></li><li><a href=/docs/k8s%E8%83%8C%E5%90%8Eservice%E6%98%AF%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C%E7%9A%84-k8s-bei-hou-service-shi-ru-he-gong-zuo-de/>2024-08-02 k8s背后service是如何工作的</a></li><li><a href=/docs/k8s%E7%9A%84%E6%9C%80%E5%90%8E%E4%B8%80%E5%9D%97%E6%8B%BC%E5%9B%BE-dbpaas-k8s-de-zui-hou-yi-kuai-pin-tu-dbpaas/>2024-08-02 K8S的最后一块拼图</a></li><li><a href=/docs/istio%E9%83%A8%E7%BD%B2-istio-bu-shu/>2024-08-02 istio部署</a></li><li><a href=/docs/istio-ingress-gateway-istio-ingress-gateway/>2024-08-02 istio-ingress-gateway</a></li><li><a href=/docs/godel-scheduler-godel-scheduler/>2024-08-02 godel-scheduler</a></li><li><a href=/docs/dockerfile%E5%AE%9A%E5%88%B6%E4%B8%93%E5%B1%9E%E9%95%9C%E5%83%8F-dockerfile-ding-zhi-zhuan-shu-jing-xiang/>2024-08-02 dockerfile定制专属镜像</a></li><li><a href=/docs/33%E6%AC%BEgitops%E4%B8%8Edevops%E4%B8%BB%E6%B5%81%E7%B3%BB%E7%BB%9F-33-kuan-gitops-yu-devops-zhu-liu-xi-tong/>2024-08-02 33款gitops与devops主流系统</a></li><li><a href=/docs/2024-8-1-linux%E8%BF%90%E7%BB%B4%E5%B7%A5%E7%A8%8B%E5%B8%8850%E4%B8%AA%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/>2024-08-01 linux面试题</a></li><li><a href=/docs/2024-8-1-%E5%B8%B8%E8%A7%81linux%E8%BF%90%E7%BB%B4%E9%9D%A2%E8%AF%95%E9%A2%98%E6%89%BE%E5%B7%A5%E4%BD%9C%E7%9A%84%E5%BF%85%E7%9C%8B/>2024-08-01 linux运维面试题</a></li><li><a href=/docs/2024-8-1-kubernetes%E9%9D%A2%E8%AF%95%E9%A2%98/>2024-08-01 k8s面试题</a></li><li><a href=/docs/openkruise%E8%AF%A6%E7%BB%86%E8%A7%A3%E9%87%8A%E4%BB%A5%E5%8F%8A%E5%8E%9F%E5%9C%B0%E5%8D%87%E7%BA%A7%E5%8F%8A%E5%85%A8%E9%93%BE%E8%B7%AF%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83%E6%96%B9%E6%A1%88-openkruise-xiang-xi-jie-shi-yi-ji-yuan-de-sheng-ji-ji-quan-lian-lu-hui-du-fa-bu-fang-an/>2024-07-22 OpenKruise详细解释以及原地升级及全链路灰度发布方案</a></li><li><a href=/docs/k8s%E4%B9%8Bingress-nginx%E5%8E%9F%E7%90%86%E5%8F%8A%E9%85%8D%E7%BD%AE-k8s-zhi-ingress-nginx-yuan-li-ji-pei-zhi/>2024-07-05 K8S之ingress-nginx原理及配置</a></li><li><a href=/docs/%E4%BD%BF%E7%94%A8cloudflarecf%E6%90%AD%E5%BB%BAdockerhub%E4%BB%A3%E7%90%86-shi-yong-cloudflarecf-da-jian-dockerhub-dai-li/>2024-06-28 使用cloudflare(CF)搭建dockerhub代理</a></li><li><a href=/docs/2024-5-14-%E5%8D%95master%E5%8D%95etcd%E6%94%B9%E9%80%A0/>2024-05-01 单master单etcd改造为3master3etcd</a></li><li><a href=/docs/2024-4-17-%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93/>2024-04-17 面试总结</a></li><li><a href=/docs/%E5%A6%82%E4%BD%95%E4%B8%BAk8s%E4%BF%9D%E9%A9%BE%E6%8A%A4%E8%88%AA-ru-he-wei-k8s-bao-jia-hu-hang/>2024-04-16 如何为K8S保驾护航</a></li><li><a href=/docs/k8s%E5%A6%82%E4%BD%95%E8%8E%B7%E5%BE%97-ip-k8s-ru-he-huo-de-ip/>2024-04-16 K8S如何获得 IP</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Bstateful_setgo%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-statefulsetgo-yuan-ma-jie-du/>2024-04-10 K8S控制器之stateful_set.go源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Bstateful_set_status_updatego%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-statefulsetstatusupdatego-yuan-ma-jie-du/>2024-04-10 K8S控制器之stateful_set_status_update.go源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Bstateful_set_controlgo%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-statefulsetcontrolgo-yuan-ma-jie-du/>2024-04-10 K8S控制器之stateful_set_control.go源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Bstateful_pod_controlgo%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-statefulpodcontrolgo-yuan-ma-jie-du/>2024-04-10 K8S控制器之stateful_pod_control.go源码解读</a></li><li><a href=/docs/k8s%E8%B0%83%E5%BA%A6%E5%99%A8-extendergo-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-diao-du-qi-extendergo-yuan-ma-jie-du/>2024-04-09 K8S调度器 extender.go 源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Bsyncgo-%E5%90%8C%E6%AD%A5-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-syncgo-tong-bu-yuan-ma-jie-du/>2024-04-09 K8S控制器之sync.go 同步 源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Brollbackgo-%E5%9B%9E%E6%BB%9A-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-rollbackgo-hui-gun-yuan-ma-jie-du/>2024-04-09 K8S控制器之rollback.go 回滚 源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Brecreatego-%E9%87%8D%E5%BB%BA-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-recreatego-zhong-jian-yuan-ma-jie-du/>2024-04-09 K8S控制器之recreate.go 重建 源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8B-schedulergo-%E8%B0%83%E5%BA%A6%E5%99%A8-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-schedulergo-diao-du-qi-yuan-ma-jie-du/>2024-04-09 K8S控制器之 scheduler.go 调度器 源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8B-rollinggo-%E6%BB%9A%E5%8A%A8%E6%9B%B4%E6%96%B0-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-rollinggo-gun-dong-geng-xin-yuan-ma-jie-du/>2024-04-09 K8S控制器之 rolling.go 滚动更新 源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8B-progressgo-%E8%BF%9B%E5%BA%A6-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-progressgo-jin-du-yuan-ma-jie-du/>2024-04-09 K8S控制器之 progress.go 进度 源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8B-deployment_controllergo%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-deploymentcontrollergo-yuan-ma-jie-du/>2024-04-09 K8S控制器之 deployment_controller.go源码解读</a></li><li><a href=/docs/k8s-%E8%B0%83%E5%BA%A6%E5%99%A8-scheduler_onego-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-diao-du-qi-scheduleronego-yuan-ma-jie-du/>2024-04-09 K8S 调度器 scheduler_one.go 源码解读</a></li><li><a href=/docs/%E5%BD%BB%E6%82%9F%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C-che-wu-rong-qi-wang-luo/>2024-04-07 彻悟容器网络</a></li><li><a href=/docs/%E9%9D%A2%E8%AF%95%E7%94%A8-golang-%E6%89%8B%E6%92%B8-lru-mian-shi-yong-golang-shou-lu-lru/>2024-04-03 面试用 Golang 手撸 LRU</a></li><li><a href=/docs/%E8%87%AA%E5%8A%A8%E5%B1%8F%E8%94%BDip%E6%94%BB%E5%87%BB-zi-dong-ping-bi-ip-gong-ji/>2024-04-03 自动屏蔽IP攻击</a></li><li><a href=/docs/%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85kubephere-li-xian-an-zhuang-kubephere/>2024-04-03 离线安装kubephere</a></li><li><a href=/docs/%E7%A3%81%E7%9B%98%E6%95%B0%E6%8D%AE%E6%81%A2%E5%A4%8D-ci-pan-shu-ju-hui-fu/>2024-04-03 磁盘数据恢复</a></li><li><a href=/docs/%E6%B8%85%E7%90%86%E6%AE%8B%E7%95%99%E7%9A%84calico%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6-qing-li-can-liu-de-calico-wang-luo/>2024-04-03 清理残留的calico网络插件</a></li><li><a href=/docs/%E6%B5%81%E9%87%8F%E4%BD%95%E5%A4%84%E6%9D%A5%E4%BD%95%E5%A4%84%E5%8E%BB-liu-liang-he-chu-lai-he-chu-qu/>2024-04-03 流量何处来何处去</a></li><li><a href=/docs/%E6%9E%81%E5%A4%A7%E6%8F%90%E9%AB%98%E5%B7%A5%E4%BD%9C%E6%95%88%E7%8E%87%E7%9A%84-linux-%E5%91%BD%E4%BB%A4-ji-da-ti-gao-gong-zuo-xiao-lv-de-linux-ming-ling/>2024-04-03 极大提高工作效率的 Linux 命令</a></li><li><a href=/docs/%E6%96%87%E5%AD%A6%E7%9A%84%E6%95%85%E4%B9%A1-wen-xue-de-gu-xiang/>2024-04-03 文学的故乡</a></li><li><a href=/docs/%E6%90%9E%E6%87%82k8s%E9%89%B4%E6%9D%83-gao-dong-k8s-jian-quan/>2024-04-03 搞懂K8S鉴权</a></li><li><a href=/docs/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E5%8E%9F%E7%90%86-rong-qi-wang-luo-yuan-li/>2024-04-03 容器网络原理</a></li><li><a href=/docs/%E5%AE%B9%E5%99%A8%E7%9A%84%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E4%B8%80-overlayfs-%E5%8E%9F%E7%90%86-rong-qi-de-wen-jian-xi-tong--yi-overlayfs-yuan-li/>2024-04-03 容器的文件系统 OverlayFS 原理</a></li><li><a href=/docs/%E5%AE%B9%E5%99%A8%E5%8E%9F%E7%90%86-rong-qi-yuan-li/>2024-04-03 容器原理</a></li><li><a href=/docs/%E5%AE%B9%E5%99%A8%E5%86%85%E7%9A%84-1-%E5%8F%B7%E8%BF%9B%E7%A8%8B-rong-qi-nei-de-1-hao-jin-cheng/>2024-04-03 容器内的 1 号进程</a></li><li><a href=/docs/%E5%AE%B9%E5%99%A8%E4%B8%AD%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90%E4%BB%A5%E5%8F%8A%E4%B8%8D%E5%90%8Cdnspolicy%E5%AF%B9%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90%E7%9A%84%E5%BD%B1%E5%93%8D-rong-qi-zhong-yu-ming-jie-xi-yi-ji-bu-tong-dnspolicy-dui-yu-ming-jie-xi-de-ying-xiang/>2024-04-03 容器中域名解析以及不同dnspolicy对域名解析的影响</a></li><li><a href=/docs/%E5%A6%82%E4%BD%95%E8%B0%83%E8%AF%95-crash-%E5%AE%B9%E5%99%A8%E7%9A%84%E7%BD%91%E7%BB%9C-ru-he-diao-shi-crash-rong-qi-de-wang-luo/>2024-04-03 如何调试 crash 容器的网络</a></li><li><a href=/docs/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8tekton%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BAcicd%E5%B9%B3%E5%8F%B0-ru-he-shi-yong-tekton-kuai-su-da-jian-cicd-ping-tai/>2024-04-03 如何使用tekton快速搭建CI/CD平台</a></li><li><a href=/docs/%E5%A4%A7%E8%A7%84%E6%A8%A1%E5%B9%B6%E5%8F%91%E4%B8%8B%E5%A6%82%E4%BD%95%E5%8A%A0%E5%BF%AB-pod-%E5%90%AF%E5%8A%A8%E9%80%9F%E5%BA%A6-da-gui-mo-bing-fa-xia-ru-he-jia-kuai-pod-qi-dong-su-du/>2024-04-03 大规模并发下如何加快 Pod 启动速度</a></li><li><a href=/docs/%E4%BD%BF%E7%94%A8kubernees-leases-%E8%BD%BB%E6%9D%BE%E5%AE%9E%E7%8E%B0leader-election-shi-yong-kuberneesleases-qing-song-shi-xian-leaderelection/>2024-04-03 使用kubernees leases 轻松实现leader election</a></li><li><a href=/docs/%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2k8s%E5%8A%A0%E8%8A%82%E7%82%B9%E6%93%8D%E4%BD%9C-er-jin-zhi-bu-shu-k8s-jia-jie-dian-cao-zuo/>2024-04-03 二进制部署K8S加节点操作</a></li><li><a href=/docs/%E4%B8%A4%E5%BC%A0%E5%9B%BE%E5%85%A8%E9%9D%A2%E7%90%86%E8%A7%A3k8s%E5%8E%9F%E7%90%86-liang-zhang-tu-quan-mian-li-jie-k8s-yuan-li/>2024-04-03 两张图全面理解K8S原理</a></li><li><a href=/docs/ssl%E8%AF%81%E4%B9%A6%E8%87%AA%E7%AD%BE%E5%8F%91-ssl-zheng-shu-zi-qian-fa/>2024-04-03 ssl证书自签发</a></li><li><a href=/docs/prometheus%E4%BC%81%E4%B8%9A%E7%BA%A7%E7%9B%91%E6%8E%A7%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93-prometheus-qi-ye-ji-jian-kong-shi-yong-zong-jie/>2024-04-03 prometheus企业级监控使用总结</a></li><li><a href=/docs/metallb-l2-%E5%8E%9F%E7%90%86-metallbl2-yuan-li/>2024-04-03 MetalLB L2 原理</a></li><li><a href=/docs/linux-%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E5%A4%A7%E5%85%A8-linux-xing-neng-you-hua-da-quan/>2024-04-03 Linux 性能优化大全</a></li><li><a href=/docs/kubernetes-%E8%AF%81%E4%B9%A6%E8%AF%A6%E8%A7%A3%E9%89%B4%E6%9D%83-kubernetes-zheng-shu-xiang-jie--jian-quan-/>2024-04-03 Kubernetes 证书详解(鉴权)</a></li><li><a href=/docs/kubernetes-%E8%AF%81%E4%B9%A6%E8%AF%A6%E8%A7%A3%E8%AE%A4%E8%AF%81-kubernetes-zheng-shu-xiang-jie--ren-zheng-/>2024-04-03 Kubernetes 证书详解(认证)</a></li><li><a href=/docs/kubernetes-%E6%BA%90%E7%A0%81%E7%BB%93%E6%9E%84-kubernetes-yuan-ma-jie-gou/>2024-04-03 Kubernetes 源码结构</a></li><li><a href=/docs/kubernetes-api-kubernetesapi/>2024-04-03 Kubernetes API</a></li><li><a href=/docs/kubekey%E6%B7%BB%E5%8A%A0%E6%96%B0%E8%8A%82%E7%82%B9-kubekey-tian-jia-xin-jie-dian/>2024-04-03 kubekey添加新节点</a></li><li><a href=/docs/k8s%E9%9D%A2%E8%AF%95%E5%AE%9D%E5%85%B8-k8s-mian-shi-bao-dian/>2024-04-03 K8S面试宝典</a></li><li><a href=/docs/k8s%E9%9D%A2%E8%AF%95%E5%A4%A7%E5%85%A8-k8s-mian-shi-da-quan/>2024-04-03 K8S面试大全</a></li><li><a href=/docs/k8s%E8%BF%90%E7%BB%B4%E4%B9%8B%E6%B8%85%E7%90%86%E7%A3%81%E7%9B%98-k8s-yun-wei-zhi-qing-li-ci-pan/>2024-04-03 k8s运维之清理磁盘</a></li><li><a href=/docs/k8s%E8%B0%83%E8%AF%95pod-k8s-diao-shi-pod/>2024-04-03 K8S调试POD</a></li><li><a href=/docs/k8s%E7%9A%84pod%E7%B1%BB%E5%9E%8B-k8s-de-pod-lei-xing/>2024-04-03 K8S的POD类型</a></li><li><a href=/docs/k8s%E5%BA%94%E7%94%A8%E7%9A%84%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5-k8s-ying-yong-de-zui-jia-shi-jian/>2024-04-03 k8s应用的最佳实践</a></li><li><a href=/docs/k8s%E5%91%BD%E4%BB%A4%E6%8C%87%E5%8D%97-k8s-ming-ling-zhi-nan/>2024-04-03 K8S命令指南</a></li><li><a href=/docs/k8s%E5%8E%9F%E5%9C%B0%E5%8D%87%E7%BA%A7-k8s-yuan-de-sheng-ji/>2024-04-03 K8S原地升级</a></li><li><a href=/docs/k8s-%E6%8E%A2%E9%92%88%E5%8E%9F%E7%90%86-k8s-tan-zhen-yuan-li/>2024-04-03 K8S 探针原理</a></li><li><a href=/docs/k8s-%E5%BC%80%E5%8F%91%E5%8F%AF%E4%B8%8D%E6%AD%A2-crud-k8s-kai-fa-ke-bu-zhi-crud/>2024-04-03 K8S 开发可不止 CRUD</a></li><li><a href=/docs/k8s-gpt-k8sgpt/>2024-04-03 K8S GPT</a></li><li><a href=/docs/k8s-csi-openebs%E5%8E%9F%E7%90%86-k8scsiopenebs-yuan-li/>2024-04-03 K8S csi openebs原理</a></li><li><a href=/docs/helm-chart%E5%92%8Crepo-helmchart-he-repo/>2024-04-03 helm chart和repo</a></li><li><a href=/docs/flanel%E7%BD%91%E7%BB%9C-flanel-wang-luo/>2024-04-03 flanel网络</a></li><li><a href=/docs/etcd%E7%A8%B3%E5%AE%9A%E6%80%A7%E5%8F%8A%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E5%AE%9E%E8%B7%B5-etcd-wen-ding-xing-ji-xing-neng-you-hua-shi-jian/>2024-04-03 ETCD稳定性及性能优化实践</a></li><li><a href=/docs/etcd%E5%A4%87%E4%BB%BD-etcd-bei-fen/>2024-04-03 ETCD备份</a></li><li><a href=/docs/docker%E9%87%8D%E8%A6%81%E7%9A%84%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86%E7%82%B9-docker-zhong-yao-de-wang-luo-zhi-shi-dian/>2024-04-03 Docker重要的网络知识点</a></li><li><a href=/docs/dockerfile%E7%9A%84copy%E5%92%8Cadd%E7%9A%84%E5%8C%BA%E5%88%AB-dockerfile-de-copy-he-add-de-qu-bie/>2024-04-03 dockerfile的copy和add的区别</a></li><li><a href=/docs/coredns%E4%B9%8B%E5%85%89-coredns-zhi-guang/>2024-04-03 COREDNS之光</a></li><li><a href=/docs/containerd-%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C-containerd-ji-ben-cao-zuo/>2024-04-03 Containerd 基本操作</a></li><li><a href=/docs/cni%E6%8F%92%E4%BB%B6%E9%80%89%E5%9E%8B-cni-cha-jian-xuan-xing/>2024-04-03 CNI插件选型</a></li><li><a href=/docs/client-go-%E6%9E%B6%E6%9E%84-client-go-jia-gou/>2024-04-03 Client-go 架构</a></li><li><a href=/docs/client-go-%E5%9B%9B%E7%A7%8D%E5%AE%A2%E6%88%B7%E7%AB%AF-client-go-si-zhong-ke-hu-duan/>2024-04-03 Client-go 四种客户端</a></li><li><a href=/docs/cicd%E6%80%9D%E8%80%83-cicd-si-kao/>2024-04-03 CICD思考</a></li><li><a href=/docs/calico%E7%BD%91%E7%BB%9C%E8%87%AA%E5%AE%9A%E4%B9%89-calico-wang-luo-zi-ding-yi/>2024-04-03 Calico网络自定义</a></li><li><a href=/docs/acme%E8%87%AA%E5%8A%A8%E6%9B%B4%E6%96%B0%E8%AF%81%E4%B9%A6-acme-zi-dong-geng-xin-zheng-shu/>2024-04-03 acme自动更新证书</a></li><li><a href=/docs/16%E4%B8%AA%E6%A6%82%E5%BF%B5%E5%B8%A6%E4%BD%A0%E5%85%A5%E9%97%A8-kubernetes-16-ge-gai-nian-dai-ni-ru-men-kubernetes/>2024-04-03 16个概念带你入门 Kubernetes</a></li><li><a href=/docs/%E9%9D%A2%E8%AF%950308-mian-shi-0308/>2024-04-03 面试0308</a></li><li><a href=/docs/600%E6%9D%A1%E6%9C%80%E5%BC%BAlinux%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93-600-tiao-zui-qiang-linux-ming-ling-zong-jie/>2024-04-03 600条最强linux命令总结</a></li><li><a href=/docs/16%E5%BC%A0%E7%A1%AC%E6%A0%B8%E5%9B%BE%E8%A7%A3k8s%E7%BD%91%E7%BB%9C-16-zhang-ying-he-tu-jie-k8s-wang-luo/>2024-04-03 16张硬核图解k8s网络</a></li><li><a href=/docs/k8s%E4%B9%8Bkubelet%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-zhi-kubelet-yuan-ma-jie-du/>2024-03-28 k8s之kubelet源码解读</a></li><li><a href=/docs/2024-3-19-%E4%B8%A4%E5%BC%A0%E5%9B%BE%E5%85%A8%E9%9D%A2%E7%90%86%E8%A7%A3k8s%E5%8E%9F%E7%90%86/>2024-03-19 两张图全面理解k8s原理</a></li><li><a href=/docs/2024-3-8-%E9%9D%A2%E8%AF%950308/>2024-03-08 面试</a></li><li><a href=/docs/2024-3-4-k8s%E6%B5%81%E9%87%8F%E9%93%BE%E8%B7%AF%E5%89%96%E6%9E%90/>2024-03-04 k8s流量链路剖析</a></li><li><a href=/docs/k8s-%E6%B5%81%E9%87%8F%E9%93%BE%E8%B7%AF%E5%89%96%E6%9E%90-k8s-liu-liang-lian-lu-pou-xi/>2024-03-04 K8S 流量链路剖析</a></li><li><a href=/docs/k8s-csi%E5%89%96%E6%9E%90%E6%BC%94%E8%BF%9B-k8scsi-pou-xi-yan-jin/>2024-03-04 K8S CSI剖析演进</a></li><li><a href=/docs/k8s-cni%E5%89%96%E6%9E%90%E6%BC%94%E8%BF%9B-k8scni-pou-xi-yan-jin/>2024-03-04 K8S CNI剖析演进</a></li><li><a href=/docs/2024-3-4-k8s-csi%E5%89%96%E6%9E%90/>2024-03-04 CSI剖析演进</a></li><li><a href=/docs/2024-3-4-cni%E5%89%96%E6%9E%90%E6%BC%94%E8%BF%9B/>2024-03-04 CNI剖析演进</a></li><li><a href=/docs/2024-2-26-%E9%9D%A2%E8%AF%95/>2024-02-26 面试</a></li><li><a href=/docs/2024-2-22-k8s%E9%9D%A2%E8%AF%95%E5%AE%9D%E5%85%B8/>2024-02-22 k8s面试宝典</a></li><li><a href=/docs/2024-2-22-k8s%E6%9E%B6%E6%9E%84%E5%B8%88%E9%9D%A2%E8%AF%95%E5%A4%A7%E5%85%A8/>2024-02-22 k8s架构师面试大全</a></li><li><a href=/docs/%E4%BD%BF%E7%94%A8-openfunction-%E5%9C%A8%E4%BB%BB%E4%BD%95%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD%E4%B8%8A%E8%BF%90%E8%A1%8C%E6%97%A0%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%B7%A5%E4%BD%9C%E8%B4%9F%E8%BD%BD-shi-yong-openfunction-zai-ren-he-ji-chu-she-shi-shang-yun-xing-wu-fu-wu-qi-gong-zuo-fu-zai/>2024-01-21 使用 OpenFunction 在任何基础设施上运行无服务器工作负载</a></li><li><a href=/docs/%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85%E9%9B%86%E7%BE%A4-li-xian-an-zhuang-ji-qun/>2023-09-28 离线安装集群</a></li><li><a href=/docs/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%AF%B4%E6%98%8E-cao-zuo-xi-tong-shuo-ming/>2023-09-28 操作系统说明</a></li><li><a href=/docs/%E5%BF%AB%E9%80%9F%E6%8C%87%E5%8D%97-kuai-su-zhi-nan/>2023-09-28 快速指南</a></li><li><a href=/docs/%E5%BC%80%E5%A7%8B%E4%BD%BF%E7%94%A8-cilium-kai-shi-shi-yong-cilium/>2023-09-28 开始使用 cilium</a></li><li><a href=/docs/%E5%A4%9A%E6%9E%B6%E6%9E%84%E6%94%AF%E6%8C%81-duo-jia-gou-zhi-chi/>2023-09-28 多架构支持</a></li><li><a href=/docs/%E5%85%AC%E6%9C%89%E4%BA%91%E4%B8%8A%E9%83%A8%E7%BD%B2-kubeasz-gong-you-yun-shang-bu-shu-kubeasz/>2023-09-28 公有云上部署</a></li><li><a href=/docs/%E4%B8%AA%E6%80%A7%E5%8C%96%E9%9B%86%E7%BE%A4%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE-ge-xing-hua-ji-qun-can-shu-pei-zhi/>2023-09-28 个性化集群参数配置</a></li><li><a href=/docs/network-check-network-check/>2023-09-28 network-check</a></li><li><a href=/docs/kube-router-%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6-kube-router-wang-luo-zu-jian/>2023-09-28 kube-router 网络组件</a></li><li><a href=/docs/ezctl-%E5%91%BD%E4%BB%A4%E8%A1%8C%E4%BB%8B%E7%BB%8D-ezctl-ming-ling-xing-jie-shao/>2023-09-28 ezctl 命令行介绍</a></li><li><a href=/docs/ex-lb-%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E9%83%A8%E7%BD%B2-ex-lb-fu-zai-jun-heng-bu-shu/>2023-09-28 EX-LB 负载均衡部署</a></li><li><a href=/docs/calico-%E9%85%8D%E7%BD%AE-bgp-route-reflectors-calico-pei-zhi-bgproutereflectors/>2023-09-28 calico 配置 BGP Route Reflectors</a></li><li><a href=/docs/07-%E5%AE%89%E8%A3%85%E9%9B%86%E7%BE%A4%E4%B8%BB%E8%A6%81%E6%8F%92%E4%BB%B6-07--an-zhuang-ji-qun-zhu-yao-cha-jian/>2023-09-28 15:26:42.651 07-安装集群主要插件</a></li><li><a href=/docs/08-k8s-%E9%9B%86%E7%BE%A4%E5%AD%98%E5%82%A8--k8s-ji-qun-cun-chu/>2023-09-28 08-K8S 集群存储</a></li><li><a href=/docs/06-%E5%AE%89%E8%A3%85%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6-06--an-zhuang-wang-luo-zu-jian/>2023-09-28 06-安装网络组件</a></li><li><a href=/docs/06-%E5%AE%89%E8%A3%85kube-ovn%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6-06--an-zhuang-kube-ovn-wang-luo-zu-jian/>2023-09-28 06-安装kube-ovn网络组件</a></li><li><a href=/docs/06-%E5%AE%89%E8%A3%85flannel%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6-06--an-zhuang-flannel-wang-luo-zu-jian/>2023-09-28 06-安装flannel网络组件</a></li><li><a href=/docs/06-%E5%AE%89%E8%A3%85cilium%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6-06--an-zhuang-cilium-wang-luo-zu-jian/>2023-09-28 06-安装cilium网络组件</a></li><li><a href=/docs/06-%E5%AE%89%E8%A3%85calico%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6-06--an-zhuang-calico-wang-luo-zu-jian/>2023-09-28 06-安装calico网络组件</a></li><li><a href=/docs/02-%E5%AE%89%E8%A3%85etcd%E9%9B%86%E7%BE%A4-02--an-zhuang-etcd-ji-qun/>2023-09-28 02-安装etcd集群</a></li><li><a href=/docs/00-%E9%9B%86%E7%BE%A4%E8%A7%84%E5%88%92%E5%92%8C%E5%9F%BA%E7%A1%80%E5%8F%82%E6%95%B0%E8%AE%BE%E5%AE%9A-00--ji-qun-gui-hua-he-ji-chu-can-shu-she-ding/>2023-09-28 00-集群规划和基础参数设定</a></li><li><a href=/docs/05-%E5%AE%89%E8%A3%85kube_node%E8%8A%82%E7%82%B9-05--an-zhuang-kubenode-jie-dian/>2023-09-28 05-安装kube_node节点</a></li><li><a href=/docs/04-%E5%AE%89%E8%A3%85kube_master%E8%8A%82%E7%82%B9-04--an-zhuang-kubemaster-jie-dian/>2023-09-28 04-安装kube_master节点</a></li><li><a href=/docs/03-%E5%AE%89%E8%A3%85%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6-03--an-zhuang-rong-qi-yun-xing-shi/>2023-09-28 03-安装容器运行时</a></li><li><a href=/docs/01-%E5%88%9B%E5%BB%BA%E8%AF%81%E4%B9%A6%E5%92%8C%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87-01--chuang-jian-zheng-shu-he-huan-jing-zhun-bei/>2023-09-28 01-创建证书和环境准备</a></li><li><a href=/docs/%E6%9C%89%E8%BF%993%E4%B8%AA%E8%BF%B9%E8%B1%A1%E4%BD%A0%E5%B0%B1%E8%AF%A5%E7%A6%BB%E8%81%8C%E4%BA%86-you-zhe-3-ge-ji-xiang--ni-jiu-gai-li-zhi-le/>2023-09-21 思考</a></li><li><a href=/docs/%E4%BD%BF%E7%94%A8-keepalived-%E5%92%8C-haproxy-%E5%88%9B%E5%BB%BA%E9%AB%98%E5%8F%AF%E7%94%A8-kubernetes-%E9%9B%86%E7%BE%A4-shi-yong-keepalived-he-haproxy-chuang-jian-gao-ke-yong-kubernetes-ji-qun/>2023-04-12 使用 Keepalived 和 HAproxy 创建高可用 Kubernetes 集群</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h3>2025-3-14 vivo面试</h3><label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><ul><li><a href=#1-5个-master-节点控制面>1. <strong>5个 Master 节点（控制面）</strong></a></li><li><a href=#2-node-节点工作节点>2. <strong>Node 节点（工作节点）</strong></a></li><li><a href=#3-联邦架构federation>3. <strong>联邦架构（Federation）</strong></a></li><li><a href=#4-ad-bc-分区>4. <strong>AD BC 分区</strong></a></li><li><a href=#5-多-az-跨机房容灾设计>5. <strong>多 AZ 跨机房容灾设计</strong></a></li><li><a href=#6-健康检查和负载均衡>6. <strong>健康检查和负载均衡</strong></a></li><li><a href=#7-灾难恢复与容错>7. <strong>灾难恢复与容错</strong></a></li><li><a href=#8-备份与恢复>8. <strong>备份与恢复</strong></a></li><li><a href=#总结>总结</a></li></ul></li></ul><ul><li><a href=#1-etcd-容灾方案><strong>1. etcd 容灾方案</strong></a><ul><li><a href=#1-etcd-高可用部署><strong>(1) etcd 高可用部署</strong></a></li><li><a href=#2-etcd-备份与恢复><strong>(2) etcd 备份与恢复</strong></a></li><li><a href=#3-etcd-自动故障转移><strong>(3) etcd 自动故障转移</strong></a></li></ul></li><li><a href=#2-kubernetes-控制面组件容灾方案><strong>2. Kubernetes 控制面组件容灾方案</strong></a><ul><li><a href=#1-高可用架构><strong>(1) 高可用架构</strong></a></li><li><a href=#2-监控与自动恢复><strong>(2) 监控与自动恢复</strong></a></li><li><a href=#3-备份与恢复><strong>(3) 备份与恢复</strong></a></li></ul></li><li><a href=#3-故障恢复策略><strong>3. 故障恢复策略</strong></a></li><li><a href=#4-结论><strong>4. 结论</strong></a></li></ul><ul><li><a href=#kubernetes-容灾演练方案设计><strong>Kubernetes 容灾演练方案设计</strong></a></li></ul><ul><li><a href=#-场景-1etcd-故障><strong>🟢 场景 1：etcd 故障</strong></a></li><li><a href=#-场景-2master-节点故障><strong>🟡 场景 2：Master 节点故障</strong></a></li><li><a href=#-场景-3node-计算节点故障><strong>🔴 场景 3：Node 计算节点故障</strong></a></li><li><a href=#-场景-4ingressservice-负载均衡异常><strong>🟠 场景 4：Ingress/Service 负载均衡异常</strong></a></li><li><a href=#-场景-5存储故障><strong>🔵 场景 5：存储故障</strong></a></li></ul><ul><li><a href=#演练发现的问题与改进方案><strong>演练发现的问题与改进方案</strong></a></li></ul><ul><li><ul><li><a href=#-问题-1etcd-选主时间过长影响-api-server-响应><strong>🛑 问题 1：etcd 选主时间过长，影响 API Server 响应</strong></a></li><li><a href=#-问题-2etcd-备份未生效恢复失败><strong>🛑 问题 2：etcd 备份未生效，恢复失败</strong></a></li></ul></li></ul><ul><li><ul><li><a href=#-问题-3kube-apiserver-宕机负载均衡未生效><strong>🛑 问题 3：kube-apiserver 宕机，负载均衡未生效</strong></a></li></ul></li></ul><ul><li><ul><li><a href=#-问题-4节点驱逐后-pod-迁移失败><strong>🛑 问题 4：节点驱逐后 Pod 迁移失败</strong></a></li></ul></li></ul><ul><li><ul><li><a href=#-问题-5ingress-nginx-宕机后业务不可访问><strong>🛑 问题 5：Ingress Nginx 宕机后，业务不可访问</strong></a></li></ul></li></ul><ul><li><ul><li><a href=#-问题-6pvc-绑定失败pod-进入><strong>🛑 问题 6：PVC 绑定失败，Pod 进入 <code>Pending</code> 状态</strong></a></li></ul></li><li><a href=#总结-1><strong>总结</strong></a></li></ul><ul><li><a href=#大规模启动-pod-失败的排查与优化方案><strong>大规模启动 Pod 失败的排查与优化方案</strong></a></li><li><a href=#-1-通过><strong>🛠 1. 通过 <code>kubectl</code> 和 <code>describe</code> 命令排查</strong></a><ul><li><a href=#-查看-pod-状态><strong>📌 查看 Pod 状态</strong></a></li><li><a href=#-获取详细信息><strong>📌 获取详细信息</strong></a></li></ul></li><li><a href=#-2-调度问题pod-无法被分配到节点><strong>🔍 2. 调度问题（Pod 无法被分配到节点）</strong></a><ul><li><a href=#-问题-1集群资源不足><strong>🛑 问题 1：集群资源不足</strong></a></li><li><a href=#-问题-2节点调度限制><strong>🛑 问题 2：节点调度限制</strong></a></li></ul></li><li><a href=#-3-容器创建问题containercreating-卡住><strong>🔍 3. 容器创建问题（ContainerCreating 卡住）</strong></a><ul><li><a href=#-问题-3cni-网络组件异常><strong>🛑 问题 3：CNI 网络组件异常</strong></a></li><li><a href=#-问题-4容器镜像拉取失败><strong>🛑 问题 4：容器镜像拉取失败</strong></a></li></ul></li><li><a href=#-4-存储问题pvc-绑定失败><strong>🔍 4. 存储问题（PVC 绑定失败）</strong></a><ul><li><a href=#-问题-5存储卷挂载失败><strong>🛑 问题 5：存储卷挂载失败</strong></a></li></ul></li><li><a href=#-5-master-组件负载过高><strong>🔍 5. Master 组件负载过高</strong></a><ul><li><a href=#-问题-6api-server-负载过高><strong>🛑 问题 6：API Server 负载过高</strong></a></li></ul></li><li><a href=#-总结><strong>🎯 总结</strong></a></li></ul><ul><li><a href=#kubernetes-中-redis-和-kafka-容器化方案><strong>Kubernetes 中 Redis 和 Kafka 容器化方案</strong></a></li></ul><ul><li><ul><li><a href=#-redis-方案设计><strong>📌 Redis 方案设计</strong></a></li><li><a href=#-方案-1redis-sentinel-高可用><strong>📌 方案 1：Redis Sentinel 高可用</strong></a></li><li><a href=#-方案-2redis-cluster分片><strong>📌 方案 2：Redis Cluster（分片）</strong></a></li></ul></li></ul><ul><li><ul><li><a href=#-kafka-方案设计><strong>📌 Kafka 方案设计</strong></a></li><li><a href=#-kafka-容器化部署><strong>📌 Kafka 容器化部署</strong></a></li></ul></li></ul><ul><li><a href=#-1-数据持久化><strong>📌 1. 数据持久化</strong></a></li><li><a href=#-2-高可用><strong>📌 2. 高可用</strong></a></li><li><a href=#-3-延迟优化><strong>📌 3. 延迟优化</strong></a></li></ul><ul><li><a href=#dragonfly-原理解析><strong>Dragonfly 原理解析</strong></a><ul><li><a href=#-dragonfly-是什么><strong>📌 Dragonfly 是什么？</strong></a></li></ul></li><li><a href=#1-dragonfly-主要组成><strong>1️⃣ Dragonfly 主要组成</strong></a></li><li><a href=#2-dragonfly-的工作原理><strong>2️⃣ Dragonfly 的工作原理</strong></a><ul><li><a href=#-1-镜像文件下载请求><strong>🛠 1. 镜像/文件下载请求</strong></a></li><li><a href=#-2-调度节点scheduler><strong>🛠 2. 调度节点（scheduler）</strong></a></li><li><a href=#-3-p2p-下载><strong>🛠 3. P2P 下载</strong></a></li><li><a href=#-4-本地缓存><strong>🛠 4. 本地缓存</strong></a></li></ul></li><li><a href=#3-dragonfly-的-p2p-下载机制><strong>3️⃣ Dragonfly 的 P2P 下载机制</strong></a><ul><li><a href=#-p2p-分块下载><strong>🔥 P2P 分块下载</strong></a></li></ul></li><li><a href=#4-dragonfly-高可用架构><strong>4️⃣ Dragonfly 高可用架构</strong></a><ul><li><a href=#-多级缓存架构><strong>🌍 多级缓存架构</strong></a></li><li><a href=#-关键特性><strong>⚡ 关键特性</strong></a></li></ul></li><li><a href=#5-dragonfly-在-kubernetes-集群中的应用><strong>5️⃣ Dragonfly 在 Kubernetes 集群中的应用</strong></a><ul><li><a href=#-在-kubernetes-使用-dragonfly><strong>🔥 在 Kubernetes 使用 Dragonfly</strong></a></li></ul></li><li><a href=#6-dragonfly-对比其他加速方案><strong>6️⃣ Dragonfly 对比其他加速方案</strong></a></li><li><a href=#-总结-2><strong>✅ 总结</strong></a></li></ul><ul><li><a href=#监控日活千万级-app-的监控体系设计><strong>监控日活千万级 App 的监控体系设计</strong></a></li><li><a href=#1-监控体系设计><strong>1. 监控体系设计</strong></a><ul><li><a href=#1监控架构><strong>（1）监控架构</strong></a></li></ul></li><li><a href=#2-关键监控指标><strong>2. 关键监控指标</strong></a><ul><li><a href=#1业务层监控><strong>（1）业务层监控</strong></a></li><li><a href=#2系统层监控><strong>（2）系统层监控</strong></a></li><li><a href=#3应用层监控apm><strong>（3）应用层监控（APM）</strong></a></li><li><a href=#4日志--异常监控><strong>（4）日志 & 异常监控</strong></a></li></ul></li><li><a href=#3-监控工具选型><strong>3. 监控工具选型</strong></a></li><li><a href=#4-告警策略><strong>4. 告警策略</strong></a><ul><li><a href=#1告警分级><strong>（1）告警分级</strong></a></li><li><a href=#2自动化运维><strong>（2）自动化运维</strong></a></li></ul></li><li><a href=#5-典型故障场景--处理><strong>5. 典型故障场景 & 处理</strong></a></li><li><a href=#总结-2><strong>总结</strong></a></li></ul><ul><li><ul><li><a href=#1-增加-prometheus-的内存限制>1. <strong>增加 Prometheus 的内存限制</strong></a></li><li><a href=#2-调整>2. <strong>调整 <code>max-memory</code> 限制</strong></a></li><li><a href=#3-减少-prometheus-存储数据的量>3. <strong>减少 Prometheus 存储数据的量</strong></a></li><li><a href=#4-使用较小的>4. <strong>使用较小的 <code>scrape_interval</code></strong></a></li><li><a href=#5-优化数据源和查询>5. <strong>优化数据源和查询</strong></a></li><li><a href=#6-启用>6. <strong>启用 <code>remote_write</code> 和外部存储</strong></a></li><li><a href=#7-提高-prometheus-性能>7. <strong>提高 Prometheus 性能</strong></a></li><li><a href=#8-扩展-prometheus-集群>8. <strong>扩展 Prometheus 集群</strong></a></li><li><a href=#总结-3>总结：</a></li><li><a href=#prometheus-联邦的核心概念><strong>Prometheus 联邦的核心概念</strong></a></li><li><a href=#联邦的工作原理><strong>联邦的工作原理</strong></a></li><li><a href=#联邦的典型用途><strong>联邦的典型用途</strong></a></li><li><a href=#prometheus-联邦的配置><strong>Prometheus 联邦的配置</strong></a></li><li><a href=#关键参数解释><strong>关键参数解释</strong></a></li><li><a href=#优势><strong>优势</strong></a></li><li><a href=#限制和挑战><strong>限制和挑战</strong></a></li><li><a href=#总结-4><strong>总结</strong></a></li></ul></li></ul><ul><li><ul><li><a href=#部署二进制集群的步骤><strong>部署二进制集群的步骤：</strong></a></li><li><a href=#保证-playbook-的幂等性><strong>保证 Playbook 的幂等性：</strong></a></li><li><a href=#保证-playbook-的安全性><strong>保证 Playbook 的安全性：</strong></a></li></ul></li></ul><ul><li><ul><li><a href=#灰度发布流水线设计思路><strong>灰度发布流水线设计思路</strong></a></li><li><a href=#设计灰度发布流水线的步骤><strong>设计灰度发布流水线的步骤：</strong></a></li><li><a href=#灰度发布中的关键技术和工具><strong>灰度发布中的关键技术和工具：</strong></a></li><li><a href=#总结-5><strong>总结</strong>：</a></li></ul></li></ul><ul><li><a href=#流量洪峰自动扩缩容方案设计-><strong>流量洪峰自动扩缩容方案设计</strong> 🚀</a></li><li><a href=#1-流量洪峰的特征><strong>1. 流量洪峰的特征</strong></a></li><li><a href=#2-自动扩缩容架构><strong>2. 自动扩缩容架构</strong></a></li><li><a href=#3-关键扩缩容策略><strong>3. 关键扩缩容策略</strong></a><ul><li><a href=#1pod-水平扩展hpa><strong>（1）Pod 水平扩展（HPA）</strong></a></li><li><a href=#2pod-垂直扩展vpa><strong>（2）Pod 垂直扩展（VPA）</strong></a></li><li><a href=#3集群节点扩展cluster-autoscaler><strong>（3）集群节点扩展（Cluster Autoscaler）</strong></a></li><li><a href=#4事件驱动扩缩容keda><strong>（4）事件驱动扩缩容（KEDA）</strong></a></li></ul></li><li><a href=#4-预案--高可用策略><strong>4. 预案 & 高可用策略</strong></a><ul><li><a href=#1流量预案><strong>（1）流量预案</strong></a></li><li><a href=#2故障恢复><strong>（2）故障恢复</strong></a></li></ul></li><li><a href=#5-方案总结><strong>5. 方案总结</strong></a><ul><li><a href=#最终架构><strong>最终架构</strong></a></li></ul></li></ul><ul><li><ul><li><a href=#1-确定视频加载缓慢的表现><strong>1. 确定视频加载缓慢的表现</strong></a></li><li><a href=#2-分析网络链路><strong>2. 分析网络链路</strong></a></li><li><a href=#3-存储层排查><strong>3. 存储层排查</strong></a></li><li><a href=#4-视频文件分析><strong>4. 视频文件分析</strong></a></li><li><a href=#5-客户端排查><strong>5. 客户端排查</strong></a></li><li><a href=#6-流程监控与日志><strong>6. 流程监控与日志</strong></a></li><li><a href=#7-进行压力测试><strong>7. 进行压力测试</strong></a></li><li><a href=#总结-6><strong>总结</strong>：</a></li></ul></li></ul><ul><li><ul><li><a href=#1-根据故障影响范围分类><strong>1. 根据故障影响范围分类</strong></a></li><li><a href=#2-根据故障恢复难易度分类><strong>2. 根据故障恢复难易度分类</strong></a></li><li><a href=#3-根据时间敏感性分类><strong>3. 根据时间敏感性分类</strong></a></li><li><a href=#4-故障分类><strong>4. 故障分类</strong></a></li><li><a href=#5-使用故障管理系统><strong>5. 使用故障管理系统</strong></a></li><li><a href=#6-基于业务价值评估><strong>6. 基于业务价值评估</strong></a></li><li><a href=#7-确定恢复的最小可行方案><strong>7. 确定恢复的最小可行方案</strong></a></li><li><a href=#排查故障的优先级处理流程><strong>排查故障的优先级处理流程</strong>：</a></li><li><a href=#总结-7><strong>总结</strong></a></li></ul></li></ul><ul><li><ul><li><a href=#二线的主要工作职责><strong>二线的主要工作职责</strong>：</a></li><li><a href=#一线反馈到二线后如何解决问题><strong>一线反馈到二线后，如何解决问题？</strong></a></li><li><a href=#总结-8><strong>总结</strong></a></li></ul></li></ul><ul><li><ul><li><a href=#1-定义和重点><strong>1. 定义和重点</strong></a></li><li><a href=#2-主要职责><strong>2. 主要职责</strong></a></li><li><a href=#3-关注点和目标><strong>3. 关注点和目标</strong></a></li><li><a href=#4-跨部门协作><strong>4. 跨部门协作</strong></a></li><li><a href=#5-运维工具与技术><strong>5. 运维工具与技术</strong></a></li><li><a href=#总结-9><strong>总结</strong></a></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=你们的k8s集群的高可用架构怎么设计的>你们的k8s集群的高可用架构怎么设计的？
<a class=anchor href=#%e4%bd%a0%e4%bb%ac%e7%9a%84k8s%e9%9b%86%e7%be%a4%e7%9a%84%e9%ab%98%e5%8f%af%e7%94%a8%e6%9e%b6%e6%9e%84%e6%80%8e%e4%b9%88%e8%ae%be%e8%ae%a1%e7%9a%84>#</a></h1><p>5个master节点 剩下是node节点，联邦，2个控制面集群和38运行面集群 组成k8s联邦，控制面k8s集群只跑paas平台的应用，运行面集群跑业务应用，AD BC 分区，多AZ夸机房容灾</p><p>Kubernetes 集群的高可用架构设计对于确保系统的稳定性、可扩展性和容灾能力非常重要。根据你描述的设计架构，以下是详细的分析和展开：</p><h3 id=1-5个-master-节点控制面>1. <strong>5个 Master 节点（控制面）</strong>
<a class=anchor href=#1-5%e4%b8%aa-master-%e8%8a%82%e7%82%b9%e6%8e%a7%e5%88%b6%e9%9d%a2>#</a></h3><ul><li><strong>多节点设计</strong>：高可用的 Kubernetes 集群需要至少三个 master 节点来提供冗余，以避免单点故障。你选择了 5 个 master 节点，这样可以进一步增强集群的高可用性，避免任何 2 个节点宕机时集群服务的不可用。</li><li><strong>控制面职责</strong>：Kubernetes master 节点负责调度、集群状态管理、API 服务器、控制器管理器等核心功能。将 master 节点分布在不同的物理或虚拟机上，并使用负载均衡器进行流量分配，可以有效避免 master 节点单点故障。</li><li><strong>负载均衡</strong>：使用负载均衡器（如 Nginx、HAProxy 或云服务商的负载均衡）对外暴露 Kubernetes API Server，使得所有的客户端（如 <code>kubectl</code>、应用）能够在多个 master 节点之间负载均衡，避免单一 master 节点的宕机导致不可用。</li></ul><h3 id=2-node-节点工作节点>2. <strong>Node 节点（工作节点）</strong>
<a class=anchor href=#2-node-%e8%8a%82%e7%82%b9%e5%b7%a5%e4%bd%9c%e8%8a%82%e7%82%b9>#</a></h3><ul><li><strong>运行业务应用</strong>：Kubernetes 的 node 节点负责运行容器化的业务应用。在该架构下，节点数目通常根据工作负载的需求进行扩展。每个 node 节点运行 <code>kubelet</code> 和 <code>kube-proxy</code>，这些组件管理节点的健康和服务发现。</li><li><strong>自动扩容</strong>：可以通过 Kubernetes 集群的 HPA（Horizontal Pod Autoscaler）来根据业务需求动态地增加或减少 node 节点的数量，从而应对负载的波动。</li></ul><h3 id=3-联邦架构federation>3. <strong>联邦架构（Federation）</strong>
<a class=anchor href=#3-%e8%81%94%e9%82%a6%e6%9e%b6%e6%9e%84federation>#</a></h3><ul><li><strong>控制面集群和运行面集群的分离</strong>：在联邦架构中，控制面集群负责管理和控制集群的整体状态，但不运行具体的业务应用。而运行面集群专注于运行业务应用，负载由控制面集群统一调度。控制面集群通常只运行 PaaS 平台相关的应用，而业务应用则运行在运行面集群中。</li><li><strong>联邦调度</strong>：Kubernetes 联邦可以跨多个集群进行调度，支持多个集群中的服务发现、跨集群负载均衡等功能。使用 Kubernetes 联邦可以实现跨地域、跨数据中心的服务扩展，从而保证高可用性。</li><li><strong>跨区域容灾</strong>：联邦架构中的多个运行面集群可以部署在不同的可用区域（Availability Zone，AZ）和不同的数据中心。通过合理的设计，可以在一个区域发生故障时，自动将流量切换到另一个区域，从而实现跨区域容灾。</li></ul><h3 id=4-ad-bc-分区>4. <strong>AD BC 分区</strong>
<a class=anchor href=#4-ad-bc-%e5%88%86%e5%8c%ba>#</a></h3><ul><li><strong>AD 和 BC 是不同的区域分区</strong>：这里的 AD 和 BC 很可能指的是不同的应用区域分区或者不同的业务功能模块分区。AD 和 BC 代表了两类不同的业务场景，每个分区可能有独立的业务需求和安全隔离要求。</li><li><strong>分区管理</strong>：通过为不同的业务模块使用不同的 Kubernetes Namespace，可以在同一集群中实现一定程度的资源隔离、权限管理和网络隔离。</li></ul><h3 id=5-多-az-跨机房容灾设计>5. <strong>多 AZ 跨机房容灾设计</strong>
<a class=anchor href=#5-%e5%a4%9a-az-%e8%b7%a8%e6%9c%ba%e6%88%bf%e5%ae%b9%e7%81%be%e8%ae%be%e8%ae%a1>#</a></h3><ul><li><strong>跨机房部署</strong>：通过将 Kubernetes 集群部署在多个数据中心或多个可用区（AZ）之间，实现高可用和灾难恢复。例如，使用多个 master 节点跨机房部署，并利用云服务提供的负载均衡和高可用性网络设计，保证 master 节点能够跨机房访问且高可用。</li><li><strong>Pod 跨 AZ 部署</strong>：在 Kubernetes 集群中，Pod 可以跨多个 AZ 部署，通过配置 <code>topologySpreadConstraints</code> 或使用适当的亲和性和反亲和性策略，确保应用在多个 AZ 中有冗余备份，避免某个 AZ 故障导致整个服务不可用。</li><li><strong>Persistent Volume 跨 AZ 存储</strong>：使用跨 AZ 的持久化存储（如云提供商的跨区域存储卷）确保在一个 AZ 故障时，应用的持久化数据能够快速恢复。某些云平台如 AWS EBS 支持跨可用区的数据复制。</li></ul><h3 id=6-健康检查和负载均衡>6. <strong>健康检查和负载均衡</strong>
<a class=anchor href=#6-%e5%81%a5%e5%ba%b7%e6%a3%80%e6%9f%a5%e5%92%8c%e8%b4%9f%e8%bd%bd%e5%9d%87%e8%a1%a1>#</a></h3><ul><li><strong>健康检查</strong>：通过存活检测和就绪检测确保每个组件（如 kube-apiserver、etcd、kubelet 等）都处于正常状态。如果某个组件失败，自动进行自愈或重新调度。</li><li><strong>Pod 调度和负载均衡</strong>：Kubernetes 调度器会根据集群的负载和容器的资源需求，将 Pod 分配到合适的 Node 节点上，同时利用 <code>Service</code> 和 <code>Ingress</code> 对外暴露应用，确保流量能够在不同的 Pod 和节点之间负载均衡。</li></ul><h3 id=7-灾难恢复与容错>7. <strong>灾难恢复与容错</strong>
<a class=anchor href=#7-%e7%81%be%e9%9a%be%e6%81%a2%e5%a4%8d%e4%b8%8e%e5%ae%b9%e9%94%99>#</a></h3><ul><li><strong>Etcd 高可用</strong>：Etcd 是 Kubernetes 集群的关键组件，存储了集群的所有配置信息和状态。通过将 Etcd 部署为高可用集群（通常是奇数个节点，如 3 个或 5 个节点），确保在任意节点故障时，集群依然能够正常工作。</li><li><strong>多区域冗余</strong>：通过跨区域部署 Etcd 节点和 Master 节点，确保在单个区域或数据中心发生故障时，其他区域可以继续提供服务。即使在大规模灾难中，也能最大程度地保持系统的可用性。</li></ul><h3 id=8-备份与恢复>8. <strong>备份与恢复</strong>
<a class=anchor href=#8-%e5%a4%87%e4%bb%bd%e4%b8%8e%e6%81%a2%e5%a4%8d>#</a></h3><ul><li><strong>定期备份</strong>：定期备份 Kubernetes 集群的核心组件（如 etcd、kube-apiserver 的配置等），以应对数据丢失或灾难恢复需求。</li><li><strong>恢复方案</strong>：测试并制定恢复方案，在发生故障时能够迅速将集群恢复到正常状态。例如，可以使用 Velero 或其他备份工具进行集群备份和恢复。</li></ul><h3 id=总结>总结
<a class=anchor href=#%e6%80%bb%e7%bb%93>#</a></h3><p>通过这样的高可用架构设计，Kubernetes 集群能够在面对节点故障、区域故障等情况下保持高度的可靠性和可扩展性。联邦架构、跨机房容灾、多 AZ 部署、Etcd 高可用等设计确保了应用在不同的业务需求下能够得到良好的支持，同时具备了强大的容错能力和扩展能力。</p><h1 id=etcd或者控制面组件的容灾方案>etcd或者控制面组件的容灾方案？
<a class=anchor href=#etcd%e6%88%96%e8%80%85%e6%8e%a7%e5%88%b6%e9%9d%a2%e7%bb%84%e4%bb%b6%e7%9a%84%e5%ae%b9%e7%81%be%e6%96%b9%e6%a1%88>#</a></h1><p>Kubernetes <strong>etcd</strong> 和 <strong>控制面（Control Plane）</strong> 组件的容灾方案至关重要，因为它们决定了整个集群的可用性。以下是 <strong>etcd</strong> 和 <strong>控制面组件</strong> 的容灾方案，包括高可用部署、备份恢复、自动故障转移等策略。</p><hr><h2 id=1-etcd-容灾方案><strong>1. etcd 容灾方案</strong>
<a class=anchor href=#1-etcd-%e5%ae%b9%e7%81%be%e6%96%b9%e6%a1%88>#</a></h2><p><strong>etcd 是 Kubernetes 的核心数据存储，存储了所有 API 资源对象的状态。如果 etcd 发生故障，整个 Kubernetes 控制面将无法正常工作。</strong></p><h3 id=1-etcd-高可用部署><strong>(1) etcd 高可用部署</strong>
<a class=anchor href=#1-etcd-%e9%ab%98%e5%8f%af%e7%94%a8%e9%83%a8%e7%bd%b2>#</a></h3><ul><li><strong>多节点集群（3、5、7 等奇数个节点）</strong><ul><li>etcd 需要使用<strong>奇数个节点</strong>，以保证 <strong>Raft 共识算法</strong>可以正确选举 leader。</li><li>通常推荐<strong>至少 3 个节点</strong>，这样即使宕掉 1 个，集群仍能继续运行。</li><li>生产环境推荐<strong>5 个节点</strong>，以提升容灾能力。</li></ul></li><li><strong>跨 AZ 或机房部署</strong><ul><li>采用 <strong>多 AZ/多机房部署</strong>，分布式放置 etcd 节点，避免单点故障（如单个机房断网、断电）。</li><li>如果使用云环境（如 AWS、阿里云），可以利用云平台的 <strong>可用区（AZ）</strong> 分布 etcd 实例。</li></ul></li><li><strong>负载均衡</strong><ul><li>etcd 集群之间的通信通常通过内部 DNS 解析，每个 etcd 成员需要使用<strong>固定 IP</strong> 或 <strong>DNS 解析</strong>来进行互联。</li><li>也可以使用 <strong>Keepalived + HAProxy</strong> 进行高可用性负载均衡，以防止某个 etcd 实例崩溃时影响集群连接。</li></ul></li></ul><hr><h3 id=2-etcd-备份与恢复><strong>(2) etcd 备份与恢复</strong>
<a class=anchor href=#2-etcd-%e5%a4%87%e4%bb%bd%e4%b8%8e%e6%81%a2%e5%a4%8d>#</a></h3><ul><li><p><strong>定期快照备份</strong></p><ul><li><p>通过</p><pre tabindex=0><code>etcdctl snapshot save
</code></pre><p>命令定期备份 etcd 数据：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ETCDCTL_API<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span> etcdctl snapshot save /backup/etcd-snapshot.db
</span></span></code></pre></div></li><li><p>定期将快照文件存储到远程存储（如 <strong>阿里云 OSS、AWS S3、Ceph</strong>）。</p></li></ul></li><li><p><strong>恢复 etcd 集群</strong></p><ul><li><p>当 etcd 发生数据损坏或集群崩溃时，可以使用快照恢复：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ETCDCTL_API<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span> etcdctl snapshot restore /backup/etcd-snapshot.db
</span></span></code></pre></div></li><li><p>重新配置 <code>etcd</code>，并启动新的 etcd 实例加入集群。</p></li></ul></li><li><p><strong>监控 etcd 状态</strong></p><ul><li><p>使用</p><pre tabindex=0><code>etcdctl endpoint status
</code></pre><p>查看 etcd 健康状态：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ETCDCTL_API<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span> etcdctl endpoint status --write-out<span style=color:#f92672>=</span>table
</span></span></code></pre></div></li><li><p>结合 <strong>Prometheus + Grafana</strong> 监控 etcd 关键指标，如 <code>etcd_server_leader_changes_seen_total</code>（leader 变更频率）、<code>etcd_disk_wal_fsync_duration_seconds</code>（写入延迟）。</p></li></ul></li></ul><hr><h3 id=3-etcd-自动故障转移><strong>(3) etcd 自动故障转移</strong>
<a class=anchor href=#3-etcd-%e8%87%aa%e5%8a%a8%e6%95%85%e9%9a%9c%e8%bd%ac%e7%a7%bb>#</a></h3><ul><li>自动选主<ul><li>etcd 使用 Raft 协议进行 leader 选举，<strong>当 leader 节点崩溃后，剩余 etcd 成员会自动选举新的 leader</strong>。</li><li>如果集群中 etcd 成员低于半数（<code>N/2+1</code>），则集群进入 <strong>Read-Only</strong> 模式，API Server 可能无法正常工作。</li><li>解决方案：<ul><li>及早发现 etcd 故障，及时扩容新的 etcd 成员。</li><li>通过 <code>etcdctl member list</code> 监控 etcd 成员状态，并在必要时手动重新加入新的 etcd 节点。</li></ul></li></ul></li></ul><hr><h2 id=2-kubernetes-控制面组件容灾方案><strong>2. Kubernetes 控制面组件容灾方案</strong>
<a class=anchor href=#2-kubernetes-%e6%8e%a7%e5%88%b6%e9%9d%a2%e7%bb%84%e4%bb%b6%e5%ae%b9%e7%81%be%e6%96%b9%e6%a1%88>#</a></h2><p>Kubernetes 控制面包括以下核心组件：</p><ol><li><strong>kube-apiserver</strong>（API 服务器）</li><li><strong>kube-scheduler</strong>（调度器）</li><li><strong>kube-controller-manager</strong>（控制器管理器）</li><li><strong>kubelet</strong>（节点管理）</li><li><strong>kube-proxy</strong>（网络代理）</li><li><strong>etcd</strong>（数据存储，前面已详细说明）</li></ol><hr><h3 id=1-高可用架构><strong>(1) 高可用架构</strong>
<a class=anchor href=#1-%e9%ab%98%e5%8f%af%e7%94%a8%e6%9e%b6%e6%9e%84>#</a></h3><h4 id=多-master-高可用><strong>多 Master 高可用</strong>
<a class=anchor href=#%e5%a4%9a-master-%e9%ab%98%e5%8f%af%e7%94%a8>#</a></h4><ul><li><p>采用 <strong>多 Master</strong> 架构（至少 3 个），避免单点故障：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>+----------------------+        +----------------------+
</span></span><span style=display:flex><span>|   Master-1 (Active)  |        |   Master-2 (Backup)  |
</span></span><span style=display:flex><span>|   kube-apiserver     |  &lt;---&gt; |   kube-apiserver     |
</span></span><span style=display:flex><span>|   kube-controller    |        |   kube-controller    |
</span></span><span style=display:flex><span>|   kube-scheduler     |        |   kube-scheduler     |
</span></span><span style=display:flex><span>+----------------------+        +----------------------+
</span></span><span style=display:flex><span>         |                                |
</span></span><span style=display:flex><span>         |                                |
</span></span><span style=display:flex><span>      +----------------------------------------+
</span></span><span style=display:flex><span>      |       HAProxy / Keepalived / VIP      |
</span></span><span style=display:flex><span>      +----------------------------------------+
</span></span><span style=display:flex><span>         |
</span></span><span style=display:flex><span>      +----------------+
</span></span><span style=display:flex><span>      |  Worker Nodes  |
</span></span><span style=display:flex><span>      +----------------+
</span></span></code></pre></div></li><li><p><strong>kube-apiserver</strong></p><ul><li>部署多个 <code>kube-apiserver</code> 实例，并使用 <strong>负载均衡器（HAProxy、Nginx、Keepalived）</strong> 进行流量分发，避免单点故障。</li></ul></li><li><p><strong>kube-controller-manager 和 kube-scheduler</strong></p><ul><li>只能运行在单个 <code>leader</code> 节点上（采用 <code>leader election</code> 机制）。</li><li>其他 master 节点会等待 leader 失效后自动接管控制权。</li></ul></li><li><p><strong>负载均衡</strong></p><ul><li><p>Keepalived + HAProxy</p><p>：</p><ul><li>Keepalived 提供 VIP 地址，实现故障时的 API Server 切换。</li><li>HAProxy 进行负载均衡，分发 API 请求。</li></ul></li></ul></li></ul><hr><h3 id=2-监控与自动恢复><strong>(2) 监控与自动恢复</strong>
<a class=anchor href=#2-%e7%9b%91%e6%8e%a7%e4%b8%8e%e8%87%aa%e5%8a%a8%e6%81%a2%e5%a4%8d>#</a></h3><ul><li><strong>Prometheus 监控</strong><ul><li>监控 <code>kube-apiserver</code> 响应时间、<code>etcd</code> 读写延迟等关键指标。</li><li>发现异常时，触发自动扩容或故障转移。</li></ul></li><li><strong>自动重启</strong><ul><li>结合 <strong>systemd</strong> 或 <strong>Kubernetes 自愈能力（livenessProbe + readinessProbe）</strong>，确保控制面组件崩溃时自动重启。</li></ul></li></ul><hr><h3 id=3-备份与恢复><strong>(3) 备份与恢复</strong>
<a class=anchor href=#3-%e5%a4%87%e4%bb%bd%e4%b8%8e%e6%81%a2%e5%a4%8d>#</a></h3><ul><li><p><strong>API Server 备份</strong></p><ul><li><p>备份</p><pre tabindex=0><code>kube-apiserver
</code></pre><p>配置文件：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cp -r /etc/kubernetes /backup/kubernetes
</span></span></code></pre></div></li><li><p>备份</p><pre tabindex=0><code>kubeconfig
</code></pre><p>文件：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cp ~/.kube/config /backup/kube-config
</span></span></code></pre></div></li></ul></li><li><p><strong>恢复 API Server</strong></p><ul><li><p>如果</p><pre tabindex=0><code>kube-apiserver
</code></pre><p>崩溃：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>systemctl restart kube-apiserver
</span></span></code></pre></div></li><li><p>如果 API Server 故障，可以切换至备用 master 节点。</p></li></ul></li></ul><hr><h2 id=3-故障恢复策略><strong>3. 故障恢复策略</strong>
<a class=anchor href=#3-%e6%95%85%e9%9a%9c%e6%81%a2%e5%a4%8d%e7%ad%96%e7%95%a5>#</a></h2><table><thead><tr><th>组件</th><th>可能的故障</th><th>容灾方案</th></tr></thead><tbody><tr><td>etcd</td><td>单节点故障</td><td>Raft 自动选举新 leader</td></tr><tr><td>etcd</td><td>多节点故障</td><td>通过快照恢复数据</td></tr><tr><td>kube-apiserver</td><td>API 无法访问</td><td>负载均衡 + 备用 master 切换</td></tr><tr><td>kube-controller-manager</td><td>leader 故障</td><td>其他 master 自动接管</td></tr><tr><td>kube-scheduler</td><td>leader 故障</td><td>其他 master 自动选举新 leader</td></tr></tbody></table><hr><h2 id=4-结论><strong>4. 结论</strong>
<a class=anchor href=#4-%e7%bb%93%e8%ae%ba>#</a></h2><ul><li><strong>etcd 高可用</strong>：至少 3 个节点，跨 AZ 部署，定期备份，自动 leader 选举。</li><li><strong>控制面高可用</strong>：多 Master 部署，Keepalived/HAProxy 负载均衡，自动选主。</li><li><strong>监控与告警</strong>：Prometheus 监控，发现异常时自动扩容或切换。</li><li><strong>备份与恢复</strong>：定期备份 etcd 数据和控制面配置，快速恢复服务。</li></ul><p>这套方案可以保证 Kubernetes 在发生故障时能够快速恢复，提升集群的可靠性和稳定性。</p><h1 id=演练怎么设计演练场景覆盖核心风险和推动改进的措施>演练，怎么设计演练场景，覆盖核心风险和推动改进的措施？
<a class=anchor href=#%e6%bc%94%e7%bb%83%e6%80%8e%e4%b9%88%e8%ae%be%e8%ae%a1%e6%bc%94%e7%bb%83%e5%9c%ba%e6%99%af%e8%a6%86%e7%9b%96%e6%a0%b8%e5%bf%83%e9%a3%8e%e9%99%a9%e5%92%8c%e6%8e%a8%e5%8a%a8%e6%94%b9%e8%bf%9b%e7%9a%84%e6%8e%aa%e6%96%bd>#</a></h1><h2 id=kubernetes-容灾演练方案设计><strong>Kubernetes 容灾演练方案设计</strong>
<a class=anchor href=#kubernetes-%e5%ae%b9%e7%81%be%e6%bc%94%e7%bb%83%e6%96%b9%e6%a1%88%e8%ae%be%e8%ae%a1>#</a></h2><p>容灾演练的目的是<strong>验证 Kubernetes 集群在关键组件故障、节点丢失、网络异常等情况下的恢复能力</strong>，并找到改进点，提升系统的高可用性。
容灾演练需覆盖 <strong>etcd、控制面、计算节点、存储、网络等核心组件</strong>，并推演各种可能的故障场景。</p><hr><h1 id=1-演练目标><strong>1. 演练目标</strong>
<a class=anchor href=#1-%e6%bc%94%e7%bb%83%e7%9b%ae%e6%a0%87>#</a></h1><p>✅ 验证 Kubernetes <strong>高可用架构</strong>是否可靠
✅ 发现可能的<strong>单点故障（SPOF）</strong>
✅ 确保 <strong>etcd 备份与恢复机制</strong>有效
✅ 测试 <strong>负载均衡与故障转移能力</strong>
✅ 提升应急响应效率，优化告警、日志、监控机制</p><hr><h1 id=2-演练核心场景><strong>2. 演练核心场景</strong>
<a class=anchor href=#2-%e6%bc%94%e7%bb%83%e6%a0%b8%e5%bf%83%e5%9c%ba%e6%99%af>#</a></h1><p>容灾演练分为 <strong>核心组件故障</strong>、<strong>网络异常</strong>、<strong>存储故障</strong>、<strong>高并发压力测试</strong>等。</p><h2 id=-场景-1etcd-故障><strong>🟢 场景 1：etcd 故障</strong>
<a class=anchor href=#-%e5%9c%ba%e6%99%af-1etcd-%e6%95%85%e9%9a%9c>#</a></h2><p>🎯 <strong>目标</strong>：确保 etcd 崩溃后 Kubernetes 仍能正常运作，并验证 etcd 快照恢复机制。</p><p>🔹 <strong>演练步骤</strong>：</p><ol><li><p>删除 etcd Leader 节点</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker stop etcd
</span></span></code></pre></div></li><li><p>观察 <code>etcdctl endpoint status</code>，确认新 Leader 选举情况。</p></li><li><p>模拟多节点 etcd 故障</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>systemctl stop etcd
</span></span></code></pre></div></li><li><p>观察 Kubernetes API 是否可用：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get nodes
</span></span></code></pre></div></li><li><p>恢复 etcd</p><ul><li><p>若单节点故障，等待 etcd 自动选主。</p></li><li><p>若集群崩溃，使用 etcd 备份恢复：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ETCDCTL_API<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span> etcdctl snapshot restore /backup/etcd-snapshot.db
</span></span></code></pre></div></li></ul></li></ol><p>🛠 <strong>改进措施</strong>：</p><ul><li>确保 <strong>etcd 备份策略有效</strong>（定期快照 + 远程存储）。</li><li>监控 <code>etcd_server_leader_changes_seen_total</code> 频率，提前发现异常。</li></ul><hr><h2 id=-场景-2master-节点故障><strong>🟡 场景 2：Master 节点故障</strong>
<a class=anchor href=#-%e5%9c%ba%e6%99%af-2master-%e8%8a%82%e7%82%b9%e6%95%85%e9%9a%9c>#</a></h2><p>🎯 <strong>目标</strong>：确保 Kubernetes <strong>控制面</strong>具备高可用性，验证 <strong>API Server 故障转移能力</strong>。</p><p>🔹 <strong>演练步骤</strong>：</p><ol><li><p>关闭主 Master 节点</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>systemctl stop kube-apiserver
</span></span></code></pre></div></li><li><p>观察 <code>kubectl get nodes</code> 和 <code>kubectl get pods -n kube-system</code>，确认其他 Master 是否接管流量。</p></li><li><p>模拟所有 Master 节点宕机</p><ul><li><p>关闭所有 API Server 进程：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>systemctl stop kube-apiserver kube-controller-manager kube-scheduler
</span></span></code></pre></div></li><li><p>观察 <strong>集群状态、负载均衡</strong>，验证 Keepalived 是否切换 VIP。</p></li><li><p>重新启动 Master：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>systemctl start kube-apiserver kube-controller-manager kube-scheduler
</span></span></code></pre></div></li></ul></li><li><p>确认控制面是否恢复。</p></li></ol><p>🛠 <strong>改进措施</strong>：</p><ul><li>确保 <strong>HAProxy / Keepalived VIP</strong> 可用，自动切换。</li><li>监控 <strong>API Server 响应时间</strong>，防止超时导致业务异常。</li></ul><hr><h2 id=-场景-3node-计算节点故障><strong>🔴 场景 3：Node 计算节点故障</strong>
<a class=anchor href=#-%e5%9c%ba%e6%99%af-3node-%e8%ae%a1%e7%ae%97%e8%8a%82%e7%82%b9%e6%95%85%e9%9a%9c>#</a></h2><p>🎯 <strong>目标</strong>：验证 <strong>Pod 迁移能力</strong>，确保业务不会因 Worker 节点故障而中断。</p><p>🔹 <strong>演练步骤</strong>：</p><ol><li><p>选取一个 Node，</p><p>驱逐该节点所有 Pod</p><p>：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl drain &lt;node-name&gt; --ignore-daemonsets --delete-local-data
</span></span></code></pre></div></li><li><p>观察 Pod 迁移</p><p>：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get pods -o wide
</span></span></code></pre></div></li><li><p>彻底下线该节点</p><p>：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl delete node &lt;node-name&gt;
</span></span></code></pre></div></li><li><p>重新加入该节点</p><p>：</p><ul><li><p>重新安装 kubelet：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubeadm reset <span style=color:#f92672>&amp;&amp;</span> kubeadm join ...
</span></span></code></pre></div></li><li><p>确认节点恢复：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get nodes
</span></span></code></pre></div></li></ul></li></ol><p>🛠 <strong>改进措施</strong>：</p><ul><li>业务应用应配置 <strong>Pod 反亲和性、HPA（自动扩缩容）</strong>，防止节点故障时业务不可用。</li><li>监控 <code>kubelet</code> 状态，确保节点故障能<strong>自动触发告警</strong>。</li></ul><hr><h2 id=-场景-4ingressservice-负载均衡异常><strong>🟠 场景 4：Ingress/Service 负载均衡异常</strong>
<a class=anchor href=#-%e5%9c%ba%e6%99%af-4ingressservice-%e8%b4%9f%e8%bd%bd%e5%9d%87%e8%a1%a1%e5%bc%82%e5%b8%b8>#</a></h2><p>🎯 <strong>目标</strong>：模拟负载均衡异常，验证流量切换是否正常。</p><p>🔹 <strong>演练步骤</strong>：</p><ol><li><p>关闭 Ingress Controller</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl delete pod -n ingress-nginx --all
</span></span></code></pre></div></li><li><p>观察外部请求是否受影响：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>curl http://&lt;ingress-url&gt;
</span></span></code></pre></div></li><li><p>删除 Service 负载均衡器</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl delete service my-lb-service
</span></span></code></pre></div></li><li><p>确认 Service 是否重新创建，并绑定新的 IP。</p></li></ol><p>🛠 <strong>改进措施</strong>：</p><ul><li>增加 <strong>多副本 Ingress Controller</strong>，避免单点故障。</li><li>使用 <strong>MetalLB / Keepalived</strong> 保障 Service 高可用。</li></ul><hr><h2 id=-场景-5存储故障><strong>🔵 场景 5：存储故障</strong>
<a class=anchor href=#-%e5%9c%ba%e6%99%af-5%e5%ad%98%e5%82%a8%e6%95%85%e9%9a%9c>#</a></h2><p>🎯 <strong>目标</strong>：确保 PVC 挂载失败时，应用能正确恢复。</p><p>🔹 <strong>演练步骤</strong>：</p><ol><li><p>删除存储节点</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl delete node storage-node-1
</span></span></code></pre></div></li><li><p>观察 PVC 状态</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get pvc -A
</span></span></code></pre></div></li><li><p>重新创建存储节点</p><p>：</p><ul><li>确保新的存储节点能正确绑定 PVC。</li><li>确认 Pod 能正确访问数据。</li></ul></li></ol><p>🛠 <strong>改进措施</strong>：</p><ul><li>采用 <strong>Ceph、NFS、GlusterFS</strong> 等分布式存储，避免单点故障。</li><li>增加 <strong>PVC 绑定策略</strong>，确保数据不会丢失。</li></ul><hr><h1 id=3-保障措施><strong>3. 保障措施</strong>
<a class=anchor href=#3-%e4%bf%9d%e9%9a%9c%e6%8e%aa%e6%96%bd>#</a></h1><p>✅ <strong>提前通知</strong>：制定演练时间，通知相关团队
✅ <strong>预案准备</strong>：提前配置恢复方案，确保快速回滚
✅ <strong>监控告警</strong>：结合 Prometheus + Grafana 监控关键指标
✅ <strong>日志分析</strong>：结合 ELK / Loki 进行故障分析
✅ <strong>演练总结</strong>：记录演练过程，持续改进</p><hr><h1 id=4-结论-1><strong>4. 结论</strong>
<a class=anchor href=#4-%e7%bb%93%e8%ae%ba-1>#</a></h1><ul><li>通过<strong>故障注入</strong>（如 <code>etcdctl snapshot restore</code>、<code>kubectl drain</code>），模拟核心组件故障，验证集群的恢复能力。</li><li>监控 <strong>etcd leader 选举、API Server 响应、Pod 迁移、负载均衡</strong> 等关键路径，找出潜在风险点。</li><li>结合 Prometheus + ELK 监控故障，提升告警和自动恢复能力。</li></ul><p>这套方案可以帮助 Kubernetes 集群在极端情况下<strong>稳定运行、快速恢复、降低业务影响</strong>。 🚀</p><h1 id=演练发现的问题和改进的地方>演练发现的问题和改进的地方？
<a class=anchor href=#%e6%bc%94%e7%bb%83%e5%8f%91%e7%8e%b0%e7%9a%84%e9%97%ae%e9%a2%98%e5%92%8c%e6%94%b9%e8%bf%9b%e7%9a%84%e5%9c%b0%e6%96%b9>#</a></h1><h2 id=演练发现的问题与改进方案><strong>演练发现的问题与改进方案</strong>
<a class=anchor href=#%e6%bc%94%e7%bb%83%e5%8f%91%e7%8e%b0%e7%9a%84%e9%97%ae%e9%a2%98%e4%b8%8e%e6%94%b9%e8%bf%9b%e6%96%b9%e6%a1%88>#</a></h2><p>在 Kubernetes 容灾演练过程中，可能会发现一些系统设计、运维管理、组件可靠性方面的<strong>潜在风险</strong>。下面总结了一些常见问题，并提供对应的改进方案。</p><hr><h1 id=1-etcd-相关问题><strong>1. etcd 相关问题</strong>
<a class=anchor href=#1-etcd-%e7%9b%b8%e5%85%b3%e9%97%ae%e9%a2%98>#</a></h1><h3 id=-问题-1etcd-选主时间过长影响-api-server-响应><strong>🛑 问题 1：etcd 选主时间过长，影响 API Server 响应</strong>
<a class=anchor href=#-%e9%97%ae%e9%a2%98-1etcd-%e9%80%89%e4%b8%bb%e6%97%b6%e9%97%b4%e8%bf%87%e9%95%bf%e5%bd%b1%e5%93%8d-api-server-%e5%93%8d%e5%ba%94>#</a></h3><ul><li><strong>现象</strong>：<ul><li><code>kubectl get nodes</code> 等命令超时</li><li><code>etcdctl endpoint status</code> 显示 <code>leader</code> 变化</li><li><code>kube-apiserver</code> 无法正常访问 etcd</li></ul></li><li><strong>原因</strong>：<ul><li>etcd 选举时间过长</li><li>网络抖动导致 etcd 连接断开</li><li>etcd 负载高，写入慢，影响 leader 选举</li></ul></li></ul><p>✅ <strong>改进方案</strong>：</p><ul><li><p>调整 etcd 选举超时时间</p><p>（默认 1000ms，建议 200ms~500ms）：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>election-timeout</span>: <span style=color:#ae81ff>250ms</span>
</span></span></code></pre></div></li><li><p>优化 etcd 性能：</p><ul><li>使用 SSD 作为存储盘，避免磁盘 IO 瓶颈</li><li><code>etcdctl defrag</code> 释放碎片化空间</li><li>限制 <code>kube-apiserver</code> 访问 etcd 频率（减少高并发请求）</li></ul></li><li><p>增加 etcd 监控：</p><ul><li><code>etcd_server_leader_changes_seen_total</code> 频繁变化 → 可能有网络问题</li><li><code>etcd_disk_wal_fsync_duration_seconds_bucket</code> 高 → 说明磁盘 IO 存在瓶颈</li></ul></li></ul><hr><h3 id=-问题-2etcd-备份未生效恢复失败><strong>🛑 问题 2：etcd 备份未生效，恢复失败</strong>
<a class=anchor href=#-%e9%97%ae%e9%a2%98-2etcd-%e5%a4%87%e4%bb%bd%e6%9c%aa%e7%94%9f%e6%95%88%e6%81%a2%e5%a4%8d%e5%a4%b1%e8%b4%a5>#</a></h3><ul><li><strong>现象</strong>：<ul><li>使用 <code>etcdctl snapshot restore</code> 恢复时，发现 <code>data-dir</code> 路径冲突</li><li><code>etcdctl member list</code> 显示数据不完整</li></ul></li><li><strong>原因</strong>：<ul><li>etcd 备份不完整，恢复时报错</li><li>备份未包含 <code>member</code> 目录，导致 <code>etcd</code> 无法正确加载快照</li></ul></li></ul><p>✅ <strong>改进方案</strong>：</p><ul><li><p>使用正确方式备份 etcd：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ETCDCTL_API<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span> etcdctl snapshot save /backup/etcd-snapshot.db
</span></span></code></pre></div></li><li><p>恢复时指定新的 <code>data-dir</code>：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ETCDCTL_API<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span> etcdctl snapshot restore /backup/etcd-snapshot.db --data-dir<span style=color:#f92672>=</span>/var/lib/etcd-new
</span></span></code></pre></div></li><li><p>定期检查 etcd 备份完整性：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>etcdctl snapshot status /backup/etcd-snapshot.db
</span></span></code></pre></div></li></ul><hr><h1 id=2-master-组件问题><strong>2. Master 组件问题</strong>
<a class=anchor href=#2-master-%e7%bb%84%e4%bb%b6%e9%97%ae%e9%a2%98>#</a></h1><h3 id=-问题-3kube-apiserver-宕机负载均衡未生效><strong>🛑 问题 3：kube-apiserver 宕机，负载均衡未生效</strong>
<a class=anchor href=#-%e9%97%ae%e9%a2%98-3kube-apiserver-%e5%ae%95%e6%9c%ba%e8%b4%9f%e8%bd%bd%e5%9d%87%e8%a1%a1%e6%9c%aa%e7%94%9f%e6%95%88>#</a></h3><ul><li><strong>现象</strong>：<ul><li><code>kubectl</code> 命令卡住，API 无法访问</li><li><code>haproxy</code> 仍然转发流量到故障 Master</li></ul></li><li><strong>原因</strong>：<ul><li><strong>负载均衡健康检查失效</strong>，导致流量仍然转发到宕机 Master</li><li><code>kube-apiserver</code> <strong>未配置多个 <code>--etcd-servers</code></strong></li><li><strong>VIP 切换失败</strong>（Keepalived 未正常触发）</li></ul></li></ul><p>✅ <strong>改进方案</strong>：</p><ul><li><p>优化负载均衡健康检查：</p><pre tabindex=0><code class=language-haproxy data-lang=haproxy>backend kube-apiservers
  option httpchk GET /livez
  default-server check fall 3 rise 2
</code></pre></li><li><p>配置 API Server 连接多个 etcd 节点：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>--<span style=color:#ae81ff>etcd-servers=https://etcd-1:2379,https://etcd-2:2379,https://etcd-3:2379</span>
</span></span></code></pre></div></li><li><p>优化 Keepalived VIP 切换：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>vrrp_script chk_apiserver <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>  script <span style=color:#e6db74>&#34;curl -s http://127.0.0.1:6443/livez || exit 1&#34;</span>
</span></span><span style=display:flex><span>  interval <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>  weight -2
</span></span><span style=display:flex><span><span style=color:#f92672>}</span>
</span></span></code></pre></div></li></ul><hr><h1 id=3-node-计算节点问题><strong>3. Node 计算节点问题</strong>
<a class=anchor href=#3-node-%e8%ae%a1%e7%ae%97%e8%8a%82%e7%82%b9%e9%97%ae%e9%a2%98>#</a></h1><h3 id=-问题-4节点驱逐后-pod-迁移失败><strong>🛑 问题 4：节点驱逐后 Pod 迁移失败</strong>
<a class=anchor href=#-%e9%97%ae%e9%a2%98-4%e8%8a%82%e7%82%b9%e9%a9%b1%e9%80%90%e5%90%8e-pod-%e8%bf%81%e7%a7%bb%e5%a4%b1%e8%b4%a5>#</a></h3><ul><li><strong>现象</strong>：<ul><li><code>kubectl drain</code> 后，Pod 没有调度到其他节点</li><li><code>kubectl get pods -o wide</code> 发现 Pod 卡在 <code>Terminating</code> 状态</li></ul></li><li><strong>原因</strong>：<ul><li><strong>Pod 没有定义资源限制</strong>，导致新节点资源不足</li><li><strong>Pod 绑定到指定 Node</strong>（<code>nodeSelector</code>、<code>affinity</code> 限制）</li><li><strong>DaemonSet 不能自动迁移</strong>，影响 Node 级组件（如 <code>fluentd</code>）</li></ul></li></ul><p>✅ <strong>改进方案</strong>：</p><ul><li><p>给 Pod 设定 <code>requests</code> 和 <code>limits</code>，确保有足够资源：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>resources</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>requests</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>cpu</span>: <span style=color:#e6db74>&#34;100m&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>memory</span>: <span style=color:#e6db74>&#34;200Mi&#34;</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>limits</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>cpu</span>: <span style=color:#e6db74>&#34;500m&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>memory</span>: <span style=color:#e6db74>&#34;1Gi&#34;</span>
</span></span></code></pre></div></li><li><p>检查 <code>nodeSelector</code> / <code>affinity</code> 规则，避免过于严格：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>affinity</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>nodeAffinity</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>requiredDuringSchedulingIgnoredDuringExecution</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>nodeSelectorTerms</span>:
</span></span><span style=display:flex><span>      - <span style=color:#f92672>matchExpressions</span>:
</span></span><span style=display:flex><span>        - <span style=color:#f92672>key</span>: <span style=color:#ae81ff>kubernetes.io/hostname</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>operator</span>: <span style=color:#ae81ff>NotIn</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>values</span>:
</span></span><span style=display:flex><span>          - <span style=color:#ae81ff>failed-node</span>
</span></span></code></pre></div></li><li><p><strong>使用 <code>kubectl delete pod --force</code> 释放卡住的 Pod</strong>。</p></li></ul><hr><h1 id=4-ingress--service-问题><strong>4. Ingress / Service 问题</strong>
<a class=anchor href=#4-ingress--service-%e9%97%ae%e9%a2%98>#</a></h1><h3 id=-问题-5ingress-nginx-宕机后业务不可访问><strong>🛑 问题 5：Ingress Nginx 宕机后，业务不可访问</strong>
<a class=anchor href=#-%e9%97%ae%e9%a2%98-5ingress-nginx-%e5%ae%95%e6%9c%ba%e5%90%8e%e4%b8%9a%e5%8a%a1%e4%b8%8d%e5%8f%af%e8%ae%bf%e9%97%ae>#</a></h3><ul><li><strong>现象</strong>：<ul><li>业务访问 <code>502 Bad Gateway</code></li><li><code>kubectl get pods -n ingress-nginx</code> 发现 Pod 处于 <code>CrashLoopBackOff</code></li></ul></li><li><strong>原因</strong>：<ul><li><strong>Ingress Controller 只有 1 个副本</strong>，没有高可用</li><li><strong>Pod 绑定了指定 Node</strong>，导致该 Node 故障时影响所有流量</li><li><strong>Backend Service 无可用 Pod</strong></li></ul></li></ul><p>✅ <strong>改进方案</strong>：</p><ul><li><p>增加 Ingress 副本数：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>replicas</span>: <span style=color:#ae81ff>3</span>
</span></span></code></pre></div></li><li><p>移除固定节点绑定：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>nodeSelector</span>: {}
</span></span></code></pre></div></li><li><p>在 Prometheus 监控 Ingress 组件状态，提前告警：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#ae81ff>rate(nginx_ingress_controller_requests[5m])</span>
</span></span></code></pre></div></li></ul><hr><h1 id=5-存储问题><strong>5. 存储问题</strong>
<a class=anchor href=#5-%e5%ad%98%e5%82%a8%e9%97%ae%e9%a2%98>#</a></h1><h3 id=-问题-6pvc-绑定失败pod-进入><strong>🛑 问题 6：PVC 绑定失败，Pod 进入 <code>Pending</code> 状态</strong>
<a class=anchor href=#-%e9%97%ae%e9%a2%98-6pvc-%e7%bb%91%e5%ae%9a%e5%a4%b1%e8%b4%a5pod-%e8%bf%9b%e5%85%a5>#</a></h3><ul><li><strong>现象</strong>：<ul><li><code>kubectl describe pvc</code> 显示 <code>Waiting for first consumer</code></li><li><code>kubectl get pods</code> 发现 <code>MountVolume.SetUp failed</code></li></ul></li><li><strong>原因</strong>：<ul><li><strong>Pod 没有调度到存储支持的节点</strong></li><li><strong>存储类 <code>StorageClass</code> 没有正确绑定 PVC</strong></li><li><strong>底层 NFS / Ceph 故障，导致存储不可用</strong></li></ul></li></ul><p>✅ <strong>改进方案</strong>：</p><ul><li><p>强制 Pod 绑定支持存储的节点：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>affinity</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>nodeAffinity</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>requiredDuringSchedulingIgnoredDuringExecution</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>nodeSelectorTerms</span>:
</span></span><span style=display:flex><span>      - <span style=color:#f92672>matchExpressions</span>:
</span></span><span style=display:flex><span>        - <span style=color:#f92672>key</span>: <span style=color:#ae81ff>storage-node</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>operator</span>: <span style=color:#ae81ff>In</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>values</span>:
</span></span><span style=display:flex><span>          - <span style=color:#e6db74>&#34;true&#34;</span>
</span></span></code></pre></div></li><li><p>提前 Provision 预分配 PVC，避免动态绑定失败：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl create pvc --storage-class<span style=color:#f92672>=</span>fast-storage
</span></span></code></pre></div></li><li><p>监控存储后端健康状态：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ceph -s
</span></span></code></pre></div></li></ul><hr><h2 id=总结-1><strong>总结</strong>
<a class=anchor href=#%e6%80%bb%e7%bb%93-1>#</a></h2><p>✅ <strong>etcd 选主优化</strong>，减少 API Server 无响应时间
✅ <strong>负载均衡健康检查</strong>，避免 Master 宕机影响集群
✅ <strong>Pod 资源优化</strong>，确保节点故障时 Pod 迁移顺畅
✅ <strong>Ingress 高可用</strong>，避免流量入口单点故障
✅ <strong>存储策略优化</strong>，确保 PVC 绑定稳定</p><p>这些改进措施能提高 Kubernetes 集群的<strong>可靠性、恢复能力</strong>，让系统在发生故障时能快速恢复！ 🚀</p><h1 id=集群大规模启动pod失败怎么定位问题并调优>集群大规模启动pod失败，怎么定位问题并调优？
<a class=anchor href=#%e9%9b%86%e7%be%a4%e5%a4%a7%e8%a7%84%e6%a8%a1%e5%90%af%e5%8a%a8pod%e5%a4%b1%e8%b4%a5%e6%80%8e%e4%b9%88%e5%ae%9a%e4%bd%8d%e9%97%ae%e9%a2%98%e5%b9%b6%e8%b0%83%e4%bc%98>#</a></h1><h2 id=大规模启动-pod-失败的排查与优化方案><strong>大规模启动 Pod 失败的排查与优化方案</strong>
<a class=anchor href=#%e5%a4%a7%e8%a7%84%e6%a8%a1%e5%90%af%e5%8a%a8-pod-%e5%a4%b1%e8%b4%a5%e7%9a%84%e6%8e%92%e6%9f%a5%e4%b8%8e%e4%bc%98%e5%8c%96%e6%96%b9%e6%a1%88>#</a></h2><p>在 Kubernetes 集群中大规模启动 Pod 时，可能会遇到 Pod 处于 <strong><code>Pending</code></strong> 或 <strong><code>ContainerCreating</code></strong> 状态，甚至部分节点负载过高，导致集群整体不可用。这里提供<strong>完整的排查流程和优化方案</strong>。</p><hr><h2 id=-1-通过><strong>🛠 1. 通过 <code>kubectl</code> 和 <code>describe</code> 命令排查</strong>
<a class=anchor href=#-1-%e9%80%9a%e8%bf%87>#</a></h2><h3 id=-查看-pod-状态><strong>📌 查看 Pod 状态</strong>
<a class=anchor href=#-%e6%9f%a5%e7%9c%8b-pod-%e7%8a%b6%e6%80%81>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get pods -A -o wide
</span></span></code></pre></div><ul><li><strong>Pending</strong>：表示 Pod 还未被调度到节点</li><li><strong>ContainerCreating</strong>：Pod 已调度到节点，但容器未能成功创建</li><li><strong>CrashLoopBackOff</strong>：Pod 不断重启，可能是应用问题</li></ul><h3 id=-获取详细信息><strong>📌 获取详细信息</strong>
<a class=anchor href=#-%e8%8e%b7%e5%8f%96%e8%af%a6%e7%bb%86%e4%bf%a1%e6%81%af>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl describe pod &lt;pod-name&gt; -n &lt;namespace&gt;
</span></span></code></pre></div><ul><li><pre tabindex=0><code>Events
</code></pre><p>字段可查看错误原因：</p><ul><li><strong>FailedScheduling</strong>：调度失败，可能资源不足</li><li><strong>Network not ready</strong>：CNI 组件故障</li><li><strong>Failed to attach volume</strong>：存储挂载失败</li></ul></li></ul><hr><h2 id=-2-调度问题pod-无法被分配到节点><strong>🔍 2. 调度问题（Pod 无法被分配到节点）</strong>
<a class=anchor href=#-2-%e8%b0%83%e5%ba%a6%e9%97%ae%e9%a2%98pod-%e6%97%a0%e6%b3%95%e8%a2%ab%e5%88%86%e9%85%8d%e5%88%b0%e8%8a%82%e7%82%b9>#</a></h2><h3 id=-问题-1集群资源不足><strong>🛑 问题 1：集群资源不足</strong>
<a class=anchor href=#-%e9%97%ae%e9%a2%98-1%e9%9b%86%e7%be%a4%e8%b5%84%e6%ba%90%e4%b8%8d%e8%b6%b3>#</a></h3><ul><li><p>现象</p><p>：</p><ul><li><pre tabindex=0><code>kubectl describe pod
</code></pre><p>显示：</p><pre tabindex=0><code>0/50 nodes are available: insufficient CPU, insufficient memory
</code></pre></li><li><p><code>kubectl get nodes</code> 显示部分节点 <code>NotReady</code></p></li></ul></li></ul><p>✅ <strong>解决方案</strong>：</p><ol><li><p><strong>检查节点可用资源</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl top nodes
</span></span></code></pre></div><p>如果 CPU 或内存已满，可以：</p><ul><li><strong>释放无用 Pod</strong></li><li><strong>扩容 Worker 节点</strong></li></ul></li><li><p><strong>调整调度策略</strong>（使用 <code>priorityClass</code> 提升关键 Pod 优先级）</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>scheduling.k8s.io/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>PriorityClass</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>high-priority</span>
</span></span><span style=display:flex><span><span style=color:#f92672>value</span>: <span style=color:#ae81ff>1000000</span>
</span></span><span style=display:flex><span><span style=color:#f92672>globalDefault</span>: <span style=color:#66d9ef>false</span>
</span></span><span style=display:flex><span><span style=color:#f92672>description</span>: <span style=color:#e6db74>&#34;High priority for critical pods&#34;</span>
</span></span></code></pre></div></li><li><p><strong>优化 Pod 资源请求</strong></p><ul><li>避免 Pod 申请过多资源导致调度失败：</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>resources</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>requests</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>cpu</span>: <span style=color:#e6db74>&#34;250m&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>memory</span>: <span style=color:#e6db74>&#34;512Mi&#34;</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>limits</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>cpu</span>: <span style=color:#e6db74>&#34;500m&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>memory</span>: <span style=color:#e6db74>&#34;1Gi&#34;</span>
</span></span></code></pre></div></li></ol><hr><h3 id=-问题-2节点调度限制><strong>🛑 问题 2：节点调度限制</strong>
<a class=anchor href=#-%e9%97%ae%e9%a2%98-2%e8%8a%82%e7%82%b9%e8%b0%83%e5%ba%a6%e9%99%90%e5%88%b6>#</a></h3><ul><li><p>现象</p><p>：</p><ul><li><pre tabindex=0><code>kubectl describe pod
</code></pre><p>显示：</p><pre tabindex=0><code>0/50 nodes match node selector
</code></pre></li><li><p>Pod 绑定了 <code>nodeSelector</code> 或 <code>affinity</code>，导致没有可用节点</p></li></ul></li></ul><p>✅ <strong>解决方案</strong>：</p><ol><li><p><strong>检查 Pod 的 <code>nodeSelector</code></strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>nodeSelector</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>node-type</span>: <span style=color:#e6db74>&#34;high-performance&#34;</span>
</span></span></code></pre></div><ul><li><p>确保有至少一个符合条件的节点：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get nodes --show-labels
</span></span></code></pre></div></li></ul></li><li><p><strong>检查 <code>affinity</code> 规则</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>affinity</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>nodeAffinity</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>requiredDuringSchedulingIgnoredDuringExecution</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>nodeSelectorTerms</span>:
</span></span><span style=display:flex><span>      - <span style=color:#f92672>matchExpressions</span>:
</span></span><span style=display:flex><span>        - <span style=color:#f92672>key</span>: <span style=color:#ae81ff>disktype</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>operator</span>: <span style=color:#ae81ff>In</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>values</span>:
</span></span><span style=display:flex><span>          - <span style=color:#ae81ff>ssd</span>
</span></span></code></pre></div><ul><li>可适当降低 <code>requiredDuringSchedulingIgnoredDuringExecution</code> 的约束级别。</li></ul></li></ol><hr><h2 id=-3-容器创建问题containercreating-卡住><strong>🔍 3. 容器创建问题（ContainerCreating 卡住）</strong>
<a class=anchor href=#-3-%e5%ae%b9%e5%99%a8%e5%88%9b%e5%bb%ba%e9%97%ae%e9%a2%98containercreating-%e5%8d%a1%e4%bd%8f>#</a></h2><h3 id=-问题-3cni-网络组件异常><strong>🛑 问题 3：CNI 网络组件异常</strong>
<a class=anchor href=#-%e9%97%ae%e9%a2%98-3cni-%e7%bd%91%e7%bb%9c%e7%bb%84%e4%bb%b6%e5%bc%82%e5%b8%b8>#</a></h3><ul><li><p>现象</p><p>：</p><ul><li><pre tabindex=0><code>kubectl describe pod
</code></pre><p>显示：</p><pre tabindex=0><code>Failed create pod sandbox: rpc error: code = Unknown desc = failed to setup network
</code></pre></li><li><p><code>kubectl get pods -n kube-system</code> 发现 <code>coredns</code>、<code>calico-node</code> 等网络组件异常</p></li></ul></li></ul><p>✅ <strong>解决方案</strong>：</p><ol><li><p><strong>检查 CNI 插件状态</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get pods -n kube-system | grep calico
</span></span><span style=display:flex><span>kubectl logs -f &lt;cni-pod-name&gt; -n kube-system
</span></span></code></pre></div><ul><li><p>如果 CNI 组件崩溃，重启它：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl delete pod &lt;cni-pod-name&gt; -n kube-system
</span></span></code></pre></div></li></ul></li><li><p><strong>检查 IP 资源是否耗尽</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get ippools -A
</span></span></code></pre></div><ul><li><p>如果 IP 地址不足，需要扩展 IP 池：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>IPPool</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>default-pool</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>cidr</span>: <span style=color:#ae81ff>192.168.1.0</span><span style=color:#ae81ff>/24</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>blockSize</span>: <span style=color:#ae81ff>26</span>
</span></span></code></pre></div></li></ul></li></ol><hr><h3 id=-问题-4容器镜像拉取失败><strong>🛑 问题 4：容器镜像拉取失败</strong>
<a class=anchor href=#-%e9%97%ae%e9%a2%98-4%e5%ae%b9%e5%99%a8%e9%95%9c%e5%83%8f%e6%8b%89%e5%8f%96%e5%a4%b1%e8%b4%a5>#</a></h3><ul><li><p>现象</p><p>：</p><ul><li><pre tabindex=0><code>kubectl describe pod
</code></pre><p>显示：</p><pre tabindex=0><code>Failed to pull image: ImagePullBackOff
</code></pre></li><li><pre tabindex=0><code>kubectl get events
</code></pre><p>发现：</p><pre tabindex=0><code>Failed to pull image &#34;myrepo/myimage:v1.0&#34;: not found
</code></pre></li></ul></li></ul><p>✅ <strong>解决方案</strong>：</p><ol><li><p><strong>检查私有镜像仓库的 Secret</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get secret -n &lt;namespace&gt;
</span></span></code></pre></div><ul><li><p>如果没有</p><pre tabindex=0><code>imagePullSecret
</code></pre><p>，创建：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl create secret docker-registry my-registry-secret <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --docker-server<span style=color:#f92672>=</span>&lt;your-registry&gt; <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --docker-username<span style=color:#f92672>=</span>&lt;your-username&gt; <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --docker-password<span style=color:#f92672>=</span>&lt;your-password&gt; <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --namespace<span style=color:#f92672>=</span>&lt;namespace&gt;
</span></span></code></pre></div></li><li><p>修改 Pod 配置：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>imagePullSecrets</span>:
</span></span><span style=display:flex><span>- <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-registry-secret</span>
</span></span></code></pre></div></li></ul></li><li><p><strong>检查镜像仓库可用性</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker login &lt;your-registry&gt;
</span></span><span style=display:flex><span>docker pull myrepo/myimage:v1.0
</span></span></code></pre></div></li></ol><hr><h2 id=-4-存储问题pvc-绑定失败><strong>🔍 4. 存储问题（PVC 绑定失败）</strong>
<a class=anchor href=#-4-%e5%ad%98%e5%82%a8%e9%97%ae%e9%a2%98pvc-%e7%bb%91%e5%ae%9a%e5%a4%b1%e8%b4%a5>#</a></h2><h3 id=-问题-5存储卷挂载失败><strong>🛑 问题 5：存储卷挂载失败</strong>
<a class=anchor href=#-%e9%97%ae%e9%a2%98-5%e5%ad%98%e5%82%a8%e5%8d%b7%e6%8c%82%e8%bd%bd%e5%a4%b1%e8%b4%a5>#</a></h3><ul><li><p>现象</p><p>：</p><ul><li><pre tabindex=0><code>kubectl describe pod
</code></pre><p>显示：</p><pre tabindex=0><code>MountVolume.SetUp failed for volume &#34;pvc-xxxx&#34; : rpc error: code = DeadlineExceeded
</code></pre></li><li><p>Pod 处于 <code>ContainerCreating</code> 状态</p></li></ul></li></ul><p>✅ <strong>解决方案</strong>：</p><ol><li><p><strong>检查 PVC 状态</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get pvc -n &lt;namespace&gt;
</span></span></code></pre></div><ul><li><p>如果</p><pre tabindex=0><code>STATUS
</code></pre><p>为</p><pre tabindex=0><code>Pending
</code></pre><p>，则需要确认存储类：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get sc
</span></span></code></pre></div></li></ul></li><li><p><strong>如果 PVC 绑定失败，尝试强制重新绑定</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl delete pod &lt;pod-name&gt; -n &lt;namespace&gt;
</span></span></code></pre></div></li></ol><hr><h2 id=-5-master-组件负载过高><strong>🔍 5. Master 组件负载过高</strong>
<a class=anchor href=#-5-master-%e7%bb%84%e4%bb%b6%e8%b4%9f%e8%bd%bd%e8%bf%87%e9%ab%98>#</a></h2><h3 id=-问题-6api-server-负载过高><strong>🛑 问题 6：API Server 负载过高</strong>
<a class=anchor href=#-%e9%97%ae%e9%a2%98-6api-server-%e8%b4%9f%e8%bd%bd%e8%bf%87%e9%ab%98>#</a></h3><ul><li><p>现象</p><p>：</p><ul><li><code>kubectl get pods</code> 响应缓慢</li><li><code>kube-apiserver</code> CPU 飙高</li></ul></li></ul><p>✅ <strong>解决方案</strong>：</p><ol><li><p><strong>检查 API Server 压力</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get --raw <span style=color:#e6db74>&#34;/metrics&#34;</span> | grep apiserver_request
</span></span></code></pre></div></li><li><p><strong>启用 <code>kube-proxy</code> IPVS 模式，减少 API Server 负载</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>mode</span>: <span style=color:#e6db74>&#34;ipvs&#34;</span>
</span></span></code></pre></div></li><li><p><strong>增加 API Server 副本数</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl scale deploy kube-apiserver --replicas<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>
</span></span></code></pre></div></li></ol><hr><h2 id=-总结><strong>🎯 总结</strong>
<a class=anchor href=#-%e6%80%bb%e7%bb%93>#</a></h2><table><thead><tr><th>问题</th><th>解决方案</th></tr></thead><tbody><tr><td>资源不足</td><td>调整 <code>requests/limits</code>，扩容节点</td></tr><tr><td>调度失败</td><td>放宽 <code>nodeSelector</code>，使用 <code>priorityClass</code></td></tr><tr><td>CNI 故障</td><td>检查 <code>coredns</code> 和 <code>calico-node</code></td></tr><tr><td>镜像拉取失败</td><td>添加 <code>imagePullSecrets</code>，检查镜像仓库</td></tr><tr><td>PVC 绑定失败</td><td>检查 <code>StorageClass</code> 和 <code>PersistentVolume</code></td></tr><tr><td>API Server 负载高</td><td>启用 IPVS，增加副本数</td></tr></tbody></table><p>如果集群大规模启动 Pod 失败，按照以上方法逐步排查，优化调度和资源分配，即可提高系统稳定性！ 🚀</p><h1 id=部署redis或者kafka中间件吗怎么设计容器化方案高可用数据持久化或者延迟的情况>部署redis或者kafka中间件吗？怎么设计容器化方案？高可用？数据持久化或者延迟的情况？
<a class=anchor href=#%e9%83%a8%e7%bd%b2redis%e6%88%96%e8%80%85kafka%e4%b8%ad%e9%97%b4%e4%bb%b6%e5%90%97%e6%80%8e%e4%b9%88%e8%ae%be%e8%ae%a1%e5%ae%b9%e5%99%a8%e5%8c%96%e6%96%b9%e6%a1%88%e9%ab%98%e5%8f%af%e7%94%a8%e6%95%b0%e6%8d%ae%e6%8c%81%e4%b9%85%e5%8c%96%e6%88%96%e8%80%85%e5%bb%b6%e8%bf%9f%e7%9a%84%e6%83%85%e5%86%b5>#</a></h1><h2 id=kubernetes-中-redis-和-kafka-容器化方案><strong>Kubernetes 中 Redis 和 Kafka 容器化方案</strong>
<a class=anchor href=#kubernetes-%e4%b8%ad-redis-%e5%92%8c-kafka-%e5%ae%b9%e5%99%a8%e5%8c%96%e6%96%b9%e6%a1%88>#</a></h2><p>在 Kubernetes 中部署 Redis 或 Kafka 需要考虑<strong>高可用、数据持久化和延迟优化</strong>等关键点。以下是详细的设计方案。</p><hr><h1 id=1-redis-容器化部署方案><strong>1️⃣ Redis 容器化部署方案</strong>
<a class=anchor href=#1-redis-%e5%ae%b9%e5%99%a8%e5%8c%96%e9%83%a8%e7%bd%b2%e6%96%b9%e6%a1%88>#</a></h1><h3 id=-redis-方案设计><strong>📌 Redis 方案设计</strong>
<a class=anchor href=#-redis-%e6%96%b9%e6%a1%88%e8%ae%be%e8%ae%a1>#</a></h3><ul><li><p>架构</p><p>：</p><ul><li><strong>Redis Sentinel + Redis 主从</strong>（高可用）</li><li><strong>Redis Cluster</strong>（分片扩展）</li></ul></li><li><p>存储持久化</p><p>：</p><ul><li><strong>AOF</strong>（Append Only File，适用于数据一致性要求高的场景）</li><li><strong>RDB</strong>（周期性快照，适用于性能优先场景）</li></ul></li><li><p>部署方式</p><p>：</p><ul><li><code>StatefulSet</code> 管理 <code>Redis Master/Slave</code></li><li><code>Sentinel</code> 监控并自动故障转移</li><li>持久化 PVC 存储 Redis 数据</li></ul></li></ul><hr><h3 id=-方案-1redis-sentinel-高可用><strong>📌 方案 1：Redis Sentinel 高可用</strong>
<a class=anchor href=#-%e6%96%b9%e6%a1%88-1redis-sentinel-%e9%ab%98%e5%8f%af%e7%94%a8>#</a></h3><h4 id=-11-创建-redis-statefulset><strong>🛠 1.1 创建 Redis StatefulSet</strong>
<a class=anchor href=#-11-%e5%88%9b%e5%bb%ba-redis-statefulset>#</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>apps/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>StatefulSet</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>redis</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>serviceName</span>: <span style=color:#ae81ff>redis</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>replicas</span>: <span style=color:#ae81ff>3</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>selector</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>matchLabels</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>app</span>: <span style=color:#ae81ff>redis</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>template</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>labels</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>app</span>: <span style=color:#ae81ff>redis</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>containers</span>:
</span></span><span style=display:flex><span>      - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>redis</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>image</span>: <span style=color:#ae81ff>redis:6.2</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>args</span>: [<span style=color:#e6db74>&#34;redis-server&#34;</span>, <span style=color:#e6db74>&#34;/redis.conf&#34;</span>]
</span></span><span style=display:flex><span>        <span style=color:#f92672>volumeMounts</span>:
</span></span><span style=display:flex><span>        - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>data</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>mountPath</span>: <span style=color:#ae81ff>/data</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>ports</span>:
</span></span><span style=display:flex><span>        - <span style=color:#f92672>containerPort</span>: <span style=color:#ae81ff>6379</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>volumes</span>:
</span></span><span style=display:flex><span>      - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>redis-config</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>configMap</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>name</span>: <span style=color:#ae81ff>redis-config</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>volumeClaimTemplates</span>:
</span></span><span style=display:flex><span>  - <span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>name</span>: <span style=color:#ae81ff>data</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>accessModes</span>: [<span style=color:#e6db74>&#34;ReadWriteOnce&#34;</span>]
</span></span><span style=display:flex><span>      <span style=color:#f92672>resources</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>requests</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>storage</span>: <span style=color:#ae81ff>1Gi</span>
</span></span></code></pre></div><h4 id=-12-创建-sentinel-部署><strong>🛠 1.2 创建 Sentinel 部署</strong>
<a class=anchor href=#-12-%e5%88%9b%e5%bb%ba-sentinel-%e9%83%a8%e7%bd%b2>#</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>apps/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Deployment</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>redis-sentinel</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>replicas</span>: <span style=color:#ae81ff>3</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>selector</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>matchLabels</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>app</span>: <span style=color:#ae81ff>redis-sentinel</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>template</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>labels</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>app</span>: <span style=color:#ae81ff>redis-sentinel</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>containers</span>:
</span></span><span style=display:flex><span>      - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>sentinel</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>image</span>: <span style=color:#ae81ff>redis:6.2</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>args</span>: [<span style=color:#e6db74>&#34;redis-sentinel&#34;</span>, <span style=color:#e6db74>&#34;/sentinel.conf&#34;</span>]
</span></span><span style=display:flex><span>        <span style=color:#f92672>ports</span>:
</span></span><span style=display:flex><span>        - <span style=color:#f92672>containerPort</span>: <span style=color:#ae81ff>26379</span>
</span></span></code></pre></div><h4 id=-高可用原理><strong>📌 高可用原理</strong>
<a class=anchor href=#-%e9%ab%98%e5%8f%af%e7%94%a8%e5%8e%9f%e7%90%86>#</a></h4><ol><li>Sentinel 监听 Redis 主从状态，如果主节点挂掉，会自动选举新的主节点。</li><li><code>StatefulSet</code> 保证 Redis 节点的稳定性，Pod 重启后 IP 不变。</li><li>PVC 持久化 Redis 数据，防止 Pod 删除导致数据丢失。</li></ol><hr><h3 id=-方案-2redis-cluster分片><strong>📌 方案 2：Redis Cluster（分片）</strong>
<a class=anchor href=#-%e6%96%b9%e6%a1%88-2redis-cluster%e5%88%86%e7%89%87>#</a></h3><ul><li><p>适用于高吞吐场景，分片存储，提高可扩展性。</p></li><li><p>需要 <code>6</code> 个 Redis Pod（3 Master + 3 Slave）。</p></li><li><p>使用</p><pre tabindex=0><code>StatefulSet
</code></pre><p>部署 Redis，每个 Pod 运行</p><pre tabindex=0><code>redis-server
</code></pre><p>并开启集群模式：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>redis-cli --cluster create &lt;master1-ip&gt;:6379 &lt;master2-ip&gt;:6379 &lt;master3-ip&gt;:6379 --cluster-replicas <span style=color:#ae81ff>1</span>
</span></span></code></pre></div></li><li><p>缺点</p><p>：</p><ul><li>需要应用端支持 Redis Cluster 访问方式（<code>JedisCluster</code> 或 <code>lettuce</code>）。</li><li>需要 <code>configmap</code> 提前配置 <code>redis.conf</code> 以支持 <code>cluster-enabled yes</code>。</li></ul></li></ul><hr><h1 id=2-kafka-容器化部署方案><strong>2️⃣ Kafka 容器化部署方案</strong>
<a class=anchor href=#2-kafka-%e5%ae%b9%e5%99%a8%e5%8c%96%e9%83%a8%e7%bd%b2%e6%96%b9%e6%a1%88>#</a></h1><p>Kafka 需要考虑<strong>高可用、数据持久化和延迟优化</strong>，推荐使用 <strong>Kafka + Zookeeper</strong> 进行管理。</p><h3 id=-kafka-方案设计><strong>📌 Kafka 方案设计</strong>
<a class=anchor href=#-kafka-%e6%96%b9%e6%a1%88%e8%ae%be%e8%ae%a1>#</a></h3><ul><li><p><strong>存储</strong>：Kafka 依赖磁盘 IO，需要 <code>PersistentVolume</code></p></li><li><p><strong>高可用</strong>：多副本 <code>replication-factor > 1</code></p></li><li><p>优化</p><p>：</p><ul><li><code>log.segment.bytes</code> 调整 segment 大小，优化日志存储</li><li><code>message.max.bytes</code> 限制单条消息大小，防止 OOM</li><li><code>num.partitions</code> 适当增加分区，提高并发能力</li></ul></li></ul><hr><h3 id=-kafka-容器化部署><strong>📌 Kafka 容器化部署</strong>
<a class=anchor href=#-kafka-%e5%ae%b9%e5%99%a8%e5%8c%96%e9%83%a8%e7%bd%b2>#</a></h3><h4 id=-21-部署-zookeeper用于管理-kafka-broker><strong>🛠 2.1 部署 Zookeeper（用于管理 Kafka Broker）</strong>
<a class=anchor href=#-21-%e9%83%a8%e7%bd%b2-zookeeper%e7%94%a8%e4%ba%8e%e7%ae%a1%e7%90%86-kafka-broker>#</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>apps/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>StatefulSet</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>zookeeper</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>serviceName</span>: <span style=color:#ae81ff>zookeeper</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>replicas</span>: <span style=color:#ae81ff>3</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>selector</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>matchLabels</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>app</span>: <span style=color:#ae81ff>zookeeper</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>template</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>labels</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>app</span>: <span style=color:#ae81ff>zookeeper</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>containers</span>:
</span></span><span style=display:flex><span>      - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>zookeeper</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>image</span>: <span style=color:#ae81ff>bitnami/zookeeper:latest</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>env</span>:
</span></span><span style=display:flex><span>        - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>ALLOW_ANONYMOUS_LOGIN</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>value</span>: <span style=color:#e6db74>&#34;yes&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>ports</span>:
</span></span><span style=display:flex><span>        - <span style=color:#f92672>containerPort</span>: <span style=color:#ae81ff>2181</span>
</span></span></code></pre></div><hr><h4 id=-22-部署-kafka-statefulset><strong>🛠 2.2 部署 Kafka StatefulSet</strong>
<a class=anchor href=#-22-%e9%83%a8%e7%bd%b2-kafka-statefulset>#</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>apps/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>StatefulSet</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>kafka</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>serviceName</span>: <span style=color:#ae81ff>kafka</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>replicas</span>: <span style=color:#ae81ff>3</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>selector</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>matchLabels</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>app</span>: <span style=color:#ae81ff>kafka</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>template</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>labels</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>app</span>: <span style=color:#ae81ff>kafka</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>containers</span>:
</span></span><span style=display:flex><span>      - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>kafka</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>image</span>: <span style=color:#ae81ff>bitnami/kafka:latest</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>env</span>:
</span></span><span style=display:flex><span>        - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>KAFKA_CFG_ZOOKEEPER_CONNECT</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>value</span>: <span style=color:#e6db74>&#34;zookeeper:2181&#34;</span>
</span></span><span style=display:flex><span>        - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>KAFKA_CFG_ADVERTISED_LISTENERS</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>value</span>: <span style=color:#e6db74>&#34;PLAINTEXT://kafka:9092&#34;</span>
</span></span><span style=display:flex><span>        - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>KAFKA_CFG_DEFAULT_REPLICATION_FACTOR</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>value</span>: <span style=color:#e6db74>&#34;2&#34;</span>
</span></span><span style=display:flex><span>        - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>KAFKA_CFG_NUM_PARTITIONS</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>value</span>: <span style=color:#e6db74>&#34;3&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>ports</span>:
</span></span><span style=display:flex><span>        - <span style=color:#f92672>containerPort</span>: <span style=color:#ae81ff>9092</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>volumeMounts</span>:
</span></span><span style=display:flex><span>        - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>data</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>mountPath</span>: <span style=color:#ae81ff>/bitnami/kafka</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>volumeClaimTemplates</span>:
</span></span><span style=display:flex><span>  - <span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>name</span>: <span style=color:#ae81ff>data</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>accessModes</span>: [<span style=color:#e6db74>&#34;ReadWriteOnce&#34;</span>]
</span></span><span style=display:flex><span>      <span style=color:#f92672>resources</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>requests</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>storage</span>: <span style=color:#ae81ff>10Gi</span>
</span></span></code></pre></div><hr><h1 id=3-关键优化点><strong>3️⃣ 关键优化点</strong>
<a class=anchor href=#3-%e5%85%b3%e9%94%ae%e4%bc%98%e5%8c%96%e7%82%b9>#</a></h1><h2 id=-1-数据持久化><strong>📌 1. 数据持久化</strong>
<a class=anchor href=#-1-%e6%95%b0%e6%8d%ae%e6%8c%81%e4%b9%85%e5%8c%96>#</a></h2><ul><li><strong>Redis</strong>：使用 PVC 存储 <code>/data</code> 目录，防止重启丢失数据。</li><li><strong>Kafka</strong>：使用 PVC 持久化 <code>/bitnami/kafka</code> 目录，防止 Broker 失效导致数据丢失。</li></ul><hr><h2 id=-2-高可用><strong>📌 2. 高可用</strong>
<a class=anchor href=#-2-%e9%ab%98%e5%8f%af%e7%94%a8>#</a></h2><table><thead><tr><th>组件</th><th>方案</th></tr></thead><tbody><tr><td>Redis</td><td>Sentinel 或 Cluster</td></tr><tr><td>Kafka</td><td><code>replication-factor=2</code>，避免单点故障</td></tr><tr><td>Zookeeper</td><td>3 个副本，避免 Leader 选举失败</td></tr></tbody></table><hr><h2 id=-3-延迟优化><strong>📌 3. 延迟优化</strong>
<a class=anchor href=#-3-%e5%bb%b6%e8%bf%9f%e4%bc%98%e5%8c%96>#</a></h2><ul><li><strong>Redis</strong>：<ul><li>关闭 <code>fsync</code> 以提高写入吞吐（但可能导致数据丢失）</li><li><code>maxclients</code> 限制并发连接，防止 OOM</li><li><code>timeout</code> 设置超时回收空闲连接</li></ul></li><li><strong>Kafka</strong>：<ul><li><code>acks=1</code> 提高生产者吞吐量（<code>acks=all</code> 可保证一致性，但会增加延迟）</li><li><code>linger.ms=5</code> 批量处理消息，减少 IO 请求</li><li><code>replication-factor=2</code> 确保消息不丢失</li></ul></li></ul><hr><h1 id=-总结-1><strong>✅ 总结</strong>
<a class=anchor href=#-%e6%80%bb%e7%bb%93-1>#</a></h1><table><thead><tr><th>组件</th><th>高可用</th><th>持久化</th><th>优化策略</th></tr></thead><tbody><tr><td>Redis</td><td>Sentinel / Cluster</td><td>PVC / AOF</td><td>fsync 关闭，maxclients 限流</td></tr><tr><td>Kafka</td><td>多 Broker + Zookeeper</td><td>PVC</td><td>acks=1, linger.ms=5</td></tr><tr><td>Zookeeper</td><td>3 副本</td><td>PVC</td><td>Leader 选举优化</td></tr></tbody></table><p>通过 Kubernetes <code>StatefulSet</code> 进行 Redis 和 Kafka 部署，可以保证<strong>高可用、持久化和低延迟</strong>，同时能够动态扩展，提高系统稳定性！ 🚀</p><h1 id=drgonfly的原理>drgonfly的原理？
<a class=anchor href=#drgonfly%e7%9a%84%e5%8e%9f%e7%90%86>#</a></h1><h2 id=dragonfly-原理解析><strong>Dragonfly 原理解析</strong>
<a class=anchor href=#dragonfly-%e5%8e%9f%e7%90%86%e8%a7%a3%e6%9e%90>#</a></h2><h3 id=-dragonfly-是什么><strong>📌 Dragonfly 是什么？</strong>
<a class=anchor href=#-dragonfly-%e6%98%af%e4%bb%80%e4%b9%88>#</a></h3><p>Dragonfly 是阿里巴巴开源的<strong>P2P 分发系统</strong>，主要用于<strong>云原生环境</strong>，优化大规模<strong>镜像、文件分发</strong>，解决 Kubernetes 集群中<strong>容器镜像拉取慢</strong>的问题。</p><hr><h2 id=1-dragonfly-主要组成><strong>1️⃣ Dragonfly 主要组成</strong>
<a class=anchor href=#1-dragonfly-%e4%b8%bb%e8%a6%81%e7%bb%84%e6%88%90>#</a></h2><p>Dragonfly 主要由以下核心组件构成：</p><table><thead><tr><th>组件</th><th>作用</th></tr></thead><tbody><tr><td><strong>dfdaemon</strong></td><td>运行在每个 Node 上的 P2P 下载守护进程，拦截 <code>docker pull</code> 请求</td></tr><tr><td><strong>scheduler</strong></td><td>调度节点，负责 Peer 节点管理、任务分发、P2P 下载调度</td></tr><tr><td><strong>cdn</strong></td><td>缓存中心，加速种子文件分发</td></tr><tr><td><strong>dfget</strong></td><td>具体执行下载任务的工具</td></tr><tr><td><strong>dfstore</strong></td><td>分布式存储，用于缓存分片数据</td></tr></tbody></table><hr><h2 id=2-dragonfly-的工作原理><strong>2️⃣ Dragonfly 的工作原理</strong>
<a class=anchor href=#2-dragonfly-%e7%9a%84%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86>#</a></h2><p>Dragonfly 主要通过<strong>P2P 方式加速文件分发</strong>，整体流程如下：</p><h3 id=-1-镜像文件下载请求><strong>🛠 1. 镜像/文件下载请求</strong>
<a class=anchor href=#-1-%e9%95%9c%e5%83%8f%e6%96%87%e4%bb%b6%e4%b8%8b%e8%bd%bd%e8%af%b7%e6%b1%82>#</a></h3><ul><li>用户在 Kubernetes 集群中执行 <code>docker pull</code> 拉取镜像。</li><li><code>dfdaemon</code> 作为<strong>本地代理</strong>，拦截 <code>docker pull</code> 请求，改为使用 Dragonfly 下载。</li></ul><h3 id=-2-调度节点scheduler><strong>🛠 2. 调度节点（scheduler）</strong>
<a class=anchor href=#-2-%e8%b0%83%e5%ba%a6%e8%8a%82%e7%82%b9scheduler>#</a></h3><ul><li><p><code>dfdaemon</code> 将请求发送给 <code>scheduler</code>，查询是否有缓存。</p></li><li><pre tabindex=0><code>scheduler
</code></pre><p>负责：</p><ol><li><strong>查询缓存</strong>：检查 <code>cdn</code> 是否已有完整文件。</li><li><strong>调度 P2P 下载</strong>：如果 <code>cdn</code> 没有完整文件，则从 <code>source</code> 下载并<strong>切分成多个分片</strong>，并调度到多个节点。</li></ol></li></ul><h3 id=-3-p2p-下载><strong>🛠 3. P2P 下载</strong>
<a class=anchor href=#-3-p2p-%e4%b8%8b%e8%bd%bd>#</a></h3><ul><li><code>scheduler</code> 选择<strong>最优 peer</strong> 作为<strong>种子节点</strong>，分发数据块。</li><li>其他 <code>dfdaemon</code> 互相请求数据，实现<strong>P2P 传输</strong>，避免所有节点都去访问 <code>cdn</code> 或 <code>registry</code>，降低带宽压力。</li></ul><h3 id=-4-本地缓存><strong>🛠 4. 本地缓存</strong>
<a class=anchor href=#-4-%e6%9c%ac%e5%9c%b0%e7%bc%93%e5%ad%98>#</a></h3><ul><li>下载完成后，<code>dfdaemon</code> 会缓存分片数据，<strong>加速后续下载</strong>，避免重复拉取。</li><li><code>dfdaemon</code> 也可以充当<strong>新种子节点</strong>，供其他 Peer 下载，提高下载速度。</li></ul><hr><h2 id=3-dragonfly-的-p2p-下载机制><strong>3️⃣ Dragonfly 的 P2P 下载机制</strong>
<a class=anchor href=#3-dragonfly-%e7%9a%84-p2p-%e4%b8%8b%e8%bd%bd%e6%9c%ba%e5%88%b6>#</a></h2><h3 id=-p2p-分块下载><strong>🔥 P2P 分块下载</strong>
<a class=anchor href=#-p2p-%e5%88%86%e5%9d%97%e4%b8%8b%e8%bd%bd>#</a></h3><ol><li><strong>任务分片</strong>：文件或镜像被拆分成多个小块（chunk）。</li><li><strong>并行下载</strong>：多个 <code>dfdaemon</code> 互相下载分片，避免单点瓶颈。</li><li><strong>动态调度</strong>：<code>scheduler</code> 计算最优 peer，减少带宽消耗。</li></ol><p><strong>🔍 对比传统下载</strong> | 方式 | 传统下载 | Dragonfly P2P | |&mdash;&mdash;|&mdash;&mdash;| | 下载模式 | 单节点直连 | 多节点 P2P | | 带宽压力 | 高 | 低 | | 速度 | 慢（受 CDN/Registry 限制） | 快（多点并行） | | 可扩展性 | 受服务器带宽限制 | 水平扩展 |</p><hr><h2 id=4-dragonfly-高可用架构><strong>4️⃣ Dragonfly 高可用架构</strong>
<a class=anchor href=#4-dragonfly-%e9%ab%98%e5%8f%af%e7%94%a8%e6%9e%b6%e6%9e%84>#</a></h2><h3 id=-多级缓存架构><strong>🌍 多级缓存架构</strong>
<a class=anchor href=#-%e5%a4%9a%e7%ba%a7%e7%bc%93%e5%ad%98%e6%9e%b6%e6%9e%84>#</a></h3><ol><li><strong>本地缓存（dfdaemon）</strong>：加速相同文件的重复拉取。</li><li><strong>集群级缓存（P2P）</strong>：不同节点互相拉取，提高整体吞吐。</li><li><strong>全局 CDN 缓存</strong>：存储完整文件，避免回源 Registry。</li></ol><h3 id=-关键特性><strong>⚡ 关键特性</strong>
<a class=anchor href=#-%e5%85%b3%e9%94%ae%e7%89%b9%e6%80%a7>#</a></h3><ul><li><strong>P2P 下载</strong>，减少单点压力，优化大规模集群的拉取性能。</li><li><strong>自动降级</strong>，如果 P2P 失败，回退为 HTTP 直连下载。</li><li><strong>对 Kubernetes 友好</strong>，可与 <code>containerd</code>、<code>docker</code> 无缝集成。</li></ul><hr><h2 id=5-dragonfly-在-kubernetes-集群中的应用><strong>5️⃣ Dragonfly 在 Kubernetes 集群中的应用</strong>
<a class=anchor href=#5-dragonfly-%e5%9c%a8-kubernetes-%e9%9b%86%e7%be%a4%e4%b8%ad%e7%9a%84%e5%ba%94%e7%94%a8>#</a></h2><p>Dragonfly 主要用于<strong>优化 Kubernetes 集群镜像拉取</strong>，特别是在<strong>大规模 Pod 启动时</strong>。</p><h3 id=-在-kubernetes-使用-dragonfly><strong>🔥 在 Kubernetes 使用 Dragonfly</strong>
<a class=anchor href=#-%e5%9c%a8-kubernetes-%e4%bd%bf%e7%94%a8-dragonfly>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>apps/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>DaemonSet</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>dfdaemon</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>selector</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>matchLabels</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>app</span>: <span style=color:#ae81ff>dfdaemon</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>template</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>labels</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>app</span>: <span style=color:#ae81ff>dfdaemon</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>containers</span>:
</span></span><span style=display:flex><span>      - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>dfdaemon</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>image</span>: <span style=color:#ae81ff>dragonflyoss/dfdaemon:latest</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>ports</span>:
</span></span><span style=display:flex><span>        - <span style=color:#f92672>containerPort</span>: <span style=color:#ae81ff>65001</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>volumeMounts</span>:
</span></span><span style=display:flex><span>        - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>dfdaemon-cache</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>mountPath</span>: <span style=color:#ae81ff>/var/cache/dragonfly</span>
</span></span></code></pre></div><h4 id=-使用方式><strong>🚀 使用方式</strong>
<a class=anchor href=#-%e4%bd%bf%e7%94%a8%e6%96%b9%e5%bc%8f>#</a></h4><ol><li><code>dfdaemon</code> 运行在每个 Node 上，拦截 <code>docker pull</code>。</li><li>调度 <code>dfget</code> 进行 P2P 下载，提高并行度。</li><li>加速大规模 Kubernetes Pod 启动，避免 Registry 压力过大。</li></ol><hr><h2 id=6-dragonfly-对比其他加速方案><strong>6️⃣ Dragonfly 对比其他加速方案</strong>
<a class=anchor href=#6-dragonfly-%e5%af%b9%e6%af%94%e5%85%b6%e4%bb%96%e5%8a%a0%e9%80%9f%e6%96%b9%e6%a1%88>#</a></h2><table><thead><tr><th>方案</th><th>传统 Registry</th><th>Dragonfly</th><th>镜像加速 Proxy</th></tr></thead><tbody><tr><td>拉取方式</td><td>单点拉取</td><td>P2P 下载</td><td>CDN 缓存</td></tr><tr><td>适用于</td><td>小规模集群</td><td>大规模分发</td><td>普通加速</td></tr><tr><td>资源消耗</td><td>高</td><td>低</td><td>适中</td></tr><tr><td>可靠性</td><td>依赖 CDN</td><td>P2P 自适应</td><td>依赖源站</td></tr></tbody></table><hr><h2 id=-总结-2><strong>✅ 总结</strong>
<a class=anchor href=#-%e6%80%bb%e7%bb%93-2>#</a></h2><ul><li>Dragonfly 采用 <strong>P2P</strong> + <strong>CDN</strong>，优化<strong>大规模镜像/文件分发</strong>。</li><li>通过 <strong>dfdaemon、scheduler、cdn</strong> 组件，实现<strong>高效调度、缓存优化</strong>。</li><li><strong>适用于 Kubernetes 集群</strong>，提升大规模 Pod 启动的下载性能，减少 Registry 压力。</li></ul><p>🚀 <strong>在 K8s 大规模集群中，Dragonfly 是解决镜像拉取慢的利器！</strong></p><h1 id=监控和cicd了解哪些监控日活千万的app设计监控体系和关注哪些指标>监控和cicd了解哪些？监控日活千万的app，设计监控体系和关注哪些指标？
<a class=anchor href=#%e7%9b%91%e6%8e%a7%e5%92%8ccicd%e4%ba%86%e8%a7%a3%e5%93%aa%e4%ba%9b%e7%9b%91%e6%8e%a7%e6%97%a5%e6%b4%bb%e5%8d%83%e4%b8%87%e7%9a%84app%e8%ae%be%e8%ae%a1%e7%9b%91%e6%8e%a7%e4%bd%93%e7%b3%bb%e5%92%8c%e5%85%b3%e6%b3%a8%e5%93%aa%e4%ba%9b%e6%8c%87%e6%a0%87>#</a></h1><h2 id=监控日活千万级-app-的监控体系设计><strong>监控日活千万级 App 的监控体系设计</strong>
<a class=anchor href=#%e7%9b%91%e6%8e%a7%e6%97%a5%e6%b4%bb%e5%8d%83%e4%b8%87%e7%ba%a7-app-%e7%9a%84%e7%9b%91%e6%8e%a7%e4%bd%93%e7%b3%bb%e8%ae%be%e8%ae%a1>#</a></h2><p>日活千万级 App 需要构建一个 <strong>高可用、低延迟、可观测性强</strong> 的监控体系，以保障稳定运行、快速发现问题、优化用户体验。以下是完整的监控设计方案：</p><hr><h2 id=1-监控体系设计><strong>1. 监控体系设计</strong>
<a class=anchor href=#1-%e7%9b%91%e6%8e%a7%e4%bd%93%e7%b3%bb%e8%ae%be%e8%ae%a1>#</a></h2><h3 id=1监控架构><strong>（1）监控架构</strong>
<a class=anchor href=#1%e7%9b%91%e6%8e%a7%e6%9e%b6%e6%9e%84>#</a></h3><p>监控体系一般由 <strong>数据采集、存储、分析、告警</strong> 四个核心部分组成：</p><ol><li><strong>数据采集（Agent/SDK/Exporter）</strong><ul><li>业务指标：埋点日志、APM（SkyWalking、Zipkin）</li><li>系统监控：Prometheus Node Exporter、cAdvisor</li><li>日志分析：ELK、Fluentd</li><li>网络监控：Ping、Traceroute、NetFlow</li></ul></li><li><strong>数据存储</strong><ul><li><strong>时序数据库</strong>：Prometheus、Thanos、VictoriaMetrics</li><li><strong>日志存储</strong>：Elasticsearch、Loki</li><li><strong>APM 分析</strong>：SkyWalking、Jaeger</li></ul></li><li><strong>数据分析</strong><ul><li><strong>指标查询</strong>：Grafana</li><li><strong>异常检测</strong>：Anomaly Detection（AI 结合 PromQL）</li></ul></li><li><strong>告警机制</strong><ul><li><strong>实时告警</strong>：Prometheus Alertmanager、企业微信、钉钉、短信</li><li><strong>自动扩缩容</strong>：HPA（Horizontal Pod Autoscaler）</li><li><strong>智能故障诊断</strong>：AI 预测 + 机器学习</li></ul></li></ol><hr><h2 id=2-关键监控指标><strong>2. 关键监控指标</strong>
<a class=anchor href=#2-%e5%85%b3%e9%94%ae%e7%9b%91%e6%8e%a7%e6%8c%87%e6%a0%87>#</a></h2><h3 id=1业务层监控><strong>（1）业务层监控</strong>
<a class=anchor href=#1%e4%b8%9a%e5%8a%a1%e5%b1%82%e7%9b%91%e6%8e%a7>#</a></h3><p>💡 业务指标能直观反映 App 运行情况：</p><table><thead><tr><th>监控项</th><th>说明</th><th>影响</th></tr></thead><tbody><tr><td><strong>DAU（日活）</strong></td><td>每日活跃用户数</td><td>低于预期需排查用户流失原因</td></tr><tr><td><strong>MAU（月活）</strong></td><td>每月活跃用户数</td><td>反映长期用户趋势</td></tr><tr><td><strong>UV/PV</strong></td><td>访问人数/页面浏览量</td><td>监控用户访问情况</td></tr><tr><td><strong>留存率</strong></td><td>N 日留存率</td><td>用户粘性 & 营销效果</td></tr><tr><td><strong>用户请求量</strong></td><td>每秒请求数（QPS）</td><td>突增可能 DDoS 攻击或活动流量激增</td></tr><tr><td><strong>平均响应时间（P99/P95）</strong></td><td>用户请求处理时间</td><td>过高说明系统或数据库有瓶颈</td></tr><tr><td><strong>支付成功率</strong></td><td>订单支付成功率</td><td>低于预期需排查支付渠道问题</td></tr><tr><td><strong>Crash 率</strong></td><td>App 崩溃率</td><td>影响用户体验</td></tr></tbody></table><hr><h3 id=2系统层监控><strong>（2）系统层监控</strong>
<a class=anchor href=#2%e7%b3%bb%e7%bb%9f%e5%b1%82%e7%9b%91%e6%8e%a7>#</a></h3><p>💡 保障 App 服务器和基础设施稳定：</p><table><thead><tr><th>监控项</th><th>说明</th><th>影响</th></tr></thead><tbody><tr><td><strong>CPU 使用率</strong></td><td>服务器 CPU 负载</td><td>持续高负载可能导致请求阻塞</td></tr><tr><td><strong>内存占用（RSS、OOM）</strong></td><td>进程内存消耗情况</td><td>OOM 可能导致服务异常</td></tr><tr><td><strong>磁盘 IO & 磁盘空间</strong></td><td>读写速率 & 剩余空间</td><td>高 IO 可能影响数据库性能</td></tr><tr><td><strong>网络流量</strong></td><td>出入带宽、丢包率、RTT</td><td>突然升高可能是攻击或用户激增</td></tr><tr><td><strong>Pod 副本数</strong></td><td>Kubernetes Pod 运行状态</td><td>Pod 异常减少说明 K8s 可能有问题</td></tr><tr><td><strong>依赖服务健康</strong></td><td>Redis、Kafka、MySQL、外部 API</td><td>依赖异常可能导致服务不可用</td></tr></tbody></table><hr><h3 id=3应用层监控apm><strong>（3）应用层监控（APM）</strong>
<a class=anchor href=#3%e5%ba%94%e7%94%a8%e5%b1%82%e7%9b%91%e6%8e%a7apm>#</a></h3><p>💡 结合分布式链路追踪（SkyWalking/Jaeger）：</p><table><thead><tr><th>监控项</th><th>说明</th><th>影响</th></tr></thead><tbody><tr><td><strong>接口响应时间</strong></td><td>API 响应时间（P99/P95）</td><td>影响用户体验</td></tr><tr><td><strong>数据库查询耗时</strong></td><td>SQL 查询慢查询分析</td><td>数据库索引优化</td></tr><tr><td><strong>HTTP 状态码（5xx、4xx）</strong></td><td>HTTP 错误比例</td><td>5xx 说明服务器异常</td></tr><tr><td><strong>线程池使用情况</strong></td><td>线程数量、阻塞率</td><td>影响吞吐量</td></tr><tr><td><strong>消息队列积压</strong></td><td>Kafka/RabbitMQ 队列长度</td><td>影响数据处理速度</td></tr></tbody></table><hr><h3 id=4日志--异常监控><strong>（4）日志 & 异常监控</strong>
<a class=anchor href=#4%e6%97%a5%e5%bf%97--%e5%bc%82%e5%b8%b8%e7%9b%91%e6%8e%a7>#</a></h3><p>💡 通过 ELK 或 Loki 监控日志异常：</p><table><thead><tr><th>监控项</th><th>说明</th><th>影响</th></tr></thead><tbody><tr><td><strong>错误日志数量</strong></td><td>每秒产生的 ERROR 级别日志</td><td>突增可能是 bug</td></tr><tr><td><strong>OOM Kill 记录</strong></td><td>进程被杀记录</td><td>容器内存不足</td></tr><tr><td><strong>API 超时日志</strong></td><td>API 响应超时情况</td><td>后端压力过大</td></tr><tr><td><strong>用户行为日志</strong></td><td>用户行为分析</td><td>用户流失诊断</td></tr><tr><td><strong>攻击行为日志</strong></td><td>404/403/SQL 注入等攻击行为</td><td>可能是安全漏洞</td></tr></tbody></table><hr><h2 id=3-监控工具选型><strong>3. 监控工具选型</strong>
<a class=anchor href=#3-%e7%9b%91%e6%8e%a7%e5%b7%a5%e5%85%b7%e9%80%89%e5%9e%8b>#</a></h2><table><thead><tr><th>监控类型</th><th>解决方案</th></tr></thead><tbody><tr><td><strong>系统监控</strong></td><td>Prometheus + Node Exporter</td></tr><tr><td><strong>日志分析</strong></td><td>ELK / Loki / Fluentd</td></tr><tr><td><strong>APM（链路追踪）</strong></td><td>SkyWalking / Jaeger</td></tr><tr><td><strong>数据库监控</strong></td><td>MySQL Exporter、Redis Exporter</td></tr><tr><td><strong>告警通知</strong></td><td>Prometheus Alertmanager + 钉钉/微信</td></tr></tbody></table><hr><h2 id=4-告警策略><strong>4. 告警策略</strong>
<a class=anchor href=#4-%e5%91%8a%e8%ad%a6%e7%ad%96%e7%95%a5>#</a></h2><h3 id=1告警分级><strong>（1）告警分级</strong>
<a class=anchor href=#1%e5%91%8a%e8%ad%a6%e5%88%86%e7%ba%a7>#</a></h3><table><thead><tr><th>级别</th><th>处理方式</th></tr></thead><tbody><tr><td><strong>严重（S1）</strong></td><td>影响全站，立即电话通知</td></tr><tr><td><strong>高（S2）</strong></td><td>影响部分用户，5 分钟内处理</td></tr><tr><td><strong>中（S3）</strong></td><td>影响少量用户，1 小时内处理</td></tr><tr><td><strong>低（S4）</strong></td><td>影响较小，业务团队跟进</td></tr></tbody></table><h3 id=2自动化运维><strong>（2）自动化运维</strong>
<a class=anchor href=#2%e8%87%aa%e5%8a%a8%e5%8c%96%e8%bf%90%e7%bb%b4>#</a></h3><ul><li><strong>自愈机制</strong>：Pod OOM 自动重启（K8s HPA）</li><li><strong>扩缩容</strong>：CPU / QPS 超阈值时自动扩容</li><li><strong>日志分析</strong>：自动识别异常日志，预测故障</li></ul><hr><h2 id=5-典型故障场景--处理><strong>5. 典型故障场景 & 处理</strong>
<a class=anchor href=#5-%e5%85%b8%e5%9e%8b%e6%95%85%e9%9a%9c%e5%9c%ba%e6%99%af--%e5%a4%84%e7%90%86>#</a></h2><table><thead><tr><th>故障类型</th><th>可能原因</th><th>解决方案</th></tr></thead><tbody><tr><td><strong>高并发 QPS 突增</strong></td><td>活动流量激增</td><td>增加 Pod 副本、开启缓存</td></tr><tr><td><strong>数据库慢查询</strong></td><td>SQL 语句未优化</td><td>增加索引、分库分表</td></tr><tr><td><strong>Redis 连接数过高</strong></td><td>连接未复用</td><td>使用连接池</td></tr><tr><td><strong>服务器 CPU 100%</strong></td><td>GC 频繁</td><td>调整 JVM 参数</td></tr><tr><td><strong>流量异常下降</strong></td><td>CDN/负载均衡异常</td><td>检查 Nginx/Ingress</td></tr><tr><td><strong>请求响应时间变长</strong></td><td>API 限流</td><td>增加实例数</td></tr></tbody></table><hr><h2 id=总结-2><strong>总结</strong>
<a class=anchor href=#%e6%80%bb%e7%bb%93-2>#</a></h2><ol><li><strong>建立完善的监控体系</strong>（采集、存储、分析、告警）</li><li><strong>覆盖业务、系统、APM、日志等多维度</strong></li><li><strong>使用 Prometheus、ELK、SkyWalking 进行全方位监控</strong></li><li><strong>通过 AI 预测和自动化运维减少人工干预</strong></li><li><strong>确保关键指标有合理的告警策略</strong></li></ol><p>这样可以 <strong>快速发现问题，保障 App 的高可用性</strong> 🚀</p><h1 id=prometheus-oom怎么解决的>prometheus oom怎么解决的？
<a class=anchor href=#prometheus-oom%e6%80%8e%e4%b9%88%e8%a7%a3%e5%86%b3%e7%9a%84>#</a></h1><p>在使用 Prometheus 进行监控时，遇到 OOM（Out Of Memory，内存溢出）问题可能是由于多种原因导致的，例如数据量过大、配置不当等。以下是解决 Prometheus OOM问题的一些常见方法：</p><h3 id=1-增加-prometheus-的内存限制>1. <strong>增加 Prometheus 的内存限制</strong>
<a class=anchor href=#1-%e5%a2%9e%e5%8a%a0-prometheus-%e7%9a%84%e5%86%85%e5%ad%98%e9%99%90%e5%88%b6>#</a></h3><ul><li><strong>方法</strong>：通过调整 Prometheus 启动参数来增加其可用的内存。</li><li><strong>参数</strong>：<code>--storage.tsdb.max-block-duration</code>、<code>--storage.tsdb.retention.time</code> 等。</li><li><strong>具体操作</strong>：可以通过在 Prometheus 的启动命令中增加如 <code>--storage.tsdb.retention.time=15d</code> 来限制数据的保留时间，减少内存压力。</li></ul><h3 id=2-调整>2. <strong>调整 <code>max-memory</code> 限制</strong>
<a class=anchor href=#2-%e8%b0%83%e6%95%b4>#</a></h3><ul><li><p>Prometheus 默认会使用更多的内存来存储时间序列数据。如果你遇到 OOM，可以增加 Prometheus 容器的内存限制。</p></li><li><p>方法</p><p>：在 Kubernetes 中可以通过配置资源请求和限制来增加内存限制。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>resources</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>requests</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>memory</span>: <span style=color:#e6db74>&#34;4Gi&#34;</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>limits</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>memory</span>: <span style=color:#e6db74>&#34;8Gi&#34;</span>
</span></span></code></pre></div></li></ul><h3 id=3-减少-prometheus-存储数据的量>3. <strong>减少 Prometheus 存储数据的量</strong>
<a class=anchor href=#3-%e5%87%8f%e5%b0%91-prometheus-%e5%ad%98%e5%82%a8%e6%95%b0%e6%8d%ae%e7%9a%84%e9%87%8f>#</a></h3><ul><li><p><strong>时间序列保留策略</strong>：调整 <code>--storage.tsdb.retention.time</code>，减少保留的数据量，可以减少内存占用。</p></li><li><p>示例</p><p>：可以将数据的保留时间设置为 7 天，而不是默认的 15 天或更长：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>--storage.tsdb.retention.time<span style=color:#f92672>=</span>7d
</span></span></code></pre></div></li></ul><h3 id=4-使用较小的>4. <strong>使用较小的 <code>scrape_interval</code></strong>
<a class=anchor href=#4-%e4%bd%bf%e7%94%a8%e8%be%83%e5%b0%8f%e7%9a%84>#</a></h3><ul><li><p><strong>问题</strong>：Prometheus 在每个 <code>scrape_interval</code> 时间间隔内拉取目标数据。如果 <code>scrape_interval</code> 太短，Prometheus 会频繁抓取数据，导致内存占用过高。</p></li><li><p>解决</p><p>：适当增加</p><pre tabindex=0><code>scrape_interval
</code></pre><p>，例如将默认的</p><pre tabindex=0><code>15s
</code></pre><p>改为</p><pre tabindex=0><code>30s
</code></pre><p>或更长，减少每次抓取的数据量。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>scrape_interval</span>: <span style=color:#ae81ff>30s</span>
</span></span></code></pre></div></li></ul><h3 id=5-优化数据源和查询>5. <strong>优化数据源和查询</strong>
<a class=anchor href=#5-%e4%bc%98%e5%8c%96%e6%95%b0%e6%8d%ae%e6%ba%90%e5%92%8c%e6%9f%a5%e8%af%a2>#</a></h3><ul><li><strong>问题</strong>：如果你的数据源包含大量的数据点，Prometheus 在进行聚合查询时可能会导致高内存占用。</li><li><strong>解决</strong>：优化查询，避免在短时间内进行大量数据的聚合，或者通过分段查询的方式减少内存使用。</li></ul><h3 id=6-启用>6. <strong>启用 <code>remote_write</code> 和外部存储</strong>
<a class=anchor href=#6-%e5%90%af%e7%94%a8>#</a></h3><ul><li><p>如果 Prometheus 内存使用较高，可以将数据通过 <code>remote_write</code> 发送到外部存储，如 Thanos 或 Cortex。这样可以将大量历史数据从 Prometheus 中移除，减少内存使用。</p></li><li><p>配置</p><p>：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>remote_write</span>:
</span></span><span style=display:flex><span>  - <span style=color:#f92672>url</span>: <span style=color:#e6db74>&#34;http://thanos-receiver:9201/api/v1/receive&#34;</span>
</span></span></code></pre></div></li></ul><h3 id=7-提高-prometheus-性能>7. <strong>提高 Prometheus 性能</strong>
<a class=anchor href=#7-%e6%8f%90%e9%ab%98-prometheus-%e6%80%a7%e8%83%bd>#</a></h3><ul><li><p><strong>方法</strong>：通过调整 TSDB（时间序列数据库）相关的配置来提升 Prometheus 性能，减少内存占用。</p></li><li><p>配置</p><p>：例如调整</p><pre tabindex=0><code>--storage.tsdb.min-block-duration
</code></pre><p>来减少数据块的大小。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>--storage.tsdb.min-block-duration<span style=color:#f92672>=</span>2h
</span></span></code></pre></div></li></ul><h3 id=8-扩展-prometheus-集群>8. <strong>扩展 Prometheus 集群</strong>
<a class=anchor href=#8-%e6%89%a9%e5%b1%95-prometheus-%e9%9b%86%e7%be%a4>#</a></h3><ul><li><strong>方法</strong>：在高负载环境下，可以考虑将 Prometheus 扩展为集群模式，使用如 Thanos 或 Cortex 等解决方案进行水平扩展。这能有效地分散负载，避免单个 Prometheus 实例内存溢出。</li></ul><h3 id=总结-3>总结：
<a class=anchor href=#%e6%80%bb%e7%bb%93-3>#</a></h3><p>解决 Prometheus OOM 问题通常需要从内存配置、数据保留策略、查询优化、外部存储等方面入手。根据实际的监控需求，合理调整这些配置，可以有效避免内存溢出，并提升 Prometheus 的性能和稳定性。</p><p>Prometheus 联邦（Federation）是 Prometheus 监控系统中的一个重要特性，旨在帮助解决大规模监控集群中的数据聚合问题，尤其是当多个 Prometheus 实例（或集群）需要汇总到一个中央 Prometheus 实例时。通过联邦，您可以跨多个 Prometheus 实例收集和聚合数据，从而在分布式系统中实现集中监控。</p><h3 id=prometheus-联邦的核心概念><strong>Prometheus 联邦的核心概念</strong>
<a class=anchor href=#prometheus-%e8%81%94%e9%82%a6%e7%9a%84%e6%a0%b8%e5%bf%83%e6%a6%82%e5%bf%b5>#</a></h3><p>Prometheus 联邦是一种将一个 Prometheus 实例的数据暴露给另一个 Prometheus 实例的机制。中央 Prometheus 实例可以定期从其他 Prometheus 实例抓取数据（scrape），从而实现跨 Prometheus 实例的数据汇聚和查询。</p><h3 id=联邦的工作原理><strong>联邦的工作原理</strong>
<a class=anchor href=#%e8%81%94%e9%82%a6%e7%9a%84%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86>#</a></h3><ol><li><strong>数据采集（Scraping）</strong>：<ul><li>每个 Prometheus 实例都会收集自己所在区域的监控数据，并将其存储在本地的 TSDB（时间序列数据库）中。</li><li>中央 Prometheus 实例（即主 Prometheus 实例）配置为定期从各个子 Prometheus 实例抓取数据。这些数据通常是通过 HTTP 访问暴露出来的时间序列数据。</li></ul></li><li><strong>数据聚合</strong>：<ul><li>联邦允许中央 Prometheus 实例通过向其他 Prometheus 实例发送抓取请求来汇总数据。每个 Prometheus 实例可以作为一个“子集”，将部分数据暴露给主 Prometheus 实例。</li><li>被抓取的数据可以包括时间序列数据、指标数据、Alertmanager 配置等。</li></ul></li><li><strong>存储和查询</strong>：<ul><li>中央 Prometheus 实例可以存储来自不同 Prometheus 实例的数据，并可以对这些数据进行统一的查询和分析。</li><li>使用 <code>Prometheus</code> 查询语言（PromQL）时，中央 Prometheus 实例可以对所有联邦的 Prometheus 实例的数据进行聚合查询。</li></ul></li></ol><h3 id=联邦的典型用途><strong>联邦的典型用途</strong>
<a class=anchor href=#%e8%81%94%e9%82%a6%e7%9a%84%e5%85%b8%e5%9e%8b%e7%94%a8%e9%80%94>#</a></h3><ol><li><strong>跨数据中心的数据汇聚</strong>：<ul><li>在分布式系统中，可能有多个数据中心或者多个独立的 Prometheus 实例运行在不同的地理位置。通过联邦，可以将所有这些实例的数据汇聚到一个中央实例，以便统一监控和报警。</li></ul></li><li><strong>跨集群监控</strong>：<ul><li>在 Kubernetes 等容器化环境中，可能有多个集群，且每个集群都有自己的 Prometheus 实例。通过 Prometheus 联邦，您可以从各个集群的 Prometheus 实例抓取数据，并集中存储和查询。</li></ul></li><li><strong>分层架构</strong>：<ul><li>使用联邦可以实现分层监控架构。例如，在大型企业环境中，可以通过联邦将子集群的数据暴露给总部的中央 Prometheus 实例，方便进行汇总、报警和全局查询。</li></ul></li><li><strong>局部数据存储与查询</strong>：<ul><li>在某些情况下，可能希望在本地 Prometheus 实例中存储和查询特定的、与该区域相关的时间序列数据，而将某些跨区域的数据聚合到中央 Prometheus 实例。</li></ul></li></ol><h3 id=prometheus-联邦的配置><strong>Prometheus 联邦的配置</strong>
<a class=anchor href=#prometheus-%e8%81%94%e9%82%a6%e7%9a%84%e9%85%8d%e7%bd%ae>#</a></h3><p>Prometheus 联邦的配置实际上是将一个 Prometheus 实例配置为从另一个 Prometheus 实例抓取数据。这可以通过 <code>scrape_config</code> 来配置，以下是一个基本的例子：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>scrape_configs</span>:
</span></span><span style=display:flex><span>  - <span style=color:#f92672>job_name</span>: <span style=color:#e6db74>&#39;federation&#39;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>scrape_interval</span>: <span style=color:#ae81ff>1m</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>metrics_path</span>: <span style=color:#e6db74>&#39;/federate&#39;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>params</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#39;match[]&#39;</span>:
</span></span><span style=display:flex><span>        - <span style=color:#e6db74>&#39;{job=&#34;kubernetes-cadvisor&#34;}&#39;</span>  <span style=color:#75715e># 只抓取特定的指标</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>static_configs</span>:
</span></span><span style=display:flex><span>      - <span style=color:#f92672>targets</span>:
</span></span><span style=display:flex><span>        - <span style=color:#e6db74>&#39;prometheus-server-1:9090&#39;</span>
</span></span><span style=display:flex><span>        - <span style=color:#e6db74>&#39;prometheus-server-2:9090&#39;</span>
</span></span></code></pre></div><h3 id=关键参数解释><strong>关键参数解释</strong>
<a class=anchor href=#%e5%85%b3%e9%94%ae%e5%8f%82%e6%95%b0%e8%a7%a3%e9%87%8a>#</a></h3><ul><li><strong><code>metrics_path: /federate</code></strong>：这是 Prometheus 联邦请求的路径，Prometheus 会从其他实例的 <code>/federate</code> 路径拉取数据。</li><li><strong><code>params</code></strong>：使用 <code>match[]</code> 参数，可以指定要抓取的指标的过滤条件。这样可以只抓取特定类型的数据，避免抓取不必要的数据。</li><li><strong><code>targets</code></strong>：指定从哪些 Prometheus 实例抓取数据，可以列出多个 Prometheus 实例。</li></ul><h3 id=优势><strong>优势</strong>
<a class=anchor href=#%e4%bc%98%e5%8a%bf>#</a></h3><ol><li><strong>集中管理</strong>：通过联邦，可以实现跨多个 Prometheus 实例的数据汇总和集中查询，方便统一管理。</li><li><strong>高可扩展性</strong>：随着监控的扩展，新的 Prometheus 实例可以很容易地加入联邦，数据会自动被集成。</li><li><strong>灵活性</strong>：可以灵活地选择哪些数据需要集中存储，哪些数据在本地处理。</li><li><strong>分布式架构支持</strong>：支持跨数据中心、跨集群的分布式监控。</li></ol><h3 id=限制和挑战><strong>限制和挑战</strong>
<a class=anchor href=#%e9%99%90%e5%88%b6%e5%92%8c%e6%8c%91%e6%88%98>#</a></h3><ol><li><strong>存储限制</strong>：联邦数据会增加中央 Prometheus 实例的存储需求。需要配置合适的存储和处理策略。</li><li><strong>查询复杂性</strong>：联邦的 Prometheus 实例间的查询可能会引入一定的延迟，因为需要从多个实例抓取数据并进行汇总。</li><li><strong>维护和管理</strong>：随着 Prometheus 实例的增加，联邦管理可能会变得更加复杂，特别是在大型环境中。</li></ol><h3 id=总结-4><strong>总结</strong>
<a class=anchor href=#%e6%80%bb%e7%bb%93-4>#</a></h3><p>Prometheus 联邦提供了一个有效的解决方案来扩展 Prometheus 监控系统，特别是在跨多个集群、数据中心或区域的环境中。通过合理配置联邦，可以实现大规模的监控数据汇聚和统一查询，但同时也需要考虑性能和存储等方面的挑战。</p><h1 id=用ansible部署二进制集群怎么保证play-book的幂等性和安全性>用ansible部署二进制集群？怎么保证play-book的幂等性和安全性？
<a class=anchor href=#%e7%94%a8ansible%e9%83%a8%e7%bd%b2%e4%ba%8c%e8%bf%9b%e5%88%b6%e9%9b%86%e7%be%a4%e6%80%8e%e4%b9%88%e4%bf%9d%e8%af%81play-book%e7%9a%84%e5%b9%82%e7%ad%89%e6%80%a7%e5%92%8c%e5%ae%89%e5%85%a8%e6%80%a7>#</a></h1><p>使用 <strong>Ansible</strong> 部署二进制集群时，可以通过编写 playbook 来管理集群的安装、配置、更新等操作。为了确保 <strong>幂等性</strong> 和 <strong>安全性</strong>，我们需要在 playbook 中遵循一些最佳实践，确保每次运行时结果一致，同时避免潜在的安全漏洞。</p><h3 id=部署二进制集群的步骤><strong>部署二进制集群的步骤：</strong>
<a class=anchor href=#%e9%83%a8%e7%bd%b2%e4%ba%8c%e8%bf%9b%e5%88%b6%e9%9b%86%e7%be%a4%e7%9a%84%e6%ad%a5%e9%aa%a4>#</a></h3><p>假设我们要使用 Ansible 部署一个基于二进制安装的 <strong>Kubernetes 集群</strong>。步骤如下：</p><h4 id=1-准备工作>1. <strong>准备工作</strong>
<a class=anchor href=#1-%e5%87%86%e5%a4%87%e5%b7%a5%e4%bd%9c>#</a></h4><ul><li>先确保每台节点的操作系统符合要求，且能通过 SSH 访问。</li><li>将需要部署的二进制文件（如 <code>kubeadm</code>、<code>kubelet</code>、<code>kubectl</code>）上传到目标节点。</li></ul><h4 id=2-ansible-playbook-示例>2. <strong>Ansible Playbook 示例</strong>
<a class=anchor href=#2-ansible-playbook-%e7%a4%ba%e4%be%8b>#</a></h4><p>以下是一个简化的 playbook 示例，演示如何通过 Ansible 在各个节点上部署 Kubernetes 二进制集群。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>- <span style=color:#f92672>name</span>: <span style=color:#ae81ff>Deploy Kubernetes Cluster</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>hosts</span>: <span style=color:#ae81ff>k8s_nodes</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>become</span>: <span style=color:#66d9ef>true</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>tasks</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>Ensure the necessary directories exist</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>file</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>path</span>: <span style=color:#e6db74>&#34;{{ item }}&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>state</span>: <span style=color:#ae81ff>directory</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>mode</span>: <span style=color:#e6db74>&#34;0755&#34;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>loop</span>:
</span></span><span style=display:flex><span>        - <span style=color:#ae81ff>/etc/kubernetes</span>
</span></span><span style=display:flex><span>        - <span style=color:#ae81ff>/etc/kubernetes/manifests</span>
</span></span><span style=display:flex><span>        - <span style=color:#ae81ff>/var/lib/kubelet</span>
</span></span><span style=display:flex><span>        - <span style=color:#ae81ff>/var/lib/kubernetes</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>Copy kubeadm, kubelet, and kubectl binaries</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>copy</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>src</span>: <span style=color:#e6db74>&#34;/path/to/binaries/{{ item }}&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>dest</span>: <span style=color:#e6db74>&#34;/usr/local/bin/{{ item }}&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>mode</span>: <span style=color:#e6db74>&#34;0755&#34;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>loop</span>:
</span></span><span style=display:flex><span>        - <span style=color:#ae81ff>kubeadm</span>
</span></span><span style=display:flex><span>        - <span style=color:#ae81ff>kubelet</span>
</span></span><span style=display:flex><span>        - <span style=color:#ae81ff>kubectl</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>notify</span>:
</span></span><span style=display:flex><span>        - <span style=color:#ae81ff>restart kubelet</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>Initialize Kubernetes master using kubeadm</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>command</span>: <span style=color:#ae81ff>kubeadm init --pod-network-cidr=10.244.0.0/16</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>when</span>: <span style=color:#ae81ff>inventory_hostname == groups[&#39;k8s_master&#39;][0]</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>register</span>: <span style=color:#ae81ff>kubeadm_init_output</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>Set up kubeconfig for the master node</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>copy</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>src</span>: <span style=color:#e6db74>&#34;/etc/kubernetes/admin.conf&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>dest</span>: <span style=color:#e6db74>&#34;~/.kube/config&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>remote_src</span>: <span style=color:#66d9ef>yes</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>when</span>: <span style=color:#ae81ff>inventory_hostname == groups[&#39;k8s_master&#39;][0]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>Apply CNI network plugin (flannel)</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>command</span>: <span style=color:#ae81ff>kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>when</span>: <span style=color:#ae81ff>inventory_hostname == groups[&#39;k8s_master&#39;][0]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>Join the worker nodes to the cluster</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>command</span>: <span style=color:#e6db74>&#34;{{ kubeadm_init_output.stdout_lines[0] | regex_replace(&#39;.*(kubeadm join.*)&#39;, &#39;\\1&#39;) }}&#34;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>when</span>: <span style=color:#ae81ff>inventory_hostname != groups[&#39;k8s_master&#39;][0]</span>
</span></span></code></pre></div><h3 id=保证-playbook-的幂等性><strong>保证 Playbook 的幂等性：</strong>
<a class=anchor href=#%e4%bf%9d%e8%af%81-playbook-%e7%9a%84%e5%b9%82%e7%ad%89%e6%80%a7>#</a></h3><p>Ansible 的核心特性之一就是 <strong>幂等性</strong>，它意味着当你多次运行同一个 playbook 时，结果是相同的，不会对系统做不必要的更改。为了保证 playbook 的幂等性，需要：</p><ol><li><p><strong>检查资源的当前状态</strong>：</p><ul><li>使用 <code>state</code> 参数来指定目标资源的状态（如：<code>file</code>、<code>service</code> 模块）。</li><li>例如，在部署 Kubernetes 二进制时，我们应该确保文件已存在且为正确的版本。</li></ul></li><li><p><strong>使用适当的 Ansible 模块</strong>：</p><ul><li>使用 <code>file</code>、<code>copy</code>、<code>yum</code>、<code>apt</code> 等模块时，它们会自动检查目标状态，并且在目标已达到期望状态时，不会再次执行。</li><li>例如，<code>copy</code> 模块会在文件已经存在并且内容一致时跳过文件的复制。</li></ul></li><li><p><strong>避免冗余操作</strong>：</p><ul><li>在 <code>command</code> 或 <code>shell</code> 模块中，确保命令不会重复执行，或者在条件满足时才执行。例如，在执行 <code>kubeadm init</code> 时，只在主节点上执行初始化操作。</li></ul></li><li><p><strong>配置文件变更</strong>：</p><ul><li>对于需要更改配置文件的操作（例如 Kubernetes 配置文件、系统参数等），确保使用 <code>lineinfile</code> 或 <code>template</code> 模块，这些模块会检查文件是否已经按预期配置，避免重复修改。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>- <span style=color:#f92672>name</span>: <span style=color:#ae81ff>Ensure kubelet service is enabled and running</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>service</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>name</span>: <span style=color:#ae81ff>kubelet</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>state</span>: <span style=color:#ae81ff>started</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>enabled</span>: <span style=color:#66d9ef>true</span>
</span></span></code></pre></div></li></ol><h4 id=幂等性相关模块示例><strong>幂等性相关模块示例：</strong>
<a class=anchor href=#%e5%b9%82%e7%ad%89%e6%80%a7%e7%9b%b8%e5%85%b3%e6%a8%a1%e5%9d%97%e7%a4%ba%e4%be%8b>#</a></h4><ul><li><strong>file</strong> 模块：确保文件和目录的状态，如权限、存在与否。</li><li><strong>copy</strong> 模块：确保文件只被复制一次，且内容一致。</li><li><strong>template</strong> 模块：使用 Jinja2 模板来生成配置文件，并确保内容一致。</li><li><strong>service</strong> 模块：确保服务启动、停止和启用的状态。</li><li><strong>lineinfile</strong> 模块：确保文件中某行是存在的或被修改为指定的内容。</li></ul><h3 id=保证-playbook-的安全性><strong>保证 Playbook 的安全性：</strong>
<a class=anchor href=#%e4%bf%9d%e8%af%81-playbook-%e7%9a%84%e5%ae%89%e5%85%a8%e6%80%a7>#</a></h3><ol><li><p><strong>避免硬编码敏感信息</strong>：</p><ul><li>在 playbook 中避免直接写入敏感信息，如密码、密钥等。可以通过 <strong>Ansible Vault</strong> 来加密敏感信息。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ansible-vault create secrets.yml
</span></span></code></pre></div><p>然后在 playbook 中引用这个文件：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>- <span style=color:#f92672>name</span>: <span style=color:#ae81ff>Deploy Kubernetes Cluster</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>vars_files</span>:
</span></span><span style=display:flex><span>    - <span style=color:#ae81ff>secrets.yml</span>
</span></span></code></pre></div></li><li><p><strong>最小权限原则</strong>：</p><ul><li>使用 <code>become: true</code> 提升权限时，确保只在必要的任务中使用 <code>sudo</code> 权限，不要在所有任务中都提升权限。</li><li>使用适当的用户和组来执行任务，确保不使用 root 用户执行所有操作。</li></ul></li><li><p><strong>确保文件权限和所有权</strong>：</p><ul><li>通过 Ansible 的 <code>file</code> 模块确保配置文件和二进制文件的权限和所有权符合安全要求，避免文件被未授权用户访问。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>- <span style=color:#f92672>name</span>: <span style=color:#ae81ff>Set permissions on kubeconfig</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>file</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>path</span>: <span style=color:#e6db74>&#34;~/.kube/config&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>owner</span>: <span style=color:#e6db74>&#34;{{ ansible_user }}&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>group</span>: <span style=color:#e6db74>&#34;{{ ansible_user }}&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>mode</span>: <span style=color:#e6db74>&#34;0600&#34;</span>
</span></span></code></pre></div></li><li><p><strong>安全的网络配置</strong>：</p><ul><li>在 Kubernetes 集群中，网络配置是安全的关键。确保只有允许的 IP 地址能访问 Kubernetes API 服务器等关键组件，使用 <code>firewalld</code> 或 <code>iptables</code> 配置网络防火墙规则。</li></ul></li><li><p><strong>使用防火墙限制访问</strong>：</p><ul><li>使用 Ansible 来管理防火墙规则，确保只有受信任的主机可以访问集群中的关键端口。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>- <span style=color:#f92672>name</span>: <span style=color:#ae81ff>Allow access to Kubernetes API server</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>ufw</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>rule</span>: <span style=color:#ae81ff>allow</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>name</span>: <span style=color:#e6db74>&#34;Kubernetes API&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>port</span>: <span style=color:#ae81ff>6443</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>proto</span>: <span style=color:#ae81ff>tcp</span>
</span></span></code></pre></div></li><li><p><strong>保持软件更新</strong>：</p><ul><li>定期更新 Kubernetes 组件和依赖项，以避免已知的安全漏洞。</li></ul></li></ol><p>通过遵循以上实践，您可以使用 <strong>Ansible</strong> 部署二进制集群，并确保 playbook 具有幂等性和安全性。</p><h1 id=业务要实现灰度发布怎么设计流水线>业务要实现灰度发布？怎么设计流水线？
<a class=anchor href=#%e4%b8%9a%e5%8a%a1%e8%a6%81%e5%ae%9e%e7%8e%b0%e7%81%b0%e5%ba%a6%e5%8f%91%e5%b8%83%e6%80%8e%e4%b9%88%e8%ae%be%e8%ae%a1%e6%b5%81%e6%b0%b4%e7%ba%bf>#</a></h1><p>灰度发布（Gray Release）是一种逐步发布新版本的策略，通过控制新版本的发布范围和用户群体，确保在发现问题时能够快速回滚，避免全量发布的风险。在实施灰度发布时，CI/CD 流水线的设计需要保证可以灵活地控制版本的发布范围，并且能够与不同的环境进行交互。</p><h3 id=灰度发布流水线设计思路><strong>灰度发布流水线设计思路</strong>
<a class=anchor href=#%e7%81%b0%e5%ba%a6%e5%8f%91%e5%b8%83%e6%b5%81%e6%b0%b4%e7%ba%bf%e8%ae%be%e8%ae%a1%e6%80%9d%e8%b7%af>#</a></h3><ol><li><strong>分阶段发布</strong>： 灰度发布通常会分为多个阶段，从最小的用户群体开始，逐步扩大到全体用户。在流水线设计上，需要支持逐步发布的过程，例如：<ul><li><strong>阶段1</strong>：仅在内部或开发环境中发布。</li><li><strong>阶段2</strong>：小范围的真实用户群体（例如，10%的生产流量）。</li><li><strong>阶段3</strong>：全量发布给所有用户。</li></ul></li><li><strong>版本控制</strong>： 灰度发布通常涉及多个版本的并存，需要流水线支持不同版本的部署。例如，你可能同时部署旧版本和新版本，确保新版本能够与旧版本兼容，并能平滑过渡。</li><li><strong>A/B 测试</strong>： 灰度发布可以与 A/B 测试结合，控制不同用户组的流量。流水线需要能够将用户流量按照设定的规则（例如 80% 流量走旧版本，20% 流量走新版本）进行分配。</li></ol><h3 id=设计灰度发布流水线的步骤><strong>设计灰度发布流水线的步骤：</strong>
<a class=anchor href=#%e8%ae%be%e8%ae%a1%e7%81%b0%e5%ba%a6%e5%8f%91%e5%b8%83%e6%b5%81%e6%b0%b4%e7%ba%bf%e7%9a%84%e6%ad%a5%e9%aa%a4>#</a></h3><ol><li><p><strong>代码提交触发</strong>： 每当代码提交到 Git 仓库时，CI 流水线开始触发。使用工具如 <strong>Jenkins</strong>、<strong>GitLab CI</strong>、<strong>GitHub Actions</strong>、<strong>ArgoCD</strong> 等。</p><ul><li><strong>构建阶段</strong>：使用构建工具（如 <strong>Maven</strong>、<strong>Gradle</strong>、<strong>Docker</strong> 等）构建应用程序并生成镜像。</li><li><strong>单元测试</strong>：运行单元测试，确保代码质量。</li><li><strong>镜像推送</strong>：将构建的镜像推送到容器镜像仓库（例如 Docker Hub、Harbor）。</li></ul></li><li><p><strong>灰度发布策略</strong>： 灰度发布策略通常基于 Kubernetes 的 <strong>Deployment</strong> 和 <strong>Ingress</strong> 控制流量的分发。流水线可以通过调整 <strong>ReplicaSet</strong> 或 <strong>Pod</strong> 的副本数来实现。</p><p>下面是一个基本的 <strong>Kubernetes</strong> 灰度发布流程：</p><ul><li><strong>阶段1：小范围发布</strong>：使用 <code>canary</code> 或 <code>blue-green</code> 模式部署新版本。<ul><li>在第一个阶段，流量只指向新版本的 <strong>Pod</strong> 的一部分。</li><li>使用 Kubernetes <strong>Deployment</strong> 控制副本数，从而控制流量比例。</li></ul></li><li><strong>阶段2：扩大范围</strong>：在新版本无问题的情况下，逐渐增加新版本的流量。<ul><li>使用 <strong>Service</strong> 的流量管理和 <strong>Ingress</strong> 控制流量比例。</li></ul></li><li><strong>阶段3：全量发布</strong>：在没有问题后，完全替换旧版本，进行全量发布。</li></ul></li><li><p><strong>灰度发布流水线的任务</strong>：</p><p>下面是一个 <strong>Jenkins</strong> 和 <strong>Kubernetes</strong> 灰度发布流水线示例：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#ae81ff>pipeline {</span>
</span></span><span style=display:flex><span>    <span style=color:#ae81ff>agent any</span>
</span></span><span style=display:flex><span>    <span style=color:#ae81ff>environment {</span>
</span></span><span style=display:flex><span>        <span style=color:#ae81ff>DOCKER_IMAGE = &#34;your_image_name&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#ae81ff>NAMESPACE = &#34;production&#34;</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>    <span style=color:#ae81ff>stages {</span>
</span></span><span style=display:flex><span>        <span style=color:#ae81ff>stage(&#39;Build&#39;) {</span>
</span></span><span style=display:flex><span>            <span style=color:#ae81ff>steps {</span>
</span></span><span style=display:flex><span>                <span style=color:#ae81ff>script {</span>
</span></span><span style=display:flex><span>                    <span style=color:#ae81ff>// 构建 Docker 镜像</span>
</span></span><span style=display:flex><span>                    <span style=color:#ae81ff>sh &#39;docker build -t ${DOCKER_IMAGE}:${BUILD_NUMBER} .&#39;</span>
</span></span><span style=display:flex><span>                    <span style=color:#ae81ff>// 推送镜像到 Docker Hub</span>
</span></span><span style=display:flex><span>                    <span style=color:#ae81ff>sh &#39;docker push ${DOCKER_IMAGE}:${BUILD_NUMBER}&#39;</span>
</span></span><span style=display:flex><span>                }
</span></span><span style=display:flex><span>            }
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>        <span style=color:#ae81ff>stage(&#39;Deploy to Staging&#39;) {</span>
</span></span><span style=display:flex><span>            <span style=color:#ae81ff>steps {</span>
</span></span><span style=display:flex><span>                <span style=color:#ae81ff>script {</span>
</span></span><span style=display:flex><span>                    <span style=color:#ae81ff>// 部署到 Staging 环境</span>
</span></span><span style=display:flex><span>                    <span style=color:#ae81ff>sh &#39;kubectl apply -f kubernetes/staging.yaml&#39;</span>
</span></span><span style=display:flex><span>                }
</span></span><span style=display:flex><span>            }
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>        <span style=color:#ae81ff>stage(&#39;Canary Release&#39;) {</span>
</span></span><span style=display:flex><span>            <span style=color:#ae81ff>steps {</span>
</span></span><span style=display:flex><span>                <span style=color:#ae81ff>script {</span>
</span></span><span style=display:flex><span>                    <span style=color:#ae81ff>// 部署少量副本到生产环境（比如 10% 流量）</span>
</span></span><span style=display:flex><span>                    <span style=color:#ae81ff>sh &#39;kubectl set image deployment/your-deployment your-container=${DOCKER_IMAGE}:${BUILD_NUMBER} --record&#39;</span>
</span></span><span style=display:flex><span>                    <span style=color:#ae81ff>sh &#39;kubectl scale deployment your-deployment --replicas=10&#39; // 部署 10 个 Pod</span>
</span></span><span style=display:flex><span>                }
</span></span><span style=display:flex><span>            }
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>        <span style=color:#ae81ff>stage(&#39;Monitor Canary&#39;) {</span>
</span></span><span style=display:flex><span>            <span style=color:#ae81ff>steps {</span>
</span></span><span style=display:flex><span>                <span style=color:#ae81ff>script {</span>
</span></span><span style=display:flex><span>                    <span style=color:#ae81ff>// 等待一段时间，监控新版本是否正常</span>
</span></span><span style=display:flex><span>                    <span style=color:#f92672>sleep(time: 5, unit</span>: <span style=color:#e6db74>&#39;MINUTES&#39;</span><span style=color:#ae81ff>)</span>
</span></span><span style=display:flex><span>                    <span style=color:#ae81ff>sh &#39;kubectl get pods&#39;</span>
</span></span><span style=display:flex><span>                }
</span></span><span style=display:flex><span>            }
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>        <span style=color:#ae81ff>stage(&#39;Rollout&#39;) {</span>
</span></span><span style=display:flex><span>            <span style=color:#ae81ff>steps {</span>
</span></span><span style=display:flex><span>                <span style=color:#ae81ff>script {</span>
</span></span><span style=display:flex><span>                    <span style=color:#ae81ff>// 扩展新版本的 Pod 数量，逐步增加流量</span>
</span></span><span style=display:flex><span>                    <span style=color:#ae81ff>sh &#39;kubectl scale deployment your-deployment --replicas=50&#39;</span>
</span></span><span style=display:flex><span>                }
</span></span><span style=display:flex><span>            }
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>        <span style=color:#ae81ff>stage(&#39;Full Release&#39;) {</span>
</span></span><span style=display:flex><span>            <span style=color:#ae81ff>steps {</span>
</span></span><span style=display:flex><span>                <span style=color:#ae81ff>script {</span>
</span></span><span style=display:flex><span>                    <span style=color:#ae81ff>// 完全替换为新版本</span>
</span></span><span style=display:flex><span>                    <span style=color:#ae81ff>sh &#39;kubectl set image deployment/your-deployment your-container=${DOCKER_IMAGE}:${BUILD_NUMBER} --record&#39;</span>
</span></span><span style=display:flex><span>                    <span style=color:#ae81ff>sh &#39;kubectl scale deployment your-deployment --replicas=100&#39;</span>
</span></span><span style=display:flex><span>                }
</span></span><span style=display:flex><span>            }
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>    <span style=color:#ae81ff>post {</span>
</span></span><span style=display:flex><span>        <span style=color:#ae81ff>always {</span>
</span></span><span style=display:flex><span>            <span style=color:#ae81ff>// 清理操作</span>
</span></span><span style=display:flex><span>            <span style=color:#ae81ff>sh &#39;kubectl delete -f kubernetes/staging.yaml&#39;</span>
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div></li></ol><h3 id=灰度发布中的关键技术和工具><strong>灰度发布中的关键技术和工具：</strong>
<a class=anchor href=#%e7%81%b0%e5%ba%a6%e5%8f%91%e5%b8%83%e4%b8%ad%e7%9a%84%e5%85%b3%e9%94%ae%e6%8a%80%e6%9c%af%e5%92%8c%e5%b7%a5%e5%85%b7>#</a></h3><ol><li><strong>Kubernetes</strong>：<ul><li><strong>Deployment</strong>：使用 <strong>Deployment</strong> 控制 Pod 副本数量，进行逐步发布。</li><li><strong>ReplicaSet</strong>：调整副本数来控制流量。</li><li><strong>RollingUpdate</strong> 策略：使得 Kubernetes 可以逐步更新 Pods。</li></ul></li><li><strong>流量控制</strong>：<ul><li><strong>Ingress Controller</strong>：可以使用 Nginx 或 Traefik 来管理 HTTP 流量，控制灰度发布流量的比例（例如：80% 旧版本，20% 新版本）。</li><li><strong>Service</strong>：控制新旧版本 Pod 的流量分配。</li></ul></li><li><strong>监控与告警</strong>：<ul><li>在灰度发布过程中，必须有完善的监控机制，确保在发布新版本时能够及时发现问题。</li><li>使用 Prometheus + Grafana 监控系统，确保应用性能正常。</li><li>设置告警机制（如在性能下降时自动回滚）。</li></ul></li><li><strong>回滚机制</strong>：<ul><li>当发现问题时，流水线应支持自动回滚到上一个稳定版本。</li><li>可以通过 Kubernetes 的 <strong>kubectl rollout undo</strong> 命令回滚部署，或者使用 <strong>ArgoCD</strong> 等工具来进行版本控制。</li></ul></li></ol><h3 id=总结-5><strong>总结</strong>：
<a class=anchor href=#%e6%80%bb%e7%bb%93-5>#</a></h3><p>灰度发布流水线的设计要保证在各个阶段都有自动化的控制和监控机制。流水线通常包括代码提交、镜像构建、部署、监控和回滚等多个步骤。通过与 Kubernetes 和流量管理工具配合，灰度发布可以有效减少发布带来的风险，并确保应用稳定运行。</p><h1 id=业务场景链路分发业务运维流量洪峰设计自动扩缩的方案>业务场景，链路分发。业务运维，流量洪峰，设计自动扩缩的方案？
<a class=anchor href=#%e4%b8%9a%e5%8a%a1%e5%9c%ba%e6%99%af%e9%93%be%e8%b7%af%e5%88%86%e5%8f%91%e4%b8%9a%e5%8a%a1%e8%bf%90%e7%bb%b4%e6%b5%81%e9%87%8f%e6%b4%aa%e5%b3%b0%e8%ae%be%e8%ae%a1%e8%87%aa%e5%8a%a8%e6%89%a9%e7%bc%a9%e7%9a%84%e6%96%b9%e6%a1%88>#</a></h1><h2 id=流量洪峰自动扩缩容方案设计-><strong>流量洪峰自动扩缩容方案设计</strong> 🚀
<a class=anchor href=#%e6%b5%81%e9%87%8f%e6%b4%aa%e5%b3%b0%e8%87%aa%e5%8a%a8%e6%89%a9%e7%bc%a9%e5%ae%b9%e6%96%b9%e6%a1%88%e8%ae%be%e8%ae%a1->#</a></h2><p>业务运维中，面对 <strong>大促、节假日、突发流量</strong> 时，<strong>自动扩缩容</strong> 是保障系统稳定的关键。针对 Kubernetes（K8s）容器化环境，我们可以结合 <strong>HPA（水平扩展）、VPA（垂直扩展）和 CA（Cluster Autoscaler）</strong> 来应对流量洪峰。</p><hr><h2 id=1-流量洪峰的特征><strong>1. 流量洪峰的特征</strong>
<a class=anchor href=#1-%e6%b5%81%e9%87%8f%e6%b4%aa%e5%b3%b0%e7%9a%84%e7%89%b9%e5%be%81>#</a></h2><ul><li><strong>突增 QPS（每秒请求数）</strong>，通常表现为访问量短时间内飙升</li><li><strong>CPU、内存飙升</strong>，单个实例资源紧张</li><li><strong>数据库连接数暴涨</strong>，导致慢查询或超时</li><li><strong>消息队列积压</strong>，消费端处理不过来</li><li><strong>外部 API 响应变慢</strong>，依赖服务压力过大</li></ul><hr><h2 id=2-自动扩缩容架构><strong>2. 自动扩缩容架构</strong>
<a class=anchor href=#2-%e8%87%aa%e5%8a%a8%e6%89%a9%e7%bc%a9%e5%ae%b9%e6%9e%b6%e6%9e%84>#</a></h2><p>🚀 <strong>核心组件</strong></p><ul><li><strong>HPA（Horizontal Pod Autoscaler）</strong>：基于 CPU、内存、QPS 等指标，自动扩展 Pod 副本</li><li><strong>VPA（Vertical Pod Autoscaler）</strong>：动态调整 Pod 资源（CPU/内存）请求</li><li><strong>Cluster Autoscaler（CA）</strong>：节点资源不足时自动扩展计算节点</li><li><strong>KEDA（Kubernetes Event-driven Autoscaling）</strong>：基于消息队列（Kafka、RabbitMQ）负载进行扩缩容</li></ul><p>📌 <strong>方案架构图</strong></p><pre tabindex=0><code>+----------------------------------+
|   Cluster Autoscaler (CA)        |  &lt;==&gt;  扩展 Kubernetes 计算节点
+----------------------------------+
          |
          v
+---------------------------+
|  Horizontal Pod Autoscaler (HPA)  |  &lt;==&gt;  扩展 Pod 数量
+---------------------------+
          |
          v
+---------------------------+
|  Vertical Pod Autoscaler (VPA)    |  &lt;==&gt;  调整 Pod 资源 (CPU/内存)
+---------------------------+
          |
          v
+---------------------------+
|  KEDA (事件驱动扩缩容)      |  &lt;==&gt;  基于消息队列负载扩容
+---------------------------+
</code></pre><hr><h2 id=3-关键扩缩容策略><strong>3. 关键扩缩容策略</strong>
<a class=anchor href=#3-%e5%85%b3%e9%94%ae%e6%89%a9%e7%bc%a9%e5%ae%b9%e7%ad%96%e7%95%a5>#</a></h2><h3 id=1pod-水平扩展hpa><strong>（1）Pod 水平扩展（HPA）</strong>
<a class=anchor href=#1pod-%e6%b0%b4%e5%b9%b3%e6%89%a9%e5%b1%95hpa>#</a></h3><p>适用于 <strong>QPS、CPU、内存、响应时间</strong> 波动的情况。</p><p>📌 <strong>示例：基于 CPU 自动扩缩容</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>autoscaling/v2</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>HorizontalPodAutoscaler</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-app-hpa</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>scaleTargetRef</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>apps/v1</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Deployment</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-app</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>minReplicas</span>: <span style=color:#ae81ff>3</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>maxReplicas</span>: <span style=color:#ae81ff>50</span>  <span style=color:#75715e># 最大扩展 50 个 Pod</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>metrics</span>:
</span></span><span style=display:flex><span>  - <span style=color:#f92672>type</span>: <span style=color:#ae81ff>Resource</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>resource</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>name</span>: <span style=color:#ae81ff>cpu</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>target</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>type</span>: <span style=color:#ae81ff>Utilization</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>averageUtilization</span>: <span style=color:#ae81ff>60</span>  <span style=color:#75715e># CPU 超过 60% 时扩容</span>
</span></span></code></pre></div><p>📌 <strong>HPA 触发条件</strong></p><ul><li><strong>CPU > 60%</strong></li><li><strong>内存占用率高</strong></li><li><strong>QPS > 阈值</strong></li></ul><hr><h3 id=2pod-垂直扩展vpa><strong>（2）Pod 垂直扩展（VPA）</strong>
<a class=anchor href=#2pod-%e5%9e%82%e7%9b%b4%e6%89%a9%e5%b1%95vpa>#</a></h3><p>适用于 <strong>单个 Pod 资源不足（CPU/内存）</strong> 的情况。</p><p>📌 <strong>示例：VPA 自动调整 Pod 资源</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>autoscaling.k8s.io/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>VerticalPodAutoscaler</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-app-vpa</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>targetRef</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>apiVersion</span>: <span style=color:#e6db74>&#34;apps/v1&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Deployment</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-app</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>updatePolicy</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>updateMode</span>: <span style=color:#e6db74>&#34;Auto&#34;</span>  <span style=color:#75715e># 自动调整 CPU 和内存</span>
</span></span></code></pre></div><p>📌 <strong>VPA 触发条件</strong></p><ul><li><strong>单个 Pod CPU 长期 100%</strong></li><li><strong>内存持续超出限额</strong></li><li><strong>高并发场景下单个 Pod 处理能力不足</strong></li></ul><hr><h3 id=3集群节点扩展cluster-autoscaler><strong>（3）集群节点扩展（Cluster Autoscaler）</strong>
<a class=anchor href=#3%e9%9b%86%e7%be%a4%e8%8a%82%e7%82%b9%e6%89%a9%e5%b1%95cluster-autoscaler>#</a></h3><p>适用于 <strong>所有 Pod 都已调度但无可用节点</strong> 的情况。</p><p>📌 <strong>如何工作</strong></p><ul><li>HPA 扩展 Pod 失败 → 计算资源不足 → 触发 CA 增加新节点</li><li>CA 会根据 Pod 需求，自动扩展云厂商上的节点（阿里云 ACK、AWS EKS、GKE）</li></ul><p>📌 <strong>配置 Cluster Autoscaler</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#ae81ff>cluster-autoscaler --nodes=3:50 </span> <span style=color:#75715e># 节点最少 3 个，最多 50 个</span>
</span></span></code></pre></div><p>📌 <strong>CA 触发条件</strong></p><ul><li><strong>Pod 处于 Pending 状态</strong>（调度失败）</li><li><strong>节点 CPU / 内存不足</strong></li></ul><hr><h3 id=4事件驱动扩缩容keda><strong>（4）事件驱动扩缩容（KEDA）</strong>
<a class=anchor href=#4%e4%ba%8b%e4%bb%b6%e9%a9%b1%e5%8a%a8%e6%89%a9%e7%bc%a9%e5%ae%b9keda>#</a></h3><p>适用于 <strong>消息队列、Kafka 订阅任务激增</strong> 的情况。</p><p>📌 <strong>示例：基于 Kafka 消费积压自动扩缩</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>keda.sh/v1alpha1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ScaledObject</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>kafka-consumer</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>scaleTargetRef</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>name</span>: <span style=color:#ae81ff>kafka-consumer-deployment</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>minReplicaCount</span>: <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>maxReplicaCount</span>: <span style=color:#ae81ff>100</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>triggers</span>:
</span></span><span style=display:flex><span>  - <span style=color:#f92672>type</span>: <span style=color:#ae81ff>kafka</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>topic</span>: <span style=color:#ae81ff>my-topic</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>bootstrapServers</span>: <span style=color:#ae81ff>my-kafka-broker:9092</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>lagThreshold</span>: <span style=color:#e6db74>&#34;1000&#34;</span>  <span style=color:#75715e># 消息积压超过 1000 条时扩容</span>
</span></span></code></pre></div><p>📌 <strong>KEDA 触发条件</strong></p><ul><li><strong>Kafka/RabbitMQ 消息积压</strong></li><li><strong>外部 API 请求排队</strong></li><li><strong>数据库连接数暴涨</strong></li></ul><hr><h2 id=4-预案--高可用策略><strong>4. 预案 & 高可用策略</strong>
<a class=anchor href=#4-%e9%a2%84%e6%a1%88--%e9%ab%98%e5%8f%af%e7%94%a8%e7%ad%96%e7%95%a5>#</a></h2><h3 id=1流量预案><strong>（1）流量预案</strong>
<a class=anchor href=#1%e6%b5%81%e9%87%8f%e9%a2%84%e6%a1%88>#</a></h3><p>📌 <strong>提前扩容</strong></p><ul><li>活动前手动扩容 Pod 和节点（预扩容）</li><li>预估 QPS 峰值，提前加机器</li></ul><p>📌 <strong>灰度发布</strong></p><ul><li><strong>蓝绿发布</strong>，确保新版本无异常</li><li><strong>金丝雀发布</strong>，逐步放量</li></ul><p>📌 <strong>弹性流量调度</strong></p><ul><li><strong>流量分级</strong>：高优先级用户优先处理</li><li><strong>熔断限流</strong>：Redis + Sentinel 做限流降级</li><li><strong>动态降级</strong>：超时接口自动降级</li></ul><hr><h3 id=2故障恢复><strong>（2）故障恢复</strong>
<a class=anchor href=#2%e6%95%85%e9%9a%9c%e6%81%a2%e5%a4%8d>#</a></h3><p>📌 <strong>自愈机制</strong></p><ul><li><strong>Pod OOM 自动重启</strong></li><li><strong>节点宕机自动迁移</strong></li><li><strong>数据库超时自动降级</strong></li></ul><p>📌 <strong>快速扩展</strong></p><ul><li><strong>手动快速扩展</strong>：</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl scale --replicas<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span> deployment/my-app
</span></span></code></pre></div><ul><li>自动恢复（Prometheus + Alertmanager）<ul><li>发现 CPU 过高 → 触发 HPA 扩容</li><li>发现 Pod 过载 → 触发 CA 增加节点</li></ul></li></ul><hr><h2 id=5-方案总结><strong>5. 方案总结</strong>
<a class=anchor href=#5-%e6%96%b9%e6%a1%88%e6%80%bb%e7%bb%93>#</a></h2><table><thead><tr><th>方案</th><th>适用场景</th><th>优势</th></tr></thead><tbody><tr><td><strong>HPA（水平扩展）</strong></td><td>CPU、QPS、内存</td><td>快速扩展 Pod 副本</td></tr><tr><td><strong>VPA（垂直扩展）</strong></td><td>CPU / 内存不足</td><td>调整单个 Pod 资源</td></tr><tr><td><strong>Cluster Autoscaler</strong></td><td>无可用节点</td><td>自动增加计算资源</td></tr><tr><td><strong>KEDA（事件驱动扩容）</strong></td><td>消息队列积压</td><td>低延迟扩缩容</td></tr></tbody></table><h3 id=最终架构><strong>最终架构</strong>
<a class=anchor href=#%e6%9c%80%e7%bb%88%e6%9e%b6%e6%9e%84>#</a></h3><ul><li><strong>HPA + VPA + CA + KEDA 组合方案</strong></li><li><strong>高并发时 HPA 迅速扩容</strong></li><li><strong>计算资源不足时 CA 扩展节点</strong></li><li><strong>VPA 确保单个 Pod 性能</strong></li><li><strong>KEDA 应对事件驱动场景</strong></li></ul><p>这样可以 <strong>低成本、自动化</strong> 处理流量洪峰，保障业务稳定 💪🚀</p><h1 id=故障排查视频加载缓慢网络-存储-链路分析怎么一个排查思路>故障排查？视频加载缓慢,网络 存储 链路分析，怎么一个排查思路？
<a class=anchor href=#%e6%95%85%e9%9a%9c%e6%8e%92%e6%9f%a5%e8%a7%86%e9%a2%91%e5%8a%a0%e8%bd%bd%e7%bc%93%e6%85%a2%e7%bd%91%e7%bb%9c-%e5%ad%98%e5%82%a8-%e9%93%be%e8%b7%af%e5%88%86%e6%9e%90%e6%80%8e%e4%b9%88%e4%b8%80%e4%b8%aa%e6%8e%92%e6%9f%a5%e6%80%9d%e8%b7%af>#</a></h1><p>排查视频加载缓慢的问题，尤其是在网络、存储和链路方面，通常需要按步骤进行细致的分析。以下是一个常见的排查思路：</p><h3 id=1-确定视频加载缓慢的表现><strong>1. 确定视频加载缓慢的表现</strong>
<a class=anchor href=#1-%e7%a1%ae%e5%ae%9a%e8%a7%86%e9%a2%91%e5%8a%a0%e8%bd%bd%e7%bc%93%e6%85%a2%e7%9a%84%e8%a1%a8%e7%8e%b0>#</a></h3><ul><li><strong>是否所有视频都慢</strong>：问题是普遍的还是仅限于某些视频？</li><li><strong>延迟在哪个环节</strong>：是视频播放开始慢，还是在播放过程中卡顿？</li><li><strong>网络或存储</strong>：是否与网络带宽、存储读写速度、缓存等因素有关？</li></ul><h3 id=2-分析网络链路><strong>2. 分析网络链路</strong>
<a class=anchor href=#2-%e5%88%86%e6%9e%90%e7%bd%91%e7%bb%9c%e9%93%be%e8%b7%af>#</a></h3><ul><li><strong>检查带宽</strong>：<ul><li><strong>下载速度</strong>：测试网络的下载速度，确保带宽足够。使用工具如 <code>speedtest-cli</code> 检查网络的上传和下载速度。</li><li><strong>是否有带宽瓶颈</strong>：使用 <code>iperf3</code> 进行带宽测试，检查是否存在网络带宽的瓶颈。</li></ul></li><li><strong>网络延迟和丢包</strong>：<ul><li><strong>ping 测试</strong>：使用 <code>ping</code> 测试到服务器的延迟，确保网络延迟不会影响视频加载。</li><li><strong>traceroute 路由追踪</strong>：使用 <code>traceroute</code> 或 <code>mtr</code> 追踪网络路由，看是否有某一跳存在显著的延迟或丢包现象。</li></ul></li><li><strong>网络质量</strong>：<ul><li><strong>TCP连接数限制</strong>：如果服务器接入了大量的并发请求，可能会导致 TCP 连接池溢出。检查服务器和客户端的连接数。</li><li><strong>内容分发网络 (CDN)</strong>：如果使用了 CDN，检查 CDN 节点是否正常。如果是单一节点出现问题，可能导致某些用户加载缓慢。</li></ul></li><li><strong>HTTP 响应时间</strong>：<ul><li><strong>使用 <code>curl</code> 或 <code>wget</code></strong>：检查视频文件请求的响应时间，确认是否是服务器端的响应慢导致加载延迟。</li><li><strong>HTTP 缓存</strong>：确认是否使用了合适的 HTTP 缓存机制，如果没有，服务器每次请求都需要重新生成视频流，可能导致延迟。</li></ul></li><li><strong>协议优化</strong>：<ul><li>如果使用的是 HTTP/1，考虑切换到 HTTP/2，后者在多连接、头部压缩、请求优先级等方面有较大改进，能有效减少延迟。</li></ul></li></ul><h3 id=3-存储层排查><strong>3. 存储层排查</strong>
<a class=anchor href=#3-%e5%ad%98%e5%82%a8%e5%b1%82%e6%8e%92%e6%9f%a5>#</a></h3><ul><li>磁盘 I/O 性能：<ul><li><strong>磁盘性能</strong>：检查存储服务器的磁盘 I/O 性能，使用工具如 <code>iostat</code>、<code>vmstat</code> 等查看磁盘的吞吐量和延迟。</li><li><strong>磁盘使用率</strong>：确认存储空间是否满，或磁盘是否达到负载上限。<code>df -h</code> 和 <code>iostat</code> 可以帮助分析存储设备的使用情况。</li></ul></li><li>存储缓存：<ul><li><strong>缓存机制</strong>：检查是否有缓存机制（如 CDN 缓存、反向代理缓存等），并验证缓存是否工作正常。如果没有缓存，存储的读取负载会导致延迟。</li><li><strong>SSD vs HDD</strong>：如果存储设备是 HDD，可能会导致读取速度较慢。SSD 可以大幅提高读取速度。</li></ul></li><li>存储系统负载：<ul><li><strong>存储服务的负载</strong>：如果是使用的共享存储或网络存储（如 NAS、SAN），确保存储系统本身没有过载，网络延迟和存储 I/O 高峰期可能会影响视频加载速度。</li></ul></li></ul><h3 id=4-视频文件分析><strong>4. 视频文件分析</strong>
<a class=anchor href=#4-%e8%a7%86%e9%a2%91%e6%96%87%e4%bb%b6%e5%88%86%e6%9e%90>#</a></h3><ul><li><strong>视频文件格式</strong>：检查视频文件的格式和编码。某些视频格式可能不适合网络传输，或者没有进行合适的编码优化。</li><li><strong>视频分辨率和码率</strong>：如果视频的分辨率和码率过高，可能会导致网络传输过程中出现缓冲。考虑提供多种分辨率的视频版本，允许客户端根据网络条件自动选择合适的码率。</li><li><strong>视频分片</strong>：确保视频文件采用了分片处理（如 HLS、DASH 等流媒体协议），这样客户端可以逐步下载视频而不是一次性下载完整视频文件。</li></ul><h3 id=5-客户端排查><strong>5. 客户端排查</strong>
<a class=anchor href=#5-%e5%ae%a2%e6%88%b7%e7%ab%af%e6%8e%92%e6%9f%a5>#</a></h3><ul><li>客户端性能：<ul><li>确认客户端设备是否性能不足（CPU、内存、GPU 等）。例如，低性能设备可能无法处理高分辨率或高码率的视频流。</li><li>检查客户端是否使用了合适的视频播放器，并且是否支持硬件解码。软件解码可能会导致播放卡顿。</li></ul></li><li>浏览器/播放器优化：<ul><li><strong>浏览器缓存</strong>：检查浏览器缓存是否正确工作。浏览器可以缓存部分视频数据，避免每次都从头开始加载。</li><li><strong>视频播放器设置</strong>：确保播放器设置了适当的缓冲策略，例如设置足够的缓冲区大小来平滑播放。</li></ul></li><li>网络环境：<ul><li>确认客户端网络是否稳定，如果使用 Wi-Fi，尝试切换到有线网络进行测试，排除无线网络带来的不稳定因素。</li></ul></li></ul><h3 id=6-流程监控与日志><strong>6. 流程监控与日志</strong>
<a class=anchor href=#6-%e6%b5%81%e7%a8%8b%e7%9b%91%e6%8e%a7%e4%b8%8e%e6%97%a5%e5%bf%97>#</a></h3><ul><li>日志分析：<ul><li><strong>视频播放器日志</strong>：查看视频播放器的日志，检查是否有加载失败或延迟的错误信息。</li><li><strong>Web 服务器日志</strong>：检查视频请求的服务器日志，查看是否存在错误或异常的 HTTP 响应码（如 404、500 等）。</li><li><strong>存储日志</strong>：如果使用分布式存储或对象存储，检查相关存储服务的日志，确保没有出现故障。</li></ul></li><li>监控系统：<ul><li>设置监控指标，监控存储、网络、服务器等关键资源的性能，利用 Prometheus、Grafana 等工具实时查看资源的负载情况。</li></ul></li></ul><h3 id=7-进行压力测试><strong>7. 进行压力测试</strong>
<a class=anchor href=#7-%e8%bf%9b%e8%a1%8c%e5%8e%8b%e5%8a%9b%e6%b5%8b%e8%af%95>#</a></h3><ul><li>在排查过程中，可以使用 <strong>压力测试</strong> 工具（如 <strong>Apache Benchmark</strong>、<strong>JMeter</strong>）模拟用户请求，查看服务器和存储的性能瓶颈，尤其是视频文件的读写能力。</li></ul><h3 id=总结-6><strong>总结</strong>：
<a class=anchor href=#%e6%80%bb%e7%bb%93-6>#</a></h3><p>视频加载缓慢问题的排查需要从 <strong>网络链路、存储性能、视频文件、客户端性能</strong> 等多个维度进行分析。通过逐步检查各个环节，定位瓶颈，并根据发现的问题进行优化。如果是网络延迟或带宽不足，可以通过升级网络设施、增加 CDN 节点等方式解决；如果是存储瓶颈，可以考虑升级存储硬件或优化存储架构；如果是视频编码或播放器问题，可以通过调整视频的分辨率和码率，或者改进客户端播放器来解决。</p><h1 id=多个故障怎么区分优先级>多个故障，怎么区分优先级？
<a class=anchor href=#%e5%a4%9a%e4%b8%aa%e6%95%85%e9%9a%9c%e6%80%8e%e4%b9%88%e5%8c%ba%e5%88%86%e4%bc%98%e5%85%88%e7%ba%a7>#</a></h1><p>区分多个故障的优先级，通常需要依据故障的严重性、对业务的影响、恢复的难易程度和时间紧迫性来进行判断。以下是一些常见的优先级分类方法，帮助你快速、有效地区分和处理多个故障。</p><h3 id=1-根据故障影响范围分类><strong>1. 根据故障影响范围分类</strong>
<a class=anchor href=#1-%e6%a0%b9%e6%8d%ae%e6%95%85%e9%9a%9c%e5%bd%b1%e5%93%8d%e8%8c%83%e5%9b%b4%e5%88%86%e7%b1%bb>#</a></h3><ul><li><strong>高优先级（Critical / P1）</strong>：<ul><li>故障导致 <strong>关键业务中断</strong>，对公司收入、客户体验、安全等产生 <strong>严重影响</strong>。</li><li><strong>例如</strong>：数据库宕机、核心系统无法访问、大规模的服务中断。</li><li><strong>处理方式</strong>：立即响应，紧急修复，甚至需要全员参与处理。</li></ul></li><li><strong>中优先级（Major / P2）</strong>：<ul><li>故障影响到部分用户或服务，导致 <strong>功能受限</strong>，但 <strong>业务仍可进行</strong>。</li><li><strong>例如</strong>：某些服务响应缓慢、单个功能不可用，部分用户受影响。</li><li><strong>处理方式</strong>：尽快响应，优先修复受影响范围广、影响较大的部分。</li></ul></li><li><strong>低优先级（Minor / P3）</strong>：<ul><li>故障对业务没有直接影响或仅造成 <strong>小范围</strong>的功能损失。</li><li><strong>例如</strong>：某些非关键服务的性能问题、UI bug，甚至是日志中的警告信息。</li><li><strong>处理方式</strong>：通常可以排在后面处理，根据实际情况分配时间。</li></ul></li></ul><h3 id=2-根据故障恢复难易度分类><strong>2. 根据故障恢复难易度分类</strong>
<a class=anchor href=#2-%e6%a0%b9%e6%8d%ae%e6%95%85%e9%9a%9c%e6%81%a2%e5%a4%8d%e9%9a%be%e6%98%93%e5%ba%a6%e5%88%86%e7%b1%bb>#</a></h3><ul><li><strong>高优先级</strong>：如果故障较难修复或需要复杂的操作，如涉及到系统重启、硬件故障、数据库恢复等，应该优先处理。</li><li><strong>低优先级</strong>：如果故障简单易修复，比如某个配置错误或小范围的代码bug，修复时间短，则优先级较低。</li></ul><h3 id=3-根据时间敏感性分类><strong>3. 根据时间敏感性分类</strong>
<a class=anchor href=#3-%e6%a0%b9%e6%8d%ae%e6%97%b6%e9%97%b4%e6%95%8f%e6%84%9f%e6%80%a7%e5%88%86%e7%b1%bb>#</a></h3><ul><li><strong>紧急（High urgency）</strong>：如果故障需要 <strong>尽快恢复</strong>，比如影响到 <strong>客户服务或收入</strong>，应该优先处理。</li><li><strong>普通（Medium urgency）</strong>：如果故障不涉及直接的业务损失，但仍然需要在一定时间内解决，如影响到非高峰时段的流量。</li><li><strong>不紧急（Low urgency）</strong>：如果故障不会在短期内对业务产生任何影响，且可以在后续的维护窗口中解决，属于低优先级。</li></ul><h3 id=4-故障分类><strong>4. 故障分类</strong>
<a class=anchor href=#4-%e6%95%85%e9%9a%9c%e5%88%86%e7%b1%bb>#</a></h3><ul><li><strong>功能性故障</strong>：例如，API 调用失败、数据库查询失败等，这类故障通常需要快速恢复。</li><li><strong>性能性故障</strong>：例如，响应时间过长、系统卡顿等，可能会影响用户体验，尤其在高并发的场景下，需要及时修复。</li><li><strong>安全性故障</strong>：如数据泄漏、权限漏洞等，这类问题具有较高的优先级，通常需要第一时间处理。</li><li><strong>可用性问题</strong>：例如服务间歇性不可用，应该尽快定位故障源，修复时进行适当监控，确保业务恢复。</li></ul><h3 id=5-使用故障管理系统><strong>5. 使用故障管理系统</strong>
<a class=anchor href=#5-%e4%bd%bf%e7%94%a8%e6%95%85%e9%9a%9c%e7%ae%a1%e7%90%86%e7%b3%bb%e7%bb%9f>#</a></h3><ul><li><strong>服务级别协议（SLA）</strong>：依据 SLA，定义每个故障类型的响应时间和恢复时间。遵守 SLA 是确保业务正常运营的关键。</li><li><strong>自动化故障监控与告警</strong>：利用 Prometheus、Grafana 等监控工具设置 <strong>告警阈值</strong>，帮助实时获取故障信息，并自动将故障分类并标记优先级。</li><li><strong>故障管理工具</strong>：使用如 <strong>Jira</strong>、<strong>ServiceNow</strong> 等故障管理工具，设定优先级、分配责任人并跟踪修复进度。</li></ul><h3 id=6-基于业务价值评估><strong>6. 基于业务价值评估</strong>
<a class=anchor href=#6-%e5%9f%ba%e4%ba%8e%e4%b8%9a%e5%8a%a1%e4%bb%b7%e5%80%bc%e8%af%84%e4%bc%b0>#</a></h3><ul><li><strong>关键系统</strong>：如果故障涉及到影响核心业务的系统或组件，应该优先处理。比如支付系统、核心数据库、用户认证系统等。</li><li><strong>非核心系统</strong>：对业务影响较小的系统可以排在后面，例如日志收集、监控系统等。</li></ul><h3 id=7-确定恢复的最小可行方案><strong>7. 确定恢复的最小可行方案</strong>
<a class=anchor href=#7-%e7%a1%ae%e5%ae%9a%e6%81%a2%e5%a4%8d%e7%9a%84%e6%9c%80%e5%b0%8f%e5%8f%af%e8%a1%8c%e6%96%b9%e6%a1%88>#</a></h3><ul><li>对于每个故障，根据业务需要，制定 <strong>最小可行恢复方案</strong>。有些故障可以临时绕过或采用简化方式恢复服务，而无需立即完全修复。</li></ul><h3 id=排查故障的优先级处理流程><strong>排查故障的优先级处理流程</strong>：
<a class=anchor href=#%e6%8e%92%e6%9f%a5%e6%95%85%e9%9a%9c%e7%9a%84%e4%bc%98%e5%85%88%e7%ba%a7%e5%a4%84%e7%90%86%e6%b5%81%e7%a8%8b>#</a></h3><ol><li><strong>紧急响应</strong>：首先处理对业务影响较大、需迅速恢复的故障（如系统宕机、用户无法访问等）。</li><li><strong>逐一处理次要故障</strong>：在处理完高优先级故障后，逐步排查并处理中优先级的故障。</li><li><strong>监控与调整</strong>：设置监控，确保业务恢复过程中没有新的问题，随时调整处理优先级。</li></ol><h3 id=总结-7><strong>总结</strong>
<a class=anchor href=#%e6%80%bb%e7%bb%93-7>#</a></h3><p>故障的优先级应该根据故障的 <strong>严重性、影响范围、恢复难度</strong> 和 <strong>时间敏感性</strong> 来判断。通过合理分类，能够确保高优先级的问题尽快得到处理，同时也不忽视较低优先级的长期改进。优先级分类需要灵活，根据不同场景和业务需求动态调整，确保业务不中断并保持最佳运行状态。</p><h1 id=二线做什么事情一线反馈到你这边怎么去解决>二线做什么事情？一线反馈到你这边，怎么去解决？
<a class=anchor href=#%e4%ba%8c%e7%ba%bf%e5%81%9a%e4%bb%80%e4%b9%88%e4%ba%8b%e6%83%85%e4%b8%80%e7%ba%bf%e5%8f%8d%e9%a6%88%e5%88%b0%e4%bd%a0%e8%bf%99%e8%be%b9%e6%80%8e%e4%b9%88%e5%8e%bb%e8%a7%a3%e5%86%b3>#</a></h1><p>在运维或技术支持中，<strong>二线支持</strong>（L2）通常是 <strong>处理和解决一线支持</strong>（L1）无法解决的更复杂问题的部门。二线支持通常处理的问题比一线更复杂，涉及到深入的技术分析、故障排查以及系统优化等工作。</p><h3 id=二线的主要工作职责><strong>二线的主要工作职责</strong>：
<a class=anchor href=#%e4%ba%8c%e7%ba%bf%e7%9a%84%e4%b8%bb%e8%a6%81%e5%b7%a5%e4%bd%9c%e8%81%8c%e8%b4%a3>#</a></h3><ol><li><strong>复杂问题排查</strong>：<ul><li>一线可能会收到来自用户的故障反馈（如系统崩溃、性能下降等），但是这些问题超出了他们的解决范围。二线需要接手这些问题，并通过更详细的日志分析、系统配置检查等方法进行故障定位和解决。</li><li>比如，针对服务端出现的性能瓶颈、数据库查询慢等问题，二线工程师需要深入到代码、数据库、配置文件以及网络层级进行排查。</li></ul></li><li><strong>问题的根本原因分析</strong>：<ul><li>一线通常通过标准流程解决已知问题，但如果问题较为复杂或没有现成的解决方案，二线会进行根本原因分析（Root Cause Analysis，RCA）。他们会查阅更详细的日志，分析系统健康状况，诊断出问题的根源并制定修复方案。</li></ul></li><li><strong>处理系统异常和故障</strong>：<ul><li>二线支持处理的一些问题可能涉及到系统崩溃、硬件故障、网络问题或是与第三方集成的故障。例如，Kubernetes 集群故障、网络连接中断等。这些都超出了基础支持的范围，二线支持需要具备较强的技术能力来解决。</li></ul></li><li><strong>部署、维护与优化</strong>：<ul><li>二线支持还会参与系统的维护工作，如升级、补丁管理、配置优化等。他们会对系统进行优化，确保系统运行稳定。</li></ul></li><li><strong>与开发团队的协作</strong>：<ul><li>当问题涉及到代码或开发层面的缺陷时，二线需要与开发团队紧密配合，进行代码排查、功能验证和修复。</li></ul></li><li><strong>知识库管理</strong>：<ul><li>在处理问题时，二线团队还会积累大量的技术文档和解决方案，这些可以帮助一线快速定位和解决常见问题。因此，二线支持需要持续更新技术文档和知识库。</li></ul></li><li><strong>提供技术支持与培训</strong>：<ul><li>二线支持还负责为一线团队提供技术培训和指导，以提高他们的技术水平，减少重复性故障。</li></ul></li></ol><h3 id=一线反馈到二线后如何解决问题><strong>一线反馈到二线后，如何解决问题？</strong>
<a class=anchor href=#%e4%b8%80%e7%ba%bf%e5%8f%8d%e9%a6%88%e5%88%b0%e4%ba%8c%e7%ba%bf%e5%90%8e%e5%a6%82%e4%bd%95%e8%a7%a3%e5%86%b3%e9%97%ae%e9%a2%98>#</a></h3><p>当一线无法解决的问题反馈给二线时，解决过程一般包括以下步骤：</p><ol><li><strong>问题确认与分类</strong>：<ul><li>首先，二线支持人员会确认问题的性质，并进行初步分类。比如，是硬件故障、软件问题还是网络问题。</li><li>确认一线已经尝试过的解决方法，并确认所有基本步骤（如重启服务、检查网络连接等）已被一线执行。</li></ul></li><li><strong>获取更多信息</strong>：<ul><li>二线支持会收集更多的背景信息，包括但不限于：<ul><li>错误日志、系统监控数据（CPU、内存、磁盘等）</li><li>故障发生时的环境（如负载、流量、用户操作等）</li><li>出现问题的具体步骤和操作流程</li></ul></li><li>可以与用户或一线进一步沟通，确认详细信息。</li></ul></li><li><strong>深度排查与分析</strong>：<ul><li>基于收集到的信息，二线工程师开始深入排查，可能会使用更多的监控工具、分析工具或者查看更详细的日志文件，检查是否有系统瓶颈、配置错误或者其他影响因素。</li><li>如涉及到复杂的系统架构问题，二线支持人员可能需要排查网络、数据库、微服务之间的依赖，查找可能的瓶颈。</li></ul></li><li><strong>解决方案的设计与验证</strong>：<ul><li>二线支持人员会根据故障的根本原因，设计一个解决方案。该方案可能涉及到：<ul><li>系统重启</li><li>应用配置修改</li><li>软件升级或回滚</li><li>增加硬件资源</li></ul></li><li>设计完方案后，二线会进行验证，确保方案有效并不会带来新的问题。</li></ul></li><li><strong>执行修复</strong>：<ul><li>确认解决方案后，二线支持团队会执行修复工作，恢复系统的正常运行。修复过程中，二线支持人员需要确保不影响其他正常运行的服务或系统。</li></ul></li><li><strong>验证与监控</strong>：<ul><li>故障修复后，二线团队会进行验证，确保系统恢复正常，并通过监控工具进行持续跟踪，确认问题没有复发。</li></ul></li><li><strong>报告与反馈</strong>：<ul><li>故障解决后，二线会向一线提供详细的解决方案与报告，帮助一线了解如何避免类似问题的发生。</li><li>记录解决方案和过程，以便后续参考。</li></ul></li><li><strong>总结与知识库更新</strong>：<ul><li>将该故障的详细处理过程、解决方法总结到知识库或文档中，方便未来相似问题的快速解决。</li></ul></li></ol><h3 id=总结-8><strong>总结</strong>
<a class=anchor href=#%e6%80%bb%e7%bb%93-8>#</a></h3><ul><li><strong>一线反馈</strong>给二线后，二线要首先确认问题的性质，收集更多的信息，进行深度分析，设计和执行修复方案。</li><li>整个过程需要根据故障的严重性、影响范围和优先级来进行，有时需要和其他团队（如开发、运维）合作解决问题。</li><li>二线支持不仅仅是修复问题，还要不断总结经验，优化工作流程，提高整体的解决效率。</li></ul><h1 id=离职原因>离职原因？
<a class=anchor href=#%e7%a6%bb%e8%81%8c%e5%8e%9f%e5%9b%a0>#</a></h1><p>家庭原因</p><h1 id=更偏基础运维-系统运维针对业务去运维>更偏基础运维 ，系统运维，针对业务去运维？
<a class=anchor href=#%e6%9b%b4%e5%81%8f%e5%9f%ba%e7%a1%80%e8%bf%90%e7%bb%b4-%e7%b3%bb%e7%bb%9f%e8%bf%90%e7%bb%b4%e9%92%88%e5%af%b9%e4%b8%9a%e5%8a%a1%e5%8e%bb%e8%bf%90%e7%bb%b4>#</a></h1><p><strong>业务运维</strong>和<strong>系统运维</strong>虽然在目标上有重叠，但侧重点和职责不同。下面是两者的主要区别：</p><h3 id=1-定义和重点><strong>1. 定义和重点</strong>
<a class=anchor href=#1-%e5%ae%9a%e4%b9%89%e5%92%8c%e9%87%8d%e7%82%b9>#</a></h3><ul><li><strong>业务运维</strong>：<ul><li><strong>定义</strong>：业务运维主要关注的是通过技术手段支持和优化业务流程，确保业务系统的高可用性、性能、扩展性、安全性等，最大化地满足业务需求。</li><li><strong>重点</strong>：业务运维的重点是根据具体的业务需求来优化系统架构和流程。它侧重于业务层面的支持，比如业务高峰期流量的保障、应用的可用性、数据处理能力等。</li><li><strong>目标</strong>：为业务提供持续、稳定、高效的运行环境，并保证业务系统能够灵活应对业务变动，提供高质量的服务。</li></ul></li><li><strong>系统运维</strong>：<ul><li><strong>定义</strong>：系统运维是指对 IT 基础设施（如服务器、网络设备、存储等）进行日常管理和维护，确保系统的稳定、可用和安全。其职责通常包括硬件、操作系统、数据库、中间件等技术层面的维护。</li><li><strong>重点</strong>：系统运维主要关注底层 IT 资源的管理和优化，如服务器、存储、网络等硬件资源的高效利用，以及操作系统、数据库、应用服务器等的软件维护。</li><li><strong>目标</strong>：确保技术基础设施稳定运行，为上层业务提供支撑，保证系统的正常运行，处理硬件故障、系统漏洞、安全性问题等。</li></ul></li></ul><h3 id=2-主要职责><strong>2. 主要职责</strong>
<a class=anchor href=#2-%e4%b8%bb%e8%a6%81%e8%81%8c%e8%b4%a3>#</a></h3><ul><li><strong>业务运维的职责</strong>：<ul><li>根据业务需求设计系统架构和资源规划，确保业务的高可用性和性能。</li><li>监控与分析业务系统的性能、流量等指标，并进行优化。</li><li>配置和维护与业务紧密相关的应用、服务（如 Web 服务、数据库集群、缓存服务等）。</li><li>管理业务级别的故障排查和恢复，确保业务不中断。</li><li>与开发团队紧密协作，确保部署流程、代码发布等与业务需求相匹配。</li><li>根据业务需求进行容量规划，支持业务增长。</li><li>灾难恢复规划和业务系统备份，确保在发生故障时业务能够快速恢复。</li></ul></li><li><strong>系统运维的职责</strong>：<ul><li>管理和维护 IT 基础设施（服务器、存储、网络等硬件资源）。</li><li>配置和维护操作系统、数据库、中间件等基础软件。</li><li>执行日常监控、故障排查和性能优化。</li><li>确保系统的安全性和稳定性，管理权限和安全策略。</li><li>执行定期备份、补丁管理、日志管理等任务。</li><li>处理硬件故障、系统崩溃、网络延迟等技术问题。</li><li>自动化运维，减少人工操作，提高运维效率。</li></ul></li></ul><h3 id=3-关注点和目标><strong>3. 关注点和目标</strong>
<a class=anchor href=#3-%e5%85%b3%e6%b3%a8%e7%82%b9%e5%92%8c%e7%9b%ae%e6%a0%87>#</a></h3><ul><li><strong>业务运维关注点</strong>：<ul><li><strong>业务需求</strong>：业务运维强调与业务部门的紧密配合，确保技术平台能够支持不断变化的业务需求。通过优化架构和资源配置，保证业务系统能够灵活扩展。</li><li><strong>用户体验</strong>：重点关注最终用户体验，确保系统能够高效响应用户请求，满足高并发、高可用等需求。</li><li><strong>容量规划</strong>：根据业务的增长趋势进行容量规划，避免系统过载或资源浪费。</li></ul></li><li><strong>系统运维关注点</strong>：<ul><li><strong>技术稳定性</strong>：系统运维重点是保障基础设施的稳定运行，关注硬件资源的高效利用，操作系统和中间件的稳定性。</li><li><strong>故障排除</strong>：当发生系统故障时，系统运维负责快速定位问题并恢复服务，避免影响系统的正常运行。</li><li><strong>安全性</strong>：系统运维会进行定期的安全检查，确保系统没有安全漏洞，防止攻击和数据泄露。</li></ul></li></ul><h3 id=4-跨部门协作><strong>4. 跨部门协作</strong>
<a class=anchor href=#4-%e8%b7%a8%e9%83%a8%e9%97%a8%e5%8d%8f%e4%bd%9c>#</a></h3><ul><li><strong>业务运维</strong>：通常需要与 <strong>业务团队</strong>、<strong>产品团队</strong>、<strong>开发团队</strong> 等进行密切的沟通和协作。业务运维人员不仅要了解技术细节，还要理解业务逻辑，以便为业务提供更精确的技术支持。</li><li><strong>系统运维</strong>：通常主要与 <strong>开发团队</strong> 和 <strong>基础设施团队</strong> 协作，关注基础设施的构建和维护。系统运维人员需要理解硬件、操作系统和中间件的工作原理，确保技术平台的稳定性。</li></ul><h3 id=5-运维工具与技术><strong>5. 运维工具与技术</strong>
<a class=anchor href=#5-%e8%bf%90%e7%bb%b4%e5%b7%a5%e5%85%b7%e4%b8%8e%e6%8a%80%e6%9c%af>#</a></h3><ul><li>业务运维的工具与技术：<ul><li><strong>监控工具</strong>：使用业务监控工具，如 Prometheus、Grafana 等，来监控与业务相关的指标（如交易量、响应时间、用户访问量等）。</li><li><strong>日志分析</strong>：使用 ELK Stack、Fluentd 等工具收集和分析业务日志，发现潜在问题。</li><li><strong>自动化部署与发布</strong>：通过 CI/CD 工具（如 Jenkins、GitLab CI 等）自动化代码的构建、测试、部署和发布，减少人工干预。</li></ul></li><li>系统运维的工具与技术：<ul><li><strong>监控工具</strong>：使用系统监控工具，如 Zabbix、Nagios、Prometheus 等，来监控服务器、网络、存储等硬件资源的状态。</li><li><strong>自动化管理工具</strong>：使用 Ansible、Puppet、Chef 等自动化工具进行资源配置、补丁管理、故障恢复等。</li><li><strong>容器与虚拟化管理</strong>：使用 Kubernetes、Docker 等容器化技术进行应用的部署、管理和调度。</li></ul></li></ul><h3 id=总结-9><strong>总结</strong>
<a class=anchor href=#%e6%80%bb%e7%bb%93-9>#</a></h3><ul><li><strong>业务运维</strong>是围绕业务需求和业务系统进行的运维工作，重点是保障业务应用的稳定运行和高效响应。</li><li><strong>系统运维</strong>是关注底层技术基础设施的运维，确保操作系统、网络、硬件等组件的稳定、安全和高效运行。</li></ul><p>两者的目标虽然不同，但最终都是为了确保 IT 系统能够稳定支持业务的运营。业务运维更多关注如何将技术与业务目标对接，而系统运维则更多关注如何管理和维护技术资源以支持上层应用。</p><h1 id=运维团队管理机器多少台>运维团队？管理机器多少台？
<a class=anchor href=#%e8%bf%90%e7%bb%b4%e5%9b%a2%e9%98%9f%e7%ae%a1%e7%90%86%e6%9c%ba%e5%99%a8%e5%a4%9a%e5%b0%91%e5%8f%b0>#</a></h1><h1 id=负责链路分发的业务运维-手机的app的响应-告警-业务需求上万台机器内容分发几千台>负责链路分发的业务运维 手机的app的响应 告警 业务需求，上万台机器，内容分发几千台？
<a class=anchor href=#%e8%b4%9f%e8%b4%a3%e9%93%be%e8%b7%af%e5%88%86%e5%8f%91%e7%9a%84%e4%b8%9a%e5%8a%a1%e8%bf%90%e7%bb%b4-%e6%89%8b%e6%9c%ba%e7%9a%84app%e7%9a%84%e5%93%8d%e5%ba%94-%e5%91%8a%e8%ad%a6-%e4%b8%9a%e5%8a%a1%e9%9c%80%e6%b1%82%e4%b8%8a%e4%b8%87%e5%8f%b0%e6%9c%ba%e5%99%a8%e5%86%85%e5%ae%b9%e5%88%86%e5%8f%91%e5%87%a0%e5%8d%83%e5%8f%b0>#</a></h1><h1 id=链路分发包含业务-一个人维护好几个业务工作很大一部分是这样对接开发的需求一线白屏化操作帮做业务达成目标在项目里面一线偏执行这块>链路分发包含业务 ，一个人维护好几个业务，工作很大一部分是这样，对接开发的需求，一线白屏化操作，帮做业务达成目标，在项目里面，一线偏执行这块。
<a class=anchor href=#%e9%93%be%e8%b7%af%e5%88%86%e5%8f%91%e5%8c%85%e5%90%ab%e4%b8%9a%e5%8a%a1-%e4%b8%80%e4%b8%aa%e4%ba%ba%e7%bb%b4%e6%8a%a4%e5%a5%bd%e5%87%a0%e4%b8%aa%e4%b8%9a%e5%8a%a1%e5%b7%a5%e4%bd%9c%e5%be%88%e5%a4%a7%e4%b8%80%e9%83%a8%e5%88%86%e6%98%af%e8%bf%99%e6%a0%b7%e5%af%b9%e6%8e%a5%e5%bc%80%e5%8f%91%e7%9a%84%e9%9c%80%e6%b1%82%e4%b8%80%e7%ba%bf%e7%99%bd%e5%b1%8f%e5%8c%96%e6%93%8d%e4%bd%9c%e5%b8%ae%e5%81%9a%e4%b8%9a%e5%8a%a1%e8%be%be%e6%88%90%e7%9b%ae%e6%a0%87%e5%9c%a8%e9%a1%b9%e7%9b%ae%e9%87%8c%e9%9d%a2%e4%b8%80%e7%ba%bf%e5%81%8f%e6%89%a7%e8%a1%8c%e8%bf%99%e5%9d%97>#</a></h1><h1 id=团队的管理和文化氛围早上9点-晚上7点9点半-7点加班不强制开会有需要就开>团队的管理和文化氛围？早上9点-晚上7点，9点半-7点，加班不强制，开会有需要就开
<a class=anchor href=#%e5%9b%a2%e9%98%9f%e7%9a%84%e7%ae%a1%e7%90%86%e5%92%8c%e6%96%87%e5%8c%96%e6%b0%9b%e5%9b%b4%e6%97%a9%e4%b8%8a9%e7%82%b9-%e6%99%9a%e4%b8%8a7%e7%82%b99%e7%82%b9%e5%8d%8a-7%e7%82%b9%e5%8a%a0%e7%8f%ad%e4%b8%8d%e5%bc%ba%e5%88%b6%e5%bc%80%e4%bc%9a%e6%9c%89%e9%9c%80%e8%a6%81%e5%b0%b1%e5%bc%80>#</a></h1><h1 id=管理没这么卷>管理没这么卷，
<a class=anchor href=#%e7%ae%a1%e7%90%86%e6%b2%a1%e8%bf%99%e4%b9%88%e5%8d%b7>#</a></h1><h1 id=日常应用发布-变更什么时间操作看业务业务稳定好几周发一次迭代的一周2-3次>日常应用发布 变更？什么时间操作？看业务，业务稳定，好几周发一次，迭代的一周2-3次
<a class=anchor href=#%e6%97%a5%e5%b8%b8%e5%ba%94%e7%94%a8%e5%8f%91%e5%b8%83-%e5%8f%98%e6%9b%b4%e4%bb%80%e4%b9%88%e6%97%b6%e9%97%b4%e6%93%8d%e4%bd%9c%e7%9c%8b%e4%b8%9a%e5%8a%a1%e4%b8%9a%e5%8a%a1%e7%a8%b3%e5%ae%9a%e5%a5%bd%e5%87%a0%e5%91%a8%e5%8f%91%e4%b8%80%e6%ac%a1%e8%bf%ad%e4%bb%a3%e7%9a%84%e4%b8%80%e5%91%a82-3%e6%ac%a1>#</a></h1><h1 id=白天操作晚上不允许变更周一到周五-早上10-下午6点>白天操作，晚上不允许变更，周一到周五 早上10-下午6点。
<a class=anchor href=#%e7%99%bd%e5%a4%a9%e6%93%8d%e4%bd%9c%e6%99%9a%e4%b8%8a%e4%b8%8d%e5%85%81%e8%ae%b8%e5%8f%98%e6%9b%b4%e5%91%a8%e4%b8%80%e5%88%b0%e5%91%a8%e4%ba%94-%e6%97%a9%e4%b8%8a10-%e4%b8%8b%e5%8d%886%e7%82%b9>#</a></h1></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><ul><li><a href=#1-5个-master-节点控制面>1. <strong>5个 Master 节点（控制面）</strong></a></li><li><a href=#2-node-节点工作节点>2. <strong>Node 节点（工作节点）</strong></a></li><li><a href=#3-联邦架构federation>3. <strong>联邦架构（Federation）</strong></a></li><li><a href=#4-ad-bc-分区>4. <strong>AD BC 分区</strong></a></li><li><a href=#5-多-az-跨机房容灾设计>5. <strong>多 AZ 跨机房容灾设计</strong></a></li><li><a href=#6-健康检查和负载均衡>6. <strong>健康检查和负载均衡</strong></a></li><li><a href=#7-灾难恢复与容错>7. <strong>灾难恢复与容错</strong></a></li><li><a href=#8-备份与恢复>8. <strong>备份与恢复</strong></a></li><li><a href=#总结>总结</a></li></ul></li></ul><ul><li><a href=#1-etcd-容灾方案><strong>1. etcd 容灾方案</strong></a><ul><li><a href=#1-etcd-高可用部署><strong>(1) etcd 高可用部署</strong></a></li><li><a href=#2-etcd-备份与恢复><strong>(2) etcd 备份与恢复</strong></a></li><li><a href=#3-etcd-自动故障转移><strong>(3) etcd 自动故障转移</strong></a></li></ul></li><li><a href=#2-kubernetes-控制面组件容灾方案><strong>2. Kubernetes 控制面组件容灾方案</strong></a><ul><li><a href=#1-高可用架构><strong>(1) 高可用架构</strong></a></li><li><a href=#2-监控与自动恢复><strong>(2) 监控与自动恢复</strong></a></li><li><a href=#3-备份与恢复><strong>(3) 备份与恢复</strong></a></li></ul></li><li><a href=#3-故障恢复策略><strong>3. 故障恢复策略</strong></a></li><li><a href=#4-结论><strong>4. 结论</strong></a></li></ul><ul><li><a href=#kubernetes-容灾演练方案设计><strong>Kubernetes 容灾演练方案设计</strong></a></li></ul><ul><li><a href=#-场景-1etcd-故障><strong>🟢 场景 1：etcd 故障</strong></a></li><li><a href=#-场景-2master-节点故障><strong>🟡 场景 2：Master 节点故障</strong></a></li><li><a href=#-场景-3node-计算节点故障><strong>🔴 场景 3：Node 计算节点故障</strong></a></li><li><a href=#-场景-4ingressservice-负载均衡异常><strong>🟠 场景 4：Ingress/Service 负载均衡异常</strong></a></li><li><a href=#-场景-5存储故障><strong>🔵 场景 5：存储故障</strong></a></li></ul><ul><li><a href=#演练发现的问题与改进方案><strong>演练发现的问题与改进方案</strong></a></li></ul><ul><li><ul><li><a href=#-问题-1etcd-选主时间过长影响-api-server-响应><strong>🛑 问题 1：etcd 选主时间过长，影响 API Server 响应</strong></a></li><li><a href=#-问题-2etcd-备份未生效恢复失败><strong>🛑 问题 2：etcd 备份未生效，恢复失败</strong></a></li></ul></li></ul><ul><li><ul><li><a href=#-问题-3kube-apiserver-宕机负载均衡未生效><strong>🛑 问题 3：kube-apiserver 宕机，负载均衡未生效</strong></a></li></ul></li></ul><ul><li><ul><li><a href=#-问题-4节点驱逐后-pod-迁移失败><strong>🛑 问题 4：节点驱逐后 Pod 迁移失败</strong></a></li></ul></li></ul><ul><li><ul><li><a href=#-问题-5ingress-nginx-宕机后业务不可访问><strong>🛑 问题 5：Ingress Nginx 宕机后，业务不可访问</strong></a></li></ul></li></ul><ul><li><ul><li><a href=#-问题-6pvc-绑定失败pod-进入><strong>🛑 问题 6：PVC 绑定失败，Pod 进入 <code>Pending</code> 状态</strong></a></li></ul></li><li><a href=#总结-1><strong>总结</strong></a></li></ul><ul><li><a href=#大规模启动-pod-失败的排查与优化方案><strong>大规模启动 Pod 失败的排查与优化方案</strong></a></li><li><a href=#-1-通过><strong>🛠 1. 通过 <code>kubectl</code> 和 <code>describe</code> 命令排查</strong></a><ul><li><a href=#-查看-pod-状态><strong>📌 查看 Pod 状态</strong></a></li><li><a href=#-获取详细信息><strong>📌 获取详细信息</strong></a></li></ul></li><li><a href=#-2-调度问题pod-无法被分配到节点><strong>🔍 2. 调度问题（Pod 无法被分配到节点）</strong></a><ul><li><a href=#-问题-1集群资源不足><strong>🛑 问题 1：集群资源不足</strong></a></li><li><a href=#-问题-2节点调度限制><strong>🛑 问题 2：节点调度限制</strong></a></li></ul></li><li><a href=#-3-容器创建问题containercreating-卡住><strong>🔍 3. 容器创建问题（ContainerCreating 卡住）</strong></a><ul><li><a href=#-问题-3cni-网络组件异常><strong>🛑 问题 3：CNI 网络组件异常</strong></a></li><li><a href=#-问题-4容器镜像拉取失败><strong>🛑 问题 4：容器镜像拉取失败</strong></a></li></ul></li><li><a href=#-4-存储问题pvc-绑定失败><strong>🔍 4. 存储问题（PVC 绑定失败）</strong></a><ul><li><a href=#-问题-5存储卷挂载失败><strong>🛑 问题 5：存储卷挂载失败</strong></a></li></ul></li><li><a href=#-5-master-组件负载过高><strong>🔍 5. Master 组件负载过高</strong></a><ul><li><a href=#-问题-6api-server-负载过高><strong>🛑 问题 6：API Server 负载过高</strong></a></li></ul></li><li><a href=#-总结><strong>🎯 总结</strong></a></li></ul><ul><li><a href=#kubernetes-中-redis-和-kafka-容器化方案><strong>Kubernetes 中 Redis 和 Kafka 容器化方案</strong></a></li></ul><ul><li><ul><li><a href=#-redis-方案设计><strong>📌 Redis 方案设计</strong></a></li><li><a href=#-方案-1redis-sentinel-高可用><strong>📌 方案 1：Redis Sentinel 高可用</strong></a></li><li><a href=#-方案-2redis-cluster分片><strong>📌 方案 2：Redis Cluster（分片）</strong></a></li></ul></li></ul><ul><li><ul><li><a href=#-kafka-方案设计><strong>📌 Kafka 方案设计</strong></a></li><li><a href=#-kafka-容器化部署><strong>📌 Kafka 容器化部署</strong></a></li></ul></li></ul><ul><li><a href=#-1-数据持久化><strong>📌 1. 数据持久化</strong></a></li><li><a href=#-2-高可用><strong>📌 2. 高可用</strong></a></li><li><a href=#-3-延迟优化><strong>📌 3. 延迟优化</strong></a></li></ul><ul><li><a href=#dragonfly-原理解析><strong>Dragonfly 原理解析</strong></a><ul><li><a href=#-dragonfly-是什么><strong>📌 Dragonfly 是什么？</strong></a></li></ul></li><li><a href=#1-dragonfly-主要组成><strong>1️⃣ Dragonfly 主要组成</strong></a></li><li><a href=#2-dragonfly-的工作原理><strong>2️⃣ Dragonfly 的工作原理</strong></a><ul><li><a href=#-1-镜像文件下载请求><strong>🛠 1. 镜像/文件下载请求</strong></a></li><li><a href=#-2-调度节点scheduler><strong>🛠 2. 调度节点（scheduler）</strong></a></li><li><a href=#-3-p2p-下载><strong>🛠 3. P2P 下载</strong></a></li><li><a href=#-4-本地缓存><strong>🛠 4. 本地缓存</strong></a></li></ul></li><li><a href=#3-dragonfly-的-p2p-下载机制><strong>3️⃣ Dragonfly 的 P2P 下载机制</strong></a><ul><li><a href=#-p2p-分块下载><strong>🔥 P2P 分块下载</strong></a></li></ul></li><li><a href=#4-dragonfly-高可用架构><strong>4️⃣ Dragonfly 高可用架构</strong></a><ul><li><a href=#-多级缓存架构><strong>🌍 多级缓存架构</strong></a></li><li><a href=#-关键特性><strong>⚡ 关键特性</strong></a></li></ul></li><li><a href=#5-dragonfly-在-kubernetes-集群中的应用><strong>5️⃣ Dragonfly 在 Kubernetes 集群中的应用</strong></a><ul><li><a href=#-在-kubernetes-使用-dragonfly><strong>🔥 在 Kubernetes 使用 Dragonfly</strong></a></li></ul></li><li><a href=#6-dragonfly-对比其他加速方案><strong>6️⃣ Dragonfly 对比其他加速方案</strong></a></li><li><a href=#-总结-2><strong>✅ 总结</strong></a></li></ul><ul><li><a href=#监控日活千万级-app-的监控体系设计><strong>监控日活千万级 App 的监控体系设计</strong></a></li><li><a href=#1-监控体系设计><strong>1. 监控体系设计</strong></a><ul><li><a href=#1监控架构><strong>（1）监控架构</strong></a></li></ul></li><li><a href=#2-关键监控指标><strong>2. 关键监控指标</strong></a><ul><li><a href=#1业务层监控><strong>（1）业务层监控</strong></a></li><li><a href=#2系统层监控><strong>（2）系统层监控</strong></a></li><li><a href=#3应用层监控apm><strong>（3）应用层监控（APM）</strong></a></li><li><a href=#4日志--异常监控><strong>（4）日志 & 异常监控</strong></a></li></ul></li><li><a href=#3-监控工具选型><strong>3. 监控工具选型</strong></a></li><li><a href=#4-告警策略><strong>4. 告警策略</strong></a><ul><li><a href=#1告警分级><strong>（1）告警分级</strong></a></li><li><a href=#2自动化运维><strong>（2）自动化运维</strong></a></li></ul></li><li><a href=#5-典型故障场景--处理><strong>5. 典型故障场景 & 处理</strong></a></li><li><a href=#总结-2><strong>总结</strong></a></li></ul><ul><li><ul><li><a href=#1-增加-prometheus-的内存限制>1. <strong>增加 Prometheus 的内存限制</strong></a></li><li><a href=#2-调整>2. <strong>调整 <code>max-memory</code> 限制</strong></a></li><li><a href=#3-减少-prometheus-存储数据的量>3. <strong>减少 Prometheus 存储数据的量</strong></a></li><li><a href=#4-使用较小的>4. <strong>使用较小的 <code>scrape_interval</code></strong></a></li><li><a href=#5-优化数据源和查询>5. <strong>优化数据源和查询</strong></a></li><li><a href=#6-启用>6. <strong>启用 <code>remote_write</code> 和外部存储</strong></a></li><li><a href=#7-提高-prometheus-性能>7. <strong>提高 Prometheus 性能</strong></a></li><li><a href=#8-扩展-prometheus-集群>8. <strong>扩展 Prometheus 集群</strong></a></li><li><a href=#总结-3>总结：</a></li><li><a href=#prometheus-联邦的核心概念><strong>Prometheus 联邦的核心概念</strong></a></li><li><a href=#联邦的工作原理><strong>联邦的工作原理</strong></a></li><li><a href=#联邦的典型用途><strong>联邦的典型用途</strong></a></li><li><a href=#prometheus-联邦的配置><strong>Prometheus 联邦的配置</strong></a></li><li><a href=#关键参数解释><strong>关键参数解释</strong></a></li><li><a href=#优势><strong>优势</strong></a></li><li><a href=#限制和挑战><strong>限制和挑战</strong></a></li><li><a href=#总结-4><strong>总结</strong></a></li></ul></li></ul><ul><li><ul><li><a href=#部署二进制集群的步骤><strong>部署二进制集群的步骤：</strong></a></li><li><a href=#保证-playbook-的幂等性><strong>保证 Playbook 的幂等性：</strong></a></li><li><a href=#保证-playbook-的安全性><strong>保证 Playbook 的安全性：</strong></a></li></ul></li></ul><ul><li><ul><li><a href=#灰度发布流水线设计思路><strong>灰度发布流水线设计思路</strong></a></li><li><a href=#设计灰度发布流水线的步骤><strong>设计灰度发布流水线的步骤：</strong></a></li><li><a href=#灰度发布中的关键技术和工具><strong>灰度发布中的关键技术和工具：</strong></a></li><li><a href=#总结-5><strong>总结</strong>：</a></li></ul></li></ul><ul><li><a href=#流量洪峰自动扩缩容方案设计-><strong>流量洪峰自动扩缩容方案设计</strong> 🚀</a></li><li><a href=#1-流量洪峰的特征><strong>1. 流量洪峰的特征</strong></a></li><li><a href=#2-自动扩缩容架构><strong>2. 自动扩缩容架构</strong></a></li><li><a href=#3-关键扩缩容策略><strong>3. 关键扩缩容策略</strong></a><ul><li><a href=#1pod-水平扩展hpa><strong>（1）Pod 水平扩展（HPA）</strong></a></li><li><a href=#2pod-垂直扩展vpa><strong>（2）Pod 垂直扩展（VPA）</strong></a></li><li><a href=#3集群节点扩展cluster-autoscaler><strong>（3）集群节点扩展（Cluster Autoscaler）</strong></a></li><li><a href=#4事件驱动扩缩容keda><strong>（4）事件驱动扩缩容（KEDA）</strong></a></li></ul></li><li><a href=#4-预案--高可用策略><strong>4. 预案 & 高可用策略</strong></a><ul><li><a href=#1流量预案><strong>（1）流量预案</strong></a></li><li><a href=#2故障恢复><strong>（2）故障恢复</strong></a></li></ul></li><li><a href=#5-方案总结><strong>5. 方案总结</strong></a><ul><li><a href=#最终架构><strong>最终架构</strong></a></li></ul></li></ul><ul><li><ul><li><a href=#1-确定视频加载缓慢的表现><strong>1. 确定视频加载缓慢的表现</strong></a></li><li><a href=#2-分析网络链路><strong>2. 分析网络链路</strong></a></li><li><a href=#3-存储层排查><strong>3. 存储层排查</strong></a></li><li><a href=#4-视频文件分析><strong>4. 视频文件分析</strong></a></li><li><a href=#5-客户端排查><strong>5. 客户端排查</strong></a></li><li><a href=#6-流程监控与日志><strong>6. 流程监控与日志</strong></a></li><li><a href=#7-进行压力测试><strong>7. 进行压力测试</strong></a></li><li><a href=#总结-6><strong>总结</strong>：</a></li></ul></li></ul><ul><li><ul><li><a href=#1-根据故障影响范围分类><strong>1. 根据故障影响范围分类</strong></a></li><li><a href=#2-根据故障恢复难易度分类><strong>2. 根据故障恢复难易度分类</strong></a></li><li><a href=#3-根据时间敏感性分类><strong>3. 根据时间敏感性分类</strong></a></li><li><a href=#4-故障分类><strong>4. 故障分类</strong></a></li><li><a href=#5-使用故障管理系统><strong>5. 使用故障管理系统</strong></a></li><li><a href=#6-基于业务价值评估><strong>6. 基于业务价值评估</strong></a></li><li><a href=#7-确定恢复的最小可行方案><strong>7. 确定恢复的最小可行方案</strong></a></li><li><a href=#排查故障的优先级处理流程><strong>排查故障的优先级处理流程</strong>：</a></li><li><a href=#总结-7><strong>总结</strong></a></li></ul></li></ul><ul><li><ul><li><a href=#二线的主要工作职责><strong>二线的主要工作职责</strong>：</a></li><li><a href=#一线反馈到二线后如何解决问题><strong>一线反馈到二线后，如何解决问题？</strong></a></li><li><a href=#总结-8><strong>总结</strong></a></li></ul></li></ul><ul><li><ul><li><a href=#1-定义和重点><strong>1. 定义和重点</strong></a></li><li><a href=#2-主要职责><strong>2. 主要职责</strong></a></li><li><a href=#3-关注点和目标><strong>3. 关注点和目标</strong></a></li><li><a href=#4-跨部门协作><strong>4. 跨部门协作</strong></a></li><li><a href=#5-运维工具与技术><strong>5. 运维工具与技术</strong></a></li><li><a href=#总结-9><strong>总结</strong></a></li></ul></li></ul></nav></div></aside></main></body></html>