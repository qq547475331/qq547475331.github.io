<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content='/*
Copyright 2015 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

// Package deployment contains all the logic for handling Kubernetes Deployments.
// It implements a set of strategies (rolling, recreate) for deploying an application,
// the means to rollback to previous versions, proportional scaling for mitigating
// risk, cleanup policy, and other useful features of Deployments.
package deployment

import (
	"context"
	"fmt"
	"reflect"
	"time"

	apps "k8s.io/api/apps/v1"
	v1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/labels"
	"k8s.io/apimachinery/pkg/types"
	utilruntime "k8s.io/apimachinery/pkg/util/runtime"
	"k8s.io/apimachinery/pkg/util/wait"
	appsinformers "k8s.io/client-go/informers/apps/v1"
	coreinformers "k8s.io/client-go/informers/core/v1"
	clientset "k8s.io/client-go/kubernetes"
	"k8s.io/client-go/kubernetes/scheme"
	v1core "k8s.io/client-go/kubernetes/typed/core/v1"
	appslisters "k8s.io/client-go/listers/apps/v1"
	corelisters "k8s.io/client-go/listers/core/v1"
	"k8s.io/client-go/tools/cache"
	"k8s.io/client-go/tools/record"
	"k8s.io/client-go/util/workqueue"
	"k8s.io/kubernetes/pkg/controller"
	"k8s.io/kubernetes/pkg/controller/deployment/util"
)

const (
	// maxRetries is the number of times a deployment will be retried before it is dropped out of the queue.
	// With the current rate-limiter in use (5ms*2^(maxRetries-1)) the following numbers represent the times
	// a deployment is going to be requeued:
	//
	// 5ms, 10ms, 20ms, 40ms, 80ms, 160ms, 320ms, 640ms, 1.3s, 2.6s, 5.1s, 10.2s, 20.4s, 41s, 82s
	maxRetries = 15
)

// controllerKind contains the schema.GroupVersionKind for this controller type.
var controllerKind = apps.SchemeGroupVersion.WithKind("Deployment")

// DeploymentController is responsible for synchronizing Deployment objects stored
// in the system with actual running replica sets and pods.
type DeploymentController struct {
	// rsControl is used for adopting/releasing replica sets.
	rsControl controller.RSControlInterface
	client    clientset.Interface

	eventBroadcaster record.EventBroadcaster
	eventRecorder    record.EventRecorder

	// To allow injection of syncDeployment for testing.
	syncHandler func(ctx context.Context, dKey string) error
	// used for unit testing
	enqueueDeployment func(deployment *apps.Deployment)

	// dLister can list/get deployments from the shared informer&#39;s store
	dLister appslisters.DeploymentLister
	// rsLister can list/get replica sets from the shared informer&#39;s store
	rsLister appslisters.ReplicaSetLister
	// podLister can list/get pods from the shared informer&#39;s store
	podLister corelisters.PodLister

	// dListerSynced returns true if the Deployment store has been synced at least once.
	// Added as a member to the struct to allow injection for testing.
	dListerSynced cache.InformerSynced
	// rsListerSynced returns true if the ReplicaSet store has been synced at least once.
	// Added as a member to the struct to allow injection for testing.
	rsListerSynced cache.InformerSynced
	// podListerSynced returns true if the pod store has been synced at least once.
	// Added as a member to the struct to allow injection for testing.
	podListerSynced cache.InformerSynced

	// Deployments that need to be synced
	queue workqueue.RateLimitingInterface
}

// NewDeploymentController creates a new DeploymentController.
func NewDeploymentController(ctx context.Context, dInformer appsinformers.DeploymentInformer, rsInformer appsinformers.ReplicaSetInformer, podInformer coreinformers.PodInformer, client clientset.Interface) (*DeploymentController, error) {
	eventBroadcaster := record.NewBroadcaster(record.WithContext(ctx))
	logger := klog.FromContext(ctx)
	dc := &amp;DeploymentController{
		client:           client,
		eventBroadcaster: eventBroadcaster,
		eventRecorder:    eventBroadcaster.NewRecorder(scheme.Scheme, v1.EventSource{Component: "deployment-controller"}),
		queue:            workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), "deployment"),
	}
	dc.rsControl = controller.RealRSControl{
		KubeClient: client,
		Recorder:   dc.eventRecorder,
	}
	//该函数用于创建一个新的DeploymentController。
	//- 接收上下文ctx以及多个informer和client作为参数。
	//- 创建一个新的事件广播器eventBroadcaster，并从上下文ctx中获取日志记录器logger 。
	//- 初始化一个DeploymentController结构体实例dc，包括client、eventBroadcaster、eventRecorder、queue和rsControl字段。
	//- dc.rsControl使用controller.RealRSControl结构体初始化，包括KubeClient和Recorder字段。
	//- 返回创建的dc实例和可能出现的错误。

	dInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
		AddFunc: func(obj interface{}) {
			dc.addDeployment(logger, obj)
		},
		UpdateFunc: func(oldObj, newObj interface{}) {
			dc.updateDeployment(logger, oldObj, newObj)
		},
		// This will enter the sync loop and no-op, because the deployment has been deleted from the store.
		DeleteFunc: func(obj interface{}) {
			dc.deleteDeployment(logger, obj)
		},
	})
	rsInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
		AddFunc: func(obj interface{}) {
			dc.addReplicaSet(logger, obj)
		},
		UpdateFunc: func(oldObj, newObj interface{}) {
			dc.updateReplicaSet(logger, oldObj, newObj)
		},
		DeleteFunc: func(obj interface{}) {
			dc.deleteReplicaSet(logger, obj)
		},
	})
	podInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
		DeleteFunc: func(obj interface{}) {
			dc.deletePod(logger, obj)
		},
	})
	//这段代码定义了三个事件处理器，分别用于处理deployment、replicaSet和pod的添加、更新和删除事件。
	//每个事件处理器都调用了对应的方法，例如对于deployment的添加事件，调用了dc.addDeployment(logger, obj)方法。
	//这些方法的具体实现可以根据具体业务逻辑进行编写。

	dc.syncHandler = dc.syncDeployment
	dc.enqueueDeployment = dc.enqueue

	dc.dLister = dInformer.Lister()
	dc.rsLister = rsInformer.Lister()
	dc.podLister = podInformer.Lister()
	dc.dListerSynced = dInformer.Informer().HasSynced
	dc.rsListerSynced = rsInformer.Informer().HasSynced
	dc.podListerSynced = podInformer.Informer().HasSynced
	return dc, nil
}

//这段代码是Go语言中的函数，主要功能是设置和初始化一个名为dc的对象，并返回该对象和nil。
//- 首先，将dc.syncHandler设置为dc.syncDeployment，将dc.enqueueDeployment设置为dc.enqueue。
//- 然后，通过dInformer、rsInformer和podInformer的Lister()方法，分别将dc.dLister、dc.rsLister和dc.podLister设置为相应的列表器。
//- 接着，通过dInformer、rsInformer和podInformer的Informer().HasSynced方法，分别将dc.dListerSynced、dc.rsListerSynced和dc.podListerSynced设置为相应的同步状态检查函数。
//- 最后，返回设置好的dc对象和nil。  这段代码主要涉及到对象的设置和初始化操作，使用了多个列表器和同步状态检查函数来管理不同资源的信息。

// Run begins watching and syncing.
func (dc *DeploymentController) Run(ctx context.Context, workers int) {
	defer utilruntime.HandleCrash()

	// Start events processing pipeline.
	dc.eventBroadcaster.StartStructuredLogging(3)
	dc.eventBroadcaster.StartRecordingToSink(&amp;v1core.EventSinkImpl{Interface: dc.client.CoreV1().Events("")})
	defer dc.eventBroadcaster.Shutdown()

	defer dc.queue.ShutDown()

	logger := klog.FromContext(ctx)
	logger.Info("Starting controller", "controller", "deployment")
	defer logger.Info("Shutting down controller", "controller", "deployment")

	if !cache.WaitForNamedCacheSync("deployment", ctx.Done(), dc.dListerSynced, dc.rsListerSynced, dc.podListerSynced) {
		return
	}

	for i := 0; i < workers; i++ {
		go wait.UntilWithContext(ctx, dc.worker, time.Second)
	}

	<-ctx.Done()
}

//该函数是DeploymentController类型的Run方法，用于启动部署控制器的监视和同步操作。
//- 首先，函数通过defer语句句柄处理崩溃情况。
//- 然后，启动事件处理管道，设置日志记录级别和目标。
//- 接着，通过defer语句关闭事件处理管道。
//- 然后，记录日志信息，表示控制器开始启动。
//- 接着，使用cache.WaitForNamedCacheSync函数等待缓存同步完成。
//- 然后，通过循环创建多个goroutine，并调用dc.worker函数进行工作。
//- 最后，等待上下文完成，并返回。
//该函数的主要功能是启动部署控制器，并使其开始监视和同步操作。

func (dc *DeploymentController) addDeployment(logger klog.Logger, obj interface{}) {
	d := obj.(*apps.Deployment)
	logger.V(4).Info("Adding deployment", "deployment", klog.KObj(d))
	dc.enqueueDeployment(d)
}

func (dc *DeploymentController) updateDeployment(logger klog.Logger, old, cur interface{}) {
	oldD := old.(*apps.Deployment)
	curD := cur.(*apps.Deployment)
	logger.V(4).Info("Updating deployment", "deployment", klog.KObj(oldD))
	dc.enqueueDeployment(curD)
}

//这两个函数都是DeploymentController的方法，用于处理Deployment的添加和更新事件。
//- addDeployment方法接收一个logger和一个obj参数，其中obj是通过类型断言转换为*apps.Deployment类型的。
//该方法首先使用logger记录添加deployment的日志信息，然后调用dc.enqueueDeployment方法将该deployment加入队列中，以便后续处理。
//- updateDeployment方法与addDeployment方法类似，但它接收的是旧的和新的Deployment对象。该方法使用logger记录更新deployment的日志信息，
//并将新的Deployment对象加入队列中进行处理。

func (dc *DeploymentController) deleteDeployment(logger klog.Logger, obj interface{}) {
	d, ok := obj.(*apps.Deployment)
	if !ok {
		tombstone, ok := obj.(cache.DeletedFinalStateUnknown)
		if !ok {
			utilruntime.HandleError(fmt.Errorf("couldn&#39;t get object from tombstone %#v", obj))
			return
		}
		d, ok = tombstone.Obj.(*apps.Deployment)
		if !ok {
			utilruntime.HandleError(fmt.Errorf("tombstone contained object that is not a Deployment %#v", obj))
			return
		}
	}
	logger.V(4).Info("Deleting deployment", "deployment", klog.KObj(d))
	dc.enqueueDeployment(d)
}

//该函数是一个Go语言函数，它定义了一个名为deleteDeployment的方法，该方法接受一个logger和一个obj参数，并没有任何返回值。
//该方法主要用于从一个部署（Deployment）中删除对象。
//首先，函数会尝试将obj参数断言为apps.Deployment类型，并检查断言是否成功。如果断言失败，
//则会尝试将obj参数断言为cache.DeletedFinalStateUnknown类型。如果这个断言也失败了，函数会记录一个错误信息并返回。
//如果断言成功，则会尝试从tombstone中获取对象，并再次断言该对象是否为apps.Deployment类型。如果断言失败，则会记录一个错误信息并返回。
//如果成功断言出对象为apps.Deployment类型，则会使用logger记录一条信息，表示正在删除该部署，
//并调用dc.enqueueDeployment方法将该部署加入队列，以便进一步处理。

// addReplicaSet enqueues the deployment that manages a ReplicaSet when the ReplicaSet is created.
func (dc *DeploymentController) addReplicaSet(logger klog.Logger, obj interface{}) {
	rs := obj.(*apps.ReplicaSet)

	if rs.DeletionTimestamp != nil {
		// On a restart of the controller manager, it&#39;s possible for an object to
		// show up in a state that is already pending deletion.
		dc.deleteReplicaSet(logger, rs)
		return
	}
	// If it has a ControllerRef, that&#39;s all that matters.
	if controllerRef := metav1.GetControllerOf(rs); controllerRef != nil {
		d := dc.resolveControllerRef(rs.Namespace, controllerRef)
		if d == nil {
			return
		}
		logger.V(4).Info("ReplicaSet added", "replicaSet", klog.KObj(rs))
		dc.enqueueDeployment(d)
		return
	}
	//该函数是Go语言编写的，用于在创建ReplicaSet时，将管理该ReplicaSet的Deployment加入队列中。
	//函数首先判断传入的obj对象是否为ReplicaSet类型，并通过判断ReplicaSet的DeletionTimestamp是否为空来确定是否需要删除该ReplicaSet。
	//如果ReplicaSet有ControllerRef，则通过resolveControllerRef函数解析出对应的Deployment，并将其加入队列中。

	// Otherwise, it&#39;s an orphan. Get a list of all matching Deployments and sync
	// them to see if anyone wants to adopt it.
	ds := dc.getDeploymentsForReplicaSet(logger, rs)
	if len(ds) == 0 {
		return
	}
	logger.V(4).Info("Orphan ReplicaSet added", "replicaSet", klog.KObj(rs))
	for _, d := range ds {
		dc.enqueueDeployment(d)
	}
}

//该函数主要处理孤儿ReplicaSet的情况。
//它会获取与该ReplicaSet匹配的所有Deployment列表，并尝试同步这些Deployment，看是否有Deployment愿意采用该ReplicaSet。
//如果找到了对应的Deployment，则将其加入到队列中以便进一步处理。

// getDeploymentsForReplicaSet returns a list of Deployments that potentially
// match a ReplicaSet.
func (dc *DeploymentController) getDeploymentsForReplicaSet(logger klog.Logger, rs *apps.ReplicaSet) []*apps.Deployment {
	deployments, err := util.GetDeploymentsForReplicaSet(dc.dLister, rs)
	if err != nil || len(deployments) == 0 {
		return nil
	}
	// Because all ReplicaSet&#39;s belonging to a deployment should have a unique label key,
	// there should never be more than one deployment returned by the above method.
	// If that happens we should probably dynamically repair the situation by ultimately
	// trying to clean up one of the controllers, for now we just return the older one
	if len(deployments) > 1 {
		// ControllerRef will ensure we don&#39;t do anything crazy, but more than one
		// item in this list nevertheless constitutes user error.
		logger.V(4).Info("user error! more than one deployment is selecting replica set",
			"replicaSet", klog.KObj(rs), "labels", rs.Labels, "deployment", klog.KObj(deployments[0]))
	}
	return deployments
}

//函数用于获取与给定ReplicaSet匹配的所有Deployment列表。
//它首先调用util.GetDeploymentsForReplicaSet函数来获取匹配的Deployment列表，如果出现错误或列表为空，则返回nil。
//如果获取的Deployment列表长度大于1，则记录错误日志，并返回列表中的第一个Deployment。
//在正常情况下，返回获取的Deployment列表。

// updateReplicaSet figures out what deployment(s) manage a ReplicaSet when the ReplicaSet
// is updated and wake them up. If the anything of the ReplicaSets have changed, we need to
// awaken both the old and new deployments. old and cur must be *apps.ReplicaSet
// types.
func (dc *DeploymentController) updateReplicaSet(logger klog.Logger, old, cur interface{}) {
	curRS := cur.(*apps.ReplicaSet)
	oldRS := old.(*apps.ReplicaSet)
	if curRS.ResourceVersion == oldRS.ResourceVersion {
		// Periodic resync will send update events for all known replica sets.
		// Two different versions of the same replica set will always have different RVs.
		return
	}
	//该函数用于在更新ReplicaSet时，确定由哪个部署管理该ReplicaSet，并唤醒它们。
	//如果ReplicaSet的任何内容发生变化，需要唤醒旧的和新的部署。old和cur必须是指向apps.ReplicaSet类型的指针。
	//函数首先将传入的old和cur参数转换为*apps.ReplicaSet类型。
	//然后，它比较两个ReplicaSet的ResourceVersion字段。
	//如果它们相等，则表示这是周期性同步发送的更新事件，而对于同一ReplicaSet的两个不同版本，它们的ResourceVersion总会有不同的值。
	//在这种情况下，函数直接返回，不做任何处理。

	curControllerRef := metav1.GetControllerOf(curRS)
	oldControllerRef := metav1.GetControllerOf(oldRS)
	controllerRefChanged := !reflect.DeepEqual(curControllerRef, oldControllerRef)
	if controllerRefChanged && oldControllerRef != nil {
		// The ControllerRef was changed. Sync the old controller, if any.
		if d := dc.resolveControllerRef(oldRS.Namespace, oldControllerRef); d != nil {
			dc.enqueueDeployment(d)
		}
	}
	//这段Go代码主要关注于检查和处理两个ReplicaSet（curRS和oldRS）的ControllerRef是否发生变化，并根据变化情况同步旧的控制器。
	//1. 首先，通过metav1.GetControllerOf函数获取curRS和oldRS的ControllerRef。
	//2. 然后，使用reflect.DeepEqual函数比较curControllerRef和oldControllerRef是否相等。
	//3. 如果两个ControllerRef不相等且oldControllerRef不为nil，则认为ControllerRef发生了变化。
	//4. 当ControllerRef发生变化时，需要同步旧的控制器。通过dc.resolveControllerRef函数解析oldRS的命名空间和ControllerRef，
	//得到对应的Deployment。
	//5. 最后，如果解析成功（d!=nil），则将该Deployment加入到调度队列中，以便进一步处理。

	// If it has a ControllerRef, that&#39;s all that matters.
	if curControllerRef != nil {
		d := dc.resolveControllerRef(curRS.Namespace, curControllerRef)
		if d == nil {
			return
		}
		logger.V(4).Info("ReplicaSet updated", "replicaSet", klog.KObj(curRS))
		dc.enqueueDeployment(d)
		return
	}
	//这个Go函数主要检查当前的curControllerRef是否为nil。
	//如果不为nil，则通过dc.resolveControllerRef方法解析curControllerRef，并检查解析结果是否为nil。
	//如果不为nil，则记录日志并使用dc.enqueueDeployment方法将解析结果加入到队列中。

	// Otherwise, it&#39;s an orphan. If anything changed, sync matching controllers
	// to see if anyone wants to adopt it now.
	labelChanged := !reflect.DeepEqual(curRS.Labels, oldRS.Labels)
	if labelChanged || controllerRefChanged {
		ds := dc.getDeploymentsForReplicaSet(logger, curRS)
		if len(ds) == 0 {
			return
		}
		logger.V(4).Info("Orphan ReplicaSet updated", "replicaSet", klog.KObj(curRS))
		for _, d := range ds {
			dc.enqueueDeployment(d)
		}
	}
}

//这段Go代码是处理孤儿ReplicaSet的逻辑。
//如果ReplicaSet的标签或控制器引用发生了变化，它会尝试同步匹配的控制器，以查看是否有控制器愿意现在采用它。具体步骤如下：
//1. 检查当前ReplicaSet的标签和旧ReplicaSet的标签是否相等，如果不相等，则标记标签发生变化；
//2. 检查控制器引用是否发生变化；
//3. 如果标签发生变化或控制器引用发生变化，则获取当前ReplicaSet对应的部署列表；
//4. 如果部署列表为空，则直接返回；
//5. 输出日志信息，表示孤儿ReplicaSet已更新；
//6. 遍历部署列表，将每个部署对象加入到队列中，以便进一步处理。

// deleteReplicaSet enqueues the deployment that manages a ReplicaSet when
// the ReplicaSet is deleted. obj could be an *apps.ReplicaSet, or
// a DeletionFinalStateUnknown marker item.
func (dc *DeploymentController) deleteReplicaSet(logger klog.Logger, obj interface{}) {
	rs, ok := obj.(*apps.ReplicaSet)
	//该函数是Go语言编写的，属于DeploymentController类型的一个方法，方法名为deleteReplicaSet。
	//该方法接收一个logger和一个obj参数，其中logger用于记录日志，obj是一个接口类型，
	//可以是*apps.ReplicaSet类型或者DeletionFinalStateUnknown标记项。
	//方法的主要功能是从obj中解析出*apps.ReplicaSet类型的rs，并判断解析是否成功。

	// When a delete is dropped, the relist will notice a pod in the store not
	// in the list, leading to the insertion of a tombstone object which contains
	// the deleted key/value. Note that this value might be stale. If the ReplicaSet
	// changed labels the new deployment will not be woken up till the periodic resync.
	if !ok {
		tombstone, ok := obj.(cache.DeletedFinalStateUnknown)
		if !ok {
			utilruntime.HandleError(fmt.Errorf("couldn&#39;t get object from tombstone %#v", obj))
			return
		}
		rs, ok = tombstone.Obj.(*apps.ReplicaSet)
		if !ok {
			utilruntime.HandleError(fmt.Errorf("tombstone contained object that is not a ReplicaSet %#v", obj))
			return
		}
	}
	//这段Go代码是处理从存储中获取对象时，如果对象已被删除的情况。
	//如果获取对象失败，会检查对象是否是一个 tombstone 对象（即已被删除的对象）。
	//如果是 tombstone 对象，则尝试从 tombstone 中恢复被删除的对象。如果恢复失败，则记录错误信息。

	controllerRef := metav1.GetControllerOf(rs)
	if controllerRef == nil {
		// No controller should care about orphans being deleted.
		return
	}
	d := dc.resolveControllerRef(rs.Namespace, controllerRef)
	if d == nil {
		return
	}
	logger.V(4).Info("ReplicaSet deleted", "replicaSet", klog.KObj(rs))
	dc.enqueueDeployment(d)
}

//该函数主要实现当一个ReplicaSet被删除时，检查是否有对应的Deployment控制器，并将该Deployment加入到队列中以便进一步处理。具体流程如下：
//1. 通过metav1.GetControllerOf(rs)获取ReplicaSet的控制器引用；
//2. 如果控制器引用为空，则表示没有对应的Deployment控制器，直接返回；
//3. 调用dc.resolveControllerRef(rs.Namespace, controllerRef)解析控制器引用，获取对应的Deployment对象；
//4. 如果解析失败或返回的Deployment对象为空，则直接返回；
//5. 使用logger.V(4).Info记录日志，表示ReplicaSet已被删除；
//6. 调用dc.enqueueDeployment(d)将对应的Deployment对象加入到队列中，以便进一步处理。

// deletePod will enqueue a Recreate Deployment once all of its pods have stopped running.
func (dc *DeploymentController) deletePod(logger klog.Logger, obj interface{}) {
	pod, ok := obj.(*v1.Pod)
	//该函数是一个Go语言函数，名为deletePod，它属于DeploymentController类型。
	//函数通过传入的logger和obj参数，将一个Recreate Deployment加入队列，但只有当该Deployment的所有Pod都停止运行时才会执行。
	//函数首先尝试将obj参数断言为*v1.Pod类型，并检查断言是否成功。

	// When a delete is dropped, the relist will notice a pod in the store not
	// in the list, leading to the insertion of a tombstone object which contains
	// the deleted key/value. Note that this value might be stale. If the Pod
	// changed labels the new deployment will not be woken up till the periodic resync.
	if !ok {
		tombstone, ok := obj.(cache.DeletedFinalStateUnknown)
		if !ok {
			utilruntime.HandleError(fmt.Errorf("couldn&#39;t get object from tombstone %#v", obj))
			return
		}
		pod, ok = tombstone.Obj.(*v1.Pod)
		if !ok {
			utilruntime.HandleError(fmt.Errorf("tombstone contained object that is not a pod %#v", obj))
			return
		}
	}
	d := dc.getDeploymentForPod(logger, pod)
	if d == nil {
		return
	}
	logger.V(4).Info("Pod deleted", "pod", klog.KObj(pod))
	if d.Spec.Strategy.Type == apps.RecreateDeploymentStrategyType {
		// Sync if this Deployment now has no more Pods.
		rsList, err := util.ListReplicaSets(d, util.RsListFromClient(dc.client.AppsV1()))
		if err != nil {
			return
		}
		podMap, err := dc.getPodMapForDeployment(d, rsList)
		if err != nil {
			return
		}
		numPods := 0
		for _, podList := range podMap {
			numPods += len(podList)
		}
		if numPods == 0 {
			dc.enqueueDeployment(d)
		}
	}
}

//这段Go代码是用于处理Kubernetes中Pod删除事件的逻辑。
//- 当一个Pod被删除时，如果控制器（例如Deployment）没有及时收到该事件，就会在存储中注意到这个Pod不在列表中，
//从而插入一个包含删除的键值对的墓碑对象（tombstone object）。
//- 如果墓碑对象中的对象不是Pod类型，函数会记录错误并返回。
//- 函数会尝试获取与该Pod关联的Deployment对象，如果获取不到则直接返回。
//- 如果该Deployment的策略是Recreate类型，则会检查该Deployment下是否已经没有Pod了。
//- 如果没有Pod了，则将该Deployment加入到队列中，以便进一步处理。
//这段代码的主要目的是确保在Pod被删除时，与之关联的Deployment能够及时地进行同步和更新。

func (dc *DeploymentController) enqueue(deployment *apps.Deployment) {
	key, err := controller.KeyFunc(deployment)
	if err != nil {
		utilruntime.HandleError(fmt.Errorf("couldn&#39;t get key for object %#v: %v", deployment, err))
		return
	}

	dc.queue.Add(key)
}

//该函数是一个Go语言的方法，定义在一个名为DeploymentController的结构体类型上。
//方法名为enqueue，它接收一个参数deployment，类型为*apps.Deployment，表示一个Kubernetes部署对象的指针。
//该方法的主要功能是将给定的部署对象加入到一个队列中，以便后续处理。具体步骤如下：
//1. 调用controller.KeyFunc(deployment)函数，尝试获取部署对象的键值（通常是一个字符串），用于在队列中唯一标识该对象。
//2. 如果获取键值时出现错误，通过utilruntime.HandleError函数处理错误，并打印错误信息。然后直接返回，不将对象加入队列。
//3. 如果成功获取了键值，将其添加到dc.queue（一个队列对象）中，以便后续处理。
//总结：该方法用于将一个Kubernetes部署对象加入到队列中，以便后续进行处理。

func (dc *DeploymentController) enqueueRateLimited(deployment *apps.Deployment) {
	key, err := controller.KeyFunc(deployment)
	if err != nil {
		utilruntime.HandleError(fmt.Errorf("couldn&#39;t get key for object %#v: %v", deployment, err))
		return
	}

	dc.queue.AddRateLimited(key)
}

//该函数用于将指定的部署对象加入到队列中，并对其进行速率限制。
//首先，通过调用controller.KeyFunc方法获取该部署对象的键值，如果获取失败，则通过utilruntime.HandleError方法记录错误信息并返回。
//如果获取成功，则调用dc.queue.AddRateLimited方法将键值加入到队列中，并对其进行速率限制。

// enqueueAfter will enqueue a deployment after the provided amount of time.
func (dc *DeploymentController) enqueueAfter(deployment *apps.Deployment, after time.Duration) {
	key, err := controller.KeyFunc(deployment)
	if err != nil {
		utilruntime.HandleError(fmt.Errorf("couldn&#39;t get key for object %#v: %v", deployment, err))
		return
	}

	dc.queue.AddAfter(key, after)
}

//该函数将一个部署对象加入到队列中，但会在指定的时间延迟后才执行。
//首先，函数通过调用controller.KeyFunc方法获取部署对象的键值，如果获取失败，则通过utilruntime.HandleError方法记录错误信息并返回。
//接着，函数使用dc.queue.AddAfter方法将键值加入到队列中，并指定延迟执行的时间。

// getDeploymentForPod returns the deployment managing the given Pod.
func (dc *DeploymentController) getDeploymentForPod(logger klog.Logger, pod *v1.Pod) *apps.Deployment {
	// Find the owning replica set
	var rs *apps.ReplicaSet
	var err error
	controllerRef := metav1.GetControllerOf(pod)
	if controllerRef == nil {
		// No controller owns this Pod.
		return nil
	}
	if controllerRef.Kind != apps.SchemeGroupVersion.WithKind("ReplicaSet").Kind {
		// Not a pod owned by a replica set.
		return nil
	}
	rs, err = dc.rsLister.ReplicaSets(pod.Namespace).Get(controllerRef.Name)
	if err != nil || rs.UID != controllerRef.UID {
		logger.V(4).Info("Cannot get replicaset for pod", "ownerReference", controllerRef.Name, "pod", klog.KObj(pod), "err", err)
		return nil
	}
	//该函数用于获取管理给定Pod的Deployment。
	//- 首先，它查找拥有该Pod的ReplicaSet。
	//- 如果找不到拥有者的Pod，则返回nil。 -
	//如果Pod的拥有者不是ReplicaSet，则返回nil。
	//- 然后，尝试根据Pod的拥有者名称获取ReplicaSet。
	//- 如果获取失败或获取到的ReplicaSet的UID与拥有者的UID不匹配，则返回nil。
	//- 最后，返回获取到的Deployment。

	// Now find the Deployment that owns that ReplicaSet.
	controllerRef = metav1.GetControllerOf(rs)
	if controllerRef == nil {
		return nil
	}
	return dc.resolveControllerRef(rs.Namespace, controllerRef)
}

//这个函数的作用是通过 ReplicaSet 的 controllerRef 找到对应的 Deployment。
//首先通过 metav1.GetControllerOf(rs) 获取到 controllerRef，
//如果 controllerRef 为空则返回 nil，否则调用 dc.resolveControllerRef(rs.Namespace, controllerRef) 解析并返回对应的 Deployment。

// resolveControllerRef returns the controller referenced by a ControllerRef,
// or nil if the ControllerRef could not be resolved to a matching controller
// of the correct Kind.
func (dc *DeploymentController) resolveControllerRef(namespace string, controllerRef *metav1.OwnerReference) *apps.Deployment {
	// We can&#39;t look up by UID, so look up by Name and then verify UID.
	// Don&#39;t even try to look up by Name if it&#39;s the wrong Kind.
	if controllerRef.Kind != controllerKind.Kind {
		return nil
	}
	d, err := dc.dLister.Deployments(namespace).Get(controllerRef.Name)
	if err != nil {
		return nil
	}
	if d.UID != controllerRef.UID {
		// The controller we found with this Name is not the same one that the
		// ControllerRef points to.
		return nil
	}
	return d
}

//该函数是一个Go语言函数，用于解析一个ControllerRef引用所指向的控制器，如果无法解析出正确的控制器，则返回nil。
//函数接受三个参数： - namespace：字符串类型，表示命名空间。
//- controllerRef：指向metav1.OwnerReference类型的指针。
//函数返回一个指向apps.Deployment类型的指针。
//函数的主要步骤如下：
//1. 首先，检查controllerRef的Kind属性是否与controllerKind.Kind相等，如果不相等，则直接返回nil。
//2. 如果controllerRef的Kind属性与controllerKind.Kind相等，则通过dc.dLister.Deployments(namespace).Get(controllerRef.Name)获取具有相同名称的部署对象。
//3. 如果获取部署对象时出现错误，则返回nil。
//4. 最后，检查获取到的部署对象的UID属性是否与controllerRef的UID属性相等，如果不相等，则返回nil，否则返回该部署对象。

// worker runs a worker thread that just dequeues items, processes them, and marks them done.
// It enforces that the syncHandler is never invoked concurrently with the same key.
func (dc *DeploymentController) worker(ctx context.Context) {
	for dc.processNextWorkItem(ctx) {
	}
}

func (dc *DeploymentController) processNextWorkItem(ctx context.Context) bool {
	key, quit := dc.queue.Get()
	if quit {
		return false
	}
	defer dc.queue.Done(key)

	err := dc.syncHandler(ctx, key.(string))
	dc.handleErr(ctx, err, key)

	return true
}

//这段Go代码定义了两个函数：worker 和 processNextWorkItem，它们用于在DeploymentController中处理工作队列中的项。
//1. worker函数是一个无限循环，它不断地调用processNextWorkItem函数来处理队列中的下一个工作项，直到没有更多工作项需要处理为止。
//该函数接受一个context.Context参数，用于控制函数的取消或超时。
//2. processNextWorkItem函数从队列中获取下一个工作项的键，并调用syncHandler函数来处理该工作项。
//如果处理成功，它会标记该工作项为已完成。
//该函数也接受一个context.Context参数，并在处理完成后对其进行取消操作。
//该函数返回一个布尔值，表示是否成功处理了工作项。
//这段代码的主要目的是在DeploymentController中使用工作队列并发地处理工作项，并确保相同的键不会被并发处理。

func (dc *DeploymentController) handleErr(ctx context.Context, err error, key interface{}) {
	logger := klog.FromContext(ctx)
	if err == nil || errors.HasStatusCause(err, v1.NamespaceTerminatingCause) {
		dc.queue.Forget(key)
		return
	}
	ns, name, keyErr := cache.SplitMetaNamespaceKey(key.(string))
	if keyErr != nil {
		logger.Error(err, "Failed to split meta namespace cache key", "cacheKey", key)
	}

	if dc.queue.NumRequeues(key) < maxRetries {
		logger.V(2).Info("Error syncing deployment", "deployment", klog.KRef(ns, name), "err", err)
		dc.queue.AddRateLimited(key)
		return
	}

	utilruntime.HandleError(err)
	logger.V(2).Info("Dropping deployment out of the queue", "deployment", klog.KRef(ns, name), "err", err)
	dc.queue.Forget(key)
}

//该函数是Go语言中的一个处理错误的函数，它属于DeploymentController类型。
//函数通过传入的上下文、错误和键来执行错误处理逻辑。
//首先，函数从上下文中获取日志记录器。如果错误为nil或错误的原因是命名空间终止，则将键从队列中忘记并返回。
//接下来，函数尝试将键拆分为命名空间和名称，并检查拆分是否成功。如果拆分失败，则记录错误信息。
//如果队列中键的重试次数小于最大重试次数，则记录错误信息，并将键添加到速率限制队列中。
//如果以上条件都不满足，则处理错误，并记录信息，将键从队列中忘记。

// getReplicaSetsForDeployment uses ControllerRefManager to reconcile
// ControllerRef by adopting and orphaning.
// It returns the list of ReplicaSets that this Deployment should manage.
func (dc *DeploymentController) getReplicaSetsForDeployment(ctx context.Context, d *apps.Deployment) ([]*apps.ReplicaSet, error) {
	// List all ReplicaSets to find those we own but that no longer match our
	// selector. They will be orphaned by ClaimReplicaSets().
	rsList, err := dc.rsLister.ReplicaSets(d.Namespace).List(labels.Everything())
	if err != nil {
		return nil, err
	}
	deploymentSelector, err := metav1.LabelSelectorAsSelector(d.Spec.Selector)
	if err != nil {
		return nil, fmt.Errorf("deployment %s/%s has invalid label selector: %v", d.Namespace, d.Name, err)
	}
	// If any adoptions are attempted, we should first recheck for deletion with
	// an uncached quorum read sometime after listing ReplicaSets (see #42639).
	canAdoptFunc := controller.RecheckDeletionTimestamp(func(ctx context.Context) (metav1.Object, error) {
		fresh, err := dc.client.AppsV1().Deployments(d.Namespace).Get(ctx, d.Name, metav1.GetOptions{})
		if err != nil {
			return nil, err
		}
		if fresh.UID != d.UID {
			return nil, fmt.Errorf("original Deployment %v/%v is gone: got uid %v, wanted %v", d.Namespace, d.Name, fresh.UID, d.UID)
		}
		return fresh, nil
	})
	cm := controller.NewReplicaSetControllerRefManager(dc.rsControl, d, deploymentSelector, controllerKind, canAdoptFunc)
	return cm.ClaimReplicaSets(ctx, rsList)
}

//该函数用于通过ControllerRefManager来协调ControllerRef，实现采用和孤儿化操作。
//它返回一个列表，其中包含该Deployment应管理的ReplicaSets。 函数首先列出所有ReplicaSets，以找到我们拥有但不再匹配我们选择器的那些。
//它们将通过ClaimReplicaSets()函数被孤儿化。
//然后，函数将根据Deployment的规范选择器生成一个标签选择器。如果任何收养尝试都进行了，
//函数将首先在列出ReplicaSets后重新检查删除时间戳（请参阅#42639）。
//最后，函数创建一个ReplicaSetControllerRefManager，并使用ClaimReplicaSets函数来声明应管理的ReplicaSets列表

// getPodMapForDeployment returns the Pods managed by a Deployment.
//
// It returns a map from ReplicaSet UID to a list of Pods controlled by that RS,
// according to the Pod&#39;s ControllerRef.
// NOTE: The pod pointers returned by this method point the pod objects in the cache and thus
// shouldn&#39;t be modified in any way.
func (dc *DeploymentController) getPodMapForDeployment(d *apps.Deployment, rsList []*apps.ReplicaSet) (map[types.UID][]*v1.Pod, error) {
	// Get all Pods that potentially belong to this Deployment.
	selector, err := metav1.LabelSelectorAsSelector(d.Spec.Selector)
	if err != nil {
		return nil, err
	}
	pods, err := dc.podLister.Pods(d.Namespace).List(selector)
	if err != nil {
		return nil, err
	}
	// Group Pods by their controller (if it&#39;s in rsList).
	podMap := make(map[types.UID][]*v1.Pod, len(rsList))
	for _, rs := range rsList {
		podMap[rs.UID] = []*v1.Pod{}
	}
	for _, pod := range pods {
		// Do not ignore inactive Pods because Recreate Deployments need to verify that no
		// Pods from older versions are running before spinning up new Pods.
		controllerRef := metav1.GetControllerOf(pod)
		if controllerRef == nil {
			continue
		}
		// Only append if we care about this UID.
		if _, ok := podMap[controllerRef.UID]; ok {
			podMap[controllerRef.UID] = append(podMap[controllerRef.UID], pod)
		}
	}
	return podMap, nil
}

//该函数用于返回由Deployment管理的Pods的映射。
//它根据Pod的ControllerRef将Pod分组为其控制器（如果在rsList中）。
//函数首先根据Deployment的规范选择器获取所有可能属于该Deployment的Pods 。
//然后，它遍历这些Pods，并通过其ControllerRef将它们分组到podMap中。
//函数返回一个映射，其中键是ReplicaSet的UID，值是由该RS控制的Pod列表。
//注意，该函数返回的Pod指针指向缓存中的Pod对象，因此不应以任何方式修改它们。

// syncDeployment will sync the deployment with the given key.
// This function is not meant to be invoked concurrently with the same key.
func (dc *DeploymentController) syncDeployment(ctx context.Context, key string) error {
	logger := klog.FromContext(ctx)
	namespace, name, err := cache.SplitMetaNamespaceKey(key)
	if err != nil {
		logger.Error(err, "Failed to split meta namespace cache key", "cacheKey", key)
		return err
	}
	//该函数是一个Go语言函数，名为syncDeployment，属于DeploymentController结构体。
	//它接收一个ctx context.Context参数和一个key string参数，并返回一个error类型值。函数主要用于同步指定键值的部署信息。
	//函数首先从上下文中获取日志记录器，然后使用cache.SplitMetaNamespaceKey函数将键值拆分为命名空间和名称。
	//如果拆分过程中出现错误，则记录错误日志并返回该错误。

	startTime := time.Now()
	logger.V(4).Info("Started syncing deployment", "deployment", klog.KRef(namespace, name), "startTime", startTime)
	defer func() {
		logger.V(4).Info("Finished syncing deployment", "deployment", klog.KRef(namespace, name), "duration", time.Since(startTime))
	}()
	//这段Go代码主要实现了在开始和结束同步部署时记录日志的功能。
	//- 首先，通过time.Now()获取当前时间作为开始时间，并使用logger.V(4).Info记录开始同步部署的日志，其中包含了部署的名称、命名空间和开始时间。
	//- 然后，使用defer关键字定义了一个匿名函数，在函数执行结束后会自动执行该函数。
	//该匿名函数使用logger.V(4).Info记录结束同步部署的日志，其中包含了部署的名称、命名空间和同步部署所花费的时间。
	//通过这种方式，可以在日志中方便地查看部署的同步状态和耗时。

	deployment, err := dc.dLister.Deployments(namespace).Get(name)
	if errors.IsNotFound(err) {
		logger.V(2).Info("Deployment has been deleted", "deployment", klog.KRef(namespace, name))
		return nil
	}
	if err != nil {
		return err
	}
	//该函数通过调用dc.dLister.Deployments(namespace).Get(name)获取指定命名空间中名为name的部署对象。
	//如果该部署对象不存在，则记录日志并返回nil；如果存在其他错误，则直接返回错误。

	// Deep-copy otherwise we are mutating our cache.
	// TODO: Deep-copy only when needed.
	d := deployment.DeepCopy()

	everything := metav1.LabelSelector{}
	if reflect.DeepEqual(d.Spec.Selector, &amp;everything) {
		dc.eventRecorder.Eventf(d, v1.EventTypeWarning, "SelectingAll", "This deployment is selecting all pods. A non-empty selector is required.")
		if d.Status.ObservedGeneration < d.Generation {
			d.Status.ObservedGeneration = d.Generation
			dc.client.AppsV1().Deployments(d.Namespace).UpdateStatus(ctx, d, metav1.UpdateOptions{})
		}
		return nil
	}
	//这段Go代码中的函数主要功能是对一个deployment对象进行深拷贝，并检查其Selector是否等于一个空的LabelSelector对象。
	//如果是，则记录一条警告事件并更新deployment的状态。

	// List ReplicaSets owned by this Deployment, while reconciling ControllerRef
	// through adoption/orphaning.
	rsList, err := dc.getReplicaSetsForDeployment(ctx, d)
	if err != nil {
		return err
	}
	// List all Pods owned by this Deployment, grouped by their ReplicaSet.
	// Current uses of the podMap are:
	//
	// * check if a Pod is labeled correctly with the pod-template-hash label.
	// * check that no old Pods are running in the middle of Recreate Deployments.
	podMap, err := dc.getPodMapForDeployment(d, rsList)
	if err != nil {
		return err
	}

	if d.DeletionTimestamp != nil {
		return dc.syncStatusOnly(ctx, d, rsList)
	}

	// Update deployment conditions with an Unknown condition when pausing/resuming
	// a deployment. In this way, we can be sure that we won&#39;t timeout when a user
	// resumes a Deployment with a set progressDeadlineSeconds.
	if err = dc.checkPausedConditions(ctx, d); err != nil {
		return err
	}

	if d.Spec.Paused {
		return dc.sync(ctx, d, rsList)
	}

	// rollback is not re-entrant in case the underlying replica sets are updated with a new
	// revision so we should ensure that we won&#39;t proceed to update replica sets until we
	// make sure that the deployment has cleaned up its rollback spec in subsequent enqueues.
	if getRollbackTo(d) != nil {
		return dc.rollback(ctx, d, rsList)
	}

	scalingEvent, err := dc.isScalingEvent(ctx, d, rsList)
	if err != nil {
		return err
	}
	if scalingEvent {
		return dc.sync(ctx, d, rsList)
	}

	switch d.Spec.Strategy.Type {
	case apps.RecreateDeploymentStrategyType:
		return dc.rolloutRecreate(ctx, d, rsList, podMap)
	case apps.RollingUpdateDeploymentStrategyType:
		return dc.rolloutRolling(ctx, d, rsList)
	}
	return fmt.Errorf("unexpected deployment strategy type: %s", d.Spec.Strategy.Type)
}

//该函数主要负责处理Deployment的更新和同步逻辑。
//1. 首先，函数会获取该Deployment所拥有的ReplicaSet列表和Pod的映射关系。
//2. 如果该Deployment已被删除，则只同步状态。
//3. 检查是否暂停，若暂停则只进行同步操作。
//4. 如果需要回滚，则执行回滚操作。
//5. 检测是否为缩放事件，若是则进行同步操作。
//6. 根据Deployment的策略类型（Recreate或RollingUpdate），执行相应的更新操作。
//7. 如果遇到意外的部署策略类型，返回错误。
'><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://qq547475331.github.io/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8B-deployment_controllergo%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-deploymentcontrollergo-yuan-ma-jie-du/"><meta property="og:site_name" content="Guichen's Blog"><meta property="og:title" content="2024-04-09 K8S控制器之 deployment_controller.go源码解读"><meta property="og:description" content='/* Copyright 2015 The Kubernetes Authors. Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */ // Package deployment contains all the logic for handling Kubernetes Deployments. // It implements a set of strategies (rolling, recreate) for deploying an application, // the means to rollback to previous versions, proportional scaling for mitigating // risk, cleanup policy, and other useful features of Deployments. package deployment import ( "context" "fmt" "reflect" "time" apps "k8s.io/api/apps/v1" v1 "k8s.io/api/core/v1" "k8s.io/apimachinery/pkg/api/errors" metav1 "k8s.io/apimachinery/pkg/apis/meta/v1" "k8s.io/apimachinery/pkg/labels" "k8s.io/apimachinery/pkg/types" utilruntime "k8s.io/apimachinery/pkg/util/runtime" "k8s.io/apimachinery/pkg/util/wait" appsinformers "k8s.io/client-go/informers/apps/v1" coreinformers "k8s.io/client-go/informers/core/v1" clientset "k8s.io/client-go/kubernetes" "k8s.io/client-go/kubernetes/scheme" v1core "k8s.io/client-go/kubernetes/typed/core/v1" appslisters "k8s.io/client-go/listers/apps/v1" corelisters "k8s.io/client-go/listers/core/v1" "k8s.io/client-go/tools/cache" "k8s.io/client-go/tools/record" "k8s.io/client-go/util/workqueue" "k8s.io/kubernetes/pkg/controller" "k8s.io/kubernetes/pkg/controller/deployment/util" ) const ( // maxRetries is the number of times a deployment will be retried before it is dropped out of the queue. // With the current rate-limiter in use (5ms*2^(maxRetries-1)) the following numbers represent the times // a deployment is going to be requeued: // // 5ms, 10ms, 20ms, 40ms, 80ms, 160ms, 320ms, 640ms, 1.3s, 2.6s, 5.1s, 10.2s, 20.4s, 41s, 82s maxRetries = 15 ) // controllerKind contains the schema.GroupVersionKind for this controller type. var controllerKind = apps.SchemeGroupVersion.WithKind("Deployment") // DeploymentController is responsible for synchronizing Deployment objects stored // in the system with actual running replica sets and pods. type DeploymentController struct { // rsControl is used for adopting/releasing replica sets. rsControl controller.RSControlInterface client clientset.Interface eventBroadcaster record.EventBroadcaster eventRecorder record.EventRecorder // To allow injection of syncDeployment for testing. syncHandler func(ctx context.Context, dKey string) error // used for unit testing enqueueDeployment func(deployment *apps.Deployment) // dLister can list/get deployments from the shared informer&#39;s store dLister appslisters.DeploymentLister // rsLister can list/get replica sets from the shared informer&#39;s store rsLister appslisters.ReplicaSetLister // podLister can list/get pods from the shared informer&#39;s store podLister corelisters.PodLister // dListerSynced returns true if the Deployment store has been synced at least once. // Added as a member to the struct to allow injection for testing. dListerSynced cache.InformerSynced // rsListerSynced returns true if the ReplicaSet store has been synced at least once. // Added as a member to the struct to allow injection for testing. rsListerSynced cache.InformerSynced // podListerSynced returns true if the pod store has been synced at least once. // Added as a member to the struct to allow injection for testing. podListerSynced cache.InformerSynced // Deployments that need to be synced queue workqueue.RateLimitingInterface } // NewDeploymentController creates a new DeploymentController. func NewDeploymentController(ctx context.Context, dInformer appsinformers.DeploymentInformer, rsInformer appsinformers.ReplicaSetInformer, podInformer coreinformers.PodInformer, client clientset.Interface) (*DeploymentController, error) { eventBroadcaster := record.NewBroadcaster(record.WithContext(ctx)) logger := klog.FromContext(ctx) dc := &amp;DeploymentController{ client: client, eventBroadcaster: eventBroadcaster, eventRecorder: eventBroadcaster.NewRecorder(scheme.Scheme, v1.EventSource{Component: "deployment-controller"}), queue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), "deployment"), } dc.rsControl = controller.RealRSControl{ KubeClient: client, Recorder: dc.eventRecorder, } //该函数用于创建一个新的DeploymentController。 //- 接收上下文ctx以及多个informer和client作为参数。 //- 创建一个新的事件广播器eventBroadcaster，并从上下文ctx中获取日志记录器logger 。 //- 初始化一个DeploymentController结构体实例dc，包括client、eventBroadcaster、eventRecorder、queue和rsControl字段。 //- dc.rsControl使用controller.RealRSControl结构体初始化，包括KubeClient和Recorder字段。 //- 返回创建的dc实例和可能出现的错误。 dInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: func(obj interface{}) { dc.addDeployment(logger, obj) }, UpdateFunc: func(oldObj, newObj interface{}) { dc.updateDeployment(logger, oldObj, newObj) }, // This will enter the sync loop and no-op, because the deployment has been deleted from the store. DeleteFunc: func(obj interface{}) { dc.deleteDeployment(logger, obj) }, }) rsInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: func(obj interface{}) { dc.addReplicaSet(logger, obj) }, UpdateFunc: func(oldObj, newObj interface{}) { dc.updateReplicaSet(logger, oldObj, newObj) }, DeleteFunc: func(obj interface{}) { dc.deleteReplicaSet(logger, obj) }, }) podInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ DeleteFunc: func(obj interface{}) { dc.deletePod(logger, obj) }, }) //这段代码定义了三个事件处理器，分别用于处理deployment、replicaSet和pod的添加、更新和删除事件。 //每个事件处理器都调用了对应的方法，例如对于deployment的添加事件，调用了dc.addDeployment(logger, obj)方法。 //这些方法的具体实现可以根据具体业务逻辑进行编写。 dc.syncHandler = dc.syncDeployment dc.enqueueDeployment = dc.enqueue dc.dLister = dInformer.Lister() dc.rsLister = rsInformer.Lister() dc.podLister = podInformer.Lister() dc.dListerSynced = dInformer.Informer().HasSynced dc.rsListerSynced = rsInformer.Informer().HasSynced dc.podListerSynced = podInformer.Informer().HasSynced return dc, nil } //这段代码是Go语言中的函数，主要功能是设置和初始化一个名为dc的对象，并返回该对象和nil。 //- 首先，将dc.syncHandler设置为dc.syncDeployment，将dc.enqueueDeployment设置为dc.enqueue。 //- 然后，通过dInformer、rsInformer和podInformer的Lister()方法，分别将dc.dLister、dc.rsLister和dc.podLister设置为相应的列表器。 //- 接着，通过dInformer、rsInformer和podInformer的Informer().HasSynced方法，分别将dc.dListerSynced、dc.rsListerSynced和dc.podListerSynced设置为相应的同步状态检查函数。 //- 最后，返回设置好的dc对象和nil。 这段代码主要涉及到对象的设置和初始化操作，使用了多个列表器和同步状态检查函数来管理不同资源的信息。 // Run begins watching and syncing. func (dc *DeploymentController) Run(ctx context.Context, workers int) { defer utilruntime.HandleCrash() // Start events processing pipeline. dc.eventBroadcaster.StartStructuredLogging(3) dc.eventBroadcaster.StartRecordingToSink(&amp;v1core.EventSinkImpl{Interface: dc.client.CoreV1().Events("")}) defer dc.eventBroadcaster.Shutdown() defer dc.queue.ShutDown() logger := klog.FromContext(ctx) logger.Info("Starting controller", "controller", "deployment") defer logger.Info("Shutting down controller", "controller", "deployment") if !cache.WaitForNamedCacheSync("deployment", ctx.Done(), dc.dListerSynced, dc.rsListerSynced, dc.podListerSynced) { return } for i := 0; i < workers; i++ { go wait.UntilWithContext(ctx, dc.worker, time.Second) } <-ctx.Done() } //该函数是DeploymentController类型的Run方法，用于启动部署控制器的监视和同步操作。 //- 首先，函数通过defer语句句柄处理崩溃情况。 //- 然后，启动事件处理管道，设置日志记录级别和目标。 //- 接着，通过defer语句关闭事件处理管道。 //- 然后，记录日志信息，表示控制器开始启动。 //- 接着，使用cache.WaitForNamedCacheSync函数等待缓存同步完成。 //- 然后，通过循环创建多个goroutine，并调用dc.worker函数进行工作。 //- 最后，等待上下文完成，并返回。 //该函数的主要功能是启动部署控制器，并使其开始监视和同步操作。 func (dc *DeploymentController) addDeployment(logger klog.Logger, obj interface{}) { d := obj.(*apps.Deployment) logger.V(4).Info("Adding deployment", "deployment", klog.KObj(d)) dc.enqueueDeployment(d) } func (dc *DeploymentController) updateDeployment(logger klog.Logger, old, cur interface{}) { oldD := old.(*apps.Deployment) curD := cur.(*apps.Deployment) logger.V(4).Info("Updating deployment", "deployment", klog.KObj(oldD)) dc.enqueueDeployment(curD) } //这两个函数都是DeploymentController的方法，用于处理Deployment的添加和更新事件。 //- addDeployment方法接收一个logger和一个obj参数，其中obj是通过类型断言转换为*apps.Deployment类型的。 //该方法首先使用logger记录添加deployment的日志信息，然后调用dc.enqueueDeployment方法将该deployment加入队列中，以便后续处理。 //- updateDeployment方法与addDeployment方法类似，但它接收的是旧的和新的Deployment对象。该方法使用logger记录更新deployment的日志信息， //并将新的Deployment对象加入队列中进行处理。 func (dc *DeploymentController) deleteDeployment(logger klog.Logger, obj interface{}) { d, ok := obj.(*apps.Deployment) if !ok { tombstone, ok := obj.(cache.DeletedFinalStateUnknown) if !ok { utilruntime.HandleError(fmt.Errorf("couldn&#39;t get object from tombstone %#v", obj)) return } d, ok = tombstone.Obj.(*apps.Deployment) if !ok { utilruntime.HandleError(fmt.Errorf("tombstone contained object that is not a Deployment %#v", obj)) return } } logger.V(4).Info("Deleting deployment", "deployment", klog.KObj(d)) dc.enqueueDeployment(d) } //该函数是一个Go语言函数，它定义了一个名为deleteDeployment的方法，该方法接受一个logger和一个obj参数，并没有任何返回值。 //该方法主要用于从一个部署（Deployment）中删除对象。 //首先，函数会尝试将obj参数断言为apps.Deployment类型，并检查断言是否成功。如果断言失败， //则会尝试将obj参数断言为cache.DeletedFinalStateUnknown类型。如果这个断言也失败了，函数会记录一个错误信息并返回。 //如果断言成功，则会尝试从tombstone中获取对象，并再次断言该对象是否为apps.Deployment类型。如果断言失败，则会记录一个错误信息并返回。 //如果成功断言出对象为apps.Deployment类型，则会使用logger记录一条信息，表示正在删除该部署， //并调用dc.enqueueDeployment方法将该部署加入队列，以便进一步处理。 // addReplicaSet enqueues the deployment that manages a ReplicaSet when the ReplicaSet is created. func (dc *DeploymentController) addReplicaSet(logger klog.Logger, obj interface{}) { rs := obj.(*apps.ReplicaSet) if rs.DeletionTimestamp != nil { // On a restart of the controller manager, it&#39;s possible for an object to // show up in a state that is already pending deletion. dc.deleteReplicaSet(logger, rs) return } // If it has a ControllerRef, that&#39;s all that matters. if controllerRef := metav1.GetControllerOf(rs); controllerRef != nil { d := dc.resolveControllerRef(rs.Namespace, controllerRef) if d == nil { return } logger.V(4).Info("ReplicaSet added", "replicaSet", klog.KObj(rs)) dc.enqueueDeployment(d) return } //该函数是Go语言编写的，用于在创建ReplicaSet时，将管理该ReplicaSet的Deployment加入队列中。 //函数首先判断传入的obj对象是否为ReplicaSet类型，并通过判断ReplicaSet的DeletionTimestamp是否为空来确定是否需要删除该ReplicaSet。 //如果ReplicaSet有ControllerRef，则通过resolveControllerRef函数解析出对应的Deployment，并将其加入队列中。 // Otherwise, it&#39;s an orphan. Get a list of all matching Deployments and sync // them to see if anyone wants to adopt it. ds := dc.getDeploymentsForReplicaSet(logger, rs) if len(ds) == 0 { return } logger.V(4).Info("Orphan ReplicaSet added", "replicaSet", klog.KObj(rs)) for _, d := range ds { dc.enqueueDeployment(d) } } //该函数主要处理孤儿ReplicaSet的情况。 //它会获取与该ReplicaSet匹配的所有Deployment列表，并尝试同步这些Deployment，看是否有Deployment愿意采用该ReplicaSet。 //如果找到了对应的Deployment，则将其加入到队列中以便进一步处理。 // getDeploymentsForReplicaSet returns a list of Deployments that potentially // match a ReplicaSet. func (dc *DeploymentController) getDeploymentsForReplicaSet(logger klog.Logger, rs *apps.ReplicaSet) []*apps.Deployment { deployments, err := util.GetDeploymentsForReplicaSet(dc.dLister, rs) if err != nil || len(deployments) == 0 { return nil } // Because all ReplicaSet&#39;s belonging to a deployment should have a unique label key, // there should never be more than one deployment returned by the above method. // If that happens we should probably dynamically repair the situation by ultimately // trying to clean up one of the controllers, for now we just return the older one if len(deployments) > 1 { // ControllerRef will ensure we don&#39;t do anything crazy, but more than one // item in this list nevertheless constitutes user error. logger.V(4).Info("user error! more than one deployment is selecting replica set", "replicaSet", klog.KObj(rs), "labels", rs.Labels, "deployment", klog.KObj(deployments[0])) } return deployments } //函数用于获取与给定ReplicaSet匹配的所有Deployment列表。 //它首先调用util.GetDeploymentsForReplicaSet函数来获取匹配的Deployment列表，如果出现错误或列表为空，则返回nil。 //如果获取的Deployment列表长度大于1，则记录错误日志，并返回列表中的第一个Deployment。 //在正常情况下，返回获取的Deployment列表。 // updateReplicaSet figures out what deployment(s) manage a ReplicaSet when the ReplicaSet // is updated and wake them up. If the anything of the ReplicaSets have changed, we need to // awaken both the old and new deployments. old and cur must be *apps.ReplicaSet // types. func (dc *DeploymentController) updateReplicaSet(logger klog.Logger, old, cur interface{}) { curRS := cur.(*apps.ReplicaSet) oldRS := old.(*apps.ReplicaSet) if curRS.ResourceVersion == oldRS.ResourceVersion { // Periodic resync will send update events for all known replica sets. // Two different versions of the same replica set will always have different RVs. return } //该函数用于在更新ReplicaSet时，确定由哪个部署管理该ReplicaSet，并唤醒它们。 //如果ReplicaSet的任何内容发生变化，需要唤醒旧的和新的部署。old和cur必须是指向apps.ReplicaSet类型的指针。 //函数首先将传入的old和cur参数转换为*apps.ReplicaSet类型。 //然后，它比较两个ReplicaSet的ResourceVersion字段。 //如果它们相等，则表示这是周期性同步发送的更新事件，而对于同一ReplicaSet的两个不同版本，它们的ResourceVersion总会有不同的值。 //在这种情况下，函数直接返回，不做任何处理。 curControllerRef := metav1.GetControllerOf(curRS) oldControllerRef := metav1.GetControllerOf(oldRS) controllerRefChanged := !reflect.DeepEqual(curControllerRef, oldControllerRef) if controllerRefChanged && oldControllerRef != nil { // The ControllerRef was changed. Sync the old controller, if any. if d := dc.resolveControllerRef(oldRS.Namespace, oldControllerRef); d != nil { dc.enqueueDeployment(d) } } //这段Go代码主要关注于检查和处理两个ReplicaSet（curRS和oldRS）的ControllerRef是否发生变化，并根据变化情况同步旧的控制器。 //1. 首先，通过metav1.GetControllerOf函数获取curRS和oldRS的ControllerRef。 //2. 然后，使用reflect.DeepEqual函数比较curControllerRef和oldControllerRef是否相等。 //3. 如果两个ControllerRef不相等且oldControllerRef不为nil，则认为ControllerRef发生了变化。 //4. 当ControllerRef发生变化时，需要同步旧的控制器。通过dc.resolveControllerRef函数解析oldRS的命名空间和ControllerRef， //得到对应的Deployment。 //5. 最后，如果解析成功（d!=nil），则将该Deployment加入到调度队列中，以便进一步处理。 // If it has a ControllerRef, that&#39;s all that matters. if curControllerRef != nil { d := dc.resolveControllerRef(curRS.Namespace, curControllerRef) if d == nil { return } logger.V(4).Info("ReplicaSet updated", "replicaSet", klog.KObj(curRS)) dc.enqueueDeployment(d) return } //这个Go函数主要检查当前的curControllerRef是否为nil。 //如果不为nil，则通过dc.resolveControllerRef方法解析curControllerRef，并检查解析结果是否为nil。 //如果不为nil，则记录日志并使用dc.enqueueDeployment方法将解析结果加入到队列中。 // Otherwise, it&#39;s an orphan. If anything changed, sync matching controllers // to see if anyone wants to adopt it now. labelChanged := !reflect.DeepEqual(curRS.Labels, oldRS.Labels) if labelChanged || controllerRefChanged { ds := dc.getDeploymentsForReplicaSet(logger, curRS) if len(ds) == 0 { return } logger.V(4).Info("Orphan ReplicaSet updated", "replicaSet", klog.KObj(curRS)) for _, d := range ds { dc.enqueueDeployment(d) } } } //这段Go代码是处理孤儿ReplicaSet的逻辑。 //如果ReplicaSet的标签或控制器引用发生了变化，它会尝试同步匹配的控制器，以查看是否有控制器愿意现在采用它。具体步骤如下： //1. 检查当前ReplicaSet的标签和旧ReplicaSet的标签是否相等，如果不相等，则标记标签发生变化； //2. 检查控制器引用是否发生变化； //3. 如果标签发生变化或控制器引用发生变化，则获取当前ReplicaSet对应的部署列表； //4. 如果部署列表为空，则直接返回； //5. 输出日志信息，表示孤儿ReplicaSet已更新； //6. 遍历部署列表，将每个部署对象加入到队列中，以便进一步处理。 // deleteReplicaSet enqueues the deployment that manages a ReplicaSet when // the ReplicaSet is deleted. obj could be an *apps.ReplicaSet, or // a DeletionFinalStateUnknown marker item. func (dc *DeploymentController) deleteReplicaSet(logger klog.Logger, obj interface{}) { rs, ok := obj.(*apps.ReplicaSet) //该函数是Go语言编写的，属于DeploymentController类型的一个方法，方法名为deleteReplicaSet。 //该方法接收一个logger和一个obj参数，其中logger用于记录日志，obj是一个接口类型， //可以是*apps.ReplicaSet类型或者DeletionFinalStateUnknown标记项。 //方法的主要功能是从obj中解析出*apps.ReplicaSet类型的rs，并判断解析是否成功。 // When a delete is dropped, the relist will notice a pod in the store not // in the list, leading to the insertion of a tombstone object which contains // the deleted key/value. Note that this value might be stale. If the ReplicaSet // changed labels the new deployment will not be woken up till the periodic resync. if !ok { tombstone, ok := obj.(cache.DeletedFinalStateUnknown) if !ok { utilruntime.HandleError(fmt.Errorf("couldn&#39;t get object from tombstone %#v", obj)) return } rs, ok = tombstone.Obj.(*apps.ReplicaSet) if !ok { utilruntime.HandleError(fmt.Errorf("tombstone contained object that is not a ReplicaSet %#v", obj)) return } } //这段Go代码是处理从存储中获取对象时，如果对象已被删除的情况。 //如果获取对象失败，会检查对象是否是一个 tombstone 对象（即已被删除的对象）。 //如果是 tombstone 对象，则尝试从 tombstone 中恢复被删除的对象。如果恢复失败，则记录错误信息。 controllerRef := metav1.GetControllerOf(rs) if controllerRef == nil { // No controller should care about orphans being deleted. return } d := dc.resolveControllerRef(rs.Namespace, controllerRef) if d == nil { return } logger.V(4).Info("ReplicaSet deleted", "replicaSet", klog.KObj(rs)) dc.enqueueDeployment(d) } //该函数主要实现当一个ReplicaSet被删除时，检查是否有对应的Deployment控制器，并将该Deployment加入到队列中以便进一步处理。具体流程如下： //1. 通过metav1.GetControllerOf(rs)获取ReplicaSet的控制器引用； //2. 如果控制器引用为空，则表示没有对应的Deployment控制器，直接返回； //3. 调用dc.resolveControllerRef(rs.Namespace, controllerRef)解析控制器引用，获取对应的Deployment对象； //4. 如果解析失败或返回的Deployment对象为空，则直接返回； //5. 使用logger.V(4).Info记录日志，表示ReplicaSet已被删除； //6. 调用dc.enqueueDeployment(d)将对应的Deployment对象加入到队列中，以便进一步处理。 // deletePod will enqueue a Recreate Deployment once all of its pods have stopped running. func (dc *DeploymentController) deletePod(logger klog.Logger, obj interface{}) { pod, ok := obj.(*v1.Pod) //该函数是一个Go语言函数，名为deletePod，它属于DeploymentController类型。 //函数通过传入的logger和obj参数，将一个Recreate Deployment加入队列，但只有当该Deployment的所有Pod都停止运行时才会执行。 //函数首先尝试将obj参数断言为*v1.Pod类型，并检查断言是否成功。 // When a delete is dropped, the relist will notice a pod in the store not // in the list, leading to the insertion of a tombstone object which contains // the deleted key/value. Note that this value might be stale. If the Pod // changed labels the new deployment will not be woken up till the periodic resync. if !ok { tombstone, ok := obj.(cache.DeletedFinalStateUnknown) if !ok { utilruntime.HandleError(fmt.Errorf("couldn&#39;t get object from tombstone %#v", obj)) return } pod, ok = tombstone.Obj.(*v1.Pod) if !ok { utilruntime.HandleError(fmt.Errorf("tombstone contained object that is not a pod %#v", obj)) return } } d := dc.getDeploymentForPod(logger, pod) if d == nil { return } logger.V(4).Info("Pod deleted", "pod", klog.KObj(pod)) if d.Spec.Strategy.Type == apps.RecreateDeploymentStrategyType { // Sync if this Deployment now has no more Pods. rsList, err := util.ListReplicaSets(d, util.RsListFromClient(dc.client.AppsV1())) if err != nil { return } podMap, err := dc.getPodMapForDeployment(d, rsList) if err != nil { return } numPods := 0 for _, podList := range podMap { numPods += len(podList) } if numPods == 0 { dc.enqueueDeployment(d) } } } //这段Go代码是用于处理Kubernetes中Pod删除事件的逻辑。 //- 当一个Pod被删除时，如果控制器（例如Deployment）没有及时收到该事件，就会在存储中注意到这个Pod不在列表中， //从而插入一个包含删除的键值对的墓碑对象（tombstone object）。 //- 如果墓碑对象中的对象不是Pod类型，函数会记录错误并返回。 //- 函数会尝试获取与该Pod关联的Deployment对象，如果获取不到则直接返回。 //- 如果该Deployment的策略是Recreate类型，则会检查该Deployment下是否已经没有Pod了。 //- 如果没有Pod了，则将该Deployment加入到队列中，以便进一步处理。 //这段代码的主要目的是确保在Pod被删除时，与之关联的Deployment能够及时地进行同步和更新。 func (dc *DeploymentController) enqueue(deployment *apps.Deployment) { key, err := controller.KeyFunc(deployment) if err != nil { utilruntime.HandleError(fmt.Errorf("couldn&#39;t get key for object %#v: %v", deployment, err)) return } dc.queue.Add(key) } //该函数是一个Go语言的方法，定义在一个名为DeploymentController的结构体类型上。 //方法名为enqueue，它接收一个参数deployment，类型为*apps.Deployment，表示一个Kubernetes部署对象的指针。 //该方法的主要功能是将给定的部署对象加入到一个队列中，以便后续处理。具体步骤如下： //1. 调用controller.KeyFunc(deployment)函数，尝试获取部署对象的键值（通常是一个字符串），用于在队列中唯一标识该对象。 //2. 如果获取键值时出现错误，通过utilruntime.HandleError函数处理错误，并打印错误信息。然后直接返回，不将对象加入队列。 //3. 如果成功获取了键值，将其添加到dc.queue（一个队列对象）中，以便后续处理。 //总结：该方法用于将一个Kubernetes部署对象加入到队列中，以便后续进行处理。 func (dc *DeploymentController) enqueueRateLimited(deployment *apps.Deployment) { key, err := controller.KeyFunc(deployment) if err != nil { utilruntime.HandleError(fmt.Errorf("couldn&#39;t get key for object %#v: %v", deployment, err)) return } dc.queue.AddRateLimited(key) } //该函数用于将指定的部署对象加入到队列中，并对其进行速率限制。 //首先，通过调用controller.KeyFunc方法获取该部署对象的键值，如果获取失败，则通过utilruntime.HandleError方法记录错误信息并返回。 //如果获取成功，则调用dc.queue.AddRateLimited方法将键值加入到队列中，并对其进行速率限制。 // enqueueAfter will enqueue a deployment after the provided amount of time. func (dc *DeploymentController) enqueueAfter(deployment *apps.Deployment, after time.Duration) { key, err := controller.KeyFunc(deployment) if err != nil { utilruntime.HandleError(fmt.Errorf("couldn&#39;t get key for object %#v: %v", deployment, err)) return } dc.queue.AddAfter(key, after) } //该函数将一个部署对象加入到队列中，但会在指定的时间延迟后才执行。 //首先，函数通过调用controller.KeyFunc方法获取部署对象的键值，如果获取失败，则通过utilruntime.HandleError方法记录错误信息并返回。 //接着，函数使用dc.queue.AddAfter方法将键值加入到队列中，并指定延迟执行的时间。 // getDeploymentForPod returns the deployment managing the given Pod. func (dc *DeploymentController) getDeploymentForPod(logger klog.Logger, pod *v1.Pod) *apps.Deployment { // Find the owning replica set var rs *apps.ReplicaSet var err error controllerRef := metav1.GetControllerOf(pod) if controllerRef == nil { // No controller owns this Pod. return nil } if controllerRef.Kind != apps.SchemeGroupVersion.WithKind("ReplicaSet").Kind { // Not a pod owned by a replica set. return nil } rs, err = dc.rsLister.ReplicaSets(pod.Namespace).Get(controllerRef.Name) if err != nil || rs.UID != controllerRef.UID { logger.V(4).Info("Cannot get replicaset for pod", "ownerReference", controllerRef.Name, "pod", klog.KObj(pod), "err", err) return nil } //该函数用于获取管理给定Pod的Deployment。 //- 首先，它查找拥有该Pod的ReplicaSet。 //- 如果找不到拥有者的Pod，则返回nil。 - //如果Pod的拥有者不是ReplicaSet，则返回nil。 //- 然后，尝试根据Pod的拥有者名称获取ReplicaSet。 //- 如果获取失败或获取到的ReplicaSet的UID与拥有者的UID不匹配，则返回nil。 //- 最后，返回获取到的Deployment。 // Now find the Deployment that owns that ReplicaSet. controllerRef = metav1.GetControllerOf(rs) if controllerRef == nil { return nil } return dc.resolveControllerRef(rs.Namespace, controllerRef) } //这个函数的作用是通过 ReplicaSet 的 controllerRef 找到对应的 Deployment。 //首先通过 metav1.GetControllerOf(rs) 获取到 controllerRef， //如果 controllerRef 为空则返回 nil，否则调用 dc.resolveControllerRef(rs.Namespace, controllerRef) 解析并返回对应的 Deployment。 // resolveControllerRef returns the controller referenced by a ControllerRef, // or nil if the ControllerRef could not be resolved to a matching controller // of the correct Kind. func (dc *DeploymentController) resolveControllerRef(namespace string, controllerRef *metav1.OwnerReference) *apps.Deployment { // We can&#39;t look up by UID, so look up by Name and then verify UID. // Don&#39;t even try to look up by Name if it&#39;s the wrong Kind. if controllerRef.Kind != controllerKind.Kind { return nil } d, err := dc.dLister.Deployments(namespace).Get(controllerRef.Name) if err != nil { return nil } if d.UID != controllerRef.UID { // The controller we found with this Name is not the same one that the // ControllerRef points to. return nil } return d } //该函数是一个Go语言函数，用于解析一个ControllerRef引用所指向的控制器，如果无法解析出正确的控制器，则返回nil。 //函数接受三个参数： - namespace：字符串类型，表示命名空间。 //- controllerRef：指向metav1.OwnerReference类型的指针。 //函数返回一个指向apps.Deployment类型的指针。 //函数的主要步骤如下： //1. 首先，检查controllerRef的Kind属性是否与controllerKind.Kind相等，如果不相等，则直接返回nil。 //2. 如果controllerRef的Kind属性与controllerKind.Kind相等，则通过dc.dLister.Deployments(namespace).Get(controllerRef.Name)获取具有相同名称的部署对象。 //3. 如果获取部署对象时出现错误，则返回nil。 //4. 最后，检查获取到的部署对象的UID属性是否与controllerRef的UID属性相等，如果不相等，则返回nil，否则返回该部署对象。 // worker runs a worker thread that just dequeues items, processes them, and marks them done. // It enforces that the syncHandler is never invoked concurrently with the same key. func (dc *DeploymentController) worker(ctx context.Context) { for dc.processNextWorkItem(ctx) { } } func (dc *DeploymentController) processNextWorkItem(ctx context.Context) bool { key, quit := dc.queue.Get() if quit { return false } defer dc.queue.Done(key) err := dc.syncHandler(ctx, key.(string)) dc.handleErr(ctx, err, key) return true } //这段Go代码定义了两个函数：worker 和 processNextWorkItem，它们用于在DeploymentController中处理工作队列中的项。 //1. worker函数是一个无限循环，它不断地调用processNextWorkItem函数来处理队列中的下一个工作项，直到没有更多工作项需要处理为止。 //该函数接受一个context.Context参数，用于控制函数的取消或超时。 //2. processNextWorkItem函数从队列中获取下一个工作项的键，并调用syncHandler函数来处理该工作项。 //如果处理成功，它会标记该工作项为已完成。 //该函数也接受一个context.Context参数，并在处理完成后对其进行取消操作。 //该函数返回一个布尔值，表示是否成功处理了工作项。 //这段代码的主要目的是在DeploymentController中使用工作队列并发地处理工作项，并确保相同的键不会被并发处理。 func (dc *DeploymentController) handleErr(ctx context.Context, err error, key interface{}) { logger := klog.FromContext(ctx) if err == nil || errors.HasStatusCause(err, v1.NamespaceTerminatingCause) { dc.queue.Forget(key) return } ns, name, keyErr := cache.SplitMetaNamespaceKey(key.(string)) if keyErr != nil { logger.Error(err, "Failed to split meta namespace cache key", "cacheKey", key) } if dc.queue.NumRequeues(key) < maxRetries { logger.V(2).Info("Error syncing deployment", "deployment", klog.KRef(ns, name), "err", err) dc.queue.AddRateLimited(key) return } utilruntime.HandleError(err) logger.V(2).Info("Dropping deployment out of the queue", "deployment", klog.KRef(ns, name), "err", err) dc.queue.Forget(key) } //该函数是Go语言中的一个处理错误的函数，它属于DeploymentController类型。 //函数通过传入的上下文、错误和键来执行错误处理逻辑。 //首先，函数从上下文中获取日志记录器。如果错误为nil或错误的原因是命名空间终止，则将键从队列中忘记并返回。 //接下来，函数尝试将键拆分为命名空间和名称，并检查拆分是否成功。如果拆分失败，则记录错误信息。 //如果队列中键的重试次数小于最大重试次数，则记录错误信息，并将键添加到速率限制队列中。 //如果以上条件都不满足，则处理错误，并记录信息，将键从队列中忘记。 // getReplicaSetsForDeployment uses ControllerRefManager to reconcile // ControllerRef by adopting and orphaning. // It returns the list of ReplicaSets that this Deployment should manage. func (dc *DeploymentController) getReplicaSetsForDeployment(ctx context.Context, d *apps.Deployment) ([]*apps.ReplicaSet, error) { // List all ReplicaSets to find those we own but that no longer match our // selector. They will be orphaned by ClaimReplicaSets(). rsList, err := dc.rsLister.ReplicaSets(d.Namespace).List(labels.Everything()) if err != nil { return nil, err } deploymentSelector, err := metav1.LabelSelectorAsSelector(d.Spec.Selector) if err != nil { return nil, fmt.Errorf("deployment %s/%s has invalid label selector: %v", d.Namespace, d.Name, err) } // If any adoptions are attempted, we should first recheck for deletion with // an uncached quorum read sometime after listing ReplicaSets (see #42639). canAdoptFunc := controller.RecheckDeletionTimestamp(func(ctx context.Context) (metav1.Object, error) { fresh, err := dc.client.AppsV1().Deployments(d.Namespace).Get(ctx, d.Name, metav1.GetOptions{}) if err != nil { return nil, err } if fresh.UID != d.UID { return nil, fmt.Errorf("original Deployment %v/%v is gone: got uid %v, wanted %v", d.Namespace, d.Name, fresh.UID, d.UID) } return fresh, nil }) cm := controller.NewReplicaSetControllerRefManager(dc.rsControl, d, deploymentSelector, controllerKind, canAdoptFunc) return cm.ClaimReplicaSets(ctx, rsList) } //该函数用于通过ControllerRefManager来协调ControllerRef，实现采用和孤儿化操作。 //它返回一个列表，其中包含该Deployment应管理的ReplicaSets。 函数首先列出所有ReplicaSets，以找到我们拥有但不再匹配我们选择器的那些。 //它们将通过ClaimReplicaSets()函数被孤儿化。 //然后，函数将根据Deployment的规范选择器生成一个标签选择器。如果任何收养尝试都进行了， //函数将首先在列出ReplicaSets后重新检查删除时间戳（请参阅#42639）。 //最后，函数创建一个ReplicaSetControllerRefManager，并使用ClaimReplicaSets函数来声明应管理的ReplicaSets列表 // getPodMapForDeployment returns the Pods managed by a Deployment. // // It returns a map from ReplicaSet UID to a list of Pods controlled by that RS, // according to the Pod&#39;s ControllerRef. // NOTE: The pod pointers returned by this method point the pod objects in the cache and thus // shouldn&#39;t be modified in any way. func (dc *DeploymentController) getPodMapForDeployment(d *apps.Deployment, rsList []*apps.ReplicaSet) (map[types.UID][]*v1.Pod, error) { // Get all Pods that potentially belong to this Deployment. selector, err := metav1.LabelSelectorAsSelector(d.Spec.Selector) if err != nil { return nil, err } pods, err := dc.podLister.Pods(d.Namespace).List(selector) if err != nil { return nil, err } // Group Pods by their controller (if it&#39;s in rsList). podMap := make(map[types.UID][]*v1.Pod, len(rsList)) for _, rs := range rsList { podMap[rs.UID] = []*v1.Pod{} } for _, pod := range pods { // Do not ignore inactive Pods because Recreate Deployments need to verify that no // Pods from older versions are running before spinning up new Pods. controllerRef := metav1.GetControllerOf(pod) if controllerRef == nil { continue } // Only append if we care about this UID. if _, ok := podMap[controllerRef.UID]; ok { podMap[controllerRef.UID] = append(podMap[controllerRef.UID], pod) } } return podMap, nil } //该函数用于返回由Deployment管理的Pods的映射。 //它根据Pod的ControllerRef将Pod分组为其控制器（如果在rsList中）。 //函数首先根据Deployment的规范选择器获取所有可能属于该Deployment的Pods 。 //然后，它遍历这些Pods，并通过其ControllerRef将它们分组到podMap中。 //函数返回一个映射，其中键是ReplicaSet的UID，值是由该RS控制的Pod列表。 //注意，该函数返回的Pod指针指向缓存中的Pod对象，因此不应以任何方式修改它们。 // syncDeployment will sync the deployment with the given key. // This function is not meant to be invoked concurrently with the same key. func (dc *DeploymentController) syncDeployment(ctx context.Context, key string) error { logger := klog.FromContext(ctx) namespace, name, err := cache.SplitMetaNamespaceKey(key) if err != nil { logger.Error(err, "Failed to split meta namespace cache key", "cacheKey", key) return err } //该函数是一个Go语言函数，名为syncDeployment，属于DeploymentController结构体。 //它接收一个ctx context.Context参数和一个key string参数，并返回一个error类型值。函数主要用于同步指定键值的部署信息。 //函数首先从上下文中获取日志记录器，然后使用cache.SplitMetaNamespaceKey函数将键值拆分为命名空间和名称。 //如果拆分过程中出现错误，则记录错误日志并返回该错误。 startTime := time.Now() logger.V(4).Info("Started syncing deployment", "deployment", klog.KRef(namespace, name), "startTime", startTime) defer func() { logger.V(4).Info("Finished syncing deployment", "deployment", klog.KRef(namespace, name), "duration", time.Since(startTime)) }() //这段Go代码主要实现了在开始和结束同步部署时记录日志的功能。 //- 首先，通过time.Now()获取当前时间作为开始时间，并使用logger.V(4).Info记录开始同步部署的日志，其中包含了部署的名称、命名空间和开始时间。 //- 然后，使用defer关键字定义了一个匿名函数，在函数执行结束后会自动执行该函数。 //该匿名函数使用logger.V(4).Info记录结束同步部署的日志，其中包含了部署的名称、命名空间和同步部署所花费的时间。 //通过这种方式，可以在日志中方便地查看部署的同步状态和耗时。 deployment, err := dc.dLister.Deployments(namespace).Get(name) if errors.IsNotFound(err) { logger.V(2).Info("Deployment has been deleted", "deployment", klog.KRef(namespace, name)) return nil } if err != nil { return err } //该函数通过调用dc.dLister.Deployments(namespace).Get(name)获取指定命名空间中名为name的部署对象。 //如果该部署对象不存在，则记录日志并返回nil；如果存在其他错误，则直接返回错误。 // Deep-copy otherwise we are mutating our cache. // TODO: Deep-copy only when needed. d := deployment.DeepCopy() everything := metav1.LabelSelector{} if reflect.DeepEqual(d.Spec.Selector, &amp;everything) { dc.eventRecorder.Eventf(d, v1.EventTypeWarning, "SelectingAll", "This deployment is selecting all pods. A non-empty selector is required.") if d.Status.ObservedGeneration < d.Generation { d.Status.ObservedGeneration = d.Generation dc.client.AppsV1().Deployments(d.Namespace).UpdateStatus(ctx, d, metav1.UpdateOptions{}) } return nil } //这段Go代码中的函数主要功能是对一个deployment对象进行深拷贝，并检查其Selector是否等于一个空的LabelSelector对象。 //如果是，则记录一条警告事件并更新deployment的状态。 // List ReplicaSets owned by this Deployment, while reconciling ControllerRef // through adoption/orphaning. rsList, err := dc.getReplicaSetsForDeployment(ctx, d) if err != nil { return err } // List all Pods owned by this Deployment, grouped by their ReplicaSet. // Current uses of the podMap are: // // * check if a Pod is labeled correctly with the pod-template-hash label. // * check that no old Pods are running in the middle of Recreate Deployments. podMap, err := dc.getPodMapForDeployment(d, rsList) if err != nil { return err } if d.DeletionTimestamp != nil { return dc.syncStatusOnly(ctx, d, rsList) } // Update deployment conditions with an Unknown condition when pausing/resuming // a deployment. In this way, we can be sure that we won&#39;t timeout when a user // resumes a Deployment with a set progressDeadlineSeconds. if err = dc.checkPausedConditions(ctx, d); err != nil { return err } if d.Spec.Paused { return dc.sync(ctx, d, rsList) } // rollback is not re-entrant in case the underlying replica sets are updated with a new // revision so we should ensure that we won&#39;t proceed to update replica sets until we // make sure that the deployment has cleaned up its rollback spec in subsequent enqueues. if getRollbackTo(d) != nil { return dc.rollback(ctx, d, rsList) } scalingEvent, err := dc.isScalingEvent(ctx, d, rsList) if err != nil { return err } if scalingEvent { return dc.sync(ctx, d, rsList) } switch d.Spec.Strategy.Type { case apps.RecreateDeploymentStrategyType: return dc.rolloutRecreate(ctx, d, rsList, podMap) case apps.RollingUpdateDeploymentStrategyType: return dc.rolloutRolling(ctx, d, rsList) } return fmt.Errorf("unexpected deployment strategy type: %s", d.Spec.Strategy.Type) } //该函数主要负责处理Deployment的更新和同步逻辑。 //1. 首先，函数会获取该Deployment所拥有的ReplicaSet列表和Pod的映射关系。 //2. 如果该Deployment已被删除，则只同步状态。 //3. 检查是否暂停，若暂停则只进行同步操作。 //4. 如果需要回滚，则执行回滚操作。 //5. 检测是否为缩放事件，若是则进行同步操作。 //6. 根据Deployment的策略类型（Recreate或RollingUpdate），执行相应的更新操作。 //7. 如果遇到意外的部署策略类型，返回错误。'><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><title>2024-04-09 K8S控制器之 deployment_controller.go源码解读 | Guichen's Blog</title>
<link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://qq547475331.github.io/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8B-deployment_controllergo%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-deploymentcontrollergo-yuan-ma-jie-du/><link rel=stylesheet href=/book.min.6c8b9d2a1fc95075ed7da46ca81060b39add8fff6741ac51259f768929281e2c.css integrity="sha256-bIudKh/JUHXtfaRsqBBgs5rdj/9nQaxRJZ92iSkoHiw=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.b51dc29324b667a5747476d9b58a36226da39b29beeb41b304370295aec323de.js integrity="sha256-tR3CkyS2Z6V0dHbZtYo2Im2jmym+60GzBDcCla7DI94=" crossorigin=anonymous></script></head><script src=https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.2.3/mermaid.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){mermaid.initialize({startOnLoad:!0});let e=document.querySelectorAll("pre > code.language-mermaid");e.forEach(e=>{let t=document.createElement("div");t.classList.add("mermaid"),t.innerHTML=e.innerText,e.parentNode.replaceWith(t)}),mermaid.init(void 0,document.querySelectorAll(".mermaid"))})</script><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>Guichen's Blog</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><a href=/docs/2025-9-29-vercel%E5%88%9B%E5%A7%8B%E4%BA%BA%E5%AF%B9%E8%AF%9D/>2025-9-29 Vercel创始人对话</a></li><li><a href=/docs/2025-9-28-spec-workflow-mcp/>2025-9-28 spec-workflow-mcp</a></li><li><a href=/docs/2025-9-28-chrome-mcp-tools/>2025-9-28 chrome-devtools-mcp</a></li><li><a href=/docs/2025-9-19-%E7%90%86%E8%A7%A3/>2025-9-19 理解</a></li><li><a href=/docs/2025-9-10-%E6%8F%90%E7%A4%BA%E8%AF%8D/>2025-9-10 提示词</a></li><li><a href=/docs/2025-9-9-music/>2025-9-09 music资源</a></li><li><a href=/docs/2025-8-29-%E8%A1%A8%E5%8D%95%E5%88%B0%E9%9B%86%E7%BE%A4/>2025-8-28 表单到集群</a></li><li><a href=/docs/2025-6-27-geminicli/>2025-6-27 geminicli</a></li><li><a href=/docs/2025-6-23-ingress-nginx-contrller-%E5%88%86%E6%9E%90/>2025-6-23 ingress nginx contrller 内存使用过高分析</a></li><li><a href=/docs/2025-6-20-oom/>2025-6-20 oom排查思路</a></li><li><a href=/docs/2025-6-16-fire%E8%A7%84%E5%88%99/>2025-6-16 Cursor RIPER-5规则</a></li><li><a href=/docs/2025-6-12-karmada/>2025-6-12 karmada介绍</a></li><li><a href=/docs/2025-6-12-flutter%E8%A7%84%E5%88%99/>2025-6-12 flutter规则</a></li><li><a href=/docs/2025-6-10-%E7%8B%AC%E7%AB%8B%E5%BC%80%E5%8F%91/>2025-6-10 独立开发</a></li><li><a href=/docs/2025-5-21-ingress%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/>2025-5-21 主Ingress副本变为0后报503问题分析</a></li><li><a href=/docs/2025-5-7-%E6%8E%A5%E5%8D%95app/>2025-5-07 接单app设计</a></li><li><a href=/docs/2025-5-7-%E5%A5%BD%E5%BF%83%E6%80%81-app/>2025-5-07 好心态app</a></li><li><a href=/docs/2025-4-28-cursor-agent-%E6%8F%90%E7%A4%BA%E5%99%A8/>2025-4-28 cursor agent 提示器</a></li><li><a href=/docs/2025-4-16-%E8%87%AA%E7%A0%94k8s%E5%B9%B3%E5%8F%B0/>2025-4-16 自研k8s平台</a></li><li><a href=/docs/2025-4-16-sleep%E7%9D%A1%E7%9C%A0%E5%BA%94%E7%94%A8/>2025-4-16 sleep睡眠应用</a></li><li><a href=/docs/2025-4-16-paas%E8%AE%BE%E8%AE%A1/>2025-4-16 paas开发记录</a></li><li><a href=/docs/2025-4-16-cursoe-free-vip/>2025-4-16 Cursor Free VIP</a></li><li><a href=/docs/2025-4-16-boss%E7%9B%B4%E8%81%98%E8%87%AA%E5%8A%A8%E6%8A%95%E9%80%92/>2025-4-16 BOSS直聘自动投递</a></li><li><a href=/docs/2025-4-14-github%E6%8E%A8%E9%80%81/>2025-4-14 github推送</a></li><li><a href=/docs/2025-3-30-metallb/>2025-3-30 metallb</a></li><li><a href=/docs/2025-3-24-%E8%87%AA%E6%88%91%E4%BB%8B%E7%BB%8D/>2025-3-24 自我介绍</a></li><li><a href=/docs/2025-3-20-victoriametrics-%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84/>2025-3-20 victoriametrics高可用架构</a></li><li><a href=/docs/2025-3-20-victoriametrics%E6%9E%B6%E6%9E%84/>2025-3-20 victoriametrics 架构</a></li><li><a href=/docs/2025-3-20-victoriametrics%E5%92%8Cthanos%E5%AF%B9%E6%AF%94/>2025-3-20 VictoriaMetrics 和 Thanos 对比</a></li><li><a href=/docs/2025-3-20-thanos%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84/>2025-3-20 thanos高可用架构</a></li><li><a href=/docs/2025-3-20-thanos%E6%9E%B6%E6%9E%84/>2025-3-20 thanos架构</a></li><li><a href=/docs/2025-3-18-5w-pod%E5%8E%8B%E6%B5%8B%E5%A4%8D%E7%9B%98/>2025-3-18 5w pod压测复盘</a></li><li><a href=/docs/2025-3-14-%E7%81%AB%E5%B1%B1%E4%BA%91%E8%BF%81%E7%A7%BB%E5%B7%A5%E7%A8%8B%E5%B8%88%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/>2025-3-14 火山云迁移工程师面试记录</a></li><li><a href=/docs/2025-3-14-vivo%E9%9D%A2%E8%AF%95/>2025-3-14 vivo面试</a></li><li><a href=/docs/2025-3-13-istio%E6%B5%81%E9%87%8F%E5%88%86%E6%9E%90/>2025-3-13 istio流量分析</a></li><li><a href=/docs/2025-3-13-calico%E4%B8%89%E7%A7%8D%E6%A8%A1%E5%BC%8F%E4%B8%8B%E6%B5%81%E9%87%8F%E4%BC%A0%E8%BE%93%E8%B7%AF%E5%BE%84%E5%88%86%E6%9E%90/>2025-3-13 calico三种模式下流量传输</a></li><li><a href=/docs/2025-3-12-%E5%A1%94%E8%B5%9E%E9%9D%A2%E8%AF%95/>2025-3-12 塔赞面试</a></li><li><a href=/docs/2025-3-12-%E8%BF%BD%E8%A7%85%E9%9D%A2%E8%AF%95/>2025-3-12 追觅面试</a></li><li><a href=/docs/2025-3-8-k8s%E5%88%A0%E9%99%A4pod-deployment%E7%9A%84%E6%B5%81%E7%A8%8B%E5%9B%BE%E8%AF%A6%E8%A7%A3/>2025-3-08 k8s删除pod或deployment的流程图详解</a></li><li><a href=/docs/2025-3-8-k8s%E5%88%9B%E5%BB%BApod-deployment%E6%B5%81%E7%A8%8B%E5%9B%BE%E8%AF%A6%E8%A7%A3/>2025-3-08 k8s创建pod流程图详解</a></li><li><a href=/docs/2025-2-28-prometheus%E9%A2%98%E7%9B%AE/>2025-2-28 prometheus面试题</a></li><li><a href=/docs/2025-2-26-%E9%9D%A2%E8%AF%950225/>2025-2-25 面试0225</a></li><li><a href=/docs/2025-2-24-%E9%AB%98%E7%BA%A7%E8%BF%90%E7%BB%B4%E9%9D%A2%E8%AF%95%E9%A2%98_ai_linux%E9%83%A8%E5%88%86/>2025-2-24 高级运维面试题-linux部分</a></li><li><a href=/docs/2025-2-24-%E4%B8%AD%E7%BA%A7%E8%BF%90%E7%BB%B4%E9%9D%A2%E8%AF%95%E9%A2%98_%E9%A2%98%E7%9B%AE/>2025-2-24 中级运维面试题</a></li><li><a href=/docs/2025-2-24-%E9%9D%A2%E8%AF%950224/>2025-2-24 0224面试</a></li><li><a href=/docs/2025-2-20-%E9%9D%A2%E8%AF%950220/>2025-2-20 面试0220</a></li><li><a href=/docs/2025-2-19-%E9%9D%A2%E8%AF%950219/>2025-2-19 面试0219</a></li><li><a href=/docs/2025-2-18-%E9%9D%A2%E8%AF%95/>2025-2-18 面试2025-0218</a></li><li><a href=/docs/2025-2-26-k8s%E7%9B%B8%E5%85%B3/>2025-2-16 k8s题目</a></li><li><a href=/docs/2025-2-12-%E9%9D%A2%E8%AF%950212/>2025-2-12 面试0212</a></li><li><a href=/docs/2025-2-11-%E9%9D%A2%E8%AF%950211/>2025-2-11 面试2025-02-11</a></li><li><a href=/docs/2025-2-7-%E8%AE%A1%E5%88%922/>2025-2-07 美国码农计划</a></li><li><a href=/docs/2025-2-7-%E8%AE%A1%E5%88%92/>2025-2-07 美国码农薪酬</a></li><li><a href=/docs/2025-2-7-k8s%E7%BB%84%E4%BB%B6/>2025-2-07 k8s组件</a></li><li><a href=/docs/2025-10-27-reconciler%E6%A8%A1%E5%BC%8F/>2025-10-27 informer模式3</a></li><li><a href=/docs/2025-10-23-informer3/>2025-10-23 informer模式3</a></li><li><a href=/docs/2025-10-23-informer2/>2025-10-23 informer模式2</a></li><li><a href=/docs/2025-10-23-informer/>2025-10-23 informer模式</a></li><li><a href=/docs/2025-1-16-k8s%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E6%8C%87%E5%8D%97/>2025-1-16 k8s常见故障指南</a></li><li><a href=/docs/2025-1-1-%E8%A6%81%E4%B8%8D%E8%A6%81%E5%88%9B%E4%B8%9A/>2025-1-1 要不要创业</a></li><li><a href=/docs/2025-1-1-%E6%97%A9%E6%9C%9F%E6%A8%A1%E5%BC%8F/>2025-1-1 早期模式</a></li><li><a href=/docs/2025-1-1-%E5%A4%A7%E5%A0%B0%E6%B2%B3-%E6%88%91%E7%9A%84%E4%BF%9D%E5%A7%86/>2025-1-1 大堰河-我的保姆</a></li><li><a href=/docs/2025-1-1-%E5%88%9D%E5%88%9B%E5%85%AC%E5%8F%B8/>2025-1-1 初创公司</a></li><li><a href=/docs/2025-1-1-%E5%88%9B%E4%B8%9A%E8%80%85%E4%BA%A4%E6%B5%81/>2025-1-1 创业者交流</a></li><li><a href=/docs/2025-1-1-%E5%88%9B%E4%B8%9A%E7%82%B9%E5%AD%90/>2025-1-1 创业点子</a></li><li><a href=/docs/2025-1-1-sealos%E8%8E%B7%E6%8A%95/>2025-1-1 sealos获投</a></li><li><a href=/docs/2024-12-10-docker-registrry/>2024-12-10 docker registrry</a></li><li><a href=/docs/2024-12-09-openstack-ssh%E8%BF%9E%E6%8E%A5/>2024-12-09 openstack ssh连接</a></li><li><a href=/docs/2024-12-08-mutilpass%E9%83%A8%E7%BD%B2openstack/>2024-12-09 mutilpass部署openstack devstack形式</a></li><li><a href=/docs/2024-12-09-helmchart-%E9%83%A8%E7%BD%B2flask%E5%BA%94%E7%94%A8/>2024-12-09 helmchart 部署flask应用</a></li><li><a href=/docs/2024-12-09-docker-daemon.json/>2024-12-09 docker daemon.json</a></li><li><a href=/docs/2024-12-08-%E5%9D%97%E5%AD%98%E5%82%A8%E5%92%8C%E5%AF%B9%E8%B1%A1%E5%82%A8%E5%AD%98%E5%8C%BA%E5%88%AB/>2024-12-08 块存储和对象储存区别</a></li><li><a href=/docs/2024-12-08-openstack%E9%9C%80%E8%A6%81%E5%87%A0%E5%8F%B0%E8%99%9A%E6%8B%9F%E6%9C%BA/>2024-12-08 openstack需要几台虚拟机</a></li><li><a href=/docs/2024-12-08-openstack%E5%92%8Ckubernetes%E5%8C%BA%E5%88%AB/>2024-12-08 openstack和kubernetes区别</a></li><li><a href=/docs/2024-12-08-nano%E6%93%8D%E4%BD%9C/>2024-12-08 nano操作</a></li><li><a href=/docs/2024-12-08-mutilpass%E6%93%8D%E4%BD%9C/>2024-12-08 mutilpass操作</a></li><li><a href=/docs/2024-12-08-devstack/>2024-12-08 devstack</a></li><li><a href=/docs/2024-12-07-microk8s/>2024-12-07 microk8s</a></li><li><a href=/docs/2024-12-05-kubeasz%E9%83%A8%E7%BD%B2k8s/>2024-12-05 kubeasz部署k8s</a></li><li><a href=/docs/2024-10-20-%E5%88%9B%E5%BB%BA%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/>2024-10-20 使用 Keepalived 和 HAproxy 创建高可用 Kubernetes 集群</a></li><li><a href=/docs/%E9%A1%B6%E7%BA%A7devops%E5%B7%A5%E5%85%B7%E5%A4%A7%E7%9B%98%E7%82%B9-ding-ji-devops-gong-ju-da-pan-dian/>2024-08-02 顶级devops工具大盘点</a></li><li><a href=/docs/%E6%B8%85%E7%90%86docker%E9%95%9C%E5%83%8F-qing-li-docker-jing-xiang/>2024-08-02 清理docker镜像</a></li><li><a href=/docs/%E6%9E%84%E5%BB%BA%E5%AE%B9%E5%99%A8%E9%95%9C%E5%83%8F%E5%88%A9%E5%99%A8buildkit-gou-jian-rong-qi-jing-xiang-li-qi-buildkit/>2024-08-02 构建容器镜像利器buildkit</a></li><li><a href=/docs/%E6%98%AF%E6%8A%80%E6%9C%AF%E5%A4%A7%E7%A5%9E%E8%BF%98%E6%98%AF%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84%E9%83%A8%E7%9A%84%E7%A5%B8%E5%AE%B3-shi-ji-shu-da-shen-hai-shi-ji-chu-jia-gou-bu-de-huo-hai/>2024-08-02 是技术大神还是基础架构部的祸害</a></li><li><a href=/docs/%E6%90%AD%E4%B8%AA%E6%97%A5%E5%BF%97%E6%89%8B%E6%9C%BA%E7%B3%BB%E7%BB%9F%E4%B8%8D%E9%A6%99%E5%90%97-da-ge-ri-zhi-shou-ji-xi-tong-bu-xiang-ma/>2024-08-02 搭个日志手机系统不香吗</a></li><li><a href=/docs/%E6%88%91%E5%8F%AA%E6%83%B3%E5%81%9A%E6%8A%80%E6%9C%AF-%E8%B5%B0%E6%8A%80%E6%9C%AF%E8%B7%AF%E7%BA%BF-wo-zhi-xiang-zuo-ji-shu-zou-ji-shu-lu-xian/>2024-08-02 我只想做技术 走技术路线</a></li><li><a href=/docs/%E5%B8%B8%E8%A7%81linux%E8%BF%90%E7%BB%B4%E9%9D%A2%E8%AF%95%E9%A2%98-chang-jian-linux-yun-wei-mian-shi-ti/>2024-08-02 常见linux运维面试题</a></li><li><a href=/docs/%E5%A4%A7%E5%8E%82%E6%80%BB%E7%BB%93nginx%E9%AB%98%E5%B9%B6%E5%8F%91%E4%BC%98%E5%8C%96%E7%AC%94%E8%AE%B0-da-chang-zong-jie-nginx-gao-bing-fa-you-hua-bi-ji/>2024-08-02 大厂总结nginx高并发优化笔记</a></li><li><a href=/docs/%E5%8F%B2%E4%B8%8A%E6%9C%80%E7%89%9Bjenkins-pipeline%E6%B5%81%E6%B0%B4%E7%BA%BF%E8%AF%A6%E8%A7%A3-shi-shang-zui-niu-jenkinspipeline-liu-shui-xian-xiang-jie/>2024-08-02 史上最牛jenkins pipeline流水线详解</a></li><li><a href=/docs/teg%E4%B8%8Eistio%E9%9B%86%E6%88%90-teg-yu-istio-ji-cheng/>2024-08-02 TEG与istio集成</a></li><li><a href=/docs/prometheus-stack-prometheus-stack/>2024-08-02 prometheus-stack</a></li><li><a href=/docs/pixie-pixie/>2024-08-02 pixie</a></li><li><a href=/docs/nginx%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E6%83%8A%E7%BE%A4%E6%95%88%E5%BA%94-nginx-ru-he-jie-jue-jing-qun-xiao-ying/>2024-08-02 nginx如何解决惊群效应</a></li><li><a href=/docs/netctl%E6%A3%80%E6%B5%8B%E9%9B%86%E7%BE%A4pod%E9%97%B4%E8%BF%9E%E9%80%9A%E6%80%A7-netctl-jian-ce-ji-qun-pod-jian-lian-tong-xing/>2024-08-02 netctl检测集群pod间连通性</a></li><li><a href=/docs/linux%E8%BF%90%E7%BB%B4%E5%B7%A5%E7%A8%8B%E5%B8%8850%E4%B8%AA%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98-linux-yun-wei-gong-cheng-shi-50-ge-chang-jian-mian-shi-ti/>2024-08-02 linux运维工程师50个常见面试题</a></li><li><a href=/docs/linux%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-%E4%B8%83%E4%B8%AA%E5%AE%9E%E6%88%98%E7%BB%8F%E9%AA%8C-linux-xi-tong-xing-neng-you-hua-qi-ge-shi-zhan-jing-yan/>2024-08-02 linux系统性能优化 七个实战经验</a></li><li><a href=/docs/linux-awk%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E5%99%A8-8%E4%B8%AA%E6%A1%88%E4%BE%8B-linuxawk-wen-ben-chu-li-qi-8-ge-an-li/>2024-08-02 linux awk文本处理器 8个案例</a></li><li><a href=/docs/kubewharf-kubewharf/>2024-08-02 kubewharf</a></li><li><a href=/docs/kruise%E5%8E%9F%E5%9C%B0%E5%8D%87%E7%BA%A7%E8%A7%A3%E6%9E%90-kruise-yuan-de-sheng-ji-jie-xi/>2024-08-02 kruise原地升级解析</a></li><li><a href=/docs/k8s%E9%9D%A2%E8%AF%95%E9%A2%98-k8s-mian-shi-ti/>2024-08-02 K8S面试题</a></li><li><a href=/docs/k8s%E8%83%8C%E5%90%8Eservice%E6%98%AF%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C%E7%9A%84-k8s-bei-hou-service-shi-ru-he-gong-zuo-de/>2024-08-02 k8s背后service是如何工作的</a></li><li><a href=/docs/k8s%E7%9A%84%E6%9C%80%E5%90%8E%E4%B8%80%E5%9D%97%E6%8B%BC%E5%9B%BE-dbpaas-k8s-de-zui-hou-yi-kuai-pin-tu-dbpaas/>2024-08-02 K8S的最后一块拼图</a></li><li><a href=/docs/istio%E9%83%A8%E7%BD%B2-istio-bu-shu/>2024-08-02 istio部署</a></li><li><a href=/docs/istio-ingress-gateway-istio-ingress-gateway/>2024-08-02 istio-ingress-gateway</a></li><li><a href=/docs/godel-scheduler-godel-scheduler/>2024-08-02 godel-scheduler</a></li><li><a href=/docs/dockerfile%E5%AE%9A%E5%88%B6%E4%B8%93%E5%B1%9E%E9%95%9C%E5%83%8F-dockerfile-ding-zhi-zhuan-shu-jing-xiang/>2024-08-02 dockerfile定制专属镜像</a></li><li><a href=/docs/33%E6%AC%BEgitops%E4%B8%8Edevops%E4%B8%BB%E6%B5%81%E7%B3%BB%E7%BB%9F-33-kuan-gitops-yu-devops-zhu-liu-xi-tong/>2024-08-02 33款gitops与devops主流系统</a></li><li><a href=/docs/2024-8-1-linux%E8%BF%90%E7%BB%B4%E5%B7%A5%E7%A8%8B%E5%B8%8850%E4%B8%AA%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/>2024-08-01 linux面试题</a></li><li><a href=/docs/2024-8-1-%E5%B8%B8%E8%A7%81linux%E8%BF%90%E7%BB%B4%E9%9D%A2%E8%AF%95%E9%A2%98%E6%89%BE%E5%B7%A5%E4%BD%9C%E7%9A%84%E5%BF%85%E7%9C%8B/>2024-08-01 linux运维面试题</a></li><li><a href=/docs/2024-8-1-kubernetes%E9%9D%A2%E8%AF%95%E9%A2%98/>2024-08-01 k8s面试题</a></li><li><a href=/docs/openkruise%E8%AF%A6%E7%BB%86%E8%A7%A3%E9%87%8A%E4%BB%A5%E5%8F%8A%E5%8E%9F%E5%9C%B0%E5%8D%87%E7%BA%A7%E5%8F%8A%E5%85%A8%E9%93%BE%E8%B7%AF%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83%E6%96%B9%E6%A1%88-openkruise-xiang-xi-jie-shi-yi-ji-yuan-de-sheng-ji-ji-quan-lian-lu-hui-du-fa-bu-fang-an/>2024-07-22 OpenKruise详细解释以及原地升级及全链路灰度发布方案</a></li><li><a href=/docs/k8s%E4%B9%8Bingress-nginx%E5%8E%9F%E7%90%86%E5%8F%8A%E9%85%8D%E7%BD%AE-k8s-zhi-ingress-nginx-yuan-li-ji-pei-zhi/>2024-07-05 K8S之ingress-nginx原理及配置</a></li><li><a href=/docs/%E4%BD%BF%E7%94%A8cloudflarecf%E6%90%AD%E5%BB%BAdockerhub%E4%BB%A3%E7%90%86-shi-yong-cloudflarecf-da-jian-dockerhub-dai-li/>2024-06-28 使用cloudflare(CF)搭建dockerhub代理</a></li><li><a href=/docs/2024-5-14-%E5%8D%95master%E5%8D%95etcd%E6%94%B9%E9%80%A0/>2024-05-01 单master单etcd改造为3master3etcd</a></li><li><a href=/docs/2024-4-17-%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93/>2024-04-17 面试总结</a></li><li><a href=/docs/%E5%A6%82%E4%BD%95%E4%B8%BAk8s%E4%BF%9D%E9%A9%BE%E6%8A%A4%E8%88%AA-ru-he-wei-k8s-bao-jia-hu-hang/>2024-04-16 如何为K8S保驾护航</a></li><li><a href=/docs/k8s%E5%A6%82%E4%BD%95%E8%8E%B7%E5%BE%97-ip-k8s-ru-he-huo-de-ip/>2024-04-16 K8S如何获得 IP</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Bstateful_setgo%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-statefulsetgo-yuan-ma-jie-du/>2024-04-10 K8S控制器之stateful_set.go源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Bstateful_set_status_updatego%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-statefulsetstatusupdatego-yuan-ma-jie-du/>2024-04-10 K8S控制器之stateful_set_status_update.go源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Bstateful_set_controlgo%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-statefulsetcontrolgo-yuan-ma-jie-du/>2024-04-10 K8S控制器之stateful_set_control.go源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Bstateful_pod_controlgo%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-statefulpodcontrolgo-yuan-ma-jie-du/>2024-04-10 K8S控制器之stateful_pod_control.go源码解读</a></li><li><a href=/docs/k8s%E8%B0%83%E5%BA%A6%E5%99%A8-extendergo-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-diao-du-qi-extendergo-yuan-ma-jie-du/>2024-04-09 K8S调度器 extender.go 源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Bsyncgo-%E5%90%8C%E6%AD%A5-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-syncgo-tong-bu-yuan-ma-jie-du/>2024-04-09 K8S控制器之sync.go 同步 源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Brollbackgo-%E5%9B%9E%E6%BB%9A-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-rollbackgo-hui-gun-yuan-ma-jie-du/>2024-04-09 K8S控制器之rollback.go 回滚 源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Brecreatego-%E9%87%8D%E5%BB%BA-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-recreatego-zhong-jian-yuan-ma-jie-du/>2024-04-09 K8S控制器之recreate.go 重建 源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8B-schedulergo-%E8%B0%83%E5%BA%A6%E5%99%A8-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-schedulergo-diao-du-qi-yuan-ma-jie-du/>2024-04-09 K8S控制器之 scheduler.go 调度器 源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8B-rollinggo-%E6%BB%9A%E5%8A%A8%E6%9B%B4%E6%96%B0-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-rollinggo-gun-dong-geng-xin-yuan-ma-jie-du/>2024-04-09 K8S控制器之 rolling.go 滚动更新 源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8B-progressgo-%E8%BF%9B%E5%BA%A6-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-progressgo-jin-du-yuan-ma-jie-du/>2024-04-09 K8S控制器之 progress.go 进度 源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8B-deployment_controllergo%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-deploymentcontrollergo-yuan-ma-jie-du/ class=active>2024-04-09 K8S控制器之 deployment_controller.go源码解读</a></li><li><a href=/docs/k8s-%E8%B0%83%E5%BA%A6%E5%99%A8-scheduler_onego-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-diao-du-qi-scheduleronego-yuan-ma-jie-du/>2024-04-09 K8S 调度器 scheduler_one.go 源码解读</a></li><li><a href=/docs/%E5%BD%BB%E6%82%9F%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C-che-wu-rong-qi-wang-luo/>2024-04-07 彻悟容器网络</a></li><li><a href=/docs/%E9%9D%A2%E8%AF%95%E7%94%A8-golang-%E6%89%8B%E6%92%B8-lru-mian-shi-yong-golang-shou-lu-lru/>2024-04-03 面试用 Golang 手撸 LRU</a></li><li><a href=/docs/%E8%87%AA%E5%8A%A8%E5%B1%8F%E8%94%BDip%E6%94%BB%E5%87%BB-zi-dong-ping-bi-ip-gong-ji/>2024-04-03 自动屏蔽IP攻击</a></li><li><a href=/docs/%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85kubephere-li-xian-an-zhuang-kubephere/>2024-04-03 离线安装kubephere</a></li><li><a href=/docs/%E7%A3%81%E7%9B%98%E6%95%B0%E6%8D%AE%E6%81%A2%E5%A4%8D-ci-pan-shu-ju-hui-fu/>2024-04-03 磁盘数据恢复</a></li><li><a href=/docs/%E6%B8%85%E7%90%86%E6%AE%8B%E7%95%99%E7%9A%84calico%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6-qing-li-can-liu-de-calico-wang-luo/>2024-04-03 清理残留的calico网络插件</a></li><li><a href=/docs/%E6%B5%81%E9%87%8F%E4%BD%95%E5%A4%84%E6%9D%A5%E4%BD%95%E5%A4%84%E5%8E%BB-liu-liang-he-chu-lai-he-chu-qu/>2024-04-03 流量何处来何处去</a></li><li><a href=/docs/%E6%9E%81%E5%A4%A7%E6%8F%90%E9%AB%98%E5%B7%A5%E4%BD%9C%E6%95%88%E7%8E%87%E7%9A%84-linux-%E5%91%BD%E4%BB%A4-ji-da-ti-gao-gong-zuo-xiao-lv-de-linux-ming-ling/>2024-04-03 极大提高工作效率的 Linux 命令</a></li><li><a href=/docs/%E6%96%87%E5%AD%A6%E7%9A%84%E6%95%85%E4%B9%A1-wen-xue-de-gu-xiang/>2024-04-03 文学的故乡</a></li><li><a href=/docs/%E6%90%9E%E6%87%82k8s%E9%89%B4%E6%9D%83-gao-dong-k8s-jian-quan/>2024-04-03 搞懂K8S鉴权</a></li><li><a href=/docs/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E5%8E%9F%E7%90%86-rong-qi-wang-luo-yuan-li/>2024-04-03 容器网络原理</a></li><li><a href=/docs/%E5%AE%B9%E5%99%A8%E7%9A%84%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E4%B8%80-overlayfs-%E5%8E%9F%E7%90%86-rong-qi-de-wen-jian-xi-tong--yi-overlayfs-yuan-li/>2024-04-03 容器的文件系统 OverlayFS 原理</a></li><li><a href=/docs/%E5%AE%B9%E5%99%A8%E5%8E%9F%E7%90%86-rong-qi-yuan-li/>2024-04-03 容器原理</a></li><li><a href=/docs/%E5%AE%B9%E5%99%A8%E5%86%85%E7%9A%84-1-%E5%8F%B7%E8%BF%9B%E7%A8%8B-rong-qi-nei-de-1-hao-jin-cheng/>2024-04-03 容器内的 1 号进程</a></li><li><a href=/docs/%E5%AE%B9%E5%99%A8%E4%B8%AD%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90%E4%BB%A5%E5%8F%8A%E4%B8%8D%E5%90%8Cdnspolicy%E5%AF%B9%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90%E7%9A%84%E5%BD%B1%E5%93%8D-rong-qi-zhong-yu-ming-jie-xi-yi-ji-bu-tong-dnspolicy-dui-yu-ming-jie-xi-de-ying-xiang/>2024-04-03 容器中域名解析以及不同dnspolicy对域名解析的影响</a></li><li><a href=/docs/%E5%A6%82%E4%BD%95%E8%B0%83%E8%AF%95-crash-%E5%AE%B9%E5%99%A8%E7%9A%84%E7%BD%91%E7%BB%9C-ru-he-diao-shi-crash-rong-qi-de-wang-luo/>2024-04-03 如何调试 crash 容器的网络</a></li><li><a href=/docs/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8tekton%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BAcicd%E5%B9%B3%E5%8F%B0-ru-he-shi-yong-tekton-kuai-su-da-jian-cicd-ping-tai/>2024-04-03 如何使用tekton快速搭建CI/CD平台</a></li><li><a href=/docs/%E5%A4%A7%E8%A7%84%E6%A8%A1%E5%B9%B6%E5%8F%91%E4%B8%8B%E5%A6%82%E4%BD%95%E5%8A%A0%E5%BF%AB-pod-%E5%90%AF%E5%8A%A8%E9%80%9F%E5%BA%A6-da-gui-mo-bing-fa-xia-ru-he-jia-kuai-pod-qi-dong-su-du/>2024-04-03 大规模并发下如何加快 Pod 启动速度</a></li><li><a href=/docs/%E4%BD%BF%E7%94%A8kubernees-leases-%E8%BD%BB%E6%9D%BE%E5%AE%9E%E7%8E%B0leader-election-shi-yong-kuberneesleases-qing-song-shi-xian-leaderelection/>2024-04-03 使用kubernees leases 轻松实现leader election</a></li><li><a href=/docs/%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2k8s%E5%8A%A0%E8%8A%82%E7%82%B9%E6%93%8D%E4%BD%9C-er-jin-zhi-bu-shu-k8s-jia-jie-dian-cao-zuo/>2024-04-03 二进制部署K8S加节点操作</a></li><li><a href=/docs/%E4%B8%A4%E5%BC%A0%E5%9B%BE%E5%85%A8%E9%9D%A2%E7%90%86%E8%A7%A3k8s%E5%8E%9F%E7%90%86-liang-zhang-tu-quan-mian-li-jie-k8s-yuan-li/>2024-04-03 两张图全面理解K8S原理</a></li><li><a href=/docs/ssl%E8%AF%81%E4%B9%A6%E8%87%AA%E7%AD%BE%E5%8F%91-ssl-zheng-shu-zi-qian-fa/>2024-04-03 ssl证书自签发</a></li><li><a href=/docs/prometheus%E4%BC%81%E4%B8%9A%E7%BA%A7%E7%9B%91%E6%8E%A7%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93-prometheus-qi-ye-ji-jian-kong-shi-yong-zong-jie/>2024-04-03 prometheus企业级监控使用总结</a></li><li><a href=/docs/metallb-l2-%E5%8E%9F%E7%90%86-metallbl2-yuan-li/>2024-04-03 MetalLB L2 原理</a></li><li><a href=/docs/linux-%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E5%A4%A7%E5%85%A8-linux-xing-neng-you-hua-da-quan/>2024-04-03 Linux 性能优化大全</a></li><li><a href=/docs/kubernetes-%E8%AF%81%E4%B9%A6%E8%AF%A6%E8%A7%A3%E9%89%B4%E6%9D%83-kubernetes-zheng-shu-xiang-jie--jian-quan-/>2024-04-03 Kubernetes 证书详解(鉴权)</a></li><li><a href=/docs/kubernetes-%E8%AF%81%E4%B9%A6%E8%AF%A6%E8%A7%A3%E8%AE%A4%E8%AF%81-kubernetes-zheng-shu-xiang-jie--ren-zheng-/>2024-04-03 Kubernetes 证书详解(认证)</a></li><li><a href=/docs/kubernetes-%E6%BA%90%E7%A0%81%E7%BB%93%E6%9E%84-kubernetes-yuan-ma-jie-gou/>2024-04-03 Kubernetes 源码结构</a></li><li><a href=/docs/kubernetes-api-kubernetesapi/>2024-04-03 Kubernetes API</a></li><li><a href=/docs/kubekey%E6%B7%BB%E5%8A%A0%E6%96%B0%E8%8A%82%E7%82%B9-kubekey-tian-jia-xin-jie-dian/>2024-04-03 kubekey添加新节点</a></li><li><a href=/docs/k8s%E9%9D%A2%E8%AF%95%E5%AE%9D%E5%85%B8-k8s-mian-shi-bao-dian/>2024-04-03 K8S面试宝典</a></li><li><a href=/docs/k8s%E9%9D%A2%E8%AF%95%E5%A4%A7%E5%85%A8-k8s-mian-shi-da-quan/>2024-04-03 K8S面试大全</a></li><li><a href=/docs/k8s%E8%BF%90%E7%BB%B4%E4%B9%8B%E6%B8%85%E7%90%86%E7%A3%81%E7%9B%98-k8s-yun-wei-zhi-qing-li-ci-pan/>2024-04-03 k8s运维之清理磁盘</a></li><li><a href=/docs/k8s%E8%B0%83%E8%AF%95pod-k8s-diao-shi-pod/>2024-04-03 K8S调试POD</a></li><li><a href=/docs/k8s%E7%9A%84pod%E7%B1%BB%E5%9E%8B-k8s-de-pod-lei-xing/>2024-04-03 K8S的POD类型</a></li><li><a href=/docs/k8s%E5%BA%94%E7%94%A8%E7%9A%84%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5-k8s-ying-yong-de-zui-jia-shi-jian/>2024-04-03 k8s应用的最佳实践</a></li><li><a href=/docs/k8s%E5%91%BD%E4%BB%A4%E6%8C%87%E5%8D%97-k8s-ming-ling-zhi-nan/>2024-04-03 K8S命令指南</a></li><li><a href=/docs/k8s%E5%8E%9F%E5%9C%B0%E5%8D%87%E7%BA%A7-k8s-yuan-de-sheng-ji/>2024-04-03 K8S原地升级</a></li><li><a href=/docs/k8s-%E6%8E%A2%E9%92%88%E5%8E%9F%E7%90%86-k8s-tan-zhen-yuan-li/>2024-04-03 K8S 探针原理</a></li><li><a href=/docs/k8s-%E5%BC%80%E5%8F%91%E5%8F%AF%E4%B8%8D%E6%AD%A2-crud-k8s-kai-fa-ke-bu-zhi-crud/>2024-04-03 K8S 开发可不止 CRUD</a></li><li><a href=/docs/k8s-gpt-k8sgpt/>2024-04-03 K8S GPT</a></li><li><a href=/docs/k8s-csi-openebs%E5%8E%9F%E7%90%86-k8scsiopenebs-yuan-li/>2024-04-03 K8S csi openebs原理</a></li><li><a href=/docs/helm-chart%E5%92%8Crepo-helmchart-he-repo/>2024-04-03 helm chart和repo</a></li><li><a href=/docs/flanel%E7%BD%91%E7%BB%9C-flanel-wang-luo/>2024-04-03 flanel网络</a></li><li><a href=/docs/etcd%E7%A8%B3%E5%AE%9A%E6%80%A7%E5%8F%8A%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E5%AE%9E%E8%B7%B5-etcd-wen-ding-xing-ji-xing-neng-you-hua-shi-jian/>2024-04-03 ETCD稳定性及性能优化实践</a></li><li><a href=/docs/etcd%E5%A4%87%E4%BB%BD-etcd-bei-fen/>2024-04-03 ETCD备份</a></li><li><a href=/docs/docker%E9%87%8D%E8%A6%81%E7%9A%84%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86%E7%82%B9-docker-zhong-yao-de-wang-luo-zhi-shi-dian/>2024-04-03 Docker重要的网络知识点</a></li><li><a href=/docs/dockerfile%E7%9A%84copy%E5%92%8Cadd%E7%9A%84%E5%8C%BA%E5%88%AB-dockerfile-de-copy-he-add-de-qu-bie/>2024-04-03 dockerfile的copy和add的区别</a></li><li><a href=/docs/coredns%E4%B9%8B%E5%85%89-coredns-zhi-guang/>2024-04-03 COREDNS之光</a></li><li><a href=/docs/containerd-%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C-containerd-ji-ben-cao-zuo/>2024-04-03 Containerd 基本操作</a></li><li><a href=/docs/cni%E6%8F%92%E4%BB%B6%E9%80%89%E5%9E%8B-cni-cha-jian-xuan-xing/>2024-04-03 CNI插件选型</a></li><li><a href=/docs/client-go-%E6%9E%B6%E6%9E%84-client-go-jia-gou/>2024-04-03 Client-go 架构</a></li><li><a href=/docs/client-go-%E5%9B%9B%E7%A7%8D%E5%AE%A2%E6%88%B7%E7%AB%AF-client-go-si-zhong-ke-hu-duan/>2024-04-03 Client-go 四种客户端</a></li><li><a href=/docs/cicd%E6%80%9D%E8%80%83-cicd-si-kao/>2024-04-03 CICD思考</a></li><li><a href=/docs/calico%E7%BD%91%E7%BB%9C%E8%87%AA%E5%AE%9A%E4%B9%89-calico-wang-luo-zi-ding-yi/>2024-04-03 Calico网络自定义</a></li><li><a href=/docs/acme%E8%87%AA%E5%8A%A8%E6%9B%B4%E6%96%B0%E8%AF%81%E4%B9%A6-acme-zi-dong-geng-xin-zheng-shu/>2024-04-03 acme自动更新证书</a></li><li><a href=/docs/16%E4%B8%AA%E6%A6%82%E5%BF%B5%E5%B8%A6%E4%BD%A0%E5%85%A5%E9%97%A8-kubernetes-16-ge-gai-nian-dai-ni-ru-men-kubernetes/>2024-04-03 16个概念带你入门 Kubernetes</a></li><li><a href=/docs/%E9%9D%A2%E8%AF%950308-mian-shi-0308/>2024-04-03 面试0308</a></li><li><a href=/docs/600%E6%9D%A1%E6%9C%80%E5%BC%BAlinux%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93-600-tiao-zui-qiang-linux-ming-ling-zong-jie/>2024-04-03 600条最强linux命令总结</a></li><li><a href=/docs/16%E5%BC%A0%E7%A1%AC%E6%A0%B8%E5%9B%BE%E8%A7%A3k8s%E7%BD%91%E7%BB%9C-16-zhang-ying-he-tu-jie-k8s-wang-luo/>2024-04-03 16张硬核图解k8s网络</a></li><li><a href=/docs/k8s%E4%B9%8Bkubelet%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-zhi-kubelet-yuan-ma-jie-du/>2024-03-28 k8s之kubelet源码解读</a></li><li><a href=/docs/2024-3-19-%E4%B8%A4%E5%BC%A0%E5%9B%BE%E5%85%A8%E9%9D%A2%E7%90%86%E8%A7%A3k8s%E5%8E%9F%E7%90%86/>2024-03-19 两张图全面理解k8s原理</a></li><li><a href=/docs/2024-3-8-%E9%9D%A2%E8%AF%950308/>2024-03-08 面试</a></li><li><a href=/docs/2024-3-4-k8s%E6%B5%81%E9%87%8F%E9%93%BE%E8%B7%AF%E5%89%96%E6%9E%90/>2024-03-04 k8s流量链路剖析</a></li><li><a href=/docs/k8s-%E6%B5%81%E9%87%8F%E9%93%BE%E8%B7%AF%E5%89%96%E6%9E%90-k8s-liu-liang-lian-lu-pou-xi/>2024-03-04 K8S 流量链路剖析</a></li><li><a href=/docs/k8s-csi%E5%89%96%E6%9E%90%E6%BC%94%E8%BF%9B-k8scsi-pou-xi-yan-jin/>2024-03-04 K8S CSI剖析演进</a></li><li><a href=/docs/k8s-cni%E5%89%96%E6%9E%90%E6%BC%94%E8%BF%9B-k8scni-pou-xi-yan-jin/>2024-03-04 K8S CNI剖析演进</a></li><li><a href=/docs/2024-3-4-k8s-csi%E5%89%96%E6%9E%90/>2024-03-04 CSI剖析演进</a></li><li><a href=/docs/2024-3-4-cni%E5%89%96%E6%9E%90%E6%BC%94%E8%BF%9B/>2024-03-04 CNI剖析演进</a></li><li><a href=/docs/2024-2-26-%E9%9D%A2%E8%AF%95/>2024-02-26 面试</a></li><li><a href=/docs/2024-2-22-k8s%E9%9D%A2%E8%AF%95%E5%AE%9D%E5%85%B8/>2024-02-22 k8s面试宝典</a></li><li><a href=/docs/2024-2-22-k8s%E6%9E%B6%E6%9E%84%E5%B8%88%E9%9D%A2%E8%AF%95%E5%A4%A7%E5%85%A8/>2024-02-22 k8s架构师面试大全</a></li><li><a href=/docs/%E4%BD%BF%E7%94%A8-openfunction-%E5%9C%A8%E4%BB%BB%E4%BD%95%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD%E4%B8%8A%E8%BF%90%E8%A1%8C%E6%97%A0%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%B7%A5%E4%BD%9C%E8%B4%9F%E8%BD%BD-shi-yong-openfunction-zai-ren-he-ji-chu-she-shi-shang-yun-xing-wu-fu-wu-qi-gong-zuo-fu-zai/>2024-01-21 使用 OpenFunction 在任何基础设施上运行无服务器工作负载</a></li><li><a href=/docs/%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85%E9%9B%86%E7%BE%A4-li-xian-an-zhuang-ji-qun/>2023-09-28 离线安装集群</a></li><li><a href=/docs/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%AF%B4%E6%98%8E-cao-zuo-xi-tong-shuo-ming/>2023-09-28 操作系统说明</a></li><li><a href=/docs/%E5%BF%AB%E9%80%9F%E6%8C%87%E5%8D%97-kuai-su-zhi-nan/>2023-09-28 快速指南</a></li><li><a href=/docs/%E5%BC%80%E5%A7%8B%E4%BD%BF%E7%94%A8-cilium-kai-shi-shi-yong-cilium/>2023-09-28 开始使用 cilium</a></li><li><a href=/docs/%E5%A4%9A%E6%9E%B6%E6%9E%84%E6%94%AF%E6%8C%81-duo-jia-gou-zhi-chi/>2023-09-28 多架构支持</a></li><li><a href=/docs/%E5%85%AC%E6%9C%89%E4%BA%91%E4%B8%8A%E9%83%A8%E7%BD%B2-kubeasz-gong-you-yun-shang-bu-shu-kubeasz/>2023-09-28 公有云上部署</a></li><li><a href=/docs/%E4%B8%AA%E6%80%A7%E5%8C%96%E9%9B%86%E7%BE%A4%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE-ge-xing-hua-ji-qun-can-shu-pei-zhi/>2023-09-28 个性化集群参数配置</a></li><li><a href=/docs/network-check-network-check/>2023-09-28 network-check</a></li><li><a href=/docs/kube-router-%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6-kube-router-wang-luo-zu-jian/>2023-09-28 kube-router 网络组件</a></li><li><a href=/docs/ezctl-%E5%91%BD%E4%BB%A4%E8%A1%8C%E4%BB%8B%E7%BB%8D-ezctl-ming-ling-xing-jie-shao/>2023-09-28 ezctl 命令行介绍</a></li><li><a href=/docs/ex-lb-%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E9%83%A8%E7%BD%B2-ex-lb-fu-zai-jun-heng-bu-shu/>2023-09-28 EX-LB 负载均衡部署</a></li><li><a href=/docs/calico-%E9%85%8D%E7%BD%AE-bgp-route-reflectors-calico-pei-zhi-bgproutereflectors/>2023-09-28 calico 配置 BGP Route Reflectors</a></li><li><a href=/docs/07-%E5%AE%89%E8%A3%85%E9%9B%86%E7%BE%A4%E4%B8%BB%E8%A6%81%E6%8F%92%E4%BB%B6-07--an-zhuang-ji-qun-zhu-yao-cha-jian/>2023-09-28 15:26:42.651 07-安装集群主要插件</a></li><li><a href=/docs/08-k8s-%E9%9B%86%E7%BE%A4%E5%AD%98%E5%82%A8--k8s-ji-qun-cun-chu/>2023-09-28 08-K8S 集群存储</a></li><li><a href=/docs/06-%E5%AE%89%E8%A3%85%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6-06--an-zhuang-wang-luo-zu-jian/>2023-09-28 06-安装网络组件</a></li><li><a href=/docs/06-%E5%AE%89%E8%A3%85kube-ovn%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6-06--an-zhuang-kube-ovn-wang-luo-zu-jian/>2023-09-28 06-安装kube-ovn网络组件</a></li><li><a href=/docs/06-%E5%AE%89%E8%A3%85flannel%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6-06--an-zhuang-flannel-wang-luo-zu-jian/>2023-09-28 06-安装flannel网络组件</a></li><li><a href=/docs/06-%E5%AE%89%E8%A3%85cilium%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6-06--an-zhuang-cilium-wang-luo-zu-jian/>2023-09-28 06-安装cilium网络组件</a></li><li><a href=/docs/06-%E5%AE%89%E8%A3%85calico%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6-06--an-zhuang-calico-wang-luo-zu-jian/>2023-09-28 06-安装calico网络组件</a></li><li><a href=/docs/02-%E5%AE%89%E8%A3%85etcd%E9%9B%86%E7%BE%A4-02--an-zhuang-etcd-ji-qun/>2023-09-28 02-安装etcd集群</a></li><li><a href=/docs/00-%E9%9B%86%E7%BE%A4%E8%A7%84%E5%88%92%E5%92%8C%E5%9F%BA%E7%A1%80%E5%8F%82%E6%95%B0%E8%AE%BE%E5%AE%9A-00--ji-qun-gui-hua-he-ji-chu-can-shu-she-ding/>2023-09-28 00-集群规划和基础参数设定</a></li><li><a href=/docs/05-%E5%AE%89%E8%A3%85kube_node%E8%8A%82%E7%82%B9-05--an-zhuang-kubenode-jie-dian/>2023-09-28 05-安装kube_node节点</a></li><li><a href=/docs/04-%E5%AE%89%E8%A3%85kube_master%E8%8A%82%E7%82%B9-04--an-zhuang-kubemaster-jie-dian/>2023-09-28 04-安装kube_master节点</a></li><li><a href=/docs/03-%E5%AE%89%E8%A3%85%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6-03--an-zhuang-rong-qi-yun-xing-shi/>2023-09-28 03-安装容器运行时</a></li><li><a href=/docs/01-%E5%88%9B%E5%BB%BA%E8%AF%81%E4%B9%A6%E5%92%8C%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87-01--chuang-jian-zheng-shu-he-huan-jing-zhun-bei/>2023-09-28 01-创建证书和环境准备</a></li><li><a href=/docs/%E6%9C%89%E8%BF%993%E4%B8%AA%E8%BF%B9%E8%B1%A1%E4%BD%A0%E5%B0%B1%E8%AF%A5%E7%A6%BB%E8%81%8C%E4%BA%86-you-zhe-3-ge-ji-xiang--ni-jiu-gai-li-zhi-le/>2023-09-21 思考</a></li><li><a href=/docs/%E4%BD%BF%E7%94%A8-keepalived-%E5%92%8C-haproxy-%E5%88%9B%E5%BB%BA%E9%AB%98%E5%8F%AF%E7%94%A8-kubernetes-%E9%9B%86%E7%BE%A4-shi-yong-keepalived-he-haproxy-chuang-jian-gao-ke-yong-kubernetes-ji-qun/>2023-04-12 使用 Keepalived 和 HAproxy 创建高可用 Kubernetes 集群</a></li><li><a href=/docs/2025-4-20-%E6%80%A7%E5%90%8C%E6%84%8Fapp/>2025 4 20 性同意app</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h3>2024-04-09 K8S控制器之 deployment_controller.go源码解读</h3><label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents></nav></aside></header><article class="markdown book-article"><pre tabindex=0><code>/*
Copyright 2015 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the &#34;License&#34;);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &#34;AS IS&#34; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

// Package deployment contains all the logic for handling Kubernetes Deployments.
// It implements a set of strategies (rolling, recreate) for deploying an application,
// the means to rollback to previous versions, proportional scaling for mitigating
// risk, cleanup policy, and other useful features of Deployments.
package deployment

import (
	&#34;context&#34;
	&#34;fmt&#34;
	&#34;reflect&#34;
	&#34;time&#34;

	apps &#34;k8s.io/api/apps/v1&#34;
	v1 &#34;k8s.io/api/core/v1&#34;
	&#34;k8s.io/apimachinery/pkg/api/errors&#34;
	metav1 &#34;k8s.io/apimachinery/pkg/apis/meta/v1&#34;
	&#34;k8s.io/apimachinery/pkg/labels&#34;
	&#34;k8s.io/apimachinery/pkg/types&#34;
	utilruntime &#34;k8s.io/apimachinery/pkg/util/runtime&#34;
	&#34;k8s.io/apimachinery/pkg/util/wait&#34;
	appsinformers &#34;k8s.io/client-go/informers/apps/v1&#34;
	coreinformers &#34;k8s.io/client-go/informers/core/v1&#34;
	clientset &#34;k8s.io/client-go/kubernetes&#34;
	&#34;k8s.io/client-go/kubernetes/scheme&#34;
	v1core &#34;k8s.io/client-go/kubernetes/typed/core/v1&#34;
	appslisters &#34;k8s.io/client-go/listers/apps/v1&#34;
	corelisters &#34;k8s.io/client-go/listers/core/v1&#34;
	&#34;k8s.io/client-go/tools/cache&#34;
	&#34;k8s.io/client-go/tools/record&#34;
	&#34;k8s.io/client-go/util/workqueue&#34;
	&#34;k8s.io/kubernetes/pkg/controller&#34;
	&#34;k8s.io/kubernetes/pkg/controller/deployment/util&#34;
)

const (
	// maxRetries is the number of times a deployment will be retried before it is dropped out of the queue.
	// With the current rate-limiter in use (5ms*2^(maxRetries-1)) the following numbers represent the times
	// a deployment is going to be requeued:
	//
	// 5ms, 10ms, 20ms, 40ms, 80ms, 160ms, 320ms, 640ms, 1.3s, 2.6s, 5.1s, 10.2s, 20.4s, 41s, 82s
	maxRetries = 15
)

// controllerKind contains the schema.GroupVersionKind for this controller type.
var controllerKind = apps.SchemeGroupVersion.WithKind(&#34;Deployment&#34;)

// DeploymentController is responsible for synchronizing Deployment objects stored
// in the system with actual running replica sets and pods.
type DeploymentController struct {
	// rsControl is used for adopting/releasing replica sets.
	rsControl controller.RSControlInterface
	client    clientset.Interface

	eventBroadcaster record.EventBroadcaster
	eventRecorder    record.EventRecorder

	// To allow injection of syncDeployment for testing.
	syncHandler func(ctx context.Context, dKey string) error
	// used for unit testing
	enqueueDeployment func(deployment *apps.Deployment)

	// dLister can list/get deployments from the shared informer&#39;s store
	dLister appslisters.DeploymentLister
	// rsLister can list/get replica sets from the shared informer&#39;s store
	rsLister appslisters.ReplicaSetLister
	// podLister can list/get pods from the shared informer&#39;s store
	podLister corelisters.PodLister

	// dListerSynced returns true if the Deployment store has been synced at least once.
	// Added as a member to the struct to allow injection for testing.
	dListerSynced cache.InformerSynced
	// rsListerSynced returns true if the ReplicaSet store has been synced at least once.
	// Added as a member to the struct to allow injection for testing.
	rsListerSynced cache.InformerSynced
	// podListerSynced returns true if the pod store has been synced at least once.
	// Added as a member to the struct to allow injection for testing.
	podListerSynced cache.InformerSynced

	// Deployments that need to be synced
	queue workqueue.RateLimitingInterface
}

// NewDeploymentController creates a new DeploymentController.
func NewDeploymentController(ctx context.Context, dInformer appsinformers.DeploymentInformer, rsInformer appsinformers.ReplicaSetInformer, podInformer coreinformers.PodInformer, client clientset.Interface) (*DeploymentController, error) {
	eventBroadcaster := record.NewBroadcaster(record.WithContext(ctx))
	logger := klog.FromContext(ctx)
	dc := &amp;DeploymentController{
		client:           client,
		eventBroadcaster: eventBroadcaster,
		eventRecorder:    eventBroadcaster.NewRecorder(scheme.Scheme, v1.EventSource{Component: &#34;deployment-controller&#34;}),
		queue:            workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), &#34;deployment&#34;),
	}
	dc.rsControl = controller.RealRSControl{
		KubeClient: client,
		Recorder:   dc.eventRecorder,
	}
	//该函数用于创建一个新的DeploymentController。
	//- 接收上下文ctx以及多个informer和client作为参数。
	//- 创建一个新的事件广播器eventBroadcaster，并从上下文ctx中获取日志记录器logger 。
	//- 初始化一个DeploymentController结构体实例dc，包括client、eventBroadcaster、eventRecorder、queue和rsControl字段。
	//- dc.rsControl使用controller.RealRSControl结构体初始化，包括KubeClient和Recorder字段。
	//- 返回创建的dc实例和可能出现的错误。

	dInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
		AddFunc: func(obj interface{}) {
			dc.addDeployment(logger, obj)
		},
		UpdateFunc: func(oldObj, newObj interface{}) {
			dc.updateDeployment(logger, oldObj, newObj)
		},
		// This will enter the sync loop and no-op, because the deployment has been deleted from the store.
		DeleteFunc: func(obj interface{}) {
			dc.deleteDeployment(logger, obj)
		},
	})
	rsInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
		AddFunc: func(obj interface{}) {
			dc.addReplicaSet(logger, obj)
		},
		UpdateFunc: func(oldObj, newObj interface{}) {
			dc.updateReplicaSet(logger, oldObj, newObj)
		},
		DeleteFunc: func(obj interface{}) {
			dc.deleteReplicaSet(logger, obj)
		},
	})
	podInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
		DeleteFunc: func(obj interface{}) {
			dc.deletePod(logger, obj)
		},
	})
	//这段代码定义了三个事件处理器，分别用于处理deployment、replicaSet和pod的添加、更新和删除事件。
	//每个事件处理器都调用了对应的方法，例如对于deployment的添加事件，调用了dc.addDeployment(logger, obj)方法。
	//这些方法的具体实现可以根据具体业务逻辑进行编写。

	dc.syncHandler = dc.syncDeployment
	dc.enqueueDeployment = dc.enqueue

	dc.dLister = dInformer.Lister()
	dc.rsLister = rsInformer.Lister()
	dc.podLister = podInformer.Lister()
	dc.dListerSynced = dInformer.Informer().HasSynced
	dc.rsListerSynced = rsInformer.Informer().HasSynced
	dc.podListerSynced = podInformer.Informer().HasSynced
	return dc, nil
}

//这段代码是Go语言中的函数，主要功能是设置和初始化一个名为dc的对象，并返回该对象和nil。
//- 首先，将dc.syncHandler设置为dc.syncDeployment，将dc.enqueueDeployment设置为dc.enqueue。
//- 然后，通过dInformer、rsInformer和podInformer的Lister()方法，分别将dc.dLister、dc.rsLister和dc.podLister设置为相应的列表器。
//- 接着，通过dInformer、rsInformer和podInformer的Informer().HasSynced方法，分别将dc.dListerSynced、dc.rsListerSynced和dc.podListerSynced设置为相应的同步状态检查函数。
//- 最后，返回设置好的dc对象和nil。  这段代码主要涉及到对象的设置和初始化操作，使用了多个列表器和同步状态检查函数来管理不同资源的信息。

// Run begins watching and syncing.
func (dc *DeploymentController) Run(ctx context.Context, workers int) {
	defer utilruntime.HandleCrash()

	// Start events processing pipeline.
	dc.eventBroadcaster.StartStructuredLogging(3)
	dc.eventBroadcaster.StartRecordingToSink(&amp;v1core.EventSinkImpl{Interface: dc.client.CoreV1().Events(&#34;&#34;)})
	defer dc.eventBroadcaster.Shutdown()

	defer dc.queue.ShutDown()

	logger := klog.FromContext(ctx)
	logger.Info(&#34;Starting controller&#34;, &#34;controller&#34;, &#34;deployment&#34;)
	defer logger.Info(&#34;Shutting down controller&#34;, &#34;controller&#34;, &#34;deployment&#34;)

	if !cache.WaitForNamedCacheSync(&#34;deployment&#34;, ctx.Done(), dc.dListerSynced, dc.rsListerSynced, dc.podListerSynced) {
		return
	}

	for i := 0; i &lt; workers; i++ {
		go wait.UntilWithContext(ctx, dc.worker, time.Second)
	}

	&lt;-ctx.Done()
}

//该函数是DeploymentController类型的Run方法，用于启动部署控制器的监视和同步操作。
//- 首先，函数通过defer语句句柄处理崩溃情况。
//- 然后，启动事件处理管道，设置日志记录级别和目标。
//- 接着，通过defer语句关闭事件处理管道。
//- 然后，记录日志信息，表示控制器开始启动。
//- 接着，使用cache.WaitForNamedCacheSync函数等待缓存同步完成。
//- 然后，通过循环创建多个goroutine，并调用dc.worker函数进行工作。
//- 最后，等待上下文完成，并返回。
//该函数的主要功能是启动部署控制器，并使其开始监视和同步操作。

func (dc *DeploymentController) addDeployment(logger klog.Logger, obj interface{}) {
	d := obj.(*apps.Deployment)
	logger.V(4).Info(&#34;Adding deployment&#34;, &#34;deployment&#34;, klog.KObj(d))
	dc.enqueueDeployment(d)
}

func (dc *DeploymentController) updateDeployment(logger klog.Logger, old, cur interface{}) {
	oldD := old.(*apps.Deployment)
	curD := cur.(*apps.Deployment)
	logger.V(4).Info(&#34;Updating deployment&#34;, &#34;deployment&#34;, klog.KObj(oldD))
	dc.enqueueDeployment(curD)
}

//这两个函数都是DeploymentController的方法，用于处理Deployment的添加和更新事件。
//- addDeployment方法接收一个logger和一个obj参数，其中obj是通过类型断言转换为*apps.Deployment类型的。
//该方法首先使用logger记录添加deployment的日志信息，然后调用dc.enqueueDeployment方法将该deployment加入队列中，以便后续处理。
//- updateDeployment方法与addDeployment方法类似，但它接收的是旧的和新的Deployment对象。该方法使用logger记录更新deployment的日志信息，
//并将新的Deployment对象加入队列中进行处理。

func (dc *DeploymentController) deleteDeployment(logger klog.Logger, obj interface{}) {
	d, ok := obj.(*apps.Deployment)
	if !ok {
		tombstone, ok := obj.(cache.DeletedFinalStateUnknown)
		if !ok {
			utilruntime.HandleError(fmt.Errorf(&#34;couldn&#39;t get object from tombstone %#v&#34;, obj))
			return
		}
		d, ok = tombstone.Obj.(*apps.Deployment)
		if !ok {
			utilruntime.HandleError(fmt.Errorf(&#34;tombstone contained object that is not a Deployment %#v&#34;, obj))
			return
		}
	}
	logger.V(4).Info(&#34;Deleting deployment&#34;, &#34;deployment&#34;, klog.KObj(d))
	dc.enqueueDeployment(d)
}

//该函数是一个Go语言函数，它定义了一个名为deleteDeployment的方法，该方法接受一个logger和一个obj参数，并没有任何返回值。
//该方法主要用于从一个部署（Deployment）中删除对象。
//首先，函数会尝试将obj参数断言为apps.Deployment类型，并检查断言是否成功。如果断言失败，
//则会尝试将obj参数断言为cache.DeletedFinalStateUnknown类型。如果这个断言也失败了，函数会记录一个错误信息并返回。
//如果断言成功，则会尝试从tombstone中获取对象，并再次断言该对象是否为apps.Deployment类型。如果断言失败，则会记录一个错误信息并返回。
//如果成功断言出对象为apps.Deployment类型，则会使用logger记录一条信息，表示正在删除该部署，
//并调用dc.enqueueDeployment方法将该部署加入队列，以便进一步处理。

// addReplicaSet enqueues the deployment that manages a ReplicaSet when the ReplicaSet is created.
func (dc *DeploymentController) addReplicaSet(logger klog.Logger, obj interface{}) {
	rs := obj.(*apps.ReplicaSet)

	if rs.DeletionTimestamp != nil {
		// On a restart of the controller manager, it&#39;s possible for an object to
		// show up in a state that is already pending deletion.
		dc.deleteReplicaSet(logger, rs)
		return
	}
	// If it has a ControllerRef, that&#39;s all that matters.
	if controllerRef := metav1.GetControllerOf(rs); controllerRef != nil {
		d := dc.resolveControllerRef(rs.Namespace, controllerRef)
		if d == nil {
			return
		}
		logger.V(4).Info(&#34;ReplicaSet added&#34;, &#34;replicaSet&#34;, klog.KObj(rs))
		dc.enqueueDeployment(d)
		return
	}
	//该函数是Go语言编写的，用于在创建ReplicaSet时，将管理该ReplicaSet的Deployment加入队列中。
	//函数首先判断传入的obj对象是否为ReplicaSet类型，并通过判断ReplicaSet的DeletionTimestamp是否为空来确定是否需要删除该ReplicaSet。
	//如果ReplicaSet有ControllerRef，则通过resolveControllerRef函数解析出对应的Deployment，并将其加入队列中。

	// Otherwise, it&#39;s an orphan. Get a list of all matching Deployments and sync
	// them to see if anyone wants to adopt it.
	ds := dc.getDeploymentsForReplicaSet(logger, rs)
	if len(ds) == 0 {
		return
	}
	logger.V(4).Info(&#34;Orphan ReplicaSet added&#34;, &#34;replicaSet&#34;, klog.KObj(rs))
	for _, d := range ds {
		dc.enqueueDeployment(d)
	}
}

//该函数主要处理孤儿ReplicaSet的情况。
//它会获取与该ReplicaSet匹配的所有Deployment列表，并尝试同步这些Deployment，看是否有Deployment愿意采用该ReplicaSet。
//如果找到了对应的Deployment，则将其加入到队列中以便进一步处理。

// getDeploymentsForReplicaSet returns a list of Deployments that potentially
// match a ReplicaSet.
func (dc *DeploymentController) getDeploymentsForReplicaSet(logger klog.Logger, rs *apps.ReplicaSet) []*apps.Deployment {
	deployments, err := util.GetDeploymentsForReplicaSet(dc.dLister, rs)
	if err != nil || len(deployments) == 0 {
		return nil
	}
	// Because all ReplicaSet&#39;s belonging to a deployment should have a unique label key,
	// there should never be more than one deployment returned by the above method.
	// If that happens we should probably dynamically repair the situation by ultimately
	// trying to clean up one of the controllers, for now we just return the older one
	if len(deployments) &gt; 1 {
		// ControllerRef will ensure we don&#39;t do anything crazy, but more than one
		// item in this list nevertheless constitutes user error.
		logger.V(4).Info(&#34;user error! more than one deployment is selecting replica set&#34;,
			&#34;replicaSet&#34;, klog.KObj(rs), &#34;labels&#34;, rs.Labels, &#34;deployment&#34;, klog.KObj(deployments[0]))
	}
	return deployments
}

//函数用于获取与给定ReplicaSet匹配的所有Deployment列表。
//它首先调用util.GetDeploymentsForReplicaSet函数来获取匹配的Deployment列表，如果出现错误或列表为空，则返回nil。
//如果获取的Deployment列表长度大于1，则记录错误日志，并返回列表中的第一个Deployment。
//在正常情况下，返回获取的Deployment列表。

// updateReplicaSet figures out what deployment(s) manage a ReplicaSet when the ReplicaSet
// is updated and wake them up. If the anything of the ReplicaSets have changed, we need to
// awaken both the old and new deployments. old and cur must be *apps.ReplicaSet
// types.
func (dc *DeploymentController) updateReplicaSet(logger klog.Logger, old, cur interface{}) {
	curRS := cur.(*apps.ReplicaSet)
	oldRS := old.(*apps.ReplicaSet)
	if curRS.ResourceVersion == oldRS.ResourceVersion {
		// Periodic resync will send update events for all known replica sets.
		// Two different versions of the same replica set will always have different RVs.
		return
	}
	//该函数用于在更新ReplicaSet时，确定由哪个部署管理该ReplicaSet，并唤醒它们。
	//如果ReplicaSet的任何内容发生变化，需要唤醒旧的和新的部署。old和cur必须是指向apps.ReplicaSet类型的指针。
	//函数首先将传入的old和cur参数转换为*apps.ReplicaSet类型。
	//然后，它比较两个ReplicaSet的ResourceVersion字段。
	//如果它们相等，则表示这是周期性同步发送的更新事件，而对于同一ReplicaSet的两个不同版本，它们的ResourceVersion总会有不同的值。
	//在这种情况下，函数直接返回，不做任何处理。

	curControllerRef := metav1.GetControllerOf(curRS)
	oldControllerRef := metav1.GetControllerOf(oldRS)
	controllerRefChanged := !reflect.DeepEqual(curControllerRef, oldControllerRef)
	if controllerRefChanged &amp;&amp; oldControllerRef != nil {
		// The ControllerRef was changed. Sync the old controller, if any.
		if d := dc.resolveControllerRef(oldRS.Namespace, oldControllerRef); d != nil {
			dc.enqueueDeployment(d)
		}
	}
	//这段Go代码主要关注于检查和处理两个ReplicaSet（curRS和oldRS）的ControllerRef是否发生变化，并根据变化情况同步旧的控制器。
	//1. 首先，通过metav1.GetControllerOf函数获取curRS和oldRS的ControllerRef。
	//2. 然后，使用reflect.DeepEqual函数比较curControllerRef和oldControllerRef是否相等。
	//3. 如果两个ControllerRef不相等且oldControllerRef不为nil，则认为ControllerRef发生了变化。
	//4. 当ControllerRef发生变化时，需要同步旧的控制器。通过dc.resolveControllerRef函数解析oldRS的命名空间和ControllerRef，
	//得到对应的Deployment。
	//5. 最后，如果解析成功（d!=nil），则将该Deployment加入到调度队列中，以便进一步处理。

	// If it has a ControllerRef, that&#39;s all that matters.
	if curControllerRef != nil {
		d := dc.resolveControllerRef(curRS.Namespace, curControllerRef)
		if d == nil {
			return
		}
		logger.V(4).Info(&#34;ReplicaSet updated&#34;, &#34;replicaSet&#34;, klog.KObj(curRS))
		dc.enqueueDeployment(d)
		return
	}
	//这个Go函数主要检查当前的curControllerRef是否为nil。
	//如果不为nil，则通过dc.resolveControllerRef方法解析curControllerRef，并检查解析结果是否为nil。
	//如果不为nil，则记录日志并使用dc.enqueueDeployment方法将解析结果加入到队列中。

	// Otherwise, it&#39;s an orphan. If anything changed, sync matching controllers
	// to see if anyone wants to adopt it now.
	labelChanged := !reflect.DeepEqual(curRS.Labels, oldRS.Labels)
	if labelChanged || controllerRefChanged {
		ds := dc.getDeploymentsForReplicaSet(logger, curRS)
		if len(ds) == 0 {
			return
		}
		logger.V(4).Info(&#34;Orphan ReplicaSet updated&#34;, &#34;replicaSet&#34;, klog.KObj(curRS))
		for _, d := range ds {
			dc.enqueueDeployment(d)
		}
	}
}

//这段Go代码是处理孤儿ReplicaSet的逻辑。
//如果ReplicaSet的标签或控制器引用发生了变化，它会尝试同步匹配的控制器，以查看是否有控制器愿意现在采用它。具体步骤如下：
//1. 检查当前ReplicaSet的标签和旧ReplicaSet的标签是否相等，如果不相等，则标记标签发生变化；
//2. 检查控制器引用是否发生变化；
//3. 如果标签发生变化或控制器引用发生变化，则获取当前ReplicaSet对应的部署列表；
//4. 如果部署列表为空，则直接返回；
//5. 输出日志信息，表示孤儿ReplicaSet已更新；
//6. 遍历部署列表，将每个部署对象加入到队列中，以便进一步处理。

// deleteReplicaSet enqueues the deployment that manages a ReplicaSet when
// the ReplicaSet is deleted. obj could be an *apps.ReplicaSet, or
// a DeletionFinalStateUnknown marker item.
func (dc *DeploymentController) deleteReplicaSet(logger klog.Logger, obj interface{}) {
	rs, ok := obj.(*apps.ReplicaSet)
	//该函数是Go语言编写的，属于DeploymentController类型的一个方法，方法名为deleteReplicaSet。
	//该方法接收一个logger和一个obj参数，其中logger用于记录日志，obj是一个接口类型，
	//可以是*apps.ReplicaSet类型或者DeletionFinalStateUnknown标记项。
	//方法的主要功能是从obj中解析出*apps.ReplicaSet类型的rs，并判断解析是否成功。

	// When a delete is dropped, the relist will notice a pod in the store not
	// in the list, leading to the insertion of a tombstone object which contains
	// the deleted key/value. Note that this value might be stale. If the ReplicaSet
	// changed labels the new deployment will not be woken up till the periodic resync.
	if !ok {
		tombstone, ok := obj.(cache.DeletedFinalStateUnknown)
		if !ok {
			utilruntime.HandleError(fmt.Errorf(&#34;couldn&#39;t get object from tombstone %#v&#34;, obj))
			return
		}
		rs, ok = tombstone.Obj.(*apps.ReplicaSet)
		if !ok {
			utilruntime.HandleError(fmt.Errorf(&#34;tombstone contained object that is not a ReplicaSet %#v&#34;, obj))
			return
		}
	}
	//这段Go代码是处理从存储中获取对象时，如果对象已被删除的情况。
	//如果获取对象失败，会检查对象是否是一个 tombstone 对象（即已被删除的对象）。
	//如果是 tombstone 对象，则尝试从 tombstone 中恢复被删除的对象。如果恢复失败，则记录错误信息。

	controllerRef := metav1.GetControllerOf(rs)
	if controllerRef == nil {
		// No controller should care about orphans being deleted.
		return
	}
	d := dc.resolveControllerRef(rs.Namespace, controllerRef)
	if d == nil {
		return
	}
	logger.V(4).Info(&#34;ReplicaSet deleted&#34;, &#34;replicaSet&#34;, klog.KObj(rs))
	dc.enqueueDeployment(d)
}

//该函数主要实现当一个ReplicaSet被删除时，检查是否有对应的Deployment控制器，并将该Deployment加入到队列中以便进一步处理。具体流程如下：
//1. 通过metav1.GetControllerOf(rs)获取ReplicaSet的控制器引用；
//2. 如果控制器引用为空，则表示没有对应的Deployment控制器，直接返回；
//3. 调用dc.resolveControllerRef(rs.Namespace, controllerRef)解析控制器引用，获取对应的Deployment对象；
//4. 如果解析失败或返回的Deployment对象为空，则直接返回；
//5. 使用logger.V(4).Info记录日志，表示ReplicaSet已被删除；
//6. 调用dc.enqueueDeployment(d)将对应的Deployment对象加入到队列中，以便进一步处理。

// deletePod will enqueue a Recreate Deployment once all of its pods have stopped running.
func (dc *DeploymentController) deletePod(logger klog.Logger, obj interface{}) {
	pod, ok := obj.(*v1.Pod)
	//该函数是一个Go语言函数，名为deletePod，它属于DeploymentController类型。
	//函数通过传入的logger和obj参数，将一个Recreate Deployment加入队列，但只有当该Deployment的所有Pod都停止运行时才会执行。
	//函数首先尝试将obj参数断言为*v1.Pod类型，并检查断言是否成功。

	// When a delete is dropped, the relist will notice a pod in the store not
	// in the list, leading to the insertion of a tombstone object which contains
	// the deleted key/value. Note that this value might be stale. If the Pod
	// changed labels the new deployment will not be woken up till the periodic resync.
	if !ok {
		tombstone, ok := obj.(cache.DeletedFinalStateUnknown)
		if !ok {
			utilruntime.HandleError(fmt.Errorf(&#34;couldn&#39;t get object from tombstone %#v&#34;, obj))
			return
		}
		pod, ok = tombstone.Obj.(*v1.Pod)
		if !ok {
			utilruntime.HandleError(fmt.Errorf(&#34;tombstone contained object that is not a pod %#v&#34;, obj))
			return
		}
	}
	d := dc.getDeploymentForPod(logger, pod)
	if d == nil {
		return
	}
	logger.V(4).Info(&#34;Pod deleted&#34;, &#34;pod&#34;, klog.KObj(pod))
	if d.Spec.Strategy.Type == apps.RecreateDeploymentStrategyType {
		// Sync if this Deployment now has no more Pods.
		rsList, err := util.ListReplicaSets(d, util.RsListFromClient(dc.client.AppsV1()))
		if err != nil {
			return
		}
		podMap, err := dc.getPodMapForDeployment(d, rsList)
		if err != nil {
			return
		}
		numPods := 0
		for _, podList := range podMap {
			numPods += len(podList)
		}
		if numPods == 0 {
			dc.enqueueDeployment(d)
		}
	}
}

//这段Go代码是用于处理Kubernetes中Pod删除事件的逻辑。
//- 当一个Pod被删除时，如果控制器（例如Deployment）没有及时收到该事件，就会在存储中注意到这个Pod不在列表中，
//从而插入一个包含删除的键值对的墓碑对象（tombstone object）。
//- 如果墓碑对象中的对象不是Pod类型，函数会记录错误并返回。
//- 函数会尝试获取与该Pod关联的Deployment对象，如果获取不到则直接返回。
//- 如果该Deployment的策略是Recreate类型，则会检查该Deployment下是否已经没有Pod了。
//- 如果没有Pod了，则将该Deployment加入到队列中，以便进一步处理。
//这段代码的主要目的是确保在Pod被删除时，与之关联的Deployment能够及时地进行同步和更新。

func (dc *DeploymentController) enqueue(deployment *apps.Deployment) {
	key, err := controller.KeyFunc(deployment)
	if err != nil {
		utilruntime.HandleError(fmt.Errorf(&#34;couldn&#39;t get key for object %#v: %v&#34;, deployment, err))
		return
	}

	dc.queue.Add(key)
}

//该函数是一个Go语言的方法，定义在一个名为DeploymentController的结构体类型上。
//方法名为enqueue，它接收一个参数deployment，类型为*apps.Deployment，表示一个Kubernetes部署对象的指针。
//该方法的主要功能是将给定的部署对象加入到一个队列中，以便后续处理。具体步骤如下：
//1. 调用controller.KeyFunc(deployment)函数，尝试获取部署对象的键值（通常是一个字符串），用于在队列中唯一标识该对象。
//2. 如果获取键值时出现错误，通过utilruntime.HandleError函数处理错误，并打印错误信息。然后直接返回，不将对象加入队列。
//3. 如果成功获取了键值，将其添加到dc.queue（一个队列对象）中，以便后续处理。
//总结：该方法用于将一个Kubernetes部署对象加入到队列中，以便后续进行处理。

func (dc *DeploymentController) enqueueRateLimited(deployment *apps.Deployment) {
	key, err := controller.KeyFunc(deployment)
	if err != nil {
		utilruntime.HandleError(fmt.Errorf(&#34;couldn&#39;t get key for object %#v: %v&#34;, deployment, err))
		return
	}

	dc.queue.AddRateLimited(key)
}

//该函数用于将指定的部署对象加入到队列中，并对其进行速率限制。
//首先，通过调用controller.KeyFunc方法获取该部署对象的键值，如果获取失败，则通过utilruntime.HandleError方法记录错误信息并返回。
//如果获取成功，则调用dc.queue.AddRateLimited方法将键值加入到队列中，并对其进行速率限制。

// enqueueAfter will enqueue a deployment after the provided amount of time.
func (dc *DeploymentController) enqueueAfter(deployment *apps.Deployment, after time.Duration) {
	key, err := controller.KeyFunc(deployment)
	if err != nil {
		utilruntime.HandleError(fmt.Errorf(&#34;couldn&#39;t get key for object %#v: %v&#34;, deployment, err))
		return
	}

	dc.queue.AddAfter(key, after)
}

//该函数将一个部署对象加入到队列中，但会在指定的时间延迟后才执行。
//首先，函数通过调用controller.KeyFunc方法获取部署对象的键值，如果获取失败，则通过utilruntime.HandleError方法记录错误信息并返回。
//接着，函数使用dc.queue.AddAfter方法将键值加入到队列中，并指定延迟执行的时间。

// getDeploymentForPod returns the deployment managing the given Pod.
func (dc *DeploymentController) getDeploymentForPod(logger klog.Logger, pod *v1.Pod) *apps.Deployment {
	// Find the owning replica set
	var rs *apps.ReplicaSet
	var err error
	controllerRef := metav1.GetControllerOf(pod)
	if controllerRef == nil {
		// No controller owns this Pod.
		return nil
	}
	if controllerRef.Kind != apps.SchemeGroupVersion.WithKind(&#34;ReplicaSet&#34;).Kind {
		// Not a pod owned by a replica set.
		return nil
	}
	rs, err = dc.rsLister.ReplicaSets(pod.Namespace).Get(controllerRef.Name)
	if err != nil || rs.UID != controllerRef.UID {
		logger.V(4).Info(&#34;Cannot get replicaset for pod&#34;, &#34;ownerReference&#34;, controllerRef.Name, &#34;pod&#34;, klog.KObj(pod), &#34;err&#34;, err)
		return nil
	}
	//该函数用于获取管理给定Pod的Deployment。
	//- 首先，它查找拥有该Pod的ReplicaSet。
	//- 如果找不到拥有者的Pod，则返回nil。 -
	//如果Pod的拥有者不是ReplicaSet，则返回nil。
	//- 然后，尝试根据Pod的拥有者名称获取ReplicaSet。
	//- 如果获取失败或获取到的ReplicaSet的UID与拥有者的UID不匹配，则返回nil。
	//- 最后，返回获取到的Deployment。

	// Now find the Deployment that owns that ReplicaSet.
	controllerRef = metav1.GetControllerOf(rs)
	if controllerRef == nil {
		return nil
	}
	return dc.resolveControllerRef(rs.Namespace, controllerRef)
}

//这个函数的作用是通过 ReplicaSet 的 controllerRef 找到对应的 Deployment。
//首先通过 metav1.GetControllerOf(rs) 获取到 controllerRef，
//如果 controllerRef 为空则返回 nil，否则调用 dc.resolveControllerRef(rs.Namespace, controllerRef) 解析并返回对应的 Deployment。

// resolveControllerRef returns the controller referenced by a ControllerRef,
// or nil if the ControllerRef could not be resolved to a matching controller
// of the correct Kind.
func (dc *DeploymentController) resolveControllerRef(namespace string, controllerRef *metav1.OwnerReference) *apps.Deployment {
	// We can&#39;t look up by UID, so look up by Name and then verify UID.
	// Don&#39;t even try to look up by Name if it&#39;s the wrong Kind.
	if controllerRef.Kind != controllerKind.Kind {
		return nil
	}
	d, err := dc.dLister.Deployments(namespace).Get(controllerRef.Name)
	if err != nil {
		return nil
	}
	if d.UID != controllerRef.UID {
		// The controller we found with this Name is not the same one that the
		// ControllerRef points to.
		return nil
	}
	return d
}

//该函数是一个Go语言函数，用于解析一个ControllerRef引用所指向的控制器，如果无法解析出正确的控制器，则返回nil。
//函数接受三个参数： - namespace：字符串类型，表示命名空间。
//- controllerRef：指向metav1.OwnerReference类型的指针。
//函数返回一个指向apps.Deployment类型的指针。
//函数的主要步骤如下：
//1. 首先，检查controllerRef的Kind属性是否与controllerKind.Kind相等，如果不相等，则直接返回nil。
//2. 如果controllerRef的Kind属性与controllerKind.Kind相等，则通过dc.dLister.Deployments(namespace).Get(controllerRef.Name)获取具有相同名称的部署对象。
//3. 如果获取部署对象时出现错误，则返回nil。
//4. 最后，检查获取到的部署对象的UID属性是否与controllerRef的UID属性相等，如果不相等，则返回nil，否则返回该部署对象。

// worker runs a worker thread that just dequeues items, processes them, and marks them done.
// It enforces that the syncHandler is never invoked concurrently with the same key.
func (dc *DeploymentController) worker(ctx context.Context) {
	for dc.processNextWorkItem(ctx) {
	}
}

func (dc *DeploymentController) processNextWorkItem(ctx context.Context) bool {
	key, quit := dc.queue.Get()
	if quit {
		return false
	}
	defer dc.queue.Done(key)

	err := dc.syncHandler(ctx, key.(string))
	dc.handleErr(ctx, err, key)

	return true
}

//这段Go代码定义了两个函数：worker 和 processNextWorkItem，它们用于在DeploymentController中处理工作队列中的项。
//1. worker函数是一个无限循环，它不断地调用processNextWorkItem函数来处理队列中的下一个工作项，直到没有更多工作项需要处理为止。
//该函数接受一个context.Context参数，用于控制函数的取消或超时。
//2. processNextWorkItem函数从队列中获取下一个工作项的键，并调用syncHandler函数来处理该工作项。
//如果处理成功，它会标记该工作项为已完成。
//该函数也接受一个context.Context参数，并在处理完成后对其进行取消操作。
//该函数返回一个布尔值，表示是否成功处理了工作项。
//这段代码的主要目的是在DeploymentController中使用工作队列并发地处理工作项，并确保相同的键不会被并发处理。

func (dc *DeploymentController) handleErr(ctx context.Context, err error, key interface{}) {
	logger := klog.FromContext(ctx)
	if err == nil || errors.HasStatusCause(err, v1.NamespaceTerminatingCause) {
		dc.queue.Forget(key)
		return
	}
	ns, name, keyErr := cache.SplitMetaNamespaceKey(key.(string))
	if keyErr != nil {
		logger.Error(err, &#34;Failed to split meta namespace cache key&#34;, &#34;cacheKey&#34;, key)
	}

	if dc.queue.NumRequeues(key) &lt; maxRetries {
		logger.V(2).Info(&#34;Error syncing deployment&#34;, &#34;deployment&#34;, klog.KRef(ns, name), &#34;err&#34;, err)
		dc.queue.AddRateLimited(key)
		return
	}

	utilruntime.HandleError(err)
	logger.V(2).Info(&#34;Dropping deployment out of the queue&#34;, &#34;deployment&#34;, klog.KRef(ns, name), &#34;err&#34;, err)
	dc.queue.Forget(key)
}

//该函数是Go语言中的一个处理错误的函数，它属于DeploymentController类型。
//函数通过传入的上下文、错误和键来执行错误处理逻辑。
//首先，函数从上下文中获取日志记录器。如果错误为nil或错误的原因是命名空间终止，则将键从队列中忘记并返回。
//接下来，函数尝试将键拆分为命名空间和名称，并检查拆分是否成功。如果拆分失败，则记录错误信息。
//如果队列中键的重试次数小于最大重试次数，则记录错误信息，并将键添加到速率限制队列中。
//如果以上条件都不满足，则处理错误，并记录信息，将键从队列中忘记。

// getReplicaSetsForDeployment uses ControllerRefManager to reconcile
// ControllerRef by adopting and orphaning.
// It returns the list of ReplicaSets that this Deployment should manage.
func (dc *DeploymentController) getReplicaSetsForDeployment(ctx context.Context, d *apps.Deployment) ([]*apps.ReplicaSet, error) {
	// List all ReplicaSets to find those we own but that no longer match our
	// selector. They will be orphaned by ClaimReplicaSets().
	rsList, err := dc.rsLister.ReplicaSets(d.Namespace).List(labels.Everything())
	if err != nil {
		return nil, err
	}
	deploymentSelector, err := metav1.LabelSelectorAsSelector(d.Spec.Selector)
	if err != nil {
		return nil, fmt.Errorf(&#34;deployment %s/%s has invalid label selector: %v&#34;, d.Namespace, d.Name, err)
	}
	// If any adoptions are attempted, we should first recheck for deletion with
	// an uncached quorum read sometime after listing ReplicaSets (see #42639).
	canAdoptFunc := controller.RecheckDeletionTimestamp(func(ctx context.Context) (metav1.Object, error) {
		fresh, err := dc.client.AppsV1().Deployments(d.Namespace).Get(ctx, d.Name, metav1.GetOptions{})
		if err != nil {
			return nil, err
		}
		if fresh.UID != d.UID {
			return nil, fmt.Errorf(&#34;original Deployment %v/%v is gone: got uid %v, wanted %v&#34;, d.Namespace, d.Name, fresh.UID, d.UID)
		}
		return fresh, nil
	})
	cm := controller.NewReplicaSetControllerRefManager(dc.rsControl, d, deploymentSelector, controllerKind, canAdoptFunc)
	return cm.ClaimReplicaSets(ctx, rsList)
}

//该函数用于通过ControllerRefManager来协调ControllerRef，实现采用和孤儿化操作。
//它返回一个列表，其中包含该Deployment应管理的ReplicaSets。 函数首先列出所有ReplicaSets，以找到我们拥有但不再匹配我们选择器的那些。
//它们将通过ClaimReplicaSets()函数被孤儿化。
//然后，函数将根据Deployment的规范选择器生成一个标签选择器。如果任何收养尝试都进行了，
//函数将首先在列出ReplicaSets后重新检查删除时间戳（请参阅#42639）。
//最后，函数创建一个ReplicaSetControllerRefManager，并使用ClaimReplicaSets函数来声明应管理的ReplicaSets列表

// getPodMapForDeployment returns the Pods managed by a Deployment.
//
// It returns a map from ReplicaSet UID to a list of Pods controlled by that RS,
// according to the Pod&#39;s ControllerRef.
// NOTE: The pod pointers returned by this method point the pod objects in the cache and thus
// shouldn&#39;t be modified in any way.
func (dc *DeploymentController) getPodMapForDeployment(d *apps.Deployment, rsList []*apps.ReplicaSet) (map[types.UID][]*v1.Pod, error) {
	// Get all Pods that potentially belong to this Deployment.
	selector, err := metav1.LabelSelectorAsSelector(d.Spec.Selector)
	if err != nil {
		return nil, err
	}
	pods, err := dc.podLister.Pods(d.Namespace).List(selector)
	if err != nil {
		return nil, err
	}
	// Group Pods by their controller (if it&#39;s in rsList).
	podMap := make(map[types.UID][]*v1.Pod, len(rsList))
	for _, rs := range rsList {
		podMap[rs.UID] = []*v1.Pod{}
	}
	for _, pod := range pods {
		// Do not ignore inactive Pods because Recreate Deployments need to verify that no
		// Pods from older versions are running before spinning up new Pods.
		controllerRef := metav1.GetControllerOf(pod)
		if controllerRef == nil {
			continue
		}
		// Only append if we care about this UID.
		if _, ok := podMap[controllerRef.UID]; ok {
			podMap[controllerRef.UID] = append(podMap[controllerRef.UID], pod)
		}
	}
	return podMap, nil
}

//该函数用于返回由Deployment管理的Pods的映射。
//它根据Pod的ControllerRef将Pod分组为其控制器（如果在rsList中）。
//函数首先根据Deployment的规范选择器获取所有可能属于该Deployment的Pods 。
//然后，它遍历这些Pods，并通过其ControllerRef将它们分组到podMap中。
//函数返回一个映射，其中键是ReplicaSet的UID，值是由该RS控制的Pod列表。
//注意，该函数返回的Pod指针指向缓存中的Pod对象，因此不应以任何方式修改它们。

// syncDeployment will sync the deployment with the given key.
// This function is not meant to be invoked concurrently with the same key.
func (dc *DeploymentController) syncDeployment(ctx context.Context, key string) error {
	logger := klog.FromContext(ctx)
	namespace, name, err := cache.SplitMetaNamespaceKey(key)
	if err != nil {
		logger.Error(err, &#34;Failed to split meta namespace cache key&#34;, &#34;cacheKey&#34;, key)
		return err
	}
	//该函数是一个Go语言函数，名为syncDeployment，属于DeploymentController结构体。
	//它接收一个ctx context.Context参数和一个key string参数，并返回一个error类型值。函数主要用于同步指定键值的部署信息。
	//函数首先从上下文中获取日志记录器，然后使用cache.SplitMetaNamespaceKey函数将键值拆分为命名空间和名称。
	//如果拆分过程中出现错误，则记录错误日志并返回该错误。

	startTime := time.Now()
	logger.V(4).Info(&#34;Started syncing deployment&#34;, &#34;deployment&#34;, klog.KRef(namespace, name), &#34;startTime&#34;, startTime)
	defer func() {
		logger.V(4).Info(&#34;Finished syncing deployment&#34;, &#34;deployment&#34;, klog.KRef(namespace, name), &#34;duration&#34;, time.Since(startTime))
	}()
	//这段Go代码主要实现了在开始和结束同步部署时记录日志的功能。
	//- 首先，通过time.Now()获取当前时间作为开始时间，并使用logger.V(4).Info记录开始同步部署的日志，其中包含了部署的名称、命名空间和开始时间。
	//- 然后，使用defer关键字定义了一个匿名函数，在函数执行结束后会自动执行该函数。
	//该匿名函数使用logger.V(4).Info记录结束同步部署的日志，其中包含了部署的名称、命名空间和同步部署所花费的时间。
	//通过这种方式，可以在日志中方便地查看部署的同步状态和耗时。

	deployment, err := dc.dLister.Deployments(namespace).Get(name)
	if errors.IsNotFound(err) {
		logger.V(2).Info(&#34;Deployment has been deleted&#34;, &#34;deployment&#34;, klog.KRef(namespace, name))
		return nil
	}
	if err != nil {
		return err
	}
	//该函数通过调用dc.dLister.Deployments(namespace).Get(name)获取指定命名空间中名为name的部署对象。
	//如果该部署对象不存在，则记录日志并返回nil；如果存在其他错误，则直接返回错误。

	// Deep-copy otherwise we are mutating our cache.
	// TODO: Deep-copy only when needed.
	d := deployment.DeepCopy()

	everything := metav1.LabelSelector{}
	if reflect.DeepEqual(d.Spec.Selector, &amp;everything) {
		dc.eventRecorder.Eventf(d, v1.EventTypeWarning, &#34;SelectingAll&#34;, &#34;This deployment is selecting all pods. A non-empty selector is required.&#34;)
		if d.Status.ObservedGeneration &lt; d.Generation {
			d.Status.ObservedGeneration = d.Generation
			dc.client.AppsV1().Deployments(d.Namespace).UpdateStatus(ctx, d, metav1.UpdateOptions{})
		}
		return nil
	}
	//这段Go代码中的函数主要功能是对一个deployment对象进行深拷贝，并检查其Selector是否等于一个空的LabelSelector对象。
	//如果是，则记录一条警告事件并更新deployment的状态。

	// List ReplicaSets owned by this Deployment, while reconciling ControllerRef
	// through adoption/orphaning.
	rsList, err := dc.getReplicaSetsForDeployment(ctx, d)
	if err != nil {
		return err
	}
	// List all Pods owned by this Deployment, grouped by their ReplicaSet.
	// Current uses of the podMap are:
	//
	// * check if a Pod is labeled correctly with the pod-template-hash label.
	// * check that no old Pods are running in the middle of Recreate Deployments.
	podMap, err := dc.getPodMapForDeployment(d, rsList)
	if err != nil {
		return err
	}

	if d.DeletionTimestamp != nil {
		return dc.syncStatusOnly(ctx, d, rsList)
	}

	// Update deployment conditions with an Unknown condition when pausing/resuming
	// a deployment. In this way, we can be sure that we won&#39;t timeout when a user
	// resumes a Deployment with a set progressDeadlineSeconds.
	if err = dc.checkPausedConditions(ctx, d); err != nil {
		return err
	}

	if d.Spec.Paused {
		return dc.sync(ctx, d, rsList)
	}

	// rollback is not re-entrant in case the underlying replica sets are updated with a new
	// revision so we should ensure that we won&#39;t proceed to update replica sets until we
	// make sure that the deployment has cleaned up its rollback spec in subsequent enqueues.
	if getRollbackTo(d) != nil {
		return dc.rollback(ctx, d, rsList)
	}

	scalingEvent, err := dc.isScalingEvent(ctx, d, rsList)
	if err != nil {
		return err
	}
	if scalingEvent {
		return dc.sync(ctx, d, rsList)
	}

	switch d.Spec.Strategy.Type {
	case apps.RecreateDeploymentStrategyType:
		return dc.rolloutRecreate(ctx, d, rsList, podMap)
	case apps.RollingUpdateDeploymentStrategyType:
		return dc.rolloutRolling(ctx, d, rsList)
	}
	return fmt.Errorf(&#34;unexpected deployment strategy type: %s&#34;, d.Spec.Strategy.Type)
}

//该函数主要负责处理Deployment的更新和同步逻辑。
//1. 首先，函数会获取该Deployment所拥有的ReplicaSet列表和Pod的映射关系。
//2. 如果该Deployment已被删除，则只同步状态。
//3. 检查是否暂停，若暂停则只进行同步操作。
//4. 如果需要回滚，则执行回滚操作。
//5. 检测是否为缩放事件，若是则进行同步操作。
//6. 根据Deployment的策略类型（Recreate或RollingUpdate），执行相应的更新操作。
//7. 如果遇到意外的部署策略类型，返回错误。
</code></pre></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents></nav></div></aside></main></body></html>