<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>欢迎来到我的技术博客 on Guichen's Blog</title><link>https://qq547475331.github.io/docs/</link><description>Recent content in 欢迎来到我的技术博客 on Guichen's Blog</description><generator>Hugo</generator><language>en-us</language><atom:link href="https://qq547475331.github.io/docs/index.xml" rel="self" type="application/rss+xml"/><item><title>2023-04-12 使用 Keepalived 和 HAproxy 创建高可用 Kubernetes 集群</title><link>https://qq547475331.github.io/docs/%E4%BD%BF%E7%94%A8-keepalived-%E5%92%8C-haproxy-%E5%88%9B%E5%BB%BA%E9%AB%98%E5%8F%AF%E7%94%A8-kubernetes-%E9%9B%86%E7%BE%A4-shi-yong-keepalived-he-haproxy-chuang-jian-gao-ke-yong-kubernetes-ji-qun/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/%E4%BD%BF%E7%94%A8-keepalived-%E5%92%8C-haproxy-%E5%88%9B%E5%BB%BA%E9%AB%98%E5%8F%AF%E7%94%A8-kubernetes-%E9%9B%86%E7%BE%A4-shi-yong-keepalived-he-haproxy-chuang-jian-gao-ke-yong-kubernetes-ji-qun/</guid><description>&lt;h1 id="使用-keepalived-和-haproxy-创建高可用-kubernetes-集群">
 使用 Keepalived 和 HAproxy 创建高可用 Kubernetes 集群
 &lt;a class="anchor" href="#%e4%bd%bf%e7%94%a8-keepalived-%e5%92%8c-haproxy-%e5%88%9b%e5%bb%ba%e9%ab%98%e5%8f%af%e7%94%a8-kubernetes-%e9%9b%86%e7%be%a4">#&lt;/a>
&lt;/h1>
&lt;p>高可用 Kubernetes 集群能够确保应用程序在运行时不会出现服务中断，这也是生产的需求之一。为此，有很多方法可供选择以实现高可用。&lt;/p>
&lt;p>本教程演示了如何配置 Keepalived 和 HAproxy 使负载均衡、实现高可用。步骤如下：&lt;/p>
&lt;ol>
&lt;li>准备主机。&lt;/li>
&lt;li>配置 Keepalived 和 HAproxy。&lt;/li>
&lt;li>使用 KubeKey 创建 Kubernetes 集群，并安装 KubeSphere。&lt;/li>
&lt;/ol>
&lt;h2 id="集群架构">
 集群架构
 &lt;a class="anchor" href="#%e9%9b%86%e7%be%a4%e6%9e%b6%e6%9e%84">#&lt;/a>
&lt;/h2>
&lt;p>示例集群有三个主节点，三个工作节点，两个用于负载均衡的节点，以及一个虚拟 IP 地址。本示例中的虚拟 IP 地址也可称为“浮动 IP 地址”。这意味着在节点故障的情况下，该 IP 地址可在节点之间漂移，从而实现高可用。&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/image-20250315185347343.png" alt="image-20250315185347343" />&lt;/p>
&lt;p>请注意，在本示例中，Keepalived 和 HAproxy 没有安装在任何主节点上。但您也可以这样做，并同时实现高可用。然而，配置两个用于负载均衡的特定节点（您可以按需增加更多此类节点）会更加安全。这两个节点上只安装 Keepalived 和 HAproxy，以避免与任何 Kubernetes 组件和服务的潜在冲突。&lt;/p>
&lt;h2 id="准备主机">
 准备主机
 &lt;a class="anchor" href="#%e5%87%86%e5%a4%87%e4%b8%bb%e6%9c%ba">#&lt;/a>
&lt;/h2>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th>IP 地址&lt;/th>
 &lt;th>主机名&lt;/th>
 &lt;th>角色&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td>172.16.0.2&lt;/td>
 &lt;td>lb1&lt;/td>
 &lt;td>Keepalived &amp;amp; HAproxy&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>172.16.0.3&lt;/td>
 &lt;td>lb2&lt;/td>
 &lt;td>Keepalived &amp;amp; HAproxy&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>172.16.0.4&lt;/td>
 &lt;td>master1&lt;/td>
 &lt;td>master, etcd&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>172.16.0.5&lt;/td>
 &lt;td>master2&lt;/td>
 &lt;td>master, etcd&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>172.16.0.6&lt;/td>
 &lt;td>master3&lt;/td>
 &lt;td>master, etcd&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>172.16.0.7&lt;/td>
 &lt;td>worker1&lt;/td>
 &lt;td>worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>172.16.0.8&lt;/td>
 &lt;td>worker2&lt;/td>
 &lt;td>worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>172.16.0.9&lt;/td>
 &lt;td>worker3&lt;/td>
 &lt;td>worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>172.16.0.10&lt;/td>
 &lt;td>&lt;/td>
 &lt;td>虚拟 IP 地址&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;p>有关更多节点、网络、依赖项等要求的信息，请参见&lt;a href="https://kubesphere.com.cn/docs/installing-on-linux/introduction/multioverview/#step-1-prepare-linux-hosts">多节点安装&lt;/a>。&lt;/p></description></item><item><title>2023-09-21 思考</title><link>https://qq547475331.github.io/docs/%E6%9C%89%E8%BF%993%E4%B8%AA%E8%BF%B9%E8%B1%A1%E4%BD%A0%E5%B0%B1%E8%AF%A5%E7%A6%BB%E8%81%8C%E4%BA%86-you-zhe-3-ge-ji-xiang--ni-jiu-gai-li-zhi-le/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/%E6%9C%89%E8%BF%993%E4%B8%AA%E8%BF%B9%E8%B1%A1%E4%BD%A0%E5%B0%B1%E8%AF%A5%E7%A6%BB%E8%81%8C%E4%BA%86-you-zhe-3-ge-ji-xiang--ni-jiu-gai-li-zhi-le/</guid><description>&lt;p>我的前下属小P，在当前这样的环境下，裸辞了。&lt;/p>
&lt;p>小P在这家公司呆了3年，平时任劳任怨，努力踏实。对一些“007”加班、背黑锅等“非常规”任务，也来者不拒，积极配合。&lt;/p>
&lt;p>本以为这样就可以获得领导的青睐，能让自己的职场之路更顺畅。但现实却没如他所愿，前段时间公司有个主管空缺，无论从哪个角度看，小P都是最佳人选，小P自己也以为十拿九稳，但没想到领导却提拔了一个刚入职不到半年的新同事，真实的原因并不是那个人能力有多强，而是因为他是领导以前的旧下属，是嫡系“子弟兵”。&lt;/p>
&lt;p>小P输给一个寸功未立的“关系户”，心里憋屈，找领导理论。但领导就是领导，一下子指出N条小P在工作中的不足，说他工作效率低、有时候还犯错、而且还缺少大局观。小P听了更感到委屈，自己平时干的都是其他同事不愿接的“脏乱差”的活，有时候一整个项目都靠自己一个人顶着，现在反而成了多做多错，而那位被领导提拔的“嫡系”，手里没啥正事，整天和领导一起吹牛抽烟，反倒成了有大局观。&lt;/p>
&lt;p>小P情绪非常低落，天天上班如上坟，对工作提不起丝毫兴趣，而且身体和心理都出现明显不适，于是就下定决心，裸辞了。&lt;/p>
&lt;p>小P的这个决定，我是支持的。如果公司的环境对你的发展不利，你也看不到任何改观的迹象，那么止损可能是当下最好的选择。下面跟你聊聊，遇到哪类情况，你应该果断离开。&lt;/p>
&lt;hr>
&lt;p>&lt;em>01&lt;/em>&lt;/p>
&lt;p>&lt;strong>领导故意打压你&lt;/strong>&lt;/p>
&lt;p>&lt;strong>脏活累活想着你，升职加薪没你份&lt;/strong>&lt;/p>
&lt;p>人在职场，要能分辨两个最起码的事实，领导是“总用你”还是“重用你”；是喜欢 “你能干活”，还是喜欢“能干活的你”。&lt;/p>
&lt;p>领导让你做事的时候，简直把你当成公司“继承人”，技术需要你攻关、客户需要你维护、团队需要你协调、项目需要你加班，好像哪里都需要你，什么活儿你都要负责。&lt;/p>
&lt;p>直到每次升职加薪的机会都和自己无缘时，上面说的小P就是典型的例子，脏活累活都丢给他，升职加薪全留给亲信，明显厚此薄彼，不公平、不厚道。出现这种情况，通常有以下几种原因：&lt;/p>
&lt;p>&lt;strong>1、你是个不会说“不”的软柿子老实人&lt;/strong>&lt;/p>
&lt;p>很多单位都一些老实本分的打工人，他们在基层踏实工作，与人为善，任何人都不想得罪，对领导更是言听计从，从不讨价还价。&lt;/p>
&lt;p>这种人确实是大家眼中的“好人”，但这样的人不懂得说“不”，不会向别人展示自己的“边界”，以至于领导或其他人会不断对他试探，把脏活累活、不该他干的活全都丢给他。&lt;/p>
&lt;p>最后，所有人都对此习以为常，而他却慢慢变成了多做多错、出力还可能不讨好的“杨白劳”。至于升职加薪，是不太可能轮到他的，他真上去了，这些脏活累活以后丢给谁干？&lt;/p>
&lt;p>&lt;strong>2、你不是领导的“自己人”&lt;/strong>&lt;/p>
&lt;p>有人的地方就有江湖，没办法，谁让中国是个人情社会呢。不管在哪个团队、哪个群体，人与人之间都会有个亲疏远近，最大的差别只不过是表现的是否明显而已。团队的各种资源、机会都是有限的，拥有权力且怀有私心的某些领导，在分配这些资源的时候，就极有可能向所谓的“自己人”倾斜。&lt;/p>
&lt;p>上文中的小P不就是败给了领导的前下属了吗？当然，我不是说所有的公司领导，都像小P的领导一样不公平公正，但如果真遇到这种明显不讲规则的人，就算你干的再多，也很难分到肉吃，有时能不能喝口汤，都要看他心情。&lt;/p>
&lt;p>&lt;strong>3、你是个“能力有限”的人&lt;/strong>&lt;/p>
&lt;p>这里说的“能力有限”，不是说工作能力有限，而是指你只会“工作”，而没有背景、资源、人脉，等其他方面的可交换价值。我曾见过一个行政部前台，半年时间被提拔为部门经理，不是说她工作能力有多强，而是她的亲叔叔是某重点中学校长，她利用这个关系为公司老总的孩子解决了上重点的问题。&lt;/p>
&lt;p>我还听过很多带“资”入职的例子，这些人到了公司，就能利用自己的资源，为公司或领导个人，解决很多棘手的问题；而你，只擅长本本分分在自己工位上按部就班的工作（而且能干这种活的人一抓一大把），即使你做再多的工作，和前一类相比，稀缺性和竞争力都是明显不能比的。有了好机会，自然会被那些有“特殊贡献”的人先抢走。&lt;/p>
&lt;hr>
&lt;p>&lt;em>02&lt;/em>&lt;/p>
&lt;p>&lt;strong>部门集体摸鱼，人浮于事&lt;/strong>&lt;/p>
&lt;p>在当前的职场生态中，不少企业人浮于事，充斥着“坐班不做事”、“领导不走我不走”等伪加班、伪忙碌的形式主义。出现这种情况，通常因为两种原因：&lt;/p>
&lt;p>&lt;strong>1、员工“形式主义”严重&lt;/strong>&lt;/p>
&lt;p>我曾听一位年轻朋友讲过他经历的一件奇葩事。他曾和另一个同事一起负责公司的某个项目，某次两人加班到9点回家，但第二天朋友却发现那位同事发了这样一条朋友圈“一不小心搞到现在，终于忙完了”，配图是办公桌上开着的电脑，电脑上显示的是一份打开的文档，而发布时间则显示“某年某月某日凌晨2点”。而且在这条朋友圈下，公司领导不仅点了赞，还写下“辛苦了”三个字的留言。&lt;/p>
&lt;p>这让朋友心里非常不爽，活儿明明是两人一起干的，功劳就全成了同事的了。除了想给领导留下勤奋努力的好印象外，还有人有其他一些目的。比如，没结婚成家的想蹭蹭公司的空调，反正回去也是打游戏，在哪玩不是玩；还有一些“勤俭持家”的是为了蹭公司的班车、食堂，甚至是为了熬点加班补贴，生活不易，能省点就省点，能捞点就捞点。总之，大家就这样互相耗着，早走你就输了。&lt;/p>
&lt;p>&lt;strong>2、公司领导的“官僚主义”&lt;/strong>&lt;/p>
&lt;p>曾有一位粉丝朋友向K哥分享过他辞职的故事。他的领导就是一位见不得别人比他早下班的人。只要别人比他早离开公司，他就觉得为别人的工作不饱和，就会在开会的时候含沙射影批评几句，或者给那个“早走”的人多安排些工作。&lt;/p>
&lt;p>在一些企业中出现这种集体摸鱼、人浮于事的情况，虽然并不是每个人都故意为之，但就像在电影院里看电影，一旦前排的人站起来，后面的人就必须跟着站起来一样，很多人被周围的环境裹挟，不得不一边讨厌着，一边照样干着。&lt;/p>
&lt;hr>
&lt;p>&lt;em>03&lt;/em>&lt;/p>
&lt;p>&lt;strong>长期情绪内耗，个人成长为零&lt;/strong>&lt;/p>
&lt;p>耐克创始人菲尔·奈特在他的自传中有句名言：人生即是成长，不成长即死亡。我们进入一家公司，除了挣钱以外，还有另外一个非常重要的诉求，即积累经验、学习技能，实现个人成长。但如果我们发现，自己的工作除了赚几两并不值得激动的碎银外，其余全是资源的消耗和精神的内耗，而不能给自己带来任何成长，这样的公司肯定也不是久留之地。&lt;/p>
&lt;p>K哥一个远房侄子，原来是某新媒体公司的骨干，后来被一家传统企业挖走，但做了不到一年，就觉得坚持不下去了。老板请他的时候，饼画的很好，说自己认清了趋势，要大力发展线上的业务运营，一定会给侄子最大的工作支持。&lt;/p>
&lt;p>但事实远非如此，侄子过去之后才发现，自己基本是一个光杆司令，公司里里外外懂新媒体运营的只有他一个。老板的想法天天变，又不给资源，就只会让他自己想办法，KPI完不成就各种PUA他。&lt;/p>
&lt;p>在这种工作环境下，侄子上班度日如年，每天都充满焦虑，并开始变得越来越不自信。&lt;/p>
&lt;p>我知道后，劝他立刻离开这家公司。再呆下去只会增加内耗，没有任何成长可言。&lt;/p>
&lt;p>后来，我通过朋友把他推荐到另一家业内知名的公司，小伙子很快又找回了状态，在那里他每天都能从同事、团队那里汲取新的养分，而不是一味的输出、做大量纯消耗的无用功。&lt;/p>
&lt;hr>
&lt;p>&lt;em>04&lt;/em>&lt;/p>
&lt;p>&lt;strong>离职前，给你3个建议&lt;/strong>&lt;/p>
&lt;p>&lt;strong>1、不要因为暂时性的困难而离职&lt;/strong>&lt;/p>
&lt;p>王阳明先生有句名言：积之不久，发之必不宏，得之不难，失之必易。意思是说，如果你积累的不深，发挥的时候就不会太持续；一样东西如果得来不费劲，那么失去它也会很容易。&lt;/p>
&lt;p>但很多职场人，往往没有这个觉悟，但凡遇到点不称心的事，就会在心里打退堂鼓。每当这个时候，你不妨问自己两个问题：&lt;/p>
&lt;p>一是这个问题是不是暂时的，是不是真的不能克服？&lt;/p>
&lt;p>二是，这个问题是不是这家公司独有的，当你换另一个公司时，是不是能确保不再出现？这些问题想清楚了，相信答案也就有了。&lt;/p>
&lt;p>千万不要轻言放弃，越能够经得起挑战，越能够真正拥有。&lt;/p>
&lt;p>&lt;strong>2、离职是因为有更好的机会，而不是逃避困难&lt;/strong>&lt;/p>
&lt;p>马克思曾经说过：“人不是出于逃避某种消极力量，而是出于展现自身积极个性，才能获得真正的自由。”离职跳槽从来都不应该出于简单的消极逃避，而应该是因为有了更积极的选择。&lt;/p>
&lt;p>什么是更好的选择？比如，有更大更有影响力的平台，愿意以30%的薪水涨幅，以更高的title邀请你加盟，这就非常值得考虑。而不能仅仅情绪上不喜欢现在公司，而慌不择路，毫无规划的随便选一家新公司做“避风港”，甚至不惜平跳、打折跳，这样做既草率又不负责。&lt;/p>
&lt;p>&lt;strong>3、要有随时离开公司的能力&lt;/strong>&lt;/p>
&lt;p>职场上的成功，不是永远不被解雇，而是永远拥有主动选择职业、随时可以离开公司的能力。这就要求我们在职场中，要注意学习新的技能和认知，要能清晰客观的认识自己，知道哪些是自己真正具备的可复制、可转移、可嫁接的个人能力，而不至于出现“错把平台当能力”的“误会”。&lt;/p>
&lt;p>同时，还要在职场中为自己积累一些能拿得出手、更有说服力的“硬通货”，比如过往的出色业绩，良好的业内口碑，这些都能作为你的背书，让你在新的舞台上，更受器重和尊重。良好的开始就是成功的一半，真正有实力的你，搞定成功的另外“一半”，想必也不是什么难事。&lt;/p>
&lt;p>大哲学家康德说过：真正的自由，不是你想做什么；而是当你不想做什么时，可以不做什么。有底气、负责任的离职，就是这句话最好的诠释，希望小伙伴们在职场中，都能从事自己喜欢的工作，受到良好的尊重和对待，有丰厚的薪水收入，还能有让人满意的个人成长。加油！&lt;/p></description></item><item><title>2023-09-28 01-创建证书和环境准备</title><link>https://qq547475331.github.io/docs/01-%E5%88%9B%E5%BB%BA%E8%AF%81%E4%B9%A6%E5%92%8C%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87-01--chuang-jian-zheng-shu-he-huan-jing-zhun-bei/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/01-%E5%88%9B%E5%BB%BA%E8%AF%81%E4%B9%A6%E5%92%8C%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87-01--chuang-jian-zheng-shu-he-huan-jing-zhun-bei/</guid><description>&lt;h1 id="01-创建证书和环境准备">
 01-创建证书和环境准备
 &lt;a class="anchor" href="#01-%e5%88%9b%e5%bb%ba%e8%af%81%e4%b9%a6%e5%92%8c%e7%8e%af%e5%a2%83%e5%87%86%e5%a4%87">#&lt;/a>
&lt;/h1>
&lt;p>本步骤主要完成:&lt;/p>
&lt;ul>
&lt;li>(optional) role:os-harden，可选系统加固，符合linux安全基线，详见&lt;a href="https://github.com/dev-sec/ansible-collection-hardening/tree/master/roles/os_hardening">upstream&lt;/a>&lt;/li>
&lt;li>(optional) role:chrony，&lt;a href="../guide/chrony.md">可选集群节点时间同步&lt;/a>&lt;/li>
&lt;li>role:deploy，创建CA证书、集群组件访问apiserver所需的各种kubeconfig&lt;/li>
&lt;li>role:prepare，系统基础环境配置、分发CA证书、kubectl客户端安装&lt;/li>
&lt;/ul>
&lt;h2 id="deploy-角色">
 deploy 角色
 &lt;a class="anchor" href="#deploy-%e8%a7%92%e8%89%b2">#&lt;/a>
&lt;/h2>
&lt;p>主要任务讲解：roles/deploy/tasks/main.yml&lt;/p>
&lt;h3 id="创建-ca-证书">
 创建 CA 证书
 &lt;a class="anchor" href="#%e5%88%9b%e5%bb%ba-ca-%e8%af%81%e4%b9%a6">#&lt;/a>
&lt;/h3>
&lt;p>kubernetes 系统各组件需要使用 TLS 证书对通信进行加密，使用 CloudFlare 的 PKI 工具集生成自签名的 CA 证书，用来签名后续创建的其它 TLS 证书。&lt;a href="https://coreos.com/os/docs/latest/generate-self-signed-certificates.html">参考阅读&lt;/a>&lt;/p>
&lt;p>根据认证对象可以将证书分成三类：服务器证书&lt;code>server cert&lt;/code>，客户端证书&lt;code>client cert&lt;/code>，对等证书&lt;code>peer cert&lt;/code>(既是&lt;code>server cert&lt;/code>又是&lt;code>client cert&lt;/code>)，在kubernetes 集群中需要的证书种类如下：&lt;/p>
&lt;ul>
&lt;li>&lt;code>etcd&lt;/code> 节点需要标识自己服务的&lt;code>server cert&lt;/code>，也需要&lt;code>client cert&lt;/code>与&lt;code>etcd&lt;/code>集群其他节点交互，当然可以分别指定2个证书，为方便这里使用一个对等证书&lt;/li>
&lt;li>&lt;code>master&lt;/code> 节点需要标识 apiserver服务的&lt;code>server cert&lt;/code>，也需要&lt;code>client cert&lt;/code>连接&lt;code>etcd&lt;/code>集群，这里也使用一个对等证书&lt;/li>
&lt;li>&lt;code>kubectl&lt;/code> &lt;code>calico&lt;/code> &lt;code>kube-proxy&lt;/code> 只需要&lt;code>client cert&lt;/code>，因此证书请求中 &lt;code>hosts&lt;/code> 字段可以为空&lt;/li>
&lt;li>&lt;code>kubelet&lt;/code> 需要标识自己服务的&lt;code>server cert&lt;/code>，也需要&lt;code>client cert&lt;/code>请求&lt;code>apiserver&lt;/code>，也使用一个对等证书&lt;/li>
&lt;/ul>
&lt;p>整个集群要使用统一的CA 证书，只需要在ansible控制端创建，然后分发给其他节点；为了保证安装的幂等性，如果已经存在CA 证书，就跳过创建CA 步骤&lt;/p>
&lt;h4 id="创建-ca-配置文件-ca-configjsonj2">
 创建 CA 配置文件 &lt;a href="../../roles/deploy/templates/ca-config.json.j2">ca-config.json.j2&lt;/a>
 &lt;a class="anchor" href="#%e5%88%9b%e5%bb%ba-ca-%e9%85%8d%e7%bd%ae%e6%96%87%e4%bb%b6-ca-configjsonj2">#&lt;/a>
&lt;/h4>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;signing&amp;#34;&lt;/span>: &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;default&amp;#34;&lt;/span>: &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;expiry&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;{{ CERT_EXPIRY }}&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;profiles&amp;#34;&lt;/span>: &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;kubernetes&amp;#34;&lt;/span>: &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;usages&amp;#34;&lt;/span>: &lt;span style="color:#f92672">[&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;signing&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;key encipherment&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;server auth&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;client auth&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">]&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;expiry&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;{{ CERT_EXPIRY }}&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;kcfg&amp;#34;&lt;/span>: &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;usages&amp;#34;&lt;/span>: &lt;span style="color:#f92672">[&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;signing&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;key encipherment&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;client auth&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">]&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;expiry&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;{{ CUSTOM_EXPIRY }}&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>&lt;code>signing&lt;/code>：表示该证书可用于签名其它证书；生成的 ca.pem 证书中 &lt;code>CA=TRUE&lt;/code>；&lt;/li>
&lt;li>&lt;code>server auth&lt;/code>：表示可以用该 CA 对 server 提供的证书进行验证；&lt;/li>
&lt;li>&lt;code>client auth&lt;/code>：表示可以用该 CA 对 client 提供的证书进行验证；&lt;/li>
&lt;li>&lt;code>profile kubernetes&lt;/code> 包含了&lt;code>server auth&lt;/code>和&lt;code>client auth&lt;/code>，所以可以签发三种不同类型证书；expiry 证书有效期，默认50年&lt;/li>
&lt;li>&lt;code>profile kcfg&lt;/code> 在后面客户端kubeconfig证书管理中用到&lt;/li>
&lt;/ul>
&lt;h4 id="创建-ca-证书签名请求-ca-csrjsonj2">
 创建 CA 证书签名请求 &lt;a href="../../roles/deploy/templates/ca-csr.json.j2">ca-csr.json.j2&lt;/a>
 &lt;a class="anchor" href="#%e5%88%9b%e5%bb%ba-ca-%e8%af%81%e4%b9%a6%e7%ad%be%e5%90%8d%e8%af%b7%e6%b1%82-ca-csrjsonj2">#&lt;/a>
&lt;/h4>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;CN&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;kubernetes-ca&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;key&amp;#34;&lt;/span>: &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;algo&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;rsa&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;size&amp;#34;&lt;/span>: &lt;span style="color:#ae81ff">2048&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;names&amp;#34;&lt;/span>: &lt;span style="color:#f92672">[&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;C&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;CN&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;ST&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;HangZhou&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;L&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;XS&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;O&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;k8s&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;OU&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;System&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">]&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;ca&amp;#34;&lt;/span>: &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;expiry&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;876000h&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>&lt;code>ca expiry&lt;/code> 指定ca证书的有效期，默认100年&lt;/li>
&lt;/ul>
&lt;h4 id="生成ca-证书和私钥">
 生成CA 证书和私钥
 &lt;a class="anchor" href="#%e7%94%9f%e6%88%90ca-%e8%af%81%e4%b9%a6%e5%92%8c%e7%a7%81%e9%92%a5">#&lt;/a>
&lt;/h4>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>cfssl gencert -initca ca-csr.json | cfssljson -bare ca
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="生成-kubeconfig-配置文件">
 生成 kubeconfig 配置文件
 &lt;a class="anchor" href="#%e7%94%9f%e6%88%90-kubeconfig-%e9%85%8d%e7%bd%ae%e6%96%87%e4%bb%b6">#&lt;/a>
&lt;/h3>
&lt;p>kubectl使用~/.kube/config 配置文件与kube-apiserver进行交互，且拥有管理 K8S集群的完全权限，&lt;/p></description></item><item><title>2023-09-28 03-安装容器运行时</title><link>https://qq547475331.github.io/docs/03-%E5%AE%89%E8%A3%85%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6-03--an-zhuang-rong-qi-yun-xing-shi/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/03-%E5%AE%89%E8%A3%85%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6-03--an-zhuang-rong-qi-yun-xing-shi/</guid><description>&lt;h1 id="03-安装容器运行时">
 03-安装容器运行时
 &lt;a class="anchor" href="#03-%e5%ae%89%e8%a3%85%e5%ae%b9%e5%99%a8%e8%bf%90%e8%a1%8c%e6%97%b6">#&lt;/a>
&lt;/h1>
&lt;p>项目根据k8s版本提供不同的默认容器运行时：&lt;/p>
&lt;ul>
&lt;li>k8s 版本 &amp;lt; 1.24 时，支持docker containerd 可选&lt;/li>
&lt;li>k8s 版本 &amp;gt;= 1.24 时，仅支持 containerd&lt;/li>
&lt;/ul>
&lt;h2 id="安装containerd">
 安装containerd
 &lt;a class="anchor" href="#%e5%ae%89%e8%a3%85containerd">#&lt;/a>
&lt;/h2>
&lt;p>作为 CNCF 毕业项目，containerd 致力于提供简洁、可靠、可扩展的容器运行时；它被设计用来集成到 kubernetes 等系统使用，而不是像 docker 那样独立使用。&lt;/p>
&lt;ul>
&lt;li>安装指南 &lt;a href="https://github.com/containerd/cri/blob/master/docs/installation.md">https://github.com/containerd/cri/blob/master/docs/installation.md&lt;/a>&lt;/li>
&lt;li>客户端 circtl 使用指南 &lt;a href="https://github.com/containerd/cri/blob/master/docs/crictl.md">https://github.com/containerd/cri/blob/master/docs/crictl.md&lt;/a>&lt;/li>
&lt;li>man 文档 &lt;a href="https://github.com/containerd/containerd/tree/master/docs/man">https://github.com/containerd/containerd/tree/master/docs/man&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="kubeasz-集成安装-containerd">
 kubeasz 集成安装 containerd
 &lt;a class="anchor" href="#kubeasz-%e9%9b%86%e6%88%90%e5%ae%89%e8%a3%85-containerd">#&lt;/a>
&lt;/h2>
&lt;ul>
&lt;li>注意：k8s 1.24以后，项目已经设置默认容器运行时为 containerd，无需手动修改&lt;/li>
&lt;li>执行安装：分步安装&lt;code>ezctl setup xxxx 03&lt;/code>，一键安装&lt;code>ezctl setup xxxx all&lt;/code>&lt;/li>
&lt;/ul>
&lt;h2 id="命令对比">
 命令对比
 &lt;a class="anchor" href="#%e5%91%bd%e4%bb%a4%e5%af%b9%e6%af%94">#&lt;/a>
&lt;/h2>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th style="text-align: left">命令&lt;/th>
 &lt;th style="text-align: left">docker&lt;/th>
 &lt;th style="text-align: left">crictl（推荐）&lt;/th>
 &lt;th style="text-align: left">ctr&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td style="text-align: left">查看容器列表&lt;/td>
 &lt;td style="text-align: left">docker ps&lt;/td>
 &lt;td style="text-align: left">crictl ps&lt;/td>
 &lt;td style="text-align: left">ctr -n k8s.io c ls&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">查看容器详情&lt;/td>
 &lt;td style="text-align: left">docker inspect&lt;/td>
 &lt;td style="text-align: left">crictl inspect&lt;/td>
 &lt;td style="text-align: left">ctr -n k8s.io c info&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">查看容器日志&lt;/td>
 &lt;td style="text-align: left">docker logs&lt;/td>
 &lt;td style="text-align: left">crictl logs&lt;/td>
 &lt;td style="text-align: left">无&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">容器内执行命令&lt;/td>
 &lt;td style="text-align: left">docker exec&lt;/td>
 &lt;td style="text-align: left">crictl exec&lt;/td>
 &lt;td style="text-align: left">无&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">挂载容器&lt;/td>
 &lt;td style="text-align: left">docker attach&lt;/td>
 &lt;td style="text-align: left">crictl attach&lt;/td>
 &lt;td style="text-align: left">无&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">容器资源使用&lt;/td>
 &lt;td style="text-align: left">docker stats&lt;/td>
 &lt;td style="text-align: left">crictl stats&lt;/td>
 &lt;td style="text-align: left">无&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">创建容器&lt;/td>
 &lt;td style="text-align: left">docker create&lt;/td>
 &lt;td style="text-align: left">crictl create&lt;/td>
 &lt;td style="text-align: left">ctr -n k8s.io c create&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">启动容器&lt;/td>
 &lt;td style="text-align: left">docker start&lt;/td>
 &lt;td style="text-align: left">crictl start&lt;/td>
 &lt;td style="text-align: left">ctr -n k8s.io run&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">停止容器&lt;/td>
 &lt;td style="text-align: left">docker stop&lt;/td>
 &lt;td style="text-align: left">crictl stop&lt;/td>
 &lt;td style="text-align: left">无&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">删除容器&lt;/td>
 &lt;td style="text-align: left">docker rm&lt;/td>
 &lt;td style="text-align: left">crictl rm&lt;/td>
 &lt;td style="text-align: left">ctr -n k8s.io c del&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">查看镜像列表&lt;/td>
 &lt;td style="text-align: left">docker images&lt;/td>
 &lt;td style="text-align: left">crictl images&lt;/td>
 &lt;td style="text-align: left">ctr -n k8s.io i ls&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">查看镜像详情&lt;/td>
 &lt;td style="text-align: left">docker inspect&lt;/td>
 &lt;td style="text-align: left">crictl inspecti&lt;/td>
 &lt;td style="text-align: left">无&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">拉取镜像&lt;/td>
 &lt;td style="text-align: left">docker pull&lt;/td>
 &lt;td style="text-align: left">crictl pull&lt;/td>
 &lt;td style="text-align: left">ctr -n k8s.io i pull&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">推送镜像&lt;/td>
 &lt;td style="text-align: left">docker push&lt;/td>
 &lt;td style="text-align: left">无&lt;/td>
 &lt;td style="text-align: left">ctr -n k8s.io i push&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">删除镜像&lt;/td>
 &lt;td style="text-align: left">docker rmi&lt;/td>
 &lt;td style="text-align: left">crictl rmi&lt;/td>
 &lt;td style="text-align: left">ctr -n k8s.io i rm&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">查看Pod列表&lt;/td>
 &lt;td style="text-align: left">无&lt;/td>
 &lt;td style="text-align: left">crictl pods&lt;/td>
 &lt;td style="text-align: left">无&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">查看Pod详情&lt;/td>
 &lt;td style="text-align: left">无&lt;/td>
 &lt;td style="text-align: left">crictl inspectp&lt;/td>
 &lt;td style="text-align: left">无&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">启动Pod&lt;/td>
 &lt;td style="text-align: left">无&lt;/td>
 &lt;td style="text-align: left">crictl runp&lt;/td>
 &lt;td style="text-align: left">无&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">停止Pod&lt;/td>
 &lt;td style="text-align: left">无&lt;/td>
 &lt;td style="text-align: left">crictl stopp&lt;/td>
 &lt;td style="text-align: left">无&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;p>[后一篇](&lt;/p></description></item><item><title>2023-09-28 04-安装kube_master节点</title><link>https://qq547475331.github.io/docs/04-%E5%AE%89%E8%A3%85kube_master%E8%8A%82%E7%82%B9-04--an-zhuang-kubemaster-jie-dian/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/04-%E5%AE%89%E8%A3%85kube_master%E8%8A%82%E7%82%B9-04--an-zhuang-kubemaster-jie-dian/</guid><description>&lt;h1 id="04-安装kube_master节点">
 04-安装kube_master节点
 &lt;a class="anchor" href="#04-%e5%ae%89%e8%a3%85kube_master%e8%8a%82%e7%82%b9">#&lt;/a>
&lt;/h1>
&lt;h5 id="部署master节点主要包含三个组件apiserver-scheduler-controller-manager其中">
 部署master节点主要包含三个组件&lt;code>apiserver&lt;/code> &lt;code>scheduler&lt;/code> &lt;code>controller-manager&lt;/code>，其中：
 &lt;a class="anchor" href="#%e9%83%a8%e7%bd%b2master%e8%8a%82%e7%82%b9%e4%b8%bb%e8%a6%81%e5%8c%85%e5%90%ab%e4%b8%89%e4%b8%aa%e7%bb%84%e4%bb%b6apiserver-scheduler-controller-manager%e5%85%b6%e4%b8%ad">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="apiserver提供集群管理的rest-api接口包括认证授权数据校验以及集群状态变更等">
 apiserver提供集群管理的REST API接口，包括认证授权、数据校验以及集群状态变更等
 &lt;a class="anchor" href="#apiserver%e6%8f%90%e4%be%9b%e9%9b%86%e7%be%a4%e7%ae%a1%e7%90%86%e7%9a%84rest-api%e6%8e%a5%e5%8f%a3%e5%8c%85%e6%8b%ac%e8%ae%a4%e8%af%81%e6%8e%88%e6%9d%83%e6%95%b0%e6%8d%ae%e6%a0%a1%e9%aa%8c%e4%bb%a5%e5%8f%8a%e9%9b%86%e7%be%a4%e7%8a%b6%e6%80%81%e5%8f%98%e6%9b%b4%e7%ad%89">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="只有api-server才直接操作etcd">
 只有API Server才直接操作etcd
 &lt;a class="anchor" href="#%e5%8f%aa%e6%9c%89api-server%e6%89%8d%e7%9b%b4%e6%8e%a5%e6%93%8d%e4%bd%9cetcd">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="其他模块通过api-server查询或修改数据">
 其他模块通过API Server查询或修改数据
 &lt;a class="anchor" href="#%e5%85%b6%e4%bb%96%e6%a8%a1%e5%9d%97%e9%80%9a%e8%bf%87api-server%e6%9f%a5%e8%af%a2%e6%88%96%e4%bf%ae%e6%94%b9%e6%95%b0%e6%8d%ae">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="提供其他模块之间的数据交互和通信的枢纽">
 提供其他模块之间的数据交互和通信的枢纽
 &lt;a class="anchor" href="#%e6%8f%90%e4%be%9b%e5%85%b6%e4%bb%96%e6%a8%a1%e5%9d%97%e4%b9%8b%e9%97%b4%e7%9a%84%e6%95%b0%e6%8d%ae%e4%ba%a4%e4%ba%92%e5%92%8c%e9%80%9a%e4%bf%a1%e7%9a%84%e6%9e%a2%e7%ba%bd">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;h5 id="scheduler负责分配调度pod到集群内的node节点">
 scheduler负责分配调度Pod到集群内的node节点
 &lt;a class="anchor" href="#scheduler%e8%b4%9f%e8%b4%a3%e5%88%86%e9%85%8d%e8%b0%83%e5%ba%a6pod%e5%88%b0%e9%9b%86%e7%be%a4%e5%86%85%e7%9a%84node%e8%8a%82%e7%82%b9">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="监听kube-apiserver查询还未分配node的pod">
 监听kube-apiserver，查询还未分配Node的Pod
 &lt;a class="anchor" href="#%e7%9b%91%e5%90%ackube-apiserver%e6%9f%a5%e8%af%a2%e8%bf%98%e6%9c%aa%e5%88%86%e9%85%8dnode%e7%9a%84pod">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="根据调度策略为这些pod分配节点">
 根据调度策略为这些Pod分配节点
 &lt;a class="anchor" href="#%e6%a0%b9%e6%8d%ae%e8%b0%83%e5%ba%a6%e7%ad%96%e7%95%a5%e4%b8%ba%e8%bf%99%e4%ba%9bpod%e5%88%86%e9%85%8d%e8%8a%82%e7%82%b9">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;h5 id="controller-manager由一系列的控制器组成它通过apiserver监控整个集群的状态并确保集群处于预期的工作状态">
 controller-manager由一系列的控制器组成，它通过apiserver监控整个集群的状态，并确保集群处于预期的工作状态
 &lt;a class="anchor" href="#controller-manager%e7%94%b1%e4%b8%80%e7%b3%bb%e5%88%97%e7%9a%84%e6%8e%a7%e5%88%b6%e5%99%a8%e7%bb%84%e6%88%90%e5%ae%83%e9%80%9a%e8%bf%87apiserver%e7%9b%91%e6%8e%a7%e6%95%b4%e4%b8%aa%e9%9b%86%e7%be%a4%e7%9a%84%e7%8a%b6%e6%80%81%e5%b9%b6%e7%a1%ae%e4%bf%9d%e9%9b%86%e7%be%a4%e5%a4%84%e4%ba%8e%e9%a2%84%e6%9c%9f%e7%9a%84%e5%b7%a5%e4%bd%9c%e7%8a%b6%e6%80%81">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h2 id="高可用机制">
 高可用机制
 &lt;a class="anchor" href="#%e9%ab%98%e5%8f%af%e7%94%a8%e6%9c%ba%e5%88%b6">#&lt;/a>
&lt;/h2>
&lt;ul>
&lt;li>
&lt;h5 id="apiserver-无状态服务可以通过外部负载均衡实现高可用如项目采用的两种高可用架构ha-1x-584和-ha-2x-585">
 apiserver 无状态服务，可以通过外部负载均衡实现高可用，如项目采用的两种高可用架构：HA-1x (#584)和 HA-2x (#585)
 &lt;a class="anchor" href="#apiserver-%e6%97%a0%e7%8a%b6%e6%80%81%e6%9c%8d%e5%8a%a1%e5%8f%af%e4%bb%a5%e9%80%9a%e8%bf%87%e5%a4%96%e9%83%a8%e8%b4%9f%e8%bd%bd%e5%9d%87%e8%a1%a1%e5%ae%9e%e7%8e%b0%e9%ab%98%e5%8f%af%e7%94%a8%e5%a6%82%e9%a1%b9%e7%9b%ae%e9%87%87%e7%94%a8%e7%9a%84%e4%b8%a4%e7%a7%8d%e9%ab%98%e5%8f%af%e7%94%a8%e6%9e%b6%e6%9e%84ha-1x-584%e5%92%8c-ha-2x-585">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="controller-manager-组件启动时会进行类似选举leader当多副本存在时如果原先leader挂掉那么会选举出新的leader从而保证高可用">
 controller-manager 组件启动时会进行类似选举（leader）；当多副本存在时，如果原先leader挂掉，那么会选举出新的leader，从而保证高可用；
 &lt;a class="anchor" href="#controller-manager-%e7%bb%84%e4%bb%b6%e5%90%af%e5%8a%a8%e6%97%b6%e4%bc%9a%e8%bf%9b%e8%a1%8c%e7%b1%bb%e4%bc%bc%e9%80%89%e4%b8%beleader%e5%bd%93%e5%a4%9a%e5%89%af%e6%9c%ac%e5%ad%98%e5%9c%a8%e6%97%b6%e5%a6%82%e6%9e%9c%e5%8e%9f%e5%85%88leader%e6%8c%82%e6%8e%89%e9%82%a3%e4%b9%88%e4%bc%9a%e9%80%89%e4%b8%be%e5%87%ba%e6%96%b0%e7%9a%84leader%e4%bb%8e%e8%80%8c%e4%bf%9d%e8%af%81%e9%ab%98%e5%8f%af%e7%94%a8">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="scheduler-类似选举机制">
 scheduler 类似选举机制
 &lt;a class="anchor" href="#scheduler-%e7%b1%bb%e4%bc%bc%e9%80%89%e4%b8%be%e6%9c%ba%e5%88%b6">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h2 id="安装流程">
 安装流程
 &lt;a class="anchor" href="#%e5%ae%89%e8%a3%85%e6%b5%81%e7%a8%8b">#&lt;/a>
&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>cat playbooks/04.kube-master.yml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- hosts: kube_master
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> roles:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - kube-lb &lt;span style="color:#75715e"># 四层负载均衡，监听在127.0.0.1:6443，转发到真实master节点apiserver服务&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - kube-master &lt;span style="color:#75715e">#&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - kube-node &lt;span style="color:#75715e"># 因为网络、监控等daemonset组件，master节点也推荐安装kubelet和kube-proxy服务&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ... 
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="创建-kubernetes-证书签名请求">
 创建 kubernetes 证书签名请求
 &lt;a class="anchor" href="#%e5%88%9b%e5%bb%ba-kubernetes-%e8%af%81%e4%b9%a6%e7%ad%be%e5%90%8d%e8%af%b7%e6%b1%82">#&lt;/a>
&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;CN&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;kubernetes&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;hosts&amp;#34;&lt;/span>: &lt;span style="color:#f92672">[&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;127.0.0.1&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">{&lt;/span>% &lt;span style="color:#66d9ef">if&lt;/span> groups&lt;span style="color:#f92672">[&lt;/span>&lt;span style="color:#e6db74">&amp;#39;ex_lb&amp;#39;&lt;/span>&lt;span style="color:#f92672">]&lt;/span>|length &amp;gt; &lt;span style="color:#ae81ff">0&lt;/span> %&lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;{{ hostvars[groups[&amp;#39;ex_lb&amp;#39;][0]][&amp;#39;EX_APISERVER_VIP&amp;#39;] }}&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">{&lt;/span>% endif %&lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">{&lt;/span>% &lt;span style="color:#66d9ef">for&lt;/span> host in groups&lt;span style="color:#f92672">[&lt;/span>&lt;span style="color:#e6db74">&amp;#39;kube_master&amp;#39;&lt;/span>&lt;span style="color:#f92672">]&lt;/span> %&lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;{{ host }}&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">{&lt;/span>% endfor %&lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;{{ CLUSTER_KUBERNETES_SVC_IP }}&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">{&lt;/span>% &lt;span style="color:#66d9ef">for&lt;/span> host in MASTER_CERT_HOSTS %&lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;{{ host }}&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">{&lt;/span>% endfor %&lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;kubernetes&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;kubernetes.default&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;kubernetes.default.svc&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;kubernetes.default.svc.cluster&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;kubernetes.default.svc.cluster.local&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">]&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;key&amp;#34;&lt;/span>: &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;algo&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;rsa&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;size&amp;#34;&lt;/span>: &lt;span style="color:#ae81ff">2048&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;names&amp;#34;&lt;/span>: &lt;span style="color:#f92672">[&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;C&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;CN&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;ST&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;HangZhou&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;L&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;XS&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;O&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;k8s&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;OU&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;System&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h5 id="kubernetes-apiserver-使用对等证书创建时hosts字段需要配置">
 kubernetes apiserver 使用对等证书，创建时hosts字段需要配置：
 &lt;a class="anchor" href="#kubernetes-apiserver-%e4%bd%bf%e7%94%a8%e5%af%b9%e7%ad%89%e8%af%81%e4%b9%a6%e5%88%9b%e5%bb%ba%e6%97%b6hosts%e5%ad%97%e6%ae%b5%e9%9c%80%e8%a6%81%e9%85%8d%e7%bd%ae">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="如果配置-ex_lb需要把-ex_apiserver_vip-也配置进去">
 如果配置 ex_lb，需要把 EX_APISERVER_VIP 也配置进去
 &lt;a class="anchor" href="#%e5%a6%82%e6%9e%9c%e9%85%8d%e7%bd%ae-ex_lb%e9%9c%80%e8%a6%81%e6%8a%8a-ex_apiserver_vip-%e4%b9%9f%e9%85%8d%e7%bd%ae%e8%bf%9b%e5%8e%bb">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="如果需要外部访问-apiserver可选在configyml配置-master_cert_hosts">
 如果需要外部访问 apiserver，可选在config.yml配置 MASTER_CERT_HOSTS
 &lt;a class="anchor" href="#%e5%a6%82%e6%9e%9c%e9%9c%80%e8%a6%81%e5%a4%96%e9%83%a8%e8%ae%bf%e9%97%ae-apiserver%e5%8f%af%e9%80%89%e5%9c%a8configyml%e9%85%8d%e7%bd%ae-master_cert_hosts">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="kubectl-get-svc-将看到集群中由api-server-创建的默认服务-kubernetes因此也要把-kubernetes-服务名和各个服务域名也添加进去">
 &lt;code>kubectl get svc&lt;/code> 将看到集群中由api-server 创建的默认服务 &lt;code>kubernetes&lt;/code>，因此也要把 &lt;code>kubernetes&lt;/code> 服务名和各个服务域名也添加进去
 &lt;a class="anchor" href="#kubectl-get-svc-%e5%b0%86%e7%9c%8b%e5%88%b0%e9%9b%86%e7%be%a4%e4%b8%ad%e7%94%b1api-server-%e5%88%9b%e5%bb%ba%e7%9a%84%e9%bb%98%e8%ae%a4%e6%9c%8d%e5%8a%a1-kubernetes%e5%9b%a0%e6%ad%a4%e4%b9%9f%e8%a6%81%e6%8a%8a-kubernetes-%e6%9c%8d%e5%8a%a1%e5%90%8d%e5%92%8c%e5%90%84%e4%b8%aa%e6%9c%8d%e5%8a%a1%e5%9f%9f%e5%90%8d%e4%b9%9f%e6%b7%bb%e5%8a%a0%e8%bf%9b%e5%8e%bb">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h3 id="创建apiserver的服务配置文件">
 创建apiserver的服务配置文件
 &lt;a class="anchor" href="#%e5%88%9b%e5%bb%baapiserver%e7%9a%84%e6%9c%8d%e5%8a%a1%e9%85%8d%e7%bd%ae%e6%96%87%e4%bb%b6">#&lt;/a>
&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>Unit&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Description&lt;span style="color:#f92672">=&lt;/span>Kubernetes API Server
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Documentation&lt;span style="color:#f92672">=&lt;/span>https://github.com/GoogleCloudPlatform/kubernetes
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>After&lt;span style="color:#f92672">=&lt;/span>network.target
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>Service&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ExecStart&lt;span style="color:#f92672">={{&lt;/span> bin_dir &lt;span style="color:#f92672">}}&lt;/span>/kube-apiserver &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --allow-privileged&lt;span style="color:#f92672">=&lt;/span>true &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --anonymous-auth&lt;span style="color:#f92672">=&lt;/span>false &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --api-audiences&lt;span style="color:#f92672">=&lt;/span>api,istio-ca &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --authorization-mode&lt;span style="color:#f92672">=&lt;/span>Node,RBAC &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --bind-address&lt;span style="color:#f92672">={{&lt;/span> inventory_hostname &lt;span style="color:#f92672">}}&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --client-ca-file&lt;span style="color:#f92672">={{&lt;/span> ca_dir &lt;span style="color:#f92672">}}&lt;/span>/ca.pem &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --endpoint-reconciler-type&lt;span style="color:#f92672">=&lt;/span>lease &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --etcd-cafile&lt;span style="color:#f92672">={{&lt;/span> ca_dir &lt;span style="color:#f92672">}}&lt;/span>/ca.pem &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --etcd-certfile&lt;span style="color:#f92672">={{&lt;/span> ca_dir &lt;span style="color:#f92672">}}&lt;/span>/kubernetes.pem &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --etcd-keyfile&lt;span style="color:#f92672">={{&lt;/span> ca_dir &lt;span style="color:#f92672">}}&lt;/span>/kubernetes-key.pem &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --etcd-servers&lt;span style="color:#f92672">={{&lt;/span> ETCD_ENDPOINTS &lt;span style="color:#f92672">}}&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --kubelet-certificate-authority&lt;span style="color:#f92672">={{&lt;/span> ca_dir &lt;span style="color:#f92672">}}&lt;/span>/ca.pem &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --kubelet-client-certificate&lt;span style="color:#f92672">={{&lt;/span> ca_dir &lt;span style="color:#f92672">}}&lt;/span>/kubernetes.pem &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --kubelet-client-key&lt;span style="color:#f92672">={{&lt;/span> ca_dir &lt;span style="color:#f92672">}}&lt;/span>/kubernetes-key.pem &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --secure-port&lt;span style="color:#f92672">={{&lt;/span> SECURE_PORT &lt;span style="color:#f92672">}}&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --service-account-issuer&lt;span style="color:#f92672">=&lt;/span>https://kubernetes.default.svc &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --service-account-signing-key-file&lt;span style="color:#f92672">={{&lt;/span> ca_dir &lt;span style="color:#f92672">}}&lt;/span>/ca-key.pem &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --service-account-key-file&lt;span style="color:#f92672">={{&lt;/span> ca_dir &lt;span style="color:#f92672">}}&lt;/span>/ca.pem &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --service-cluster-ip-range&lt;span style="color:#f92672">={{&lt;/span> SERVICE_CIDR &lt;span style="color:#f92672">}}&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --service-node-port-range&lt;span style="color:#f92672">={{&lt;/span> NODE_PORT_RANGE &lt;span style="color:#f92672">}}&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --tls-cert-file&lt;span style="color:#f92672">={{&lt;/span> ca_dir &lt;span style="color:#f92672">}}&lt;/span>/kubernetes.pem &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --tls-private-key-file&lt;span style="color:#f92672">={{&lt;/span> ca_dir &lt;span style="color:#f92672">}}&lt;/span>/kubernetes-key.pem &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --requestheader-client-ca-file&lt;span style="color:#f92672">={{&lt;/span> ca_dir &lt;span style="color:#f92672">}}&lt;/span>/ca.pem &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --requestheader-allowed-names&lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --requestheader-extra-headers-prefix&lt;span style="color:#f92672">=&lt;/span>X-Remote-Extra- &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --requestheader-group-headers&lt;span style="color:#f92672">=&lt;/span>X-Remote-Group &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --requestheader-username-headers&lt;span style="color:#f92672">=&lt;/span>X-Remote-User &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --proxy-client-cert-file&lt;span style="color:#f92672">={{&lt;/span> ca_dir &lt;span style="color:#f92672">}}&lt;/span>/aggregator-proxy.pem &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --proxy-client-key-file&lt;span style="color:#f92672">={{&lt;/span> ca_dir &lt;span style="color:#f92672">}}&lt;/span>/aggregator-proxy-key.pem &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --enable-aggregator-routing&lt;span style="color:#f92672">=&lt;/span>true &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --v&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Restart&lt;span style="color:#f92672">=&lt;/span>always
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>RestartSec&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">5&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Type&lt;span style="color:#f92672">=&lt;/span>notify
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>LimitNOFILE&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">65536&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>Install&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>WantedBy&lt;span style="color:#f92672">=&lt;/span>multi-user.target
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>
&lt;h5 id="kubernetes-对-api-访问需要依次经过认证授权和准入控制admission-controll认证解决用户是谁的问题授权解决用户能做什么的问题admission-control则是资源管理方面的作用">
 Kubernetes 对 API 访问需要依次经过认证、授权和准入控制(admission controll)，认证解决用户是谁的问题，授权解决用户能做什么的问题，Admission Control则是资源管理方面的作用。
 &lt;a class="anchor" href="#kubernetes-%e5%af%b9-api-%e8%ae%bf%e9%97%ae%e9%9c%80%e8%a6%81%e4%be%9d%e6%ac%a1%e7%bb%8f%e8%bf%87%e8%ae%a4%e8%af%81%e6%8e%88%e6%9d%83%e5%92%8c%e5%87%86%e5%85%a5%e6%8e%a7%e5%88%b6admission-controll%e8%ae%a4%e8%af%81%e8%a7%a3%e5%86%b3%e7%94%a8%e6%88%b7%e6%98%af%e8%b0%81%e7%9a%84%e9%97%ae%e9%a2%98%e6%8e%88%e6%9d%83%e8%a7%a3%e5%86%b3%e7%94%a8%e6%88%b7%e8%83%bd%e5%81%9a%e4%bb%80%e4%b9%88%e7%9a%84%e9%97%ae%e9%a2%98admission-control%e5%88%99%e6%98%af%e8%b5%84%e6%ba%90%e7%ae%a1%e7%90%86%e6%96%b9%e9%9d%a2%e7%9a%84%e4%bd%9c%e7%94%a8">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="关于authorization-modenoderbac-v17支持node授权配合noderestriction准入控制来限制kubelet仅可访问nodeendpointpodservice以及secretconfigmappv和pvc等相关的资源需要注意的是v17中node-授权是默认开启的v18中需要显式配置开启否则-node无法正常工作">
 关于authorization-mode=Node,RBAC v1.7+支持Node授权，配合NodeRestriction准入控制来限制kubelet仅可访问node、endpoint、pod、service以及secret、configmap、PV和PVC等相关的资源；需要注意的是v1.7中Node 授权是默认开启的，v1.8中需要显式配置开启，否则 Node无法正常工作
 &lt;a class="anchor" href="#%e5%85%b3%e4%ba%8eauthorization-modenoderbac-v17%e6%94%af%e6%8c%81node%e6%8e%88%e6%9d%83%e9%85%8d%e5%90%88noderestriction%e5%87%86%e5%85%a5%e6%8e%a7%e5%88%b6%e6%9d%a5%e9%99%90%e5%88%b6kubelet%e4%bb%85%e5%8f%af%e8%ae%bf%e9%97%aenodeendpointpodservice%e4%bb%a5%e5%8f%8asecretconfigmappv%e5%92%8cpvc%e7%ad%89%e7%9b%b8%e5%85%b3%e7%9a%84%e8%b5%84%e6%ba%90%e9%9c%80%e8%a6%81%e6%b3%a8%e6%84%8f%e7%9a%84%e6%98%afv17%e4%b8%adnode-%e6%8e%88%e6%9d%83%e6%98%af%e9%bb%98%e8%ae%a4%e5%bc%80%e5%90%af%e7%9a%84v18%e4%b8%ad%e9%9c%80%e8%a6%81%e6%98%be%e5%bc%8f%e9%85%8d%e7%bd%ae%e5%bc%80%e5%90%af%e5%90%a6%e5%88%99-node%e6%97%a0%e6%b3%95%e6%ad%a3%e5%b8%b8%e5%b7%a5%e4%bd%9c">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="详细参数配置请参考kube-apiserver---help关于认证授权和准入控制请阅读">
 详细参数配置请参考&lt;code>kube-apiserver --help&lt;/code>，关于认证、授权和准入控制请&lt;a href="https://github.com/feiskyer/kubernetes-handbook/blob/master/components/apiserver.md">阅读&lt;/a>
 &lt;a class="anchor" href="#%e8%af%a6%e7%bb%86%e5%8f%82%e6%95%b0%e9%85%8d%e7%bd%ae%e8%af%b7%e5%8f%82%e8%80%83kube-apiserver---help%e5%85%b3%e4%ba%8e%e8%ae%a4%e8%af%81%e6%8e%88%e6%9d%83%e5%92%8c%e5%87%86%e5%85%a5%e6%8e%a7%e5%88%b6%e8%af%b7%e9%98%85%e8%af%bb">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="增加了访问kubelet使用的证书配置防止匿名访问kubelet的安全漏洞详见漏洞说明">
 增加了访问kubelet使用的证书配置，防止匿名访问kubelet的安全漏洞，详见&lt;a href="../mixes/01.fix_kubelet_annoymous_access.md">漏洞说明&lt;/a>
 &lt;a class="anchor" href="#%e5%a2%9e%e5%8a%a0%e4%ba%86%e8%ae%bf%e9%97%aekubelet%e4%bd%bf%e7%94%a8%e7%9a%84%e8%af%81%e4%b9%a6%e9%85%8d%e7%bd%ae%e9%98%b2%e6%ad%a2%e5%8c%bf%e5%90%8d%e8%ae%bf%e9%97%aekubelet%e7%9a%84%e5%ae%89%e5%85%a8%e6%bc%8f%e6%b4%9e%e8%af%a6%e8%a7%81%e6%bc%8f%e6%b4%9e%e8%af%b4%e6%98%8e">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h3 id="创建controller-manager-的服务文件">
 创建controller-manager 的服务文件
 &lt;a class="anchor" href="#%e5%88%9b%e5%bb%bacontroller-manager-%e7%9a%84%e6%9c%8d%e5%8a%a1%e6%96%87%e4%bb%b6">#&lt;/a>
&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>Unit&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Description&lt;span style="color:#f92672">=&lt;/span>Kubernetes Controller Manager
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Documentation&lt;span style="color:#f92672">=&lt;/span>https://github.com/GoogleCloudPlatform/kubernetes
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>Service&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ExecStart&lt;span style="color:#f92672">={{&lt;/span> bin_dir &lt;span style="color:#f92672">}}&lt;/span>/kube-controller-manager &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --allocate-node-cidrs&lt;span style="color:#f92672">=&lt;/span>true &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --authentication-kubeconfig&lt;span style="color:#f92672">=&lt;/span>/etc/kubernetes/kube-controller-manager.kubeconfig &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --authorization-kubeconfig&lt;span style="color:#f92672">=&lt;/span>/etc/kubernetes/kube-controller-manager.kubeconfig &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --bind-address&lt;span style="color:#f92672">=&lt;/span>0.0.0.0 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --cluster-cidr&lt;span style="color:#f92672">={{&lt;/span> CLUSTER_CIDR &lt;span style="color:#f92672">}}&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --cluster-name&lt;span style="color:#f92672">=&lt;/span>kubernetes &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --cluster-signing-cert-file&lt;span style="color:#f92672">={{&lt;/span> ca_dir &lt;span style="color:#f92672">}}&lt;/span>/ca.pem &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --cluster-signing-key-file&lt;span style="color:#f92672">={{&lt;/span> ca_dir &lt;span style="color:#f92672">}}&lt;/span>/ca-key.pem &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --kubeconfig&lt;span style="color:#f92672">=&lt;/span>/etc/kubernetes/kube-controller-manager.kubeconfig &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --leader-elect&lt;span style="color:#f92672">=&lt;/span>true &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --node-cidr-mask-size&lt;span style="color:#f92672">={{&lt;/span> NODE_CIDR_LEN &lt;span style="color:#f92672">}}&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --root-ca-file&lt;span style="color:#f92672">={{&lt;/span> ca_dir &lt;span style="color:#f92672">}}&lt;/span>/ca.pem &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --service-account-private-key-file&lt;span style="color:#f92672">={{&lt;/span> ca_dir &lt;span style="color:#f92672">}}&lt;/span>/ca-key.pem &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --service-cluster-ip-range&lt;span style="color:#f92672">={{&lt;/span> SERVICE_CIDR &lt;span style="color:#f92672">}}&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --use-service-account-credentials&lt;span style="color:#f92672">=&lt;/span>true &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --v&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Restart&lt;span style="color:#f92672">=&lt;/span>always
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>RestartSec&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">5&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>Install&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>WantedBy&lt;span style="color:#f92672">=&lt;/span>multi-user.target
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>
&lt;h5 id="cluster-cidr-指定-cluster-中-pod-的-cidr-范围该网段在各-node-间必须路由可达flannelcalico-等网络插件实现">
 &amp;ndash;cluster-cidr 指定 Cluster 中 Pod 的 CIDR 范围，该网段在各 Node 间必须路由可达(flannel/calico 等网络插件实现)
 &lt;a class="anchor" href="#cluster-cidr-%e6%8c%87%e5%ae%9a-cluster-%e4%b8%ad-pod-%e7%9a%84-cidr-%e8%8c%83%e5%9b%b4%e8%af%a5%e7%bd%91%e6%ae%b5%e5%9c%a8%e5%90%84-node-%e9%97%b4%e5%bf%85%e9%a1%bb%e8%b7%af%e7%94%b1%e5%8f%af%e8%be%beflannelcalico-%e7%ad%89%e7%bd%91%e7%bb%9c%e6%8f%92%e4%bb%b6%e5%ae%9e%e7%8e%b0">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="service-cluster-ip-range-参数指定-cluster-中-service-的cidr范围必须和-kube-apiserver-中的参数一致">
 &amp;ndash;service-cluster-ip-range 参数指定 Cluster 中 Service 的CIDR范围，必须和 kube-apiserver 中的参数一致
 &lt;a class="anchor" href="#service-cluster-ip-range-%e5%8f%82%e6%95%b0%e6%8c%87%e5%ae%9a-cluster-%e4%b8%ad-service-%e7%9a%84cidr%e8%8c%83%e5%9b%b4%e5%bf%85%e9%a1%bb%e5%92%8c-kube-apiserver-%e4%b8%ad%e7%9a%84%e5%8f%82%e6%95%b0%e4%b8%80%e8%87%b4">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="cluster-signing--指定的证书和私钥文件用来签名为-tls-bootstrap-创建的证书和私钥">
 &amp;ndash;cluster-signing-* 指定的证书和私钥文件用来签名为 TLS BootStrap 创建的证书和私钥
 &lt;a class="anchor" href="#cluster-signing--%e6%8c%87%e5%ae%9a%e7%9a%84%e8%af%81%e4%b9%a6%e5%92%8c%e7%a7%81%e9%92%a5%e6%96%87%e4%bb%b6%e7%94%a8%e6%9d%a5%e7%ad%be%e5%90%8d%e4%b8%ba-tls-bootstrap-%e5%88%9b%e5%bb%ba%e7%9a%84%e8%af%81%e4%b9%a6%e5%92%8c%e7%a7%81%e9%92%a5">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="root-ca-file-用来对-kube-apiserver-证书进行校验指定该参数后才会在pod-容器的-serviceaccount-中放置该-ca-证书文件">
 &amp;ndash;root-ca-file 用来对 kube-apiserver 证书进行校验，指定该参数后，才会在Pod 容器的 ServiceAccount 中放置该 CA 证书文件
 &lt;a class="anchor" href="#root-ca-file-%e7%94%a8%e6%9d%a5%e5%af%b9-kube-apiserver-%e8%af%81%e4%b9%a6%e8%bf%9b%e8%a1%8c%e6%a0%a1%e9%aa%8c%e6%8c%87%e5%ae%9a%e8%af%a5%e5%8f%82%e6%95%b0%e5%90%8e%e6%89%8d%e4%bc%9a%e5%9c%a8pod-%e5%ae%b9%e5%99%a8%e7%9a%84-serviceaccount-%e4%b8%ad%e6%94%be%e7%bd%ae%e8%af%a5-ca-%e8%af%81%e4%b9%a6%e6%96%87%e4%bb%b6">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="leader-electtrue-使用多节点选主的方式选择主节点只有主节点才会启动所有控制器而其他从节点则仅执行选主算法">
 &amp;ndash;leader-elect=true 使用多节点选主的方式选择主节点。只有主节点才会启动所有控制器，而其他从节点则仅执行选主算法
 &lt;a class="anchor" href="#leader-electtrue-%e4%bd%bf%e7%94%a8%e5%a4%9a%e8%8a%82%e7%82%b9%e9%80%89%e4%b8%bb%e7%9a%84%e6%96%b9%e5%bc%8f%e9%80%89%e6%8b%a9%e4%b8%bb%e8%8a%82%e7%82%b9%e5%8f%aa%e6%9c%89%e4%b8%bb%e8%8a%82%e7%82%b9%e6%89%8d%e4%bc%9a%e5%90%af%e5%8a%a8%e6%89%80%e6%9c%89%e6%8e%a7%e5%88%b6%e5%99%a8%e8%80%8c%e5%85%b6%e4%bb%96%e4%bb%8e%e8%8a%82%e7%82%b9%e5%88%99%e4%bb%85%e6%89%a7%e8%a1%8c%e9%80%89%e4%b8%bb%e7%ae%97%e6%b3%95">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h3 id="创建scheduler-的服务文件">
 创建scheduler 的服务文件
 &lt;a class="anchor" href="#%e5%88%9b%e5%bb%bascheduler-%e7%9a%84%e6%9c%8d%e5%8a%a1%e6%96%87%e4%bb%b6">#&lt;/a>
&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>Unit&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Description&lt;span style="color:#f92672">=&lt;/span>Kubernetes Scheduler
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Documentation&lt;span style="color:#f92672">=&lt;/span>https://github.com/GoogleCloudPlatform/kubernetes
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>Service&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ExecStart&lt;span style="color:#f92672">={{&lt;/span> bin_dir &lt;span style="color:#f92672">}}&lt;/span>/kube-scheduler &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --authentication-kubeconfig&lt;span style="color:#f92672">=&lt;/span>/etc/kubernetes/kube-scheduler.kubeconfig &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --authorization-kubeconfig&lt;span style="color:#f92672">=&lt;/span>/etc/kubernetes/kube-scheduler.kubeconfig &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --bind-address&lt;span style="color:#f92672">=&lt;/span>0.0.0.0 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --kubeconfig&lt;span style="color:#f92672">=&lt;/span>/etc/kubernetes/kube-scheduler.kubeconfig &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --leader-elect&lt;span style="color:#f92672">=&lt;/span>true &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --v&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Restart&lt;span style="color:#f92672">=&lt;/span>always
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>RestartSec&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">5&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>Install&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>WantedBy&lt;span style="color:#f92672">=&lt;/span>multi-user.target
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>
&lt;h5 id="leader-electtrue-部署多台机器组成的-master-集群时选举产生一个处于工作状态的-kube-controller-manager-进程">
 &amp;ndash;leader-elect=true 部署多台机器组成的 master 集群时选举产生一个处于工作状态的 kube-controller-manager 进程
 &lt;a class="anchor" href="#leader-electtrue-%e9%83%a8%e7%bd%b2%e5%a4%9a%e5%8f%b0%e6%9c%ba%e5%99%a8%e7%bb%84%e6%88%90%e7%9a%84-master-%e9%9b%86%e7%be%a4%e6%97%b6%e9%80%89%e4%b8%be%e4%ba%a7%e7%94%9f%e4%b8%80%e4%b8%aa%e5%a4%84%e4%ba%8e%e5%b7%a5%e4%bd%9c%e7%8a%b6%e6%80%81%e7%9a%84-kube-controller-manager-%e8%bf%9b%e7%a8%8b">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h3 id="在master-节点安装-node-服务-kubelet-kube-proxy">
 在master 节点安装 node 服务: kubelet kube-proxy
 &lt;a class="anchor" href="#%e5%9c%a8master-%e8%8a%82%e7%82%b9%e5%ae%89%e8%a3%85-node-%e6%9c%8d%e5%8a%a1-kubelet-kube-proxy">#&lt;/a>
&lt;/h3>
&lt;h5 id="项目master-分支使用-daemonset-方式安装网络插件如果master-节点不安装-kubelet-服务是无法安装网络插件的如果-master-节点不安装网络插件那么通过apiserver-方式无法访问-dashboard-kibana等管理界面issues-130">
 项目master 分支使用 DaemonSet 方式安装网络插件，如果master 节点不安装 kubelet 服务是无法安装网络插件的，如果 master 节点不安装网络插件，那么通过&lt;code>apiserver&lt;/code> 方式无法访问 &lt;code>dashboard&lt;/code> &lt;code>kibana&lt;/code>等管理界面，&lt;a href="https://github.com/easzlab/kubeasz/issues/130">ISSUES #130&lt;/a>
 &lt;a class="anchor" href="#%e9%a1%b9%e7%9b%aemaster-%e5%88%86%e6%94%af%e4%bd%bf%e7%94%a8-daemonset-%e6%96%b9%e5%bc%8f%e5%ae%89%e8%a3%85%e7%bd%91%e7%bb%9c%e6%8f%92%e4%bb%b6%e5%a6%82%e6%9e%9cmaster-%e8%8a%82%e7%82%b9%e4%b8%8d%e5%ae%89%e8%a3%85-kubelet-%e6%9c%8d%e5%8a%a1%e6%98%af%e6%97%a0%e6%b3%95%e5%ae%89%e8%a3%85%e7%bd%91%e7%bb%9c%e6%8f%92%e4%bb%b6%e7%9a%84%e5%a6%82%e6%9e%9c-master-%e8%8a%82%e7%82%b9%e4%b8%8d%e5%ae%89%e8%a3%85%e7%bd%91%e7%bb%9c%e6%8f%92%e4%bb%b6%e9%82%a3%e4%b9%88%e9%80%9a%e8%bf%87apiserver-%e6%96%b9%e5%bc%8f%e6%97%a0%e6%b3%95%e8%ae%bf%e9%97%ae-dashboard-kibana%e7%ad%89%e7%ae%a1%e7%90%86%e7%95%8c%e9%9d%a2issues-130">#&lt;/a>
&lt;/h5>
&lt;h5 id="在master-节点也同时成为-node-节点后默认业务-pod也会调度到-master节点可以使用-kubectl-cordon命令禁止业务-pod调度到-master节点">
 在master 节点也同时成为 node 节点后，默认业务 POD也会调度到 master节点；可以使用 &lt;code>kubectl cordon&lt;/code>命令禁止业务 POD调度到 master节点。
 &lt;a class="anchor" href="#%e5%9c%a8master-%e8%8a%82%e7%82%b9%e4%b9%9f%e5%90%8c%e6%97%b6%e6%88%90%e4%b8%ba-node-%e8%8a%82%e7%82%b9%e5%90%8e%e9%bb%98%e8%ae%a4%e4%b8%9a%e5%8a%a1-pod%e4%b9%9f%e4%bc%9a%e8%b0%83%e5%ba%a6%e5%88%b0-master%e8%8a%82%e7%82%b9%e5%8f%af%e4%bb%a5%e4%bd%bf%e7%94%a8-kubectl-cordon%e5%91%bd%e4%bb%a4%e7%a6%81%e6%ad%a2%e4%b8%9a%e5%8a%a1-pod%e8%b0%83%e5%ba%a6%e5%88%b0-master%e8%8a%82%e7%82%b9">#&lt;/a>
&lt;/h5>
&lt;h3 id="master-集群的验证">
 master 集群的验证
 &lt;a class="anchor" href="#master-%e9%9b%86%e7%be%a4%e7%9a%84%e9%aa%8c%e8%af%81">#&lt;/a>
&lt;/h3>
&lt;h5 id="运行-ansible-playbook-04kube-masteryml-成功后验证-master节点的主要组件">
 运行 &lt;code>ansible-playbook 04.kube-master.yml&lt;/code> 成功后，验证 master节点的主要组件：
 &lt;a class="anchor" href="#%e8%bf%90%e8%a1%8c-ansible-playbook-04kube-masteryml-%e6%88%90%e5%8a%9f%e5%90%8e%e9%aa%8c%e8%af%81-master%e8%8a%82%e7%82%b9%e7%9a%84%e4%b8%bb%e8%a6%81%e7%bb%84%e4%bb%b6">#&lt;/a>
&lt;/h5>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 查看进程状态&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>systemctl status kube-apiserver
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>systemctl status kube-controller-manager
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>systemctl status kube-scheduler
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 查看进程运行日志&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>journalctl -u kube-apiserver
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>journalctl -u kube-controller-manager
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>journalctl -u kube-scheduler
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>执行 &lt;code>kubectl get componentstatus&lt;/code> 可以看到&lt;/p></description></item><item><title>2023-09-28 05-安装kube_node节点</title><link>https://qq547475331.github.io/docs/05-%E5%AE%89%E8%A3%85kube_node%E8%8A%82%E7%82%B9-05--an-zhuang-kubenode-jie-dian/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/05-%E5%AE%89%E8%A3%85kube_node%E8%8A%82%E7%82%B9-05--an-zhuang-kubenode-jie-dian/</guid><description>&lt;h2 id="05-安装kube_node节点">
 05-安装kube_node节点
 &lt;a class="anchor" href="#05-%e5%ae%89%e8%a3%85kube_node%e8%8a%82%e7%82%b9">#&lt;/a>
&lt;/h2>
&lt;h5 id="kube_node-是集群中运行工作负载的节点前置条件需要先部署好kube_master节点它需要部署如下组件">
 &lt;code>kube_node&lt;/code> 是集群中运行工作负载的节点，前置条件需要先部署好&lt;code>kube_master&lt;/code>节点，它需要部署如下组件：
 &lt;a class="anchor" href="#kube_node-%e6%98%af%e9%9b%86%e7%be%a4%e4%b8%ad%e8%bf%90%e8%a1%8c%e5%b7%a5%e4%bd%9c%e8%b4%9f%e8%bd%bd%e7%9a%84%e8%8a%82%e7%82%b9%e5%89%8d%e7%bd%ae%e6%9d%a1%e4%bb%b6%e9%9c%80%e8%a6%81%e5%85%88%e9%83%a8%e7%bd%b2%e5%a5%bdkube_master%e8%8a%82%e7%82%b9%e5%ae%83%e9%9c%80%e8%a6%81%e9%83%a8%e7%bd%b2%e5%a6%82%e4%b8%8b%e7%bb%84%e4%bb%b6">#&lt;/a>
&lt;/h5>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>cat playbooks/05.kube-node.yml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- hosts: kube_node
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> roles:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">{&lt;/span> role: kube-lb, when: &lt;span style="color:#e6db74">&amp;#34;inventory_hostname not in groups[&amp;#39;kube_master&amp;#39;]&amp;#34;&lt;/span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">{&lt;/span> role: kube-node, when: &lt;span style="color:#e6db74">&amp;#34;inventory_hostname not in groups[&amp;#39;kube_master&amp;#39;]&amp;#34;&lt;/span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>
&lt;h5 id="kube-lb由nginx裁剪编译的四层负载均衡用于将请求转发到主节点的-apiserver服务">
 kube-lb：由nginx裁剪编译的四层负载均衡，用于将请求转发到主节点的 apiserver服务
 &lt;a class="anchor" href="#kube-lb%e7%94%b1nginx%e8%a3%81%e5%89%aa%e7%bc%96%e8%af%91%e7%9a%84%e5%9b%9b%e5%b1%82%e8%b4%9f%e8%bd%bd%e5%9d%87%e8%a1%a1%e7%94%a8%e4%ba%8e%e5%b0%86%e8%af%b7%e6%b1%82%e8%bd%ac%e5%8f%91%e5%88%b0%e4%b8%bb%e8%8a%82%e7%82%b9%e7%9a%84-apiserver%e6%9c%8d%e5%8a%a1">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="kubeletkube_node上最主要的组件">
 kubelet：kube_node上最主要的组件
 &lt;a class="anchor" href="#kubeletkube_node%e4%b8%8a%e6%9c%80%e4%b8%bb%e8%a6%81%e7%9a%84%e7%bb%84%e4%bb%b6">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="kube-proxy-发布应用服务与负载均衡">
 kube-proxy： 发布应用服务与负载均衡
 &lt;a class="anchor" href="#kube-proxy-%e5%8f%91%e5%b8%83%e5%ba%94%e7%94%a8%e6%9c%8d%e5%8a%a1%e4%b8%8e%e8%b4%9f%e8%bd%bd%e5%9d%87%e8%a1%a1">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h3 id="创建cni-基础网络插件配置文件">
 创建cni 基础网络插件配置文件
 &lt;a class="anchor" href="#%e5%88%9b%e5%bb%bacni-%e5%9f%ba%e7%a1%80%e7%bd%91%e7%bb%9c%e6%8f%92%e4%bb%b6%e9%85%8d%e7%bd%ae%e6%96%87%e4%bb%b6">#&lt;/a>
&lt;/h3>
&lt;h5 id="因为后续需要用-daemonset-pod方式运行k8s网络插件所以kubeletserver服务必须开启cni相关参数并且提供cni网络配置文件">
 因为后续需要用 &lt;code>DaemonSet Pod&lt;/code>方式运行k8s网络插件，所以kubelet.server服务必须开启cni相关参数，并且提供cni网络配置文件
 &lt;a class="anchor" href="#%e5%9b%a0%e4%b8%ba%e5%90%8e%e7%bb%ad%e9%9c%80%e8%a6%81%e7%94%a8-daemonset-pod%e6%96%b9%e5%bc%8f%e8%bf%90%e8%a1%8ck8s%e7%bd%91%e7%bb%9c%e6%8f%92%e4%bb%b6%e6%89%80%e4%bb%a5kubeletserver%e6%9c%8d%e5%8a%a1%e5%bf%85%e9%a1%bb%e5%bc%80%e5%90%afcni%e7%9b%b8%e5%85%b3%e5%8f%82%e6%95%b0%e5%b9%b6%e4%b8%94%e6%8f%90%e4%be%9bcni%e7%bd%91%e7%bb%9c%e9%85%8d%e7%bd%ae%e6%96%87%e4%bb%b6">#&lt;/a>
&lt;/h5>
&lt;h5 id="创建-kubelet-的服务文件">
 创建 kubelet 的服务文件
 &lt;a class="anchor" href="#%e5%88%9b%e5%bb%ba-kubelet-%e7%9a%84%e6%9c%8d%e5%8a%a1%e6%96%87%e4%bb%b6">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="根据官方建议独立使用-kubelet-配置文件详见roleskube-nodetemplateskubelet-configyamlj2">
 根据官方建议独立使用 kubelet 配置文件，详见roles/kube-node/templates/kubelet-config.yaml.j2
 &lt;a class="anchor" href="#%e6%a0%b9%e6%8d%ae%e5%ae%98%e6%96%b9%e5%bb%ba%e8%ae%ae%e7%8b%ac%e7%ab%8b%e4%bd%bf%e7%94%a8-kubelet-%e9%85%8d%e7%bd%ae%e6%96%87%e4%bb%b6%e8%af%a6%e8%a7%81roleskube-nodetemplateskubelet-configyamlj2">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="必须先创建工作目录-varlibkubelet">
 必须先创建工作目录 &lt;code>/var/lib/kubelet&lt;/code>
 &lt;a class="anchor" href="#%e5%bf%85%e9%a1%bb%e5%85%88%e5%88%9b%e5%bb%ba%e5%b7%a5%e4%bd%9c%e7%9b%ae%e5%bd%95-varlibkubelet">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>Unit&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Description&lt;span style="color:#f92672">=&lt;/span>Kubernetes Kubelet
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Documentation&lt;span style="color:#f92672">=&lt;/span>https://github.com/GoogleCloudPlatform/kubernetes
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>Service&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>WorkingDirectory&lt;span style="color:#f92672">=&lt;/span>/var/lib/kubelet
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ExecStartPre&lt;span style="color:#f92672">=&lt;/span>/bin/mount -o remount,rw &lt;span style="color:#e6db74">&amp;#39;/sys/fs/cgroup&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">{&lt;/span>% &lt;span style="color:#66d9ef">if&lt;/span> KUBE_RESERVED_ENABLED &lt;span style="color:#f92672">==&lt;/span> &lt;span style="color:#e6db74">&amp;#34;yes&amp;#34;&lt;/span> or SYS_RESERVED_ENABLED &lt;span style="color:#f92672">==&lt;/span> &lt;span style="color:#e6db74">&amp;#34;yes&amp;#34;&lt;/span> %&lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ExecStartPre&lt;span style="color:#f92672">=&lt;/span>/bin/mkdir -p /sys/fs/cgroup/cpu/podruntime.slice
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ExecStartPre&lt;span style="color:#f92672">=&lt;/span>/bin/mkdir -p /sys/fs/cgroup/cpuacct/podruntime.slice
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ExecStartPre&lt;span style="color:#f92672">=&lt;/span>/bin/mkdir -p /sys/fs/cgroup/cpuset/podruntime.slice
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ExecStartPre&lt;span style="color:#f92672">=&lt;/span>/bin/mkdir -p /sys/fs/cgroup/memory/podruntime.slice
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ExecStartPre&lt;span style="color:#f92672">=&lt;/span>/bin/mkdir -p /sys/fs/cgroup/pids/podruntime.slice
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ExecStartPre&lt;span style="color:#f92672">=&lt;/span>/bin/mkdir -p /sys/fs/cgroup/systemd/podruntime.slice
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ExecStartPre&lt;span style="color:#f92672">=&lt;/span>/bin/mkdir -p /sys/fs/cgroup/cpu/system.slice
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ExecStartPre&lt;span style="color:#f92672">=&lt;/span>/bin/mkdir -p /sys/fs/cgroup/cpuacct/system.slice
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ExecStartPre&lt;span style="color:#f92672">=&lt;/span>/bin/mkdir -p /sys/fs/cgroup/cpuset/system.slice
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ExecStartPre&lt;span style="color:#f92672">=&lt;/span>/bin/mkdir -p /sys/fs/cgroup/memory/system.slice
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ExecStartPre&lt;span style="color:#f92672">=&lt;/span>/bin/mkdir -p /sys/fs/cgroup/pids/system.slice
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ExecStartPre&lt;span style="color:#f92672">=&lt;/span>/bin/mkdir -p /sys/fs/cgroup/systemd/system.slice
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ExecStartPre&lt;span style="color:#f92672">=&lt;/span>/bin/mkdir -p /sys/fs/cgroup/hugetlb/podruntime.slice
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ExecStartPre&lt;span style="color:#f92672">=&lt;/span>/bin/mkdir -p /sys/fs/cgroup/hugetlb/system.slice
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">{&lt;/span>% endif %&lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ExecStart&lt;span style="color:#f92672">={{&lt;/span> bin_dir &lt;span style="color:#f92672">}}&lt;/span>/kubelet &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --config&lt;span style="color:#f92672">=&lt;/span>/var/lib/kubelet/config.yaml &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --container-runtime-endpoint&lt;span style="color:#f92672">=&lt;/span>unix:///run/containerd/containerd.sock &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --hostname-override&lt;span style="color:#f92672">={{&lt;/span> K8S_NODENAME &lt;span style="color:#f92672">}}&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --kubeconfig&lt;span style="color:#f92672">=&lt;/span>/etc/kubernetes/kubelet.kubeconfig &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --root-dir&lt;span style="color:#f92672">={{&lt;/span> KUBELET_ROOT_DIR &lt;span style="color:#f92672">}}&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --v&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Restart&lt;span style="color:#f92672">=&lt;/span>always
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>RestartSec&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">5&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>Install&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>WantedBy&lt;span style="color:#f92672">=&lt;/span>multi-user.target
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>
&lt;h5 id="execstartprebinmkdir--p-xxx-对于某些系统centos7cpuset和hugetlb-是默认没有初始化systemslice-的需要手动创建否则在启用kube-reserved-cgroup-时会报错failed-to-start-containermanager-failed-to-enforce-system-reserved-cgroup-limits">
 &amp;ndash;ExecStartPre=/bin/mkdir -p xxx 对于某些系统（centos7）cpuset和hugetlb 是默认没有初始化system.slice 的，需要手动创建，否则在启用&amp;ndash;kube-reserved-cgroup 时会报错Failed to start ContainerManager Failed to enforce System Reserved Cgroup Limits
 &lt;a class="anchor" href="#execstartprebinmkdir--p-xxx-%e5%af%b9%e4%ba%8e%e6%9f%90%e4%ba%9b%e7%b3%bb%e7%bb%9fcentos7cpuset%e5%92%8chugetlb-%e6%98%af%e9%bb%98%e8%ae%a4%e6%b2%a1%e6%9c%89%e5%88%9d%e5%a7%8b%e5%8c%96systemslice-%e7%9a%84%e9%9c%80%e8%a6%81%e6%89%8b%e5%8a%a8%e5%88%9b%e5%bb%ba%e5%90%a6%e5%88%99%e5%9c%a8%e5%90%af%e7%94%a8kube-reserved-cgroup-%e6%97%b6%e4%bc%9a%e6%8a%a5%e9%94%99failed-to-start-containermanager-failed-to-enforce-system-reserved-cgroup-limits">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="关于kubelet资源预留相关配置请参考">
 关于kubelet资源预留相关配置请参考 &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/">https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/&lt;/a>
 &lt;a class="anchor" href="#%e5%85%b3%e4%ba%8ekubelet%e8%b5%84%e6%ba%90%e9%a2%84%e7%95%99%e7%9b%b8%e5%85%b3%e9%85%8d%e7%bd%ae%e8%af%b7%e5%8f%82%e8%80%83">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h3 id="创建-kube-proxy-kubeconfig-文件">
 创建 kube-proxy kubeconfig 文件
 &lt;a class="anchor" href="#%e5%88%9b%e5%bb%ba-kube-proxy-kubeconfig-%e6%96%87%e4%bb%b6">#&lt;/a>
&lt;/h3>
&lt;h5 id="该步骤已经在-deploy节点完成rolesdeploytasksmainyml">
 该步骤已经在 deploy节点完成，&lt;a href="../../roles/deploy/tasks/main.yml">roles/deploy/tasks/main.yml&lt;/a>
 &lt;a class="anchor" href="#%e8%af%a5%e6%ad%a5%e9%aa%a4%e5%b7%b2%e7%bb%8f%e5%9c%a8-deploy%e8%8a%82%e7%82%b9%e5%ae%8c%e6%88%90rolesdeploytasksmainyml">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="生成的kube-proxykubeconfig-配置文件需要移动到etckubernetes目录后续kube-proxy服务启动参数里面需要指定">
 生成的kube-proxy.kubeconfig 配置文件需要移动到/etc/kubernetes/目录，后续kube-proxy服务启动参数里面需要指定
 &lt;a class="anchor" href="#%e7%94%9f%e6%88%90%e7%9a%84kube-proxykubeconfig-%e9%85%8d%e7%bd%ae%e6%96%87%e4%bb%b6%e9%9c%80%e8%a6%81%e7%a7%bb%e5%8a%a8%e5%88%b0etckubernetes%e7%9b%ae%e5%bd%95%e5%90%8e%e7%bb%adkube-proxy%e6%9c%8d%e5%8a%a1%e5%90%af%e5%8a%a8%e5%8f%82%e6%95%b0%e9%87%8c%e9%9d%a2%e9%9c%80%e8%a6%81%e6%8c%87%e5%ae%9a">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h3 id="创建-kube-proxy服务文件">
 创建 kube-proxy服务文件
 &lt;a class="anchor" href="#%e5%88%9b%e5%bb%ba-kube-proxy%e6%9c%8d%e5%8a%a1%e6%96%87%e4%bb%b6">#&lt;/a>
&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>Unit&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Description&lt;span style="color:#f92672">=&lt;/span>Kubernetes Kube-Proxy Server
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Documentation&lt;span style="color:#f92672">=&lt;/span>https://github.com/GoogleCloudPlatform/kubernetes
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>After&lt;span style="color:#f92672">=&lt;/span>network.target
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>Service&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>WorkingDirectory&lt;span style="color:#f92672">=&lt;/span>/var/lib/kube-proxy
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ExecStart&lt;span style="color:#f92672">={{&lt;/span> bin_dir &lt;span style="color:#f92672">}}&lt;/span>/kube-proxy &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --config&lt;span style="color:#f92672">=&lt;/span>/var/lib/kube-proxy/kube-proxy-config.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Restart&lt;span style="color:#f92672">=&lt;/span>always
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>RestartSec&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">5&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>LimitNOFILE&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">65536&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>Install&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>WantedBy&lt;span style="color:#f92672">=&lt;/span>multi-user.target
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>请注意 &lt;a href="../../roles/kube-node/templates/kube-proxy-config.yaml.j2">kube-proxy-config&lt;/a> 文件的注释说明&lt;/p></description></item><item><title>2023-09-28 00-集群规划和基础参数设定</title><link>https://qq547475331.github.io/docs/00-%E9%9B%86%E7%BE%A4%E8%A7%84%E5%88%92%E5%92%8C%E5%9F%BA%E7%A1%80%E5%8F%82%E6%95%B0%E8%AE%BE%E5%AE%9A-00--ji-qun-gui-hua-he-ji-chu-can-shu-she-ding/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/00-%E9%9B%86%E7%BE%A4%E8%A7%84%E5%88%92%E5%92%8C%E5%9F%BA%E7%A1%80%E5%8F%82%E6%95%B0%E8%AE%BE%E5%AE%9A-00--ji-qun-gui-hua-he-ji-chu-can-shu-she-ding/</guid><description>&lt;h2 id="0-集群规划和基础参数设定">
 0-集群规划和基础参数设定
 &lt;a class="anchor" href="#0-%e9%9b%86%e7%be%a4%e8%a7%84%e5%88%92%e5%92%8c%e5%9f%ba%e7%a1%80%e5%8f%82%e6%95%b0%e8%ae%be%e5%ae%9a">#&lt;/a>
&lt;/h2>
&lt;h3 id="ha-architecture">
 HA architecture
 &lt;a class="anchor" href="#ha-architecture">#&lt;/a>
&lt;/h3>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/image-20241204235826382.png" alt="image-20241204235826382" />&lt;/p>
&lt;ul>
&lt;li>注意1：确保各节点时区设置一致、时间同步。 如果你的环境没有提供NTP 时间同步，推荐集成安装&lt;a href="../guide/chrony.md">chrony&lt;/a>&lt;/li>
&lt;li>注意2：确保在干净的系统上开始安装，不要使用曾经装过kubeadm或其他k8s发行版的环境&lt;/li>
&lt;li>注意3：建议操作系统升级到新的稳定内核，请结合阅读&lt;a href="../guide/kernel_upgrade.md">内核升级文档&lt;/a>&lt;/li>
&lt;li>注意4：在公有云上创建多主集群，请结合阅读&lt;a href="kubeasz_on_public_cloud.md">在公有云上部署 kubeasz&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="高可用集群所需节点配置如下">
 高可用集群所需节点配置如下
 &lt;a class="anchor" href="#%e9%ab%98%e5%8f%af%e7%94%a8%e9%9b%86%e7%be%a4%e6%89%80%e9%9c%80%e8%8a%82%e7%82%b9%e9%85%8d%e7%bd%ae%e5%a6%82%e4%b8%8b">#&lt;/a>
&lt;/h2>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th style="text-align: left">角色&lt;/th>
 &lt;th style="text-align: left">数量&lt;/th>
 &lt;th style="text-align: left">描述&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td style="text-align: left">部署节点&lt;/td>
 &lt;td style="text-align: left">1&lt;/td>
 &lt;td style="text-align: left">运行ansible/ezctl命令，一般复用第一个master节点&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">etcd节点&lt;/td>
 &lt;td style="text-align: left">3&lt;/td>
 &lt;td style="text-align: left">注意etcd集群需要1,3,5,&amp;hellip;奇数个节点，一般复用master节点&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">master节点&lt;/td>
 &lt;td style="text-align: left">2&lt;/td>
 &lt;td style="text-align: left">高可用集群至少2个master节点&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">node节点&lt;/td>
 &lt;td style="text-align: left">n&lt;/td>
 &lt;td style="text-align: left">运行应用负载的节点，可根据需要提升机器配置/增加节点数&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;p>机器配置：&lt;/p>
&lt;ul>
&lt;li>master节点：4c/8g内存/50g硬盘&lt;/li>
&lt;li>worker节点：建议8c/32g内存/200g硬盘以上&lt;/li>
&lt;/ul>
&lt;p>注意：默认配置下容器运行时和kubelet会占用/var的磁盘空间，如果磁盘分区特殊，可以设置config.yml中的容器运行时和kubelet数据目录：&lt;code>CONTAINERD_STORAGE_DIR&lt;/code> &lt;code>DOCKER_STORAGE_DIR&lt;/code> &lt;code>KUBELET_ROOT_DIR&lt;/code>&lt;/p>
&lt;p>在 kubeasz 2x 版本，多节点高可用集群安装可以使用2种方式&lt;/p>
&lt;ul>
&lt;li>1.按照本文步骤先规划准备，预先配置节点信息后，直接安装多节点高可用集群&lt;/li>
&lt;li>2.先部署单节点集群 &lt;a href="quickStart.md">AllinOne部署&lt;/a>，然后通过 &lt;a href="../op/op-index.md">节点添加&lt;/a> 扩容成高可用集群&lt;/li>
&lt;/ul>
&lt;h2 id="部署步骤">
 部署步骤
 &lt;a class="anchor" href="#%e9%83%a8%e7%bd%b2%e6%ad%a5%e9%aa%a4">#&lt;/a>
&lt;/h2>
&lt;p>以下示例创建一个4节点的多主高可用集群，文档中命令默认都需要root权限运行。&lt;/p>
&lt;h3 id="1基础系统配置">
 1.基础系统配置
 &lt;a class="anchor" href="#1%e5%9f%ba%e7%a1%80%e7%b3%bb%e7%bb%9f%e9%85%8d%e7%bd%ae">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>2c/4g内存/40g硬盘（该配置仅测试用）&lt;/li>
&lt;li>最小化安装&lt;code>Ubuntu 16.04 server&lt;/code>或者&lt;code>CentOS 7 Minimal&lt;/code>&lt;/li>
&lt;li>配置基础网络、更新源、SSH登录等&lt;/li>
&lt;/ul>
&lt;h3 id="2在每个节点安装依赖工具">
 2.在每个节点安装依赖工具
 &lt;a class="anchor" href="#2%e5%9c%a8%e6%af%8f%e4%b8%aa%e8%8a%82%e7%82%b9%e5%ae%89%e8%a3%85%e4%be%9d%e8%b5%96%e5%b7%a5%e5%85%b7">#&lt;/a>
&lt;/h3>
&lt;p>推荐使用ansible in docker 容器化方式运行，无需安装额外依赖。&lt;/p>
&lt;h3 id="3准备ssh免密登陆">
 3.准备ssh免密登陆
 &lt;a class="anchor" href="#3%e5%87%86%e5%a4%87ssh%e5%85%8d%e5%af%86%e7%99%bb%e9%99%86">#&lt;/a>
&lt;/h3>
&lt;p>配置从部署节点能够ssh免密登陆所有节点，并且设置python软连接&lt;/p></description></item><item><title>2023-09-28 02-安装etcd集群</title><link>https://qq547475331.github.io/docs/02-%E5%AE%89%E8%A3%85etcd%E9%9B%86%E7%BE%A4-02--an-zhuang-etcd-ji-qun/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/02-%E5%AE%89%E8%A3%85etcd%E9%9B%86%E7%BE%A4-02--an-zhuang-etcd-ji-qun/</guid><description>&lt;h2 id="02-安装etcd集群">
 02-安装etcd集群
 &lt;a class="anchor" href="#02-%e5%ae%89%e8%a3%85etcd%e9%9b%86%e7%be%a4">#&lt;/a>
&lt;/h2>
&lt;p>kuberntes 集群使用 etcd 存储所有数据，是最重要的组件之一，注意 etcd集群需要奇数个节点(1,3,5&amp;hellip;)，本文档使用3个节点做集群。&lt;/p>
&lt;p>请在另外窗口打开&lt;a href="../../roles/etcd/tasks/main.yml">roles/etcd/tasks/main.yml&lt;/a> 文件，对照看以下讲解内容。&lt;/p>
&lt;h3 id="创建etcd证书">
 创建etcd证书
 &lt;a class="anchor" href="#%e5%88%9b%e5%bb%baetcd%e8%af%81%e4%b9%a6">#&lt;/a>
&lt;/h3>
&lt;p>注意：证书是在部署节点创建好之后推送到目标etcd节点上去的，以增加ca证书的安全性&lt;/p>
&lt;p>创建ectd证书请求 &lt;a href="../../roles/etcd/templates/etcd-csr.json.j2">etcd-csr.json.j2&lt;/a>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;CN&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;etcd&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;hosts&amp;#34;&lt;/span>: &lt;span style="color:#f92672">[&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">{&lt;/span>% &lt;span style="color:#66d9ef">for&lt;/span> host in groups&lt;span style="color:#f92672">[&lt;/span>&lt;span style="color:#e6db74">&amp;#39;etcd&amp;#39;&lt;/span>&lt;span style="color:#f92672">]&lt;/span> %&lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;{{ host }}&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">{&lt;/span>% endfor %&lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;127.0.0.1&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">]&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;key&amp;#34;&lt;/span>: &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;algo&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;rsa&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;size&amp;#34;&lt;/span>: &lt;span style="color:#ae81ff">2048&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;names&amp;#34;&lt;/span>: &lt;span style="color:#f92672">[&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;C&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;CN&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;ST&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;HangZhou&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;L&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;XS&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;O&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;k8s&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;OU&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;System&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>etcd使用对等证书，hosts 字段必须指定授权使用该证书的 etcd 节点 IP，这里枚举了所有ectd节点的地址&lt;/li>
&lt;/ul>
&lt;h3 id="创建etcd-服务文件-etcdservicej2">
 创建etcd 服务文件 &lt;a href="../../roles/etcd/templates/etcd.service.j2">etcd.service.j2&lt;/a>
 &lt;a class="anchor" href="#%e5%88%9b%e5%bb%baetcd-%e6%9c%8d%e5%8a%a1%e6%96%87%e4%bb%b6-etcdservicej2">#&lt;/a>
&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>Unit&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Description&lt;span style="color:#f92672">=&lt;/span>Etcd Server
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>After&lt;span style="color:#f92672">=&lt;/span>network.target
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>After&lt;span style="color:#f92672">=&lt;/span>network-online.target
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Wants&lt;span style="color:#f92672">=&lt;/span>network-online.target
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Documentation&lt;span style="color:#f92672">=&lt;/span>https://github.com/coreos
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>Service&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Type&lt;span style="color:#f92672">=&lt;/span>notify
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>WorkingDirectory&lt;span style="color:#f92672">={{&lt;/span> ETCD_DATA_DIR &lt;span style="color:#f92672">}}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ExecStart&lt;span style="color:#f92672">={{&lt;/span> bin_dir &lt;span style="color:#f92672">}}&lt;/span>/etcd &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --name&lt;span style="color:#f92672">=&lt;/span>etcd-&lt;span style="color:#f92672">{{&lt;/span> inventory_hostname &lt;span style="color:#f92672">}}&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --cert-file&lt;span style="color:#f92672">={{&lt;/span> ca_dir &lt;span style="color:#f92672">}}&lt;/span>/etcd.pem &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --key-file&lt;span style="color:#f92672">={{&lt;/span> ca_dir &lt;span style="color:#f92672">}}&lt;/span>/etcd-key.pem &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --peer-cert-file&lt;span style="color:#f92672">={{&lt;/span> ca_dir &lt;span style="color:#f92672">}}&lt;/span>/etcd.pem &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --peer-key-file&lt;span style="color:#f92672">={{&lt;/span> ca_dir &lt;span style="color:#f92672">}}&lt;/span>/etcd-key.pem &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --trusted-ca-file&lt;span style="color:#f92672">={{&lt;/span> ca_dir &lt;span style="color:#f92672">}}&lt;/span>/ca.pem &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --peer-trusted-ca-file&lt;span style="color:#f92672">={{&lt;/span> ca_dir &lt;span style="color:#f92672">}}&lt;/span>/ca.pem &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --initial-advertise-peer-urls&lt;span style="color:#f92672">=&lt;/span>https://&lt;span style="color:#f92672">{{&lt;/span> inventory_hostname &lt;span style="color:#f92672">}}&lt;/span>:2380 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --listen-peer-urls&lt;span style="color:#f92672">=&lt;/span>https://&lt;span style="color:#f92672">{{&lt;/span> inventory_hostname &lt;span style="color:#f92672">}}&lt;/span>:2380 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --listen-client-urls&lt;span style="color:#f92672">=&lt;/span>https://&lt;span style="color:#f92672">{{&lt;/span> inventory_hostname &lt;span style="color:#f92672">}}&lt;/span>:2379,http://127.0.0.1:2379 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --advertise-client-urls&lt;span style="color:#f92672">=&lt;/span>https://&lt;span style="color:#f92672">{{&lt;/span> inventory_hostname &lt;span style="color:#f92672">}}&lt;/span>:2379 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --initial-cluster-token&lt;span style="color:#f92672">=&lt;/span>etcd-cluster-0 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --initial-cluster&lt;span style="color:#f92672">={{&lt;/span> ETCD_NODES &lt;span style="color:#f92672">}}&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --initial-cluster-state&lt;span style="color:#f92672">={{&lt;/span> CLUSTER_STATE &lt;span style="color:#f92672">}}&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --data-dir&lt;span style="color:#f92672">={{&lt;/span> ETCD_DATA_DIR &lt;span style="color:#f92672">}}&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --wal-dir&lt;span style="color:#f92672">={{&lt;/span> ETCD_WAL_DIR &lt;span style="color:#f92672">}}&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --snapshot-count&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">50000&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --auto-compaction-retention&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --auto-compaction-mode&lt;span style="color:#f92672">=&lt;/span>periodic &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --max-request-bytes&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">10485760&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --quota-backend-bytes&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">8589934592&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Restart&lt;span style="color:#f92672">=&lt;/span>always
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>RestartSec&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">15&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>LimitNOFILE&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">65536&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>OOMScoreAdjust&lt;span style="color:#f92672">=&lt;/span>-999
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>Install&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>WantedBy&lt;span style="color:#f92672">=&lt;/span>multi-user.target
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>完整参数列表请使用 &lt;code>etcd --help&lt;/code> 查询&lt;/li>
&lt;li>注意etcd 即需要服务器证书也需要客户端证书，为方便使用一个peer 证书代替两个证书&lt;/li>
&lt;li>&lt;code>--initial-cluster-state&lt;/code> 值为 &lt;code>new&lt;/code> 时，&lt;code>--name&lt;/code> 的参数值必须位于 &lt;code>--initial-cluster&lt;/code> 列表中&lt;/li>
&lt;li>&lt;code>--snapshot-count&lt;/code> &lt;code>--auto-compaction-retention&lt;/code> 一些性能优化参数，请查阅etcd项目文档&lt;/li>
&lt;li>设置&lt;code>--data-dir&lt;/code> 和&lt;code>--wal-dir&lt;/code> 使用不同磁盘目录，可以避免磁盘io竞争，提高性能，具体请参考etcd项目文档&lt;/li>
&lt;/ul>
&lt;h3 id="验证etcd集群状态">
 验证etcd集群状态
 &lt;a class="anchor" href="#%e9%aa%8c%e8%af%81etcd%e9%9b%86%e7%be%a4%e7%8a%b6%e6%80%81">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>systemctl status etcd 查看服务状态&lt;/li>
&lt;li>journalctl -u etcd 查看运行日志&lt;/li>
&lt;li>在任一 etcd 集群节点上执行如下命令&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 根据hosts中配置设置shell变量 $NODE_IPS&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>export NODE_IPS&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;192.168.1.1 192.168.1.2 192.168.1.3&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">for&lt;/span> ip in &lt;span style="color:#e6db74">${&lt;/span>NODE_IPS&lt;span style="color:#e6db74">}&lt;/span>; &lt;span style="color:#66d9ef">do&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ETCDCTL_API&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">3&lt;/span> etcdctl &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --endpoints&lt;span style="color:#f92672">=&lt;/span>https://&lt;span style="color:#e6db74">${&lt;/span>ip&lt;span style="color:#e6db74">}&lt;/span>:2379 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --cacert&lt;span style="color:#f92672">=&lt;/span>/etc/kubernetes/ssl/ca.pem &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --cert&lt;span style="color:#f92672">=&lt;/span>/etc/kubernetes/ssl/etcd.pem &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --key&lt;span style="color:#f92672">=&lt;/span>/etc/kubernetes/ssl/etcd-key.pem &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> endpoint health; &lt;span style="color:#66d9ef">done&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">for&lt;/span> ip in &lt;span style="color:#e6db74">${&lt;/span>NODE_IPS&lt;span style="color:#e6db74">}&lt;/span>; &lt;span style="color:#66d9ef">do&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ETCDCTL_API&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">3&lt;/span> etcdctl &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --endpoints&lt;span style="color:#f92672">=&lt;/span>https://&lt;span style="color:#e6db74">${&lt;/span>ip&lt;span style="color:#e6db74">}&lt;/span>:2379 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --cacert&lt;span style="color:#f92672">=&lt;/span>/etc/kubernetes/ssl/ca.pem &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --cert&lt;span style="color:#f92672">=&lt;/span>/etc/kubernetes/ssl/etcd.pem &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --key&lt;span style="color:#f92672">=&lt;/span>/etc/kubernetes/ssl/etcd-key.pem &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --write-out&lt;span style="color:#f92672">=&lt;/span>table endpoint status; &lt;span style="color:#66d9ef">done&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre tabindex="0">&lt;code># 根据hosts中配置设置shell变量 $NODE_IPS
export NODE_IPS=&amp;#34;192.168.0.51 192.168.0.52 192.168.0.53&amp;#34;
for ip in ${NODE_IPS}; do
 ETCDCTL_API=3 etcdctl \
 --endpoints=https://${ip}:2379 \
 --cacert=/etc/kubernetes/ssl/ca.pem \
 --cert=/etc/kubernetes/ssl/etcd.pem \
 --key=/etc/kubernetes/ssl/etcd-key.pem \
 endpoint health; done

for ip in ${NODE_IPS}; do
 ETCDCTL_API=3 etcdctl \
 --endpoints=https://${ip}:2379 \
 --cacert=/etc/kubernetes/ssl/ca.pem \
 --cert=/etc/kubernetes/ssl/etcd.pem \
 --key=/etc/kubernetes/ssl/etcd-key.pem \
 --write-out=table endpoint status; done
&lt;/code>&lt;/pre>&lt;p>预期结果：&lt;/p></description></item><item><title>2023-09-28 06-安装calico网络组件</title><link>https://qq547475331.github.io/docs/06-%E5%AE%89%E8%A3%85calico%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6-06--an-zhuang-calico-wang-luo-zu-jian/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/06-%E5%AE%89%E8%A3%85calico%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6-06--an-zhuang-calico-wang-luo-zu-jian/</guid><description>&lt;h2 id="06-安装calico网络组件md">
 06-安装calico网络组件.md
 &lt;a class="anchor" href="#06-%e5%ae%89%e8%a3%85calico%e7%bd%91%e7%bb%9c%e7%bb%84%e4%bb%b6md">#&lt;/a>
&lt;/h2>
&lt;h5 id="calico-是k8s社区最流行的网络插件之一也是k8s-conformance-test-默认使用的网络插件功能丰富支持network-policy是当前kubeasz项目的默认网络插件">
 calico 是k8s社区最流行的网络插件之一，也是k8s-conformance test 默认使用的网络插件，功能丰富，支持network policy；是当前kubeasz项目的默认网络插件。
 &lt;a class="anchor" href="#calico-%e6%98%afk8s%e7%a4%be%e5%8c%ba%e6%9c%80%e6%b5%81%e8%a1%8c%e7%9a%84%e7%bd%91%e7%bb%9c%e6%8f%92%e4%bb%b6%e4%b9%8b%e4%b8%80%e4%b9%9f%e6%98%afk8s-conformance-test-%e9%bb%98%e8%ae%a4%e4%bd%bf%e7%94%a8%e7%9a%84%e7%bd%91%e7%bb%9c%e6%8f%92%e4%bb%b6%e5%8a%9f%e8%83%bd%e4%b8%b0%e5%af%8c%e6%94%af%e6%8c%81network-policy%e6%98%af%e5%bd%93%e5%89%8dkubeasz%e9%a1%b9%e7%9b%ae%e7%9a%84%e9%bb%98%e8%ae%a4%e7%bd%91%e7%bb%9c%e6%8f%92%e4%bb%b6">#&lt;/a>
&lt;/h5>
&lt;h5 id="如果需要安装calico请在clustersxxxxhosts文件中设置变量-cluster_networkcalico参考这里">
 如果需要安装calico，请在&lt;code>clusters/xxxx/hosts&lt;/code>文件中设置变量 &lt;code>CLUSTER_NETWORK=&amp;quot;calico&amp;quot;&lt;/code>，参考&lt;a href="../config_guide.md">这里&lt;/a>
 &lt;a class="anchor" href="#%e5%a6%82%e6%9e%9c%e9%9c%80%e8%a6%81%e5%ae%89%e8%a3%85calico%e8%af%b7%e5%9c%a8clustersxxxxhosts%e6%96%87%e4%bb%b6%e4%b8%ad%e8%ae%be%e7%bd%ae%e5%8f%98%e9%87%8f-cluster_networkcalico%e5%8f%82%e8%80%83%e8%bf%99%e9%87%8c">#&lt;/a>
&lt;/h5>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>roles/calico/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>├── tasks
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│   └── main.yml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>├── templates
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│   ├── calico-csr.json.j2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│   ├── calicoctl.cfg.j2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│   ├── calico-v3.15.yaml.j2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│   ├── calico-v3.19.yaml.j2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│   └── calico-v3.8.yaml.j2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>└── vars
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> └── main.yml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>请在另外窗口打开&lt;code>roles/calico/tasks/main.yml&lt;/code>文件，对照看以下讲解内容。&lt;/p>
&lt;h3 id="创建calico-证书申请">
 创建calico 证书申请
 &lt;a class="anchor" href="#%e5%88%9b%e5%bb%bacalico-%e8%af%81%e4%b9%a6%e7%94%b3%e8%af%b7">#&lt;/a>
&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;CN&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;calico&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;hosts&amp;#34;&lt;/span>: &lt;span style="color:#f92672">[]&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;key&amp;#34;&lt;/span>: &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;algo&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;rsa&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;size&amp;#34;&lt;/span>: &lt;span style="color:#ae81ff">2048&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;names&amp;#34;&lt;/span>: &lt;span style="color:#f92672">[&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;C&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;CN&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;ST&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;HangZhou&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;L&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;XS&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;O&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;k8s&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;OU&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;System&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h5 id="calico-使用客户端证书所以hosts字段可以为空后续可以看到calico证书用在四个地方">
 calico 使用客户端证书，所以hosts字段可以为空；后续可以看到calico证书用在四个地方：
 &lt;a class="anchor" href="#calico-%e4%bd%bf%e7%94%a8%e5%ae%a2%e6%88%b7%e7%ab%af%e8%af%81%e4%b9%a6%e6%89%80%e4%bb%a5hosts%e5%ad%97%e6%ae%b5%e5%8f%af%e4%bb%a5%e4%b8%ba%e7%a9%ba%e5%90%8e%e7%bb%ad%e5%8f%af%e4%bb%a5%e7%9c%8b%e5%88%b0calico%e8%af%81%e4%b9%a6%e7%94%a8%e5%9c%a8%e5%9b%9b%e4%b8%aa%e5%9c%b0%e6%96%b9">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="caliconode-这个docker-容器运行时访问-etcd-使用证书">
 calico/node 这个docker 容器运行时访问 etcd 使用证书
 &lt;a class="anchor" href="#caliconode-%e8%bf%99%e4%b8%aadocker-%e5%ae%b9%e5%99%a8%e8%bf%90%e8%a1%8c%e6%97%b6%e8%ae%bf%e9%97%ae-etcd-%e4%bd%bf%e7%94%a8%e8%af%81%e4%b9%a6">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="cni-配置文件中cni-插件需要访问-etcd-使用证书">
 cni 配置文件中，cni 插件需要访问 etcd 使用证书
 &lt;a class="anchor" href="#cni-%e9%85%8d%e7%bd%ae%e6%96%87%e4%bb%b6%e4%b8%adcni-%e6%8f%92%e4%bb%b6%e9%9c%80%e8%a6%81%e8%ae%bf%e9%97%ae-etcd-%e4%bd%bf%e7%94%a8%e8%af%81%e4%b9%a6">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="calicoctl-操作集群网络时访问-etcd-使用证书">
 calicoctl 操作集群网络时访问 etcd 使用证书
 &lt;a class="anchor" href="#calicoctl-%e6%93%8d%e4%bd%9c%e9%9b%86%e7%be%a4%e7%bd%91%e7%bb%9c%e6%97%b6%e8%ae%bf%e9%97%ae-etcd-%e4%bd%bf%e7%94%a8%e8%af%81%e4%b9%a6">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="calicokube-controllers-同步集群网络策略时访问-etcd-使用证书">
 calico/kube-controllers 同步集群网络策略时访问 etcd 使用证书
 &lt;a class="anchor" href="#calicokube-controllers-%e5%90%8c%e6%ad%a5%e9%9b%86%e7%be%a4%e7%bd%91%e7%bb%9c%e7%ad%96%e7%95%a5%e6%97%b6%e8%ae%bf%e9%97%ae-etcd-%e4%bd%bf%e7%94%a8%e8%af%81%e4%b9%a6">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h3 id="创建-calico-daemonset-yaml文件和rbac-文件">
 创建 calico DaemonSet yaml文件和rbac 文件
 &lt;a class="anchor" href="#%e5%88%9b%e5%bb%ba-calico-daemonset-yaml%e6%96%87%e4%bb%b6%e5%92%8crbac-%e6%96%87%e4%bb%b6">#&lt;/a>
&lt;/h3>
&lt;pre tabindex="0">&lt;code>请对照 roles/calico/templates/calico.yaml.j2文件注释和以下注意内容

详细配置参数请参考[calico官方文档](https://projectcalico.docs.tigera.io/reference/node/configuration)

配置ETCD_ENDPOINTS 、CA、证书等，所有{{ }}变量与ansible hosts文件中设置对应

配置集群POD网络 CALICO_IPV4POOL_CIDR={{ CLUSTER_CIDR }}

配置FELIX_DEFAULTENDPOINTTOHOSTACTION=ACCEPT 默认允许Pod到Node的网络流量，更多[felix配置选项
&lt;/code>&lt;/pre>&lt;h5 id="heading">
 &lt;a href="https://projectcalico.docs.tigera.io/reference/felix/configuration">https://projectcalico.docs.tigera.io/reference/felix/configuration&lt;/a>)
 &lt;a class="anchor" href="#heading">#&lt;/a>
&lt;/h5>
&lt;h3 id="安装calico-网络">
 安装calico 网络
 &lt;a class="anchor" href="#%e5%ae%89%e8%a3%85calico-%e7%bd%91%e7%bb%9c">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>
&lt;h5 id="安装前检查主机名不能有大写字母只能由小写字母----组成-name-must-consist-of-lower-case-alphanumeric-characters---or--regex-a-z0-9a-z0-9calico-node-v306以上已经解决主机大写字母问题">
 安装前检查主机名不能有大写字母，只能由&lt;code>小写字母&lt;/code> &lt;code>-&lt;/code> &lt;code>.&lt;/code> 组成 (name must consist of lower case alphanumeric characters, &amp;lsquo;-&amp;rsquo; or &amp;lsquo;.&amp;rsquo; (regex: &lt;a href="[-a-z0-9]*[a-z0-9]">a-z0-9&lt;/a>?(.&lt;a href="[-a-z0-9]*[a-z0-9]">a-z0-9&lt;/a>?)*))(calico-node v3.0.6以上已经解决主机大写字母问题)
 &lt;a class="anchor" href="#%e5%ae%89%e8%a3%85%e5%89%8d%e6%a3%80%e6%9f%a5%e4%b8%bb%e6%9c%ba%e5%90%8d%e4%b8%8d%e8%83%bd%e6%9c%89%e5%a4%a7%e5%86%99%e5%ad%97%e6%af%8d%e5%8f%aa%e8%83%bd%e7%94%b1%e5%b0%8f%e5%86%99%e5%ad%97%e6%af%8d----%e7%bb%84%e6%88%90-name-must-consist-of-lower-case-alphanumeric-characters---or--regex-a-z0-9a-z0-9calico-node-v306%e4%bb%a5%e4%b8%8a%e5%b7%b2%e7%bb%8f%e8%a7%a3%e5%86%b3%e4%b8%bb%e6%9c%ba%e5%a4%a7%e5%86%99%e5%ad%97%e6%af%8d%e9%97%ae%e9%a2%98">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="安装前必须确保各节点主机名不重复-calico-node-name-由节点主机名决定如果重复那么重复节点在etcd中只存储一份配置bgp-邻居也不会建立">
 &lt;strong>安装前必须确保各节点主机名不重复&lt;/strong> ，calico node name 由节点主机名决定，如果重复，那么重复节点在etcd中只存储一份配置，BGP 邻居也不会建立。
 &lt;a class="anchor" href="#%e5%ae%89%e8%a3%85%e5%89%8d%e5%bf%85%e9%a1%bb%e7%a1%ae%e4%bf%9d%e5%90%84%e8%8a%82%e7%82%b9%e4%b8%bb%e6%9c%ba%e5%90%8d%e4%b8%8d%e9%87%8d%e5%a4%8d-calico-node-name-%e7%94%b1%e8%8a%82%e7%82%b9%e4%b8%bb%e6%9c%ba%e5%90%8d%e5%86%b3%e5%ae%9a%e5%a6%82%e6%9e%9c%e9%87%8d%e5%a4%8d%e9%82%a3%e4%b9%88%e9%87%8d%e5%a4%8d%e8%8a%82%e7%82%b9%e5%9c%a8etcd%e4%b8%ad%e5%8f%aa%e5%ad%98%e5%82%a8%e4%b8%80%e4%bb%bd%e9%85%8d%e7%bd%aebgp-%e9%82%bb%e5%b1%85%e4%b9%9f%e4%b8%8d%e4%bc%9a%e5%bb%ba%e7%ab%8b">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="安装之前必须确保kube_master和kube_node节点已经成功部署">
 安装之前必须确保&lt;code>kube_master&lt;/code>和&lt;code>kube_node&lt;/code>节点已经成功部署
 &lt;a class="anchor" href="#%e5%ae%89%e8%a3%85%e4%b9%8b%e5%89%8d%e5%bf%85%e9%a1%bb%e7%a1%ae%e4%bf%9dkube_master%e5%92%8ckube_node%e8%8a%82%e7%82%b9%e5%b7%b2%e7%bb%8f%e6%88%90%e5%8a%9f%e9%83%a8%e7%bd%b2">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="轮询等待calico-网络插件安装完成删除之前kube_node安装时默认cni网络配置">
 轮询等待calico 网络插件安装完成，删除之前kube_node安装时默认cni网络配置
 &lt;a class="anchor" href="#%e8%bd%ae%e8%af%a2%e7%ad%89%e5%be%85calico-%e7%bd%91%e7%bb%9c%e6%8f%92%e4%bb%b6%e5%ae%89%e8%a3%85%e5%ae%8c%e6%88%90%e5%88%a0%e9%99%a4%e4%b9%8b%e5%89%8dkube_node%e5%ae%89%e8%a3%85%e6%97%b6%e9%bb%98%e8%ae%a4cni%e7%bd%91%e7%bb%9c%e9%85%8d%e7%bd%ae">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h3 id="可选配置calicoctl工具-calicoctlcfgj2">
 [可选]配置calicoctl工具 &lt;a href="roles/calico/templates/calicoctl.cfg.j2">calicoctl.cfg.j2&lt;/a>
 &lt;a class="anchor" href="#%e5%8f%af%e9%80%89%e9%85%8d%e7%bd%aecalicoctl%e5%b7%a5%e5%85%b7-calicoctlcfgj2">#&lt;/a>
&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>apiVersion: projectcalico.org/v3
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: CalicoAPIConfig
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> datastoreType: &lt;span style="color:#e6db74">&amp;#34;etcdv3&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> etcdEndpoints: &lt;span style="color:#f92672">{{&lt;/span> ETCD_ENDPOINTS &lt;span style="color:#f92672">}}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> etcdKeyFile: /etc/calico/ssl/calico-key.pem
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> etcdCertFile: /etc/calico/ssl/calico.pem
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> etcdCACertFile: &lt;span style="color:#f92672">{{&lt;/span> ca_dir &lt;span style="color:#f92672">}}&lt;/span>/ca.pem
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="验证calico网络">
 验证calico网络
 &lt;a class="anchor" href="#%e9%aa%8c%e8%af%81calico%e7%bd%91%e7%bb%9c">#&lt;/a>
&lt;/h3>
&lt;h5 id="执行calico安装成功后可以验证如下需要等待镜像下载完成有时候即便上一步已经配置了docker国内加速还是可能比较慢请确认以下容器运行起来以后再执行后续验证步骤">
 执行calico安装成功后可以验证如下：(需要等待镜像下载完成，有时候即便上一步已经配置了docker国内加速，还是可能比较慢，请确认以下容器运行起来以后，再执行后续验证步骤)
 &lt;a class="anchor" href="#%e6%89%a7%e8%a1%8ccalico%e5%ae%89%e8%a3%85%e6%88%90%e5%8a%9f%e5%90%8e%e5%8f%af%e4%bb%a5%e9%aa%8c%e8%af%81%e5%a6%82%e4%b8%8b%e9%9c%80%e8%a6%81%e7%ad%89%e5%be%85%e9%95%9c%e5%83%8f%e4%b8%8b%e8%bd%bd%e5%ae%8c%e6%88%90%e6%9c%89%e6%97%b6%e5%80%99%e5%8d%b3%e4%be%bf%e4%b8%8a%e4%b8%80%e6%ad%a5%e5%b7%b2%e7%bb%8f%e9%85%8d%e7%bd%ae%e4%ba%86docker%e5%9b%bd%e5%86%85%e5%8a%a0%e9%80%9f%e8%bf%98%e6%98%af%e5%8f%af%e8%83%bd%e6%af%94%e8%be%83%e6%85%a2%e8%af%b7%e7%a1%ae%e8%ae%a4%e4%bb%a5%e4%b8%8b%e5%ae%b9%e5%99%a8%e8%bf%90%e8%a1%8c%e8%b5%b7%e6%9d%a5%e4%bb%a5%e5%90%8e%e5%86%8d%e6%89%a7%e8%a1%8c%e5%90%8e%e7%bb%ad%e9%aa%8c%e8%af%81%e6%ad%a5%e9%aa%a4">#&lt;/a>
&lt;/h5>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl get pod --all-namespaces
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAMESPACE NAME READY STATUS RESTARTS AGE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kube-system calico-kube-controllers-5c6b98d9df-xj2n4 1/1 Running &lt;span style="color:#ae81ff">0&lt;/span> 1m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kube-system calico-node-4hr52 2/2 Running &lt;span style="color:#ae81ff">0&lt;/span> 1m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kube-system calico-node-8ctc2 2/2 Running &lt;span style="color:#ae81ff">0&lt;/span> 1m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kube-system calico-node-9t8md 2/2 Running &lt;span style="color:#ae81ff">0&lt;/span> 1m
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>查看网卡和路由信息&lt;/strong>&lt;/p></description></item><item><title>2023-09-28 06-安装cilium网络组件</title><link>https://qq547475331.github.io/docs/06-%E5%AE%89%E8%A3%85cilium%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6-06--an-zhuang-cilium-wang-luo-zu-jian/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/06-%E5%AE%89%E8%A3%85cilium%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6-06--an-zhuang-cilium-wang-luo-zu-jian/</guid><description>&lt;h1 id="06-安装cilium网络组件">
 06-安装cilium网络组件
 &lt;a class="anchor" href="#06-%e5%ae%89%e8%a3%85cilium%e7%bd%91%e7%bb%9c%e7%bb%84%e4%bb%b6">#&lt;/a>
&lt;/h1>
&lt;p>&lt;code>cilium&lt;/code> 是一个革新的网络与安全组件；基于 linux 内核新技术&amp;ndash;&lt;code>BPF&lt;/code>，它可以透明、零侵入地实现服务间安全策略与可视化，主要优势如下：&lt;/p>
&lt;ul>
&lt;li>支持L3/L4, L7(如：HTTP/gRPC/Kafka)的安全策略&lt;/li>
&lt;li>支持基于安全ID而不是地址+端口的传统防火墙策略&lt;/li>
&lt;li>支持基于Overlay或Native Routing的扁平多节点pod网络
&lt;ul>
&lt;li>Overlay VXLAN 方式类似于 flannel 的VXLAN后端&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>高性能负载均衡，支持DSR&lt;/li>
&lt;li>支持事件、策略跟踪和监控集成&lt;/li>
&lt;/ul>
&lt;p>cilium 项目当前文档比较完整，建议仔细阅读下&lt;a href="">官网文档&lt;/a>&lt;/p>
&lt;h2 id="kubeasz-集成安装-cilium">
 kubeasz 集成安装 cilium
 &lt;a class="anchor" href="#kubeasz-%e9%9b%86%e6%88%90%e5%ae%89%e8%a3%85-cilium">#&lt;/a>
&lt;/h2>
&lt;p>kubeasz 3.3.1 更新重写了cilium 安装流程，使用helm charts 方式，配置文件在 roles/cilium/templates/values.yaml.j2，请阅读原charts中values.yaml 文件后自定义修改。&lt;/p>
&lt;ul>
&lt;li>相关镜像已经离线打包并推送到本地镜像仓库，通过 &lt;code>ezdown -X&lt;/code> 命令下载cilium等额外镜像&lt;/li>
&lt;/ul>
&lt;h3 id="0升级内核并重启">
 0.升级内核并重启
 &lt;a class="anchor" href="#0%e5%8d%87%e7%ba%a7%e5%86%85%e6%a0%b8%e5%b9%b6%e9%87%8d%e5%90%af">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>Linux kernel &amp;gt;= 4.9.17，请阅读文档&lt;a href="guide/kernel_upgrade.md">升级内核&lt;/a>&lt;/li>
&lt;li>etcd &amp;gt;= 3.1.0 or consul &amp;gt;= 0.6.4&lt;/li>
&lt;/ul>
&lt;h3 id="1选择cilium网络后安装">
 1.选择cilium网络后安装
 &lt;a class="anchor" href="#1%e9%80%89%e6%8b%a9cilium%e7%bd%91%e7%bb%9c%e5%90%8e%e5%ae%89%e8%a3%85">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>参考&lt;a href="quickStart.md">快速指南&lt;/a>，设置&lt;code>/etc/kubeasz/clusters/xxx/hosts&lt;/code>文件中变量 &lt;code>CLUSTER_NETWORK=&amp;quot;cilium&amp;quot;&lt;/code>&lt;/li>
&lt;li>下载额外镜像 &lt;code>./ezdown -X cilium 和 ./ezdown -X network-check&lt;/code>&lt;/li>
&lt;li>执行集群安装 &lt;code>dk ezctl setup xxx all&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>注意默认安装后集成了cilium_connectivity_check 和 cilium_hubble，可以在&lt;code>/etc/kubeasz/clusters/xxx/config.yml&lt;/code>配置关闭&lt;/p></description></item><item><title>2023-09-28 06-安装flannel网络组件</title><link>https://qq547475331.github.io/docs/06-%E5%AE%89%E8%A3%85flannel%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6-06--an-zhuang-flannel-wang-luo-zu-jian/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/06-%E5%AE%89%E8%A3%85flannel%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6-06--an-zhuang-flannel-wang-luo-zu-jian/</guid><description>&lt;h2 id="06-安装flannel网络组件md">
 06-安装flannel网络组件.md
 &lt;a class="anchor" href="#06-%e5%ae%89%e8%a3%85flannel%e7%bd%91%e7%bb%9c%e7%bb%84%e4%bb%b6md">#&lt;/a>
&lt;/h2>
&lt;p>&lt;code>Flannel&lt;/code>是最早应用到k8s集群的网络插件之一，简单高效，且提供多个后端&lt;code>backend&lt;/code>模式供选择；本文介绍以&lt;code>DaemonSet Pod&lt;/code>方式集成到k8s集群，需要在所有master节点和node节点安装。&lt;/p>
&lt;h3 id="kubeasz-集成安装flannel">
 kubeasz 集成安装flannel
 &lt;a class="anchor" href="#kubeasz-%e9%9b%86%e6%88%90%e5%ae%89%e8%a3%85flannel">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>参考&lt;a href="quickStart.md">快速指南&lt;/a>，设置&lt;code>/etc/kubeasz/clusters/xxx/hosts&lt;/code>文件中变量 &lt;code>CLUSTER_NETWORK=&amp;quot;flannel&amp;quot;&lt;/code>&lt;/li>
&lt;li>下载额外镜像 &lt;code>./ezdown -X flannel&lt;/code>&lt;/li>
&lt;li>执行集群安装 &lt;code>dk ezctl setup xxx all&lt;/code>&lt;/li>
&lt;/ul>
&lt;h3 id="配置介绍">
 配置介绍
 &lt;a class="anchor" href="#%e9%85%8d%e7%bd%ae%e4%bb%8b%e7%bb%8d">#&lt;/a>
&lt;/h3>
&lt;p>Flannel CNI 插件的配置文件可以包含多个&lt;code>plugin&lt;/code> 或由其调用其他&lt;code>plugin&lt;/code>；&lt;code>Flannel DaemonSet Pod&lt;/code>运行以后会生成&lt;code>/run/flannel/subnet.env &lt;/code>文件，例如：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>FLANNEL_NETWORK&lt;span style="color:#f92672">=&lt;/span>10.1.0.0/16
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>FLANNEL_SUBNET&lt;span style="color:#f92672">=&lt;/span>10.1.17.1/24
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>FLANNEL_MTU&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1472&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>FLANNEL_IPMASQ&lt;span style="color:#f92672">=&lt;/span>true
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>然后它利用这个文件信息去配置和调用&lt;code>bridge&lt;/code>插件来生成容器网络，调用&lt;code>host-local&lt;/code>来管理&lt;code>IP&lt;/code>地址，例如：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>	&lt;span style="color:#e6db74">&amp;#34;name&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;mynet&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>	&lt;span style="color:#e6db74">&amp;#34;type&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;bridge&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>	&lt;span style="color:#e6db74">&amp;#34;mtu&amp;#34;&lt;/span>: 1472,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>	&lt;span style="color:#e6db74">&amp;#34;ipMasq&amp;#34;&lt;/span>: false,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>	&lt;span style="color:#e6db74">&amp;#34;isGateway&amp;#34;&lt;/span>: true,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>	&lt;span style="color:#e6db74">&amp;#34;ipam&amp;#34;&lt;/span>: &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>		&lt;span style="color:#e6db74">&amp;#34;type&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;host-local&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>		&lt;span style="color:#e6db74">&amp;#34;subnet&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;10.1.17.0/24&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>	&lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>
&lt;p>更多相关介绍请阅读：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/coreos/flannel/blob/master/Documentation/kubernetes.md">flannel kubernetes 集成&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/containernetworking/plugins/tree/master/plugins/meta/flannel">flannel cni 插件&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/containernetworking/plugins">更多 cni 插件&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;code>Flannel DaemonSet&lt;/code> yaml配置文件&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>请阅读 &lt;code>roles/flannel/templates/kube-flannel.yaml.j2&lt;/code> 内容，注意：&lt;/p>
&lt;ul>
&lt;li>注意：本安装方式，flannel 通过 apiserver 接口读取 podCidr 信息，详见 &lt;a href="https://github.com/coreos/flannel/issues/847">https://github.com/coreos/flannel/issues/847&lt;/a>；因此想要修改节点pod网段掩码，请在&lt;code>clusters/xxxx/config.yml&lt;/code> 中修改&lt;code>NODE_CIDR_LEN&lt;/code>配置项&lt;/li>
&lt;li>配置相关RBAC 权限和 &lt;code>service account&lt;/code>&lt;/li>
&lt;li>配置&lt;code>ConfigMap&lt;/code>包含 CNI配置和 flannel配置(指定backend等)，在文件中相关设置对应&lt;/li>
&lt;/ul>
&lt;h3 id="验证flannel网络">
 验证flannel网络
 &lt;a class="anchor" href="#%e9%aa%8c%e8%af%81flannel%e7%bd%91%e7%bb%9c">#&lt;/a>
&lt;/h3>
&lt;p>执行flannel安装成功后可以验证如下：(需要等待镜像下载完成，有时候即便上一步已经配置了docker国内加速，还是可能比较慢，请确认以下容器运行起来以后，再执行后续验证步骤)&lt;/p></description></item><item><title>2023-09-28 06-安装kube-ovn网络组件</title><link>https://qq547475331.github.io/docs/06-%E5%AE%89%E8%A3%85kube-ovn%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6-06--an-zhuang-kube-ovn-wang-luo-zu-jian/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/06-%E5%AE%89%E8%A3%85kube-ovn%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6-06--an-zhuang-kube-ovn-wang-luo-zu-jian/</guid><description>&lt;h2 id="06-安装kube-ovn网络组件md">
 06-安装kube-ovn网络组件.md
 &lt;a class="anchor" href="#06-%e5%ae%89%e8%a3%85kube-ovn%e7%bd%91%e7%bb%9c%e7%bb%84%e4%bb%b6md">#&lt;/a>
&lt;/h2>
&lt;p>(以下文档暂未更新，以插件官网文档为准)&lt;/p>
&lt;p>由灵雀云开源的网络组件 kube-ovn，将已被 openstack 社区采用的成熟网络虚拟化技术 ovs/ovn 引入 kubernetes 平台；为 kubernetes 网络打开了新的大门，令人耳目一新；强烈推荐大家试用该网络组件，反馈建议以帮助项目早日走向成熟。&lt;/p>
&lt;ul>
&lt;li>介绍 &lt;a href="https://blog.csdn.net/alauda_andy/article/details/88886128">https://blog.csdn.net/alauda_andy/article/details/88886128&lt;/a>&lt;/li>
&lt;li>项目地址 &lt;a href="https://github.com/alauda/kube-ovn">https://github.com/alauda/kube-ovn&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="特性介绍">
 特性介绍
 &lt;a class="anchor" href="#%e7%89%b9%e6%80%a7%e4%bb%8b%e7%bb%8d">#&lt;/a>
&lt;/h3>
&lt;p>kube-ovn 提供了针对企业应用场景下容器网络实用功能，并为实现更高级的网络管理控制提供了可能性；现有主要功能:&lt;/p>
&lt;ul>
&lt;li>1.Namespace 和子网的绑定，以及子网间的访问控制;&lt;/li>
&lt;li>2.静态IP分配;&lt;/li>
&lt;li>3.动态QoS;&lt;/li>
&lt;li>4.分布式和集中式网关;&lt;/li>
&lt;li>5.内嵌 LoadBalancer;&lt;/li>
&lt;li>6.Pod IP对外直接暴露&lt;/li>
&lt;li>7.流量镜像&lt;/li>
&lt;li>8.IPv6&lt;/li>
&lt;/ul>
&lt;h3 id="kubeasz-集成安装-kube-ovn">
 kubeasz 集成安装 kube-ovn
 &lt;a class="anchor" href="#kubeasz-%e9%9b%86%e6%88%90%e5%ae%89%e8%a3%85-kube-ovn">#&lt;/a>
&lt;/h3>
&lt;p>kube-ovn 的安装十分简单，详见项目的安装文档；基于 kubeasz，以下两步将安装一个集成了 kube-ovn 网络的 k8s 集群；&lt;/p>
&lt;ul>
&lt;li>在 ansible hosts 中设置变量 &lt;code>CLUSTER_NETWORK=&amp;quot;kube-ovn&amp;quot;&lt;/code>&lt;/li>
&lt;li>执行安装 &lt;code>ansible-playbook 90.setup.yml&lt;/code> 或者 &lt;code>ezctl setup&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>kubeasz 项目为&lt;code>kube-ovn&lt;/code>网络生成的 ansible role 如下：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>roles/kube-ovn
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>├── defaults
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│   └── main.yml		&lt;span style="color:#75715e"># kube-ovn 相关配置文件&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>├── tasks
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│   └── main.yml		&lt;span style="color:#75715e"># 安装执行文件&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>└── templates
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ├── crd.yaml.j2	 &lt;span style="color:#75715e"># crd 模板&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ├── kube-ovn.yaml.j2	&lt;span style="color:#75715e"># kube-ovn yaml 模板&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> └── ovn.yaml.j2		 &lt;span style="color:#75715e"># ovn yaml 模板&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>安装成功后，可以验证所有 k8s 集群功能正常，查看集群的 pod 网络如下：&lt;/p></description></item><item><title>2023-09-28 06-安装网络组件</title><link>https://qq547475331.github.io/docs/06-%E5%AE%89%E8%A3%85%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6-06--an-zhuang-wang-luo-zu-jian/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/06-%E5%AE%89%E8%A3%85%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6-06--an-zhuang-wang-luo-zu-jian/</guid><description>&lt;h2 id="06-安装网络组件">
 06-安装网络组件
 &lt;a class="anchor" href="#06-%e5%ae%89%e8%a3%85%e7%bd%91%e7%bb%9c%e7%bb%84%e4%bb%b6">#&lt;/a>
&lt;/h2>
&lt;h5 id="首先回顾下k8s网络设计原则在配置集群网络插件或者实践k8s-应用服务部署请牢记这些原则">
 首先回顾下K8S网络设计原则，在配置集群网络插件或者实践K8S 应用/服务部署请牢记这些原则：
 &lt;a class="anchor" href="#%e9%a6%96%e5%85%88%e5%9b%9e%e9%a1%be%e4%b8%8bk8s%e7%bd%91%e7%bb%9c%e8%ae%be%e8%ae%a1%e5%8e%9f%e5%88%99%e5%9c%a8%e9%85%8d%e7%bd%ae%e9%9b%86%e7%be%a4%e7%bd%91%e7%bb%9c%e6%8f%92%e4%bb%b6%e6%88%96%e8%80%85%e5%ae%9e%e8%b7%b5k8s-%e5%ba%94%e7%94%a8%e6%9c%8d%e5%8a%a1%e9%83%a8%e7%bd%b2%e8%af%b7%e7%89%a2%e8%ae%b0%e8%bf%99%e4%ba%9b%e5%8e%9f%e5%88%99">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="1每个pod都拥有一个独立ip地址pod内所有容器共享一个网络命名空间">
 1.每个Pod都拥有一个独立IP地址，Pod内所有容器共享一个网络命名空间
 &lt;a class="anchor" href="#1%e6%af%8f%e4%b8%aapod%e9%83%bd%e6%8b%a5%e6%9c%89%e4%b8%80%e4%b8%aa%e7%8b%ac%e7%ab%8bip%e5%9c%b0%e5%9d%80pod%e5%86%85%e6%89%80%e6%9c%89%e5%ae%b9%e5%99%a8%e5%85%b1%e4%ba%ab%e4%b8%80%e4%b8%aa%e7%bd%91%e7%bb%9c%e5%91%bd%e5%90%8d%e7%a9%ba%e9%97%b4">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="2集群内所有pod都在一个直接连通的扁平网络中可通过ip直接访问">
 2.集群内所有Pod都在一个直接连通的扁平网络中，可通过IP直接访问
 &lt;a class="anchor" href="#2%e9%9b%86%e7%be%a4%e5%86%85%e6%89%80%e6%9c%89pod%e9%83%bd%e5%9c%a8%e4%b8%80%e4%b8%aa%e7%9b%b4%e6%8e%a5%e8%bf%9e%e9%80%9a%e7%9a%84%e6%89%81%e5%b9%b3%e7%bd%91%e7%bb%9c%e4%b8%ad%e5%8f%af%e9%80%9a%e8%bf%87ip%e7%9b%b4%e6%8e%a5%e8%ae%bf%e9%97%ae">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="所有容器之间无需nat就可以直接互相访问">
 所有容器之间无需NAT就可以直接互相访问
 &lt;a class="anchor" href="#%e6%89%80%e6%9c%89%e5%ae%b9%e5%99%a8%e4%b9%8b%e9%97%b4%e6%97%a0%e9%9c%80nat%e5%b0%b1%e5%8f%af%e4%bb%a5%e7%9b%b4%e6%8e%a5%e4%ba%92%e7%9b%b8%e8%ae%bf%e9%97%ae">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="所有node和所有容器之间无需nat就可以直接互相访问">
 所有Node和所有容器之间无需NAT就可以直接互相访问
 &lt;a class="anchor" href="#%e6%89%80%e6%9c%89node%e5%92%8c%e6%89%80%e6%9c%89%e5%ae%b9%e5%99%a8%e4%b9%8b%e9%97%b4%e6%97%a0%e9%9c%80nat%e5%b0%b1%e5%8f%af%e4%bb%a5%e7%9b%b4%e6%8e%a5%e4%ba%92%e7%9b%b8%e8%ae%bf%e9%97%ae">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="容器自己看到的ip跟其他容器看到的一样">
 容器自己看到的IP跟其他容器看到的一样
 &lt;a class="anchor" href="#%e5%ae%b9%e5%99%a8%e8%87%aa%e5%b7%b1%e7%9c%8b%e5%88%b0%e7%9a%84ip%e8%b7%9f%e5%85%b6%e4%bb%96%e5%ae%b9%e5%99%a8%e7%9c%8b%e5%88%b0%e7%9a%84%e4%b8%80%e6%a0%b7">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;h5 id="3service-cluster-ip只可在集群内部访问外部请求需要通过nodeportloadbalance或者ingress来访问">
 3.Service cluster IP只可在集群内部访问，外部请求需要通过NodePort、LoadBalance或者Ingress来访问
 &lt;a class="anchor" href="#3service-cluster-ip%e5%8f%aa%e5%8f%af%e5%9c%a8%e9%9b%86%e7%be%a4%e5%86%85%e9%83%a8%e8%ae%bf%e9%97%ae%e5%a4%96%e9%83%a8%e8%af%b7%e6%b1%82%e9%9c%80%e8%a6%81%e9%80%9a%e8%bf%87nodeportloadbalance%e6%88%96%e8%80%85ingress%e6%9d%a5%e8%ae%bf%e9%97%ae">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h5 id="container-network-interface-cni是目前cncf主推的网络模型它由两部分组成">
 &lt;code>Container Network Interface (CNI)&lt;/code>是目前CNCF主推的网络模型，它由两部分组成：
 &lt;a class="anchor" href="#container-network-interface-cni%e6%98%af%e7%9b%ae%e5%89%8dcncf%e4%b8%bb%e6%8e%a8%e7%9a%84%e7%bd%91%e7%bb%9c%e6%a8%a1%e5%9e%8b%e5%ae%83%e7%94%b1%e4%b8%a4%e9%83%a8%e5%88%86%e7%bb%84%e6%88%90">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="cni-plugin负责给容器配置网络它包括两个基本的接口">
 CNI Plugin负责给容器配置网络，它包括两个基本的接口
 &lt;a class="anchor" href="#cni-plugin%e8%b4%9f%e8%b4%a3%e7%bb%99%e5%ae%b9%e5%99%a8%e9%85%8d%e7%bd%ae%e7%bd%91%e7%bb%9c%e5%ae%83%e5%8c%85%e6%8b%ac%e4%b8%a4%e4%b8%aa%e5%9f%ba%e6%9c%ac%e7%9a%84%e6%8e%a5%e5%8f%a3">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="配置网络-addnetworknet-networkconfig-rt-runtimeconf-typesresult-error">
 配置网络: AddNetwork(net *NetworkConfig, rt *RuntimeConf) (types.Result, error)
 &lt;a class="anchor" href="#%e9%85%8d%e7%bd%ae%e7%bd%91%e7%bb%9c-addnetworknet-networkconfig-rt-runtimeconf-typesresult-error">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="清理网络-delnetworknet-networkconfig-rt-runtimeconf-error">
 清理网络: DelNetwork(net *NetworkConfig, rt *RuntimeConf) error
 &lt;a class="anchor" href="#%e6%b8%85%e7%90%86%e7%bd%91%e7%bb%9c-delnetworknet-networkconfig-rt-runtimeconf-error">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;h5 id="ipam-plugin负责给容器分配ip地址">
 IPAM Plugin负责给容器分配IP地址
 &lt;a class="anchor" href="#ipam-plugin%e8%b4%9f%e8%b4%a3%e7%bb%99%e5%ae%b9%e5%99%a8%e5%88%86%e9%85%8dip%e5%9c%b0%e5%9d%80">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h5 id="kubernetes-pod的网络是这样创建的">
 Kubernetes Pod的网络是这样创建的：
 &lt;a class="anchor" href="#kubernetes-pod%e7%9a%84%e7%bd%91%e7%bb%9c%e6%98%af%e8%bf%99%e6%a0%b7%e5%88%9b%e5%bb%ba%e7%9a%84">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;ol start="0">
&lt;li>
&lt;h5 id="每个pod除了创建时指定的容器外都有一个kubelet启动时指定的基础容器即pause容器">
 每个Pod除了创建时指定的容器外，都有一个kubelet启动时指定的&lt;code>基础容器&lt;/code>，即&lt;code>pause&lt;/code>容器
 &lt;a class="anchor" href="#%e6%af%8f%e4%b8%aapod%e9%99%a4%e4%ba%86%e5%88%9b%e5%bb%ba%e6%97%b6%e6%8c%87%e5%ae%9a%e7%9a%84%e5%ae%b9%e5%99%a8%e5%a4%96%e9%83%bd%e6%9c%89%e4%b8%80%e4%b8%aakubelet%e5%90%af%e5%8a%a8%e6%97%b6%e6%8c%87%e5%ae%9a%e7%9a%84%e5%9f%ba%e7%a1%80%e5%ae%b9%e5%99%a8%e5%8d%b3pause%e5%ae%b9%e5%99%a8">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;ol>
&lt;li>
&lt;h5 id="kubelet创建基础容器生成network-namespace">
 kubelet创建&lt;code>基础容器&lt;/code>生成network namespace
 &lt;a class="anchor" href="#kubelet%e5%88%9b%e5%bb%ba%e5%9f%ba%e7%a1%80%e5%ae%b9%e5%99%a8%e7%94%9f%e6%88%90network-namespace">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;ol start="2">
&lt;li>
&lt;h5 id="kubelet调用网络cni-driver由它根据配置调用具体的cni-插件">
 kubelet调用网络CNI driver，由它根据配置调用具体的CNI 插件
 &lt;a class="anchor" href="#kubelet%e8%b0%83%e7%94%a8%e7%bd%91%e7%bb%9ccni-driver%e7%94%b1%e5%ae%83%e6%a0%b9%e6%8d%ae%e9%85%8d%e7%bd%ae%e8%b0%83%e7%94%a8%e5%85%b7%e4%bd%93%e7%9a%84cni-%e6%8f%92%e4%bb%b6">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;ol start="3">
&lt;li>
&lt;h5 id="cni-插件给基础容器配置网络">
 CNI 插件给&lt;code>基础容器&lt;/code>配置网络
 &lt;a class="anchor" href="#cni-%e6%8f%92%e4%bb%b6%e7%bb%99%e5%9f%ba%e7%a1%80%e5%ae%b9%e5%99%a8%e9%85%8d%e7%bd%ae%e7%bd%91%e7%bb%9c">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;ol start="4">
&lt;li>
&lt;h5 id="pod-中其他的容器共享使用基础容器的网络">
 Pod 中其他的容器共享使用&lt;code>基础容器&lt;/code>的网络
 &lt;a class="anchor" href="#pod-%e4%b8%ad%e5%85%b6%e4%bb%96%e7%9a%84%e5%ae%b9%e5%99%a8%e5%85%b1%e4%ba%ab%e4%bd%bf%e7%94%a8%e5%9f%ba%e7%a1%80%e5%ae%b9%e5%99%a8%e7%9a%84%e7%bd%91%e7%bb%9c">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;h5 id="本项目基于cni-driver-调用各种网络插件来配置kubernetes的网络常用cni插件有-flannel-calico-cilium等等这些插件各有优势也在互相借鉴学习优点比如在所有node节点都在一个二层网络时候flannel提供hostgw实现避免vxlan实现的udp封装开销估计是目前最高效的calico也针对l3-fabric推出了ipinip的选项利用了gre隧道封装因此这些插件都能适合很多实际应用场景">
 本项目基于CNI driver 调用各种网络插件来配置kubernetes的网络，常用CNI插件有 &lt;code>flannel&lt;/code> &lt;code>calico&lt;/code> &lt;code>cilium&lt;/code>等等，这些插件各有优势，也在互相借鉴学习优点，比如：在所有node节点都在一个二层网络时候，flannel提供hostgw实现，避免vxlan实现的udp封装开销，估计是目前最高效的；calico也针对L3 Fabric，推出了IPinIP的选项，利用了GRE隧道封装；因此这些插件都能适合很多实际应用场景。
 &lt;a class="anchor" href="#%e6%9c%ac%e9%a1%b9%e7%9b%ae%e5%9f%ba%e4%ba%8ecni-driver-%e8%b0%83%e7%94%a8%e5%90%84%e7%a7%8d%e7%bd%91%e7%bb%9c%e6%8f%92%e4%bb%b6%e6%9d%a5%e9%85%8d%e7%bd%aekubernetes%e7%9a%84%e7%bd%91%e7%bb%9c%e5%b8%b8%e7%94%a8cni%e6%8f%92%e4%bb%b6%e6%9c%89-flannel-calico-cilium%e7%ad%89%e7%ad%89%e8%bf%99%e4%ba%9b%e6%8f%92%e4%bb%b6%e5%90%84%e6%9c%89%e4%bc%98%e5%8a%bf%e4%b9%9f%e5%9c%a8%e4%ba%92%e7%9b%b8%e5%80%9f%e9%89%b4%e5%ad%a6%e4%b9%a0%e4%bc%98%e7%82%b9%e6%af%94%e5%a6%82%e5%9c%a8%e6%89%80%e6%9c%89node%e8%8a%82%e7%82%b9%e9%83%bd%e5%9c%a8%e4%b8%80%e4%b8%aa%e4%ba%8c%e5%b1%82%e7%bd%91%e7%bb%9c%e6%97%b6%e5%80%99flannel%e6%8f%90%e4%be%9bhostgw%e5%ae%9e%e7%8e%b0%e9%81%bf%e5%85%8dvxlan%e5%ae%9e%e7%8e%b0%e7%9a%84udp%e5%b0%81%e8%a3%85%e5%bc%80%e9%94%80%e4%bc%b0%e8%ae%a1%e6%98%af%e7%9b%ae%e5%89%8d%e6%9c%80%e9%ab%98%e6%95%88%e7%9a%84calico%e4%b9%9f%e9%92%88%e5%af%b9l3-fabric%e6%8e%a8%e5%87%ba%e4%ba%86ipinip%e7%9a%84%e9%80%89%e9%a1%b9%e5%88%a9%e7%94%a8%e4%ba%86gre%e9%9a%a7%e9%81%93%e5%b0%81%e8%a3%85%e5%9b%a0%e6%ad%a4%e8%bf%99%e4%ba%9b%e6%8f%92%e4%bb%b6%e9%83%bd%e8%83%bd%e9%80%82%e5%90%88%e5%be%88%e5%a4%9a%e5%ae%9e%e9%99%85%e5%ba%94%e7%94%a8%e5%9c%ba%e6%99%af">#&lt;/a>
&lt;/h5>
&lt;h5 id="项目当前内置支持的网络插件有calico-cilium-flannel-kube-ovn-kube-router">
 项目当前内置支持的网络插件有：&lt;code>calico&lt;/code> &lt;code>cilium&lt;/code> &lt;code>flannel&lt;/code> &lt;code>kube-ovn&lt;/code> &lt;code>kube-router&lt;/code>
 &lt;a class="anchor" href="#%e9%a1%b9%e7%9b%ae%e5%bd%93%e5%89%8d%e5%86%85%e7%bd%ae%e6%94%af%e6%8c%81%e7%9a%84%e7%bd%91%e7%bb%9c%e6%8f%92%e4%bb%b6%e6%9c%89calico-cilium-flannel-kube-ovn-kube-router">#&lt;/a>
&lt;/h5>
&lt;h3 id="安装讲解">
 安装讲解
 &lt;a class="anchor" href="#%e5%ae%89%e8%a3%85%e8%ae%b2%e8%a7%a3">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="network-plugin/calico.md">安装calico&lt;/a>&lt;/li>
&lt;li>&lt;a href="network-plugin/cilium.md">安装cilium&lt;/a>&lt;/li>
&lt;li>&lt;a href="network-plugin/flannel.md">安装flannel&lt;/a>&lt;/li>
&lt;li>&lt;a href="network-plugin/kube-ovn.md">安装kube-ovn&lt;/a> 暂未更新&lt;/li>
&lt;li>&lt;a href="network-plugin/kube-router.md">安装kube-router&lt;/a> 暂未更新&lt;/li>
&lt;/ul>
&lt;h3 id="参考">
 参考
 &lt;a class="anchor" href="#%e5%8f%82%e8%80%83">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/">kubernetes.io networking docs&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/feiskyer/kubernetes-handbook/blob/master/zh/network/network.md">feiskyer-kubernetes指南网络章节&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>&lt;a href="07-install_cluster_addon.md">后一篇&lt;/a>&lt;/p></description></item><item><title>2023-09-28 08-K8S 集群存储</title><link>https://qq547475331.github.io/docs/08-k8s-%E9%9B%86%E7%BE%A4%E5%AD%98%E5%82%A8--k8s-ji-qun-cun-chu/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/08-k8s-%E9%9B%86%E7%BE%A4%E5%AD%98%E5%82%A8--k8s-ji-qun-cun-chu/</guid><description>&lt;h1 id="k8s-集群存储">
 K8S 集群存储
 &lt;a class="anchor" href="#k8s-%e9%9b%86%e7%be%a4%e5%ad%98%e5%82%a8">#&lt;/a>
&lt;/h1>
&lt;h2 id="前言">
 前言
 &lt;a class="anchor" href="#%e5%89%8d%e8%a8%80">#&lt;/a>
&lt;/h2>
&lt;p>在kubernetes(k8s)中对于存储的资源抽象了两个概念，分别是PersistentVolume(PV)、PersistentVolumeClaim(PVC)。&lt;/p>
&lt;ul>
&lt;li>
&lt;h5 id="pv是集群中的资源">
 PV是集群中的资源
 &lt;a class="anchor" href="#pv%e6%98%af%e9%9b%86%e7%be%a4%e4%b8%ad%e7%9a%84%e8%b5%84%e6%ba%90">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="pvc是对这些资源的请求">
 PVC是对这些资源的请求。
 &lt;a class="anchor" href="#pvc%e6%98%af%e5%af%b9%e8%bf%99%e4%ba%9b%e8%b5%84%e6%ba%90%e7%9a%84%e8%af%b7%e6%b1%82">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h5 id="如上面所说pv和pvc都只是抽象的概念在k8s中是通过插件的方式提供具体的存储实现目前包含有nfsiscsi和云提供商指定的存储系统更多的存储实现参考官方文档">
 如上面所说PV和PVC都只是抽象的概念，在k8s中是通过插件的方式提供具体的存储实现。目前包含有NFS、iSCSI和云提供商指定的存储系统，更多的存储实现&lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes">参考官方文档&lt;/a>。
 &lt;a class="anchor" href="#%e5%a6%82%e4%b8%8a%e9%9d%a2%e6%89%80%e8%af%b4pv%e5%92%8cpvc%e9%83%bd%e5%8f%aa%e6%98%af%e6%8a%bd%e8%b1%a1%e7%9a%84%e6%a6%82%e5%bf%b5%e5%9c%a8k8s%e4%b8%ad%e6%98%af%e9%80%9a%e8%bf%87%e6%8f%92%e4%bb%b6%e7%9a%84%e6%96%b9%e5%bc%8f%e6%8f%90%e4%be%9b%e5%85%b7%e4%bd%93%e7%9a%84%e5%ad%98%e5%82%a8%e5%ae%9e%e7%8e%b0%e7%9b%ae%e5%89%8d%e5%8c%85%e5%90%ab%e6%9c%89nfsiscsi%e5%92%8c%e4%ba%91%e6%8f%90%e4%be%9b%e5%95%86%e6%8c%87%e5%ae%9a%e7%9a%84%e5%ad%98%e5%82%a8%e7%b3%bb%e7%bb%9f%e6%9b%b4%e5%a4%9a%e7%9a%84%e5%ad%98%e5%82%a8%e5%ae%9e%e7%8e%b0%e5%8f%82%e8%80%83%e5%ae%98%e6%96%b9%e6%96%87%e6%a1%a3">#&lt;/a>
&lt;/h5>
&lt;h5 id="以下介绍两种provisioner-可以提供静态或者动态的pv">
 以下介绍两种&lt;code>provisioner&lt;/code>, 可以提供静态或者动态的PV
 &lt;a class="anchor" href="#%e4%bb%a5%e4%b8%8b%e4%bb%8b%e7%bb%8d%e4%b8%a4%e7%a7%8dprovisioner-%e5%8f%af%e4%bb%a5%e6%8f%90%e4%be%9b%e9%9d%99%e6%80%81%e6%88%96%e8%80%85%e5%8a%a8%e6%80%81%e7%9a%84pv">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="nfs-provisioner-nfs存储目录供应者">
 &lt;a href="https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner">nfs-provisioner&lt;/a>: NFS存储目录供应者
 &lt;a class="anchor" href="#nfs-provisioner-nfs%e5%ad%98%e5%82%a8%e7%9b%ae%e5%bd%95%e4%be%9b%e5%ba%94%e8%80%85">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="local-path-provisioner-本地存储目录供应者">
 &lt;a href="https://github.com/rancher/local-path-provisioner">local-path-provisioner&lt;/a>: 本地存储目录供应者
 &lt;a class="anchor" href="#local-path-provisioner-%e6%9c%ac%e5%9c%b0%e5%ad%98%e5%82%a8%e7%9b%ae%e5%bd%95%e4%be%9b%e5%ba%94%e8%80%85">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h2 id="nfs存储目录供应者">
 NFS存储目录供应者
 &lt;a class="anchor" href="#nfs%e5%ad%98%e5%82%a8%e7%9b%ae%e5%bd%95%e4%be%9b%e5%ba%94%e8%80%85">#&lt;/a>
&lt;/h2>
&lt;h5 id="首先我们需要一个nfs服务器用于提供底层存储通过文档nfs-server我们可以创建一个nfs服务器">
 首先我们需要一个NFS服务器，用于提供底层存储。通过文档&lt;a href="../guide/nfs-server.md">nfs-server&lt;/a>，我们可以创建一个NFS服务器。
 &lt;a class="anchor" href="#%e9%a6%96%e5%85%88%e6%88%91%e4%bb%ac%e9%9c%80%e8%a6%81%e4%b8%80%e4%b8%aanfs%e6%9c%8d%e5%8a%a1%e5%99%a8%e7%94%a8%e4%ba%8e%e6%8f%90%e4%be%9b%e5%ba%95%e5%b1%82%e5%ad%98%e5%82%a8%e9%80%9a%e8%bf%87%e6%96%87%e6%a1%a3nfs-server%e6%88%91%e4%bb%ac%e5%8f%af%e4%bb%a5%e5%88%9b%e5%bb%ba%e4%b8%80%e4%b8%aanfs%e6%9c%8d%e5%8a%a1%e5%99%a8">#&lt;/a>
&lt;/h5>
&lt;h3 id="静态-pv">
 静态 PV
 &lt;a class="anchor" href="#%e9%9d%99%e6%80%81-pv">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>
&lt;h5 id="创建静态-pv指定容量访问模式回收策略存储类等">
 创建静态 pv，指定容量，访问模式，回收策略，存储类等
 &lt;a class="anchor" href="#%e5%88%9b%e5%bb%ba%e9%9d%99%e6%80%81-pv%e6%8c%87%e5%ae%9a%e5%ae%b9%e9%87%8f%e8%ae%bf%e9%97%ae%e6%a8%a1%e5%bc%8f%e5%9b%9e%e6%94%b6%e7%ad%96%e7%95%a5%e5%ad%98%e5%82%a8%e7%b1%bb%e7%ad%89">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: PersistentVolume
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: pv-es-0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> capacity:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> storage: 4Gi
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> accessModes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - ReadWriteMany
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> volumeMode: Filesystem
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> persistentVolumeReclaimPolicy: Recycle
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> storageClassName: &lt;span style="color:#e6db74">&amp;#34;es-storage-class&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> nfs:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 根据实际共享目录修改&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> path: /share/es0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 根据实际 nfs服务器地址修改&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> server: 192.168.1.208
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>
&lt;h5 id="创建-pvc即可绑定使用上述-pv了具体请看后文-test-pod例子">
 创建 pvc即可绑定使用上述 pv了，具体请看后文 test pod例子
 &lt;a class="anchor" href="#%e5%88%9b%e5%bb%ba-pvc%e5%8d%b3%e5%8f%af%e7%bb%91%e5%ae%9a%e4%bd%bf%e7%94%a8%e4%b8%8a%e8%bf%b0-pv%e4%ba%86%e5%85%b7%e4%bd%93%e8%af%b7%e7%9c%8b%e5%90%8e%e6%96%87-test-pod%e4%be%8b%e5%ad%90">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h3 id="创建动态pv">
 创建动态PV
 &lt;a class="anchor" href="#%e5%88%9b%e5%bb%ba%e5%8a%a8%e6%80%81pv">#&lt;/a>
&lt;/h3>
&lt;h5 id="在一个工作k8s-集群中pvc请求会很多如果每次都需要管理员手动去创建对应的-pv资源那就很不方便因此-k8s还提供了多种-provisioner来动态创建-pv不仅节省了管理员的时间还可以根据storageclasses封装不同类型的存储供-pvc-选用">
 在一个工作k8s 集群中，&lt;code>PVC&lt;/code>请求会很多，如果每次都需要管理员手动去创建对应的 &lt;code>PV&lt;/code>资源，那就很不方便；因此 K8S还提供了多种 &lt;code>provisioner&lt;/code>来动态创建 &lt;code>PV&lt;/code>，不仅节省了管理员的时间，还可以根据&lt;code>StorageClasses&lt;/code>封装不同类型的存储供 PVC 选用。
 &lt;a class="anchor" href="#%e5%9c%a8%e4%b8%80%e4%b8%aa%e5%b7%a5%e4%bd%9ck8s-%e9%9b%86%e7%be%a4%e4%b8%adpvc%e8%af%b7%e6%b1%82%e4%bc%9a%e5%be%88%e5%a4%9a%e5%a6%82%e6%9e%9c%e6%af%8f%e6%ac%a1%e9%83%bd%e9%9c%80%e8%a6%81%e7%ae%a1%e7%90%86%e5%91%98%e6%89%8b%e5%8a%a8%e5%8e%bb%e5%88%9b%e5%bb%ba%e5%af%b9%e5%ba%94%e7%9a%84-pv%e8%b5%84%e6%ba%90%e9%82%a3%e5%b0%b1%e5%be%88%e4%b8%8d%e6%96%b9%e4%be%bf%e5%9b%a0%e6%ad%a4-k8s%e8%bf%98%e6%8f%90%e4%be%9b%e4%ba%86%e5%a4%9a%e7%a7%8d-provisioner%e6%9d%a5%e5%8a%a8%e6%80%81%e5%88%9b%e5%bb%ba-pv%e4%b8%8d%e4%bb%85%e8%8a%82%e7%9c%81%e4%ba%86%e7%ae%a1%e7%90%86%e5%91%98%e7%9a%84%e6%97%b6%e9%97%b4%e8%bf%98%e5%8f%af%e4%bb%a5%e6%a0%b9%e6%8d%aestorageclasses%e5%b0%81%e8%a3%85%e4%b8%8d%e5%90%8c%e7%b1%bb%e5%9e%8b%e7%9a%84%e5%ad%98%e5%82%a8%e4%be%9b-pvc-%e9%80%89%e7%94%a8">#&lt;/a>
&lt;/h5>
&lt;h5 id="项目中以nfs-client-provisioner为例">
 项目中以nfs-client-provisioner为例 &lt;a href="https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner">https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner&lt;/a>
 &lt;a class="anchor" href="#%e9%a1%b9%e7%9b%ae%e4%b8%ad%e4%bb%a5nfs-client-provisioner%e4%b8%ba%e4%be%8b">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="1编辑集群配置文件clusters集群名configyml">
 1.编辑集群配置文件：clusters/${集群名}/config.yml
 &lt;a class="anchor" href="#1%e7%bc%96%e8%be%91%e9%9b%86%e7%be%a4%e9%85%8d%e7%bd%ae%e6%96%87%e4%bb%b6clusters%e9%9b%86%e7%be%a4%e5%90%8dconfigyml">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>... 省略
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 在role:cluster-addon 中启用nfs-provisioner 安装&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nfs_provisioner_install: &lt;span style="color:#e6db74">&amp;#34;yes&amp;#34;&lt;/span>			&lt;span style="color:#75715e"># 修改为yes&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nfs_provisioner_namespace: &lt;span style="color:#e6db74">&amp;#34;kube-system&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nfs_provisioner_ver: &lt;span style="color:#e6db74">&amp;#34;v4.0.1&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nfs_storage_class: &lt;span style="color:#e6db74">&amp;#34;managed-nfs-storage&amp;#34;&lt;/span>	
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nfs_server: &lt;span style="color:#e6db74">&amp;#34;192.168.31.244&amp;#34;&lt;/span>			&lt;span style="color:#75715e"># 修改为实际nfs server地址&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nfs_path: &lt;span style="color:#e6db74">&amp;#34;/data/nfs&amp;#34;&lt;/span>				&lt;span style="color:#75715e"># 修改为实际的nfs共享目录&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>2.创建 nfs provisioner&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ dk ezctl setup &lt;span style="color:#e6db74">${&lt;/span>集群名&lt;span style="color:#e6db74">}&lt;/span> &lt;span style="color:#ae81ff">07&lt;/span> 
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 执行成功后验证&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ kubectl get pod --all-namespaces |grep nfs-client
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kube-system nfs-client-provisioner-84ff87c669-ksw95 1/1 Running &lt;span style="color:#ae81ff">0&lt;/span> 21m
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>
&lt;h5 id="3验证使用动态-pv">
 3.验证使用动态 PV
 &lt;a class="anchor" href="#3%e9%aa%8c%e8%af%81%e4%bd%bf%e7%94%a8%e5%8a%a8%e6%80%81-pv">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h5 id="在目录clusters集群名ymlnfs-provisioner-有个测试例子">
 在目录clusters/${集群名}/yml/nfs-provisioner/ 有个测试例子
 &lt;a class="anchor" href="#%e5%9c%a8%e7%9b%ae%e5%bd%95clusters%e9%9b%86%e7%be%a4%e5%90%8dymlnfs-provisioner-%e6%9c%89%e4%b8%aa%e6%b5%8b%e8%af%95%e4%be%8b%e5%ad%90">#&lt;/a>
&lt;/h5>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl apply -f /etc/kubeasz/clusters/hello/yml/nfs-provisioner/test-pod.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 验证测试pod&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl get pod
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME READY STATUS RESTARTS AGE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>test-pod 0/1 Completed &lt;span style="color:#ae81ff">0&lt;/span> 6h36m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 验证自动创建的pv 资源，&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl get pv
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>pvc-44d34a50-e00b-4f6c-8005-40f5cc54af18 2Mi RWX Delete Bound default/test-claim managed-nfs-storage 6h36m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 验证PVC已经绑定成功：STATUS字段为 Bound&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl get pvc
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>test-claim Bound pvc-44d34a50-e00b-4f6c-8005-40f5cc54af18 2Mi RWX managed-nfs-storage 6h37m
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h5 id="另外pod启动完成后在挂载的目录中创建一个success文件我们可以到nfs服务器去看下">
 另外，Pod启动完成后，在挂载的目录中创建一个&lt;code>SUCCESS&lt;/code>文件。我们可以到NFS服务器去看下：
 &lt;a class="anchor" href="#%e5%8f%a6%e5%a4%96pod%e5%90%af%e5%8a%a8%e5%ae%8c%e6%88%90%e5%90%8e%e5%9c%a8%e6%8c%82%e8%bd%bd%e7%9a%84%e7%9b%ae%e5%bd%95%e4%b8%ad%e5%88%9b%e5%bb%ba%e4%b8%80%e4%b8%aasuccess%e6%96%87%e4%bb%b6%e6%88%91%e4%bb%ac%e5%8f%af%e4%bb%a5%e5%88%b0nfs%e6%9c%8d%e5%8a%a1%e5%99%a8%e5%8e%bb%e7%9c%8b%e4%b8%8b">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>.
└── default-test-claim-pvc-44d34a50-e00b-4f6c-8005-40f5cc54af18
 └── SUCCESS
&lt;/code>&lt;/pre>&lt;h5 id="如上可以发现挂载的时候nfs-client根据pvc自动创建了一个目录我们pod中挂载的mnt实际引用的就是该目录而我们在mnt下创建的success文件也自动写入到了这里">
 如上，可以发现挂载的时候，nfs-client根据PVC自动创建了一个目录，我们Pod中挂载的&lt;code>/mnt&lt;/code>，实际引用的就是该目录，而我们在&lt;code>/mnt&lt;/code>下创建的&lt;code>SUCCESS&lt;/code>文件，也自动写入到了这里。
 &lt;a class="anchor" href="#%e5%a6%82%e4%b8%8a%e5%8f%af%e4%bb%a5%e5%8f%91%e7%8e%b0%e6%8c%82%e8%bd%bd%e7%9a%84%e6%97%b6%e5%80%99nfs-client%e6%a0%b9%e6%8d%aepvc%e8%87%aa%e5%8a%a8%e5%88%9b%e5%bb%ba%e4%ba%86%e4%b8%80%e4%b8%aa%e7%9b%ae%e5%bd%95%e6%88%91%e4%bb%acpod%e4%b8%ad%e6%8c%82%e8%bd%bd%e7%9a%84mnt%e5%ae%9e%e9%99%85%e5%bc%95%e7%94%a8%e7%9a%84%e5%b0%b1%e6%98%af%e8%af%a5%e7%9b%ae%e5%bd%95%e8%80%8c%e6%88%91%e4%bb%ac%e5%9c%a8mnt%e4%b8%8b%e5%88%9b%e5%bb%ba%e7%9a%84success%e6%96%87%e4%bb%b6%e4%b9%9f%e8%87%aa%e5%8a%a8%e5%86%99%e5%85%a5%e5%88%b0%e4%ba%86%e8%bf%99%e9%87%8c">#&lt;/a>
&lt;/h5>
&lt;h5 id="后面当我们需要为上层应用提供持久化存储时只需要提供storageclass即可很多应用都会根据storageclass来创建他们的所需的pvc-最后再把pvc挂载到他们的deployment或statefulset中使用比如efkjenkins等">
 后面当我们需要为上层应用提供持久化存储时，只需要提供&lt;code>StorageClass&lt;/code>即可。很多应用都会根据&lt;code>StorageClass&lt;/code>来创建他们的所需的PVC, 最后再把PVC挂载到他们的Deployment或StatefulSet中使用，比如：efk、jenkins等
 &lt;a class="anchor" href="#%e5%90%8e%e9%9d%a2%e5%bd%93%e6%88%91%e4%bb%ac%e9%9c%80%e8%a6%81%e4%b8%ba%e4%b8%8a%e5%b1%82%e5%ba%94%e7%94%a8%e6%8f%90%e4%be%9b%e6%8c%81%e4%b9%85%e5%8c%96%e5%ad%98%e5%82%a8%e6%97%b6%e5%8f%aa%e9%9c%80%e8%a6%81%e6%8f%90%e4%be%9bstorageclass%e5%8d%b3%e5%8f%af%e5%be%88%e5%a4%9a%e5%ba%94%e7%94%a8%e9%83%bd%e4%bc%9a%e6%a0%b9%e6%8d%aestorageclass%e6%9d%a5%e5%88%9b%e5%bb%ba%e4%bb%96%e4%bb%ac%e7%9a%84%e6%89%80%e9%9c%80%e7%9a%84pvc-%e6%9c%80%e5%90%8e%e5%86%8d%e6%8a%8apvc%e6%8c%82%e8%bd%bd%e5%88%b0%e4%bb%96%e4%bb%ac%e7%9a%84deployment%e6%88%96statefulset%e4%b8%ad%e4%bd%bf%e7%94%a8%e6%af%94%e5%a6%82efkjenkins%e7%ad%89">#&lt;/a>
&lt;/h5>
&lt;h2 id="本地存储目录供应者">
 本地存储目录供应者
 &lt;a class="anchor" href="#%e6%9c%ac%e5%9c%b0%e5%ad%98%e5%82%a8%e7%9b%ae%e5%bd%95%e4%be%9b%e5%ba%94%e8%80%85">#&lt;/a>
&lt;/h2>
&lt;h5 id="当应用对于磁盘io性能要求高比较适合本地文件目录存储特别地可以本地挂载ssd磁盘注意本地磁盘需要配置raid冗余策略local-path-provisioner-可以方便地在k8s集群中使用本地文件目录存储">
 当应用对于磁盘I/O性能要求高，比较适合本地文件目录存储，特别地可以本地挂载SSD磁盘（注意本地磁盘需要配置raid冗余策略）。Local Path Provisioner 可以方便地在k8s集群中使用本地文件目录存储。
 &lt;a class="anchor" href="#%e5%bd%93%e5%ba%94%e7%94%a8%e5%af%b9%e4%ba%8e%e7%a3%81%e7%9b%98io%e6%80%a7%e8%83%bd%e8%a6%81%e6%b1%82%e9%ab%98%e6%af%94%e8%be%83%e9%80%82%e5%90%88%e6%9c%ac%e5%9c%b0%e6%96%87%e4%bb%b6%e7%9b%ae%e5%bd%95%e5%ad%98%e5%82%a8%e7%89%b9%e5%88%ab%e5%9c%b0%e5%8f%af%e4%bb%a5%e6%9c%ac%e5%9c%b0%e6%8c%82%e8%bd%bdssd%e7%a3%81%e7%9b%98%e6%b3%a8%e6%84%8f%e6%9c%ac%e5%9c%b0%e7%a3%81%e7%9b%98%e9%9c%80%e8%a6%81%e9%85%8d%e7%bd%aeraid%e5%86%97%e4%bd%99%e7%ad%96%e7%95%a5local-path-provisioner-%e5%8f%af%e4%bb%a5%e6%96%b9%e4%be%bf%e5%9c%b0%e5%9c%a8k8s%e9%9b%86%e7%be%a4%e4%b8%ad%e4%bd%bf%e7%94%a8%e6%9c%ac%e5%9c%b0%e6%96%87%e4%bb%b6%e7%9b%ae%e5%bd%95%e5%ad%98%e5%82%a8">#&lt;/a>
&lt;/h5>
&lt;h5 id="在kubeasz项目中集成安装">
 在kubeasz项目中集成安装
 &lt;a class="anchor" href="#%e5%9c%a8kubeasz%e9%a1%b9%e7%9b%ae%e4%b8%ad%e9%9b%86%e6%88%90%e5%ae%89%e8%a3%85">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="1编辑集群配置文件clusters集群名configyml-1">
 1.编辑集群配置文件：clusters/${集群名}/config.yml
 &lt;a class="anchor" href="#1%e7%bc%96%e8%be%91%e9%9b%86%e7%be%a4%e9%85%8d%e7%bd%ae%e6%96%87%e4%bb%b6clusters%e9%9b%86%e7%be%a4%e5%90%8dconfigyml-1">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>... 省略
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>local_path_provisioner_install: &lt;span style="color:#e6db74">&amp;#34;yes&amp;#34;&lt;/span> &lt;span style="color:#75715e"># 修改为yes&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 设置默认本地存储路径&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>local_path_provisioner_dir: &lt;span style="color:#e6db74">&amp;#34;/opt/local-path-provisioner&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>
&lt;h5 id="2创建-local-path-provisioner">
 2.创建 local path provisioner
 &lt;a class="anchor" href="#2%e5%88%9b%e5%bb%ba-local-path-provisioner">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ dk ezctl setup &lt;span style="color:#e6db74">${&lt;/span>集群名&lt;span style="color:#e6db74">}&lt;/span> &lt;span style="color:#ae81ff">07&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 执行成功后验证&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ kubectl get pod --all-namespaces |grep nfs-client-provisioner
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>
&lt;h5 id="3验证使用略">
 3.验证使用（略）
 &lt;a class="anchor" href="#3%e9%aa%8c%e8%af%81%e4%bd%bf%e7%94%a8%e7%95%a5">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul></description></item><item><title>2023-09-28 15:26:42.651 07-安装集群主要插件</title><link>https://qq547475331.github.io/docs/07-%E5%AE%89%E8%A3%85%E9%9B%86%E7%BE%A4%E4%B8%BB%E8%A6%81%E6%8F%92%E4%BB%B6-07--an-zhuang-ji-qun-zhu-yao-cha-jian/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/07-%E5%AE%89%E8%A3%85%E9%9B%86%E7%BE%A4%E4%B8%BB%E8%A6%81%E6%8F%92%E4%BB%B6-07--an-zhuang-ji-qun-zhu-yao-cha-jian/</guid><description>&lt;h1 id="07-安装集群主要插件">
 07-安装集群主要插件
 &lt;a class="anchor" href="#07-%e5%ae%89%e8%a3%85%e9%9b%86%e7%be%a4%e4%b8%bb%e8%a6%81%e6%8f%92%e4%bb%b6">#&lt;/a>
&lt;/h1>
&lt;h5 id="目前挑选一些常用必要的插件自动集成到安装脚本之中">
 目前挑选一些常用、必要的插件自动集成到安装脚本之中:
 &lt;a class="anchor" href="#%e7%9b%ae%e5%89%8d%e6%8c%91%e9%80%89%e4%b8%80%e4%ba%9b%e5%b8%b8%e7%94%a8%e5%bf%85%e8%a6%81%e7%9a%84%e6%8f%92%e4%bb%b6%e8%87%aa%e5%8a%a8%e9%9b%86%e6%88%90%e5%88%b0%e5%ae%89%e8%a3%85%e8%84%9a%e6%9c%ac%e4%b9%8b%e4%b8%ad">#&lt;/a>
&lt;/h5>
&lt;h2 id="集群默认安装">
 集群默认安装
 &lt;a class="anchor" href="#%e9%9b%86%e7%be%a4%e9%bb%98%e8%ae%a4%e5%ae%89%e8%a3%85">#&lt;/a>
&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="../guide/kubedns.md">coredns&lt;/a>&lt;/li>
&lt;li>&lt;a href="../guide/kubedns.md">nodelocaldns&lt;/a>&lt;/li>
&lt;li>&lt;a href="../guide/metrics-server.md">metrics-server&lt;/a>&lt;/li>
&lt;li>&lt;a href="../guide/dashboard.md">dashboard&lt;/a>&lt;/li>
&lt;/ul>
&lt;h5 id="kubeasz-默认安装上述基础插件并支持离线方式安装ezdown--d-命令会自动下载组件镜像并推送到本地镜像仓库easzlabiolocal5000">
 kubeasz 默认安装上述基础插件，并支持离线方式安装(./ezdown -D 命令会自动下载组件镜像，并推送到本地镜像仓库easzlab.io.local:5000)
 &lt;a class="anchor" href="#kubeasz-%e9%bb%98%e8%ae%a4%e5%ae%89%e8%a3%85%e4%b8%8a%e8%bf%b0%e5%9f%ba%e7%a1%80%e6%8f%92%e4%bb%b6%e5%b9%b6%e6%94%af%e6%8c%81%e7%a6%bb%e7%ba%bf%e6%96%b9%e5%bc%8f%e5%ae%89%e8%a3%85ezdown--d-%e5%91%bd%e4%bb%a4%e4%bc%9a%e8%87%aa%e5%8a%a8%e4%b8%8b%e8%bd%bd%e7%bb%84%e4%bb%b6%e9%95%9c%e5%83%8f%e5%b9%b6%e6%8e%a8%e9%80%81%e5%88%b0%e6%9c%ac%e5%9c%b0%e9%95%9c%e5%83%8f%e4%bb%93%e5%ba%93easzlabiolocal5000">#&lt;/a>
&lt;/h5>
&lt;h2 id="集群可选安装">
 集群可选安装
 &lt;a class="anchor" href="#%e9%9b%86%e7%be%a4%e5%8f%af%e9%80%89%e5%ae%89%e8%a3%85">#&lt;/a>
&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="../guide/prometheus.md">prometheus&lt;/a>&lt;/li>
&lt;li>&lt;a href="network-plugin/network-check.md">network_check&lt;/a>&lt;/li>
&lt;li>&lt;a href="">nfs_provisioner&lt;/a>&lt;/li>
&lt;/ul>
&lt;h5 id="kubeasz-默认不安装上述插件可以在配置文件clustersxxxconfigyml中开启支持离线方式安装ezdown--x-会额外下载这些组件镜像并推送到本地镜像仓库easzlabiolocal5000">
 kubeasz 默认不安装上述插件，可以在配置文件(clusters/xxx/config.yml)中开启，支持离线方式安装(./ezdown -X 会额外下载这些组件镜像，并推送到本地镜像仓库easzlab.io.local:5000)
 &lt;a class="anchor" href="#kubeasz-%e9%bb%98%e8%ae%a4%e4%b8%8d%e5%ae%89%e8%a3%85%e4%b8%8a%e8%bf%b0%e6%8f%92%e4%bb%b6%e5%8f%af%e4%bb%a5%e5%9c%a8%e9%85%8d%e7%bd%ae%e6%96%87%e4%bb%b6clustersxxxconfigyml%e4%b8%ad%e5%bc%80%e5%90%af%e6%94%af%e6%8c%81%e7%a6%bb%e7%ba%bf%e6%96%b9%e5%bc%8f%e5%ae%89%e8%a3%85ezdown--x-%e4%bc%9a%e9%a2%9d%e5%a4%96%e4%b8%8b%e8%bd%bd%e8%bf%99%e4%ba%9b%e7%bb%84%e4%bb%b6%e9%95%9c%e5%83%8f%e5%b9%b6%e6%8e%a8%e9%80%81%e5%88%b0%e6%9c%ac%e5%9c%b0%e9%95%9c%e5%83%8f%e4%bb%93%e5%ba%93easzlabiolocal5000">#&lt;/a>
&lt;/h5>
&lt;h2 id="安装脚本">
 安装脚本
 &lt;a class="anchor" href="#%e5%ae%89%e8%a3%85%e8%84%9a%e6%9c%ac">#&lt;/a>
&lt;/h2>
&lt;h5 id="详见rolescluster-addon-目录">
 详见&lt;code>roles/cluster-addon/&lt;/code> 目录
 &lt;a class="anchor" href="#%e8%af%a6%e8%a7%81rolescluster-addon-%e7%9b%ae%e5%bd%95">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="1根据hosts文件中配置的cluster_dns_svc_ip-cluster_dns_domain等参数生成kubednsyaml和corednsyaml文件">
 1.根据hosts文件中配置的&lt;code>CLUSTER_DNS_SVC_IP&lt;/code> &lt;code>CLUSTER_DNS_DOMAIN&lt;/code>等参数生成kubedns.yaml和coredns.yaml文件
 &lt;a class="anchor" href="#1%e6%a0%b9%e6%8d%aehosts%e6%96%87%e4%bb%b6%e4%b8%ad%e9%85%8d%e7%bd%ae%e7%9a%84cluster_dns_svc_ip-cluster_dns_domain%e7%ad%89%e5%8f%82%e6%95%b0%e7%94%9f%e6%88%90kubednsyaml%e5%92%8ccorednsyaml%e6%96%87%e4%bb%b6">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="2注册变量pod_infopod_info用来判断现有集群是否已经运行各种插件">
 2.注册变量pod_info，pod_info用来判断现有集群是否已经运行各种插件
 &lt;a class="anchor" href="#2%e6%b3%a8%e5%86%8c%e5%8f%98%e9%87%8fpod_infopod_info%e7%94%a8%e6%9d%a5%e5%88%a4%e6%96%ad%e7%8e%b0%e6%9c%89%e9%9b%86%e7%be%a4%e6%98%af%e5%90%a6%e5%b7%b2%e7%bb%8f%e8%bf%90%e8%a1%8c%e5%90%84%e7%a7%8d%e6%8f%92%e4%bb%b6">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="3根据pod_info和配置开关逐个进行跳过插件安装">
 3.根据pod_info和&lt;code>配置开关&lt;/code>逐个进行/跳过插件安装
 &lt;a class="anchor" href="#3%e6%a0%b9%e6%8d%aepod_info%e5%92%8c%e9%85%8d%e7%bd%ae%e5%bc%80%e5%85%b3%e9%80%90%e4%b8%aa%e8%bf%9b%e8%a1%8c%e8%b7%b3%e8%bf%87%e6%8f%92%e4%bb%b6%e5%ae%89%e8%a3%85">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h2 id="下一步">
 下一步
 &lt;a class="anchor" href="#%e4%b8%8b%e4%b8%80%e6%ad%a5">#&lt;/a>
&lt;/h2>
&lt;ul>
&lt;li>
&lt;h5 id="创建ex_-向集群外提供高可用apiserver">
 &lt;a href="ex-lb.md">创建ex_lb节点组&lt;/a>, 向集群外提供高可用apiserver
 &lt;a class="anchor" href="#%e5%88%9b%e5%bb%baex_-%e5%90%91%e9%9b%86%e7%be%a4%e5%a4%96%e6%8f%90%e4%be%9b%e9%ab%98%e5%8f%af%e7%94%a8apiserver">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="08-cluster-storage.md">创建集群持久化存储&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>2023-09-28 calico 配置 BGP Route Reflectors</title><link>https://qq547475331.github.io/docs/calico-%E9%85%8D%E7%BD%AE-bgp-route-reflectors-calico-pei-zhi-bgproutereflectors/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/calico-%E9%85%8D%E7%BD%AE-bgp-route-reflectors-calico-pei-zhi-bgproutereflectors/</guid><description>&lt;h1 id="calico-配置-bgp-route-reflectors">
 calico 配置 BGP Route Reflectors
 &lt;a class="anchor" href="#calico-%e9%85%8d%e7%bd%ae-bgp-route-reflectors">#&lt;/a>
&lt;/h1>
&lt;h5 id="calico作为k8s的一个流行网络插件它依赖bgp路由协议实现集群节点上的pod路由互通而路由互通的前提是节点间建立-bgp-peer-连接bgp-路由反射器route-reflectors简称-rr可以简化集群bgp-peer的连接方式它是解决bgp扩展性问题的有效方式具体来说">
 &lt;code>Calico&lt;/code>作为&lt;code>k8s&lt;/code>的一个流行网络插件，它依赖&lt;code>BGP&lt;/code>路由协议实现集群节点上的&lt;code>POD&lt;/code>路由互通；而路由互通的前提是节点间建立 BGP Peer 连接。BGP 路由反射器（Route Reflectors，简称 RR）可以简化集群BGP Peer的连接方式，它是解决BGP扩展性问题的有效方式；具体来说：
 &lt;a class="anchor" href="#calico%e4%bd%9c%e4%b8%bak8s%e7%9a%84%e4%b8%80%e4%b8%aa%e6%b5%81%e8%a1%8c%e7%bd%91%e7%bb%9c%e6%8f%92%e4%bb%b6%e5%ae%83%e4%be%9d%e8%b5%96bgp%e8%b7%af%e7%94%b1%e5%8d%8f%e8%ae%ae%e5%ae%9e%e7%8e%b0%e9%9b%86%e7%be%a4%e8%8a%82%e7%82%b9%e4%b8%8a%e7%9a%84pod%e8%b7%af%e7%94%b1%e4%ba%92%e9%80%9a%e8%80%8c%e8%b7%af%e7%94%b1%e4%ba%92%e9%80%9a%e7%9a%84%e5%89%8d%e6%8f%90%e6%98%af%e8%8a%82%e7%82%b9%e9%97%b4%e5%bb%ba%e7%ab%8b-bgp-peer-%e8%bf%9e%e6%8e%a5bgp-%e8%b7%af%e7%94%b1%e5%8f%8d%e5%b0%84%e5%99%a8route-reflectors%e7%ae%80%e7%a7%b0-rr%e5%8f%af%e4%bb%a5%e7%ae%80%e5%8c%96%e9%9b%86%e7%be%a4bgp-peer%e7%9a%84%e8%bf%9e%e6%8e%a5%e6%96%b9%e5%bc%8f%e5%ae%83%e6%98%af%e8%a7%a3%e5%86%b3bgp%e6%89%a9%e5%b1%95%e6%80%a7%e9%97%ae%e9%a2%98%e7%9a%84%e6%9c%89%e6%95%88%e6%96%b9%e5%bc%8f%e5%85%b7%e4%bd%93%e6%9d%a5%e8%af%b4">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="没有-rr-时所有节点之间需要两两建立连接ibgp全互联节点数量增加将导致连接数剧增资源占用剧增">
 没有 RR 时，所有节点之间需要两两建立连接（IBGP全互联），节点数量增加将导致连接数剧增、资源占用剧增
 &lt;a class="anchor" href="#%e6%b2%a1%e6%9c%89-rr-%e6%97%b6%e6%89%80%e6%9c%89%e8%8a%82%e7%82%b9%e4%b9%8b%e9%97%b4%e9%9c%80%e8%a6%81%e4%b8%a4%e4%b8%a4%e5%bb%ba%e7%ab%8b%e8%bf%9e%e6%8e%a5ibgp%e5%85%a8%e4%ba%92%e8%81%94%e8%8a%82%e7%82%b9%e6%95%b0%e9%87%8f%e5%a2%9e%e5%8a%a0%e5%b0%86%e5%af%bc%e8%87%b4%e8%bf%9e%e6%8e%a5%e6%95%b0%e5%89%a7%e5%a2%9e%e8%b5%84%e6%ba%90%e5%8d%a0%e7%94%a8%e5%89%a7%e5%a2%9e">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="引入-rr-后其他-bgp-路由器只需要与它建立连接并交换路由信息节点数量增加连接数只是线性增加节省系统资源">
 引入 RR 后，其他 BGP 路由器只需要与它建立连接并交换路由信息，节点数量增加连接数只是线性增加，节省系统资源
 &lt;a class="anchor" href="#%e5%bc%95%e5%85%a5-rr-%e5%90%8e%e5%85%b6%e4%bb%96-bgp-%e8%b7%af%e7%94%b1%e5%99%a8%e5%8f%aa%e9%9c%80%e8%a6%81%e4%b8%8e%e5%ae%83%e5%bb%ba%e7%ab%8b%e8%bf%9e%e6%8e%a5%e5%b9%b6%e4%ba%a4%e6%8d%a2%e8%b7%af%e7%94%b1%e4%bf%a1%e6%81%af%e8%8a%82%e7%82%b9%e6%95%b0%e9%87%8f%e5%a2%9e%e5%8a%a0%e8%bf%9e%e6%8e%a5%e6%95%b0%e5%8f%aa%e6%98%af%e7%ba%bf%e6%80%a7%e5%a2%9e%e5%8a%a0%e8%8a%82%e7%9c%81%e7%b3%bb%e7%bb%9f%e8%b5%84%e6%ba%90">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h5 id="calico-node-版本-v33-开始支持内建路由反射器非常方便因此使用-calico-作为网络插件可以支持大规模节点数的k8s集群">
 calico-node 版本 v3.3 开始支持内建路由反射器，非常方便，因此使用 calico 作为网络插件可以支持大规模节点数的&lt;code>K8S&lt;/code>集群。
 &lt;a class="anchor" href="#calico-node-%e7%89%88%e6%9c%ac-v33-%e5%bc%80%e5%a7%8b%e6%94%af%e6%8c%81%e5%86%85%e5%bb%ba%e8%b7%af%e7%94%b1%e5%8f%8d%e5%b0%84%e5%99%a8%e9%9d%9e%e5%b8%b8%e6%96%b9%e4%be%bf%e5%9b%a0%e6%ad%a4%e4%bd%bf%e7%94%a8-calico-%e4%bd%9c%e4%b8%ba%e7%bd%91%e7%bb%9c%e6%8f%92%e4%bb%b6%e5%8f%af%e4%bb%a5%e6%94%af%e6%8c%81%e5%a4%a7%e8%a7%84%e6%a8%a1%e8%8a%82%e7%82%b9%e6%95%b0%e7%9a%84k8s%e9%9b%86%e7%be%a4">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="建议集群节点数大于50时应用bgp-route-reflectors-特性">
 建议集群节点数大于50时，应用BGP Route Reflectors 特性
 &lt;a class="anchor" href="#%e5%bb%ba%e8%ae%ae%e9%9b%86%e7%be%a4%e8%8a%82%e7%82%b9%e6%95%b0%e5%a4%a7%e4%ba%8e50%e6%97%b6%e5%ba%94%e7%94%a8bgp-route-reflectors-%e7%89%b9%e6%80%a7">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h5 id="前提条件">
 前提条件
 &lt;a class="anchor" href="#%e5%89%8d%e6%8f%90%e6%9d%a1%e4%bb%b6">#&lt;/a>
&lt;/h5>
&lt;h5 id="k8s-集群使用calico网络插件部署成功本实验环境为按照kubeasz安装的2主2从集群calico-版本-v3194">
 k8s 集群使用calico网络插件部署成功。本实验环境为按照kubeasz安装的2主2从集群，calico 版本 v3.19.4。
 &lt;a class="anchor" href="#k8s-%e9%9b%86%e7%be%a4%e4%bd%bf%e7%94%a8calico%e7%bd%91%e7%bb%9c%e6%8f%92%e4%bb%b6%e9%83%a8%e7%bd%b2%e6%88%90%e5%8a%9f%e6%9c%ac%e5%ae%9e%e9%aa%8c%e7%8e%af%e5%a2%83%e4%b8%ba%e6%8c%89%e7%85%a7kubeasz%e5%ae%89%e8%a3%85%e7%9a%842%e4%b8%bb2%e4%bb%8e%e9%9b%86%e7%be%a4calico-%e7%89%88%e6%9c%ac-v3194">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>$ kubectl get node
NAME STATUS ROLES AGE VERSION
192.168.1.1 Ready,SchedulingDisabled master 178m v1.13.1
192.168.1.2 Ready,SchedulingDisabled master 178m v1.13.1
192.168.1.3 Ready node 178m v1.13.1
192.168.1.4 Ready node 178m v1.13.1
$ kubectl get pod -n kube-system -o wide | grep calico
calico-kube-controllers-77487546bd-jqrlc 1/1 Running 0 179m 192.168.1.3 192.168.1.3 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
calico-node-67t5m 2/2 Running 0 179m 192.168.1.1 192.168.1.1 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
calico-node-drmhq 2/2 Running 0 179m 192.168.1.2 192.168.1.2 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
calico-node-rjtkv 2/2 Running 0 179m 192.168.1.4 192.168.1.4 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
calico-node-xtspl 2/2 Running 0 179m 192.168.1.3 192.168.1.3 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
&lt;/code>&lt;/pre>&lt;h5 id="查看当前集群中bgp连接情况可以看到集群中4个节点两两建立了-bgp-连接">
 查看当前集群中BGP连接情况：可以看到集群中4个节点两两建立了 BGP 连接
 &lt;a class="anchor" href="#%e6%9f%a5%e7%9c%8b%e5%bd%93%e5%89%8d%e9%9b%86%e7%be%a4%e4%b8%adbgp%e8%bf%9e%e6%8e%a5%e6%83%85%e5%86%b5%e5%8f%af%e4%bb%a5%e7%9c%8b%e5%88%b0%e9%9b%86%e7%be%a4%e4%b8%ad4%e4%b8%aa%e8%8a%82%e7%82%b9%e4%b8%a4%e4%b8%a4%e5%bb%ba%e7%ab%8b%e4%ba%86-bgp-%e8%bf%9e%e6%8e%a5">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>$ dk ansible -i /etc/kubeasz/clusters/xxx/hosts all -m shell -a &amp;#39;/opt/kube/bin/calicoctl node status&amp;#39;
192.168.1.3 | SUCCESS | rc=0 &amp;gt;&amp;gt;
Calico process is running.

IPv4 BGP status
+--------------+-------------------+-------+----------+-------------+
| PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO |
+--------------+-------------------+-------+----------+-------------+
| 192.168.1.1 | node-to-node mesh | up | 03:08:20 | Established |
| 192.168.1.2 | node-to-node mesh | up | 03:08:18 | Established |
| 192.168.1.4 | node-to-node mesh | up | 03:08:19 | Established |
+--------------+-------------------+-------+----------+-------------+

IPv6 BGP status
No IPv6 peers found.

192.168.1.2 | SUCCESS | rc=0 &amp;gt;&amp;gt;
Calico process is running.

IPv4 BGP status
+--------------+-------------------+-------+----------+-------------+
| PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO |
+--------------+-------------------+-------+----------+-------------+
| 192.168.1.4 | node-to-node mesh | up | 03:08:17 | Established |
| 192.168.1.3 | node-to-node mesh | up | 03:08:18 | Established |
| 192.168.1.1 | node-to-node mesh | up | 03:08:20 | Established |
+--------------+-------------------+-------+----------+-------------+

IPv6 BGP status
No IPv6 peers found.

192.168.1.1 | SUCCESS | rc=0 &amp;gt;&amp;gt;
Calico process is running.

IPv4 BGP status
+--------------+-------------------+-------+----------+-------------+
| PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO |
+--------------+-------------------+-------+----------+-------------+
| 192.168.1.2 | node-to-node mesh | up | 03:08:21 | Established |
| 192.168.1.3 | node-to-node mesh | up | 03:08:21 | Established |
| 192.168.1.4 | node-to-node mesh | up | 03:08:21 | Established |
+--------------+-------------------+-------+----------+-------------+

IPv6 BGP status
No IPv6 peers found.

192.168.1.4 | SUCCESS | rc=0 &amp;gt;&amp;gt;
Calico process is running.

IPv4 BGP status
+--------------+-------------------+-------+----------+-------------+
| PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO |
+--------------+-------------------+-------+----------+-------------+
| 192.168.1.2 | node-to-node mesh | up | 03:08:17 | Established |
| 192.168.1.3 | node-to-node mesh | up | 03:08:19 | Established |
| 192.168.1.1 | node-to-node mesh | up | 03:08:20 | Established |
+--------------+-------------------+-------+----------+-------------+

IPv6 BGP status
No IPv6 peers found.
&lt;/code>&lt;/pre>&lt;h2 id="kubeasz-自动安装启用-route-reflector">
 kubeasz 自动安装启用 route reflector
 &lt;a class="anchor" href="#kubeasz-%e8%87%aa%e5%8a%a8%e5%ae%89%e8%a3%85%e5%90%af%e7%94%a8-route-reflector">#&lt;/a>
&lt;/h2>
&lt;ul>
&lt;li>
&lt;h5 id="修改etckubeaszclustersxxxconfigyml文件设置配置项calico_rr_enabled-true">
 修改&lt;code>/etc/kubeasz/clusters/xxx/config.yml&lt;/code>文件，设置配置项&lt;code>CALICO_RR_ENABLED: true&lt;/code>
 &lt;a class="anchor" href="#%e4%bf%ae%e6%94%b9etckubeaszclustersxxxconfigyml%e6%96%87%e4%bb%b6%e8%ae%be%e7%bd%ae%e9%85%8d%e7%bd%ae%e9%a1%b9calico_rr_enabled-true">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="重新执行网络安装-dk-ezctl-setup-xxx-07">
 重新执行网络安装 &lt;code>dk ezctl setup xxx 07&lt;/code>
 &lt;a class="anchor" href="#%e9%87%8d%e6%96%b0%e6%89%a7%e8%a1%8c%e7%bd%91%e7%bb%9c%e5%ae%89%e8%a3%85-dk-ezctl-setup-xxx-07">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h5 id="执行完成检查bgp连接验证即可">
 执行完成，检查bgp连接验证即可。
 &lt;a class="anchor" href="#%e6%89%a7%e8%a1%8c%e5%ae%8c%e6%88%90%e6%a3%80%e6%9f%a5bgp%e8%bf%9e%e6%8e%a5%e9%aa%8c%e8%af%81%e5%8d%b3%e5%8f%af">#&lt;/a>
&lt;/h5>
&lt;h3 id="附手动安装route-reflector-过程讲解">
 附：手动安装route reflector 过程讲解
 &lt;a class="anchor" href="#%e9%99%84%e6%89%8b%e5%8a%a8%e5%ae%89%e8%a3%85route-reflector-%e8%bf%87%e7%a8%8b%e8%ae%b2%e8%a7%a3">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>选择并配置 Route Reflector 节点&lt;/li>
&lt;/ul>
&lt;p>首先查看当前集群中的节点：&lt;/p></description></item><item><title>2023-09-28 EX-LB 负载均衡部署</title><link>https://qq547475331.github.io/docs/ex-lb-%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E9%83%A8%E7%BD%B2-ex-lb-fu-zai-jun-heng-bu-shu/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/ex-lb-%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E9%83%A8%E7%BD%B2-ex-lb-fu-zai-jun-heng-bu-shu/</guid><description>&lt;h2 id="ex-lb-负载均衡部署">
 EX-LB 负载均衡部署
 &lt;a class="anchor" href="#ex-lb-%e8%b4%9f%e8%bd%bd%e5%9d%87%e8%a1%a1%e9%83%a8%e7%bd%b2">#&lt;/a>
&lt;/h2>
&lt;h5 id="根据ha-2x架构k8s集群自身高可用已经不依赖于外部-lb-服务但是有时我们要从外部访问-apiserver比如-ci-流程就需要-ex_lb-来请求多个-apiserver">
 根据&lt;a href="00-planning_and_overall_intro.md">HA 2x架构&lt;/a>，k8s集群自身高可用已经不依赖于外部 lb 服务；但是有时我们要从外部访问 apiserver（比如 CI 流程），就需要 ex_lb 来请求多个 apiserver；
 &lt;a class="anchor" href="#%e6%a0%b9%e6%8d%aeha-2x%e6%9e%b6%e6%9e%84k8s%e9%9b%86%e7%be%a4%e8%87%aa%e8%ba%ab%e9%ab%98%e5%8f%af%e7%94%a8%e5%b7%b2%e7%bb%8f%e4%b8%8d%e4%be%9d%e8%b5%96%e4%ba%8e%e5%a4%96%e9%83%a8-lb-%e6%9c%8d%e5%8a%a1%e4%bd%86%e6%98%af%e6%9c%89%e6%97%b6%e6%88%91%e4%bb%ac%e8%a6%81%e4%bb%8e%e5%a4%96%e9%83%a8%e8%ae%bf%e9%97%ae-apiserver%e6%af%94%e5%a6%82-ci-%e6%b5%81%e7%a8%8b%e5%b0%b1%e9%9c%80%e8%a6%81-ex_lb-%e6%9d%a5%e8%af%b7%e6%b1%82%e5%a4%9a%e4%b8%aa-apiserver">#&lt;/a>
&lt;/h5>
&lt;h5 id="还有一种情况是需要负载转发到ingress服务也需要部署ex_lb">
 还有一种情况是需要&lt;a href="../op/loadballance_ingress_nodeport.md">负载转发到ingress服务&lt;/a>，也需要部署ex_lb；
 &lt;a class="anchor" href="#%e8%bf%98%e6%9c%89%e4%b8%80%e7%a7%8d%e6%83%85%e5%86%b5%e6%98%af%e9%9c%80%e8%a6%81%e8%b4%9f%e8%bd%bd%e8%bd%ac%e5%8f%91%e5%88%b0ingress%e6%9c%8d%e5%8a%a1%e4%b9%9f%e9%9c%80%e8%a6%81%e9%83%a8%e7%bd%b2ex_lb">#&lt;/a>
&lt;/h5>
&lt;p>&lt;strong>注意：当遇到公有云环境无法自建 ex_lb 服务时，可以配置对应的云负载均衡服务&lt;/strong>&lt;/p>
&lt;h3 id="ex_lb-服务组件">
 ex_lb 服务组件
 &lt;a class="anchor" href="#ex_lb-%e6%9c%8d%e5%8a%a1%e7%bb%84%e4%bb%b6">#&lt;/a>
&lt;/h3>
&lt;h5 id="更新kubeasz-302-重写了ex-lb服务安装利用最小化依赖编译安装的二进制文件不依赖于linux发行版优点是可以统一版本和简化离线安装部署并且理论上能够支持更多linux发行版">
 更新：kubeasz 3.0.2 重写了ex-lb服务安装，利用最小化依赖编译安装的二进制文件，不依赖于linux发行版；优点是可以统一版本和简化离线安装部署，并且理论上能够支持更多linux发行版
 &lt;a class="anchor" href="#%e6%9b%b4%e6%96%b0kubeasz-302-%e9%87%8d%e5%86%99%e4%ba%86ex-lb%e6%9c%8d%e5%8a%a1%e5%ae%89%e8%a3%85%e5%88%a9%e7%94%a8%e6%9c%80%e5%b0%8f%e5%8c%96%e4%be%9d%e8%b5%96%e7%bc%96%e8%af%91%e5%ae%89%e8%a3%85%e7%9a%84%e4%ba%8c%e8%bf%9b%e5%88%b6%e6%96%87%e4%bb%b6%e4%b8%8d%e4%be%9d%e8%b5%96%e4%ba%8elinux%e5%8f%91%e8%a1%8c%e7%89%88%e4%bc%98%e7%82%b9%e6%98%af%e5%8f%af%e4%bb%a5%e7%bb%9f%e4%b8%80%e7%89%88%e6%9c%ac%e5%92%8c%e7%ae%80%e5%8c%96%e7%a6%bb%e7%ba%bf%e5%ae%89%e8%a3%85%e9%83%a8%e7%bd%b2%e5%b9%b6%e4%b8%94%e7%90%86%e8%ae%ba%e4%b8%8a%e8%83%bd%e5%a4%9f%e6%94%af%e6%8c%81%e6%9b%b4%e5%a4%9alinux%e5%8f%91%e8%a1%8c%e7%89%88">#&lt;/a>
&lt;/h5>
&lt;h5 id="ex_lb-服务由-keepalived-和-l4lb-组成">
 ex_lb 服务由 keepalived 和 l4lb 组成：
 &lt;a class="anchor" href="#ex_lb-%e6%9c%8d%e5%8a%a1%e7%94%b1-keepalived-%e5%92%8c-l4lb-%e7%bb%84%e6%88%90">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="l4lb是一个精简版仅支持四层转发的nginx编译二进制版本">
 l4lb：是一个精简版（仅支持四层转发）的nginx编译二进制版本
 &lt;a class="anchor" href="#l4lb%e6%98%af%e4%b8%80%e4%b8%aa%e7%b2%be%e7%ae%80%e7%89%88%e4%bb%85%e6%94%af%e6%8c%81%e5%9b%9b%e5%b1%82%e8%bd%ac%e5%8f%91%e7%9a%84nginx%e7%bc%96%e8%af%91%e4%ba%8c%e8%bf%9b%e5%88%b6%e7%89%88%e6%9c%ac">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="keepalived利用主备节点vrrp协议通信和虚拟地址消除l4lb的单点故障keepalived保持存活它是基于vrrp协议保证所谓的高可用或热备的这里用来预防l4lb的单点故障">
 keepalived：利用主备节点vrrp协议通信和虚拟地址，消除l4lb的单点故障；keepalived保持存活，它是基于VRRP协议保证所谓的高可用或热备的，这里用来预防l4lb的单点故障。
 &lt;a class="anchor" href="#keepalived%e5%88%a9%e7%94%a8%e4%b8%bb%e5%a4%87%e8%8a%82%e7%82%b9vrrp%e5%8d%8f%e8%ae%ae%e9%80%9a%e4%bf%a1%e5%92%8c%e8%99%9a%e6%8b%9f%e5%9c%b0%e5%9d%80%e6%b6%88%e9%99%a4l4lb%e7%9a%84%e5%8d%95%e7%82%b9%e6%95%85%e9%9a%9ckeepalived%e4%bf%9d%e6%8c%81%e5%ad%98%e6%b4%bb%e5%ae%83%e6%98%af%e5%9f%ba%e4%ba%8evrrp%e5%8d%8f%e8%ae%ae%e4%bf%9d%e8%af%81%e6%89%80%e8%b0%93%e7%9a%84%e9%ab%98%e5%8f%af%e7%94%a8%e6%88%96%e7%83%ad%e5%a4%87%e7%9a%84%e8%bf%99%e9%87%8c%e7%94%a8%e6%9d%a5%e9%a2%84%e9%98%b2l4lb%e7%9a%84%e5%8d%95%e7%82%b9%e6%95%85%e9%9a%9c">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h5 id="keepalived与l4lb配合实现master的高可用过程如下">
 keepalived与l4lb配合，实现master的高可用过程如下：
 &lt;a class="anchor" href="#keepalived%e4%b8%8el4lb%e9%85%8d%e5%90%88%e5%ae%9e%e7%8e%b0master%e7%9a%84%e9%ab%98%e5%8f%af%e7%94%a8%e8%bf%87%e7%a8%8b%e5%a6%82%e4%b8%8b">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="1keepalived利用vrrp协议生成一个虚拟地址vip正常情况下vip存活在keepalive的主节点当主节点故障时vip能够漂移到keepalived的备节点保障vip地址高可用性">
 1.keepalived利用vrrp协议生成一个虚拟地址(VIP)，正常情况下VIP存活在keepalive的主节点，当主节点故障时，VIP能够漂移到keepalived的备节点，保障VIP地址高可用性。
 &lt;a class="anchor" href="#1keepalived%e5%88%a9%e7%94%a8vrrp%e5%8d%8f%e8%ae%ae%e7%94%9f%e6%88%90%e4%b8%80%e4%b8%aa%e8%99%9a%e6%8b%9f%e5%9c%b0%e5%9d%80vip%e6%ad%a3%e5%b8%b8%e6%83%85%e5%86%b5%e4%b8%8bvip%e5%ad%98%e6%b4%bb%e5%9c%a8keepalive%e7%9a%84%e4%b8%bb%e8%8a%82%e7%82%b9%e5%bd%93%e4%b8%bb%e8%8a%82%e7%82%b9%e6%95%85%e9%9a%9c%e6%97%b6vip%e8%83%bd%e5%a4%9f%e6%bc%82%e7%a7%bb%e5%88%b0keepalived%e7%9a%84%e5%a4%87%e8%8a%82%e7%82%b9%e4%bf%9d%e9%9a%9cvip%e5%9c%b0%e5%9d%80%e9%ab%98%e5%8f%af%e7%94%a8%e6%80%a7">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="2在keepalived的主备节点都配置相同l4lb负载配置并且监听客户端请求在vip的地址上保障随时都有一个l4lb负载均衡在正常工作并且keepalived启用对l4lb进程的存活检测一旦主节点l4lb进程故障vip也能切换到备节点从而让备节点的l4lb进行负载工作">
 2.在keepalived的主备节点都配置相同l4lb负载配置，并且监听客户端请求在VIP的地址上，保障随时都有一个l4lb负载均衡在正常工作。并且keepalived启用对l4lb进程的存活检测，一旦主节点l4lb进程故障，VIP也能切换到备节点，从而让备节点的l4lb进行负载工作。
 &lt;a class="anchor" href="#2%e5%9c%a8keepalived%e7%9a%84%e4%b8%bb%e5%a4%87%e8%8a%82%e7%82%b9%e9%83%bd%e9%85%8d%e7%bd%ae%e7%9b%b8%e5%90%8cl4lb%e8%b4%9f%e8%bd%bd%e9%85%8d%e7%bd%ae%e5%b9%b6%e4%b8%94%e7%9b%91%e5%90%ac%e5%ae%a2%e6%88%b7%e7%ab%af%e8%af%b7%e6%b1%82%e5%9c%a8vip%e7%9a%84%e5%9c%b0%e5%9d%80%e4%b8%8a%e4%bf%9d%e9%9a%9c%e9%9a%8f%e6%97%b6%e9%83%bd%e6%9c%89%e4%b8%80%e4%b8%aal4lb%e8%b4%9f%e8%bd%bd%e5%9d%87%e8%a1%a1%e5%9c%a8%e6%ad%a3%e5%b8%b8%e5%b7%a5%e4%bd%9c%e5%b9%b6%e4%b8%94keepalived%e5%90%af%e7%94%a8%e5%af%b9l4lb%e8%bf%9b%e7%a8%8b%e7%9a%84%e5%ad%98%e6%b4%bb%e6%a3%80%e6%b5%8b%e4%b8%80%e6%97%a6%e4%b8%bb%e8%8a%82%e7%82%b9l4lb%e8%bf%9b%e7%a8%8b%e6%95%85%e9%9a%9cvip%e4%b9%9f%e8%83%bd%e5%88%87%e6%8d%a2%e5%88%b0%e5%a4%87%e8%8a%82%e7%82%b9%e4%bb%8e%e8%80%8c%e8%ae%a9%e5%a4%87%e8%8a%82%e7%82%b9%e7%9a%84l4lb%e8%bf%9b%e8%a1%8c%e8%b4%9f%e8%bd%bd%e5%b7%a5%e4%bd%9c">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="3在l4lb的配置中配置多个后端真实kube-apiserver的endpoints并启用存活监测后端kube-apiserver如果一个kube-apiserver故障l4lb会将其剔除负载池">
 3.在l4lb的配置中配置多个后端真实kube-apiserver的endpoints，并启用存活监测后端kube-apiserver，如果一个kube-apiserver故障，l4lb会将其剔除负载池。
 &lt;a class="anchor" href="#3%e5%9c%a8l4lb%e7%9a%84%e9%85%8d%e7%bd%ae%e4%b8%ad%e9%85%8d%e7%bd%ae%e5%a4%9a%e4%b8%aa%e5%90%8e%e7%ab%af%e7%9c%9f%e5%ae%9ekube-apiserver%e7%9a%84endpoints%e5%b9%b6%e5%90%af%e7%94%a8%e5%ad%98%e6%b4%bb%e7%9b%91%e6%b5%8b%e5%90%8e%e7%ab%afkube-apiserver%e5%a6%82%e6%9e%9c%e4%b8%80%e4%b8%aakube-apiserver%e6%95%85%e9%9a%9cl4lb%e4%bc%9a%e5%b0%86%e5%85%b6%e5%89%94%e9%99%a4%e8%b4%9f%e8%bd%bd%e6%b1%a0">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h4 id="安装l4lb">
 安装l4lb
 &lt;a class="anchor" href="#%e5%ae%89%e8%a3%85l4lb">#&lt;/a>
&lt;/h4>
&lt;h4 id="配置l4lb-rolesex-lbtemplatesl4lbconfj2">
 配置l4lb (roles/ex-lb/templates/l4lb.conf.j2)
 &lt;a class="anchor" href="#%e9%85%8d%e7%bd%ael4lb-rolesex-lbtemplatesl4lbconfj2">#&lt;/a>
&lt;/h4>
&lt;h5 id="配置由全局配置和三个upstream-servers配置组成">
 配置由全局配置和三个upstream servers配置组成：
 &lt;a class="anchor" href="#%e9%85%8d%e7%bd%ae%e7%94%b1%e5%85%a8%e5%b1%80%e9%85%8d%e7%bd%ae%e5%92%8c%e4%b8%89%e4%b8%aaupstream-servers%e9%85%8d%e7%bd%ae%e7%bb%84%e6%88%90">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="apiservers-用于转发至多个apiserver">
 apiservers 用于转发至多个apiserver
 &lt;a class="anchor" href="#apiservers-%e7%94%a8%e4%ba%8e%e8%bd%ac%e5%8f%91%e8%87%b3%e5%a4%9a%e4%b8%aaapiserver">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="ingress-nodes-用于转发至node节点的ingress-http服务参阅">
 ingress-nodes 用于转发至node节点的ingress http服务，&lt;a href="../op/loadballance_ingress_nodeport.md">参阅&lt;/a>
 &lt;a class="anchor" href="#ingress-nodes-%e7%94%a8%e4%ba%8e%e8%bd%ac%e5%8f%91%e8%87%b3node%e8%8a%82%e7%82%b9%e7%9a%84ingress-http%e6%9c%8d%e5%8a%a1%e5%8f%82%e9%98%85">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="ingress-tls-nodes-用于转发至node节点的ingress-https服务">
 ingress-tls-nodes 用于转发至node节点的ingress https服务
 &lt;a class="anchor" href="#ingress-tls-nodes-%e7%94%a8%e4%ba%8e%e8%bd%ac%e5%8f%91%e8%87%b3node%e8%8a%82%e7%82%b9%e7%9a%84ingress-https%e6%9c%8d%e5%8a%a1">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h4 id="安装keepalived">
 安装keepalived
 &lt;a class="anchor" href="#%e5%ae%89%e8%a3%85keepalived">#&lt;/a>
&lt;/h4>
&lt;h4 id="配置keepalived主节点-keepalived-masterconfj2">
 配置keepalived主节点 &lt;a href="../../roles/ex-lb/templates/keepalived-master.conf.j2">keepalived-master.conf.j2&lt;/a>
 &lt;a class="anchor" href="#%e9%85%8d%e7%bd%aekeepalived%e4%b8%bb%e8%8a%82%e7%82%b9-keepalived-masterconfj2">#&lt;/a>
&lt;/h4>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>global_defs &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>vrrp_track_process check-l4lb &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> process l4lb
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> weight -60
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> delay &lt;span style="color:#ae81ff">3&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>vrrp_instance VI-01 &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> state MASTER
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> priority &lt;span style="color:#ae81ff">120&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> unicast_src_ip &lt;span style="color:#f92672">{{&lt;/span> inventory_hostname &lt;span style="color:#f92672">}}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> unicast_peer &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">{&lt;/span>% &lt;span style="color:#66d9ef">for&lt;/span> h in groups&lt;span style="color:#f92672">[&lt;/span>&lt;span style="color:#e6db74">&amp;#39;ex_lb&amp;#39;&lt;/span>&lt;span style="color:#f92672">]&lt;/span> %&lt;span style="color:#f92672">}{&lt;/span>% &lt;span style="color:#66d9ef">if&lt;/span> h !&lt;span style="color:#f92672">=&lt;/span> inventory_hostname %&lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">{{&lt;/span> h &lt;span style="color:#f92672">}}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">{&lt;/span>% endif %&lt;span style="color:#f92672">}{&lt;/span>% endfor %&lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> dont_track_primary
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> interface &lt;span style="color:#f92672">{{&lt;/span> LB_IF &lt;span style="color:#f92672">}}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> virtual_router_id &lt;span style="color:#f92672">{{&lt;/span> ROUTER_ID &lt;span style="color:#f92672">}}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> advert_int &lt;span style="color:#ae81ff">3&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> track_process &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> check-l4lb
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> virtual_ipaddress &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">{{&lt;/span> EX_APISERVER_VIP &lt;span style="color:#f92672">}}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>
&lt;h5 id="vrrp_track_process-定义了监测l4lb进程是否存活如果进程不存在根据weight--60设置将主节点优先级降低60这样原先备节点将变成主节点">
 vrrp_track_process 定义了监测l4lb进程是否存活，如果进程不存在，根据&lt;code>weight -60&lt;/code>设置将主节点优先级降低60，这样原先备节点将变成主节点。
 &lt;a class="anchor" href="#vrrp_track_process-%e5%ae%9a%e4%b9%89%e4%ba%86%e7%9b%91%e6%b5%8bl4lb%e8%bf%9b%e7%a8%8b%e6%98%af%e5%90%a6%e5%ad%98%e6%b4%bb%e5%a6%82%e6%9e%9c%e8%bf%9b%e7%a8%8b%e4%b8%8d%e5%ad%98%e5%9c%a8%e6%a0%b9%e6%8d%aeweight--60%e8%ae%be%e7%bd%ae%e5%b0%86%e4%b8%bb%e8%8a%82%e7%82%b9%e4%bc%98%e5%85%88%e7%ba%a7%e9%99%8d%e4%bd%8e60%e8%bf%99%e6%a0%b7%e5%8e%9f%e5%85%88%e5%a4%87%e8%8a%82%e7%82%b9%e5%b0%86%e5%8f%98%e6%88%90%e4%b8%bb%e8%8a%82%e7%82%b9">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="vrrp_instance-定义了vrrp组包括优先级使用端口router_id心跳频率检测脚本虚拟地址vip等">
 vrrp_instance 定义了vrrp组，包括优先级、使用端口、router_id、心跳频率、检测脚本、虚拟地址VIP等
 &lt;a class="anchor" href="#vrrp_instance-%e5%ae%9a%e4%b9%89%e4%ba%86vrrp%e7%bb%84%e5%8c%85%e6%8b%ac%e4%bc%98%e5%85%88%e7%ba%a7%e4%bd%bf%e7%94%a8%e7%ab%af%e5%8f%a3router_id%e5%bf%83%e8%b7%b3%e9%a2%91%e7%8e%87%e6%a3%80%e6%b5%8b%e8%84%9a%e6%9c%ac%e8%99%9a%e6%8b%9f%e5%9c%b0%e5%9d%80vip%e7%ad%89">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="特别注意-virtual_router_id-标识了一个-vrrp组在同网段下必须唯一否则出现-keepalived_vrrp-bogus-vrrp-packet-received-on-eth0-类似报错">
 特别注意 &lt;code>virtual_router_id&lt;/code> 标识了一个 VRRP组，在同网段下必须唯一，否则出现 &lt;code>Keepalived_vrrp: bogus VRRP packet received on eth0 !!!&lt;/code>类似报错
 &lt;a class="anchor" href="#%e7%89%b9%e5%88%ab%e6%b3%a8%e6%84%8f-virtual_router_id-%e6%a0%87%e8%af%86%e4%ba%86%e4%b8%80%e4%b8%aa-vrrp%e7%bb%84%e5%9c%a8%e5%90%8c%e7%bd%91%e6%ae%b5%e4%b8%8b%e5%bf%85%e9%a1%bb%e5%94%af%e4%b8%80%e5%90%a6%e5%88%99%e5%87%ba%e7%8e%b0-keepalived_vrrp-bogus-vrrp-packet-received-on-eth0-%e7%b1%bb%e4%bc%bc%e6%8a%a5%e9%94%99">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="配置-vrrp-协议通过单播发送">
 配置 vrrp 协议通过单播发送
 &lt;a class="anchor" href="#%e9%85%8d%e7%bd%ae-vrrp-%e5%8d%8f%e8%ae%ae%e9%80%9a%e8%bf%87%e5%8d%95%e6%92%ad%e5%8f%91%e9%80%81">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h4 id="配置keepalived备节点-keepalived-backupconfj2">
 配置keepalived备节点 &lt;a href="../../roles/ex-lb/templates/keepalived-backup.conf.j2">keepalived-backup.conf.j2&lt;/a>
 &lt;a class="anchor" href="#%e9%85%8d%e7%bd%aekeepalived%e5%a4%87%e8%8a%82%e7%82%b9-keepalived-backupconfj2">#&lt;/a>
&lt;/h4>
&lt;ul>
&lt;li>
&lt;h5 id="备节点的配置类似主节点除了优先级和检测脚本其他如-virtual_router_id-advert_int-virtual_ipaddress必须与主节点一致">
 备节点的配置类似主节点，除了优先级和检测脚本，其他如 &lt;code>virtual_router_id&lt;/code> &lt;code>advert_int&lt;/code> &lt;code>virtual_ipaddress&lt;/code>必须与主节点一致
 &lt;a class="anchor" href="#%e5%a4%87%e8%8a%82%e7%82%b9%e7%9a%84%e9%85%8d%e7%bd%ae%e7%b1%bb%e4%bc%bc%e4%b8%bb%e8%8a%82%e7%82%b9%e9%99%a4%e4%ba%86%e4%bc%98%e5%85%88%e7%ba%a7%e5%92%8c%e6%a3%80%e6%b5%8b%e8%84%9a%e6%9c%ac%e5%85%b6%e4%bb%96%e5%a6%82-virtual_router_id-advert_int-virtual_ipaddress%e5%bf%85%e9%a1%bb%e4%b8%8e%e4%b8%bb%e8%8a%82%e7%82%b9%e4%b8%80%e8%87%b4">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h3 id="启动-keepalived-和-l4lb-后验证">
 启动 keepalived 和 l4lb 后验证
 &lt;a class="anchor" href="#%e5%90%af%e5%8a%a8-keepalived-%e5%92%8c-l4lb-%e5%90%8e%e9%aa%8c%e8%af%81">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>lb 节点验证&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>systemctl status l4lb 	&lt;span style="color:#75715e"># 检查进程状态&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>journalctl -u l4lb		&lt;span style="color:#75715e"># 检查进程日志是否有报错信息&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>systemctl status keepalived 	&lt;span style="color:#75715e"># 检查进程状态&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>journalctl -u keepalived	&lt;span style="color:#75715e"># 检查进程日志是否有报错信息&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>在 keepalived 主节点&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>ip a				&lt;span style="color:#75715e"># 检查 master的 VIP地址是否存在&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="keepalived-主备切换演练">
 keepalived 主备切换演练
 &lt;a class="anchor" href="#keepalived-%e4%b8%bb%e5%a4%87%e5%88%87%e6%8d%a2%e6%bc%94%e7%bb%83">#&lt;/a>
&lt;/h3>
&lt;ol>
&lt;li>
&lt;h5 id="尝试关闭-keepalived主节点上的-l4lb进程然后在keepalived-备节点上查看-master的-vip地址是否能够漂移过来并依次检查上一步中的验证项">
 尝试关闭 keepalived主节点上的 l4lb进程，然后在keepalived 备节点上查看 master的 VIP地址是否能够漂移过来，并依次检查上一步中的验证项。
 &lt;a class="anchor" href="#%e5%b0%9d%e8%af%95%e5%85%b3%e9%97%ad-keepalived%e4%b8%bb%e8%8a%82%e7%82%b9%e4%b8%8a%e7%9a%84-l4lb%e8%bf%9b%e7%a8%8b%e7%84%b6%e5%90%8e%e5%9c%a8keepalived-%e5%a4%87%e8%8a%82%e7%82%b9%e4%b8%8a%e6%9f%a5%e7%9c%8b-master%e7%9a%84-vip%e5%9c%b0%e5%9d%80%e6%98%af%e5%90%a6%e8%83%bd%e5%a4%9f%e6%bc%82%e7%a7%bb%e8%bf%87%e6%9d%a5%e5%b9%b6%e4%be%9d%e6%ac%a1%e6%a3%80%e6%9f%a5%e4%b8%8a%e4%b8%80%e6%ad%a5%e4%b8%ad%e7%9a%84%e9%aa%8c%e8%af%81%e9%a1%b9">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="尝试直接关闭-keepalived-主节点系统检查各验证项">
 尝试直接关闭 keepalived 主节点系统，检查各验证项。
 &lt;a class="anchor" href="#%e5%b0%9d%e8%af%95%e7%9b%b4%e6%8e%a5%e5%85%b3%e9%97%ad-keepalived-%e4%b8%bb%e8%8a%82%e7%82%b9%e7%b3%bb%e7%bb%9f%e6%a3%80%e6%9f%a5%e5%90%84%e9%aa%8c%e8%af%81%e9%a1%b9">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ol></description></item><item><title>2023-09-28 ezctl 命令行介绍</title><link>https://qq547475331.github.io/docs/ezctl-%E5%91%BD%E4%BB%A4%E8%A1%8C%E4%BB%8B%E7%BB%8D-ezctl-ming-ling-xing-jie-shao/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/ezctl-%E5%91%BD%E4%BB%A4%E8%A1%8C%E4%BB%8B%E7%BB%8D-ezctl-ming-ling-xing-jie-shao/</guid><description>&lt;h1 id="ezctl-命令行介绍">
 ezctl 命令行介绍
 &lt;a class="anchor" href="#ezctl-%e5%91%bd%e4%bb%a4%e8%a1%8c%e4%bb%8b%e7%bb%8d">#&lt;/a>
&lt;/h1>
&lt;h2 id="为什么使用-ezctl">
 为什么使用 ezctl
 &lt;a class="anchor" href="#%e4%b8%ba%e4%bb%80%e4%b9%88%e4%bd%bf%e7%94%a8-ezctl">#&lt;/a>
&lt;/h2>
&lt;p>kubeasz 项目使用ezctl 方便地创建和管理多个k8s 集群，ezctl 使用shell 脚本封装ansible-playbook 执行命令，它十分轻量、简单和易于扩展。&lt;/p>
&lt;h3 id="使用帮助">
 使用帮助
 &lt;a class="anchor" href="#%e4%bd%bf%e7%94%a8%e5%b8%ae%e5%8a%a9">#&lt;/a>
&lt;/h3>
&lt;p>随时运行 ezctl 获取命令行提示信息，如下&lt;/p>
&lt;pre tabindex="0">&lt;code>Usage: ezctl COMMAND [args]
-------------------------------------------------------------------------------------
Cluster setups:
 list		 to list all of the managed clusters
 checkout &amp;lt;cluster&amp;gt; to switch default kubeconfig of the cluster
 new &amp;lt;cluster&amp;gt; to start a new k8s deploy with name &amp;#39;cluster&amp;#39;
 setup &amp;lt;cluster&amp;gt; &amp;lt;step&amp;gt; to setup a cluster, also supporting a step-by-step way
 start &amp;lt;cluster&amp;gt; to start all of the k8s services stopped by &amp;#39;ezctl stop&amp;#39;
 stop &amp;lt;cluster&amp;gt; to stop all of the k8s services temporarily
 upgrade &amp;lt;cluster&amp;gt; to upgrade the k8s cluster
 destroy &amp;lt;cluster&amp;gt; to destroy the k8s cluster
 backup &amp;lt;cluster&amp;gt; to backup the cluster state (etcd snapshot)
 restore &amp;lt;cluster&amp;gt; to restore the cluster state from backups
 start-aio		 to quickly setup an all-in-one cluster with &amp;#39;default&amp;#39; settings

Cluster ops:
 add-etcd &amp;lt;cluster&amp;gt; &amp;lt;ip&amp;gt; to add a etcd-node to the etcd cluster
 add-master &amp;lt;cluster&amp;gt; &amp;lt;ip&amp;gt; to add a master node to the k8s cluster
 add-node &amp;lt;cluster&amp;gt; &amp;lt;ip&amp;gt; to add a work node to the k8s cluster
 del-etcd &amp;lt;cluster&amp;gt; &amp;lt;ip&amp;gt; to delete a etcd-node from the etcd cluster
 del-master &amp;lt;cluster&amp;gt; &amp;lt;ip&amp;gt; to delete a master node from the k8s cluster
 del-node &amp;lt;cluster&amp;gt; &amp;lt;ip&amp;gt; to delete a work node from the k8s cluster

Extra operation:
 kcfg-adm &amp;lt;cluster&amp;gt; &amp;lt;args&amp;gt; to manage client kubeconfig of the k8s cluster

Use &amp;#34;ezctl help &amp;lt;command&amp;gt;&amp;#34; for more information about a given command.

ezctl checkout
ezctl destroy 

ezctl setup k8s-01 all
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>命令集 1：集群安装相关操作
&lt;ul>
&lt;li>显示当前所有管理的集群&lt;/li>
&lt;li>切换默认集群&lt;/li>
&lt;li>创建新集群配置&lt;/li>
&lt;li>安装新集群&lt;/li>
&lt;li>启动临时停止的集群&lt;/li>
&lt;li>临时停止某个集群（包括集群内运行的pod）&lt;/li>
&lt;li>升级集群k8s组件版本&lt;/li>
&lt;li>删除集群&lt;/li>
&lt;li>备份集群（仅etcd数据，不包括pv数据和业务应用数据）&lt;/li>
&lt;li>从备份中恢复集群&lt;/li>
&lt;li>创建单机集群（类似 minikube）&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>命令集 2：集群节点操作
&lt;ul>
&lt;li>增加 etcd 节点&lt;/li>
&lt;li>增加主节点&lt;/li>
&lt;li>增加工作节点&lt;/li>
&lt;li>删除 etcd 节点&lt;/li>
&lt;li>删除主节点&lt;/li>
&lt;li>删除工作节点&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>命令集3：额外操作
&lt;ul>
&lt;li>管理客户端kubeconfig&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="举例创建安装新集群流程">
 举例创建、安装新集群流程
 &lt;a class="anchor" href="#%e4%b8%be%e4%be%8b%e5%88%9b%e5%bb%ba%e5%ae%89%e8%a3%85%e6%96%b0%e9%9b%86%e7%be%a4%e6%b5%81%e7%a8%8b">#&lt;/a>
&lt;/h4>
&lt;ul>
&lt;li>1.首先创建集群配置实例&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>~# ezctl new k8s-01
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>2021-01-19 10:48:23 DEBUG generate custom cluster files in /etc/kubeasz/clusters/k8s-01
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>2021-01-19 10:48:23 DEBUG set version of common plugins
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>2021-01-19 10:48:23 DEBUG cluster k8s-01: files successfully created.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>2021-01-19 10:48:23 INFO next steps 1: to config &lt;span style="color:#e6db74">&amp;#39;/etc/kubeasz/clusters/k8s-01/hosts&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>2021-01-19 10:48:23 INFO next steps 2: to config &lt;span style="color:#e6db74">&amp;#39;/etc/kubeasz/clusters/k8s-01/config.yml&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>然后根据提示配置&amp;rsquo;/etc/kubeasz/clusters/k8s-01/hosts&amp;rsquo; 和 &amp;lsquo;/etc/kubeasz/clusters/k8s-01/config.yml&amp;rsquo;；为方便测试我们在hosts里面设置单节点集群（etcd/kube_master/kube_node配置同一个节点，注意节点需先设置ssh免密码登陆）, config.yml 使用默认配置即可。&lt;/p></description></item><item><title>2023-09-28 kube-router 网络组件</title><link>https://qq547475331.github.io/docs/kube-router-%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6-kube-router-wang-luo-zu-jian/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/kube-router-%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6-kube-router-wang-luo-zu-jian/</guid><description>&lt;h1 id="kube-router-网络组件">
 kube-router 网络组件
 &lt;a class="anchor" href="#kube-router-%e7%bd%91%e7%bb%9c%e7%bb%84%e4%bb%b6">#&lt;/a>
&lt;/h1>
&lt;h5 id="以下文档暂未更新以插件官网文档为准">
 (以下文档暂未更新，以插件官网文档为准)
 &lt;a class="anchor" href="#%e4%bb%a5%e4%b8%8b%e6%96%87%e6%a1%a3%e6%9a%82%e6%9c%aa%e6%9b%b4%e6%96%b0%e4%bb%a5%e6%8f%92%e4%bb%b6%e5%ae%98%e7%bd%91%e6%96%87%e6%a1%a3%e4%b8%ba%e5%87%86">#&lt;/a>
&lt;/h5>
&lt;h5 id="kube-router是一个简单高效的网络插件它提供一揽子解决方案">
 kube-router是一个简单、高效的网络插件，它提供一揽子解决方案：
 &lt;a class="anchor" href="#kube-router%e6%98%af%e4%b8%80%e4%b8%aa%e7%ae%80%e5%8d%95%e9%ab%98%e6%95%88%e7%9a%84%e7%bd%91%e7%bb%9c%e6%8f%92%e4%bb%b6%e5%ae%83%e6%8f%90%e4%be%9b%e4%b8%80%e6%8f%bd%e5%ad%90%e8%a7%a3%e5%86%b3%e6%96%b9%e6%a1%88">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="基于gobgp-提供pod-网络互联routing">
 基于GoBGP 提供Pod 网络互联（Routing）
 &lt;a class="anchor" href="#%e5%9f%ba%e4%ba%8egobgp-%e6%8f%90%e4%be%9bpod-%e7%bd%91%e7%bb%9c%e4%ba%92%e8%81%94routing">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="使用ipsets优化的iptables-提供网络策略支持firewallnetworkpolicy">
 使用ipsets优化的iptables 提供网络策略支持（Firewall/NetworkPolicy）
 &lt;a class="anchor" href="#%e4%bd%bf%e7%94%a8ipsets%e4%bc%98%e5%8c%96%e7%9a%84iptables-%e6%8f%90%e4%be%9b%e7%bd%91%e7%bb%9c%e7%ad%96%e7%95%a5%e6%94%af%e6%8c%81firewallnetworkpolicy">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="基于ipvslvs-提供高性能服务代理service-proxy注由于-k8s-新版本中-ipvs-已可用因此这里不选择启用kube-router基于ipvs的service-proxy">
 基于IPVS/LVS 提供高性能服务代理（Service Proxy）(注：由于 k8s 新版本中 ipvs 已可用，因此这里不选择启用kube-router基于ipvs的service proxy)
 &lt;a class="anchor" href="#%e5%9f%ba%e4%ba%8eipvslvs-%e6%8f%90%e4%be%9b%e9%ab%98%e6%80%a7%e8%83%bd%e6%9c%8d%e5%8a%a1%e4%bb%a3%e7%90%86service-proxy%e6%b3%a8%e7%94%b1%e4%ba%8e-k8s-%e6%96%b0%e7%89%88%e6%9c%ac%e4%b8%ad-ipvs-%e5%b7%b2%e5%8f%af%e7%94%a8%e5%9b%a0%e6%ad%a4%e8%bf%99%e9%87%8c%e4%b8%8d%e9%80%89%e6%8b%a9%e5%90%af%e7%94%a8kube-router%e5%9f%ba%e4%ba%8eipvs%e7%9a%84service-proxy">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h5 id="更多介绍请前往httpsgithubcomcloudnativelabskube-router">
 更多介绍请前往&lt;code>https://github.com/cloudnativelabs/kube-router&lt;/code>
 &lt;a class="anchor" href="#%e6%9b%b4%e5%a4%9a%e4%bb%8b%e7%bb%8d%e8%af%b7%e5%89%8d%e5%be%80httpsgithubcomcloudnativelabskube-router">#&lt;/a>
&lt;/h5>
&lt;h2 id="配置">
 配置
 &lt;a class="anchor" href="#%e9%85%8d%e7%bd%ae">#&lt;/a>
&lt;/h2>
&lt;h5 id="本项目提供多种网络插件可选如果需要安装kube-router请在etckubeaszhosts文件中设置变量-cluster_networkkube-router更多设置请查看roleskube-routerdefaultsmainyml">
 本项目提供多种网络插件可选，如果需要安装kube-router，请在/etc/kubeasz/hosts文件中设置变量 &lt;code>CLUSTER_NETWORK=&amp;quot;kube-router&amp;quot;&lt;/code>，更多设置请查看&lt;code>roles/kube-router/defaults/main.yml&lt;/code>
 &lt;a class="anchor" href="#%e6%9c%ac%e9%a1%b9%e7%9b%ae%e6%8f%90%e4%be%9b%e5%a4%9a%e7%a7%8d%e7%bd%91%e7%bb%9c%e6%8f%92%e4%bb%b6%e5%8f%af%e9%80%89%e5%a6%82%e6%9e%9c%e9%9c%80%e8%a6%81%e5%ae%89%e8%a3%85kube-router%e8%af%b7%e5%9c%a8etckubeaszhosts%e6%96%87%e4%bb%b6%e4%b8%ad%e8%ae%be%e7%bd%ae%e5%8f%98%e9%87%8f-cluster_networkkube-router%e6%9b%b4%e5%a4%9a%e8%ae%be%e7%bd%ae%e8%af%b7%e6%9f%a5%e7%9c%8broleskube-routerdefaultsmainyml">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="kube-router需要在所有master节点和node节点安装">
 kube-router需要在所有master节点和node节点安装
 &lt;a class="anchor" href="#kube-router%e9%9c%80%e8%a6%81%e5%9c%a8%e6%89%80%e6%9c%89master%e8%8a%82%e7%82%b9%e5%92%8cnode%e8%8a%82%e7%82%b9%e5%ae%89%e8%a3%85">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h2 id="安装">
 安装
 &lt;a class="anchor" href="#%e5%ae%89%e8%a3%85">#&lt;/a>
&lt;/h2>
&lt;ul>
&lt;li>
&lt;h5 id="单步安装已经集成ansible-playbook-90setupyml">
 单步安装已经集成：&lt;code>ansible-playbook 90.setup.yml&lt;/code>
 &lt;a class="anchor" href="#%e5%8d%95%e6%ad%a5%e5%ae%89%e8%a3%85%e5%b7%b2%e7%bb%8f%e9%9b%86%e6%88%90ansible-playbook-90setupyml">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="分步安装请执行ansible-playbook-06networkyml">
 分步安装请执行：&lt;code>ansible-playbook 06.network.yml&lt;/code>
 &lt;a class="anchor" href="#%e5%88%86%e6%ad%a5%e5%ae%89%e8%a3%85%e8%af%b7%e6%89%a7%e8%a1%8cansible-playbook-06networkyml">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h2 id="验证">
 验证
 &lt;a class="anchor" href="#%e9%aa%8c%e8%af%81">#&lt;/a>
&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>1.pod间网络联通性：略&lt;/p>
&lt;/li>
&lt;li>
&lt;p>2.host路由表&lt;/p>
&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># master上路由&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>root@master1:~$ ip route
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>172.20.1.0/24 via 192.168.1.2 dev ens3 proto &lt;span style="color:#ae81ff">17&lt;/span> 
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>172.20.2.0/24 via 192.168.1.3 dev ens3 proto &lt;span style="color:#ae81ff">17&lt;/span> 
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># node3上路由&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>root@node3:~$ ip route
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>... 
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>172.20.0.0/24 via 192.168.1.1 dev ens3 proto &lt;span style="color:#ae81ff">17&lt;/span> 
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>172.20.1.0/24 via 192.168.1.2 dev ens3 proto &lt;span style="color:#ae81ff">17&lt;/span> 
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>172.20.2.0/24 dev kube-bridge proto kernel scope link src 172.20.2.1 
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>3.bgp连接状态&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># master上&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>root@master1:~$ netstat -antlp|grep router|grep LISH|grep &lt;span style="color:#ae81ff">179&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>tcp &lt;span style="color:#ae81ff">0&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span> 192.168.1.1:179 192.168.1.3:58366 ESTABLISHED 26062/kube-router
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>tcp &lt;span style="color:#ae81ff">0&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span> 192.168.1.1:42537 192.168.1.2:179 ESTABLISHED 26062/kube-router
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># node3上&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>root@node3:~$ netstat -antlp|grep router|grep LISH|grep &lt;span style="color:#ae81ff">179&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>tcp &lt;span style="color:#ae81ff">0&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span> 192.168.1.3:58366 192.168.1.1:179 ESTABLISHED 18897/kube-router
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>tcp &lt;span style="color:#ae81ff">0&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span> 192.168.1.3:179 192.168.1.2:43928 ESTABLISHED 18897/kube-router
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>
&lt;p>4.NetworkPolicy有效性，验证参照&lt;a href="../../guide/networkpolicy.md">这里&lt;/a>&lt;/p></description></item><item><title>2023-09-28 network-check</title><link>https://qq547475331.github.io/docs/network-check-network-check/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/network-check-network-check/</guid><description>&lt;h1 id="network-check">
 network-check
 &lt;a class="anchor" href="#network-check">#&lt;/a>
&lt;/h1>
&lt;p>网络测试组件，根据cilium connectivity-check 脚本修改而来；利用cronjob 定期检测集群各节点、容器、serviceip、nodeport等之间的网络联通性；可以方便的判断当前集群网络是否正常。&lt;/p>
&lt;p>目前检测如下：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl get cronjobs.batch -n network-test
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>test01-pod-to-container */5 * * * * False &lt;span style="color:#ae81ff">0&lt;/span> 3m19s 6d3h
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>test02-pod-to-node-nodeport */5 * * * * False &lt;span style="color:#ae81ff">0&lt;/span> 3m19s 6d3h
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>test03-pod-to-multi-node-clusterip */5 * * * * False &lt;span style="color:#ae81ff">1&lt;/span> 6d3h 6d3h
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>test04-pod-to-multi-node-headless */5 * * * * False &lt;span style="color:#ae81ff">1&lt;/span> 6d3h 6d3h
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>test05-pod-to-multi-node-nodeport */5 * * * * False &lt;span style="color:#ae81ff">1&lt;/span> 6d3h 6d3h
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>test06-pod-to-external-1111 */5 * * * * False &lt;span style="color:#ae81ff">0&lt;/span> 3m19s 6d3h
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>test07-pod-to-external-fqdn-baidu */5 * * * * False &lt;span style="color:#ae81ff">0&lt;/span> 3m19s 6d3h
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>test08-host-to-multi-node-clusterip */5 * * * * False &lt;span style="color:#ae81ff">1&lt;/span> 6d3h 6d3h
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>test09-host-to-multi-node-headless */5 * * * * False &lt;span style="color:#ae81ff">1&lt;/span> 6d3h 6d3h
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>带&lt;code>multi-node&lt;/code>的测试需要多节点集群才能运行，如果单节点集群，测试pod会处于&lt;code>Pending&lt;/code>状态&lt;/li>
&lt;li>带&lt;code>external&lt;/code>的测试需要节点能够访问互联网，否则测试会失败&lt;/li>
&lt;/ul>
&lt;h2 id="启用网络检测">
 启用网络检测
 &lt;a class="anchor" href="#%e5%90%af%e7%94%a8%e7%bd%91%e7%bb%9c%e6%a3%80%e6%b5%8b">#&lt;/a>
&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>下载额外容器镜像 &lt;code>./ezdown -X network-check&lt;/code>&lt;/p></description></item><item><title>2023-09-28 个性化集群参数配置</title><link>https://qq547475331.github.io/docs/%E4%B8%AA%E6%80%A7%E5%8C%96%E9%9B%86%E7%BE%A4%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE-ge-xing-hua-ji-qun-can-shu-pei-zhi/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/%E4%B8%AA%E6%80%A7%E5%8C%96%E9%9B%86%E7%BE%A4%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE-ge-xing-hua-ji-qun-can-shu-pei-zhi/</guid><description>&lt;h1 id="个性化集群参数配置">
 个性化集群参数配置
 &lt;a class="anchor" href="#%e4%b8%aa%e6%80%a7%e5%8c%96%e9%9b%86%e7%be%a4%e5%8f%82%e6%95%b0%e9%85%8d%e7%bd%ae">#&lt;/a>
&lt;/h1>
&lt;p>&lt;code>kubeasz&lt;/code>创建集群主要在以下两个地方进行配置：(假设集群名xxxx)&lt;/p>
&lt;ul>
&lt;li>clusters/xxxx/hosts 文件（模板在example/hosts.multi-node）：集群主要节点定义和主要参数配置、全局变量&lt;/li>
&lt;li>clusters/xxxx/config.yml（模板在examples/config.yml）：其他参数配置或者部分组件附加参数&lt;/li>
&lt;/ul>
&lt;h2 id="clustersxxxxhosts-ansible-hosts">
 clusters/xxxx/hosts (ansible hosts)
 &lt;a class="anchor" href="#clustersxxxxhosts-ansible-hosts">#&lt;/a>
&lt;/h2>
&lt;p>如&lt;a href="00-planning_and_overall_intro.md">集群规划与安装概览&lt;/a>中介绍，主要包括集群节点定义和集群范围的主要参数配置&lt;/p>
&lt;ul>
&lt;li>尽量保持配置简单灵活&lt;/li>
&lt;li>尽量保持配置项稳定&lt;/li>
&lt;/ul>
&lt;p>常用设置项：&lt;/p>
&lt;ul>
&lt;li>修改容器运行时: CONTAINER_RUNTIME=&amp;ldquo;containerd&amp;rdquo;&lt;/li>
&lt;li>修改集群网络插件：CLUSTER_NETWORK=&amp;ldquo;calico&amp;rdquo;&lt;/li>
&lt;li>修改容器网络地址：CLUSTER_CIDR=&amp;ldquo;192.168.0.0/16&amp;rdquo;&lt;/li>
&lt;li>修改NodePort范围：NODE_PORT_RANGE=&amp;ldquo;30000-32767&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;h2 id="clustersxxxxconfigyml">
 clusters/xxxx/config.yml
 &lt;a class="anchor" href="#clustersxxxxconfigyml">#&lt;/a>
&lt;/h2>
&lt;p>主要包括集群某个具体组件的个性化配置，具体组件的配置项可能会不断增加；可以在不做任何配置更改情况下使用默认值创建集群&lt;/p>
&lt;p>根据实际需要配置 k8s 集群，常用举例&lt;/p>
&lt;ul>
&lt;li>配置使用离线安装系统包：INSTALL_SOURCE: &amp;ldquo;offline&amp;rdquo; （需要ezdown -P 下载离线系统软件）&lt;/li>
&lt;li>配置CA证书以及其签发证书的有效期&lt;/li>
&lt;li>配置 apiserver 支持公网域名：MASTER_CERT_HOSTS&lt;/li>
&lt;li>配置 cluster-addon 组件安装&lt;/li>
&lt;li>&amp;hellip;&lt;/li>
&lt;/ul></description></item><item><title>2023-09-28 公有云上部署</title><link>https://qq547475331.github.io/docs/%E5%85%AC%E6%9C%89%E4%BA%91%E4%B8%8A%E9%83%A8%E7%BD%B2-kubeasz-gong-you-yun-shang-bu-shu-kubeasz/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/%E5%85%AC%E6%9C%89%E4%BA%91%E4%B8%8A%E9%83%A8%E7%BD%B2-kubeasz-gong-you-yun-shang-bu-shu-kubeasz/</guid><description>&lt;h1 id="公有云上部署-kubeasz">
 公有云上部署 kubeasz
 &lt;a class="anchor" href="#%e5%85%ac%e6%9c%89%e4%ba%91%e4%b8%8a%e9%83%a8%e7%bd%b2-kubeasz">#&lt;/a>
&lt;/h1>
&lt;p>在公有云上使用&lt;code>kubeasz&lt;/code>部署&lt;code>k8s&lt;/code>集群需要注意以下几个常见问题。&lt;/p>
&lt;h3 id="安全组">
 安全组
 &lt;a class="anchor" href="#%e5%ae%89%e5%85%a8%e7%bb%84">#&lt;/a>
&lt;/h3>
&lt;p>注意虚机的安全组规则配置，一般集群内部节点之间端口全部放开即可；&lt;/p>
&lt;h3 id="网络组件">
 网络组件
 &lt;a class="anchor" href="#%e7%bd%91%e7%bb%9c%e7%bb%84%e4%bb%b6">#&lt;/a>
&lt;/h3>
&lt;p>一般公有云对网络限制较多，跨节点 pod 通讯需要使用 OVERLAY 添加报头；默认配置详见example/config.yml&lt;/p>
&lt;ul>
&lt;li>flannel 使用 vxlan 模式：&lt;code>FLANNEL_BACKEND: &amp;quot;vxlan&amp;quot;&lt;/code>&lt;/li>
&lt;li>calico 开启 ipinip：&lt;code>CALICO_IPV4POOL_IPIP: &amp;quot;Always&amp;quot;&lt;/code>&lt;/li>
&lt;li>kube-router 开启 ipinip：&lt;code>OVERLAY_TYPE: &amp;quot;full&amp;quot;&lt;/code>&lt;/li>
&lt;/ul>
&lt;h3 id="节点公网访问">
 节点公网访问
 &lt;a class="anchor" href="#%e8%8a%82%e7%82%b9%e5%85%ac%e7%bd%91%e8%ae%bf%e9%97%ae">#&lt;/a>
&lt;/h3>
&lt;p>可以在安装时每个节点绑定&lt;code>弹性公网地址&lt;/code>(EIP)，装完集群解绑；也可以开通NAT网关，或者利用iptables自建上网网关等方式&lt;/p>
&lt;h3 id="负载均衡">
 负载均衡
 &lt;a class="anchor" href="#%e8%b4%9f%e8%bd%bd%e5%9d%87%e8%a1%a1">#&lt;/a>
&lt;/h3>
&lt;p>一般云厂商会限制使用&lt;code>keepalived+haproxy&lt;/code>自建负载均衡，你可以根据云厂商文档使用云负载均衡（内网）四层TCP负载模式；&lt;/p>
&lt;ul>
&lt;li>kubeasz 2x 版本已无需依赖外部负载均衡实现apiserver的高可用，详见 &lt;a href="https://github.com/easzlab/kubeasz/blob/dev2/docs/setup/00-planning_and_overall_intro.md#ha-architecture">2x架构&lt;/a>&lt;/li>
&lt;li>kubeasz 1x 及以前版本需要负载均衡实现apiserver高可用，详见 &lt;a href="https://github.com/easzlab/kubeasz/blob/dev1/docs/setup/00-planning_and_overall_intro.md#ha-architecture">1x架构&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="时间同步">
 时间同步
 &lt;a class="anchor" href="#%e6%97%b6%e9%97%b4%e5%90%8c%e6%ad%a5">#&lt;/a>
&lt;/h3>
&lt;p>一般云厂商提供的虚机都已默认安装时间同步服务，无需自行安装。&lt;/p>
&lt;h3 id="访问-apiserver">
 访问 APISERVER
 &lt;a class="anchor" href="#%e8%ae%bf%e9%97%ae-apiserver">#&lt;/a>
&lt;/h3>
&lt;p>在公有云上安装完集群后，需要在公网访问集群 apiserver，而我们在安装前可能没有规划公网IP或者公网域名；而 apiserver 肯定需要 https 方式访问，在证书创建时需要加入公网ip/域名；可以参考这里&lt;a href="../op/ch_apiserver_cert.md">修改 APISERVER（MASTER）证书&lt;/a>&lt;/p>
&lt;h2 id="在公有云上部署多主高可用集群">
 在公有云上部署多主高可用集群
 &lt;a class="anchor" href="#%e5%9c%a8%e5%85%ac%e6%9c%89%e4%ba%91%e4%b8%8a%e9%83%a8%e7%bd%b2%e5%a4%9a%e4%b8%bb%e9%ab%98%e5%8f%af%e7%94%a8%e9%9b%86%e7%be%a4">#&lt;/a>
&lt;/h2>
&lt;p>处理好以上讨论的常见问题后，在公有云上使用 kubeasz 安装集群与自有环境没有差异。&lt;/p>
&lt;ul>
&lt;li>使用 kubeasz 2x 版本安装单节点、单主多节点、多主多节点 k8s 集群，云上云下的预期安装体验完全一致&lt;/li>
&lt;/ul></description></item><item><title>2023-09-28 多架构支持</title><link>https://qq547475331.github.io/docs/%E5%A4%9A%E6%9E%B6%E6%9E%84%E6%94%AF%E6%8C%81-duo-jia-gou-zhi-chi/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/%E5%A4%9A%E6%9E%B6%E6%9E%84%E6%94%AF%E6%8C%81-duo-jia-gou-zhi-chi/</guid><description>&lt;h1 id="多架构支持">
 多架构支持
 &lt;a class="anchor" href="#%e5%a4%9a%e6%9e%b6%e6%9e%84%e6%94%af%e6%8c%81">#&lt;/a>
&lt;/h1>
&lt;h5 id="kubeasz-341-以后支持多cpu架构当前已支持linux-amd64和linux-arm64更多架构支持根据后续需求来计划">
 kubeasz 3.4.1 以后支持多CPU架构，当前已支持linux amd64和linux arm64，更多架构支持根据后续需求来计划。
 &lt;a class="anchor" href="#kubeasz-341-%e4%bb%a5%e5%90%8e%e6%94%af%e6%8c%81%e5%a4%9acpu%e6%9e%b6%e6%9e%84%e5%bd%93%e5%89%8d%e5%b7%b2%e6%94%af%e6%8c%81linux-amd64%e5%92%8clinux-arm64%e6%9b%b4%e5%a4%9a%e6%9e%b6%e6%9e%84%e6%94%af%e6%8c%81%e6%a0%b9%e6%8d%ae%e5%90%8e%e7%bb%ad%e9%9c%80%e6%b1%82%e6%9d%a5%e8%ae%a1%e5%88%92">#&lt;/a>
&lt;/h5>
&lt;h2 id="使用方式">
 使用方式
 &lt;a class="anchor" href="#%e4%bd%bf%e7%94%a8%e6%96%b9%e5%bc%8f">#&lt;/a>
&lt;/h2>
&lt;h5 id="kubeasz-多架构安装逻辑根据部署机器执行ezdownezctl命令的机器的架构会自动判断下载对应amd64arm64的二进制文件和容器镜像然后推送安装到整个集群">
 kubeasz 多架构安装逻辑：根据部署机器（执行ezdown/ezctl命令的机器）的架构，会自动判断下载对应amd64/arm64的二进制文件和容器镜像，然后推送安装到整个集群。
 &lt;a class="anchor" href="#kubeasz-%e5%a4%9a%e6%9e%b6%e6%9e%84%e5%ae%89%e8%a3%85%e9%80%bb%e8%be%91%e6%a0%b9%e6%8d%ae%e9%83%a8%e7%bd%b2%e6%9c%ba%e5%99%a8%e6%89%a7%e8%a1%8cezdownezctl%e5%91%bd%e4%bb%a4%e7%9a%84%e6%9c%ba%e5%99%a8%e7%9a%84%e6%9e%b6%e6%9e%84%e4%bc%9a%e8%87%aa%e5%8a%a8%e5%88%a4%e6%96%ad%e4%b8%8b%e8%bd%bd%e5%af%b9%e5%ba%94amd64arm64%e7%9a%84%e4%ba%8c%e8%bf%9b%e5%88%b6%e6%96%87%e4%bb%b6%e5%92%8c%e5%ae%b9%e5%99%a8%e9%95%9c%e5%83%8f%e7%84%b6%e5%90%8e%e6%8e%a8%e9%80%81%e5%ae%89%e8%a3%85%e5%88%b0%e6%95%b4%e4%b8%aa%e9%9b%86%e7%be%a4">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="暂不支持不同架构的机器加入到同一个集群">
 暂不支持不同架构的机器加入到同一个集群。
 &lt;a class="anchor" href="#%e6%9a%82%e4%b8%8d%e6%94%af%e6%8c%81%e4%b8%8d%e5%90%8c%e6%9e%b6%e6%9e%84%e7%9a%84%e6%9c%ba%e5%99%a8%e5%8a%a0%e5%85%a5%e5%88%b0%e5%90%8c%e4%b8%80%e4%b8%aa%e9%9b%86%e7%be%a4">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="harbor目前仅支持amd64安装">
 harbor目前仅支持amd64安装
 &lt;a class="anchor" href="#harbor%e7%9b%ae%e5%89%8d%e4%bb%85%e6%94%af%e6%8c%81amd64%e5%ae%89%e8%a3%85">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h2 id="架构支持备忘">
 架构支持备忘
 &lt;a class="anchor" href="#%e6%9e%b6%e6%9e%84%e6%94%af%e6%8c%81%e5%a4%87%e5%bf%98">#&lt;/a>
&lt;/h2>
&lt;h4 id="k8s核心组件本身提供多架构的二进制文件容器镜像下载项目调整了下载二进制文件的容器dockerfile">
 k8s核心组件本身提供多架构的二进制文件/容器镜像下载，项目调整了下载二进制文件的容器dockerfile
 &lt;a class="anchor" href="#k8s%e6%a0%b8%e5%bf%83%e7%bb%84%e4%bb%b6%e6%9c%ac%e8%ba%ab%e6%8f%90%e4%be%9b%e5%a4%9a%e6%9e%b6%e6%9e%84%e7%9a%84%e4%ba%8c%e8%bf%9b%e5%88%b6%e6%96%87%e4%bb%b6%e5%ae%b9%e5%99%a8%e9%95%9c%e5%83%8f%e4%b8%8b%e8%bd%bd%e9%a1%b9%e7%9b%ae%e8%b0%83%e6%95%b4%e4%ba%86%e4%b8%8b%e8%bd%bd%e4%ba%8c%e8%bf%9b%e5%88%b6%e6%96%87%e4%bb%b6%e7%9a%84%e5%ae%b9%e5%99%a8dockerfile">#&lt;/a>
&lt;/h4>
&lt;ul>
&lt;li>&lt;a href="https://github.com/easzlab/dockerfile-kubeasz-k8s-bin">https://github.com/easzlab/dockerfile-kubeasz-k8s-bin&lt;/a>&lt;/li>
&lt;/ul>
&lt;h4 id="kubeasz其他用到的二进制或镜像重新调整了容器创建dockerfile">
 kubeasz其他用到的二进制或镜像，重新调整了容器创建dockerfile
 &lt;a class="anchor" href="#kubeasz%e5%85%b6%e4%bb%96%e7%94%a8%e5%88%b0%e7%9a%84%e4%ba%8c%e8%bf%9b%e5%88%b6%e6%88%96%e9%95%9c%e5%83%8f%e9%87%8d%e6%96%b0%e8%b0%83%e6%95%b4%e4%ba%86%e5%ae%b9%e5%99%a8%e5%88%9b%e5%bb%badockerfile">#&lt;/a>
&lt;/h4>
&lt;ul>
&lt;li>&lt;a href="https://github.com/easzlab/dockerfile-kubeasz-ext-bin">https://github.com/easzlab/dockerfile-kubeasz-ext-bin&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/easzlab/dockerfile-kubeasz-ext-build">https://github.com/easzlab/dockerfile-kubeasz-ext-build&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/easzlab/dockerfile-kubeasz-sys-pkg">https://github.com/easzlab/dockerfile-kubeasz-sys-pkg&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/easzlab/dockerfile-kubeasz-mirrored-images">https://github.com/easzlab/dockerfile-kubeasz-mirrored-images&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/easzlab/dockerfile-kubeasz">https://github.com/easzlab/dockerfile-kubeasz&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/easzlab/dockerfile-ansible">https://github.com/easzlab/dockerfile-ansible&lt;/a>&lt;/li>
&lt;/ul>
&lt;h4 id="其他组件corednsnetwork-plugindashboardmetrics-server等一般都提供多架构的容器镜像可以直接下载拉取">
 其他组件(coredns/network plugin/dashboard/metrics-server等)一般都提供多架构的容器镜像，可以直接下载拉取
 &lt;a class="anchor" href="#%e5%85%b6%e4%bb%96%e7%bb%84%e4%bb%b6corednsnetwork-plugindashboardmetrics-server%e7%ad%89%e4%b8%80%e8%88%ac%e9%83%bd%e6%8f%90%e4%be%9b%e5%a4%9a%e6%9e%b6%e6%9e%84%e7%9a%84%e5%ae%b9%e5%99%a8%e9%95%9c%e5%83%8f%e5%8f%af%e4%bb%a5%e7%9b%b4%e6%8e%a5%e4%b8%8b%e8%bd%bd%e6%8b%89%e5%8f%96">#&lt;/a>
&lt;/h4></description></item><item><title>2023-09-28 开始使用 cilium</title><link>https://qq547475331.github.io/docs/%E5%BC%80%E5%A7%8B%E4%BD%BF%E7%94%A8-cilium-kai-shi-shi-yong-cilium/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/%E5%BC%80%E5%A7%8B%E4%BD%BF%E7%94%A8-cilium-kai-shi-shi-yong-cilium/</guid><description>&lt;h2 id="开始使用-cilium">
 开始使用 cilium
 &lt;a class="anchor" href="#%e5%bc%80%e5%a7%8b%e4%bd%bf%e7%94%a8-cilium">#&lt;/a>
&lt;/h2>
&lt;p>以下为简要翻译 &lt;code>cilium doc&lt;/code>上的一个应用示例&lt;a href="https://docs.cilium.io/en/stable/gettingstarted/http/">原文&lt;/a>，部署在单节点k8s 环境的实践。&lt;/p>
&lt;h3 id="部署示例应用">
 部署示例应用
 &lt;a class="anchor" href="#%e9%83%a8%e7%bd%b2%e7%a4%ba%e4%be%8b%e5%ba%94%e7%94%a8">#&lt;/a>
&lt;/h3>
&lt;p>官方文档用几个&lt;code>pod/svc&lt;/code> 抽象一个有趣的应用场景（星战迷）：星战中帝国方建造了被称为“终极武器”的“死星”，它是一个卫星大小的战斗空间站，它的核心是使用凯伯晶体（Kyber Crystal）的超级激光炮，剧中它的首秀就以完全火力摧毁了“杰达圣城”（Jedha）。下面将用运行于 k8s上的 pod/svc/cilium 等模拟“死星“的一个“飞船登陆”系统安全策略设计。&lt;/p>
&lt;ul>
&lt;li>deploy/deathstar：作为控制整个“死星”的飞船登陆管理系统，它暴露一个SVC，提供HTTP REST 接口给飞船请求登陆使用；&lt;/li>
&lt;li>pod/tiefighter：作为“帝国”方的常规战斗飞船，它会调用上述 HTTP 接口，请求登陆“死星”；&lt;/li>
&lt;li>pod/xwing：作为“盟军”方的飞行舰，它也尝试调用 HTTP 接口，请求登陆“死星”；&lt;/li>
&lt;/ul>
&lt;!-- raw HTML omitted -->
&lt;p>根据文件&lt;a href="../../../roles/cilium/files/star_war_example/http-sw-app.yaml">http-sw-app.yaml&lt;/a> 创建 &lt;code>$ kubectl create -f http-sw-app.yaml&lt;/code> 后，验证如下：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl get pods,svc
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME READY STATUS RESTARTS AGE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>pod/deathstar-5fc7c7795d-djf2q 1/1 Running &lt;span style="color:#ae81ff">0&lt;/span> 4h
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>pod/deathstar-5fc7c7795d-hrgst 1/1 Running &lt;span style="color:#ae81ff">0&lt;/span> 4h
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>pod/tiefighter 1/1 Running &lt;span style="color:#ae81ff">0&lt;/span> 4h
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>pod/xwing 1/1 Running &lt;span style="color:#ae81ff">0&lt;/span> 4h
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME TYPE CLUSTER-IP EXTERNAL-IP PORT&lt;span style="color:#f92672">(&lt;/span>S&lt;span style="color:#f92672">)&lt;/span> AGE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>service/deathstar ClusterIP 10.68.242.130 &amp;lt;none&amp;gt; 80/TCP 4h
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>service/kubernetes ClusterIP 10.68.0.1 &amp;lt;none&amp;gt; 443/TCP 5h
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>每个 POD 在 &lt;code>cilium&lt;/code> 中都表示为 &lt;code>Endpoint&lt;/code>，初始每个 &lt;code>Endpoint&lt;/code> 的”进出安全策略“状态均为 &lt;code>Disabled&lt;/code>，如下：(已省略部分无关 POD 信息)&lt;/p></description></item><item><title>2023-09-28 快速指南</title><link>https://qq547475331.github.io/docs/%E5%BF%AB%E9%80%9F%E6%8C%87%E5%8D%97-kuai-su-zhi-nan/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/%E5%BF%AB%E9%80%9F%E6%8C%87%E5%8D%97-kuai-su-zhi-nan/</guid><description>&lt;h2 id="快速指南">
 快速指南
 &lt;a class="anchor" href="#%e5%bf%ab%e9%80%9f%e6%8c%87%e5%8d%97">#&lt;/a>
&lt;/h2>
&lt;h5 id="本文档适用于kubeasz-331以上版本部署单节点集群aio作为快速体验k8s集群的测试环境">
 本文档适用于kubeasz 3.3.1以上版本，部署单节点集群(aio)，作为快速体验k8s集群的测试环境。
 &lt;a class="anchor" href="#%e6%9c%ac%e6%96%87%e6%a1%a3%e9%80%82%e7%94%a8%e4%ba%8ekubeasz-331%e4%bb%a5%e4%b8%8a%e7%89%88%e6%9c%ac%e9%83%a8%e7%bd%b2%e5%8d%95%e8%8a%82%e7%82%b9%e9%9b%86%e7%be%a4aio%e4%bd%9c%e4%b8%ba%e5%bf%ab%e9%80%9f%e4%bd%93%e9%aa%8ck8s%e9%9b%86%e7%be%a4%e7%9a%84%e6%b5%8b%e8%af%95%e7%8e%af%e5%a2%83">#&lt;/a>
&lt;/h5>
&lt;h3 id="1基础系统配置">
 1.基础系统配置
 &lt;a class="anchor" href="#1%e5%9f%ba%e7%a1%80%e7%b3%bb%e7%bb%9f%e9%85%8d%e7%bd%ae">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>
&lt;h5 id="准备一台虚机配置内存2g硬盘30g以上">
 准备一台虚机配置内存2G/硬盘30G以上
 &lt;a class="anchor" href="#%e5%87%86%e5%a4%87%e4%b8%80%e5%8f%b0%e8%99%9a%e6%9c%ba%e9%85%8d%e7%bd%ae%e5%86%85%e5%ad%982g%e7%a1%ac%e7%9b%9830g%e4%bb%a5%e4%b8%8a">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="最小化安装ubuntu-1604-server或者centos-7-minimal">
 最小化安装&lt;code>Ubuntu 16.04 server或者CentOS 7 Minimal&lt;/code>
 &lt;a class="anchor" href="#%e6%9c%80%e5%b0%8f%e5%8c%96%e5%ae%89%e8%a3%85ubuntu-1604-server%e6%88%96%e8%80%85centos-7-minimal">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="配置基础网络更新源ssh登录等">
 配置基础网络、更新源、SSH登录等
 &lt;a class="anchor" href="#%e9%85%8d%e7%bd%ae%e5%9f%ba%e7%a1%80%e7%bd%91%e7%bb%9c%e6%9b%b4%e6%96%b0%e6%ba%90ssh%e7%99%bb%e5%bd%95%e7%ad%89">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h5 id="注意-确保在干净的系统上开始安装不能使用曾经装过kubeadm或其他k8s发行版的环境">
 &lt;strong>注意:&lt;/strong> 确保在干净的系统上开始安装，不能使用曾经装过kubeadm或其他k8s发行版的环境
 &lt;a class="anchor" href="#%e6%b3%a8%e6%84%8f-%e7%a1%ae%e4%bf%9d%e5%9c%a8%e5%b9%b2%e5%87%80%e7%9a%84%e7%b3%bb%e7%bb%9f%e4%b8%8a%e5%bc%80%e5%a7%8b%e5%ae%89%e8%a3%85%e4%b8%8d%e8%83%bd%e4%bd%bf%e7%94%a8%e6%9b%be%e7%bb%8f%e8%a3%85%e8%bf%87kubeadm%e6%88%96%e5%85%b6%e4%bb%96k8s%e5%8f%91%e8%a1%8c%e7%89%88%e7%9a%84%e7%8e%af%e5%a2%83">#&lt;/a>
&lt;/h5>
&lt;h3 id="2下载文件">
 2.下载文件
 &lt;a class="anchor" href="#2%e4%b8%8b%e8%bd%bd%e6%96%87%e4%bb%b6">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>
&lt;h5 id="下载工具脚本ezdown举例使用kubeasz版本350">
 下载工具脚本ezdown，举例使用kubeasz版本3.5.0
 &lt;a class="anchor" href="#%e4%b8%8b%e8%bd%bd%e5%b7%a5%e5%85%b7%e8%84%9a%e6%9c%acezdown%e4%b8%be%e4%be%8b%e4%bd%bf%e7%94%a8kubeasz%e7%89%88%e6%9c%ac350">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>export release&lt;span style="color:#f92672">=&lt;/span>3.5.0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>wget https://github.com/easzlab/kubeasz/releases/download/&lt;span style="color:#e6db74">${&lt;/span>release&lt;span style="color:#e6db74">}&lt;/span>/ezdown
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>chmod +x ./ezdown
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>使用工具脚本下载（更多关于ezdown的参数，运行./ezdown 查看）&lt;/li>
&lt;/ul>
&lt;p>下载kubeasz代码、二进制、默认容器镜像&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 国内环境&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>./ezdown -D
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 海外环境&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">#./ezdown -D -m standard&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>【可选】下载额外容器镜像（cilium,flannel,prometheus等）&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 按需下载&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>./ezdown -X flannel
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>./ezdown -X prometheus
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>【可选】下载离线系统包 (适用于无法使用yum/apt仓库情形)&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>./ezdown -P
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>上述脚本运行成功后，所有文件（kubeasz代码、二进制、离线镜像）均已整理好放入目录&lt;code>/etc/kubeasz&lt;/code>&lt;/p>
&lt;ul>
&lt;li>&lt;code>/etc/kubeasz&lt;/code> 包含 kubeasz 版本为 ${release} 的发布代码&lt;/li>
&lt;li>&lt;code>/etc/kubeasz/bin&lt;/code> 包含 k8s/etcd/docker/cni 等二进制文件&lt;/li>
&lt;li>&lt;code>/etc/kubeasz/down&lt;/code> 包含集群安装时需要的离线容器镜像&lt;/li>
&lt;li>&lt;code>/etc/kubeasz/down/packages&lt;/code> 包含集群安装时需要的系统基础软件&lt;/li>
&lt;/ul>
&lt;h3 id="3安装集群">
 3.安装集群
 &lt;a class="anchor" href="#3%e5%ae%89%e8%a3%85%e9%9b%86%e7%be%a4">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>容器化运行 kubeasz&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>./ezdown -S
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>使用默认配置安装 aio 集群&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>docker exec -it kubeasz ezctl start-aio
# 如果安装失败，查看日志排除后，使用如下命令重新安装aio集群
# docker exec -it kubeasz ezctl setup default all
&lt;/code>&lt;/pre>&lt;h3 id="4验证安装">
 4.验证安装
 &lt;a class="anchor" href="#4%e9%aa%8c%e8%af%81%e5%ae%89%e8%a3%85">#&lt;/a>
&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ source ~/.bashrc
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ kubectl version &lt;span style="color:#75715e"># 验证集群版本 &lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ kubectl get node &lt;span style="color:#75715e"># 验证节点就绪 (Ready) 状态&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ kubectl get pod -A &lt;span style="color:#75715e"># 验证集群pod状态，默认已安装网络插件、coredns、metrics-server等&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ kubectl get svc -A &lt;span style="color:#75715e"># 验证集群服务状态&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>登录 &lt;code>dashboard&lt;/code>可以查看和管理集群，更多内容请查阅&lt;a href="../guide/dashboard.md">dashboard文档&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="5清理">
 5.清理
 &lt;a class="anchor" href="#5%e6%b8%85%e7%90%86">#&lt;/a>
&lt;/h3>
&lt;p>以上步骤创建的K8S开发测试环境请尽情折腾，碰到错误尽量通过查看日志、上网搜索、提交&lt;code>issues&lt;/code>等方式解决；当然你也可以清理集群后重新创建。&lt;/p></description></item><item><title>2023-09-28 操作系统说明</title><link>https://qq547475331.github.io/docs/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%AF%B4%E6%98%8E-cao-zuo-xi-tong-shuo-ming/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%AF%B4%E6%98%8E-cao-zuo-xi-tong-shuo-ming/</guid><description>&lt;h1 id="操作系统说明">
 操作系统说明
 &lt;a class="anchor" href="#%e6%93%8d%e4%bd%9c%e7%b3%bb%e7%bb%9f%e8%af%b4%e6%98%8e">#&lt;/a>
&lt;/h1>
&lt;p>目前发现部分使用新内核的linux发行版，k8s 安装使用 cgroup v2版本时，有时候安装会失败，需要删除/清理集群后重新安装。已报告可能发生于 Alma Linux 9, Rocky Linux 9, Fedora 37；建议如下步骤处理：&lt;/p>
&lt;ul>
&lt;li>1.确认系统使用的cgroup v2版本&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>stat -fc %T /sys/fs/cgroup/ 
cgroup2fs
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>
&lt;p>2.初次安装时kubelet可能启动失败，日志报错类似：err=&amp;ldquo;openat2 /sys/fs/cgroup/kubepods.slice/cpu.weight: no such file or directory&amp;rdquo;&lt;/p>
&lt;/li>
&lt;li>
&lt;p>3.建议删除集群然后重新安装，一般能够成功&lt;/p>
&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code># 删除集群
dk ezctl destroy xxxx

# 重启
reboot

# 启动后重新安装
dk ezctl setup xxxx all
&lt;/code>&lt;/pre>&lt;h2 id="debian">
 Debian
 &lt;a class="anchor" href="#debian">#&lt;/a>
&lt;/h2>
&lt;ul>
&lt;li>Debian 11：默认可能没有安装iptables，使用kubeasz 安装前需要执行：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>apt update
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apt install iptables -y
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="alibaba">
 Alibaba
 &lt;a class="anchor" href="#alibaba">#&lt;/a>
&lt;/h2>
&lt;ul>
&lt;li>Alibaba Linux 3.2104 LTS：安装前需要设置如下：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 修改使用dnf包管理&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sed -i &lt;span style="color:#e6db74">&amp;#39;s/package/dnf/g&amp;#39;&lt;/span> /etc/kubeasz/roles/prepare/tasks/redhat.yml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="opensuse">
 openSUSE
 &lt;a class="anchor" href="#opensuse">#&lt;/a>
&lt;/h2>
&lt;ul>
&lt;li>openSUSE Leap 15.4：需要安装iptables&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>zypper install iptables
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ln -s /usr/sbin/iptables /sbin/iptables
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>2023-09-28 离线安装集群</title><link>https://qq547475331.github.io/docs/%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85%E9%9B%86%E7%BE%A4-li-xian-an-zhuang-ji-qun/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85%E9%9B%86%E7%BE%A4-li-xian-an-zhuang-ji-qun/</guid><description>&lt;h1 id="离线安装集群">
 离线安装集群
 &lt;a class="anchor" href="#%e7%a6%bb%e7%ba%bf%e5%ae%89%e8%a3%85%e9%9b%86%e7%be%a4">#&lt;/a>
&lt;/h1>
&lt;h5 id="使用kubeasz-离线安装-k8s集群需要下载四个部分">
 使用kubeasz 离线安装 k8s集群需要下载四个部分：
 &lt;a class="anchor" href="#%e4%bd%bf%e7%94%a8kubeasz-%e7%a6%bb%e7%ba%bf%e5%ae%89%e8%a3%85-k8s%e9%9b%86%e7%be%a4%e9%9c%80%e8%a6%81%e4%b8%8b%e8%bd%bd%e5%9b%9b%e4%b8%aa%e9%83%a8%e5%88%86">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="kubeasz-项目代码">
 kubeasz 项目代码
 &lt;a class="anchor" href="#kubeasz-%e9%a1%b9%e7%9b%ae%e4%bb%a3%e7%a0%81">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="二进制文件k8setcdcontainerd等组件">
 二进制文件（k8s、etcd、containerd等组件）
 &lt;a class="anchor" href="#%e4%ba%8c%e8%bf%9b%e5%88%b6%e6%96%87%e4%bb%b6k8setcdcontainerd%e7%ad%89%e7%bb%84%e4%bb%b6">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="容器镜像文件calicocorednsmetrics-server等容器镜像">
 容器镜像文件（calico、coredns、metrics-server等容器镜像）
 &lt;a class="anchor" href="#%e5%ae%b9%e5%99%a8%e9%95%9c%e5%83%8f%e6%96%87%e4%bb%b6calicocorednsmetrics-server%e7%ad%89%e5%ae%b9%e5%99%a8%e9%95%9c%e5%83%8f">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="系统软件安装包ipsetlibseccomp2等仅无法使用本地yumapt源时需要">
 系统软件安装包（ipset、libseccomp2等，仅无法使用本地yum/apt源时需要）
 &lt;a class="anchor" href="#%e7%b3%bb%e7%bb%9f%e8%bd%af%e4%bb%b6%e5%ae%89%e8%a3%85%e5%8c%85ipsetlibseccomp2%e7%ad%89%e4%bb%85%e6%97%a0%e6%b3%95%e4%bd%bf%e7%94%a8%e6%9c%ac%e5%9c%b0yumapt%e6%ba%90%e6%97%b6%e9%9c%80%e8%a6%81">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h2 id="离线文件准备">
 离线文件准备
 &lt;a class="anchor" href="#%e7%a6%bb%e7%ba%bf%e6%96%87%e4%bb%b6%e5%87%86%e5%a4%87">#&lt;/a>
&lt;/h2>
&lt;h5 id="在一台能够访问互联网的服务器上执行">
 在一台能够访问互联网的服务器上执行：
 &lt;a class="anchor" href="#%e5%9c%a8%e4%b8%80%e5%8f%b0%e8%83%bd%e5%a4%9f%e8%ae%bf%e9%97%ae%e4%ba%92%e8%81%94%e7%bd%91%e7%9a%84%e6%9c%8d%e5%8a%a1%e5%99%a8%e4%b8%8a%e6%89%a7%e8%a1%8c">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="下载工具脚本ezdown举例使用kubeasz版本360">
 下载工具脚本ezdown，举例使用kubeasz版本3.6.0
 &lt;a class="anchor" href="#%e4%b8%8b%e8%bd%bd%e5%b7%a5%e5%85%b7%e8%84%9a%e6%9c%acezdown%e4%b8%be%e4%be%8b%e4%bd%bf%e7%94%a8kubeasz%e7%89%88%e6%9c%ac360">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>export release&lt;span style="color:#f92672">=&lt;/span>3.6.0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>wget https://github.com/easzlab/kubeasz/releases/download/&lt;span style="color:#e6db74">${&lt;/span>release&lt;span style="color:#e6db74">}&lt;/span>/ezdown
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>chmod +x ./ezdown
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>使用工具脚本下载（更多关于ezdown的参数，运行./ezdown 查看）&lt;/li>
&lt;/ul>
&lt;p>下载kubeasz代码、二进制、默认容器镜像&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 国内环境&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>./ezdown -D
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>[可选]如果需要更多组件，请下载额外容器镜像（cilium,flannel,prometheus等）&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>./ezdown -X flannel
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>./ezdown -X prometheus
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>下载离线系统包 (适用于无法使用yum/apt仓库情形)&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 如果操作系统是ubuntu 22.04&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>./ezdown -P ubuntu_22
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>上述脚本运行成功后，所有文件（kubeasz代码、二进制、离线镜像）均已整理好放入目录&lt;code>/etc/kubeasz&lt;/code>&lt;/p>
&lt;ul>
&lt;li>&lt;code>/etc/kubeasz&lt;/code> 包含 kubeasz 版本为 ${release} 的发布代码&lt;/li>
&lt;li>&lt;code>/etc/kubeasz/bin&lt;/code> 包含 k8s/etcd/docker/cni 等二进制文件&lt;/li>
&lt;li>&lt;code>/etc/kubeasz/down&lt;/code> 包含集群安装时需要的离线容器镜像&lt;/li>
&lt;li>&lt;code>/etc/kubeasz/down/packages&lt;/code> 包含集群安装时需要的系统基础软件&lt;/li>
&lt;/ul>
&lt;h2 id="离线安装">
 离线安装
 &lt;a class="anchor" href="#%e7%a6%bb%e7%ba%bf%e5%ae%89%e8%a3%85">#&lt;/a>
&lt;/h2>
&lt;p>上述下载完成后，把&lt;code>/etc/kubeasz&lt;/code>整个目录复制到目标离线服务器相同目录，然后在离线服务器/etc/kubeasz目录下执行：&lt;/p></description></item><item><title>2024-01-21 使用 OpenFunction 在任何基础设施上运行无服务器工作负载</title><link>https://qq547475331.github.io/docs/%E4%BD%BF%E7%94%A8-openfunction-%E5%9C%A8%E4%BB%BB%E4%BD%95%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD%E4%B8%8A%E8%BF%90%E8%A1%8C%E6%97%A0%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%B7%A5%E4%BD%9C%E8%B4%9F%E8%BD%BD-shi-yong-openfunction-zai-ren-he-ji-chu-she-shi-shang-yun-xing-wu-fu-wu-qi-gong-zuo-fu-zai/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/%E4%BD%BF%E7%94%A8-openfunction-%E5%9C%A8%E4%BB%BB%E4%BD%95%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD%E4%B8%8A%E8%BF%90%E8%A1%8C%E6%97%A0%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%B7%A5%E4%BD%9C%E8%B4%9F%E8%BD%BD-shi-yong-openfunction-zai-ren-he-ji-chu-she-shi-shang-yun-xing-wu-fu-wu-qi-gong-zuo-fu-zai/</guid><description>&lt;p>云原生技术的崛起使得我们可以以相同的方式在公有云、私有云或本地数据中心运行应用程序或工作负载。但是，对于需要访问不同云或开源中间件的各种 BaaS 服务的无服务器工作负载来说，这并不容易。在这次演讲中，OpenFunction 维护者将详细介绍如何使用 OpenFunction 解决这个问题，以及 OpenFunction 的最新更新和路线图：&lt;/p>
&lt;p>使用 Dapr 将 FaaS 与 BaaS 解耦
使用 Dapr 代理而不是 Dapr sidecar 来加速函数启动
使用 Kubernetes Gateway API 构建 OpenFunction 网关
使用 WasmEdge 运行时运行 WebAssembly 函数
OpenFunction 在自动驾驶行业的应用案例
最新更新和路线图&lt;/p>
&lt;p>构建开源 FaaS 平台的必要性
什么是 Serverless？加州大学伯克利分校在论文 《A Berkeley View on Serverless Computing》给出了明确定义：Serverless computing = FaaS + BaaS。&lt;/p>
&lt;p>对于函数计算平台，函数是不可或缺的，即 FaaS 是主体。同时，FaaS 也需要和后端的 BaaS 服务产生联系，所以丰富的后端服务是函数的重要依托。&lt;/p>
&lt;p>云厂商通常提供托管的函数计算（FaaS）和各类后端中间件服务，这样就会把开发者锁定在自己的云平台之上。&lt;/p>
&lt;p>现阶段我们也看到，有一些公司因为云上的成本过高，想要下云或者从一个云迁移到另一个云也就是跨云迁移。如果其函数绑定在云的 BaaS 服务上，则不利于跨云的迁移。所以，跨云迁移之后如何去处理各个云厂商 BaaS 服务接口的差异，成了目前较大的挑战。
&lt;img src="https://qq547475331.github.io/upload/2024/01/image-1705780828750.png" alt="image-1705780828750" />&lt;/p>
&lt;p>从另一个角度看，一个 FaaS 平台通常需要支持多种语言，也会利用到众多后端服务。举例来讲，5 种语言需要和 10 种后端服务对接，那么这样做就会有 5×10 即 50 种实现， 还是比较复杂的。
&lt;img src="https://qq547475331.github.io/upload/2024/01/image-1705780817864.png" alt="image-1705780817864" />&lt;/p></description></item><item><title>2024-03-04 K8S CNI剖析演进</title><link>https://qq547475331.github.io/docs/k8s-cni%E5%89%96%E6%9E%90%E6%BC%94%E8%BF%9B-k8scni-pou-xi-yan-jin/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/k8s-cni%E5%89%96%E6%9E%90%E6%BC%94%E8%BF%9B-k8scni-pou-xi-yan-jin/</guid><description>&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041014067.png" alt="image-20240304101409913" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041014936.png" alt="image-20240304101422866" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041014097.png" alt="image-20240304101435883" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041014809.png" alt="image-20240304101449734" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041015326.png" alt="image-20240304101505237" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041015443.png" alt="image-20240304101517303" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041015831.png" alt="image-20240304101532753" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041015668.png" alt="image-20240304101551582" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041016109.png" alt="image-20240304101616039" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041016914.png" alt="image-20240304101631832" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041016340.png" alt="image-20240304101648264" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041017525.png" alt="image-20240304101701442" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041017817.png" alt="image-20240304101716735" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041018482.png" alt="image-20240304101812231" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041018902.png" alt="image-20240304101856828" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041019129.png" alt="image-20240304101921866" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041019595.png" alt="image-20240304101946408" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041019969.png" alt="image-20240304101956900" />&lt;/p></description></item><item><title>2024-03-04 K8S CSI剖析演进</title><link>https://qq547475331.github.io/docs/k8s-csi%E5%89%96%E6%9E%90%E6%BC%94%E8%BF%9B-k8scsi-pou-xi-yan-jin/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/k8s-csi%E5%89%96%E6%9E%90%E6%BC%94%E8%BF%9B-k8scsi-pou-xi-yan-jin/</guid><description>&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041031863.png" alt="image-20240304103140771" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041031858.png" alt="image-20240304103156693" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041032808.png" alt="image-20240304103208726" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041032162.png" alt="image-20240304103222095" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041032246.png" alt="image-20240304103234176" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041032383.png" alt="image-20240304103247304" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041032280.png" alt="image-20240304103259103" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041033536.png" alt="image-20240304103317466" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041033181.png" alt="image-20240304103342109" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041034771.png" alt="image-20240304103402521" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041034936.png" alt="image-20240304103428654" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041035425.png" alt="image-20240304103525215" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041035946.png" alt="image-20240304103541858" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041036235.png" alt="image-20240304103600021" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041036393.png" alt="image-20240304103612299" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041036180.png" alt="image-20240304103636086" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041036722.png" alt="image-20240304103656479" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041037060.png" alt="image-20240304103712989" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041038383.png" alt="image-20240304103829153" />&lt;/p></description></item><item><title>2024-03-04 K8S 流量链路剖析</title><link>https://qq547475331.github.io/docs/k8s-%E6%B5%81%E9%87%8F%E9%93%BE%E8%B7%AF%E5%89%96%E6%9E%90-k8s-liu-liang-lian-lu-pou-xi/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/k8s-%E6%B5%81%E9%87%8F%E9%93%BE%E8%B7%AF%E5%89%96%E6%9E%90-k8s-liu-liang-lian-lu-pou-xi/</guid><description>&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041021558.png" alt="image-20240304102137134" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041021997.png" alt="image-20240304102150912" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041022459.png" alt="image-20240304102205296" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041022999.png" alt="image-20240304102225793" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041022638.png" alt="image-20240304102242571" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041022927.png" alt="image-20240304102254845" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041023817.png" alt="image-20240304102312740" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041023599.png" alt="image-20240304102335528" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041024648.png" alt="image-20240304102410438" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041024130.png" alt="image-20240304102426059" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041024501.png" alt="image-20240304102442427" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041025905.png" alt="image-20240304102514776" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041025454.png" alt="image-20240304102530257" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041025640.png" alt="image-20240304102543566" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041025682.png" alt="image-20240304102555618" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041027558.png" alt="image-20240304102704294" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041027144.png" alt="image-20240304102728769" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041027257.png" alt="image-20240304102755026" />&lt;/p></description></item><item><title>2024-03-28 k8s之kubelet源码解读</title><link>https://qq547475331.github.io/docs/k8s%E4%B9%8Bkubelet%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-zhi-kubelet-yuan-ma-jie-du/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/k8s%E4%B9%8Bkubelet%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-zhi-kubelet-yuan-ma-jie-du/</guid><description>&lt;pre tabindex="0">&lt;code>/*
Copyright 2015 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the &amp;#34;License&amp;#34;);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

 http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &amp;#34;AS IS&amp;#34; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package kubelet

import (
	&amp;#34;context&amp;#34;
	&amp;#34;crypto/tls&amp;#34;
	&amp;#34;errors&amp;#34;
	&amp;#34;fmt&amp;#34;
	&amp;#34;math&amp;#34;
	&amp;#34;net&amp;#34;
	&amp;#34;net/http&amp;#34;
	&amp;#34;os&amp;#34;
	&amp;#34;path/filepath&amp;#34;
	sysruntime &amp;#34;runtime&amp;#34;
	&amp;#34;sort&amp;#34;
	&amp;#34;sync&amp;#34;
	&amp;#34;sync/atomic&amp;#34;
	&amp;#34;time&amp;#34;

	cadvisorapi &amp;#34;github.com/google/cadvisor/info/v1&amp;#34;
	&amp;#34;github.com/google/go-cmp/cmp&amp;#34;
	&amp;#34;go.opentelemetry.io/otel/attribute&amp;#34;
	semconv &amp;#34;go.opentelemetry.io/otel/semconv/v1.12.0&amp;#34;
	&amp;#34;go.opentelemetry.io/otel/trace&amp;#34;
	&amp;#34;k8s.io/client-go/informers&amp;#34;

	netutils &amp;#34;k8s.io/utils/net&amp;#34;

	v1 &amp;#34;k8s.io/api/core/v1&amp;#34;
	metav1 &amp;#34;k8s.io/apimachinery/pkg/apis/meta/v1&amp;#34;
	&amp;#34;k8s.io/apimachinery/pkg/fields&amp;#34;
	&amp;#34;k8s.io/apimachinery/pkg/labels&amp;#34;
	&amp;#34;k8s.io/apimachinery/pkg/types&amp;#34;
	utilruntime &amp;#34;k8s.io/apimachinery/pkg/util/runtime&amp;#34;
	&amp;#34;k8s.io/apimachinery/pkg/util/sets&amp;#34;
	&amp;#34;k8s.io/apimachinery/pkg/util/wait&amp;#34;
	utilfeature &amp;#34;k8s.io/apiserver/pkg/util/feature&amp;#34;
	clientset &amp;#34;k8s.io/client-go/kubernetes&amp;#34;
	v1core &amp;#34;k8s.io/client-go/kubernetes/typed/core/v1&amp;#34;
	corelisters &amp;#34;k8s.io/client-go/listers/core/v1&amp;#34;
	&amp;#34;k8s.io/client-go/tools/cache&amp;#34;
	&amp;#34;k8s.io/client-go/tools/record&amp;#34;
	&amp;#34;k8s.io/client-go/util/certificate&amp;#34;
	&amp;#34;k8s.io/client-go/util/flowcontrol&amp;#34;
	cloudprovider &amp;#34;k8s.io/cloud-provider&amp;#34;
	&amp;#34;k8s.io/component-helpers/apimachinery/lease&amp;#34;
	internalapi &amp;#34;k8s.io/cri-api/pkg/apis&amp;#34;
	runtimeapi &amp;#34;k8s.io/cri-api/pkg/apis/runtime/v1&amp;#34;
	pluginwatcherapi &amp;#34;k8s.io/kubelet/pkg/apis/pluginregistration/v1&amp;#34;
	statsapi &amp;#34;k8s.io/kubelet/pkg/apis/stats/v1alpha1&amp;#34;
	podutil &amp;#34;k8s.io/kubernetes/pkg/api/v1/pod&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/api/v1/resource&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/features&amp;#34;
	kubeletconfiginternal &amp;#34;k8s.io/kubernetes/pkg/kubelet/apis/config&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/kubelet/apis/podresources&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/kubelet/cadvisor&amp;#34;
	kubeletcertificate &amp;#34;k8s.io/kubernetes/pkg/kubelet/certificate&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/kubelet/cloudresource&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/kubelet/clustertrustbundle&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/kubelet/cm&amp;#34;
	draplugin &amp;#34;k8s.io/kubernetes/pkg/kubelet/cm/dra/plugin&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/kubelet/config&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/kubelet/configmap&amp;#34;
	kubecontainer &amp;#34;k8s.io/kubernetes/pkg/kubelet/container&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/kubelet/cri/remote&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/kubelet/events&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/kubelet/eviction&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/kubelet/images&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/kubelet/kuberuntime&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/kubelet/lifecycle&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/kubelet/logs&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/kubelet/metrics&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/kubelet/metrics/collectors&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/kubelet/network/dns&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/kubelet/nodeshutdown&amp;#34;
	oomwatcher &amp;#34;k8s.io/kubernetes/pkg/kubelet/oom&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/kubelet/pleg&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/kubelet/pluginmanager&amp;#34;
	plugincache &amp;#34;k8s.io/kubernetes/pkg/kubelet/pluginmanager/cache&amp;#34;
	kubepod &amp;#34;k8s.io/kubernetes/pkg/kubelet/pod&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/kubelet/preemption&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/kubelet/prober&amp;#34;
	proberesults &amp;#34;k8s.io/kubernetes/pkg/kubelet/prober/results&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/kubelet/runtimeclass&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/kubelet/secret&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/kubelet/server&amp;#34;
	servermetrics &amp;#34;k8s.io/kubernetes/pkg/kubelet/server/metrics&amp;#34;
	serverstats &amp;#34;k8s.io/kubernetes/pkg/kubelet/server/stats&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/kubelet/stats&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/kubelet/status&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/kubelet/sysctl&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/kubelet/token&amp;#34;
	kubetypes &amp;#34;k8s.io/kubernetes/pkg/kubelet/types&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/kubelet/userns&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/kubelet/userns/inuserns&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/kubelet/util&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/kubelet/util/manager&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/kubelet/util/queue&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/kubelet/util/sliceutils&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/kubelet/volumemanager&amp;#34;
	httpprobe &amp;#34;k8s.io/kubernetes/pkg/probe/http&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/security/apparmor&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/util/oom&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/volume&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/volume/csi&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/volume/util/hostutil&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/volume/util/subpath&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/volume/util/volumepathhandler&amp;#34;
	&amp;#34;k8s.io/utils/clock&amp;#34;
)

const (
	// Max amount of time to wait for the container runtime to come up.
	maxWaitForContainerRuntime = 30 * time.Second

	// nodeStatusUpdateRetry specifies how many times kubelet retries when posting node status failed.
	nodeStatusUpdateRetry = 5

	// nodeReadyGracePeriod is the period to allow for before fast status update is
	// terminated and container runtime not being ready is logged without verbosity guard.
	nodeReadyGracePeriod = 120 * time.Second

	// DefaultContainerLogsDir is the location of container logs.
	DefaultContainerLogsDir = &amp;#34;/var/log/containers&amp;#34;

	// MaxContainerBackOff is the max backoff period, exported for the e2e test
	MaxContainerBackOff = 300 * time.Second

	// Period for performing global cleanup tasks.
	housekeepingPeriod = time.Second * 2

	// Duration at which housekeeping failed to satisfy the invariant that
	// housekeeping should be fast to avoid blocking pod config (while
	// housekeeping is running no new pods are started or deleted).
	housekeepingWarningDuration = time.Second * 1

	// Period after which the runtime cache expires - set to slightly longer than
	// the expected length between housekeeping periods, which explicitly refreshes
	// the cache.
	runtimeCacheRefreshPeriod = housekeepingPeriod + housekeepingWarningDuration

	// Period for performing eviction monitoring.
	// ensure this is kept in sync with internal cadvisor housekeeping.
	evictionMonitoringPeriod = time.Second * 10

	// The path in containers&amp;#39; filesystems where the hosts file is mounted.
	linuxEtcHostsPath = &amp;#34;/etc/hosts&amp;#34;
	windowsEtcHostsPath = &amp;#34;C:\\Windows\\System32\\drivers\\etc\\hosts&amp;#34;

	// Capacity of the channel for receiving pod lifecycle events. This number
	// is a bit arbitrary and may be adjusted in the future.
	plegChannelCapacity = 1000

	// Generic PLEG relies on relisting for discovering container events.
	// A longer period means that kubelet will take longer to detect container
	// changes and to update pod status. On the other hand, a shorter period
	// will cause more frequent relisting (e.g., container runtime operations),
	// leading to higher cpu usage.
	// Note that even though we set the period to 1s, the relisting itself can
	// take more than 1s to finish if the container runtime responds slowly
	// and/or when there are many container changes in one cycle.
	genericPlegRelistPeriod = time.Second * 1
	genericPlegRelistThreshold = time.Minute * 3

	// Generic PLEG relist period and threshold when used with Evented PLEG.
	eventedPlegRelistPeriod = time.Second * 300
	eventedPlegRelistThreshold = time.Minute * 10
	eventedPlegMaxStreamRetries = 5

	// backOffPeriod is the period to back off when pod syncing results in an
	// error. It is also used as the base period for the exponential backoff
	// container restarts and image pulls.
	backOffPeriod = time.Second * 10

	// ContainerGCPeriod is the period for performing container garbage collection.
	ContainerGCPeriod = time.Minute
	// ImageGCPeriod is the period for performing image garbage collection.
	ImageGCPeriod = 5 * time.Minute

	// Minimum number of dead containers to keep in a pod
	minDeadContainerInPod = 1

	// nodeLeaseRenewIntervalFraction is the fraction of lease duration to renew the lease
	nodeLeaseRenewIntervalFraction = 0.25

	// instrumentationScope is the name of OpenTelemetry instrumentation scope
	instrumentationScope = &amp;#34;k8s.io/kubernetes/pkg/kubelet&amp;#34;
)

//这段代码定义了一系列常量，用于配置和控制Kubernetes节点上的Kubelet组件的行为。
//Kubelet负责管理节点上的容器，包括启动、停止、监控和更新容器的状态。
//- maxWaitForContainerRuntime：等待容器运行时启动的最大时间。
//- nodeStatusUpdateRetry：当发布节点状态失败时，kubelet重试的次数。
//- nodeReadyGracePeriod：在快速状态更新终止之前允许的时间，如果容器运行时未准备好，则记录日志而不进行详细保护。
//- DefaultContainerLogsDir：容器日志的默认位置。 - MaxContainerBackOff：容器重启的最大退避期，用于E2E测试。
//- housekeepingPeriod：执行全局清理任务的周期。
//- housekeepingWarningDuration：如果清理工作未能满足清理工作应快速进行的不变性条件（在清理工作运行时，不会启动或删除新的Pod），则会发出警告的时间。
//- runtimeCacheRefreshPeriod：运行时缓存的刷新周期，设置为比清理周期稍长的时间，清理周期显式刷新缓存。
//- evictionMonitoringPeriod：执行驱逐监控的周期，确保与内部cadvisor清理保持同步。
//- linuxEtcHostsPath：在Linux容器的文件系统中，主机文件被挂载的路径。
//- windowsEtcHostsPath：在Windows容器的文件系统中，主机文件被挂载的路径。
//- plegChannelCapacity：接收Pod生命周期事件的通道容量。
//- genericPlegRelistPeriod：通用PLEG重新列出的周期，用于发现容器事件。
//较长的周期意味着kubelet需要更长的时间来检测容器更改并更新Pod状态。
//另一方面，较短的周期会导致更频繁的重新列出（例如，容器运行时操作），导致CPU使用率更高。 -
//genericPlegRelistThreshold：通用PLEG重新列出的阈值。
//- eventedPlegRelistPeriod：使用Evented PLEG时的重新列出周期。
//- eventedPlegRelistThreshold：使用Evented PLEG时的重新列出阈值。
//- eventedPlegMaxStreamRetries：使用Evented PLEG时的最大流重试次数。
//- backOffPeriod：当Pod同步结果出现错误时的退避周期。它还用作容器重启和图像拉取的指数退避的基周期。
//- ContainerGCPeriod：执行容器垃圾收集的周期。
//- ImageGCPeriod：执行图像垃圾收集的周期。
//- minDeadContainerInPod：在Pod中保留的最少死亡容器数。
//- nodeLeaseRenewIntervalFraction：租约更新间隔的分数，为租约持续时间的四分之一。
//- instrumentationScope：OpenTelemetry仪器作用域的名称。

var (
	// ContainerLogsDir can be overwritten for testing usage
	ContainerLogsDir = DefaultContainerLogsDir
	etcHostsPath = getContainerEtcHostsPath()
)

//这段Go代码定义了两个变量：
//1. ContainerLogsDir：用于存储容器日志的目录路径。它被初始化为 DefaultContainerLogsDir 的值。在测试场景下，可以重写该变量。
//2. etcHostsPath：表示容器中 hosts 文件的路径。它通过调用 getContainerEtcHostsPath() 函数进行初始化。
//这段代码的主要作用是配置容器日志目录和 hosts 文件路径，方便后续代码使用。

func getContainerEtcHostsPath() string {
	if sysruntime.GOOS == &amp;#34;windows&amp;#34; {
		return windowsEtcHostsPath
	}
	return linuxEtcHostsPath
}

//该函数用于获取容器的etc/hosts文件路径。
//根据操作系统的不同，返回不同的路径。
//如果操作系统是Windows，则返回windowsEtcHostsPath；否则返回linuxEtcHostsPath。

// SyncHandler is an interface implemented by Kubelet, for testability
type SyncHandler interface {
	HandlePodAdditions(pods []*v1.Pod)
	HandlePodUpdates(pods []*v1.Pod)
	HandlePodRemoves(pods []*v1.Pod)
	HandlePodReconcile(pods []*v1.Pod)
	HandlePodSyncs(pods []*v1.Pod)
	HandlePodCleanups(ctx context.Context) error
}

//这是一个定义了多个处理Pod操作的方法的接口，用于Kubelet的测试。
//其中的方法包括处理Pod的添加、更新、移除、协调和同步，以及Pod的清理操作。
//这些方法分别接受一个Pod列表或上下文作为参数，并可能返回一个错误。

// Option is a functional option type for Kubelet
type Option func(*Kubelet)

//该函数是一个类型为Option的函数，参数为指向Kubelet类型的指针，并且没有返回值。Option类型定义了为Kubelet提供功能选项的方法。

// Bootstrap is a bootstrapping interface for kubelet, targets the initialization protocol
type Bootstrap interface {
	GetConfiguration() kubeletconfiginternal.KubeletConfiguration
	BirthCry()
	StartGarbageCollection()
	ListenAndServe(kubeCfg *kubeletconfiginternal.KubeletConfiguration, tlsOptions *server.TLSOptions, auth server.AuthInterface, tp trace.TracerProvider)
	ListenAndServeReadOnly(address net.IP, port uint)
	ListenAndServePodResources()
	Run(&amp;lt;-chan kubetypes.PodUpdate)
	RunOnce(&amp;lt;-chan kubetypes.PodUpdate) ([]RunPodResult, error)
}

//这个Go代码定义了一个名为Bootstrap的接口，用于Kubelet的初始化协议。该接口包含以下方法：
//1. GetConfiguration()：返回KubeletConfiguration对象，表示Kubelet的配置。
//2. BirthCry()：进行初始化时的“出生哭泣”操作，可能用于一些初始化的日志记录或资源申请等。
//3. StartGarbageCollection()：启动垃圾回收机制，用于管理Kubelet运行时的资源清理。
//4. ListenAndServe()：根据提供的Kubelet配置、TLS选项、认证接口和跟踪提供器，启动Kubelet的服务并进行监听。
//5. ListenAndServeReadOnly()：在指定的IP地址和端口上启动只读服务，用于提供Kubelet的只读API。
//6. ListenAndServePodResources()：启动一个服务来监听和处理Pod资源相关请求。
//7. Run()：根据传入的Pod更新通道，持续运行Kubelet，处理Pod的生命周期事件。
//8. RunOnce()：根据传入的Pod更新通道，一次性运行Kubelet，处理Pod的生命周期事件，并返回运行结果。
//这些方法涵盖了Kubelet初始化、配置、垃圾回收、服务监听和Pod管理等方面的功能。

// Dependencies is a bin for things we might consider &amp;#34;injected dependencies&amp;#34; -- objects constructed
// at runtime that are necessary for running the Kubelet. This is a temporary solution for grouping
// these objects while we figure out a more comprehensive dependency injection story for the Kubelet.
type Dependencies struct {
	Options []Option

	// Injected Dependencies
	Auth server.AuthInterface
	CAdvisorInterface cadvisor.Interface
	Cloud cloudprovider.Interface
	ContainerManager cm.ContainerManager
	EventClient v1core.EventsGetter
	HeartbeatClient clientset.Interface
	OnHeartbeatFailure func()
	KubeClient clientset.Interface
	Mounter mount.Interface
	HostUtil hostutil.HostUtils
	OOMAdjuster *oom.OOMAdjuster
	OSInterface kubecontainer.OSInterface
	PodConfig *config.PodConfig
	ProbeManager prober.Manager
	Recorder record.EventRecorder
	Subpather subpath.Interface
	TracerProvider trace.TracerProvider
	VolumePlugins []volume.VolumePlugin
	DynamicPluginProber volume.DynamicPluginProber
	TLSOptions *server.TLSOptions
	RemoteRuntimeService internalapi.RuntimeService
	RemoteImageService internalapi.ImageManagerService
	PodStartupLatencyTracker util.PodStartupLatencyTracker
	NodeStartupLatencyTracker util.NodeStartupLatencyTracker
	// remove it after cadvisor.UsingLegacyCadvisorStats dropped.
	useLegacyCadvisorStats bool
}

//该Go代码定义了一个名为Dependencies的结构体，用于存储Kubelet运行时所需的各种依赖项。
//这些依赖项包括各种接口、对象和配置，
//例如：
//- Auth：认证接口
//- CADvisorInterface：CADvisor接口
//- Cloud：云提供商接口
//- ContainerManager：容器管理器
//- EventClient：事件客户端
//- HeartbeatClient：心跳客户端
//- OnHeartbeatFailure：心跳失败时的回调函数
//- KubeClient：Kubernetes客户端
//- Mounter：挂载器
//- HostUtil：主机工具
//- OOMAdjuster：OOM调整器
//- OSInterface：操作系统接口
//- PodConfig：Pod配置
//- ProbeManager：探针管理器
//- Recorder：事件记录器
//- Subpather：子路径接口
//- TracerProvider：跟踪器提供者
//- VolumePlugins：卷插件列表
//- DynamicPluginProber：动态插件探针
//- TLSOptions：TLS选项
//- RemoteRuntimeService：远程运行时服务
//- RemoteImageService：远程镜像服务
//- PodStartupLatencyTracker：Pod启动延迟跟踪器
//- NodeStartupLatencyTracker：节点启动延迟跟踪器
//- useLegacyCadvisorStats：是否使用传统CADvisor统计信息的标志
//这些依赖项通过结构体成员变量的方式进行定义和存储，其中一些成员变量还包含了函数类型的变量。
//这个结构体的作用是为了临时解决依赖注入的问题，以便于管理和组织Kubelet的依赖项。

// makePodSourceConfig creates a config.PodConfig from the given
// KubeletConfiguration or returns an error.
func makePodSourceConfig(kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *Dependencies, nodeName types.NodeName, nodeHasSynced func() bool) (*config.PodConfig, error) {
	manifestURLHeader := make(http.Header)
	if len(kubeCfg.StaticPodURLHeader) &amp;gt; 0 {
		for k, v := range kubeCfg.StaticPodURLHeader {
			for i := range v {
				manifestURLHeader.Add(k, v[i])
			}
		}
	}
	//该函数根据给定的KubeletConfiguration，Dependencies，NodeName和nodeHasSynced函数创建一个config.PodConfig对象或返回错误。
	//函数首先创建一个空的http.Header对象用于存储静态Pod URL的头部信息，
	//然后根据kubeCfg.StaticPodURLHeader的值向manifestURLHeader中添加头部信息。
	//最后，函数返回一个新创建的config.PodConfig对象和可能的错误。

	// source of all configuration
	cfg := config.NewPodConfig(config.PodConfigNotificationIncremental, kubeDeps.Recorder, kubeDeps.PodStartupLatencyTracker)
	//这个函数是配置管理的一部分，它创建了一个包含了所有配置信息的cfg对象。
	//这个对象会被用来管理pod的配置信息，包括增量配置通知、事件记录器和Pod启动延迟追踪器。

	// TODO: it needs to be replaced by a proper context in the future
	ctx := context.TODO()

	// define file config source
	if kubeCfg.StaticPodPath != &amp;#34;&amp;#34; {
		klog.InfoS(&amp;#34;Adding static pod path&amp;#34;, &amp;#34;path&amp;#34;, kubeCfg.StaticPodPath)
		config.NewSourceFile(kubeCfg.StaticPodPath, nodeName, kubeCfg.FileCheckFrequency.Duration, cfg.Channel(ctx, kubetypes.FileSource))
	}
	//这段Go代码中，首先创建了一个context.TODO()上下文，之后根据kubeCfg.StaticPodPath的值，如果其不为空，
	//则通过config.NewSourceFile()函数将静态Pod路径添加到配置源中，并设置了节点名和文件检查频率。

	// define url config source
	if kubeCfg.StaticPodURL != &amp;#34;&amp;#34; {
		klog.InfoS(&amp;#34;Adding pod URL with HTTP header&amp;#34;, &amp;#34;URL&amp;#34;, kubeCfg.StaticPodURL, &amp;#34;header&amp;#34;, manifestURLHeader)
		config.NewSourceURL(kubeCfg.StaticPodURL, manifestURLHeader, nodeName, kubeCfg.HTTPCheckFrequency.Duration, cfg.Channel(ctx, kubetypes.HTTPSource))
	}
	//这段Go代码是在配置Kubernetes集群时，用于添加静态Pod的URL地址作为配置源的代码片段。
	//具体功能如下：
	//1. 首先，代码会判断kubeCfg.StaticPodURL是否为空，如果不为空，则执行下面的操作。
	//2. 通过klog.InfoS打印日志信息，记录正在添加的Pod URL地址以及相关的HTTP头信息。
	//3. 调用config.NewSourceURL函数，将静态Pod的URL地址、HTTP头信息、节点名称、HTTP检查频率和配置通道等参数传入，创建一个新的URL配置源。
	//4. 通过cfg.Channel获取配置通道，并将其作为参数传递给config.NewSourceURL函数。
	//这段代码的主要作用是在Kubernetes集群中添加静态Pod的URL地址作为配置源，以便集群能够从该URL地址获取Pod的配置信息。

	if kubeDeps.KubeClient != nil {
		klog.InfoS(&amp;#34;Adding apiserver pod source&amp;#34;)
		config.NewSourceApiserver(kubeDeps.KubeClient, nodeName, nodeHasSynced, cfg.Channel(ctx, kubetypes.ApiserverSource))
	}
	return cfg, nil
}

//这段Go代码是在一个函数中的if语句块，主要功能是根据条件判断是否需要添加一个名为“apiserver pod source”的源。
//- 首先，它会检查kubeDeps.KubeClient是否为nil，如果不为nil，则执行下面的代码。
//- 接着，通过klog.InfoS打印一条日志信息，表示正在添加“apiserver pod source”。
//- 然后，调用config.NewSourceApiserver函数，传入kubeDeps.KubeClient、nodeName、nodeHasSynced以及cfg.Channel(ctx, kubetypes.ApiserverSource)作为参数，
//来创建并添加这个源。
//- 最后，函数返回cfg和nil。 总结：这段代码的功能是在满足一定条件时，向某个配置中添加一个名为“apiserver pod source”的源。

// PreInitRuntimeService will init runtime service before RunKubelet.
func PreInitRuntimeService(kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *Dependencies) error {
	remoteImageEndpoint := kubeCfg.ImageServiceEndpoint
	if remoteImageEndpoint == &amp;#34;&amp;#34; &amp;amp;&amp;amp; kubeCfg.ContainerRuntimeEndpoint != &amp;#34;&amp;#34; {
		remoteImageEndpoint = kubeCfg.ContainerRuntimeEndpoint
	}
	var err error
	if kubeDeps.RemoteRuntimeService, err = remote.NewRemoteRuntimeService(kubeCfg.ContainerRuntimeEndpoint, kubeCfg.RuntimeRequestTimeout.Duration, kubeDeps.TracerProvider); err != nil {
		return err
	}
	if kubeDeps.RemoteImageService, err = remote.NewRemoteImageService(remoteImageEndpoint, kubeCfg.RuntimeRequestTimeout.Duration, kubeDeps.TracerProvider); err != nil {
		return err
	}

	kubeDeps.useLegacyCadvisorStats = cadvisor.UsingLegacyCadvisorStats(kubeCfg.ContainerRuntimeEndpoint)

	return nil
}

//该函数在运行Kubelet之前初始化运行时服务。
//- 首先，它将根据kubeCfg中的配置确定远程镜像服务的端点remoteImageEndpoint。
//- 如果remoteImageEndpoint为空且kubeCfg.ContainerRuntimeEndpoint不为空，
//则将kubeCfg.ContainerRuntimeEndpoint赋值给remoteImageEndpoint。
//- 然后，尝试创建远程运行时服务kubeDeps.RemoteRuntimeService，如果创建失败则返回错误。
//- 接着，尝试创建远程镜像服务kubeDeps.RemoteImageService，如果创建失败则返回错误。
//- 最后，根据kubeCfg.ContainerRuntimeEndpoint确定是否使用传统的cadvisor统计信息，并将结果赋值给kubeDeps.useLegacyCadvisorStats。
//如果在创建服务时发生错误，函数将返回错误。否则，返回nil表示成功。

// NewMainKubelet instantiates a new Kubelet object along with all the required internal modules.
// No initialization of Kubelet and its modules should happen here.
func NewMainKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration,
	kubeDeps *Dependencies,
	crOptions *config.ContainerRuntimeOptions,
	hostname string,
	hostnameOverridden bool,
	nodeName types.NodeName,
	nodeIPs []net.IP,
	providerID string,
	cloudProvider string,
	certDirectory string,
	rootDirectory string,
	podLogsDirectory string,
	imageCredentialProviderConfigFile string,
	imageCredentialProviderBinDir string,
	registerNode bool,
	registerWithTaints []v1.Taint,
	allowedUnsafeSysctls []string,
	experimentalMounterPath string,
	kernelMemcgNotification bool,
	experimentalNodeAllocatableIgnoreEvictionThreshold bool,
	minimumGCAge metav1.Duration,
	maxPerPodContainerCount int32,
	maxContainerCount int32,
	registerSchedulable bool,
	keepTerminatedPodVolumes bool,
	nodeLabels map[string]string,
	nodeStatusMaxImages int32,
	seccompDefault bool,
) (*Kubelet, error) {
	ctx := context.Background()
	logger := klog.TODO()
	//该函数用于创建一个包含所有必需内部模块的新Kubelet对象。
	//函数参数包括Kubelet配置、Kubelet依赖项、容器运行时选项、主机名、是否覆盖主机名、节点名、节点IP地址、提供商ID、云提供商、证书目录、根目录、
	//Pod日志目录、镜像凭证提供程序配置文件、镜像凭证提供程序二进制目录、是否注册节点、注册时的污点、允许的不安全Sysctls、实验性挂载器路径、
	//内核Memcg通知、实验性节点可分配忽略驱逐阈值、最小GC年龄、每个Pod的最大容器数、最大容器数、是否注册可调度、是否保留已终止的Pod卷、节点标签、
	//节点状态最大图像数、seccomp默认设置等。
	//函数内部会根据传入的参数创建相应的对象和模块，并进行一些初始化操作。
	//最后会返回创建的Kubelet对象和可能的错误。

	if rootDirectory == &amp;#34;&amp;#34; {
		return nil, fmt.Errorf(&amp;#34;invalid root directory %q&amp;#34;, rootDirectory)
	}
	if podLogsDirectory == &amp;#34;&amp;#34; {
		return nil, errors.New(&amp;#34;pod logs root directory is empty&amp;#34;)
	}
	if kubeCfg.SyncFrequency.Duration &amp;lt;= 0 {
		return nil, fmt.Errorf(&amp;#34;invalid sync frequency %d&amp;#34;, kubeCfg.SyncFrequency.Duration)
	}

	if utilfeature.DefaultFeatureGate.Enabled(features.DisableCloudProviders) &amp;amp;&amp;amp; cloudprovider.IsDeprecatedInternal(cloudProvider) {
		cloudprovider.DisableWarningForProvider(cloudProvider)
		return nil, fmt.Errorf(&amp;#34;cloud provider %q was specified, but built-in cloud providers are disabled. Please set --cloud-provider=external and migrate to an external cloud provider&amp;#34;, cloudProvider)
	}
	//该函数主要进行一系列的参数校验，如果参数不符合要求则返回相应的错误信息。
	//1. 校验rootDirectory是否为空，若为空则返回invalid root directory错误信息。
	//2. 校验podLogsDirectory是否为空，若为空则返回pod logs root directory is empty错误信息。
	//3. 校验kubeCfg.SyncFrequency.Duration是否大于0，若不大于0则返回invalid sync frequency错误信息。
	//4. 若utilfeature.DefaultFeatureGate.Enabled(features.DisableCloudProviders)为true且cloudprovider.IsDeprecatedInternal(cloudProvider)为true，
	//则返回cloud provider指定但内置云提供商被禁用的错误信息。
	//并调用cloudprovider.DisableWarningForProvider(cloudProvider)方法。

	var nodeHasSynced cache.InformerSynced
	var nodeLister corelisters.NodeLister

	// If kubeClient == nil, we are running in standalone mode (i.e. no API servers)
	// If not nil, we are running as part of a cluster and should sync w/API
	if kubeDeps.KubeClient != nil {
		kubeInformers := informers.NewSharedInformerFactoryWithOptions(kubeDeps.KubeClient, 0, informers.WithTweakListOptions(func(options *metav1.ListOptions) {
			options.FieldSelector = fields.Set{metav1.ObjectNameField: string(nodeName)}.String()
		}))
		nodeLister = kubeInformers.Core().V1().Nodes().Lister()
		nodeHasSynced = func() bool {
			return kubeInformers.Core().V1().Nodes().Informer().HasSynced()
		}
		kubeInformers.Start(wait.NeverStop)
		klog.InfoS(&amp;#34;Attempting to sync node with API server&amp;#34;)
	} else {
		// we don&amp;#39;t have a client to sync!
		nodeIndexer := cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers{})
		nodeLister = corelisters.NewNodeLister(nodeIndexer)
		nodeHasSynced = func() bool { return true }
		klog.InfoS(&amp;#34;Kubelet is running in standalone mode, will skip API server sync&amp;#34;)
	}
	//这段Go代码是用于配置和启动一个节点同步器的。
	//首先，定义了nodeHasSynced和nodeLister两个全局变量，分别用于检查节点是否已经与API服务器同步，并获取节点列表。
	//接着，通过条件语句判断是否是在集群中运行。
	//如果是，在集群模式下，会使用kubeDeps.KubeClient创建一个kubeInformers共享informers工厂，并为节点资源创建一个自定义的listers和synced函数。
	//其中，通过WithTweakListOptions函数设置了节点的字段选择器，仅选择名称为nodeName的节点。
	//然后，启动kubeInformers并记录日志，尝试将节点与API服务器同步。
	//如果不在集群中运行，则不会进行节点同步操作。
	//相反，它将创建一个独立的节点索引器，并使用该索引器创建一个nodeLister，并设置nodeHasSynced函数始终返回true，表示节点已经同步。
	//这段代码是节点同步器的一部分，用于根据运行模式配置和启动节点同步器，以保持节点信息的最新状态。

	if kubeDeps.PodConfig == nil {
		var err error
		kubeDeps.PodConfig, err = makePodSourceConfig(kubeCfg, kubeDeps, nodeName, nodeHasSynced)
		if err != nil {
			return nil, err
		}
	}

	containerGCPolicy := kubecontainer.GCPolicy{
		MinAge: minimumGCAge.Duration,
		MaxPerPodContainer: int(maxPerPodContainerCount),
		MaxContainers: int(maxContainerCount),
	}
	//这段Go代码主要功能是根据条件判断来初始化kubeDeps.PodConfig变量，并设置containerGCPolicy。
	//首先，通过条件判断检查kubeDeps.PodConfig是否为nil，
	//如果是，则调用makePodSourceConfig函数来创建并初始化kubeDeps.PodConfig，同时捕获可能的错误并返回。
	//接下来，代码创建一个containerGCPolicy结构体实例，并设置其中的字段值。
	//其中，MinAge字段表示容器的最小存活时间，MaxPerPodContainer字段表示每个Pod中最多保留的容器个数，
	//MaxContainers字段表示节点上最多保留的容器个数。
	//这段代码的主要目的是在初始化kubeDeps.PodConfig后，设置容器的垃圾回收策略。

	daemonEndpoints := &amp;amp;v1.NodeDaemonEndpoints{
		KubeletEndpoint: v1.DaemonEndpoint{Port: kubeCfg.Port},
	}

	imageGCPolicy := images.ImageGCPolicy{
		MinAge: kubeCfg.ImageMinimumGCAge.Duration,
		HighThresholdPercent: int(kubeCfg.ImageGCHighThresholdPercent),
		LowThresholdPercent: int(kubeCfg.ImageGCLowThresholdPercent),
	}
	//这段代码中定义了两个结构体实例。
	//首先，通过v1.NodeDaemonEndpoints结构体创建了一个名为daemonEndpoints的实例，
	//其中KubeletEndpoint的Port字段被初始化为kubeCfg.Port的值。
	//然后，通过images.ImageGCPolicy结构体创建了一个名为imageGCPolicy的实例，
	//其中MinAge字段被初始化为kubeCfg.ImageMinimumGCAge.Duration的值，
	//HighThresholdPercent字段被初始化为kubeCfg.ImageGCHighThresholdPercent的值转换为int类型，
	//LowThresholdPercent字段被初始化为kubeCfg.ImageGCLowThresholdPercent的值转换为int类型。

	if utilfeature.DefaultFeatureGate.Enabled(features.ImageMaximumGCAge) {
		imageGCPolicy.MaxAge = kubeCfg.ImageMaximumGCAge.Duration
	} else if kubeCfg.ImageMaximumGCAge.Duration != 0 {
		klog.InfoS(&amp;#34;ImageMaximumGCAge flag enabled, but corresponding feature gate is not enabled. Ignoring flag.&amp;#34;)
	}
	//这段Go代码主要根据功能门控和配置来设置镜像最大年龄策略。
	//如果默认功能门控中启用了ImageMaximumGCAge，则将imageGCPolicy.MaxAge设置为kubeCfg.ImageMaximumGCAge.Duration；
	//否则，如果kubeCfg.ImageMaximumGCAge.Duration不为0，则记录一条信息表示忽略了该标志。

	enforceNodeAllocatable := kubeCfg.EnforceNodeAllocatable
	if experimentalNodeAllocatableIgnoreEvictionThreshold {
		// Do not provide kubeCfg.EnforceNodeAllocatable to eviction threshold parsing if we are not enforcing Evictions
		enforceNodeAllocatable = []string{}
	}
	thresholds, err := eviction.ParseThresholdConfig(enforceNodeAllocatable, kubeCfg.EvictionHard, kubeCfg.EvictionSoft, kubeCfg.EvictionSoftGracePeriod, kubeCfg.EvictionMinimumReclaim)
	if err != nil {
		return nil, err
	}
	evictionConfig := eviction.Config{
		PressureTransitionPeriod: kubeCfg.EvictionPressureTransitionPeriod.Duration,
		MaxPodGracePeriodSeconds: int64(kubeCfg.EvictionMaxPodGracePeriod),
		Thresholds: thresholds,
		KernelMemcgNotification: kernelMemcgNotification,
		PodCgroupRoot: kubeDeps.ContainerManager.GetPodCgroupRoot(),
	}
	//这个Go函数主要功能是配置和返回一个关于Kubernetes节点上资源驱逐的配置信息。具体步骤如下：
	//1. 将kubeCfg.EnforceNodeAllocatable的值赋给enforceNodeAllocatable变量。
	//2. 如果experimentalNodeAllocatableIgnoreEvictionThreshold为true，则将enforceNodeAllocatable设置为空字符串数组。
	//3. 调用eviction.ParseThresholdConfig函数解析驱逐阈值配置，
	//参数包括enforceNodeAllocatable、kubeCfg.EvictionHard、kubeCfg.EvictionSoft、kubeCfg.EvictionSoftGracePeriod和kubeCfg.EvictionMinimumReclaim。
	//如果解析出错，函数会返回nil和错误信息。
	//4. 创建并返回一个eviction.Config结构体实例，其中包括压力转换期、最大Pod优雅终止时间、驱逐阈值、内核Memcg通知以及Pod容器组根路径等配置信息。

	var serviceLister corelisters.ServiceLister
	var serviceHasSynced cache.InformerSynced
	if kubeDeps.KubeClient != nil {
		kubeInformers := informers.NewSharedInformerFactoryWithOptions(kubeDeps.KubeClient, 0)
		serviceLister = kubeInformers.Core().V1().Services().Lister()
		serviceHasSynced = kubeInformers.Core().V1().Services().Informer().HasSynced
		kubeInformers.Start(wait.NeverStop)
	} else {
		serviceIndexer := cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers{cache.NamespaceIndex: cache.MetaNamespaceIndexFunc})
		serviceLister = corelisters.NewServiceLister(serviceIndexer)
		serviceHasSynced = func() bool { return true }
	}
	//这段Go代码中定义了一个serviceLister变量和一个serviceHasSynced变量，
	//它们分别用于列出Kubernetes服务和检查服务是否已同步。
	//根据是否设置了kubeDeps.KubeClient，代码通过不同的方式初始化这两个变量。
	//如果kubeDeps.KubeClient不为空，代码使用kubeInformers来创建serviceLister和serviceHasSynced；
	//否则，代码使用cache.NewIndexer创建serviceLister，并将serviceHasSynced设置为始终返回true的函数。
	//最后，如果kubeDeps.KubeClient不为空，还调用kubeInformers.Start启动信息处理器。

	// construct a node reference used for events
	nodeRef := &amp;amp;v1.ObjectReference{
		Kind: &amp;#34;Node&amp;#34;,
		Name: string(nodeName),
		UID: types.UID(nodeName),
		Namespace: &amp;#34;&amp;#34;,
	}
	//这段Go代码创建了一个v1.ObjectReference类型的指针变量nodeRef，
	//用于表示一个节点引用，常用于事件处理中。其中，节点的名称、UID和命名空间分别被设置为nodeName参数对应的字符串值、
	//types.UID转换后的值和空字符串。

	oomWatcher, err := oomwatcher.NewWatcher(kubeDeps.Recorder)
	if err != nil {
		if inuserns.RunningInUserNS() {
			if utilfeature.DefaultFeatureGate.Enabled(features.KubeletInUserNamespace) {
				// oomwatcher.NewWatcher returns &amp;#34;open /dev/kmsg: operation not permitted&amp;#34; error,
				// when running in a user namespace with sysctl value `kernel.dmesg_restrict=1`.
				klog.V(2).InfoS(&amp;#34;Failed to create an oomWatcher (running in UserNS, ignoring)&amp;#34;, &amp;#34;err&amp;#34;, err)
				oomWatcher = nil
			} else {
				klog.ErrorS(err, &amp;#34;Failed to create an oomWatcher (running in UserNS, Hint: enable KubeletInUserNamespace feature flag to ignore the error)&amp;#34;)
				return nil, err
			}
		} else {
			return nil, err
		}
	}
	//这段Go代码的主要功能是尝试创建一个OOM watcher实例，通过oomwatcher.NewWatcher(kubeDeps.Recorder)来实现。
	//如果创建失败，会根据当前是否在用户命名空间中运行以及KubeletInUserNamespace特性标志的启用状态来决定是记录错误信息、忽略错误还是返回错误。
	//具体逻辑如下：
	//1. 首先，尝试创建OOM watcher，将返回的实例和可能发生的错误保存在oomWatcher和err变量中。
	//2. 如果err不为nil，即创建OOM watcher失败，则会进一步判断是否在用户命名空间中运行。
	//3. 如果在用户命名空间中运行，并且KubeletInUserNamespace特性标志已被启用，则记录一条信息，指示由于kernel.dmesg_restrict=1的sysctl值，
	//运行在用户命名空间中时，oomwatcher.NewWatcher会返回&amp;#34;open /dev/kmsg: operation not permitted&amp;#34;错误，并将oomWatcher设置为nil。
	//4. 如果在用户命名空间中运行，但KubeletInUserNamespace特性标志未启用，则记录一条错误信息，指示创建OOM watcher失败，
	//并建议启用KubeletInUserNamespace特性标志来忽略该错误，然后返回nil和错误err。
	//5. 如果不在用户命名空间中运行，则直接返回nil和错误err。
	//6. 如果成功创建OOM watcher（即err为nil），则可以继续后续操作。

	clusterDNS := make([]net.IP, 0, len(kubeCfg.ClusterDNS))
	for _, ipEntry := range kubeCfg.ClusterDNS {
		ip := netutils.ParseIPSloppy(ipEntry)
		if ip == nil {
			klog.InfoS(&amp;#34;Invalid clusterDNS IP&amp;#34;, &amp;#34;IP&amp;#34;, ipEntry)
		} else {
			clusterDNS = append(clusterDNS, ip)
		}
	}
	//该函数用于将kubeCfg.ClusterDNS中的IP地址解析并存储到clusterDNS切片中。
	//具体来说，函数首先根据kubeCfg.ClusterDNS的长度初始化一个空的clusterDNS切片，
	//然后遍历kubeCfg.ClusterDNS中的每个IP地址。
	//对于每个IP地址，函数使用netutils.ParseIPSloppy函数进行解析，
	//如果解析失败，则记录一条日志信息；
	//如果解析成功，则将解析后的IP地址添加到clusterDNS切片中。
	//最终，函数返回存储了有效IP地址的clusterDNS切片。

	// A TLS transport is needed to make HTTPS-based container lifecycle requests,
	// but we do not have the information necessary to do TLS verification.
	//
	// This client must not be modified to include credentials, because it is
	// critical that credentials not leak from the client to arbitrary hosts.
	insecureContainerLifecycleHTTPClient := &amp;amp;http.Client{
		Transport: &amp;amp;http.Transport{
			TLSClientConfig: &amp;amp;tls.Config{InsecureSkipVerify: true},
		},
		CheckRedirect: httpprobe.RedirectChecker(false),
	}
	//这段Go代码创建了一个不进行TLS验证的HTTP客户端。该客户端用于进行基于HTTPS的容器生命周期请求。
	//由于没有必要的信息来进行TLS验证，因此必须确保该客户端不被修改为包含凭证，以防止凭证泄露到任意主机。
	//该客户端的重定向检查器被设置为禁止重定向。

	tracer := kubeDeps.TracerProvider.Tracer(instrumentationScope)

	klet := &amp;amp;Kubelet{
		hostname: hostname,
		hostnameOverridden: hostnameOverridden,
		nodeName: nodeName,
		kubeClient: kubeDeps.KubeClient,
		heartbeatClient: kubeDeps.HeartbeatClient,
		onRepeatedHeartbeatFailure: kubeDeps.OnHeartbeatFailure,
		rootDirectory: filepath.Clean(rootDirectory),
		podLogsDirectory: podLogsDirectory,
		resyncInterval: kubeCfg.SyncFrequency.Duration,
		sourcesReady: config.NewSourcesReady(kubeDeps.PodConfig.SeenAllSources),
		registerNode: registerNode,
		registerWithTaints: registerWithTaints,
		registerSchedulable: registerSchedulable,
		dnsConfigurer: dns.NewConfigurer(kubeDeps.Recorder, nodeRef, nodeIPs, clusterDNS, kubeCfg.ClusterDomain, kubeCfg.ResolverConfig),
		serviceLister: serviceLister,
		serviceHasSynced: serviceHasSynced,
		nodeLister: nodeLister,
		nodeHasSynced: nodeHasSynced,
		streamingConnectionIdleTimeout: kubeCfg.StreamingConnectionIdleTimeout.Duration,
		recorder: kubeDeps.Recorder,
		cadvisor: kubeDeps.CAdvisorInterface,
		cloud: kubeDeps.Cloud,
		externalCloudProvider: cloudprovider.IsExternal(cloudProvider),
		providerID: providerID,
		nodeRef: nodeRef,
		nodeLabels: nodeLabels,
		nodeStatusUpdateFrequency: kubeCfg.NodeStatusUpdateFrequency.Duration,
		nodeStatusReportFrequency: kubeCfg.NodeStatusReportFrequency.Duration,
		os: kubeDeps.OSInterface,
		oomWatcher: oomWatcher,
		cgroupsPerQOS: kubeCfg.CgroupsPerQOS,
		cgroupRoot: kubeCfg.CgroupRoot,
		mounter: kubeDeps.Mounter,
		hostutil: kubeDeps.HostUtil,
		subpather: kubeDeps.Subpather,
		maxPods: int(kubeCfg.MaxPods),
		podsPerCore: int(kubeCfg.PodsPerCore),
		syncLoopMonitor: atomic.Value{},
		daemonEndpoints: daemonEndpoints,
		containerManager: kubeDeps.ContainerManager,
		nodeIPs: nodeIPs,
		nodeIPValidator: validateNodeIP,
		clock: clock.RealClock{},
		enableControllerAttachDetach: kubeCfg.EnableControllerAttachDetach,
		makeIPTablesUtilChains: kubeCfg.MakeIPTablesUtilChains,
		keepTerminatedPodVolumes: keepTerminatedPodVolumes,
		nodeStatusMaxImages: nodeStatusMaxImages,
		tracer: tracer,
		nodeStartupLatencyTracker: kubeDeps.NodeStartupLatencyTracker,
	}
	//这段Go代码初始化了一个Kubelet对象。
	//Kubelet是Kubernetes集群中的一个核心组件，负责管理节点上的Pods。
	//该对象的属性包括节点的主机名、是否被覆盖、节点名、Kubernetes客户端、心跳客户端、重复心跳失败的处理函数、根目录、Pod日志目录、同步频率、
	//是否准备好源、注册节点函数、是否带污点注册节点、是否注册为可调度节点、DNS配置器、服务列表器、服务是否已同步、节点列表器、节点是否已同步、
	//流连接空闲超时时间、记录器、cadvisor接口、云提供者、外部云提供者标志、提供者ID、节点引用、节点标签、节点状态更新频率、节点状态报告频率、
	//操作系统接口、OOM观察者、是否支持QoS级别的cgroups、cgroup根目录、安装器、主机工具、子路径处理工具、最大Pod数、每核Pod数、同步循环监视器、
	//守护进程端点、容器管理器、节点IPs、节点IP验证器、时钟、是否启用控制器附加/分离、是否创建iptables工具链、是否保留已终止的Pod卷、
	//节点状态最大图片数、跟踪器和节点启动延迟追踪器。
	//综上所述，这段代码主要用于初始化一个Kubelet对象，以便在Kubernetes集群中管理节点上的Pods。

	if klet.cloud != nil {
		klet.cloudResourceSyncManager = cloudresource.NewSyncManager(klet.cloud, nodeName, klet.nodeStatusUpdateFrequency)
	}

	var secretManager secret.Manager
	var configMapManager configmap.Manager
	if klet.kubeClient != nil {
		switch kubeCfg.ConfigMapAndSecretChangeDetectionStrategy {
		case kubeletconfiginternal.WatchChangeDetectionStrategy:
			secretManager = secret.NewWatchingSecretManager(klet.kubeClient, klet.resyncInterval)
			configMapManager = configmap.NewWatchingConfigMapManager(klet.kubeClient, klet.resyncInterval)
		case kubeletconfiginternal.TTLCacheChangeDetectionStrategy:
			secretManager = secret.NewCachingSecretManager(
				klet.kubeClient, manager.GetObjectTTLFromNodeFunc(klet.GetNode))
			configMapManager = configmap.NewCachingConfigMapManager(
				klet.kubeClient, manager.GetObjectTTLFromNodeFunc(klet.GetNode))
		case kubeletconfiginternal.GetChangeDetectionStrategy:
			secretManager = secret.NewSimpleSecretManager(klet.kubeClient)
			configMapManager = configmap.NewSimpleConfigMapManager(klet.kubeClient)
		default:
			return nil, fmt.Errorf(&amp;#34;unknown configmap and secret manager mode: %v&amp;#34;, kubeCfg.ConfigMapAndSecretChangeDetectionStrategy)
		}
		//这段Go代码根据不同的条件初始化了不同的secretManager和configMapManager。
		//首先，如果klet.cloud不为空，则创建一个新的cloudResourceSyncManager。
		//接下来，根据klet.kubeClient和kubeCfg.ConfigMapAndSecretChangeDetectionStrategy的值，
		//分别初始化secretManager和configMapManager。
		//- 如果kubeCfg.ConfigMapAndSecretChangeDetectionStrategy的值是kubeletconfiginternal.WatchChangeDetectionStrategy，
		//则分别创建一个watchingSecretManager和watchingConfigMapManager。
		//- 如果kubeCfg.ConfigMapAndSecretChangeDetectionStrategy的值是kubeletconfiginternal.TTLCacheChangeDetectionStrategy，
		//则分别创建一个cachingSecretManager和cachingConfigMapManager。
		//- 如果kubeCfg.ConfigMapAndSecretChangeDetectionStrategy的值是kubeletconfiginternal.GetChangeDetectionStrategy，
		//则分别创建一个simpleSecretManager和simpleConfigMapManager。
		//- 如果kubeCfg.ConfigMapAndSecretChangeDetectionStrategy的值不是以上三种情况，则返回一个错误。
		//最后，返回secretManager和configMapManager。

		klet.secretManager = secretManager
		klet.configMapManager = configMapManager
		//这个代码片段是Go语言中的简单赋值语句。它将secretManager赋值给klet.secretManager，
		//将configMapManager赋值给klet.configMapManager。这两个操作将使得klet对象能够管理秘密和配置映射。

	}

	machineInfo, err := klet.cadvisor.MachineInfo()
	if err != nil {
		return nil, err
	}
	// Avoid collector collects it as a timestamped metric
	// See PR #95210 and #97006 for more details.
	machineInfo.Timestamp = time.Time{}
	klet.setCachedMachineInfo(machineInfo)

	imageBackOff := flowcontrol.NewBackOff(backOffPeriod, MaxContainerBackOff)

	klet.livenessManager = proberesults.NewManager()
	klet.readinessManager = proberesults.NewManager()
	klet.startupManager = proberesults.NewManager()
	klet.podCache = kubecontainer.NewCache()

	klet.mirrorPodClient = kubepod.NewBasicMirrorClient(klet.kubeClient, string(nodeName), nodeLister)
	klet.podManager = kubepod.NewBasicPodManager()

	klet.statusManager = status.NewManager(klet.kubeClient, klet.podManager, klet, kubeDeps.PodStartupLatencyTracker, klet.getRootDir())

	klet.resourceAnalyzer = serverstats.NewResourceAnalyzer(klet, kubeCfg.VolumeStatsAggPeriod.Duration, kubeDeps.Recorder)

	klet.runtimeService = kubeDeps.RemoteRuntimeService
	//该Go函数主要是进行初始化操作。
	//1. 首先通过cadvisor获取机器信息，并设置Timestamp为time.Time{}避免被收集为时间戳指标。
	//2. 初始化imageBackOff（容器启动失败时的退避策略）。
	//3. 创建并初始化livenessManager、readinessManager、startupManager（用于管理容器的存活、就绪和启动状态）。
	//4. 创建并初始化podCache（用于缓存Pod信息）。
	//5. 创建并初始化mirrorPodClient和podManager（用于管理镜像Pod和Pod）。
	//6. 创建并初始化statusManager（用于管理Pod的状态）。
	//7. 创建并初始化resourceAnalyzer（用于分析资源使用情况）。
	//8. 将runtimeService设置为kubeDeps.RemoteRuntimeService。
	//这个函数主要是为了初始化kubelet的各种组件和管理器，并设置相应的参数和配置。

	if kubeDeps.KubeClient != nil {
		klet.runtimeClassManager = runtimeclass.NewManager(kubeDeps.KubeClient)
	}

	// setup containerLogManager for CRI container runtime
	containerLogManager, err := logs.NewContainerLogManager(
		klet.runtimeService,
		kubeDeps.OSInterface,
		kubeCfg.ContainerLogMaxSize,
		int(kubeCfg.ContainerLogMaxFiles),
		int(kubeCfg.ContainerLogMaxWorkers),
		kubeCfg.ContainerLogMonitorInterval,
	)
	if err != nil {
		return nil, fmt.Errorf(&amp;#34;failed to initialize container log manager: %v&amp;#34;, err)
	}
	klet.containerLogManager = containerLogManager
	//这段Go代码主要执行了两个步骤：
	//1. 若kubeDeps.KubeClient不为空，则创建一个runtimeclass.Manager实例，并将其赋值给klet.runtimeClassManager。
	//2. 创建一个logs.ContainerLogManager实例，并将其赋值给klet.containerLogManager。
	//此实例用于管理容器日志，包括日志文件的最大大小、最大文件数、最大工作线程数等配置。若创建过程中出现错误，则返回错误信息。
	//总结：这段代码主要负责初始化两个管理器：runtimeclass.Manager和logs.ContainerLogManager，用于管理运行时类和容器日志。

	klet.reasonCache = NewReasonCache()
	klet.workQueue = queue.NewBasicWorkQueue(klet.clock)
	klet.podWorkers = newPodWorkers(
		klet,
		kubeDeps.Recorder,
		klet.workQueue,
		klet.resyncInterval,
		backOffPeriod,
		klet.podCache,
	)
	//这段代码是Go语言中的函数调用和赋值语句，主要进行了三个对象的初始化和赋值操作。
	//- 首先，通过NewReasonCache()函数创建了一个reasonCache对象，并将其赋值给klet.reasonCache。
	//- 然后，通过queue.NewBasicWorkQueue(klet.clock)函数创建了一个workQueue对象，并将其赋值给klet.workQueue。
	//- 最后，通过newPodWorkers()函数创建了一个podWorkers对象，并将其赋值给klet.podWorkers。newPodWorkers()函数的参数包括klet对象、
	//Recorder对象、workQueue对象、resyncInterval时间间隔、backOffPeriod时间间隔、podCache对象等

	runtime, err := kuberuntime.NewKubeGenericRuntimeManager(
		kubecontainer.FilterEventRecorder(kubeDeps.Recorder),
		klet.livenessManager,
		klet.readinessManager,
		klet.startupManager,
		rootDirectory,
		podLogsDirectory,
		machineInfo,
		klet.podWorkers,
		kubeDeps.OSInterface,
		klet,
		insecureContainerLifecycleHTTPClient,
		imageBackOff,
		kubeCfg.SerializeImagePulls,
		kubeCfg.MaxParallelImagePulls,
		float32(kubeCfg.RegistryPullQPS),
		int(kubeCfg.RegistryBurst),
		imageCredentialProviderConfigFile,
		imageCredentialProviderBinDir,
		kubeCfg.CPUCFSQuota,
		kubeCfg.CPUCFSQuotaPeriod,
		kubeDeps.RemoteRuntimeService,
		kubeDeps.RemoteImageService,
		kubeDeps.ContainerManager,
		klet.containerLogManager,
		klet.runtimeClassManager,
		seccompDefault,
		kubeCfg.MemorySwap.SwapBehavior,
		kubeDeps.ContainerManager.GetNodeAllocatableAbsolute,
		*kubeCfg.MemoryThrottlingFactor,
		kubeDeps.PodStartupLatencyTracker,
		kubeDeps.TracerProvider,
	)
	if err != nil {
		return nil, err
	}
	klet.containerRuntime = runtime
	klet.streamingRuntime = runtime
	klet.runner = runtime
	//这个函数主要用于创建并初始化一个KubeGenericRuntimeManager对象，
	//它是kubernetes中的一个容器运行时管理器，主要负责管理容器的生命周期。
	//函数中使用了许多参数，它们主要用于配置运行时管理器的各种属性，例如日志目录、镜像拉取策略、CPU和内存限制等。
	//函数返回一个runtime对象，它包含了运行时管理器的各种接口，如启动容器、停止容器等。
	//最后，将runtime对象赋值给klet结构体的相应字段，以便在后续代码中使用。

	runtimeCache, err := kubecontainer.NewRuntimeCache(klet.containerRuntime, runtimeCacheRefreshPeriod)
	if err != nil {
		return nil, err
	}
	klet.runtimeCache = runtimeCache

	// common provider to get host file system usage associated with a pod managed by kubelet
	hostStatsProvider := stats.NewHostStatsProvider(kubecontainer.RealOS{}, func(podUID types.UID) string {
		return getEtcHostsPath(klet.getPodDir(podUID))
	}, podLogsDirectory)
	if kubeDeps.useLegacyCadvisorStats {
		klet.StatsProvider = stats.NewCadvisorStatsProvider(
			klet.cadvisor,
			klet.resourceAnalyzer,
			klet.podManager,
			klet.runtimeCache,
			klet.containerRuntime,
			klet.statusManager,
			hostStatsProvider)
	} else {
		klet.StatsProvider = stats.NewCRIStatsProvider(
			klet.cadvisor,
			klet.resourceAnalyzer,
			klet.podManager,
			klet.runtimeCache,
			kubeDeps.RemoteRuntimeService,
			kubeDeps.RemoteImageService,
			hostStatsProvider,
			utilfeature.DefaultFeatureGate.Enabled(features.PodAndContainerStatsFromCRI))
	}
	//该Go函数主要功能是初始化Kubelet的统计提供者。
	//首先，函数通过调用kubecontainer.NewRuntimeCache方法创建一个运行时缓存，并将其赋值给runtimeCache变量。
	//如果创建过程中出现错误，则会返回nil和错误信息。
	//接下来，函数通过调用stats.NewHostStatsProvider方法创建一个主机状态提供者，
	//用于获取与Kubelet管理的Pod相关的主机文件系统使用情况。
	//该方法接收三个参数：一个实现了OS接口的对象（这里使用kubecontainer.RealOS{}），
	//一个函数（用于根据Pod的UID获取/etc/hosts的路径），以及Pod日志目录的路径。
	//然后，函数根据kubeDeps.useLegacyCadvisorStats的值选择合适的统计提供者。
	//如果为true，则调用stats.NewCadvisorStatsProvider方法创建一个基于Cadvisor的统计提供者；
	//否则，调用stats.NewCRIStatsProvider方法创建一个基于CRI的统计提供者。
	//这两个方法都接收多个参数，包括cadvisor接口、resourceAnalyzer接口、podManager接口、runtimeCache对象、containerRuntime接口、
	//statusManager接口以及之前创建的hostStatsProvider对象。
	//其中，NewCRIStatsProvider方法还额外接收RemoteRuntimeService和RemoteImageService接口，以及一个表示是否启用特定功能的布尔值。
	//最后，函数将创建的统计提供者赋值给klet.StatsProvider字段。

	eventChannel := make(chan *pleg.PodLifecycleEvent, plegChannelCapacity)

	if utilfeature.DefaultFeatureGate.Enabled(features.EventedPLEG) {
		// adjust Generic PLEG relisting period and threshold to higher value when Evented PLEG is turned on
		genericRelistDuration := &amp;amp;pleg.RelistDuration{
			RelistPeriod: eventedPlegRelistPeriod,
			RelistThreshold: eventedPlegRelistThreshold,
		}
		//这段代码中定义了一个名为eventChannel的通道，类型为*pleg.PodLifecycleEvent，并设置了其容量为plegChannelCapacity。
		//接着，通过utilfeature.DefaultFeatureGate.Enabled(features.EventedPLEG)判断是否启用了EventedPLEG特性。
		//如果启用了，就会调整Generic PLEG的重新列表周期和阈值，将其设置为eventedPlegRelistPeriod和eventedPlegRelistThreshold所指定的值。
		//这段代码的功能是根据是否启用了EventedPLEG特性，来调整Generic PLEG的重新列表周期和阈值。

		klet.pleg = pleg.NewGenericPLEG(klet.containerRuntime, eventChannel, genericRelistDuration, klet.podCache, clock.RealClock{})
		// In case Evented PLEG has to fall back on Generic PLEG due to an error,
		// Evented PLEG should be able to reset the Generic PLEG relisting duration
		// to the default value.
		eventedRelistDuration := &amp;amp;pleg.RelistDuration{
			RelistPeriod: genericPlegRelistPeriod,
			RelistThreshold: genericPlegRelistThreshold,
		}
		//这段Go代码是Kubernetes中的一个片段，它创建了一个PLEG(Pod Lifecycle Event Generator)实例，用于监听容器生命周期事件。
		//klet.pleg = pleg.NewGenericPLEG(klet.containerRuntime, eventChannel, genericRelistDuration, klet.podCache,
		//clock.RealClock{})这行代码创建了一个PLEG实例，其中NewGenericPLEG是一个工厂方法，用于根据提供的参数创建PLEG实例。
		//这个方法接收5个参数：
		//1. klet.containerRuntime：容器运行时接口，用于与底层容器引擎交互。
		//2. eventChannel：一个通道，用于接收容器事件。
		//3. genericRelistDuration：重新列出容器的间隔时间。
		//4. klet.podCache：一个缓存，用于存储Pod的信息。
		//5. clock.RealClock{}：一个时钟对象，用于提供当前时间。
		//第二段代码eventedRelistDuration := &amp;amp;pleg.RelistDuration{RelistPeriod: genericPlegRelistPeriod,
		//RelistThreshold: genericPlegRelistThreshold}创建了一个RelistDuration结构体实例，用于设置PLEG重新列出容器的周期和阈值。
		//其中：
		//1. RelistPeriod：重新列出容器的时间间隔。
		//2. RelistThreshold：在达到此阈值后，PLEG将重新列出容器。
		//这段代码的主要目的是创建并配置一个PLEG实例，用于监听容器生命周期事件，并设置重新列出容器的间隔时间和阈值。

		klet.eventedPleg, err = pleg.NewEventedPLEG(klet.containerRuntime, klet.runtimeService, eventChannel,
			klet.podCache, klet.pleg, eventedPlegMaxStreamRetries, eventedRelistDuration, clock.RealClock{})
		if err != nil {
			return nil, err
		}
	} else {
		genericRelistDuration := &amp;amp;pleg.RelistDuration{
			RelistPeriod: genericPlegRelistPeriod,
			RelistThreshold: genericPlegRelistThreshold,
		}
		klet.pleg = pleg.NewGenericPLEG(klet.containerRuntime, eventChannel, genericRelistDuration, klet.podCache, clock.RealClock{})
	}
	//这段Go代码中，根据条件创建了两种不同类型的PLEG（Pod Lifecycle Event Generator）对象，并将其赋值给klet.pleg。
	//如果某个条件满足，则创建一个EventedPLEG对象，并将klet.eventedPleg指向它。
	//NewEventedPLEG函数的参数包括容器运行时、运行时服务、事件通道、Pod缓存、当前的klet.pleg对象、最大流重试次数、重新列出持续时间和实际时钟。
	//否则，创建一个GenericPLEG对象，并将klet.pleg指向它。
	//NewGenericPLEG函数的参数包括容器运行时、事件通道、重新列出持续时间、Pod缓存和实际时钟。
	//这段代码的主要目的是初始化并配置PLEG对象，用于监控和生成Pod生命周期事件。

	klet.runtimeState = newRuntimeState(maxWaitForContainerRuntime)
	klet.runtimeState.addHealthCheck(&amp;#34;PLEG&amp;#34;, klet.pleg.Healthy)
	if utilfeature.DefaultFeatureGate.Enabled(features.EventedPLEG) {
		klet.runtimeState.addHealthCheck(&amp;#34;EventedPLEG&amp;#34;, klet.eventedPleg.Healthy)
	}
	if _, err := klet.updatePodCIDR(ctx, kubeCfg.PodCIDR); err != nil {
		klog.ErrorS(err, &amp;#34;Pod CIDR update failed&amp;#34;)
	}
	//这段Go代码中的函数功能如下：
	//- 首先，通过newRuntimeState(maxWaitForContainerRuntime)创建一个新的runtimeState实例，并将其赋值给klet.runtimeState。
	//- 接着，调用addHealthCheck方法向runtimeState实例添加了一个名为&amp;#34;PLEG&amp;#34;的健康检查，其健康状态为klet.pleg.Healthy。
	//- 如果features.EventedPLEG特性门已启用，则调用addHealthCheck方法向runtimeState实例添加了一个名为&amp;#34;EventedPLEG&amp;#34;的健康检查，
	//其健康状态为klet.eventedPleg.Healthy。
	//- 最后，调用updatePodCIDR方法尝试更新Pod CIDR，如果更新失败则记录错误日志。
	//总结：这段代码主要通过runtimeState实例管理容器运行时的状态和健康检查，并尝试更新Pod CIDR，如果更新失败则记录错误日志。

	// setup containerGC
	containerGC, err := kubecontainer.NewContainerGC(klet.containerRuntime, containerGCPolicy, klet.sourcesReady)
	if err != nil {
		return nil, err
	}
	klet.containerGC = containerGC
	klet.containerDeletor = newPodContainerDeletor(klet.containerRuntime, max(containerGCPolicy.MaxPerPodContainer, minDeadContainerInPod))
	//这段代码主要功能是创建并初始化一个ContainerGC对象，
	//并将其赋值给klet的containerGC和containerDeletor字段。
	//其中，ContainerGC是用来定期清理和回收容器的组件；newPodContainerDeletor是用来删除过期或无效的容器的函数。
	//max和minDeadContainerInPod是两个辅助函数，用来计算最大存活容器数和最小死亡容器数。
	//如果创建ContainerGC对象失败，则会返回错误。

	// setup imageManager
	imageManager, err := images.NewImageGCManager(klet.containerRuntime, klet.StatsProvider, kubeDeps.Recorder, nodeRef, imageGCPolicy, kubeDeps.TracerProvider)
	if err != nil {
		return nil, fmt.Errorf(&amp;#34;failed to initialize image manager: %v&amp;#34;, err)
	}
	klet.imageManager = imageManager

	if kubeCfg.ServerTLSBootstrap &amp;amp;&amp;amp; kubeDeps.TLSOptions != nil &amp;amp;&amp;amp; utilfeature.DefaultFeatureGate.Enabled(features.RotateKubeletServerCertificate) {
		klet.serverCertificateManager, err = kubeletcertificate.NewKubeletServerCertificateManager(klet.kubeClient, kubeCfg, klet.nodeName, klet.getLastObservedNodeAddresses, certDirectory)
		if err != nil {
			return nil, fmt.Errorf(&amp;#34;failed to initialize certificate manager: %v&amp;#34;, err)
		}
		kubeDeps.TLSOptions.Config.GetCertificate = func(*tls.ClientHelloInfo) (*tls.Certificate, error) {
			cert := klet.serverCertificateManager.Current()
			if cert == nil {
				return nil, fmt.Errorf(&amp;#34;no serving certificate available for the kubelet&amp;#34;)
			}
			return cert, nil
		}
	}
	//这段Go代码主要进行了一系列的初始化操作。
	//首先，它初始化了一个名为imageManager的对象，并将其赋值给klet.imageManager。
	//这个对象是由images.NewImageGCManager函数创建的，它依赖于containerRuntime、StatsProvider、Recorder、nodeRef、
	//imageGCPolicy和TracerProvider等参数。
	//接着，代码检查了是否启用了服务器TLS引导和相关的功能门控。
	//如果满足条件，它会初始化一个名为serverCertificateManager的对象，并将其赋值给klet.serverCertificateManager。
	//这个对象是由kubeletcertificate.NewKubeletServerCertificateManager函数创建的，
	//它依赖于kubeClient、kubeCfg、nodeName、getLastObservedNodeAddresses和certDirectory等参数。
	//最后，代码将serverCertificateManager.Current()方法设置为TLSOptions.Config.GetCertificate的实现。
	//这个方法会返回当前有效的TLS证书，如果证书不存在，则会返回一个错误。
	//整体上，这段代码主要是为了初始化imageManager和serverCertificateManager两个对象，并设置相关的配置和依赖

	if kubeDeps.ProbeManager != nil {
		klet.probeManager = kubeDeps.ProbeManager
	} else {
		klet.probeManager = prober.NewManager(
			klet.statusManager,
			klet.livenessManager,
			klet.readinessManager,
			klet.startupManager,
			klet.runner,
			kubeDeps.Recorder)
	}
	//这段Go代码主要实现了根据条件来初始化klet.probeManager变量。
	//- 首先，它会检查kubeDeps.ProbeManager是否为nil，如果是非nil，则直接将其赋值给klet.probeManager；
	//- 如果kubeDeps.ProbeManager为nil，则通过调用prober.NewManager方法来创建一个新的probeManager实例，
	//并将其赋值给klet.probeManager。
	//prober.NewManager方法接受多个参数，包括klet.statusManager、klet.livenessManager、klet.readinessManager、
	//klet.startupManager、klet.runner和kubeDeps.Recorder。这些参数用于配置和初始化新的probeManager实例。
	//总结一下，这段代码的作用是在kubeDeps.ProbeManager非nil时直接使用它，否则创建一个新的probeManager实例并配置初始化。

	tokenManager := token.NewManager(kubeDeps.KubeClient)

	var clusterTrustBundleManager clustertrustbundle.Manager
	if kubeDeps.KubeClient != nil &amp;amp;&amp;amp; utilfeature.DefaultFeatureGate.Enabled(features.ClusterTrustBundleProjection) {
		kubeInformers := informers.NewSharedInformerFactoryWithOptions(kubeDeps.KubeClient, 0)
		clusterTrustBundleManager, err = clustertrustbundle.NewInformerManager(kubeInformers.Certificates().V1alpha1().ClusterTrustBundles(), 2*int(kubeCfg.MaxPods), 5*time.Minute)
		if err != nil {
			return nil, fmt.Errorf(&amp;#34;while starting informer-based ClusterTrustBundle manager: %w&amp;#34;, err)
		}
		kubeInformers.Start(wait.NeverStop)
		klog.InfoS(&amp;#34;Started ClusterTrustBundle informer&amp;#34;)
	} else {
		// In static kubelet mode, use a no-op manager.
		clusterTrustBundleManager = &amp;amp;clustertrustbundle.NoopManager{}
		klog.InfoS(&amp;#34;Not starting ClusterTrustBundle informer because we are in static kubelet mode&amp;#34;)
	}
	//该函数主要创建并初始化一个token.Manager实例和一个clustertrustbundle.Manager实例。
	//- 首先，根据kubeDeps.KubeClient创建一个token.Manager实例。
	//- 然后，判断kubeDeps.KubeClient是否为空且features.ClusterTrustBundleProjection特性门是否开启，
	//如果满足条件，则使用kubeDeps.KubeClient创建一个informers.SharedInformerFactory实例，
	//并根据该实例创建一个clustertrustbundle.InformerManager实例，同时设置clusterTrustBundleManager。
	//如果不满足条件，则创建一个clustertrustbundle.NoopManager实例，并设置clusterTrustBundleManager。
	//- 最后，如果创建clustertrustbundle.InformerManager实例成功，则启动kubeInformers并记录日志；否则返回错误信息。
	//这段代码主要是为了管理集群信任捆绑(token和cluster trust bundle)而设计的。

	// NewInitializedVolumePluginMgr initializes some storageErrors on the Kubelet runtimeState (in csi_plugin.go init)
	// which affects node ready status. This function must be called before Kubelet is initialized so that the Node
	// ReadyState is accurate with the storage state.
	klet.volumePluginMgr, err =
		NewInitializedVolumePluginMgr(klet, secretManager, configMapManager, tokenManager, clusterTrustBundleManager, kubeDeps.VolumePlugins, kubeDeps.DynamicPluginProber)
	if err != nil {
		return nil, err
	}
	klet.pluginManager = pluginmanager.NewPluginManager(
		klet.getPluginsRegistrationDir(), /* sockDir */
		kubeDeps.Recorder,
	)
	//这段Go代码中的函数是用于初始化Kubelet的卷插件管理器(volumePluginMgr)的。
	//- NewInitializedVolumePluginMgr函数会初始化Kubelet运行时状态中的某些存储错误，这将影响节点的就绪状态。
	//这个函数必须在Kubelet初始化之前调用，以确保节点的就绪状态与存储状态准确无误。
	//- NewInitializedVolumePluginMgr函数的参数包括klet（Kubelet的实例）、secretManager、configMapManager、tokenManager、
	//clusterTrustBundleManager、kubeDeps.VolumePlugins和kubeDeps.DynamicPluginProber。
	//这些参数用于配置和初始化卷插件管理器。
	//- NewInitializedVolumePluginMgr函数返回一个初始化后的卷插件管理器实例和一个错误（如果有）。
	//- pluginmanager.NewPluginManager函数用于创建一个新的插件管理器实例。
	//- pluginmanager.NewPluginManager函数的参数包括插件注册目录（sockDir）和记录器(kubeDeps.Recorder)。
	//- 最后，将创建的卷插件管理器实例赋值给klet.pluginManager。
	//这个函数的主要目的是在Kubelet初始化之前，设置和初始化与存储相关的插件管理器，以便节点的就绪状态能准确反映存储状态。

	// If the experimentalMounterPathFlag is set, we do not want to
	// check node capabilities since the mount path is not the default
	if len(experimentalMounterPath) != 0 {
		// Replace the nameserver in containerized-mounter&amp;#39;s rootfs/etc/resolv.conf with kubelet.ClusterDNS
		// so that service name could be resolved
		klet.dnsConfigurer.SetupDNSinContainerizedMounter(experimentalMounterPath)
	}
	//这段Go代码是一个条件语句，其功能是根据$experimentalMounterPathFlag$的设置来决定是否需要检查节点能力。
	//如果$experimentalMounterPathFlag$被设置（即$experimentalMounterPath$不为空），
	//则会调用$klet.dnsConfigurer.SetupDNSinContainerizedMounter(experimentalMounterPath)$方法，
	//用kubelet.ClusterDNS替换containerized-mounter根文件系统/etc/resolv.conf中的nameserver，以便能够解析服务名称。

	// setup volumeManager
	klet.volumeManager = volumemanager.NewVolumeManager(
		kubeCfg.EnableControllerAttachDetach,
		nodeName,
		klet.podManager,
		klet.podWorkers,
		klet.kubeClient,
		klet.volumePluginMgr,
		klet.containerRuntime,
		kubeDeps.Mounter,
		kubeDeps.HostUtil,
		klet.getPodsDir(),
		kubeDeps.Recorder,
		keepTerminatedPodVolumes,
		volumepathhandler.NewBlockVolumePathHandler())

	klet.backOff = flowcontrol.NewBackOff(backOffPeriod, MaxContainerBackOff)
	//该代码段是Go语言编写的，其中定义了两个不同的内容：一个是volumeManager的设置，另一个是backOff的设置。
	//首先，关于volumeManager的设置：
	//- 通过调用volumemanager.NewVolumeManager()函数创建一个新的volumeManager。
	//- 该函数接受多个参数，包括是否启用控制器挂载/卸载、节点名称、pod管理器、pod workers、kubeClient、volume插件管理器、容器运行时、
	//挂载器、主机工具、pod目录、事件记录器、是否保留已终止的pod卷以及块卷路径处理器。
	//- 这些参数用于配置和初始化新的volumeManager，以便管理节点上的卷。
	//接下来，关于backOff的设置：
	//- 通过调用flowcontrol.NewBackOff()函数创建一个新的backOff对象。
	//- 该函数接受两个参数，即回退周期和最大容器回退时间。
	//- 这些参数用于配置回退对象的行为，以便在容器启动失败时进行重试，避免频繁地立即重试。

	// setup eviction manager
	evictionManager, evictionAdmitHandler := eviction.NewManager(klet.resourceAnalyzer, evictionConfig,
		killPodNow(klet.podWorkers, kubeDeps.Recorder), klet.imageManager, klet.containerGC, kubeDeps.Recorder, nodeRef, klet.clock, kubeCfg.LocalStorageCapacityIsolation)

	klet.evictionManager = evictionManager
	klet.admitHandlers.AddPodAdmitHandler(evictionAdmitHandler)
	//这段代码是设置驱逐管理器的。它通过调用eviction.NewManager函数创建一个驱逐管理器和一个驱逐准入处理器。
	//其中，驱逐管理器用于管理节点的资源驱逐策略，驱逐准入处理器用于在Pod准入阶段进行驱逐检查和处理。
	//接着，将驱逐管理器赋值给klet.evictionManager，将驱逐准入处理器添加到klet.admitHandlers的Pod准入处理器列表中。

	// Safe, allowed sysctls can always be used as unsafe sysctls in the spec.
	// Hence, we concatenate those two lists.
	safeAndUnsafeSysctls := append(sysctl.SafeSysctlAllowlist(), allowedUnsafeSysctls...)
	sysctlsAllowlist, err := sysctl.NewAllowlist(safeAndUnsafeSysctls)
	if err != nil {
		return nil, err
	}
	klet.admitHandlers.AddPodAdmitHandler(sysctlsAllowlist)
	//这段Go代码主要实现了以下功能：
	//1. 将安全的sysctls和不安全的sysctls合并成一个列表 safeAndUnsafeSysctls。
	//2. 使用合并后的列表创建一个sysctl允许列表 sysctlsAllowlist。
	//3. 如果创建允许列表时发生错误，则返回 nil 和错误信息。
	//4. 将sysctls允许列表添加到 klet.admitHandlers 中作为Pod的审核处理器。
	//这段代码的作用是通过合并安全和不安全的sysctls列表，并创建一个允许列表，然后将其添加为Pod的审核处理器，
	//以确保只有允许的sysctls才能在系统中使用。

	// enable active deadline handler
	activeDeadlineHandler, err := newActiveDeadlineHandler(klet.statusManager, kubeDeps.Recorder, klet.clock)
	if err != nil {
		return nil, err
	}
	klet.AddPodSyncLoopHandler(activeDeadlineHandler)
	klet.AddPodSyncHandler(activeDeadlineHandler)
	//这段Go代码的功能是启用一个激活期限处理器。
	//它首先通过调用newActiveDeadlineHandler函数创建一个激活期限处理器，并将其与状态管理器、事件记录器和时钟进行关联。
	//接着，通过调用AddPodSyncLoopHandler和AddPodSyncHandler方法，
	//将激活期限处理器添加到Pod同步循环处理器和Pod同步处理器中，以便在处理Pod时能够检查是否超过了激活期限。
	//如果在创建激活期限处理器时出现错误，函数将返回nil和错误信息。

	klet.admitHandlers.AddPodAdmitHandler(klet.containerManager.GetAllocateResourcesPodAdmitHandler())

	criticalPodAdmissionHandler := preemption.NewCriticalPodAdmissionHandler(klet.GetActivePods, killPodNow(klet.podWorkers, kubeDeps.Recorder), kubeDeps.Recorder)
	klet.admitHandlers.AddPodAdmitHandler(lifecycle.NewPredicateAdmitHandler(klet.getNodeAnyWay, criticalPodAdmissionHandler, klet.containerManager.UpdatePluginResources))
	// apply functional Option&amp;#39;s
	for _, opt := range kubeDeps.Options {
		opt(klet)
	}
	//这段Go代码中定义了一个函数，函数中包含多个步骤。
	//首先，通过klet.admitHandlers.AddPodAdmitHandler()方法添加了一个Pod Admit Handler。
	//然后，创建了一个Critical Pod Admit Handler，并通过klet.admitHandlers.AddPodAdmitHandler()方法将其添加到admitHandlers中。
	//接着，创建了一个Predicate Admit Handler，并通过klet.admitHandlers.AddPodAdmitHandler()方法将其添加到admitHandlers中。
	//最后，对klet应用了一系列的函数式选项。

	if sysruntime.GOOS == &amp;#34;linux&amp;#34; {
		// AppArmor is a Linux kernel security module and it does not support other operating systems.
		klet.appArmorValidator = apparmor.NewValidator()
		klet.softAdmitHandlers.AddPodAdmitHandler(lifecycle.NewAppArmorAdmitHandler(klet.appArmorValidator))
	}
	//这段Go代码是在检查当前运行的操作系统是否为Linux，如果是Linux，则创建一个AppArmorValidator对象，
	//并将其添加到softAdmitHandlers的PodAdmitHandler中。
	//AppArmor是Linux内核的一个安全模块，这段代码的目的是在Kubernetes中启用AppArmor的安全功能。

	leaseDuration := time.Duration(kubeCfg.NodeLeaseDurationSeconds) * time.Second
	renewInterval := time.Duration(float64(leaseDuration) * nodeLeaseRenewIntervalFraction)
	klet.nodeLeaseController = lease.NewController(
		klet.clock,
		klet.heartbeatClient,
		string(klet.nodeName),
		kubeCfg.NodeLeaseDurationSeconds,
		klet.onRepeatedHeartbeatFailure,
		renewInterval,
		string(klet.nodeName),
		v1.NamespaceNodeLease,
		util.SetNodeOwnerFunc(klet.heartbeatClient, string(klet.nodeName)))
	//这个Go函数主要目的是创建并初始化一个node lease控制器。
	//该控制器用于管理节点的租约，以确保节点在Kubernetes集群中保持活动状态。
	//函数首先根据配置确定租约的持续时间和更新间隔，然后使用这些值以及提供的客户端、节点名和其他参数来创建一个新的lease控制器实例。

	// setup node shutdown manager
	shutdownManager, shutdownAdmitHandler := nodeshutdown.NewManager(&amp;amp;nodeshutdown.Config{
		Logger: logger,
		ProbeManager: klet.probeManager,
		Recorder: kubeDeps.Recorder,
		NodeRef: nodeRef,
		GetPodsFunc: klet.GetActivePods,
		KillPodFunc: killPodNow(klet.podWorkers, kubeDeps.Recorder),
		SyncNodeStatusFunc: klet.syncNodeStatus,
		ShutdownGracePeriodRequested: kubeCfg.ShutdownGracePeriod.Duration,
		ShutdownGracePeriodCriticalPods: kubeCfg.ShutdownGracePeriodCriticalPods.Duration,
		ShutdownGracePeriodByPodPriority: kubeCfg.ShutdownGracePeriodByPodPriority,
		StateDirectory: rootDirectory,
	})
	klet.shutdownManager = shutdownManager
	klet.usernsManager, err = userns.MakeUserNsManager(klet)
	if err != nil {
		return nil, err
	}
	klet.admitHandlers.AddPodAdmitHandler(shutdownAdmitHandler)
	//这段代码是Go语言编写的，用于设置节点关闭管理器。
	//首先，通过调用nodeshutdown.NewManager函数创建一个新的节点关闭管理器和一个节点关闭准入处理器。
	//在创建过程中，传入了日志记录器、探针管理器、事件记录器、节点引用、获取活跃Pods的函数、杀死Pod的函数、同步节点状态的函数、
	//节点关闭的优雅等待时间、节点关闭的优雅等待时间（针对关键Pods）、按Pod优先级设置的节点关闭的优雅等待时间以及状态目录等配置参数。
	//然后，将节点关闭管理器赋值给klet.shutdownManager，并将用户命名空间管理器赋值给klet.usernsManager。
	//如果创建用户命名空间管理器失败，则返回错误。
	//最后，将节点关闭准入处理器添加到klet.admitHandlers中的Pod准入处理器列表中。

	// Finally, put the most recent version of the config on the Kubelet, so
	// people can see how it was configured.
	klet.kubeletConfiguration = *kubeCfg

	// Generating the status funcs should be the last thing we do,
	// since this relies on the rest of the Kubelet having been constructed.
	klet.setNodeStatusFuncs = klet.defaultNodeStatusFuncs()

	return klet, nil
}

//这段Go代码是Kubernetes中的Kubelet初始化过程的一部分。在这个函数中，主要完成了以下两步操作：
//1. 将最新的配置版本应用到Kubelet上，这样用户就可以查看Kubelet是如何配置的。
//2. 生成节点状态报告函数，这应该是我们做的最后一件事，因为它依赖于Kubelet的其他部分已经构建完成。
//最后，该函数返回初始化完成的Kubelet实例和一个nil的错误值。

type serviceLister interface {
	List(labels.Selector) ([]*v1.Service, error)
}

//这段Go代码定义了一个名为serviceLister的接口，其中包含一个List方法。
//该方法接收一个labels.Selector类型的参数，并返回一个[]*v1.Service类型（即服务列表）和一个错误值。
//这个接口的作用是用于列举出符合给定标签选择器的服务列表。

// Kubelet is the main kubelet implementation.
type Kubelet struct {
	kubeletConfiguration kubeletconfiginternal.KubeletConfiguration

	// hostname is the hostname the kubelet detected or was given via flag/config
	hostname string
	// hostnameOverridden indicates the hostname was overridden via flag/config
	hostnameOverridden bool
	//该Go代码定义了一个名为Kubelet的结构体，它表示一个Kubelet实例。
	//- kubeletConfiguration是kubeletconfiginternal.KubeletConfiguration类型的一个字段，用于存储Kubelet的配置信息。
	//- hostname是一个字符串字段，表示Kubelet检测到或通过标志/配置给定的主机名。
	//- hostnameOverridden是一个布尔字段，指示主机名是否通过标志/配置被覆盖。

	nodeName types.NodeName
	runtimeCache kubecontainer.RuntimeCache
	kubeClient clientset.Interface
	heartbeatClient clientset.Interface
	// mirrorPodClient is used to create and delete mirror pods in the API for static
	// pods.
	mirrorPodClient kubepod.MirrorClient
	//该Go函数定义了一个结构体，包含以下成员变量：
	//- nodeName：类型为types.NodeName，表示节点名称。
	//- runtimeCache：类型为kubecontainer.RuntimeCache，表示运行时缓存。
	//- kubeClient：类型为clientset.Interface，用于与Kubernetes集群进行交互。
	//- heartbeatClient：类型为clientset.Interface，用于发送心跳等维护操作。
	//- mirrorPodClient：类型为kubepod.MirrorClient，用于在API中创建和删除静态Pod的镜像Pod。

	rootDirectory string
	podLogsDirectory string

	lastObservedNodeAddressesMux sync.RWMutex
	lastObservedNodeAddresses []v1.NodeAddress

	// onRepeatedHeartbeatFailure is called when a heartbeat operation fails more than once. optional.
	onRepeatedHeartbeatFailure func()
	//这段Go代码定义了以下内容：
	//- rootDirectory和podLogsDirectory两个字符串变量，分别表示根目录和Pod日志目录。
	//- lastObservedNodeAddressesMux是一个读写锁，用于保护lastObservedNodeAddresses变量的并发访问。
	//- lastObservedNodeAddresses是一个包含v1.NodeAddress类型的切片，存储了最近观察到的节点地址。
	//- onRepeatedHeartbeatFailure是一个可选的回调函数，当心跳操作失败多次时会被调用。

	// podManager stores the desired set of admitted pods and mirror pods that the kubelet should be
	// running. The actual set of running pods is stored on the podWorkers. The manager is populated
	// by the kubelet config loops which abstracts receiving configuration from many different sources
	// (api for regular pods, local filesystem or http for static pods). The manager may be consulted
	// by other components that need to see the set of desired pods. Note that not all desired pods are
	// running, and not all running pods are in the podManager - for instance, force deleting a pod
	// from the apiserver will remove it from the podManager, but the pod may still be terminating and
	// tracked by the podWorkers. Components that need to know the actual consumed resources of the
	// node or are driven by podWorkers and the sync*Pod methods (status, volume, stats) should also
	// consult the podWorkers when reconciling.
	//
	// TODO: review all kubelet components that need the actual set of pods (vs the desired set)
	// and update them to use podWorkers instead of podManager. This may introduce latency in some
	// methods, but avoids race conditions and correctly accounts for terminating pods that have
	// been force deleted or static pods that have been updated.
	// https://github.com/kubernetes/kubernetes/issues/116970
	podManager kubepod.Manager
	//podManager是一个存储了kubelet应该运行的期望的Pod集合（包括已承认的Pod和镜像Pod）。
	//它的实际运行Pod集合存储在podWorkers中。podManager通过kubelet配置循环来抽象从不同来源接收配置
	//（来自API的普通Pod，本地文件系统或HTTP的静态Pod）。
	//其他组件可以查询podManager来获取期望的Pod集合。
	//注意，不是所有期望的Pod都在运行，也不是所有运行的Pod都在podManager中。
	//例如，从apiserver强制删除一个Pod会将其从podManager中移除，但该Pod可能仍在终止中，并由podWorkers跟踪。
	//需要知道节点实际消耗资源的组件或由podWorkers和sync*Pod方法（状态，卷，统计信息）驱动的组件也应该在协调时咨询podWorkers。

	// podWorkers is responsible for driving the lifecycle state machine of each pod. The worker is
	// notified of config changes, updates, periodic reconciliation, container runtime updates, and
	// evictions of all desired pods and will invoke reconciliation methods per pod in separate
	// goroutines. The podWorkers are authoritative in the kubelet for what pods are actually being
	// run and their current state:
	//
	// * syncing: pod should be running (syncPod)
	// * terminating: pod should be stopped (syncTerminatingPod)
	// * terminated: pod should have all resources cleaned up (syncTerminatedPod)
	//
	// and invoke the handler methods that correspond to each state. Components within the
	// kubelet that need to know the phase of the pod in order to correctly set up or tear down
	// resources must consult the podWorkers.
	//
	// Once a pod has been accepted by the pod workers, no other pod with that same UID (and
	// name+namespace, for static pods) will be started until the first pod has fully terminated
	// and been cleaned up by SyncKnownPods. This means a pod may be desired (in API), admitted
	// (in pod manager), and requested (by invoking UpdatePod) but not start for an arbitrarily
	// long interval because a prior pod is still terminating.
	//
	// As an event-driven (by UpdatePod) controller, the podWorkers must periodically be resynced
	// by the kubelet invoking SyncKnownPods with the desired state (admitted pods in podManager).
	// Since the podManager may be unaware of some running pods due to force deletion, the
	// podWorkers are responsible for triggering a sync of pods that are no longer desired but
	// must still run to completion.
	podWorkers PodWorkers
	//podWorkers是一个负责驱动每个Pod生命周期状态机的函数。
	//它会在配置更改、更新、定期同步、容器运行时更新和驱逐所有期望的Pod时被通知，
	//并在单独的goroutine中对每个Pod调用和解方法。
	//podWorkers是kubelet中Pod实际运行状态和当前状态的权威。
	//它通过调用对应于每个状态的处理方法，来实现Pod的同步、终止和清理等操作。
	//此外，podWorkers还负责触发不再期望运行但必须完成同步的Pod。

	// evictionManager observes the state of the node for situations that could impact node stability
	// and evicts pods (sets to phase Failed with reason Evicted) to reduce resource pressure. The
	// eviction manager acts on the actual state of the node and considers the podWorker to be
	// authoritative.
	evictionManager eviction.Manager
	//该函数定义了一个名为evictionManager的变量，其类型为eviction.Manager。
	//这个变量的目的是监控节点的状态，以便在可能影响节点稳定性的场景下进行操作。
	//具体的操作是通过驱逐（设置为失败状态且附加 Evicted 原因）Pod来减少资源压力。
	//驱逐管理器依据节点的实际状态进行操作，并将podWorker视为权威。

	// probeManager tracks the set of running pods and ensures any user-defined periodic checks are
	// run to introspect the state of each pod. The probe manager acts on the actual state of the node
	// and is notified of pods by the podWorker. The probe manager is the authoritative source of the
	// most recent probe status and is responsible for notifying the status manager, which
	// synthesizes them into the overall pod status.
	probeManager prober.Manager
	//该函数定义了一个名为probeManager的变量，其类型为prober.Manager。
	//probeManager的作用是跟踪正在运行的Pod集合，并确保运行用户定义的周期性检查，以检查每个Pod的状态。
	//它是根据节点的实际状态进行操作的，并由podWorker通知Pod的状态。
	//probeManager是最新探针状态的权威来源，并负责通知状态管理器，将其合成到整体Pod状态中。

	// secretManager caches the set of secrets used by running pods on this node. The podWorkers
	// notify the secretManager when pods are started and terminated, and the secretManager must
	// then keep the needed secrets up-to-date as they change.
	secretManager secret.Manager
	//这段代码定义了一个名为secretManager的变量，
	//其类型为secret.Manager。secretManager是一个缓存，
	//用于存储当前节点上运行的Pod所使用的秘密集。
	//podWorkers在Pod启动和终止时通知secretManager，然后secretManager必须根据秘密的变化保持所需秘密的更新。

	// configMapManager caches the set of config maps used by running pods on this node. The
	// podWorkers notify the configMapManager when pods are started and terminated, and the
	// configMapManager must then keep the needed config maps up-to-date as they change.
	configMapManager configmap.Manager
	//这段代码定义了一个名为configMapManager的变量，它的类型是configmap.Manager。
	//该变量用于缓存当前节点上运行的Pod所使用的ConfigMap集合。
	//PodWorkers会在Pod启动和终止时通知configMapManager，然后configMapManager必须保持所需ConfigMap的更新。

	// volumeManager observes the set of running pods and is responsible for attaching, mounting,
	// unmounting, and detaching as those pods move through their lifecycle. It periodically
	// synchronizes the set of known volumes to the set of actually desired volumes and cleans up
	// any orphaned volumes. The volume manager considers the podWorker to be authoritative for
	// which pods are running.
	volumeManager volumemanager.VolumeManager
	//这段代码定义了一个名为volumeManager的变量，它的类型是volumemanager.VolumeManager。
	//该变量用于观察运行中的Pod集合，并负责在Pod生命周期中进行挂载、卸载和分离等操作。
	//它会周期性地将已知的卷同步到期望的卷集合中，并清理任何孤儿卷。volumeManager将podWorker视为运行中Pod的权威。

	// statusManager receives updated pod status updates from the podWorker and updates the API
	// status of those pods to match. The statusManager is authoritative for the synthesized
	// status of the pod from the kubelet&amp;#39;s perspective (other components own the individual
	// elements of status) and should be consulted by components in preference to assembling
	// that status themselves. Note that the status manager is downstream of the pod worker
	// and components that need to check whether a pod is still running should instead directly
	// consult the pod worker.
	statusManager status.Manager
	//这段代码定义了一个名为statusManager的变量，它的类型是status.Manager。
	//statusManager的作用是接收来自podWorker的更新后的Pod状态，并将这些Pod的状态更新为与API状态匹配。
	//statusManager是关于kubelet视角下Pod合成状态的权威组件（其他组件各自拥有状态的各个元素），
	//组件应该优先咨询statusManager而不是自己组装状态。需要注意的是，statusManager是在podWorker的下游，
	//如果组件需要检查Pod是否仍在运行，应该直接咨询podWorker。

	// resyncInterval is the interval between periodic full reconciliations of
	// pods on this node.
	resyncInterval time.Duration
	//该函数定义了一个名为resyncInterval的变量，其类型为time.Duration，用于表示节点上周期性完全同步Pod的间隔时间。

	// sourcesReady records the sources seen by the kubelet, it is thread-safe.
	sourcesReady config.SourcesReady
	//sourcesReady是一个config.SourcesReady类型的变量，用于记录kubelet看到的源。它是线程安全的。

	// Optional, defaults to /logs/ from /var/log
	logServer http.Handler
	// Optional, defaults to simple Docker implementation
	runner kubecontainer.CommandRunner
	//这段代码定义了两个变量：
	//1. logServer：一个类型为http.Handler的变量，用于处理日志服务的请求，默认值为/logs/路径下的日志文件。
	//2. runner：一个类型为kubecontainer.CommandRunner的变量，用于执行容器内部的命令，默认实现为简单的Docker命令执行器。
	//这两个变量都是可选的，如果没有显式地设置，它们将使用默认值。

	// cAdvisor used for container information.
	cadvisor cadvisor.Interface
	//该行代码定义了一个名为cadvisor.Interface类型的变量cAdvisor，用于获取容器的信息

	// Set to true to have the node register itself with the apiserver.
	registerNode bool
	// List of taints to add to a node object when the kubelet registers itself.
	registerWithTaints []v1.Taint
	// Set to true to have the node register itself as schedulable.
	registerSchedulable bool
	// for internal book keeping; access only from within registerWithApiserver
	registrationCompleted bool
	//这段代码定义了四个变量，用于配置节点在注册到 apiserver 时的一些行为：
	//- registerNode：设置为 true 时，节点会自动注册到 apiserver。
	//- registerWithTaints：节点注册时要添加的 taint 列表。
	//- registerSchedulable：设置为 true 时，节点注册时会将自己标记为可调度的。
	//- registrationCompleted：内部使用，用于记录节点是否已完成注册。

	// dnsConfigurer is used for setting up DNS resolver configuration when launching pods.
	dnsConfigurer *dns.Configurer

	// serviceLister knows how to list services
	serviceLister serviceLister
	// serviceHasSynced indicates whether services have been sync&amp;#39;d at least once.
	// Check this before trusting a response from the lister.
	serviceHasSynced cache.InformerSynced
	// nodeLister knows how to list nodes
	nodeLister corelisters.NodeLister
	// nodeHasSynced indicates whether nodes have been sync&amp;#39;d at least once.
	// Check this before trusting a response from the node lister.
	nodeHasSynced cache.InformerSynced
	// a list of node labels to register
	nodeLabels map[string]string
	//这段代码定义了几个字段，用于配置和管理Kubernetes中的DNS解析器、服务列表、节点列表等。
	//- dnsConfigurer *dns.Configurer：用于在启动Pod时设置DNS解析器的配置。
	//- serviceLister serviceLister：用于列举服务。
	//- serviceHasSynced cache.InformerSynced：指示服务是否至少同步过一次。在信任列表器的响应之前，应该检查此字段。
	//- nodeLister corelisters.NodeLister：用于列举节点。
	//- nodeHasSynced cache.InformerSynced：指示节点是否至少同步过一次。
	//在信任节点列表器的响应之前，应该检查此字段。
	//- nodeLabels map[string]string：要注册的节点标签列表。

	// Last timestamp when runtime responded on ping.
	// Mutex is used to protect this value.
	runtimeState *runtimeState

	// Volume plugins.
	volumePluginMgr *volume.VolumePluginMgr

	// Manages container health check results.
	livenessManager proberesults.Manager
	readinessManager proberesults.Manager
	startupManager proberesults.Manager
	//这段代码定义了四个变量，分别用于记录运行时响应ping的最后一个时间戳、管理卷插件、管理容器健康检查结果和管理容器就绪检查结果。
	//其中，runtimeState使用互斥锁来保护，volumePluginMgr是卷插件管理器，
	//livenessManager、readinessManager和startupManager分别用于管理容器的存活检查结果、就绪检查结果和启动检查结果。

	// How long to keep idle streaming command execution/port forwarding
	// connections open before terminating them
	streamingConnectionIdleTimeout time.Duration
	//该函数定义了一个时间间隔，用于控制在终止闲置的流式命令执行/端口转发连接之前保持连接打开的时间长度。

	// The EventRecorder to use
	recorder record.EventRecorder
	//这段代码定义了一个名为recorder的变量，其类型为record.EventRecorder。这是一个用于记录Kubernetes事件的接口。

	// Policy for handling garbage collection of dead containers.
	containerGC kubecontainer.GC
	//该函数定义了一个处理垃圾回收的策略，用于回收死亡的容器。它使用了kubecontainer.GC类型，表示具体的垃圾回收策略。

	// Manager for image garbage collection.
	imageManager images.ImageGCManager
	//该代码定义了一个名为imageManager的变量，其类型为images.ImageGCManager。这是一个用于管理图像垃圾收集的管理器。

	// Manager for container logs.
	containerLogManager logs.ContainerLogManager
	//该函数定义了一个名为containerLogManager的接口，该接口用于管理容器日志。

	// Cached MachineInfo returned by cadvisor.
	machineInfoLock sync.RWMutex
	machineInfo *cadvisorapi.MachineInfo
	//这段Go代码定义了一个全局变量machineInfo，它是一个指向cadvisorapi.MachineInfo类型的指针，用于存储由cadvisor返回的机器信息。
	//同时，为了保证并发安全，使用了sync.RWMutex类型的machineInfoLock变量作为锁。

	// Handles certificate rotations.
	serverCertificateManager certificate.Manager
	//该代码定义了一个名为serverCertificateManager的变量，其类型为certificate.Manager，用于处理证书轮换。

	// Cloud provider interface.
	cloud cloudprovider.Interface
	// Handles requests to cloud provider with timeout
	cloudResourceSyncManager cloudresource.SyncManager
	//这段代码定义了两个变量：
	//1. cloud：代表云提供商的接口，具体实现取决于云提供商，它定义了与云提供商交互的方法。
	//2. cloudResourceSyncManager：代表一个同步管理器，用于处理与云提供商资源的同步操作，
	//具体实现可能包括同步云提供商的虚拟机、网络、存储等资源信息到本地缓存，并定期更新这些信息以保持数据的同步。

	// Indicates that the node initialization happens in an external cloud controller
	externalCloudProvider bool
	// Reference to this node.
	nodeRef *v1.ObjectReference
	//这段代码定义了两个Go语言的变量。
	//1. externalCloudProvider是一个布尔类型的变量，用于指示节点初始化是在外部云控制器中进行的。
	//2. nodeRef是一个指向v1.ObjectReference类型的指针，用于引用该节点。

	// Container runtime.
	containerRuntime kubecontainer.Runtime
	//该代码定义了一个变量 containerRuntime，其类型为 kubecontainer.Runtime，用于表示容器运行时环境。

	// Streaming runtime handles container streaming.
	streamingRuntime kubecontainer.StreamingRuntime
	//该函数是一个Go语言函数声明，函数名称为streamingRuntime，函数参数为kubecontainer.StreamingRuntime类型。
	//该函数用于处理容器的流式传输。

	// Container runtime service (needed by container runtime Start()).
	runtimeService internalapi.RuntimeService
	//这个Go函数定义了一个名为runtimeService的变量，它是一个internalapi.RuntimeService类型的接口。
	//这个接口用于与容器运行时服务进行交互，是容器运行时启动所必需的。

	// reasonCache caches the failure reason of the last creation of all containers, which is
	// used for generating ContainerStatus.
	reasonCache *ReasonCache
	//该行代码定义了一个名为reasonCache的变量，其类型为*ReasonCache。
	//ReasonCache是一个用于缓存所有容器最后创建失败原因的结构体，这些失败原因用于生成ContainerStatus。

	// containerRuntimeReadyExpected indicates whether container runtime being ready is expected
	// so errors are logged without verbosity guard, to avoid excessive error logs at node startup.
	// It&amp;#39;s false during the node initialization period of nodeReadyGracePeriod, and after that
	// it&amp;#39;s set to true by fastStatusUpdateOnce when it exits.
	containerRuntimeReadyExpected bool
	//该函数用于指示容器运行时是否预期准备好，以便在节点启动时避免过多错误日志。
	//在节点初始化期间为false，之后通过fastStatusUpdateOnce退出时设置为true。

	// nodeStatusUpdateFrequency specifies how often kubelet computes node status. If node lease
	// feature is not enabled, it is also the frequency that kubelet posts node status to master.
	// In that case, be cautious when changing the constant, it must work with nodeMonitorGracePeriod
	// in nodecontroller. There are several constraints:
	// 1. nodeMonitorGracePeriod must be N times more than nodeStatusUpdateFrequency, where
	// N means number of retries allowed for kubelet to post node status. It is pointless
	// to make nodeMonitorGracePeriod be less than nodeStatusUpdateFrequency, since there
	// will only be fresh values from Kubelet at an interval of nodeStatusUpdateFrequency.
	// The constant must be less than podEvictionTimeout.
	// 2. nodeStatusUpdateFrequency needs to be large enough for kubelet to generate node
	// status. Kubelet may fail to update node status reliably if the value is too small,
	// as it takes time to gather all necessary node information.
	nodeStatusUpdateFrequency time.Duration
	//这个go函数定义了一个时间间隔，表示kubelet计算节点状态的频率。
	//如果未启用节点租约功能，则该频率也是kubelet向主节点发布节点状态的频率。
	//在更改该常量时需要谨慎，它必须与节点控制器中的nodeMonitorGracePeriod配合使用，并且需要满足以下约束条件：
	//1. nodeMonitorGracePeriod必须是nodeStatusUpdateFrequency的N倍，其中N表示kubelet发布节点状态允许的重试次数。
	//将nodeMonitorGracePeriod设置为小于nodeStatusUpdateFrequency没有意义，
	//因为kubelet只会以nodeStatusUpdateFrequency的时间间隔提供新鲜值。
	//该常量必须小于podEvictionTimeout。
	//2. nodeStatusUpdateFrequency需要足够大，以便kubelet生成节点状态。
	//如果该值太小，kubelet可能无法可靠地更新节点状态，
	//因为它需要时间来收集所有必要的节点信息。

	// nodeStatusReportFrequency is the frequency that kubelet posts node
	// status to master. It is only used when node lease feature is enabled.
	nodeStatusReportFrequency time.Duration
	//该代码定义了一个变量nodeStatusReportFrequency，它是kubelet向master报告节点状态的频率。
	//这个变量只在启用了节点租约功能时使用。

	// lastStatusReportTime is the time when node status was last reported.
	lastStatusReportTime time.Time
	//该函数定义了一个变量lastStatusReportTime，它表示节点状态最后一次报告的时间，其类型为time.Time。

	// syncNodeStatusMux is a lock on updating the node status, because this path is not thread-safe.
	// This lock is used by Kubelet.syncNodeStatus and Kubelet.fastNodeStatusUpdate functions and shouldn&amp;#39;t be used anywhere else.
	syncNodeStatusMux sync.Mutex
	//这个Go函数定义了一个名为syncNodeStatusMux的互斥锁，用于保护节点状态的更新操作，因为这条路径不是线程安全的。
	//这个互斥锁仅被Kubelet.syncNodeStatus和Kubelet.fastNodeStatusUpdate函数使用，不建议在其他地方使用。

	// updatePodCIDRMux is a lock on updating pod CIDR, because this path is not thread-safe.
	// This lock is used by Kubelet.updatePodCIDR function and shouldn&amp;#39;t be used anywhere else.
	updatePodCIDRMux sync.Mutex
	//这段代码定义了一个名为updatePodCIDRMux的互斥锁，用于保证更新Pod CIDR的路径的线程安全性。
	//这个互斥锁仅被Kubelet的updatePodCIDR函数使用，不建议在其他地方使用。

	// updateRuntimeMux is a lock on updating runtime, because this path is not thread-safe.
	// This lock is used by Kubelet.updateRuntimeUp, Kubelet.fastNodeStatusUpdate and
	// Kubelet.HandlerSupportsUserNamespaces functions and shouldn&amp;#39;t be used anywhere else.
	updateRuntimeMux sync.Mutex
	//这个函数定义了一个名为updateRuntimeMux的互斥锁，用于保证更新运行时的路径线程安全。
	//这个互斥锁被Kubelet.updateRuntimeUp、Kubelet.fastNodeStatusUpdate和Kubelet.HandlerSupportsUserNamespaces函数使用，
	//不应该在其他地方使用。

	// nodeLeaseController claims and renews the node lease for this Kubelet
	nodeLeaseController lease.Controller
	//nodeLeaseController 是一个 lease.Controller 类型的变量。它的功能是声明和更新当前 Kubelet 的节点租约。

	// pleg observes the state of the container runtime and notifies the kubelet of changes to containers, which
	// notifies the podWorkers to reconcile the state of the pod (for instance, if a container dies and needs to
	// be restarted).
	pleg pleg.PodLifecycleEventGenerator
	//该函数定义了一个名为pleg的变量，其类型为pleg.PodLifecycleEventGenerator。
	//这个函数的作用是观察容器运行时的状态，并将容器的变化通知给kubelet，进而通知podWorkers去协调pod的状态。
	//例如，如果一个容器死亡需要重启，pleg会通知kubelet和podWorkers去处理。

	// eventedPleg supplements the pleg to deliver edge-driven container changes with low-latency.
	eventedPleg pleg.PodLifecycleEventGenerator
	//该Go函数定义了一个名为eventedPleg的变量，其类型为pleg.PodLifecycleEventGenerator。
	//这个函数的作用是补充(pleg)以提供边缘驱动的容器变化的低延迟。

	// Store kubecontainer.PodStatus for all pods.
	podCache kubecontainer.Cache
	//该代码定义了一个名为podCache的变量，其类型为kubecontainer.Cache。
	//这个变量的作用是存储所有Pod的kubecontainer.PodStatus信息。

	// os is a facade for various syscalls that need to be mocked during testing.
	os kubecontainer.OSInterface
	//该函数定义了一个类型os，它是一个kubecontainer.OSInterface的别名。
	//这个类型的目的是作为一个Facade，用于在测试期间模拟各种系统调用。

	// Watcher of out of memory events.
	oomWatcher oomwatcher.Watcher
	//该Go函数定义了一个名为oomWatcher的变量，其类型为oomwatcher.Watcher。这个变量用于监控内存溢出事件。

	// Monitor resource usage
	resourceAnalyzer serverstats.ResourceAnalyzer
	//该函数定义了一个资源分析器，类型为serverstats.ResourceAnalyzer。通过该资源分析器可以监控服务器的资源使用情况。

	// Whether or not we should have the QOS cgroup hierarchy for resource management
	cgroupsPerQOS bool
	//该函数定义了一个名为cgroupsPerQOS的变量，其类型为bool。该变量用于指示是否应该为资源管理而创建QOS级别的cgroup层次结构。

	// If non-empty, pass this to the container runtime as the root cgroup.
	cgroupRoot string
	//该函数定义了一个名为cgroupRoot的字符串变量，它表示容器运行时的根cgroup。如果cgroupRoot不为空，则将其传递给容器运行时作为根cgroup。

	// Mounter to use for volumes.
	mounter mount.Interface
	//这段代码定义了一个名为Mounter的接口，用于挂载卷。

	// hostutil to interact with filesystems
	hostutil hostutil.HostUtils
	//该函数定义了一个名为hostutil的变量，它是一个hostutil.HostUtils类型的变量。
	//这个变量是用来与文件系统进行交互的工具类。它可以提供一系列的方法，如读取文件、写入文件、创建文件夹等操作。
	//通过这个变量，可以方便地对文件系统进行操作。

	// subpather to execute subpath actions
	subpather subpath.Interface
	//该函数定义了一个名为subpather的变量，它是一个subpath.Interface类型的变量。这个变量是用来执行子路径操作的接口。

	// Manager of non-Runtime containers.
	containerManager cm.ContainerManager
	//该代码定义了一个名为containerManager的变量，它是一个cm.ContainerManager类型的容器管理器。
	//具体功能和用途需要查看cm.ContainerManager该代码的实现和相关文档才能确定。

	// Maximum Number of Pods which can be run by this Kubelet
	maxPods int
	//该函数定义了一个名为maxPods的整型变量，它表示这个Kubelet能够运行的最大Pods数量。

	// Monitor Kubelet&amp;#39;s sync loop
	syncLoopMonitor atomic.Value
	//该函数是一个Go语言的函数，名为syncLoopMonitor，它用于监控Kubelet的同步循环。
	//该函数使用了atomic.Value类型，atomic.Value是Go语言标准库中的一种原子类型，可以用于安全地存储和加载值。
	//在这个函数中，syncLoopMonitor被定义为atomic.Value类型，可以用来存储和加载Kubelet的同步循环的状态信息。
	//具体来说，syncLoopMonitor可以用来监控Kubelet的同步循环是否正常运行，以及同步循环的运行状态如何。
	//通过使用atomic.Value类型，可以在多线程环境下安全地读写syncLoopMonitor的值，确保了数据的一致性和安全性。
	//总之，syncLoopMonitor函数用于监控Kubelet的同步循环，并使用atomic.Value类型来保证数据的一致性和安全性。

	// Container restart Backoff
	backOff *flowcontrol.Backoff
	//这个Go函数定义了一个名为&amp;#34;backOff&amp;#34;的变量，它是flowcontrol包中的Backoff类型。这个变量用于实现容器重启的退避策略。

	// Information about the ports which are opened by daemons on Node running this Kubelet server.
	daemonEndpoints *v1.NodeDaemonEndpoints
	//该函数用于获取由运行此Kubelet服务器的节点上的守护进程打开的端口的信息。返回一个类型为*v1.NodeDaemonEndpoints的指针。

	// A queue used to trigger pod workers.
	workQueue queue.WorkQueue
	//该函数定义了一个名为workQueue的队列，用于触发Pod workers。
	//这是一个工作队列，可以用来存储待处理的任务，并且按照一定的顺序进行处理。
	//在Go语言中，队列通常被用来实现并发编程中的生产者-消费者模式，其中生产者负责向队列中添加任务，消费者负责从队列中取出任务并进行处理。
	//这个workQueue可以被用来协调多个Pod workers的工作，确保任务能够被有序、高效地处理。

	// oneTimeInitializer is used to initialize modules that are dependent on the runtime to be up.
	oneTimeInitializer sync.Once
	//这个函数是一个用于初始化依赖运行时的模块的同步函数。它使用了sync.Once来确保初始化只执行一次。

	// If set, use this IP address or addresses for the node
	nodeIPs []net.IP
	//该函数用于设置节点的IP地址或地址列表。
	//- nodeIPs []net.IP：表示要设置的节点IP地址或地址列表，类型为[]net.IP，即可以是一个IP地址或多个IP地址的切片。

	// use this function to validate the kubelet nodeIP
	nodeIPValidator func(net.IP) error
	//该函数用于验证Kubelet节点的IP地址是否有效。
	//其输入参数为一个net.IP类型的IP地址，返回值为一个error类型的错误信息。
	//具体验证过程和逻辑未在代码片段中展示。

	// If non-nil, this is a unique identifier for the node in an external database, eg. cloudprovider
	providerID string
	//该函数用于设置节点在外部数据库中的唯一标识符，例如云提供商。

	// clock is an interface that provides time related functionality in a way that makes it
	// easy to test the code.
	clock clock.WithTicker
	//该函数实现了一个时钟接口clock，其中WithTicker方法返回一个定时器，该定时器会在指定的时间间隔后发出时间信号。
	//这个接口的目的是为了方便测试代码，因为它可以模拟时间的流逝和定时器的行为。

	// handlers called during the tryUpdateNodeStatus cycle
	setNodeStatusFuncs []func(context.Context, *v1.Node) error
	//这段Go代码定义了一个名为setNodeStatusFuncs的切片，其元素是一个函数类型，函数接受context.Context和*v1.Node作为参数，
	//并返回一个error类型。 这个切片主要用于存储在tryUpdateNodeStatus周期中被调用的处理函数。

	lastNodeUnschedulableLock sync.Mutex
	// maintains Node.Spec.Unschedulable value from previous run of tryUpdateNodeStatus()
	lastNodeUnschedulable bool
	//这段代码定义了一个互斥锁lastNodeUnschedulableLock和一个布尔值lastNodeUnschedulable，
	//用于维护节点的Spec.Unschedulable值从上一次运行tryUpdateNodeStatus()函数开始就没有改变过。

	// the list of handlers to call during pod admission.
	admitHandlers lifecycle.PodAdmitHandlers
	//该函数定义了一个名为admitHandlers的变量，它是一个lifecycle.PodAdmitHandlers类型的切片。
	//这个变量用于存储在Pod准入阶段需要调用的一系列处理器（handlers）。
	//lifecycle.PodAdmitHandlers是一个结构体类型，它包含了多个处理Pod准入的处理器，例如PreAdmit、Admit和PostAdmit等。
	//每个处理器都是一个函数，它们会在Pod准入的不同阶段被调用，以执行相应的准入控制逻辑。
	//通过这个admitHandlers变量，可以在处理Pod准入时动态添加或移除处理器，以灵活地控制Pod的准入流程。

	// softAdmithandlers are applied to the pod after it is admitted by the Kubelet, but before it is
	// run. A pod rejected by a softAdmitHandler will be left in a Pending state indefinitely. If a
	// rejected pod should not be recreated, or the scheduler is not aware of the rejection rule, the
	// admission rule should be applied by a softAdmitHandler.
	softAdmitHandlers lifecycle.PodAdmitHandlers
	//这段Go代码定义了一个名为softAdmitHandlers的变量，它是一个lifecycle.PodAdmitHandlers类型的变量。
	//这个变量是一个软准入处理器，它会在Kubelet接纳Pod之后、运行Pod之前对Pod进行处理。
	//如果一个Pod被软准入处理器拒绝，它将无限期地保持Pending状态。
	//如果一个被拒绝的Pod不应该被重新创建，或者调度器不知道拒绝规则，那么应该由软准入处理器来应用准入规则。

	// the list of handlers to call during pod sync loop.
	lifecycle.PodSyncLoopHandlers
	//这段代码定义了一个名为PodSyncLoopHandlers的变量，它是一个lifecycle.PodSyncLoopHandlers类型的手册。
	//这个变量用于在Pod同步循环中调用的一系列处理程序。

	// the list of handlers to call during pod sync.
	lifecycle.PodSyncHandlers
	//lifecycle.PodSyncHandlers是一个包含了在同步pod时需要调用的一系列处理程序的列表。

	// the number of allowed pods per core
	podsPerCore int
	//该代码片段定义了一个名为podsPerCore的整型变量，用于表示每核允许的Pod数量。

	// enableControllerAttachDetach indicates the Attach/Detach controller
	// should manage attachment/detachment of volumes scheduled to this node,
	// and disable kubelet from executing any attach/detach operations
	enableControllerAttachDetach bool
	//该函数用于设置是否启用控制器来管理卷的挂载和卸载操作，并禁用kubelet执行任何挂载/卸载操作。
	//其中，参数enableControllerAttachDetach为一个布尔值，若设置为true则启用控制器管理卷的挂载和卸载，
	//若设置为false则禁用控制器管理卷的挂载和卸载。

	// trigger deleting containers in a pod
	containerDeletor *podContainerDeletor
	//这个Go代码定义了一个名为containerDeletor的变量，它是podContainerDeletor类型的指针。
	//podContainerDeletor是一个结构体，用于触发删除Pod中的容器。

	// config iptables util rules
	makeIPTablesUtilChains bool
	//该代码片段定义了一个名为makeIPTablesUtilChains的布尔类型变量，用于配置iptables实用程序的规则。

	// The AppArmor validator for checking whether AppArmor is supported.
	appArmorValidator apparmor.Validator
	//这段代码定义了一个名为appArmorValidator的变量，其类型为apparmor.Validator。
	//该变量用于验证是否支持AppArmor。
	//AppArmor（ApplicationArmor）是一种安全模块，用于增强Linux操作系统的安全性。
	//它通过为每个应用程序定义安全策略来限制其访问系统资源的权限。
	//apparmor.Validator是一个接口，用于检查系统中是否启用了AppArmor，并验证其配置是否正确。
	//综上所述，appArmorValidator是一个用于验证系统是否支持AppArmor的变量。

	// StatsProvider provides the node and the container stats.
	StatsProvider *stats.Provider
	//该Go函数定义了一个名为StatsProvider的变量，该变量指向一个stats.Provider类型的对象。
	//stats.Provider是一个接口，用于提供节点和容器的统计信息。

	// This flag, if set, instructs the kubelet to keep volumes from terminated pods mounted to the node.
	// This can be useful for debugging volume related issues.
	keepTerminatedPodVolumes bool // DEPRECATED
	//该函数定义了一个名为keepTerminatedPodVolumes的布尔型变量，通过该变量控制kubelet是否将已终止pod的卷保持挂载到节点上。
	//该特性用于调试与卷相关的故障。注意，该变量已废弃。

	// pluginmanager runs a set of asynchronous loops that figure out which
	// plugins need to be registered/unregistered based on this node and makes it so.
	pluginManager pluginmanager.PluginManager
	//该函数定义了一个名为pluginManager的变量，其类型为pluginmanager.PluginManager。
	//该变量运行一组异步循环，用于确定基于当前节点需要注册/注销的插件，并进行相应操作。

	// This flag sets a maximum number of images to report in the node status.
	nodeStatusMaxImages int32
	//该函数用于设置节点状态报告中最大图像数量的上限。

	// Handles RuntimeClass objects for the Kubelet.
	runtimeClassManager *runtimeclass.Manager
	//这个Go函数定义了一个名为runtimeClassManager的变量，它是runtimeclass.Manager类型的一个实例。
	//这个变量用于处理Kubelet的RuntimeClass对象。

	// Handles node shutdown events for the Node.
	shutdownManager nodeshutdown.Manager
	//该函数用于处理节点关闭事件。
	//- 参数：
	//- nodeshutdown.Manager：节点关闭管理器。
	//- 功能：
	//- 监听节点关闭事件。
	//- 当节点关闭时，执行相应的处理逻辑。

	// Manage user namespaces
	usernsManager *userns.UsernsManager
	//该代码定义了一个变量usernsManager，其类型为*userns.UsernsManager，用于管理用户命名空间。
	//用户命名空间是Linux系统中的一个特性，可以为不同用户创建独立的文件系统、网络等环境，实现资源的隔离。
	//userns.UsernsManager是一个用于管理用户命名空间的封装对象，提供了创建、删除、查询等操作接口。

	// Mutex to serialize new pod admission and existing pod resizing
	podResizeMutex sync.Mutex
	//这段代码定义了一个名为podResizeMutex的sync.Mutex类型变量。
	//sync.Mutex是Go标准库中的一个互斥锁类型，用于控制并发访问共享资源。
	//在本例中，podResizeMutex用于确保新建Pod的准入和现有Pod的调整大小操作的序列化执行，以避免并发冲突。

	// OpenTelemetry Tracer
	tracer trace.Tracer
	//该函数定义了一个OpenTelemetry Tracer。

	// Track node startup latencies
	nodeStartupLatencyTracker util.NodeStartupLatencyTracker
	//该代码定义了一个名为nodeStartupLatencyTracker的变量，它的类型是util.NodeStartupLatencyTracker。根据变量的命名，
	//可以推测这是一个用于追踪节点启动延迟的工具。具体实现和功能细节需要查看util.NodeStartupLatencyTracker的定义和实现。

}

// ListPodStats is delegated to StatsProvider, which implements stats.Provider interface
func (kl *Kubelet) ListPodStats(ctx context.Context) ([]statsapi.PodStats, error) {
	return kl.StatsProvider.ListPodStats(ctx)
}

//这个函数是Kubelet的一个方法，用于列出所有Pod的统计信息。
//它将请求委托给实现了stats.Provider接口的StatsProvider对象的ListPodStats方法，
//返回一个包含所有Pod统计信息的切片。

// ListPodCPUAndMemoryStats is delegated to StatsProvider, which implements stats.Provider interface
func (kl *Kubelet) ListPodCPUAndMemoryStats(ctx context.Context) ([]statsapi.PodStats, error) {
	return kl.StatsProvider.ListPodCPUAndMemoryStats(ctx)
}

//该函数是一个代理函数，将调用委托给实现了stats.Provider接口的StatsProvider对象的ListPodCPUAndMemoryStats方法。
//该方法用于获取Pod的CPU和内存统计信息。

// ListPodStatsAndUpdateCPUNanoCoreUsage is delegated to StatsProvider, which implements stats.Provider interface
func (kl *Kubelet) ListPodStatsAndUpdateCPUNanoCoreUsage(ctx context.Context) ([]statsapi.PodStats, error) {
	return kl.StatsProvider.ListPodStatsAndUpdateCPUNanoCoreUsage(ctx)
}

//这个函数是Kubelet的一个方法，用于列出Pod的统计信息并更新CPU使用量。
//它委托给实现了stats.Provider接口的StatsProvider来完成具体的操作。

// ImageFsStats is delegated to StatsProvider, which implements stats.Provider interface
func (kl *Kubelet) ImageFsStats(ctx context.Context) (*statsapi.FsStats, *statsapi.FsStats, error) {
	return kl.StatsProvider.ImageFsStats(ctx)
}

//该函数是Kubelet的一个方法，用于获取镜像文件系统的统计信息。
//它将请求委托给实现了stats.Provider接口的StatsProvider对象的ImageFsStats方法，返回镜像文件系统的使用情况统计信息。

// GetCgroupStats is delegated to StatsProvider, which implements stats.Provider interface
func (kl *Kubelet) GetCgroupStats(cgroupName string, updateStats bool) (*statsapi.ContainerStats, *statsapi.NetworkStats, error) {
	return kl.StatsProvider.GetCgroupStats(cgroupName, updateStats)
}

//该函数是一个委托函数，将获取cgroup统计信息的任务委托给实现了stats.Provider接口的StatsProvider对象。
//函数接收一个cgroup名称和一个布尔值updateStats作为参数，
//并返回一个包含容器统计信息的statsapi.ContainerStats指针、一个包含网络统计信息的statsapi.NetworkStats指针以及可能出现的错误。
//具体实现由StatsProvider对象完成。

// GetCgroupCPUAndMemoryStats is delegated to StatsProvider, which implements stats.Provider interface
func (kl *Kubelet) GetCgroupCPUAndMemoryStats(cgroupName string, updateStats bool) (*statsapi.ContainerStats, error) {
	return kl.StatsProvider.GetCgroupCPUAndMemoryStats(cgroupName, updateStats)
}

//该函数是一个委托函数，将获取cgroup的CPU和内存统计信息的任务委托给实现了stats.Provider接口的StatsProvider对象。
//函数接收一个cgroup名称和一个更新统计信息的布尔值作为参数，并返回一个指向statsapi.ContainerStats的指针和一个错误。

// RootFsStats is delegated to StatsProvider, which implements stats.Provider interface
func (kl *Kubelet) RootFsStats() (*statsapi.FsStats, error) {
	return kl.StatsProvider.RootFsStats()
}

//这个函数的功能是获取Kubelet根文件系统的统计信息。
//它将请求委托给实现了stats.Provider接口的StatsProvider对象的RootFsStats方法，
//返回一个包含文件系统统计信息的statsapi.FsStats指针和一个错误值。

// RlimitStats is delegated to StatsProvider, which implements stats.Provider interface
func (kl *Kubelet) RlimitStats() (*statsapi.RlimitStats, error) {
	return kl.StatsProvider.RlimitStats()
}

//该函数是Kubelet的RlimitStats方法，它被委托给StatsProvider来实现stats.Provider接口。
//该方法返回一个*statsapi.RlimitStats类型的对象和一个错误（如果有的话）。

// setupDataDirs creates:
// 1. the root directory
// 2. the pods directory
// 3. the plugins directory
// 4. the pod-resources directory
// 5. the checkpoint directory
// 6. the pod logs root directory
func (kl *Kubelet) setupDataDirs() error {
	if cleanedRoot := filepath.Clean(kl.rootDirectory); cleanedRoot != kl.rootDirectory {
		return fmt.Errorf(&amp;#34;rootDirectory not in canonical form: expected %s, was %s&amp;#34;, cleanedRoot, kl.rootDirectory)
	}
	//该函数用于设置Kubelet的数据目录。
	//首先，它通过filepath.Clean函数清理kl.rootDirectory根目录的路径，
	//然后检查清理后的路径是否与原始路径不同。
	//如果不同，则返回一个错误，指出根目录路径不是规范形式。

	pluginRegistrationDir := kl.getPluginsRegistrationDir()
	pluginsDir := kl.getPluginsDir()
	if err := os.MkdirAll(kl.getRootDir(), 0750); err != nil {
		return fmt.Errorf(&amp;#34;error creating root directory: %v&amp;#34;, err)
	}
	if err := os.MkdirAll(kl.getPodLogsDir(), 0750); err != nil {
		return fmt.Errorf(&amp;#34;error creating pod logs root directory %q: %w&amp;#34;, kl.getPodLogsDir(), err)
	}
	//这段Go代码主要实现了创建Kubernetes集群中两个目录的功能。
	//1. 首先，通过调用kl.getPluginsRegistrationDir()方法获取插件注册目录的路径，然后调用os.MkdirAll()方法创建该目录，
	//如果创建失败，则返回错误信息。
	//2. 接着，通过调用kl.getPluginsDir()方法获取插件目录的路径，同样使用os.MkdirAll()方法创建该目录，如果创建失败，则返回错误信息。
	//3. 最后，通过调用kl.getRootDir()方法获取根目录的路径，并使用os.MkdirAll()方法创建该目录，如果创建失败，则返回错误信息。
	//需要注意的是，创建目录时的权限设置为0750，即所有者具有读、写和执行权限，而组和其他用户只具有读和执行权限。
	//总结起来，这段代码的主要作用是在Kubernetes集群中创建插件注册目录、插件目录和Pod日志目录。

	//该函数的作用是创建Kubelet的数据目录，包括根目录、Pods目录、插件目录、Pod资源目录、检查点目录和Pod日志根目录。
	//函数首先检查根目录的规范性，然后创建各个目录，并返回可能发生的错误。
	if err := kl.hostutil.MakeRShared(kl.getRootDir()); err != nil {
		return fmt.Errorf(&amp;#34;error configuring root directory: %v&amp;#34;, err)
	}
	//该函数尝试将根目录配置为共享目录。如果执行失败，函数将返回一个错误消息。

	if err := os.MkdirAll(kl.getPodsDir(), 0750); err != nil {
		return fmt.Errorf(&amp;#34;error creating pods directory: %v&amp;#34;, err)
	}
	if err := os.MkdirAll(kl.getPluginsDir(), 0750); err != nil {
		return fmt.Errorf(&amp;#34;error creating plugins directory: %v&amp;#34;, err)
	}
	if err := os.MkdirAll(kl.getPluginsRegistrationDir(), 0750); err != nil {
		return fmt.Errorf(&amp;#34;error creating plugins registry directory: %v&amp;#34;, err)
	}
	//这段Go代码中包含了一个匿名的if条件语句，
	//其主要功能是创建三个不同的目录：pod目录、插件目录和插件注册目录。
	//具体来说，它通过调用os.MkdirAll函数来创建目录，并使用kl.getPodsDir()、
	//kl.getPluginsDir()和kl.getPluginsRegistrationDir()方法分别获取目录的路径。
	//如果创建目录时出现错误，函数会返回一个自定义的错误信息。

	if err := os.MkdirAll(kl.getPodResourcesDir(), 0750); err != nil {
		return fmt.Errorf(&amp;#34;error creating podresources directory: %v&amp;#34;, err)
	}
	//该函数的主要功能是在给定路径上创建一个目录。
	//具体而言，它通过调用kl.getPodResourcesDir()获取目录路径，然后使用os.MkdirAll系统调用创建该目录及其所有父目录（如果不存在）。
	//创建目录时指定的权限为0750。如果在创建目录的过程中遇到任何错误，函数将返回一个格式化的错误信息字符串，指示出无法创建目录的原因。

	if utilfeature.DefaultFeatureGate.Enabled(features.ContainerCheckpoint) {
		if err := os.MkdirAll(kl.getCheckpointsDir(), 0700); err != nil {
			return fmt.Errorf(&amp;#34;error creating checkpoint directory: %v&amp;#34;, err)
		}
	}
	//这段Go代码是条件语句，首先检查utilfeature.DefaultFeatureGate是否启用了features.ContainerCheckpoint特性，
	//如果启用了，则尝试创建一个目录，目录路径通过kl.getCheckpointsDir()方法获取，权限设置为0700。
	//如果创建目录过程中出现错误，则返回一个错误信息。

	if selinux.GetEnabled() {
		err := selinux.SetFileLabel(pluginRegistrationDir, config.KubeletPluginsDirSELinuxLabel)
		if err != nil {
			klog.InfoS(&amp;#34;Unprivileged containerized plugins might not work, could not set selinux context on plugin registration dir&amp;#34;, &amp;#34;path&amp;#34;, pluginRegistrationDir, &amp;#34;err&amp;#34;, err)
		}
		err = selinux.SetFileLabel(pluginsDir, config.KubeletPluginsDirSELinuxLabel)
		if err != nil {
			klog.InfoS(&amp;#34;Unprivileged containerized plugins might not work, could not set selinux context on plugins dir&amp;#34;, &amp;#34;path&amp;#34;, pluginsDir, &amp;#34;err&amp;#34;, err)
		}
	}
	return nil
}

//该函数主要功能是检查Selinux是否启用，如果启用，则尝试为插件注册目录和插件目录设置Selinux上下文。
//如果设置失败，函数会记录一条信息但不会影响程序执行，最后返回nil。

// StartGarbageCollection starts garbage collection threads.
func (kl *Kubelet) StartGarbageCollection() {
	loggedContainerGCFailure := false
	go wait.Until(func() {
		ctx := context.Background()
		if err := kl.containerGC.GarbageCollect(ctx); err != nil {
			klog.ErrorS(err, &amp;#34;Container garbage collection failed&amp;#34;)
			kl.recorder.Eventf(kl.nodeRef, v1.EventTypeWarning, events.ContainerGCFailed, err.Error())
			loggedContainerGCFailure = true
			//该函数是Kubelet的StartGarbageCollection方法，用于启动垃圾回收线程。
			//它首先定义了一个布尔变量loggedContainerGCFailure，并初始化为false，用于记录是否已经记录了容器垃圾回收失败的日志。
			//然后使用goroutine启动一个无限循环，定期执行垃圾回收操作。
			//在每次循环中，通过调用kl.containerGC.GarbageCollect方法执行垃圾回收，并检查返回的错误。
			//如果发生错误，会使用klog记录错误日志，并通过kl.recorder.Eventf方法向事件记录器发送一条警告事件，通知容器垃圾回收失败。
			//同时，将loggedContainerGCFailure设置为true，表示已经记录了容器垃圾回收失败的日志。
			//注意，以上只是函数的一部分，代码片段在记录了日志后就中断了，没有展示完整的函数实现。

		} else {
			var vLevel klog.Level = 4
			if loggedContainerGCFailure {
				vLevel = 1
				loggedContainerGCFailure = false
			}
			//这段Go代码是条件语句的else部分，它声明了一个名为vLevel的变量并将其初始化为4。
			//接着，它检查loggedContainerGCFailure变量的值，如果是true，则将vLevel设置为1，并将loggedContainerGCFailure重置为false。
			//这段代码的作用是根据loggedContainerGCFailure的值来设置vLevel的值，并重置loggedContainerGCFailure。

			klog.V(vLevel).InfoS(&amp;#34;Container garbage collection succeeded&amp;#34;)
		}
	}, ContainerGCPeriod, wait.NeverStop)
	//这段这段GoGo代码是使用Kubernetes的logging代码是使用Kubernetes的logging库k库klog，来记录容器垃圾收集log，
	//来记录容器垃圾收集成功的日成功的日志信息。函数体内部志信息。
	//函数体内部使用了klog使用了klog的InfoS方法，的InfoS方法，该方法用于输出该方法用于输出日志信息。
	//日志信息。其中，vLevel是其中，vLevel是日志级别，日志级别，根据具体需求设定。
	//根据具体需求设定。
	//该函数作为一个该函数作为一个匿名函数被传递给一个匿名函数被传递给一个名为After的名为After的函数，
	//并设置了一个定时器函数，并设置了一个定时器，使得，使得该函数会在ContainerGCPeriod时间该函数会在ContainerGCPeriod时间后执行后执行。
	//wait.NeverStop则表示该函数。wait.NeverStop则表示该函数会会一直等待执行，不会停止。

	// when the high threshold is set to 100, and the max age is 0 (or the max age feature is disabled)
	// stub the image GC manager
	if kl.kubeletConfiguration.ImageGCHighThresholdPercent == 100 &amp;amp;&amp;amp;
		(!utilfeature.DefaultFeatureGate.Enabled(features.ImageMaximumGCAge) || kl.kubeletConfiguration.ImageMaximumGCAge.Duration == 0) {
		klog.V(2).InfoS(&amp;#34;ImageGCHighThresholdPercent is set 100 and ImageMaximumGCAge is 0, Disable image GC&amp;#34;)
		return
	}
	//这段代码的功能是检查Kubelet配置中的图像垃圾回收高阈值是否设置为100，并且最大年龄是否为0或最大年龄功能是否禁用。
	//如果是，则禁用图像垃圾回收，并记录相关信息。

	prevImageGCFailed := false
	beganGC := time.Now()
	go wait.Until(func() {
		ctx := context.Background()
		if err := kl.imageManager.GarbageCollect(ctx, beganGC); err != nil {
			if prevImageGCFailed {
				klog.ErrorS(err, &amp;#34;Image garbage collection failed multiple times in a row&amp;#34;)
				// Only create an event for repeated failures
				kl.recorder.Eventf(kl.nodeRef, v1.EventTypeWarning, events.ImageGCFailed, err.Error())
			} else {
				klog.ErrorS(err, &amp;#34;Image garbage collection failed once. Stats initialization may not have completed yet&amp;#34;)
			}
			prevImageGCFailed = true
			//这个Go函数用于定期执行镜像垃圾回收操作。
			//具体来说，它通过wait.Until函数以一定间隔调用一个匿名函数，该匿名函数执行以下操作：
			//1. 创建一个后台上下文ctx。
			//2. 调用kl.imageManager.GarbageCollect方法执行镜像垃圾回收操作，传入ctx和开始垃圾回收的时间beganGC作为参数。
			//3. 如果垃圾回收操作出现错误：
			//- 如果之前垃圾回收操作也失败过，则记录错误日志，并且创建一个事件通知，表示垃圾回收失败多次。
			//- 如果之前垃圾回收操作成功过，则记录错误日志，提示垃圾回收操作失败一次，可能是统计信息初始化未完成。
			//4. 将prevImageGCFailed标志设置为true，表示垃圾回收操作失败。

		} else {
			var vLevel klog.Level = 4
			if prevImageGCFailed {
				vLevel = 1
				prevImageGCFailed = false
			}
			//这段Go代码是条件语句的else部分，它首先声明了一个名为vLevel的klog.Level类型变量，并将其初始化为4。
			//然后，通过判断prevImageGCFailed的值，如果其为true，则将vLevel设置为1，并将prevImageGCFailed重置为false。
			//这段代码的主要作用是根据prevImageGCFailed的值来设置vLevel的值。

			klog.V(vLevel).InfoS(&amp;#34;Image garbage collection succeeded&amp;#34;)
		}
	}, ImageGCPeriod, wait.NeverStop)
	//这是一个Go语言函数，它通过klog库输出一条日志信息。函数体内部使用了klog.V(vLevel).InfoS方法，
	//参数为&amp;#34;Image garbage collection succeeded&amp;#34;，表示成功进行了镜像垃圾回收。
	//该函数每间隔ImageGCPeriod时间执行一次，直到等待过程被终止。

}

// initializeModules will initialize internal modules that do not require the container runtime to be up.
// Note that the modules here must not depend on modules that are not initialized here.
func (kl *Kubelet) initializeModules() error {
	// Prometheus metrics.
	metrics.Register(
		collectors.NewVolumeStatsCollector(kl),
		collectors.NewLogMetricsCollector(kl.StatsProvider.ListPodStats),
	)
	metrics.SetNodeName(kl.nodeName)
	servermetrics.Register()
	//该函数是Kubelet的初始化模块函数，用于初始化内部模块，这些模块不需要容器运行时支持。
	//函数主要进行三个方面的操作：
	//1. 注册Prometheus指标，包括卷统计信息和日志指标；
	//2. 设置节点名称；
	//3. 注册服务器指标。

	// Setup filesystem directories.
	if err := kl.setupDataDirs(); err != nil {
		return err
	}

	// If the container logs directory does not exist, create it.
	if _, err := os.Stat(ContainerLogsDir); err != nil {
		if err := kl.os.MkdirAll(ContainerLogsDir, 0755); err != nil {
			return fmt.Errorf(&amp;#34;failed to create directory %q: %v&amp;#34;, ContainerLogsDir, err)
		}
	}
	//这段Go代码主要实现了两个功能：
	//1. 调用kl.setupDataDirs()函数设置文件系统目录，如果设置失败则返回错误。
	//2. 检查容器日志目录ContainerLogsDir是否存在，如果不存在则尝试创建该目录，如果创建失败则返回错误。
	//其中，kl是一个对象，kl.os是kl对象中的一个操作系统相关的属性，kl.os.MkdirAll()函数用于创建目录。

	// Start the image manager.
	kl.imageManager.Start()

	// Start the certificate manager if it was enabled.
	if kl.serverCertificateManager != nil {
		kl.serverCertificateManager.Start()
	}
	//这段Go代码中包含两个启动操作。
	//首先，它启动了一个名为imageManager的图像管理器。
	//然后，如果serverCertificateManager不为空，则启动证书管理器。

	// Start out of memory watcher.
	if kl.oomWatcher != nil {
		if err := kl.oomWatcher.Start(kl.nodeRef); err != nil {
			return fmt.Errorf(&amp;#34;failed to start OOM watcher: %w&amp;#34;, err)
		}
	}

	// Start resource analyzer
	kl.resourceAnalyzer.Start()

	return nil
	//这段Go代码主要包含两个部分的功能：启动内存溢出监视器（OOM watcher）和启动资源分析器（resource analyzer）。
	//如果kl.oomWatcher不为空，则尝试启动OOM watcher，并通过kl.nodeRef传递节点引用。
	//如果启动失败，则返回相应的错误信息。 然后，无论如何都会启动kl.resourceAnalyzer。
	//最后，如果没有发生错误，则返回nil。

}

// initializeRuntimeDependentModules will initialize internal modules that require the container runtime to be up.
func (kl *Kubelet) initializeRuntimeDependentModules() {
	if err := kl.cadvisor.Start(); err != nil {
		// Fail kubelet and rely on the babysitter to retry starting kubelet.
		klog.ErrorS(err, &amp;#34;Failed to start cAdvisor&amp;#34;)
		os.Exit(1)
	}
	//该函数是Kubelet的一个方法，用于初始化需要容器运行时才能启动的内部模块。
	//主要工作是尝试启动cAdvisor，如果启动失败，则记录错误日志并退出Kubelet进程。

	// trigger on-demand stats collection once so that we have capacity information for ephemeral storage.
	// ignore any errors, since if stats collection is not successful, the container manager will fail to start below.
	kl.StatsProvider.GetCgroupStats(&amp;#34;/&amp;#34;, true)
	// Start container manager.
	node, err := kl.getNodeAnyWay()
	if err != nil {
		// Fail kubelet and rely on the babysitter to retry starting kubelet.
		klog.ErrorS(err, &amp;#34;Kubelet failed to get node info&amp;#34;)
		os.Exit(1)
	}
	//这个Go函数主要执行了两个操作：
	//1. 首先，它调用kl.StatsProvider.GetCgroupStats(&amp;#34;/&amp;#34;, true)来触发一次按需统计信息的收集，以便我们有关临时存储容量的信息。
	//即使统计信息收集不成功，容器管理器也不会启动，因此这里忽略了任何错误。
	//2. 然后，它尝试调用kl.getNodeAnyWay()来启动容器管理器。如果获取节点信息失败，则记录错误并退出Kubelet进程，
	//依赖babysitter来重试启动Kubelet。

	// containerManager must start after cAdvisor because it needs filesystem capacity information
	if err := kl.containerManager.Start(node, kl.GetActivePods, kl.sourcesReady, kl.statusManager, kl.runtimeService, kl.supportLocalStorageCapacityIsolation()); err != nil {
		// Fail kubelet and rely on the babysitter to retry starting kubelet.
		klog.ErrorS(err, &amp;#34;Failed to start ContainerManager&amp;#34;)
		os.Exit(1)
	}
	// eviction manager must start after cadvisor because it needs to know if the container runtime has a dedicated imagefs
	kl.evictionManager.Start(kl.StatsProvider, kl.GetActivePods, kl.PodIsFinished, evictionMonitoringPeriod)
	//这段Go代码是Kubernetes中的kubelet组件的一部分，用于启动containerManager和evictionManager两个组件。
	//首先，如果containerManager启动失败，则会记录错误日志并退出kubelet进程。
	//containerManager的启动需要依赖cAdvisor提供的文件系统容量信息。
	//然后，evictionManager必须在cadvisor之后启动，因为它需要知道容器运行时是否有专门的imagefs。
	//evictionManager的启动需要传入StatsProvider、GetActivePods和PodIsFinished等参数，以及evictionMonitoringPeriod参数。
	//总的来说，这段代码用于在kubelet启动过程中初始化和启动containerManager和evictionManager两个组件。

	// container log manager must start after container runtime is up to retrieve information from container runtime
	// and inform container to reopen log file after log rotation.
	kl.containerLogManager.Start()
	// Adding Registration Callback function for CSI Driver
	kl.pluginManager.AddHandler(pluginwatcherapi.CSIPlugin, plugincache.PluginHandler(csi.PluginHandler))
	// Adding Registration Callback function for DRA Plugin
	if utilfeature.DefaultFeatureGate.Enabled(features.DynamicResourceAllocation) {
		kl.pluginManager.AddHandler(pluginwatcherapi.DRAPlugin, plugincache.PluginHandler(draplugin.NewRegistrationHandler(kl.kubeClient, kl.getNodeAnyWay)))
	}
	// Adding Registration Callback function for Device Manager
	kl.pluginManager.AddHandler(pluginwatcherapi.DevicePlugin, kl.containerManager.GetPluginRegistrationHandler())
	//这段代码主要在启动时执行了三个操作：
	//1. 启动了container log manager，用于从容器运行时检索信息并通知容器在日志轮转后重新打开日志文件。
	//2. 为CSI驱动添加了一个注册回调函数。
	//3. 根据功能门控的开启情况，为DRA插件和设备管理器分别添加了一个注册回调函数。

	// Start the plugin manager
	klog.V(4).InfoS(&amp;#34;Starting plugin manager&amp;#34;)
	go kl.pluginManager.Run(kl.sourcesReady, wait.NeverStop)

	err = kl.shutdownManager.Start()
	if err != nil {
		// The shutdown manager is not critical for kubelet, so log failure, but don&amp;#39;t block Kubelet startup if there was a failure starting it.
		klog.ErrorS(err, &amp;#34;Failed to start node shutdown manager&amp;#34;)
	}
}

//这段Go代码主要实现了启动插件管理器和节点关闭管理器的功能。
//1. 首先，通过klog.V(4).InfoS(&amp;#34;Starting plugin manager&amp;#34;)日志记录插件管理器的启动信息。
//2. 然后，使用go关键字开启一个新的goroutine来运行插件管理器，通过kl.pluginManager.Run(kl.sourcesReady, wait.NeverStop)来实现插件管理器的异步执行。
//其中，kl.sourcesReady表示插件管理器的源准备就绪，wait.NeverStop表示永远不停止运行。
//3. 接着，通过kl.shutdownManager.Start()启动节点关闭管理器。如果启动失败，会通过klog.ErrorS(err, &amp;#34;Failed to start node shutdown manager&amp;#34;)记录错误日志，
//但不会阻塞Kubelet的启动。
//总结：这段代码的目的是为了启动插件管理器和节点关闭管理器，以实现插件管理和节点关闭的功能。

// Run starts the kubelet reacting to config updates
func (kl *Kubelet) Run(updates &amp;lt;-chan kubetypes.PodUpdate) {
	ctx := context.Background()
	if kl.logServer == nil {
		file := http.FileServer(http.Dir(nodeLogDir))
		if utilfeature.DefaultFeatureGate.Enabled(features.NodeLogQuery) &amp;amp;&amp;amp; kl.kubeletConfiguration.EnableSystemLogQuery {
			kl.logServer = http.StripPrefix(&amp;#34;/logs/&amp;#34;, http.HandlerFunc(func(w http.ResponseWriter, req *http.Request) {
				if nlq, errs := newNodeLogQuery(req.URL.Query()); len(errs) &amp;gt; 0 {
					http.Error(w, errs.ToAggregate().Error(), http.StatusBadRequest)
					return
					//该函数是Kubelet的Run方法，用于启动kubelet并响应配置更新。
					//它通过监听updates通道来接收Pod更新信息。在功能上，它主要完成了以下几点：
					//1. 创建了一个后台上下文ctx。
					//2. 检查logServer是否为nil，如果是，则通过file服务器提供节点日志查询服务。
					//如果启用了系统日志查询功能（通过DefaultFeatureGate判断），
					//则使用http.HandlerFunc处理日志查询请求，并通过newNodeLogQuery解析请求URL的查询参数。如果解析出错，则返回400错误。
					//注意：以上是根据代码片段进行的功能描述，可能不完整，具体实现细节还需要结合上下文代码进行理解。

				} else if nlq != nil {
					if req.URL.Path != &amp;#34;/&amp;#34; &amp;amp;&amp;amp; req.URL.Path != &amp;#34;&amp;#34; {
						http.Error(w, &amp;#34;path not allowed in query mode&amp;#34;, http.StatusNotAcceptable)
						return
					}
					if errs := nlq.validate(); len(errs) &amp;gt; 0 {
						http.Error(w, errs.ToAggregate().Error(), http.StatusNotAcceptable)
						return
					}
					//这段代码是处理节点日志查询请求的，如果newNodeLogQuery解析成功且请求的URL路径不是&amp;#34;/&amp;#34;或为空，则返回400错误。
					//然后通过调用nlq.validate()方法验证查询参数的合法性，如果有错误则返回406错误。

					// Validation ensures that the request does not query services and files at the same time
					if len(nlq.Services) &amp;gt; 0 {
						journal.ServeHTTP(w, req)
						return
					}
					// Validation ensures that the request does not explicitly query multiple files at the same time
					if len(nlq.Files) == 1 {
						// Account for the \ being used on Windows clients
						req.URL.Path = filepath.ToSlash(nlq.Files[0])
					}
				}
				//这段Go代码包含了一个条件判断逻辑。
				//首先，它验证请求是否同时查询了服务和文件，如果是，就通过journal.ServeHTTP(w, req)方法处理请求，并返回。
				//接着，如果请求没有同时查询服务和文件，它会进一步验证是否显式地查询了多个文件。
				//如果请求查询了一个文件，代码会将请求的URL路径中的反斜杠（Windows客户端中使用的路径分隔符）转换为斜杠，然后继续处理请求。
				//这段代码的主要目的是确保请求不会同时查询服务和文件，也不会显式地查询多个文件。

				// Fall back in case the caller is directly trying to query a file
				// Example: kubectl get --raw /api/v1/nodes/$name/proxy/logs/foo.log
				file.ServeHTTP(w, req)
			}))
		} else {
			kl.logServer = http.StripPrefix(&amp;#34;/logs/&amp;#34;, file)
			//这段Go代码是关于处理HTTP请求的。 首先，如果条件成立，则使用file.ServeHTTP(w, req)函数来处理请求。
			//file是一个http.FileServer对象，它表示一个文件服务器，w是响应写入器，用于向客户端发送响应，req是客户端的请求。
			//这个函数会根据请求路径从文件服务器中找到对应的文件并将其内容发送给客户端。
			//如果条件不成立，则将kl.logServer设置为http.StripPrefix(&amp;#34;/logs/&amp;#34;, file)的结果。http.StripPrefix()函数是一个处理HTTP请求的中间件，
			//它会将请求路径中的指定前缀去除后再将请求传递给下一个处理程序。
			//在这里，它会将路径中的/logs/前缀去除，然后将处理请求的任务交给file对象。
			//这样做是为了确保文件服务器能够正确地处理请求中的路径。

		}
	}
	if kl.kubeClient == nil {
		klog.InfoS(&amp;#34;No API server defined - no node status update will be sent&amp;#34;)
	}

	// Start the cloud provider sync manager
	if kl.cloudResourceSyncManager != nil {
		go kl.cloudResourceSyncManager.Run(wait.NeverStop)
	}

	if err := kl.initializeModules(); err != nil {
		kl.recorder.Eventf(kl.nodeRef, v1.EventTypeWarning, events.KubeletSetupFailed, err.Error())
		klog.ErrorS(err, &amp;#34;Failed to initialize internal modules&amp;#34;)
		os.Exit(1)
	}
	//这段Go代码是Kubernetes中的一个函数片段，主要功能是初始化Kubelet。
	//1. 首先检查kl.kubeClient是否为nil，如果是，则记录一条日志，表示没有定义API服务器，因此不会发送节点状态更新。
	//2. 然后，如果kl.cloudResourceSyncManager不为nil，则通过go关键字启动云提供商资源同步管理器的运行。
	//3. 最后，调用kl.initializeModules()初始化内部模块，如果初始化失败，则记录事件和错误日志，并通过os.Exit(1)退出程序。

	// Start volume manager
	go kl.volumeManager.Run(kl.sourcesReady, wait.NeverStop)

	if kl.kubeClient != nil {
		// Start two go-routines to update the status.
		//
		// The first will report to the apiserver every nodeStatusUpdateFrequency and is aimed to provide regular status intervals,
		// while the second is used to provide a more timely status update during initialization and runs an one-shot update to the apiserver
		// once the node becomes ready, then exits afterwards.
		//
		// Introduce some small jittering to ensure that over time the requests won&amp;#39;t start
		// accumulating at approximately the same time from the set of nodes due to priority and
		// fairness effect.
		go wait.JitterUntil(kl.syncNodeStatus, kl.nodeStatusUpdateFrequency, 0.04, true, wait.NeverStop)
		go kl.fastStatusUpdateOnce()

		// start syncing lease
		go kl.nodeLeaseController.Run(context.Background())
	}
	//这段Go代码主要启动了几个goroutine来管理Kubernetes节点的状态和租约。
	//1. 首先使用kl.volumeManager.Run()启动了一个volume manager来管理卷。
	//2. 如果kl.kubeClient不为空，则启动了两个goroutine来更新节点状态：
	//- 第一个goroutine使用wait.JitterUntil()函数，每隔kl.nodeStatusUpdateFrequency时间向API服务器报告一次节点状态，提供规律的状态更新。
	//- 第二个goroutine使用kl.fastStatusUpdateOnce()函数，仅在节点准备就绪时向API服务器进行一次状态更新，然后退出。
	//3. 最后，使用kl.nodeLeaseController.Run()启动了一个goroutine来同步租约。

	go wait.Until(kl.updateRuntimeUp, 5*time.Second, wait.NeverStop)

	// Set up iptables util rules
	if kl.makeIPTablesUtilChains {
		kl.initNetworkUtil()
	}

	// Start component sync loops.
	kl.statusManager.Start()

	// Start syncing RuntimeClasses if enabled.
	if kl.runtimeClassManager != nil {
		kl.runtimeClassManager.Start(wait.NeverStop)
	}
	//这段代码是Go语言编写的，主要执行了以下几个操作：
	//1. 使用wait.Until函数，每隔5秒调用一次kl.updateRuntimeUp函数，并且永远不停止。
	//2. 如果kl.makeIPTablesUtilChains为true，则调用kl.initNetworkUtil函数初始化iptables工具规则。
	//3. 调用kl.statusManager.Start()函数，启动组件同步循环。
	//4. 如果kl.runtimeClassManager不为nil，则调用kl.runtimeClassManager.Start(wait.NeverStop)函数，启动RuntimeClasses的同步。
	//这段代码的主要目的是在Kubernetes中启动组件和功能的同步，并设置iptables规则。

	// Start the pod lifecycle event generator.
	kl.pleg.Start()

	// Start eventedPLEG only if EventedPLEG feature gate is enabled.
	if utilfeature.DefaultFeatureGate.Enabled(features.EventedPLEG) {
		kl.eventedPleg.Start()
	}

	kl.syncLoop(ctx, updates, kl)
}

//这段Go代码主要涉及到了Kubernetes中Pod生命周期事件生成器的启动和同步循环的逻辑。
//首先，kl.pleg.Start()启动了Pod生命周期事件生成器，它会监控Pod的状态变化，并生成相应的事件。
//接着，通过判断features.EventedPLEG是否启用，来决定是否启动kl.eventedPleg.Start()。
//EventedPLEG是一种更高效的事件处理机制，当有多个事件发生时，它能够批量处理这些事件，减少处理时间。
//最后，kl.syncLoop(ctx, updates, kl)启动了一个同步循环，它会不断地从updates通道中获取Pod状态的更新，并调用kl对象的方法来处理这些更新。
//这个循环会一直运行，直到ctx上下文被取消或终止。
//总的来说，这段代码主要是启动了Kubernetes中Pod生命周期事件的监控和处理逻辑，确保能够及时地处理Pod状态的变化。

// SyncPod is the transaction script for the sync of a single pod (setting up)
// a pod. This method is reentrant and expected to converge a pod towards the
// desired state of the spec. The reverse (teardown) is handled in
// SyncTerminatingPod and SyncTerminatedPod. If SyncPod exits without error,
// then the pod runtime state is in sync with the desired configuration state
// (pod is running). If SyncPod exits with a transient error, the next
// invocation of SyncPod is expected to make progress towards reaching the
// desired state. SyncPod exits with isTerminal when the pod was detected to
// have reached a terminal lifecycle phase due to container exits (for
// RestartNever or RestartOnFailure) and the next method invoked will be
// SyncTerminatingPod. If the pod terminates for any other reason, SyncPod
// will receive a context cancellation and should exit as soon as possible.
//
// Arguments:
//
// updateType - whether this is a create (first time) or an update, should
// only be used for metrics since this method must be reentrant
//
// pod - the pod that is being set up
//
// mirrorPod - the mirror pod known to the kubelet for this pod, if any
//
// podStatus - the most recent pod status observed for this pod which can
// be used to determine the set of actions that should be taken during
// this loop of SyncPod
//
// The workflow is:
// - If the pod is being created, record pod worker start latency
// - Call generateAPIPodStatus to prepare an v1.PodStatus for the pod
// - If the pod is being seen as running for the first time, record pod
// start latency
// - Update the status of the pod in the status manager
// - Stop the pod&amp;#39;s containers if it should not be running due to soft
// admission
// - Ensure any background tracking for a runnable pod is started
// - Create a mirror pod if the pod is a static pod, and does not
// already have a mirror pod
// - Create the data directories for the pod if they do not exist
// - Wait for volumes to attach/mount
// - Fetch the pull secrets for the pod
// - Call the container runtime&amp;#39;s SyncPod callback
// - Update the traffic shaping for the pod&amp;#39;s ingress and egress limits
//
// If any step of this workflow errors, the error is returned, and is repeated
// on the next SyncPod call.
//
// This operation writes all events that are dispatched in order to provide
// the most accurate information possible about an error situation to aid debugging.
// Callers should not write an event if this operation returns an error.
func (kl *Kubelet) SyncPod(ctx context.Context, updateType kubetypes.SyncPodType, pod, mirrorPod *v1.Pod, podStatus *kubecontainer.PodStatus) (isTerminal bool, err error) {
	ctx, otelSpan := kl.tracer.Start(ctx, &amp;#34;syncPod&amp;#34;, trace.WithAttributes(
		semconv.K8SPodUIDKey.String(string(pod.UID)),
		attribute.String(&amp;#34;k8s.pod&amp;#34;, klog.KObj(pod).String()),
		semconv.K8SPodNameKey.String(pod.Name),
		attribute.String(&amp;#34;k8s.pod.update_type&amp;#34;, updateType.String()),
		semconv.K8SNamespaceNameKey.String(pod.Namespace),
		//该函数是Kubelet的syncPod方法，用于同步Pod的状态。它根据传入的Pod、mirrorPod和podStatus参数，更新Pod的状态，
		//并返回一个布尔值和一个错误。
		//函数使用了OpenTelemetry进行跟踪，并设置了多个属性，
		//包括Pod的UID、名称、命名空间和更新类型等。

	))
	klog.V(4).InfoS(&amp;#34;SyncPod enter&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod), &amp;#34;podUID&amp;#34;, pod.UID)
	defer func() {
		klog.V(4).InfoS(&amp;#34;SyncPod exit&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod), &amp;#34;podUID&amp;#34;, pod.UID, &amp;#34;isTerminal&amp;#34;, isTerminal)
		otelSpan.End()
	}()
	//该函数是Kubelet的syncPod方法，用于同步Pod的状态。
	//它根据传入的Pod、mirrorPod和podStatus参数，更新Pod的状态，并返回一个布尔值isTerminal和一个错误err。
	//在函数开始时，它通过kl.tracer.Start创建了一个名为syncPod的trace span，并在函数结束时通过otelSpan.End结束该span。
	//函数内部的具体逻辑没有在代码片段中展示出来。
	//这段Go代码主要使用了klog和OpenTelemetry两个库，功能是在进入和退出SyncPod函数时记录日志，并通过OpenTelemetry跟踪操作。
	//- klog.V(4).InfoS(&amp;#34;SyncPod enter&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod), &amp;#34;podUID&amp;#34;, pod.UID)：记录SyncPod函数进入的日志，日志级别为4，
	//包含pod的信息和podUID。
	//- defer关键字定义了一个延迟执行的函数，会在SyncPod函数退出时执行。
	//- klog.V(4).InfoS(&amp;#34;SyncPod exit&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod), &amp;#34;podUID&amp;#34;, pod.UID, &amp;#34;isTerminal&amp;#34;, isTerminal)：
	//记录SyncPod函数退出的日志，包含pod的信息、podUID和isTerminal。
	//- otelSpan.End()：结束OpenTelemetry的跟踪操作。

	// Latency measurements for the main workflow are relative to the
	// first time the pod was seen by kubelet.
	var firstSeenTime time.Time
	if firstSeenTimeStr, ok := pod.Annotations[kubetypes.ConfigFirstSeenAnnotationKey]; ok {
		firstSeenTime = kubetypes.ConvertToTimestamp(firstSeenTimeStr).Get()
	}
	//这段Go代码主要进行了一个延迟测量的操作。
	//具体来说，它首先检查了一个名为pod.Annotations的结构体中是否存在键为kubetypes.ConfigFirstSeenAnnotationKey的元素，
	//如果存在，则将该元素的值转换为时间戳类型并赋值给变量firstSeenTime。
	//这个操作主要是为了记录主工作流的延迟时间，而延迟的起始时间是相对于kubelet第一次看到pod的时间来计算的。

	// Record pod worker start latency if being created
	// TODO: make pod workers record their own latencies
	if updateType == kubetypes.SyncPodCreate {
		if !firstSeenTime.IsZero() {
			// This is the first time we are syncing the pod. Record the latency
			// since kubelet first saw the pod if firstSeenTime is set.
			metrics.PodWorkerStartDuration.Observe(metrics.SinceInSeconds(firstSeenTime))
		} else {
			klog.V(3).InfoS(&amp;#34;First seen time not recorded for pod&amp;#34;,
				&amp;#34;podUID&amp;#34;, pod.UID,
				&amp;#34;pod&amp;#34;, klog.KObj(pod))
		}
	}
	//该函数用于记录Pod创建时的工作线程启动延迟。
	//如果updateType等于kubetypes.SyncPodCreate，则会判断firstSeenTime是否为零，如果不为零，
	//则记录自kubelet首次看到Pod以来的延迟；
	//如果为零，则输出日志信息。

	// Generate final API pod status with pod and status manager status
	apiPodStatus := kl.generateAPIPodStatus(pod, podStatus, false)
	// The pod IP may be changed in generateAPIPodStatus if the pod is using host network. (See #24576)
	// TODO(random-liu): After writing pod spec into container labels, check whether pod is using host network, and
	// set pod IP to hostIP directly in runtime.GetPodStatus
	podStatus.IPs = make([]string, 0, len(apiPodStatus.PodIPs))
	for _, ipInfo := range apiPodStatus.PodIPs {
		podStatus.IPs = append(podStatus.IPs, ipInfo.IP)
	}
	if len(podStatus.IPs) == 0 &amp;amp;&amp;amp; len(apiPodStatus.PodIP) &amp;gt; 0 {
		podStatus.IPs = []string{apiPodStatus.PodIP}
	}
	//这段Go代码是用于生成最终的API Pod状态的。
	//它首先调用kl.generateAPIPodStatus(pod, podStatus, false)来生成一个API Pod状态对象，
	//然后根据这个对象来设置podStatus对象的IPs字段。
	//如果pod使用了宿主机网络，那么在generateAPIPodStatus函数中可能会更改pod的IP地址。
	//代码中还提到，以后可能会将pod的规格写入容器标签，并在runtime.GetPodStatus中直接将pod IP设置为宿主机IP。
	//最后，如果podStatus.IPs为空且apiPodStatus.PodIP不为空，则将apiPodStatus.PodIP作为podStatus.IPs的值。

	// If the pod is terminal, we don&amp;#39;t need to continue to setup the pod
	if apiPodStatus.Phase == v1.PodSucceeded || apiPodStatus.Phase == v1.PodFailed {
		kl.statusManager.SetPodStatus(pod, apiPodStatus)
		isTerminal = true
		return isTerminal, nil
	}
	//该函数用于判断Pod是否处于终止状态（PodSucceeded或PodFailed），如果是，则设置Pod状态为终止状态，并返回true。否则返回false。

	// If the pod should not be running, we request the pod&amp;#39;s containers be stopped. This is not the same
	// as termination (we want to stop the pod, but potentially restart it later if soft admission allows
	// it later). Set the status and phase appropriately
	runnable := kl.canRunPod(pod)
	if !runnable.Admit {
		// Pod is not runnable; and update the Pod and Container statuses to why.
		if apiPodStatus.Phase != v1.PodFailed &amp;amp;&amp;amp; apiPodStatus.Phase != v1.PodSucceeded {
			apiPodStatus.Phase = v1.PodPending
		}
		apiPodStatus.Reason = runnable.Reason
		apiPodStatus.Message = runnable.Message
		// Waiting containers are not creating.
		const waitingReason = &amp;#34;Blocked&amp;#34;
		//该函数用于判断Pod是否应该运行，并根据判断结果更新Pod的状态和阶段。
		//如果Pod不可运行，则将其状态和阶段更新为Pending，并设置相应的Reason和Message。
		//其中，如果Pod的状态不是Failed或Succeeded，则将其阶段更新为Pending。

		for _, cs := range apiPodStatus.InitContainerStatuses {
			if cs.State.Waiting != nil {
				cs.State.Waiting.Reason = waitingReason
			}
		}
		for _, cs := range apiPodStatus.ContainerStatuses {
			if cs.State.Waiting != nil {
				cs.State.Waiting.Reason = waitingReason
			}
		}
	}
	//这段Go代码中有两个for循环，分别遍历了apiPodStatus.InitContainerStatuses和apiPodStatus.ContainerStatuses两个切片。
	//其中，如果遍历到的ContainerStatus的State.Waiting不为nil，则将其Reason字段设置为waitingReason。
	//这段代码的作用是给Pod中所有处于等待状态的初始化容器和容器设置等待原因。

	// Record the time it takes for the pod to become running
	// since kubelet first saw the pod if firstSeenTime is set.
	existingStatus, ok := kl.statusManager.GetPodStatus(pod.UID)
	if !ok || existingStatus.Phase == v1.PodPending &amp;amp;&amp;amp; apiPodStatus.Phase == v1.PodRunning &amp;amp;&amp;amp;
		!firstSeenTime.IsZero() {
		metrics.PodStartDuration.Observe(metrics.SinceInSeconds(firstSeenTime))
	}
	//这段Go代码的功能是记录Pod从kubelet首次看到它到变为运行状态所花费的时间。
	//具体来说，它首先从kl.statusManager获取Pod的现有状态，
	//如果获取成功并且Pod的现有状态为Pending，而apiPodStatus的Phase为Running，
	//并且firstSeenTime不为零，则使用metrics.SinceInSeconds函数记录从firstSeenTime到当前时间的时间差，
	//并将其作为观察值传递给metrics.PodStartDuration。

	kl.statusManager.SetPodStatus(pod, apiPodStatus)

	// Pods that are not runnable must be stopped - return a typed error to the pod worker
	if !runnable.Admit {
		klog.V(2).InfoS(&amp;#34;Pod is not runnable and must have running containers stopped&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod), &amp;#34;podUID&amp;#34;, pod.UID, &amp;#34;message&amp;#34;, runnable.Message)
		var syncErr error
		p := kubecontainer.ConvertPodStatusToRunningPod(kl.getRuntime().Type(), podStatus)
		if err := kl.killPod(ctx, pod, p, nil); err != nil {
			if !wait.Interrupted(err) {
				kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedToKillPod, &amp;#34;error killing pod: %v&amp;#34;, err)
				syncErr = fmt.Errorf(&amp;#34;error killing pod: %w&amp;#34;, err)
				utilruntime.HandleError(syncErr)
				//该函数用于设置Pod的状态，并根据Pod是否可运行来决定是否停止Pod。如果Pod不可运行，则通过kl.killPod函数停止Pod，并记录事件和错误信息。
			}
		} else {
			// There was no error killing the pod, but the pod cannot be run.
			// Return an error to signal that the sync loop should back off.
			syncErr = fmt.Errorf(&amp;#34;pod cannot be run: %v&amp;#34;, runnable.Message)
		}
		return false, syncErr
		//这段Go代码是一个函数的一部分，根据给定的条件执行不同的操作，并返回两个值：一个布尔值和一个错误。
		//首先，它检查是否有错误杀死Pod（容器），如果有，则将错误记录并返回。
		//如果没有错误，但Pod无法运行，则将自定义错误返回，以指示同步循环应退避。
		//最后，该函数返回false和同步错误。

	}

	// If the network plugin is not ready, only start the pod if it uses the host network
	if err := kl.runtimeState.networkErrors(); err != nil &amp;amp;&amp;amp; !kubecontainer.IsHostNetworkPod(pod) {
		kl.recorder.Eventf(pod, v1.EventTypeWarning, events.NetworkNotReady, &amp;#34;%s: %v&amp;#34;, NetworkNotReadyErrorMsg, err)
		return false, fmt.Errorf(&amp;#34;%s: %v&amp;#34;, NetworkNotReadyErrorMsg, err)
	}
	//该函数主要检查网络插件是否准备就绪，如果未就绪且Pod不使用宿主机网络，则记录事件并返回错误。

	// ensure the kubelet knows about referenced secrets or configmaps used by the pod
	if !kl.podWorkers.IsPodTerminationRequested(pod.UID) {
		if kl.secretManager != nil {
			kl.secretManager.RegisterPod(pod)
		}
		if kl.configMapManager != nil {
			kl.configMapManager.RegisterPod(pod)
		}
	}
	//该函数用于确保kubelet知道pod中使用到的secret或configmap。它首先检查pod是否被请求终止，
	//如果不是，则分别调用secretManager和configMapManager的RegisterPod方法来注册pod。
	// Create Cgroups for the pod and apply resource parameters
	// to them if cgroups-per-qos flag is enabled.
	pcm := kl.containerManager.NewPodContainerManager()
	// If pod has already been terminated then we need not create
	// or update the pod&amp;#39;s cgroup
	// TODO: once context cancellation is added this check can be removed
	if !kl.podWorkers.IsPodTerminationRequested(pod.UID) {
		// When the kubelet is restarted with the cgroups-per-qos
		// flag enabled, all the pod&amp;#39;s running containers
		// should be killed intermittently and brought back up
		// under the qos cgroup hierarchy.
		// Check if this is the pod&amp;#39;s first sync
		firstSync := true
		//这段Go代码是Kubernetes中的一个片段，用于创建和管理Pod的Cgroups。
		//函数首先创建一个PodContainerManager实例，然后检查Pod是否已经被终止。
		//如果Pod没有被终止，则会检查是否是Pod的第一个同步。如果是第一次同步，将会为Pod创建或更新Cgroups，并应用资源参数。
		//这段代码的主要目的是在Kubernetes中管理Pod的资源限制和隔离。

		for _, containerStatus := range apiPodStatus.ContainerStatuses {
			if containerStatus.State.Running != nil {
				firstSync = false
				break
			}
		}
		// Don&amp;#39;t kill containers in pod if pod&amp;#39;s cgroups already
		// exists or the pod is running for the first time
		podKilled := false
		//这段Go代码是用于遍历Pod中的容器状态，检查是否有正在运行的容器。
		//如果存在正在运行的容器，则将firstSync标记为false并退出循环。
		//接下来，根据podKilled的值决定是否终止Pod中的容器。

		if !pcm.Exists(pod) &amp;amp;&amp;amp; !firstSync {
			p := kubecontainer.ConvertPodStatusToRunningPod(kl.getRuntime().Type(), podStatus)
			if err := kl.killPod(ctx, pod, p, nil); err == nil {
				if wait.Interrupted(err) {
					return false, err
				}
				podKilled = true
			} else {
				klog.ErrorS(err, &amp;#34;KillPod failed&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod), &amp;#34;podStatus&amp;#34;, podStatus)
			}
		}
		//这段Go代码是一个条件判断语句，其主要功能是在特定条件下杀死一个Pod。
		//具体来说，函数首先检查Pod是否存在于pcm中，以及firstSync是否为false。
		//如果两个条件都满足，则会将podStatus转换为一个运行中的Pod，并尝试通过killPod函数杀死该Pod。
		//如果杀死操作成功，且返回的错误是由于等待中断导致的，则返回false和该错误。
		//如果杀死操作失败，则会记录一个错误日志。

		// Create and Update pod&amp;#39;s Cgroups
		// Don&amp;#39;t create cgroups for run once pod if it was killed above
		// The current policy is not to restart the run once pods when
		// the kubelet is restarted with the new flag as run once pods are
		// expected to run only once and if the kubelet is restarted then
		// they are not expected to run again.
		// We don&amp;#39;t create and apply updates to cgroup if its a run once pod and was killed above
		if !(podKilled &amp;amp;&amp;amp; pod.Spec.RestartPolicy == v1.RestartPolicyNever) {
			if !pcm.Exists(pod) {
				if err := kl.containerManager.UpdateQOSCgroups(); err != nil {
					klog.V(2).InfoS(&amp;#34;Failed to update QoS cgroups while syncing pod&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod), &amp;#34;err&amp;#34;, err)
				}
				if err := pcm.EnsureExists(pod); err != nil {
					kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedToCreatePodContainer, &amp;#34;unable to ensure pod container exists: %v&amp;#34;, err)
					return false, fmt.Errorf(&amp;#34;failed to ensure that the pod: %v cgroups exist and are correctly applied: %v&amp;#34;, pod.UID, err)
				}
			}
		}
	}
	//这段Go代码是Kubernetes的一部分，用于创建和更新Pod的Cgroups。Cgroups是Linux操作系统中的一个功能，可以限制和监控进程的资源使用。
	//这段代码首先检查Pod是否被杀死且重启策略为Never，如果是，则不创建Cgroups。
	//然后，它检查Pod的Cgroups是否存在，如果不存在，则尝试更新QoS cgroups并确保Pod的Cgroups存在并正确应用。
	//如果更新QoS cgroups或确保Cgroups存在失败，则记录事件并返回错误。

	// Create Mirror Pod for Static Pod if it doesn&amp;#39;t already exist
	if kubetypes.IsStaticPod(pod) {
		deleted := false
		if mirrorPod != nil {
			if mirrorPod.DeletionTimestamp != nil || !kubepod.IsMirrorPodOf(mirrorPod, pod) {
				// The mirror pod is semantically different from the static pod. Remove
				// it. The mirror pod will get recreated later.
				klog.InfoS(&amp;#34;Trying to delete pod&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod), &amp;#34;podUID&amp;#34;, mirrorPod.ObjectMeta.UID)
				podFullName := kubecontainer.GetPodFullName(pod)
				var err error
				deleted, err = kl.mirrorPodClient.DeleteMirrorPod(podFullName, &amp;amp;mirrorPod.ObjectMeta.UID)
				if deleted {
					klog.InfoS(&amp;#34;Deleted mirror pod because it is outdated&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(mirrorPod))
				} else if err != nil {
					klog.ErrorS(err, &amp;#34;Failed deleting mirror pod&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(mirrorPod))
				}
			}
		}
		//这段Go代码主要功能是检查给定的Pod是否为静态Pod，如果是，则检查是否存在与之对应的镜像Pod。
		//如果存在但与静态Pod语义不同或已被标记删除，则删除该镜像Pod。
		//具体流程如下：
		//1. 首先判断给定的Pod是否为静态Pod，如果是则继续执行后续逻辑。
		//2. 检查是否存在镜像Pod，如果存在则进一步判断镜像Pod是否与静态Pod语义相同或未被标记删除。
		//3. 如果镜像Pod与静态Pod语义不同或已被标记删除，则尝试删除该镜像Pod。
		//4. 如果删除成功，则记录日志；如果删除失败，则记录错误日志。
		//该函数通过调用mirrorPodClient的DeleteMirrorPod方法来删除镜像Pod。

		if mirrorPod == nil || deleted {
			node, err := kl.GetNode()
			if err != nil {
				klog.V(4).ErrorS(err, &amp;#34;No need to create a mirror pod, since failed to get node info from the cluster&amp;#34;, &amp;#34;node&amp;#34;, klog.KRef(&amp;#34;&amp;#34;, string(kl.nodeName)))
			} else if node.DeletionTimestamp != nil {
				klog.V(4).InfoS(&amp;#34;No need to create a mirror pod, since node has been removed from the cluster&amp;#34;, &amp;#34;node&amp;#34;, klog.KRef(&amp;#34;&amp;#34;, string(kl.nodeName)))
			} else {
				klog.V(4).InfoS(&amp;#34;Creating a mirror pod for static pod&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod))
				if err := kl.mirrorPodClient.CreateMirrorPod(pod); err != nil {
					klog.ErrorS(err, &amp;#34;Failed creating a mirror pod for&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod))
				}
			}
		}
	}
	//这段代码主要功能是根据条件判断是否需要创建一个镜像Pod。
	//如果mirrorPod为nil或deleted标志为true，则执行以下操作：
	//1. 通过kl.GetNode()获取节点信息。
	//2. 如果获取节点信息时发生错误，则记录错误日志。
	//3. 如果节点信息获取成功，检查节点是否已被删除。
	//4. 如果节点已被删除，则记录一条信息日志。
	//5. 如果节点未被删除，则记录一条信息日志，表示正在创建镜像Pod。
	//6. 调用kl.mirrorPodClient.CreateMirrorPod(pod)尝试创建镜像Pod，如果创建失败则记录错误日志。
	//这段代码通过日志记录了操作的结果，并根据条件判断是否需要创建镜像Pod。

	// Make data directories for the pod
	if err := kl.makePodDataDirs(pod); err != nil {
		kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedToMakePodDataDirectories, &amp;#34;error making pod data directories: %v&amp;#34;, err)
		klog.ErrorS(err, &amp;#34;Unable to make pod data directories for pod&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod))
		return false, err
	}
	//该函数是用于为Pod创建数据目录的。它首先调用kl.makePodDataDirs(pod)函数来创建目录，如果创建失败，则记录事件并返回错误。

	// Wait for volumes to attach/mount
	if err := kl.volumeManager.WaitForAttachAndMount(ctx, pod); err != nil {
		if !wait.Interrupted(err) {
			kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedMountVolume, &amp;#34;Unable to attach or mount volumes: %v&amp;#34;, err)
			klog.ErrorS(err, &amp;#34;Unable to attach or mount volumes for pod; skipping pod&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod))
		}
		return false, err
	}
	//这段代码的功能是等待卷附加和挂载操作完成。
	//具体而言，它通过调用kl.volumeManager.WaitForAttachAndMount(ctx, pod)来等待卷的附加和挂载操作完成。
	//如果操作失败且不是由于中断引起，则会通过kl.recorder.Eventf()记录事件，并通过klog.ErrorS()记录错误日志，并最终返回false和错误信息err。

	// Fetch the pull secrets for the pod
	pullSecrets := kl.getPullSecretsForPod(pod)

	// Ensure the pod is being probed
	kl.probeManager.AddPod(pod)

	if utilfeature.DefaultFeatureGate.Enabled(features.InPlacePodVerticalScaling) {
		// Handle pod resize here instead of doing it in HandlePodUpdates because
		// this conveniently retries any Deferred resize requests
		// TODO(vinaykul,InPlacePodVerticalScaling): Investigate doing this in HandlePodUpdates + periodic SyncLoop scan
		// See: https://github.com/kubernetes/kubernetes/pull/102884#discussion_r663160060
		if kl.podWorkers.CouldHaveRunningContainers(pod.UID) &amp;amp;&amp;amp; !kubetypes.IsStaticPod(pod) {
			pod = kl.handlePodResourcesResize(pod)
		}
	}
	//这段Go代码是Kubernetes中kubelet的一个片段，主要功能是处理Pod的镜像拉取凭证、添加Pod到探测管理器中、以及处理Pod的资源缩放。
	//1. 首先，函数getPullSecretsForPod从kl（kubelet实例）中获取Pod的镜像拉取凭证pullSecrets。
	//2. 然后，通过调用probeManager.AddPod方法，将该Pod添加到探测管理器中，以确保Pod被探测。
	//3. 接着，通过判断是否启用了InPlacePodVerticalScaling特性，决定是否处理Pod的资源缩放。
	//如果启用了该特性，并且当前Pod有运行中的容器且不是静态Pod，则调用handlePodResourcesResize方法处理Pod的资源缩放。
	//4. handlePodResourcesResize方法会尝试处理Pod的资源缩放请求，并返回缩放后的Pod对象。
	//这段代码的主要目的是在Pod更新时处理镜像拉取凭证、探测和资源缩放等操作。
	//其中，资源缩放功能通过检查Pod的状态和特性门控来决定是否执行，以实现Pod的垂直缩放。

	// TODO(#113606): use cancellation from the incoming context parameter, which comes from the pod worker.
	// Currently, using cancellation from that context causes test failures. To remove this WithoutCancel,
	// any wait.Interrupted errors need to be filtered from result and bypass the reasonCache - cancelling
	// the context for SyncPod is a known and deliberate error, not a generic error.
	// Use WithoutCancel instead of a new context.TODO() to propagate trace context
	// Call the container runtime&amp;#39;s SyncPod callback
	sctx := context.WithoutCancel(ctx)
	result := kl.containerRuntime.SyncPod(sctx, pod, podStatus, pullSecrets, kl.backOff)
	kl.reasonCache.Update(pod.UID, result)
	if err := result.Error(); err != nil {
		// Do not return error if the only failures were pods in backoff
		for _, r := range result.SyncResults {
			if r.Error != kubecontainer.ErrCrashLoopBackOff &amp;amp;&amp;amp; r.Error != images.ErrImagePullBackOff {
				// Do not record an event here, as we keep all event logging for sync pod failures
				// local to container runtime, so we get better errors.
				return false, err
			}
		}

		return false, nil
	}
	//这个Go函数用于同步Pod，并执行以下操作：
	//- 创建一个不取消的上下文sctx。
	//- 调用容器运行时的SyncPod回调函数，传入sctx、Pod、Pod状态、pullSecrets和backOff。
	//- 使用结果更新Pod的状态。
	//- 如果有错误，遍历结果中的同步结果，如果错误不是ErrCrashLoopBackOff或ErrImagePullBackOff，则返回错误。
	//- 否则，返回false和nil。

	if utilfeature.DefaultFeatureGate.Enabled(features.InPlacePodVerticalScaling) &amp;amp;&amp;amp; isPodResizeInProgress(pod, &amp;amp;apiPodStatus) {
		// While resize is in progress, periodically call PLEG to update pod cache
		runningPod := kubecontainer.ConvertPodStatusToRunningPod(kl.getRuntime().Type(), podStatus)
		if err, _ := kl.pleg.UpdateCache(&amp;amp;runningPod, pod.UID); err != nil {
			klog.ErrorS(err, &amp;#34;Failed to update pod cache&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod))
			return false, err
		}
	}

	return false, nil
}

//这段Go代码的功能是在Kubernetes中处理Pod的垂直扩展。如果启用了InPlacePodVerticalScaling功能并且Pod正在进行大小调整，
//则会定期调用PLEG来更新Pod的缓存。
//如果更新失败，则会记录错误并返回错误信息。如果条件不满足，则直接返回false和nil。

// SyncTerminatingPod is expected to terminate all running containers in a pod. Once this method
// returns without error, the pod is considered to be terminated and it will be safe to clean up any
// pod state that is tied to the lifetime of running containers. The next method invoked will be
// SyncTerminatedPod. This method is expected to return with the grace period provided and the
// provided context may be cancelled if the duration is exceeded. The method may also be interrupted
// with a context cancellation if the grace period is shortened by the user or the kubelet (such as
// during eviction). This method is not guaranteed to be called if a pod is force deleted from the
// configuration and the kubelet is restarted - SyncTerminatingRuntimePod handles those orphaned
// pods.
func (kl *Kubelet) SyncTerminatingPod(_ context.Context, pod *v1.Pod, podStatus *kubecontainer.PodStatus, gracePeriod *int64, podStatusFn func(*v1.PodStatus)) error {
	// TODO(#113606): connect this with the incoming context parameter, which comes from the pod worker.
	// Currently, using that context causes test failures.
	ctx, otelSpan := kl.tracer.Start(context.Background(), &amp;#34;syncTerminatingPod&amp;#34;, trace.WithAttributes(
		semconv.K8SPodUIDKey.String(string(pod.UID)),
		attribute.String(&amp;#34;k8s.pod&amp;#34;, klog.KObj(pod).String()),
		semconv.K8SPodNameKey.String(pod.Name),
		semconv.K8SNamespaceNameKey.String(pod.Namespace),
	))
	defer otelSpan.End()
	klog.V(4).InfoS(&amp;#34;SyncTerminatingPod enter&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod), &amp;#34;podUID&amp;#34;, pod.UID)
	defer klog.V(4).InfoS(&amp;#34;SyncTerminatingPod exit&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod), &amp;#34;podUID&amp;#34;, pod.UID)
	//该函数是Kubelet的一个方法，用于同步终止Pod中的所有运行中的容器。
	//当此方法在没有错误的情况下返回时，Pod被认为已经终止，并且可以安全地清理与运行容器的生命周期相关的任何Pod状态。
	//下一个调用的方法将是SyncTerminatedPod。
	//此方法期望在提供的优雅期间内返回，并且提供的上下文可能会在持续时间超过时被取消。
	//该方法也可能因用户或kubelet（如驱逐期间）缩短优雅期间而被上下文取消。
	//如果Pod被强制从配置中删除，并且kubelet被重新启动，则不会调用此方法
	//- SyncTerminatingRuntimePod处理这些孤儿Pod。
	//函数内部使用了OpenTelemetry进行日志记录和跟踪，并通过klog记录进入和退出函数的日志。

	apiPodStatus := kl.generateAPIPodStatus(pod, podStatus, false)
	if podStatusFn != nil {
		podStatusFn(&amp;amp;apiPodStatus)
	}
	kl.statusManager.SetPodStatus(pod, apiPodStatus)
	//这段Go代码中的函数实现了以下功能：
	//- 首先，它调用kl.generateAPIPodStatus(pod, podStatus, false)生成一个apiPodStatus对象。
	//- 然后，如果podStatusFn不为空，它将调用podStatusFn(&amp;amp;apiPodStatus)来对apiPodStatus进行进一步的处理。
	//- 最后，它调用kl.statusManager.SetPodStatus(pod, apiPodStatus)来设置Pod的状态。
	//总的来说，这个函数通过调用kl.generateAPIPodStatus生成一个Pod状态对象apiPodStatus，并将其设置为Pod的实际状态。
	//如果提供了podStatusFn函数，它还会对apiPodStatus进行额外的处理。

	if gracePeriod != nil {
		klog.V(4).InfoS(&amp;#34;Pod terminating with grace period&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod), &amp;#34;podUID&amp;#34;, pod.UID, &amp;#34;gracePeriod&amp;#34;, *gracePeriod)
	} else {
		klog.V(4).InfoS(&amp;#34;Pod terminating with grace period&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod), &amp;#34;podUID&amp;#34;, pod.UID, &amp;#34;gracePeriod&amp;#34;, nil)
	}
	//这段Go代码主要进行日志记录，根据gracePeriod是否为nil，记录不同级别的日志信息。
	//- 如果gracePeriod不为nil，则调用klog.V(4).InfoS函数记录日志，
	//日志内容包括：&amp;#34;Pod terminating with grace period&amp;#34;、&amp;#34;pod&amp;#34;、&amp;#34;podUID&amp;#34;、&amp;#34;gracePeriod&amp;#34;。
	//- 如果gracePeriod为nil，则调用klog.V(4).InfoS函数记录日志，
	//日志内容包括：&amp;#34;Pod terminating with grace period&amp;#34;、&amp;#34;pod&amp;#34;、&amp;#34;podUID&amp;#34;、&amp;#34;gracePeriod&amp;#34;，其中&amp;#34;gracePeriod&amp;#34;的值为nil。
	//这里的klog是一个日志库，V(4)表示日志级别为4，InfoS表示记录信息级别的日志，并可以传入多个键值对参数来丰富日志内容。
	//pod和pod.UID是函数的参数，用于提供上下文信息。

	kl.probeManager.StopLivenessAndStartup(pod)

	p := kubecontainer.ConvertPodStatusToRunningPod(kl.getRuntime().Type(), podStatus)
	if err := kl.killPod(ctx, pod, p, gracePeriod); err != nil {
		kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedToKillPod, &amp;#34;error killing pod: %v&amp;#34;, err)
		// there was an error killing the pod, so we return that error directly
		utilruntime.HandleError(err)
		return err
	}
	//该函数的功能是停止Pod的liveness和startup探针管理器，将Pod状态转换为运行中的Pod，然后尝试杀死Pod。
	//如果杀死Pod时出现错误，则记录事件并返回该错误。

	// Once the containers are stopped, we can stop probing for liveness and readiness.
	// TODO: once a pod is terminal, certain probes (liveness exec) could be stopped immediately after
	// the detection of a container shutdown or (for readiness) after the first failure. Tracked as
	// https://github.com/kubernetes/kubernetes/issues/107894 although may not be worth optimizing.
	kl.probeManager.RemovePod(pod)
	//该函数用于从探针管理器中移除指定的Pod。
	//在容器停止后，可以停止对Pod的存活和就绪状态的探测。
	//此函数是Kubernetes中的一个组件，用于管理Pod的探针。
	//通过调用kl.probeManager.RemovePod(pod)，可以将指定的Pod从探针管理器中移除，从而停止对该Pod的存活和就绪状态的探测。

	// Guard against consistency issues in KillPod implementations by checking that there are no
	// running containers. This method is invoked infrequently so this is effectively free and can
	// catch race conditions introduced by callers updating pod status out of order.
	// TODO: have KillPod return the terminal status of stopped containers and write that into the
	// cache immediately
	podStatus, err := kl.containerRuntime.GetPodStatus(ctx, pod.UID, pod.Name, pod.Namespace)
	if err != nil {
		klog.ErrorS(err, &amp;#34;Unable to read pod status prior to final pod termination&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod), &amp;#34;podUID&amp;#34;, pod.UID)
		return err
	}
	//这段Go代码中的函数主要功能是在终止Pod之前，通过检查是否有正在运行的容器来防止一致性问题。
	//该方法被调用的频率较低，因此基本上是免费的，可以捕获调用者按错误顺序更新Pod状态时引入的竞态条件。
	//通过获取Pod的状态来实现检查，如果无法读取Pod状态，则记录错误日志并返回错误。

	var runningContainers []string
	type container struct {
		Name string
		State string
		ExitCode int
		FinishedAt string
	}
	var containers []container
	klogV := klog.V(4)
	klogVEnabled := klogV.Enabled()
	//这段Go代码定义了一个container结构体类型，包含名称、状态、退出码和完成时间四个字段；
	//同时定义了一个runningContainers切片和一个containers切片。
	//klogV和klogVEnabled是klog库的函数调用，用于判断日志级别是否启用。

	for _, s := range podStatus.ContainerStatuses {
		if s.State == kubecontainer.ContainerStateRunning {
			runningContainers = append(runningContainers, s.ID.String())
		}
		//该Go代码片段是一个for循环，遍历了podStatus.ContainerStatuses切片中的每个元素，
		//并将处于运行状态（ContainerStateRunning）的容器的ID以字符串形式保存到runningContainers切片中。

		if klogVEnabled {
			containers = append(containers, container{Name: s.Name, State: string(s.State), ExitCode: s.ExitCode, FinishedAt: s.FinishedAt.UTC().Format(time.RFC3339Nano)})
		}
		//这段Go代码是一个条件语句，判断klogVEnabled是否为true，如果是，则将一个容器信息追加到containers切片中。
		//追加的容器信息包括容器的名称、状态、退出码和完成时间（使用UTC格式）。

	}
	if klogVEnabled {
		sort.Slice(containers, func(i, j int) bool { return containers[i].Name &amp;lt; containers[j].Name })
		klog.V(4).InfoS(&amp;#34;Post-termination container state&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod), &amp;#34;podUID&amp;#34;, pod.UID, &amp;#34;containers&amp;#34;, containers)
	}
	//这段Go代码是在klogVEnabled为true时对containers进行排序，并通过klog.V(4).InfoS输出日志信息。
	//其中，sort.Slice根据containers中Name字段的值对containers进行升序排序；
	//klog.V(4).InfoS输出的日志信息包括&amp;#34;Post-termination container state&amp;#34;、&amp;#34;pod&amp;#34;、&amp;#34;podUID&amp;#34;和&amp;#34;containers&amp;#34;等字段的值。
	if len(runningContainers) &amp;gt; 0 {
		return fmt.Errorf(&amp;#34;detected running containers after a successful KillPod, CRI violation: %v&amp;#34;, runningContainers)
	}
	//该函数检测到运行中的容器后，在成功调用KillPod后返回一个错误，指示CRI违规。

	// NOTE: resources must be unprepared AFTER all containers have stopped
	// and BEFORE the pod status is changed on the API server
	// to avoid race conditions with the resource deallocation code in kubernetes core.
	if utilfeature.DefaultFeatureGate.Enabled(features.DynamicResourceAllocation) {
		if err := kl.UnprepareDynamicResources(pod); err != nil {
			return err
		}
	}
	//这段Go代码是Kubernetes中的一个函数片段，它的主要功能是在动态资源分配的特性开启时，
	//调用kl.UnprepareDynamicResources(pod)方法来释放动态资源。
	//- 首先，代码通过utilfeature.DefaultFeatureGate.Enabled(features.DynamicResourceAllocation)判断当前是否启用了动态资源分配的特性。
	//- 如果特性开启，则调用kl.UnprepareDynamicResources(pod)方法来释放与给定Pod相关的动态资源。
	//- 如果在释放资源的过程中出现错误，则函数会返回该错误。
	//需要注意的是，该函数的注释指出，在调用该函数释放资源之前，必须确保所有容器已经停止，
	//并且在修改Pod的状态之前调用，以避免与Kubernetes核心代码中的资源释放逻辑发生竞态条件。

	// Compute and update the status in cache once the pods are no longer running.
	// The computation is done here to ensure the pod status used for it contains
	// information about the container end states (including exit codes) - when
	// SyncTerminatedPod is called the containers may already be removed.
	apiPodStatus = kl.generateAPIPodStatus(pod, podStatus, true)
	kl.statusManager.SetPodStatus(pod, apiPodStatus)

	// we have successfully stopped all containers, the pod is terminating, our status is &amp;#34;done&amp;#34;
	klog.V(4).InfoS(&amp;#34;Pod termination stopped all running containers&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod), &amp;#34;podUID&amp;#34;, pod.UID)

	return nil
	//这段Go代码是一个函数片段，它用于计算和更新Pod的状态缓存，当Pod不再运行时。
	//函数首先调用kl.generateAPIPodStatus生成一个API Pod状态，然后使用kl.statusManager.SetPodStatus设置Pod的状态。
	//最后，函数返回nil。
	//这段代码属于Kubernetes的一部分，用于管理Pod的生命周期。
	//在Pod终止时，该函数被调用，确保容器的终止状态（包括退出码）被包含在Pod状态中。
	//函数通过设置Pod状态来记录Pod的终止状态。

}

// SyncTerminatingRuntimePod is expected to terminate running containers in a pod that we have no
// configuration for. Once this method returns without error, any remaining local state can be safely
// cleaned up by background processes in each subsystem. Unlike syncTerminatingPod, we lack
// knowledge of the full pod spec and so cannot perform lifecycle related operations, only ensure
// that the remnant of the running pod is terminated and allow garbage collection to proceed. We do
// not update the status of the pod because with the source of configuration removed, we have no
// place to send that status.
func (kl *Kubelet) SyncTerminatingRuntimePod(_ context.Context, runningPod *kubecontainer.Pod) error {
	// TODO(#113606): connect this with the incoming context parameter, which comes from the pod worker.
	// Currently, using that context causes test failures.
	ctx := context.Background()
	pod := runningPod.ToAPIPod()
	klog.V(4).InfoS(&amp;#34;SyncTerminatingRuntimePod enter&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod), &amp;#34;podUID&amp;#34;, pod.UID)
	defer klog.V(4).InfoS(&amp;#34;SyncTerminatingRuntimePod exit&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod), &amp;#34;podUID&amp;#34;, pod.UID)
	//该函数是Kubelet的一个方法，用于同步终止无配置的Pod中的运行时容器。
	//该方法不更新Pod的状态，仅确保运行Pod的剩余部分被终止，以便进行垃圾回收。
	//函数首先创建一个背景上下文，然后将运行Pod转换为API Pod。通过记录日志来标记函数的进入和退出。

	// we kill the pod directly since we have lost all other information about the pod.
	klog.V(4).InfoS(&amp;#34;Orphaned running pod terminating without grace period&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod), &amp;#34;podUID&amp;#34;, pod.UID)
	// TODO: this should probably be zero, to bypass any waiting (needs fixes in container runtime)
	gracePeriod := int64(1)
	if err := kl.killPod(ctx, pod, *runningPod, &amp;amp;gracePeriod); err != nil {
		kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedToKillPod, &amp;#34;error killing pod: %v&amp;#34;, err)
		// there was an error killing the pod, so we return that error directly
		utilruntime.HandleError(err)
		return err
	}
	klog.V(4).InfoS(&amp;#34;Pod termination stopped all running orphaned containers&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod), &amp;#34;podUID&amp;#34;, pod.UID)
	return nil
}

//这段Go代码是一个函数，用于直接终止一个孤立的正在运行的Pod（容器）。
//函数首先记录一条日志信息，表示将要终止Pod，并指出该Pod已经没有其他相关信息。
//然后，函数调用kl.killPod方法来终止Pod，给定的优雅终止时间是1秒。
//如果终止Pod时出现错误，函数会记录一条事件日志，并直接返回该错误。
//最后，函数记录一条日志信息，表示已经成功终止了Pod的所有正在运行的容器，并返回nil表示没有错误。

// SyncTerminatedPod cleans up a pod that has terminated (has no running containers).
// The invocations in this call are expected to tear down all pod resources.
// When this method exits the pod is expected to be ready for cleanup. This method
// reduces the latency of pod cleanup but is not guaranteed to get called in all scenarios.
//
// Because the kubelet has no local store of information, all actions in this method that modify
// on-disk state must be reentrant and be garbage collected by HandlePodCleanups or a separate loop.
// This typically occurs when a pod is force deleted from configuration (local disk or API) and the
// kubelet restarts in the middle of the action.
func (kl *Kubelet) SyncTerminatedPod(ctx context.Context, pod *v1.Pod, podStatus *kubecontainer.PodStatus) error {
	ctx, otelSpan := kl.tracer.Start(ctx, &amp;#34;syncTerminatedPod&amp;#34;, trace.WithAttributes(
		semconv.K8SPodUIDKey.String(string(pod.UID)),
		attribute.String(&amp;#34;k8s.pod&amp;#34;, klog.KObj(pod).String()),
		semconv.K8SPodNameKey.String(pod.Name),
		semconv.K8SNamespaceNameKey.String(pod.Namespace),
	))
	defer otelSpan.End()
	klog.V(4).InfoS(&amp;#34;SyncTerminatedPod enter&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod), &amp;#34;podUID&amp;#34;, pod.UID)
	defer klog.V(4).InfoS(&amp;#34;SyncTerminatedPod exit&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod), &amp;#34;podUID&amp;#34;, pod.UID)
	//该函数是Kubelet的一个方法，用于清理已经终止（没有运行中的容器）的Pod。
	//调用该方法的目的是销毁Pod的所有资源，当方法退出时，Pod应该准备好进行清理。
	//该方法可以减少Pod清理的延迟，但在所有场景中都可能不会被调用。
	//由于Kubelet没有本地存储信息，该方法中所有修改本地磁盘状态的操作必须是可重入的，并且可以通过HandlePodCleanups或单独的循环进行垃圾回收。
	//这通常发生在Pod从配置（本地磁盘或API）中强制删除，并且kubelet在操作中间重新启动时。
	//该函数首先通过kl.tracer.Start创建一个名为syncTerminatedPod的跟踪Span，并设置相关属性。
	//然后使用defer语句在函数退出时结束该Span。接下来，使用klog.V(4).InfoS记录函数的进入和退出日志。
	//函数的主要逻辑部分在这些日志记录和跟踪Span的开启和结束之间。

	// generate the final status of the pod
	// TODO: should we simply fold this into TerminatePod? that would give a single pod update
	apiPodStatus := kl.generateAPIPodStatus(pod, podStatus, true)

	kl.statusManager.SetPodStatus(pod, apiPodStatus)

	// volumes are unmounted after the pod worker reports ShouldPodRuntimeBeRemoved (which is satisfied
	// before syncTerminatedPod is invoked)
	if err := kl.volumeManager.WaitForUnmount(ctx, pod); err != nil {
		return err
	}
	klog.V(4).InfoS(&amp;#34;Pod termination unmounted volumes&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod), &amp;#34;podUID&amp;#34;, pod.UID)
	//这段Go代码是Kubernetes中Kubelet的一部分，用于处理已终止的Pod的最终状态。
	//首先，它调用kl.generateAPIPodStatus(pod, podStatus, true)生成Pod的最终状态。
	//这个函数会根据Pod的实际状态和配置生成一个API兼容的Pod状态对象。
	//接下来，它调用kl.statusManager.SetPodStatus(pod, apiPodStatus)将生成的Pod状态设置为Kubernetes API中记录的状态。
	//最后，它调用kl.volumeManager.WaitForUnmount(ctx, pod)等待所有卷被卸载。
	//这个函数会等待所有与Pod相关的卷被成功卸载，以确保Pod的所有资源都被正确清理。
	//如果卷卸载失败，则会返回错误，终止Pod的处理将停止。
	//如果成功，则会记录一条日志消息，表示Pod的卸载已经完成。

	if !kl.keepTerminatedPodVolumes {
		// This waiting loop relies on the background cleanup which starts after pod workers respond
		// true for ShouldPodRuntimeBeRemoved, which happens after `SyncTerminatingPod` is completed.
		if err := wait.PollUntilContextCancel(ctx, 100*time.Millisecond, true, func(ctx context.Context) (bool, error) {
			volumesExist := kl.podVolumesExist(pod.UID)
			if volumesExist {
				klog.V(3).InfoS(&amp;#34;Pod is terminated, but some volumes have not been cleaned up&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod), &amp;#34;podUID&amp;#34;, pod.UID)
			}
			return !volumesExist, nil
		}); err != nil {
			return err
		}
		klog.V(3).InfoS(&amp;#34;Pod termination cleaned up volume paths&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod), &amp;#34;podUID&amp;#34;, pod.UID)
	}
	//这段Go代码是一个等待循环，用于检查已终止的Pod的卷是否已被清理。
	//如果kl.keepTerminatedPodVolumes为false，则会进入循环，通过wait.PollUntilContextCancel函数每隔100毫秒检查一次。
	//循环会一直进行，直到Pod的卷不存在，或者上下文被取消。如果循环结束且没有错误发生，则会记录一条日志表示Pod终止时清理了卷路径。

	// After volume unmount is complete, let the secret and configmap managers know we&amp;#39;re done with this pod
	if kl.secretManager != nil {
		kl.secretManager.UnregisterPod(pod)
	}
	if kl.configMapManager != nil {
		kl.configMapManager.UnregisterPod(pod)
	}
	//这段Go代码中的函数是在完成卷卸载后，通知密钥和配置管理器此Pod不再使用。
	//具体来说，它首先检查kl.secretManager是否不为空，如果非空则从secretManager中注销此Pod，接着检查kl.configMapManager是否不为空，
	//如果非空则从configMapManager中注销此Pod。

	// Note: we leave pod containers to be reclaimed in the background since dockershim requires the
	// container for retrieving logs and we want to make sure logs are available until the pod is
	// physically deleted.

	// remove any cgroups in the hierarchy for pods that are no longer running.
	if kl.cgroupsPerQOS {
		pcm := kl.containerManager.NewPodContainerManager()
		name, _ := pcm.GetPodContainerName(pod)
		if err := pcm.Destroy(name); err != nil {
			return err
		}
		klog.V(4).InfoS(&amp;#34;Pod termination removed cgroups&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod), &amp;#34;podUID&amp;#34;, pod.UID)
	}
	//这段Go代码是一个函数，用于删除不再运行的Pod的所有cgroups（控制组）。
	//- 首先，代码检查kl.cgroupsPerQOS是否为true。如果为true，则会创建一个新的PodContainerManager实例。
	//- 然后，通过pcm.GetPodContainerName(pod)获取Pod的容器名称。
	//- 最后，调用pcm.Destroy(name)来销毁该Pod的所有cgroups，并在成功删除后记录日志。
	//这个函数的主要作用是在Pod终止时清理相关的cgroups资源。

	kl.usernsManager.Release(pod.UID)

	// mark the final pod status
	kl.statusManager.TerminatePod(pod)
	klog.V(4).InfoS(&amp;#34;Pod is terminated and will need no more status updates&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod), &amp;#34;podUID&amp;#34;, pod.UID)

	return nil
	//这段Go代码中的函数调用是针对Kubernetes中某个Pod的释放和状态标记操作。
	//首先，kl.usernsManager.Release(pod.UID)调用释放了与Pod关联的用户命名空间资源。
	//接下来，kl.statusManager.TerminatePod(pod)调用标记了Pod的最终状态为终止。
	//最后，通过klog.V(4).InfoS(...)记录日志信息，表示Pod已被终止，不再需要更新状态。
	//函数执行完毕后，返回nil表示操作成功完成。

}

// Get pods which should be resynchronized. Currently, the following pod should be resynchronized:
// - pod whose work is ready.
// - internal modules that request sync of a pod.
//
// This method does not return orphaned pods (those known only to the pod worker that may have
// been deleted from configuration). Those pods are synced by HandlePodCleanups as a consequence
// of driving the state machine to completion.
//
// TODO: Consider synchronizing all pods which have not recently been acted on to be resilient
// to bugs that might prevent updates from being delivered (such as the previous bug with
// orphaned pods). Instead of asking the work queue for pending work, consider asking the
// PodWorker which pods should be synced.
func (kl *Kubelet) getPodsToSync() []*v1.Pod {
	allPods := kl.podManager.GetPods()
	podUIDs := kl.workQueue.GetWork()
	podUIDSet := sets.NewString()
	for _, podUID := range podUIDs {
		podUIDSet.Insert(string(podUID))
	}
	//该函数用于获取需要重新同步的Pod。
	//具体来说，它会返回所有工作准备就绪的Pod和内部模块请求同步的Pod。
	//函数首先获取所有Pod的信息，然后从工作队列中获取需要同步的Pod的UID，并将其放入一个字符串集合中。
	//最后，函数会返回这个集合中所有Pod的信息。需要注意的是，该函数不会返回孤立的Pod（那些只被Pod工作者知道，可能已经被从配置中删除的Pod），
	//这些Pod会由HandlePodCleanups函数进行同步。
	//函数的注释中还提到，可能会考虑同步所有最近没有被操作的Pod，以增加对错误的鲁棒性。

	var podsToSync []*v1.Pod
	for _, pod := range allPods {
		if podUIDSet.Has(string(pod.UID)) {
			// The work of the pod is ready
			podsToSync = append(podsToSync, pod)
			continue
		}
		for _, podSyncLoopHandler := range kl.PodSyncLoopHandlers {
			if podSyncLoopHandler.ShouldSync(pod) {
				podsToSync = append(podsToSync, pod)
				break
			}
		}
	}
	return podsToSync
}

//该函数是一个简单的for循环，用于遍历allPods切片，并根据条件筛选出需要同步的Pod，将其添加到podsToSync切片中。
//具体来说，它首先检查podUIDSet中是否包含当前Pod的UID，如果是，则将该Pod添加到podsToSync中。
//然后，它遍历kl.PodSyncLoopHandlers中的每个podSyncLoopHandler，调用其ShouldSync方法检查是否需要同步该Pod，
//如果是，则将该Pod添加到podsToSync中并跳出循环。最后，函数返回podsToSync切片。

// deletePod deletes the pod from the internal state of the kubelet by:
// 1. stopping the associated pod worker asynchronously
// 2. signaling to kill the pod by sending on the podKillingCh channel
//
// deletePod returns an error if not all sources are ready or the pod is not
// found in the runtime cache.
func (kl *Kubelet) deletePod(pod *v1.Pod) error {
	if pod == nil {
		return fmt.Errorf(&amp;#34;deletePod does not allow nil pod&amp;#34;)
	}
	if !kl.sourcesReady.AllReady() {
		// If the sources aren&amp;#39;t ready, skip deletion, as we may accidentally delete pods
		// for sources that haven&amp;#39;t reported yet.
		return fmt.Errorf(&amp;#34;skipping delete because sources aren&amp;#39;t ready yet&amp;#34;)
	}
	klog.V(3).InfoS(&amp;#34;Pod has been deleted and must be killed&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod), &amp;#34;podUID&amp;#34;, pod.UID)
	kl.podWorkers.UpdatePod(UpdatePodOptions{
		Pod: pod,
		UpdateType: kubetypes.SyncPodKill,
	})
	// We leave the volume/directory cleanup to the periodic cleanup routine.
	return nil
}

//该函数是Kubelet的一个方法，用于从kubelet的内部状态中删除一个pod，
//具体操作包括：
//1. 停止关联的pod worker（异步进行）。
//2. 通过发送信号到podKillingCh通道来终止pod。
//如果输入的pod为nil，函数会返回一个错误。
//如果相关的数据源未准备就绪，函数也会返回一个错误，以防止意外删除未报告的pod。
//在执行删除操作后，函数会更新pod worker，并将其标记为需要被终止。
//然后函数返回nil，表示删除成功。
//**注意：**该函数不会立即清理相关的卷和目录，而是将其留给了周期性的清理程序。

// rejectPod records an event about the pod with the given reason and message,
// and updates the pod to the failed phase in the status manager.
func (kl *Kubelet) rejectPod(pod *v1.Pod, reason, message string) {
	kl.recorder.Eventf(pod, v1.EventTypeWarning, reason, message)
	kl.statusManager.SetPodStatus(pod, v1.PodStatus{
		Phase: v1.PodFailed,
		Reason: reason,
		Message: &amp;#34;Pod was rejected: &amp;#34; + message})
}

//该函数用于记录一个关于给定pod的事件，并在状态管理器中将pod的状态更新为失败。
//具体实现中，函数使用kl.recorder.Eventf方法记录事件，然后使用kl.statusManager.SetPodStatus方法将pod的状态设置为失败，并提供原因和消息。

// canAdmitPod determines if a pod can be admitted, and gives a reason if it
// cannot. &amp;#34;pod&amp;#34; is new pod, while &amp;#34;pods&amp;#34; are all admitted pods
// The function returns a boolean value indicating whether the pod
// can be admitted, a brief single-word reason and a message explaining why
// the pod cannot be admitted.
func (kl *Kubelet) canAdmitPod(pods []*v1.Pod, pod *v1.Pod) (bool, string, string) {
	// the kubelet will invoke each pod admit handler in sequence
	// if any handler rejects, the pod is rejected.
	// TODO: move out of disk check into a pod admitter
	// TODO: out of resource eviction should have a pod admitter call-out
	attrs := &amp;amp;lifecycle.PodAdmitAttributes{Pod: pod, OtherPods: pods}
	//该函数是Kubelet的一个方法，用于判断是否可以接纳一个Pod，并给出不能接纳的原因。
	//函数传入参数为已接纳的Pod列表和待判断的Pod，返回一个布尔值表示是否可以接纳，一个简短的单个单词原因和一个解释不能接纳原因的消息。
	//函数内部会按顺序调用每个Pod的准入处理器，如果有任何一个处理器拒绝，则Pod会被拒绝接纳。
	//函数还有两个TODO注释，表示后续会将磁盘检查和资源不足的驱逐处理移动到Pod的准入处理器中。

	if utilfeature.DefaultFeatureGate.Enabled(features.InPlacePodVerticalScaling) {
		// Use allocated resources values from checkpoint store (source of truth) to determine fit
		otherPods := make([]*v1.Pod, 0, len(pods))
		for _, p := range pods {
			op := p.DeepCopy()
			kl.updateContainerResourceAllocation(op)

			otherPods = append(otherPods, op)
		}
		attrs.OtherPods = otherPods
	}
	//这段Go代码是用于在Kubernetes中进行Pod垂直扩展的功能。
	//具体来说，如果启用了InPlacePodVerticalScaling特性，它会使用checkpoint存储中记录的已分配资源值来确定Pod是否适合进行垂直扩展。
	//代码首先创建一个空的Pod数组，然后遍历传入的Pods，并对每个Pod进行深拷贝。
	//然后，通过调用updateContainerResourceAllocation方法来更新每个Pod的容器资源分配情况，并将更新后的Pod添加到数组中。
	//最后，将这个更新后的Pod数组赋值给attrs.OtherPods属性。

	for _, podAdmitHandler := range kl.admitHandlers {
		if result := podAdmitHandler.Admit(attrs); !result.Admit {
			return false, result.Reason, result.Message
		}
	}

	return true, &amp;#34;&amp;#34;, &amp;#34;&amp;#34;
}

//该函数的作用是逐一检查kl.admitHandlers中的每个podAdmitHandler是否能够通过attrs属性的检验。
//具体流程如下：
//1. 使用for range循环遍历kl.admitHandlers，每次迭代将一个podAdmitHandler赋值给podAdmitHandler变量。
//2. 在循环内部，调用当前podAdmitHandler的Admit方法，传入attrs参数，并将返回的结果赋值给result变量。
//3. 如果result的Admit字段为false，则函数立即返回false，同时返回result的Reason和Message字段作为错误信息。
//4. 如果循环结束后没有返回，则函数最终返回true，同时返回空字符串作为错误信息。
//总之，该函数的作用是检验kl.admitHandlers中的每个podAdmitHandler是否能够通过attrs属性的检验，并返回检验结果以及相应的错误信息。

func (kl *Kubelet) canRunPod(pod *v1.Pod) lifecycle.PodAdmitResult {
	attrs := &amp;amp;lifecycle.PodAdmitAttributes{Pod: pod}
	// Get &amp;#34;OtherPods&amp;#34;. Rejected pods are failed, so only include admitted pods that are alive.
	attrs.OtherPods = kl.GetActivePods()

	for _, handler := range kl.softAdmitHandlers {
		if result := handler.Admit(attrs); !result.Admit {
			return result
		}
	}

	return lifecycle.PodAdmitResult{Admit: true}
}

//该函数是Kubelet的一个方法，用于判断是否可以运行一个Pod。
//它首先创建一个PodAdmitAttributes对象，其中包含了要判断的Pod和其他活跃的Pod。
//然后遍历Kubelet的softAdmitHandlers列表，依次调用每个handler的Admit方法，并传入PodAdmitAttributes对象。
//如果有任何一个handler的Admit方法返回不承认（Admit为false），则函数立即返回该结果。
//如果所有handler都承认（Admit为true），则函数返回一个承认的结果。

// syncLoop is the main loop for processing changes. It watches for changes from
// three channels (file, apiserver, and http) and creates a union of them. For
// any new change seen, will run a sync against desired state and running state. If
// no changes are seen to the configuration, will synchronize the last known desired
// state every sync-frequency seconds. Never returns.
func (kl *Kubelet) syncLoop(ctx context.Context, updates &amp;lt;-chan kubetypes.PodUpdate, handler SyncHandler) {
	klog.InfoS(&amp;#34;Starting kubelet main sync loop&amp;#34;)
	// The syncTicker wakes up kubelet to checks if there are any pod workers
	// that need to be sync&amp;#39;d. A one-second period is sufficient because the
	// sync interval is defaulted to 10s.
	syncTicker := time.NewTicker(time.Second)
	defer syncTicker.Stop()
	housekeepingTicker := time.NewTicker(housekeepingPeriod)
	defer housekeepingTicker.Stop()
	plegCh := kl.pleg.Watch()
	const (
		base = 100 * time.Millisecond
		max = 5 * time.Second
		factor = 2
	)
	//该函数是kubelet的主要同步循环，用于处理来自三个渠道（文件、apiserver和http）的更改。
	//它将这些更改合并，并对期望状态和运行状态进行同步。如果配置没有更改，它将每sync-frequency秒同步最后一次已知的期望状态。
	//该函数不会返回。 详细解释：
	//- syncLoop 函数是 Kubelet 类的一个方法，它接收一个上下文对象 ctx、一个 PodUpdate 类型的通道 updates 和一个 SyncHandler 类型的处理程序 handler。
	//- 函数首先使用 klog.InfoS 函数记录信息日志，表示开始启动 kubelet 的主同步循环。
	//- 然后，函数创建一个 syncTicker，用于唤醒 kubelet 检查是否有需要同步的 pod 工作线程。
	//由于同步间隔默认为 10s，因此使用 1s 的周期就足够了。
	//- 函数还创建一个 housekeepingTicker，用于执行清理操作。
	//- 接下来，函数通过调用 kl.pleg.Watch() 方法来监听 pod 状态变化事件，并将返回的通道赋值给 plegCh 变量。
	//- 函数定义了一些常量，用于控制重试的时间间隔。
	//- 在无限循环中，函数首先通过 select 语句监听多个通道。如果 ctx.Done() 通道被关闭，函数将退出循环。
	//- 如果 updates 通道有新的更改，函数将调用 handler 处理程序进行同步。
	//- 如果 plegCh 通道有新的 pod 状态变化事件，函数将调用 kl.pleg.Grab() 方法进行处理。
	//- 如果 housekeepingTicker.C 通道触发清理操作，函数将调用 kl.doHousekeeping() 方法进行清理。
	//- 如果没有监听到任何事件，则使用 time.Sleep() 函数等待一段时间后继续循环。
	//- 如果在尝试同步时发生错误，函数将根据重试策略进行重试。重试时间间隔会根据失败次数进行指数增加，最大间隔为 5s。
	//- 如果同步成功，函数将重置重试时间间隔为基本值 100ms。
	//- 如果同步失败超过一定次数，函数将记录错误日志并退出循环。

	duration := base
	// Responsible for checking limits in resolv.conf
	// The limits do not have anything to do with individual pods
	// Since this is called in syncLoop, we don&amp;#39;t need to call it anywhere else
	if kl.dnsConfigurer != nil &amp;amp;&amp;amp; kl.dnsConfigurer.ResolverConfig != &amp;#34;&amp;#34; {
		kl.dnsConfigurer.CheckLimitsForResolvConf()
	}
	//这段Go代码中的函数主要负责检查resolv.conf文件中的限制。
	//该函数会判断kl.dnsConfigurer是否为空，且其ResolverConfig是否已设置，如果满足条件，
	//则调用kl.dnsConfigurer的CheckLimitsForResolvConf方法进行限制检查。
	//由于该函数是在syncLoop中调用的，因此不需要在其他地方再次调用。

	for {
		if err := kl.runtimeState.runtimeErrors(); err != nil {
			klog.ErrorS(err, &amp;#34;Skipping pod synchronization&amp;#34;)
			// exponential backoff
			time.Sleep(duration)
			duration = time.Duration(math.Min(float64(max), factor*float64(duration)))
			continue
		}
		// reset backoff if we have a success
		duration = base

		kl.syncLoopMonitor.Store(kl.clock.Now())
		if !kl.syncLoopIteration(ctx, updates, handler, syncTicker.C, housekeepingTicker.C, plegCh) {
			break
		}
		kl.syncLoopMonitor.Store(kl.clock.Now())
	}
}

//这段Go代码是一个无限循环，用于进行Pod同步。循环中包含两个主要部分：错误处理和同步逻辑。
//1. 错误处理： - 使用runtimeErrors()检查运行时错误。
//- 如果存在错误，则记录错误日志，并进行指数退避。
//- 通过time.Sleep()暂停执行，暂停时间根据退避策略计算得出。
//- 退避策略是通过将duration乘以factor并限制最大值来计算的。
//- 出现错误时，使用continue语句继续下一次循环。
//2. 同步逻辑： - 在没有错误的情况下，重置退避时间至初始值。
//- 更新syncLoopMonitor的值为当前时间。
//- 调用syncLoopIteration()函数进行Pod同步。
//- 如果syncLoopIteration()返回false，则退出循环。
//- 更新syncLoopMonitor的值为当前时间。
//这段代码的主要目的是在遇到错误时进行指数退避，并在成功时重置退避时间，持续进行Pod同步操作。

// syncLoopIteration reads from various channels and dispatches pods to the
// given handler.
//
// Arguments:
// 1. configCh: a channel to read config events from
// 2. handler: the SyncHandler to dispatch pods to
// 3. syncCh: a channel to read periodic sync events from
// 4. housekeepingCh: a channel to read housekeeping events from
// 5. plegCh: a channel to read PLEG updates from
//
// Events are also read from the kubelet liveness manager&amp;#39;s update channel.
//
// The workflow is to read from one of the channels, handle that event, and
// update the timestamp in the sync loop monitor.
//
// Here is an appropriate place to note that despite the syntactical
// similarity to the switch statement, the case statements in a select are
// evaluated in a pseudorandom order if there are multiple channels ready to
// read from when the select is evaluated. In other words, case statements
// are evaluated in random order, and you can not assume that the case
// statements evaluate in order if multiple channels have events.
//
// With that in mind, in truly no particular order, the different channels
// are handled as follows:
//
// - configCh: dispatch the pods for the config change to the appropriate
// handler callback for the event type
// - plegCh: update the runtime cache; sync pod
// - syncCh: sync all pods waiting for sync
// - housekeepingCh: trigger cleanup of pods
// - health manager: sync pods that have failed or in which one or more
// containers have failed health checks
func (kl *Kubelet) syncLoopIteration(ctx context.Context, configCh &amp;lt;-chan kubetypes.PodUpdate, handler SyncHandler,
	syncCh &amp;lt;-chan time.Time, housekeepingCh &amp;lt;-chan time.Time, plegCh &amp;lt;-chan *pleg.PodLifecycleEvent) bool {
	select {
	case u, open := &amp;lt;-configCh:
		// Update from a config source; dispatch it to the right handler
		// callback.
		//该函数是一个同步循环，用于从多个通道读取事件并将其分发给给定的处理程序。
		//它接收5个参数：configCh（配置事件的通道）、handler（分发pod的SyncHandler处理程序）、syncCh（定期同步事件的通道）、
		//housekeepingCh（清理事件的通道）和plegCh（PLEG更新的通道）。
		//事件还会从kubelet存活管理器的更新通道中读取。该函数的工作流程是读取一个通道中的事件，处理该事件，并在同步循环监视器中更新时间戳。
		//在处理这些通道中的事件时，需要注意case语句的评估顺序是随机的。
		//具体来说，不同通道的处理方式如下：
		//- configCh：将配置更改分发给适当的处理程序回调。
		//- plegCh：更新运行时缓存并同步pod。
		//- syncCh：同步所有等待同步的pod。
		//- housekeepingCh：触发清理pod。
		//- 健康管理器：同步失败或其中一个或多个容器失败的健康检查的pod。

		if !open {
			klog.ErrorS(nil, &amp;#34;Update channel is closed, exiting the sync loop&amp;#34;)
			return false
		}
		//这段Go代码是一个条件判断语句，判断变量open是否为false，如果为false，则通过klog.ErrorS函数记录错误日志，并返回false。
		//这段代码的作用是在某个更新通道关闭时，退出同步循环，并记录错误日志。

		switch u.Op {
		case kubetypes.ADD:
			klog.V(2).InfoS(&amp;#34;SyncLoop ADD&amp;#34;, &amp;#34;source&amp;#34;, u.Source, &amp;#34;pods&amp;#34;, klog.KObjSlice(u.Pods))
			// After restarting, kubelet will get all existing pods through
			// ADD as if they are new pods. These pods will then go through the
			// admission process and *may* be rejected. This can be resolved
			// once we have checkpointing.
			handler.HandlePodAdditions(u.Pods)
		case kubetypes.UPDATE:
			klog.V(2).InfoS(&amp;#34;SyncLoop UPDATE&amp;#34;, &amp;#34;source&amp;#34;, u.Source, &amp;#34;pods&amp;#34;, klog.KObjSlice(u.Pods))
			handler.HandlePodUpdates(u.Pods)
		case kubetypes.REMOVE:
			klog.V(2).InfoS(&amp;#34;SyncLoop REMOVE&amp;#34;, &amp;#34;source&amp;#34;, u.Source, &amp;#34;pods&amp;#34;, klog.KObjSlice(u.Pods))
			handler.HandlePodRemoves(u.Pods)
		case kubetypes.RECONCILE:
			klog.V(4).InfoS(&amp;#34;SyncLoop RECONCILE&amp;#34;, &amp;#34;source&amp;#34;, u.Source, &amp;#34;pods&amp;#34;, klog.KObjSlice(u.Pods))
			handler.HandlePodReconcile(u.Pods)
		case kubetypes.DELETE:
			klog.V(2).InfoS(&amp;#34;SyncLoop DELETE&amp;#34;, &amp;#34;source&amp;#34;, u.Source, &amp;#34;pods&amp;#34;, klog.KObjSlice(u.Pods))
			// DELETE is treated as a UPDATE because of graceful deletion.
			handler.HandlePodUpdates(u.Pods)
		case kubetypes.SET:
			// TODO: Do we want to support this?
			klog.ErrorS(nil, &amp;#34;Kubelet does not support snapshot update&amp;#34;)
		default:
			klog.ErrorS(nil, &amp;#34;Invalid operation type received&amp;#34;, &amp;#34;operation&amp;#34;, u.Op)
		}
		//该函数根据传入的操作类型（u.Op）执行相应的处理逻辑。
		//- 当操作类型为kubetypes.ADD时，会记录日志信息并调用handler.HandlePodAdditions(u.Pods)处理Pod的添加操作。
		//- 当操作类型为kubetypes.UPDATE时，会记录日志信息并调用handler.HandlePodUpdates(u.Pods)处理Pod的更新操作。
		//- 当操作类型为kubetypes.REMOVE时，会记录日志信息并调用handler.HandlePodRemoves(u.Pods)处理Pod的移除操作。
		//- 当操作类型为kubetypes.RECONCILE时，会记录日志信息并调用handler.HandlePodReconcile(u.Pods)处理Pod的协调操作。
		//- 当操作类型为kubetypes.DELETE时，会记录日志信息并调用handler.HandlePodUpdates(u.Pods)处理Pod的删除操作（被当做更新操作处理）。 - 当操作类型为kubetypes.SET时，会记录错误信息，表示当前不支持该操作。 - 当操作类型为其他值时，会记录错误信息，表示收到了无效的操作类型

		kl.sourcesReady.AddSource(u.Source)

	case e := &amp;lt;-plegCh:
		if isSyncPodWorthy(e) {
			// PLEG event for a pod; sync it.
			if pod, ok := kl.podManager.GetPodByUID(e.ID); ok {
				klog.V(2).InfoS(&amp;#34;SyncLoop (PLEG): event for pod&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod), &amp;#34;event&amp;#34;, e)
				handler.HandlePodSyncs([]*v1.Pod{pod})
			} else {
				// If the pod no longer exists, ignore the event.
				klog.V(4).InfoS(&amp;#34;SyncLoop (PLEG): pod does not exist, ignore irrelevant event&amp;#34;, &amp;#34;event&amp;#34;, e)
			}
		}
		//这段Go代码是在一个select语句的分支中，首先将u.Source添加到kl.sourcesReady中。
		//随后，从plegCh通道中接收一个事件e，并通过isSyncPodWorthy(e)函数判断是否需要同步该事件对应的Pod。
		//如果需要同步，通过kl.podManager.GetPodByUID(e.ID)获取Pod对象，并调用handler.HandlePodSyncs([]*v1.Pod{pod})进行同步操作。
		//如果Pod不存在，则在日志中记录相关信息并忽略该事件。

		if e.Type == pleg.ContainerDied {
			if containerID, ok := e.Data.(string); ok {
				kl.cleanUpContainersInPod(e.ID, containerID)
			}
		}
		//该代码片段是一个if语句，判断e.Type是否等于pleg.ContainerDied，
		//如果是，则进一步判断e.Data是否为字符串类型，并将字符串类型的e.Data赋值给containerID。
		//如果上述判断都为真，则调用kl.cleanUpContainersInPod(e.ID, containerID)方法。
		//这段代码的主要作用是在容器死亡时，清理Pod中的容器。

	case &amp;lt;-syncCh:
		// Sync pods waiting for sync
		podsToSync := kl.getPodsToSync()
		if len(podsToSync) == 0 {
			break
		}
		klog.V(4).InfoS(&amp;#34;SyncLoop (SYNC) pods&amp;#34;, &amp;#34;total&amp;#34;, len(podsToSync), &amp;#34;pods&amp;#34;, klog.KObjSlice(podsToSync))
		handler.HandlePodSyncs(podsToSync)
	case update := &amp;lt;-kl.livenessManager.Updates():
		if update.Result == proberesults.Failure {
			handleProbeSync(kl, update, handler, &amp;#34;liveness&amp;#34;, &amp;#34;unhealthy&amp;#34;)
		}
		//这是一个Go语言的代码片段，它通过监听两个通道（syncCh和kl.livenessManager.Updates()）来实现Pod的同步和健康检查。
		//具体功能如下：
		//1. 如果监听到syncCh通道有消息，会执行以下操作：
		//- 调用kl.getPodsToSync()获取需要同步的Pod列表；
		//- 如果列表为空，则跳出当前循环；
		//- 如果列表不为空，则通过klog记录日志信息，并调用handler.HandlePodSyncs(podsToSync)来处理Pod的同步操作。
		//2. 如果监听到kl.livenessManager.Updates()通道有消息，会执行以下操作：
		//- 检查消息的结果是否为失败（proberesults.Failure），如果是，
		//则调用handleProbeSync(kl, update, handler, &amp;#34;liveness&amp;#34;, &amp;#34;unhealthy&amp;#34;)来处理不健康的Pod。
		//这段代码通过使用Go语言的并发特性，实现了在同步Pod的同时进行健康检查的功能。

	case update := &amp;lt;-kl.readinessManager.Updates():
		ready := update.Result == proberesults.Success
		kl.statusManager.SetContainerReadiness(update.PodUID, update.ContainerID, ready)

		status := &amp;#34;&amp;#34;
		if ready {
			status = &amp;#34;ready&amp;#34;
		}
		handleProbeSync(kl, update, handler, &amp;#34;readiness&amp;#34;, status)
	case update := &amp;lt;-kl.startupManager.Updates():
		started := update.Result == proberesults.Success
		kl.statusManager.SetContainerStartup(update.PodUID, update.ContainerID, started)

		status := &amp;#34;unhealthy&amp;#34;
		if started {
			status = &amp;#34;started&amp;#34;
		}
		handleProbeSync(kl, update, handler, &amp;#34;startup&amp;#34;, status)
		//这段Go代码包含两个case语句，它们分别处理来自kl.readinessManager.Updates()和kl.startupManager.Updates()的更新。
		//每个case语句内部都会根据更新的结果来更新容器的就绪状态或启动状态，并调用handleProbeSync函数。
		//1. 第一个case语句处理就绪状态的更新：
		//- 通过update.Result == proberesults.Success判断就绪检查是否成功，将结果赋值给ready变量。
		//- 调用kl.statusManager.SetContainerReadiness来设置容器的就绪状态。
		//- 根据ready的值决定status的值，如果就绪则为&amp;#34;ready&amp;#34;，否则为空字符串。
		//- 调用handleProbeSync函数处理就绪探针的同步，传入参数包括kl、update、handler、探针类型&amp;#34;readiness&amp;#34;和status。
		//2. 第二个case语句处理启动状态的更新：
		//- 通过update.Result == proberesults.Success判断启动检查是否成功，将结果赋值给started变量。
		//- 调用kl.statusManager.SetContainerStartup来设置容器的启动状态。
		//- 根据started的值决定status的值，如果启动成功则为&amp;#34;started&amp;#34;，否则为&amp;#34;unhealthy&amp;#34;。
		//- 调用handleProbeSync函数处理启动探针的同步，传入参数包括kl、update、handler、探针类型&amp;#34;startup&amp;#34;和status。
		//综上所述，这段代码主要负责处理容器的就绪和启动状态更新，并通过handleProbeSync函数进行进一步处理。

	case &amp;lt;-housekeepingCh:
		if !kl.sourcesReady.AllReady() {
			// If the sources aren&amp;#39;t ready or volume manager has not yet synced the states,
			// skip housekeeping, as we may accidentally delete pods from unready sources.
			klog.V(4).InfoS(&amp;#34;SyncLoop (housekeeping, skipped): sources aren&amp;#39;t ready yet&amp;#34;)
		} else {
			start := time.Now()
			klog.V(4).InfoS(&amp;#34;SyncLoop (housekeeping)&amp;#34;)
			if err := handler.HandlePodCleanups(ctx); err != nil {
				klog.ErrorS(err, &amp;#34;Failed cleaning pods&amp;#34;)
			}
			duration := time.Since(start)
			if duration &amp;gt; housekeepingWarningDuration {
				klog.ErrorS(fmt.Errorf(&amp;#34;housekeeping took too long&amp;#34;), &amp;#34;Housekeeping took longer than expected&amp;#34;, &amp;#34;expected&amp;#34;, housekeepingWarningDuration, &amp;#34;actual&amp;#34;, duration.Round(time.Millisecond))
			}
			klog.V(4).InfoS(&amp;#34;SyncLoop (housekeeping) end&amp;#34;, &amp;#34;duration&amp;#34;, duration.Round(time.Millisecond))
		}
	}
	return true
	//该Go函数是一个case语句，从housekeepingCh通道接收信号。
	//当接收到信号时，它会检查kl.sourcesReady.AllReady()是否为true，如果不为true，则跳过清理工作，因为可能从未准备好的源意外删除Pod。
	//如果kl.sourcesReady.AllReady()为true，则开始执行清理工作，并记录开始时间。
	//清理工作由handler.HandlePodCleanups(ctx)函数执行，如果执行失败，则记录错误信息。
	//最后，计算清理工作的时间，如果时间超过预期的housekeepingWarningDuration，则记录警告信息。
	//最后，函数返回true。

}

func handleProbeSync(kl *Kubelet, update proberesults.Update, handler SyncHandler, probe, status string) {
	// We should not use the pod from manager, because it is never updated after initialization.
	pod, ok := kl.podManager.GetPodByUID(update.PodUID)
	if !ok {
		// If the pod no longer exists, ignore the update.
		klog.V(4).InfoS(&amp;#34;SyncLoop (probe): ignore irrelevant update&amp;#34;, &amp;#34;probe&amp;#34;, probe, &amp;#34;status&amp;#34;, status, &amp;#34;update&amp;#34;, update)
		return
	}
	klog.V(1).InfoS(&amp;#34;SyncLoop (probe)&amp;#34;, &amp;#34;probe&amp;#34;, probe, &amp;#34;status&amp;#34;, status, &amp;#34;pod&amp;#34;, klog.KObj(pod))
	handler.HandlePodSyncs([]*v1.Pod{pod})
}

//该函数用于处理探针同步操作。
//1. 首先，它通过update获取Pod的UID，并尝试从Kubelet的podManager中获取对应的Pod对象。
//2. 如果找不到Pod，则忽略更新，并记录日志。
//3. 如果找到Pod，则记录日志，并调用handler处理Pod的同步操作，将该Pod对象以切片的形式传递给HandlePodSyncs方法。

// HandlePodAdditions is the callback in SyncHandler for pods being added from
// a config source.
func (kl *Kubelet) HandlePodAdditions(pods []*v1.Pod) {
	start := kl.clock.Now()
	sort.Sort(sliceutils.PodsByCreationTime(pods))
	if utilfeature.DefaultFeatureGate.Enabled(features.InPlacePodVerticalScaling) {
		kl.podResizeMutex.Lock()
		defer kl.podResizeMutex.Unlock()
	}
	//该函数是Kubelet的SyncHandler回调函数，用于处理从配置源添加的Pods。
	//函数首先记录开始时间，然后对Pods进行排序。
	//如果启用了InPlacePodVerticalScaling特性，则加锁并释放锁以进行Pod的垂直扩展。

	for _, pod := range pods {
		existingPods := kl.podManager.GetPods()
		// Always add the pod to the pod manager. Kubelet relies on the pod
		// manager as the source of truth for the desired state. If a pod does
		// not exist in the pod manager, it means that it has been deleted in
		// the apiserver and no action (other than cleanup) is required.
		kl.podManager.AddPod(pod)
		//这段Go代码是一个for循环，其中pods是一个Pod的切片。
		//对于每个Pod，它会执行以下操作：
		//1. 调用kl.podManager.GetPods()获取当前已存在的Pods。
		//2. 调用kl.podManager.AddPod(pod)将当前Pod添加到podManager中。
		//这段代码的主要目的是将所有的Pod添加到podManager中。
		//podManager是Kubelet的核心组件之一，它负责管理Pod的生命周期。
		//通过将Pod添加到podManager中，Kubelet可以确保它知道集群中所有Pod的期望状态，以便它可以正确地管理和同步Pod的状态与实际状态。

		pod, mirrorPod, wasMirror := kl.podManager.GetPodAndMirrorPod(pod)
		if wasMirror {
			if pod == nil {
				klog.V(2).InfoS(&amp;#34;Unable to find pod for mirror pod, skipping&amp;#34;, &amp;#34;mirrorPod&amp;#34;, klog.KObj(mirrorPod), &amp;#34;mirrorPodUID&amp;#34;, mirrorPod.UID)
				continue
			}
			kl.podWorkers.UpdatePod(UpdatePodOptions{
				Pod: pod,
				MirrorPod: mirrorPod,
				UpdateType: kubetypes.SyncPodUpdate,
				StartTime: start,
			})
			continue
		}
		//这段Go代码是Kubernetes中kubelet的一个片段，主要功能是处理mirror pod和其对应的pod的关系。
		//- kl.podManager.GetPodAndMirrorPod(pod)函数用于获取mirror pod和其对应的pod。
		//- 如果wasMirror为true，表示当前处理的是mirror pod。
		//- 如果pod为空，则记录日志并跳过当前循环。
		//- 如果pod不为空，则调用kl.podWorkers.UpdatePod()函数更新pod信息，其中UpdateType为kubetypes.SyncPodUpdate，表示同步更新pod。
		//总结：这段代码主要处理mirror pod的更新操作。

		// Only go through the admission process if the pod is not requested
		// for termination by another part of the kubelet. If the pod is already
		// using resources (previously admitted), the pod worker is going to be
		// shutting it down. If the pod hasn&amp;#39;t started yet, we know that when
		// the pod worker is invoked it will also avoid setting up the pod, so
		// we simply avoid doing any work.
		if !kl.podWorkers.IsPodTerminationRequested(pod.UID) {
			// We failed pods that we rejected, so activePods include all admitted
			// pods that are alive.
			activePods := kl.filterOutInactivePods(existingPods)
			//这段Go代码中的函数是一个条件判断语句，其作用是判断某个Pod是否被其他部分请求终止。
			//如果该Pod没有被请求终止，并且之前已经被允许使用资源（先前已通过准入控制），则该函数会让Pod worker继续关闭该Pod。
			//如果该Pod还没有启动，函数会知道当Pod worker被调用时，它也会避免设置Pod，因此函数会避免执行任何操作。
			//如果Pod没有被请求终止，则函数会过滤掉所有非活跃的Pod，只保留活跃的Pod。

			if utilfeature.DefaultFeatureGate.Enabled(features.InPlacePodVerticalScaling) {
				// To handle kubelet restarts, test pod admissibility using AllocatedResources values
				// (for cpu &amp;amp; memory) from checkpoint store. If found, that is the source of truth.
				podCopy := pod.DeepCopy()
				kl.updateContainerResourceAllocation(podCopy)
				//这段Go代码中的函数主要做的是检查是否启用了InPlacePodVerticalScaling特性，
				//如果启用了，则会通过allocatedResources值（来自checkpoint store的CPU和内存）来测试pod的可接受性。
				//如果找到allocatedResources，则将其视为真相来源，并对pod进行深拷贝并更新容器资源分配。

				// Check if we can admit the pod; if not, reject it.
				if ok, reason, message := kl.canAdmitPod(activePods, podCopy); !ok {
					kl.rejectPod(pod, reason, message)
					continue
				}
				//这段Go代码是一个条件判断语句，其主要功能是检查是否可以接受一个Pod（容器的组合），如果不能接受，则拒绝该Pod。
				//具体来说，函数kl.canAdmitPod(activePods, podCopy)会返回一个布尔值ok、一个原因reason和一条消息message。
				//如果ok为false，则表示不能接受该Pod，此时会调用kl.rejectPod(pod, reason, message)函数来拒绝该Pod，并继续执行下一次循环。
				//需要注意的是，这里的kl是一个对象实例，它调用了canAdmitPod和rejectPod两个方法。
				//根据代码上下文来理解，这段代码应该是在处理 Kubernetes 中的 Pod 调度逻辑。

				// For new pod, checkpoint the resource values at which the Pod has been admitted
				if err := kl.statusManager.SetPodAllocation(podCopy); err != nil {
					//TODO(vinaykul,InPlacePodVerticalScaling): Can we recover from this in some way? Investigate
					klog.ErrorS(err, &amp;#34;SetPodAllocation failed&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod))
				}
				//该函数用于在创建新的Pod时，设置Pod的资源分配情况。具体操作是调用kl.statusManager.SetPodAllocation(podCopy)方法，
				//将Pod的资源分配信息保存到Pod的状态中。如果设置失败，会记录错误日志，并标记为待调查的问题。

			} else {
				// Check if we can admit the pod; if not, reject it.
				if ok, reason, message := kl.canAdmitPod(activePods, pod); !ok {
					kl.rejectPod(pod, reason, message)
					continue
					//这段代码是Kubernetes中处理Pod调度的逻辑。如果Pod不能被当前节点接纳，则会拒绝该Pod，并继续处理下一个Pod。
					//kl.canAdmitPod()函数检查Pod是否可以被接纳，
					//如果不可以，则调用kl.rejectPod()函数拒绝Pod，并提供拒绝的原因和信息。

				}
			}
		}
		kl.podWorkers.UpdatePod(UpdatePodOptions{
			Pod: pod,
			MirrorPod: mirrorPod,
			UpdateType: kubetypes.SyncPodCreate,
			StartTime: start,
			//该函数用于更新Pod的状态信息。
			//- 参数UpdatePodOptions包含了更新Pod所需的各项参数：
			//- Pod: 要更新的Pod对象。 - MirrorPod: Pod的镜像对象。
			//- UpdateType: 更新类型，此处为SyncPodCreate，表示创建Pod。 - StartTime: 更新开始时间。
			//- 该函数会根据传入的参数更新Pod的状态，并进行相应的日志记录和状态同步操作

		})
	}
}

//这段Go代码是Kubernetes中kubelet组件的一部分，用于处理新创建的Pod的分配和准入控制。
//1. 如果是新Pod，会调用kl.statusManager.SetPodAllocation(podCopy)函数来记录Pod被接纳时的资源值。如果记录失败，会打印错误日志。
//2. 如果Pod不是新创建的，则会调用kl.canAdmitPod(activePods, pod)函数来检查是否可以接纳该Pod。
//如果不能接纳，则会调用kl.rejectPod(pod, reason, message)函数来拒绝Pod，并继续处理下一个Pod。
//3. 如果Pod可以接纳，则会调用kl.podWorkers.UpdatePod()函数来更新Pod的状态。
//这段代码的主要作用是实现Pod的准入控制和状态更新。

// updateContainerResourceAllocation updates AllocatedResources values
// (for cpu &amp;amp; memory) from checkpoint store
func (kl *Kubelet) updateContainerResourceAllocation(pod *v1.Pod) {
	for _, c := range pod.Spec.Containers {
		allocatedResources, found := kl.statusManager.GetContainerResourceAllocation(string(pod.UID), c.Name)
		if c.Resources.Requests != nil &amp;amp;&amp;amp; found {
			if _, ok := allocatedResources[v1.ResourceCPU]; ok {
				c.Resources.Requests[v1.ResourceCPU] = allocatedResources[v1.ResourceCPU]
			}
			if _, ok := allocatedResources[v1.ResourceMemory]; ok {
				c.Resources.Requests[v1.ResourceMemory] = allocatedResources[v1.ResourceMemory]
			}
		}
	}
}

//该函数的作用是更新容器的资源分配信息（包括CPU和内存）。
//具体实现如下：
//1. 遍历Pod中所有的容器。
//2. 对于每个容器，通过调用kl.statusManager.GetContainerResourceAllocation方法，获取该容器的资源分配信息。
//3. 如果该容器的资源请求（c.Resources.Requests）不为空且成功获取到资源分配信息（allocatedResources），
//则将分配的CPU和内存资源信息更新到容器的资源请求中。

// HandlePodUpdates is the callback in the SyncHandler interface for pods
// being updated from a config source.
func (kl *Kubelet) HandlePodUpdates(pods []*v1.Pod) {
	start := kl.clock.Now()
	for _, pod := range pods {
		kl.podManager.UpdatePod(pod)

		pod, mirrorPod, wasMirror := kl.podManager.GetPodAndMirrorPod(pod)
		if wasMirror {
			if pod == nil {
				klog.V(2).InfoS(&amp;#34;Unable to find pod for mirror pod, skipping&amp;#34;, &amp;#34;mirrorPod&amp;#34;, klog.KObj(mirrorPod), &amp;#34;mirrorPodUID&amp;#34;, mirrorPod.UID)
				continue
			}
		}
		//该函数是Kubelet的HandlePodUpdates方法，用于处理从配置源更新的Pod。
		//它遍历传入的Pod数组，并通过kl.podManager.UpdatePod(pod)更新每个Pod。
		//然后，它获取每个Pod及其镜像Pod，并根据是否是镜像Pod进行不同的处理。
		//如果找不到对应的Pod，则跳过该镜像Pod。

		kl.podWorkers.UpdatePod(UpdatePodOptions{
			Pod: pod,
			MirrorPod: mirrorPod,
			UpdateType: kubetypes.SyncPodUpdate,
			StartTime: start,
		})
	}
	//该函数用于更新Pod的状态信息。
	//- 参数pod是要更新的Pod对象。
	//- 参数mirrorPod是Pod的镜像对象。
	//- 参数updateType指定更新的类型，这里是同步更新。
	//- 参数startTime记录更新开始的时间。
	//该函数会根据参数更新Pod的相关状态，并记录更新的开始时间。

}

// HandlePodRemoves is the callback in the SyncHandler interface for pods
// being removed from a config source.
func (kl *Kubelet) HandlePodRemoves(pods []*v1.Pod) {
	start := kl.clock.Now()
	for _, pod := range pods {
		kl.podManager.RemovePod(pod)
		//该函数是Kubelet的HandlePodRemoves方法，用于处理从配置源中移除的Pods。
		//它遍历传入的Pods数组，并通过kl.podManager.RemovePod方法将每个Pod从管理器中移除。

		pod, mirrorPod, wasMirror := kl.podManager.GetPodAndMirrorPod(pod)
		if wasMirror {
			if pod == nil {
				klog.V(2).InfoS(&amp;#34;Unable to find pod for mirror pod, skipping&amp;#34;, &amp;#34;mirrorPod&amp;#34;, klog.KObj(mirrorPod), &amp;#34;mirrorPodUID&amp;#34;, mirrorPod.UID)
				continue
			}
			kl.podWorkers.UpdatePod(UpdatePodOptions{
				Pod: pod,
				MirrorPod: mirrorPod,
				UpdateType: kubetypes.SyncPodUpdate,
				StartTime: start,
			})
			continue
		}
		//这段Go代码是Kubernetes中kubelet的一个片段，主要功能是处理mirror pod和其对应的pod的关系。
		//- kl.podManager.GetPodAndMirrorPod(pod)函数用于获取mirror pod和其对应的pod。
		//- 如果wasMirror为true，表示当前处理的是mirror pod。
		//- 如果pod为空，则记录日志并跳过当前循环。
		//- 如果pod不为空，则调用kl.podWorkers.UpdatePod()函数更新pod信息，其中UpdateType为kubetypes.SyncPodUpdate，表示同步更新pod。
		//总结：这段代码主要处理mirror pod的更新操作。

		// Deletion is allowed to fail because the periodic cleanup routine
		// will trigger deletion again.
		if err := kl.deletePod(pod); err != nil {
			klog.V(2).InfoS(&amp;#34;Failed to delete pod&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod), &amp;#34;err&amp;#34;, err)
		}
	}
}

//这段Go代码中的函数是一个if条件语句，其主要功能是尝试删除一个pod（Kubernetes中的一个容器化应用程序实例）。
//如果删除操作失败，函数会记录一条日志信息。 具体分析如下：
//1. 这段代码首先调用kl.deletePod(pod)方法尝试删除pod。
//2. 如果删除操作失败，即err != nil，则会执行if语句内部的代码块。
//3. 在代码块中，会使用klog.V(2).InfoS方法记录一条日志信息，包含以下内容：
//- &amp;#34;Failed to delete pod&amp;#34;：表示删除pod失败。
//- &amp;#34;pod&amp;#34;：表示被删除的pod的详细信息。
//- &amp;#34;err&amp;#34;：表示删除操作失败时返回的错误信息。
//4. 该函数的主要目的是允许删除操作失败，因为有一个定期清理程序会再次触发删除操作。
//总结：这段代码的主要功能是尝试删除一个pod，并在删除失败时记录一条日志信息。

// HandlePodReconcile is the callback in the SyncHandler interface for pods
// that should be reconciled. Pods are reconciled when only the status of the
// pod is updated in the API.
func (kl *Kubelet) HandlePodReconcile(pods []*v1.Pod) {
	start := kl.clock.Now()
	for _, pod := range pods {
		// Update the pod in pod manager, status manager will do periodically reconcile according
		// to the pod manager.
		kl.podManager.UpdatePod(pod)
		//该函数是Kubelet实现SyncHandler接口的回调函数，用于处理需要进行状态同步的Pod。
		//当Pod的状态在API中被更新时，就会触发Pod的同步。
		//函数遍历传入的Pod列表，通过调用Kubelet的podManager的UpdatePod方法，
		//更新Pod的状态，并由statusManager定期进行状态同步。

		pod, mirrorPod, wasMirror := kl.podManager.GetPodAndMirrorPod(pod)
		if wasMirror {
			if pod == nil {
				klog.V(2).InfoS(&amp;#34;Unable to find pod for mirror pod, skipping&amp;#34;, &amp;#34;mirrorPod&amp;#34;, klog.KObj(mirrorPod), &amp;#34;mirrorPodUID&amp;#34;, mirrorPod.UID)
				continue
			}
			// Static pods should be reconciled the same way as regular pods
		}
		//这段Go代码是Kubernetes中的一部分，用于处理Pod和其对应的Mirror Pod的关系。
		//下面是这段代码的功能解释：
		//- kl.podManager.GetPodAndMirrorPod(pod)：获取给定Pod及其对应的Mirror Pod。
		//- wasMirror：判断返回的Mirror Pod是否是之前已经存在的。
		//- 如果wasMirror为true，则进一步判断pod是否为nil。
		//- 如果pod为nil，则记录日志信息，并跳过当前循环。
		//- 如果pod不为nil，则将Static Pods与普通Pods进行一致性处理。
		//这段代码的主要作用是在处理Pod的过程中，对Mirror Pod进行相应的处理。

		// TODO: reconcile being calculated in the config manager is questionable, and avoiding
		// extra syncs may no longer be necessary. Reevaluate whether Reconcile and Sync can be
		// merged (after resolving the next two TODOs).

		// Reconcile Pod &amp;#34;Ready&amp;#34; condition if necessary. Trigger sync pod for reconciliation.
		// TODO: this should be unnecessary today - determine what is the cause for this to
		// be different than Sync, or if there is a better place for it. For instance, we have
		// needsReconcile in kubelet/config, here, and in status_manager.
		if status.NeedToReconcilePodReadiness(pod) {
			kl.podWorkers.UpdatePod(UpdatePodOptions{
				Pod: pod,
				MirrorPod: mirrorPod,
				UpdateType: kubetypes.SyncPodSync,
				StartTime: start,
			})
		}
		//这段Go代码中的函数是一个待完成的任务，它的功能是重新调整Pod的&amp;#34;Ready&amp;#34;状态，如果需要的话。
		//该函数的实现需要进一步评估和确定，因为它可能不再必要，而且可能会导致额外的同步操作。
		//函数的具体实现包括检查Pod是否需要重新调整状态，并根据需要触发同步操作。
		//这个函数存在一些待解决的问题和TODO，需要进一步解决和改进。

		// After an evicted pod is synced, all dead containers in the pod can be removed.
		// TODO: this is questionable - status read is async and during eviction we already
		// expect to not have some container info. The pod worker knows whether a pod has
		// been evicted, so if this is about minimizing the time to react to an eviction we
		// can do better. If it&amp;#39;s about preserving pod status info we can also do better.
		if eviction.PodIsEvicted(pod.Status) {
			if podStatus, err := kl.podCache.Get(pod.UID); err == nil {
				kl.containerDeletor.deleteContainersInPod(&amp;#34;&amp;#34;, podStatus, true)
			}
		}
	}
	//这段Go代码中的函数用于在Pod被驱逐后同步删除Pod中的所有死亡容器。
	//函数首先检查Pod是否已被驱逐，如果是，则尝试从podCache中获取Pod的状态信息。
	//如果获取成功，则调用kl.containerDeletor.deleteContainersInPod函数删除Pod中的所有容器。

}

// HandlePodSyncs is the callback in the syncHandler interface for pods
// that should be dispatched to pod workers for sync.
func (kl *Kubelet) HandlePodSyncs(pods []*v1.Pod) {
	start := kl.clock.Now()
	for _, pod := range pods {
		pod, mirrorPod, wasMirror := kl.podManager.GetPodAndMirrorPod(pod)
		if wasMirror {
			if pod == nil {
				klog.V(2).InfoS(&amp;#34;Unable to find pod for mirror pod, skipping&amp;#34;, &amp;#34;mirrorPod&amp;#34;, klog.KObj(mirrorPod), &amp;#34;mirrorPodUID&amp;#34;, mirrorPod.UID)
				continue
			}
			//该函数是Kubelet的HandlePodSyncs方法，用于处理同步Pod的工作。
			//它接收一个Pod列表作为参数，然后遍历每个Pod，获取对应的Pod和镜像Pod，并判断是否为镜像Pod。
			//如果是镜像Pod且找不到对应的Pod，则跳过该Pod的处理。

			// Syncing a mirror pod is a programmer error since the intent of sync is to
			// batch notify all pending work. We should make it impossible to double sync,
			// but for now log a programmer error to prevent accidental introduction.
			klog.V(3).InfoS(&amp;#34;Programmer error, HandlePodSyncs does not expect to receive mirror pods&amp;#34;, &amp;#34;podUID&amp;#34;, pod.UID, &amp;#34;mirrorPodUID&amp;#34;, mirrorPod.UID)
			continue
		}
		kl.podWorkers.UpdatePod(UpdatePodOptions{
			Pod: pod,
			MirrorPod: mirrorPod,
			UpdateType: kubetypes.SyncPodSync,
			StartTime: start,
		})
	}
}

//该函数是Kubelet的HandlePodSyncs方法，用于处理Pod的同步操作。
//它接收一个Pod列表作为参数，遍历每个Pod，获取对应的Pod和mirror Pod，并根据情况执行相应的操作。
//如果传入的是mirror Pod，则记录日志并跳过。
//否则，通过podWorkers更新Pod的状态。

func isPodResizeInProgress(pod *v1.Pod, podStatus *v1.PodStatus) bool {
	for _, c := range pod.Spec.Containers {
		if cs, ok := podutil.GetContainerStatus(podStatus.ContainerStatuses, c.Name); ok {
			if cs.Resources == nil {
				continue
			}
			if !cmp.Equal(c.Resources.Limits, cs.Resources.Limits) || !cmp.Equal(cs.AllocatedResources, cs.Resources.Requests) {
				return true
			}
		}
	}
	return false
}

//该函数用于检查Pod中是否有容器的资源正在扩容中。
//它遍历Pod的容器规格，获取每个容器的状态，然后比较容器的资源限制和请求是否相等。
//如果不相等，则表示容器资源正在扩容，函数返回true；
//否则，返回false。

func (kl *Kubelet) canResizePod(pod *v1.Pod) (bool, *v1.Pod, v1.PodResizeStatus) {
	var otherActivePods []*v1.Pod

	node, err := kl.getNodeAnyWay()
	if err != nil {
		klog.ErrorS(err, &amp;#34;getNodeAnyway function failed&amp;#34;)
		return false, nil, &amp;#34;&amp;#34;
	}
	podCopy := pod.DeepCopy()
	cpuAvailable := node.Status.Allocatable.Cpu().MilliValue()
	memAvailable := node.Status.Allocatable.Memory().Value()
	cpuRequests := resource.GetResourceRequest(podCopy, v1.ResourceCPU)
	memRequests := resource.GetResourceRequest(podCopy, v1.ResourceMemory)
	if cpuRequests &amp;gt; cpuAvailable || memRequests &amp;gt; memAvailable {
		klog.V(3).InfoS(&amp;#34;Resize is not feasible as request exceeds allocatable node resources&amp;#34;, &amp;#34;pod&amp;#34;, podCopy.Name)
		return false, podCopy, v1.PodResizeStatusInfeasible
		//该函数的主要功能是判断是否可以对给定的Pod进行扩容。具体步骤包括：
		//通过kl.getNodeAnyWay()获取一个Node节点，深拷贝Pod对象，计算Node节点的可用CPU和内存资源量，获取Pod的CPU和内存请求量，
		//如果Pod的请求量超过了Node的可用资源量，则认为扩容不可行，并返回相应的信息。

	}
	//该函数是Kubelet的一个方法，用于判断是否可以调整Pod的大小。它首先获取当前节点的信息，并创建Pod的深拷贝。
	//然后，它比较节点可用的CPU和内存资源与Pod请求的资源，如果请求的资源超过了节点的可用资源，则返回不可调整大小的错误信息。

	// Treat the existing pod needing resize as a new pod with desired resources seeking admit.
	// If desired resources don&amp;#39;t fit, pod continues to run with currently allocated resources.
	activePods := kl.GetActivePods()
	for _, p := range activePods {
		if p.UID != pod.UID {
			otherActivePods = append(otherActivePods, p)
		}
	}
	//该函数用于处理需要调整资源的现有Pod，将其视为具有所需资源的新Pod寻求准入。
	//如果所需资源无法适应，则Pod将继续以目前分配的资源运行。
	//函数首先获取所有活跃的Pod，然后遍历这些Pod，将与给定Pod UID不相同的Pod添加到otherActivePods列表中。

	if ok, failReason, failMessage := kl.canAdmitPod(otherActivePods, podCopy); !ok {
		// Log reason and return. Let the next sync iteration retry the resize
		klog.V(3).InfoS(&amp;#34;Resize cannot be accommodated&amp;#34;, &amp;#34;pod&amp;#34;, podCopy.Name, &amp;#34;reason&amp;#34;, failReason, &amp;#34;message&amp;#34;, failMessage)
		return false, podCopy, v1.PodResizeStatusDeferred
	}
	//该函数是kl.canAdmitPod()的调用方，用于判断是否可以调整pod的大小。
	//如果kl.canAdmitPod()返回不成功，即不满足调整大小的条件，则记录原因并返回false，
	//同时返回不成功的pod副本和v1.PodResizeStatusDeferred状态。

	for _, container := range podCopy.Spec.Containers {
		idx, found := podutil.GetIndexOfContainerStatus(podCopy.Status.ContainerStatuses, container.Name)
		if found {
			for rName, rQuantity := range container.Resources.Requests {
				podCopy.Status.ContainerStatuses[idx].AllocatedResources[rName] = rQuantity
			}
		}
	}
	return true, podCopy, v1.PodResizeStatusInProgress
}

//该函数的功能是更新podCopy的状态，将每个容器的资源请求分配给对应的容器状态。
//具体来说，它遍历podCopy的Spec.Containers，通过podutil.GetIndexOfContainerStatus函数获取每个容器在podCopy.Status.ContainerStatuses中的索引，
//如果找到，则遍历该容器的Resources.Requests，将其分配给podCopy.Status.ContainerStatuses中对应容器的AllocatedResources。
//最后，函数返回true，podCopy和v1.PodResizeStatusInProgress，表示更新成功并且正在进行调整大小操作。

func (kl *Kubelet) handlePodResourcesResize(pod *v1.Pod) *v1.Pod {
	if pod.Status.Phase != v1.PodRunning {
		return pod
	}
	podResized := false
	for _, container := range pod.Spec.Containers {
		if len(container.Resources.Requests) == 0 {
			continue
		}
		containerStatus, found := podutil.GetContainerStatus(pod.Status.ContainerStatuses, container.Name)
		if !found {
			klog.V(5).InfoS(&amp;#34;ContainerStatus not found&amp;#34;, &amp;#34;pod&amp;#34;, pod.Name, &amp;#34;container&amp;#34;, container.Name)
			break
		}
		//该函数是Kubelet的一个方法，用于处理Pod资源的调整。
		//当Pod的状态不是运行中时，直接返回该Pod。
		//遍历Pod的容器，如果容器的资源请求不为空，则尝试获取容器的状态。
		//如果找不到容器状态，则记录日志并退出循环。

		if len(containerStatus.AllocatedResources) != len(container.Resources.Requests) {
			klog.V(5).InfoS(&amp;#34;ContainerStatus.AllocatedResources length mismatch&amp;#34;, &amp;#34;pod&amp;#34;, pod.Name, &amp;#34;container&amp;#34;, container.Name)
			break
		}
		if !cmp.Equal(container.Resources.Requests, containerStatus.AllocatedResources) {
			podResized = true
			break
		}
	}
	//这段Go代码是在检查容器状态（containerStatus）中已分配的资源与容器请求的资源（container.Resources.Requests）是否一致。
	//首先，它会比较两个资源列表的长度，如果不相等，则记录一条信息并跳出循环。
	//接着，它使用cmp.Equal函数来比较两个资源列表的内容是否完全相等，如果不相等，则将podResized标记为true并跳出循环。
	//总的来说，这段代码的主要作用是检测容器的资源请求是否已经被正确分配。

	if !podResized {
		return pod
	}

	kl.podResizeMutex.Lock()
	defer kl.podResizeMutex.Unlock()
	fit, updatedPod, resizeStatus := kl.canResizePod(pod)
	if updatedPod == nil {
		return pod
	}
	if fit {
		// Update pod resource allocation checkpoint
		if err := kl.statusManager.SetPodAllocation(updatedPod); err != nil {
			//TODO(vinaykul,InPlacePodVerticalScaling): Can we recover from this in some way? Investigate
			klog.ErrorS(err, &amp;#34;SetPodAllocation failed&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(updatedPod))
			return pod
		}
	}
	//这段Go代码主要实现了Pod的垂直扩展功能。
	//具体来说，它首先检查Pod是否已经进行了大小调整，如果没有，则直接返回原始Pod。
	//然后，它使用互斥锁来确保对Pod大小调整操作的原子性。
	//接下来，它调用canResizePod函数来检查是否可以对Pod进行大小调整，
	//如果返回的updatedPod为nil，则表示无法调整大小，直接返回原始Pod。如果可以调整大小，则将更新后的Pod的资源分配状态保存到状态管理器中。
	//如果保存失败，则记录错误日志并返回原始Pod。

	if resizeStatus != &amp;#34;&amp;#34; {
		// Save resize decision to checkpoint
		if err := kl.statusManager.SetPodResizeStatus(updatedPod.UID, resizeStatus); err != nil {
			//TODO(vinaykul,InPlacePodVerticalScaling): Can we recover from this in some way? Investigate
			klog.ErrorS(err, &amp;#34;SetPodResizeStatus failed&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(updatedPod))
			return pod
		}
		updatedPod.Status.Resize = resizeStatus
	}
	kl.podManager.UpdatePod(updatedPod)
	kl.statusManager.SetPodStatus(updatedPod, updatedPod.Status)
	return updatedPod
}

//这段Go代码中的函数逻辑如下：
//首先，检查resizeStatus是否为空。如果不为空，则执行以下操作：
//1. 尝试将调整大小的决策保存到检查点，调用kl.statusManager.SetPodResizeStatus方法，并传入updatedPod.UID和resizeStatus作为参数。
//如果出现错误，则记录错误日志并返回原始pod对象。
//2. 更新updatedPod的状态，将resizeStatus赋值给updatedPod.Status.Resize。
//接下来，无论resizeStatus是否为空，都会执行以下操作：
//1. 调用kl.podManager.UpdatePod方法，传入updatedPod对象进行更新。
//2. 调用kl.statusManager.SetPodStatus方法，传入updatedPod对象和updatedPod.Status，以更新Pod的状态。
//最后，返回更新后的updatedPod对象。
//这段代码的主要目的是更新Pod的状态，并根据resizeStatus是否为空来决定是否保存调整大小的决策到检查点并更新Pod的状态。

// LatestLoopEntryTime returns the last time in the sync loop monitor.
func (kl *Kubelet) LatestLoopEntryTime() time.Time {
	val := kl.syncLoopMonitor.Load()
	if val == nil {
		return time.Time{}
	}
	return val.(time.Time)
}

//该函数用于获取Kubelet同步循环监视器中的最后一个时间入口。
//它首先使用kl.syncLoopMonitor.Load()方法加载监视器的值，如果值为nil，则返回空的时间类型；
//否则，将值转换为时间类型并返回。

// updateRuntimeUp calls the container runtime status callback, initializing
// the runtime dependent modules when the container runtime first comes up,
// and returns an error if the status check fails. If the status check is OK,
// update the container runtime uptime in the kubelet runtimeState.
func (kl *Kubelet) updateRuntimeUp() {
	kl.updateRuntimeMux.Lock()
	defer kl.updateRuntimeMux.Unlock()
	ctx := context.Background()
	//该函数是Kubelet的一个方法，用于更新容器运行时的状态。
	//它首先通过调用容器运行时的状态回调函数来初始化运行时依赖的模块，然后检查容器运行时的状态是否正常。
	//如果状态检查失败，会返回一个错误。
	//如果状态检查正常，则会更新容器运行时的运行时间。函数中使用了互斥锁来保证更新操作的原子性。

	s, err := kl.containerRuntime.Status(ctx)
	if err != nil {
		klog.ErrorS(err, &amp;#34;Container runtime sanity check failed&amp;#34;)
		return
	}
	if s == nil {
		klog.ErrorS(nil, &amp;#34;Container runtime status is nil&amp;#34;)
		return
	}
	//该Go函数通过调用kl.containerRuntime.Status(ctx)获取容器运行时的状态，并根据状态进行相应的错误处理。
	//- 如果获取状态时出现错误，会使用klog.ErrorS记录错误日志，并返回。
	//- 如果获取的状态为nil，会使用klog.ErrorS记录错误日志，并返回。

	// Periodically log the whole runtime status for debugging.
	klog.V(4).InfoS(&amp;#34;Container runtime status&amp;#34;, &amp;#34;status&amp;#34;, s)
	klogErrorS := klog.ErrorS
	if !kl.containerRuntimeReadyExpected {
		klogErrorS = klog.V(4).ErrorS
	}
	networkReady := s.GetRuntimeCondition(kubecontainer.NetworkReady)
	//该代码片段是Go语言编写的，它主要实现了定期打印容器运行时状态的功能，用于调试。
	//具体来说，函数首先使用klog.V(4).InfoS方法记录一条日志，内容为&amp;#34;Container runtime status&amp;#34;和状态s。
	//然后，根据变量kl.containerRuntimeReadyExpected的值，选择使用不同的日志级别记录错误信息。
	//最后，该函数获取容器运行时的网络状态，并将其保存在变量networkReady中。

	if networkReady == nil || !networkReady.Status {
		klogErrorS(nil, &amp;#34;Container runtime network not ready&amp;#34;, &amp;#34;networkReady&amp;#34;, networkReady)
		kl.runtimeState.setNetworkState(fmt.Errorf(&amp;#34;container runtime network not ready: %v&amp;#34;, networkReady))
	} else {
		// Set nil if the container runtime network is ready.
		kl.runtimeState.setNetworkState(nil)
	}
	// information in RuntimeReady condition will be propagated to NodeReady condition.
	runtimeReady := s.GetRuntimeCondition(kubecontainer.RuntimeReady)
	// If RuntimeReady is not set or is false, report an error.
	//这段Go代码主要功能是检查容器运行时网络是否准备就绪，并更新运行时状态。
	//首先，它检查networkReady是否为nil或状态为false。
	//如果是，则记录错误并设置运行时状态为&amp;#34;container runtime network not ready&amp;#34;。
	//否则，将运行时状态设置为nil。
	//接下来，它获取运行时准备就绪的条件，并检查是否设置且为true。
	//如果不是，则报告错误。 这段代码的目的是确保容器运行时网络已准备就绪，并更新运行时状态。

	if runtimeReady == nil || !runtimeReady.Status {
		klogErrorS(nil, &amp;#34;Container runtime not ready&amp;#34;, &amp;#34;runtimeReady&amp;#34;, runtimeReady)
		kl.runtimeState.setRuntimeState(fmt.Errorf(&amp;#34;container runtime not ready: %v&amp;#34;, runtimeReady))
		return
	}
	//这段Go代码是用于定期记录容器运行时状态的，主要用于调试。
	//函数首先使用klog.V(4).InfoS记录容器运行时状态的信息。
	//然后根据kl.containerRuntimeReadyExpected的值决定使用哪个级别的日志记录错误信息。
	//接着检查容器运行时的网络状态，如果不Ready，则记录错误信息并设置相应的状态。
	//最后检查容器运行时的RuntimeReady状态，如果不Ready，则记录错误信息并设置相应的状态，并返回。

	kl.runtimeState.setRuntimeState(nil)
	kl.runtimeState.setRuntimeHandlers(s.Handlers)
	kl.oneTimeInitializer.Do(kl.initializeRuntimeDependentModules)
	kl.runtimeState.setRuntimeSync(kl.clock.Now())
}

//这段Go代码是用于定期记录容器运行时状态的，主要用于调试。
//函数首先使用klog.V(4).InfoS记录容器运行时状态的信息，然后根据kl.containerRuntimeReadyExpected的值决定使用哪个级别的日志记录错误信息。
//接着检查容器运行时的网络状态，如果不Ready，则记录错误信息并设置相应的状态。
//最后检查容器运行时的RuntimeReady状态，如果不Ready，则记录错误信息并设置相应的状态，并返回。
//如果容器运行时状态正常，则会执行以下操作：
//1. 调用kl.runtimeState.setRuntimeState(nil)设置运行时状态为正常。
//2. 调用kl.runtimeState.setRuntimeHandlers(s.Handlers)设置运行时处理程序。
//3. 调用kl.oneTimeInitializer.Do(kl.initializeRuntimeDependentModules)执行一次初始化运行时依赖模块的操作。
//4. 调用kl.runtimeState.setRuntimeSync(kl.clock.Now())设置运行时同步时间。

// GetConfiguration returns the KubeletConfiguration used to configure the kubelet.
func (kl *Kubelet) GetConfiguration() kubeletconfiginternal.KubeletConfiguration {
	return kl.kubeletConfiguration
}

//该函数是一个Go语言函数，名为GetConfiguration，它属于Kubelet结构体。
//函数的作用是返回用于配置kubelet的KubeletConfiguration。
//- kl *Kubelet：函数接收一个Kubelet类型的指针作为参数。
//- kubeletconfiginternal.KubeletConfiguration：函数返回一个KubeletConfiguration类型的结果。
//函数实现很简单，就是直接返回kl对象的kubeletConfiguration属性。

// BirthCry sends an event that the kubelet has started up.
func (kl *Kubelet) BirthCry() {
	// Make an event that kubelet restarted.
	kl.recorder.Eventf(kl.nodeRef, v1.EventTypeNormal, events.StartingKubelet, &amp;#34;Starting kubelet.&amp;#34;)
}

//这个函数的功能是发送一个事件，表示kubelet已经启动。
//它通过调用kl.recorder.Eventf方法，传入kl.nodeRef、v1.EventTypeNormal、events.StartingKubelet和&amp;#34;Starting kubelet.&amp;#34;作为参数，
//来记录kubelet重启的事件。

// ResyncInterval returns the interval used for periodic syncs.
func (kl *Kubelet) ResyncInterval() time.Duration {
	return kl.resyncInterval
}

//该函数是Kubelet的一个方法，返回用于周期性同步的时间间隔。
//- kl *Kubelet：表示Kubelet的实例。
//- return kl.resyncInterval：返回Kubelet实例中的resyncInterval属性值，其类型为time.Duration，表示时间间隔。

// ListenAndServe runs the kubelet HTTP server.
func (kl *Kubelet) ListenAndServe(kubeCfg *kubeletconfiginternal.KubeletConfiguration, tlsOptions *server.TLSOptions,
	auth server.AuthInterface, tp trace.TracerProvider) {
	server.ListenAndServeKubeletServer(kl, kl.resourceAnalyzer, kubeCfg, tlsOptions, auth, tp)
}

//该函数是Kubelet的ListenAndServe方法，用于启动kubelet的HTTP服务器。
//它通过调用server.ListenAndServeKubeletServer方法来实现，传入Kubelet实例、资源分析器、kubelet配置、TLS选项、认证接口和跟踪提供者作为参数。

// ListenAndServeReadOnly runs the kubelet HTTP server in read-only mode.
func (kl *Kubelet) ListenAndServeReadOnly(address net.IP, port uint) {
	server.ListenAndServeKubeletReadOnlyServer(kl, kl.resourceAnalyzer, address, port)
}

//该函数用于以只读模式启动kubelet HTTP服务器。它通过调用server.ListenAndServeKubeletReadOnlyServer函数来实现，
//该函数接受kl（Kubelet实例）、kl.resourceAnalyzer（资源分析器）以及地址和端口作为参数，用于监听和提供服务。

// ListenAndServePodResources runs the kubelet podresources grpc service
func (kl *Kubelet) ListenAndServePodResources() {
	endpoint, err := util.LocalEndpoint(kl.getPodResourcesDir(), podresources.Socket)
	if err != nil {
		klog.V(2).InfoS(&amp;#34;Failed to get local endpoint for PodResources endpoint&amp;#34;, &amp;#34;err&amp;#34;, err)
		return
	}
	//该函数用于启动kubelet的podresources gRPC服务。
	//它首先通过调用util.LocalEndpoint函数获取本地端点，该函数的参数是kubelet的pod资源目录和Socket。
	//如果获取本地端点失败，则通过klog.V(2).InfoS记录日志信息，并返回。

	providers := podresources.PodResourcesProviders{
		Pods: kl.podManager,
		Devices: kl.containerManager,
		Cpus: kl.containerManager,
		Memory: kl.containerManager,
		DynamicResources: kl.containerManager,
	}

	server.ListenAndServePodResources(endpoint, providers)
}

//这段Go代码创建了一个podresources.PodResourcesProviders结构体实例providers，
//其中包含了Pods、Devices、Cpus、Memory和DynamicResources等字段，它们都是kl.containerManager的实例。
//接着，代码通过调用server.ListenAndServePodResources函数，启动了一个服务，监听指定的endpoint，并使用providers作为参数提供Pod资源。

// Delete the eligible dead container instances in a pod. Depending on the configuration, the latest dead containers may be kept around.
func (kl *Kubelet) cleanUpContainersInPod(podID types.UID, exitedContainerID string) {
	if podStatus, err := kl.podCache.Get(podID); err == nil {
		// When an evicted or deleted pod has already synced, all containers can be removed.
		removeAll := kl.podWorkers.ShouldPodContentBeRemoved(podID)
		kl.containerDeletor.deleteContainersInPod(exitedContainerID, podStatus, removeAll)
	}
}

//该函数用于清理Pod中的死亡容器实例。根据配置，可能会保留最新的死亡容器。
//函数首先通过kl.podCache.Get(podID)获取Pod的状态。
//然后，根据kl.podWorkers.ShouldPodContentBeRemoved(podID)的返回值，
//调用kl.containerDeletor.deleteContainersInPod(exitedContainerID, podStatus, removeAll)来删除容器。

// fastStatusUpdateOnce starts a loop that checks if the current state of kubelet + container runtime
// would be able to turn the node ready, and sync the ready state to the apiserver as soon as possible.
// Function returns after the node status update after such event, or when the node is already ready.
// Function is executed only during Kubelet start which improves latency to ready node by updating
// kubelet state, runtime status and node statuses ASAP.
func (kl *Kubelet) fastStatusUpdateOnce() {
	ctx := context.Background()
	start := kl.clock.Now()
	stopCh := make(chan struct{})
	//该函数用于在kubelet启动时快速更新节点状态。
	//它会循环检查kubelet和容器运行时的状态，如果发现节点可以变为准备就绪状态，就会立即同步该状态到apiserver。
	//函数会在节点状态更新后或节点已经准备就绪时返回。
	//该函数通过启动一个上下文对象、记录开始时间并创建一个停止通道来实现循环的控制和终止。

	// Keep trying to make fast node status update until either timeout is reached or an update is successful.
	wait.Until(func() {
		// fastNodeStatusUpdate returns true when it succeeds or when the grace period has expired
		// (status was not updated within nodeReadyGracePeriod and the second argument below gets true),
		// then we close the channel and abort the loop.
		if kl.fastNodeStatusUpdate(ctx, kl.clock.Since(start) &amp;gt;= nodeReadyGracePeriod) {
			close(stopCh)
		}
	}, 100*time.Millisecond, stopCh)
}

//这段Go代码使用了wait.Until函数，它会在给定的stopCh通道关闭之前，周期性地执行传入的函数。
//函数的主要作用是尝试进行快速节点状态更新，直到超时或更新成功。
//具体实现中，函数内部调用了kl.fastNodeStatusUpdate方法尝试更新节点状态。
//如果更新成功或超过了nodeReadyGracePeriod时间间隔，则会关闭stopCh通道，结束循环。
//总结起来，这段代码的功能是在一定时间内持续尝试更新节点状态，直到更新成功或超时。

// CheckpointContainer tries to checkpoint a container. The parameters are used to
// look up the specified container. If the container specified by the given parameters
// cannot be found an error is returned. If the container is found the container
// engine will be asked to checkpoint the given container into the kubelet&amp;#39;s default
// checkpoint directory.
func (kl *Kubelet) CheckpointContainer(
	ctx context.Context,
	podUID types.UID,
	podFullName,
	containerName string,
	options *runtimeapi.CheckpointContainerRequest,
) error {
	container, err := kl.findContainer(ctx, podFullName, podUID, containerName)
	if err != nil {
		return err
	}
	if container == nil {
		return fmt.Errorf(&amp;#34;container %v not found&amp;#34;, containerName)
	}

	options.Location = filepath.Join(
		kl.getCheckpointsDir(),
		fmt.Sprintf(
			&amp;#34;checkpoint-%s-%s-%s.tar&amp;#34;,
			podFullName,
			containerName,
			time.Now().Format(time.RFC3339),
		),
	)
	//该函数的功能是尝试对一个容器进行检查点操作。
	//函数通过给定的参数查找指定的容器，如果找不到指定的容器，则返回一个错误。
	//如果找到了容器，则会请求容器引擎将该容器检查点保存到kubelet的默认检查点目录中。
	//具体来说，函数首先调用kl.findContainer函数来查找指定的容器，如果找不到容器或发生错误，则直接返回错误或nil。
	//如果找到了容器，则通过调用filepath.Join函数来设置检查点的存储位置。
	//存储位置由kubelet的默认检查点目录、pod全名、容器名和当前时间组成。最后，函数返回错误或nil。

	options.ContainerId = string(container.ID.ID)

	if err := kl.containerRuntime.CheckpointContainer(ctx, options); err != nil {
		return err
	}

	return nil
}

//该函数的功能是在容器上执行检查点操作。
//首先，它将容器的ID转换为字符串，并将其赋值给options.ContainerId。
//然后，它调用kl.containerRuntime.CheckpointContainer函数来执行实际的检查点操作，将上下文和options作为参数传递。
//如果该函数执行失败并返回错误，则该函数将错误返回。
//如果该函数成功执行，则该函数将返回nil。

// ListMetricDescriptors gets the descriptors for the metrics that will be returned in ListPodSandboxMetrics.
func (kl *Kubelet) ListMetricDescriptors(ctx context.Context) ([]*runtimeapi.MetricDescriptor, error) {
	return kl.containerRuntime.ListMetricDescriptors(ctx)
}

//该函数是Kubelet的一个方法，用于获取容器运行时的指标描述符。
//它通过调用containerRuntime的ListMetricDescriptors方法来实现。
//返回值是指标描述符的切片和可能的错误。

// ListPodSandboxMetrics retrieves the metrics for all pod sandboxes.
func (kl *Kubelet) ListPodSandboxMetrics(ctx context.Context) ([]*runtimeapi.PodSandboxMetrics, error) {
	return kl.containerRuntime.ListPodSandboxMetrics(ctx)
}

//该函数是Kubelet的一个方法，用于获取所有Pod沙箱的指标信息。
//它通过调用containerRuntime的ListPodSandboxMetrics方法来实现。
//返回值是一个包含多个Pod沙箱指标信息的切片和一个错误（如果有）。

func (kl *Kubelet) supportLocalStorageCapacityIsolation() bool {
	return kl.GetConfiguration().LocalStorageCapacityIsolation
}

//该函数用于判断Kubelet是否支持本地存储容量隔离。
//它通过调用Kubelet的GetConfiguration方法获取Kubelet的配置信息，
//然后返回LocalStorageCapacityIsolation字段的值来表示是否支持本地存储容量隔离。

// isSyncPodWorthy filters out events that are not worthy of pod syncing
func isSyncPodWorthy(event *pleg.PodLifecycleEvent) bool {
	// ContainerRemoved doesn&amp;#39;t affect pod state
	return event.Type != pleg.ContainerRemoved
}

//该函数用于过滤掉不值得进行Pod同步的事件。具体实现中，判断事件的类型是否为ContainerRemoved，
//若不是，则返回true，表示该事件值得进行Pod同步；
//若是，则返回false，表示该事件不值得进行Pod同步。

// PrepareDynamicResources calls the container Manager PrepareDynamicResources API
// This method implements the RuntimeHelper interface
func (kl *Kubelet) PrepareDynamicResources(pod *v1.Pod) error {
	return kl.containerManager.PrepareDynamicResources(pod)
}

//该函数是Kubelet的PrepareDynamicResources方法，实现了RuntimeHelper接口。
//它调用了container Manager的PrepareDynamicResources API，用于准备容器的动态资源。

// UnprepareDynamicResources calls the container Manager UnprepareDynamicResources API
// This method implements the RuntimeHelper interface
func (kl *Kubelet) UnprepareDynamicResources(pod *v1.Pod) error {
	return kl.containerManager.UnprepareDynamicResources(pod)
}

//该函数是Kubelet的一个方法，实现了RuntimeHelper接口。
//它调用了container Manager的UnprepareDynamicResources API，用于准备Pod的动态资源。
&lt;/code>&lt;/pre></description></item><item><title>2024-04-03 16张硬核图解k8s网络</title><link>https://qq547475331.github.io/docs/16%E5%BC%A0%E7%A1%AC%E6%A0%B8%E5%9B%BE%E8%A7%A3k8s%E7%BD%91%E7%BB%9C-16-zhang-ying-he-tu-jie-k8s-wang-luo/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/16%E5%BC%A0%E7%A1%AC%E6%A0%B8%E5%9B%BE%E8%A7%A3k8s%E7%BD%91%E7%BB%9C-16-zhang-ying-he-tu-jie-k8s-wang-luo/</guid><description>&lt;h2 id="overview">
 &lt;strong>Overview&lt;/strong>
 &lt;a class="anchor" href="#overview">#&lt;/a>
&lt;/h2>
&lt;p>本文将探讨 Kubernetes 中的网络模型，以及对各种网络模型进行分析。&lt;/p>
&lt;h2 id="underlay-network-model">
 &lt;strong>Underlay Network Model&lt;/strong>
 &lt;a class="anchor" href="#underlay-network-model">#&lt;/a>
&lt;/h2>
&lt;h3 id="什么是-underlay-network">
 &lt;strong>什么是 Underlay Network&lt;/strong>
 &lt;a class="anchor" href="#%e4%bb%80%e4%b9%88%e6%98%af-underlay-network">#&lt;/a>
&lt;/h3>
&lt;p>底层网络 &lt;code>Underlay Network&lt;/code> 顾名思义是指网络设备基础设施，如交换机，路由器, &lt;code>DWDM&lt;/code> 使用网络介质将其链接成的物理网络拓扑，负责网络之间的数据包传输。&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202402221122830.png" alt="image-20240222112212739" />Underlay network topology&lt;/p>
&lt;p>&lt;code>underlay network&lt;/code> 可以是二层，也可以是三层；二层的典型例子是以太网 &lt;code>Ethernet&lt;/code>，三层是的典型例子是互联网 &lt;code>Internet&lt;/code>。&lt;/p>
&lt;p>而工作于二层的技术是 &lt;code>vlan&lt;/code>，工作在三层的技术是由 &lt;code>OSPF&lt;/code>, &lt;code>BGP&lt;/code> 等协议组成。&lt;/p>
&lt;h3 id="k8s-中的-underlay-network">
 &lt;strong>k8s 中的 underlay network&lt;/strong>
 &lt;a class="anchor" href="#k8s-%e4%b8%ad%e7%9a%84-underlay-network">#&lt;/a>
&lt;/h3>
&lt;p>在 kubernetes 中，&lt;code>underlay network&lt;/code> 中比较典型的例子是通过将宿主机作为路由器设备，Pod 的网络则通过学习路由条目从而实现跨节点通讯。&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202402221122426.png" alt="image-20240222112231365" />underlay network topology in kubernetes&lt;/p>
&lt;p>这种模型下典型的有 &lt;code>flannel&lt;/code> 的 &lt;code>host-gw&lt;/code> 模式与 &lt;code>calico&lt;/code> &lt;code>BGP&lt;/code> 模式。&lt;/p>
&lt;h4 id="flannel-host-gw">
 flannel host-gw
 &lt;a class="anchor" href="#flannel-host-gw">#&lt;/a>
&lt;/h4>
&lt;p>&lt;code>flannel host-gw&lt;/code> 模式中每个 Node 需要在同一个二层网络中，并将 Node 作为一个路由器，跨节点通讯将通过路由表方式进行，这样方式下将网络模拟成一个&lt;code>underlay network&lt;/code>。&lt;/p></description></item><item><title>2024-04-03 600条最强linux命令总结</title><link>https://qq547475331.github.io/docs/600%E6%9D%A1%E6%9C%80%E5%BC%BAlinux%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93-600-tiao-zui-qiang-linux-ming-ling-zong-jie/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/600%E6%9D%A1%E6%9C%80%E5%BC%BAlinux%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93-600-tiao-zui-qiang-linux-ming-ling-zong-jie/</guid><description>&lt;h2 id="1-基本命令">
 1. 基本命令
 &lt;a class="anchor" href="#1-%e5%9f%ba%e6%9c%ac%e5%91%bd%e4%bb%a4">#&lt;/a>
&lt;/h2>
&lt;pre tabindex="0">&lt;code>uname -m 显示机器的处理器架构
uname -r 显示正在使用的内核版本
dmidecode -q 显示硬件系统部件
(SMBIOS / DMI) hdparm -i /dev/hda 罗列一个磁盘的架构特性
hdparm -tT /dev/sda 在磁盘上执行测试性读取操作系统信息
arch 显示机器的处理器架构
uname -m 显示机器的处理器架构
uname -r 显示正在使用的内核版本
dmidecode -q 显示硬件系统部件 - (SMBIOS / DMI)
hdparm -i /dev/hda 罗列一个磁盘的架构特性
hdparm -tT /dev/sda 在磁盘上执行测试性读取操作
cat /proc/cpuinfo 显示CPU info的信息
cat /proc/interrupts 显示中断
cat /proc/meminfo 校验内存使用
cat /proc/swaps 显示哪些swap被使用
cat /proc/version 显示内核的版本
cat /proc/net/dev 显示网络适配器及统计
cat /proc/mounts 显示已加载的文件系统
lspci -tv 罗列 PCI 设备
lsusb -tv 显示 USB 设备
date 显示系统日期
cal 2007 显示2007年的日历表
date 041217002007.00 设置日期和时间 - 月日时分年.秒
clock -w 将时间修改保存到 BIOS
&lt;/code>&lt;/pre>&lt;h2 id="2-关机">
 2. 关机
 &lt;a class="anchor" href="#2-%e5%85%b3%e6%9c%ba">#&lt;/a>
&lt;/h2>
&lt;pre tabindex="0">&lt;code>shutdown -h now 关闭系统(1)
init 0 关闭系统(2)
telinit 0 关闭系统(3)
shutdown -h hours:minutes &amp;amp; 按预定时间关闭系统
shutdown -c 取消按预定时间关闭系统
shutdown -r now 重启(1)
reboot 重启(2)
logout 注销
&lt;/code>&lt;/pre>&lt;h2 id="3-文件和目录">
 3. 文件和目录
 &lt;a class="anchor" href="#3-%e6%96%87%e4%bb%b6%e5%92%8c%e7%9b%ae%e5%bd%95">#&lt;/a>
&lt;/h2>
&lt;pre tabindex="0">&lt;code>cd /home 进入 &amp;#39;/ home&amp;#39; 目录&amp;#39;
cd .. 返回上一级目录
cd ../.. 返回上两级目录
cd 进入个人的主目录
cd ~user1 进入个人的主目录
cd - 返回上次所在的目录
pwd 显示工作路径
ls 查看目录中的文件
ls -F 查看目录中的文件
ls -l 显示文件和目录的详细资料
ls -a 显示隐藏文件
ls *[0-9]* 显示包含数字的文件名和目录名
tree 显示文件和目录由根目录开始的树形结构(1)
lstree 显示文件和目录由根目录开始的树形结构(2)
mkdir dir1 创建一个叫做 &amp;#39;dir1&amp;#39; 的目录&amp;#39;
mkdir dir1 dir2 同时创建两个目录
mkdir -p /tmp/dir1/dir2 创建一个目录树
rm -f file1 删除一个叫做 &amp;#39;file1&amp;#39; 的文件&amp;#39;
rmdir dir1 删除一个叫做 &amp;#39;dir1&amp;#39; 的目录&amp;#39;
rm -rf dir1 删除一个叫做 &amp;#39;dir1&amp;#39; 的目录并同时删除其内容
rm -rf dir1 dir2 同时删除两个目录及它们的内容
mv dir1 new_dir 重命名/移动 一个目录
cp file1 file2 复制一个文件
cp dir/* . 复制一个目录下的所有文件到当前工作目录
cp -a /tmp/dir1 . 复制一个目录到当前工作目录
cp -a dir1 dir2 复制一个目录
ln -s file1 lnk1 创建一个指向文件或目录的软链接
ln file1 lnk1 创建一个指向文件或目录的物理链接
touch -t 0712250000 file1 修改一个文件或目录的时间戳 - (YYMMDDhhmm)
file file1 outputs the mime type of the file as text
iconv -l 列出已知的编码
iconv -f fromEncoding -t toEncoding inputFile &amp;gt; outputFile creates a new from the given input file by assuming it is encoded in fromEncoding and converting it to toEncoding.
find . -maxdepth 1 -name *.jpg -print -exec convert &amp;#34;{}&amp;#34; -resize 80x60 &amp;#34;thumbs/{}&amp;#34; \; batch resize files in the current directory and send them to a thumbnails directory (requires convert from Imagemagick)
&lt;/code>&lt;/pre>&lt;h2 id="4-文件搜索">
 4. 文件搜索
 &lt;a class="anchor" href="#4-%e6%96%87%e4%bb%b6%e6%90%9c%e7%b4%a2">#&lt;/a>
&lt;/h2>
&lt;pre tabindex="0">&lt;code>find / -name file1 从 &amp;#39;/&amp;#39; 开始进入根文件系统搜索文件和目录
find / -user user1 搜索属于用户 &amp;#39;user1&amp;#39; 的文件和目录
find /home/user1 -name \*.bin 在目录 &amp;#39;/ home/user1&amp;#39; 中搜索带有&amp;#39;.bin&amp;#39; 结尾的文件
find /usr/bin -type f -atime +100 搜索在过去100天内未被使用过的执行文件
find /usr/bin -type f -mtime -10 搜索在10天内被创建或者修改过的文件
find / -name \*.rpm -exec chmod 755 &amp;#39;{}&amp;#39; \; 搜索以 &amp;#39;.rpm&amp;#39; 结尾的文件并定义其权限
find / -xdev -name \*.rpm 搜索以 &amp;#39;.rpm&amp;#39; 结尾的文件，忽略光驱、捷盘等可移动设备
locate \*.ps 寻找以 &amp;#39;.ps&amp;#39; 结尾的文件 - 先运行 &amp;#39;updatedb&amp;#39; 命令
whereis halt 显示一个二进制文件、源码或man的位置
which halt 显示一个二进制文件或可执行文件的完整路径
&lt;/code>&lt;/pre>&lt;h2 id="5-挂载一个文件系统">
 5. 挂载一个文件系统
 &lt;a class="anchor" href="#5-%e6%8c%82%e8%bd%bd%e4%b8%80%e4%b8%aa%e6%96%87%e4%bb%b6%e7%b3%bb%e7%bb%9f">#&lt;/a>
&lt;/h2>
&lt;pre tabindex="0">&lt;code>mount /dev/hda2 /mnt/hda2 挂载一个叫做hda2的盘 - 确定目录 &amp;#39;/ mnt/hda2&amp;#39; 已经存在
umount /dev/hda2 卸载一个叫做hda2的盘 - 先从挂载点 &amp;#39;/ mnt/hda2&amp;#39; 退出
fuser -km /mnt/hda2 当设备繁忙时强制卸载
umount -n /mnt/hda2 运行卸载操作而不写入 /etc/mtab 文件- 当文件为只读或当磁盘写满时非常有用
mount /dev/fd0 /mnt/floppy 挂载一个软盘
mount /dev/cdrom /mnt/cdrom 挂载一个cdrom或dvdrom
mount /dev/hdc /mnt/cdrecorder 挂载一个cdrw或dvdrom
mount /dev/hdb /mnt/cdrecorder 挂载一个cdrw或dvdrom
mount -o loop file.iso /mnt/cdrom 挂载一个文件或ISO镜像文件
mount -t vfat /dev/hda5 /mnt/hda5 挂载一个Windows FAT32文件系统
mount /dev/sda1 /mnt/usbdisk 挂载一个usb 捷盘或闪存设备
mount -t smbfs -o username=user,password=pass //WinClient/share /mnt/share 挂载一个windows网络共享
&lt;/code>&lt;/pre>&lt;h2 id="6-磁盘空间">
 6. 磁盘空间
 &lt;a class="anchor" href="#6-%e7%a3%81%e7%9b%98%e7%a9%ba%e9%97%b4">#&lt;/a>
&lt;/h2>
&lt;pre tabindex="0">&lt;code>df -h 显示已经挂载的分区列表
ls -lSr |more 以尺寸大小排列文件和目录
du -sh dir1 估算目录 &amp;#39;dir1&amp;#39; 已经使用的磁盘空间&amp;#39;
du -sk * | sort -rn 以容量大小为依据依次显示文件和目录的大小
rpm -q -a --qf &amp;#39;%10{SIZE}t%{NAME}n&amp;#39; | sort -k1,1n 以大小为依据依次显示已安装的rpm包所使用的空间 (fedora, redhat类系统)
dpkg-query -W -f=&amp;#39;${Installed-Size;10}t${Package}n&amp;#39; | sort -k1,1n 以大小为依据显示已安装的deb包所使用的空间 (ubuntu, debian类系统)
&lt;/code>&lt;/pre>&lt;h2 id="7-用户和群组">
 7. 用户和群组
 &lt;a class="anchor" href="#7-%e7%94%a8%e6%88%b7%e5%92%8c%e7%be%a4%e7%bb%84">#&lt;/a>
&lt;/h2>
&lt;pre tabindex="0">&lt;code>groupadd group_name 创建一个新用户组
groupdel group_name 删除一个用户组
groupmod -n new_group_name old_group_name 重命名一个用户组
useradd -c &amp;#34;Name Surname &amp;#34; -g admin -d /home/user1 -s /bin/bash user1 创建一个属于 &amp;#34;admin&amp;#34; 用户组的用户
useradd user1 创建一个新用户
userdel -r user1 删除一个用户 ( &amp;#39;-r&amp;#39; 排除主目录)
usermod -c &amp;#34;User FTP&amp;#34; -g system -d /ftp/user1 -s /bin/nologin user1 修改用户属性
passwd 修改口令
passwd user1 修改一个用户的口令 (只允许root执行)
chage -E 2005-12-31 user1 设置用户口令的失效期限
pwck 检查 &amp;#39;/etc/passwd&amp;#39; 的文件格式和语法修正以及存在的用户
grpck 检查 &amp;#39;/etc/passwd&amp;#39; 的文件格式和语法修正以及存在的群组
newgrp group_name 登陆进一个新的群组以改变新创建文件的预设群组
&lt;/code>&lt;/pre>&lt;h2 id="8-文件的权限-使用--设置权限使用---用于取消">
 8. 文件的权限 使用 “+” 设置权限，使用 “-” 用于取消
 &lt;a class="anchor" href="#8-%e6%96%87%e4%bb%b6%e7%9a%84%e6%9d%83%e9%99%90-%e4%bd%bf%e7%94%a8--%e8%ae%be%e7%bd%ae%e6%9d%83%e9%99%90%e4%bd%bf%e7%94%a8---%e7%94%a8%e4%ba%8e%e5%8f%96%e6%b6%88">#&lt;/a>
&lt;/h2>
&lt;pre tabindex="0">&lt;code>ls -lh 显示权限
ls /tmp | pr -T5 -W$COLUMNS 将终端划分成5栏显示
chmod ugo+rwx directory1 设置目录的所有人(u)、群组(g)以及其他人(o)以读（r ）、写(w)和执行(x)的权限
chmod go-rwx directory1 删除群组(g)与其他人(o)对目录的读写执行权限
chown user1 file1 改变一个文件的所有人属性
chown -R user1 directory1 改变一个目录的所有人属性并同时改变改目录下所有文件的属性
chgrp group1 file1 改变文件的群组
chown user1:group1 file1 改变一个文件的所有人和群组属性
find / -perm -u+s 罗列一个系统中所有使用了SUID控制的文件
chmod u+s /bin/file1 设置一个二进制文件的 SUID 位 - 运行该文件的用户也被赋予和所有者同样的权限
chmod u-s /bin/file1 禁用一个二进制文件的 SUID位
chmod g+s /home/public 设置一个目录的SGID 位 - 类似SUID ，不过这是针对目录的
chmod g-s /home/public 禁用一个目录的 SGID 位
chmod o+t /home/public 设置一个文件的 STIKY 位 - 只允许合法所有人删除文件
chmod o-t /home/public 禁用一个目录的 STIKY 位
chmod +x 文件路径 为所有者、所属组和其他用户添加执行的权限
chmod -x 文件路径 为所有者、所属组和其他用户删除执行的权限
chmod u+x 文件路径 为所有者添加执行的权限
chmod g+x 文件路径 为所属组添加执行的权限
chmod o+x 文件路径 为其他用户添加执行的权限
chmod ug+x 文件路径 为所有者、所属组添加执行的权限
chmod =wx 文件路径 为所有者、所属组和其他用户添加写、执行的权限，取消读权限
chmod ug=wx 文件路径 为所有者、所属组添加写、执行的权限，取消读权限
&lt;/code>&lt;/pre>&lt;h2 id="9-文件的特殊属性-使用--设置权限使用---用于取消">
 9. 文件的特殊属性 ，使用 “+” 设置权限，使用 “-” 用于取消
 &lt;a class="anchor" href="#9-%e6%96%87%e4%bb%b6%e7%9a%84%e7%89%b9%e6%ae%8a%e5%b1%9e%e6%80%a7-%e4%bd%bf%e7%94%a8--%e8%ae%be%e7%bd%ae%e6%9d%83%e9%99%90%e4%bd%bf%e7%94%a8---%e7%94%a8%e4%ba%8e%e5%8f%96%e6%b6%88">#&lt;/a>
&lt;/h2>
&lt;pre tabindex="0">&lt;code>chattr +a file1 只允许以追加方式读写文件
chattr +c file1 允许这个文件能被内核自动压缩/解压
chattr +d file1 在进行文件系统备份时，dump程序将忽略这个文件
chattr +i file1 设置成不可变的文件，不能被删除、修改、重命名或者链接
chattr +s file1 允许一个文件被安全地删除
chattr +S file1 一旦应用程序对这个文件执行了写操作，使系统立刻把修改的结果写到磁盘
chattr +u file1 若文件被删除，系统会允许你在以后恢复这个被删除的文件
lsattr 显示特殊的属性
&lt;/code>&lt;/pre>&lt;h2 id="10-打包和压缩文件">
 10. 打包和压缩文件
 &lt;a class="anchor" href="#10-%e6%89%93%e5%8c%85%e5%92%8c%e5%8e%8b%e7%bc%a9%e6%96%87%e4%bb%b6">#&lt;/a>
&lt;/h2>
&lt;pre tabindex="0">&lt;code>bunzip2 file1.bz2 解压一个叫做 &amp;#39;file1.bz2&amp;#39;的文件
bzip2 file1 压缩一个叫做 &amp;#39;file1&amp;#39; 的文件
gunzip file1.gz 解压一个叫做 &amp;#39;file1.gz&amp;#39;的文件
gzip file1 压缩一个叫做 &amp;#39;file1&amp;#39;的文件
gzip -9 file1 最大程度压缩
rar a file1.rar test_file 创建一个叫做 &amp;#39;file1.rar&amp;#39; 的包
rar a file1.rar file1 file2 dir1 同时压缩 &amp;#39;file1&amp;#39;, &amp;#39;file2&amp;#39; 以及目录 &amp;#39;dir1&amp;#39;
rar x file1.rar 解压rar包
unrar x file1.rar 解压rar包
tar -cvf archive.tar file1 创建一个非压缩的 tarball
tar -cvf archive.tar file1 file2 dir1 创建一个包含了 &amp;#39;file1&amp;#39;, &amp;#39;file2&amp;#39; 以及 &amp;#39;dir1&amp;#39;的档案文件
tar -tf archive.tar 显示一个包中的内容
tar -xvf archive.tar 释放一个包
tar -xvf archive.tar -C /tmp 将压缩包释放到 /tmp目录下
tar -cvfj archive.tar.bz2 dir1 创建一个bzip2格式的压缩包
tar -xvfj archive.tar.bz2 解压一个bzip2格式的压缩包
tar -cvfz archive.tar.gz dir1 创建一个gzip格式的压缩包
tar -xvfz archive.tar.gz 解压一个gzip格式的压缩包
zip file1.zip file1 创建一个zip格式的压缩包
zip -r file1.zip file1 file2 dir1 将几个文件和目录同时压缩成一个zip格式的压缩包
unzip file1.zip 解压一个zip格式压缩包
&lt;/code>&lt;/pre>&lt;h2 id="11-rpm-包">
 11. RPM 包
 &lt;a class="anchor" href="#11-rpm-%e5%8c%85">#&lt;/a>
&lt;/h2>
&lt;pre tabindex="0">&lt;code>rpm -ivh package.rpm 安装一个rpm包
rpm -ivh --nodeeps package.rpm 安装一个rpm包而忽略依赖关系警告
rpm -U package.rpm 更新一个rpm包但不改变其配置文件
rpm -F package.rpm 更新一个确定已经安装的rpm包
rpm -e package_name.rpm 删除一个rpm包
rpm -qa 显示系统中所有已经安装的rpm包
rpm -qa | grep httpd 显示所有名称中包含 &amp;#34;httpd&amp;#34; 字样的rpm包
rpm -qi package_name 获取一个已安装包的特殊信息
rpm -qg &amp;#34;System Environment/Daemons&amp;#34; 显示一个组件的rpm包
rpm -ql package_name 显示一个已经安装的rpm包提供的文件列表
rpm -qc package_name 显示一个已经安装的rpm包提供的配置文件列表
rpm -q package_name --whatrequires 显示与一个rpm包存在依赖关系的列表
rpm -q package_name --whatprovides 显示一个rpm包所占的体积
rpm -q package_name --scripts 显示在安装/删除期间所执行的脚本l
rpm -q package_name --changelog 显示一个rpm包的修改历史
rpm -qf /etc/httpd/conf/httpd.conf 确认所给的文件由哪个rpm包所提供
rpm -qp package.rpm -l 显示由一个尚未安装的rpm包提供的文件列表
rpm --import /media/cdrom/RPM-GPG-KEY 导入公钥数字证书
rpm --checksig package.rpm 确认一个rpm包的完整性
rpm -qa gpg-pubkey 确认已安装的所有rpm包的完整性
rpm -V package_name 检查文件尺寸、 许可、类型、所有者、群组、MD5检查以及最后修改时间
rpm -Va 检查系统中所有已安装的rpm包- 小心使用
rpm -Vp package.rpm 确认一个rpm包还未安装
rpm2cpio package.rpm | cpio --extract --make-directories *bin* 从一个rpm包运行可执行文件
rpm -ivh /usr/src/redhat/RPMS/`arch`/package.rpm 从一个rpm源码安装一个构建好的包
rpmbuild --rebuild package_name.src.rpm 从一个rpm源码构建一个 rpm 包
&lt;/code>&lt;/pre>&lt;h2 id="12-yum-软件包升级器">
 12. YUM 软件包升级器
 &lt;a class="anchor" href="#12-yum-%e8%bd%af%e4%bb%b6%e5%8c%85%e5%8d%87%e7%ba%a7%e5%99%a8">#&lt;/a>
&lt;/h2>
&lt;pre tabindex="0">&lt;code>yum install package_name 下载并安装一个rpm包
yum localinstall package_name.rpm 将安装一个rpm包，使用你自己的软件仓库为你解决所有依赖关系
yum update package_name.rpm 更新当前系统中所有安装的rpm包
yum update package_name 更新一个rpm包
yum remove package_name 删除一个rpm包
yum list 列出当前系统中安装的所有包
yum search package_name 在rpm仓库中搜寻软件包
yum clean packages 清理rpm缓存删除下载的包
yum clean headers 删除所有头文件
yum clean all 删除所有缓存的包和头文件
&lt;/code>&lt;/pre>&lt;h2 id="13-deb-包">
 13. deb 包
 &lt;a class="anchor" href="#13-deb-%e5%8c%85">#&lt;/a>
&lt;/h2>
&lt;pre tabindex="0">&lt;code>dpkg -i package.deb 安装/更新一个 deb 包
dpkg -r package_name 从系统删除一个 deb 包
dpkg -l 显示系统中所有已经安装的 deb 包
dpkg -l | grep httpd 显示所有名称中包含 &amp;#34;httpd&amp;#34; 字样的deb包
dpkg -s package_name 获得已经安装在系统中一个特殊包的信息
dpkg -L package_name 显示系统中已经安装的一个deb包所提供的文件列表
dpkg --contents package.deb 显示尚未安装的一个包所提供的文件列表
dpkg -S /bin/ping 确认所给的文件由哪个deb包提供
APT 软件工具 (Debian, Ubuntu 以及类似系统)
apt-get install package_name 安装/更新一个 deb 包
apt-cdrom install package_name 从光盘安装/更新一个 deb 包
apt-get update 升级列表中的软件包
apt-get upgrade 升级所有已安装的软件
apt-get remove package_name 从系统删除一个deb包
apt-get check 确认依赖的软件仓库正确
apt-get clean 从下载的软件包中清理缓存
apt-cache search searched-package 返回包含所要搜索字符串的软件包名称
&lt;/code>&lt;/pre>&lt;h2 id="14-查看文件内容">
 14. 查看文件内容
 &lt;a class="anchor" href="#14-%e6%9f%a5%e7%9c%8b%e6%96%87%e4%bb%b6%e5%86%85%e5%ae%b9">#&lt;/a>
&lt;/h2>
&lt;pre tabindex="0">&lt;code>cat file1 从第一个字节开始正向查看文件的内容
tac file1 从最后一行开始反向查看一个文件的内容
more file1 查看一个长文件的内容
less file1 类似于 &amp;#39;more&amp;#39; 命令，但是它允许在文件中和正向操作一样的反向操作
head -2 file1 查看一个文件的前两行
tail -2 file1 查看一个文件的最后两行
tail -f /var/log/messages 实时查看被添加到一个文件中的内容
&lt;/code>&lt;/pre>&lt;h2 id="15-文本处理">
 15. 文本处理
 &lt;a class="anchor" href="#15-%e6%96%87%e6%9c%ac%e5%a4%84%e7%90%86">#&lt;/a>
&lt;/h2>
&lt;pre tabindex="0">&lt;code>cat file1 file2 ... | command &amp;lt;&amp;gt; file1_in.txt_or_file1_out.txt general syntax for text manipulation using PIPE, STDIN and STDOUT
cat file1 | command( sed, grep, awk, grep, etc...) &amp;gt; result.txt 合并一个文件的详细说明文本，并将简介写入一个新文件中
cat file1 | command( sed, grep, awk, grep, etc...) &amp;gt;&amp;gt; result.txt 合并一个文件的详细说明文本，并将简介写入一个已有的文件中
grep Aug /var/log/messages 在文件 &amp;#39;/var/log/messages&amp;#39;中查找关键词&amp;#34;Aug&amp;#34;
grep ^Aug /var/log/messages 在文件 &amp;#39;/var/log/messages&amp;#39;中查找以&amp;#34;Aug&amp;#34;开始的词汇
grep [0-9] /var/log/messages 选择 &amp;#39;/var/log/messages&amp;#39; 文件中所有包含数字的行
grep Aug -R /var/log/* 在目录 &amp;#39;/var/log&amp;#39; 及随后的目录中搜索字符串&amp;#34;Aug&amp;#34;
sed &amp;#39;s/stringa1/stringa2/g&amp;#39; example.txt 将example.txt文件中的 &amp;#34;string1&amp;#34; 替换成 &amp;#34;string2&amp;#34;
sed &amp;#39;/^$/d&amp;#39; example.txt 从example.txt文件中删除所有空白行
sed &amp;#39;/ *#/d; /^$/d&amp;#39; example.txt 从example.txt文件中删除所有注释和空白行
echo &amp;#39;esempio&amp;#39; | tr &amp;#39;[:lower:]&amp;#39; &amp;#39;[:upper:]&amp;#39; 合并上下单元格内容
sed -e &amp;#39;1d&amp;#39; result.txt 从文件example.txt 中排除第一行
sed -n &amp;#39;/stringa1/p&amp;#39; 查看只包含词汇 &amp;#34;string1&amp;#34;的行
sed -e &amp;#39;s/ *$//&amp;#39; example.txt 删除每一行最后的空白字符
sed -e &amp;#39;s/stringa1//g&amp;#39; example.txt 从文档中只删除词汇 &amp;#34;string1&amp;#34; 并保留剩余全部
sed -n &amp;#39;1,5p;5q&amp;#39; example.txt 查看从第一行到第5行内容
sed -n &amp;#39;5p;5q&amp;#39; example.txt 查看第5行
sed -e &amp;#39;s/00*/0/g&amp;#39; example.txt 用单个零替换多个零
cat -n file1 标示文件的行数
cat example.txt | awk &amp;#39;NR%2==1&amp;#39; 删除example.txt文件中的所有偶数行
echo a b c | awk &amp;#39;{print $1}&amp;#39; 查看一行第一栏
echo a b c | awk &amp;#39;{print $1,$3}&amp;#39; 查看一行的第一和第三栏
paste file1 file2 合并两个文件或两栏的内容
paste -d &amp;#39;+&amp;#39; file1 file2 合并两个文件或两栏的内容，中间用&amp;#34;+&amp;#34;区分
sort file1 file2 排序两个文件的内容
sort file1 file2 | uniq 取出两个文件的并集(重复的行只保留一份)
sort file1 file2 | uniq -u 删除交集，留下其他的行
sort file1 file2 | uniq -d 取出两个文件的交集(只留下同时存在于两个文件中的文件)
comm -1 file1 file2 比较两个文件的内容只删除 &amp;#39;file1&amp;#39; 所包含的内容
comm -2 file1 file2 比较两个文件的内容只删除 &amp;#39;file2&amp;#39; 所包含的内容
comm -3 file1 file2 比较两个文件的内容只删除两个文件共有的部分
&lt;/code>&lt;/pre>&lt;h2 id="16-字符设置和文件格式转换">
 16. 字符设置和文件格式转换
 &lt;a class="anchor" href="#16-%e5%ad%97%e7%ac%a6%e8%ae%be%e7%bd%ae%e5%92%8c%e6%96%87%e4%bb%b6%e6%a0%bc%e5%bc%8f%e8%bd%ac%e6%8d%a2">#&lt;/a>
&lt;/h2>
&lt;pre tabindex="0">&lt;code>dos2unix filedos.txt fileunix.txt 将一个文本文件的格式从MSDOS转换成UNIX
unix2dos fileunix.txt filedos.txt 将一个文本文件的格式从UNIX转换成MSDOS
recode ..HTML &amp;lt; page.txt &amp;gt; page.html 将一个文本文件转换成html
recode -l | more 显示所有允许的转换格式
&lt;/code>&lt;/pre>&lt;h2 id="17-文件系统分析">
 17. 文件系统分析
 &lt;a class="anchor" href="#17-%e6%96%87%e4%bb%b6%e7%b3%bb%e7%bb%9f%e5%88%86%e6%9e%90">#&lt;/a>
&lt;/h2>
&lt;pre tabindex="0">&lt;code>badblocks -v /dev/hda1 检查磁盘hda1上的坏磁块
fsck /dev/hda1 修复/检查hda1磁盘上linux文件系统的完整性
fsck.ext2 /dev/hda1 修复/检查hda1磁盘上ext2文件系统的完整性
e2fsck /dev/hda1 修复/检查hda1磁盘上ext2文件系统的完整性
e2fsck -j /dev/hda1 修复/检查hda1磁盘上ext3文件系统的完整性
fsck.ext3 /dev/hda1 修复/检查hda1磁盘上ext3文件系统的完整性
fsck.vfat /dev/hda1 修复/检查hda1磁盘上fat文件系统的完整性
fsck.msdos /dev/hda1 修复/检查hda1磁盘上dos文件系统的完整性
dosfsck /dev/hda1 修复/检查hda1磁盘上dos文件系统的完整性
&lt;/code>&lt;/pre>&lt;h2 id="18-初始化一个文件系统">
 18. 初始化一个文件系统
 &lt;a class="anchor" href="#18-%e5%88%9d%e5%a7%8b%e5%8c%96%e4%b8%80%e4%b8%aa%e6%96%87%e4%bb%b6%e7%b3%bb%e7%bb%9f">#&lt;/a>
&lt;/h2>
&lt;pre tabindex="0">&lt;code>mkfs /dev/hda1 在hda1分区创建一个文件系统
mke2fs /dev/hda1 在hda1分区创建一个linux ext2的文件系统
mke2fs -j /dev/hda1 在hda1分区创建一个linux ext3(日志型)的文件系统
mkfs -t vfat 32 -F /dev/hda1 创建一个 FAT32 文件系统
fdformat -n /dev/fd0 格式化一个软盘
mkswap /dev/hda3 创建一个swap文件系统
&lt;/code>&lt;/pre>&lt;h2 id="19-swap-文件系统">
 19. SWAP 文件系统
 &lt;a class="anchor" href="#19-swap-%e6%96%87%e4%bb%b6%e7%b3%bb%e7%bb%9f">#&lt;/a>
&lt;/h2>
&lt;pre tabindex="0">&lt;code>mkswap /dev/hda3 创建一个swap文件系统
swapon /dev/hda3 启用一个新的swap文件系统
swapon /dev/hda2 /dev/hdb3 启用两个swap分区
&lt;/code>&lt;/pre>&lt;h2 id="20-备份">
 20. 备份
 &lt;a class="anchor" href="#20-%e5%a4%87%e4%bb%bd">#&lt;/a>
&lt;/h2>
&lt;pre tabindex="0">&lt;code>dump -0aj -f /tmp/home0.bak /home 制作一个 &amp;#39;/home&amp;#39; 目录的完整备份
dump -1aj -f /tmp/home0.bak /home 制作一个 &amp;#39;/home&amp;#39; 目录的交互式备份
restore -if /tmp/home0.bak 还原一个交互式备份
rsync -rogpav --delete /home /tmp 同步两边的目录
rsync -rogpav -e ssh --delete /home ip_address:/tmp 通过SSH通道rsync
rsync -az -e ssh --delete ip_addr:/home/public /home/local 通过ssh和压缩将一个远程目录同步到本地目录
rsync -az -e ssh --delete /home/local ip_addr:/home/public 通过ssh和压缩将本地目录同步到远程目录
dd bs=1M if=/dev/hda | gzip | ssh user@ip_addr &amp;#39;dd of=hda.gz&amp;#39; 通过ssh在远程主机上执行一次备份本地磁盘的操作
dd if=/dev/sda of=/tmp/file1 备份磁盘内容到一个文件
tar -Puf backup.tar /home/user 执行一次对 &amp;#39;/home/user&amp;#39; 目录的交互式备份操作
( cd /tmp/local/ &amp;amp;&amp;amp; tar c . ) | ssh -C user@ip_addr &amp;#39;cd /home/share/ &amp;amp;&amp;amp; tar x -p&amp;#39; 通过ssh在远程目录中复制一个目录内容
( tar c /home ) | ssh -C user@ip_addr &amp;#39;cd /home/backup-home &amp;amp;&amp;amp; tar x -p&amp;#39; 通过ssh在远程目录中复制一个本地目录
tar cf - . | (cd /tmp/backup ; tar xf - ) 本地将一个目录复制到另一个地方，保留原有权限及链接
find /home/user1 -name &amp;#39;*.txt&amp;#39; | xargs cp -av --target-directory=/home/backup/ --parents 从一个目录查找并复制所有以 &amp;#39;.txt&amp;#39; 结尾的文件到另一个目录
find /var/log -name &amp;#39;*.log&amp;#39; | tar cv --files-from=- | bzip2 &amp;gt; log.tar.bz2 查找所有以 &amp;#39;.log&amp;#39; 结尾的文件并做成一个bzip包
dd if=/dev/hda of=/dev/fd0 bs=512 count=1 做一个将 MBR (Master Boot Record)内容复制到软盘的动作
dd if=/dev/fd0 of=/dev/hda bs=512 count=1 从已经保存到软盘的备份中恢复MBR内容
&lt;/code>&lt;/pre>&lt;h2 id="21-光盘">
 21. 光盘
 &lt;a class="anchor" href="#21-%e5%85%89%e7%9b%98">#&lt;/a>
&lt;/h2>
&lt;pre tabindex="0">&lt;code>cdrecord -v gracetime=2 dev=/dev/cdrom -eject blank=fast -force 清空一个可复写的光盘内容
mkisofs /dev/cdrom &amp;gt; cd.iso 在磁盘上创建一个光盘的iso镜像文件
mkisofs /dev/cdrom | gzip &amp;gt; cd_iso.gz 在磁盘上创建一个压缩了的光盘iso镜像文件
mkisofs -J -allow-leading-dots -R -V &amp;#34;Label CD&amp;#34; -iso-level 4 -o ./cd.iso data_cd 创建一个目录的iso镜像文件
cdrecord -v dev=/dev/cdrom cd.iso 刻录一个ISO镜像文件
gzip -dc cd_iso.gz | cdrecord dev=/dev/cdrom - 刻录一个压缩了的ISO镜像文件
mount -o loop cd.iso /mnt/iso 挂载一个ISO镜像文件
cd-paranoia -B 从一个CD光盘转录音轨到 wav 文件中
cd-paranoia -- &amp;#34;-3&amp;#34; 从一个CD光盘转录音轨到 wav 文件中（参数-3）
cdrecord --scanbus 扫描总线以识别scsi通道
dd if=/dev/hdc | md5sum 校验一个设备的md5sum编码，例如一张 CD
&lt;/code>&lt;/pre>&lt;h2 id="22-网络以太网和-wifi-无线">
 22. 网络（以太网和 WIFI 无线）
 &lt;a class="anchor" href="#22-%e7%bd%91%e7%bb%9c%e4%bb%a5%e5%a4%aa%e7%bd%91%e5%92%8c-wifi-%e6%97%a0%e7%ba%bf">#&lt;/a>
&lt;/h2>
&lt;pre tabindex="0">&lt;code>ifconfig eth0 显示一个以太网卡的配置
ifup eth0 启用一个 &amp;#39;eth0&amp;#39; 网络设备
ifdown eth0 禁用一个 &amp;#39;eth0&amp;#39; 网络设备
ifconfig eth0 192.168.1.1 netmask 255.255.255.0 控制IP地址
ifconfig eth0 promisc 设置 &amp;#39;eth0&amp;#39; 成混杂模式以嗅探数据包 (sniffing)
dhclient eth0 以dhcp模式启用 &amp;#39;eth0&amp;#39;
route -n show routing table
route add -net 0/0 gw IP_Gateway configura default gateway
route add -net 192.168.0.0 netmask 255.255.0.0 gw 192.168.1.1 configure static route to reach network &amp;#39;192.168.0.0/16&amp;#39;
route del 0/0 gw IP_gateway remove static route
echo &amp;#34;1&amp;#34; &amp;gt; /proc/sys/net/ipv4/ip_forward activate ip routing
hostname show hostname of system
host www.example.com lookup hostname to resolve name to ip address and viceversa(1)
nslookup www.example.com lookup hostname to resolve name to ip address and viceversa(2)
ip link show show link status of all interfaces
mii-tool eth0 show link status of &amp;#39;eth0&amp;#39;
ethtool eth0 show statistics of network card &amp;#39;eth0&amp;#39;
netstat -tup show all active network connections and their PID
netstat -tupl show all network services listening on the system and their PID
tcpdump tcp port 80 show all HTTP traffic
iwlist scan show wireless networks
iwconfig eth1 show configuration of a wireless network card
hostname show hostname
host www.example.com lookup hostname to resolve name to ip address and viceversa
nslookup www.example.com lookup hostname to resolve name to ip address and viceversa
whois www.example.com lookup on Whois database
&lt;/code>&lt;/pre>&lt;h2 id="23-列出目录内容">
 23. 列出目录内容
 &lt;a class="anchor" href="#23-%e5%88%97%e5%87%ba%e7%9b%ae%e5%bd%95%e5%86%85%e5%ae%b9">#&lt;/a>
&lt;/h2>
&lt;pre tabindex="0">&lt;code>ls -a：显示所有文件（包括隐藏文件）；
ls -l：显示详细信息；
ls -R：递归显示子目录结构；
ls -ld：显示目录和链接信息；
ctrl+r：历史记录中所搜命令（输入命令中的任意一个字符）；
Linux中以.开头的文件是隐藏文件；
pwd:显示当前目录
&lt;/code>&lt;/pre>&lt;h2 id="24-查看文件的类型">
 24. 查看文件的类型
 &lt;a class="anchor" href="#24-%e6%9f%a5%e7%9c%8b%e6%96%87%e4%bb%b6%e7%9a%84%e7%b1%bb%e5%9e%8b">#&lt;/a>
&lt;/h2>
&lt;pre tabindex="0">&lt;code>file:查看文件的类型
&lt;/code>&lt;/pre>&lt;h2 id="25-复制文件目录">
 25. 复制文件目录
 &lt;a class="anchor" href="#25-%e5%a4%8d%e5%88%b6%e6%96%87%e4%bb%b6%e7%9b%ae%e5%bd%95">#&lt;/a>
&lt;/h2>
&lt;p>1、&lt;strong>cp&lt;/strong>：复制文件和目录 cp 源文件（文件夹）目标文件（文件夹）&lt;/p></description></item><item><title>2024-04-03 面试0308</title><link>https://qq547475331.github.io/docs/%E9%9D%A2%E8%AF%950308-mian-shi-0308/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/%E9%9D%A2%E8%AF%950308-mian-shi-0308/</guid><description>&lt;h3 id="根据nginx日志--过滤nginx前十访问量的ip">
 根据nginx日志 过滤nginx前十访问量的ip
 &lt;a class="anchor" href="#%e6%a0%b9%e6%8d%aenginx%e6%97%a5%e5%bf%97--%e8%bf%87%e6%bb%a4nginx%e5%89%8d%e5%8d%81%e8%ae%bf%e9%97%ae%e9%87%8f%e7%9a%84ip">#&lt;/a>
&lt;/h3>
&lt;h5 id="要根据nginx日志过滤出前十访问量的ip你可以使用awksort和head命令组合来完成这个任务假设你的nginx访问日志格式如下这是nginx的默认格式">
 要根据Nginx日志过滤出前十访问量的IP，你可以使用&lt;code>awk&lt;/code>、&lt;code>sort&lt;/code>和&lt;code>head&lt;/code>命令组合来完成这个任务。假设你的Nginx访问日志格式如下（这是Nginx的默认格式）：
 &lt;a class="anchor" href="#%e8%a6%81%e6%a0%b9%e6%8d%aenginx%e6%97%a5%e5%bf%97%e8%bf%87%e6%bb%a4%e5%87%ba%e5%89%8d%e5%8d%81%e8%ae%bf%e9%97%ae%e9%87%8f%e7%9a%84ip%e4%bd%a0%e5%8f%af%e4%bb%a5%e4%bd%bf%e7%94%a8awksort%e5%92%8chead%e5%91%bd%e4%bb%a4%e7%bb%84%e5%90%88%e6%9d%a5%e5%ae%8c%e6%88%90%e8%bf%99%e4%b8%aa%e4%bb%bb%e5%8a%a1%e5%81%87%e8%ae%be%e4%bd%a0%e7%9a%84nginx%e8%ae%bf%e9%97%ae%e6%97%a5%e5%bf%97%e6%a0%bc%e5%bc%8f%e5%a6%82%e4%b8%8b%e8%bf%99%e6%98%afnginx%e7%9a%84%e9%bb%98%e8%ae%a4%e6%a0%bc%e5%bc%8f">#&lt;/a>
&lt;/h5>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>bash复制代码
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>127.0.0.1 - - &lt;span style="color:#f92672">[&lt;/span>10/Oct/2023:14:05:01 +0000&lt;span style="color:#f92672">]&lt;/span> &lt;span style="color:#e6db74">&amp;#34;GET / HTTP/1.1&amp;#34;&lt;/span> &lt;span style="color:#ae81ff">200&lt;/span> &lt;span style="color:#ae81ff">612&lt;/span> &lt;span style="color:#e6db74">&amp;#34;-&amp;#34;&lt;/span> &lt;span style="color:#e6db74">&amp;#34;curl/7.68.0&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h5 id="在这个日志中第一个字段是客户端的ip地址">
 在这个日志中，第一个字段是客户端的IP地址。
 &lt;a class="anchor" href="#%e5%9c%a8%e8%bf%99%e4%b8%aa%e6%97%a5%e5%bf%97%e4%b8%ad%e7%ac%ac%e4%b8%80%e4%b8%aa%e5%ad%97%e6%ae%b5%e6%98%af%e5%ae%a2%e6%88%b7%e7%ab%af%e7%9a%84ip%e5%9c%b0%e5%9d%80">#&lt;/a>
&lt;/h5>
&lt;h5 id="你可以使用以下命令来过滤出前十访问量的ip">
 你可以使用以下命令来过滤出前十访问量的IP：
 &lt;a class="anchor" href="#%e4%bd%a0%e5%8f%af%e4%bb%a5%e4%bd%bf%e7%94%a8%e4%bb%a5%e4%b8%8b%e5%91%bd%e4%bb%a4%e6%9d%a5%e8%bf%87%e6%bb%a4%e5%87%ba%e5%89%8d%e5%8d%81%e8%ae%bf%e9%97%ae%e9%87%8f%e7%9a%84ip">#&lt;/a>
&lt;/h5>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>bash复制代码
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>awk &lt;span style="color:#e6db74">&amp;#39;{print $1}&amp;#39;&lt;/span> access.log | sort | uniq -c | sort -nr | head -n &lt;span style="color:#ae81ff">10&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cat /var/log/nginx/access.log |awk &lt;span style="color:#e6db74">&amp;#39;{print $1}&amp;#39;&lt;/span>|sort|uniq -c |sort -nr|head -n &lt;span style="color:#ae81ff">10&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h5 id="这个命令的解释如下">
 这个命令的解释如下：
 &lt;a class="anchor" href="#%e8%bf%99%e4%b8%aa%e5%91%bd%e4%bb%a4%e7%9a%84%e8%a7%a3%e9%87%8a%e5%a6%82%e4%b8%8b">#&lt;/a>
&lt;/h5>
&lt;ol>
&lt;li>
&lt;h5 id="awk-print-1-accesslog使用awk命令从accesslog文件中提取每行的第一个字段即ip地址">
 &lt;code>awk '{print $1}' access.log&lt;/code>：使用&lt;code>awk&lt;/code>命令从&lt;code>access.log&lt;/code>文件中提取每行的第一个字段（即IP地址）。
 &lt;a class="anchor" href="#awk-print-1-accesslog%e4%bd%bf%e7%94%a8awk%e5%91%bd%e4%bb%a4%e4%bb%8eaccesslog%e6%96%87%e4%bb%b6%e4%b8%ad%e6%8f%90%e5%8f%96%e6%af%8f%e8%a1%8c%e7%9a%84%e7%ac%ac%e4%b8%80%e4%b8%aa%e5%ad%97%e6%ae%b5%e5%8d%b3ip%e5%9c%b0%e5%9d%80">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="sort对提取出的ip地址进行排序">
 &lt;code>sort&lt;/code>：对提取出的IP地址进行排序。
 &lt;a class="anchor" href="#sort%e5%af%b9%e6%8f%90%e5%8f%96%e5%87%ba%e7%9a%84ip%e5%9c%b0%e5%9d%80%e8%bf%9b%e8%a1%8c%e6%8e%92%e5%ba%8f">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="uniq--c统计每个唯一ip地址的出现次数">
 &lt;code>uniq -c&lt;/code>：统计每个唯一IP地址的出现次数。
 &lt;a class="anchor" href="#uniq--c%e7%bb%9f%e8%ae%a1%e6%af%8f%e4%b8%aa%e5%94%af%e4%b8%80ip%e5%9c%b0%e5%9d%80%e7%9a%84%e5%87%ba%e7%8e%b0%e6%ac%a1%e6%95%b0">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="sort--nr按数字进行逆序排序这样访问量最多的ip会排在最前面">
 &lt;code>sort -nr&lt;/code>：按数字进行逆序排序，这样访问量最多的IP会排在最前面。
 &lt;a class="anchor" href="#sort--nr%e6%8c%89%e6%95%b0%e5%ad%97%e8%bf%9b%e8%a1%8c%e9%80%86%e5%ba%8f%e6%8e%92%e5%ba%8f%e8%bf%99%e6%a0%b7%e8%ae%bf%e9%97%ae%e9%87%8f%e6%9c%80%e5%a4%9a%e7%9a%84ip%e4%bc%9a%e6%8e%92%e5%9c%a8%e6%9c%80%e5%89%8d%e9%9d%a2">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="head--n-10只显示前10行即访问量最多的前10个ip">
 &lt;code>head -n 10&lt;/code>：只显示前10行，即访问量最多的前10个IP。
 &lt;a class="anchor" href="#head--n-10%e5%8f%aa%e6%98%be%e7%a4%ba%e5%89%8d10%e8%a1%8c%e5%8d%b3%e8%ae%bf%e9%97%ae%e9%87%8f%e6%9c%80%e5%a4%9a%e7%9a%84%e5%89%8d10%e4%b8%aaip">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ol>
&lt;h5 id="请注意这个命令假设你的nginx日志文件名是accesslog如果你的日志文件名不同请相应地替换文件名此外这个命令也假设你的日志格式与上面所示的默认格式相匹配如果你的日志格式不同你可能需要调整awk命令中的字段选择器">
 请注意，这个命令假设你的Nginx日志文件名是&lt;code>access.log&lt;/code>。如果你的日志文件名不同，请相应地替换文件名。此外，这个命令也假设你的日志格式与上面所示的默认格式相匹配。如果你的日志格式不同，你可能需要调整&lt;code>awk&lt;/code>命令中的字段选择器。
 &lt;a class="anchor" href="#%e8%af%b7%e6%b3%a8%e6%84%8f%e8%bf%99%e4%b8%aa%e5%91%bd%e4%bb%a4%e5%81%87%e8%ae%be%e4%bd%a0%e7%9a%84nginx%e6%97%a5%e5%bf%97%e6%96%87%e4%bb%b6%e5%90%8d%e6%98%afaccesslog%e5%a6%82%e6%9e%9c%e4%bd%a0%e7%9a%84%e6%97%a5%e5%bf%97%e6%96%87%e4%bb%b6%e5%90%8d%e4%b8%8d%e5%90%8c%e8%af%b7%e7%9b%b8%e5%ba%94%e5%9c%b0%e6%9b%bf%e6%8d%a2%e6%96%87%e4%bb%b6%e5%90%8d%e6%ad%a4%e5%a4%96%e8%bf%99%e4%b8%aa%e5%91%bd%e4%bb%a4%e4%b9%9f%e5%81%87%e8%ae%be%e4%bd%a0%e7%9a%84%e6%97%a5%e5%bf%97%e6%a0%bc%e5%bc%8f%e4%b8%8e%e4%b8%8a%e9%9d%a2%e6%89%80%e7%a4%ba%e7%9a%84%e9%bb%98%e8%ae%a4%e6%a0%bc%e5%bc%8f%e7%9b%b8%e5%8c%b9%e9%85%8d%e5%a6%82%e6%9e%9c%e4%bd%a0%e7%9a%84%e6%97%a5%e5%bf%97%e6%a0%bc%e5%bc%8f%e4%b8%8d%e5%90%8c%e4%bd%a0%e5%8f%af%e8%83%bd%e9%9c%80%e8%a6%81%e8%b0%83%e6%95%b4awk%e5%91%bd%e4%bb%a4%e4%b8%ad%e7%9a%84%e5%ad%97%e6%ae%b5%e9%80%89%e6%8b%a9%e5%99%a8">#&lt;/a>
&lt;/h5>
&lt;h3 id="awk的-nf啥意思">
 awk的 $nf啥意思
 &lt;a class="anchor" href="#awk%e7%9a%84-nf%e5%95%a5%e6%84%8f%e6%80%9d">#&lt;/a>
&lt;/h3>
&lt;h5 id="在awk命令中n是一个字段引用其中n是一个数字表示当前行的第n个字段awk默认使用空白字符通常是空格或制表符作为字段分隔符因此123等分别代表每行的第一个第二个第三个字段依此类推">
 在&lt;code>awk&lt;/code>命令中，&lt;code>$n&lt;/code>是一个字段引用，其中&lt;code>n&lt;/code>是一个数字，表示当前行的第&lt;code>n&lt;/code>个字段。&lt;code>awk&lt;/code>默认使用空白字符（通常是空格或制表符）作为字段分隔符，因此&lt;code>$1&lt;/code>、&lt;code>$2&lt;/code>、&lt;code>$3&lt;/code>等分别代表每行的第一个、第二个、第三个字段，依此类推。
 &lt;a class="anchor" href="#%e5%9c%a8awk%e5%91%bd%e4%bb%a4%e4%b8%adn%e6%98%af%e4%b8%80%e4%b8%aa%e5%ad%97%e6%ae%b5%e5%bc%95%e7%94%a8%e5%85%b6%e4%b8%adn%e6%98%af%e4%b8%80%e4%b8%aa%e6%95%b0%e5%ad%97%e8%a1%a8%e7%a4%ba%e5%bd%93%e5%89%8d%e8%a1%8c%e7%9a%84%e7%ac%acn%e4%b8%aa%e5%ad%97%e6%ae%b5awk%e9%bb%98%e8%ae%a4%e4%bd%bf%e7%94%a8%e7%a9%ba%e7%99%bd%e5%ad%97%e7%ac%a6%e9%80%9a%e5%b8%b8%e6%98%af%e7%a9%ba%e6%a0%bc%e6%88%96%e5%88%b6%e8%a1%a8%e7%ac%a6%e4%bd%9c%e4%b8%ba%e5%ad%97%e6%ae%b5%e5%88%86%e9%9a%94%e7%ac%a6%e5%9b%a0%e6%ad%a4123%e7%ad%89%e5%88%86%e5%88%ab%e4%bb%a3%e8%a1%a8%e6%af%8f%e8%a1%8c%e7%9a%84%e7%ac%ac%e4%b8%80%e4%b8%aa%e7%ac%ac%e4%ba%8c%e4%b8%aa%e7%ac%ac%e4%b8%89%e4%b8%aa%e5%ad%97%e6%ae%b5%e4%be%9d%e6%ad%a4%e7%b1%bb%e6%8e%a8">#&lt;/a>
&lt;/h5>
&lt;h5 id="在处理nginx的访问日志时通常每个字段都是由特定的分隔符如空格或特殊字符分隔的例如在nginx的默认日志格式中">
 在处理Nginx的访问日志时，通常每个字段都是由特定的分隔符（如空格或特殊字符）分隔的。例如，在Nginx的默认日志格式中：
 &lt;a class="anchor" href="#%e5%9c%a8%e5%a4%84%e7%90%86nginx%e7%9a%84%e8%ae%bf%e9%97%ae%e6%97%a5%e5%bf%97%e6%97%b6%e9%80%9a%e5%b8%b8%e6%af%8f%e4%b8%aa%e5%ad%97%e6%ae%b5%e9%83%bd%e6%98%af%e7%94%b1%e7%89%b9%e5%ae%9a%e7%9a%84%e5%88%86%e9%9a%94%e7%ac%a6%e5%a6%82%e7%a9%ba%e6%a0%bc%e6%88%96%e7%89%b9%e6%ae%8a%e5%ad%97%e7%ac%a6%e5%88%86%e9%9a%94%e7%9a%84%e4%be%8b%e5%a6%82%e5%9c%a8nginx%e7%9a%84%e9%bb%98%e8%ae%a4%e6%97%a5%e5%bf%97%e6%a0%bc%e5%bc%8f%e4%b8%ad">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>复制代码

127.0.0.1 - - [10/Oct/2023:14:05:01 +0000] &amp;#34;GET / HTTP/1.1&amp;#34; 200 612 &amp;#34;-&amp;#34; &amp;#34;curl/7.68.0&amp;#34;
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>
&lt;h5 id="1-通常是客户端的ip地址">
 &lt;code>$1&lt;/code> 通常是客户端的IP地址。
 &lt;a class="anchor" href="#1-%e9%80%9a%e5%b8%b8%e6%98%af%e5%ae%a2%e6%88%b7%e7%ab%af%e7%9a%84ip%e5%9c%b0%e5%9d%80">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="2-通常是身份验证的用户名在这个例子中是--表示没有用户名">
 &lt;code>$2&lt;/code> 通常是身份验证的用户名（在这个例子中是 &lt;code>-&lt;/code>，表示没有用户名）。
 &lt;a class="anchor" href="#2-%e9%80%9a%e5%b8%b8%e6%98%af%e8%ba%ab%e4%bb%bd%e9%aa%8c%e8%af%81%e7%9a%84%e7%94%a8%e6%88%b7%e5%90%8d%e5%9c%a8%e8%bf%99%e4%b8%aa%e4%be%8b%e5%ad%90%e4%b8%ad%e6%98%af--%e8%a1%a8%e7%a4%ba%e6%b2%a1%e6%9c%89%e7%94%a8%e6%88%b7%e5%90%8d">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="3-通常是用户的身份通常也是--">
 &lt;code>$3&lt;/code> 通常是用户的身份（通常也是 &lt;code>-&lt;/code>）。
 &lt;a class="anchor" href="#3-%e9%80%9a%e5%b8%b8%e6%98%af%e7%94%a8%e6%88%b7%e7%9a%84%e8%ba%ab%e4%bb%bd%e9%80%9a%e5%b8%b8%e4%b9%9f%e6%98%af--">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="4-是请求时间">
 &lt;code>$4&lt;/code> 是请求时间。
 &lt;a class="anchor" href="#4-%e6%98%af%e8%af%b7%e6%b1%82%e6%97%b6%e9%97%b4">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="5-是请求行例如-get--http11">
 &lt;code>$5&lt;/code> 是请求行（例如 &lt;code>&amp;quot;GET / HTTP/1.1&amp;quot;&lt;/code>）。
 &lt;a class="anchor" href="#5-%e6%98%af%e8%af%b7%e6%b1%82%e8%a1%8c%e4%be%8b%e5%a6%82-get--http11">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="6-是响应状态码例如-200">
 &lt;code>$6&lt;/code> 是响应状态码（例如 &lt;code>200&lt;/code>）。
 &lt;a class="anchor" href="#6-%e6%98%af%e5%93%8d%e5%ba%94%e7%8a%b6%e6%80%81%e7%a0%81%e4%be%8b%e5%a6%82-200">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="7-是响应体的大小例如-612">
 &lt;code>$7&lt;/code> 是响应体的大小（例如 &lt;code>612&lt;/code>）。
 &lt;a class="anchor" href="#7-%e6%98%af%e5%93%8d%e5%ba%94%e4%bd%93%e7%9a%84%e5%a4%a7%e5%b0%8f%e4%be%8b%e5%a6%82-612">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="后续的字段可能包括引用页用户代理等">
 后续的字段可能包括引用页、用户代理等。
 &lt;a class="anchor" href="#%e5%90%8e%e7%bb%ad%e7%9a%84%e5%ad%97%e6%ae%b5%e5%8f%af%e8%83%bd%e5%8c%85%e6%8b%ac%e5%bc%95%e7%94%a8%e9%a1%b5%e7%94%a8%e6%88%b7%e4%bb%a3%e7%90%86%e7%ad%89">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h5 id="当你使用awk-print-1-accesslog时你正在告诉awk打印出每一行的第一个字段即客户端的ip地址">
 当你使用&lt;code>awk '{print $1}' access.log&lt;/code>时，你正在告诉&lt;code>awk&lt;/code>打印出每一行的第一个字段，即客户端的IP地址。
 &lt;a class="anchor" href="#%e5%bd%93%e4%bd%a0%e4%bd%bf%e7%94%a8awk-print-1-accesslog%e6%97%b6%e4%bd%a0%e6%ad%a3%e5%9c%a8%e5%91%8a%e8%af%89awk%e6%89%93%e5%8d%b0%e5%87%ba%e6%af%8f%e4%b8%80%e8%a1%8c%e7%9a%84%e7%ac%ac%e4%b8%80%e4%b8%aa%e5%ad%97%e6%ae%b5%e5%8d%b3%e5%ae%a2%e6%88%b7%e7%ab%af%e7%9a%84ip%e5%9c%b0%e5%9d%80">#&lt;/a>
&lt;/h5>
&lt;h5 id="awk也允许你通过-f选项来指定一个不同的字段分隔符例如如果nginx日志使用不同的分隔符你可以这样指定">
 &lt;code>awk&lt;/code>也允许你通过&lt;code>-F&lt;/code>选项来指定一个不同的字段分隔符。例如，如果Nginx日志使用不同的分隔符，你可以这样指定：
 &lt;a class="anchor" href="#awk%e4%b9%9f%e5%85%81%e8%ae%b8%e4%bd%a0%e9%80%9a%e8%bf%87-f%e9%80%89%e9%a1%b9%e6%9d%a5%e6%8c%87%e5%ae%9a%e4%b8%80%e4%b8%aa%e4%b8%8d%e5%90%8c%e7%9a%84%e5%ad%97%e6%ae%b5%e5%88%86%e9%9a%94%e7%ac%a6%e4%be%8b%e5%a6%82%e5%a6%82%e6%9e%9cnginx%e6%97%a5%e5%bf%97%e4%bd%bf%e7%94%a8%e4%b8%8d%e5%90%8c%e7%9a%84%e5%88%86%e9%9a%94%e7%ac%a6%e4%bd%a0%e5%8f%af%e4%bb%a5%e8%bf%99%e6%a0%b7%e6%8c%87%e5%ae%9a">#&lt;/a>
&lt;/h5>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>bash复制代码
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>awk -F&lt;span style="color:#e6db74">&amp;#39; &amp;#39;&lt;/span> &lt;span style="color:#e6db74">&amp;#39;{print $1}&amp;#39;&lt;/span> access.log
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>这里的&lt;code>-F' '&lt;/code>告诉&lt;code>awk&lt;/code>使用空格作为字段分隔符。但是，请注意，Nginx日志中的时间戳通常包含在方括号内，这可能会影响字段的计数。如果时间戳是一个字段，并且你想跳过它，你可能需要使用更复杂的&lt;code>awk&lt;/code>脚本或者不同的方法来处理日志。&lt;/p></description></item><item><title>2024-04-03 16个概念带你入门 Kubernetes</title><link>https://qq547475331.github.io/docs/16%E4%B8%AA%E6%A6%82%E5%BF%B5%E5%B8%A6%E4%BD%A0%E5%85%A5%E9%97%A8-kubernetes-16-ge-gai-nian-dai-ni-ru-men-kubernetes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/16%E4%B8%AA%E6%A6%82%E5%BF%B5%E5%B8%A6%E4%BD%A0%E5%85%A5%E9%97%A8-kubernetes-16-ge-gai-nian-dai-ni-ru-men-kubernetes/</guid><description>&lt;h5 id="kubernetes是google开源的容器集群管理系统是google多年规模容器管理技术borg的开源版本主要功能包括">
 Kubernetes是Google开源的容器集群管理系统，是Google多年⼤规模容器管理技术Borg的开源版本，主要功能包括:
 &lt;a class="anchor" href="#kubernetes%e6%98%afgoogle%e5%bc%80%e6%ba%90%e7%9a%84%e5%ae%b9%ef%a8%b8%e9%9b%86%e7%be%a4%e7%ae%a1%ef%a7%a4%e7%b3%bb%e7%bb%9f%e6%98%afgoogle%e5%a4%9a%ef%a6%8e%e8%a7%84%e6%a8%a1%e5%ae%b9%ef%a8%b8%e7%ae%a1%ef%a7%a4%e6%8a%80%e6%9c%afborg%e7%9a%84%e5%bc%80%e6%ba%90%e7%89%88%e6%9c%ac%e4%b8%bb%e8%a6%81%e5%8a%9f%e8%83%bd%e5%8c%85%e6%8b%ac">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="基于容器的应用部署维护和滚动升级">
 基于容器的应用部署、维护和滚动升级
 &lt;a class="anchor" href="#%e5%9f%ba%e4%ba%8e%e5%ae%b9%e5%99%a8%e7%9a%84%e5%ba%94%e7%94%a8%e9%83%a8%e7%bd%b2%e7%bb%b4%e6%8a%a4%e5%92%8c%e6%bb%9a%e5%8a%a8%e5%8d%87%e7%ba%a7">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="负载均衡和服务发现">
 负载均衡和服务发现
 &lt;a class="anchor" href="#%e8%b4%9f%e8%bd%bd%e5%9d%87%e8%a1%a1%e5%92%8c%e6%9c%8d%e5%8a%a1%e5%8f%91%e7%8e%b0">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="跨机器和跨地区的集群调度">
 跨机器和跨地区的集群调度
 &lt;a class="anchor" href="#%e8%b7%a8%e6%9c%ba%e5%99%a8%e5%92%8c%e8%b7%a8%e5%9c%b0%e5%8c%ba%e7%9a%84%e9%9b%86%e7%be%a4%e8%b0%83%e5%ba%a6">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="自动伸缩">
 自动伸缩
 &lt;a class="anchor" href="#%e8%87%aa%e5%8a%a8%e4%bc%b8%e7%bc%a9">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="无状态服务和有状态服务">
 无状态服务和有状态服务
 &lt;a class="anchor" href="#%e6%97%a0%e7%8a%b6%e6%80%81%e6%9c%8d%e5%8a%a1%e5%92%8c%e6%9c%89%e7%8a%b6%e6%80%81%e6%9c%8d%e5%8a%a1">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="广泛的volume支持">
 广泛的Volume支持
 &lt;a class="anchor" href="#%e5%b9%bf%e6%b3%9b%e7%9a%84volume%e6%94%af%e6%8c%81">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="插件机制保证扩展性">
 插件机制保证扩展性
 &lt;a class="anchor" href="#%e6%8f%92%e4%bb%b6%e6%9c%ba%e5%88%b6%e4%bf%9d%e8%af%81%e6%89%a9%e5%b1%95%e6%80%a7">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h5 id="kubernetes发展非常迅速已经成为容器编排领域的领导者接下来我们将讲解kubernetes中涉及到的一些主要概念">
 Kubernetes发展非常迅速，已经成为容器编排领域的领导者，接下来我们将讲解Kubernetes中涉及到的一些主要概念。
 &lt;a class="anchor" href="#kubernetes%e5%8f%91%e5%b1%95%e9%9d%9e%e5%b8%b8%e8%bf%85%e9%80%9f%e5%b7%b2%e7%bb%8f%e6%88%90%e4%b8%ba%e5%ae%b9%e5%99%a8%e7%bc%96%e6%8e%92%e9%a2%86%e5%9f%9f%e7%9a%84%e9%a2%86%e5%af%bc%e8%80%85%e6%8e%a5%e4%b8%8b%e6%9d%a5%e6%88%91%e4%bb%ac%e5%b0%86%e8%ae%b2%e8%a7%a3kubernetes%e4%b8%ad%e6%b6%89%e5%8f%8a%e5%88%b0%e7%9a%84%e4%b8%80%e4%ba%9b%e4%b8%bb%e8%a6%81%e6%a6%82%e5%bf%b5">#&lt;/a>
&lt;/h5>
&lt;h4 id="">
 
 &lt;a class="anchor" href="#">#&lt;/a>
&lt;/h4>
&lt;h4 id="1pod">
 &lt;strong>1、Pod&lt;/strong>
 &lt;a class="anchor" href="#1pod">#&lt;/a>
&lt;/h4>
&lt;h5 id="pod是一组紧密关联的容器集合支持多个容器在一个pod中共享网络和文件系统可以通过进程间通信和文件共享这种简单高效的方式完成服务是kubernetes调度的基本单位pod的设计理念是每个pod都有一个唯一的ip">
 Pod是一组紧密关联的容器集合，支持多个容器在一个Pod中共享网络和文件系统，可以通过进程间通信和文件共享这种简单高效的方式完成服务，是Kubernetes调度的基本单位。Pod的设计理念是每个Pod都有一个唯一的IP。
 &lt;a class="anchor" href="#pod%e6%98%af%e4%b8%80%e7%bb%84%e7%b4%a7%e5%af%86%e5%85%b3%e8%81%94%e7%9a%84%e5%ae%b9%e5%99%a8%e9%9b%86%e5%90%88%e6%94%af%e6%8c%81%e5%a4%9a%e4%b8%aa%e5%ae%b9%e5%99%a8%e5%9c%a8%e4%b8%80%e4%b8%aapod%e4%b8%ad%e5%85%b1%e4%ba%ab%e7%bd%91%e7%bb%9c%e5%92%8c%e6%96%87%e4%bb%b6%e7%b3%bb%e7%bb%9f%e5%8f%af%e4%bb%a5%e9%80%9a%e8%bf%87%e8%bf%9b%e7%a8%8b%e9%97%b4%e9%80%9a%e4%bf%a1%e5%92%8c%e6%96%87%e4%bb%b6%e5%85%b1%e4%ba%ab%e8%bf%99%e7%a7%8d%e7%ae%80%e5%8d%95%e9%ab%98%e6%95%88%e7%9a%84%e6%96%b9%e5%bc%8f%e5%ae%8c%e6%88%90%e6%9c%8d%e5%8a%a1%e6%98%afkubernetes%e8%b0%83%e5%ba%a6%e7%9a%84%e5%9f%ba%e6%9c%ac%e5%8d%95%e4%bd%8dpod%e7%9a%84%e8%ae%be%e8%ae%a1%e7%90%86%e5%bf%b5%e6%98%af%e6%af%8f%e4%b8%aapod%e9%83%bd%e6%9c%89%e4%b8%80%e4%b8%aa%e5%94%af%e4%b8%80%e7%9a%84ip">#&lt;/a>
&lt;/h5>
&lt;h5 id="pod具有如下特征">
 Pod具有如下特征：
 &lt;a class="anchor" href="#pod%e5%85%b7%e6%9c%89%e5%a6%82%e4%b8%8b%e7%89%b9%e5%be%81">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="包含多个共享ipcnetwork和utc-namespace的容器可直接通过localhost通信">
 包含多个共享IPC、Network和UTC namespace的容器，可直接通过localhost通信
 &lt;a class="anchor" href="#%e5%8c%85%e5%90%ab%e5%a4%9a%e4%b8%aa%e5%85%b1%e4%ba%abipcnetwork%e5%92%8cutc-namespace%e7%9a%84%e5%ae%b9%e5%99%a8%e5%8f%af%e7%9b%b4%e6%8e%a5%e9%80%9a%e8%bf%87localhost%e9%80%9a%e4%bf%a1">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="所有pod内容器都可以访问共享的volume可以访问共享数据">
 所有Pod内容器都可以访问共享的Volume，可以访问共享数据
 &lt;a class="anchor" href="#%e6%89%80%e6%9c%89pod%e5%86%85%e5%ae%b9%e5%99%a8%e9%83%bd%e5%8f%af%e4%bb%a5%e8%ae%bf%e9%97%ae%e5%85%b1%e4%ba%ab%e7%9a%84volume%e5%8f%af%e4%bb%a5%e8%ae%bf%e9%97%ae%e5%85%b1%e4%ba%ab%e6%95%b0%e6%8d%ae">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="优雅终止pod删除的时候先给其内的进程发送sigterm等待一段时间grace-period后才强制停止依然还在运行的进程">
 优雅终止:Pod删除的时候先给其内的进程发送SIGTERM，等待一段时间(grace period)后才强制停止依然还在运行的进程
 &lt;a class="anchor" href="#%e4%bc%98%e9%9b%85%e7%bb%88%e6%ad%a2pod%e5%88%a0%e9%99%a4%e7%9a%84%e6%97%b6%e5%80%99%e5%85%88%e7%bb%99%e5%85%b6%e5%86%85%e7%9a%84%e8%bf%9b%e7%a8%8b%e5%8f%91%e9%80%81sigterm%e7%ad%89%e5%be%85%e4%b8%80%e6%ae%b5%e6%97%b6%e9%97%b4grace-period%e5%90%8e%e6%89%8d%e5%bc%ba%e5%88%b6%e5%81%9c%e6%ad%a2%e4%be%9d%e7%84%b6%e8%bf%98%e5%9c%a8%e8%bf%90%e8%a1%8c%e7%9a%84%e8%bf%9b%e7%a8%8b">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="特权容器通过securitycontext配置具有改变系统配置的权限在网络插件中大量应用">
 特权容器(通过SecurityContext配置)具有改变系统配置的权限(在网络插件中大量应用)
 &lt;a class="anchor" href="#%e7%89%b9%e6%9d%83%e5%ae%b9%e5%99%a8%e9%80%9a%e8%bf%87securitycontext%e9%85%8d%e7%bd%ae%e5%85%b7%e6%9c%89%e6%94%b9%e5%8f%98%e7%b3%bb%e7%bb%9f%e9%85%8d%e7%bd%ae%e7%9a%84%e6%9d%83%e9%99%90%e5%9c%a8%e7%bd%91%e7%bb%9c%e6%8f%92%e4%bb%b6%e4%b8%ad%e5%a4%a7%e9%87%8f%e5%ba%94%e7%94%a8">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="支持三种重启策略restartpolicy分别是alwaysonfailurenever">
 支持三种重启策略（restartPolicy），分别是：Always、OnFailure、Never
 &lt;a class="anchor" href="#%e6%94%af%e6%8c%81%e4%b8%89%e7%a7%8d%e9%87%8d%e5%90%af%e7%ad%96%e7%95%a5restartpolicy%e5%88%86%e5%88%ab%e6%98%afalwaysonfailurenever">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="支持三种镜像拉取策略imagepullpolicy分别是alwaysneverifnotpresent">
 支持三种镜像拉取策略（imagePullPolicy），分别是：Always、Never、IfNotPresent
 &lt;a class="anchor" href="#%e6%94%af%e6%8c%81%e4%b8%89%e7%a7%8d%e9%95%9c%e5%83%8f%e6%8b%89%e5%8f%96%e7%ad%96%e7%95%a5imagepullpolicy%e5%88%86%e5%88%ab%e6%98%afalwaysneverifnotpresent">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="资源限制kubernetes通过cgroup限制容器的cpu以及内存等资源可以设置request以及limit值">
 资源限制，Kubernetes通过CGroup限制容器的CPU以及内存等资源，可以设置request以及limit值
 &lt;a class="anchor" href="#%e8%b5%84%e6%ba%90%e9%99%90%e5%88%b6kubernetes%e9%80%9a%e8%bf%87cgroup%e9%99%90%e5%88%b6%e5%ae%b9%e5%99%a8%e7%9a%84cpu%e4%bb%a5%e5%8f%8a%e5%86%85%e5%ad%98%e7%ad%89%e8%b5%84%e6%ba%90%e5%8f%af%e4%bb%a5%e8%ae%be%e7%bd%aerequest%e4%bb%a5%e5%8f%8alimit%e5%80%bc">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="健康检查提供两种健康检查探针分别是livenessprobe和redinessprobe前者用于探测容器是否存活如果探测失败则根据重启策略进行重启操作后者用于检查容器状态是否正常如果检查容器状态不正常则请求不会到达该pod">
 健康检查，提供两种健康检查探针，分别是livenessProbe和redinessProbe，前者用于探测容器是否存活，如果探测失败，则根据重启策略进行重启操作，后者用于检查容器状态是否正常，如果检查容器状态不正常，则请求不会到达该Pod
 &lt;a class="anchor" href="#%e5%81%a5%e5%ba%b7%e6%a3%80%e6%9f%a5%e6%8f%90%e4%be%9b%e4%b8%a4%e7%a7%8d%e5%81%a5%e5%ba%b7%e6%a3%80%e6%9f%a5%e6%8e%a2%e9%92%88%e5%88%86%e5%88%ab%e6%98%aflivenessprobe%e5%92%8credinessprobe%e5%89%8d%e8%80%85%e7%94%a8%e4%ba%8e%e6%8e%a2%e6%b5%8b%e5%ae%b9%e5%99%a8%e6%98%af%e5%90%a6%e5%ad%98%e6%b4%bb%e5%a6%82%e6%9e%9c%e6%8e%a2%e6%b5%8b%e5%a4%b1%e8%b4%a5%e5%88%99%e6%a0%b9%e6%8d%ae%e9%87%8d%e5%90%af%e7%ad%96%e7%95%a5%e8%bf%9b%e8%a1%8c%e9%87%8d%e5%90%af%e6%93%8d%e4%bd%9c%e5%90%8e%e8%80%85%e7%94%a8%e4%ba%8e%e6%a3%80%e6%9f%a5%e5%ae%b9%e5%99%a8%e7%8a%b6%e6%80%81%e6%98%af%e5%90%a6%e6%ad%a3%e5%b8%b8%e5%a6%82%e6%9e%9c%e6%a3%80%e6%9f%a5%e5%ae%b9%e5%99%a8%e7%8a%b6%e6%80%81%e4%b8%8d%e6%ad%a3%e5%b8%b8%e5%88%99%e8%af%b7%e6%b1%82%e4%b8%8d%e4%bc%9a%e5%88%b0%e8%be%be%e8%af%a5pod">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="init-container在所有容器运行之前执行常用来初始化配置">
 Init container在所有容器运行之前执行，常用来初始化配置
 &lt;a class="anchor" href="#init-container%e5%9c%a8%e6%89%80%e6%9c%89%e5%ae%b9%e5%99%a8%e8%bf%90%e8%a1%8c%e4%b9%8b%e5%89%8d%e6%89%a7%e8%a1%8c%e5%b8%b8%e7%94%a8%e6%9d%a5%e5%88%9d%e5%a7%8b%e5%8c%96%e9%85%8d%e7%bd%ae">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="容器生命周期钩子函数用于监听容器生命周期的特定事件并在事件发生时执行已注册的回调函数支持两种钩子函数poststart和prestop前者是在容器启动后执行后者是在容器停止前执行">
 容器生命周期钩子函数，用于监听容器生命周期的特定事件，并在事件发生时执行已注册的回调函数，支持两种钩子函数：postStart和preStop，前者是在容器启动后执行，后者是在容器停止前执行
 &lt;a class="anchor" href="#%e5%ae%b9%e5%99%a8%e7%94%9f%e5%91%bd%e5%91%a8%e6%9c%9f%e9%92%a9%e5%ad%90%e5%87%bd%e6%95%b0%e7%94%a8%e4%ba%8e%e7%9b%91%e5%90%ac%e5%ae%b9%e5%99%a8%e7%94%9f%e5%91%bd%e5%91%a8%e6%9c%9f%e7%9a%84%e7%89%b9%e5%ae%9a%e4%ba%8b%e4%bb%b6%e5%b9%b6%e5%9c%a8%e4%ba%8b%e4%bb%b6%e5%8f%91%e7%94%9f%e6%97%b6%e6%89%a7%e8%a1%8c%e5%b7%b2%e6%b3%a8%e5%86%8c%e7%9a%84%e5%9b%9e%e8%b0%83%e5%87%bd%e6%95%b0%e6%94%af%e6%8c%81%e4%b8%a4%e7%a7%8d%e9%92%a9%e5%ad%90%e5%87%bd%e6%95%b0poststart%e5%92%8cprestop%e5%89%8d%e8%80%85%e6%98%af%e5%9c%a8%e5%ae%b9%e5%99%a8%e5%90%af%e5%8a%a8%e5%90%8e%e6%89%a7%e8%a1%8c%e5%90%8e%e8%80%85%e6%98%af%e5%9c%a8%e5%ae%b9%e5%99%a8%e5%81%9c%e6%ad%a2%e5%89%8d%e6%89%a7%e8%a1%8c">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h4 id="2namespace">
 &lt;strong>2、Namespace&lt;/strong>
 &lt;a class="anchor" href="#2namespace">#&lt;/a>
&lt;/h4>
&lt;h5 id="namespace命名空间是对一组资源和对象的抽象集合比如可以用来将系统内部的对象划分为不同的项目组或者用户组常见的podservicereplicaset和deployment等都是属于某一个namespace的默认是default而node-persistentvolumes等则不属于任何namespace">
 Namespace（命名空间）是对一组资源和对象的抽象集合，比如可以用来将系统内部的对象划分为不同的项目组或者用户组。常见的pod、service、replicaSet和deployment等都是属于某一个namespace的(默认是default)，而node, persistentVolumes等则不属于任何namespace。
 &lt;a class="anchor" href="#namespace%e5%91%bd%e5%90%8d%e7%a9%ba%e9%97%b4%e6%98%af%e5%af%b9%e4%b8%80%e7%bb%84%e8%b5%84%e6%ba%90%e5%92%8c%e5%af%b9%e8%b1%a1%e7%9a%84%e6%8a%bd%e8%b1%a1%e9%9b%86%e5%90%88%e6%af%94%e5%a6%82%e5%8f%af%e4%bb%a5%e7%94%a8%e6%9d%a5%e5%b0%86%e7%b3%bb%e7%bb%9f%e5%86%85%e9%83%a8%e7%9a%84%e5%af%b9%e8%b1%a1%e5%88%92%e5%88%86%e4%b8%ba%e4%b8%8d%e5%90%8c%e7%9a%84%e9%a1%b9%e7%9b%ae%e7%bb%84%e6%88%96%e8%80%85%e7%94%a8%e6%88%b7%e7%bb%84%e5%b8%b8%e8%a7%81%e7%9a%84podservicereplicaset%e5%92%8cdeployment%e7%ad%89%e9%83%bd%e6%98%af%e5%b1%9e%e4%ba%8e%e6%9f%90%e4%b8%80%e4%b8%aanamespace%e7%9a%84%e9%bb%98%e8%ae%a4%e6%98%afdefault%e8%80%8cnode-persistentvolumes%e7%ad%89%e5%88%99%e4%b8%8d%e5%b1%9e%e4%ba%8e%e4%bb%bb%e4%bd%95namespace">#&lt;/a>
&lt;/h5>
&lt;h5 id="常用namespace操作">
 常用namespace操作：
 &lt;a class="anchor" href="#%e5%b8%b8%e7%94%a8namespace%e6%93%8d%e4%bd%9c">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;pre tabindex="0">&lt;code>kubectl get namespace
&lt;/code>&lt;/pre>&lt;h5 id="-查询所有namespace">
 , 查询所有namespace
 &lt;a class="anchor" href="#-%e6%9f%a5%e8%af%a2%e6%89%80%e6%9c%89namespace">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;pre tabindex="0">&lt;code>kubectl create namespacens-name
&lt;/code>&lt;/pre>&lt;h5 id="创建namespace">
 ，创建namespace
 &lt;a class="anchor" href="#%e5%88%9b%e5%bb%banamespace">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;pre tabindex="0">&lt;code>kubectl delete namespacens-name
&lt;/code>&lt;/pre>&lt;h5 id="-删除namespace">
 , 删除namespace
 &lt;a class="anchor" href="#-%e5%88%a0%e9%99%a4namespace">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h5 id="删除命名空间时需注意以下几点">
 删除命名空间时，需注意以下几点：
 &lt;a class="anchor" href="#%e5%88%a0%e9%99%a4%e5%91%bd%e5%90%8d%e7%a9%ba%e9%97%b4%e6%97%b6%e9%9c%80%e6%b3%a8%e6%84%8f%e4%bb%a5%e4%b8%8b%e5%87%a0%e7%82%b9">#&lt;/a>
&lt;/h5>
&lt;ol>
&lt;li>
&lt;h5 id="删除一个namespace会自动删除所有属于该namespace的资源">
 删除一个namespace会自动删除所有属于该namespace的资源。
 &lt;a class="anchor" href="#%e5%88%a0%e9%99%a4%e4%b8%80%e4%b8%aanamespace%e4%bc%9a%e8%87%aa%e5%8a%a8%e5%88%a0%e9%99%a4%e6%89%80%e6%9c%89%e5%b1%9e%e4%ba%8e%e8%af%a5namespace%e7%9a%84%e8%b5%84%e6%ba%90">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="default-和-kube-system-命名空间不可删除">
 default 和 kube-system 命名空间不可删除。
 &lt;a class="anchor" href="#default-%e5%92%8c-kube-system-%e5%91%bd%e5%90%8d%e7%a9%ba%e9%97%b4%e4%b8%8d%e5%8f%af%e5%88%a0%e9%99%a4">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="persistentvolumes是不属于任何namespace的但persistentvolumeclaim是属于某个特定namespace的">
 PersistentVolumes是不属于任何namespace的，但PersistentVolumeClaim是属于某个特定namespace的。
 &lt;a class="anchor" href="#persistentvolumes%e6%98%af%e4%b8%8d%e5%b1%9e%e4%ba%8e%e4%bb%bb%e4%bd%95namespace%e7%9a%84%e4%bd%86persistentvolumeclaim%e6%98%af%e5%b1%9e%e4%ba%8e%e6%9f%90%e4%b8%aa%e7%89%b9%e5%ae%9anamespace%e7%9a%84">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="events是否属于namespace取决于产生events的对象">
 Events是否属于namespace取决于产生events的对象。
 &lt;a class="anchor" href="#events%e6%98%af%e5%90%a6%e5%b1%9e%e4%ba%8enamespace%e5%8f%96%e5%86%b3%e4%ba%8e%e4%ba%a7%e7%94%9fevents%e7%9a%84%e5%af%b9%e8%b1%a1">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ol>
&lt;h4 id="">
 
 &lt;a class="anchor" href="#">#&lt;/a>
&lt;/h4>
&lt;h4 id="-">
 ** **
 &lt;a class="anchor" href="#-">#&lt;/a>
&lt;/h4>
&lt;h4 id="3node">
 &lt;strong>3、Node&lt;/strong>
 &lt;a class="anchor" href="#3node">#&lt;/a>
&lt;/h4>
&lt;h5 id="node是pod真正运行的主机可以是物理机也可以是虚拟机node本质上不是kubernetes来创建的-kubernetes只是管理node上的资源为了管理pod每个node节点上至少需要运行container-runtimedockerkubelet和kube-proxy服务">
 Node是Pod真正运行的主机，可以是物理机也可以是虚拟机。Node本质上不是Kubernetes来创建的， Kubernetes只是管理Node上的资源。为了管理Pod，每个Node节点上至少需要运行container runtime（Docker）、kubelet和kube-proxy服务。
 &lt;a class="anchor" href="#node%e6%98%afpod%e7%9c%9f%e6%ad%a3%e8%bf%90%e8%a1%8c%e7%9a%84%e4%b8%bb%e6%9c%ba%e5%8f%af%e4%bb%a5%e6%98%af%e7%89%a9%e7%90%86%e6%9c%ba%e4%b9%9f%e5%8f%af%e4%bb%a5%e6%98%af%e8%99%9a%e6%8b%9f%e6%9c%banode%e6%9c%ac%e8%b4%a8%e4%b8%8a%e4%b8%8d%e6%98%afkubernetes%e6%9d%a5%e5%88%9b%e5%bb%ba%e7%9a%84-kubernetes%e5%8f%aa%e6%98%af%e7%ae%a1%e7%90%86node%e4%b8%8a%e7%9a%84%e8%b5%84%e6%ba%90%e4%b8%ba%e4%ba%86%e7%ae%a1%e7%90%86pod%e6%af%8f%e4%b8%aanode%e8%8a%82%e7%82%b9%e4%b8%8a%e8%87%b3%e5%b0%91%e9%9c%80%e8%a6%81%e8%bf%90%e8%a1%8ccontainer-runtimedockerkubelet%e5%92%8ckube-proxy%e6%9c%8d%e5%8a%a1">#&lt;/a>
&lt;/h5>
&lt;h5 id="常用node操作">
 常用node操作：
 &lt;a class="anchor" href="#%e5%b8%b8%e7%94%a8node%e6%93%8d%e4%bd%9c">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;pre tabindex="0">&lt;code>kubectl get nodes
&lt;/code>&lt;/pre>&lt;h5 id="查询所有node">
 ，查询所有node
 &lt;a class="anchor" href="#%e6%9f%a5%e8%af%a2%e6%89%80%e6%9c%89node">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;pre tabindex="0">&lt;code>kubectl cordon $nodename
&lt;/code>&lt;/pre>&lt;h5 id="-将node标志为不可调度">
 , 将node标志为不可调度
 &lt;a class="anchor" href="#-%e5%b0%86node%e6%a0%87%e5%bf%97%e4%b8%ba%e4%b8%8d%e5%8f%af%e8%b0%83%e5%ba%a6">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;pre tabindex="0">&lt;code>kubectl uncordon $nodename
&lt;/code>&lt;/pre>&lt;h5 id="-将node标志为可调度">
 , 将node标志为可调度
 &lt;a class="anchor" href="#-%e5%b0%86node%e6%a0%87%e5%bf%97%e4%b8%ba%e5%8f%af%e8%b0%83%e5%ba%a6">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h5 id="taint污点">
 taint(污点)
 &lt;a class="anchor" href="#taint%e6%b1%a1%e7%82%b9">#&lt;/a>
&lt;/h5>
&lt;h5 id="使用kubectl-taint命令可以给某个node节点设置污点node被设置上污点之后就和pod之间存在了一种相斥的关系可以让node拒绝pod的调度执行甚至将node已经存在的pod驱逐出去每个污点的组成keyvalueeffect当前taint-effect支持如下三个选项">
 使用kubectl taint命令可以给某个Node节点设置污点，Node被设置上污点之后就和Pod之间存在了一种相斥的关系，可以让Node拒绝Pod的调度执行，甚至将Node已经存在的Pod驱逐出去。每个污点的组成：key=value:effect，当前taint effect支持如下三个选项：
 &lt;a class="anchor" href="#%e4%bd%bf%e7%94%a8kubectl-taint%e5%91%bd%e4%bb%a4%e5%8f%af%e4%bb%a5%e7%bb%99%e6%9f%90%e4%b8%aanode%e8%8a%82%e7%82%b9%e8%ae%be%e7%bd%ae%e6%b1%a1%e7%82%b9node%e8%a2%ab%e8%ae%be%e7%bd%ae%e4%b8%8a%e6%b1%a1%e7%82%b9%e4%b9%8b%e5%90%8e%e5%b0%b1%e5%92%8cpod%e4%b9%8b%e9%97%b4%e5%ad%98%e5%9c%a8%e4%ba%86%e4%b8%80%e7%a7%8d%e7%9b%b8%e6%96%a5%e7%9a%84%e5%85%b3%e7%b3%bb%e5%8f%af%e4%bb%a5%e8%ae%a9node%e6%8b%92%e7%bb%9dpod%e7%9a%84%e8%b0%83%e5%ba%a6%e6%89%a7%e8%a1%8c%e7%94%9a%e8%87%b3%e5%b0%86node%e5%b7%b2%e7%bb%8f%e5%ad%98%e5%9c%a8%e7%9a%84pod%e9%a9%b1%e9%80%90%e5%87%ba%e5%8e%bb%e6%af%8f%e4%b8%aa%e6%b1%a1%e7%82%b9%e7%9a%84%e7%bb%84%e6%88%90keyvalueeffect%e5%bd%93%e5%89%8dtaint-effect%e6%94%af%e6%8c%81%e5%a6%82%e4%b8%8b%e4%b8%89%e4%b8%aa%e9%80%89%e9%a1%b9">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="noschedule表示k8s将不会将pod调度到具有该污点的node上">
 NoSchedule：表示k8s将不会将Pod调度到具有该污点的Node上
 &lt;a class="anchor" href="#noschedule%e8%a1%a8%e7%a4%bak8s%e5%b0%86%e4%b8%8d%e4%bc%9a%e5%b0%86pod%e8%b0%83%e5%ba%a6%e5%88%b0%e5%85%b7%e6%9c%89%e8%af%a5%e6%b1%a1%e7%82%b9%e7%9a%84node%e4%b8%8a">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="prefernoschedule表示k8s将尽量避免将pod调度到具有该污点的node上">
 PreferNoSchedule：表示k8s将尽量避免将Pod调度到具有该污点的Node上
 &lt;a class="anchor" href="#prefernoschedule%e8%a1%a8%e7%a4%bak8s%e5%b0%86%e5%b0%bd%e9%87%8f%e9%81%bf%e5%85%8d%e5%b0%86pod%e8%b0%83%e5%ba%a6%e5%88%b0%e5%85%b7%e6%9c%89%e8%af%a5%e6%b1%a1%e7%82%b9%e7%9a%84node%e4%b8%8a">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="noexecute表示k8s将不会将pod调度到具有该污点的node上同时会将node上已经存在的pod驱逐出去">
 NoExecute：表示k8s将不会将Pod调度到具有该污点的Node上，同时会将Node上已经存在的Pod驱逐出去
 &lt;a class="anchor" href="#noexecute%e8%a1%a8%e7%a4%bak8s%e5%b0%86%e4%b8%8d%e4%bc%9a%e5%b0%86pod%e8%b0%83%e5%ba%a6%e5%88%b0%e5%85%b7%e6%9c%89%e8%af%a5%e6%b1%a1%e7%82%b9%e7%9a%84node%e4%b8%8a%e5%90%8c%e6%97%b6%e4%bc%9a%e5%b0%86node%e4%b8%8a%e5%b7%b2%e7%bb%8f%e5%ad%98%e5%9c%a8%e7%9a%84pod%e9%a9%b1%e9%80%90%e5%87%ba%e5%8e%bb">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;p>常用命令如下：&lt;/p></description></item><item><title>2024-04-03 acme自动更新证书</title><link>https://qq547475331.github.io/docs/acme%E8%87%AA%E5%8A%A8%E6%9B%B4%E6%96%B0%E8%AF%81%E4%B9%A6-acme-zi-dong-geng-xin-zheng-shu/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/acme%E8%87%AA%E5%8A%A8%E6%9B%B4%E6%96%B0%E8%AF%81%E4%B9%A6-acme-zi-dong-geng-xin-zheng-shu/</guid><description>&lt;p>&lt;strong>acme.sh&lt;/strong> 实现了 &lt;code>acme&lt;/code> 协议, 可以从 letsencrypt 生成免费的证书.&lt;/p>
&lt;p>主要步骤:&lt;/p>
&lt;ol>
&lt;li>安装 &lt;strong>acme.sh&lt;/strong>&lt;/li>
&lt;li>生成证书&lt;/li>
&lt;li>copy 证书到 nginx/apache 或者其他服务&lt;/li>
&lt;li>更新证书&lt;/li>
&lt;li>更新 &lt;strong>acme.sh&lt;/strong>&lt;/li>
&lt;li>出错怎么办, 如何调试&lt;/li>
&lt;/ol>
&lt;p>下面详细介绍.&lt;/p>
&lt;h1 id="1-安装-acmesh">
 1. 安装 &lt;strong>acme.sh&lt;/strong>
 &lt;a class="anchor" href="#1-%e5%ae%89%e8%a3%85-acmesh">#&lt;/a>
&lt;/h1>
&lt;p>安装很简单, 一个命令:&lt;/p>
&lt;pre tabindex="0">&lt;code>curl https://get.acme.sh | sh -s email=my@example.com
&lt;/code>&lt;/pre>&lt;p>普通用户和 root 用户都可以安装使用. 安装过程进行了以下几步:&lt;/p>
&lt;ol>
&lt;li>把 acme.sh 安装到你的 &lt;strong>home&lt;/strong> 目录下:&lt;/li>
&lt;/ol>
&lt;pre tabindex="0">&lt;code>~/.acme.sh/
&lt;/code>&lt;/pre>&lt;p>并创建 一个 shell 的 alias, 例如 .bashrc，方便你的使用: &lt;code>alias acme.sh=~/.acme.sh/acme.sh&lt;/code>&lt;/p>
&lt;ol>
&lt;li>自动为你创建 cronjob, 每天 0:00 点自动检测所有的证书, 如果快过期了, 需要更新, 则会自动更新证书.&lt;/li>
&lt;/ol>
&lt;p>更高级的安装选项请参考: &lt;a href="https://github.com/Neilpang/acme.sh/wiki/How-to-install">https://github.com/Neilpang/acme.sh/wiki/How-to-install&lt;/a>&lt;/p>
&lt;p>&lt;strong>安装过程不会污染已有的系统任何功能和文件&lt;/strong>, 所有的修改都限制在安装目录中: &lt;code>~/.acme.sh/&lt;/code>&lt;/p>
&lt;h1 id="2-生成证书">
 2. 生成证书
 &lt;a class="anchor" href="#2-%e7%94%9f%e6%88%90%e8%af%81%e4%b9%a6">#&lt;/a>
&lt;/h1>
&lt;p>&lt;strong>acme.sh&lt;/strong> 实现了 &lt;strong>acme&lt;/strong> 协议支持的所有验证协议. 一般有两种方式验证: http 和 dns 验证.&lt;/p></description></item><item><title>2024-04-03 Calico网络自定义</title><link>https://qq547475331.github.io/docs/calico%E7%BD%91%E7%BB%9C%E8%87%AA%E5%AE%9A%E4%B9%89-calico-wang-luo-zi-ding-yi/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/calico%E7%BD%91%E7%BB%9C%E8%87%AA%E5%AE%9A%E4%B9%89-calico-wang-luo-zi-ding-yi/</guid><description>&lt;h1 id="一文学会calico网络自定义">
 一文学会Calico网络自定义
 &lt;a class="anchor" href="#%e4%b8%80%e6%96%87%e5%ad%a6%e4%bc%9acalico%e7%bd%91%e7%bb%9c%e8%87%aa%e5%ae%9a%e4%b9%89">#&lt;/a>
&lt;/h1>
&lt;h1 id="一确定最佳的网络选项">
 一、确定最佳的网络选项
 &lt;a class="anchor" href="#%e4%b8%80%e7%a1%ae%e5%ae%9a%e6%9c%80%e4%bd%b3%e7%9a%84%e7%bd%91%e7%bb%9c%e9%80%89%e9%a1%b9">#&lt;/a>
&lt;/h1>
&lt;p>Calico支持多个容器网络选项，用于可伸缩性、网络性能和与现有基础设施的互操作性。&lt;/p>
&lt;h4 id="1-值">
 1. 值
 &lt;a class="anchor" href="#1-%e5%80%bc">#&lt;/a>
&lt;/h4>
&lt;p>不同的网络实现更适合不同的环境。Calico提供了几种不需要封装的基于IP路由的网络实现。如果您的部署需要封装，Calico提供覆盖网络(IP in IP或VXLAN)。Calico还支持使用其他Kubernetes网络选项来执行策略。本文档帮助您为集群选择最佳的网络选项。&lt;/p>
&lt;h4 id="2-概念">
 2. 概念
 &lt;a class="anchor" href="#2-%e6%a6%82%e5%bf%b5">#&lt;/a>
&lt;/h4>
&lt;h5 id="21-关于calico-networking">
 2.1 关于calico networking
 &lt;a class="anchor" href="#21-%e5%85%b3%e4%ba%8ecalico-networking">#&lt;/a>
&lt;/h5>
&lt;p>Calico提供了一些方法，允许pod连接到其他pod、主机和外部网络(例如internet)。&lt;/p>
&lt;p>Calico网络：&lt;/p>
&lt;ul>
&lt;li>使用Calico的IP地址管理(IPAM)将IP地址分配给pods&lt;/li>
&lt;li>编写本地节点的路由表&lt;/li>
&lt;li>将路由分配给其他节点和网络设备&lt;/li>
&lt;/ul>
&lt;h5 id="22-关于bgp">
 2.2 关于BGP
 &lt;a class="anchor" href="#22-%e5%85%b3%e4%ba%8ebgp">#&lt;/a>
&lt;/h5>
&lt;p>Calico支持使用边界网关协议(BGP)将路由信息共享到网络中。Calico支持节点到节点的全网格部署(有和没有路由反射器)，以及BGP直接对机架(ToR)路由器顶部的现场部署;允许流量直接路由到工作负载，而不需要NAT或封装。&lt;/p>
&lt;h5 id="23-其它kubernetes-网络选项">
 2.3 其它Kubernetes 网络选项
 &lt;a class="anchor" href="#23-%e5%85%b6%e5%ae%83kubernetes-%e7%bd%91%e7%bb%9c%e9%80%89%e9%a1%b9">#&lt;/a>
&lt;/h5>
&lt;p>Calico可以使用许多其他Kubernetes网络选项执行网络策略强制。&lt;/p>
&lt;ul>
&lt;li>Flannel&lt;/li>
&lt;li>Amazon AWS VPC CNI&lt;/li>
&lt;li>Azure CNI&lt;/li>
&lt;li>Google cloud networking&lt;/li>
&lt;/ul>
&lt;p>下表显示了使用Calico时常见的网络选项。&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202306301203127.png" alt="image-20230630120304024" />&lt;/p>
&lt;p>网络选项&lt;/p>
&lt;h4 id="3-基本说明">
 3. 基本说明
 &lt;a class="anchor" href="#3-%e5%9f%ba%e6%9c%ac%e8%af%b4%e6%98%8e">#&lt;/a>
&lt;/h4>
&lt;p>本节提供更多关于Calico的内置网络选项的细节:&lt;/p>
&lt;ul>
&lt;li>Unencapsulated, peered with physical infrastructure&lt;/li>
&lt;li>Unencapsulated, not peered with physical infrastructure&lt;/li>
&lt;li>IP in IP or VXLAN encapsulation&lt;/li>
&lt;/ul>
&lt;h5 id="31-unencapsulated-peered-with-physical-infrastructure">
 3.1 Unencapsulated, peered with physical infrastructure
 &lt;a class="anchor" href="#31-unencapsulated-peered-with-physical-infrastructure">#&lt;/a>
&lt;/h5>
&lt;p>Calico可以与你的路由器使用BGP对等。这提供了出色的性能和易于调试的非封装流量，以及广泛的网络拓扑和连接选项。&lt;/p></description></item><item><title>2024-04-03 CICD思考</title><link>https://qq547475331.github.io/docs/cicd%E6%80%9D%E8%80%83-cicd-si-kao/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/cicd%E6%80%9D%E8%80%83-cicd-si-kao/</guid><description>&lt;h5 id="本文旨在介绍zbj-devops团队倾力打造的devops平台中关于cicd流水线部分的实践历经三次大版本迭代更新的流水线完美切合zbj各种业务发展需求在满足高频率交付的同时提高了研发效率降低了研发成本保证了交付质量">
 本文旨在介绍ZBJ DevOps团队倾力打造的DevOps平台中关于CI/CD流水线部分的实践。历经三次大版本迭代更新的流水线，完美切合ZBJ各种业务发展需求，在满足高频率交付的同时，提高了研发效率，降低了研发成本，保证了交付质量。
 &lt;a class="anchor" href="#%e6%9c%ac%e6%96%87%e6%97%a8%e5%9c%a8%e4%bb%8b%e7%bb%8dzbj-devops%e5%9b%a2%e9%98%9f%e5%80%be%e5%8a%9b%e6%89%93%e9%80%a0%e7%9a%84devops%e5%b9%b3%e5%8f%b0%e4%b8%ad%e5%85%b3%e4%ba%8ecicd%e6%b5%81%e6%b0%b4%e7%ba%bf%e9%83%a8%e5%88%86%e7%9a%84%e5%ae%9e%e8%b7%b5%e5%8e%86%e7%bb%8f%e4%b8%89%e6%ac%a1%e5%a4%a7%e7%89%88%e6%9c%ac%e8%bf%ad%e4%bb%a3%e6%9b%b4%e6%96%b0%e7%9a%84%e6%b5%81%e6%b0%b4%e7%ba%bf%e5%ae%8c%e7%be%8e%e5%88%87%e5%90%88zbj%e5%90%84%e7%a7%8d%e4%b8%9a%e5%8a%a1%e5%8f%91%e5%b1%95%e9%9c%80%e6%b1%82%e5%9c%a8%e6%bb%a1%e8%b6%b3%e9%ab%98%e9%a2%91%e7%8e%87%e4%ba%a4%e4%bb%98%e7%9a%84%e5%90%8c%e6%97%b6%e6%8f%90%e9%ab%98%e4%ba%86%e7%a0%94%e5%8f%91%e6%95%88%e7%8e%87%e9%99%8d%e4%bd%8e%e4%ba%86%e7%a0%94%e5%8f%91%e6%88%90%e6%9c%ac%e4%bf%9d%e8%af%81%e4%ba%86%e4%ba%a4%e4%bb%98%e8%b4%a8%e9%87%8f">#&lt;/a>
&lt;/h5>
&lt;h5 id="持续集成continuous-integration简称ci持续集成强调开发人员提交了新代码之后立刻进行构建单元测试根据结果我们可以确定新代码和原有代码能否正确地集成在一起持续集成过程中很重视自动化测试验证结果对可能出现的一些问题进行预警以保障最终集成的代码没有问题持续交付continuous-delivery简称cd持续交付在持续集成的基础上将集成后的代码部署到更贴近真实运行环境的类生产环境testtesting中然后交付给质量团队以供评审如果评审通过代码就进入生产阶段持续交付并不是指软件每一个改动都要尽快部署到产品环境中它指的是任何的代码修改都可以在任何时候实施部署有的人也把cd称为continuous-deployment持续部署持续部署是指当交付的代码通过评审之后可以部署到生产环境中这里需要注意的是持续部署应该是持续交付的最高阶段持续交付是一种能力持续部署是一种持续交付的表现方式">
 持续集成（Continuous Integration）简称CI，持续集成强调开发人员提交了新代码之后，立刻进行构建、（单元）测试。根据结果，我们可以确定新代码和原有代码能否正确地集成在一起。持续集成过程中很重视自动化测试验证结果，对可能出现的一些问题进行预警，以保障最终集成的代码没有问题。持续交付（Continuous Delivery）简称CD，持续交付在持续集成的基础上，将集成后的代码部署到更贴近真实运行环境的「类生产环境」（test，testing）中，然后交付给质量团队，以供评审。如果评审通过，代码就进入生产阶段。持续交付并不是指软件每一个改动都要尽快部署到产品环境中，它指的是任何的代码修改都可以在任何时候实施部署。有的人也把CD称为Continuous Deployment（持续部署），持续部署是指当交付的代码通过评审之后，可以部署到生产环境中。这里需要注意的是，持续部署应该是持续交付的最高阶段，持续交付是一种能力，持续部署是一种持续交付的表现方式。
 &lt;a class="anchor" href="#%e6%8c%81%e7%bb%ad%e9%9b%86%e6%88%90continuous-integration%e7%ae%80%e7%a7%b0ci%e6%8c%81%e7%bb%ad%e9%9b%86%e6%88%90%e5%bc%ba%e8%b0%83%e5%bc%80%e5%8f%91%e4%ba%ba%e5%91%98%e6%8f%90%e4%ba%a4%e4%ba%86%e6%96%b0%e4%bb%a3%e7%a0%81%e4%b9%8b%e5%90%8e%e7%ab%8b%e5%88%bb%e8%bf%9b%e8%a1%8c%e6%9e%84%e5%bb%ba%e5%8d%95%e5%85%83%e6%b5%8b%e8%af%95%e6%a0%b9%e6%8d%ae%e7%bb%93%e6%9e%9c%e6%88%91%e4%bb%ac%e5%8f%af%e4%bb%a5%e7%a1%ae%e5%ae%9a%e6%96%b0%e4%bb%a3%e7%a0%81%e5%92%8c%e5%8e%9f%e6%9c%89%e4%bb%a3%e7%a0%81%e8%83%bd%e5%90%a6%e6%ad%a3%e7%a1%ae%e5%9c%b0%e9%9b%86%e6%88%90%e5%9c%a8%e4%b8%80%e8%b5%b7%e6%8c%81%e7%bb%ad%e9%9b%86%e6%88%90%e8%bf%87%e7%a8%8b%e4%b8%ad%e5%be%88%e9%87%8d%e8%a7%86%e8%87%aa%e5%8a%a8%e5%8c%96%e6%b5%8b%e8%af%95%e9%aa%8c%e8%af%81%e7%bb%93%e6%9e%9c%e5%af%b9%e5%8f%af%e8%83%bd%e5%87%ba%e7%8e%b0%e7%9a%84%e4%b8%80%e4%ba%9b%e9%97%ae%e9%a2%98%e8%bf%9b%e8%a1%8c%e9%a2%84%e8%ad%a6%e4%bb%a5%e4%bf%9d%e9%9a%9c%e6%9c%80%e7%bb%88%e9%9b%86%e6%88%90%e7%9a%84%e4%bb%a3%e7%a0%81%e6%b2%a1%e6%9c%89%e9%97%ae%e9%a2%98%e6%8c%81%e7%bb%ad%e4%ba%a4%e4%bb%98continuous-delivery%e7%ae%80%e7%a7%b0cd%e6%8c%81%e7%bb%ad%e4%ba%a4%e4%bb%98%e5%9c%a8%e6%8c%81%e7%bb%ad%e9%9b%86%e6%88%90%e7%9a%84%e5%9f%ba%e7%a1%80%e4%b8%8a%e5%b0%86%e9%9b%86%e6%88%90%e5%90%8e%e7%9a%84%e4%bb%a3%e7%a0%81%e9%83%a8%e7%bd%b2%e5%88%b0%e6%9b%b4%e8%b4%b4%e8%bf%91%e7%9c%9f%e5%ae%9e%e8%bf%90%e8%a1%8c%e7%8e%af%e5%a2%83%e7%9a%84%e7%b1%bb%e7%94%9f%e4%ba%a7%e7%8e%af%e5%a2%83testtesting%e4%b8%ad%e7%84%b6%e5%90%8e%e4%ba%a4%e4%bb%98%e7%bb%99%e8%b4%a8%e9%87%8f%e5%9b%a2%e9%98%9f%e4%bb%a5%e4%be%9b%e8%af%84%e5%ae%a1%e5%a6%82%e6%9e%9c%e8%af%84%e5%ae%a1%e9%80%9a%e8%bf%87%e4%bb%a3%e7%a0%81%e5%b0%b1%e8%bf%9b%e5%85%a5%e7%94%9f%e4%ba%a7%e9%98%b6%e6%ae%b5%e6%8c%81%e7%bb%ad%e4%ba%a4%e4%bb%98%e5%b9%b6%e4%b8%8d%e6%98%af%e6%8c%87%e8%bd%af%e4%bb%b6%e6%af%8f%e4%b8%80%e4%b8%aa%e6%94%b9%e5%8a%a8%e9%83%bd%e8%a6%81%e5%b0%bd%e5%bf%ab%e9%83%a8%e7%bd%b2%e5%88%b0%e4%ba%a7%e5%93%81%e7%8e%af%e5%a2%83%e4%b8%ad%e5%ae%83%e6%8c%87%e7%9a%84%e6%98%af%e4%bb%bb%e4%bd%95%e7%9a%84%e4%bb%a3%e7%a0%81%e4%bf%ae%e6%94%b9%e9%83%bd%e5%8f%af%e4%bb%a5%e5%9c%a8%e4%bb%bb%e4%bd%95%e6%97%b6%e5%80%99%e5%ae%9e%e6%96%bd%e9%83%a8%e7%bd%b2%e6%9c%89%e7%9a%84%e4%ba%ba%e4%b9%9f%e6%8a%8acd%e7%a7%b0%e4%b8%bacontinuous-deployment%e6%8c%81%e7%bb%ad%e9%83%a8%e7%bd%b2%e6%8c%81%e7%bb%ad%e9%83%a8%e7%bd%b2%e6%98%af%e6%8c%87%e5%bd%93%e4%ba%a4%e4%bb%98%e7%9a%84%e4%bb%a3%e7%a0%81%e9%80%9a%e8%bf%87%e8%af%84%e5%ae%a1%e4%b9%8b%e5%90%8e%e5%8f%af%e4%bb%a5%e9%83%a8%e7%bd%b2%e5%88%b0%e7%94%9f%e4%ba%a7%e7%8e%af%e5%a2%83%e4%b8%ad%e8%bf%99%e9%87%8c%e9%9c%80%e8%a6%81%e6%b3%a8%e6%84%8f%e7%9a%84%e6%98%af%e6%8c%81%e7%bb%ad%e9%83%a8%e7%bd%b2%e5%ba%94%e8%af%a5%e6%98%af%e6%8c%81%e7%bb%ad%e4%ba%a4%e4%bb%98%e7%9a%84%e6%9c%80%e9%ab%98%e9%98%b6%e6%ae%b5%e6%8c%81%e7%bb%ad%e4%ba%a4%e4%bb%98%e6%98%af%e4%b8%80%e7%a7%8d%e8%83%bd%e5%8a%9b%e6%8c%81%e7%bb%ad%e9%83%a8%e7%bd%b2%e6%98%af%e4%b8%80%e7%a7%8d%e6%8c%81%e7%bb%ad%e4%ba%a4%e4%bb%98%e7%9a%84%e8%a1%a8%e7%8e%b0%e6%96%b9%e5%bc%8f">#&lt;/a>
&lt;/h5>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202402231546203.png" alt="image-20240223154614128" />&lt;/p>
&lt;h5 id="cicd过程示意图">
 CI/CD过程示意图
 &lt;a class="anchor" href="#cicd%e8%bf%87%e7%a8%8b%e7%a4%ba%e6%84%8f%e5%9b%be">#&lt;/a>
&lt;/h5>
&lt;p>&lt;strong>背景介绍&lt;/strong>&lt;/p>
&lt;h5 id="在提到zbj-devops流水线之前先交代一下历史背景2015年前猪八戒网80的项目都是用php语言开发的剩下的少部分使用的是nodejs和java2015年zbj研发中心进行了自发性的工业革命腾云七号行动使用java语言将核心业务代码进行了重构和拆解建立了以dubbo为核心的soa微服务框架使用zookeeperswoole为核心的业务调用提供机制满足新业务使用java语言编写老业务仍然使用php编写同时支持两种语言nodejsphp调用dubbo服务的能力">
 在提到ZBJ DevOps流水线之前，先交代一下历史背景。2015年前，猪八戒网80%的项目都是用PHP语言开发的，剩下的少部分使用的是Nodejs和Java。2015年，ZBJ研发中心进行了自发性的“工业革命”——腾云七号行动——使用Java语言将核心业务代码进行了重构和拆解，建立了以Dubbo为核心的SOA微服务框架，使用ZooKeeper+Swoole为核心的业务调用提供机制。满足新业务使用Java语言编写、老业务仍然使用PHP编写，同时支持两种语言（Nodejs&amp;amp;PHP）调用Dubbo服务的能力。
 &lt;a class="anchor" href="#%e5%9c%a8%e6%8f%90%e5%88%b0zbj-devops%e6%b5%81%e6%b0%b4%e7%ba%bf%e4%b9%8b%e5%89%8d%e5%85%88%e4%ba%a4%e4%bb%a3%e4%b8%80%e4%b8%8b%e5%8e%86%e5%8f%b2%e8%83%8c%e6%99%af2015%e5%b9%b4%e5%89%8d%e7%8c%aa%e5%85%ab%e6%88%92%e7%bd%9180%e7%9a%84%e9%a1%b9%e7%9b%ae%e9%83%bd%e6%98%af%e7%94%a8php%e8%af%ad%e8%a8%80%e5%bc%80%e5%8f%91%e7%9a%84%e5%89%a9%e4%b8%8b%e7%9a%84%e5%b0%91%e9%83%a8%e5%88%86%e4%bd%bf%e7%94%a8%e7%9a%84%e6%98%afnodejs%e5%92%8cjava2015%e5%b9%b4zbj%e7%a0%94%e5%8f%91%e4%b8%ad%e5%bf%83%e8%bf%9b%e8%a1%8c%e4%ba%86%e8%87%aa%e5%8f%91%e6%80%a7%e7%9a%84%e5%b7%a5%e4%b8%9a%e9%9d%a9%e5%91%bd%e8%85%be%e4%ba%91%e4%b8%83%e5%8f%b7%e8%a1%8c%e5%8a%a8%e4%bd%bf%e7%94%a8java%e8%af%ad%e8%a8%80%e5%b0%86%e6%a0%b8%e5%bf%83%e4%b8%9a%e5%8a%a1%e4%bb%a3%e7%a0%81%e8%bf%9b%e8%a1%8c%e4%ba%86%e9%87%8d%e6%9e%84%e5%92%8c%e6%8b%86%e8%a7%a3%e5%bb%ba%e7%ab%8b%e4%ba%86%e4%bb%a5dubbo%e4%b8%ba%e6%a0%b8%e5%bf%83%e7%9a%84soa%e5%be%ae%e6%9c%8d%e5%8a%a1%e6%a1%86%e6%9e%b6%e4%bd%bf%e7%94%a8zookeeperswoole%e4%b8%ba%e6%a0%b8%e5%bf%83%e7%9a%84%e4%b8%9a%e5%8a%a1%e8%b0%83%e7%94%a8%e6%8f%90%e4%be%9b%e6%9c%ba%e5%88%b6%e6%bb%a1%e8%b6%b3%e6%96%b0%e4%b8%9a%e5%8a%a1%e4%bd%bf%e7%94%a8java%e8%af%ad%e8%a8%80%e7%bc%96%e5%86%99%e8%80%81%e4%b8%9a%e5%8a%a1%e4%bb%8d%e7%84%b6%e4%bd%bf%e7%94%a8php%e7%bc%96%e5%86%99%e5%90%8c%e6%97%b6%e6%94%af%e6%8c%81%e4%b8%a4%e7%a7%8d%e8%af%ad%e8%a8%80nodejsphp%e8%b0%83%e7%94%a8dubbo%e6%9c%8d%e5%8a%a1%e7%9a%84%e8%83%bd%e5%8a%9b">#&lt;/a>
&lt;/h5>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202402231546898.png" alt="image-20240223154641828" />&lt;/p>
&lt;h5 id="之后开始全面推行前后端分离于是流行了沿用至今的主流架构">
 之后，开始全面推行前后端分离，于是流行了沿用至今的主流架构：
 &lt;a class="anchor" href="#%e4%b9%8b%e5%90%8e%e5%bc%80%e5%a7%8b%e5%85%a8%e9%9d%a2%e6%8e%a8%e8%a1%8c%e5%89%8d%e5%90%8e%e7%ab%af%e5%88%86%e7%a6%bb%e4%ba%8e%e6%98%af%e6%b5%81%e8%a1%8c%e4%ba%86%e6%b2%bf%e7%94%a8%e8%87%b3%e4%bb%8a%e7%9a%84%e4%b8%bb%e6%b5%81%e6%9e%b6%e6%9e%84">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="nodejs负责前端">
 Nodejs：负责前端
 &lt;a class="anchor" href="#nodejs%e8%b4%9f%e8%b4%a3%e5%89%8d%e7%ab%af">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="java负责后端">
 Java：负责后端
 &lt;a class="anchor" href="#java%e8%b4%9f%e8%b4%a3%e5%90%8e%e7%ab%af">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="php负责老项目维护">
 PHP：负责老项目维护
 &lt;a class="anchor" href="#php%e8%b4%9f%e8%b4%a3%e8%80%81%e9%a1%b9%e7%9b%ae%e7%bb%b4%e6%8a%a4">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h5 id="剩余部分小系统或者边缘化的工具使用其他语言开发或者在此三种语言基础上的一些变种">
 剩余部分小系统或者边缘化的工具使用其他语言开发，或者在此三种语言基础上的一些变种：
 &lt;a class="anchor" href="#%e5%89%a9%e4%bd%99%e9%83%a8%e5%88%86%e5%b0%8f%e7%b3%bb%e7%bb%9f%e6%88%96%e8%80%85%e8%be%b9%e7%bc%98%e5%8c%96%e7%9a%84%e5%b7%a5%e5%85%b7%e4%bd%bf%e7%94%a8%e5%85%b6%e4%bb%96%e8%af%ad%e8%a8%80%e5%bc%80%e5%8f%91%e6%88%96%e8%80%85%e5%9c%a8%e6%ad%a4%e4%b8%89%e7%a7%8d%e8%af%ad%e8%a8%80%e5%9f%ba%e7%a1%80%e4%b8%8a%e7%9a%84%e4%b8%80%e4%ba%9b%e5%8f%98%e7%a7%8d">#&lt;/a>
&lt;/h5>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202402231546923.png" alt="image-20240223154653807" />&lt;/p>
&lt;h5 id="而随着项目工程数量的快速增长交付开始变得频繁传统开发模式的一两个月交付一次远远不能满足交付要求改变迫在眉睫">
 而随着项目工程数量的快速增长，交付开始变得频繁，传统开发模式的一两个月交付一次远远不能满足交付要求，改变迫在眉睫。
 &lt;a class="anchor" href="#%e8%80%8c%e9%9a%8f%e7%9d%80%e9%a1%b9%e7%9b%ae%e5%b7%a5%e7%a8%8b%e6%95%b0%e9%87%8f%e7%9a%84%e5%bf%ab%e9%80%9f%e5%a2%9e%e9%95%bf%e4%ba%a4%e4%bb%98%e5%bc%80%e5%a7%8b%e5%8f%98%e5%be%97%e9%a2%91%e7%b9%81%e4%bc%a0%e7%bb%9f%e5%bc%80%e5%8f%91%e6%a8%a1%e5%bc%8f%e7%9a%84%e4%b8%80%e4%b8%a4%e4%b8%aa%e6%9c%88%e4%ba%a4%e4%bb%98%e4%b8%80%e6%ac%a1%e8%bf%9c%e8%bf%9c%e4%b8%8d%e8%83%bd%e6%bb%a1%e8%b6%b3%e4%ba%a4%e4%bb%98%e8%a6%81%e6%b1%82%e6%94%b9%e5%8f%98%e8%bf%ab%e5%9c%a8%e7%9c%89%e7%9d%ab">#&lt;/a>
&lt;/h5>
&lt;h5 id="2016年q3季度感受到改变迫在眉睫zbj研发中心经充分准备后决定抽调部分运维同学和开发同学组建一支名为devops取经团的团队力图打造属于zbj自己的以提高研发效率为目标的平台">
 2016年Q3季度，感受到改变迫在眉睫，ZBJ研发中心经充分准备后决定抽调部分运维同学和开发同学组建一支名为“DevOps取经团”的团队，力图打造属于ZBJ自己的以提高研发效率为目标的平台。
 &lt;a class="anchor" href="#2016%e5%b9%b4q3%e5%ad%a3%e5%ba%a6%e6%84%9f%e5%8f%97%e5%88%b0%e6%94%b9%e5%8f%98%e8%bf%ab%e5%9c%a8%e7%9c%89%e7%9d%abzbj%e7%a0%94%e5%8f%91%e4%b8%ad%e5%bf%83%e7%bb%8f%e5%85%85%e5%88%86%e5%87%86%e5%a4%87%e5%90%8e%e5%86%b3%e5%ae%9a%e6%8a%bd%e8%b0%83%e9%83%a8%e5%88%86%e8%bf%90%e7%bb%b4%e5%90%8c%e5%ad%a6%e5%92%8c%e5%bc%80%e5%8f%91%e5%90%8c%e5%ad%a6%e7%bb%84%e5%bb%ba%e4%b8%80%e6%94%af%e5%90%8d%e4%b8%badevops%e5%8f%96%e7%bb%8f%e5%9b%a2%e7%9a%84%e5%9b%a2%e9%98%9f%e5%8a%9b%e5%9b%be%e6%89%93%e9%80%a0%e5%b1%9e%e4%ba%8ezbj%e8%87%aa%e5%b7%b1%e7%9a%84%e4%bb%a5%e6%8f%90%e9%ab%98%e7%a0%94%e5%8f%91%e6%95%88%e7%8e%87%e4%b8%ba%e7%9b%ae%e6%a0%87%e7%9a%84%e5%b9%b3%e5%8f%b0">#&lt;/a>
&lt;/h5>
&lt;p>&lt;strong>ZBJ CI/CD发展史&lt;/strong>&lt;/p>
&lt;p>&lt;strong>第一阶段：2015年以前&lt;/strong>&lt;/p>
&lt;h5 id="2015年以前此时工业革命还未开始zbj所谓的流水线先后经历了大锅饭年代公交车模式">
 2015年以前，此时“工业革命”还未开始，ZBJ所谓的流水线先后经历了“大锅饭年代”、“公交车模式”。
 &lt;a class="anchor" href="#2015%e5%b9%b4%e4%bb%a5%e5%89%8d%e6%ad%a4%e6%97%b6%e5%b7%a5%e4%b8%9a%e9%9d%a9%e5%91%bd%e8%bf%98%e6%9c%aa%e5%bc%80%e5%a7%8bzbj%e6%89%80%e8%b0%93%e7%9a%84%e6%b5%81%e6%b0%b4%e7%ba%bf%e5%85%88%e5%90%8e%e7%bb%8f%e5%8e%86%e4%ba%86%e5%a4%a7%e9%94%85%e9%a5%ad%e5%b9%b4%e4%bb%a3%e5%85%ac%e4%ba%a4%e8%bd%a6%e6%a8%a1%e5%bc%8f">#&lt;/a>
&lt;/h5>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202402231547776.png" alt="image-20240223154720656" />&lt;/p>
&lt;h5 id="可以看到无论是大锅饭年代还是公交车模式都会面临很多问题">
 可以看到，无论是“大锅饭年代”，还是“公交车模式”都会面临很多问题：
 &lt;a class="anchor" href="#%e5%8f%af%e4%bb%a5%e7%9c%8b%e5%88%b0%e6%97%a0%e8%ae%ba%e6%98%af%e5%a4%a7%e9%94%85%e9%a5%ad%e5%b9%b4%e4%bb%a3%e8%bf%98%e6%98%af%e5%85%ac%e4%ba%a4%e8%bd%a6%e6%a8%a1%e5%bc%8f%e9%83%bd%e4%bc%9a%e9%9d%a2%e4%b8%b4%e5%be%88%e5%a4%9a%e9%97%ae%e9%a2%98">#&lt;/a>
&lt;/h5>
&lt;h5 id="">
 
 &lt;a class="anchor" href="#">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="项目耦合度太高容易导致合并冲突环境冲突等">
 项目耦合度太高，容易导致合并冲突，环境冲突等。
 &lt;a class="anchor" href="#%e9%a1%b9%e7%9b%ae%e8%80%a6%e5%90%88%e5%ba%a6%e5%a4%aa%e9%ab%98%e5%ae%b9%e6%98%93%e5%af%bc%e8%87%b4%e5%90%88%e5%b9%b6%e5%86%b2%e7%aa%81%e7%8e%af%e5%a2%83%e5%86%b2%e7%aa%81%e7%ad%89">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="集成过程中未对代码进行审查错误代码发布到测试环境后会影响依赖方的测试">
 集成过程中未对代码进行审查，错误代码发布到测试环境后，会影响依赖方的测试。
 &lt;a class="anchor" href="#%e9%9b%86%e6%88%90%e8%bf%87%e7%a8%8b%e4%b8%ad%e6%9c%aa%e5%af%b9%e4%bb%a3%e7%a0%81%e8%bf%9b%e8%a1%8c%e5%ae%a1%e6%9f%a5%e9%94%99%e8%af%af%e4%bb%a3%e7%a0%81%e5%8f%91%e5%b8%83%e5%88%b0%e6%b5%8b%e8%af%95%e7%8e%af%e5%a2%83%e5%90%8e%e4%bc%9a%e5%bd%b1%e5%93%8d%e4%be%9d%e8%b5%96%e6%96%b9%e7%9a%84%e6%b5%8b%e8%af%95">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="发布受限制必须在专门时间由专人发布">
 发布受限制，必须在专门时间由专人发布。
 &lt;a class="anchor" href="#%e5%8f%91%e5%b8%83%e5%8f%97%e9%99%90%e5%88%b6%e5%bf%85%e9%a1%bb%e5%9c%a8%e4%b8%93%e9%97%a8%e6%97%b6%e9%97%b4%e7%94%b1%e4%b8%93%e4%ba%ba%e5%8f%91%e5%b8%83">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="发布异常时回滚工作异常艰难">
 发布异常时，回滚工作异常艰难。
 &lt;a class="anchor" href="#%e5%8f%91%e5%b8%83%e5%bc%82%e5%b8%b8%e6%97%b6%e5%9b%9e%e6%bb%9a%e5%b7%a5%e4%bd%9c%e5%bc%82%e5%b8%b8%e8%89%b0%e9%9a%be">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>第二阶段（2016-2017）：私家车模式&lt;/strong>&lt;/p></description></item><item><title>2024-04-03 Client-go 四种客户端</title><link>https://qq547475331.github.io/docs/client-go-%E5%9B%9B%E7%A7%8D%E5%AE%A2%E6%88%B7%E7%AB%AF-client-go-si-zhong-ke-hu-duan/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/client-go-%E5%9B%9B%E7%A7%8D%E5%AE%A2%E6%88%B7%E7%AB%AF-client-go-si-zhong-ke-hu-duan/</guid><description>&lt;h2 id="简介">
 简介
 &lt;a class="anchor" href="#%e7%ae%80%e4%bb%8b">#&lt;/a>
&lt;/h2>
&lt;h5 id="client-go-共提供了-4-种与-kubernetes-apiserver-交互的客户端分别是-restclientdiscoveryclientclientsetdynamicclient">
 Client-Go 共提供了 4 种与 Kubernetes APIServer 交互的客户端。分别是 &lt;strong>RESTClient、DiscoveryClient、ClientSet、DynamicClient&lt;/strong>。
 &lt;a class="anchor" href="#client-go-%e5%85%b1%e6%8f%90%e4%be%9b%e4%ba%86-4-%e7%a7%8d%e4%b8%8e-kubernetes-apiserver-%e4%ba%a4%e4%ba%92%e7%9a%84%e5%ae%a2%e6%88%b7%e7%ab%af%e5%88%86%e5%88%ab%e6%98%af-restclientdiscoveryclientclientsetdynamicclient">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="-restclient最基础的客户端主要是对-http-请求进行了封装支持-json-和-protobuf-格式的数据">
 • **RESTClient：**最基础的客户端，主要是对 HTTP 请求进行了封装，支持 Json 和 Protobuf 格式的数据。
 &lt;a class="anchor" href="#-restclient%e6%9c%80%e5%9f%ba%e7%a1%80%e7%9a%84%e5%ae%a2%e6%88%b7%e7%ab%af%e4%b8%bb%e8%a6%81%e6%98%af%e5%af%b9-http-%e8%af%b7%e6%b1%82%e8%bf%9b%e8%a1%8c%e4%ba%86%e5%b0%81%e8%a3%85%e6%94%af%e6%8c%81-json-%e5%92%8c-protobuf-%e6%a0%bc%e5%bc%8f%e7%9a%84%e6%95%b0%e6%8d%ae">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-discoveryclient发现客户端负责发现-apiserver-支持的资源组资源版本和资源信息的">
 • **DiscoveryClient：**发现客户端，负责发现 APIServer 支持的资源组、资源版本和资源信息的。
 &lt;a class="anchor" href="#-discoveryclient%e5%8f%91%e7%8e%b0%e5%ae%a2%e6%88%b7%e7%ab%af%e8%b4%9f%e8%b4%a3%e5%8f%91%e7%8e%b0-apiserver-%e6%94%af%e6%8c%81%e7%9a%84%e8%b5%84%e6%ba%90%e7%bb%84%e8%b5%84%e6%ba%90%e7%89%88%e6%9c%ac%e5%92%8c%e8%b5%84%e6%ba%90%e4%bf%a1%e6%81%af%e7%9a%84">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-clientset负责操作-kubernetes-内置的资源对象例如podservice等">
 • **ClientSet：**负责操作 Kubernetes 内置的资源对象，例如：Pod、Service等。
 &lt;a class="anchor" href="#-clientset%e8%b4%9f%e8%b4%a3%e6%93%8d%e4%bd%9c-kubernetes-%e5%86%85%e7%bd%ae%e7%9a%84%e8%b5%84%e6%ba%90%e5%af%b9%e8%b1%a1%e4%be%8b%e5%a6%82podservice%e7%ad%89">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-dynamicclient动态客户端可以对任意的-kubernetes-资源对象进行通用操作包括-crd">
 • **DynamicClient：**动态客户端，可以对任意的 Kubernetes 资源对象进行通用操作，包括 CRD。
 &lt;a class="anchor" href="#-dynamicclient%e5%8a%a8%e6%80%81%e5%ae%a2%e6%88%b7%e7%ab%af%e5%8f%af%e4%bb%a5%e5%af%b9%e4%bb%bb%e6%84%8f%e7%9a%84-kubernetes-%e8%b5%84%e6%ba%90%e5%af%b9%e8%b1%a1%e8%bf%9b%e8%a1%8c%e9%80%9a%e7%94%a8%e6%93%8d%e4%bd%9c%e5%8c%85%e6%8b%ac-crd">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403191503211.png" alt="image-20240319150324140" />&lt;/p>
&lt;hr>
&lt;h2 id="restclient">
 RESTClient
 &lt;a class="anchor" href="#restclient">#&lt;/a>
&lt;/h2>
&lt;h5 id="上图可以看出-restclient-是所有-client-的父类">
 上图可以看出 &lt;strong>RESTClient&lt;/strong> 是所有 Client 的父类
 &lt;a class="anchor" href="#%e4%b8%8a%e5%9b%be%e5%8f%af%e4%bb%a5%e7%9c%8b%e5%87%ba-restclient-%e6%98%af%e6%89%80%e6%9c%89-client-%e7%9a%84%e7%88%b6%e7%b1%bb">#&lt;/a>
&lt;/h5>
&lt;h5 id="它就是对-http-request-进行了封装实现了-restful-风格的-api可以直接通过-restclient-提供的-restful-方法-getputpostdelete-操作数据">
 它就是对 HTTP Request 进行了封装，实现了 RESTFul 风格的 API，可以直接通过 RESTClient 提供的 RESTful 方法 &lt;code>GET()，PUT()，POST()，DELETE()&lt;/code> 操作数据
 &lt;a class="anchor" href="#%e5%ae%83%e5%b0%b1%e6%98%af%e5%af%b9-http-request-%e8%bf%9b%e8%a1%8c%e4%ba%86%e5%b0%81%e8%a3%85%e5%ae%9e%e7%8e%b0%e4%ba%86-restful-%e9%a3%8e%e6%a0%bc%e7%9a%84-api%e5%8f%af%e4%bb%a5%e7%9b%b4%e6%8e%a5%e9%80%9a%e8%bf%87-restclient-%e6%8f%90%e4%be%9b%e7%9a%84-restful-%e6%96%b9%e6%b3%95-getputpostdelete-%e6%93%8d%e4%bd%9c%e6%95%b0%e6%8d%ae">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="-同时支持-json-和-protobuf">
 • 同时支持 json 和 protobuf
 &lt;a class="anchor" href="#-%e5%90%8c%e6%97%b6%e6%94%af%e6%8c%81-json-%e5%92%8c-protobuf">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-支持所有原生资源和-crd">
 • 支持所有原生资源和 CRD
 &lt;a class="anchor" href="#-%e6%94%af%e6%8c%81%e6%89%80%e6%9c%89%e5%8e%9f%e7%94%9f%e8%b5%84%e6%ba%90%e5%92%8c-crd">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h3 id="示例">
 示例
 &lt;a class="anchor" href="#%e7%a4%ba%e4%be%8b">#&lt;/a>
&lt;/h3>
&lt;h5 id="使用-restclient-获取-k8s-集群-pod-资源">
 使用 &lt;strong>RESTClient&lt;/strong> 获取 K8S 集群 pod 资源
 &lt;a class="anchor" href="#%e4%bd%bf%e7%94%a8-restclient-%e8%8e%b7%e5%8f%96-k8s-%e9%9b%86%e7%be%a4-pod-%e8%b5%84%e6%ba%90">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>package main

import (
 &amp;#34;context&amp;#34;
 &amp;#34;flag&amp;#34;
 &amp;#34;fmt&amp;#34;
 &amp;#34;os&amp;#34;
 &amp;#34;path/filepath&amp;#34;

 corev1 &amp;#34;k8s.io/api/core/v1&amp;#34;
 &amp;#34;k8s.io/client-go/kubernetes/scheme&amp;#34;
 &amp;#34;k8s.io/client-go/rest&amp;#34;
 &amp;#34;k8s.io/client-go/tools/clientcmd&amp;#34;
)

// 获取系统家目录
func homeDir() string {
 if h := os.Getenv(&amp;#34;HOME&amp;#34;); h != &amp;#34;&amp;#34; {
 return h
 }

 // for windows
 return os.Getenv(&amp;#34;USERPROFILE&amp;#34;)
}

func main() {
 var kubeConfig *string
 var err error
 var config *rest.Config

 // 获取 kubeconfig 文件路径
 if h := homeDir(); h != &amp;#34;&amp;#34; {
 kubeConfig = flag.String(&amp;#34;kubeConfig&amp;#34;, filepath.Join(h, &amp;#34;.kube&amp;#34;, &amp;#34;config&amp;#34;), &amp;#34;use kubeconfig to access kube-apiserver&amp;#34;)
 } else {
 kubeConfig = flag.String(&amp;#34;kubeConfig&amp;#34;, &amp;#34;&amp;#34;, &amp;#34;use kubeconfig to access kube-apiserver&amp;#34;)
 }
 flag.Parse()

 // 获取 kubeconfig
 config, err = clientcmd.BuildConfigFromFlags(&amp;#34;&amp;#34;, *kubeConfig)
 if err != nil {
 panic(err.Error())
 }

 // 使用 RESTClient 需要开发者自行设置资源 URL
 // pod 资源没有 group，在核心组，所以前缀是 api
 config.APIPath = &amp;#34;api&amp;#34;
 // 设置 corev1 groupVersion
 config.GroupVersion = &amp;amp;corev1.SchemeGroupVersion
 // 设置解析器，用于用于解析 scheme
 config.NegotiatedSerializer = scheme.Codecs.WithoutConversion()
 // 初始化 RESTClient
 restClient, err := rest.RESTClientFor(config)
 if err != nil {
 panic(err.Error())
 }
 // 调用结果用 podList 解析
 result := &amp;amp;corev1.PodList{}
 // 获取 kube-system 命名空间的 pod
 namespace := &amp;#34;kube-system&amp;#34;
 // 链式调用 RESTClient 方法获取，并将结果解析到 corev1.PodList{}
 err = restClient.Get().Namespace(namespace).Resource(&amp;#34;pods&amp;#34;).Do(context.TODO()).Into(result)
 if err != nil {
 panic(err.Error())
 }

 // 打印结果
 for _, pod := range result.Items {
 fmt.Printf(&amp;#34;namespace: %s, pod: %s\n&amp;#34;, pod.Namespace, pod.Name)
 }
}
&lt;/code>&lt;/pre>&lt;h5 id="程序结果如下">
 程序结果如下：
 &lt;a class="anchor" href="#%e7%a8%8b%e5%ba%8f%e7%bb%93%e6%9e%9c%e5%a6%82%e4%b8%8b">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>namespace: kube-system, pod: coredns-697ddfb55c-5lk74
namespace: kube-system, pod: coredns-697ddfb55c-nnkhp
namespace: kube-system, pod: etcd-master-172-31-97-104
namespace: kube-system, pod: kube-apiserver-master-172-31-97-104
namespace: kube-system, pod: kube-controller-manager-master-172-31-97-104
namespace: kube-system, pod: kube-lvscare-node-172-31-97-105
namespace: kube-system, pod: kube-proxy-49k8k
namespace: kube-system, pod: kube-proxy-fvf57
namespace: kube-system, pod: kube-scheduler-master-172-31-97-104
namespace: kube-system, pod: metrics-server-7f6f9649f9-qvvj8
&lt;/code>&lt;/pre>&lt;h3 id="restclient-原理">
 RESTClient 原理
 &lt;a class="anchor" href="#restclient-%e5%8e%9f%e7%90%86">#&lt;/a>
&lt;/h3>
&lt;h5 id="初始化-restclient可以发现对原生-http-库进行了封装了">
 初始化 &lt;strong>RESTClient&lt;/strong>，可以发现对原生 HTTP 库进行了封装了
 &lt;a class="anchor" href="#%e5%88%9d%e5%a7%8b%e5%8c%96-restclient%e5%8f%af%e4%bb%a5%e5%8f%91%e7%8e%b0%e5%af%b9%e5%8e%9f%e7%94%9f-http-%e5%ba%93%e8%bf%9b%e8%a1%8c%e4%ba%86%e5%b0%81%e8%a3%85%e4%ba%86">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>func RESTClientFor(config *Config) (*RESTClient, error) {
 if config.GroupVersion == nil {
 return nil, fmt.Errorf(&amp;#34;GroupVersion is required when initializing a RESTClient&amp;#34;)
 }
 if config.NegotiatedSerializer == nil {
 return nil, fmt.Errorf(&amp;#34;NegotiatedSerializer is required when initializing a RESTClient&amp;#34;)
 }

 // Validate config.Host before constructing the transport/client so we can fail fast.
 // ServerURL will be obtained later in RESTClientForConfigAndClient()
 _, _, err := defaultServerUrlFor(config)
 if err != nil {
 return nil, err
 }

 // 获取原生 http client
 httpClient, err := HTTPClientFor(config)
 if err != nil {
 return nil, err
 }
 
 // 初始化 RESTClient
 return RESTClientForConfigAndClient(config, httpClient)
}
&lt;/code>&lt;/pre>&lt;h5 id="restclient-实现了-interface-接口">
 &lt;strong>RESTClient&lt;/strong> 实现了 &lt;code>Interface&lt;/code> 接口
 &lt;a class="anchor" href="#restclient-%e5%ae%9e%e7%8e%b0%e4%ba%86-interface-%e6%8e%a5%e5%8f%a3">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>type Interface interface {
 GetRateLimiter() flowcontrol.RateLimiter
 Verb(verb string) *Request
 Post() *Request
 Put() *Request
 Patch(pt types.PatchType) *Request
 Get() *Request
 Delete() *Request
 APIVersion() schema.GroupVersion
}
&lt;/code>&lt;/pre>&lt;h5 id="restclient-的链式调用主要是设置-namespace资源-name一些选择器等最终调用-do-方法网络调用">
 &lt;strong>RESTClient&lt;/strong> 的链式调用主要是设置 namespace，资源 name，一些选择器等，最终调用 &lt;code>Do()&lt;/code> 方法网络调用
 &lt;a class="anchor" href="#restclient-%e7%9a%84%e9%93%be%e5%bc%8f%e8%b0%83%e7%94%a8%e4%b8%bb%e8%a6%81%e6%98%af%e8%ae%be%e7%bd%ae-namespace%e8%b5%84%e6%ba%90-name%e4%b8%80%e4%ba%9b%e9%80%89%e6%8b%a9%e5%99%a8%e7%ad%89%e6%9c%80%e7%bb%88%e8%b0%83%e7%94%a8-do-%e6%96%b9%e6%b3%95%e7%bd%91%e7%bb%9c%e8%b0%83%e7%94%a8">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>func (r *Request) Do(ctx context.Context) Result {
 var result Result
 err := r.request(ctx, func(req *http.Request, resp *http.Response) {
 result = r.transformResponse(resp, req)
 })
 if err != nil {
 return Result{err: err}
 }
 return result
}

func (r *Request) request(ctx context.Context, fn func(*http.Request, *http.Response)) error {
 //Metrics for total request latency
 start := time.Now()
 defer func() {
 metrics.RequestLatency.Observe(ctx, r.verb, r.finalURLTemplate(), time.Since(start))
 }()

 if r.err != nil {
 klog.V(4).Infof(&amp;#34;Error in request: %v&amp;#34;, r.err)
 return r.err
 }

 if err := r.requestPreflightCheck(); err != nil {
 return err
 }

 client := r.c.Client
 if client == nil {
 client = http.DefaultClient
 }

 // Throttle the first try before setting up the timeout configured on the
 // client. We don&amp;#39;t want a throttled client to return timeouts to callers
 // before it makes a single request.
 if err := r.tryThrottle(ctx); err != nil {
 return err
 }

 if r.timeout &amp;gt; 0 {
 var cancel context.CancelFunc
 ctx, cancel = context.WithTimeout(ctx, r.timeout)
 defer cancel()
 }

 // Right now we make about ten retry attempts if we get a Retry-After response.
 var retryAfter *RetryAfter
 for {
 // 初始化网络请求
 req, err := r.newHTTPRequest(ctx)
 if err != nil {
 return err
 }

 r.backoff.Sleep(r.backoff.CalculateBackoff(r.URL()))
 if retryAfter != nil {
 // We are retrying the request that we already send to apiserver
 // at least once before.
 // This request should also be throttled with the client-internal rate limiter.
 if err := r.tryThrottleWithInfo(ctx, retryAfter.Reason); err != nil {
 return err
 }
 retryAfter = nil
 }
 // 发起网络调用
 resp, err := client.Do(req)
 updateURLMetrics(ctx, r, resp, err)
 if err != nil {
 r.backoff.UpdateBackoff(r.URL(), err, 0)
 } else {
 r.backoff.UpdateBackoff(r.URL(), err, resp.StatusCode)
 }

 done := func() bool {
 defer readAndCloseResponseBody(resp)

 // if the the server returns an error in err, the response will be nil.
 f := func(req *http.Request, resp *http.Response) {
 if resp == nil {
 return
 }
 fn(req, resp)
 }

 var retry bool
 retryAfter, retry = r.retry.NextRetry(req, resp, err, func(req *http.Request, err error) bool {
 // &amp;#34;Connection reset by peer&amp;#34; or &amp;#34;apiserver is shutting down&amp;#34; are usually a transient errors.
 // Thus in case of &amp;#34;GET&amp;#34; operations, we simply retry it.
 // We are not automatically retrying &amp;#34;write&amp;#34; operations, as they are not idempotent.
 if r.verb != &amp;#34;GET&amp;#34; {
 return false
 }
 // For connection errors and apiserver shutdown errors retry.
 if net.IsConnectionReset(err) || net.IsProbableEOF(err) {
 return true
 }
 return false
 })
 if retry {
 err := r.retry.BeforeNextRetry(ctx, r.backoff, retryAfter, req.URL.String(), r.body)
 if err == nil {
 return false
 }
 klog.V(4).Infof(&amp;#34;Could not retry request - %v&amp;#34;, err)
 }

 f(req, resp)
 return true
 }()
 if done {
 return err
 }
 }
}
&lt;/code>&lt;/pre>&lt;hr>
&lt;h2 id="clientset">
 ClientSet
 &lt;a class="anchor" href="#clientset">#&lt;/a>
&lt;/h2>
&lt;h5 id="clientset-在调用-kubernetes-内置资源非常常用但是无法操作自定义资源需要实现自定义资源的-clientset-才能操作">
 &lt;strong>ClientSet&lt;/strong> 在调用 Kubernetes 内置资源非常常用，但是无法操作自定义资源，需要实现自定义资源的 &lt;strong>ClientSet&lt;/strong> 才能操作。
 &lt;a class="anchor" href="#clientset-%e5%9c%a8%e8%b0%83%e7%94%a8-kubernetes-%e5%86%85%e7%bd%ae%e8%b5%84%e6%ba%90%e9%9d%9e%e5%b8%b8%e5%b8%b8%e7%94%a8%e4%bd%86%e6%98%af%e6%97%a0%e6%b3%95%e6%93%8d%e4%bd%9c%e8%87%aa%e5%ae%9a%e4%b9%89%e8%b5%84%e6%ba%90%e9%9c%80%e8%a6%81%e5%ae%9e%e7%8e%b0%e8%87%aa%e5%ae%9a%e4%b9%89%e8%b5%84%e6%ba%90%e7%9a%84-clientset-%e6%89%8d%e8%83%bd%e6%93%8d%e4%bd%9c">#&lt;/a>
&lt;/h5>
&lt;h5 id="clientset-是在-restclient-的基础上封装了对-resource-和-version-的管理方法client-go-对-kubernetes-每一个内置资源都封装了-client而-clientset-就是多个-client-的集合">
 &lt;strong>ClientSet&lt;/strong> 是在 &lt;strong>RESTClient&lt;/strong> 的基础上封装了对 Resource 和 Version 的管理方法，Client-go 对 Kubernetes 每一个&lt;strong>内置资源&lt;/strong>都封装了 Client，而 &lt;strong>ClientSet&lt;/strong> 就是多个 Client 的集合。
 &lt;a class="anchor" href="#clientset-%e6%98%af%e5%9c%a8-restclient-%e7%9a%84%e5%9f%ba%e7%a1%80%e4%b8%8a%e5%b0%81%e8%a3%85%e4%ba%86%e5%af%b9-resource-%e5%92%8c-version-%e7%9a%84%e7%ae%a1%e7%90%86%e6%96%b9%e6%b3%95client-go-%e5%af%b9-kubernetes-%e6%af%8f%e4%b8%80%e4%b8%aa%e5%86%85%e7%bd%ae%e8%b5%84%e6%ba%90%e9%83%bd%e5%b0%81%e8%a3%85%e4%ba%86-client%e8%80%8c-clientset-%e5%b0%b1%e6%98%af%e5%a4%9a%e4%b8%aa-client-%e7%9a%84%e9%9b%86%e5%90%88">#&lt;/a>
&lt;/h5>
&lt;h3 id="示例-1">
 示例
 &lt;a class="anchor" href="#%e7%a4%ba%e4%be%8b-1">#&lt;/a>
&lt;/h3>
&lt;h5 id="使用-clientset-获取-k8s-集群-pod-资源">
 使用 &lt;strong>ClientSet&lt;/strong> 获取 K8S 集群 pod 资源
 &lt;a class="anchor" href="#%e4%bd%bf%e7%94%a8-clientset-%e8%8e%b7%e5%8f%96-k8s-%e9%9b%86%e7%be%a4-pod-%e8%b5%84%e6%ba%90">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>package main

import (
 &amp;#34;context&amp;#34;
 &amp;#34;flag&amp;#34;
 &amp;#34;fmt&amp;#34;
 &amp;#34;os&amp;#34;
 &amp;#34;path/filepath&amp;#34;

 metav1 &amp;#34;k8s.io/apimachinery/pkg/apis/meta/v1&amp;#34;
 &amp;#34;k8s.io/client-go/kubernetes&amp;#34;
 &amp;#34;k8s.io/client-go/tools/clientcmd&amp;#34;
)

func homeDir() string {
 if h := os.Getenv(&amp;#34;HOME&amp;#34;); h != &amp;#34;&amp;#34; {
 return h
 }

 return os.Getenv(&amp;#34;USERROFILE&amp;#34;)
}

func main() {
 var kubeConfig *string

 if h := homeDir(); h != &amp;#34;&amp;#34; {
 kubeConfig = flag.String(&amp;#34;kubeConfig&amp;#34;, filepath.Join(h, &amp;#34;.kube&amp;#34;, &amp;#34;config&amp;#34;), &amp;#34;use kubeconfig to access kube-apiserver&amp;#34;)
 } else {
 kubeConfig = flag.String(&amp;#34;kubeConfig&amp;#34;, &amp;#34;&amp;#34;, &amp;#34;use kubeconfig to access kube-apiserver&amp;#34;)
 }
 flag.Parse()

 config, err := clientcmd.BuildConfigFromFlags(&amp;#34;&amp;#34;, *kubeConfig)
 if err != nil {
 panic(err.Error())
 }

 // 获取 clientSet
 clientSet, err := kubernetes.NewForConfig(config)
 if err != nil {
 panic(err.Error())
 }

 namespace := &amp;#34;kube-system&amp;#34;
 // 链式调用 ClientSet 获取 pod 列表
 podList, err := clientSet.CoreV1().Pods(namespace).List(context.TODO(), metav1.ListOptions{})
 if err != nil {
 panic(err.Error())
 }

 for _, pod := range podList.Items {
 fmt.Printf(&amp;#34;namespace: %s, pod: %s\n&amp;#34;, pod.Namespace, pod.Name)
 }
}
&lt;/code>&lt;/pre>&lt;h3 id="clientset-原理">
 ClientSet 原理
 &lt;a class="anchor" href="#clientset-%e5%8e%9f%e7%90%86">#&lt;/a>
&lt;/h3>
&lt;h5 id="newforconfig-获取-clientset">
 &lt;code>NewForConfig&lt;/code> 获取 &lt;strong>ClientSet&lt;/strong>
 &lt;a class="anchor" href="#newforconfig-%e8%8e%b7%e5%8f%96-clientset">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>// k8s.io/client-go/kubernetes/clientset.go:413
func NewForConfig(c *rest.Config) (*Clientset, error) {
 configShallowCopy := *c

 // share the transport between all clients
 httpClient, err := rest.HTTPClientFor(&amp;amp;configShallowCopy)
 if err != nil {
 return nil, err
 }

 return NewForConfigAndClient(&amp;amp;configShallowCopy, httpClient)
}
&lt;/code>&lt;/pre>&lt;h5 id="newforconfigandclient-获取每个-groupversion-下的资源-client">
 &lt;code>NewForConfigAndClient&lt;/code> 获取每个 &lt;code>groupVersion&lt;/code> 下的资源 Client
 &lt;a class="anchor" href="#newforconfigandclient-%e8%8e%b7%e5%8f%96%e6%af%8f%e4%b8%aa-groupversion-%e4%b8%8b%e7%9a%84%e8%b5%84%e6%ba%90-client">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>func NewForConfigAndClient(c *rest.Config, httpClient *http.Client) (*Clientset, error) {
 configShallowCopy := *c
 if configShallowCopy.RateLimiter == nil &amp;amp;&amp;amp; configShallowCopy.QPS &amp;gt; 0 {
 if configShallowCopy.Burst &amp;lt;= 0 {
 return nil, fmt.Errorf(&amp;#34;burst is required to be greater than 0 when RateLimiter is not set and QPS is set to greater than 0&amp;#34;)
 }
 configShallowCopy.RateLimiter = flowcontrol.NewTokenBucketRateLimiter(configShallowCopy.QPS, configShallowCopy.Burst)
 }

 var cs Clientset
 var err error
 cs.admissionregistrationV1, err = admissionregistrationv1.NewForConfigAndClient(&amp;amp;configShallowCopy, httpClient)
 if err != nil {
 return nil, err
 }
 cs.admissionregistrationV1beta1, err = admissionregistrationv1beta1.NewForConfigAndClient(&amp;amp;configShallowCopy, httpClient)
 if err != nil {
 return nil, err
 }
 ...
 return &amp;amp;cs, nil
}
&lt;/code>&lt;/pre>&lt;h5 id="拿-admissionregistrationv1newforconfigandclient-介绍">
 拿 &lt;code>admissionregistrationv1.NewForConfigAndClient&lt;/code> 介绍
 &lt;a class="anchor" href="#%e6%8b%bf-admissionregistrationv1newforconfigandclient-%e4%bb%8b%e7%bb%8d">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>func NewForConfigAndClient(c *rest.Config, h *http.Client) (*AdmissionregistrationV1Client, error) {
 config := *c
 // 设置 client 参数
 if err := setConfigDefaults(&amp;amp;config); err != nil {
 return nil, err
 }
 // 最终调用 RESTClientForConfigAndClient 生成 RESTClient
 client, err := rest.RESTClientForConfigAndClient(&amp;amp;config, h)
 if err != nil {
 return nil, err
 }
 return &amp;amp;AdmissionregistrationV1Client{client}, nil
}

// 可以发现，这些参数跟上面 RESTClient 差不多
func setConfigDefaults(config *rest.Config) error {
 gv := v1.SchemeGroupVersion
 config.GroupVersion = &amp;amp;gv
 config.APIPath = &amp;#34;/apis&amp;#34;
 config.NegotiatedSerializer = scheme.Codecs.WithoutConversion()

 if config.UserAgent == &amp;#34;&amp;#34; {
 config.UserAgent = rest.DefaultKubernetesUserAgent()
 }

 return nil
}
&lt;/code>&lt;/pre>&lt;h5 id="pod-资源实现了一系列方法比如-list可以发现最终调用-restclient-的方法">
 pod 资源实现了一系列方法，比如 &lt;code>List()&lt;/code>，可以发现最终调用 &lt;strong>RESTClient&lt;/strong> 的方法
 &lt;a class="anchor" href="#pod-%e8%b5%84%e6%ba%90%e5%ae%9e%e7%8e%b0%e4%ba%86%e4%b8%80%e7%b3%bb%e5%88%97%e6%96%b9%e6%b3%95%e6%af%94%e5%a6%82-list%e5%8f%af%e4%bb%a5%e5%8f%91%e7%8e%b0%e6%9c%80%e7%bb%88%e8%b0%83%e7%94%a8-restclient-%e7%9a%84%e6%96%b9%e6%b3%95">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>func (c *pods) List(ctx context.Context, opts metav1.ListOptions) (result *v1.PodList, err error) {
 var timeout time.Duration
 if opts.TimeoutSeconds != nil {
 timeout = time.Duration(*opts.TimeoutSeconds) * time.Second
 }
 result = &amp;amp;v1.PodList{}
 err = c.client.Get().
 Namespace(c.ns).
 Resource(&amp;#34;pods&amp;#34;).
 VersionedParams(&amp;amp;opts, scheme.ParameterCodec).
 Timeout(timeout).
 Do(ctx).
 Into(result)
 return
}
&lt;/code>&lt;/pre>&lt;hr>
&lt;h2 id="dynamicclient">
 DynamicClient
 &lt;a class="anchor" href="#dynamicclient">#&lt;/a>
&lt;/h2>
&lt;h5 id="dynamicclient-见名知意一种动态客户端通过动态指定资源组资源版本和资源信息来操作任意的-kubernetes-资源对象dynamicclient-不仅能操作-kubernetes-内置资源还能操作-crd-">
 &lt;strong>DynamicClient&lt;/strong> 见名知意，一种动态客户端，通过动态指定资源组，资源版本和资源信息，来操作任意的 Kubernetes 资源对象。&lt;strong>DynamicClient&lt;/strong> 不仅能操作 Kubernetes 内置资源，还能操作 CRD 。
 &lt;a class="anchor" href="#dynamicclient-%e8%a7%81%e5%90%8d%e7%9f%a5%e6%84%8f%e4%b8%80%e7%a7%8d%e5%8a%a8%e6%80%81%e5%ae%a2%e6%88%b7%e7%ab%af%e9%80%9a%e8%bf%87%e5%8a%a8%e6%80%81%e6%8c%87%e5%ae%9a%e8%b5%84%e6%ba%90%e7%bb%84%e8%b5%84%e6%ba%90%e7%89%88%e6%9c%ac%e5%92%8c%e8%b5%84%e6%ba%90%e4%bf%a1%e6%81%af%e6%9d%a5%e6%93%8d%e4%bd%9c%e4%bb%bb%e6%84%8f%e7%9a%84-kubernetes-%e8%b5%84%e6%ba%90%e5%af%b9%e8%b1%a1dynamicclient-%e4%b8%8d%e4%bb%85%e8%83%bd%e6%93%8d%e4%bd%9c-kubernetes-%e5%86%85%e7%bd%ae%e8%b5%84%e6%ba%90%e8%bf%98%e8%83%bd%e6%93%8d%e4%bd%9c-crd-">#&lt;/a>
&lt;/h5>
&lt;p>&lt;strong>DynamicClient&lt;/strong> 与 &lt;strong>ClientSet&lt;/strong> 都是对 &lt;strong>RESTClient&lt;/strong> 进行了封装&lt;/p></description></item><item><title>2024-04-03 Client-go 架构</title><link>https://qq547475331.github.io/docs/client-go-%E6%9E%B6%E6%9E%84-client-go-jia-gou/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/client-go-%E6%9E%B6%E6%9E%84-client-go-jia-gou/</guid><description>&lt;h2 id="简介">
 简介
 &lt;a class="anchor" href="#%e7%ae%80%e4%bb%8b">#&lt;/a>
&lt;/h2>
&lt;h5 id="client-go-源码位于-是-kubernetes-项目非常重要的子项目client-go-是负责与-kubernetes-apiserver-服务进行交互的客户端库利用-client-go-与kubernetes-apiserver-进行的交互访问来对-kubernetes-中的各类资源对象进行管理操作包括内置的资源对象及crd在云原生开发的项目中使用频率非常高例如开发-operatork8s-管理平台等">
 Client-go 源码位于 &lt;a href="https://github.com/kubernetes/client-go">https://github.com/kubernetes/client-go&lt;/a>，是 Kubernetes 项目非常重要的子项目，Client-Go 是负责与 Kubernetes APIServer 服务进行交互的客户端库，利用 Client-Go 与Kubernetes APIServer 进行的交互访问，来对 Kubernetes 中的各类资源对象进行管理操作，包括内置的资源对象及CRD。在云原生开发的项目中使用频率非常高，例如开发 operator，k8s 管理平台等。
 &lt;a class="anchor" href="#client-go-%e6%ba%90%e7%a0%81%e4%bd%8d%e4%ba%8e-%e6%98%af-kubernetes-%e9%a1%b9%e7%9b%ae%e9%9d%9e%e5%b8%b8%e9%87%8d%e8%a6%81%e7%9a%84%e5%ad%90%e9%a1%b9%e7%9b%aeclient-go-%e6%98%af%e8%b4%9f%e8%b4%a3%e4%b8%8e-kubernetes-apiserver-%e6%9c%8d%e5%8a%a1%e8%bf%9b%e8%a1%8c%e4%ba%a4%e4%ba%92%e7%9a%84%e5%ae%a2%e6%88%b7%e7%ab%af%e5%ba%93%e5%88%a9%e7%94%a8-client-go-%e4%b8%8ekubernetes-apiserver-%e8%bf%9b%e8%a1%8c%e7%9a%84%e4%ba%a4%e4%ba%92%e8%ae%bf%e9%97%ae%e6%9d%a5%e5%af%b9-kubernetes-%e4%b8%ad%e7%9a%84%e5%90%84%e7%b1%bb%e8%b5%84%e6%ba%90%e5%af%b9%e8%b1%a1%e8%bf%9b%e8%a1%8c%e7%ae%a1%e7%90%86%e6%93%8d%e4%bd%9c%e5%8c%85%e6%8b%ac%e5%86%85%e7%bd%ae%e7%9a%84%e8%b5%84%e6%ba%90%e5%af%b9%e8%b1%a1%e5%8f%8acrd%e5%9c%a8%e4%ba%91%e5%8e%9f%e7%94%9f%e5%bc%80%e5%8f%91%e7%9a%84%e9%a1%b9%e7%9b%ae%e4%b8%ad%e4%bd%bf%e7%94%a8%e9%a2%91%e7%8e%87%e9%9d%9e%e5%b8%b8%e9%ab%98%e4%be%8b%e5%a6%82%e5%bc%80%e5%8f%91-operatork8s-%e7%ae%a1%e7%90%86%e5%b9%b3%e5%8f%b0%e7%ad%89">#&lt;/a>
&lt;/h5>
&lt;p>公众号&lt;/p>
&lt;hr>
&lt;h2 id="架构">
 架构
 &lt;a class="anchor" href="#%e6%9e%b6%e6%9e%84">#&lt;/a>
&lt;/h2>
&lt;h5 id="client-go-架构比较复杂极大程度运用异步处理提高服务运行效率">
 Client-go 架构比较复杂，极大程度运用异步处理，提高服务运行效率。
 &lt;a class="anchor" href="#client-go-%e6%9e%b6%e6%9e%84%e6%af%94%e8%be%83%e5%a4%8d%e6%9d%82%e6%9e%81%e5%a4%a7%e7%a8%8b%e5%ba%a6%e8%bf%90%e7%94%a8%e5%bc%82%e6%ad%a5%e5%a4%84%e7%90%86%e6%8f%90%e9%ab%98%e6%9c%8d%e5%8a%a1%e8%bf%90%e8%a1%8c%e6%95%88%e7%8e%87">#&lt;/a>
&lt;/h5>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403191512504.png" alt="image-20240319151204442" />&lt;/p>
&lt;h5 id="上图上半部分是-client-go-代码组件下半部分是需要开发者自己实现主要实现如何对资源事件进行处理但是-workqueue-也是-client-go-实现的只不过是在开发者写的程序中进行使用的">
 上图上半部分是 client-go 代码组件，下半部分是需要开发者自己实现，主要实现如何对资源事件进行处理，但是 workqueue 也是 client-go 实现的，只不过是在开发者写的程序中进行使用的。
 &lt;a class="anchor" href="#%e4%b8%8a%e5%9b%be%e4%b8%8a%e5%8d%8a%e9%83%a8%e5%88%86%e6%98%af-client-go-%e4%bb%a3%e7%a0%81%e7%bb%84%e4%bb%b6%e4%b8%8b%e5%8d%8a%e9%83%a8%e5%88%86%e6%98%af%e9%9c%80%e8%a6%81%e5%bc%80%e5%8f%91%e8%80%85%e8%87%aa%e5%b7%b1%e5%ae%9e%e7%8e%b0%e4%b8%bb%e8%a6%81%e5%ae%9e%e7%8e%b0%e5%a6%82%e4%bd%95%e5%af%b9%e8%b5%84%e6%ba%90%e4%ba%8b%e4%bb%b6%e8%bf%9b%e8%a1%8c%e5%a4%84%e7%90%86%e4%bd%86%e6%98%af-workqueue-%e4%b9%9f%e6%98%af-client-go-%e5%ae%9e%e7%8e%b0%e7%9a%84%e5%8f%aa%e4%b8%8d%e8%bf%87%e6%98%af%e5%9c%a8%e5%bc%80%e5%8f%91%e8%80%85%e5%86%99%e7%9a%84%e7%a8%8b%e5%ba%8f%e4%b8%ad%e8%bf%9b%e8%a1%8c%e4%bd%bf%e7%94%a8%e7%9a%84">#&lt;/a>
&lt;/h5>
&lt;h3 id="reflector">
 Reflector
 &lt;a class="anchor" href="#reflector">#&lt;/a>
&lt;/h3>
&lt;h5 id="reflector-用于监控watch指定的-kubernetes-资源当监控的资源发生变化时例如-add-事件update-事件delete-事件并将其资源对象存放到本地缓存-deltafifo-中">
 Reflector 用于监控（Watch）指定的 Kubernetes 资源，当监控的资源发生变化时，例如 Add 事件、Update 事件、Delete 事件，并将其资源对象存放到本地缓存 DeltaFIFO 中。
 &lt;a class="anchor" href="#reflector-%e7%94%a8%e4%ba%8e%e7%9b%91%e6%8e%a7watch%e6%8c%87%e5%ae%9a%e7%9a%84-kubernetes-%e8%b5%84%e6%ba%90%e5%bd%93%e7%9b%91%e6%8e%a7%e7%9a%84%e8%b5%84%e6%ba%90%e5%8f%91%e7%94%9f%e5%8f%98%e5%8c%96%e6%97%b6%e4%be%8b%e5%a6%82-add-%e4%ba%8b%e4%bb%b6update-%e4%ba%8b%e4%bb%b6delete-%e4%ba%8b%e4%bb%b6%e5%b9%b6%e5%b0%86%e5%85%b6%e8%b5%84%e6%ba%90%e5%af%b9%e8%b1%a1%e5%ad%98%e6%94%be%e5%88%b0%e6%9c%ac%e5%9c%b0%e7%bc%93%e5%ad%98-deltafifo-%e4%b8%ad">#&lt;/a>
&lt;/h5>
&lt;h3 id="deltafifo">
 Deltafifo
 &lt;a class="anchor" href="#deltafifo">#&lt;/a>
&lt;/h3>
&lt;h5 id="deltafifo-是一个生产者-消费者的队列生产者是-reflector消费者是-informer-pop-函数fifo-是一个先进先出的队列而-delta-是一个资源对象存储它可以保存资源对象的操作类型例如-add-操作类型update-操作类型delete-操作类型sync-操作类型等">
 DeltaFIFO 是一个生产者-消费者的队列，生产者是 Reflector，消费者是 informer Pop 函数，FIFO 是一个先进先出的队列，而 Delta 是一个资源对象存储，它可以保存资源对象的操作类型，例如 Add 操作类型、Update 操作类型、Delete 操作类型、Sync 操作类型等。
 &lt;a class="anchor" href="#deltafifo-%e6%98%af%e4%b8%80%e4%b8%aa%e7%94%9f%e4%ba%a7%e8%80%85-%e6%b6%88%e8%b4%b9%e8%80%85%e7%9a%84%e9%98%9f%e5%88%97%e7%94%9f%e4%ba%a7%e8%80%85%e6%98%af-reflector%e6%b6%88%e8%b4%b9%e8%80%85%e6%98%af-informer-pop-%e5%87%bd%e6%95%b0fifo-%e6%98%af%e4%b8%80%e4%b8%aa%e5%85%88%e8%bf%9b%e5%85%88%e5%87%ba%e7%9a%84%e9%98%9f%e5%88%97%e8%80%8c-delta-%e6%98%af%e4%b8%80%e4%b8%aa%e8%b5%84%e6%ba%90%e5%af%b9%e8%b1%a1%e5%ad%98%e5%82%a8%e5%ae%83%e5%8f%af%e4%bb%a5%e4%bf%9d%e5%ad%98%e8%b5%84%e6%ba%90%e5%af%b9%e8%b1%a1%e7%9a%84%e6%93%8d%e4%bd%9c%e7%b1%bb%e5%9e%8b%e4%be%8b%e5%a6%82-add-%e6%93%8d%e4%bd%9c%e7%b1%bb%e5%9e%8bupdate-%e6%93%8d%e4%bd%9c%e7%b1%bb%e5%9e%8bdelete-%e6%93%8d%e4%bd%9c%e7%b1%bb%e5%9e%8bsync-%e6%93%8d%e4%bd%9c%e7%b1%bb%e5%9e%8b%e7%ad%89">#&lt;/a>
&lt;/h5>
&lt;h3 id="indexer">
 Indexer
 &lt;a class="anchor" href="#indexer">#&lt;/a>
&lt;/h3>
&lt;h5 id="indexer-是-client-go-用来存储资源对象并自带索引功能的本地存储informer-从-deltafifo-中将消费出来的资源对象存储至-indexerindexer-与-etcd-集群中的数据保持完全一致这样我们就可以很方便地从本地存储中读取相应的资源对象数据而无须每次从远程-apiserver-中读取以减轻服务器的压力">
 Indexer 是 client-go 用来存储资源对象并自带索引功能的本地存储，informer 从 DeltaFIFO 中将消费出来的资源对象存储至 Indexer。Indexer 与 Etcd 集群中的数据保持完全一致。这样我们就可以很方便地从本地存储中读取相应的资源对象数据，而无须每次从远程 APIServer 中读取，以减轻服务器的压力。
 &lt;a class="anchor" href="#indexer-%e6%98%af-client-go-%e7%94%a8%e6%9d%a5%e5%ad%98%e5%82%a8%e8%b5%84%e6%ba%90%e5%af%b9%e8%b1%a1%e5%b9%b6%e8%87%aa%e5%b8%a6%e7%b4%a2%e5%bc%95%e5%8a%9f%e8%83%bd%e7%9a%84%e6%9c%ac%e5%9c%b0%e5%ad%98%e5%82%a8informer-%e4%bb%8e-deltafifo-%e4%b8%ad%e5%b0%86%e6%b6%88%e8%b4%b9%e5%87%ba%e6%9d%a5%e7%9a%84%e8%b5%84%e6%ba%90%e5%af%b9%e8%b1%a1%e5%ad%98%e5%82%a8%e8%87%b3-indexerindexer-%e4%b8%8e-etcd-%e9%9b%86%e7%be%a4%e4%b8%ad%e7%9a%84%e6%95%b0%e6%8d%ae%e4%bf%9d%e6%8c%81%e5%ae%8c%e5%85%a8%e4%b8%80%e8%87%b4%e8%bf%99%e6%a0%b7%e6%88%91%e4%bb%ac%e5%b0%b1%e5%8f%af%e4%bb%a5%e5%be%88%e6%96%b9%e4%be%bf%e5%9c%b0%e4%bb%8e%e6%9c%ac%e5%9c%b0%e5%ad%98%e5%82%a8%e4%b8%ad%e8%af%bb%e5%8f%96%e7%9b%b8%e5%ba%94%e7%9a%84%e8%b5%84%e6%ba%90%e5%af%b9%e8%b1%a1%e6%95%b0%e6%8d%ae%e8%80%8c%e6%97%a0%e9%a1%bb%e6%af%8f%e6%ac%a1%e4%bb%8e%e8%bf%9c%e7%a8%8b-apiserver-%e4%b8%ad%e8%af%bb%e5%8f%96%e4%bb%a5%e5%87%8f%e8%bd%bb%e6%9c%8d%e5%8a%a1%e5%99%a8%e7%9a%84%e5%8e%8b%e5%8a%9b">#&lt;/a>
&lt;/h5>
&lt;h3 id="informer">
 Informer
 &lt;a class="anchor" href="#informer">#&lt;/a>
&lt;/h3>
&lt;h5 id="informer-将上述三个组件协同运行起来保证整个流程串联起来是-client-go-中的大脑">
 Informer 将上述三个组件协同运行起来，保证整个流程串联起来，是 client-go 中的大脑。
 &lt;a class="anchor" href="#informer-%e5%b0%86%e4%b8%8a%e8%bf%b0%e4%b8%89%e4%b8%aa%e7%bb%84%e4%bb%b6%e5%8d%8f%e5%90%8c%e8%bf%90%e8%a1%8c%e8%b5%b7%e6%9d%a5%e4%bf%9d%e8%af%81%e6%95%b4%e4%b8%aa%e6%b5%81%e7%a8%8b%e4%b8%b2%e8%81%94%e8%b5%b7%e6%9d%a5%e6%98%af-client-go-%e4%b8%ad%e7%9a%84%e5%a4%a7%e8%84%91">#&lt;/a>
&lt;/h5>
&lt;h3 id="workqueue">
 workqueue
 &lt;a class="anchor" href="#workqueue">#&lt;/a>
&lt;/h3>
&lt;h5 id="workqueue-是一个先进先出的队列informer-将事件获取到并不及时处理先将事件-push-到-workqueue-中然后再从-workqueue-消费处理大大提高运行效率">
 Workqueue 是一个先进先出的队列，informer 将事件获取到并不及时处理，先将事件 push 到 workqueue 中，然后再从 workqueue 消费处理。大大提高运行效率
 &lt;a class="anchor" href="#workqueue-%e6%98%af%e4%b8%80%e4%b8%aa%e5%85%88%e8%bf%9b%e5%85%88%e5%87%ba%e7%9a%84%e9%98%9f%e5%88%97informer-%e5%b0%86%e4%ba%8b%e4%bb%b6%e8%8e%b7%e5%8f%96%e5%88%b0%e5%b9%b6%e4%b8%8d%e5%8f%8a%e6%97%b6%e5%a4%84%e7%90%86%e5%85%88%e5%b0%86%e4%ba%8b%e4%bb%b6-push-%e5%88%b0-workqueue-%e4%b8%ad%e7%84%b6%e5%90%8e%e5%86%8d%e4%bb%8e-workqueue-%e6%b6%88%e8%b4%b9%e5%a4%84%e7%90%86%e5%a4%a7%e5%a4%a7%e6%8f%90%e9%ab%98%e8%bf%90%e8%a1%8c%e6%95%88%e7%8e%87">#&lt;/a>
&lt;/h5>
&lt;hr>
&lt;h2 id="运行流程">
 运行流程
 &lt;a class="anchor" href="#%e8%bf%90%e8%a1%8c%e6%b5%81%e7%a8%8b">#&lt;/a>
&lt;/h2>
&lt;h5 id="例如现在创建一个-podskubelet-中的-controller-是如何运行的k8s-中源码中也大量使用-client-go主要是大量的-controller">
 例如现在创建一个 pods，kubelet 中的 controller 是如何运行的(K8s 中源码中也大量使用 client-go，主要是大量的 controller)
 &lt;a class="anchor" href="#%e4%be%8b%e5%a6%82%e7%8e%b0%e5%9c%a8%e5%88%9b%e5%bb%ba%e4%b8%80%e4%b8%aa-podskubelet-%e4%b8%ad%e7%9a%84-controller-%e6%98%af%e5%a6%82%e4%bd%95%e8%bf%90%e8%a1%8c%e7%9a%84k8s-%e4%b8%ad%e6%ba%90%e7%a0%81%e4%b8%ad%e4%b9%9f%e5%a4%a7%e9%87%8f%e4%bd%bf%e7%94%a8-client-go%e4%b8%bb%e8%a6%81%e6%98%af%e5%a4%a7%e9%87%8f%e7%9a%84-controller">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="-初始化并启动-informerinformer-启动会初始化并启动-reflectorreflector-从-kube-apiserver-list-所有-pod-资源并-sync-到-deltafifo-中">
 • 初始化并启动 informer，informer 启动会初始化并启动 reflector，reflector 从 kube-apiserver list 所有 pod 资源，并 sync 到 Deltafifo 中。
 &lt;a class="anchor" href="#-%e5%88%9d%e5%a7%8b%e5%8c%96%e5%b9%b6%e5%90%af%e5%8a%a8-informerinformer-%e5%90%af%e5%8a%a8%e4%bc%9a%e5%88%9d%e5%a7%8b%e5%8c%96%e5%b9%b6%e5%90%af%e5%8a%a8-reflectorreflector-%e4%bb%8e-kube-apiserver-list-%e6%89%80%e6%9c%89-pod-%e8%b5%84%e6%ba%90%e5%b9%b6-sync-%e5%88%b0-deltafifo-%e4%b8%ad">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-deltafifo-存有全部-pod-资源informer-通过-pop-函数消费-deltafifo-事件并存储到-indexer-中">
 • Deltafifo 存有全部 pod 资源，informer 通过 pop 函数消费 deltafifo 事件并存储到 indexer 中。
 &lt;a class="anchor" href="#-deltafifo-%e5%ad%98%e6%9c%89%e5%85%a8%e9%83%a8-pod-%e8%b5%84%e6%ba%90informer-%e9%80%9a%e8%bf%87-pop-%e5%87%bd%e6%95%b0%e6%b6%88%e8%b4%b9-deltafifo-%e4%ba%8b%e4%bb%b6%e5%b9%b6%e5%ad%98%e5%82%a8%e5%88%b0-indexer-%e4%b8%ad">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-如果需要调用-pod-资源那么可以直接从-indexer-中获取">
 • 如果需要调用 pod 资源，那么可以直接从 indexer 中获取
 &lt;a class="anchor" href="#-%e5%a6%82%e6%9e%9c%e9%9c%80%e8%a6%81%e8%b0%83%e7%94%a8-pod-%e8%b5%84%e6%ba%90%e9%82%a3%e4%b9%88%e5%8f%af%e4%bb%a5%e7%9b%b4%e6%8e%a5%e4%bb%8e-indexer-%e4%b8%ad%e8%8e%b7%e5%8f%96">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-informer-初始化完成后reflector-开始-watch-pod-相关的事件">
 • informer 初始化完成后，Reflector 开始 Watch Pod 相关的事件
 &lt;a class="anchor" href="#-informer-%e5%88%9d%e5%a7%8b%e5%8c%96%e5%ae%8c%e6%88%90%e5%90%8ereflector-%e5%bc%80%e5%a7%8b-watch-pod-%e7%9b%b8%e5%85%b3%e7%9a%84%e4%ba%8b%e4%bb%b6">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-如果创建一个-pod1-那么-reflector-会监听到这个事件然后将这个事件发送到-deltafifo-中">
 • 如果创建一个 pod，1. 那么 Reflector 会监听到这个事件，然后将这个事件发送到 DeltaFIFO 中
 &lt;a class="anchor" href="#-%e5%a6%82%e6%9e%9c%e5%88%9b%e5%bb%ba%e4%b8%80%e4%b8%aa-pod1-%e9%82%a3%e4%b9%88-reflector-%e4%bc%9a%e7%9b%91%e5%90%ac%e5%88%b0%e8%bf%99%e4%b8%aa%e4%ba%8b%e4%bb%b6%e7%84%b6%e5%90%8e%e5%b0%86%e8%bf%99%e4%b8%aa%e4%ba%8b%e4%bb%b6%e5%8f%91%e9%80%81%e5%88%b0-deltafifo-%e4%b8%ad">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-informer-pop-消费改-add-事件并将该-pod-存储到-indexer">
 • informer pop 消费改 ADD 事件，并将该 pod 存储到 indexer
 &lt;a class="anchor" href="#-informer-pop-%e6%b6%88%e8%b4%b9%e6%94%b9-add-%e4%ba%8b%e4%bb%b6%e5%b9%b6%e5%b0%86%e8%af%a5-pod-%e5%ad%98%e5%82%a8%e5%88%b0-indexer">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-informer-处理器函数同样拿到该-add-事件去处理该事件通过workqueue获取到事件的key再通过indexer获取到真正操作的对象">
 • informer 处理器函数同样拿到该 ADD 事件去处理该事件，通过workqueue获取到事件的key，再通过indexer获取到真正操作的对象
 &lt;a class="anchor" href="#-informer-%e5%a4%84%e7%90%86%e5%99%a8%e5%87%bd%e6%95%b0%e5%90%8c%e6%a0%b7%e6%8b%bf%e5%88%b0%e8%af%a5-add-%e4%ba%8b%e4%bb%b6%e5%8e%bb%e5%a4%84%e7%90%86%e8%af%a5%e4%ba%8b%e4%bb%b6%e9%80%9a%e8%bf%87workqueue%e8%8e%b7%e5%8f%96%e5%88%b0%e4%ba%8b%e4%bb%b6%e7%9a%84key%e5%86%8d%e9%80%9a%e8%bf%87indexer%e8%8e%b7%e5%8f%96%e5%88%b0%e7%9c%9f%e6%ad%a3%e6%93%8d%e4%bd%9c%e7%9a%84%e5%af%b9%e8%b1%a1">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-reflector-会周期性将-indexer-数据同步到-deltafifo防止一些事件处理失败重新处理">
 • reflector 会周期性将 indexer 数据同步到 Deltafifo，防止一些事件处理失败，重新处理。
 &lt;a class="anchor" href="#-reflector-%e4%bc%9a%e5%91%a8%e6%9c%9f%e6%80%a7%e5%b0%86-indexer-%e6%95%b0%e6%8d%ae%e5%90%8c%e6%ad%a5%e5%88%b0-deltafifo%e9%98%b2%e6%ad%a2%e4%b8%80%e4%ba%9b%e4%ba%8b%e4%bb%b6%e5%a4%84%e7%90%86%e5%a4%b1%e8%b4%a5%e9%87%8d%e6%96%b0%e5%a4%84%e7%90%86">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="总结">
 总结
 &lt;a class="anchor" href="#%e6%80%bb%e7%bb%93">#&lt;/a>
&lt;/h2>
&lt;p>理解 client-go 原理是非常重要的，里面很多设计值得我们去学习，也可以运用到自己项目中。&lt;/p></description></item><item><title>2024-04-03 CNI插件选型</title><link>https://qq547475331.github.io/docs/cni%E6%8F%92%E4%BB%B6%E9%80%89%E5%9E%8B-cni-cha-jian-xuan-xing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/cni%E6%8F%92%E4%BB%B6%E9%80%89%E5%9E%8B-cni-cha-jian-xuan-xing/</guid><description>&lt;p>本文介绍容器环境常见网络应用场景及对应场景的 Kubernetes CNI 插件功能实现。帮助搭建和使用云原生环境的小伙伴&lt;strong>快速选择心仪的网络工具&lt;/strong>。&lt;/p>
&lt;h2 id="常见网络插件">
 常见网络插件
 &lt;a class="anchor" href="#%e5%b8%b8%e8%a7%81%e7%bd%91%e7%bb%9c%e6%8f%92%e4%bb%b6">#&lt;/a>
&lt;/h2>
&lt;p>我们在学习容器网络的时候，肯定都听说过 Docker 的 bridge 网络，Vethpair，VxLAN 等术语，从 Docker 到 kubernetes 后，学习了 Flannel、Calico 等主流网络插件，分别代表了 Overlay 和 Underlay 的两种网络传输模式，也是很经典的两款 CNI 网络插件。那么，还有哪些好用的 CNI 插件呢 ? 我们看看 CNCF Landscape:&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403061737738.png" alt="image-20240306173752677" />&lt;/p>
&lt;p>常见网络插件&lt;/p>
&lt;p>抛去商业版 CNI，此次分享来聊聊几款热门开源 CNI 插件，分别为 Kube-OVN、Antrea、Cilium。Kube-OVN 和 Antrea 都是基于 OpenvSwitch 的项目，Cilium 使用 eBPF 这款革命性的技术作为数据路径，亦是这两年很火热的一个开源容器项目。&lt;/p>
&lt;p>行万里路，此处相逢，共话云原生之道。 偶逗趣事，明月清风，与君同坐。&lt;/p>
&lt;p>55篇原创内容&lt;/p>
&lt;p>公众号&lt;/p>
&lt;h5 id="那么又回到学习新产品的第一步如何快速部署-k8s-体验不同地-cni-插件呢我们可以使用-kubekey-">
 那么，又回到学习新产品的第一步，如何快速部署 K8s 体验不同地 CNI 插件呢？我们可以使用 Kubekey 。
 &lt;a class="anchor" href="#%e9%82%a3%e4%b9%88%e5%8f%88%e5%9b%9e%e5%88%b0%e5%ad%a6%e4%b9%a0%e6%96%b0%e4%ba%a7%e5%93%81%e7%9a%84%e7%ac%ac%e4%b8%80%e6%ad%a5%e5%a6%82%e4%bd%95%e5%bf%ab%e9%80%9f%e9%83%a8%e7%bd%b2-k8s-%e4%bd%93%e9%aa%8c%e4%b8%8d%e5%90%8c%e5%9c%b0-cni-%e6%8f%92%e4%bb%b6%e5%91%a2%e6%88%91%e4%bb%ac%e5%8f%af%e4%bb%a5%e4%bd%bf%e7%94%a8-kubekey-">#&lt;/a>
&lt;/h5>
&lt;h5 id="kubekey-作为一个开源的-kubernetes-和-kubesphere-集群部署工具可以轻松的部署-kubernetes-集群提供节点管理操作系统安全加固容器运行时管理网络存储安装etcd-管理等kubekey-支持一键部署-calico--flannel--cilium--kube-ovn-等网络插件只需在-kk-的配置文件中注明-network-的-plugin-值即可">
 Kubekey 作为一个开源的 Kubernetes 和 KubeSphere 集群部署工具，可以轻松的部署 Kubernetes 集群，提供节点管理、操作系统安全加固、容器运行时管理、网络存储安装、Etcd 管理等。Kubekey 支持一键部署 Calico / Flannel / Cilium / Kube-OVN 等网络插件，只需在 kk 的配置文件中注明 network 的 plugin 值即可：
 &lt;a class="anchor" href="#kubekey-%e4%bd%9c%e4%b8%ba%e4%b8%80%e4%b8%aa%e5%bc%80%e6%ba%90%e7%9a%84-kubernetes-%e5%92%8c-kubesphere-%e9%9b%86%e7%be%a4%e9%83%a8%e7%bd%b2%e5%b7%a5%e5%85%b7%e5%8f%af%e4%bb%a5%e8%bd%bb%e6%9d%be%e7%9a%84%e9%83%a8%e7%bd%b2-kubernetes-%e9%9b%86%e7%be%a4%e6%8f%90%e4%be%9b%e8%8a%82%e7%82%b9%e7%ae%a1%e7%90%86%e6%93%8d%e4%bd%9c%e7%b3%bb%e7%bb%9f%e5%ae%89%e5%85%a8%e5%8a%a0%e5%9b%ba%e5%ae%b9%e5%99%a8%e8%bf%90%e8%a1%8c%e6%97%b6%e7%ae%a1%e7%90%86%e7%bd%91%e7%bb%9c%e5%ad%98%e5%82%a8%e5%ae%89%e8%a3%85etcd-%e7%ae%a1%e7%90%86%e7%ad%89kubekey-%e6%94%af%e6%8c%81%e4%b8%80%e9%94%ae%e9%83%a8%e7%bd%b2-calico--flannel--cilium--kube-ovn-%e7%ad%89%e7%bd%91%e7%bb%9c%e6%8f%92%e4%bb%b6%e5%8f%aa%e9%9c%80%e5%9c%a8-kk-%e7%9a%84%e9%85%8d%e7%bd%ae%e6%96%87%e4%bb%b6%e4%b8%ad%e6%b3%a8%e6%98%8e-network-%e7%9a%84-plugin-%e5%80%bc%e5%8d%b3%e5%8f%af">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code> network:
 plugin: calico/kubeovn/cilium
 kubePodsCIDR: 10.233.64.0/18
 kubeServiceCIDR: 10.233.0.0/18
&lt;/code>&lt;/pre>&lt;h5 id="对于-antrea由于版本较新目前可通过-addon-的形式添加-helm-文件的形式进行一键安装">
 对于 antrea，由于版本较新，目前可通过 addon 的形式添加 helm 文件的形式进行一键安装：
 &lt;a class="anchor" href="#%e5%af%b9%e4%ba%8e-antrea%e7%94%b1%e4%ba%8e%e7%89%88%e6%9c%ac%e8%be%83%e6%96%b0%e7%9b%ae%e5%89%8d%e5%8f%af%e9%80%9a%e8%bf%87-addon-%e7%9a%84%e5%bd%a2%e5%bc%8f%e6%b7%bb%e5%8a%a0-helm-%e6%96%87%e4%bb%b6%e7%9a%84%e5%bd%a2%e5%bc%8f%e8%bf%9b%e8%a1%8c%e4%b8%80%e9%94%ae%e5%ae%89%e8%a3%85">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code> addons:
 - name: antrea
 namespace: kube-system
 sources: 
 chart: 
 name: antrea
 repo: https://charts.antrea.io
 # values:
&lt;/code>&lt;/pre>&lt;h5 id="在此基础上可以通过以下一条命令">
 在此基础上，可以通过以下一条命令
 &lt;a class="anchor" href="#%e5%9c%a8%e6%ad%a4%e5%9f%ba%e7%a1%80%e4%b8%8a%e5%8f%af%e4%bb%a5%e9%80%9a%e8%bf%87%e4%bb%a5%e4%b8%8b%e4%b8%80%e6%9d%a1%e5%91%bd%e4%bb%a4">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>🐳 → kk create cluster --with-kubernetes --with-kubesphere
&lt;/code>&lt;/pre>&lt;h2 id="网络应用场景">
 网络应用场景
 &lt;a class="anchor" href="#%e7%bd%91%e7%bb%9c%e5%ba%94%e7%94%a8%e5%9c%ba%e6%99%af">#&lt;/a>
&lt;/h2>
&lt;h5 id="现在我们已经有了一个-kubernetes-集群先来思考一下容器网络除了让集群正常运行能让安装-kubernetes-后-pending-的-coredns-running-起来抖个鸡灵-_-以外还有哪些使用场景">
 现在我们已经有了一个 Kubernetes 集群，先来思考一下，容器网络除了让集群正常运行，能让安装 Kubernetes 后 Pending 的 CoreDNS running 起来（抖个鸡灵-_-）以外还有哪些使用场景？
 &lt;a class="anchor" href="#%e7%8e%b0%e5%9c%a8%e6%88%91%e4%bb%ac%e5%b7%b2%e7%bb%8f%e6%9c%89%e4%ba%86%e4%b8%80%e4%b8%aa-kubernetes-%e9%9b%86%e7%be%a4%e5%85%88%e6%9d%a5%e6%80%9d%e8%80%83%e4%b8%80%e4%b8%8b%e5%ae%b9%e5%99%a8%e7%bd%91%e7%bb%9c%e9%99%a4%e4%ba%86%e8%ae%a9%e9%9b%86%e7%be%a4%e6%ad%a3%e5%b8%b8%e8%bf%90%e8%a1%8c%e8%83%bd%e8%ae%a9%e5%ae%89%e8%a3%85-kubernetes-%e5%90%8e-pending-%e7%9a%84-coredns-running-%e8%b5%b7%e6%9d%a5%e6%8a%96%e4%b8%aa%e9%b8%a1%e7%81%b5-_-%e4%bb%a5%e5%a4%96%e8%bf%98%e6%9c%89%e5%93%aa%e4%ba%9b%e4%bd%bf%e7%94%a8%e5%9c%ba%e6%99%af">#&lt;/a>
&lt;/h5>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403061738752.png" alt="image-20240306173850669" />&lt;/p></description></item><item><title>2024-04-03 Containerd 基本操作</title><link>https://qq547475331.github.io/docs/containerd-%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C-containerd-ji-ben-cao-zuo/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/containerd-%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C-containerd-ji-ben-cao-zuo/</guid><description>&lt;h2 id="镜像基本操作">
 镜像基本操作
 &lt;a class="anchor" href="#%e9%95%9c%e5%83%8f%e5%9f%ba%e6%9c%ac%e6%93%8d%e4%bd%9c">#&lt;/a>
&lt;/h2>
&lt;p>Containerd 默认提供 CLI 命令行工具 ctr，ctr 命名提供基本的镜像和容器操作功能，可以通过如下查看命令帮助：&lt;/p>
&lt;pre tabindex="0">&lt;code>[root@localhost ~]# ctr -help
&lt;/code>&lt;/pre>&lt;p>镜像基本操作主要是 ctr image 命令，查看命令帮助&lt;/p>
&lt;pre tabindex="0">&lt;code>[root@localhost ~]# ctr images -h
NAME:
 ctr images - Manage images

USAGE:
 ctr images command [command options] [arguments...]

COMMANDS:
 check Check existing images to ensure all content is available locally
 export Export images
 import Import images
 list, ls List images known to containerd
 mount Mount an image to a target path
 unmount Unmount the image from the target
 pull Pull an image from a remote
 push Push an image to a remote
 prune Remove unused images
 delete, del, remove, rm Remove one or more images by reference
 tag Tag an image
 label Set and clear labels for an image
 convert Convert an image
 usage Display usage of snapshots for a given image ref

OPTIONS:
 --help, -h show help
&lt;/code>&lt;/pre>&lt;h2 id="拉取镜像">
 拉取镜像
 &lt;a class="anchor" href="#%e6%8b%89%e5%8f%96%e9%95%9c%e5%83%8f">#&lt;/a>
&lt;/h2>
&lt;p>containerd支持oci标准的镜像，所以可以直接使用docker官方或dockerfile构建的镜像 需要注意的是，与docker不同，拉取镜像时要加上 ?docker.io/liarary&lt;/p></description></item><item><title>2024-04-03 COREDNS之光</title><link>https://qq547475331.github.io/docs/coredns%E4%B9%8B%E5%85%89-coredns-zhi-guang/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/coredns%E4%B9%8B%E5%85%89-coredns-zhi-guang/</guid><description>&lt;p>在 Kubernetes 中，DNS 名称被分配给 Pod 和服务，以便通过名称而不是 IP 地址进行通信。&lt;/p>
&lt;p>集群内DNS解析默认使用的域名为&lt;code>cluster.local&lt;/code>，可以根据需要自定义。Service 的 DNS 名称遵循&lt;code>&amp;lt;service-name&amp;gt;.&amp;lt;namespace&amp;gt;.svc.cluster.local&lt;/code>格式，而Pod 的 DNS 名称遵循&lt;code>&amp;lt;pod-ip-address-replace-dot-with-hyphen&amp;gt;.&amp;lt;namespace&amp;gt;.pod.cluster.local&lt;/code>格式。&lt;/p>
&lt;p>CoreDNS 的运行基于名为&lt;code>Corefile&lt;/code>的配置文件，该文件指定 DNS 服务器应如何运行并响应传入请求。&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403191408043.png" alt="image-20240319140856976" />&lt;/p>
&lt;p>Kubernetes：DNS&lt;/p>
&lt;h1 id="dns解析">
 DNS解析
 &lt;a class="anchor" href="#dns%e8%a7%a3%e6%9e%90">#&lt;/a>
&lt;/h1>
&lt;p>在 Kubernetes 中，分配给 Pod 和服务的DNS 名称用集群内的名称解析，允许Pod 和服务通过名称而不是 IP 地址相互通信。&lt;/p>
&lt;p>行万里路，此处相逢，共话云原生之道。 偶逗趣事，明月清风，与君同坐。&lt;/p>
&lt;h2 id="默认域名clusterlocal">
 默认域名：cluster.local
 &lt;a class="anchor" href="#%e9%bb%98%e8%ae%a4%e5%9f%9f%e5%90%8dclusterlocal">#&lt;/a>
&lt;/h2>
&lt;p>在 Kubernetes 中，&lt;code>cluster.local&lt;/code>是集群内用于 DNS 解析的默认域名。当对同一命名空间内的服务或 Pod 进行 DNS 查询时，Kubernetes DNS 服务会将命名空间和&lt;code>cluster.local&lt;/code>后缀附加到名称中，以形成完全限定域名 (FQDN)。虽然它是默认域名，但如果需要，可以自定义使用不同的域名。&lt;/p>
&lt;h2 id="service的-dns-名称">
 Service的 DNS 名称
 &lt;a class="anchor" href="#service%e7%9a%84-dns-%e5%90%8d%e7%a7%b0">#&lt;/a>
&lt;/h2>
&lt;p>Kubernetes 中服务的 DNS 名称遵循以下格式：&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403191409789.png" alt="image-20240319140930749" />&lt;/p>
&lt;p>Service的 DNS 名称&lt;/p>
&lt;pre tabindex="0">&lt;code>&amp;lt;service-name&amp;gt;.&amp;lt;namespace&amp;gt;.svc.cluster.local
&lt;/code>&lt;/pre>&lt;p>&lt;code>service-name&lt;/code>的是Service的名称，而&lt;code>namespace&lt;/code>表示Service运行的命名空间。&lt;/p>
&lt;p>例如，如果一个名为的服务&lt;code>my-service&lt;/code>正在&lt;code>my-namespace&lt;/code>命名空间中运行，则相应的 DNS 名称将为：&lt;/p>
&lt;pre tabindex="0">&lt;code>my-service.my-namespace.svc.cluster.local
&lt;/code>&lt;/pre>&lt;h2 id="pod-的-dns-名称">
 Pod 的 DNS 名称
 &lt;a class="anchor" href="#pod-%e7%9a%84-dns-%e5%90%8d%e7%a7%b0">#&lt;/a>
&lt;/h2>
&lt;p>Kubernetes 中 Pod 的 DNS 名称遵循以下格式：&lt;/p></description></item><item><title>2024-04-03 dockerfile的copy和add的区别</title><link>https://qq547475331.github.io/docs/dockerfile%E7%9A%84copy%E5%92%8Cadd%E7%9A%84%E5%8C%BA%E5%88%AB-dockerfile-de-copy-he-add-de-qu-bie/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/dockerfile%E7%9A%84copy%E5%92%8Cadd%E7%9A%84%E5%8C%BA%E5%88%AB-dockerfile-de-copy-he-add-de-qu-bie/</guid><description>&lt;h5 id="dockerfile-中的-add-与-copy-指令都可以用于将本地文件或目录复制到-docker-镜像中但它们之间有一些区别">
 Dockerfile 中的 &lt;code>ADD&lt;/code> 与 &lt;code>COPY&lt;/code> 指令都可以用于将本地文件或目录复制到 Docker 镜像中，但它们之间有一些区别。
 &lt;a class="anchor" href="#dockerfile-%e4%b8%ad%e7%9a%84-add-%e4%b8%8e-copy-%e6%8c%87%e4%bb%a4%e9%83%bd%e5%8f%af%e4%bb%a5%e7%94%a8%e4%ba%8e%e5%b0%86%e6%9c%ac%e5%9c%b0%e6%96%87%e4%bb%b6%e6%88%96%e7%9b%ae%e5%bd%95%e5%a4%8d%e5%88%b6%e5%88%b0-docker-%e9%95%9c%e5%83%8f%e4%b8%ad%e4%bd%86%e5%ae%83%e4%bb%ac%e4%b9%8b%e9%97%b4%e6%9c%89%e4%b8%80%e4%ba%9b%e5%8c%ba%e5%88%ab">#&lt;/a>
&lt;/h5>
&lt;h5 id="1--add-指令支持自动解压缩功能">
 1. ADD 指令支持自动解压缩功能
 &lt;a class="anchor" href="#1--add-%e6%8c%87%e4%bb%a4%e6%94%af%e6%8c%81%e8%87%aa%e5%8a%a8%e8%a7%a3%e5%8e%8b%e7%bc%a9%e5%8a%9f%e8%83%bd">#&lt;/a>
&lt;/h5>
&lt;h5 id="当使用-add-指令将本地文件复制到-docker-镜像中时如果该文件是压缩包格式docker-会自动解压缩该文件例如">
 当使用 &lt;code>ADD&lt;/code> 指令将本地文件复制到 Docker 镜像中时，如果该文件是压缩包格式，Docker 会自动解压缩该文件。例如：
 &lt;a class="anchor" href="#%e5%bd%93%e4%bd%bf%e7%94%a8-add-%e6%8c%87%e4%bb%a4%e5%b0%86%e6%9c%ac%e5%9c%b0%e6%96%87%e4%bb%b6%e5%a4%8d%e5%88%b6%e5%88%b0-docker-%e9%95%9c%e5%83%8f%e4%b8%ad%e6%97%b6%e5%a6%82%e6%9e%9c%e8%af%a5%e6%96%87%e4%bb%b6%e6%98%af%e5%8e%8b%e7%bc%a9%e5%8c%85%e6%a0%bc%e5%bc%8fdocker-%e4%bc%9a%e8%87%aa%e5%8a%a8%e8%a7%a3%e5%8e%8b%e7%bc%a9%e8%af%a5%e6%96%87%e4%bb%b6%e4%be%8b%e5%a6%82">#&lt;/a>
&lt;/h5>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>ADD nginx-1.21.0.tar.gz /usr/local/
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h5 id="上述例子中在将本地的-nginx-1210targz-文件复制到-docker-镜像的-usrlocal-目录下时docker-会将该文件解压缩">
 上述例子中在将本地的 &lt;code>nginx-1.21.0.tar.gz&lt;/code> 文件复制到 Docker 镜像的 &lt;code>/usr/local/&lt;/code> 目录下时，Docker 会将该文件解压缩。
 &lt;a class="anchor" href="#%e4%b8%8a%e8%bf%b0%e4%be%8b%e5%ad%90%e4%b8%ad%e5%9c%a8%e5%b0%86%e6%9c%ac%e5%9c%b0%e7%9a%84-nginx-1210targz-%e6%96%87%e4%bb%b6%e5%a4%8d%e5%88%b6%e5%88%b0-docker-%e9%95%9c%e5%83%8f%e7%9a%84-usrlocal-%e7%9b%ae%e5%bd%95%e4%b8%8b%e6%97%b6docker-%e4%bc%9a%e5%b0%86%e8%af%a5%e6%96%87%e4%bb%b6%e8%a7%a3%e5%8e%8b%e7%bc%a9">#&lt;/a>
&lt;/h5>
&lt;h5 id="而-copy-指令并不支持自动解压缩功能需要手动解压缩后再复制进镜像">
 而 &lt;code>COPY&lt;/code> 指令并不支持自动解压缩功能，需要手动解压缩后再复制进镜像。
 &lt;a class="anchor" href="#%e8%80%8c-copy-%e6%8c%87%e4%bb%a4%e5%b9%b6%e4%b8%8d%e6%94%af%e6%8c%81%e8%87%aa%e5%8a%a8%e8%a7%a3%e5%8e%8b%e7%bc%a9%e5%8a%9f%e8%83%bd%e9%9c%80%e8%a6%81%e6%89%8b%e5%8a%a8%e8%a7%a3%e5%8e%8b%e7%bc%a9%e5%90%8e%e5%86%8d%e5%a4%8d%e5%88%b6%e8%bf%9b%e9%95%9c%e5%83%8f">#&lt;/a>
&lt;/h5>
&lt;h5 id="2--add-指令可以从-url-复制内容">
 2. ADD 指令可以从 URL 复制内容
 &lt;a class="anchor" href="#2--add-%e6%8c%87%e4%bb%a4%e5%8f%af%e4%bb%a5%e4%bb%8e-url-%e5%a4%8d%e5%88%b6%e5%86%85%e5%ae%b9">#&lt;/a>
&lt;/h5>
&lt;h5 id="add-指令除了能够复制本地文件和目录外还可以复制远程文件例如从-url-下载到-docker-镜像中例如">
 &lt;code>ADD&lt;/code> 指令除了能够复制本地文件和目录外，还可以复制远程文件（例如从 URL 下载）到 Docker 镜像中。例如：
 &lt;a class="anchor" href="#add-%e6%8c%87%e4%bb%a4%e9%99%a4%e4%ba%86%e8%83%bd%e5%a4%9f%e5%a4%8d%e5%88%b6%e6%9c%ac%e5%9c%b0%e6%96%87%e4%bb%b6%e5%92%8c%e7%9b%ae%e5%bd%95%e5%a4%96%e8%bf%98%e5%8f%af%e4%bb%a5%e5%a4%8d%e5%88%b6%e8%bf%9c%e7%a8%8b%e6%96%87%e4%bb%b6%e4%be%8b%e5%a6%82%e4%bb%8e-url-%e4%b8%8b%e8%bd%bd%e5%88%b0-docker-%e9%95%9c%e5%83%8f%e4%b8%ad%e4%be%8b%e5%a6%82">#&lt;/a>
&lt;/h5>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>ADD https://example.com/nginx-1.21.0.tar.gz /usr/local/
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h5 id="上述例子中会从远程-url-httpsexamplecomnginx-1210targz-下载文件并将其复制到-docker-镜像的-usrlocal-目录下">
 上述例子中会从远程 URL &lt;a href="https://links.jianshu.com/go?to=https%3A%2F%2Fexample.com%2Fnginx-1.21.0.tar.gz">https://example.com/nginx-1.21.0.tar.gz&lt;/a> 下载文件，并将其复制到 Docker 镜像的 &lt;code>/usr/local/&lt;/code> 目录下。
 &lt;a class="anchor" href="#%e4%b8%8a%e8%bf%b0%e4%be%8b%e5%ad%90%e4%b8%ad%e4%bc%9a%e4%bb%8e%e8%bf%9c%e7%a8%8b-url-httpsexamplecomnginx-1210targz-%e4%b8%8b%e8%bd%bd%e6%96%87%e4%bb%b6%e5%b9%b6%e5%b0%86%e5%85%b6%e5%a4%8d%e5%88%b6%e5%88%b0-docker-%e9%95%9c%e5%83%8f%e7%9a%84-usrlocal-%e7%9b%ae%e5%bd%95%e4%b8%8b">#&lt;/a>
&lt;/h5>
&lt;h5 id="而-copy-指令只能复制本地文件和目录">
 而 &lt;code>COPY&lt;/code> 指令只能复制本地文件和目录。
 &lt;a class="anchor" href="#%e8%80%8c-copy-%e6%8c%87%e4%bb%a4%e5%8f%aa%e8%83%bd%e5%a4%8d%e5%88%b6%e6%9c%ac%e5%9c%b0%e6%96%87%e4%bb%b6%e5%92%8c%e7%9b%ae%e5%bd%95">#&lt;/a>
&lt;/h5>
&lt;h5 id="3--add-指令具有隐式的文件拷贝功能">
 3. ADD 指令具有隐式的文件拷贝功能
 &lt;a class="anchor" href="#3--add-%e6%8c%87%e4%bb%a4%e5%85%b7%e6%9c%89%e9%9a%90%e5%bc%8f%e7%9a%84%e6%96%87%e4%bb%b6%e6%8b%b7%e8%b4%9d%e5%8a%9f%e8%83%bd">#&lt;/a>
&lt;/h5>
&lt;h5 id="add-指令除了能够复制本地文件和目录外还具有一个隐式的文件拷贝功能当复制一个压缩包文件到容器中时docker-会自动解压并且可以直接从-url-下载文件并解压缩">
 &lt;code>ADD&lt;/code> 指令除了能够复制本地文件和目录外，还具有一个隐式的文件拷贝功能：当复制一个压缩包文件到容器中时，Docker 会自动解压，并且可以直接从 URL 下载文件并解压缩。
 &lt;a class="anchor" href="#add-%e6%8c%87%e4%bb%a4%e9%99%a4%e4%ba%86%e8%83%bd%e5%a4%9f%e5%a4%8d%e5%88%b6%e6%9c%ac%e5%9c%b0%e6%96%87%e4%bb%b6%e5%92%8c%e7%9b%ae%e5%bd%95%e5%a4%96%e8%bf%98%e5%85%b7%e6%9c%89%e4%b8%80%e4%b8%aa%e9%9a%90%e5%bc%8f%e7%9a%84%e6%96%87%e4%bb%b6%e6%8b%b7%e8%b4%9d%e5%8a%9f%e8%83%bd%e5%bd%93%e5%a4%8d%e5%88%b6%e4%b8%80%e4%b8%aa%e5%8e%8b%e7%bc%a9%e5%8c%85%e6%96%87%e4%bb%b6%e5%88%b0%e5%ae%b9%e5%99%a8%e4%b8%ad%e6%97%b6docker-%e4%bc%9a%e8%87%aa%e5%8a%a8%e8%a7%a3%e5%8e%8b%e5%b9%b6%e4%b8%94%e5%8f%af%e4%bb%a5%e7%9b%b4%e6%8e%a5%e4%bb%8e-url-%e4%b8%8b%e8%bd%bd%e6%96%87%e4%bb%b6%e5%b9%b6%e8%a7%a3%e5%8e%8b%e7%bc%a9">#&lt;/a>
&lt;/h5>
&lt;p>例如：&lt;/p></description></item><item><title>2024-04-03 Docker重要的网络知识点</title><link>https://qq547475331.github.io/docs/docker%E9%87%8D%E8%A6%81%E7%9A%84%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86%E7%82%B9-docker-zhong-yao-de-wang-luo-zhi-shi-dian/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/docker%E9%87%8D%E8%A6%81%E7%9A%84%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86%E7%82%B9-docker-zhong-yao-de-wang-luo-zhi-shi-dian/</guid><description>&lt;h4 id="一bridge网络">
 &lt;strong>一、Bridge网络&lt;/strong>
 &lt;a class="anchor" href="#%e4%b8%80bridge%e7%bd%91%e7%bb%9c">#&lt;/a>
&lt;/h4>
&lt;p>bridge网络表现形式就是docker0这个网络接口。容器默认都是通过docker0这个接口进行通信。也可以通过docker0去和本机的以太网接口连接，这样容器内部才能访问互联网。&lt;/p>
&lt;p>查看docker0网络，在默认环境中，一个名为docker0的linux bridge自动被创建好了，其上有一个 docker0 内部接口，IP地址为172.17.0.1/16。通过命令：&lt;code>ip a&lt;/code>&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202402221138515.png" alt="image-20240222113826449" />&lt;/p>
&lt;p>查看docker网络&lt;/p>
&lt;blockquote>
&lt;p>docker network ls&lt;/p>&lt;/blockquote>
&lt;p>查看bridge网络详情。主要关注Containers节点信息。&lt;/p>
&lt;blockquote>
&lt;pre tabindex="0">&lt;code>docker network inspect bridge
&lt;/code>&lt;/pre>&lt;/blockquote>
&lt;p>输出如下：&lt;/p>
&lt;pre tabindex="0">&lt;code>[
 {
 &amp;#34;Name&amp;#34;: &amp;#34;bridge&amp;#34;,
 &amp;#34;Id&amp;#34;: &amp;#34;8f2bb7721e07845f809dab6cfeefccc5145cbb99953bbe090d5c349af5bf5fa1&amp;#34;,
 &amp;#34;Created&amp;#34;: &amp;#34;2024-01-22T22:29:27.110338803+08:00&amp;#34;,
 &amp;#34;Scope&amp;#34;: &amp;#34;local&amp;#34;,
 &amp;#34;Driver&amp;#34;: &amp;#34;bridge&amp;#34;,
 &amp;#34;EnableIPv6&amp;#34;: false,
 &amp;#34;IPAM&amp;#34;: {
 &amp;#34;Driver&amp;#34;: &amp;#34;default&amp;#34;,
 &amp;#34;Options&amp;#34;: null,
 &amp;#34;Config&amp;#34;: [
 {
 &amp;#34;Subnet&amp;#34;: &amp;#34;172.17.0.0/16&amp;#34;,
 &amp;#34;Gateway&amp;#34;: &amp;#34;172.17.0.1&amp;#34;
 }
 ]
 },
 &amp;#34;Internal&amp;#34;: false,
 &amp;#34;Attachable&amp;#34;: false,
 &amp;#34;Ingress&amp;#34;: false,
 &amp;#34;ConfigFrom&amp;#34;: {
 &amp;#34;Network&amp;#34;: &amp;#34;&amp;#34;
 },
 &amp;#34;ConfigOnly&amp;#34;: false,
 &amp;#34;Containers&amp;#34;: {},
 &amp;#34;Options&amp;#34;: {
 &amp;#34;com.docker.network.bridge.default_bridge&amp;#34;: &amp;#34;true&amp;#34;,
 &amp;#34;com.docker.network.bridge.enable_icc&amp;#34;: &amp;#34;true&amp;#34;,
 &amp;#34;com.docker.network.bridge.enable_ip_masquerade&amp;#34;: &amp;#34;true&amp;#34;,
 &amp;#34;com.docker.network.bridge.host_binding_ipv4&amp;#34;: &amp;#34;0.0.0.0&amp;#34;,
 &amp;#34;com.docker.network.bridge.name&amp;#34;: &amp;#34;docker0&amp;#34;,
 &amp;#34;com.docker.network.driver.mtu&amp;#34;: &amp;#34;1500&amp;#34;
 },
 &amp;#34;Labels&amp;#34;: {}
 }
]
&lt;/code>&lt;/pre>&lt;p>我们看到Containers这个节点下，没有值，也就是没有任何容器加入这个网络环境&lt;/p></description></item><item><title>2024-04-03 ETCD备份</title><link>https://qq547475331.github.io/docs/etcd%E5%A4%87%E4%BB%BD-etcd-bei-fen/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/etcd%E5%A4%87%E4%BB%BD-etcd-bei-fen/</guid><description>&lt;pre tabindex="0">&lt;code>#cat /data/backup/etcd/etcd-backup.sh

#!/bin/bash
bakpath=/data/backup/etcd
cd $bakpath
baktime=`date +%Y%m%d`
#备份当天数据
ETCDCTL_API=3 etcdctl --endpoints http://127.0.0.1:1159 \
snapshot save /data/backup/etcd/etcd-data-bak-${baktime}.db \
&amp;gt;&amp;gt;${bakpath}/etcd-backup.log
#清理半个月前数据
/usr/bin/find $bakpath -name *.db -mtime +15 -exec rm -f {} \;





0 23 * * * sh /data/backup/etcd/etcd-backup.sh&amp;gt;/dev/null 2&amp;gt;&amp;amp;1


那就改成/apps目录
你们装的的机器是/data目录
&lt;/code>&lt;/pre></description></item><item><title>2024-04-03 ETCD稳定性及性能优化实践</title><link>https://qq547475331.github.io/docs/etcd%E7%A8%B3%E5%AE%9A%E6%80%A7%E5%8F%8A%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E5%AE%9E%E8%B7%B5-etcd-wen-ding-xing-ji-xing-neng-you-hua-shi-jian/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/etcd%E7%A8%B3%E5%AE%9A%E6%80%A7%E5%8F%8A%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E5%AE%9E%E8%B7%B5-etcd-wen-ding-xing-ji-xing-neng-you-hua-shi-jian/</guid><description>&lt;h2 id="背景与挑战">
 背景与挑战
 &lt;a class="anchor" href="#%e8%83%8c%e6%99%af%e4%b8%8e%e6%8c%91%e6%88%98">#&lt;/a>
&lt;/h2>
&lt;p>随着腾讯自研上云及公有云用户的迅速增长，一方面，腾讯云容器服务TKE服务数量和核数大幅增长, 另一方面我们提供的容器服务类型（TKE托管及独立集群、EKS弹性集群、edge边缘计算集群、mesh服务网格、serverless knative）也越来越丰富。各类容器服务类型背后的核心都是K8s，K8s核心的存储etcd又统一由我们基于K8s构建的etcd平台进行管理。基于它我们目前管理了千级etcd集群，背后支撑了万级K8s集群。&lt;/p>
&lt;p>&lt;strong>在万级K8s集群规模下的我们如何高效保障etcd集群的稳定性?&lt;/strong>&lt;/p>
&lt;p>etcd集群的稳定性风险又来自哪里？&lt;/p>
&lt;p>我们通过基于业务场景、历史遗留问题、现网运营经验等进行稳定性风险模型分析，&lt;strong>风险主要来自旧TKE etcd架构设计不合理、etcd稳定性、etcd性能部分场景无法满足业务、测试用例覆盖不足、变更管理不严谨、监控是否全面覆盖、隐患点是否能自动巡检发现、极端灾难故障数据安全是否能保障。&lt;/strong>&lt;/p>
&lt;p>前面所描述的etcd平台已经从架构设计上、变更管理上、监控及巡检、数据迁移、备份几个方面程度解决了我们管理的各类容器服务的etcd可扩展性、可运维性、可观测性以及数据安全性，因此&lt;strong>本文将重点描述我们在万级K8s场景下面临的etcd内核稳定性及性能挑战&lt;/strong>，比如:&lt;/p>
&lt;ul>
&lt;li>数据不一致&lt;/li>
&lt;li>内存泄露&lt;/li>
&lt;li>死锁&lt;/li>
&lt;li>进程Crash&lt;/li>
&lt;li>大包请求导致etcd OOM及丢包&lt;/li>
&lt;li>较大数据量场景下启动慢&lt;/li>
&lt;li>鉴权及查询key数量、查询指定数量记录接口性能较差&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403011117394.png" alt="image-20240301111728306" />&lt;/p>
&lt;p>本文将简易描述我们是如何发现、分析、复现、解决以上问题及挑战，以及从以上过程中我们获得了哪些经验及教训，并将之应用到我们的各类容器服务存储稳定性保障中。&lt;/p>
&lt;p>同时，&lt;strong>我们将解决方案全部贡献、回馈给etcd开源社区， 截止目前我们贡献的30+ pr已全部合并到社区。腾讯云TKE etcd团队是etcd社区2020年上半年最活跃的贡献团队之一, 为etcd的发展贡献我们的一点力量, 在这过程中特别感谢社区AWS、Google、Ali等maintainer的支持与帮助。&lt;/strong>&lt;/p>
&lt;h2 id="稳定性优化案例剖析">
 稳定性优化案例剖析
 &lt;a class="anchor" href="#%e7%a8%b3%e5%ae%9a%e6%80%a7%e4%bc%98%e5%8c%96%e6%a1%88%e4%be%8b%e5%89%96%e6%9e%90">#&lt;/a>
&lt;/h2>
&lt;p>从GitLab误删主库丢失部分数据到GitHub数据不一致导致中断24小时，再到号称&amp;quot;不沉航母&amp;quot;的AWS S3故障数小时等，无一例外都是存储服务。稳定性对于一个存储服务、乃至一个公司的口碑而言至关重要，它决定着一个产品生与死。稳定性优化案例我们将从数据不一致的严重性、两个etcd数据不一致的bug、lease内存泄露、mvcc 死锁、wal crash方面阐述，我们是如何发现、分析、复现、解决以上case，并分享我们从每个case中的获得的收获和反思，从中汲取经验，防患于未然。&lt;/p>
&lt;h3 id="数据不一致data-inconsistency">
 &lt;strong>数据不一致（Data Inconsistency）&lt;/strong>
 &lt;a class="anchor" href="#%e6%95%b0%e6%8d%ae%e4%b8%8d%e4%b8%80%e8%87%b4data-inconsistency">#&lt;/a>
&lt;/h3>
&lt;h5 id="谈到数据不一致导致的大故障就不得不详细提下github在18年一次因网络设备的例行维护工作导致的美国东海岸网络中心与东海岸主要数据中心之间的连接断开虽然网络的连通性在43秒内得以恢复但是短暂的中断引发了一系列事件最终导致github-24小时11分钟的服务降级部分功能不可用">
 谈到数据不一致导致的大故障，就不得不详细提下GitHub在18年一次因网络设备的例行维护工作导致的美国东海岸网络中心与东海岸主要数据中心之间的连接断开。虽然网络的连通性在43秒内得以恢复，但是短暂的中断引发了一系列事件，最终导致GitHub 24小时11分钟的服务降级，部分功能不可用。
 &lt;a class="anchor" href="#%e8%b0%88%e5%88%b0%e6%95%b0%e6%8d%ae%e4%b8%8d%e4%b8%80%e8%87%b4%e5%af%bc%e8%87%b4%e7%9a%84%e5%a4%a7%e6%95%85%e9%9a%9c%e5%b0%b1%e4%b8%8d%e5%be%97%e4%b8%8d%e8%af%a6%e7%bb%86%e6%8f%90%e4%b8%8bgithub%e5%9c%a818%e5%b9%b4%e4%b8%80%e6%ac%a1%e5%9b%a0%e7%bd%91%e7%bb%9c%e8%ae%be%e5%a4%87%e7%9a%84%e4%be%8b%e8%a1%8c%e7%bb%b4%e6%8a%a4%e5%b7%a5%e4%bd%9c%e5%af%bc%e8%87%b4%e7%9a%84%e7%be%8e%e5%9b%bd%e4%b8%9c%e6%b5%b7%e5%b2%b8%e7%bd%91%e7%bb%9c%e4%b8%ad%e5%bf%83%e4%b8%8e%e4%b8%9c%e6%b5%b7%e5%b2%b8%e4%b8%bb%e8%a6%81%e6%95%b0%e6%8d%ae%e4%b8%ad%e5%bf%83%e4%b9%8b%e9%97%b4%e7%9a%84%e8%bf%9e%e6%8e%a5%e6%96%ad%e5%bc%80%e8%99%bd%e7%84%b6%e7%bd%91%e7%bb%9c%e7%9a%84%e8%bf%9e%e9%80%9a%e6%80%a7%e5%9c%a843%e7%a7%92%e5%86%85%e5%be%97%e4%bb%a5%e6%81%a2%e5%a4%8d%e4%bd%86%e6%98%af%e7%9f%ad%e6%9a%82%e7%9a%84%e4%b8%ad%e6%96%ad%e5%bc%95%e5%8f%91%e4%ba%86%e4%b8%80%e7%b3%bb%e5%88%97%e4%ba%8b%e4%bb%b6%e6%9c%80%e7%bb%88%e5%af%bc%e8%87%b4github-24%e5%b0%8f%e6%97%b611%e5%88%86%e9%92%9f%e7%9a%84%e6%9c%8d%e5%8a%a1%e9%99%8d%e7%ba%a7%e9%83%a8%e5%88%86%e5%8a%9f%e8%83%bd%e4%b8%8d%e5%8f%af%e7%94%a8">#&lt;/a>
&lt;/h5>
&lt;h5 id="github使用了大量的mysql集群存储github的meta-data如issueprpage等等同时做了东西海岸跨城级别的容灾故障核心原因是网络异常时github的mysql仲裁服务orchestrator进行了故障转移将写入数据定向到美国西海岸的mysql集群故障前primary在东海岸然而美国东海岸的mysql包含一小段写入尚未复制到美国西海岸集群同时故障转移后由于两个数据中心的集群现在都包含另一个数据中心中不存在的写入因此又无法安全地将主数据库故障转移回美国东海岸">
 GitHub使用了大量的MySQL集群存储GitHub的meta data,如issue、pr、page等等，同时做了东西海岸跨城级别的容灾。故障核心原因是网络异常时GitHub的MySQL仲裁服务Orchestrator进行了故障转移，将写入数据定向到美国西海岸的MySQL集群(故障前primary在东海岸)，然而美国东海岸的MySQL包含一小段写入，尚未复制到美国西海岸集群，同时故障转移后由于两个数据中心的集群现在都包含另一个数据中心中不存在的写入，因此又无法安全地将主数据库故障转移回美国东海岸。
 &lt;a class="anchor" href="#github%e4%bd%bf%e7%94%a8%e4%ba%86%e5%a4%a7%e9%87%8f%e7%9a%84mysql%e9%9b%86%e7%be%a4%e5%ad%98%e5%82%a8github%e7%9a%84meta-data%e5%a6%82issueprpage%e7%ad%89%e7%ad%89%e5%90%8c%e6%97%b6%e5%81%9a%e4%ba%86%e4%b8%9c%e8%a5%bf%e6%b5%b7%e5%b2%b8%e8%b7%a8%e5%9f%8e%e7%ba%a7%e5%88%ab%e7%9a%84%e5%ae%b9%e7%81%be%e6%95%85%e9%9a%9c%e6%a0%b8%e5%bf%83%e5%8e%9f%e5%9b%a0%e6%98%af%e7%bd%91%e7%bb%9c%e5%bc%82%e5%b8%b8%e6%97%b6github%e7%9a%84mysql%e4%bb%b2%e8%a3%81%e6%9c%8d%e5%8a%a1orchestrator%e8%bf%9b%e8%a1%8c%e4%ba%86%e6%95%85%e9%9a%9c%e8%bd%ac%e7%a7%bb%e5%b0%86%e5%86%99%e5%85%a5%e6%95%b0%e6%8d%ae%e5%ae%9a%e5%90%91%e5%88%b0%e7%be%8e%e5%9b%bd%e8%a5%bf%e6%b5%b7%e5%b2%b8%e7%9a%84mysql%e9%9b%86%e7%be%a4%e6%95%85%e9%9a%9c%e5%89%8dprimary%e5%9c%a8%e4%b8%9c%e6%b5%b7%e5%b2%b8%e7%84%b6%e8%80%8c%e7%be%8e%e5%9b%bd%e4%b8%9c%e6%b5%b7%e5%b2%b8%e7%9a%84mysql%e5%8c%85%e5%90%ab%e4%b8%80%e5%b0%8f%e6%ae%b5%e5%86%99%e5%85%a5%e5%b0%9a%e6%9c%aa%e5%a4%8d%e5%88%b6%e5%88%b0%e7%be%8e%e5%9b%bd%e8%a5%bf%e6%b5%b7%e5%b2%b8%e9%9b%86%e7%be%a4%e5%90%8c%e6%97%b6%e6%95%85%e9%9a%9c%e8%bd%ac%e7%a7%bb%e5%90%8e%e7%94%b1%e4%ba%8e%e4%b8%a4%e4%b8%aa%e6%95%b0%e6%8d%ae%e4%b8%ad%e5%bf%83%e7%9a%84%e9%9b%86%e7%be%a4%e7%8e%b0%e5%9c%a8%e9%83%bd%e5%8c%85%e5%90%ab%e5%8f%a6%e4%b8%80%e4%b8%aa%e6%95%b0%e6%8d%ae%e4%b8%ad%e5%bf%83%e4%b8%ad%e4%b8%8d%e5%ad%98%e5%9c%a8%e7%9a%84%e5%86%99%e5%85%a5%e5%9b%a0%e6%ad%a4%e5%8f%88%e6%97%a0%e6%b3%95%e5%ae%89%e5%85%a8%e5%9c%b0%e5%b0%86%e4%b8%bb%e6%95%b0%e6%8d%ae%e5%ba%93%e6%95%85%e9%9a%9c%e8%bd%ac%e7%a7%bb%e5%9b%9e%e7%be%8e%e5%9b%bd%e4%b8%9c%e6%b5%b7%e5%b2%b8">#&lt;/a>
&lt;/h5>
&lt;h5 id="最终-为了保证保证用户数据不丢失github不得不以24小时的服务降级为代价来修复数据一致性">
 最终, 为了保证保证用户数据不丢失，GitHub不得不以24小时的服务降级为代价来修复数据一致性。
 &lt;a class="anchor" href="#%e6%9c%80%e7%bb%88-%e4%b8%ba%e4%ba%86%e4%bf%9d%e8%af%81%e4%bf%9d%e8%af%81%e7%94%a8%e6%88%b7%e6%95%b0%e6%8d%ae%e4%b8%8d%e4%b8%a2%e5%a4%b1github%e4%b8%8d%e5%be%97%e4%b8%8d%e4%bb%a524%e5%b0%8f%e6%97%b6%e7%9a%84%e6%9c%8d%e5%8a%a1%e9%99%8d%e7%ba%a7%e4%b8%ba%e4%bb%a3%e4%bb%b7%e6%9d%a5%e4%bf%ae%e5%a4%8d%e6%95%b0%e6%8d%ae%e4%b8%80%e8%87%b4%e6%80%a7">#&lt;/a>
&lt;/h5>
&lt;h5 id="数据不一致的故障严重性不言而喻然而etcd是基于raft协议实现的分布式高可靠存储系统我们也并未做跨城容灾按理数据不一致这种看起来高大上bug我们是很难遇到的然而梦想是美好的现实是残酷的我们不仅遇到了不可思议的数据不一致bug-还一踩就是两个一个是重启etcd有较低的概率触发一个是升级etcd版本时如果开启了鉴权在k8s场景下较大概率触发在详细讨论这两个bug前我们先看看在k8s场景下etcd数据不一致会导致哪些问题呢">
 数据不一致的故障严重性不言而喻，然而etcd是基于raft协议实现的分布式高可靠存储系统，我们也并未做跨城容灾，按理数据不一致这种看起来高大上bug我们是很难遇到的。然而梦想是美好的，现实是残酷的，我们不仅遇到了不可思议的数据不一致bug, 还一踩就是两个，一个是重启etcd有较低的概率触发，一个是升级etcd版本时如果开启了鉴权，在K8s场景下较大概率触发。在详细讨论这两个bug前，我们先看看在K8s场景下etcd数据不一致会导致哪些问题呢？
 &lt;a class="anchor" href="#%e6%95%b0%e6%8d%ae%e4%b8%8d%e4%b8%80%e8%87%b4%e7%9a%84%e6%95%85%e9%9a%9c%e4%b8%a5%e9%87%8d%e6%80%a7%e4%b8%8d%e8%a8%80%e8%80%8c%e5%96%bb%e7%84%b6%e8%80%8cetcd%e6%98%af%e5%9f%ba%e4%ba%8eraft%e5%8d%8f%e8%ae%ae%e5%ae%9e%e7%8e%b0%e7%9a%84%e5%88%86%e5%b8%83%e5%bc%8f%e9%ab%98%e5%8f%af%e9%9d%a0%e5%ad%98%e5%82%a8%e7%b3%bb%e7%bb%9f%e6%88%91%e4%bb%ac%e4%b9%9f%e5%b9%b6%e6%9c%aa%e5%81%9a%e8%b7%a8%e5%9f%8e%e5%ae%b9%e7%81%be%e6%8c%89%e7%90%86%e6%95%b0%e6%8d%ae%e4%b8%8d%e4%b8%80%e8%87%b4%e8%bf%99%e7%a7%8d%e7%9c%8b%e8%b5%b7%e6%9d%a5%e9%ab%98%e5%a4%a7%e4%b8%8abug%e6%88%91%e4%bb%ac%e6%98%af%e5%be%88%e9%9a%be%e9%81%87%e5%88%b0%e7%9a%84%e7%84%b6%e8%80%8c%e6%a2%a6%e6%83%b3%e6%98%af%e7%be%8e%e5%a5%bd%e7%9a%84%e7%8e%b0%e5%ae%9e%e6%98%af%e6%ae%8b%e9%85%b7%e7%9a%84%e6%88%91%e4%bb%ac%e4%b8%8d%e4%bb%85%e9%81%87%e5%88%b0%e4%ba%86%e4%b8%8d%e5%8f%af%e6%80%9d%e8%ae%ae%e7%9a%84%e6%95%b0%e6%8d%ae%e4%b8%8d%e4%b8%80%e8%87%b4bug-%e8%bf%98%e4%b8%80%e8%b8%a9%e5%b0%b1%e6%98%af%e4%b8%a4%e4%b8%aa%e4%b8%80%e4%b8%aa%e6%98%af%e9%87%8d%e5%90%afetcd%e6%9c%89%e8%be%83%e4%bd%8e%e7%9a%84%e6%a6%82%e7%8e%87%e8%a7%a6%e5%8f%91%e4%b8%80%e4%b8%aa%e6%98%af%e5%8d%87%e7%ba%a7etcd%e7%89%88%e6%9c%ac%e6%97%b6%e5%a6%82%e6%9e%9c%e5%bc%80%e5%90%af%e4%ba%86%e9%89%b4%e6%9d%83%e5%9c%a8k8s%e5%9c%ba%e6%99%af%e4%b8%8b%e8%be%83%e5%a4%a7%e6%a6%82%e7%8e%87%e8%a7%a6%e5%8f%91%e5%9c%a8%e8%af%a6%e7%bb%86%e8%ae%a8%e8%ae%ba%e8%bf%99%e4%b8%a4%e4%b8%aabug%e5%89%8d%e6%88%91%e4%bb%ac%e5%85%88%e7%9c%8b%e7%9c%8b%e5%9c%a8k8s%e5%9c%ba%e6%99%af%e4%b8%8betcd%e6%95%b0%e6%8d%ae%e4%b8%8d%e4%b8%80%e8%87%b4%e4%bc%9a%e5%af%bc%e8%87%b4%e5%93%aa%e4%ba%9b%e9%97%ae%e9%a2%98%e5%91%a2">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="数据不一致最恐怖之处在于client写入是成功的但可能在部分节点读取到空或者是旧数据client无法感知到写入在部分节点是失败的和可能读到旧数据">
 数据不一致最恐怖之处在于client写入是成功的，但可能在部分节点读取到空或者是旧数据,client无法感知到写入在部分节点是失败的和可能读到旧数据
 &lt;a class="anchor" href="#%e6%95%b0%e6%8d%ae%e4%b8%8d%e4%b8%80%e8%87%b4%e6%9c%80%e6%81%90%e6%80%96%e4%b9%8b%e5%a4%84%e5%9c%a8%e4%ba%8eclient%e5%86%99%e5%85%a5%e6%98%af%e6%88%90%e5%8a%9f%e7%9a%84%e4%bd%86%e5%8f%af%e8%83%bd%e5%9c%a8%e9%83%a8%e5%88%86%e8%8a%82%e7%82%b9%e8%af%bb%e5%8f%96%e5%88%b0%e7%a9%ba%e6%88%96%e8%80%85%e6%98%af%e6%97%a7%e6%95%b0%e6%8d%aeclient%e6%97%a0%e6%b3%95%e6%84%9f%e7%9f%a5%e5%88%b0%e5%86%99%e5%85%a5%e5%9c%a8%e9%83%a8%e5%88%86%e8%8a%82%e7%82%b9%e6%98%af%e5%a4%b1%e8%b4%a5%e7%9a%84%e5%92%8c%e5%8f%af%e8%83%bd%e8%af%bb%e5%88%b0%e6%97%a7%e6%95%b0%e6%8d%ae">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="读到空可能会导致业务node消失pod消失node上service路由规则消失一般场景下只会影响用户变更的服务">
 读到空可能会导致业务Node消失、Pod消失、Node上Service路由规则消失，一般场景下，只会影响用户变更的服务
 &lt;a class="anchor" href="#%e8%af%bb%e5%88%b0%e7%a9%ba%e5%8f%af%e8%83%bd%e4%bc%9a%e5%af%bc%e8%87%b4%e4%b8%9a%e5%8a%a1node%e6%b6%88%e5%a4%b1pod%e6%b6%88%e5%a4%b1node%e4%b8%8aservice%e8%b7%af%e7%94%b1%e8%a7%84%e5%88%99%e6%b6%88%e5%a4%b1%e4%b8%80%e8%88%ac%e5%9c%ba%e6%99%af%e4%b8%8b%e5%8f%aa%e4%bc%9a%e5%bd%b1%e5%93%8d%e7%94%a8%e6%88%b7%e5%8f%98%e6%9b%b4%e7%9a%84%e6%9c%8d%e5%8a%a1">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="读到老数据会导致业务变更不生效如服务扩缩容service-rs替换变更镜像异常等待一般场景下只会影响用户变更的服务">
 读到老数据会导致业务变更不生效，如服务扩缩容、Service rs替换、变更镜像异常等待，一般场景下，只会影响用户变更的服务
 &lt;a class="anchor" href="#%e8%af%bb%e5%88%b0%e8%80%81%e6%95%b0%e6%8d%ae%e4%bc%9a%e5%af%bc%e8%87%b4%e4%b8%9a%e5%8a%a1%e5%8f%98%e6%9b%b4%e4%b8%8d%e7%94%9f%e6%95%88%e5%a6%82%e6%9c%8d%e5%8a%a1%e6%89%a9%e7%bc%a9%e5%ae%b9service-rs%e6%9b%bf%e6%8d%a2%e5%8f%98%e6%9b%b4%e9%95%9c%e5%83%8f%e5%bc%82%e5%b8%b8%e7%ad%89%e5%be%85%e4%b8%80%e8%88%ac%e5%9c%ba%e6%99%af%e4%b8%8b%e5%8f%aa%e4%bc%9a%e5%bd%b1%e5%93%8d%e7%94%a8%e6%88%b7%e5%8f%98%e6%9b%b4%e7%9a%84%e6%9c%8d%e5%8a%a1">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="在etcd平台迁移场景下client无法感知到写入失败若校验数据一致性也无异常时校验时连接到了正常节点会导致迁移后整个集群全面故障apiserver连接到了异常节点用户的node部署的服务lb等会被全部删除严重影响用户业务">
 在etcd平台迁移场景下，client无法感知到写入失败，若校验数据一致性也无异常时（校验时连接到了正常节点），会导致迁移后整个集群全面故障（apiserver连接到了异常节点），用户的Node、部署的服务、lb等会被全部删除，严重影响用户业务
 &lt;a class="anchor" href="#%e5%9c%a8etcd%e5%b9%b3%e5%8f%b0%e8%bf%81%e7%a7%bb%e5%9c%ba%e6%99%af%e4%b8%8bclient%e6%97%a0%e6%b3%95%e6%84%9f%e7%9f%a5%e5%88%b0%e5%86%99%e5%85%a5%e5%a4%b1%e8%b4%a5%e8%8b%a5%e6%a0%a1%e9%aa%8c%e6%95%b0%e6%8d%ae%e4%b8%80%e8%87%b4%e6%80%a7%e4%b9%9f%e6%97%a0%e5%bc%82%e5%b8%b8%e6%97%b6%e6%a0%a1%e9%aa%8c%e6%97%b6%e8%bf%9e%e6%8e%a5%e5%88%b0%e4%ba%86%e6%ad%a3%e5%b8%b8%e8%8a%82%e7%82%b9%e4%bc%9a%e5%af%bc%e8%87%b4%e8%bf%81%e7%a7%bb%e5%90%8e%e6%95%b4%e4%b8%aa%e9%9b%86%e7%be%a4%e5%85%a8%e9%9d%a2%e6%95%85%e9%9a%9capiserver%e8%bf%9e%e6%8e%a5%e5%88%b0%e4%ba%86%e5%bc%82%e5%b8%b8%e8%8a%82%e7%82%b9%e7%94%a8%e6%88%b7%e7%9a%84node%e9%83%a8%e7%bd%b2%e7%9a%84%e6%9c%8d%e5%8a%a1lb%e7%ad%89%e4%bc%9a%e8%a2%ab%e5%85%a8%e9%83%a8%e5%88%a0%e9%99%a4%e4%b8%a5%e9%87%8d%e5%bd%b1%e5%93%8d%e7%94%a8%e6%88%b7%e4%b8%9a%e5%8a%a1">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h5 id="首先第一个不一致bug是重启etcd过程中遇到的人工尝试复现多次皆失败分析定位复现解决这个bug之路几经波折过程很有趣并充满挑战最终通过我对关键点增加debug日志编写chaos-monkey模拟各种异常场景边界条件实现复现成功最后的真凶竟然是一个授权接口在重启后重放导致鉴权版本号不一致然后放大导致多版本数据库不一致-部分节点无法写入新数据-影响所有v3版本的3年之久bug">
 首先第一个不一致bug是重启etcd过程中遇到的，人工尝试复现多次皆失败，分析、定位、复现、解决这个bug之路几经波折，过程很有趣并充满挑战，最终通过我对关键点增加debug日志，编写chaos monkey模拟各种异常场景、边界条件，实现复现成功。最后的真凶竟然是一个授权接口在重启后重放导致鉴权版本号不一致，然后放大导致多版本数据库不一致, 部分节点无法写入新数据, 影响所有v3版本的3年之久bug。
 &lt;a class="anchor" href="#%e9%a6%96%e5%85%88%e7%ac%ac%e4%b8%80%e4%b8%aa%e4%b8%8d%e4%b8%80%e8%87%b4bug%e6%98%af%e9%87%8d%e5%90%afetcd%e8%bf%87%e7%a8%8b%e4%b8%ad%e9%81%87%e5%88%b0%e7%9a%84%e4%ba%ba%e5%b7%a5%e5%b0%9d%e8%af%95%e5%a4%8d%e7%8e%b0%e5%a4%9a%e6%ac%a1%e7%9a%86%e5%a4%b1%e8%b4%a5%e5%88%86%e6%9e%90%e5%ae%9a%e4%bd%8d%e5%a4%8d%e7%8e%b0%e8%a7%a3%e5%86%b3%e8%bf%99%e4%b8%aabug%e4%b9%8b%e8%b7%af%e5%87%a0%e7%bb%8f%e6%b3%a2%e6%8a%98%e8%bf%87%e7%a8%8b%e5%be%88%e6%9c%89%e8%b6%a3%e5%b9%b6%e5%85%85%e6%bb%a1%e6%8c%91%e6%88%98%e6%9c%80%e7%bb%88%e9%80%9a%e8%bf%87%e6%88%91%e5%af%b9%e5%85%b3%e9%94%ae%e7%82%b9%e5%a2%9e%e5%8a%a0debug%e6%97%a5%e5%bf%97%e7%bc%96%e5%86%99chaos-monkey%e6%a8%a1%e6%8b%9f%e5%90%84%e7%a7%8d%e5%bc%82%e5%b8%b8%e5%9c%ba%e6%99%af%e8%be%b9%e7%95%8c%e6%9d%a1%e4%bb%b6%e5%ae%9e%e7%8e%b0%e5%a4%8d%e7%8e%b0%e6%88%90%e5%8a%9f%e6%9c%80%e5%90%8e%e7%9a%84%e7%9c%9f%e5%87%b6%e7%ab%9f%e7%84%b6%e6%98%af%e4%b8%80%e4%b8%aa%e6%8e%88%e6%9d%83%e6%8e%a5%e5%8f%a3%e5%9c%a8%e9%87%8d%e5%90%af%e5%90%8e%e9%87%8d%e6%94%be%e5%af%bc%e8%87%b4%e9%89%b4%e6%9d%83%e7%89%88%e6%9c%ac%e5%8f%b7%e4%b8%8d%e4%b8%80%e8%87%b4%e7%84%b6%e5%90%8e%e6%94%be%e5%a4%a7%e5%af%bc%e8%87%b4%e5%a4%9a%e7%89%88%e6%9c%ac%e6%95%b0%e6%8d%ae%e5%ba%93%e4%b8%8d%e4%b8%80%e8%87%b4-%e9%83%a8%e5%88%86%e8%8a%82%e7%82%b9%e6%97%a0%e6%b3%95%e5%86%99%e5%85%a5%e6%96%b0%e6%95%b0%e6%8d%ae-%e5%bd%b1%e5%93%8d%e6%89%80%e6%9c%89v3%e7%89%88%e6%9c%ac%e7%9a%843%e5%b9%b4%e4%b9%8b%e4%b9%85bug">#&lt;/a>
&lt;/h5>
&lt;h5 id="随后我们提交若干个相关pr到社区-并全部合并了-最新的etcd-v3491v33222已修复此问题-同时google的jingyih也已经提k8s-issue和pr3将k8s-119的etcd-client及server版本升级到最新的v349此bug详细可参考超凡同学写的文章三年之久的-etcd3-数据不一致-bug-分析">
 随后我们提交若干个相关pr到社区, 并全部合并了, 最新的etcd &lt;strong>v3.4.9&lt;/strong>[1]，&lt;strong>v3.3.22&lt;/strong>[2]已修复此问题, 同时google的jingyih也已经提&lt;strong>K8s issue和pr&lt;/strong>[3]将K8s 1.19的etcd client及server版本升级到最新的v3.4.9。此bug详细可参考超凡同学写的文章&lt;a href="http://mp.weixin.qq.com/s?__biz=Mzg5NjA1MjkxNw==&amp;amp;mid=2247483930&amp;amp;idx=1&amp;amp;sn=536046b272038b92a4ddf4ec30f8aad4&amp;amp;chksm=c007b9c0f77030d6bf9a5038906dfb553b5a618d95578bfa65798ff1bf418c8dde216806a861&amp;amp;scene=21#wechat_redirect">三年之久的 etcd3 数据不一致 bug 分析&lt;/a>。
 &lt;a class="anchor" href="#%e9%9a%8f%e5%90%8e%e6%88%91%e4%bb%ac%e6%8f%90%e4%ba%a4%e8%8b%a5%e5%b9%b2%e4%b8%aa%e7%9b%b8%e5%85%b3pr%e5%88%b0%e7%a4%be%e5%8c%ba-%e5%b9%b6%e5%85%a8%e9%83%a8%e5%90%88%e5%b9%b6%e4%ba%86-%e6%9c%80%e6%96%b0%e7%9a%84etcd-v3491v33222%e5%b7%b2%e4%bf%ae%e5%a4%8d%e6%ad%a4%e9%97%ae%e9%a2%98-%e5%90%8c%e6%97%b6google%e7%9a%84jingyih%e4%b9%9f%e5%b7%b2%e7%bb%8f%e6%8f%90k8s-issue%e5%92%8cpr3%e5%b0%86k8s-119%e7%9a%84etcd-client%e5%8f%8aserver%e7%89%88%e6%9c%ac%e5%8d%87%e7%ba%a7%e5%88%b0%e6%9c%80%e6%96%b0%e7%9a%84v349%e6%ad%a4bug%e8%af%a6%e7%bb%86%e5%8f%af%e5%8f%82%e8%80%83%e8%b6%85%e5%87%a1%e5%90%8c%e5%ad%a6%e5%86%99%e7%9a%84%e6%96%87%e7%ab%a0%e4%b8%89%e5%b9%b4%e4%b9%8b%e4%b9%85%e7%9a%84-etcd3-%e6%95%b0%e6%8d%ae%e4%b8%8d%e4%b8%80%e8%87%b4-bug-%e5%88%86%e6%9e%90">#&lt;/a>
&lt;/h5>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403011117778.png" alt="image-20240301111757719" />&lt;/p></description></item><item><title>2024-04-03 flanel网络</title><link>https://qq547475331.github.io/docs/flanel%E7%BD%91%E7%BB%9C-flanel-wang-luo/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/flanel%E7%BD%91%E7%BB%9C-flanel-wang-luo/</guid><description>&lt;h5 id="近几年企业基础设施云原生化的趋势越来越强烈从最开始的-iaas-化到现在的微服务化客户的颗粒度精细化和可观测性的需求更加强烈容器网络为了满足客户更高性能和更高的密度也一直在高速的发展和演进中这必然对客户对云原生网络的可观测性带来了极高的门槛和挑战为了提高云原生网络的可观测性同时便于客户和前后线同学增加对业务链路的可读性ack-产研和-aes-联合共建合作开发-ack-net-exporter-和云原生网络数据面可观测性系列帮助客户和前后线同学了解云原生网络架构体系简化对云原生网络的可观测性的门槛优化客户运维和售后同学处理疑难问题的体验-提高云原生网络的链路的稳定性">
 近几年，企业基础设施云原生化的趋势越来越强烈，从最开始的 IaaS 化到现在的微服务化，客户的颗粒度精细化和可观测性的需求更加强烈。容器网络为了满足客户更高性能和更高的密度，也一直在高速的发展和演进中，这必然对客户对云原生网络的可观测性带来了极高的门槛和挑战。为了提高云原生网络的可观测性，同时便于客户和前后线同学增加对业务链路的可读性，ACK 产研和 AES 联合共建，合作开发 ack net-exporter 和云原生网络数据面可观测性系列，帮助客户和前后线同学了解云原生网络架构体系，简化对云原生网络的可观测性的门槛，优化客户运维和售后同学处理疑难问题的体验 ，提高云原生网络的链路的稳定性。
 &lt;a class="anchor" href="#%e8%bf%91%e5%87%a0%e5%b9%b4%e4%bc%81%e4%b8%9a%e5%9f%ba%e7%a1%80%e8%ae%be%e6%96%bd%e4%ba%91%e5%8e%9f%e7%94%9f%e5%8c%96%e7%9a%84%e8%b6%8b%e5%8a%bf%e8%b6%8a%e6%9d%a5%e8%b6%8a%e5%bc%ba%e7%83%88%e4%bb%8e%e6%9c%80%e5%bc%80%e5%a7%8b%e7%9a%84-iaas-%e5%8c%96%e5%88%b0%e7%8e%b0%e5%9c%a8%e7%9a%84%e5%be%ae%e6%9c%8d%e5%8a%a1%e5%8c%96%e5%ae%a2%e6%88%b7%e7%9a%84%e9%a2%97%e7%b2%92%e5%ba%a6%e7%b2%be%e7%bb%86%e5%8c%96%e5%92%8c%e5%8f%af%e8%a7%82%e6%b5%8b%e6%80%a7%e7%9a%84%e9%9c%80%e6%b1%82%e6%9b%b4%e5%8a%a0%e5%bc%ba%e7%83%88%e5%ae%b9%e5%99%a8%e7%bd%91%e7%bb%9c%e4%b8%ba%e4%ba%86%e6%bb%a1%e8%b6%b3%e5%ae%a2%e6%88%b7%e6%9b%b4%e9%ab%98%e6%80%a7%e8%83%bd%e5%92%8c%e6%9b%b4%e9%ab%98%e7%9a%84%e5%af%86%e5%ba%a6%e4%b9%9f%e4%b8%80%e7%9b%b4%e5%9c%a8%e9%ab%98%e9%80%9f%e7%9a%84%e5%8f%91%e5%b1%95%e5%92%8c%e6%bc%94%e8%bf%9b%e4%b8%ad%e8%bf%99%e5%bf%85%e7%84%b6%e5%af%b9%e5%ae%a2%e6%88%b7%e5%af%b9%e4%ba%91%e5%8e%9f%e7%94%9f%e7%bd%91%e7%bb%9c%e7%9a%84%e5%8f%af%e8%a7%82%e6%b5%8b%e6%80%a7%e5%b8%a6%e6%9d%a5%e4%ba%86%e6%9e%81%e9%ab%98%e7%9a%84%e9%97%a8%e6%a7%9b%e5%92%8c%e6%8c%91%e6%88%98%e4%b8%ba%e4%ba%86%e6%8f%90%e9%ab%98%e4%ba%91%e5%8e%9f%e7%94%9f%e7%bd%91%e7%bb%9c%e7%9a%84%e5%8f%af%e8%a7%82%e6%b5%8b%e6%80%a7%e5%90%8c%e6%97%b6%e4%be%bf%e4%ba%8e%e5%ae%a2%e6%88%b7%e5%92%8c%e5%89%8d%e5%90%8e%e7%ba%bf%e5%90%8c%e5%ad%a6%e5%a2%9e%e5%8a%a0%e5%af%b9%e4%b8%9a%e5%8a%a1%e9%93%be%e8%b7%af%e7%9a%84%e5%8f%af%e8%af%bb%e6%80%a7ack-%e4%ba%a7%e7%a0%94%e5%92%8c-aes-%e8%81%94%e5%90%88%e5%85%b1%e5%bb%ba%e5%90%88%e4%bd%9c%e5%bc%80%e5%8f%91-ack-net-exporter-%e5%92%8c%e4%ba%91%e5%8e%9f%e7%94%9f%e7%bd%91%e7%bb%9c%e6%95%b0%e6%8d%ae%e9%9d%a2%e5%8f%af%e8%a7%82%e6%b5%8b%e6%80%a7%e7%b3%bb%e5%88%97%e5%b8%ae%e5%8a%a9%e5%ae%a2%e6%88%b7%e5%92%8c%e5%89%8d%e5%90%8e%e7%ba%bf%e5%90%8c%e5%ad%a6%e4%ba%86%e8%a7%a3%e4%ba%91%e5%8e%9f%e7%94%9f%e7%bd%91%e7%bb%9c%e6%9e%b6%e6%9e%84%e4%bd%93%e7%b3%bb%e7%ae%80%e5%8c%96%e5%af%b9%e4%ba%91%e5%8e%9f%e7%94%9f%e7%bd%91%e7%bb%9c%e7%9a%84%e5%8f%af%e8%a7%82%e6%b5%8b%e6%80%a7%e7%9a%84%e9%97%a8%e6%a7%9b%e4%bc%98%e5%8c%96%e5%ae%a2%e6%88%b7%e8%bf%90%e7%bb%b4%e5%92%8c%e5%94%ae%e5%90%8e%e5%90%8c%e5%ad%a6%e5%a4%84%e7%90%86%e7%96%91%e9%9a%be%e9%97%ae%e9%a2%98%e7%9a%84%e4%bd%93%e9%aa%8c-%e6%8f%90%e9%ab%98%e4%ba%91%e5%8e%9f%e7%94%9f%e7%bd%91%e7%bb%9c%e7%9a%84%e9%93%be%e8%b7%af%e7%9a%84%e7%a8%b3%e5%ae%9a%e6%80%a7">#&lt;/a>
&lt;/h5>
&lt;h5 id="鸟瞰容器网络整个容器网络可以分为三个部分pod-网段service-网段和-node-网段这三个网络要实现互联互通和访问控制那么实现的技术原理是什么整个链路又是什么限制又是什么呢flannel-terway-有啥区别不同模式下网络性能如何这些需要客户在下搭建容器之前就要依据自己的业务场景进行选择而搭建完毕后相关的架构又是无法转变所以客户需要对每种架构特点要有充分了解比如下图是个简图pod-网络既要实现同一个ecs的pod间的网络互通和控制又要实现不同-ecs-pod-间的访问-pod-访问-svc-的后端可能在同一个-ecs-也可能是其他-ecs这些在不同模式下数据链转发模式是不同的从业务侧表现结果也是不一样的">
 鸟瞰容器网络，整个容器网络可以分为三个部分：Pod 网段，Service 网段和 Node 网段。这三个网络要实现互联互通和访问控制，那么实现的技术原理是什么？整个链路又是什么，限制又是什么呢？Flannel， Terway 有啥区别？不同模式下网络性能如何？这些，需要客户在下搭建容器之前，就要依据自己的业务场景进行选择，而搭建完毕后，相关的架构又是无法转变，所以客户需要对每种架构特点要有充分了解。比如下图是个简图，Pod 网络既要实现同一个ECS的Pod间的网络互通和控制，又要实现不同 ECS Pod 间的访问， Pod 访问 SVC 的后端可能在同一个 ECS 也可能是其他 ECS，这些在不同模式下，数据链转发模式是不同的，从业务侧表现结果也是不一样的。
 &lt;a class="anchor" href="#%e9%b8%9f%e7%9e%b0%e5%ae%b9%e5%99%a8%e7%bd%91%e7%bb%9c%e6%95%b4%e4%b8%aa%e5%ae%b9%e5%99%a8%e7%bd%91%e7%bb%9c%e5%8f%af%e4%bb%a5%e5%88%86%e4%b8%ba%e4%b8%89%e4%b8%aa%e9%83%a8%e5%88%86pod-%e7%bd%91%e6%ae%b5service-%e7%bd%91%e6%ae%b5%e5%92%8c-node-%e7%bd%91%e6%ae%b5%e8%bf%99%e4%b8%89%e4%b8%aa%e7%bd%91%e7%bb%9c%e8%a6%81%e5%ae%9e%e7%8e%b0%e4%ba%92%e8%81%94%e4%ba%92%e9%80%9a%e5%92%8c%e8%ae%bf%e9%97%ae%e6%8e%a7%e5%88%b6%e9%82%a3%e4%b9%88%e5%ae%9e%e7%8e%b0%e7%9a%84%e6%8a%80%e6%9c%af%e5%8e%9f%e7%90%86%e6%98%af%e4%bb%80%e4%b9%88%e6%95%b4%e4%b8%aa%e9%93%be%e8%b7%af%e5%8f%88%e6%98%af%e4%bb%80%e4%b9%88%e9%99%90%e5%88%b6%e5%8f%88%e6%98%af%e4%bb%80%e4%b9%88%e5%91%a2flannel-terway-%e6%9c%89%e5%95%a5%e5%8c%ba%e5%88%ab%e4%b8%8d%e5%90%8c%e6%a8%a1%e5%bc%8f%e4%b8%8b%e7%bd%91%e7%bb%9c%e6%80%a7%e8%83%bd%e5%a6%82%e4%bd%95%e8%bf%99%e4%ba%9b%e9%9c%80%e8%a6%81%e5%ae%a2%e6%88%b7%e5%9c%a8%e4%b8%8b%e6%90%ad%e5%bb%ba%e5%ae%b9%e5%99%a8%e4%b9%8b%e5%89%8d%e5%b0%b1%e8%a6%81%e4%be%9d%e6%8d%ae%e8%87%aa%e5%b7%b1%e7%9a%84%e4%b8%9a%e5%8a%a1%e5%9c%ba%e6%99%af%e8%bf%9b%e8%a1%8c%e9%80%89%e6%8b%a9%e8%80%8c%e6%90%ad%e5%bb%ba%e5%ae%8c%e6%af%95%e5%90%8e%e7%9b%b8%e5%85%b3%e7%9a%84%e6%9e%b6%e6%9e%84%e5%8f%88%e6%98%af%e6%97%a0%e6%b3%95%e8%bd%ac%e5%8f%98%e6%89%80%e4%bb%a5%e5%ae%a2%e6%88%b7%e9%9c%80%e8%a6%81%e5%af%b9%e6%af%8f%e7%a7%8d%e6%9e%b6%e6%9e%84%e7%89%b9%e7%82%b9%e8%a6%81%e6%9c%89%e5%85%85%e5%88%86%e4%ba%86%e8%a7%a3%e6%af%94%e5%a6%82%e4%b8%8b%e5%9b%be%e6%98%af%e4%b8%aa%e7%ae%80%e5%9b%bepod-%e7%bd%91%e7%bb%9c%e6%97%a2%e8%a6%81%e5%ae%9e%e7%8e%b0%e5%90%8c%e4%b8%80%e4%b8%aaecs%e7%9a%84pod%e9%97%b4%e7%9a%84%e7%bd%91%e7%bb%9c%e4%ba%92%e9%80%9a%e5%92%8c%e6%8e%a7%e5%88%b6%e5%8f%88%e8%a6%81%e5%ae%9e%e7%8e%b0%e4%b8%8d%e5%90%8c-ecs-pod-%e9%97%b4%e7%9a%84%e8%ae%bf%e9%97%ae-pod-%e8%ae%bf%e9%97%ae-svc-%e7%9a%84%e5%90%8e%e7%ab%af%e5%8f%af%e8%83%bd%e5%9c%a8%e5%90%8c%e4%b8%80%e4%b8%aa-ecs-%e4%b9%9f%e5%8f%af%e8%83%bd%e6%98%af%e5%85%b6%e4%bb%96-ecs%e8%bf%99%e4%ba%9b%e5%9c%a8%e4%b8%8d%e5%90%8c%e6%a8%a1%e5%bc%8f%e4%b8%8b%e6%95%b0%e6%8d%ae%e9%93%be%e8%bd%ac%e5%8f%91%e6%a8%a1%e5%bc%8f%e6%98%af%e4%b8%8d%e5%90%8c%e7%9a%84%e4%bb%8e%e4%b8%9a%e5%8a%a1%e4%be%a7%e8%a1%a8%e7%8e%b0%e7%bb%93%e6%9e%9c%e4%b9%9f%e6%98%af%e4%b8%8d%e4%b8%80%e6%a0%b7%e7%9a%84">#&lt;/a>
&lt;/h5>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403011511760.png" alt="image-20240301151100557" />&lt;/p>
&lt;h5 id="本文是全景剖析容器网络数据链路第一部分主要介绍-kubernetes-flannel-模式下数据面链路的转转发链路一是通过了解不同场景下的数据面转发链路从而探知客户在不同的场景下访问结果表现的原因帮助客户进一步优化业务架构另一方面通过深入了解转发链路从而在遇到容器网络抖动时候客户运维以及阿里云同学可以知道在哪些链路点进行部署观测手动从而进一步定界问题方向和原因">
 本文是[全景剖析容器网络数据链路]第一部分，主要介绍 Kubernetes Flannel 模式下，数据面链路的转转发链路，一是通过了解不同场景下的数据面转发链路，从而探知客户在不同的场景下访问结果表现的原因，帮助客户进一步优化业务架构；另一方面，通过深入了解转发链路，从而在遇到容器网络抖动时候，客户运维以及阿里云同学可以知道在哪些链路点进行部署观测手动，从而进一步定界问题方向和原因。
 &lt;a class="anchor" href="#%e6%9c%ac%e6%96%87%e6%98%af%e5%85%a8%e6%99%af%e5%89%96%e6%9e%90%e5%ae%b9%e5%99%a8%e7%bd%91%e7%bb%9c%e6%95%b0%e6%8d%ae%e9%93%be%e8%b7%af%e7%ac%ac%e4%b8%80%e9%83%a8%e5%88%86%e4%b8%bb%e8%a6%81%e4%bb%8b%e7%bb%8d-kubernetes-flannel-%e6%a8%a1%e5%bc%8f%e4%b8%8b%e6%95%b0%e6%8d%ae%e9%9d%a2%e9%93%be%e8%b7%af%e7%9a%84%e8%bd%ac%e8%bd%ac%e5%8f%91%e9%93%be%e8%b7%af%e4%b8%80%e6%98%af%e9%80%9a%e8%bf%87%e4%ba%86%e8%a7%a3%e4%b8%8d%e5%90%8c%e5%9c%ba%e6%99%af%e4%b8%8b%e7%9a%84%e6%95%b0%e6%8d%ae%e9%9d%a2%e8%bd%ac%e5%8f%91%e9%93%be%e8%b7%af%e4%bb%8e%e8%80%8c%e6%8e%a2%e7%9f%a5%e5%ae%a2%e6%88%b7%e5%9c%a8%e4%b8%8d%e5%90%8c%e7%9a%84%e5%9c%ba%e6%99%af%e4%b8%8b%e8%ae%bf%e9%97%ae%e7%bb%93%e6%9e%9c%e8%a1%a8%e7%8e%b0%e7%9a%84%e5%8e%9f%e5%9b%a0%e5%b8%ae%e5%8a%a9%e5%ae%a2%e6%88%b7%e8%bf%9b%e4%b8%80%e6%ad%a5%e4%bc%98%e5%8c%96%e4%b8%9a%e5%8a%a1%e6%9e%b6%e6%9e%84%e5%8f%a6%e4%b8%80%e6%96%b9%e9%9d%a2%e9%80%9a%e8%bf%87%e6%b7%b1%e5%85%a5%e4%ba%86%e8%a7%a3%e8%bd%ac%e5%8f%91%e9%93%be%e8%b7%af%e4%bb%8e%e8%80%8c%e5%9c%a8%e9%81%87%e5%88%b0%e5%ae%b9%e5%99%a8%e7%bd%91%e7%bb%9c%e6%8a%96%e5%8a%a8%e6%97%b6%e5%80%99%e5%ae%a2%e6%88%b7%e8%bf%90%e7%bb%b4%e4%bb%a5%e5%8f%8a%e9%98%bf%e9%87%8c%e4%ba%91%e5%90%8c%e5%ad%a6%e5%8f%af%e4%bb%a5%e7%9f%a5%e9%81%93%e5%9c%a8%e5%93%aa%e4%ba%9b%e9%93%be%e8%b7%af%e7%82%b9%e8%bf%9b%e8%a1%8c%e9%83%a8%e7%bd%b2%e8%a7%82%e6%b5%8b%e6%89%8b%e5%8a%a8%e4%bb%8e%e8%80%8c%e8%bf%9b%e4%b8%80%e6%ad%a5%e5%ae%9a%e7%95%8c%e9%97%ae%e9%a2%98%e6%96%b9%e5%90%91%e5%92%8c%e5%8e%9f%e5%9b%a0">#&lt;/a>
&lt;/h5>
&lt;p>系列二：&lt;/p>
&lt;p>全景剖析阿里云容器网络数据链路（二）：Terway ENI&lt;/p>
&lt;p>系列三：&lt;/p>
&lt;p>全景剖析阿里云容器网络数据链路（三）：Terway ENIIP&lt;/p>
&lt;p>系列四：&lt;/p>
&lt;p>全景剖析阿里云容器网络数据链路（四）：Terway IPVLAN+EBPF&lt;/p>
&lt;p>系列五：&lt;/p>
&lt;p>全景剖析阿里云容器网络数据链路（五）：Terway ENI-Trunking&lt;/p>
&lt;p>系列六：&lt;/p>
&lt;p>全景剖析阿里云容器网络数据链路（六）：ASM Istio&lt;/p>
&lt;p>&lt;em>&lt;strong>01&lt;/strong>&lt;/em>&lt;/p>
&lt;p>&lt;strong>Flannel 模式架构设计&lt;/strong>&lt;/p>
&lt;p>Flannel 模式下，ECS 只有一个主网卡 ENI，无其他附属网卡，ECS 和节点上的 Pod 与外部通信都需要通过主网卡进行。ACK Flannel 会在每个节点创建 cni0 虚拟网卡作为 Pod 网络和 ECS 的主网卡 eth0 之间的桥梁。&lt;/p></description></item><item><title>2024-04-03 helm chart和repo</title><link>https://qq547475331.github.io/docs/helm-chart%E5%92%8Crepo-helmchart-he-repo/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/helm-chart%E5%92%8Crepo-helmchart-he-repo/</guid><description>&lt;h3 id="chart构成">
 chart构成
 &lt;a class="anchor" href="#chart%e6%9e%84%e6%88%90">#&lt;/a>
&lt;/h3>
&lt;p>创建一个名为mychart的chart，查看文件结构&lt;/p>
&lt;pre tabindex="0">&lt;code class="language-avrasm" data-lang="avrasm">helm create mychart
[root@k8s-master ~]# tree mychart/
mychart/
├── charts
├── Chart.yaml
├── templates
│ ├── deployment.yaml
│ ├── _helpers.tpl
│ ├── ingress.yaml
│ ├── NOTES.txt
│ └── service.yaml
└── values.yaml
&lt;/code>&lt;/pre>&lt;h5 id="所有kubenetes要执行的yaml模板都存放在templates文件夹下例如上个例子中存放了deploymentserviceingress三个kubenetes资源对象">
 所有kubenetes要执行的yaml模板都存放在templates文件夹下，例如上个例子中存放了deployment，service，ingress三个kubenetes资源对象。
 &lt;a class="anchor" href="#%e6%89%80%e6%9c%89kubenetes%e8%a6%81%e6%89%a7%e8%a1%8c%e7%9a%84yaml%e6%a8%a1%e6%9d%bf%e9%83%bd%e5%ad%98%e6%94%be%e5%9c%a8templates%e6%96%87%e4%bb%b6%e5%a4%b9%e4%b8%8b%e4%be%8b%e5%a6%82%e4%b8%8a%e4%b8%aa%e4%be%8b%e5%ad%90%e4%b8%ad%e5%ad%98%e6%94%be%e4%ba%86deploymentserviceingress%e4%b8%89%e4%b8%aakubenetes%e8%b5%84%e6%ba%90%e5%af%b9%e8%b1%a1">#&lt;/a>
&lt;/h5>
&lt;h5 id="valuesyaml文件存放yaml模板中定义的默认值">
 values.yaml文件存放yaml模板中定义的默认值。
 &lt;a class="anchor" href="#valuesyaml%e6%96%87%e4%bb%b6%e5%ad%98%e6%94%beyaml%e6%a8%a1%e6%9d%bf%e4%b8%ad%e5%ae%9a%e4%b9%89%e7%9a%84%e9%bb%98%e8%ae%a4%e5%80%bc">#&lt;/a>
&lt;/h5>
&lt;h5 id="chartyaml存放cahrt的版本信息">
 Chart.yaml存放cahrt的版本信息。
 &lt;a class="anchor" href="#chartyaml%e5%ad%98%e6%94%becahrt%e7%9a%84%e7%89%88%e6%9c%ac%e4%bf%a1%e6%81%af">#&lt;/a>
&lt;/h5>
&lt;h5 id="notestxt显示cahrt的release运行后的帮助信息">
 NOTES.txt显示cahrt的release运行后的帮助信息。
 &lt;a class="anchor" href="#notestxt%e6%98%be%e7%a4%bacahrt%e7%9a%84release%e8%bf%90%e8%a1%8c%e5%90%8e%e7%9a%84%e5%b8%ae%e5%8a%a9%e4%bf%a1%e6%81%af">#&lt;/a>
&lt;/h5>
&lt;h3 id="helm转移chart">
 helm转移chart
 &lt;a class="anchor" href="#helm%e8%bd%ac%e7%a7%bbchart">#&lt;/a>
&lt;/h3>
&lt;h5 id="首先编辑和配置好本地的chart文件然后使用helm打包成tar文件">
 首先，编辑和配置好本地的chart文件，然后使用helm打包成tar文件。
 &lt;a class="anchor" href="#%e9%a6%96%e5%85%88%e7%bc%96%e8%be%91%e5%92%8c%e9%85%8d%e7%bd%ae%e5%a5%bd%e6%9c%ac%e5%9c%b0%e7%9a%84chart%e6%96%87%e4%bb%b6%e7%84%b6%e5%90%8e%e4%bd%bf%e7%94%a8helm%e6%89%93%e5%8c%85%e6%88%90tar%e6%96%87%e4%bb%b6">#&lt;/a>
&lt;/h5>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-lua" data-lang="lua">&lt;span style="display:flex;">&lt;span>helm package .&lt;span style="color:#f92672">/&lt;/span>mychart
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h5 id="把tar包拷入到另一个环境通过helm-install命令指定tar名称导入">
 把tar包拷入到另一个环境，通过helm install命令指定tar名称导入。
 &lt;a class="anchor" href="#%e6%8a%8atar%e5%8c%85%e6%8b%b7%e5%85%a5%e5%88%b0%e5%8f%a6%e4%b8%80%e4%b8%aa%e7%8e%af%e5%a2%83%e9%80%9a%e8%bf%87helm-install%e5%91%bd%e4%bb%a4%e6%8c%87%e5%ae%9atar%e5%90%8d%e7%a7%b0%e5%af%bc%e5%85%a5">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code class="language-avrasm" data-lang="avrasm">helm install --name example3 mychart-0.1.0.tgz --set service.type=NodePort
&lt;/code>&lt;/pre>&lt;h4 id="helm本地repo">
 helm本地repo
 &lt;a class="anchor" href="#helm%e6%9c%ac%e5%9c%b0repo">#&lt;/a>
&lt;/h4>
&lt;h5 id="helm可以启动一个本地http服务器作为一个为本地chart的repo服务">
 helm可以启动一个本地HTTP服务器，作为一个为本地chart的repo服务。
 &lt;a class="anchor" href="#helm%e5%8f%af%e4%bb%a5%e5%90%af%e5%8a%a8%e4%b8%80%e4%b8%aa%e6%9c%ac%e5%9c%b0http%e6%9c%8d%e5%8a%a1%e5%99%a8%e4%bd%9c%e4%b8%ba%e4%b8%80%e4%b8%aa%e4%b8%ba%e6%9c%ac%e5%9c%b0chart%e7%9a%84repo%e6%9c%8d%e5%8a%a1">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code class="language-undefined" data-lang="undefined">helm serve
&lt;/code>&lt;/pre>&lt;h5 id="另开一个终端可以搜索和安装本地repository中的chart">
 另开一个终端，可以搜索和安装本地repository中的chart。
 &lt;a class="anchor" href="#%e5%8f%a6%e5%bc%80%e4%b8%80%e4%b8%aa%e7%bb%88%e7%ab%af%e5%8f%af%e4%bb%a5%e6%90%9c%e7%b4%a2%e5%92%8c%e5%ae%89%e8%a3%85%e6%9c%ac%e5%9c%b0repository%e4%b8%ad%e7%9a%84chart">#&lt;/a>
&lt;/h5>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-lua" data-lang="lua">&lt;span style="display:flex;">&lt;span>helm search &lt;span style="color:#66d9ef">local&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre tabindex="0">&lt;code class="language-delphi" data-lang="delphi">helm install --name example4 local/mychart --set service.type=NodePort
&lt;/code>&lt;/pre>&lt;h3 id="chart中定义依赖">
 chart中定义依赖
 &lt;a class="anchor" href="#chart%e4%b8%ad%e5%ae%9a%e4%b9%89%e4%be%9d%e8%b5%96">#&lt;/a>
&lt;/h3>
&lt;h5 id="可以在chart目录中创建一个requirementsyaml文件定义该chart的依赖">
 可以在chart目录中创建一个requirements.yaml文件定义该chart的依赖。
 &lt;a class="anchor" href="#%e5%8f%af%e4%bb%a5%e5%9c%a8chart%e7%9b%ae%e5%bd%95%e4%b8%ad%e5%88%9b%e5%bb%ba%e4%b8%80%e4%b8%aarequirementsyaml%e6%96%87%e4%bb%b6%e5%ae%9a%e4%b9%89%e8%af%a5chart%e7%9a%84%e4%be%9d%e8%b5%96">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code class="language-avrasm" data-lang="avrasm">$ cat &amp;gt; ./mychart/requirements.yaml &amp;lt;&amp;lt;EOF
dependencies:
- name: mariadb
 version: 0.6.0
 repository: https://kubernetes-charts.storage.googleapis.com
EOF
&lt;/code>&lt;/pre>&lt;h5 id="通过helm命令更新和下载cahrt的依赖">
 通过helm命令更新和下载cahrt的依赖
 &lt;a class="anchor" href="#%e9%80%9a%e8%bf%87helm%e5%91%bd%e4%bb%a4%e6%9b%b4%e6%96%b0%e5%92%8c%e4%b8%8b%e8%bd%bdcahrt%e7%9a%84%e4%be%9d%e8%b5%96">#&lt;/a>
&lt;/h5>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sql" data-lang="sql">&lt;span style="display:flex;">&lt;span>helm dep &lt;span style="color:#66d9ef">update&lt;/span> .&lt;span style="color:#f92672">/&lt;/span>mychart
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h5 id="在次安装运行chart时会把依赖中定义的chart运行起来">
 在次安装运行chart时会把依赖中定义的chart运行起来。
 &lt;a class="anchor" href="#%e5%9c%a8%e6%ac%a1%e5%ae%89%e8%a3%85%e8%bf%90%e8%a1%8cchart%e6%97%b6%e4%bc%9a%e6%8a%8a%e4%be%9d%e8%b5%96%e4%b8%ad%e5%ae%9a%e4%b9%89%e7%9a%84chart%e8%bf%90%e8%a1%8c%e8%b5%b7%e6%9d%a5">#&lt;/a>
&lt;/h5>
&lt;h3 id="自定义chart-repository">
 自定义chart repository
 &lt;a class="anchor" href="#%e8%87%aa%e5%ae%9a%e4%b9%89chart-repository">#&lt;/a>
&lt;/h3>
&lt;h5 id="首先把每个chart打包的tar文件集中存放到charts目录使用以下命令生成indexyaml文件">
 首先，把每个chart打包的tar文件集中存放到charts目录，使用以下命令生成index.yaml文件。
 &lt;a class="anchor" href="#%e9%a6%96%e5%85%88%e6%8a%8a%e6%af%8f%e4%b8%aachart%e6%89%93%e5%8c%85%e7%9a%84tar%e6%96%87%e4%bb%b6%e9%9b%86%e4%b8%ad%e5%ad%98%e6%94%be%e5%88%b0charts%e7%9b%ae%e5%bd%95%e4%bd%bf%e7%94%a8%e4%bb%a5%e4%b8%8b%e5%91%bd%e4%bb%a4%e7%94%9f%e6%88%90indexyaml%e6%96%87%e4%bb%b6">#&lt;/a>
&lt;/h5>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>mv mychart-0.1.0.tgz charts/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ helm serve --repo-path ./charts
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h5 id="命令执行完后">
 命令执行完后
 &lt;a class="anchor" href="#%e5%91%bd%e4%bb%a4%e6%89%a7%e8%a1%8c%e5%ae%8c%e5%90%8e">#&lt;/a>
&lt;/h5>
&lt;h5 id="charts目录结构如下">
 charts目录结构如下
 &lt;a class="anchor" href="#charts%e7%9b%ae%e5%bd%95%e7%bb%93%e6%9e%84%e5%a6%82%e4%b8%8b">#&lt;/a>
&lt;/h5>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-perl" data-lang="perl">&lt;span style="display:flex;">&lt;span>[root@k8s&lt;span style="color:#f92672">-&lt;/span>master &lt;span style="color:#f92672">~&lt;/span>]&lt;span style="color:#75715e"># tree ./charts&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">./&lt;/span>charts
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#960050;background-color:#1e0010">├──&lt;/span> index&lt;span style="color:#f92672">.&lt;/span>yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#960050;background-color:#1e0010">└──&lt;/span> mychart&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">0.1.0&lt;/span>&lt;span style="color:#f92672">.&lt;/span>tgz
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">0&lt;/span> directories, &lt;span style="color:#ae81ff">2&lt;/span> files
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h5 id="把charts目录在远端web服务器上复制一份保持连个文件里面tar包文件一样执行以下命令在本地和远端生成新的indexyaml文件该文件的url为远端web服务器的url">
 把charts目录在远端web服务器上复制一份，保持连个文件里面tar包文件一样。执行以下命令在本地和远端生成新的index.yaml文件，该文件的url为远端web服务器的url。
 &lt;a class="anchor" href="#%e6%8a%8acharts%e7%9b%ae%e5%bd%95%e5%9c%a8%e8%bf%9c%e7%ab%afweb%e6%9c%8d%e5%8a%a1%e5%99%a8%e4%b8%8a%e5%a4%8d%e5%88%b6%e4%b8%80%e4%bb%bd%e4%bf%9d%e6%8c%81%e8%bf%9e%e4%b8%aa%e6%96%87%e4%bb%b6%e9%87%8c%e9%9d%a2tar%e5%8c%85%e6%96%87%e4%bb%b6%e4%b8%80%e6%a0%b7%e6%89%a7%e8%a1%8c%e4%bb%a5%e4%b8%8b%e5%91%bd%e4%bb%a4%e5%9c%a8%e6%9c%ac%e5%9c%b0%e5%92%8c%e8%bf%9c%e7%ab%af%e7%94%9f%e6%88%90%e6%96%b0%e7%9a%84indexyaml%e6%96%87%e4%bb%b6%e8%af%a5%e6%96%87%e4%bb%b6%e7%9a%84url%e4%b8%ba%e8%bf%9c%e7%ab%afweb%e6%9c%8d%e5%8a%a1%e5%99%a8%e7%9a%84url">#&lt;/a>
&lt;/h5>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">helm repo index charts --url http://192.168.122.1:81/charts&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[&lt;span style="color:#ae81ff">root@k8s-master ~]# cat charts/index.yaml&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">entries&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">mychart&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">created&lt;/span>: &lt;span style="color:#e6db74">2018-01-16T11:53:46.922200367&lt;/span>&lt;span style="color:#ae81ff">+08&lt;/span>:&lt;span style="color:#ae81ff">00&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">description&lt;/span>: &lt;span style="color:#ae81ff">A Helm chart for Kubernetes&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">digest&lt;/span>: &lt;span style="color:#ae81ff">7471a2a8496517b4ce1014b2787d3dc745b981fb69c9e53a257ccd7ac390d036&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">mychart&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">urls&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">http://192.168.122.1:81/charts/mychart-0.1.0.tgz&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">version&lt;/span>: &lt;span style="color:#ae81ff">0.1.0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">generated&lt;/span>: &lt;span style="color:#e6db74">2018-01-16T11:53:46.921691858&lt;/span>&lt;span style="color:#ae81ff">+08&lt;/span>:&lt;span style="color:#ae81ff">00&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="增加chart">
 增加chart
 &lt;a class="anchor" href="#%e5%a2%9e%e5%8a%a0chart">#&lt;/a>
&lt;/h4>
&lt;h5 id="在本地charts目录和远端web服务器目录增加新chart的tar文件然后执行以下命令重建indexyaml">
 在本地charts目录和远端web服务器目录增加新chart的tar文件，然后执行以下命令重建index.yaml。
 &lt;a class="anchor" href="#%e5%9c%a8%e6%9c%ac%e5%9c%b0charts%e7%9b%ae%e5%bd%95%e5%92%8c%e8%bf%9c%e7%ab%afweb%e6%9c%8d%e5%8a%a1%e5%99%a8%e7%9b%ae%e5%bd%95%e5%a2%9e%e5%8a%a0%e6%96%b0chart%e7%9a%84tar%e6%96%87%e4%bb%b6%e7%84%b6%e5%90%8e%e6%89%a7%e8%a1%8c%e4%bb%a5%e4%b8%8b%e5%91%bd%e4%bb%a4%e9%87%8d%e5%bb%baindexyaml">#&lt;/a>
&lt;/h5>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-perl" data-lang="perl">&lt;span style="display:flex;">&lt;span>helm repo index charts &lt;span style="color:#f92672">--&lt;/span>url http:&lt;span style="color:#e6db74">//&lt;/span>&lt;span style="color:#ae81ff">192.168.122.1&lt;/span>:&lt;span style="color:#ae81ff">81&lt;/span>&lt;span style="color:#f92672">/&lt;/span>charts
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h5 id="或者在indexyaml中之增加新cahrt的元数据信息">
 或者，在index.yaml中之增加新cahrt的元数据信息。
 &lt;a class="anchor" href="#%e6%88%96%e8%80%85%e5%9c%a8indexyaml%e4%b8%ad%e4%b9%8b%e5%a2%9e%e5%8a%a0%e6%96%b0cahrt%e7%9a%84%e5%85%83%e6%95%b0%e6%8d%ae%e4%bf%a1%e6%81%af">#&lt;/a>
&lt;/h5>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-perl" data-lang="perl">&lt;span style="display:flex;">&lt;span>helm repo index charts &lt;span style="color:#f92672">--&lt;/span>url http:&lt;span style="color:#e6db74">//&lt;/span>&lt;span style="color:#ae81ff">192.168.122.1&lt;/span>:&lt;span style="color:#ae81ff">81&lt;/span>&lt;span style="color:#f92672">/&lt;/span>charts &lt;span style="color:#f92672">--&lt;/span>merge
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h5 id="把新生成的indexyaml拷贝到远端web服务器上">
 把新生成的index.yaml拷贝到远端web服务器上。
 &lt;a class="anchor" href="#%e6%8a%8a%e6%96%b0%e7%94%9f%e6%88%90%e7%9a%84indexyaml%e6%8b%b7%e8%b4%9d%e5%88%b0%e8%bf%9c%e7%ab%afweb%e6%9c%8d%e5%8a%a1%e5%99%a8%e4%b8%8a">#&lt;/a>
&lt;/h5>
&lt;h4 id="其他人使用该repo">
 其他人使用该repo
 &lt;a class="anchor" href="#%e5%85%b6%e4%bb%96%e4%ba%ba%e4%bd%bf%e7%94%a8%e8%af%a5repo">#&lt;/a>
&lt;/h4>
&lt;h5 id="通过以下命令增加repo">
 通过以下命令增加repo
 &lt;a class="anchor" href="#%e9%80%9a%e8%bf%87%e4%bb%a5%e4%b8%8b%e5%91%bd%e4%bb%a4%e5%a2%9e%e5%8a%a0repo">#&lt;/a>
&lt;/h5>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-ruby" data-lang="ruby">&lt;span style="display:flex;">&lt;span>helm repo add charts &lt;span style="color:#e6db74">http&lt;/span>:&lt;span style="color:#e6db74">//&lt;/span>&lt;span style="color:#ae81ff">192&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#ae81ff">168&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#ae81ff">122&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>:&lt;span style="color:#ae81ff">81&lt;/span>&lt;span style="color:#f92672">/&lt;/span>charts
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>root@k8s&lt;span style="color:#f92672">-&lt;/span>master &lt;span style="color:#f92672">~]&lt;/span>&lt;span style="color:#75715e"># helm repo list&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">NAME&lt;/span> &lt;span style="color:#66d9ef">URL&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>local &lt;span style="color:#e6db74">http&lt;/span>:&lt;span style="color:#e6db74">//&lt;/span>&lt;span style="color:#ae81ff">127&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>:&lt;span style="color:#ae81ff">8879&lt;/span>&lt;span style="color:#f92672">/&lt;/span>charts
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>stable &lt;span style="color:#e6db74">https&lt;/span>:&lt;span style="color:#e6db74">//&lt;/span>kubernetes&lt;span style="color:#f92672">.&lt;/span>oss&lt;span style="color:#f92672">-&lt;/span>cn&lt;span style="color:#f92672">-&lt;/span>hangzhou&lt;span style="color:#f92672">.&lt;/span>aliyuncs&lt;span style="color:#f92672">.&lt;/span>com&lt;span style="color:#f92672">/&lt;/span>charts
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>monocular &lt;span style="color:#e6db74">https&lt;/span>:&lt;span style="color:#e6db74">//&lt;/span>kubernetes&lt;span style="color:#f92672">-&lt;/span>helm&lt;span style="color:#f92672">.&lt;/span>github&lt;span style="color:#f92672">.&lt;/span>io&lt;span style="color:#f92672">/&lt;/span>monocular
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>charts &lt;span style="color:#e6db74">http&lt;/span>:&lt;span style="color:#e6db74">//&lt;/span>&lt;span style="color:#ae81ff">192&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#ae81ff">168&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#ae81ff">122&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>:&lt;span style="color:#ae81ff">81&lt;/span>&lt;span style="color:#f92672">/&lt;/span>charts
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h5 id="如果repo有更新执行repo-update命令会更新所以已增加的repo">
 如果repo有更新，执行repo update命令会更新所以已增加的repo
 &lt;a class="anchor" href="#%e5%a6%82%e6%9e%9crepo%e6%9c%89%e6%9b%b4%e6%96%b0%e6%89%a7%e8%a1%8crepo-update%e5%91%bd%e4%bb%a4%e4%bc%9a%e6%9b%b4%e6%96%b0%e6%89%80%e4%bb%a5%e5%b7%b2%e5%a2%9e%e5%8a%a0%e7%9a%84repo">#&lt;/a>
&lt;/h5>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sql" data-lang="sql">&lt;span style="display:flex;">&lt;span>helm repo &lt;span style="color:#66d9ef">update&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h5 id="欢迎沟通交流-cslccloud">
 欢迎沟通交流 CslcCloud
 &lt;a class="anchor" href="#%e6%ac%a2%e8%bf%8e%e6%b2%9f%e9%80%9a%e4%ba%a4%e6%b5%81-cslccloud">#&lt;/a>
&lt;/h5></description></item><item><title>2024-04-03 K8S csi openebs原理</title><link>https://qq547475331.github.io/docs/k8s-csi-openebs%E5%8E%9F%E7%90%86-k8scsiopenebs-yuan-li/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/k8s-csi-openebs%E5%8E%9F%E7%90%86-k8scsiopenebs-yuan-li/</guid><description>&lt;h1 id="kubernetes-csi-二-openebs-原理">
 Kubernetes CSI (二): OpenEBS 原理
 &lt;a class="anchor" href="#kubernetes-csi-%e4%ba%8c-openebs-%e5%8e%9f%e7%90%86">#&lt;/a>
&lt;/h1>
&lt;h2 id="简介">
 简介
 &lt;a class="anchor" href="#%e7%ae%80%e4%bb%8b">#&lt;/a>
&lt;/h2>
&lt;h5 id="在-kubernetes-csi-一-kubernetes-存储原理-一文中详细讲解了-kubernetes-csi-的原理本篇文章通过原理和源码走读形式讲解-openebs-原理">
 在 Kubernetes CSI (一): Kubernetes 存储原理 一文中详细讲解了 Kubernetes CSI 的原理，本篇文章通过原理和源码走读形式讲解 &lt;strong>OpenEBS&lt;/strong> 原理。
 &lt;a class="anchor" href="#%e5%9c%a8-kubernetes-csi-%e4%b8%80-kubernetes-%e5%ad%98%e5%82%a8%e5%8e%9f%e7%90%86-%e4%b8%80%e6%96%87%e4%b8%ad%e8%af%a6%e7%bb%86%e8%ae%b2%e8%a7%a3%e4%ba%86-kubernetes-csi-%e7%9a%84%e5%8e%9f%e7%90%86%e6%9c%ac%e7%af%87%e6%96%87%e7%ab%a0%e9%80%9a%e8%bf%87%e5%8e%9f%e7%90%86%e5%92%8c%e6%ba%90%e7%a0%81%e8%b5%b0%e8%af%bb%e5%bd%a2%e5%bc%8f%e8%ae%b2%e8%a7%a3-openebs-%e5%8e%9f%e7%90%86">#&lt;/a>
&lt;/h5>
&lt;h5 id="openebs-是一款使用go语言编写的基于容器的块存储开源软件openebs-使得在容器中运行关键性任务和需要数据持久化的负载变得更可靠实现了-kubernetes-csi所以可以很方便对接-kubernetes-存储功能">
 OpenEBS 是一款使用Go语言编写的基于容器的块存储开源软件。OpenEBS 使得在容器中运行关键性任务和需要数据持久化的负载变得更可靠。实现了 &lt;strong>Kubernetes CSI&lt;/strong>，所以可以很方便对接 Kubernetes 存储功能。
 &lt;a class="anchor" href="#openebs-%e6%98%af%e4%b8%80%e6%ac%be%e4%bd%bf%e7%94%a8go%e8%af%ad%e8%a8%80%e7%bc%96%e5%86%99%e7%9a%84%e5%9f%ba%e4%ba%8e%e5%ae%b9%e5%99%a8%e7%9a%84%e5%9d%97%e5%ad%98%e5%82%a8%e5%bc%80%e6%ba%90%e8%bd%af%e4%bb%b6openebs-%e4%bd%bf%e5%be%97%e5%9c%a8%e5%ae%b9%e5%99%a8%e4%b8%ad%e8%bf%90%e8%a1%8c%e5%85%b3%e9%94%ae%e6%80%a7%e4%bb%bb%e5%8a%a1%e5%92%8c%e9%9c%80%e8%a6%81%e6%95%b0%e6%8d%ae%e6%8c%81%e4%b9%85%e5%8c%96%e7%9a%84%e8%b4%9f%e8%bd%bd%e5%8f%98%e5%be%97%e6%9b%b4%e5%8f%af%e9%9d%a0%e5%ae%9e%e7%8e%b0%e4%ba%86-kubernetes-csi%e6%89%80%e4%bb%a5%e5%8f%af%e4%bb%a5%e5%be%88%e6%96%b9%e4%be%bf%e5%af%b9%e6%8e%a5-kubernetes-%e5%ad%98%e5%82%a8%e5%8a%9f%e8%83%bd">#&lt;/a>
&lt;/h5>
&lt;h5 id="对于大部分第三方存储厂商它们都只实现了分布式存储openebs-可以为-kubernetes-有状态负载-statefulset--提供本地存储卷和分布式存储卷">
 对于大部分第三方存储厂商，它们都只实现了分布式存储，OpenEBS 可以为 Kubernetes 有状态负载( StatefulSet ) 提供本地存储卷和分布式存储卷。
 &lt;a class="anchor" href="#%e5%af%b9%e4%ba%8e%e5%a4%a7%e9%83%a8%e5%88%86%e7%ac%ac%e4%b8%89%e6%96%b9%e5%ad%98%e5%82%a8%e5%8e%82%e5%95%86%e5%ae%83%e4%bb%ac%e9%83%bd%e5%8f%aa%e5%ae%9e%e7%8e%b0%e4%ba%86%e5%88%86%e5%b8%83%e5%bc%8f%e5%ad%98%e5%82%a8openebs-%e5%8f%af%e4%bb%a5%e4%b8%ba-kubernetes-%e6%9c%89%e7%8a%b6%e6%80%81%e8%b4%9f%e8%bd%bd-statefulset--%e6%8f%90%e4%be%9b%e6%9c%ac%e5%9c%b0%e5%ad%98%e5%82%a8%e5%8d%b7%e5%92%8c%e5%88%86%e5%b8%83%e5%bc%8f%e5%ad%98%e5%82%a8%e5%8d%b7">#&lt;/a>
&lt;/h5>
&lt;p>本篇文章重点讲解 OpenEBS 的&lt;strong>本地存储卷&lt;/strong>。&lt;/p>
&lt;hr>
&lt;h2 id="本地存储卷">
 本地存储卷
 &lt;a class="anchor" href="#%e6%9c%ac%e5%9c%b0%e5%ad%98%e5%82%a8%e5%8d%b7">#&lt;/a>
&lt;/h2>
&lt;h5 id="本地存储卷很容易理解在-kubernetes-中本身就支持-hostpath-类型的存储卷">
 本地存储卷很容易理解，在 Kubernetes 中本身就支持 &lt;strong>Hostpath&lt;/strong> 类型的存储卷，
 &lt;a class="anchor" href="#%e6%9c%ac%e5%9c%b0%e5%ad%98%e5%82%a8%e5%8d%b7%e5%be%88%e5%ae%b9%e6%98%93%e7%90%86%e8%a7%a3%e5%9c%a8-kubernetes-%e4%b8%ad%e6%9c%ac%e8%ba%ab%e5%b0%b1%e6%94%af%e6%8c%81-hostpath-%e7%b1%bb%e5%9e%8b%e7%9a%84%e5%ad%98%e5%82%a8%e5%8d%b7">#&lt;/a>
&lt;/h5>
&lt;h5 id="使用-hostpath-有一个局限性就是我们的-pod-不能随便漂移需要固定到一个节点上因为一旦漂移到其他节点上去了宿主机上面就没有对应的数据了所以我们在使用-hostpath-的时候都会搭配-nodeselector-来进行使用">
 使用 HostPath 有一个局限性就是，我们的 Pod 不能随便漂移，需要固定到一个节点上，因为一旦漂移到其他节点上去了宿主机上面就没有对应的数据了，所以我们在使用 HostPath 的时候都会搭配 nodeSelector 来进行使用。
 &lt;a class="anchor" href="#%e4%bd%bf%e7%94%a8-hostpath-%e6%9c%89%e4%b8%80%e4%b8%aa%e5%b1%80%e9%99%90%e6%80%a7%e5%b0%b1%e6%98%af%e6%88%91%e4%bb%ac%e7%9a%84-pod-%e4%b8%8d%e8%83%bd%e9%9a%8f%e4%be%bf%e6%bc%82%e7%a7%bb%e9%9c%80%e8%a6%81%e5%9b%ba%e5%ae%9a%e5%88%b0%e4%b8%80%e4%b8%aa%e8%8a%82%e7%82%b9%e4%b8%8a%e5%9b%a0%e4%b8%ba%e4%b8%80%e6%97%a6%e6%bc%82%e7%a7%bb%e5%88%b0%e5%85%b6%e4%bb%96%e8%8a%82%e7%82%b9%e4%b8%8a%e5%8e%bb%e4%ba%86%e5%ae%bf%e4%b8%bb%e6%9c%ba%e4%b8%8a%e9%9d%a2%e5%b0%b1%e6%b2%a1%e6%9c%89%e5%af%b9%e5%ba%94%e7%9a%84%e6%95%b0%e6%8d%ae%e4%ba%86%e6%89%80%e4%bb%a5%e6%88%91%e4%bb%ac%e5%9c%a8%e4%bd%bf%e7%94%a8-hostpath-%e7%9a%84%e6%97%b6%e5%80%99%e9%83%bd%e4%bc%9a%e6%90%ad%e9%85%8d-nodeselector-%e6%9d%a5%e8%bf%9b%e8%a1%8c%e4%bd%bf%e7%94%a8">#&lt;/a>
&lt;/h5>
&lt;h5 id="但是使用-hostpath-明显也有一些好处的因为-pv-直接使用的是本地磁盘尤其是-ssd-盘它的读写性能相比于大多数远程存储来说要好得多所以对于一些对磁盘-io-要求比较高的应用比如-etcd-就非常实用了不过呢相比于正常的-pv-来说使用了-hostpath-的这些节点一旦宕机数据就可能丢失所以这就要求使用-hostpath-的应用必须具备数据备份和恢复的能力允许你把这些数据定时备份在其他位置">
 但是使用 HostPath 明显也有一些好处的，因为 PV 直接使用的是本地磁盘，尤其是 SSD 盘，它的读写性能相比于大多数远程存储来说，要好得多，所以对于一些对磁盘 IO 要求比较高的应用，比如 etcd 就非常实用了。不过呢，相比于正常的 PV 来说，使用了 HostPath 的这些节点一旦宕机数据就可能丢失，所以这就要求使用 HostPath 的应用必须具备数据备份和恢复的能力，允许你把这些数据定时备份在其他位置。
 &lt;a class="anchor" href="#%e4%bd%86%e6%98%af%e4%bd%bf%e7%94%a8-hostpath-%e6%98%8e%e6%98%be%e4%b9%9f%e6%9c%89%e4%b8%80%e4%ba%9b%e5%a5%bd%e5%a4%84%e7%9a%84%e5%9b%a0%e4%b8%ba-pv-%e7%9b%b4%e6%8e%a5%e4%bd%bf%e7%94%a8%e7%9a%84%e6%98%af%e6%9c%ac%e5%9c%b0%e7%a3%81%e7%9b%98%e5%b0%a4%e5%85%b6%e6%98%af-ssd-%e7%9b%98%e5%ae%83%e7%9a%84%e8%af%bb%e5%86%99%e6%80%a7%e8%83%bd%e7%9b%b8%e6%af%94%e4%ba%8e%e5%a4%a7%e5%a4%9a%e6%95%b0%e8%bf%9c%e7%a8%8b%e5%ad%98%e5%82%a8%e6%9d%a5%e8%af%b4%e8%a6%81%e5%a5%bd%e5%be%97%e5%a4%9a%e6%89%80%e4%bb%a5%e5%af%b9%e4%ba%8e%e4%b8%80%e4%ba%9b%e5%af%b9%e7%a3%81%e7%9b%98-io-%e8%a6%81%e6%b1%82%e6%af%94%e8%be%83%e9%ab%98%e7%9a%84%e5%ba%94%e7%94%a8%e6%af%94%e5%a6%82-etcd-%e5%b0%b1%e9%9d%9e%e5%b8%b8%e5%ae%9e%e7%94%a8%e4%ba%86%e4%b8%8d%e8%bf%87%e5%91%a2%e7%9b%b8%e6%af%94%e4%ba%8e%e6%ad%a3%e5%b8%b8%e7%9a%84-pv-%e6%9d%a5%e8%af%b4%e4%bd%bf%e7%94%a8%e4%ba%86-hostpath-%e7%9a%84%e8%bf%99%e4%ba%9b%e8%8a%82%e7%82%b9%e4%b8%80%e6%97%a6%e5%ae%95%e6%9c%ba%e6%95%b0%e6%8d%ae%e5%b0%b1%e5%8f%af%e8%83%bd%e4%b8%a2%e5%a4%b1%e6%89%80%e4%bb%a5%e8%bf%99%e5%b0%b1%e8%a6%81%e6%b1%82%e4%bd%bf%e7%94%a8-hostpath-%e7%9a%84%e5%ba%94%e7%94%a8%e5%bf%85%e9%a1%bb%e5%85%b7%e5%a4%87%e6%95%b0%e6%8d%ae%e5%a4%87%e4%bb%bd%e5%92%8c%e6%81%a2%e5%a4%8d%e7%9a%84%e8%83%bd%e5%8a%9b%e5%85%81%e8%ae%b8%e4%bd%a0%e6%8a%8a%e8%bf%99%e4%ba%9b%e6%95%b0%e6%8d%ae%e5%ae%9a%e6%97%b6%e5%a4%87%e4%bb%bd%e5%9c%a8%e5%85%b6%e4%bb%96%e4%bd%8d%e7%bd%ae">#&lt;/a>
&lt;/h5>
&lt;h5 id="所以在-hostpath-的基础上kubernetes-依靠-pvpvc-实现了一个新的特性这个特性的名字叫作local-persistent-volume也就是我们说的-local-pv">
 所以在 HostPath 的基础上，Kubernetes 依靠 PV、PVC 实现了一个新的特性，这个特性的名字叫作：&lt;code>Local Persistent Volume&lt;/code>，也就是我们说的 &lt;code>Local PV&lt;/code>。
 &lt;a class="anchor" href="#%e6%89%80%e4%bb%a5%e5%9c%a8-hostpath-%e7%9a%84%e5%9f%ba%e7%a1%80%e4%b8%8akubernetes-%e4%be%9d%e9%9d%a0-pvpvc-%e5%ae%9e%e7%8e%b0%e4%ba%86%e4%b8%80%e4%b8%aa%e6%96%b0%e7%9a%84%e7%89%b9%e6%80%a7%e8%bf%99%e4%b8%aa%e7%89%b9%e6%80%a7%e7%9a%84%e5%90%8d%e5%ad%97%e5%8f%ab%e4%bd%9clocal-persistent-volume%e4%b9%9f%e5%b0%b1%e6%98%af%e6%88%91%e4%bb%ac%e8%af%b4%e7%9a%84-local-pv">#&lt;/a>
&lt;/h5>
&lt;h5 id="要想使用-local-pv-考虑的因素也比较多下面详细看看">
 要想使用 &lt;code>Local PV&lt;/code> 考虑的因素也比较多，下面详细看看：
 &lt;a class="anchor" href="#%e8%a6%81%e6%83%b3%e4%bd%bf%e7%94%a8-local-pv-%e8%80%83%e8%99%91%e7%9a%84%e5%9b%a0%e7%b4%a0%e4%b9%9f%e6%af%94%e8%be%83%e5%a4%9a%e4%b8%8b%e9%9d%a2%e8%af%a6%e7%bb%86%e7%9c%8b%e7%9c%8b">#&lt;/a>
&lt;/h5>
&lt;h3 id="local-pv">
 Local PV
 &lt;a class="anchor" href="#local-pv">#&lt;/a>
&lt;/h3>
&lt;h5 id="其实-local-pv-实现的功能就非常类似于-hostpath-加上-nodeaffinity-即表示该-pv-就是一个-hostpath-类型卷">
 其实 &lt;code>Local PV&lt;/code> 实现的功能就非常类似于 &lt;code>HostPath&lt;/code> 加上 &lt;code>nodeAffinity&lt;/code> ，即表示该 PV 就是一个 &lt;code>Hostpath&lt;/code> 类型卷。
 &lt;a class="anchor" href="#%e5%85%b6%e5%ae%9e-local-pv-%e5%ae%9e%e7%8e%b0%e7%9a%84%e5%8a%9f%e8%83%bd%e5%b0%b1%e9%9d%9e%e5%b8%b8%e7%b1%bb%e4%bc%bc%e4%ba%8e-hostpath-%e5%8a%a0%e4%b8%8a-nodeaffinity-%e5%8d%b3%e8%a1%a8%e7%a4%ba%e8%af%a5-pv-%e5%b0%b1%e6%98%af%e4%b8%80%e4%b8%aa-hostpath-%e7%b1%bb%e5%9e%8b%e5%8d%b7">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>apiVersion: v1
kind: PersistentVolume
metadata:
 name: pv-local
spec:
 capacity:
 storage: 5Gi
 volumeMode: Filesystem
 accessModes:
 - ReadWriteOnce
 persistentVolumeReclaimPolicy: Delete
 storageClassName: local-storage
 local:
 path: /data/k8s/localpv # 对应主机上数据目录
 # 该 pv 与节点绑定
 nodeAffinity:
 required:
 nodeSelectorTerms:
 - matchExpressions:
 - key: kubernetes.io/hostname
 operator: In
 values:
 - node-1
&lt;/code>&lt;/pre>&lt;h5 id="local-pv-不仅仅支持-filesystem-类型存储还支持-blocklvm-类型">
 &lt;code>Local PV&lt;/code> 不仅仅支持 Filesystem 类型存储，还支持 Block，LVM 类型
 &lt;a class="anchor" href="#local-pv-%e4%b8%8d%e4%bb%85%e4%bb%85%e6%94%af%e6%8c%81-filesystem-%e7%b1%bb%e5%9e%8b%e5%ad%98%e5%82%a8%e8%bf%98%e6%94%af%e6%8c%81-blocklvm-%e7%b1%bb%e5%9e%8b">#&lt;/a>
&lt;/h5>
&lt;h3 id="延迟绑定">
 延迟绑定
 &lt;a class="anchor" href="#%e5%bb%b6%e8%bf%9f%e7%bb%91%e5%ae%9a">#&lt;/a>
&lt;/h3>
&lt;h5 id="延迟绑定就是在-pod-调度完成之后再绑定对应的-pvcpv对于使用-local-pv-的-pod必须延迟绑定">
 &lt;strong>延迟绑定&lt;/strong>就是在 Pod 调度完成之后，再绑定对应的 PVC、PV。对于使用 &lt;code>Local PV&lt;/code> 的 Pod，必须延迟绑定。
 &lt;a class="anchor" href="#%e5%bb%b6%e8%bf%9f%e7%bb%91%e5%ae%9a%e5%b0%b1%e6%98%af%e5%9c%a8-pod-%e8%b0%83%e5%ba%a6%e5%ae%8c%e6%88%90%e4%b9%8b%e5%90%8e%e5%86%8d%e7%bb%91%e5%ae%9a%e5%af%b9%e5%ba%94%e7%9a%84-pvcpv%e5%af%b9%e4%ba%8e%e4%bd%bf%e7%94%a8-local-pv-%e7%9a%84-pod%e5%bf%85%e9%a1%bb%e5%bb%b6%e8%bf%9f%e7%bb%91%e5%ae%9a">#&lt;/a>
&lt;/h5>
&lt;h5 id="比如现在明确规定这个-pod-只能运行在-node-1-这个节点上且该-pod-申请了一个-pvc对于没有延迟属性的-storageclass那么就会在-pod-调度到某个节点之前就将该-pvc-绑定到合适的-pv如果集群-node-1node-2-都存在-pv-可以和该-pvc-绑定那么就有可能该-pvc-绑定了-node-2-的-pv就会导致-pod-调度失败">
 比如现在明确规定，这个 Pod 只能运行在 &lt;code>node-1&lt;/code> 这个节点上，且该 Pod 申请了一个 PVC。对于没有延迟属性的 StorageClass，那么就会在 Pod 调度到某个节点之前就将该 PVC 绑定到合适的 PV，如果集群 &lt;code>node-1，node-2&lt;/code> 都存在 PV 可以和该 PVC 绑定，那么就有可能该 PVC 绑定了 &lt;code>node-2&lt;/code> 的 PV，就会导致 Pod 调度失败。
 &lt;a class="anchor" href="#%e6%af%94%e5%a6%82%e7%8e%b0%e5%9c%a8%e6%98%8e%e7%a1%ae%e8%a7%84%e5%ae%9a%e8%bf%99%e4%b8%aa-pod-%e5%8f%aa%e8%83%bd%e8%bf%90%e8%a1%8c%e5%9c%a8-node-1-%e8%bf%99%e4%b8%aa%e8%8a%82%e7%82%b9%e4%b8%8a%e4%b8%94%e8%af%a5-pod-%e7%94%b3%e8%af%b7%e4%ba%86%e4%b8%80%e4%b8%aa-pvc%e5%af%b9%e4%ba%8e%e6%b2%a1%e6%9c%89%e5%bb%b6%e8%bf%9f%e5%b1%9e%e6%80%a7%e7%9a%84-storageclass%e9%82%a3%e4%b9%88%e5%b0%b1%e4%bc%9a%e5%9c%a8-pod-%e8%b0%83%e5%ba%a6%e5%88%b0%e6%9f%90%e4%b8%aa%e8%8a%82%e7%82%b9%e4%b9%8b%e5%89%8d%e5%b0%b1%e5%b0%86%e8%af%a5-pvc-%e7%bb%91%e5%ae%9a%e5%88%b0%e5%90%88%e9%80%82%e7%9a%84-pv%e5%a6%82%e6%9e%9c%e9%9b%86%e7%be%a4-node-1node-2-%e9%83%bd%e5%ad%98%e5%9c%a8-pv-%e5%8f%af%e4%bb%a5%e5%92%8c%e8%af%a5-pvc-%e7%bb%91%e5%ae%9a%e9%82%a3%e4%b9%88%e5%b0%b1%e6%9c%89%e5%8f%af%e8%83%bd%e8%af%a5-pvc-%e7%bb%91%e5%ae%9a%e4%ba%86-node-2-%e7%9a%84-pv%e5%b0%b1%e4%bc%9a%e5%af%bc%e8%87%b4-pod-%e8%b0%83%e5%ba%a6%e5%a4%b1%e8%b4%a5">#&lt;/a>
&lt;/h5>
&lt;h5 id="所以为了避免这种现象就必须在-pvc-和-pv-绑定之前就将-pod-调度完成所以我们在使用-local-pv-的时候就必须延迟绑定操作即延迟到-pod-调度完成之后再绑定-pvc">
 所以为了避免这种现象，就必须在 PVC 和 PV 绑定之前就将 Pod 调度完成。所以我们在使用 &lt;code>Local PV&lt;/code> 的时候，就必须&lt;strong>延迟绑定&lt;/strong>操作，即延迟到 Pod 调度完成之后再绑定 PVC。
 &lt;a class="anchor" href="#%e6%89%80%e4%bb%a5%e4%b8%ba%e4%ba%86%e9%81%bf%e5%85%8d%e8%bf%99%e7%a7%8d%e7%8e%b0%e8%b1%a1%e5%b0%b1%e5%bf%85%e9%a1%bb%e5%9c%a8-pvc-%e5%92%8c-pv-%e7%bb%91%e5%ae%9a%e4%b9%8b%e5%89%8d%e5%b0%b1%e5%b0%86-pod-%e8%b0%83%e5%ba%a6%e5%ae%8c%e6%88%90%e6%89%80%e4%bb%a5%e6%88%91%e4%bb%ac%e5%9c%a8%e4%bd%bf%e7%94%a8-local-pv-%e7%9a%84%e6%97%b6%e5%80%99%e5%b0%b1%e5%bf%85%e9%a1%bb%e5%bb%b6%e8%bf%9f%e7%bb%91%e5%ae%9a%e6%93%8d%e4%bd%9c%e5%8d%b3%e5%bb%b6%e8%bf%9f%e5%88%b0-pod-%e8%b0%83%e5%ba%a6%e5%ae%8c%e6%88%90%e4%b9%8b%e5%90%8e%e5%86%8d%e7%bb%91%e5%ae%9a-pvc">#&lt;/a>
&lt;/h5>
&lt;h5 id="那么怎么才能实现延迟绑定">
 那么怎么才能实现&lt;strong>延迟绑定&lt;/strong>？
 &lt;a class="anchor" href="#%e9%82%a3%e4%b9%88%e6%80%8e%e4%b9%88%e6%89%8d%e8%83%bd%e5%ae%9e%e7%8e%b0%e5%bb%b6%e8%bf%9f%e7%bb%91%e5%ae%9a">#&lt;/a>
&lt;/h5>
&lt;h5 id="对于-local-pv-类型的-storageclass-需要配置-volumebindingmodewaitforfirstconsumer-的属性就是告诉-kubernetes-在发现这个-storageclass-关联的-pvc-与-pv-可以绑定在一起但不要现在就立刻执行绑定操作即设置-pvc-的-volumename-字段而是要等到第一个声明使用该-pvc-的-pod-出现在调度器之后调度器再综合考虑所有的调度规则当然也包括每个-pv所在的节点位置来统一决定这个-pod-声明的-pvc到底应该跟哪个-pv-进行绑定通过这个延迟绑定机制原本实时发生的-pvc-和-pv-的绑定过程就被延迟到了-pod-第一次调度的时候在调度器中进行从而保证了这个绑定结果不会影响-pod-的正常调度">
 对于 &lt;code>Local PV&lt;/code> 类型的 StorageClass 需要配置 &lt;code>volumeBindingMode=WaitForFirstConsumer&lt;/code> 的属性，就是告诉 Kubernetes 在发现这个 StorageClass 关联的 PVC 与 PV 可以绑定在一起，但不要现在就立刻执行绑定操作（即：设置 PVC 的 VolumeName 字段），而是要等到第一个声明使用该 PVC 的 Pod 出现在调度器之后，调度器再综合考虑所有的调度规则，当然也包括每个 PV所在的节点位置，来统一决定。这个 Pod 声明的 PVC，到底应该跟哪个 PV 进行绑定。通过这个延迟绑定机制，原本实时发生的 PVC 和 PV 的绑定过程，就被延迟到了 Pod 第一次调度的时候在调度器中进行，从而保证了这个绑定结果不会影响 Pod 的正常调度。
 &lt;a class="anchor" href="#%e5%af%b9%e4%ba%8e-local-pv-%e7%b1%bb%e5%9e%8b%e7%9a%84-storageclass-%e9%9c%80%e8%a6%81%e9%85%8d%e7%bd%ae-volumebindingmodewaitforfirstconsumer-%e7%9a%84%e5%b1%9e%e6%80%a7%e5%b0%b1%e6%98%af%e5%91%8a%e8%af%89-kubernetes-%e5%9c%a8%e5%8f%91%e7%8e%b0%e8%bf%99%e4%b8%aa-storageclass-%e5%85%b3%e8%81%94%e7%9a%84-pvc-%e4%b8%8e-pv-%e5%8f%af%e4%bb%a5%e7%bb%91%e5%ae%9a%e5%9c%a8%e4%b8%80%e8%b5%b7%e4%bd%86%e4%b8%8d%e8%a6%81%e7%8e%b0%e5%9c%a8%e5%b0%b1%e7%ab%8b%e5%88%bb%e6%89%a7%e8%a1%8c%e7%bb%91%e5%ae%9a%e6%93%8d%e4%bd%9c%e5%8d%b3%e8%ae%be%e7%bd%ae-pvc-%e7%9a%84-volumename-%e5%ad%97%e6%ae%b5%e8%80%8c%e6%98%af%e8%a6%81%e7%ad%89%e5%88%b0%e7%ac%ac%e4%b8%80%e4%b8%aa%e5%a3%b0%e6%98%8e%e4%bd%bf%e7%94%a8%e8%af%a5-pvc-%e7%9a%84-pod-%e5%87%ba%e7%8e%b0%e5%9c%a8%e8%b0%83%e5%ba%a6%e5%99%a8%e4%b9%8b%e5%90%8e%e8%b0%83%e5%ba%a6%e5%99%a8%e5%86%8d%e7%bb%bc%e5%90%88%e8%80%83%e8%99%91%e6%89%80%e6%9c%89%e7%9a%84%e8%b0%83%e5%ba%a6%e8%a7%84%e5%88%99%e5%bd%93%e7%84%b6%e4%b9%9f%e5%8c%85%e6%8b%ac%e6%af%8f%e4%b8%aa-pv%e6%89%80%e5%9c%a8%e7%9a%84%e8%8a%82%e7%82%b9%e4%bd%8d%e7%bd%ae%e6%9d%a5%e7%bb%9f%e4%b8%80%e5%86%b3%e5%ae%9a%e8%bf%99%e4%b8%aa-pod-%e5%a3%b0%e6%98%8e%e7%9a%84-pvc%e5%88%b0%e5%ba%95%e5%ba%94%e8%af%a5%e8%b7%9f%e5%93%aa%e4%b8%aa-pv-%e8%bf%9b%e8%a1%8c%e7%bb%91%e5%ae%9a%e9%80%9a%e8%bf%87%e8%bf%99%e4%b8%aa%e5%bb%b6%e8%bf%9f%e7%bb%91%e5%ae%9a%e6%9c%ba%e5%88%b6%e5%8e%9f%e6%9c%ac%e5%ae%9e%e6%97%b6%e5%8f%91%e7%94%9f%e7%9a%84-pvc-%e5%92%8c-pv-%e7%9a%84%e7%bb%91%e5%ae%9a%e8%bf%87%e7%a8%8b%e5%b0%b1%e8%a2%ab%e5%bb%b6%e8%bf%9f%e5%88%b0%e4%ba%86-pod-%e7%ac%ac%e4%b8%80%e6%ac%a1%e8%b0%83%e5%ba%a6%e7%9a%84%e6%97%b6%e5%80%99%e5%9c%a8%e8%b0%83%e5%ba%a6%e5%99%a8%e4%b8%ad%e8%bf%9b%e8%a1%8c%e4%bb%8e%e8%80%8c%e4%bf%9d%e8%af%81%e4%ba%86%e8%bf%99%e4%b8%aa%e7%bb%91%e5%ae%9a%e7%bb%93%e6%9e%9c%e4%b8%8d%e4%bc%9a%e5%bd%b1%e5%93%8d-pod-%e7%9a%84%e6%ad%a3%e5%b8%b8%e8%b0%83%e5%ba%a6">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
 name: local-storage
provisioner: kubernetes.io/no-provisioner
# 延迟绑定属性
volumeBindingMode: WaitForFirstConsumer
&lt;/code>&lt;/pre>&lt;h3 id="原地重启">
 原地重启
 &lt;a class="anchor" href="#%e5%8e%9f%e5%9c%b0%e9%87%8d%e5%90%af">#&lt;/a>
&lt;/h3>
&lt;h5 id="我们知道使用-hostpath-存储卷类型-pod-如果不设置节点选择器那么重启后会调度到其他节点上运行这样就会导致之前数据丢失">
 我们知道使用 Hostpath 存储卷类型 Pod 如果不设置节点选择器，那么重启后会调度到其他节点上运行，这样就会导致之前数据丢失。
 &lt;a class="anchor" href="#%e6%88%91%e4%bb%ac%e7%9f%a5%e9%81%93%e4%bd%bf%e7%94%a8-hostpath-%e5%ad%98%e5%82%a8%e5%8d%b7%e7%b1%bb%e5%9e%8b-pod-%e5%a6%82%e6%9e%9c%e4%b8%8d%e8%ae%be%e7%bd%ae%e8%8a%82%e7%82%b9%e9%80%89%e6%8b%a9%e5%99%a8%e9%82%a3%e4%b9%88%e9%87%8d%e5%90%af%e5%90%8e%e4%bc%9a%e8%b0%83%e5%ba%a6%e5%88%b0%e5%85%b6%e4%bb%96%e8%8a%82%e7%82%b9%e4%b8%8a%e8%bf%90%e8%a1%8c%e8%bf%99%e6%a0%b7%e5%b0%b1%e4%bc%9a%e5%af%bc%e8%87%b4%e4%b9%8b%e5%89%8d%e6%95%b0%e6%8d%ae%e4%b8%a2%e5%a4%b1">#&lt;/a>
&lt;/h5>
&lt;h5 id="使用-local-pv-存储卷就无需担心该问题因为根据-local-pv-的特性会保证该-pod-重启后始终在当前节点运行当然这里涉及到-kubernetes-调度器和-pvcpv的相关原理下面我们简单描述下">
 使用 Local PV 存储卷就无需担心该问题，因为根据 Local PV 的特性会保证该 Pod 重启后始终在&lt;strong>当前节点&lt;/strong>运行。当然这里涉及到 Kubernetes 调度器和 PVC、PV的相关原理，下面我们简单描述下。
 &lt;a class="anchor" href="#%e4%bd%bf%e7%94%a8-local-pv-%e5%ad%98%e5%82%a8%e5%8d%b7%e5%b0%b1%e6%97%a0%e9%9c%80%e6%8b%85%e5%bf%83%e8%af%a5%e9%97%ae%e9%a2%98%e5%9b%a0%e4%b8%ba%e6%a0%b9%e6%8d%ae-local-pv-%e7%9a%84%e7%89%b9%e6%80%a7%e4%bc%9a%e4%bf%9d%e8%af%81%e8%af%a5-pod-%e9%87%8d%e5%90%af%e5%90%8e%e5%a7%8b%e7%bb%88%e5%9c%a8%e5%bd%93%e5%89%8d%e8%8a%82%e7%82%b9%e8%bf%90%e8%a1%8c%e5%bd%93%e7%84%b6%e8%bf%99%e9%87%8c%e6%b6%89%e5%8f%8a%e5%88%b0-kubernetes-%e8%b0%83%e5%ba%a6%e5%99%a8%e5%92%8c-pvcpv%e7%9a%84%e7%9b%b8%e5%85%b3%e5%8e%9f%e7%90%86%e4%b8%8b%e9%9d%a2%e6%88%91%e4%bb%ac%e7%ae%80%e5%8d%95%e6%8f%8f%e8%bf%b0%e4%b8%8b">#&lt;/a>
&lt;/h5>
&lt;h5 id="当-pod-重启后调度器首先判断该-pod下的-pvc-是否已经绑定如果已经绑定那么就根据-pvc-annotation-volumekubernetesioselected-node-字段过滤到其他-node">
 当 Pod 重启后，调度器首先判断该 Pod下的 PVC 是否已经绑定，如果已经绑定，那么就根据 PVC Annotation &lt;code>volume.kubernetes.io/selected-node&lt;/code> 字段过滤到其他 node。
 &lt;a class="anchor" href="#%e5%bd%93-pod-%e9%87%8d%e5%90%af%e5%90%8e%e8%b0%83%e5%ba%a6%e5%99%a8%e9%a6%96%e5%85%88%e5%88%a4%e6%96%ad%e8%af%a5-pod%e4%b8%8b%e7%9a%84-pvc-%e6%98%af%e5%90%a6%e5%b7%b2%e7%bb%8f%e7%bb%91%e5%ae%9a%e5%a6%82%e6%9e%9c%e5%b7%b2%e7%bb%8f%e7%bb%91%e5%ae%9a%e9%82%a3%e4%b9%88%e5%b0%b1%e6%a0%b9%e6%8d%ae-pvc-annotation-volumekubernetesioselected-node-%e5%ad%97%e6%ae%b5%e8%bf%87%e6%bb%a4%e5%88%b0%e5%85%b6%e4%bb%96-node">#&lt;/a>
&lt;/h5>
&lt;h5 id="这样就保证该-pod-就一直在-pv-所在-node-上运行">
 这样就保证该 Pod 就一直在 PV 所在 node 上运行。
 &lt;a class="anchor" href="#%e8%bf%99%e6%a0%b7%e5%b0%b1%e4%bf%9d%e8%af%81%e8%af%a5-pod-%e5%b0%b1%e4%b8%80%e7%9b%b4%e5%9c%a8-pv-%e6%89%80%e5%9c%a8-node-%e4%b8%8a%e8%bf%90%e8%a1%8c">#&lt;/a>
&lt;/h5>
&lt;h5 id="这段代码逻辑参考以下">
 这段代码逻辑参考以下：
 &lt;a class="anchor" href="#%e8%bf%99%e6%ae%b5%e4%bb%a3%e7%a0%81%e9%80%bb%e8%be%91%e5%8f%82%e8%80%83%e4%bb%a5%e4%b8%8b">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>// For PVCs that are bound, then it checks that the corresponding PV&amp;#39;s node affinity is
// satisfied by the given node.

// kubernetes/pkg/scheduler/framework/plugins/volumebinding/binder.go:316
func (b *volumeBinder) FindPodVolumes(pod *v1.Pod, boundClaims, claimsToBind []*v1.PersistentVolumeClaim, node *v1.Node) (podVolumes *PodVolumes, reasons ConflictReasons, err error) {
...
 // Find matching volumes and node for unbound claims
 if len(claimsToBind) &amp;gt; 0 {
 var (
 claimsToFindMatching []*v1.PersistentVolumeClaim
 claimsToProvision []*v1.PersistentVolumeClaim
 )

 // 调度器对集群中的 node 与 pvc Annotation volume.kubernetes.io/selected-node 字段比较，过滤掉不匹配 node
 for _, claim := range claimsToBind {
 if selectedNode, ok := claim.Annotations[volume.AnnSelectedNode]; ok {
 if selectedNode != node.Name {
 // Fast path, skip unmatched node.
 unboundVolumesSatisfied = false
 return
 }
 claimsToProvision = append(claimsToProvision, claim)
 } else {
 claimsToFindMatching = append(claimsToFindMatching, claim)
 }
 }
...
}
&lt;/code>&lt;/pre>&lt;h5 id="上面说的-pvc-annotation-是在-pod-调度完成后调度器设置该-annotation其-value-是节点名称">
 上面说的 PVC Annotation 是在 Pod 调度完成后，调度器设置该 Annotation，其 value 是&lt;strong>节点名称&lt;/strong>。
 &lt;a class="anchor" href="#%e4%b8%8a%e9%9d%a2%e8%af%b4%e7%9a%84-pvc-annotation-%e6%98%af%e5%9c%a8-pod-%e8%b0%83%e5%ba%a6%e5%ae%8c%e6%88%90%e5%90%8e%e8%b0%83%e5%ba%a6%e5%99%a8%e8%ae%be%e7%bd%ae%e8%af%a5-annotation%e5%85%b6-value-%e6%98%af%e8%8a%82%e7%82%b9%e5%90%8d%e7%a7%b0">#&lt;/a>
&lt;/h5>
&lt;h5 id="这段代码逻辑参考以下-1">
 这段代码逻辑参考以下：
 &lt;a class="anchor" href="#%e8%bf%99%e6%ae%b5%e4%bb%a3%e7%a0%81%e9%80%bb%e8%be%91%e5%8f%82%e8%80%83%e4%bb%a5%e4%b8%8b-1">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>// AssumePodVolumes will take the matching PVs and PVCs to provision in pod&amp;#39;s
// volume information for the chosen node, and:
// 1. Update the pvCache with the new prebound PV.
// 2. Update the pvcCache with the new PVCs with annotations set
// 3. Update PodVolumes again with cached API updates for PVs and PVCs.

// kubernetes/pkg/scheduler/framework/plugins/volumebinding/binder.go:359
func (b *volumeBinder) AssumePodVolumes(assumedPod *v1.Pod, nodeName string, podVolumes *PodVolumes) (allFullyBound bool, err error) {
 ...

 newProvisionedPVCs := []*v1.PersistentVolumeClaim{}
 for _, claim := range podVolumes.DynamicProvisions {
 claimClone := claim.DeepCopy()
 // 设置 volume.kubernetes.io/selected-node annotation，value 为该 pod 调度的 nodeName
 metav1.SetMetaDataAnnotation(&amp;amp;claimClone.ObjectMeta, volume.AnnSelectedNode, nodeName)
 err = b.pvcCache.Assume(claimClone)
 if err != nil {
 b.revertAssumedPVs(newBindings)
 b.revertAssumedPVCs(newProvisionedPVCs)
 return
 }

 newProvisionedPVCs = append(newProvisionedPVCs, claimClone)
 }

 podVolumes.StaticBindings = newBindings
 podVolumes.DynamicProvisions = newProvisionedPVCs
 return
}
&lt;/code>&lt;/pre>&lt;hr>
&lt;h2 id="openebs-使用">
 OpenEBS 使用
 &lt;a class="anchor" href="#openebs-%e4%bd%bf%e7%94%a8">#&lt;/a>
&lt;/h2>
&lt;h5 id="openebs-对本地存储卷功能支持非常好">
 OpenEBS 对本地存储卷功能支持非常好：
 &lt;a class="anchor" href="#openebs-%e5%af%b9%e6%9c%ac%e5%9c%b0%e5%ad%98%e5%82%a8%e5%8d%b7%e5%8a%9f%e8%83%bd%e6%94%af%e6%8c%81%e9%9d%9e%e5%b8%b8%e5%a5%bd">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="-openebs可以使用宿主机裸块设备或分区或者使用hostpaths上的子目录或者使用lvmzfs来创建持久化卷">
 • &lt;code>OpenEBS&lt;/code>可以使用宿主机&lt;code>裸块设备或分区&lt;/code>，或者使用&lt;code>Hostpaths&lt;/code>上的子目录，或者使用&lt;code>LVM&lt;/code>、&lt;code>ZFS&lt;/code>来创建持久化卷
 &lt;a class="anchor" href="#-openebs%e5%8f%af%e4%bb%a5%e4%bd%bf%e7%94%a8%e5%ae%bf%e4%b8%bb%e6%9c%ba%e8%a3%b8%e5%9d%97%e8%ae%be%e5%a4%87%e6%88%96%e5%88%86%e5%8c%ba%e6%88%96%e8%80%85%e4%bd%bf%e7%94%a8hostpaths%e4%b8%8a%e7%9a%84%e5%ad%90%e7%9b%ae%e5%bd%95%e6%88%96%e8%80%85%e4%bd%bf%e7%94%a8lvmzfs%e6%9d%a5%e5%88%9b%e5%bb%ba%e6%8c%81%e4%b9%85%e5%8c%96%e5%8d%b7">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-本地卷直接挂载到statefulset-pod中而不需要openebs在数据路径中增加任何开销">
 • 本地卷直接挂载到&lt;code>StatefulSet Pod&lt;/code>中，而不需要&lt;code>OpenEBS&lt;/code>在数据路径中增加任何开销
 &lt;a class="anchor" href="#-%e6%9c%ac%e5%9c%b0%e5%8d%b7%e7%9b%b4%e6%8e%a5%e6%8c%82%e8%bd%bd%e5%88%b0statefulset-pod%e4%b8%ad%e8%80%8c%e4%b8%8d%e9%9c%80%e8%a6%81openebs%e5%9c%a8%e6%95%b0%e6%8d%ae%e8%b7%af%e5%be%84%e4%b8%ad%e5%a2%9e%e5%8a%a0%e4%bb%bb%e4%bd%95%e5%bc%80%e9%94%80">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-openebs为本地卷提供了额外的工具用于监控备份恢复灾难恢复由zfs或lvm支持的快照等">
 • &lt;code>OpenEBS&lt;/code>为本地卷提供了额外的工具，用于监控、备份/恢复、灾难恢复、由&lt;code>ZFS&lt;/code>或&lt;code>LVM&lt;/code>支持的快照等
 &lt;a class="anchor" href="#-openebs%e4%b8%ba%e6%9c%ac%e5%9c%b0%e5%8d%b7%e6%8f%90%e4%be%9b%e4%ba%86%e9%a2%9d%e5%a4%96%e7%9a%84%e5%b7%a5%e5%85%b7%e7%94%a8%e4%ba%8e%e7%9b%91%e6%8e%a7%e5%a4%87%e4%bb%bd%e6%81%a2%e5%a4%8d%e7%81%be%e9%9a%be%e6%81%a2%e5%a4%8d%e7%94%b1zfs%e6%88%96lvm%e6%94%af%e6%8c%81%e7%9a%84%e5%bf%ab%e7%85%a7%e7%ad%89">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h5 id="同时-openebs-屏蔽了我们使用-local-pv-复杂性openebs-部署及使用也是非常方便">
 同时 OpenEBS 屏蔽了我们使用 Local PV 复杂性。OpenEBS 部署及使用也是非常方便。
 &lt;a class="anchor" href="#%e5%90%8c%e6%97%b6-openebs-%e5%b1%8f%e8%94%bd%e4%ba%86%e6%88%91%e4%bb%ac%e4%bd%bf%e7%94%a8-local-pv-%e5%a4%8d%e6%9d%82%e6%80%a7openebs-%e9%83%a8%e7%bd%b2%e5%8f%8a%e4%bd%bf%e7%94%a8%e4%b9%9f%e6%98%af%e9%9d%9e%e5%b8%b8%e6%96%b9%e4%be%bf">#&lt;/a>
&lt;/h5>
&lt;h5 id="这里我们只使用-local-pv-hostpath即使用本地文件系统的存储根据官网介绍不需要提前安装-iscsi">
 这里我们只使用 &lt;strong>local-pv-hostpath&lt;/strong>，即使用本地文件系统的存储，根据官网介绍，不需要提前安装 &lt;strong>iscsi&lt;/strong>
 &lt;a class="anchor" href="#%e8%bf%99%e9%87%8c%e6%88%91%e4%bb%ac%e5%8f%aa%e4%bd%bf%e7%94%a8-local-pv-hostpath%e5%8d%b3%e4%bd%bf%e7%94%a8%e6%9c%ac%e5%9c%b0%e6%96%87%e4%bb%b6%e7%b3%bb%e7%bb%9f%e7%9a%84%e5%ad%98%e5%82%a8%e6%a0%b9%e6%8d%ae%e5%ae%98%e7%bd%91%e4%bb%8b%e7%bb%8d%e4%b8%8d%e9%9c%80%e8%a6%81%e6%8f%90%e5%89%8d%e5%ae%89%e8%a3%85-iscsi">#&lt;/a>
&lt;/h5>
&lt;h3 id="helm-部署">
 Helm 部署
 &lt;a class="anchor" href="#helm-%e9%83%a8%e7%bd%b2">#&lt;/a>
&lt;/h3>
&lt;pre tabindex="0">&lt;code>$ helm repo add openebs https://openebs.github.io/charts
$ helm repo update
$ helm install openebs --namespace openebs openebs/openebs --create-namespace
&lt;/code>&lt;/pre>&lt;h3 id="kubectl-部署">
 kubectl 部署
 &lt;a class="anchor" href="#kubectl-%e9%83%a8%e7%bd%b2">#&lt;/a>
&lt;/h3>
&lt;pre tabindex="0">&lt;code>kubectl apply -f https://openebs.github.io/charts/openebs-operator.yaml
&lt;/code>&lt;/pre>&lt;h5 id="安装成功后默认会部署两个-storageclass-我们目前需要-openebs-hostpath">
 安装成功后，默认会部署两个 &lt;strong>storageclass&lt;/strong> ，我们目前需要 &lt;strong>openebs-hostpath。&lt;/strong>
 &lt;a class="anchor" href="#%e5%ae%89%e8%a3%85%e6%88%90%e5%8a%9f%e5%90%8e%e9%bb%98%e8%ae%a4%e4%bc%9a%e9%83%a8%e7%bd%b2%e4%b8%a4%e4%b8%aa-storageclass-%e6%88%91%e4%bb%ac%e7%9b%ae%e5%89%8d%e9%9c%80%e8%a6%81-openebs-hostpath">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>$ kubectl get pods -n openebs
NAME READY STATUS RESTARTS AGE
openebs-localpv-provisioner-69c8648db7-cnj45 1/1 Running 0 33m
openebs-ndm-bbgpv 1/1 Running 0 33m
openebs-ndm-bxsbb 1/1 Running 0 33m
openebs-ndm-cluster-exporter-9d75d564d-qvqz6 1/1 Running 0 33m
openebs-ndm-kdg7b 1/1 Running 0 33m
openebs-ndm-node-exporter-9zm62 1/1 Running 0 33m
openebs-ndm-node-exporter-hlj7h 1/1 Running 0 33m
openebs-ndm-node-exporter-j6wj7 1/1 Running 0 33m
openebs-ndm-node-exporter-s5hk4 1/1 Running 0 33m
openebs-ndm-operator-789985cc47-r4hwj 1/1 Running 0 33m
openebs-ndm-qm4sw 1/1 Running 0 33m
$ kubectl get sc
NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE
openebs-device openebs.io/local Delete WaitForFirstConsumer false 116s
openebs-hostpath openebs.io/local Delete WaitForFirstConsumer false 116s
&lt;/code>&lt;/pre>&lt;h5 id="默认存储目录为-varopenebslocal-如果需要更改的话直接修改-openebs-operatoryamlvalue-字段即可">
 默认存储目录为 ****&lt;code>/var/openebs/local&lt;/code>, 如果需要更改的话，直接修改 &lt;code>openebs-operator.yaml&lt;/code>，&lt;code>value&lt;/code> 字段即可
 &lt;a class="anchor" href="#%e9%bb%98%e8%ae%a4%e5%ad%98%e5%82%a8%e7%9b%ae%e5%bd%95%e4%b8%ba-varopenebslocal-%e5%a6%82%e6%9e%9c%e9%9c%80%e8%a6%81%e6%9b%b4%e6%94%b9%e7%9a%84%e8%af%9d%e7%9b%b4%e6%8e%a5%e4%bf%ae%e6%94%b9-openebs-operatoryamlvalue-%e5%ad%97%e6%ae%b5%e5%8d%b3%e5%8f%af">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
 name: openebs-hostpath
 annotations:
 openebs.io/cas-type: local
 cas.openebs.io/config: |
 #hostpath type will create a PV by 
 # creating a sub-directory under the
 # BASEPATH provided below.
 - name: StorageType
 value: &amp;#34;hostpath&amp;#34;
 #Specify the location (directory) where
 # where PV(volume) data will be saved. 
 # A sub-directory with pv-name will be 
 # created. When the volume is deleted, 
 # the PV sub-directory will be deleted.
 #Default value is /var/openebs/local
 - name: BasePath
 value: &amp;#34;/var/openebs/local/&amp;#34;
&lt;/code>&lt;/pre>&lt;h3 id="验证">
 验证
 &lt;a class="anchor" href="#%e9%aa%8c%e8%af%81">#&lt;/a>
&lt;/h3>
&lt;h5 id="接下来我们创建一个-pvc-资源对象pod-使用这个-pvc-就可以从-openebs-动态-local-pv-provisioner-中请求-hostpath-local-pv-了">
 接下来我们创建一个 PVC 资源对象，Pod 使用这个 PVC 就可以从 OpenEBS 动态 Local PV Provisioner 中请求 Hostpath Local PV 了。
 &lt;a class="anchor" href="#%e6%8e%a5%e4%b8%8b%e6%9d%a5%e6%88%91%e4%bb%ac%e5%88%9b%e5%bb%ba%e4%b8%80%e4%b8%aa-pvc-%e8%b5%84%e6%ba%90%e5%af%b9%e8%b1%a1pod-%e4%bd%bf%e7%94%a8%e8%bf%99%e4%b8%aa-pvc-%e5%b0%b1%e5%8f%af%e4%bb%a5%e4%bb%8e-openebs-%e5%8a%a8%e6%80%81-local-pv-provisioner-%e4%b8%ad%e8%af%b7%e6%b1%82-hostpath-local-pv-%e4%ba%86">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code># local-hostpath-pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
 name: local-hostpath-pvc
spec:
 storageClassName: openebs-hostpath
 accessModes:
 - ReadWriteOnce
 resources:
 requests:
 storage: 5Gi
&lt;/code>&lt;/pre>&lt;h5 id="直接创建这个-pvc-即可">
 直接创建这个 PVC 即可：
 &lt;a class="anchor" href="#%e7%9b%b4%e6%8e%a5%e5%88%9b%e5%bb%ba%e8%bf%99%e4%b8%aa-pvc-%e5%8d%b3%e5%8f%af">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>$ kubectl apply -f local-hostpath-pvc.yaml
$ kubectl get pvc local-hostpath-pvc
NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE
local-hostpath-pvc Pending openebs-hostpath 12s
&lt;/code>&lt;/pre>&lt;h5 id="我们可以看到这个-pvc-的状态是-pending这是因为对应的-storageclass-是延迟绑定模式所以需要等到-pod-消费这个-pvc-后才会去绑定接下来我们去创建一个-pod-来使用这个-pvc">
 我们可以看到这个 PVC 的状态是 &lt;code>Pending&lt;/code>，这是因为对应的 StorageClass 是延迟绑定模式，所以需要等到 Pod 消费这个 PVC 后才会去绑定，接下来我们去创建一个 Pod 来使用这个 PVC。
 &lt;a class="anchor" href="#%e6%88%91%e4%bb%ac%e5%8f%af%e4%bb%a5%e7%9c%8b%e5%88%b0%e8%bf%99%e4%b8%aa-pvc-%e7%9a%84%e7%8a%b6%e6%80%81%e6%98%af-pending%e8%bf%99%e6%98%af%e5%9b%a0%e4%b8%ba%e5%af%b9%e5%ba%94%e7%9a%84-storageclass-%e6%98%af%e5%bb%b6%e8%bf%9f%e7%bb%91%e5%ae%9a%e6%a8%a1%e5%bc%8f%e6%89%80%e4%bb%a5%e9%9c%80%e8%a6%81%e7%ad%89%e5%88%b0-pod-%e6%b6%88%e8%b4%b9%e8%bf%99%e4%b8%aa-pvc-%e5%90%8e%e6%89%8d%e4%bc%9a%e5%8e%bb%e7%bb%91%e5%ae%9a%e6%8e%a5%e4%b8%8b%e6%9d%a5%e6%88%91%e4%bb%ac%e5%8e%bb%e5%88%9b%e5%bb%ba%e4%b8%80%e4%b8%aa-pod-%e6%9d%a5%e4%bd%bf%e7%94%a8%e8%bf%99%e4%b8%aa-pvc">#&lt;/a>
&lt;/h5>
&lt;h5 id="声明一个如下所示的-pod-资源清单">
 声明一个如下所示的 Pod 资源清单：
 &lt;a class="anchor" href="#%e5%a3%b0%e6%98%8e%e4%b8%80%e4%b8%aa%e5%a6%82%e4%b8%8b%e6%89%80%e7%a4%ba%e7%9a%84-pod-%e8%b5%84%e6%ba%90%e6%b8%85%e5%8d%95">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code># local-hostpath-pod.yaml
apiVersion: v1
kind: Pod
metadata:
 name: hello-local-hostpath-pod
spec:
 volumes:
 - name: local-storage
 persistentVolumeClaim:
 claimName: local-hostpath-pvc
 containers:
 - name: hello-container
 image: busybox
 command:
 - sh
 - -c
 - &amp;#39;while true; do echo &amp;#34;`date` [`hostname`] Hello from OpenEBS Local PV.&amp;#34; &amp;gt;&amp;gt; /mnt/store/greet.txt; sleep $(($RANDOM % 5 + 300)); done&amp;#39;
 volumeMounts:
 - mountPath: /mnt/store
 name: local-storage
&lt;/code>&lt;/pre>&lt;h5 id="直接创建这个-pod">
 直接创建这个 Pod：
 &lt;a class="anchor" href="#%e7%9b%b4%e6%8e%a5%e5%88%9b%e5%bb%ba%e8%bf%99%e4%b8%aa-pod">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>$ kubectl apply -f local-hostpath-pod.yaml
$ kubectl get pods hello-local-hostpath-pod
NAME READY STATUS RESTARTS AGE
hello-local-hostpath-pod 1/1 Running 0 2m7s
$ kubectl get pvc local-hostpath-pvc
NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE
local-hostpath-pvc Bound pvc-3f4a1a65-6cbc-42bf-a1f8-87ad238c0b88 5Gi RWO openebs-hostpath 5m41s
&lt;/code>&lt;/pre>&lt;h5 id="可以看到-pod-运行成功后pvc-也绑定上了一个自动生成的-pv我们可以查看这个-pv-的详细信息">
 可以看到 Pod 运行成功后，PVC 也绑定上了一个自动生成的 PV，我们可以查看这个 PV 的详细信息：
 &lt;a class="anchor" href="#%e5%8f%af%e4%bb%a5%e7%9c%8b%e5%88%b0-pod-%e8%bf%90%e8%a1%8c%e6%88%90%e5%8a%9f%e5%90%8epvc-%e4%b9%9f%e7%bb%91%e5%ae%9a%e4%b8%8a%e4%ba%86%e4%b8%80%e4%b8%aa%e8%87%aa%e5%8a%a8%e7%94%9f%e6%88%90%e7%9a%84-pv%e6%88%91%e4%bb%ac%e5%8f%af%e4%bb%a5%e6%9f%a5%e7%9c%8b%e8%bf%99%e4%b8%aa-pv-%e7%9a%84%e8%af%a6%e7%bb%86%e4%bf%a1%e6%81%af">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>$ kubectl get pv pvc-3f4a1a65-6cbc-42bf-a1f8-87ad238c0b88 -o yaml
apiVersion: v1
kind: PersistentVolume
metadata:
 annotations:
 pv.kubernetes.io/provisioned-by: openebs.io/local
 creationTimestamp: &amp;#34;2021-01-07T02:48:14Z&amp;#34;
 finalizers:
 - kubernetes.io/pv-protection
 labels:
 openebs.io/cas-type: local-hostpath
 ......
 name: pvc-3f4a1a65-6cbc-42bf-a1f8-87ad238c0b88
 resourceVersion: &amp;#34;21193802&amp;#34;
 selfLink: /api/v1/persistentvolumes/pvc-3f4a1a65-6cbc-42bf-a1f8-87ad238c0b88
 uid: f7cccdb3-d23a-4831-86c3-4363eb1a8dee
spec:
 accessModes:
 - ReadWriteOnce
 capacity:
 storage: 5Gi
 claimRef:
 apiVersion: v1
 kind: PersistentVolumeClaim
 name: local-hostpath-pvc
 namespace: default
 resourceVersion: &amp;#34;21193645&amp;#34;
 uid: 3f4a1a65-6cbc-42bf-a1f8-87ad238c0b88
 local:
 fsType: &amp;#34;&amp;#34;
 path: /var/openebs/local/pvc-3f4a1a65-6cbc-42bf-a1f8-87ad238c0b88
 nodeAffinity:
 required:
 nodeSelectorTerms:
 - matchExpressions:
 - key: kubernetes.io/hostname
 operator: In
 values:
 - node2
 persistentVolumeReclaimPolicy: Delete
 storageClassName: openebs-hostpath
 volumeMode: Filesystem
status:
 phase: Bound
&lt;/code>&lt;/pre>&lt;h5 id="本地数据目录位于-varopenebslocalpvc-3f4a1a65-6cbc-42bf-a1f8-87ad238c0b88-下面">
 本地数据目录位于 &lt;code>/var/openebs/local/pvc-3f4a1a65-6cbc-42bf-a1f8-87ad238c0b88&lt;/code> 下面。
 &lt;a class="anchor" href="#%e6%9c%ac%e5%9c%b0%e6%95%b0%e6%8d%ae%e7%9b%ae%e5%bd%95%e4%bd%8d%e4%ba%8e-varopenebslocalpvc-3f4a1a65-6cbc-42bf-a1f8-87ad238c0b88-%e4%b8%8b%e9%9d%a2">#&lt;/a>
&lt;/h5>
&lt;h5 id="接着我们来验证下-volume-数据前往-node2-节点查看下上面的数据目录中的数据">
 接着我们来验证下 volume 数据，前往 node2 节点查看下上面的数据目录中的数据：
 &lt;a class="anchor" href="#%e6%8e%a5%e7%9d%80%e6%88%91%e4%bb%ac%e6%9d%a5%e9%aa%8c%e8%af%81%e4%b8%8b-volume-%e6%95%b0%e6%8d%ae%e5%89%8d%e5%be%80-node2-%e8%8a%82%e7%82%b9%e6%9f%a5%e7%9c%8b%e4%b8%8b%e4%b8%8a%e9%9d%a2%e7%9a%84%e6%95%b0%e6%8d%ae%e7%9b%ae%e5%bd%95%e4%b8%ad%e7%9a%84%e6%95%b0%e6%8d%ae">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>$ ls /var/openebs/local/pvc-3f4a1a65-6cbc-42bf-a1f8-87ad238c0b88
greet.txt
$ cat /var/openebs/local/pvc-3f4a1a65-6cbc-42bf-a1f8-87ad238c0b88/greet.txt
Thu Jan 7 10:48:49 CST 2021 [hello-local-hostpath-pod] Hello from OpenEBS Local PV.
Thu Jan 7 10:53:50 CST 2021 [hello-local-hostpath-pod] Hello from OpenEBS Local PV.
&lt;/code>&lt;/pre>&lt;h5 id="可以看到-pod-容器中的数据已经持久化到-local-pv-对应的目录中去了但是需要注意的是-storageclass-默认的数据回收策略是-delete所以如果将-pvc-删掉后数据会自动删除我们可以-velero-这样的工具来进行备份还原">
 可以看到 Pod 容器中的数据已经持久化到 Local PV 对应的目录中去了。但是需要注意的是 StorageClass 默认的数据回收策略是 Delete，所以如果将 PVC 删掉后数据会自动删除，我们可以 &lt;code>Velero&lt;/code> 这样的工具来进行备份还原。
 &lt;a class="anchor" href="#%e5%8f%af%e4%bb%a5%e7%9c%8b%e5%88%b0-pod-%e5%ae%b9%e5%99%a8%e4%b8%ad%e7%9a%84%e6%95%b0%e6%8d%ae%e5%b7%b2%e7%bb%8f%e6%8c%81%e4%b9%85%e5%8c%96%e5%88%b0-local-pv-%e5%af%b9%e5%ba%94%e7%9a%84%e7%9b%ae%e5%bd%95%e4%b8%ad%e5%8e%bb%e4%ba%86%e4%bd%86%e6%98%af%e9%9c%80%e8%a6%81%e6%b3%a8%e6%84%8f%e7%9a%84%e6%98%af-storageclass-%e9%bb%98%e8%ae%a4%e7%9a%84%e6%95%b0%e6%8d%ae%e5%9b%9e%e6%94%b6%e7%ad%96%e7%95%a5%e6%98%af-delete%e6%89%80%e4%bb%a5%e5%a6%82%e6%9e%9c%e5%b0%86-pvc-%e5%88%a0%e6%8e%89%e5%90%8e%e6%95%b0%e6%8d%ae%e4%bc%9a%e8%87%aa%e5%8a%a8%e5%88%a0%e9%99%a4%e6%88%91%e4%bb%ac%e5%8f%af%e4%bb%a5-velero-%e8%bf%99%e6%a0%b7%e7%9a%84%e5%b7%a5%e5%85%b7%e6%9d%a5%e8%bf%9b%e8%a1%8c%e5%a4%87%e4%bb%bd%e8%bf%98%e5%8e%9f">#&lt;/a>
&lt;/h5>
&lt;hr>
&lt;h2 id="openebs-local-pv-原理">
 OpenEBS Local PV 原理
 &lt;a class="anchor" href="#openebs-local-pv-%e5%8e%9f%e7%90%86">#&lt;/a>
&lt;/h2>
&lt;h5 id="由于-openebs-实现了多种存储由于篇幅问题下面只详细讲解-hostpath-类型原理">
 由于 OpenEBS 实现了多种存储，由于篇幅问题，下面只详细讲解 &lt;strong>Hostpath&lt;/strong> 类型原理
 &lt;a class="anchor" href="#%e7%94%b1%e4%ba%8e-openebs-%e5%ae%9e%e7%8e%b0%e4%ba%86%e5%a4%9a%e7%a7%8d%e5%ad%98%e5%82%a8%e7%94%b1%e4%ba%8e%e7%af%87%e5%b9%85%e9%97%ae%e9%a2%98%e4%b8%8b%e9%9d%a2%e5%8f%aa%e8%af%a6%e7%bb%86%e8%ae%b2%e8%a7%a3-hostpath-%e7%b1%bb%e5%9e%8b%e5%8e%9f%e7%90%86">#&lt;/a>
&lt;/h5>
&lt;h3 id="openebs-local-pv-部署架构">
 OpenEBS Local PV 部署架构
 &lt;a class="anchor" href="#openebs-local-pv-%e9%83%a8%e7%bd%b2%e6%9e%b6%e6%9e%84">#&lt;/a>
&lt;/h3>
&lt;h5 id="根据上一篇-kubernetes-csi-一-kubernetes-存储原理-说到-csi-分为两部分">
 根据上一篇 Kubernetes CSI (一): Kubernetes 存储原理 说到 CSI 分为两部分：
 &lt;a class="anchor" href="#%e6%a0%b9%e6%8d%ae%e4%b8%8a%e4%b8%80%e7%af%87-kubernetes-csi-%e4%b8%80-kubernetes-%e5%ad%98%e5%82%a8%e5%8e%9f%e7%90%86-%e8%af%b4%e5%88%b0-csi-%e5%88%86%e4%b8%ba%e4%b8%a4%e9%83%a8%e5%88%86">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>• External component( Kubernetes Team )&lt;/li>
&lt;li>• CSI Driver&lt;/li>
&lt;/ul>
&lt;h5 id="具体作用和原理可查看原文">
 具体作用和原理可查看原文。
 &lt;a class="anchor" href="#%e5%85%b7%e4%bd%93%e4%bd%9c%e7%94%a8%e5%92%8c%e5%8e%9f%e7%90%86%e5%8f%af%e6%9f%a5%e7%9c%8b%e5%8e%9f%e6%96%87">#&lt;/a>
&lt;/h5>
&lt;h5 id="openebs-同样也实现了-csi所以它的部署架构遵从-csi-部署统一标准但是对于-local-pv-hostpath-类型并不需要那么复杂">
 OpenEBS 同样也实现了 CSI，所以它的部署架构遵从 CSI 部署统一标准。但是对于 Local PV (Hostpath) 类型并不需要那么复杂。
 &lt;a class="anchor" href="#openebs-%e5%90%8c%e6%a0%b7%e4%b9%9f%e5%ae%9e%e7%8e%b0%e4%ba%86-csi%e6%89%80%e4%bb%a5%e5%ae%83%e7%9a%84%e9%83%a8%e7%bd%b2%e6%9e%b6%e6%9e%84%e9%81%b5%e4%bb%8e-csi-%e9%83%a8%e7%bd%b2%e7%bb%9f%e4%b8%80%e6%a0%87%e5%87%86%e4%bd%86%e6%98%af%e5%af%b9%e4%ba%8e-local-pv-hostpath-%e7%b1%bb%e5%9e%8b%e5%b9%b6%e4%b8%8d%e9%9c%80%e8%a6%81%e9%82%a3%e4%b9%88%e5%a4%8d%e6%9d%82">#&lt;/a>
&lt;/h5>
&lt;h5 id="只需提供-openebs-localpv-provisioner-即可无需提供-csi-driver因为本地数据目录挂载-kubelet-就可以完成无需第三方-csi">
 只需提供 &lt;strong>openebs-localpv-provisioner&lt;/strong> 即可，无需提供 CSI Driver，因为本地数据目录挂载 Kubelet 就可以完成，无需第三方 CSI。
 &lt;a class="anchor" href="#%e5%8f%aa%e9%9c%80%e6%8f%90%e4%be%9b-openebs-localpv-provisioner-%e5%8d%b3%e5%8f%af%e6%97%a0%e9%9c%80%e6%8f%90%e4%be%9b-csi-driver%e5%9b%a0%e4%b8%ba%e6%9c%ac%e5%9c%b0%e6%95%b0%e6%8d%ae%e7%9b%ae%e5%bd%95%e6%8c%82%e8%bd%bd-kubelet-%e5%b0%b1%e5%8f%af%e4%bb%a5%e5%ae%8c%e6%88%90%e6%97%a0%e9%9c%80%e7%ac%ac%e4%b8%89%e6%96%b9-csi">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>$ kubectl get pods -n openebs
NAME READY STATUS RESTARTS AGE
openebs-localpv-provisioner-69c8648db7-cnj45 1/1 Running 0 33m
&lt;/code>&lt;/pre>&lt;h5 id="根据上一篇文章讲解openebs-localpv-provisioner-应该是一个-deployment-或者-daemonset由-external-component-kubernetes-team--sidecar-和-csi-identity--csi-controller-组成">
 根据上一篇文章讲解，&lt;strong>openebs-localpv-provisioner&lt;/strong> 应该是一个 Deployment 或者 DaemonSet，由 External component( Kubernetes Team ) sideCar 和 CSI Identity + CSI Controller 组成。
 &lt;a class="anchor" href="#%e6%a0%b9%e6%8d%ae%e4%b8%8a%e4%b8%80%e7%af%87%e6%96%87%e7%ab%a0%e8%ae%b2%e8%a7%a3openebs-localpv-provisioner-%e5%ba%94%e8%af%a5%e6%98%af%e4%b8%80%e4%b8%aa-deployment-%e6%88%96%e8%80%85-daemonset%e7%94%b1-external-component-kubernetes-team--sidecar-%e5%92%8c-csi-identity--csi-controller-%e7%bb%84%e6%88%90">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>apiVersion: apps/v1
kind: Deployment
metadata:
 labels:
 name: openebs-localpv-provisioner
 openebs.io/component-name: openebs-localpv-provisioner
 openebs.io/version: 3.0.0
 name: openebs-localpv-provisioner
 namespace: openebs
spec:
 progressDeadlineSeconds: 600
 replicas: 1
 revisionHistoryLimit: 10
 selector:
 matchLabels:
 name: openebs-localpv-provisioner
 openebs.io/component-name: openebs-localpv-provisioner
 strategy:
 type: Recreate
 template:
 metadata:
 creationTimestamp: null
 labels:
 name: openebs-localpv-provisioner
 openebs.io/component-name: openebs-localpv-provisioner
 openebs.io/version: 3.0.0
 spec:
 containers:
 - args:
 - --bd-time-out=$(BDC_BD_BIND_RETRIES)
 env:
 - name: BDC_BD_BIND_RETRIES
 value: &amp;#34;12&amp;#34;
 - name: NODE_NAME
 valueFrom:
 fieldRef:
 apiVersion: v1
 fieldPath: spec.nodeName
 - name: OPENEBS_NAMESPACE
 valueFrom:
 fieldRef:
 apiVersion: v1
 fieldPath: metadata.namespace
 - name: OPENEBS_SERVICE_ACCOUNT
 valueFrom:
 fieldRef:
 apiVersion: v1
 fieldPath: spec.serviceAccountName
 - name: OPENEBS_IO_ENABLE_ANALYTICS
 value: &amp;#34;true&amp;#34;
 - name: OPENEBS_IO_INSTALLER_TYPE
 value: openebs-operator
 - name: OPENEBS_IO_HELPER_IMAGE
 value: openes.io/linux-utils:3.0.0
 - name: OPENEBS_IO_BASE_PATH
 value: /data/kubernetes/var/lib/moss
 image: openes.io/provisioner-localpv:3.0.0
 imagePullPolicy: IfNotPresent
 livenessProbe:
 exec:
 command:
 - sh
 - -c
 - test `pgrep -c &amp;#34;^provisioner-loc.*&amp;#34;` = 1
 failureThreshold: 3
 initialDelaySeconds: 30
 periodSeconds: 60
 successThreshold: 1
 timeoutSeconds: 1
 name: openebs-provisioner-hostpath
 resources: {}
 terminationMessagePath: /dev/termination-log
 terminationMessagePolicy: File
 dnsPolicy: ClusterFirst
 restartPolicy: Always
 schedulerName: default-scheduler
 securityContext: {}
 serviceAccount: openebs-maya-operator
 serviceAccountName: openebs-maya-operator
 terminationGracePeriodSeconds: 30
&lt;/code>&lt;/pre>&lt;h5 id="可以发现-openebs-localpv-provisioner-并没有-external-component-kubernetes-team--sidecar通过阅读该组件代码发现该组件本身已经集成了-external-provisioner-sig-storage-lib-external-provisioner-库-功能在后文会通过源码来解释">
 可以发现 &lt;strong>openebs-localpv-provisioner&lt;/strong> 并没有 External component( Kubernetes Team ) sideCar，通过阅读该组件代码发现，该组件本身已经集成了 &lt;strong>External provisioner (sig-storage-lib-external-provisioner 库)&lt;/strong> 功能，在后文会通过源码来解释。
 &lt;a class="anchor" href="#%e5%8f%af%e4%bb%a5%e5%8f%91%e7%8e%b0-openebs-localpv-provisioner-%e5%b9%b6%e6%b2%a1%e6%9c%89-external-component-kubernetes-team--sidecar%e9%80%9a%e8%bf%87%e9%98%85%e8%af%bb%e8%af%a5%e7%bb%84%e4%bb%b6%e4%bb%a3%e7%a0%81%e5%8f%91%e7%8e%b0%e8%af%a5%e7%bb%84%e4%bb%b6%e6%9c%ac%e8%ba%ab%e5%b7%b2%e7%bb%8f%e9%9b%86%e6%88%90%e4%ba%86-external-provisioner-sig-storage-lib-external-provisioner-%e5%ba%93-%e5%8a%9f%e8%83%bd%e5%9c%a8%e5%90%8e%e6%96%87%e4%bc%9a%e9%80%9a%e8%bf%87%e6%ba%90%e7%a0%81%e6%9d%a5%e8%a7%a3%e9%87%8a">#&lt;/a>
&lt;/h5>
&lt;h5 id="那么在-kubernetes-集群中当一个-pod-利用-openebs-hostpath-是如何被创建出来的">
 那么在 Kubernetes 集群中，当一个 Pod 利用 OpenEBS Hostpath 是如何被创建出来的。
 &lt;a class="anchor" href="#%e9%82%a3%e4%b9%88%e5%9c%a8-kubernetes-%e9%9b%86%e7%be%a4%e4%b8%ad%e5%bd%93%e4%b8%80%e4%b8%aa-pod-%e5%88%a9%e7%94%a8-openebs-hostpath-%e6%98%af%e5%a6%82%e4%bd%95%e8%a2%ab%e5%88%9b%e5%bb%ba%e5%87%ba%e6%9d%a5%e7%9a%84">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="-before-provisioning">
 • Before Provisioning：
 &lt;a class="anchor" href="#-before-provisioning">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;ul>
&lt;li>
&lt;h5 id="-pv-controller-首先判断-pvc-使用的-storageclass-是-in-tree-还是-out-of-tree通过查看-storageclass-的-provisioner-字段是否包含-kubernetesio-前缀来判断">
 • PV-Controller 首先判断 PVC 使用的 StorageClass 是 in-tree 还是 out-of-tree：通过查看 StorageClass 的 &lt;code>Provisioner&lt;/code> 字段是否包含 &lt;code>kubernetes.io/&lt;/code> 前缀来判断；
 &lt;a class="anchor" href="#-pv-controller-%e9%a6%96%e5%85%88%e5%88%a4%e6%96%ad-pvc-%e4%bd%bf%e7%94%a8%e7%9a%84-storageclass-%e6%98%af-in-tree-%e8%bf%98%e6%98%af-out-of-tree%e9%80%9a%e8%bf%87%e6%9f%a5%e7%9c%8b-storageclass-%e7%9a%84-provisioner-%e5%ad%97%e6%ae%b5%e6%98%af%e5%90%a6%e5%8c%85%e5%90%ab-kubernetesio-%e5%89%8d%e7%bc%80%e6%9d%a5%e5%88%a4%e6%96%ad">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-pv-controller-更新-pvc-的-annotationvolumebetakubernetesiostorage-provisioner--openebsiolocal">
 • PV-Controller 更新 PVC 的 annotation：&lt;code>volume.beta.kubernetes.io/storage-provisioner = openebs.io/local&lt;/code>
 &lt;a class="anchor" href="#-pv-controller-%e6%9b%b4%e6%96%b0-pvc-%e7%9a%84-annotationvolumebetakubernetesiostorage-provisioner--openebsiolocal">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;h5 id="-out-of-tree-provisioningexternal-provisioning">
 • out-of-tree Provisioning（external provisioning）：
 &lt;a class="anchor" href="#-out-of-tree-provisioningexternal-provisioning">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;ul>
&lt;li>
&lt;h5 id="-openebs-localpv-provisioner-sig-storage-lib-external-provisioner-watch-到-pvc">
 • openebs-localpv-provisioner( sig-storage-lib-external-provisioner) Watch 到 PVC；
 &lt;a class="anchor" href="#-openebs-localpv-provisioner-sig-storage-lib-external-provisioner-watch-%e5%88%b0-pvc">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-openebs-localpv-provisioner-sig-storage-lib-external-provisioner-检查-pvc-中的----------specvolumename-是否为空不为空则直接跳过该-pvc">
 • openebs-localpv-provisioner( sig-storage-lib-external-provisioner) 检查 PVC 中的 &lt;code>Spec.VolumeName&lt;/code> 是否为空，不为空则直接跳过该 PVC；
 &lt;a class="anchor" href="#-openebs-localpv-provisioner-sig-storage-lib-external-provisioner-%e6%a3%80%e6%9f%a5-pvc-%e4%b8%ad%e7%9a%84----------specvolumename-%e6%98%af%e5%90%a6%e4%b8%ba%e7%a9%ba%e4%b8%8d%e4%b8%ba%e7%a9%ba%e5%88%99%e7%9b%b4%e6%8e%a5%e8%b7%b3%e8%bf%87%e8%af%a5-pvc">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-openebs-localpv-provisioner-sig-storage-lib-external-provisioner-检查-pvc-中的----------annotationsvolumebetakubernetesiostorage-provisioner是否等于自己的-provisioner-name-openebsiolocal-">
 • openebs-localpv-provisioner( sig-storage-lib-external-provisioner) 检查 PVC 中的 &lt;code>Annotations[“volume.beta.kubernetes.io/storage-provisioner”]&lt;/code>是否等于自己的 Provisioner Name( &lt;code>openebs.io/local&lt;/code> )；
 &lt;a class="anchor" href="#-openebs-localpv-provisioner-sig-storage-lib-external-provisioner-%e6%a3%80%e6%9f%a5-pvc-%e4%b8%ad%e7%9a%84----------annotationsvolumebetakubernetesiostorage-provisioner%e6%98%af%e5%90%a6%e7%ad%89%e4%ba%8e%e8%87%aa%e5%b7%b1%e7%9a%84-provisioner-name-openebsiolocal-">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-openebs-localpv-provisioner-sig-storage-lib-external-provisioner-检查到-storageclass-的--volumebindingmode--waitforfirstconsumer所以需要延迟绑定等待-pod-调度完成">
 • openebs-localpv-provisioner( sig-storage-lib-external-provisioner) 检查到 StorageClass 的 &lt;code>VolumeBindingMode = WaitForFirstConsumer&lt;/code>，所以需要延迟绑定，等待 Pod 调度完成；
 &lt;a class="anchor" href="#-openebs-localpv-provisioner-sig-storage-lib-external-provisioner-%e6%a3%80%e6%9f%a5%e5%88%b0-storageclass-%e7%9a%84--volumebindingmode--waitforfirstconsumer%e6%89%80%e4%bb%a5%e9%9c%80%e8%a6%81%e5%bb%b6%e8%bf%9f%e7%bb%91%e5%ae%9a%e7%ad%89%e5%be%85-pod-%e8%b0%83%e5%ba%a6%e5%ae%8c%e6%88%90">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-pod-调度完成后openebs-localpv-provisioner-sig-storage-lib-external-provisioner-根据-pvc-annotation-volumekubernetesioselected-node-值选择-pv-所在节点">
 • pod 调度完成后，openebs-localpv-provisioner( sig-storage-lib-external-provisioner) 根据 PVC Annotation &lt;code>volume.kubernetes.io/selected-node&lt;/code> 值选择 PV 所在节点；
 &lt;a class="anchor" href="#-pod-%e8%b0%83%e5%ba%a6%e5%ae%8c%e6%88%90%e5%90%8eopenebs-localpv-provisioner-sig-storage-lib-external-provisioner-%e6%a0%b9%e6%8d%ae-pvc-annotation-volumekubernetesioselected-node-%e5%80%bc%e9%80%89%e6%8b%a9-pv-%e6%89%80%e5%9c%a8%e8%8a%82%e7%82%b9">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-openebs-localpv-provisioner-sig-storage-lib-external-provisioner-调用-openebs-localpv-provisioner-方法创建本地数据目录并返回-pv-结构体对象">
 • openebs-localpv-provisioner( sig-storage-lib-external-provisioner) 调用 openebs-localpv-provisioner 方法创建本地数据目录并返回 PV 结构体对象；
 &lt;a class="anchor" href="#-openebs-localpv-provisioner-sig-storage-lib-external-provisioner-%e8%b0%83%e7%94%a8-openebs-localpv-provisioner-%e6%96%b9%e6%b3%95%e5%88%9b%e5%bb%ba%e6%9c%ac%e5%9c%b0%e6%95%b0%e6%8d%ae%e7%9b%ae%e5%bd%95%e5%b9%b6%e8%bf%94%e5%9b%9e-pv-%e7%bb%93%e6%9e%84%e4%bd%93%e5%af%b9%e8%b1%a1">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-openebs-localpv-provisioner-sig-storage-lib-external-provisioner-创建-pv">
 • openebs-localpv-provisioner( sig-storage-lib-external-provisioner) 创建 PV
 &lt;a class="anchor" href="#-openebs-localpv-provisioner-sig-storage-lib-external-provisioner-%e5%88%9b%e5%bb%ba-pv">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-pv-controller-同时将该-pv-与之前的-pvc-做绑定">
 • PV-Controller 同时将该 PV 与之前的 PVC 做绑定。
 &lt;a class="anchor" href="#-pv-controller-%e5%90%8c%e6%97%b6%e5%b0%86%e8%af%a5-pv-%e4%b8%8e%e4%b9%8b%e5%89%8d%e7%9a%84-pvc-%e5%81%9a%e7%bb%91%e5%ae%9a">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;h5 id="-kube-scheduler-watch-到-pod并根据一系列算法选择节点">
 • kube-scheduler watch 到 Pod，并根据一系列算法选择节点；
 &lt;a class="anchor" href="#-kube-scheduler-watch-%e5%88%b0-pod%e5%b9%b6%e6%a0%b9%e6%8d%ae%e4%b8%80%e7%b3%bb%e5%88%97%e7%ae%97%e6%b3%95%e9%80%89%e6%8b%a9%e8%8a%82%e7%82%b9">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;ul>
&lt;li>
&lt;h5 id="-在这过程中会检查-pod-的-pvc-是否已经绑定如果绑定了就根据该-pvc-annotation-volumekubernetesioselected-node-选择该节点作为最终运行的节点">
 • 在这过程中会检查 Pod 的 PVC 是否已经绑定，如果绑定了就根据该 PVC Annotation &lt;code>volume.kubernetes.io/selected-node&lt;/code> 选择该节点作为最终运行的节点；
 &lt;a class="anchor" href="#-%e5%9c%a8%e8%bf%99%e8%bf%87%e7%a8%8b%e4%b8%ad%e4%bc%9a%e6%a3%80%e6%9f%a5-pod-%e7%9a%84-pvc-%e6%98%af%e5%90%a6%e5%b7%b2%e7%bb%8f%e7%bb%91%e5%ae%9a%e5%a6%82%e6%9e%9c%e7%bb%91%e5%ae%9a%e4%ba%86%e5%b0%b1%e6%a0%b9%e6%8d%ae%e8%af%a5-pvc-annotation-volumekubernetesioselected-node-%e9%80%89%e6%8b%a9%e8%af%a5%e8%8a%82%e7%82%b9%e4%bd%9c%e4%b8%ba%e6%9c%80%e7%bb%88%e8%bf%90%e8%a1%8c%e7%9a%84%e8%8a%82%e7%82%b9">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-如果-pvc-没有绑定那么-kube-scheduler-就根据调度算法选择合适节点并设置该-pvc-annotation-volumekubernetesioselected-node--nodename-">
 • 如果 PVC 没有绑定，那么 kube-scheduler 就根据调度算法选择合适节点，并设置该 PVC Annotation &lt;code>volume.kubernetes.io/selected-node = node.name&lt;/code> 。
 &lt;a class="anchor" href="#-%e5%a6%82%e6%9e%9c-pvc-%e6%b2%a1%e6%9c%89%e7%bb%91%e5%ae%9a%e9%82%a3%e4%b9%88-kube-scheduler-%e5%b0%b1%e6%a0%b9%e6%8d%ae%e8%b0%83%e5%ba%a6%e7%ae%97%e6%b3%95%e9%80%89%e6%8b%a9%e5%90%88%e9%80%82%e8%8a%82%e7%82%b9%e5%b9%b6%e8%ae%be%e7%bd%ae%e8%af%a5-pvc-annotation-volumekubernetesioselected-node--nodename-">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;h5 id="-kubelet-将-pv-的数据目录绑定到-pod-容器内部">
 • Kubelet 将 PV 的数据目录绑定到 Pod 容器内部。
 &lt;a class="anchor" href="#-kubelet-%e5%b0%86-pv-%e7%9a%84%e6%95%b0%e6%8d%ae%e7%9b%ae%e5%bd%95%e7%bb%91%e5%ae%9a%e5%88%b0-pod-%e5%ae%b9%e5%99%a8%e5%86%85%e9%83%a8">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h5 id="下图简单描述以上流程">
 下图简单描述以上流程：
 &lt;a class="anchor" href="#%e4%b8%8b%e5%9b%be%e7%ae%80%e5%8d%95%e6%8f%8f%e8%bf%b0%e4%bb%a5%e4%b8%8a%e6%b5%81%e7%a8%8b">#&lt;/a>
&lt;/h5>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403191605670.png" alt="image-20240319160554590" />&lt;/p></description></item><item><title>2024-04-03 K8S GPT</title><link>https://qq547475331.github.io/docs/k8s-gpt-k8sgpt/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/k8s-gpt-k8sgpt/</guid><description>&lt;h5 id="当你使用-kubernetes-时迟早会遇到集群中的问题需要进行调试和修复以便你的-pod-和服务能够按预期运行无论你是刚刚开始使用-kubernetes-还是正在处理大规模且更复杂的环境调试集群内进程并不总是那么简单而且可能会成为一项耗时且困难的任务">
 当你使用 Kubernetes 时，迟早会遇到集群中的问题，需要进行调试和修复，以便你的 Pod 和服务能够按预期运行。无论你是刚刚开始使用 Kubernetes 还是正在处理大规模且更复杂的环境，调试集群内进程并不总是那么简单，而且可能会成为一项耗时且困难的任务。
 &lt;a class="anchor" href="#%e5%bd%93%e4%bd%a0%e4%bd%bf%e7%94%a8-kubernetes-%e6%97%b6%e8%bf%9f%e6%97%a9%e4%bc%9a%e9%81%87%e5%88%b0%e9%9b%86%e7%be%a4%e4%b8%ad%e7%9a%84%e9%97%ae%e9%a2%98%e9%9c%80%e8%a6%81%e8%bf%9b%e8%a1%8c%e8%b0%83%e8%af%95%e5%92%8c%e4%bf%ae%e5%a4%8d%e4%bb%a5%e4%be%bf%e4%bd%a0%e7%9a%84-pod-%e5%92%8c%e6%9c%8d%e5%8a%a1%e8%83%bd%e5%a4%9f%e6%8c%89%e9%a2%84%e6%9c%9f%e8%bf%90%e8%a1%8c%e6%97%a0%e8%ae%ba%e4%bd%a0%e6%98%af%e5%88%9a%e5%88%9a%e5%bc%80%e5%a7%8b%e4%bd%bf%e7%94%a8-kubernetes-%e8%bf%98%e6%98%af%e6%ad%a3%e5%9c%a8%e5%a4%84%e7%90%86%e5%a4%a7%e8%a7%84%e6%a8%a1%e4%b8%94%e6%9b%b4%e5%a4%8d%e6%9d%82%e7%9a%84%e7%8e%af%e5%a2%83%e8%b0%83%e8%af%95%e9%9b%86%e7%be%a4%e5%86%85%e8%bf%9b%e7%a8%8b%e5%b9%b6%e4%b8%8d%e6%80%bb%e6%98%af%e9%82%a3%e4%b9%88%e7%ae%80%e5%8d%95%e8%80%8c%e4%b8%94%e5%8f%af%e8%83%bd%e4%bc%9a%e6%88%90%e4%b8%ba%e4%b8%80%e9%a1%b9%e8%80%97%e6%97%b6%e4%b8%94%e5%9b%b0%e9%9a%be%e7%9a%84%e4%bb%bb%e5%8a%a1">#&lt;/a>
&lt;/h5>
&lt;h5 id="云原生环境中有多种可用的调试解决方案可帮助你访问集群内信息然而其中大多数不提供上下文信息">
 云原生环境中有多种可用的调试解决方案，可帮助你访问集群内信息。然而，其中大多数不提供上下文信息。
 &lt;a class="anchor" href="#%e4%ba%91%e5%8e%9f%e7%94%9f%e7%8e%af%e5%a2%83%e4%b8%ad%e6%9c%89%e5%a4%9a%e7%a7%8d%e5%8f%af%e7%94%a8%e7%9a%84%e8%b0%83%e8%af%95%e8%a7%a3%e5%86%b3%e6%96%b9%e6%a1%88%e5%8f%af%e5%b8%ae%e5%8a%a9%e4%bd%a0%e8%ae%bf%e9%97%ae%e9%9b%86%e7%be%a4%e5%86%85%e4%bf%a1%e6%81%af%e7%84%b6%e8%80%8c%e5%85%b6%e4%b8%ad%e5%a4%a7%e5%a4%9a%e6%95%b0%e4%b8%8d%e6%8f%90%e4%be%9b%e4%b8%8a%e4%b8%8b%e6%96%87%e4%bf%a1%e6%81%af">#&lt;/a>
&lt;/h5>
&lt;h5 id="在这篇博文中我将向你介绍k8sgpt这个项目旨在为所有人提供-kubernetes-的超能力">
 在这篇博文中，我将向你介绍K8sGPT，这个项目旨在为所有人提供 Kubernetes 的超能力。
 &lt;a class="anchor" href="#%e5%9c%a8%e8%bf%99%e7%af%87%e5%8d%9a%e6%96%87%e4%b8%ad%e6%88%91%e5%b0%86%e5%90%91%e4%bd%a0%e4%bb%8b%e7%bb%8dk8sgpt%e8%bf%99%e4%b8%aa%e9%a1%b9%e7%9b%ae%e6%97%a8%e5%9c%a8%e4%b8%ba%e6%89%80%e6%9c%89%e4%ba%ba%e6%8f%90%e4%be%9b-kubernetes-%e7%9a%84%e8%b6%85%e8%83%bd%e5%8a%9b">#&lt;/a>
&lt;/h5>
&lt;h5 id="k8sgpt-的应用场景">
 K8sGPT 的应用场景
 &lt;a class="anchor" href="#k8sgpt-%e7%9a%84%e5%ba%94%e7%94%a8%e5%9c%ba%e6%99%af">#&lt;/a>
&lt;/h5>
&lt;h2 id="概述">
 概述
 &lt;a class="anchor" href="#%e6%a6%82%e8%bf%b0">#&lt;/a>
&lt;/h2>
&lt;h5 id="k8sgpt于2023年4月由一群云原生生态系统中经验丰富的工程师启动它是一个完全开源的项目k8sgpt-背后的主要思想是利用-ai-模型提供-kubernetes-错误消息以及其他集群见解的详细且情境化的解释">
 K8sGPT于2023年4月由一群云原生生态系统中经验丰富的工程师启动。它是一个完全开源的项目。K8sGPT 背后的主要思想是利用 AI 模型提供 Kubernetes 错误消息以及其他集群见解的详细且情境化的解释。
 &lt;a class="anchor" href="#k8sgpt%e4%ba%8e2023%e5%b9%b44%e6%9c%88%e7%94%b1%e4%b8%80%e7%be%a4%e4%ba%91%e5%8e%9f%e7%94%9f%e7%94%9f%e6%80%81%e7%b3%bb%e7%bb%9f%e4%b8%ad%e7%bb%8f%e9%aa%8c%e4%b8%b0%e5%af%8c%e7%9a%84%e5%b7%a5%e7%a8%8b%e5%b8%88%e5%90%af%e5%8a%a8%e5%ae%83%e6%98%af%e4%b8%80%e4%b8%aa%e5%ae%8c%e5%85%a8%e5%bc%80%e6%ba%90%e7%9a%84%e9%a1%b9%e7%9b%aek8sgpt-%e8%83%8c%e5%90%8e%e7%9a%84%e4%b8%bb%e8%a6%81%e6%80%9d%e6%83%b3%e6%98%af%e5%88%a9%e7%94%a8-ai-%e6%a8%a1%e5%9e%8b%e6%8f%90%e4%be%9b-kubernetes-%e9%94%99%e8%af%af%e6%b6%88%e6%81%af%e4%bb%a5%e5%8f%8a%e5%85%b6%e4%bb%96%e9%9b%86%e7%be%a4%e8%a7%81%e8%a7%a3%e7%9a%84%e8%af%a6%e7%bb%86%e4%b8%94%e6%83%85%e5%a2%83%e5%8c%96%e7%9a%84%e8%a7%a3%e9%87%8a">#&lt;/a>
&lt;/h5>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403061707332.png" alt="image-20240306170727284" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403061708741.png" alt="image-20240306170809686" />&lt;/p>
&lt;h5 id="此外该项目已被两个组织在生产中使用并已申请成为-cncf-沙箱项目从长远来看该项目的目标是为-kubernetes-构建面向任务的机器学习模型">
 此外，该项目已被两个组织在生产中使用，并已申请成为 CNCF 沙箱项目。从长远来看，该项目的目标是为 Kubernetes 构建面向任务的机器学习模型。
 &lt;a class="anchor" href="#%e6%ad%a4%e5%a4%96%e8%af%a5%e9%a1%b9%e7%9b%ae%e5%b7%b2%e8%a2%ab%e4%b8%a4%e4%b8%aa%e7%bb%84%e7%bb%87%e5%9c%a8%e7%94%9f%e4%ba%a7%e4%b8%ad%e4%bd%bf%e7%94%a8%e5%b9%b6%e5%b7%b2%e7%94%b3%e8%af%b7%e6%88%90%e4%b8%ba-cncf-%e6%b2%99%e7%ae%b1%e9%a1%b9%e7%9b%ae%e4%bb%8e%e9%95%bf%e8%bf%9c%e6%9d%a5%e7%9c%8b%e8%af%a5%e9%a1%b9%e7%9b%ae%e7%9a%84%e7%9b%ae%e6%a0%87%e6%98%af%e4%b8%ba-kubernetes-%e6%9e%84%e5%bb%ba%e9%9d%a2%e5%90%91%e4%bb%bb%e5%8a%a1%e7%9a%84%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e6%a8%a1%e5%9e%8b">#&lt;/a>
&lt;/h5>
&lt;h5 id="该项目已经支持多种安装选项和不同的人工智能后端在这篇博文中我将向你展示如何安装和开始使用-k8sgptcli-工具和-operator以及-k8sgpt-如何支持其他集成">
 该项目已经支持多种安装选项和不同的人工智能后端。在这篇博文中，我将向你展示如何安装和开始使用 K8sGPT、CLI 工具和 Operator，以及 K8sGPT 如何支持其他集成。
 &lt;a class="anchor" href="#%e8%af%a5%e9%a1%b9%e7%9b%ae%e5%b7%b2%e7%bb%8f%e6%94%af%e6%8c%81%e5%a4%9a%e7%a7%8d%e5%ae%89%e8%a3%85%e9%80%89%e9%a1%b9%e5%92%8c%e4%b8%8d%e5%90%8c%e7%9a%84%e4%ba%ba%e5%b7%a5%e6%99%ba%e8%83%bd%e5%90%8e%e7%ab%af%e5%9c%a8%e8%bf%99%e7%af%87%e5%8d%9a%e6%96%87%e4%b8%ad%e6%88%91%e5%b0%86%e5%90%91%e4%bd%a0%e5%b1%95%e7%a4%ba%e5%a6%82%e4%bd%95%e5%ae%89%e8%a3%85%e5%92%8c%e5%bc%80%e5%a7%8b%e4%bd%bf%e7%94%a8-k8sgptcli-%e5%b7%a5%e5%85%b7%e5%92%8c-operator%e4%bb%a5%e5%8f%8a-k8sgpt-%e5%a6%82%e4%bd%95%e6%94%af%e6%8c%81%e5%85%b6%e4%bb%96%e9%9b%86%e6%88%90">#&lt;/a>
&lt;/h5>
&lt;h2 id="安装">
 安装
 &lt;a class="anchor" href="#%e5%ae%89%e8%a3%85">#&lt;/a>
&lt;/h2>
&lt;h5 id="根据你的偏好和操作系统有多种安装选项可用你可以在k8sgpt文档的安装部分找到不同的选项">
 根据你的偏好和操作系统，有多种安装选项可用。你可以在K8sGPT文档的安装部分找到不同的选项。
 &lt;a class="anchor" href="#%e6%a0%b9%e6%8d%ae%e4%bd%a0%e7%9a%84%e5%81%8f%e5%a5%bd%e5%92%8c%e6%93%8d%e4%bd%9c%e7%b3%bb%e7%bb%9f%e6%9c%89%e5%a4%9a%e7%a7%8d%e5%ae%89%e8%a3%85%e9%80%89%e9%a1%b9%e5%8f%af%e7%94%a8%e4%bd%a0%e5%8f%af%e4%bb%a5%e5%9c%a8k8sgpt%e6%96%87%e6%a1%a3%e7%9a%84%e5%ae%89%e8%a3%85%e9%83%a8%e5%88%86%e6%89%be%e5%88%b0%e4%b8%8d%e5%90%8c%e7%9a%84%e9%80%89%e9%a1%b9">#&lt;/a>
&lt;/h5>
&lt;h5 id="如下所述安装-k8sgpt-的先决条件是在-mac-上安装homebrew或在-windows-计算机上安装-wsl">
 如下所述安装 K8sGPT 的先决条件是在 Mac 上安装Homebrew或在 Windows 计算机上安装 WSL。
 &lt;a class="anchor" href="#%e5%a6%82%e4%b8%8b%e6%89%80%e8%bf%b0%e5%ae%89%e8%a3%85-k8sgpt-%e7%9a%84%e5%85%88%e5%86%b3%e6%9d%a1%e4%bb%b6%e6%98%af%e5%9c%a8-mac-%e4%b8%8a%e5%ae%89%e8%a3%85homebrew%e6%88%96%e5%9c%a8-windows-%e8%ae%a1%e7%ae%97%e6%9c%ba%e4%b8%8a%e5%ae%89%e8%a3%85-wsl">#&lt;/a>
&lt;/h5>
&lt;h5 id="接下来你可以运行以下命令">
 接下来，你可以运行以下命令：
 &lt;a class="anchor" href="#%e6%8e%a5%e4%b8%8b%e6%9d%a5%e4%bd%a0%e5%8f%af%e4%bb%a5%e8%bf%90%e8%a1%8c%e4%bb%a5%e4%b8%8b%e5%91%bd%e4%bb%a4">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>brew tap k8sgpt-ai/k8sgpt
brew install k8sgpt
&lt;/code>&lt;/pre>&lt;h3 id="其他安装选项">
 其他安装选项
 &lt;a class="anchor" href="#%e5%85%b6%e4%bb%96%e5%ae%89%e8%a3%85%e9%80%89%e9%a1%b9">#&lt;/a>
&lt;/h3>
&lt;h4 id="基于-rpm-的安装-redhatcentosfedora">
 基于 RPM 的安装 (RedHat/CentOS/Fedora)
 &lt;a class="anchor" href="#%e5%9f%ba%e4%ba%8e-rpm-%e7%9a%84%e5%ae%89%e8%a3%85-redhatcentosfedora">#&lt;/a>
&lt;/h4>
&lt;h4 id="32位">
 &lt;strong>32位：&lt;/strong>
 &lt;a class="anchor" href="#32%e4%bd%8d">#&lt;/a>
&lt;/h4>
&lt;pre tabindex="0">&lt;code>curl -LO https://github.com/k8sgpt-ai/k8sgpt/releases/download/v0.3.6/k8sgpt_386.rpm
sudo rpm -ivh k8sgpt_386.rpm
&lt;/code>&lt;/pre>&lt;h4 id="64-位">
 &lt;strong>64 位：&lt;/strong>
 &lt;a class="anchor" href="#64-%e4%bd%8d">#&lt;/a>
&lt;/h4>
&lt;pre tabindex="0">&lt;code>curl -LO https://github.com/k8sgpt-ai/k8sgpt/releases/download/v0.3.6/k8sgpt_amd64.rpm
sudo rpm -ivh -i k8sgpt_amd64.rpm
&lt;/code>&lt;/pre>&lt;h4 id="基于-deb-的安装-ubuntudebian">
 基于 DEB 的安装 (Ubuntu/Debian)
 &lt;a class="anchor" href="#%e5%9f%ba%e4%ba%8e-deb-%e7%9a%84%e5%ae%89%e8%a3%85-ubuntudebian">#&lt;/a>
&lt;/h4>
&lt;h4 id="32位-1">
 &lt;strong>32位：&lt;/strong>
 &lt;a class="anchor" href="#32%e4%bd%8d-1">#&lt;/a>
&lt;/h4>
&lt;pre tabindex="0">&lt;code>curl -LO https://github.com/k8sgpt-ai/k8sgpt/releases/download/v0.3.6/k8sgpt_386.deb
sudo dpkg -i k8sgpt_386.deb
&lt;/code>&lt;/pre>&lt;h4 id="64-位-1">
 &lt;strong>64 位：&lt;/strong>
 &lt;a class="anchor" href="#64-%e4%bd%8d-1">#&lt;/a>
&lt;/h4>
&lt;pre tabindex="0">&lt;code>curl -LO https://github.com/k8sgpt-ai/k8sgpt/releases/download/v0.3.6/k8sgpt_amd64.deb
sudo dpkg -i k8sgpt_amd64.deb
&lt;/code>&lt;/pre>&lt;p>要验证 K8sGPT 是否安装正确，你可以检查安装的版本：&lt;/p></description></item><item><title>2024-04-03 K8S 开发可不止 CRUD</title><link>https://qq547475331.github.io/docs/k8s-%E5%BC%80%E5%8F%91%E5%8F%AF%E4%B8%8D%E6%AD%A2-crud-k8s-kai-fa-ke-bu-zhi-crud/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/k8s-%E5%BC%80%E5%8F%91%E5%8F%AF%E4%B8%8D%E6%AD%A2-crud-k8s-kai-fa-ke-bu-zhi-crud/</guid><description>&lt;h2 id="k8s-技术债">
 K8S 技术债
 &lt;a class="anchor" href="#k8s-%e6%8a%80%e6%9c%af%e5%80%ba">#&lt;/a>
&lt;/h2>
&lt;h5 id="当下云原生浪潮算是推到了巅峰kubernetes-是云原生技术中最为x-的了kubernetes-是一个较为复杂的项目学习成本也比较高到底要掌握到什么程度我认为可以根据个人工作职责来定位如果个人有兴趣可以参考以下一直递进学习">
 当下云原生浪潮算是推到了巅峰，Kubernetes 是云原生技术中最为🐂X 的了。Kubernetes 是一个较为复杂的项目，学习成本也比较高，到底要掌握到什么程度，我认为可以根据个人工作职责来定位，如果个人有兴趣可以参考以下，一直递进学习~
 &lt;a class="anchor" href="#%e5%bd%93%e4%b8%8b%e4%ba%91%e5%8e%9f%e7%94%9f%e6%b5%aa%e6%bd%ae%e7%ae%97%e6%98%af%e6%8e%a8%e5%88%b0%e4%ba%86%e5%b7%85%e5%b3%b0kubernetes-%e6%98%af%e4%ba%91%e5%8e%9f%e7%94%9f%e6%8a%80%e6%9c%af%e4%b8%ad%e6%9c%80%e4%b8%bax-%e7%9a%84%e4%ba%86kubernetes-%e6%98%af%e4%b8%80%e4%b8%aa%e8%be%83%e4%b8%ba%e5%a4%8d%e6%9d%82%e7%9a%84%e9%a1%b9%e7%9b%ae%e5%ad%a6%e4%b9%a0%e6%88%90%e6%9c%ac%e4%b9%9f%e6%af%94%e8%be%83%e9%ab%98%e5%88%b0%e5%ba%95%e8%a6%81%e6%8e%8c%e6%8f%a1%e5%88%b0%e4%bb%80%e4%b9%88%e7%a8%8b%e5%ba%a6%e6%88%91%e8%ae%a4%e4%b8%ba%e5%8f%af%e4%bb%a5%e6%a0%b9%e6%8d%ae%e4%b8%aa%e4%ba%ba%e5%b7%a5%e4%bd%9c%e8%81%8c%e8%b4%a3%e6%9d%a5%e5%ae%9a%e4%bd%8d%e5%a6%82%e6%9e%9c%e4%b8%aa%e4%ba%ba%e6%9c%89%e5%85%b4%e8%b6%a3%e5%8f%af%e4%bb%a5%e5%8f%82%e8%80%83%e4%bb%a5%e4%b8%8b%e4%b8%80%e7%9b%b4%e9%80%92%e8%bf%9b%e5%ad%a6%e4%b9%a0">#&lt;/a>
&lt;/h5>
&lt;h3 id="业务开发">
 业务开发
 &lt;a class="anchor" href="#%e4%b8%9a%e5%8a%a1%e5%bc%80%e5%8f%91">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>
&lt;h5 id="-了解-kubernetesdocker-的作用">
 • 了解 Kubernetes、Docker 的作用
 &lt;a class="anchor" href="#-%e4%ba%86%e8%a7%a3-kubernetesdocker-%e7%9a%84%e4%bd%9c%e7%94%a8">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-掌握-dockerfileyamlchart-以及常用的-kubectl-命令">
 • 掌握 Dockerfile/Yaml/Chart 以及常用的 Kubectl 命令
 &lt;a class="anchor" href="#-%e6%8e%8c%e6%8f%a1-dockerfileyamlchart-%e4%bb%a5%e5%8f%8a%e5%b8%b8%e7%94%a8%e7%9a%84-kubectl-%e5%91%bd%e4%bb%a4">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-掌握-kubernetes-一些常用资源对象的作用和使用场景例如deploymentserviceconfigmap-等">
 • 掌握 Kubernetes 一些常用资源对象的作用和使用场景，例如：Deployment、Service、Configmap 等
 &lt;a class="anchor" href="#-%e6%8e%8c%e6%8f%a1-kubernetes-%e4%b8%80%e4%ba%9b%e5%b8%b8%e7%94%a8%e8%b5%84%e6%ba%90%e5%af%b9%e8%b1%a1%e7%9a%84%e4%bd%9c%e7%94%a8%e5%92%8c%e4%bd%bf%e7%94%a8%e5%9c%ba%e6%99%af%e4%be%8b%e5%a6%82deploymentserviceconfigmap-%e7%ad%89">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-知道一个服务怎样才能部署到-kubernetes-中">
 • 知道一个服务怎样才能部署到 Kubernetes 中
 &lt;a class="anchor" href="#-%e7%9f%a5%e9%81%93%e4%b8%80%e4%b8%aa%e6%9c%8d%e5%8a%a1%e6%80%8e%e6%a0%b7%e6%89%8d%e8%83%bd%e9%83%a8%e7%bd%b2%e5%88%b0-kubernetes-%e4%b8%ad">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h3 id="k8s-运维">
 K8S 运维
 &lt;a class="anchor" href="#k8s-%e8%bf%90%e7%bb%b4">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>
&lt;h5 id="-如何快速部署一个高可用-kubernetes-集群">
 • 如何快速部署一个高可用 Kubernetes 集群
 &lt;a class="anchor" href="#-%e5%a6%82%e4%bd%95%e5%bf%ab%e9%80%9f%e9%83%a8%e7%bd%b2%e4%b8%80%e4%b8%aa%e9%ab%98%e5%8f%af%e7%94%a8-kubernetes-%e9%9b%86%e7%be%a4">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-能够从全局的角度掌握-kubernetes-每个组件的作用和工作原理包括一些常用的-cricsicri">
 • 能够从全局的角度掌握 Kubernetes 每个组件的作用和工作原理，包括一些常用的 CRI、CSI、CRI
 &lt;a class="anchor" href="#-%e8%83%bd%e5%a4%9f%e4%bb%8e%e5%85%a8%e5%b1%80%e7%9a%84%e8%a7%92%e5%ba%a6%e6%8e%8c%e6%8f%a1-kubernetes-%e6%af%8f%e4%b8%aa%e7%bb%84%e4%bb%b6%e7%9a%84%e4%bd%9c%e7%94%a8%e5%92%8c%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86%e5%8c%85%e6%8b%ac%e4%b8%80%e4%ba%9b%e5%b8%b8%e7%94%a8%e7%9a%84-cricsicri">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-具备定位解决-kubernetes-的疑难杂症的能力">
 • 具备定位、解决 Kubernetes 的疑难杂症的能力
 &lt;a class="anchor" href="#-%e5%85%b7%e5%a4%87%e5%ae%9a%e4%bd%8d%e8%a7%a3%e5%86%b3-kubernetes-%e7%9a%84%e7%96%91%e9%9a%be%e6%9d%82%e7%97%87%e7%9a%84%e8%83%bd%e5%8a%9b">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-掌握-golang-语言可以看懂-kubernetes-源码这个对以后定位问题非常有帮助-可选">
 • 掌握 Golang 语言，可以看懂 Kubernetes 源码，这个对以后定位问题非常有帮助 (可选)
 &lt;a class="anchor" href="#-%e6%8e%8c%e6%8f%a1-golang-%e8%af%ad%e8%a8%80%e5%8f%af%e4%bb%a5%e7%9c%8b%e6%87%82-kubernetes-%e6%ba%90%e7%a0%81%e8%bf%99%e4%b8%aa%e5%af%b9%e4%bb%a5%e5%90%8e%e5%ae%9a%e4%bd%8d%e9%97%ae%e9%a2%98%e9%9d%9e%e5%b8%b8%e6%9c%89%e5%b8%ae%e5%8a%a9-%e5%8f%af%e9%80%89">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h3 id="k8s-开发">
 K8S 开发
 &lt;a class="anchor" href="#k8s-%e5%bc%80%e5%8f%91">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>
&lt;h5 id="-掌握-golang-语言具备阅读-kubernetes-源码的能力">
 • 掌握 Golang 语言，具备阅读 Kubernetes 源码的能力
 &lt;a class="anchor" href="#-%e6%8e%8c%e6%8f%a1-golang-%e8%af%ad%e8%a8%80%e5%85%b7%e5%a4%87%e9%98%85%e8%af%bb-kubernetes-%e6%ba%90%e7%a0%81%e7%9a%84%e8%83%bd%e5%8a%9b">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-掌握-kubernetes-常用-sdk-使用例如client-gocontroller-runtime-等">
 • 掌握 Kubernetes 常用 SDK 使用，例如：Client-go，Controller-runtime 等
 &lt;a class="anchor" href="#-%e6%8e%8c%e6%8f%a1-kubernetes-%e5%b8%b8%e7%94%a8-sdk-%e4%bd%bf%e7%94%a8%e4%be%8b%e5%a6%82client-gocontroller-runtime-%e7%ad%89">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-掌握-kubernetes-管理平台开发">
 • 掌握 Kubernetes 管理平台开发
 &lt;a class="anchor" href="#-%e6%8e%8c%e6%8f%a1-kubernetes-%e7%ae%a1%e7%90%86%e5%b9%b3%e5%8f%b0%e5%bc%80%e5%8f%91">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-掌握-operator-开发能够根据场景设计-crd-以及-kube-apiserver-聚合-api-开发">
 • 掌握 Operator 开发，能够根据场景设计 CRD 以及 Kube-apiserver 聚合 API 开发
 &lt;a class="anchor" href="#-%e6%8e%8c%e6%8f%a1-operator-%e5%bc%80%e5%8f%91%e8%83%bd%e5%a4%9f%e6%a0%b9%e6%8d%ae%e5%9c%ba%e6%99%af%e8%ae%be%e8%ae%a1-crd-%e4%bb%a5%e5%8f%8a-kube-apiserver-%e8%81%9a%e5%90%88-api-%e5%bc%80%e5%8f%91">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-掌握-admission-webhook-开发">
 • 掌握 admission Webhook 开发
 &lt;a class="anchor" href="#-%e6%8e%8c%e6%8f%a1-admission-webhook-%e5%bc%80%e5%8f%91">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-能够自定义-cnicsicrischeduler-frameworkdevice-plugin-等">
 • 能够自定义 CNI、CSI、CRI、Scheduler-framework、Device-plugin 等
 &lt;a class="anchor" href="#-%e8%83%bd%e5%a4%9f%e8%87%aa%e5%ae%9a%e4%b9%89-cnicsicrischeduler-frameworkdevice-plugin-%e7%ad%89">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-持续关注-kubernetes-社区与-cncf-社区等">
 • 持续关注 Kubernetes 社区与 CNCF 社区等
 &lt;a class="anchor" href="#-%e6%8c%81%e7%bb%ad%e5%85%b3%e6%b3%a8-kubernetes-%e7%a4%be%e5%8c%ba%e4%b8%8e-cncf-%e7%a4%be%e5%8c%ba%e7%ad%89">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h5 id="上面三种工作分类基本能够包含-kubernetes-开发技术所有内容内容由浅入深比如云原生开发肯定需要掌握业务开发k8s-运维那些技能">
 上面三种工作分类基本能够包含 Kubernetes 开发技术所有内容，内容由浅入深，比如云原生开发肯定需要掌握业务开发、K8S 运维那些技能。
 &lt;a class="anchor" href="#%e4%b8%8a%e9%9d%a2%e4%b8%89%e7%a7%8d%e5%b7%a5%e4%bd%9c%e5%88%86%e7%b1%bb%e5%9f%ba%e6%9c%ac%e8%83%bd%e5%a4%9f%e5%8c%85%e5%90%ab-kubernetes-%e5%bc%80%e5%8f%91%e6%8a%80%e6%9c%af%e6%89%80%e6%9c%89%e5%86%85%e5%ae%b9%e5%86%85%e5%ae%b9%e7%94%b1%e6%b5%85%e5%85%a5%e6%b7%b1%e6%af%94%e5%a6%82%e4%ba%91%e5%8e%9f%e7%94%9f%e5%bc%80%e5%8f%91%e8%82%af%e5%ae%9a%e9%9c%80%e8%a6%81%e6%8e%8c%e6%8f%a1%e4%b8%9a%e5%8a%a1%e5%bc%80%e5%8f%91k8s-%e8%bf%90%e7%bb%b4%e9%82%a3%e4%ba%9b%e6%8a%80%e8%83%bd">#&lt;/a>
&lt;/h5>
&lt;hr>
&lt;h2 id="k8s-开发技术栈">
 K8S 开发技术栈
 &lt;a class="anchor" href="#k8s-%e5%bc%80%e5%8f%91%e6%8a%80%e6%9c%af%e6%a0%88">#&lt;/a>
&lt;/h2>
&lt;p>下面列出了关于 Kubernetes 开发的技术栈路线图，仅供参考。&lt;/p></description></item><item><title>2024-04-03 K8S 探针原理</title><link>https://qq547475331.github.io/docs/k8s-%E6%8E%A2%E9%92%88%E5%8E%9F%E7%90%86-k8s-tan-zhen-yuan-li/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/k8s-%E6%8E%A2%E9%92%88%E5%8E%9F%E7%90%86-k8s-tan-zhen-yuan-li/</guid><description>&lt;h1 id="k8s-探针的背景">
 K8S 探针的背景
 &lt;a class="anchor" href="#k8s-%e6%8e%a2%e9%92%88%e7%9a%84%e8%83%8c%e6%99%af">#&lt;/a>
&lt;/h1>
&lt;h5 id="kubernetes-可以对业务进行故障自愈即针对运行异常的-pod-进行重启那么-k8s-是如何认定-pod-是否异常呢">
 Kubernetes 可以对业务进行故障自愈，即针对运行异常的 Pod 进行重启。那么 K8S 是如何认定 Pod 是否异常呢？
 &lt;a class="anchor" href="#kubernetes-%e5%8f%af%e4%bb%a5%e5%af%b9%e4%b8%9a%e5%8a%a1%e8%bf%9b%e8%a1%8c%e6%95%85%e9%9a%9c%e8%87%aa%e6%84%88%e5%8d%b3%e9%92%88%e5%af%b9%e8%bf%90%e8%a1%8c%e5%bc%82%e5%b8%b8%e7%9a%84-pod-%e8%bf%9b%e8%a1%8c%e9%87%8d%e5%90%af%e9%82%a3%e4%b9%88-k8s-%e6%98%af%e5%a6%82%e4%bd%95%e8%ae%a4%e5%ae%9a-pod-%e6%98%af%e5%90%a6%e5%bc%82%e5%b8%b8%e5%91%a2">#&lt;/a>
&lt;/h5>
&lt;h5 id="kubelet-组件根据-pod-中容器退出状态码判定-pod-是否异常然后重启-pod进而达到故障自愈的效果但是有些复杂场景这种判定-pod-异常的机制就无法满足了">
 Kubelet 组件根据 Pod 中容器退出状态码判定 Pod 是否异常，然后重启 Pod，进而达到故障自愈的效果。但是有些复杂场景，这种判定 Pod 异常的机制就无法满足了。
 &lt;a class="anchor" href="#kubelet-%e7%bb%84%e4%bb%b6%e6%a0%b9%e6%8d%ae-pod-%e4%b8%ad%e5%ae%b9%e5%99%a8%e9%80%80%e5%87%ba%e7%8a%b6%e6%80%81%e7%a0%81%e5%88%a4%e5%ae%9a-pod-%e6%98%af%e5%90%a6%e5%bc%82%e5%b8%b8%e7%84%b6%e5%90%8e%e9%87%8d%e5%90%af-pod%e8%bf%9b%e8%80%8c%e8%be%be%e5%88%b0%e6%95%85%e9%9a%9c%e8%87%aa%e6%84%88%e7%9a%84%e6%95%88%e6%9e%9c%e4%bd%86%e6%98%af%e6%9c%89%e4%ba%9b%e5%a4%8d%e6%9d%82%e5%9c%ba%e6%99%af%e8%bf%99%e7%a7%8d%e5%88%a4%e5%ae%9a-pod-%e5%bc%82%e5%b8%b8%e7%9a%84%e6%9c%ba%e5%88%b6%e5%b0%b1%e6%97%a0%e6%b3%95%e6%bb%a1%e8%b6%b3%e4%ba%86">#&lt;/a>
&lt;/h5>
&lt;h5 id="例如pod-中容器进程依然存在但是容器死锁了那么服务肯定是异常了但是这时候利用上述异常检测机制就无法认定-pod-异常了从而无法重启-pod">
 例如，Pod 中容器进程依然存在，但是容器死锁了，那么服务肯定是异常了，但是这时候利用上述异常检测机制就无法认定 Pod 异常了，从而无法重启 Pod。
 &lt;a class="anchor" href="#%e4%be%8b%e5%a6%82pod-%e4%b8%ad%e5%ae%b9%e5%99%a8%e8%bf%9b%e7%a8%8b%e4%be%9d%e7%84%b6%e5%ad%98%e5%9c%a8%e4%bd%86%e6%98%af%e5%ae%b9%e5%99%a8%e6%ad%bb%e9%94%81%e4%ba%86%e9%82%a3%e4%b9%88%e6%9c%8d%e5%8a%a1%e8%82%af%e5%ae%9a%e6%98%af%e5%bc%82%e5%b8%b8%e4%ba%86%e4%bd%86%e6%98%af%e8%bf%99%e6%97%b6%e5%80%99%e5%88%a9%e7%94%a8%e4%b8%8a%e8%bf%b0%e5%bc%82%e5%b8%b8%e6%a3%80%e6%b5%8b%e6%9c%ba%e5%88%b6%e5%b0%b1%e6%97%a0%e6%b3%95%e8%ae%a4%e5%ae%9a-pod-%e5%bc%82%e5%b8%b8%e4%ba%86%e4%bb%8e%e8%80%8c%e6%97%a0%e6%b3%95%e9%87%8d%e5%90%af-pod">#&lt;/a>
&lt;/h5>
&lt;h5 id="这时候就需要利用-k8s-中的探针检测机制了探针检测机制的意思是-kubelet-k8s-中有三种探针">
 这时候就需要利用 K8S 中的探针检测机制了，探针检测机制的意思是 Kubelet K8S 中有三种探针：
 &lt;a class="anchor" href="#%e8%bf%99%e6%97%b6%e5%80%99%e5%b0%b1%e9%9c%80%e8%a6%81%e5%88%a9%e7%94%a8-k8s-%e4%b8%ad%e7%9a%84%e6%8e%a2%e9%92%88%e6%a3%80%e6%b5%8b%e6%9c%ba%e5%88%b6%e4%ba%86%e6%8e%a2%e9%92%88%e6%a3%80%e6%b5%8b%e6%9c%ba%e5%88%b6%e7%9a%84%e6%84%8f%e6%80%9d%e6%98%af-kubelet-k8s-%e4%b8%ad%e6%9c%89%e4%b8%89%e7%a7%8d%e6%8e%a2%e9%92%88">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="-livenessprobe存活探针即探测容器是否运行存活">
 • &lt;strong>livenessProbe&lt;/strong>：存活探针，即探测容器是否运行、存活；
 &lt;a class="anchor" href="#-livenessprobe%e5%ad%98%e6%b4%bb%e6%8e%a2%e9%92%88%e5%8d%b3%e6%8e%a2%e6%b5%8b%e5%ae%b9%e5%99%a8%e6%98%af%e5%90%a6%e8%bf%90%e8%a1%8c%e5%ad%98%e6%b4%bb">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-readinessprobe就绪探针探测容器是否就绪是否能够正常提供服务了">
 • &lt;strong>readinessProbe&lt;/strong>：就绪探针，探测容器是否就绪，是否能够正常提供服务了；
 &lt;a class="anchor" href="#-readinessprobe%e5%b0%b1%e7%bb%aa%e6%8e%a2%e9%92%88%e6%8e%a2%e6%b5%8b%e5%ae%b9%e5%99%a8%e6%98%af%e5%90%a6%e5%b0%b1%e7%bb%aa%e6%98%af%e5%90%a6%e8%83%bd%e5%a4%9f%e6%ad%a3%e5%b8%b8%e6%8f%90%e4%be%9b%e6%9c%8d%e5%8a%a1%e4%ba%86">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-startupprobe启动探针探测容器是否启动">
 • &lt;strong>startupProbe&lt;/strong>：启动探针，探测容器是否启动。
 &lt;a class="anchor" href="#-startupprobe%e5%90%af%e5%8a%a8%e6%8e%a2%e9%92%88%e6%8e%a2%e6%b5%8b%e5%ae%b9%e5%99%a8%e6%98%af%e5%90%a6%e5%90%af%e5%8a%a8">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h5 id="下面针对以上三种探针展开说下每个探针的使用场景作用使用方式">
 下面针对以上三种探针展开说下每个探针的使用场景、作用、使用方式。
 &lt;a class="anchor" href="#%e4%b8%8b%e9%9d%a2%e9%92%88%e5%af%b9%e4%bb%a5%e4%b8%8a%e4%b8%89%e7%a7%8d%e6%8e%a2%e9%92%88%e5%b1%95%e5%bc%80%e8%af%b4%e4%b8%8b%e6%af%8f%e4%b8%aa%e6%8e%a2%e9%92%88%e7%9a%84%e4%bd%bf%e7%94%a8%e5%9c%ba%e6%99%af%e4%bd%9c%e7%94%a8%e4%bd%bf%e7%94%a8%e6%96%b9%e5%bc%8f">#&lt;/a>
&lt;/h5>
&lt;h1 id="探针原理">
 探针原理
 &lt;a class="anchor" href="#%e6%8e%a2%e9%92%88%e5%8e%9f%e7%90%86">#&lt;/a>
&lt;/h1>
&lt;h5 id="k8s-中探针的原理实际上就是利用业务服务自身提供的健康检查接口kubelet-根据策略去探测该接口">
 K8S 中探针的原理，实际上就是利用业务服务自身提供的健康检查接口，Kubelet 根据策略去探测该接口。
 &lt;a class="anchor" href="#k8s-%e4%b8%ad%e6%8e%a2%e9%92%88%e7%9a%84%e5%8e%9f%e7%90%86%e5%ae%9e%e9%99%85%e4%b8%8a%e5%b0%b1%e6%98%af%e5%88%a9%e7%94%a8%e4%b8%9a%e5%8a%a1%e6%9c%8d%e5%8a%a1%e8%87%aa%e8%ba%ab%e6%8f%90%e4%be%9b%e7%9a%84%e5%81%a5%e5%ba%b7%e6%a3%80%e6%9f%a5%e6%8e%a5%e5%8f%a3kubelet-%e6%a0%b9%e6%8d%ae%e7%ad%96%e7%95%a5%e5%8e%bb%e6%8e%a2%e6%b5%8b%e8%af%a5%e6%8e%a5%e5%8f%a3">#&lt;/a>
&lt;/h5>
&lt;h5 id="探针定义在-podspeccontainers-字段中例如下面是一个-livenessprobe-例子">
 探针定义在 &lt;code>pod.spec.containers&lt;/code> 字段中，例如下面是一个 &lt;strong>livenessProbe&lt;/strong> 例子：
 &lt;a class="anchor" href="#%e6%8e%a2%e9%92%88%e5%ae%9a%e4%b9%89%e5%9c%a8-podspeccontainers-%e5%ad%97%e6%ae%b5%e4%b8%ad%e4%be%8b%e5%a6%82%e4%b8%8b%e9%9d%a2%e6%98%af%e4%b8%80%e4%b8%aa-livenessprobe-%e4%be%8b%e5%ad%90">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>apiVersion: v1
kind: Pod
metadata:
 labels:
 test: liveness
 name: liveness-exec
spec:
 containers:
 - name: liveness
 image: registry.k8s.io/busybox
 args:
 - /bin/sh
 - -c
 - touch /tmp/healthy; sleep 30; rm -f /tmp/healthy; sleep 600
 livenessProbe:
 exec:
 command:
 - cat
 - /tmp/healthy
 initialDelaySeconds: 5
 periodSeconds: 5
&lt;/code>&lt;/pre>&lt;h5 id="在这个配置文件中可以看到-pod-中只有一个-container-periodseconds-字段指定了-kubelet-应该每-5-秒执行一次存活探测-initialdelayseconds-字段告诉-kubelet-在执行第一次探测前应该等待-5-秒kubelet-在容器内执行命令-cat-tmphealthy-来进行探测如果命令执行成功并且返回值为-0kubelet-就会认为这个容器是健康存活的如果这个命令返回非-0-值kubelet-会根据-pod-restartpolicy-决定是否杀死这个容器并重新启动它">
 在这个配置文件中，可以看到 Pod 中只有一个 &lt;code>Container&lt;/code>。 &lt;code>periodSeconds&lt;/code> 字段指定了 Kubelet 应该每 5 秒执行一次存活探测。 &lt;code>initialDelaySeconds&lt;/code> 字段告诉 Kubelet 在执行第一次探测前应该等待 5 秒。Kubelet 在容器内执行命令 &lt;code>cat /tmp/healthy&lt;/code> 来进行探测。如果命令执行成功并且返回值为 0，Kubelet 就会认为这个容器是健康存活的。如果这个命令返回非 0 值，Kubelet 会根据 pod &lt;code>restartPolicy&lt;/code> 决定是否杀死这个容器并重新启动它。
 &lt;a class="anchor" href="#%e5%9c%a8%e8%bf%99%e4%b8%aa%e9%85%8d%e7%bd%ae%e6%96%87%e4%bb%b6%e4%b8%ad%e5%8f%af%e4%bb%a5%e7%9c%8b%e5%88%b0-pod-%e4%b8%ad%e5%8f%aa%e6%9c%89%e4%b8%80%e4%b8%aa-container-periodseconds-%e5%ad%97%e6%ae%b5%e6%8c%87%e5%ae%9a%e4%ba%86-kubelet-%e5%ba%94%e8%af%a5%e6%af%8f-5-%e7%a7%92%e6%89%a7%e8%a1%8c%e4%b8%80%e6%ac%a1%e5%ad%98%e6%b4%bb%e6%8e%a2%e6%b5%8b-initialdelayseconds-%e5%ad%97%e6%ae%b5%e5%91%8a%e8%af%89-kubelet-%e5%9c%a8%e6%89%a7%e8%a1%8c%e7%ac%ac%e4%b8%80%e6%ac%a1%e6%8e%a2%e6%b5%8b%e5%89%8d%e5%ba%94%e8%af%a5%e7%ad%89%e5%be%85-5-%e7%a7%92kubelet-%e5%9c%a8%e5%ae%b9%e5%99%a8%e5%86%85%e6%89%a7%e8%a1%8c%e5%91%bd%e4%bb%a4-cat-tmphealthy-%e6%9d%a5%e8%bf%9b%e8%a1%8c%e6%8e%a2%e6%b5%8b%e5%a6%82%e6%9e%9c%e5%91%bd%e4%bb%a4%e6%89%a7%e8%a1%8c%e6%88%90%e5%8a%9f%e5%b9%b6%e4%b8%94%e8%bf%94%e5%9b%9e%e5%80%bc%e4%b8%ba-0kubelet-%e5%b0%b1%e4%bc%9a%e8%ae%a4%e4%b8%ba%e8%bf%99%e4%b8%aa%e5%ae%b9%e5%99%a8%e6%98%af%e5%81%a5%e5%ba%b7%e5%ad%98%e6%b4%bb%e7%9a%84%e5%a6%82%e6%9e%9c%e8%bf%99%e4%b8%aa%e5%91%bd%e4%bb%a4%e8%bf%94%e5%9b%9e%e9%9d%9e-0-%e5%80%bckubelet-%e4%bc%9a%e6%a0%b9%e6%8d%ae-pod-restartpolicy-%e5%86%b3%e5%ae%9a%e6%98%af%e5%90%a6%e6%9d%80%e6%ad%bb%e8%bf%99%e4%b8%aa%e5%ae%b9%e5%99%a8%e5%b9%b6%e9%87%8d%e6%96%b0%e5%90%af%e5%8a%a8%e5%ae%83">#&lt;/a>
&lt;/h5>
&lt;h3 id="restartpolicy">
 restartPolicy
 &lt;a class="anchor" href="#restartpolicy">#&lt;/a>
&lt;/h3>
&lt;h5 id="kubelet-在知道容器异常后是根据-restartpolicy-字段来决定如何操作">
 Kubelet 在知道容器异常后，是根据 &lt;code>restartPolicy&lt;/code> 字段来决定如何操作。
 &lt;a class="anchor" href="#kubelet-%e5%9c%a8%e7%9f%a5%e9%81%93%e5%ae%b9%e5%99%a8%e5%bc%82%e5%b8%b8%e5%90%8e%e6%98%af%e6%a0%b9%e6%8d%ae-restartpolicy-%e5%ad%97%e6%ae%b5%e6%9d%a5%e5%86%b3%e5%ae%9a%e5%a6%82%e4%bd%95%e6%93%8d%e4%bd%9c">#&lt;/a>
&lt;/h5>
&lt;h5 id="在-pod-的-spec-中有一个-restartpolicy-字段如下">
 在 Pod 的 &lt;code>spec&lt;/code> 中有一个 &lt;code>restartPolicy&lt;/code> 字段，如下：
 &lt;a class="anchor" href="#%e5%9c%a8-pod-%e7%9a%84-spec-%e4%b8%ad%e6%9c%89%e4%b8%80%e4%b8%aa-restartpolicy-%e5%ad%97%e6%ae%b5%e5%a6%82%e4%b8%8b">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>apiVersion: v1
kind: Pod
metadata:
 name: xxx
spec:
 restartPolicy: Always
 ...
&lt;/code>&lt;/pre>&lt;p>&lt;strong>restartPolicy&lt;/strong> 的值有三个：&lt;/p></description></item><item><title>2024-04-03 K8S原地升级</title><link>https://qq547475331.github.io/docs/k8s%E5%8E%9F%E5%9C%B0%E5%8D%87%E7%BA%A7-k8s-yuan-de-sheng-ji/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/k8s%E5%8E%9F%E5%9C%B0%E5%8D%87%E7%BA%A7-k8s-yuan-de-sheng-ji/</guid><description>&lt;p>&lt;strong>原地升级&lt;/strong>一词中，**“升级”**不难理解，是将应用实例的版本由旧版替换为新版。那么如何结合 Kubernetes 环境来理解“原地”呢？&lt;/p>
&lt;h5 id="我们先来看看-k8s-原生-workload-的发布方式这里假设我们需要部署一个应用包括-foobar-两个容器在-pod-中其中foo-容器第一次部署时用的镜像版本是-v1我们需要将其升级为-v2-版本镜像该怎么做呢">
 我们先来看看 K8s 原生 workload 的发布方式。这里假设我们需要部署一个应用，包括 foo、bar 两个容器在 Pod 中。其中，foo 容器第一次部署时用的镜像版本是 v1，我们需要将其升级为 v2 版本镜像，该怎么做呢？
 &lt;a class="anchor" href="#%e6%88%91%e4%bb%ac%e5%85%88%e6%9d%a5%e7%9c%8b%e7%9c%8b-k8s-%e5%8e%9f%e7%94%9f-workload-%e7%9a%84%e5%8f%91%e5%b8%83%e6%96%b9%e5%bc%8f%e8%bf%99%e9%87%8c%e5%81%87%e8%ae%be%e6%88%91%e4%bb%ac%e9%9c%80%e8%a6%81%e9%83%a8%e7%bd%b2%e4%b8%80%e4%b8%aa%e5%ba%94%e7%94%a8%e5%8c%85%e6%8b%ac-foobar-%e4%b8%a4%e4%b8%aa%e5%ae%b9%e5%99%a8%e5%9c%a8-pod-%e4%b8%ad%e5%85%b6%e4%b8%adfoo-%e5%ae%b9%e5%99%a8%e7%ac%ac%e4%b8%80%e6%ac%a1%e9%83%a8%e7%bd%b2%e6%97%b6%e7%94%a8%e7%9a%84%e9%95%9c%e5%83%8f%e7%89%88%e6%9c%ac%e6%98%af-v1%e6%88%91%e4%bb%ac%e9%9c%80%e8%a6%81%e5%b0%86%e5%85%b6%e5%8d%87%e7%ba%a7%e4%b8%ba-v2-%e7%89%88%e6%9c%ac%e9%95%9c%e5%83%8f%e8%af%a5%e6%80%8e%e4%b9%88%e5%81%9a%e5%91%a2">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="如果这个应用使用-deployment-部署那么升级过程中-deployment-会触发新版本-replicaset-创建-pod并删除旧版本-pod如下图所示">
 如果这个应用使用 Deployment 部署，那么升级过程中 Deployment 会触发新版本 ReplicaSet 创建 Pod，并删除旧版本 Pod。如下图所示：
 &lt;a class="anchor" href="#%e5%a6%82%e6%9e%9c%e8%bf%99%e4%b8%aa%e5%ba%94%e7%94%a8%e4%bd%bf%e7%94%a8-deployment-%e9%83%a8%e7%bd%b2%e9%82%a3%e4%b9%88%e5%8d%87%e7%ba%a7%e8%bf%87%e7%a8%8b%e4%b8%ad-deployment-%e4%bc%9a%e8%a7%a6%e5%8f%91%e6%96%b0%e7%89%88%e6%9c%ac-replicaset-%e5%88%9b%e5%bb%ba-pod%e5%b9%b6%e5%88%a0%e9%99%a4%e6%97%a7%e7%89%88%e6%9c%ac-pod%e5%a6%82%e4%b8%8b%e5%9b%be%e6%89%80%e7%a4%ba">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403011759919.png" alt="image-20240301175921848" />&lt;/p>
&lt;h5 id="在本次升级过程中原-pod-对象被删除一个新-pod-对象被创建新-pod-被调度到另一个-node-上分配到一个新的-ip并把-foobar-两个容器在这个-node-上重新拉取镜像启动容器">
 在本次升级过程中，原 Pod 对象被删除，一个新 Pod 对象被创建。新 Pod 被调度到另一个 Node 上，分配到一个新的 IP，并把 foo、bar 两个容器在这个 Node 上重新拉取镜像、启动容器。
 &lt;a class="anchor" href="#%e5%9c%a8%e6%9c%ac%e6%ac%a1%e5%8d%87%e7%ba%a7%e8%bf%87%e7%a8%8b%e4%b8%ad%e5%8e%9f-pod-%e5%af%b9%e8%b1%a1%e8%a2%ab%e5%88%a0%e9%99%a4%e4%b8%80%e4%b8%aa%e6%96%b0-pod-%e5%af%b9%e8%b1%a1%e8%a2%ab%e5%88%9b%e5%bb%ba%e6%96%b0-pod-%e8%a2%ab%e8%b0%83%e5%ba%a6%e5%88%b0%e5%8f%a6%e4%b8%80%e4%b8%aa-node-%e4%b8%8a%e5%88%86%e9%85%8d%e5%88%b0%e4%b8%80%e4%b8%aa%e6%96%b0%e7%9a%84-ip%e5%b9%b6%e6%8a%8a-foobar-%e4%b8%a4%e4%b8%aa%e5%ae%b9%e5%99%a8%e5%9c%a8%e8%bf%99%e4%b8%aa-node-%e4%b8%8a%e9%87%8d%e6%96%b0%e6%8b%89%e5%8f%96%e9%95%9c%e5%83%8f%e5%90%af%e5%8a%a8%e5%ae%b9%e5%99%a8">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="如果这个应该使用-statefulset-部署那么升级过程中-statefulset-会先删除旧-pod-对象等删除完成后用同样的名字在创建一个新的-pod-对象如下图所示">
 如果这个应该使用 StatefulSet 部署，那么升级过程中 StatefulSet 会先删除旧 Pod 对象，等删除完成后用同样的名字在创建一个新的 Pod 对象。如下图所示：
 &lt;a class="anchor" href="#%e5%a6%82%e6%9e%9c%e8%bf%99%e4%b8%aa%e5%ba%94%e8%af%a5%e4%bd%bf%e7%94%a8-statefulset-%e9%83%a8%e7%bd%b2%e9%82%a3%e4%b9%88%e5%8d%87%e7%ba%a7%e8%bf%87%e7%a8%8b%e4%b8%ad-statefulset-%e4%bc%9a%e5%85%88%e5%88%a0%e9%99%a4%e6%97%a7-pod-%e5%af%b9%e8%b1%a1%e7%ad%89%e5%88%a0%e9%99%a4%e5%ae%8c%e6%88%90%e5%90%8e%e7%94%a8%e5%90%8c%e6%a0%b7%e7%9a%84%e5%90%8d%e5%ad%97%e5%9c%a8%e5%88%9b%e5%bb%ba%e4%b8%80%e4%b8%aa%e6%96%b0%e7%9a%84-pod-%e5%af%b9%e8%b1%a1%e5%a6%82%e4%b8%8b%e5%9b%be%e6%89%80%e7%a4%ba">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403011759434.png" alt="image-20240301175945368" />&lt;/p>
&lt;h5 id="值得注意的是尽管新旧两个-pod-名字都叫-pod-0但其实是两个完全不同的-pod-对象uid也变了statefulset-等到原先的-pod-0-对象完全从-kubernetes-集群中被删除后才会提交创建一个新的-pod-0-对象而这个新的-pod-也会被重新调度分配ip拉镜像启动容器">
 值得注意的是，尽管新旧两个 Pod 名字都叫 pod-0，但其实是两个完全不同的 Pod 对象（uid也变了）。StatefulSet 等到原先的 pod-0 对象完全从 Kubernetes 集群中被删除后，才会提交创建一个新的 pod-0 对象。而这个新的 Pod 也会被重新调度、分配IP、拉镜像、启动容器。
 &lt;a class="anchor" href="#%e5%80%bc%e5%be%97%e6%b3%a8%e6%84%8f%e7%9a%84%e6%98%af%e5%b0%bd%e7%ae%a1%e6%96%b0%e6%97%a7%e4%b8%a4%e4%b8%aa-pod-%e5%90%8d%e5%ad%97%e9%83%bd%e5%8f%ab-pod-0%e4%bd%86%e5%85%b6%e5%ae%9e%e6%98%af%e4%b8%a4%e4%b8%aa%e5%ae%8c%e5%85%a8%e4%b8%8d%e5%90%8c%e7%9a%84-pod-%e5%af%b9%e8%b1%a1uid%e4%b9%9f%e5%8f%98%e4%ba%86statefulset-%e7%ad%89%e5%88%b0%e5%8e%9f%e5%85%88%e7%9a%84-pod-0-%e5%af%b9%e8%b1%a1%e5%ae%8c%e5%85%a8%e4%bb%8e-kubernetes-%e9%9b%86%e7%be%a4%e4%b8%ad%e8%a2%ab%e5%88%a0%e9%99%a4%e5%90%8e%e6%89%8d%e4%bc%9a%e6%8f%90%e4%ba%a4%e5%88%9b%e5%bb%ba%e4%b8%80%e4%b8%aa%e6%96%b0%e7%9a%84-pod-0-%e5%af%b9%e8%b1%a1%e8%80%8c%e8%bf%99%e4%b8%aa%e6%96%b0%e7%9a%84-pod-%e4%b9%9f%e4%bc%9a%e8%a2%ab%e9%87%8d%e6%96%b0%e8%b0%83%e5%ba%a6%e5%88%86%e9%85%8dip%e6%8b%89%e9%95%9c%e5%83%8f%e5%90%af%e5%8a%a8%e5%ae%b9%e5%99%a8">#&lt;/a>
&lt;/h5>
&lt;h5 id="-而所谓原地升级模式就是在应用升级过程中避免将整个-pod-对象删除新建而是基于原有的-pod-对象升级其中某一个或多个容器的镜像版本">
 -而所谓原地升级模式，就是在应用升级过程中避免将整个 Pod 对象删除、新建，而是基于原有的 Pod 对象升级其中某一个或多个容器的镜像版本：
 &lt;a class="anchor" href="#-%e8%80%8c%e6%89%80%e8%b0%93%e5%8e%9f%e5%9c%b0%e5%8d%87%e7%ba%a7%e6%a8%a1%e5%bc%8f%e5%b0%b1%e6%98%af%e5%9c%a8%e5%ba%94%e7%94%a8%e5%8d%87%e7%ba%a7%e8%bf%87%e7%a8%8b%e4%b8%ad%e9%81%bf%e5%85%8d%e5%b0%86%e6%95%b4%e4%b8%aa-pod-%e5%af%b9%e8%b1%a1%e5%88%a0%e9%99%a4%e6%96%b0%e5%bb%ba%e8%80%8c%e6%98%af%e5%9f%ba%e4%ba%8e%e5%8e%9f%e6%9c%89%e7%9a%84-pod-%e5%af%b9%e8%b1%a1%e5%8d%87%e7%ba%a7%e5%85%b6%e4%b8%ad%e6%9f%90%e4%b8%80%e4%b8%aa%e6%88%96%e5%a4%9a%e4%b8%aa%e5%ae%b9%e5%99%a8%e7%9a%84%e9%95%9c%e5%83%8f%e7%89%88%e6%9c%ac">#&lt;/a>
&lt;/h5>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403011800698.png" alt="image-20240301180014646" />&lt;/p></description></item><item><title>2024-04-03 K8S命令指南</title><link>https://qq547475331.github.io/docs/k8s%E5%91%BD%E4%BB%A4%E6%8C%87%E5%8D%97-k8s-ming-ling-zhi-nan/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/k8s%E5%91%BD%E4%BB%A4%E6%8C%87%E5%8D%97-k8s-ming-ling-zhi-nan/</guid><description>&lt;h1 id="一基础命令">
 一、基础命令
 &lt;a class="anchor" href="#%e4%b8%80%e5%9f%ba%e7%a1%80%e5%91%bd%e4%bb%a4">#&lt;/a>
&lt;/h1>
&lt;p>在Kubernetes中，基础命令用于日常的查询和基本操作。以下表格展示了这些基础命令，它们的说明，以及相应的使用举例。&lt;/p>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th style="text-align: left">命令&lt;/th>
 &lt;th style="text-align: left">说明&lt;/th>
 &lt;th style="text-align: left">举例&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td style="text-align: left">&lt;code>kubectl version&lt;/code>&lt;/td>
 &lt;td style="text-align: left">显示客户端和服务器的Kubernetes版本。&lt;/td>
 &lt;td style="text-align: left">&lt;code>kubectl version&lt;/code>&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">&lt;code>kubectl api-versions&lt;/code>&lt;/td>
 &lt;td style="text-align: left">列出可用的API版本。&lt;/td>
 &lt;td style="text-align: left">&lt;code>kubectl api-versions&lt;/code>&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">&lt;code>kubectl get&lt;/code>&lt;/td>
 &lt;td style="text-align: left">列出一个或多个资源。&lt;/td>
 &lt;td style="text-align: left">&lt;code>kubectl get pods&lt;/code>&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">&lt;code>kubectl describe&lt;/code>&lt;/td>
 &lt;td style="text-align: left">显示一个或多个资源的详细信息。&lt;/td>
 &lt;td style="text-align: left">&lt;code>kubectl describe nodes my-node&lt;/code>&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">&lt;code>kubectl logs&lt;/code>&lt;/td>
 &lt;td style="text-align: left">打印容器的日志。&lt;/td>
 &lt;td style="text-align: left">&lt;code>kubectl logs my-pod&lt;/code>&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">&lt;code>kubectl attach&lt;/code>&lt;/td>
 &lt;td style="text-align: left">附加到正在运行的容器进行交互。&lt;/td>
 &lt;td style="text-align: left">&lt;code>kubectl attach my-pod -i&lt;/code>&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">&lt;code>kubectl exec&lt;/code>&lt;/td>
 &lt;td style="text-align: left">在容器内执行命令。&lt;/td>
 &lt;td style="text-align: left">&lt;code>kubectl exec my-pod -- ls /&lt;/code>&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">&lt;code>kubectl port-forward&lt;/code>&lt;/td>
 &lt;td style="text-align: left">为Pod中的容器端口转发。&lt;/td>
 &lt;td style="text-align: left">&lt;code>kubectl port-forward my-pod 5000:6000&lt;/code>&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">&lt;code>kubectl proxy&lt;/code>&lt;/td>
 &lt;td style="text-align: left">运行一个代理到Kubernetes API服务器。&lt;/td>
 &lt;td style="text-align: left">&lt;code>kubectl proxy&lt;/code>&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">&lt;code>kubectl cp&lt;/code>&lt;/td>
 &lt;td style="text-align: left">在容器和本地文件系统之间复制文件/目录。&lt;/td>
 &lt;td style="text-align: left">&lt;code>kubectl cp /tmp/foo_dir my-pod:/tmp/bar_dir&lt;/code>&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">&lt;code>kubectl run&lt;/code>&lt;/td>
 &lt;td style="text-align: left">在集群中快速启动一个指定的镜像。&lt;/td>
 &lt;td style="text-align: left">&lt;code>kubectl run nginx --image=nginx&lt;/code>&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">&lt;code>kubectl expose&lt;/code>&lt;/td>
 &lt;td style="text-align: left">将Pod或其他资源类型暴露为Kubernetes服务。&lt;/td>
 &lt;td style="text-align: left">&lt;code>kubectl expose deployment nginx --port=80&lt;/code>&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;p>这些命令为Kubernetes用户提供了强大的工具集，用于管理和调试在Kubernetes集群中运行的应用。&lt;/p></description></item><item><title>2024-04-03 k8s应用的最佳实践</title><link>https://qq547475331.github.io/docs/k8s%E5%BA%94%E7%94%A8%E7%9A%84%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5-k8s-ying-yong-de-zui-jia-shi-jian/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/k8s%E5%BA%94%E7%94%A8%E7%9A%84%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5-k8s-ying-yong-de-zui-jia-shi-jian/</guid><description>&lt;h1 id="一文带你检查kubernetes应用是否为最佳实践">
 一文带你检查Kubernetes应用是否为最佳实践
 &lt;a class="anchor" href="#%e4%b8%80%e6%96%87%e5%b8%a6%e4%bd%a0%e6%a3%80%e6%9f%a5kubernetes%e5%ba%94%e7%94%a8%e6%98%af%e5%90%a6%e4%b8%ba%e6%9c%80%e4%bd%b3%e5%ae%9e%e8%b7%b5">#&lt;/a>
&lt;/h1>
&lt;blockquote>
&lt;p>一篇从应用部署/服务管治/集群配置三个方便来check你的K8S使用姿势是否正确,包含单不限于服务监控检查/资源使用/标签/HPA，VPA/安全策略/RBAC/日志/监控是否为最佳实践的check list。&lt;/p>&lt;/blockquote>
&lt;h1 id="一-应用部署">
 一 应用部署
 &lt;a class="anchor" href="#%e4%b8%80-%e5%ba%94%e7%94%a8%e9%83%a8%e7%bd%b2">#&lt;/a>
&lt;/h1>
&lt;h2 id="11-健康检查">
 1.1 健康检查
 &lt;a class="anchor" href="#11-%e5%81%a5%e5%ba%b7%e6%a3%80%e6%9f%a5">#&lt;/a>
&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>readiness probe&lt;/strong>确定容器何时可以接收流量。&lt;/li>
&lt;/ul>
&lt;p>Kubelet执行检查并确定应用程序是否可以接收流量。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>liveness probe&lt;/strong>确定何时应重新启动容器。&lt;/li>
&lt;/ul>
&lt;p>kubelet执行检查并确定是否应重新启动容器。&lt;/p>
&lt;h3 id="111-容器就绪性探针">
 1.1.1 容器就绪性探针
 &lt;a class="anchor" href="#111-%e5%ae%b9%e5%99%a8%e5%b0%b1%e7%bb%aa%e6%80%a7%e6%8e%a2%e9%92%88">#&lt;/a>
&lt;/h3>
&lt;p>就绪性和存活性探针没有默认值，如果您未设置就绪探针，则kubelet会假定该应用程序已准备就绪，可以在容器启动后立即接收流量。&lt;/p>
&lt;h3 id="112-发生致命错误时容器崩溃">
 1.1.2 发生致命错误时容器崩溃
 &lt;a class="anchor" href="#112-%e5%8f%91%e7%94%9f%e8%87%b4%e5%91%bd%e9%94%99%e8%af%af%e6%97%b6%e5%ae%b9%e5%99%a8%e5%b4%a9%e6%ba%83">#&lt;/a>
&lt;/h3>
&lt;p>如果应用程序遇到不可恢复的错误，则应使其崩溃，例如：&lt;/p>
&lt;ul>
&lt;li>未捕获的异常&lt;/li>
&lt;li>代码中的错字（用于动态语言）&lt;/li>
&lt;li>无法加载标头或依赖项&lt;/li>
&lt;/ul>
&lt;p>请注意，您不应发信号通知Liveness探针失败，相反，您应该立即退出该过程，并让kubelet重新启动容器。&lt;/p>
&lt;h3 id="113-配置被动的存活性探针">
 1.1.3 配置被动的存活性探针
 &lt;a class="anchor" href="#113-%e9%85%8d%e7%bd%ae%e8%a2%ab%e5%8a%a8%e7%9a%84%e5%ad%98%e6%b4%bb%e6%80%a7%e6%8e%a2%e9%92%88">#&lt;/a>
&lt;/h3>
&lt;p>Liveness探针旨在在卡住容器时重新启动容器，例如：&lt;/p>
&lt;p>如果您的应用程序正在处理无限循环，则无法退出或寻求帮助。&lt;/p>
&lt;p>当该进程消耗100％的CPU时，将没有时间回复（其他）Readiness探针检查，并且最终将其从服务中删除。但是，该Pod仍被注册为当前Deployment的活动副本。如果没有Liveness探针，它将保持运行状态，但与服务分离。换句话说，该过程不仅不处理任何请求，而且还消耗资源。&lt;/p>
&lt;p>此时应该怎么办：&lt;/p>
&lt;ul>
&lt;li>从您的应用程序公开端点&lt;/li>
&lt;li>端点总是回复成功响应&lt;/li>
&lt;li>使用“活力”探针获取端点&lt;/li>
&lt;/ul>
&lt;p>请注意，您不应该使用Liveness探针来处理应用程序中的致命错误，并要求Kubernetes重新启动应用程序。相反，您应该让应用程序崩溃。&lt;/p>
&lt;p>仅在过程无响应的情况下，才应将“活动性”探针用作恢复机制。&lt;/p>
&lt;h3 id="114-存活性探针与就绪性探针的区别">
 1.1.4 存活性探针与就绪性探针的区别
 &lt;a class="anchor" href="#114-%e5%ad%98%e6%b4%bb%e6%80%a7%e6%8e%a2%e9%92%88%e4%b8%8e%e5%b0%b1%e7%bb%aa%e6%80%a7%e6%8e%a2%e9%92%88%e7%9a%84%e5%8c%ba%e5%88%ab">#&lt;/a>
&lt;/h3>
&lt;p>当“活力”和“就绪”探针指向相同的端点时，探针的作用会合并在一起。&lt;/p>
&lt;p>当应用程序发出信号表明尚未准备就绪或尚待运行时，kubelet会将容器与服务分离并同时将其删除。&lt;/p>
&lt;p>您可能会注意到连接断开，因为容器没有足够的时间耗尽当前连接或处理传入的连接。&lt;/p>
&lt;p>可以参考：article that discussed graceful shutdown.&lt;/p>
&lt;h2 id="12-应用的独立性">
 1.2 应用的独立性
 &lt;a class="anchor" href="#12-%e5%ba%94%e7%94%a8%e7%9a%84%e7%8b%ac%e7%ab%8b%e6%80%a7">#&lt;/a>
&lt;/h2>
&lt;p>如果应用程序连接到数据库，也许你认为如果数据库为就绪就返回一个失败的就绪就ok了，但是事实并非如此，例如您有一个依赖于后端API的前端应用程序。如果该API不稳定（例如由于错误而有时不可用），则就绪探测器将失败，并且前端应用程序中的相关就绪也将失败。这就会导致停机时间。更一般而言，下游依赖项的故障可能会传播到上游的所有应用程序，并最终也降低前端面层&lt;/p>
&lt;h3 id="121-就绪性探针应该独立">
 1.2.1 就绪性探针应该独立
 &lt;a class="anchor" href="#121-%e5%b0%b1%e7%bb%aa%e6%80%a7%e6%8e%a2%e9%92%88%e5%ba%94%e8%af%a5%e7%8b%ac%e7%ab%8b">#&lt;/a>
&lt;/h3>
&lt;p>就绪性探针不应该依以下服务：&lt;/p>
&lt;ul>
&lt;li>数据库&lt;/li>
&lt;li>数据库迁移&lt;/li>
&lt;li>APIs&lt;/li>
&lt;li>第三方服务&lt;/li>
&lt;/ul>
&lt;p>详细可参考：explore what happens when there’re dependencies in the readiness probes in this essay.&lt;/p></description></item><item><title>2024-04-03 K8S的POD类型</title><link>https://qq547475331.github.io/docs/k8s%E7%9A%84pod%E7%B1%BB%E5%9E%8B-k8s-de-pod-lei-xing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/k8s%E7%9A%84pod%E7%B1%BB%E5%9E%8B-k8s-de-pod-lei-xing/</guid><description>&lt;p>在 Kubernetes 中，containers被部署和管理在 Pod 中。Pod 是 Kubernetes 对象模型中最小和最简单的单元，可以被创建、部署和管理。在这里，您可以在单个 pod 中使用不同的容器类型来实现特定的功能。以下是在 Kubernetes 中常用的一些容器类型：&lt;/p>
&lt;ul>
&lt;li>Init Container: 初始化容器&lt;/li>
&lt;li>Sidecar Container: 边车容器&lt;/li>
&lt;li>Ephemeral Container: 临时容器&lt;/li>
&lt;li>Multi Container: 多容器&lt;/li>
&lt;/ul>
&lt;h2 id="init-container">
 Init Container
 &lt;a class="anchor" href="#init-container">#&lt;/a>
&lt;/h2>
&lt;p>一个Pod 可以在其中运行多个容器来运行应用程序，但它也可以有一个或多个 init 容器，在应用程序容器启动之前运行。Init 容器旨在在主应用程序容器启动之前运行初始化任务。它们可用于设置配置文件、初始化数据库或等待外部服务准备就绪等任务。Init 容器与常规容器完全相同，只是：&lt;/p>
&lt;ul>
&lt;li>初始化容器始终运行至完成。&lt;/li>
&lt;li>每个初始化容器必须在下一个初始化容器开始之前成功完成。&lt;/li>
&lt;/ul>
&lt;h3 id="案例">
 案例
 &lt;a class="anchor" href="#%e6%a1%88%e4%be%8b">#&lt;/a>
&lt;/h3>
&lt;pre tabindex="0">&lt;code>apiVersion: v1
kind: Pod
metadata:
 name: nginx-init
 labels:
 app: nginx
spec:
 initContainers:
 #Initializing container 
 - name: init-container
 image: alpine
 command: [&amp;#39;sh&amp;#39;, &amp;#39;-c&amp;#39;, &amp;#39;echo &amp;#34;&amp;lt;h1&amp;gt;This is from INIT container&amp;lt;/h1&amp;gt;&amp;#34; &amp;gt;&amp;gt; /usr/share/nginx/html/index.html&amp;#39;]
 volumeMounts:
 - name: data
 mountPath: /usr/share/nginx/html
 containers:
 # application container i.e., main container
 - name: app
 image: nginx
 volumeMounts:
 - name: data
 mountPath: /usr/share/nginx/html
 volumes:
 - name: data
 emptyDir: {}
&lt;/code>&lt;/pre>&lt;p>这里的 init-container 将使用数据卷的data 覆盖 nginx 主页的 index.html。&lt;/p></description></item><item><title>2024-04-03 K8S调试POD</title><link>https://qq547475331.github.io/docs/k8s%E8%B0%83%E8%AF%95pod-k8s-diao-shi-pod/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/k8s%E8%B0%83%E8%AF%95pod-k8s-diao-shi-pod/</guid><description>&lt;h5 id="曾几何时我们将自己的应用运行在kubernetes上每当出现容器异常崩溃时我们往往都是一边重启容器一边面对崩溃的容器无从下手通常在业务研发自己build的镜像内包含了shell我们还能通过在command中嵌入一个sleep-3600命令来阻塞容器内服务启动不过也有时候会出现不知道从哪里冒出来一个distroless镜像这时可能最先崩溃的就是运维了那是一种运维这个职业自诞生以来第一次感受到手足无措并脱离掌控的无助感于是在k8s环境下无法debug容器的梗开始在坊间广为吐槽">
 曾几何时，我们将自己的应用运行在Kubernetes上，每当出现&lt;a href="https://cloud.tencent.com/product/tke?from_column=20065&amp;amp;from=20065">容器&lt;/a>异常崩溃时，我们往往都是一边重启容器，一边面对崩溃的容器无从下手。通常在业务研发自己build的镜像内包含了shell，我们还能通过在command中嵌入一个&lt;code>[&amp;quot;sleep&amp;quot;, &amp;quot;3600&amp;quot;]&lt;/code>命令来阻塞容器内服务启动，不过也有时候会出现不知道从哪里冒出来一个&lt;code>distroless&lt;/code>镜像，这时可能最先崩溃的就是&lt;a href="https://cloud.tencent.com/solution/operation?from_column=20065&amp;amp;from=20065">运维&lt;/a>了。&lt;code>那是一种运维这个职业自诞生以来，第一次感受到手足无措并脱离掌控的无助感&lt;/code>。于是在k8s环境下无法debug容器的梗开始在坊间广为吐槽。
 &lt;a class="anchor" href="#%e6%9b%be%e5%87%a0%e4%bd%95%e6%97%b6%e6%88%91%e4%bb%ac%e5%b0%86%e8%87%aa%e5%b7%b1%e7%9a%84%e5%ba%94%e7%94%a8%e8%bf%90%e8%a1%8c%e5%9c%a8kubernetes%e4%b8%8a%e6%af%8f%e5%bd%93%e5%87%ba%e7%8e%b0%e5%ae%b9%e5%99%a8%e5%bc%82%e5%b8%b8%e5%b4%a9%e6%ba%83%e6%97%b6%e6%88%91%e4%bb%ac%e5%be%80%e5%be%80%e9%83%bd%e6%98%af%e4%b8%80%e8%be%b9%e9%87%8d%e5%90%af%e5%ae%b9%e5%99%a8%e4%b8%80%e8%be%b9%e9%9d%a2%e5%af%b9%e5%b4%a9%e6%ba%83%e7%9a%84%e5%ae%b9%e5%99%a8%e6%97%a0%e4%bb%8e%e4%b8%8b%e6%89%8b%e9%80%9a%e5%b8%b8%e5%9c%a8%e4%b8%9a%e5%8a%a1%e7%a0%94%e5%8f%91%e8%87%aa%e5%b7%b1build%e7%9a%84%e9%95%9c%e5%83%8f%e5%86%85%e5%8c%85%e5%90%ab%e4%ba%86shell%e6%88%91%e4%bb%ac%e8%bf%98%e8%83%bd%e9%80%9a%e8%bf%87%e5%9c%a8command%e4%b8%ad%e5%b5%8c%e5%85%a5%e4%b8%80%e4%b8%aasleep-3600%e5%91%bd%e4%bb%a4%e6%9d%a5%e9%98%bb%e5%a1%9e%e5%ae%b9%e5%99%a8%e5%86%85%e6%9c%8d%e5%8a%a1%e5%90%af%e5%8a%a8%e4%b8%8d%e8%bf%87%e4%b9%9f%e6%9c%89%e6%97%b6%e5%80%99%e4%bc%9a%e5%87%ba%e7%8e%b0%e4%b8%8d%e7%9f%a5%e9%81%93%e4%bb%8e%e5%93%aa%e9%87%8c%e5%86%92%e5%87%ba%e6%9d%a5%e4%b8%80%e4%b8%aadistroless%e9%95%9c%e5%83%8f%e8%bf%99%e6%97%b6%e5%8f%af%e8%83%bd%e6%9c%80%e5%85%88%e5%b4%a9%e6%ba%83%e7%9a%84%e5%b0%b1%e6%98%af%e8%bf%90%e7%bb%b4%e4%ba%86%e9%82%a3%e6%98%af%e4%b8%80%e7%a7%8d%e8%bf%90%e7%bb%b4%e8%bf%99%e4%b8%aa%e8%81%8c%e4%b8%9a%e8%87%aa%e8%af%9e%e7%94%9f%e4%bb%a5%e6%9d%a5%e7%ac%ac%e4%b8%80%e6%ac%a1%e6%84%9f%e5%8f%97%e5%88%b0%e6%89%8b%e8%b6%b3%e6%97%a0%e6%8e%aa%e5%b9%b6%e8%84%b1%e7%a6%bb%e6%8e%8c%e6%8e%a7%e7%9a%84%e6%97%a0%e5%8a%a9%e6%84%9f%e4%ba%8e%e6%98%af%e5%9c%a8k8s%e7%8e%af%e5%a2%83%e4%b8%8b%e6%97%a0%e6%b3%95debug%e5%ae%b9%e5%99%a8%e7%9a%84%e6%a2%97%e5%bc%80%e5%a7%8b%e5%9c%a8%e5%9d%8a%e9%97%b4%e5%b9%bf%e4%b8%ba%e5%90%90%e6%a7%bd">#&lt;/a>
&lt;/h5>
&lt;h5 id="第一个打破魔咒的是kubectl-debug它包含了agent和debug-tools两个部分也是目前全网内搜到文档最全的解决方案不过目前它的开发似乎已经停止上一次提交还是在8个月之前而最近一次release版本也停留在两年前更难以接受的是当前它无法被集成在容器运行时为containerd的k8s集群">
 第一个打破魔咒的是kubectl-debug，它包含了&lt;strong>agent&lt;/strong>和&lt;strong>debug-tools&lt;/strong>两个部分。也是目前全网内搜到文档最全的解决方案。不过目前它的开发似乎已经停止，上一次提交还是在8个月之前，而最近一次Release版本也停留在两年前。更难以接受的是，当前它无法被集成在容器运行时为Containerd的k8s集群。
 &lt;a class="anchor" href="#%e7%ac%ac%e4%b8%80%e4%b8%aa%e6%89%93%e7%a0%b4%e9%ad%94%e5%92%92%e7%9a%84%e6%98%afkubectl-debug%e5%ae%83%e5%8c%85%e5%90%ab%e4%ba%86agent%e5%92%8cdebug-tools%e4%b8%a4%e4%b8%aa%e9%83%a8%e5%88%86%e4%b9%9f%e6%98%af%e7%9b%ae%e5%89%8d%e5%85%a8%e7%bd%91%e5%86%85%e6%90%9c%e5%88%b0%e6%96%87%e6%a1%a3%e6%9c%80%e5%85%a8%e7%9a%84%e8%a7%a3%e5%86%b3%e6%96%b9%e6%a1%88%e4%b8%8d%e8%bf%87%e7%9b%ae%e5%89%8d%e5%ae%83%e7%9a%84%e5%bc%80%e5%8f%91%e4%bc%bc%e4%b9%8e%e5%b7%b2%e7%bb%8f%e5%81%9c%e6%ad%a2%e4%b8%8a%e4%b8%80%e6%ac%a1%e6%8f%90%e4%ba%a4%e8%bf%98%e6%98%af%e5%9c%a88%e4%b8%aa%e6%9c%88%e4%b9%8b%e5%89%8d%e8%80%8c%e6%9c%80%e8%bf%91%e4%b8%80%e6%ac%a1release%e7%89%88%e6%9c%ac%e4%b9%9f%e5%81%9c%e7%95%99%e5%9c%a8%e4%b8%a4%e5%b9%b4%e5%89%8d%e6%9b%b4%e9%9a%be%e4%bb%a5%e6%8e%a5%e5%8f%97%e7%9a%84%e6%98%af%e5%bd%93%e5%89%8d%e5%ae%83%e6%97%a0%e6%b3%95%e8%a2%ab%e9%9b%86%e6%88%90%e5%9c%a8%e5%ae%b9%e5%99%a8%e8%bf%90%e8%a1%8c%e6%97%b6%e4%b8%bacontainerd%e7%9a%84k8s%e9%9b%86%e7%be%a4">#&lt;/a>
&lt;/h5>
&lt;h5 id="尽管kubectl-debug曾经确实是一款非常好用的容器调试工具但如今kubernetes已经有了更好的容器调试解决方案ephemeral-containers">
 尽管kubectl-debug曾经确实是一款非常好用的容器调试工具，但如今Kubernetes已经有了更好的容器调试解决方案，&lt;code>Ephemeral Containers&lt;/code>
 &lt;a class="anchor" href="#%e5%b0%bd%e7%ae%a1kubectl-debug%e6%9b%be%e7%bb%8f%e7%a1%ae%e5%ae%9e%e6%98%af%e4%b8%80%e6%ac%be%e9%9d%9e%e5%b8%b8%e5%a5%bd%e7%94%a8%e7%9a%84%e5%ae%b9%e5%99%a8%e8%b0%83%e8%af%95%e5%b7%a5%e5%85%b7%e4%bd%86%e5%a6%82%e4%bb%8akubernetes%e5%b7%b2%e7%bb%8f%e6%9c%89%e4%ba%86%e6%9b%b4%e5%a5%bd%e7%9a%84%e5%ae%b9%e5%99%a8%e8%b0%83%e8%af%95%e8%a7%a3%e5%86%b3%e6%96%b9%e6%a1%88ephemeral-containers">#&lt;/a>
&lt;/h5>
&lt;h3 id="ephemeral-containers">
 &lt;strong>Ephemeral Containers&lt;/strong>
 &lt;a class="anchor" href="#ephemeral-containers">#&lt;/a>
&lt;/h3>
&lt;h5 id="ephemeral-containers字如其名它就是一个临时容器这是一个自kubernetes-v116中作为alpha引入的新功能虽然当前它还没有ga不过自从在kubernetes-v118之后在kubectl内已经集成了debug客户端我们几乎可以完整的使用并体验它的新特性">
 Ephemeral Containers字如其名，它就是一个临时容器。这是一个自Kubernetes v1.16中作为alpha引入的新功能，虽然当前它还没有GA，不过自从在Kubernetes v1.18之后，在kubectl内已经集成了debug客户端，我们几乎可以完整的使用并体验它的新特性。
 &lt;a class="anchor" href="#ephemeral-containers%e5%ad%97%e5%a6%82%e5%85%b6%e5%90%8d%e5%ae%83%e5%b0%b1%e6%98%af%e4%b8%80%e4%b8%aa%e4%b8%b4%e6%97%b6%e5%ae%b9%e5%99%a8%e8%bf%99%e6%98%af%e4%b8%80%e4%b8%aa%e8%87%aakubernetes-v116%e4%b8%ad%e4%bd%9c%e4%b8%baalpha%e5%bc%95%e5%85%a5%e7%9a%84%e6%96%b0%e5%8a%9f%e8%83%bd%e8%99%bd%e7%84%b6%e5%bd%93%e5%89%8d%e5%ae%83%e8%bf%98%e6%b2%a1%e6%9c%89ga%e4%b8%8d%e8%bf%87%e8%87%aa%e4%bb%8e%e5%9c%a8kubernetes-v118%e4%b9%8b%e5%90%8e%e5%9c%a8kubectl%e5%86%85%e5%b7%b2%e7%bb%8f%e9%9b%86%e6%88%90%e4%ba%86debug%e5%ae%a2%e6%88%b7%e7%ab%af%e6%88%91%e4%bb%ac%e5%87%a0%e4%b9%8e%e5%8f%af%e4%bb%a5%e5%ae%8c%e6%95%b4%e7%9a%84%e4%bd%bf%e7%94%a8%e5%b9%b6%e4%bd%93%e9%aa%8c%e5%ae%83%e7%9a%84%e6%96%b0%e7%89%b9%e6%80%a7">#&lt;/a>
&lt;/h5>
&lt;h5 id="临时容器的目标是为kubernetes用户提供一个故障诊断工具同时具备满足以下需求">
 临时容器的目标是为Kubernetes用户提供一个故障诊断工具，同时具备满足以下需求：
 &lt;a class="anchor" href="#%e4%b8%b4%e6%97%b6%e5%ae%b9%e5%99%a8%e7%9a%84%e7%9b%ae%e6%a0%87%e6%98%af%e4%b8%bakubernetes%e7%94%a8%e6%88%b7%e6%8f%90%e4%be%9b%e4%b8%80%e4%b8%aa%e6%95%85%e9%9a%9c%e8%af%8a%e6%96%ad%e5%b7%a5%e5%85%b7%e5%90%8c%e6%97%b6%e5%85%b7%e5%a4%87%e6%bb%a1%e8%b6%b3%e4%bb%a5%e4%b8%8b%e9%9c%80%e6%b1%82">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="作为一个开箱即用的平台化工具">
 作为一个开箱即用的平台化工具
 &lt;a class="anchor" href="#%e4%bd%9c%e4%b8%ba%e4%b8%80%e4%b8%aa%e5%bc%80%e7%ae%b1%e5%8d%b3%e7%94%a8%e7%9a%84%e5%b9%b3%e5%8f%b0%e5%8c%96%e5%b7%a5%e5%85%b7">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="不依赖于已经包含在容器镜像中的工具">
 不依赖于已经包含在&lt;a href="https://cloud.tencent.com/product/tcr?from_column=20065&amp;amp;from=20065">容器镜像&lt;/a>中的工具
 &lt;a class="anchor" href="#%e4%b8%8d%e4%be%9d%e8%b5%96%e4%ba%8e%e5%b7%b2%e7%bb%8f%e5%8c%85%e5%90%ab%e5%9c%a8%e5%ae%b9%e5%99%a8%e9%95%9c%e5%83%8f%e4%b8%ad%e7%9a%84%e5%b7%a5%e5%85%b7">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="不需要直接登陆计算节点可以通过kubernetes-api的管理访问node">
 不需要直接登陆计算节点(可以通过Kubernetes API的管理访问Node)
 &lt;a class="anchor" href="#%e4%b8%8d%e9%9c%80%e8%a6%81%e7%9b%b4%e6%8e%a5%e7%99%bb%e9%99%86%e8%ae%a1%e7%ae%97%e8%8a%82%e7%82%b9%e5%8f%af%e4%bb%a5%e9%80%9a%e8%bf%87kubernetes-api%e7%9a%84%e7%ae%a1%e7%90%86%e8%ae%bf%e9%97%aenode">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h5 id="不过也有东西是临时容器不计划支持的比如对windows上启用临时容器就不太友好">
 不过也有东西是临时容器不计划支持的，比如对windows上启用临时容器就不太友好。
 &lt;a class="anchor" href="#%e4%b8%8d%e8%bf%87%e4%b9%9f%e6%9c%89%e4%b8%9c%e8%a5%bf%e6%98%af%e4%b8%b4%e6%97%b6%e5%ae%b9%e5%99%a8%e4%b8%8d%e8%ae%a1%e5%88%92%e6%94%af%e6%8c%81%e7%9a%84%e6%af%94%e5%a6%82%e5%af%b9windows%e4%b8%8a%e5%90%af%e7%94%a8%e4%b8%b4%e6%97%b6%e5%ae%b9%e5%99%a8%e5%b0%b1%e4%b8%8d%e5%a4%aa%e5%8f%8b%e5%a5%bd">#&lt;/a>
&lt;/h5>
&lt;h5 id="启用临时容器的特性也非常简单在kubernetes-v116之后的版本中将启动参数--feature-gatesephemeralcontainerstrue配置到kube-api和kubelet服务上重启即可">
 启用临时容器的特性也非常简单，在kubernetes v1.16之后的版本中将启动参数&lt;code>--feature-gates=EphemeralContainers=true&lt;/code>配置到kube-api和kubelet服务上重启即可。
 &lt;a class="anchor" href="#%e5%90%af%e7%94%a8%e4%b8%b4%e6%97%b6%e5%ae%b9%e5%99%a8%e7%9a%84%e7%89%b9%e6%80%a7%e4%b9%9f%e9%9d%9e%e5%b8%b8%e7%ae%80%e5%8d%95%e5%9c%a8kubernetes-v116%e4%b9%8b%e5%90%8e%e7%9a%84%e7%89%88%e6%9c%ac%e4%b8%ad%e5%b0%86%e5%90%af%e5%8a%a8%e5%8f%82%e6%95%b0--feature-gatesephemeralcontainerstrue%e9%85%8d%e7%bd%ae%e5%88%b0kube-api%e5%92%8ckubelet%e6%9c%8d%e5%8a%a1%e4%b8%8a%e9%87%8d%e5%90%af%e5%8d%b3%e5%8f%af">#&lt;/a>
&lt;/h5>
&lt;blockquote>
&lt;h5 id="在120之前kubectl-debug工具被放在alpha中注意不同版本的命令操作差别-这里推荐使用客户端为120的版本体验会更好">
 在1.20之前，kubectl debug工具被放在alpha中，注意不同版本的命令操作差别 这里推荐使用客户端为1.20+的版本体验会更好
 &lt;a class="anchor" href="#%e5%9c%a8120%e4%b9%8b%e5%89%8dkubectl-debug%e5%b7%a5%e5%85%b7%e8%a2%ab%e6%94%be%e5%9c%a8alpha%e4%b8%ad%e6%b3%a8%e6%84%8f%e4%b8%8d%e5%90%8c%e7%89%88%e6%9c%ac%e7%9a%84%e5%91%bd%e4%bb%a4%e6%93%8d%e4%bd%9c%e5%b7%ae%e5%88%ab-%e8%bf%99%e9%87%8c%e6%8e%a8%e8%8d%90%e4%bd%bf%e7%94%a8%e5%ae%a2%e6%88%b7%e7%ab%af%e4%b8%ba120%e7%9a%84%e7%89%88%e6%9c%ac%e4%bd%93%e9%aa%8c%e4%bc%9a%e6%9b%b4%e5%a5%bd">#&lt;/a>
&lt;/h5>&lt;/blockquote>
&lt;h5 id="那么我们有了ephemeral-containers能做哪些事情呢">
 那么我们有了Ephemeral Containers能做哪些事情呢？
 &lt;a class="anchor" href="#%e9%82%a3%e4%b9%88%e6%88%91%e4%bb%ac%e6%9c%89%e4%ba%86ephemeral-containers%e8%83%bd%e5%81%9a%e5%93%aa%e4%ba%9b%e4%ba%8b%e6%83%85%e5%91%a2">#&lt;/a>
&lt;/h5>
&lt;h4 id="1-pod-troubleshooting">
 &lt;strong>1. POD Troubleshooting&lt;/strong>
 &lt;a class="anchor" href="#1-pod-troubleshooting">#&lt;/a>
&lt;/h4>
&lt;h5 id="如上文所说我们可以直接通过kubectl-debug命令进行容器调试最直接简单的对一个pod进行调试命令如下">
 如上文所说，我们可以直接通过kubectl debug命令进行容器调试。最直接简单的对一个pod进行调试命令如下：
 &lt;a class="anchor" href="#%e5%a6%82%e4%b8%8a%e6%96%87%e6%89%80%e8%af%b4%e6%88%91%e4%bb%ac%e5%8f%af%e4%bb%a5%e7%9b%b4%e6%8e%a5%e9%80%9a%e8%bf%87kubectl-debug%e5%91%bd%e4%bb%a4%e8%bf%9b%e8%a1%8c%e5%ae%b9%e5%99%a8%e8%b0%83%e8%af%95%e6%9c%80%e7%9b%b4%e6%8e%a5%e7%ae%80%e5%8d%95%e7%9a%84%e5%af%b9%e4%b8%80%e4%b8%aapod%e8%bf%9b%e8%a1%8c%e8%b0%83%e8%af%95%e5%91%bd%e4%bb%a4%e5%a6%82%e4%b8%8b">#&lt;/a>
&lt;/h5>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-javascript" data-lang="javascript">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">kubectl&lt;/span> &lt;span style="color:#a6e22e">debug&lt;/span> &lt;span style="color:#a6e22e">mypod&lt;/span> &lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#a6e22e">it&lt;/span> &lt;span style="color:#f92672">--&lt;/span>&lt;span style="color:#a6e22e">image&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">busybox&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>复制&lt;/p>
&lt;p>默认情况下用户不指定临时容器名称的话，debug容器名称就由kubectl自动生成一个唯一id的名称。如果用户需要自己指定容器名称则使用&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-javascript" data-lang="javascript">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">kubectl&lt;/span> &lt;span style="color:#a6e22e">debug&lt;/span> &lt;span style="color:#a6e22e">mypod&lt;/span> &lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span> &lt;span style="color:#66d9ef">debugger&lt;/span> &lt;span style="color:#f92672">--&lt;/span>&lt;span style="color:#a6e22e">image&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">busybox&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>复制&lt;/p>
&lt;p>有了临时容器除了日常debug功能外，我们可以扩展出很多新花样的玩法。比如批量跑某个命名空间下的安全扫描的脚本而不用干扰原容器。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-javascript" data-lang="javascript">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">for&lt;/span> &lt;span style="color:#a6e22e">pod&lt;/span> &lt;span style="color:#66d9ef">in&lt;/span> &lt;span style="color:#a6e22e">$&lt;/span>(&lt;span style="color:#a6e22e">kubectl&lt;/span> &lt;span style="color:#a6e22e">get&lt;/span> &lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#a6e22e">o&lt;/span> &lt;span style="color:#a6e22e">name&lt;/span> &lt;span style="color:#a6e22e">pod&lt;/span>); 
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">do&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">kubectl&lt;/span> &lt;span style="color:#a6e22e">debug&lt;/span> &lt;span style="color:#f92672">--&lt;/span>&lt;span style="color:#a6e22e">image&lt;/span> &lt;span style="color:#a6e22e">security&lt;/span>&lt;span style="color:#f92672">/&lt;/span>&lt;span style="color:#a6e22e">pod_scanner&lt;/span> &lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#a6e22e">p&lt;/span> &lt;span style="color:#a6e22e">$pod&lt;/span> &lt;span style="color:#f92672">/&lt;/span>&lt;span style="color:#a6e22e">sanner&lt;/span>.&lt;span style="color:#a6e22e">sh&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">done&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>复制&lt;/p></description></item><item><title>2024-04-03 k8s运维之清理磁盘</title><link>https://qq547475331.github.io/docs/k8s%E8%BF%90%E7%BB%B4%E4%B9%8B%E6%B8%85%E7%90%86%E7%A3%81%E7%9B%98-k8s-yun-wei-zhi-qing-li-ci-pan/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/k8s%E8%BF%90%E7%BB%B4%E4%B9%8B%E6%B8%85%E7%90%86%E7%A3%81%E7%9B%98-k8s-yun-wei-zhi-qing-li-ci-pan/</guid><description>&lt;h2 id="清理无用的镜像和关闭的容器">
 &lt;strong>清理无用的镜像和关闭的容器&lt;/strong>
 &lt;a class="anchor" href="#%e6%b8%85%e7%90%86%e6%97%a0%e7%94%a8%e7%9a%84%e9%95%9c%e5%83%8f%e5%92%8c%e5%85%b3%e9%97%ad%e7%9a%84%e5%ae%b9%e5%99%a8">#&lt;/a>
&lt;/h2>
&lt;p>使用命令&lt;/p>
&lt;pre tabindex="0">&lt;code>docker system df
docker system prune -a
&lt;/code>&lt;/pre>&lt;p>&lt;strong>docker system df&lt;/strong> 命令，类似于 &lt;strong>Linux&lt;/strong>上的 &lt;strong>df&lt;/strong> 命令，用于查看 &lt;strong>Docker&lt;/strong> 的磁盘使用情况：&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202402221104478.png" alt="image-20240222110448424" />&lt;/p>
&lt;p>&lt;strong>TYPE&lt;/strong>列出了 &lt;strong>Docker&lt;/strong> 使用磁盘的 &lt;strong>4&lt;/strong> 种类型：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Images&lt;/strong> ：所有镜像占用的空间，包括拉取下来的镜像，和本地构建的。&lt;/li>
&lt;li>&lt;strong>Containers&lt;/strong> ：运行的容器占用的空间，表示每个容器的读写层的空间。&lt;/li>
&lt;li>&lt;strong>Local Volumes&lt;/strong> ：容器挂载本地数据卷的空间。&lt;/li>
&lt;li>&lt;strong>Build Cache&lt;/strong> ：镜像构建过程中产生的缓存空间（只有在使用 &lt;strong>BuildKit&lt;/strong> 时才有，&lt;strong>Docker 18.09&lt;/strong> 以后可用）。&lt;/li>
&lt;/ul>
&lt;p>最后的 &lt;strong>RECLAIMABLE&lt;/strong> 是可回收大小。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>docker system prune&lt;/strong> : 可以用于清理磁盘，删除关闭的容器、无用的数据卷和网络，以及 &lt;strong>dangling&lt;/strong> 镜像（即无 &lt;strong>tag&lt;/strong> 的镜像）。&lt;/li>
&lt;li>&lt;strong>docker system prune -a&lt;/strong> : 清理得更加彻底，可以将没有容器使用 &lt;strong>Docker&lt;/strong>镜像都删掉。 注意，这两个命令会把你暂时关闭的容器，以及暂时没有用到的 &lt;strong>Docker&lt;/strong> 镜像都删掉了。&lt;/li>
&lt;/ul>
&lt;h2 id="清理有问题的容器">
 清理有问题的容器
 &lt;a class="anchor" href="#%e6%b8%85%e7%90%86%e6%9c%89%e9%97%ae%e9%a2%98%e7%9a%84%e5%ae%b9%e5%99%a8">#&lt;/a>
&lt;/h2>
&lt;p>如果磁盘水位还是比较高。那大概率是某个容器有问题，比如疯狂往磁盘上记日志。&lt;/p>
&lt;p>使用命令&lt;/p>
&lt;pre tabindex="0">&lt;code>docker system df -v
&lt;/code>&lt;/pre>&lt;p>可以看到每次容器的size。比如这个玩意&lt;/p></description></item><item><title>2024-04-03 K8S面试大全</title><link>https://qq547475331.github.io/docs/k8s%E9%9D%A2%E8%AF%95%E5%A4%A7%E5%85%A8-k8s-mian-shi-da-quan/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/k8s%E9%9D%A2%E8%AF%95%E5%A4%A7%E5%85%A8-k8s-mian-shi-da-quan/</guid><description>&lt;p>&lt;strong>一、Kubernetes 基础知识面试题10 道面试题&lt;/strong>&lt;/p>
&lt;p>&lt;strong>1、什么是 Kubernetes？&lt;/strong>&lt;/p>
&lt;h5 id="kubernetes-是一个开源容器管理工具负责容器部署容器扩缩容以及负载平衡它提供了出色的社区并与所有云提供商合作因此我们可以说-kubernetes-不是一个容器化平台而是一个多容器管理解决方案">
 Kubernetes 是一个开源容器管理工具，负责容器部署，容器扩缩容以及负载平衡。它提供了出色的社区，并与所有云提供商合作。因此，我们可以说 Kubernetes 不是一个容器化平台，而是一个多容器管理解决方案。
 &lt;a class="anchor" href="#kubernetes-%e6%98%af%e4%b8%80%e4%b8%aa%e5%bc%80%e6%ba%90%e5%ae%b9%e5%99%a8%e7%ae%a1%e7%90%86%e5%b7%a5%e5%85%b7%e8%b4%9f%e8%b4%a3%e5%ae%b9%e5%99%a8%e9%83%a8%e7%bd%b2%e5%ae%b9%e5%99%a8%e6%89%a9%e7%bc%a9%e5%ae%b9%e4%bb%a5%e5%8f%8a%e8%b4%9f%e8%bd%bd%e5%b9%b3%e8%a1%a1%e5%ae%83%e6%8f%90%e4%be%9b%e4%ba%86%e5%87%ba%e8%89%b2%e7%9a%84%e7%a4%be%e5%8c%ba%e5%b9%b6%e4%b8%8e%e6%89%80%e6%9c%89%e4%ba%91%e6%8f%90%e4%be%9b%e5%95%86%e5%90%88%e4%bd%9c%e5%9b%a0%e6%ad%a4%e6%88%91%e4%bb%ac%e5%8f%af%e4%bb%a5%e8%af%b4-kubernetes-%e4%b8%8d%e6%98%af%e4%b8%80%e4%b8%aa%e5%ae%b9%e5%99%a8%e5%8c%96%e5%b9%b3%e5%8f%b0%e8%80%8c%e6%98%af%e4%b8%80%e4%b8%aa%e5%a4%9a%e5%ae%b9%e5%99%a8%e7%ae%a1%e7%90%86%e8%a7%a3%e5%86%b3%e6%96%b9%e6%a1%88">#&lt;/a>
&lt;/h5>
&lt;p>&lt;strong>2、 Kubernetes 与 docker 什么关系？&lt;/strong>&lt;/p>
&lt;h5 id="docker-提供容器的生命周期管理docker-镜像构建运行时容器但是由于这些单独的容器必须通信因此使用-kubernetes因此我们说-docker-构建容器这些容器通过-kubernetes-相互通信因此可以使用-kubernetes-手动关联和编排在多个主机上运行的容器">
 Docker 提供容器的生命周期管理，Docker 镜像构建运行时容器。但是，由于这些单独的容器必须通信，因此使用 Kubernetes。因此，我们说 Docker 构建容器，这些容器通过 Kubernetes 相互通信。因此，可以使用 Kubernetes 手动关联和编排在多个主机上运行的容器。
 &lt;a class="anchor" href="#docker-%e6%8f%90%e4%be%9b%e5%ae%b9%e5%99%a8%e7%9a%84%e7%94%9f%e5%91%bd%e5%91%a8%e6%9c%9f%e7%ae%a1%e7%90%86docker-%e9%95%9c%e5%83%8f%e6%9e%84%e5%bb%ba%e8%bf%90%e8%a1%8c%e6%97%b6%e5%ae%b9%e5%99%a8%e4%bd%86%e6%98%af%e7%94%b1%e4%ba%8e%e8%bf%99%e4%ba%9b%e5%8d%95%e7%8b%ac%e7%9a%84%e5%ae%b9%e5%99%a8%e5%bf%85%e9%a1%bb%e9%80%9a%e4%bf%a1%e5%9b%a0%e6%ad%a4%e4%bd%bf%e7%94%a8-kubernetes%e5%9b%a0%e6%ad%a4%e6%88%91%e4%bb%ac%e8%af%b4-docker-%e6%9e%84%e5%bb%ba%e5%ae%b9%e5%99%a8%e8%bf%99%e4%ba%9b%e5%ae%b9%e5%99%a8%e9%80%9a%e8%bf%87-kubernetes-%e7%9b%b8%e4%ba%92%e9%80%9a%e4%bf%a1%e5%9b%a0%e6%ad%a4%e5%8f%af%e4%bb%a5%e4%bd%bf%e7%94%a8-kubernetes-%e6%89%8b%e5%8a%a8%e5%85%b3%e8%81%94%e5%92%8c%e7%bc%96%e6%8e%92%e5%9c%a8%e5%a4%9a%e4%b8%aa%e4%b8%bb%e6%9c%ba%e4%b8%8a%e8%bf%90%e8%a1%8c%e7%9a%84%e5%ae%b9%e5%99%a8">#&lt;/a>
&lt;/h5>
&lt;p>&lt;strong>3、Kubernetes 与 Docker Swarm 的区别？&lt;/strong>&lt;/p>
&lt;h5 id="docker-swarm-和-kubernetes-都可以用于类似目的它们都是容器编排工具">
 Docker Swarm 和 Kubernetes 都可以用于类似目的。它们都是容器编排工具。
 &lt;a class="anchor" href="#docker-swarm-%e5%92%8c-kubernetes-%e9%83%bd%e5%8f%af%e4%bb%a5%e7%94%a8%e4%ba%8e%e7%b1%bb%e4%bc%bc%e7%9b%ae%e7%9a%84%e5%ae%83%e4%bb%ac%e9%83%bd%e6%98%af%e5%ae%b9%e5%99%a8%e7%bc%96%e6%8e%92%e5%b7%a5%e5%85%b7">#&lt;/a>
&lt;/h5>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202402221059641.png" alt="image-20240222105935530" />&lt;/p>
&lt;p>&lt;strong>4、在主机和容器上部署应用程序有什么区别？&lt;/strong>&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202402221059626.png" alt="image-20240222105949456" />&lt;/p>
&lt;h5 id="上图左侧架构表示在主机上部署应用程序因此这种架构将具有操作系统然后操作系统将具有内核该内核将在应用程序所需的操作系统上安装各种库因此在这种框架中你可以拥有-n-个应用程序并且所有应用程序将共享该操作系统中存在的库">
 &lt;strong>上图左&lt;/strong>侧架构表示在主机上部署应用程序。因此，这种架构将具有操作系统，然后操作系统将具有内核，该内核将在应用程序所需的操作系统上安装各种库。因此，在这种框架中，你可以拥有 n 个应用程序，并且所有应用程序将共享该操作系统中存在的库。
 &lt;a class="anchor" href="#%e4%b8%8a%e5%9b%be%e5%b7%a6%e4%be%a7%e6%9e%b6%e6%9e%84%e8%a1%a8%e7%a4%ba%e5%9c%a8%e4%b8%bb%e6%9c%ba%e4%b8%8a%e9%83%a8%e7%bd%b2%e5%ba%94%e7%94%a8%e7%a8%8b%e5%ba%8f%e5%9b%a0%e6%ad%a4%e8%bf%99%e7%a7%8d%e6%9e%b6%e6%9e%84%e5%b0%86%e5%85%b7%e6%9c%89%e6%93%8d%e4%bd%9c%e7%b3%bb%e7%bb%9f%e7%84%b6%e5%90%8e%e6%93%8d%e4%bd%9c%e7%b3%bb%e7%bb%9f%e5%b0%86%e5%85%b7%e6%9c%89%e5%86%85%e6%a0%b8%e8%af%a5%e5%86%85%e6%a0%b8%e5%b0%86%e5%9c%a8%e5%ba%94%e7%94%a8%e7%a8%8b%e5%ba%8f%e6%89%80%e9%9c%80%e7%9a%84%e6%93%8d%e4%bd%9c%e7%b3%bb%e7%bb%9f%e4%b8%8a%e5%ae%89%e8%a3%85%e5%90%84%e7%a7%8d%e5%ba%93%e5%9b%a0%e6%ad%a4%e5%9c%a8%e8%bf%99%e7%a7%8d%e6%a1%86%e6%9e%b6%e4%b8%ad%e4%bd%a0%e5%8f%af%e4%bb%a5%e6%8b%a5%e6%9c%89-n-%e4%b8%aa%e5%ba%94%e7%94%a8%e7%a8%8b%e5%ba%8f%e5%b9%b6%e4%b8%94%e6%89%80%e6%9c%89%e5%ba%94%e7%94%a8%e7%a8%8b%e5%ba%8f%e5%b0%86%e5%85%b1%e4%ba%ab%e8%af%a5%e6%93%8d%e4%bd%9c%e7%b3%bb%e7%bb%9f%e4%b8%ad%e5%ad%98%e5%9c%a8%e7%9a%84%e5%ba%93">#&lt;/a>
&lt;/h5>
&lt;h5 id="上图右侧架构是容器中部署应用程序这种架构将有一个内核这是唯一一个在所有应用程序之间唯一共同的东西各个块基本上是容器化的并且这些块与其他应用程序隔离因此应用程序具有与系统其余部分隔离的必要库和二进制文件并且不能被任何其他应用程序侵占">
 &lt;strong>上图右&lt;/strong>侧架构是容器中部署应用程序。这种架构将有一个内核，这是唯一一个在所有应用程序之间唯一共同的东西。各个块基本上是容器化的，并且这些块与其他应用程序隔离。因此，应用程序具有与系统其余部分隔离的必要库和二进制文件，并且不能被任何其他应用程序侵占。
 &lt;a class="anchor" href="#%e4%b8%8a%e5%9b%be%e5%8f%b3%e4%be%a7%e6%9e%b6%e6%9e%84%e6%98%af%e5%ae%b9%e5%99%a8%e4%b8%ad%e9%83%a8%e7%bd%b2%e5%ba%94%e7%94%a8%e7%a8%8b%e5%ba%8f%e8%bf%99%e7%a7%8d%e6%9e%b6%e6%9e%84%e5%b0%86%e6%9c%89%e4%b8%80%e4%b8%aa%e5%86%85%e6%a0%b8%e8%bf%99%e6%98%af%e5%94%af%e4%b8%80%e4%b8%80%e4%b8%aa%e5%9c%a8%e6%89%80%e6%9c%89%e5%ba%94%e7%94%a8%e7%a8%8b%e5%ba%8f%e4%b9%8b%e9%97%b4%e5%94%af%e4%b8%80%e5%85%b1%e5%90%8c%e7%9a%84%e4%b8%9c%e8%a5%bf%e5%90%84%e4%b8%aa%e5%9d%97%e5%9f%ba%e6%9c%ac%e4%b8%8a%e6%98%af%e5%ae%b9%e5%99%a8%e5%8c%96%e7%9a%84%e5%b9%b6%e4%b8%94%e8%bf%99%e4%ba%9b%e5%9d%97%e4%b8%8e%e5%85%b6%e4%bb%96%e5%ba%94%e7%94%a8%e7%a8%8b%e5%ba%8f%e9%9a%94%e7%a6%bb%e5%9b%a0%e6%ad%a4%e5%ba%94%e7%94%a8%e7%a8%8b%e5%ba%8f%e5%85%b7%e6%9c%89%e4%b8%8e%e7%b3%bb%e7%bb%9f%e5%85%b6%e4%bd%99%e9%83%a8%e5%88%86%e9%9a%94%e7%a6%bb%e7%9a%84%e5%bf%85%e8%a6%81%e5%ba%93%e5%92%8c%e4%ba%8c%e8%bf%9b%e5%88%b6%e6%96%87%e4%bb%b6%e5%b9%b6%e4%b8%94%e4%b8%8d%e8%83%bd%e8%a2%ab%e4%bb%bb%e4%bd%95%e5%85%b6%e4%bb%96%e5%ba%94%e7%94%a8%e7%a8%8b%e5%ba%8f%e4%be%b5%e5%8d%a0">#&lt;/a>
&lt;/h5>
&lt;p>&lt;strong>5、Kubernetes 如何简化容器化部署？&lt;/strong>&lt;/p>
&lt;h5 id="跨主机的容器都需要相互通信因此要做到这一点你需要一些能够负载平衡扩展和监控容器的东西由于-kubernetes-与云无关并且可以在任何公共私有提供商上运行因此可以简化容器化部署程序">
 跨主机的容器都需要相互通信。因此，要做到这一点，你需要一些能够负载平衡，扩展和监控容器的东西。由于 Kubernetes 与云无关并且可以在任何公共/私有提供商上运行，因此可以简化容器化部署程序。
 &lt;a class="anchor" href="#%e8%b7%a8%e4%b8%bb%e6%9c%ba%e7%9a%84%e5%ae%b9%e5%99%a8%e9%83%bd%e9%9c%80%e8%a6%81%e7%9b%b8%e4%ba%92%e9%80%9a%e4%bf%a1%e5%9b%a0%e6%ad%a4%e8%a6%81%e5%81%9a%e5%88%b0%e8%bf%99%e4%b8%80%e7%82%b9%e4%bd%a0%e9%9c%80%e8%a6%81%e4%b8%80%e4%ba%9b%e8%83%bd%e5%a4%9f%e8%b4%9f%e8%bd%bd%e5%b9%b3%e8%a1%a1%e6%89%a9%e5%b1%95%e5%92%8c%e7%9b%91%e6%8e%a7%e5%ae%b9%e5%99%a8%e7%9a%84%e4%b8%9c%e8%a5%bf%e7%94%b1%e4%ba%8e-kubernetes-%e4%b8%8e%e4%ba%91%e6%97%a0%e5%85%b3%e5%b9%b6%e4%b8%94%e5%8f%af%e4%bb%a5%e5%9c%a8%e4%bb%bb%e4%bd%95%e5%85%ac%e5%85%b1%e7%a7%81%e6%9c%89%e6%8f%90%e4%be%9b%e5%95%86%e4%b8%8a%e8%bf%90%e8%a1%8c%e5%9b%a0%e6%ad%a4%e5%8f%af%e4%bb%a5%e7%ae%80%e5%8c%96%e5%ae%b9%e5%99%a8%e5%8c%96%e9%83%a8%e7%bd%b2%e7%a8%8b%e5%ba%8f">#&lt;/a>
&lt;/h5>
&lt;p>&lt;strong>6、什么是 kubectl？&lt;/strong>&lt;/p>
&lt;h5 id="kubectl-是一个平台可以使用该平台将命令传递给集群因此它基本上为cli-提供了针对-kubernetes-集群运行命令的方法以及创建和管理-kubernetes组件的各种方法">
 Kubectl 是一个平台，可以使用该平台将命令传递给集群。因此，它基本上为CLI 提供了针对 Kubernetes 集群运行命令的方法，以及创建和管理 Kubernetes组件的各种方法。
 &lt;a class="anchor" href="#kubectl-%e6%98%af%e4%b8%80%e4%b8%aa%e5%b9%b3%e5%8f%b0%e5%8f%af%e4%bb%a5%e4%bd%bf%e7%94%a8%e8%af%a5%e5%b9%b3%e5%8f%b0%e5%b0%86%e5%91%bd%e4%bb%a4%e4%bc%a0%e9%80%92%e7%bb%99%e9%9b%86%e7%be%a4%e5%9b%a0%e6%ad%a4%e5%ae%83%e5%9f%ba%e6%9c%ac%e4%b8%8a%e4%b8%bacli-%e6%8f%90%e4%be%9b%e4%ba%86%e9%92%88%e5%af%b9-kubernetes-%e9%9b%86%e7%be%a4%e8%bf%90%e8%a1%8c%e5%91%bd%e4%bb%a4%e7%9a%84%e6%96%b9%e6%b3%95%e4%bb%a5%e5%8f%8a%e5%88%9b%e5%bb%ba%e5%92%8c%e7%ae%a1%e7%90%86-kubernetes%e7%bb%84%e4%bb%b6%e7%9a%84%e5%90%84%e7%a7%8d%e6%96%b9%e6%b3%95">#&lt;/a>
&lt;/h5>
&lt;p>&lt;strong>7、什么是 kubelet？&lt;/strong>&lt;/p></description></item><item><title>2024-04-03 K8S面试宝典</title><link>https://qq547475331.github.io/docs/k8s%E9%9D%A2%E8%AF%95%E5%AE%9D%E5%85%B8-k8s-mian-shi-bao-dian/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/k8s%E9%9D%A2%E8%AF%95%E5%AE%9D%E5%85%B8-k8s-mian-shi-bao-dian/</guid><description>&lt;h2 id="创建-pod的主要流程">
 创建 Pod的主要流程?
 &lt;a class="anchor" href="#%e5%88%9b%e5%bb%ba-pod%e7%9a%84%e4%b8%bb%e8%a6%81%e6%b5%81%e7%a8%8b">#&lt;/a>
&lt;/h2>
&lt;ol>
&lt;li>
&lt;h5 id="客户端提交-pod-的配置信息可以是-yaml-文件定义的信息到-kube-apiserver">
 客户端提交 Pod 的配置信息(可以是 yaml 文件定义的信息)到 kube-apiserver.
 &lt;a class="anchor" href="#%e5%ae%a2%e6%88%b7%e7%ab%af%e6%8f%90%e4%ba%a4-pod-%e7%9a%84%e9%85%8d%e7%bd%ae%e4%bf%a1%e6%81%af%e5%8f%af%e4%bb%a5%e6%98%af-yaml-%e6%96%87%e4%bb%b6%e5%ae%9a%e4%b9%89%e7%9a%84%e4%bf%a1%e6%81%af%e5%88%b0-kube-apiserver">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="apiserver-收到指令后通知-controllr-manager-创建一个资源对象">
 Apiserver 收到指令后,通知 controllr-manager 创建一个资源对象
 &lt;a class="anchor" href="#apiserver-%e6%94%b6%e5%88%b0%e6%8c%87%e4%bb%a4%e5%90%8e%e9%80%9a%e7%9f%a5-controllr-manager-%e5%88%9b%e5%bb%ba%e4%b8%80%e4%b8%aa%e8%b5%84%e6%ba%90%e5%af%b9%e8%b1%a1">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="controller-manager-通过-apiserver-将-pod-的配置信息存储到-etcd-数据中心中">
 controller-manager 通过 apiserver 将 pod 的配置信息存储到 ETCD 数据中心中
 &lt;a class="anchor" href="#controller-manager-%e9%80%9a%e8%bf%87-apiserver-%e5%b0%86-pod-%e7%9a%84%e9%85%8d%e7%bd%ae%e4%bf%a1%e6%81%af%e5%ad%98%e5%82%a8%e5%88%b0-etcd-%e6%95%b0%e6%8d%ae%e4%b8%ad%e5%bf%83%e4%b8%ad">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="kube-scheduler-检查到-pod-信息会开始调度预选会先过滤不符合-pod-资源配置要求的节点然后开始调度调优主要是挑选出更适合运行的-pod-节点然后将-pod-的资源配置单发送到-node-节点上的-kubelet-组件上">
 kube-scheduler 检查到 pod 信息会开始调度预选,会先过滤不符合 Pod 资源配置要求的节点,然后开始调度调优,主要是挑选出更适合运行的 pod 节点,然后将 pod 的资源配置单发送到 node 节点上的 kubelet 组件上
 &lt;a class="anchor" href="#kube-scheduler-%e6%a3%80%e6%9f%a5%e5%88%b0-pod-%e4%bf%a1%e6%81%af%e4%bc%9a%e5%bc%80%e5%a7%8b%e8%b0%83%e5%ba%a6%e9%a2%84%e9%80%89%e4%bc%9a%e5%85%88%e8%bf%87%e6%bb%a4%e4%b8%8d%e7%ac%a6%e5%90%88-pod-%e8%b5%84%e6%ba%90%e9%85%8d%e7%bd%ae%e8%a6%81%e6%b1%82%e7%9a%84%e8%8a%82%e7%82%b9%e7%84%b6%e5%90%8e%e5%bc%80%e5%a7%8b%e8%b0%83%e5%ba%a6%e8%b0%83%e4%bc%98%e4%b8%bb%e8%a6%81%e6%98%af%e6%8c%91%e9%80%89%e5%87%ba%e6%9b%b4%e9%80%82%e5%90%88%e8%bf%90%e8%a1%8c%e7%9a%84-pod-%e8%8a%82%e7%82%b9%e7%84%b6%e5%90%8e%e5%b0%86-pod-%e7%9a%84%e8%b5%84%e6%ba%90%e9%85%8d%e7%bd%ae%e5%8d%95%e5%8f%91%e9%80%81%e5%88%b0-node-%e8%8a%82%e7%82%b9%e4%b8%8a%e7%9a%84-kubelet-%e7%bb%84%e4%bb%b6%e4%b8%8a">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="kubelet-根据-scheduler-发来的资源配置单运行-pod运行成功后将-pod-的运行的信息返回-scheduler-scheduler-将返回的-pod-运行状况的信息存储到-etcd-数据中心">
 kubelet 根据 scheduler 发来的资源配置单运行 pod,运行成功后,将 pod 的运行的信息返回 scheduler, scheduler 将返回的 pod 运行状况的信息存储到 etcd 数据中心
 &lt;a class="anchor" href="#kubelet-%e6%a0%b9%e6%8d%ae-scheduler-%e5%8f%91%e6%9d%a5%e7%9a%84%e8%b5%84%e6%ba%90%e9%85%8d%e7%bd%ae%e5%8d%95%e8%bf%90%e8%a1%8c-pod%e8%bf%90%e8%a1%8c%e6%88%90%e5%8a%9f%e5%90%8e%e5%b0%86-pod-%e7%9a%84%e8%bf%90%e8%a1%8c%e7%9a%84%e4%bf%a1%e6%81%af%e8%bf%94%e5%9b%9e-scheduler-scheduler-%e5%b0%86%e8%bf%94%e5%9b%9e%e7%9a%84-pod-%e8%bf%90%e8%a1%8c%e7%8a%b6%e5%86%b5%e7%9a%84%e4%bf%a1%e6%81%af%e5%ad%98%e5%82%a8%e5%88%b0-etcd-%e6%95%b0%e6%8d%ae%e4%b8%ad%e5%bf%83">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ol>
&lt;h2 id="pod-的重启策略">
 Pod 的重启策略
 &lt;a class="anchor" href="#pod-%e7%9a%84%e9%87%8d%e5%90%af%e7%ad%96%e7%95%a5">#&lt;/a>
&lt;/h2>
&lt;ul>
&lt;li>
&lt;h5 id="-pod-重启策略restartpolicy应用于-pod-内的所有容器并且仅再-pod-所处的-node-上由-kubelet-进行判断和重启操作当某个容器异常退出或健康检查失败时kubele-将根据-restartpolicy-的设置来进行相应操作">
 • Pod 重启策略(RestartPolicy)应用于 Pod 内的所有容器,并且仅再 Pod 所处的 Node 上由 Kubelet 进行判断和重启操作.当某个容器异常退出或健康检查失败时,kubele 将根据 RestartPolicy 的设置来进行相应操作
 &lt;a class="anchor" href="#-pod-%e9%87%8d%e5%90%af%e7%ad%96%e7%95%a5restartpolicy%e5%ba%94%e7%94%a8%e4%ba%8e-pod-%e5%86%85%e7%9a%84%e6%89%80%e6%9c%89%e5%ae%b9%e5%99%a8%e5%b9%b6%e4%b8%94%e4%bb%85%e5%86%8d-pod-%e6%89%80%e5%a4%84%e7%9a%84-node-%e4%b8%8a%e7%94%b1-kubelet-%e8%bf%9b%e8%a1%8c%e5%88%a4%e6%96%ad%e5%92%8c%e9%87%8d%e5%90%af%e6%93%8d%e4%bd%9c%e5%bd%93%e6%9f%90%e4%b8%aa%e5%ae%b9%e5%99%a8%e5%bc%82%e5%b8%b8%e9%80%80%e5%87%ba%e6%88%96%e5%81%a5%e5%ba%b7%e6%a3%80%e6%9f%a5%e5%a4%b1%e8%b4%a5%e6%97%b6kubele-%e5%b0%86%e6%a0%b9%e6%8d%ae-restartpolicy-%e7%9a%84%e8%ae%be%e7%bd%ae%e6%9d%a5%e8%bf%9b%e8%a1%8c%e7%9b%b8%e5%ba%94%e6%93%8d%e4%bd%9c">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-pod-的重启策略包括-alwaysonfaliure-和-never默认值为-always">
 • pod 的重启策略包括 Always,OnFaliure 和 Never,默认值为 Always
 &lt;a class="anchor" href="#-pod-%e7%9a%84%e9%87%8d%e5%90%af%e7%ad%96%e7%95%a5%e5%8c%85%e6%8b%ac-alwaysonfaliure-%e5%92%8c-never%e9%bb%98%e8%ae%a4%e5%80%bc%e4%b8%ba-always">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;ul>
&lt;li>
&lt;h5 id="-always-当容器失效时由-kubelet-自动重启该容器">
 • Always: 当容器失效时由 kubelet 自动重启该容器
 &lt;a class="anchor" href="#-always-%e5%bd%93%e5%ae%b9%e5%99%a8%e5%a4%b1%e6%95%88%e6%97%b6%e7%94%b1-kubelet-%e8%87%aa%e5%8a%a8%e9%87%8d%e5%90%af%e8%af%a5%e5%ae%b9%e5%99%a8">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-onfailure当容器终止运行且退出不为-0-时-由-kubelet-自动重启该容器">
 • OnFailure:当容器终止运行且退出不为 0 时, 由 kubelet 自动重启该容器
 &lt;a class="anchor" href="#-onfailure%e5%bd%93%e5%ae%b9%e5%99%a8%e7%bb%88%e6%ad%a2%e8%bf%90%e8%a1%8c%e4%b8%94%e9%80%80%e5%87%ba%e4%b8%8d%e4%b8%ba-0-%e6%97%b6-%e7%94%b1-kubelet-%e8%87%aa%e5%8a%a8%e9%87%8d%e5%90%af%e8%af%a5%e5%ae%b9%e5%99%a8">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-nerve-不论容器运行状态如何kubelet-都不会重启该容器">
 • Nerve: 不论容器运行状态如何,kubelet 都不会重启该容器
 &lt;a class="anchor" href="#-nerve-%e4%b8%8d%e8%ae%ba%e5%ae%b9%e5%99%a8%e8%bf%90%e8%a1%8c%e7%8a%b6%e6%80%81%e5%a6%82%e4%bd%95kubelet-%e9%83%bd%e4%b8%8d%e4%bc%9a%e9%87%8d%e5%90%af%e8%af%a5%e5%ae%b9%e5%99%a8">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;h5 id="-同时-pod-的容器策略与控制方式关联当前可用于管理-pod-的控制器包括-relicatoncontroller">
 • 同时 pod 的容器策略与控制方式关联,当前可用于管理 Pod 的控制器包括 RelicatonController
 &lt;a class="anchor" href="#-%e5%90%8c%e6%97%b6-pod-%e7%9a%84%e5%ae%b9%e5%99%a8%e7%ad%96%e7%95%a5%e4%b8%8e%e6%8e%a7%e5%88%b6%e6%96%b9%e5%bc%8f%e5%85%b3%e8%81%94%e5%bd%93%e5%89%8d%e5%8f%af%e7%94%a8%e4%ba%8e%e7%ae%a1%e7%90%86-pod-%e7%9a%84%e6%8e%a7%e5%88%b6%e5%99%a8%e5%8c%85%e6%8b%ac-relicatoncontroller">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h2 id="pod-的健康检查方式">
 Pod 的健康检查方式
 &lt;a class="anchor" href="#pod-%e7%9a%84%e5%81%a5%e5%ba%b7%e6%a3%80%e6%9f%a5%e6%96%b9%e5%bc%8f">#&lt;/a>
&lt;/h2>
&lt;ul>
&lt;li>
&lt;h5 id="-livenessprobe探针用于判断容器是否存活running状态如果livenessprobe探针探测到容器不健康则kubelet将杀掉该容器并根据容器的重启策略做相应处理若一个容器不包含livenessprobe探针kubelet认为该容器的livenessprobe探针返回值用于是success">
 • LivenessProbe探针：用于判断容器是否存活（running状态），如果LivenessProbe探针探测到容器不健康，则kubelet将杀掉该容器，并根据容器的重启策略做相应处理。若一个容器不包含LivenessProbe探针，kubelet认为该容器的LivenessProbe探针返回值用于是“Success”。
 &lt;a class="anchor" href="#-livenessprobe%e6%8e%a2%e9%92%88%e7%94%a8%e4%ba%8e%e5%88%a4%e6%96%ad%e5%ae%b9%e5%99%a8%e6%98%af%e5%90%a6%e5%ad%98%e6%b4%bbrunning%e7%8a%b6%e6%80%81%e5%a6%82%e6%9e%9clivenessprobe%e6%8e%a2%e9%92%88%e6%8e%a2%e6%b5%8b%e5%88%b0%e5%ae%b9%e5%99%a8%e4%b8%8d%e5%81%a5%e5%ba%b7%e5%88%99kubelet%e5%b0%86%e6%9d%80%e6%8e%89%e8%af%a5%e5%ae%b9%e5%99%a8%e5%b9%b6%e6%a0%b9%e6%8d%ae%e5%ae%b9%e5%99%a8%e7%9a%84%e9%87%8d%e5%90%af%e7%ad%96%e7%95%a5%e5%81%9a%e7%9b%b8%e5%ba%94%e5%a4%84%e7%90%86%e8%8b%a5%e4%b8%80%e4%b8%aa%e5%ae%b9%e5%99%a8%e4%b8%8d%e5%8c%85%e5%90%ablivenessprobe%e6%8e%a2%e9%92%88kubelet%e8%ae%a4%e4%b8%ba%e8%af%a5%e5%ae%b9%e5%99%a8%e7%9a%84livenessprobe%e6%8e%a2%e9%92%88%e8%bf%94%e5%9b%9e%e5%80%bc%e7%94%a8%e4%ba%8e%e6%98%afsuccess">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-readineeprobe探针用于判断容器是否启动完成ready状态如果readinessprobe探针探测到失败则pod的状态将被修改endpoint-controller将从service的endpoint中删除包含该容器所在pod的eenpoint">
 • ReadineeProbe探针：用于判断容器是否启动完成（ready状态）。如果ReadinessProbe探针探测到失败，则Pod的状态将被修改。Endpoint Controller将从Service的Endpoint中删除包含该容器所在Pod的Eenpoint。
 &lt;a class="anchor" href="#-readineeprobe%e6%8e%a2%e9%92%88%e7%94%a8%e4%ba%8e%e5%88%a4%e6%96%ad%e5%ae%b9%e5%99%a8%e6%98%af%e5%90%a6%e5%90%af%e5%8a%a8%e5%ae%8c%e6%88%90ready%e7%8a%b6%e6%80%81%e5%a6%82%e6%9e%9creadinessprobe%e6%8e%a2%e9%92%88%e6%8e%a2%e6%b5%8b%e5%88%b0%e5%a4%b1%e8%b4%a5%e5%88%99pod%e7%9a%84%e7%8a%b6%e6%80%81%e5%b0%86%e8%a2%ab%e4%bf%ae%e6%94%b9endpoint-controller%e5%b0%86%e4%bb%8eservice%e7%9a%84endpoint%e4%b8%ad%e5%88%a0%e9%99%a4%e5%8c%85%e5%90%ab%e8%af%a5%e5%ae%b9%e5%99%a8%e6%89%80%e5%9c%a8pod%e7%9a%84eenpoint">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-startupprobe探针启动检查机制应用一些启动缓慢的业务避免业务长时间启动而被上面两类探针kill掉">
 • startupProbe探针：启动检查机制，应用一些启动缓慢的业务，避免业务长时间启动而被上面两类探针kill掉。
 &lt;a class="anchor" href="#-startupprobe%e6%8e%a2%e9%92%88%e5%90%af%e5%8a%a8%e6%a3%80%e6%9f%a5%e6%9c%ba%e5%88%b6%e5%ba%94%e7%94%a8%e4%b8%80%e4%ba%9b%e5%90%af%e5%8a%a8%e7%bc%93%e6%85%a2%e7%9a%84%e4%b8%9a%e5%8a%a1%e9%81%bf%e5%85%8d%e4%b8%9a%e5%8a%a1%e9%95%bf%e6%97%b6%e9%97%b4%e5%90%af%e5%8a%a8%e8%80%8c%e8%a2%ab%e4%b8%8a%e9%9d%a2%e4%b8%a4%e7%b1%bb%e6%8e%a2%e9%92%88kill%e6%8e%89">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h2 id="pod-探针常见方式">
 Pod 探针常见方式
 &lt;a class="anchor" href="#pod-%e6%8e%a2%e9%92%88%e5%b8%b8%e8%a7%81%e6%96%b9%e5%bc%8f">#&lt;/a>
&lt;/h2>
&lt;ul>
&lt;li>
&lt;h5 id="-execaction在容器内执行一个命令若返回码为0则表明容器健康">
 • ExecAction：在容器内执行一个命令，若返回码为0，则表明容器健康。
 &lt;a class="anchor" href="#-execaction%e5%9c%a8%e5%ae%b9%e5%99%a8%e5%86%85%e6%89%a7%e8%a1%8c%e4%b8%80%e4%b8%aa%e5%91%bd%e4%bb%a4%e8%8b%a5%e8%bf%94%e5%9b%9e%e7%a0%81%e4%b8%ba0%e5%88%99%e8%a1%a8%e6%98%8e%e5%ae%b9%e5%99%a8%e5%81%a5%e5%ba%b7">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-tcpsocketaction通过容器的ip地址和端口号执行tcp检查若能建立tcp连接则表明容器健康">
 • TCPSocketAction：通过容器的IP地址和端口号执行TCP检查，若能建立TCP连接，则表明容器健康。
 &lt;a class="anchor" href="#-tcpsocketaction%e9%80%9a%e8%bf%87%e5%ae%b9%e5%99%a8%e7%9a%84ip%e5%9c%b0%e5%9d%80%e5%92%8c%e7%ab%af%e5%8f%a3%e5%8f%b7%e6%89%a7%e8%a1%8ctcp%e6%a3%80%e6%9f%a5%e8%8b%a5%e8%83%bd%e5%bb%ba%e7%ab%8btcp%e8%bf%9e%e6%8e%a5%e5%88%99%e8%a1%a8%e6%98%8e%e5%ae%b9%e5%99%a8%e5%81%a5%e5%ba%b7">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-httpgetaction通过容器的ip地址端口号及路径调用http-get方法若响应的状态码大于等于200且小于400则表明容器健康">
 • HTTPGetAction：通过容器的IP地址、端口号及路径调用HTTP Get方法，若响应的状态码大于等于200且小于400，则表明容器健康。
 &lt;a class="anchor" href="#-httpgetaction%e9%80%9a%e8%bf%87%e5%ae%b9%e5%99%a8%e7%9a%84ip%e5%9c%b0%e5%9d%80%e7%ab%af%e5%8f%a3%e5%8f%b7%e5%8f%8a%e8%b7%af%e5%be%84%e8%b0%83%e7%94%a8http-get%e6%96%b9%e6%b3%95%e8%8b%a5%e5%93%8d%e5%ba%94%e7%9a%84%e7%8a%b6%e6%80%81%e7%a0%81%e5%a4%a7%e4%ba%8e%e7%ad%89%e4%ba%8e200%e4%b8%94%e5%b0%8f%e4%ba%8e400%e5%88%99%e8%a1%a8%e6%98%8e%e5%ae%b9%e5%99%a8%e5%81%a5%e5%ba%b7">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h2 id="pod-常见的调度方式">
 Pod 常见的调度方式
 &lt;a class="anchor" href="#pod-%e5%b8%b8%e8%a7%81%e7%9a%84%e8%b0%83%e5%ba%a6%e6%96%b9%e5%bc%8f">#&lt;/a>
&lt;/h2>
&lt;ul>
&lt;li>
&lt;h5 id="-deployment或rc该调度策略主要功能就是自动部署一个容器应用的多份副本以及持续监控副本的数量在集群内始终维持用户指定的副本数量">
 • Deployment或RC：该调度策略主要功能就是自动部署一个容器应用的多份副本，以及持续监控副本的数量，在集群内始终维持用户指定的副本数量。
 &lt;a class="anchor" href="#-deployment%e6%88%96rc%e8%af%a5%e8%b0%83%e5%ba%a6%e7%ad%96%e7%95%a5%e4%b8%bb%e8%a6%81%e5%8a%9f%e8%83%bd%e5%b0%b1%e6%98%af%e8%87%aa%e5%8a%a8%e9%83%a8%e7%bd%b2%e4%b8%80%e4%b8%aa%e5%ae%b9%e5%99%a8%e5%ba%94%e7%94%a8%e7%9a%84%e5%a4%9a%e4%bb%bd%e5%89%af%e6%9c%ac%e4%bb%a5%e5%8f%8a%e6%8c%81%e7%bb%ad%e7%9b%91%e6%8e%a7%e5%89%af%e6%9c%ac%e7%9a%84%e6%95%b0%e9%87%8f%e5%9c%a8%e9%9b%86%e7%be%a4%e5%86%85%e5%a7%8b%e7%bb%88%e7%bb%b4%e6%8c%81%e7%94%a8%e6%88%b7%e6%8c%87%e5%ae%9a%e7%9a%84%e5%89%af%e6%9c%ac%e6%95%b0%e9%87%8f">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-nodeselector定向调度当需要手动指定将pod调度到特定node上可以通过node的标签label和pod的nodeselector属性相匹配">
 • NodeSelector：定向调度，当需要手动指定将Pod调度到特定Node上，可以通过Node的标签（Label）和Pod的nodeSelector属性相匹配。
 &lt;a class="anchor" href="#-nodeselector%e5%ae%9a%e5%90%91%e8%b0%83%e5%ba%a6%e5%bd%93%e9%9c%80%e8%a6%81%e6%89%8b%e5%8a%a8%e6%8c%87%e5%ae%9a%e5%b0%86pod%e8%b0%83%e5%ba%a6%e5%88%b0%e7%89%b9%e5%ae%9anode%e4%b8%8a%e5%8f%af%e4%bb%a5%e9%80%9a%e8%bf%87node%e7%9a%84%e6%a0%87%e7%ad%belabel%e5%92%8cpod%e7%9a%84nodeselector%e5%b1%9e%e6%80%a7%e7%9b%b8%e5%8c%b9%e9%85%8d">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-nodeaffinity亲和性调度亲和性调度机制极大的扩展了pod的调度能力目前有两种节点亲和力表达">
 • NodeAffinity亲和性调度：亲和性调度机制极大的扩展了Pod的调度能力，目前有两种节点亲和力表达：
 &lt;a class="anchor" href="#-nodeaffinity%e4%ba%b2%e5%92%8c%e6%80%a7%e8%b0%83%e5%ba%a6%e4%ba%b2%e5%92%8c%e6%80%a7%e8%b0%83%e5%ba%a6%e6%9c%ba%e5%88%b6%e6%9e%81%e5%a4%a7%e7%9a%84%e6%89%a9%e5%b1%95%e4%ba%86pod%e7%9a%84%e8%b0%83%e5%ba%a6%e8%83%bd%e5%8a%9b%e7%9b%ae%e5%89%8d%e6%9c%89%e4%b8%a4%e7%a7%8d%e8%8a%82%e7%82%b9%e4%ba%b2%e5%92%8c%e5%8a%9b%e8%a1%a8%e8%be%be">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-requiredduringschedulingignoredduringexecution硬规则必须满足指定的规则调度器才可以调度pod至node上类似nodeselector语法不同">
 • requiredDuringSchedulingIgnoredDuringExecution：硬规则，必须满足指定的规则，调度器才可以调度Pod至Node上（类似nodeSelector，语法不同）。
 &lt;a class="anchor" href="#-requiredduringschedulingignoredduringexecution%e7%a1%ac%e8%a7%84%e5%88%99%e5%bf%85%e9%a1%bb%e6%bb%a1%e8%b6%b3%e6%8c%87%e5%ae%9a%e7%9a%84%e8%a7%84%e5%88%99%e8%b0%83%e5%ba%a6%e5%99%a8%e6%89%8d%e5%8f%af%e4%bb%a5%e8%b0%83%e5%ba%a6pod%e8%87%b3node%e4%b8%8a%e7%b1%bb%e4%bc%bcnodeselector%e8%af%ad%e6%b3%95%e4%b8%8d%e5%90%8c">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-preferredduringschedulingignoredduringexecution软规则优先调度至满足的node的节点但不强求多个优先级规则还可以设置权重值">
 • preferredDuringSchedulingIgnoredDuringExecution：软规则，优先调度至满足的Node的节点，但不强求，多个优先级规则还可以设置权重值。
 &lt;a class="anchor" href="#-preferredduringschedulingignoredduringexecution%e8%bd%af%e8%a7%84%e5%88%99%e4%bc%98%e5%85%88%e8%b0%83%e5%ba%a6%e8%87%b3%e6%bb%a1%e8%b6%b3%e7%9a%84node%e7%9a%84%e8%8a%82%e7%82%b9%e4%bd%86%e4%b8%8d%e5%bc%ba%e6%b1%82%e5%a4%9a%e4%b8%aa%e4%bc%98%e5%85%88%e7%ba%a7%e8%a7%84%e5%88%99%e8%bf%98%e5%8f%af%e4%bb%a5%e8%ae%be%e7%bd%ae%e6%9d%83%e9%87%8d%e5%80%bc">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-taints和tolerations污点和容忍">
 • Taints和Tolerations（污点和容忍）：
 &lt;a class="anchor" href="#-taints%e5%92%8ctolerations%e6%b1%a1%e7%82%b9%e5%92%8c%e5%ae%b9%e5%bf%8d">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;ul>
&lt;li>
&lt;h5 id="-taint使node拒绝特定pod运行">
 • Taint：使Node拒绝特定Pod运行；
 &lt;a class="anchor" href="#-taint%e4%bd%bfnode%e6%8b%92%e7%bb%9d%e7%89%b9%e5%ae%9apod%e8%bf%90%e8%a1%8c">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-toleration为pod的属性表示pod能容忍运行标注了taint的node">
 • Toleration：为Pod的属性，表示Pod能容忍（运行）标注了Taint的Node。
 &lt;a class="anchor" href="#-toleration%e4%b8%bapod%e7%9a%84%e5%b1%9e%e6%80%a7%e8%a1%a8%e7%a4%bapod%e8%83%bd%e5%ae%b9%e5%bf%8d%e8%bf%90%e8%a1%8c%e6%a0%87%e6%b3%a8%e4%ba%86taint%e7%9a%84node">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="deployment升级策略">
 deployment升级策略?
 &lt;a class="anchor" href="#deployment%e5%8d%87%e7%ba%a7%e7%ad%96%e7%95%a5">#&lt;/a>
&lt;/h2>
&lt;ul>
&lt;li>
&lt;h5 id="-在deployment的定义中可以通过specstrategy指定pod更新的策略目前支持两种策略recreate重建和rollingupdate滚动更新默认值为rollingupdate">
 • 在Deployment的定义中，可以通过spec.strategy指定Pod更新的策略，目前支持两种策略：Recreate（重建）和RollingUpdate（滚动更新），默认值为RollingUpdate。
 &lt;a class="anchor" href="#-%e5%9c%a8deployment%e7%9a%84%e5%ae%9a%e4%b9%89%e4%b8%ad%e5%8f%af%e4%bb%a5%e9%80%9a%e8%bf%87specstrategy%e6%8c%87%e5%ae%9apod%e6%9b%b4%e6%96%b0%e7%9a%84%e7%ad%96%e7%95%a5%e7%9b%ae%e5%89%8d%e6%94%af%e6%8c%81%e4%b8%a4%e7%a7%8d%e7%ad%96%e7%95%a5recreate%e9%87%8d%e5%bb%ba%e5%92%8crollingupdate%e6%bb%9a%e5%8a%a8%e6%9b%b4%e6%96%b0%e9%bb%98%e8%ae%a4%e5%80%bc%e4%b8%barollingupdate">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-recreate设置specstrategytyperecreate表示deployment在更新pod时会先杀掉所有正在运行的pod然后创建新的pod">
 • Recreate：设置spec.strategy.type=Recreate，表示Deployment在更新Pod时，会先杀掉所有正在运行的Pod，然后创建新的Pod。
 &lt;a class="anchor" href="#-recreate%e8%ae%be%e7%bd%aespecstrategytyperecreate%e8%a1%a8%e7%a4%badeployment%e5%9c%a8%e6%9b%b4%e6%96%b0pod%e6%97%b6%e4%bc%9a%e5%85%88%e6%9d%80%e6%8e%89%e6%89%80%e6%9c%89%e6%ad%a3%e5%9c%a8%e8%bf%90%e8%a1%8c%e7%9a%84pod%e7%84%b6%e5%90%8e%e5%88%9b%e5%bb%ba%e6%96%b0%e7%9a%84pod">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-rollingupdate设置specstrategytyperollingupdate表示deployment会以滚动更新的方式来逐个更新pod同时可以通过设置specstrategyrollingupdate下的两个参数maxunavailable和maxsurge来控制滚动更新的过程">
 • RollingUpdate：设置spec.strategy.type=RollingUpdate，表示Deployment会以滚动更新的方式来逐个更新Pod。同时，可以通过设置spec.strategy.rollingUpdate下的两个参数（maxUnavailable和maxSurge）来控制滚动更新的过程
 &lt;a class="anchor" href="#-rollingupdate%e8%ae%be%e7%bd%aespecstrategytyperollingupdate%e8%a1%a8%e7%a4%badeployment%e4%bc%9a%e4%bb%a5%e6%bb%9a%e5%8a%a8%e6%9b%b4%e6%96%b0%e7%9a%84%e6%96%b9%e5%bc%8f%e6%9d%a5%e9%80%90%e4%b8%aa%e6%9b%b4%e6%96%b0pod%e5%90%8c%e6%97%b6%e5%8f%af%e4%bb%a5%e9%80%9a%e8%bf%87%e8%ae%be%e7%bd%aespecstrategyrollingupdate%e4%b8%8b%e7%9a%84%e4%b8%a4%e4%b8%aa%e5%8f%82%e6%95%b0maxunavailable%e5%92%8cmaxsurge%e6%9d%a5%e6%8e%a7%e5%88%b6%e6%bb%9a%e5%8a%a8%e6%9b%b4%e6%96%b0%e7%9a%84%e8%bf%87%e7%a8%8b">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h2 id="kubernetes-service类型">
 Kubernetes Service类型?
 &lt;a class="anchor" href="#kubernetes-service%e7%b1%bb%e5%9e%8b">#&lt;/a>
&lt;/h2>
&lt;h5 id="通过创建service可以为一组具有相同功能的容器应用提供一个统一的入口地址并且将请求负载分发到后端的各个容器应用上其主要类型有">
 通过创建Service，可以为一组具有相同功能的容器应用提供一个统一的入口地址，并且将请求负载分发到后端的各个容器应用上。其主要类型有：
 &lt;a class="anchor" href="#%e9%80%9a%e8%bf%87%e5%88%9b%e5%bb%baservice%e5%8f%af%e4%bb%a5%e4%b8%ba%e4%b8%80%e7%bb%84%e5%85%b7%e6%9c%89%e7%9b%b8%e5%90%8c%e5%8a%9f%e8%83%bd%e7%9a%84%e5%ae%b9%e5%99%a8%e5%ba%94%e7%94%a8%e6%8f%90%e4%be%9b%e4%b8%80%e4%b8%aa%e7%bb%9f%e4%b8%80%e7%9a%84%e5%85%a5%e5%8f%a3%e5%9c%b0%e5%9d%80%e5%b9%b6%e4%b8%94%e5%b0%86%e8%af%b7%e6%b1%82%e8%b4%9f%e8%bd%bd%e5%88%86%e5%8f%91%e5%88%b0%e5%90%8e%e7%ab%af%e7%9a%84%e5%90%84%e4%b8%aa%e5%ae%b9%e5%99%a8%e5%ba%94%e7%94%a8%e4%b8%8a%e5%85%b6%e4%b8%bb%e8%a6%81%e7%b1%bb%e5%9e%8b%e6%9c%89">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="-clusterip虚拟的服务ip地址该地址用于kubernetes集群内部的pod访问在node上kube-proxy通过设置的iptables规则进行转发">
 • ClusterIP：虚拟的服务IP地址，该地址用于Kubernetes集群内部的Pod访问，在Node上kube-proxy通过设置的iptables规则进行转发；
 &lt;a class="anchor" href="#-clusterip%e8%99%9a%e6%8b%9f%e7%9a%84%e6%9c%8d%e5%8a%a1ip%e5%9c%b0%e5%9d%80%e8%af%a5%e5%9c%b0%e5%9d%80%e7%94%a8%e4%ba%8ekubernetes%e9%9b%86%e7%be%a4%e5%86%85%e9%83%a8%e7%9a%84pod%e8%ae%bf%e9%97%ae%e5%9c%a8node%e4%b8%8akube-proxy%e9%80%9a%e8%bf%87%e8%ae%be%e7%bd%ae%e7%9a%84iptables%e8%a7%84%e5%88%99%e8%bf%9b%e8%a1%8c%e8%bd%ac%e5%8f%91">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-nodeport使用宿主机的端口使能够访问各node的外部客户端通过node的ip地址和端口号就能访问服务">
 • NodePort：使用宿主机的端口，使能够访问各Node的外部客户端通过Node的IP地址和端口号就能访问服务；
 &lt;a class="anchor" href="#-nodeport%e4%bd%bf%e7%94%a8%e5%ae%bf%e4%b8%bb%e6%9c%ba%e7%9a%84%e7%ab%af%e5%8f%a3%e4%bd%bf%e8%83%bd%e5%a4%9f%e8%ae%bf%e9%97%ae%e5%90%84node%e7%9a%84%e5%a4%96%e9%83%a8%e5%ae%a2%e6%88%b7%e7%ab%af%e9%80%9a%e8%bf%87node%e7%9a%84ip%e5%9c%b0%e5%9d%80%e5%92%8c%e7%ab%af%e5%8f%a3%e5%8f%b7%e5%b0%b1%e8%83%bd%e8%ae%bf%e9%97%ae%e6%9c%8d%e5%8a%a1">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-loadbalancer使用外接负载均衡器完成到服务的负载分发需要在specstatusloadbalancer字段指定外部负载均衡器的ip地址通常用于公有云">
 • LoadBalancer：使用外接负载均衡器完成到服务的负载分发，需要在spec.status.loadBalancer字段指定外部负载均衡器的IP地址，通常用于公有云。
 &lt;a class="anchor" href="#-loadbalancer%e4%bd%bf%e7%94%a8%e5%a4%96%e6%8e%a5%e8%b4%9f%e8%bd%bd%e5%9d%87%e8%a1%a1%e5%99%a8%e5%ae%8c%e6%88%90%e5%88%b0%e6%9c%8d%e5%8a%a1%e7%9a%84%e8%b4%9f%e8%bd%bd%e5%88%86%e5%8f%91%e9%9c%80%e8%a6%81%e5%9c%a8specstatusloadbalancer%e5%ad%97%e6%ae%b5%e6%8c%87%e5%ae%9a%e5%a4%96%e9%83%a8%e8%b4%9f%e8%bd%bd%e5%9d%87%e8%a1%a1%e5%99%a8%e7%9a%84ip%e5%9c%b0%e5%9d%80%e9%80%9a%e5%b8%b8%e7%94%a8%e4%ba%8e%e5%85%ac%e6%9c%89%e4%ba%91">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h2 id="service分发后端的策略">
 Service分发后端的策略?
 &lt;a class="anchor" href="#service%e5%88%86%e5%8f%91%e5%90%8e%e7%ab%af%e7%9a%84%e7%ad%96%e7%95%a5">#&lt;/a>
&lt;/h2>
&lt;h5 id="service负载分发的策略有roundrobin和sessionaffinity">
 Service负载分发的策略有：RoundRobin和SessionAffinity
 &lt;a class="anchor" href="#service%e8%b4%9f%e8%bd%bd%e5%88%86%e5%8f%91%e7%9a%84%e7%ad%96%e7%95%a5%e6%9c%89roundrobin%e5%92%8csessionaffinity">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="-roundrobin默认为轮询模式即轮询将请求转发到后端的各个pod上">
 • RoundRobin：默认为轮询模式，即轮询将请求转发到后端的各个Pod上。
 &lt;a class="anchor" href="#-roundrobin%e9%bb%98%e8%ae%a4%e4%b8%ba%e8%bd%ae%e8%af%a2%e6%a8%a1%e5%bc%8f%e5%8d%b3%e8%bd%ae%e8%af%a2%e5%b0%86%e8%af%b7%e6%b1%82%e8%bd%ac%e5%8f%91%e5%88%b0%e5%90%8e%e7%ab%af%e7%9a%84%e5%90%84%e4%b8%aapod%e4%b8%8a">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-sessionaffinity基于客户端ip地址进行会话保持的模式即第1次将某个客户端发起的请求转发到后端的某个pod上之后从相同的客户端发起的请求都将被转发到后端相同的pod上">
 • SessionAffinity：基于客户端IP地址进行会话保持的模式，即第1次将某个客户端发起的请求转发到后端的某个Pod上，之后从相同的客户端发起的请求都将被转发到后端相同的Pod上。
 &lt;a class="anchor" href="#-sessionaffinity%e5%9f%ba%e4%ba%8e%e5%ae%a2%e6%88%b7%e7%ab%afip%e5%9c%b0%e5%9d%80%e8%bf%9b%e8%a1%8c%e4%bc%9a%e8%af%9d%e4%bf%9d%e6%8c%81%e7%9a%84%e6%a8%a1%e5%bc%8f%e5%8d%b3%e7%ac%ac1%e6%ac%a1%e5%b0%86%e6%9f%90%e4%b8%aa%e5%ae%a2%e6%88%b7%e7%ab%af%e5%8f%91%e8%b5%b7%e7%9a%84%e8%af%b7%e6%b1%82%e8%bd%ac%e5%8f%91%e5%88%b0%e5%90%8e%e7%ab%af%e7%9a%84%e6%9f%90%e4%b8%aapod%e4%b8%8a%e4%b9%8b%e5%90%8e%e4%bb%8e%e7%9b%b8%e5%90%8c%e7%9a%84%e5%ae%a2%e6%88%b7%e7%ab%af%e5%8f%91%e8%b5%b7%e7%9a%84%e8%af%b7%e6%b1%82%e9%83%bd%e5%b0%86%e8%a2%ab%e8%bd%ac%e5%8f%91%e5%88%b0%e5%90%8e%e7%ab%af%e7%9b%b8%e5%90%8c%e7%9a%84pod%e4%b8%8a">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h2 id="kubernetes外部如何访问集群内的服务">
 Kubernetes外部如何访问集群内的服务?
 &lt;a class="anchor" href="#kubernetes%e5%a4%96%e9%83%a8%e5%a6%82%e4%bd%95%e8%ae%bf%e9%97%ae%e9%9b%86%e7%be%a4%e5%86%85%e7%9a%84%e6%9c%8d%e5%8a%a1">#&lt;/a>
&lt;/h2>
&lt;ul>
&lt;li>
&lt;h5 id="-映射pod到物理机将pod端口号映射到宿主机即在pod中采用hostport方式以使客户端应用能够通过物理机访问容器应用">
 • 映射Pod到物理机：将Pod端口号映射到宿主机，即在Pod中采用hostPort方式，以使客户端应用能够通过物理机访问容器应用。
 &lt;a class="anchor" href="#-%e6%98%a0%e5%b0%84pod%e5%88%b0%e7%89%a9%e7%90%86%e6%9c%ba%e5%b0%86pod%e7%ab%af%e5%8f%a3%e5%8f%b7%e6%98%a0%e5%b0%84%e5%88%b0%e5%ae%bf%e4%b8%bb%e6%9c%ba%e5%8d%b3%e5%9c%a8pod%e4%b8%ad%e9%87%87%e7%94%a8hostport%e6%96%b9%e5%bc%8f%e4%bb%a5%e4%bd%bf%e5%ae%a2%e6%88%b7%e7%ab%af%e5%ba%94%e7%94%a8%e8%83%bd%e5%a4%9f%e9%80%9a%e8%bf%87%e7%89%a9%e7%90%86%e6%9c%ba%e8%ae%bf%e9%97%ae%e5%ae%b9%e5%99%a8%e5%ba%94%e7%94%a8">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-映射service到物理机将service端口号映射到宿主机即在service中采用nodeport方式以使客户端应用能够通过物理机访问容器应用">
 • 映射Service到物理机：将Service端口号映射到宿主机，即在Service中采用nodePort方式，以使客户端应用能够通过物理机访问容器应用。
 &lt;a class="anchor" href="#-%e6%98%a0%e5%b0%84service%e5%88%b0%e7%89%a9%e7%90%86%e6%9c%ba%e5%b0%86service%e7%ab%af%e5%8f%a3%e5%8f%b7%e6%98%a0%e5%b0%84%e5%88%b0%e5%ae%bf%e4%b8%bb%e6%9c%ba%e5%8d%b3%e5%9c%a8service%e4%b8%ad%e9%87%87%e7%94%a8nodeport%e6%96%b9%e5%bc%8f%e4%bb%a5%e4%bd%bf%e5%ae%a2%e6%88%b7%e7%ab%af%e5%ba%94%e7%94%a8%e8%83%bd%e5%a4%9f%e9%80%9a%e8%bf%87%e7%89%a9%e7%90%86%e6%9c%ba%e8%ae%bf%e9%97%ae%e5%ae%b9%e5%99%a8%e5%ba%94%e7%94%a8">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-映射sercie到loadbalancer通过设置loadbalancer映射到云服务商提供的loadbalancer地址这种用法仅用于在公有云服务提供商的云平台上设置service的场景">
 • 映射Sercie到LoadBalancer：通过设置LoadBalancer映射到云服务商提供的LoadBalancer地址。这种用法仅用于在公有云服务提供商的云平台上设置Service的场景。
 &lt;a class="anchor" href="#-%e6%98%a0%e5%b0%84sercie%e5%88%b0loadbalancer%e9%80%9a%e8%bf%87%e8%ae%be%e7%bd%aeloadbalancer%e6%98%a0%e5%b0%84%e5%88%b0%e4%ba%91%e6%9c%8d%e5%8a%a1%e5%95%86%e6%8f%90%e4%be%9b%e7%9a%84loadbalancer%e5%9c%b0%e5%9d%80%e8%bf%99%e7%a7%8d%e7%94%a8%e6%b3%95%e4%bb%85%e7%94%a8%e4%ba%8e%e5%9c%a8%e5%85%ac%e6%9c%89%e4%ba%91%e6%9c%8d%e5%8a%a1%e6%8f%90%e4%be%9b%e5%95%86%e7%9a%84%e4%ba%91%e5%b9%b3%e5%8f%b0%e4%b8%8a%e8%ae%be%e7%bd%aeservice%e7%9a%84%e5%9c%ba%e6%99%af">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h2 id="kubernetes-ingress">
 Kubernetes ingress?
 &lt;a class="anchor" href="#kubernetes-ingress">#&lt;/a>
&lt;/h2>
&lt;ul>
&lt;li>
&lt;h5 id="-kubernetes的ingress资源对象用于将不同url的访问请求转发到后端不同的service以实现http层的业务路由机制">
 • Kubernetes的Ingress资源对象，用于将不同URL的访问请求转发到后端不同的Service，以实现HTTP层的业务路由机制。
 &lt;a class="anchor" href="#-kubernetes%e7%9a%84ingress%e8%b5%84%e6%ba%90%e5%af%b9%e8%b1%a1%e7%94%a8%e4%ba%8e%e5%b0%86%e4%b8%8d%e5%90%8curl%e7%9a%84%e8%ae%bf%e9%97%ae%e8%af%b7%e6%b1%82%e8%bd%ac%e5%8f%91%e5%88%b0%e5%90%8e%e7%ab%af%e4%b8%8d%e5%90%8c%e7%9a%84service%e4%bb%a5%e5%ae%9e%e7%8e%b0http%e5%b1%82%e7%9a%84%e4%b8%9a%e5%8a%a1%e8%b7%af%e7%94%b1%e6%9c%ba%e5%88%b6">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-kubernetes使用了ingress策略和ingress-controller两者结合并实现了一个完整的ingress负载均衡器使用ingress进行负载分发时ingress-controller基于ingress规则将客户端请求直接转发到service对应的后端endpointpod上从而跳过kube-proxy的转发功能kube-proxy不再起作用全过程为ingress-controller--ingress-规则---services">
 • Kubernetes使用了Ingress策略和Ingress Controller，两者结合并实现了一个完整的Ingress负载均衡器。使用Ingress进行负载分发时，Ingress Controller基于Ingress规则将客户端请求直接转发到Service对应的后端Endpoint（Pod）上，从而跳过kube-proxy的转发功能，kube-proxy不再起作用，全过程为：ingress controller + ingress 规则 &amp;mdash;-&amp;gt; services。
 &lt;a class="anchor" href="#-kubernetes%e4%bd%bf%e7%94%a8%e4%ba%86ingress%e7%ad%96%e7%95%a5%e5%92%8cingress-controller%e4%b8%a4%e8%80%85%e7%bb%93%e5%90%88%e5%b9%b6%e5%ae%9e%e7%8e%b0%e4%ba%86%e4%b8%80%e4%b8%aa%e5%ae%8c%e6%95%b4%e7%9a%84ingress%e8%b4%9f%e8%bd%bd%e5%9d%87%e8%a1%a1%e5%99%a8%e4%bd%bf%e7%94%a8ingress%e8%bf%9b%e8%a1%8c%e8%b4%9f%e8%bd%bd%e5%88%86%e5%8f%91%e6%97%b6ingress-controller%e5%9f%ba%e4%ba%8eingress%e8%a7%84%e5%88%99%e5%b0%86%e5%ae%a2%e6%88%b7%e7%ab%af%e8%af%b7%e6%b1%82%e7%9b%b4%e6%8e%a5%e8%bd%ac%e5%8f%91%e5%88%b0service%e5%af%b9%e5%ba%94%e7%9a%84%e5%90%8e%e7%ab%afendpointpod%e4%b8%8a%e4%bb%8e%e8%80%8c%e8%b7%b3%e8%bf%87kube-proxy%e7%9a%84%e8%bd%ac%e5%8f%91%e5%8a%9f%e8%83%bdkube-proxy%e4%b8%8d%e5%86%8d%e8%b5%b7%e4%bd%9c%e7%94%a8%e5%85%a8%e8%bf%87%e7%a8%8b%e4%b8%baingress-controller--ingress-%e8%a7%84%e5%88%99---services">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-同时当ingress-controller提供的是对外服务则实际上实现的是边缘路由器的功能">
 • 同时当Ingress Controller提供的是对外服务，则实际上实现的是边缘路由器的功能。
 &lt;a class="anchor" href="#-%e5%90%8c%e6%97%b6%e5%bd%93ingress-controller%e6%8f%90%e4%be%9b%e7%9a%84%e6%98%af%e5%af%b9%e5%a4%96%e6%9c%8d%e5%8a%a1%e5%88%99%e5%ae%9e%e9%99%85%e4%b8%8a%e5%ae%9e%e7%8e%b0%e7%9a%84%e6%98%af%e8%be%b9%e7%bc%98%e8%b7%af%e7%94%b1%e5%99%a8%e7%9a%84%e5%8a%9f%e8%83%bd">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h2 id="kubernetes镜像的下载策略">
 Kubernetes镜像的下载策略?
 &lt;a class="anchor" href="#kubernetes%e9%95%9c%e5%83%8f%e7%9a%84%e4%b8%8b%e8%bd%bd%e7%ad%96%e7%95%a5">#&lt;/a>
&lt;/h2>
&lt;h5 id="k8s的镜像下载策略有三种alwaysneverifnotpresent">
 K8s的镜像下载策略有三种：Always、Never、IFNotPresent。
 &lt;a class="anchor" href="#k8s%e7%9a%84%e9%95%9c%e5%83%8f%e4%b8%8b%e8%bd%bd%e7%ad%96%e7%95%a5%e6%9c%89%e4%b8%89%e7%a7%8dalwaysneverifnotpresent">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="-always镜像标签为latest时总是从指定的仓库中获取镜像">
 • Always：镜像标签为latest时，总是从指定的仓库中获取镜像。
 &lt;a class="anchor" href="#-always%e9%95%9c%e5%83%8f%e6%a0%87%e7%ad%be%e4%b8%balatest%e6%97%b6%e6%80%bb%e6%98%af%e4%bb%8e%e6%8c%87%e5%ae%9a%e7%9a%84%e4%bb%93%e5%ba%93%e4%b8%ad%e8%8e%b7%e5%8f%96%e9%95%9c%e5%83%8f">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-never禁止从仓库中下载镜像也就是说只能使用本地镜像">
 • Never：禁止从仓库中下载镜像，也就是说只能使用本地镜像。
 &lt;a class="anchor" href="#-never%e7%a6%81%e6%ad%a2%e4%bb%8e%e4%bb%93%e5%ba%93%e4%b8%ad%e4%b8%8b%e8%bd%bd%e9%95%9c%e5%83%8f%e4%b9%9f%e5%b0%b1%e6%98%af%e8%af%b4%e5%8f%aa%e8%83%bd%e4%bd%bf%e7%94%a8%e6%9c%ac%e5%9c%b0%e9%95%9c%e5%83%8f">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-ifnotpresent仅当本地没有对应镜像时才从目标仓库中下载">
 • IfNotPresent：仅当本地没有对应镜像时，才从目标仓库中下载。
 &lt;a class="anchor" href="#-ifnotpresent%e4%bb%85%e5%bd%93%e6%9c%ac%e5%9c%b0%e6%b2%a1%e6%9c%89%e5%af%b9%e5%ba%94%e9%95%9c%e5%83%8f%e6%97%b6%e6%89%8d%e4%bb%8e%e7%9b%ae%e6%a0%87%e4%bb%93%e5%ba%93%e4%b8%ad%e4%b8%8b%e8%bd%bd">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h5 id="默认的镜像下载策略是当镜像标签是latest时默认策略是always当镜像标签是自定义时也就是标签不是latest那么默认策略是ifnotpresent">
 默认的镜像下载策略是：当镜像标签是latest时，默认策略是Always；当镜像标签是自定义时（也就是标签不是latest），那么默认策略是IfNotPresent。
 &lt;a class="anchor" href="#%e9%bb%98%e8%ae%a4%e7%9a%84%e9%95%9c%e5%83%8f%e4%b8%8b%e8%bd%bd%e7%ad%96%e7%95%a5%e6%98%af%e5%bd%93%e9%95%9c%e5%83%8f%e6%a0%87%e7%ad%be%e6%98%aflatest%e6%97%b6%e9%bb%98%e8%ae%a4%e7%ad%96%e7%95%a5%e6%98%afalways%e5%bd%93%e9%95%9c%e5%83%8f%e6%a0%87%e7%ad%be%e6%98%af%e8%87%aa%e5%ae%9a%e4%b9%89%e6%97%b6%e4%b9%9f%e5%b0%b1%e6%98%af%e6%a0%87%e7%ad%be%e4%b8%8d%e6%98%aflatest%e9%82%a3%e4%b9%88%e9%bb%98%e8%ae%a4%e7%ad%96%e7%95%a5%e6%98%afifnotpresent">#&lt;/a>
&lt;/h5>
&lt;h2 id="kubernetes-kubelet的作用">
 Kubernetes kubelet的作用?
 &lt;a class="anchor" href="#kubernetes-kubelet%e7%9a%84%e4%bd%9c%e7%94%a8">#&lt;/a>
&lt;/h2>
&lt;ul>
&lt;li>
&lt;h5 id="-在kubernetes集群中在每个node又称worker上都会启动一个kubelet服务进程该进程用于处理master下发到本节点的任务管理pod及pod中的容器每个kubelet进程都会在api-server上注册节点自身的信息定期向master汇报节点资源的使用情况并通过cadvisor监控容器和节点资源">
 • 在Kubernetes集群中，在每个Node（又称Worker）上都会启动一个kubelet服务进程。该进程用于处理Master下发到本节点的任务，管理Pod及Pod中的容器。每个kubelet进程都会在API Server上注册节点自身的信息，定期向Master汇报节点资源的使用情况，并通过cAdvisor监控容器和节点资源。
 &lt;a class="anchor" href="#-%e5%9c%a8kubernetes%e9%9b%86%e7%be%a4%e4%b8%ad%e5%9c%a8%e6%af%8f%e4%b8%aanode%e5%8f%88%e7%a7%b0worker%e4%b8%8a%e9%83%bd%e4%bc%9a%e5%90%af%e5%8a%a8%e4%b8%80%e4%b8%aakubelet%e6%9c%8d%e5%8a%a1%e8%bf%9b%e7%a8%8b%e8%af%a5%e8%bf%9b%e7%a8%8b%e7%94%a8%e4%ba%8e%e5%a4%84%e7%90%86master%e4%b8%8b%e5%8f%91%e5%88%b0%e6%9c%ac%e8%8a%82%e7%82%b9%e7%9a%84%e4%bb%bb%e5%8a%a1%e7%ae%a1%e7%90%86pod%e5%8f%8apod%e4%b8%ad%e7%9a%84%e5%ae%b9%e5%99%a8%e6%af%8f%e4%b8%aakubelet%e8%bf%9b%e7%a8%8b%e9%83%bd%e4%bc%9a%e5%9c%a8api-server%e4%b8%8a%e6%b3%a8%e5%86%8c%e8%8a%82%e7%82%b9%e8%87%aa%e8%ba%ab%e7%9a%84%e4%bf%a1%e6%81%af%e5%ae%9a%e6%9c%9f%e5%90%91master%e6%b1%87%e6%8a%a5%e8%8a%82%e7%82%b9%e8%b5%84%e6%ba%90%e7%9a%84%e4%bd%bf%e7%94%a8%e6%83%85%e5%86%b5%e5%b9%b6%e9%80%9a%e8%bf%87cadvisor%e7%9b%91%e6%8e%a7%e5%ae%b9%e5%99%a8%e5%92%8c%e8%8a%82%e7%82%b9%e8%b5%84%e6%ba%90">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h2 id="secret有哪些使用方式">
 Secret有哪些使用方式?
 &lt;a class="anchor" href="#secret%e6%9c%89%e5%93%aa%e4%ba%9b%e4%bd%bf%e7%94%a8%e6%96%b9%e5%bc%8f">#&lt;/a>
&lt;/h2>
&lt;ul>
&lt;li>
&lt;h5 id="-创建完secret之后可通过如下三种方式使用">
 • 创建完secret之后，可通过如下三种方式使用：
 &lt;a class="anchor" href="#-%e5%88%9b%e5%bb%ba%e5%ae%8csecret%e4%b9%8b%e5%90%8e%e5%8f%af%e9%80%9a%e8%bf%87%e5%a6%82%e4%b8%8b%e4%b8%89%e7%a7%8d%e6%96%b9%e5%bc%8f%e4%bd%bf%e7%94%a8">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-在创建pod时通过为pod指定service-account来自动使用该secret">
 • 在创建Pod时，通过为Pod指定Service Account来自动使用该Secret。
 &lt;a class="anchor" href="#-%e5%9c%a8%e5%88%9b%e5%bb%bapod%e6%97%b6%e9%80%9a%e8%bf%87%e4%b8%bapod%e6%8c%87%e5%ae%9aservice-account%e6%9d%a5%e8%87%aa%e5%8a%a8%e4%bd%bf%e7%94%a8%e8%af%a5secret">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-通过挂载该secret到pod来使用它">
 • 通过挂载该Secret到Pod来使用它。
 &lt;a class="anchor" href="#-%e9%80%9a%e8%bf%87%e6%8c%82%e8%bd%bd%e8%af%a5secret%e5%88%b0pod%e6%9d%a5%e4%bd%bf%e7%94%a8%e5%ae%83">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-在docker镜像下载时使用通过指定pod的spcimagepullsecrets来引用它">
 • 在Docker镜像下载时使用，通过指定Pod的spc.ImagePullSecrets来引用它。
 &lt;a class="anchor" href="#-%e5%9c%a8docker%e9%95%9c%e5%83%8f%e4%b8%8b%e8%bd%bd%e6%97%b6%e4%bd%bf%e7%94%a8%e9%80%9a%e8%bf%87%e6%8c%87%e5%ae%9apod%e7%9a%84spcimagepullsecrets%e6%9d%a5%e5%bc%95%e7%94%a8%e5%ae%83">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h2 id="kubernetes-cni模型">
 Kubernetes CNI模型?
 &lt;a class="anchor" href="#kubernetes-cni%e6%a8%a1%e5%9e%8b">#&lt;/a>
&lt;/h2>
&lt;ul>
&lt;li>
&lt;h5 id="-cni提供了一种应用容器的插件化网络解决方案定义对容器网络进行操作和配置的规范通过插件的形式对cni接口进行实现cni仅关注在创建容器时分配网络资源和在销毁容器时删除网络资源在cni模型中只涉及两个概念容器和网络">
 • CNI提供了一种应用容器的插件化网络解决方案，定义对容器网络进行操作和配置的规范，通过插件的形式对CNI接口进行实现。CNI仅关注在创建容器时分配网络资源，和在销毁容器时删除网络资源。在CNI模型中只涉及两个概念：容器和网络。
 &lt;a class="anchor" href="#-cni%e6%8f%90%e4%be%9b%e4%ba%86%e4%b8%80%e7%a7%8d%e5%ba%94%e7%94%a8%e5%ae%b9%e5%99%a8%e7%9a%84%e6%8f%92%e4%bb%b6%e5%8c%96%e7%bd%91%e7%bb%9c%e8%a7%a3%e5%86%b3%e6%96%b9%e6%a1%88%e5%ae%9a%e4%b9%89%e5%af%b9%e5%ae%b9%e5%99%a8%e7%bd%91%e7%bb%9c%e8%bf%9b%e8%a1%8c%e6%93%8d%e4%bd%9c%e5%92%8c%e9%85%8d%e7%bd%ae%e7%9a%84%e8%a7%84%e8%8c%83%e9%80%9a%e8%bf%87%e6%8f%92%e4%bb%b6%e7%9a%84%e5%bd%a2%e5%bc%8f%e5%af%b9cni%e6%8e%a5%e5%8f%a3%e8%bf%9b%e8%a1%8c%e5%ae%9e%e7%8e%b0cni%e4%bb%85%e5%85%b3%e6%b3%a8%e5%9c%a8%e5%88%9b%e5%bb%ba%e5%ae%b9%e5%99%a8%e6%97%b6%e5%88%86%e9%85%8d%e7%bd%91%e7%bb%9c%e8%b5%84%e6%ba%90%e5%92%8c%e5%9c%a8%e9%94%80%e6%af%81%e5%ae%b9%e5%99%a8%e6%97%b6%e5%88%a0%e9%99%a4%e7%bd%91%e7%bb%9c%e8%b5%84%e6%ba%90%e5%9c%a8cni%e6%a8%a1%e5%9e%8b%e4%b8%ad%e5%8f%aa%e6%b6%89%e5%8f%8a%e4%b8%a4%e4%b8%aa%e6%a6%82%e5%bf%b5%e5%ae%b9%e5%99%a8%e5%92%8c%e7%bd%91%e7%bb%9c">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-容器container是拥有独立linux网络命名空间的环境例如使用docker或rkt创建的容器容器需要拥有自己的linux网络命名空间这是加入网络的必要条件">
 • 容器（Container）：是拥有独立Linux网络命名空间的环境，例如使用Docker或rkt创建的容器。容器需要拥有自己的Linux网络命名空间，这是加入网络的必要条件。
 &lt;a class="anchor" href="#-%e5%ae%b9%e5%99%a8container%e6%98%af%e6%8b%a5%e6%9c%89%e7%8b%ac%e7%ab%8blinux%e7%bd%91%e7%bb%9c%e5%91%bd%e5%90%8d%e7%a9%ba%e9%97%b4%e7%9a%84%e7%8e%af%e5%a2%83%e4%be%8b%e5%a6%82%e4%bd%bf%e7%94%a8docker%e6%88%96rkt%e5%88%9b%e5%bb%ba%e7%9a%84%e5%ae%b9%e5%99%a8%e5%ae%b9%e5%99%a8%e9%9c%80%e8%a6%81%e6%8b%a5%e6%9c%89%e8%87%aa%e5%b7%b1%e7%9a%84linux%e7%bd%91%e7%bb%9c%e5%91%bd%e5%90%8d%e7%a9%ba%e9%97%b4%e8%bf%99%e6%98%af%e5%8a%a0%e5%85%a5%e7%bd%91%e7%bb%9c%e7%9a%84%e5%bf%85%e8%a6%81%e6%9d%a1%e4%bb%b6">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-网络network表示可以互连的一组实体这些实体拥有各自独立唯一的ip地址可以是容器物理机或者其他网络设备比如路由器等">
 • 网络（Network）：表示可以互连的一组实体，这些实体拥有各自独立、唯一的IP地址，可以是容器、物理机或者其他网络设备（比如路由器）等。
 &lt;a class="anchor" href="#-%e7%bd%91%e7%bb%9cnetwork%e8%a1%a8%e7%a4%ba%e5%8f%af%e4%bb%a5%e4%ba%92%e8%bf%9e%e7%9a%84%e4%b8%80%e7%bb%84%e5%ae%9e%e4%bd%93%e8%bf%99%e4%ba%9b%e5%ae%9e%e4%bd%93%e6%8b%a5%e6%9c%89%e5%90%84%e8%87%aa%e7%8b%ac%e7%ab%8b%e5%94%af%e4%b8%80%e7%9a%84ip%e5%9c%b0%e5%9d%80%e5%8f%af%e4%bb%a5%e6%98%af%e5%ae%b9%e5%99%a8%e7%89%a9%e7%90%86%e6%9c%ba%e6%88%96%e8%80%85%e5%85%b6%e4%bb%96%e7%bd%91%e7%bb%9c%e8%ae%be%e5%a4%87%e6%af%94%e5%a6%82%e8%b7%af%e7%94%b1%e5%99%a8%e7%ad%89">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-对容器网络的设置和操作都通过插件plugin进行具体实现cni插件包括两种类型cni-plugin和ipamip-address-managementplugincni-plugin负责为容器配置网络资源ipam-plugin负责对容器的ip地址进行分配和管理ipam-plugin作为cni-plugin的一部分与cni-plugin协同工作">
 • 对容器网络的设置和操作都通过插件（Plugin）进行具体实现，CNI插件包括两种类型：CNI Plugin和IPAM（IP Address Management）Plugin。CNI Plugin负责为容器配置网络资源，IPAM Plugin负责对容器的IP地址进行分配和管理。IPAM Plugin作为CNI Plugin的一部分，与CNI Plugin协同工作。
 &lt;a class="anchor" href="#-%e5%af%b9%e5%ae%b9%e5%99%a8%e7%bd%91%e7%bb%9c%e7%9a%84%e8%ae%be%e7%bd%ae%e5%92%8c%e6%93%8d%e4%bd%9c%e9%83%bd%e9%80%9a%e8%bf%87%e6%8f%92%e4%bb%b6plugin%e8%bf%9b%e8%a1%8c%e5%85%b7%e4%bd%93%e5%ae%9e%e7%8e%b0cni%e6%8f%92%e4%bb%b6%e5%8c%85%e6%8b%ac%e4%b8%a4%e7%a7%8d%e7%b1%bb%e5%9e%8bcni-plugin%e5%92%8cipamip-address-managementplugincni-plugin%e8%b4%9f%e8%b4%a3%e4%b8%ba%e5%ae%b9%e5%99%a8%e9%85%8d%e7%bd%ae%e7%bd%91%e7%bb%9c%e8%b5%84%e6%ba%90ipam-plugin%e8%b4%9f%e8%b4%a3%e5%af%b9%e5%ae%b9%e5%99%a8%e7%9a%84ip%e5%9c%b0%e5%9d%80%e8%bf%9b%e8%a1%8c%e5%88%86%e9%85%8d%e5%92%8c%e7%ae%a1%e7%90%86ipam-plugin%e4%bd%9c%e4%b8%bacni-plugin%e7%9a%84%e4%b8%80%e9%83%a8%e5%88%86%e4%b8%8ecni-plugin%e5%8d%8f%e5%90%8c%e5%b7%a5%e4%bd%9c">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h2 id="kubernetes-pv和pvc">
 Kubernetes PV和PVC?
 &lt;a class="anchor" href="#kubernetes-pv%e5%92%8cpvc">#&lt;/a>
&lt;/h2>
&lt;ul>
&lt;li>
&lt;h5 id="-pv是对底层网络共享存储的抽象将共享存储定义为一种资源">
 • PV是对底层网络共享存储的抽象，将共享存储定义为一种“资源”。
 &lt;a class="anchor" href="#-pv%e6%98%af%e5%af%b9%e5%ba%95%e5%b1%82%e7%bd%91%e7%bb%9c%e5%85%b1%e4%ba%ab%e5%ad%98%e5%82%a8%e7%9a%84%e6%8a%bd%e8%b1%a1%e5%b0%86%e5%85%b1%e4%ba%ab%e5%ad%98%e5%82%a8%e5%ae%9a%e4%b9%89%e4%b8%ba%e4%b8%80%e7%a7%8d%e8%b5%84%e6%ba%90">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-pvc则是用户对存储资源的一个申请">
 • PVC则是用户对存储资源的一个“申请”。
 &lt;a class="anchor" href="#-pvc%e5%88%99%e6%98%af%e7%94%a8%e6%88%b7%e5%af%b9%e5%ad%98%e5%82%a8%e8%b5%84%e6%ba%90%e7%9a%84%e4%b8%80%e4%b8%aa%e7%94%b3%e8%af%b7">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h2 id="pv生命周期内的阶段">
 PV生命周期内的阶段?
 &lt;a class="anchor" href="#pv%e7%94%9f%e5%91%bd%e5%91%a8%e6%9c%9f%e5%86%85%e7%9a%84%e9%98%b6%e6%ae%b5">#&lt;/a>
&lt;/h2>
&lt;h5 id="某个pv在生命周期中可能处于以下4个阶段phaes之一">
 某个PV在生命周期中可能处于以下4个阶段（Phaes）之一。
 &lt;a class="anchor" href="#%e6%9f%90%e4%b8%aapv%e5%9c%a8%e7%94%9f%e5%91%bd%e5%91%a8%e6%9c%9f%e4%b8%ad%e5%8f%af%e8%83%bd%e5%a4%84%e4%ba%8e%e4%bb%a5%e4%b8%8b4%e4%b8%aa%e9%98%b6%e6%ae%b5phaes%e4%b9%8b%e4%b8%80">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="-available可用状态还未与某个pvc绑定">
 • Available：可用状态，还未与某个PVC绑定。
 &lt;a class="anchor" href="#-available%e5%8f%af%e7%94%a8%e7%8a%b6%e6%80%81%e8%bf%98%e6%9c%aa%e4%b8%8e%e6%9f%90%e4%b8%aapvc%e7%bb%91%e5%ae%9a">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-bound已与某个pvc绑定">
 • Bound：已与某个PVC绑定。
 &lt;a class="anchor" href="#-bound%e5%b7%b2%e4%b8%8e%e6%9f%90%e4%b8%aapvc%e7%bb%91%e5%ae%9a">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-released绑定的pvc已经删除资源已释放但没有被集群回收">
 • Released：绑定的PVC已经删除，资源已释放，但没有被集群回收。
 &lt;a class="anchor" href="#-released%e7%bb%91%e5%ae%9a%e7%9a%84pvc%e5%b7%b2%e7%bb%8f%e5%88%a0%e9%99%a4%e8%b5%84%e6%ba%90%e5%b7%b2%e9%87%8a%e6%94%be%e4%bd%86%e6%b2%a1%e6%9c%89%e8%a2%ab%e9%9b%86%e7%be%a4%e5%9b%9e%e6%94%b6">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-failed自动资源回收失败">
 • Failed：自动资源回收失败。
 &lt;a class="anchor" href="#-failed%e8%87%aa%e5%8a%a8%e8%b5%84%e6%ba%90%e5%9b%9e%e6%94%b6%e5%a4%b1%e8%b4%a5">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h2 id="calico-网络模式">
 calico 网络模式
 &lt;a class="anchor" href="#calico-%e7%bd%91%e7%bb%9c%e6%a8%a1%e5%bc%8f">#&lt;/a>
&lt;/h2>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th>&lt;strong>模式&lt;/strong>&lt;/th>
 &lt;th>&lt;strong>数据包封包&lt;/strong>&lt;/th>
 &lt;th>&lt;strong>优点&lt;/strong>&lt;/th>
 &lt;th>&lt;strong>缺点&lt;/strong>&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td>vxlan&lt;/td>
 &lt;td>封包， 在vxlan设备上将pod发来的数据包源、目的mac替换为本机vxlan网卡和对端节点vxlan网卡的mac。外层udp目的ip地址根据路由和对端vxlan的mac查fdb表获取&lt;/td>
 &lt;td>只要k8s节点间三层互通， 可以跨网段， 对主机网关路由没有特殊要求。各个node节点通过vxlan设备实现基于三层的”二层”互通, 三层即vxlan包封装在udp数据包中， 要求udp在k8s节点间三层可达；二层即vxlan封包的源mac地址和目的mac地址是自己的vxlan设备mac和对端vxlan设备mac。&lt;/td>
 &lt;td>需要进行vxlan的数据包封包和解包会存在一定的性能损耗&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>ipip&lt;/td>
 &lt;td>封包，在tunl0设备上将pod发来的数据包的mac层去掉，留下ip层封包。 外层数据包目的ip地址根据路由得到。&lt;/td>
 &lt;td>只要k8s节点间三层互通， 可以跨网段， 对主机网关路由没有特殊要求。&lt;/td>
 &lt;td>需要进行ipip的数据包封包和解包会存在一定的性能损耗&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>bgp&lt;/td>
 &lt;td>不需要进行数据包封包&lt;/td>
 &lt;td>不用封包解包，通过bgp协议可实现pod网络在主机间的三层可达， k8s节点不跨网段时和flannel的host-gw相似；&lt;/td>
 &lt;td>&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>支持跨网段， 满足复杂的网络架构&lt;/td>
 &lt;td>跨网段时，需要主机网关路由也充当BGP Speaker能够学习到pod子网路由并实现pod子网路由的转发&lt;/td>
 &lt;td>&lt;/td>
 &lt;td>&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;h2 id="fannel三种模式">
 fannel三种模式
 &lt;a class="anchor" href="#fannel%e4%b8%89%e7%a7%8d%e6%a8%a1%e5%bc%8f">#&lt;/a>
&lt;/h2>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th>&lt;strong>fannel三种模式&lt;/strong>&lt;/th>
 &lt;th>&lt;strong>效率&lt;/strong>&lt;/th>
 &lt;th>&lt;strong>calico 模式&lt;/strong>&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td>UDP&lt;/td>
 &lt;td>性能较差，封包解包涉及到多次用户态和内核态交互&lt;/td>
 &lt;td>类似 IPIP&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>VXLAN&lt;/td>
 &lt;td>性能较好，封包解包在内核态实现，内核转发数据，flanneld负责动态配置ARP和FDB（转发数据库）表项更新&lt;/td>
 &lt;td>类似VXLAN&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>host-gw&lt;/td>
 &lt;td>性能最好，不需要再次封包，正常发包，目的容器所在的主机充当网关&lt;/td>
 &lt;td>&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>flanneld 负责主机上路由表的刷新&lt;/td>
 &lt;td>类似 BGP&lt;/td>
 &lt;td>&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;h2 id="你知道的几种cni网络插件并详述其工作原理k8s常用的cni网络插件-calico--flannel简述一下它们的工作原理和区别">
 你知道的几种CNI网络插件，并详述其工作原理。K8s常用的CNI网络插件 （calico &amp;amp;&amp;amp; flannel），简述一下它们的工作原理和区别。
 &lt;a class="anchor" href="#%e4%bd%a0%e7%9f%a5%e9%81%93%e7%9a%84%e5%87%a0%e7%a7%8dcni%e7%bd%91%e7%bb%9c%e6%8f%92%e4%bb%b6%e5%b9%b6%e8%af%a6%e8%bf%b0%e5%85%b6%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86k8s%e5%b8%b8%e7%94%a8%e7%9a%84cni%e7%bd%91%e7%bb%9c%e6%8f%92%e4%bb%b6-calico--flannel%e7%ae%80%e8%bf%b0%e4%b8%80%e4%b8%8b%e5%ae%83%e4%bb%ac%e7%9a%84%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86%e5%92%8c%e5%8c%ba%e5%88%ab">#&lt;/a>
&lt;/h2>
&lt;ol>
&lt;li>
&lt;h5 id="1-calico根据iptables规则进行路由转发并没有进行封包解包的过程这和flannel比起来效率就会快多-calico包括如下重要组件felixetcdbgp-clientbgp-route-reflector下面分别说明一下这些组件">
 \1. calico根据iptables规则进行路由转发，并没有进行封包，解包的过程，这和flannel比起来效率就会快多 calico包括如下重要组件：Felix，etcd，BGP Client，BGP Route Reflector。下面分别说明一下这些组件。
 &lt;a class="anchor" href="#1-calico%e6%a0%b9%e6%8d%aeiptables%e8%a7%84%e5%88%99%e8%bf%9b%e8%a1%8c%e8%b7%af%e7%94%b1%e8%bd%ac%e5%8f%91%e5%b9%b6%e6%b2%a1%e6%9c%89%e8%bf%9b%e8%a1%8c%e5%b0%81%e5%8c%85%e8%a7%a3%e5%8c%85%e7%9a%84%e8%bf%87%e7%a8%8b%e8%bf%99%e5%92%8cflannel%e6%af%94%e8%b5%b7%e6%9d%a5%e6%95%88%e7%8e%87%e5%b0%b1%e4%bc%9a%e5%bf%ab%e5%a4%9a-calico%e5%8c%85%e6%8b%ac%e5%a6%82%e4%b8%8b%e9%87%8d%e8%a6%81%e7%bb%84%e4%bb%b6felixetcdbgp-clientbgp-route-reflector%e4%b8%8b%e9%9d%a2%e5%88%86%e5%88%ab%e8%af%b4%e6%98%8e%e4%b8%80%e4%b8%8b%e8%bf%99%e4%ba%9b%e7%bb%84%e4%bb%b6">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ol>
&lt;h5 id="felix主要负责路由配置以及acls规则的配置以及下发它存在在每个node节点上-etcd分布式键值存储主要负责网络元数据一致性确保calico网络状态的准确性可以与kubernetes共用-bgpclientbird-主要负责把-felix写入-kernel的路由信息分发到当前-calico网络确保-workload间的通信的有效性-bgproute-reflectorbird-大规模部署时使用摒弃所有节点互联的mesh模式通过一个或者多个-bgproute-reflector-来完成集中式的路由分发-通过将整个互联网的可扩展-ip网络原则压缩到数据中心级别calico在每一个计算节点利用-linuxkernel-实现了一个高效的-vrouter来负责数据转发而每个vrouter通过-bgp协议负责把自己上运行的-workload的路由信息向整个calico网络内传播小规模部署可以直接互联大规模下可通过指定的bgproute-reflector-来完成这样保证最终所有的workload之间的数据流量都是通过-ip包的方式完成互联的">
 Felix：主要负责路由配置以及ACLS规则的配置以及下发，它存在在每个node节点上。 etcd：分布式键值存储，主要负责网络元数据一致性，确保Calico网络状态的准确性，可以与kubernetes共用； BGPClient(BIRD), 主要负责把 Felix写入 kernel的路由信息分发到当前 Calico网络，确保 workload间的通信的有效性； BGPRoute Reflector(BIRD), 大规模部署时使用，摒弃所有节点互联的mesh模式，通过一个或者多个 BGPRoute Reflector 来完成集中式的路由分发 通过将整个互联网的可扩展 IP网络原则压缩到数据中心级别，Calico在每一个计算节点利用 Linuxkernel 实现了一个高效的 vRouter来负责数据转发，而每个vRouter通过 BGP协议负责把自己上运行的 workload的路由信息向整个Calico网络内传播，小规模部署可以直接互联，大规模下可通过指定的BGProute reflector 来完成。这样保证最终所有的workload之间的数据流量都是通过 IP包的方式完成互联的。
 &lt;a class="anchor" href="#felix%e4%b8%bb%e8%a6%81%e8%b4%9f%e8%b4%a3%e8%b7%af%e7%94%b1%e9%85%8d%e7%bd%ae%e4%bb%a5%e5%8f%8aacls%e8%a7%84%e5%88%99%e7%9a%84%e9%85%8d%e7%bd%ae%e4%bb%a5%e5%8f%8a%e4%b8%8b%e5%8f%91%e5%ae%83%e5%ad%98%e5%9c%a8%e5%9c%a8%e6%af%8f%e4%b8%aanode%e8%8a%82%e7%82%b9%e4%b8%8a-etcd%e5%88%86%e5%b8%83%e5%bc%8f%e9%94%ae%e5%80%bc%e5%ad%98%e5%82%a8%e4%b8%bb%e8%a6%81%e8%b4%9f%e8%b4%a3%e7%bd%91%e7%bb%9c%e5%85%83%e6%95%b0%e6%8d%ae%e4%b8%80%e8%87%b4%e6%80%a7%e7%a1%ae%e4%bf%9dcalico%e7%bd%91%e7%bb%9c%e7%8a%b6%e6%80%81%e7%9a%84%e5%87%86%e7%a1%ae%e6%80%a7%e5%8f%af%e4%bb%a5%e4%b8%8ekubernetes%e5%85%b1%e7%94%a8-bgpclientbird-%e4%b8%bb%e8%a6%81%e8%b4%9f%e8%b4%a3%e6%8a%8a-felix%e5%86%99%e5%85%a5-kernel%e7%9a%84%e8%b7%af%e7%94%b1%e4%bf%a1%e6%81%af%e5%88%86%e5%8f%91%e5%88%b0%e5%bd%93%e5%89%8d-calico%e7%bd%91%e7%bb%9c%e7%a1%ae%e4%bf%9d-workload%e9%97%b4%e7%9a%84%e9%80%9a%e4%bf%a1%e7%9a%84%e6%9c%89%e6%95%88%e6%80%a7-bgproute-reflectorbird-%e5%a4%a7%e8%a7%84%e6%a8%a1%e9%83%a8%e7%bd%b2%e6%97%b6%e4%bd%bf%e7%94%a8%e6%91%92%e5%bc%83%e6%89%80%e6%9c%89%e8%8a%82%e7%82%b9%e4%ba%92%e8%81%94%e7%9a%84mesh%e6%a8%a1%e5%bc%8f%e9%80%9a%e8%bf%87%e4%b8%80%e4%b8%aa%e6%88%96%e8%80%85%e5%a4%9a%e4%b8%aa-bgproute-reflector-%e6%9d%a5%e5%ae%8c%e6%88%90%e9%9b%86%e4%b8%ad%e5%bc%8f%e7%9a%84%e8%b7%af%e7%94%b1%e5%88%86%e5%8f%91-%e9%80%9a%e8%bf%87%e5%b0%86%e6%95%b4%e4%b8%aa%e4%ba%92%e8%81%94%e7%bd%91%e7%9a%84%e5%8f%af%e6%89%a9%e5%b1%95-ip%e7%bd%91%e7%bb%9c%e5%8e%9f%e5%88%99%e5%8e%8b%e7%bc%a9%e5%88%b0%e6%95%b0%e6%8d%ae%e4%b8%ad%e5%bf%83%e7%ba%a7%e5%88%abcalico%e5%9c%a8%e6%af%8f%e4%b8%80%e4%b8%aa%e8%ae%a1%e7%ae%97%e8%8a%82%e7%82%b9%e5%88%a9%e7%94%a8-linuxkernel-%e5%ae%9e%e7%8e%b0%e4%ba%86%e4%b8%80%e4%b8%aa%e9%ab%98%e6%95%88%e7%9a%84-vrouter%e6%9d%a5%e8%b4%9f%e8%b4%a3%e6%95%b0%e6%8d%ae%e8%bd%ac%e5%8f%91%e8%80%8c%e6%af%8f%e4%b8%aavrouter%e9%80%9a%e8%bf%87-bgp%e5%8d%8f%e8%ae%ae%e8%b4%9f%e8%b4%a3%e6%8a%8a%e8%87%aa%e5%b7%b1%e4%b8%8a%e8%bf%90%e8%a1%8c%e7%9a%84-workload%e7%9a%84%e8%b7%af%e7%94%b1%e4%bf%a1%e6%81%af%e5%90%91%e6%95%b4%e4%b8%aacalico%e7%bd%91%e7%bb%9c%e5%86%85%e4%bc%a0%e6%92%ad%e5%b0%8f%e8%a7%84%e6%a8%a1%e9%83%a8%e7%bd%b2%e5%8f%af%e4%bb%a5%e7%9b%b4%e6%8e%a5%e4%ba%92%e8%81%94%e5%a4%a7%e8%a7%84%e6%a8%a1%e4%b8%8b%e5%8f%af%e9%80%9a%e8%bf%87%e6%8c%87%e5%ae%9a%e7%9a%84bgproute-reflector-%e6%9d%a5%e5%ae%8c%e6%88%90%e8%bf%99%e6%a0%b7%e4%bf%9d%e8%af%81%e6%9c%80%e7%bb%88%e6%89%80%e6%9c%89%e7%9a%84workload%e4%b9%8b%e9%97%b4%e7%9a%84%e6%95%b0%e6%8d%ae%e6%b5%81%e9%87%8f%e9%83%bd%e6%98%af%e9%80%9a%e8%bf%87-ip%e5%8c%85%e7%9a%84%e6%96%b9%e5%bc%8f%e5%ae%8c%e6%88%90%e4%ba%92%e8%81%94%e7%9a%84">#&lt;/a>
&lt;/h5>
&lt;ol>
&lt;li>
&lt;h5 id="1-flannel的工作原理-flannel实质上是一种覆盖网络overlay-network也就是将tcp数据包装在另一种网络包里面进行路由转发和通信目前已经支持udpvxlanaws-vpc和gce路由等数据转发方式">
 \1. Flannel的工作原理： Flannel实质上是一种“覆盖网络(overlay network)”，也就是将TCP数据包装在另一种网络包里面进行路由转发和通信，目前已经支持UDP、VxLAN、AWS VPC和GCE路由等数据转发方式。
 &lt;a class="anchor" href="#1-flannel%e7%9a%84%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86-flannel%e5%ae%9e%e8%b4%a8%e4%b8%8a%e6%98%af%e4%b8%80%e7%a7%8d%e8%a6%86%e7%9b%96%e7%bd%91%e7%bb%9coverlay-network%e4%b9%9f%e5%b0%b1%e6%98%af%e5%b0%86tcp%e6%95%b0%e6%8d%ae%e5%8c%85%e8%a3%85%e5%9c%a8%e5%8f%a6%e4%b8%80%e7%a7%8d%e7%bd%91%e7%bb%9c%e5%8c%85%e9%87%8c%e9%9d%a2%e8%bf%9b%e8%a1%8c%e8%b7%af%e7%94%b1%e8%bd%ac%e5%8f%91%e5%92%8c%e9%80%9a%e4%bf%a1%e7%9b%ae%e5%89%8d%e5%b7%b2%e7%bb%8f%e6%94%af%e6%8c%81udpvxlanaws-vpc%e5%92%8cgce%e8%b7%af%e7%94%b1%e7%ad%89%e6%95%b0%e6%8d%ae%e8%bd%ac%e5%8f%91%e6%96%b9%e5%bc%8f">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ol>
&lt;h5 id="默认的节点间数据通信方式是udp转发-工作原理-数据从源容器中发出后经由所在主机的docker0虚拟网卡转发到flannel0虚拟网卡先可以不经过docker0网卡使用cni模式这是个p2p的虚拟网卡flanneld服务监听在网卡的另外一端-flannel通过etcd服务维护了一张节点间的路由表详细记录了各节点子网网段--源主机的flanneld服务将原本的数据内容udp封装后根据自己的路由表投递给目的节点的flanneld服务数据到达以后被解包然后直接进入目的节点的flannel0虚拟网卡然后被转发到目的主机的docker0虚拟网卡最后就像本机容器通信一下的有docker0路由到达目标容器-flannel在进行路由转发的基础上进行了封包解包的操作这样浪费了cpu的计算资源">
 默认的节点间数据通信方式是UDP转发。 工作原理： 数据从源容器中发出后，经由所在主机的docker0虚拟网卡转发到flannel0虚拟网卡（先可以不经过docker0网卡，使用cni模式），这是个P2P的虚拟网卡，flanneld服务监听在网卡的另外一端。 Flannel通过Etcd服务维护了一张节点间的路由表，详细记录了各节点子网网段 。 源主机的flanneld服务将原本的数据内容UDP封装后根据自己的路由表投递给目的节点的flanneld服务，数据到达以后被解包，然后直接进入目的节点的flannel0虚拟网卡，然后被转发到目的主机的docker0虚拟网卡，最后就像本机容器通信一下的有docker0路由到达目标容器。 flannel在进行路由转发的基础上进行了封包解包的操作，这样浪费了CPU的计算资源。
 &lt;a class="anchor" href="#%e9%bb%98%e8%ae%a4%e7%9a%84%e8%8a%82%e7%82%b9%e9%97%b4%e6%95%b0%e6%8d%ae%e9%80%9a%e4%bf%a1%e6%96%b9%e5%bc%8f%e6%98%afudp%e8%bd%ac%e5%8f%91-%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86-%e6%95%b0%e6%8d%ae%e4%bb%8e%e6%ba%90%e5%ae%b9%e5%99%a8%e4%b8%ad%e5%8f%91%e5%87%ba%e5%90%8e%e7%bb%8f%e7%94%b1%e6%89%80%e5%9c%a8%e4%b8%bb%e6%9c%ba%e7%9a%84docker0%e8%99%9a%e6%8b%9f%e7%bd%91%e5%8d%a1%e8%bd%ac%e5%8f%91%e5%88%b0flannel0%e8%99%9a%e6%8b%9f%e7%bd%91%e5%8d%a1%e5%85%88%e5%8f%af%e4%bb%a5%e4%b8%8d%e7%bb%8f%e8%bf%87docker0%e7%bd%91%e5%8d%a1%e4%bd%bf%e7%94%a8cni%e6%a8%a1%e5%bc%8f%e8%bf%99%e6%98%af%e4%b8%aap2p%e7%9a%84%e8%99%9a%e6%8b%9f%e7%bd%91%e5%8d%a1flanneld%e6%9c%8d%e5%8a%a1%e7%9b%91%e5%90%ac%e5%9c%a8%e7%bd%91%e5%8d%a1%e7%9a%84%e5%8f%a6%e5%a4%96%e4%b8%80%e7%ab%af-flannel%e9%80%9a%e8%bf%87etcd%e6%9c%8d%e5%8a%a1%e7%bb%b4%e6%8a%a4%e4%ba%86%e4%b8%80%e5%bc%a0%e8%8a%82%e7%82%b9%e9%97%b4%e7%9a%84%e8%b7%af%e7%94%b1%e8%a1%a8%e8%af%a6%e7%bb%86%e8%ae%b0%e5%bd%95%e4%ba%86%e5%90%84%e8%8a%82%e7%82%b9%e5%ad%90%e7%bd%91%e7%bd%91%e6%ae%b5--%e6%ba%90%e4%b8%bb%e6%9c%ba%e7%9a%84flanneld%e6%9c%8d%e5%8a%a1%e5%b0%86%e5%8e%9f%e6%9c%ac%e7%9a%84%e6%95%b0%e6%8d%ae%e5%86%85%e5%ae%b9udp%e5%b0%81%e8%a3%85%e5%90%8e%e6%a0%b9%e6%8d%ae%e8%87%aa%e5%b7%b1%e7%9a%84%e8%b7%af%e7%94%b1%e8%a1%a8%e6%8a%95%e9%80%92%e7%bb%99%e7%9b%ae%e7%9a%84%e8%8a%82%e7%82%b9%e7%9a%84flanneld%e6%9c%8d%e5%8a%a1%e6%95%b0%e6%8d%ae%e5%88%b0%e8%be%be%e4%bb%a5%e5%90%8e%e8%a2%ab%e8%a7%a3%e5%8c%85%e7%84%b6%e5%90%8e%e7%9b%b4%e6%8e%a5%e8%bf%9b%e5%85%a5%e7%9b%ae%e7%9a%84%e8%8a%82%e7%82%b9%e7%9a%84flannel0%e8%99%9a%e6%8b%9f%e7%bd%91%e5%8d%a1%e7%84%b6%e5%90%8e%e8%a2%ab%e8%bd%ac%e5%8f%91%e5%88%b0%e7%9b%ae%e7%9a%84%e4%b8%bb%e6%9c%ba%e7%9a%84docker0%e8%99%9a%e6%8b%9f%e7%bd%91%e5%8d%a1%e6%9c%80%e5%90%8e%e5%b0%b1%e5%83%8f%e6%9c%ac%e6%9c%ba%e5%ae%b9%e5%99%a8%e9%80%9a%e4%bf%a1%e4%b8%80%e4%b8%8b%e7%9a%84%e6%9c%89docker0%e8%b7%af%e7%94%b1%e5%88%b0%e8%be%be%e7%9b%ae%e6%a0%87%e5%ae%b9%e5%99%a8-flannel%e5%9c%a8%e8%bf%9b%e8%a1%8c%e8%b7%af%e7%94%b1%e8%bd%ac%e5%8f%91%e7%9a%84%e5%9f%ba%e7%a1%80%e4%b8%8a%e8%bf%9b%e8%a1%8c%e4%ba%86%e5%b0%81%e5%8c%85%e8%a7%a3%e5%8c%85%e7%9a%84%e6%93%8d%e4%bd%9c%e8%bf%99%e6%a0%b7%e6%b5%aa%e8%b4%b9%e4%ba%86cpu%e7%9a%84%e8%ae%a1%e7%ae%97%e8%b5%84%e6%ba%90">#&lt;/a>
&lt;/h5>
&lt;h2 id="worker节点宕机简述pods驱逐流程">
 Worker节点宕机，简述Pods驱逐流程。
 &lt;a class="anchor" href="#worker%e8%8a%82%e7%82%b9%e5%ae%95%e6%9c%ba%e7%ae%80%e8%bf%b0pods%e9%a9%b1%e9%80%90%e6%b5%81%e7%a8%8b">#&lt;/a>
&lt;/h2>
&lt;ol>
&lt;li>
&lt;h5 id="1-在-kubernetes-集群中当节点由于某些原因网络宕机等不能正常工作时会被认定为不可用状态unknown-或者-false-状态当时间超过了-pod-eviction-timeout-值时那么节点上的所有-pod-都会被节点控制器计划删除">
 \1. 在 Kubernetes 集群中，当节点由于某些原因（网络、宕机等）不能正常工作时会被认定为不可用状态（Unknown 或者 False 状态），当时间超过了 pod-eviction-timeout 值时，那么节点上的所有 Pod 都会被节点控制器计划删除。
 &lt;a class="anchor" href="#1-%e5%9c%a8-kubernetes-%e9%9b%86%e7%be%a4%e4%b8%ad%e5%bd%93%e8%8a%82%e7%82%b9%e7%94%b1%e4%ba%8e%e6%9f%90%e4%ba%9b%e5%8e%9f%e5%9b%a0%e7%bd%91%e7%bb%9c%e5%ae%95%e6%9c%ba%e7%ad%89%e4%b8%8d%e8%83%bd%e6%ad%a3%e5%b8%b8%e5%b7%a5%e4%bd%9c%e6%97%b6%e4%bc%9a%e8%a2%ab%e8%ae%a4%e5%ae%9a%e4%b8%ba%e4%b8%8d%e5%8f%af%e7%94%a8%e7%8a%b6%e6%80%81unknown-%e6%88%96%e8%80%85-false-%e7%8a%b6%e6%80%81%e5%bd%93%e6%97%b6%e9%97%b4%e8%b6%85%e8%bf%87%e4%ba%86-pod-eviction-timeout-%e5%80%bc%e6%97%b6%e9%82%a3%e4%b9%88%e8%8a%82%e7%82%b9%e4%b8%8a%e7%9a%84%e6%89%80%e6%9c%89-pod-%e9%83%bd%e4%bc%9a%e8%a2%ab%e8%8a%82%e7%82%b9%e6%8e%a7%e5%88%b6%e5%99%a8%e8%ae%a1%e5%88%92%e5%88%a0%e9%99%a4">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="2-kubernetes-集群中有一个节点生命周期控制器node_lifecycle_controllergo它会与每一个节点上的-kubelet-进行通信以收集各个节点已经节点上容器的相关状态信息当超出一定时间后不能与-kubelet-通信那么就会标记该节点为-unknown-状态并且节点生命周期控制器会自动创建代表状况的污点用于防止调度器调度-pod-到该节点">
 \2. Kubernetes 集群中有一个节点生命周期控制器：node_lifecycle_controller.go。它会与每一个节点上的 kubelet 进行通信，以收集各个节点已经节点上容器的相关状态信息。当超出一定时间后不能与 kubelet 通信，那么就会标记该节点为 Unknown 状态。并且节点生命周期控制器会自动创建代表状况的污点，用于防止调度器调度 pod 到该节点。
 &lt;a class="anchor" href="#2-kubernetes-%e9%9b%86%e7%be%a4%e4%b8%ad%e6%9c%89%e4%b8%80%e4%b8%aa%e8%8a%82%e7%82%b9%e7%94%9f%e5%91%bd%e5%91%a8%e6%9c%9f%e6%8e%a7%e5%88%b6%e5%99%a8node_lifecycle_controllergo%e5%ae%83%e4%bc%9a%e4%b8%8e%e6%af%8f%e4%b8%80%e4%b8%aa%e8%8a%82%e7%82%b9%e4%b8%8a%e7%9a%84-kubelet-%e8%bf%9b%e8%a1%8c%e9%80%9a%e4%bf%a1%e4%bb%a5%e6%94%b6%e9%9b%86%e5%90%84%e4%b8%aa%e8%8a%82%e7%82%b9%e5%b7%b2%e7%bb%8f%e8%8a%82%e7%82%b9%e4%b8%8a%e5%ae%b9%e5%99%a8%e7%9a%84%e7%9b%b8%e5%85%b3%e7%8a%b6%e6%80%81%e4%bf%a1%e6%81%af%e5%bd%93%e8%b6%85%e5%87%ba%e4%b8%80%e5%ae%9a%e6%97%b6%e9%97%b4%e5%90%8e%e4%b8%8d%e8%83%bd%e4%b8%8e-kubelet-%e9%80%9a%e4%bf%a1%e9%82%a3%e4%b9%88%e5%b0%b1%e4%bc%9a%e6%a0%87%e8%ae%b0%e8%af%a5%e8%8a%82%e7%82%b9%e4%b8%ba-unknown-%e7%8a%b6%e6%80%81%e5%b9%b6%e4%b8%94%e8%8a%82%e7%82%b9%e7%94%9f%e5%91%bd%e5%91%a8%e6%9c%9f%e6%8e%a7%e5%88%b6%e5%99%a8%e4%bc%9a%e8%87%aa%e5%8a%a8%e5%88%9b%e5%bb%ba%e4%bb%a3%e8%a1%a8%e7%8a%b6%e5%86%b5%e7%9a%84%e6%b1%a1%e7%82%b9%e7%94%a8%e4%ba%8e%e9%98%b2%e6%ad%a2%e8%b0%83%e5%ba%a6%e5%99%a8%e8%b0%83%e5%ba%a6-pod-%e5%88%b0%e8%af%a5%e8%8a%82%e7%82%b9">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="3-那么-unknown-状态的节点上已经运行的-pod-会怎么处理呢节点上的所有-pod-都会被污点管理器taint_managergo计划删除而在节点被认定为不可用状态到删除节点上的-pod-之间是有一段时间的这段时间被称为容忍度如果在不配置的情况下kubernetes-会自动给-pod-添加一个-key-为-nodekubernetesionot-ready-的容忍度-并配置-tolerationseconds300同样kubernetes-会给-pod-添加一个-key-为-nodekubernetesiounreachable-的容忍度-并配置-tolerationseconds300">
 \3. 那么 Unknown 状态的节点上已经运行的 pod 会怎么处理呢？节点上的所有 Pod 都会被污点管理器（taint_manager.go）计划删除。而在节点被认定为不可用状态到删除节点上的 Pod 之间是有一段时间的，这段时间被称为容忍度。如果在不配置的情况下，Kubernetes 会自动给 Pod 添加一个 key 为 node.kubernetes.io/not-ready 的容忍度 并配置 tolerationSeconds=300，同样，Kubernetes 会给 Pod 添加一个 key 为 node.kubernetes.io/unreachable 的容忍度 并配置 tolerationSeconds=300。
 &lt;a class="anchor" href="#3-%e9%82%a3%e4%b9%88-unknown-%e7%8a%b6%e6%80%81%e7%9a%84%e8%8a%82%e7%82%b9%e4%b8%8a%e5%b7%b2%e7%bb%8f%e8%bf%90%e8%a1%8c%e7%9a%84-pod-%e4%bc%9a%e6%80%8e%e4%b9%88%e5%a4%84%e7%90%86%e5%91%a2%e8%8a%82%e7%82%b9%e4%b8%8a%e7%9a%84%e6%89%80%e6%9c%89-pod-%e9%83%bd%e4%bc%9a%e8%a2%ab%e6%b1%a1%e7%82%b9%e7%ae%a1%e7%90%86%e5%99%a8taint_managergo%e8%ae%a1%e5%88%92%e5%88%a0%e9%99%a4%e8%80%8c%e5%9c%a8%e8%8a%82%e7%82%b9%e8%a2%ab%e8%ae%a4%e5%ae%9a%e4%b8%ba%e4%b8%8d%e5%8f%af%e7%94%a8%e7%8a%b6%e6%80%81%e5%88%b0%e5%88%a0%e9%99%a4%e8%8a%82%e7%82%b9%e4%b8%8a%e7%9a%84-pod-%e4%b9%8b%e9%97%b4%e6%98%af%e6%9c%89%e4%b8%80%e6%ae%b5%e6%97%b6%e9%97%b4%e7%9a%84%e8%bf%99%e6%ae%b5%e6%97%b6%e9%97%b4%e8%a2%ab%e7%a7%b0%e4%b8%ba%e5%ae%b9%e5%bf%8d%e5%ba%a6%e5%a6%82%e6%9e%9c%e5%9c%a8%e4%b8%8d%e9%85%8d%e7%bd%ae%e7%9a%84%e6%83%85%e5%86%b5%e4%b8%8bkubernetes-%e4%bc%9a%e8%87%aa%e5%8a%a8%e7%bb%99-pod-%e6%b7%bb%e5%8a%a0%e4%b8%80%e4%b8%aa-key-%e4%b8%ba-nodekubernetesionot-ready-%e7%9a%84%e5%ae%b9%e5%bf%8d%e5%ba%a6-%e5%b9%b6%e9%85%8d%e7%bd%ae-tolerationseconds300%e5%90%8c%e6%a0%b7kubernetes-%e4%bc%9a%e7%bb%99-pod-%e6%b7%bb%e5%8a%a0%e4%b8%80%e4%b8%aa-key-%e4%b8%ba-nodekubernetesiounreachable-%e7%9a%84%e5%ae%b9%e5%bf%8d%e5%ba%a6-%e5%b9%b6%e9%85%8d%e7%bd%ae-tolerationseconds300">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="4-当到了删除-pod-时污点管理器会创建污点标记事件然后驱逐-pod-这里需要注意的是由于已经不能与-kubelet-通信所以该节点上的-pod-在管理后台看到的是处于灰色标记但是此时如果去获取-pod-的状态其实还是处于-running-状态每种类型的资源都有相应的资源控制器controller例如deployment_controllergostateful_set_controlgo每种控制器都在监听资源变化从而做出相应的动作执行deployment-控制器在监听到-pod-被驱逐后会创建一个新的-pod-出来但是-statefulset-控制器并不会创建出新的-pod原因是因为它可能会违反-statefulset-固有的至多一个的语义可能出现具有相同身份的多个成员这将可能是灾难性的并且可能导致数据丢失">
 \4. 当到了删除 Pod 时，污点管理器会创建污点标记事件，然后驱逐 pod 。这里需要注意的是由于已经不能与 kubelet 通信，所以该节点上的 Pod 在管理后台看到的是处于灰色标记，但是此时如果去获取 pod 的状态其实还是处于 Running 状态。每种类型的资源都有相应的资源控制器（Controller），例如：deployment_controller.go、stateful_set_control.go。每种控制器都在监听资源变化，从而做出相应的动作执行。deployment 控制器在监听到 Pod 被驱逐后会创建一个新的 Pod 出来，但是 Statefulset 控制器并不会创建出新的 Pod，原因是因为它可能会违反 StatefulSet 固有的至多一个的语义，可能出现具有相同身份的多个成员，这将可能是灾难性的，并且可能导致数据丢失。
 &lt;a class="anchor" href="#4-%e5%bd%93%e5%88%b0%e4%ba%86%e5%88%a0%e9%99%a4-pod-%e6%97%b6%e6%b1%a1%e7%82%b9%e7%ae%a1%e7%90%86%e5%99%a8%e4%bc%9a%e5%88%9b%e5%bb%ba%e6%b1%a1%e7%82%b9%e6%a0%87%e8%ae%b0%e4%ba%8b%e4%bb%b6%e7%84%b6%e5%90%8e%e9%a9%b1%e9%80%90-pod-%e8%bf%99%e9%87%8c%e9%9c%80%e8%a6%81%e6%b3%a8%e6%84%8f%e7%9a%84%e6%98%af%e7%94%b1%e4%ba%8e%e5%b7%b2%e7%bb%8f%e4%b8%8d%e8%83%bd%e4%b8%8e-kubelet-%e9%80%9a%e4%bf%a1%e6%89%80%e4%bb%a5%e8%af%a5%e8%8a%82%e7%82%b9%e4%b8%8a%e7%9a%84-pod-%e5%9c%a8%e7%ae%a1%e7%90%86%e5%90%8e%e5%8f%b0%e7%9c%8b%e5%88%b0%e7%9a%84%e6%98%af%e5%a4%84%e4%ba%8e%e7%81%b0%e8%89%b2%e6%a0%87%e8%ae%b0%e4%bd%86%e6%98%af%e6%ad%a4%e6%97%b6%e5%a6%82%e6%9e%9c%e5%8e%bb%e8%8e%b7%e5%8f%96-pod-%e7%9a%84%e7%8a%b6%e6%80%81%e5%85%b6%e5%ae%9e%e8%bf%98%e6%98%af%e5%a4%84%e4%ba%8e-running-%e7%8a%b6%e6%80%81%e6%af%8f%e7%a7%8d%e7%b1%bb%e5%9e%8b%e7%9a%84%e8%b5%84%e6%ba%90%e9%83%bd%e6%9c%89%e7%9b%b8%e5%ba%94%e7%9a%84%e8%b5%84%e6%ba%90%e6%8e%a7%e5%88%b6%e5%99%a8controller%e4%be%8b%e5%a6%82deployment_controllergostateful_set_controlgo%e6%af%8f%e7%a7%8d%e6%8e%a7%e5%88%b6%e5%99%a8%e9%83%bd%e5%9c%a8%e7%9b%91%e5%90%ac%e8%b5%84%e6%ba%90%e5%8f%98%e5%8c%96%e4%bb%8e%e8%80%8c%e5%81%9a%e5%87%ba%e7%9b%b8%e5%ba%94%e7%9a%84%e5%8a%a8%e4%bd%9c%e6%89%a7%e8%a1%8cdeployment-%e6%8e%a7%e5%88%b6%e5%99%a8%e5%9c%a8%e7%9b%91%e5%90%ac%e5%88%b0-pod-%e8%a2%ab%e9%a9%b1%e9%80%90%e5%90%8e%e4%bc%9a%e5%88%9b%e5%bb%ba%e4%b8%80%e4%b8%aa%e6%96%b0%e7%9a%84-pod-%e5%87%ba%e6%9d%a5%e4%bd%86%e6%98%af-statefulset-%e6%8e%a7%e5%88%b6%e5%99%a8%e5%b9%b6%e4%b8%8d%e4%bc%9a%e5%88%9b%e5%bb%ba%e5%87%ba%e6%96%b0%e7%9a%84-pod%e5%8e%9f%e5%9b%a0%e6%98%af%e5%9b%a0%e4%b8%ba%e5%ae%83%e5%8f%af%e8%83%bd%e4%bc%9a%e8%bf%9d%e5%8f%8d-statefulset-%e5%9b%ba%e6%9c%89%e7%9a%84%e8%87%b3%e5%a4%9a%e4%b8%80%e4%b8%aa%e7%9a%84%e8%af%ad%e4%b9%89%e5%8f%af%e8%83%bd%e5%87%ba%e7%8e%b0%e5%85%b7%e6%9c%89%e7%9b%b8%e5%90%8c%e8%ba%ab%e4%bb%bd%e7%9a%84%e5%a4%9a%e4%b8%aa%e6%88%90%e5%91%98%e8%bf%99%e5%b0%86%e5%8f%af%e8%83%bd%e6%98%af%e7%81%be%e9%9a%be%e6%80%a7%e7%9a%84%e5%b9%b6%e4%b8%94%e5%8f%af%e8%83%bd%e5%af%bc%e8%87%b4%e6%95%b0%e6%8d%ae%e4%b8%a2%e5%a4%b1">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ol>
&lt;h2 id="你知道的k8s中几种controller控制器并详述其工作原理">
 你知道的K8s中几种Controller控制器，并详述其工作原理
 &lt;a class="anchor" href="#%e4%bd%a0%e7%9f%a5%e9%81%93%e7%9a%84k8s%e4%b8%ad%e5%87%a0%e7%a7%8dcontroller%e6%8e%a7%e5%88%b6%e5%99%a8%e5%b9%b6%e8%af%a6%e8%bf%b0%e5%85%b6%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86">#&lt;/a>
&lt;/h2>
&lt;ol>
&lt;li>
&lt;h5 id="1-deployment适合无状态的服务部署-适合部署无状态的应用服务用来管理pod和replicaset具有上线部署副本设定滚动更新回滚等功能还可提供声明式更新例如只更新一个新的image">
 \1. deployment：适合无状态的服务部署 适合部署无状态的应用服务，用来管理pod和replicaset，具有上线部署、副本设定、滚动更新、回滚等功能，还可提供声明式更新，例如只更新一个新的Image
 &lt;a class="anchor" href="#1-deployment%e9%80%82%e5%90%88%e6%97%a0%e7%8a%b6%e6%80%81%e7%9a%84%e6%9c%8d%e5%8a%a1%e9%83%a8%e7%bd%b2-%e9%80%82%e5%90%88%e9%83%a8%e7%bd%b2%e6%97%a0%e7%8a%b6%e6%80%81%e7%9a%84%e5%ba%94%e7%94%a8%e6%9c%8d%e5%8a%a1%e7%94%a8%e6%9d%a5%e7%ae%a1%e7%90%86pod%e5%92%8creplicaset%e5%85%b7%e6%9c%89%e4%b8%8a%e7%ba%bf%e9%83%a8%e7%bd%b2%e5%89%af%e6%9c%ac%e8%ae%be%e5%ae%9a%e6%bb%9a%e5%8a%a8%e6%9b%b4%e6%96%b0%e5%9b%9e%e6%bb%9a%e7%ad%89%e5%8a%9f%e8%83%bd%e8%bf%98%e5%8f%af%e6%8f%90%e4%be%9b%e5%a3%b0%e6%98%8e%e5%bc%8f%e6%9b%b4%e6%96%b0%e4%be%8b%e5%a6%82%e5%8f%aa%e6%9b%b4%e6%96%b0%e4%b8%80%e4%b8%aa%e6%96%b0%e7%9a%84image">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>
&lt;h5 id="-编写yaml文件并创建nginx服务pod资源">
 • 编写yaml文件，并创建nginx服务pod资源。
 &lt;a class="anchor" href="#-%e7%bc%96%e5%86%99yaml%e6%96%87%e4%bb%b6%e5%b9%b6%e5%88%9b%e5%bb%banginx%e6%9c%8d%e5%8a%a1pod%e8%b5%84%e6%ba%90">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;ol>
&lt;li>
&lt;h5 id="1-statefullset适合有状态的服务部署-适合部署有状态应用解决pod的独立生命周期保持pod启动顺序和唯一性">
 \1. StatefullSet：适合有状态的服务部署 适合部署有状态应用，解决Pod的独立生命周期，保持Pod启动顺序和唯一性。
 &lt;a class="anchor" href="#1-statefullset%e9%80%82%e5%90%88%e6%9c%89%e7%8a%b6%e6%80%81%e7%9a%84%e6%9c%8d%e5%8a%a1%e9%83%a8%e7%bd%b2-%e9%80%82%e5%90%88%e9%83%a8%e7%bd%b2%e6%9c%89%e7%8a%b6%e6%80%81%e5%ba%94%e7%94%a8%e8%a7%a3%e5%86%b3pod%e7%9a%84%e7%8b%ac%e7%ab%8b%e7%94%9f%e5%91%bd%e5%91%a8%e6%9c%9f%e4%bf%9d%e6%8c%81pod%e5%90%af%e5%8a%a8%e9%a1%ba%e5%ba%8f%e5%92%8c%e5%94%af%e4%b8%80%e6%80%a7">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>
&lt;h5 id="-稳定唯一的网络标识符持久存储例如etcd配置文件节点地址发生变化将无法使用">
 • 稳定，唯一的网络标识符，持久存储（例如：etcd配置文件，节点地址发生变化，将无法使用）
 &lt;a class="anchor" href="#-%e7%a8%b3%e5%ae%9a%e5%94%af%e4%b8%80%e7%9a%84%e7%bd%91%e7%bb%9c%e6%a0%87%e8%af%86%e7%ac%a6%e6%8c%81%e4%b9%85%e5%ad%98%e5%82%a8%e4%be%8b%e5%a6%82etcd%e9%85%8d%e7%bd%ae%e6%96%87%e4%bb%b6%e8%8a%82%e7%82%b9%e5%9c%b0%e5%9d%80%e5%8f%91%e7%94%9f%e5%8f%98%e5%8c%96%e5%b0%86%e6%97%a0%e6%b3%95%e4%bd%bf%e7%94%a8">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-有序优雅的部署和扩展删除和终止例如mysql主从关系先启动主再启动从有序滚动更新">
 • 有序，优雅的部署和扩展、删除和终止（例如：mysql主从关系，先启动主，再启动从）有序，滚动更新
 &lt;a class="anchor" href="#-%e6%9c%89%e5%ba%8f%e4%bc%98%e9%9b%85%e7%9a%84%e9%83%a8%e7%bd%b2%e5%92%8c%e6%89%a9%e5%b1%95%e5%88%a0%e9%99%a4%e5%92%8c%e7%bb%88%e6%ad%a2%e4%be%8b%e5%a6%82mysql%e4%b8%bb%e4%bb%8e%e5%85%b3%e7%b3%bb%e5%85%88%e5%90%af%e5%8a%a8%e4%b8%bb%e5%86%8d%e5%90%af%e5%8a%a8%e4%bb%8e%e6%9c%89%e5%ba%8f%e6%bb%9a%e5%8a%a8%e6%9b%b4%e6%96%b0">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-应用场景例如数据库">
 • 应用场景：例如数据库
 &lt;a class="anchor" href="#-%e5%ba%94%e7%94%a8%e5%9c%ba%e6%99%af%e4%be%8b%e5%a6%82%e6%95%b0%e6%8d%ae%e5%ba%93">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h5 id="无状态服务的特点">
 无状态服务的特点：
 &lt;a class="anchor" href="#%e6%97%a0%e7%8a%b6%e6%80%81%e6%9c%8d%e5%8a%a1%e7%9a%84%e7%89%b9%e7%82%b9">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="-deployment-认为所有的pod都是一样的">
 • deployment 认为所有的pod都是一样的
 &lt;a class="anchor" href="#-deployment-%e8%ae%a4%e4%b8%ba%e6%89%80%e6%9c%89%e7%9a%84pod%e9%83%bd%e6%98%af%e4%b8%80%e6%a0%b7%e7%9a%84">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-不用考虑顺序的要求">
 • 不用考虑顺序的要求
 &lt;a class="anchor" href="#-%e4%b8%8d%e7%94%a8%e8%80%83%e8%99%91%e9%a1%ba%e5%ba%8f%e7%9a%84%e8%a6%81%e6%b1%82">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-不用考虑在哪个node节点上运行">
 • 不用考虑在哪个node节点上运行
 &lt;a class="anchor" href="#-%e4%b8%8d%e7%94%a8%e8%80%83%e8%99%91%e5%9c%a8%e5%93%aa%e4%b8%aanode%e8%8a%82%e7%82%b9%e4%b8%8a%e8%bf%90%e8%a1%8c">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-可以随意扩容和缩容">
 • 可以随意扩容和缩容
 &lt;a class="anchor" href="#-%e5%8f%af%e4%bb%a5%e9%9a%8f%e6%84%8f%e6%89%a9%e5%ae%b9%e5%92%8c%e7%bc%a9%e5%ae%b9">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h5 id="有状态服务的特点">
 有状态服务的特点：
 &lt;a class="anchor" href="#%e6%9c%89%e7%8a%b6%e6%80%81%e6%9c%8d%e5%8a%a1%e7%9a%84%e7%89%b9%e7%82%b9">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="-实例之间有差别每个实例都有自己的独特性元数据不同例如etcdzookeeper">
 • 实例之间有差别，每个实例都有自己的独特性，元数据不同，例如etcd，zookeeper
 &lt;a class="anchor" href="#-%e5%ae%9e%e4%be%8b%e4%b9%8b%e9%97%b4%e6%9c%89%e5%b7%ae%e5%88%ab%e6%af%8f%e4%b8%aa%e5%ae%9e%e4%be%8b%e9%83%bd%e6%9c%89%e8%87%aa%e5%b7%b1%e7%9a%84%e7%8b%ac%e7%89%b9%e6%80%a7%e5%85%83%e6%95%b0%e6%8d%ae%e4%b8%8d%e5%90%8c%e4%be%8b%e5%a6%82etcdzookeeper">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-实例之间不对等的关系以及依靠外部存储的应用">
 • 实例之间不对等的关系，以及依靠外部存储的应用
 &lt;a class="anchor" href="#-%e5%ae%9e%e4%be%8b%e4%b9%8b%e9%97%b4%e4%b8%8d%e5%af%b9%e7%ad%89%e7%9a%84%e5%85%b3%e7%b3%bb%e4%bb%a5%e5%8f%8a%e4%be%9d%e9%9d%a0%e5%a4%96%e9%83%a8%e5%ad%98%e5%82%a8%e7%9a%84%e5%ba%94%e7%94%a8">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-常规的service服务和无头服务的区别">
 • 常规的service服务和无头服务的区别
 &lt;a class="anchor" href="#-%e5%b8%b8%e8%a7%84%e7%9a%84service%e6%9c%8d%e5%8a%a1%e5%92%8c%e6%97%a0%e5%a4%b4%e6%9c%8d%e5%8a%a1%e7%9a%84%e5%8c%ba%e5%88%ab">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-service一组pod访问策略提供cluster-ip群集之间通讯还提供负载均衡和服务发现">
 • service：一组Pod访问策略，提供cluster-IP群集之间通讯，还提供负载均衡和服务发现
 &lt;a class="anchor" href="#-service%e4%b8%80%e7%bb%84pod%e8%ae%bf%e9%97%ae%e7%ad%96%e7%95%a5%e6%8f%90%e4%be%9bcluster-ip%e7%be%a4%e9%9b%86%e4%b9%8b%e9%97%b4%e9%80%9a%e8%ae%af%e8%bf%98%e6%8f%90%e4%be%9b%e8%b4%9f%e8%bd%bd%e5%9d%87%e8%a1%a1%e5%92%8c%e6%9c%8d%e5%8a%a1%e5%8f%91%e7%8e%b0">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-headless-service-无头服务不需要cluster-ip直接绑定具体的pod的ip无头服务经常用于statefulset的有状态部署">
 • Headless service 无头服务，不需要cluster-IP，直接绑定具体的Pod的IP，无头服务经常用于statefulset的有状态部署
 &lt;a class="anchor" href="#-headless-service-%e6%97%a0%e5%a4%b4%e6%9c%8d%e5%8a%a1%e4%b8%8d%e9%9c%80%e8%a6%81cluster-ip%e7%9b%b4%e6%8e%a5%e7%bb%91%e5%ae%9a%e5%85%b7%e4%bd%93%e7%9a%84pod%e7%9a%84ip%e6%97%a0%e5%a4%b4%e6%9c%8d%e5%8a%a1%e7%bb%8f%e5%b8%b8%e7%94%a8%e4%ba%8estatefulset%e7%9a%84%e6%9c%89%e7%8a%b6%e6%80%81%e9%83%a8%e7%bd%b2">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-创建无头服务的service资源和dns资源由于有状态服务的ip地址是动态的所以使用无头服务的时候要绑定dns服务">
 • 创建无头服务的service资源和dns资源，由于有状态服务的IP地址是动态的，所以使用无头服务的时候要绑定dns服务
 &lt;a class="anchor" href="#-%e5%88%9b%e5%bb%ba%e6%97%a0%e5%a4%b4%e6%9c%8d%e5%8a%a1%e7%9a%84service%e8%b5%84%e6%ba%90%e5%92%8cdns%e8%b5%84%e6%ba%90%e7%94%b1%e4%ba%8e%e6%9c%89%e7%8a%b6%e6%80%81%e6%9c%8d%e5%8a%a1%e7%9a%84ip%e5%9c%b0%e5%9d%80%e6%98%af%e5%8a%a8%e6%80%81%e7%9a%84%e6%89%80%e4%bb%a5%e4%bd%bf%e7%94%a8%e6%97%a0%e5%a4%b4%e6%9c%8d%e5%8a%a1%e7%9a%84%e6%97%b6%e5%80%99%e8%a6%81%e7%bb%91%e5%ae%9adns%e6%9c%8d%e5%8a%a1">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;ol>
&lt;li>
&lt;h5 id="1-daemonset一次部署所有的node节点都会部署例如一些典型的应用场景-运行集群存储-daemon例如在每个node上运行-glusterdceph">
 \1. DaemonSet：一次部署，所有的node节点都会部署，例如一些典型的应用场景： 运行集群存储 daemon，例如在每个Node上运行 glusterd、ceph
 &lt;a class="anchor" href="#1-daemonset%e4%b8%80%e6%ac%a1%e9%83%a8%e7%bd%b2%e6%89%80%e6%9c%89%e7%9a%84node%e8%8a%82%e7%82%b9%e9%83%bd%e4%bc%9a%e9%83%a8%e7%bd%b2%e4%be%8b%e5%a6%82%e4%b8%80%e4%ba%9b%e5%85%b8%e5%9e%8b%e7%9a%84%e5%ba%94%e7%94%a8%e5%9c%ba%e6%99%af-%e8%bf%90%e8%a1%8c%e9%9b%86%e7%be%a4%e5%ad%98%e5%82%a8-daemon%e4%be%8b%e5%a6%82%e5%9c%a8%e6%af%8f%e4%b8%aanode%e4%b8%8a%e8%bf%90%e8%a1%8c-glusterdceph">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>
&lt;h5 id="-在每个node上运行日志收集-daemon例如-fluentd-logstash">
 • 在每个Node上运行日志收集 daemon，例如 fluentd、 logstash
 &lt;a class="anchor" href="#-%e5%9c%a8%e6%af%8f%e4%b8%aanode%e4%b8%8a%e8%bf%90%e8%a1%8c%e6%97%a5%e5%bf%97%e6%94%b6%e9%9b%86-daemon%e4%be%8b%e5%a6%82-fluentd-logstash">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-在每个node上运行监控-daemon例如-prometheus-node-exporter">
 • 在每个Node上运行监控 daemon，例如 Prometheus Node Exporter
 &lt;a class="anchor" href="#-%e5%9c%a8%e6%af%8f%e4%b8%aanode%e4%b8%8a%e8%bf%90%e8%a1%8c%e7%9b%91%e6%8e%a7-daemon%e4%be%8b%e5%a6%82-prometheus-node-exporter">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-在每一个node上运行一个pod">
 • 在每一个Node上运行一个Pod
 &lt;a class="anchor" href="#-%e5%9c%a8%e6%af%8f%e4%b8%80%e4%b8%aanode%e4%b8%8a%e8%bf%90%e8%a1%8c%e4%b8%80%e4%b8%aapod">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-新加入的node也同样会自动运行一个pod">
 • 新加入的Node也同样会自动运行一个Pod
 &lt;a class="anchor" href="#-%e6%96%b0%e5%8a%a0%e5%85%a5%e7%9a%84node%e4%b9%9f%e5%90%8c%e6%a0%b7%e4%bc%9a%e8%87%aa%e5%8a%a8%e8%bf%90%e8%a1%8c%e4%b8%80%e4%b8%aapod">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-应用场景监控分布式存储日志收集等">
 • 应用场景：监控，分布式存储，日志收集等
 &lt;a class="anchor" href="#-%e5%ba%94%e7%94%a8%e5%9c%ba%e6%99%af%e7%9b%91%e6%8e%a7%e5%88%86%e5%b8%83%e5%bc%8f%e5%ad%98%e5%82%a8%e6%97%a5%e5%bf%97%e6%94%b6%e9%9b%86%e7%ad%89">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;ol>
&lt;li>
&lt;h5 id="1-job一次性的执行任务">
 \1. Job：一次性的执行任务
 &lt;a class="anchor" href="#1-job%e4%b8%80%e6%ac%a1%e6%80%a7%e7%9a%84%e6%89%a7%e8%a1%8c%e4%bb%bb%e5%8a%a1">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>
&lt;h5 id="-一次性执行任务类似linux中的job">
 • 一次性执行任务，类似Linux中的job
 &lt;a class="anchor" href="#-%e4%b8%80%e6%ac%a1%e6%80%a7%e6%89%a7%e8%a1%8c%e4%bb%bb%e5%8a%a1%e7%b1%bb%e4%bc%bclinux%e4%b8%ad%e7%9a%84job">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-应用场景如离线数据处理视频解码等业务">
 • 应用场景：如离线数据处理，视频解码等业务
 &lt;a class="anchor" href="#-%e5%ba%94%e7%94%a8%e5%9c%ba%e6%99%af%e5%a6%82%e7%a6%bb%e7%ba%bf%e6%95%b0%e6%8d%ae%e5%a4%84%e7%90%86%e8%a7%86%e9%a2%91%e8%a7%a3%e7%a0%81%e7%ad%89%e4%b8%9a%e5%8a%a1">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;ol>
&lt;li>
&lt;h5 id="1-cronjob周期性的执行任务">
 \1. Cronjob：周期性的执行任务
 &lt;a class="anchor" href="#1-cronjob%e5%91%a8%e6%9c%9f%e6%80%a7%e7%9a%84%e6%89%a7%e8%a1%8c%e4%bb%bb%e5%8a%a1">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>
&lt;h5 id="-周期性任务像linux的crontab一样">
 • 周期性任务，像Linux的Crontab一样
 &lt;a class="anchor" href="#-%e5%91%a8%e6%9c%9f%e6%80%a7%e4%bb%bb%e5%8a%a1%e5%83%8flinux%e7%9a%84crontab%e4%b8%80%e6%a0%b7">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-应用场景如通知备份等">
 • 应用场景：如通知，备份等
 &lt;a class="anchor" href="#-%e5%ba%94%e7%94%a8%e5%9c%ba%e6%99%af%e5%a6%82%e9%80%9a%e7%9f%a5%e5%a4%87%e4%bb%bd%e7%ad%89">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-使用cronjob要慎重用完之后要删掉不然会占用很多资源">
 • 使用cronjob要慎重，用完之后要删掉，不然会占用很多资源
 &lt;a class="anchor" href="#-%e4%bd%bf%e7%94%a8cronjob%e8%a6%81%e6%85%8e%e9%87%8d%e7%94%a8%e5%ae%8c%e4%b9%8b%e5%90%8e%e8%a6%81%e5%88%a0%e6%8e%89%e4%b8%8d%e7%84%b6%e4%bc%9a%e5%8d%a0%e7%94%a8%e5%be%88%e5%a4%9a%e8%b5%84%e6%ba%90">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h2 id="ingress-controller的工作机制">
 ingress-controller的工作机制
 &lt;a class="anchor" href="#ingress-controller%e7%9a%84%e5%b7%a5%e4%bd%9c%e6%9c%ba%e5%88%b6">#&lt;/a>
&lt;/h2>
&lt;h5 id="通常情况下service和pod的ip仅可在集群内部访问">
 通常情况下，service和pod的IP仅可在集群内部访问
 &lt;a class="anchor" href="#%e9%80%9a%e5%b8%b8%e6%83%85%e5%86%b5%e4%b8%8bservice%e5%92%8cpod%e7%9a%84ip%e4%bb%85%e5%8f%af%e5%9c%a8%e9%9b%86%e7%be%a4%e5%86%85%e9%83%a8%e8%ae%bf%e9%97%ae">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="-k8s提供了service方式nodeport-来提供对外的服务外部的服务可以通过访问node节点ipnodeport端口来访问集群内部的资源外部的请求先到达service所选中的节点上然后负载均衡到每一个节点上">
 • k8s提供了service方式：NodePort 来提供对外的服务，外部的服务可以通过访问Node节点ip+NodePort端口来访问集群内部的资源，外部的请求先到达service所选中的节点上，然后负载均衡到每一个节点上。
 &lt;a class="anchor" href="#-k8s%e6%8f%90%e4%be%9b%e4%ba%86service%e6%96%b9%e5%bc%8fnodeport-%e6%9d%a5%e6%8f%90%e4%be%9b%e5%af%b9%e5%a4%96%e7%9a%84%e6%9c%8d%e5%8a%a1%e5%a4%96%e9%83%a8%e7%9a%84%e6%9c%8d%e5%8a%a1%e5%8f%af%e4%bb%a5%e9%80%9a%e8%bf%87%e8%ae%bf%e9%97%aenode%e8%8a%82%e7%82%b9ipnodeport%e7%ab%af%e5%8f%a3%e6%9d%a5%e8%ae%bf%e9%97%ae%e9%9b%86%e7%be%a4%e5%86%85%e9%83%a8%e7%9a%84%e8%b5%84%e6%ba%90%e5%a4%96%e9%83%a8%e7%9a%84%e8%af%b7%e6%b1%82%e5%85%88%e5%88%b0%e8%be%beservice%e6%89%80%e9%80%89%e4%b8%ad%e7%9a%84%e8%8a%82%e7%82%b9%e4%b8%8a%e7%84%b6%e5%90%8e%e8%b4%9f%e8%bd%bd%e5%9d%87%e8%a1%a1%e5%88%b0%e6%af%8f%e4%b8%80%e4%b8%aa%e8%8a%82%e7%82%b9%e4%b8%8a">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h5 id="nodeport虽然提供了对外的方式但也有很大弊端">
 NodePort虽然提供了对外的方式但也有很大弊端：
 &lt;a class="anchor" href="#nodeport%e8%99%bd%e7%84%b6%e6%8f%90%e4%be%9b%e4%ba%86%e5%af%b9%e5%a4%96%e7%9a%84%e6%96%b9%e5%bc%8f%e4%bd%86%e4%b9%9f%e6%9c%89%e5%be%88%e5%a4%a7%e5%bc%8a%e7%ab%af">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="-由于service的实现方式user_space-iptebles-3-ipvs方式这三种方式只支持在4层协议通信不支持7层协议因此nodeport不能代理https服务">
 • 由于service的实现方式：user_space 、iptebles、 3 ipvs、方式这三种方式只支持在4层协议通信，不支持7层协议，因此NodePort不能代理https服务。
 &lt;a class="anchor" href="#-%e7%94%b1%e4%ba%8eservice%e7%9a%84%e5%ae%9e%e7%8e%b0%e6%96%b9%e5%bc%8fuser_space-iptebles-3-ipvs%e6%96%b9%e5%bc%8f%e8%bf%99%e4%b8%89%e7%a7%8d%e6%96%b9%e5%bc%8f%e5%8f%aa%e6%94%af%e6%8c%81%e5%9c%a84%e5%b1%82%e5%8d%8f%e8%ae%ae%e9%80%9a%e4%bf%a1%e4%b8%8d%e6%94%af%e6%8c%817%e5%b1%82%e5%8d%8f%e8%ae%ae%e5%9b%a0%e6%ad%a4nodeport%e4%b8%8d%e8%83%bd%e4%bb%a3%e7%90%86https%e6%9c%8d%e5%8a%a1">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-nodeport-需要暴露service所属每个node节点上端口当需求越来越多端口数量过多导致维护成本过高并且集群不好管理">
 • NodePort 需要暴露service所属每个node节点上端口，当需求越来越多，端口数量过多，导致维护成本过高，并且集群不好管理。
 &lt;a class="anchor" href="#-nodeport-%e9%9c%80%e8%a6%81%e6%9a%b4%e9%9c%b2service%e6%89%80%e5%b1%9e%e6%af%8f%e4%b8%aanode%e8%8a%82%e7%82%b9%e4%b8%8a%e7%ab%af%e5%8f%a3%e5%bd%93%e9%9c%80%e6%b1%82%e8%b6%8a%e6%9d%a5%e8%b6%8a%e5%a4%9a%e7%ab%af%e5%8f%a3%e6%95%b0%e9%87%8f%e8%bf%87%e5%a4%9a%e5%af%bc%e8%87%b4%e7%bb%b4%e6%8a%a4%e6%88%90%e6%9c%ac%e8%bf%87%e9%ab%98%e5%b9%b6%e4%b8%94%e9%9b%86%e7%be%a4%e4%b8%8d%e5%a5%bd%e7%ae%a1%e7%90%86">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h5 id="原理">
 原理
 &lt;a class="anchor" href="#%e5%8e%9f%e7%90%86">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="-ingress也是kubernetes-api的标准资源类型之一它其实就是一组基于dns名称host或url路径把请求转发到指定的service资源的规则用于将集群外部的请求流量转发到集群内部完成的服务发布我们需要明白的是ingress资源自身不能进行流量穿透仅仅是一组规则的集合这些集合规则还需要其他功能的辅助比如监听某套接字然后根据这些规则的匹配进行路由转发这些能够为ingress资源监听套接字并将流量转发的组件就是ingress-controller">
 • Ingress也是Kubernetes API的标准资源类型之一，它其实就是一组基于DNS名称（host）或URL路径把请求转发到指定的Service资源的规则。用于将集群外部的请求流量转发到集群内部完成的服务发布。我们需要明白的是，Ingress资源自身不能进行“流量穿透”，仅仅是一组规则的集合，这些集合规则还需要其他功能的辅助，比如监听某套接字，然后根据这些规则的匹配进行路由转发，这些能够为Ingress资源监听套接字并将流量转发的组件就是Ingress Controller。
 &lt;a class="anchor" href="#-ingress%e4%b9%9f%e6%98%afkubernetes-api%e7%9a%84%e6%a0%87%e5%87%86%e8%b5%84%e6%ba%90%e7%b1%bb%e5%9e%8b%e4%b9%8b%e4%b8%80%e5%ae%83%e5%85%b6%e5%ae%9e%e5%b0%b1%e6%98%af%e4%b8%80%e7%bb%84%e5%9f%ba%e4%ba%8edns%e5%90%8d%e7%a7%b0host%e6%88%96url%e8%b7%af%e5%be%84%e6%8a%8a%e8%af%b7%e6%b1%82%e8%bd%ac%e5%8f%91%e5%88%b0%e6%8c%87%e5%ae%9a%e7%9a%84service%e8%b5%84%e6%ba%90%e7%9a%84%e8%a7%84%e5%88%99%e7%94%a8%e4%ba%8e%e5%b0%86%e9%9b%86%e7%be%a4%e5%a4%96%e9%83%a8%e7%9a%84%e8%af%b7%e6%b1%82%e6%b5%81%e9%87%8f%e8%bd%ac%e5%8f%91%e5%88%b0%e9%9b%86%e7%be%a4%e5%86%85%e9%83%a8%e5%ae%8c%e6%88%90%e7%9a%84%e6%9c%8d%e5%8a%a1%e5%8f%91%e5%b8%83%e6%88%91%e4%bb%ac%e9%9c%80%e8%a6%81%e6%98%8e%e7%99%bd%e7%9a%84%e6%98%afingress%e8%b5%84%e6%ba%90%e8%87%aa%e8%ba%ab%e4%b8%8d%e8%83%bd%e8%bf%9b%e8%a1%8c%e6%b5%81%e9%87%8f%e7%a9%bf%e9%80%8f%e4%bb%85%e4%bb%85%e6%98%af%e4%b8%80%e7%bb%84%e8%a7%84%e5%88%99%e7%9a%84%e9%9b%86%e5%90%88%e8%bf%99%e4%ba%9b%e9%9b%86%e5%90%88%e8%a7%84%e5%88%99%e8%bf%98%e9%9c%80%e8%a6%81%e5%85%b6%e4%bb%96%e5%8a%9f%e8%83%bd%e7%9a%84%e8%be%85%e5%8a%a9%e6%af%94%e5%a6%82%e7%9b%91%e5%90%ac%e6%9f%90%e5%a5%97%e6%8e%a5%e5%ad%97%e7%84%b6%e5%90%8e%e6%a0%b9%e6%8d%ae%e8%bf%99%e4%ba%9b%e8%a7%84%e5%88%99%e7%9a%84%e5%8c%b9%e9%85%8d%e8%bf%9b%e8%a1%8c%e8%b7%af%e7%94%b1%e8%bd%ac%e5%8f%91%e8%bf%99%e4%ba%9b%e8%83%bd%e5%a4%9f%e4%b8%baingress%e8%b5%84%e6%ba%90%e7%9b%91%e5%90%ac%e5%a5%97%e6%8e%a5%e5%ad%97%e5%b9%b6%e5%b0%86%e6%b5%81%e9%87%8f%e8%bd%ac%e5%8f%91%e7%9a%84%e7%bb%84%e4%bb%b6%e5%b0%b1%e6%98%afingress-controller">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-ingress-控制器不同于deployment-等pod控制器的是ingress控制器不直接运行为kube-controller-manager的一部分它仅仅是kubernetes集群的一个附件类似于coredns需要在集群上单独部署">
 • Ingress 控制器不同于Deployment 等pod控制器的是，Ingress控制器不直接运行为kube-controller-manager的一部分，它仅仅是Kubernetes集群的一个附件，类似于CoreDNS，需要在集群上单独部署。
 &lt;a class="anchor" href="#-ingress-%e6%8e%a7%e5%88%b6%e5%99%a8%e4%b8%8d%e5%90%8c%e4%ba%8edeployment-%e7%ad%89pod%e6%8e%a7%e5%88%b6%e5%99%a8%e7%9a%84%e6%98%afingress%e6%8e%a7%e5%88%b6%e5%99%a8%e4%b8%8d%e7%9b%b4%e6%8e%a5%e8%bf%90%e8%a1%8c%e4%b8%bakube-controller-manager%e7%9a%84%e4%b8%80%e9%83%a8%e5%88%86%e5%ae%83%e4%bb%85%e4%bb%85%e6%98%afkubernetes%e9%9b%86%e7%be%a4%e7%9a%84%e4%b8%80%e4%b8%aa%e9%99%84%e4%bb%b6%e7%b1%bb%e4%bc%bc%e4%ba%8ecoredns%e9%9c%80%e8%a6%81%e5%9c%a8%e9%9b%86%e7%be%a4%e4%b8%8a%e5%8d%95%e7%8b%ac%e9%83%a8%e7%bd%b2">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-ingress-controller通过监视api-server获取相关ingressserviceendpointsecretnodeconfigmap对象并在程序内部不断循环监视相关service是否有新的endpoints变化一旦发生变化则自动更新nginxconf模板配置并产生新的配置文件进行reload">
 • ingress controller通过监视api server获取相关ingress、service、endpoint、secret、node、configmap对象，并在程序内部不断循环监视相关service是否有新的endpoints变化，一旦发生变化则自动更新nginx.conf模板配置并产生新的配置文件进行reload
 &lt;a class="anchor" href="#-ingress-controller%e9%80%9a%e8%bf%87%e7%9b%91%e8%a7%86api-server%e8%8e%b7%e5%8f%96%e7%9b%b8%e5%85%b3ingressserviceendpointsecretnodeconfigmap%e5%af%b9%e8%b1%a1%e5%b9%b6%e5%9c%a8%e7%a8%8b%e5%ba%8f%e5%86%85%e9%83%a8%e4%b8%8d%e6%96%ad%e5%be%aa%e7%8e%af%e7%9b%91%e8%a7%86%e7%9b%b8%e5%85%b3service%e6%98%af%e5%90%a6%e6%9c%89%e6%96%b0%e7%9a%84endpoints%e5%8f%98%e5%8c%96%e4%b8%80%e6%97%a6%e5%8f%91%e7%94%9f%e5%8f%98%e5%8c%96%e5%88%99%e8%87%aa%e5%8a%a8%e6%9b%b4%e6%96%b0nginxconf%e6%a8%a1%e6%9d%bf%e9%85%8d%e7%bd%ae%e5%b9%b6%e4%ba%a7%e7%94%9f%e6%96%b0%e7%9a%84%e9%85%8d%e7%bd%ae%e6%96%87%e4%bb%b6%e8%bf%9b%e8%a1%8creload">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h2 id="k8s的调度机制">
 k8s的调度机制
 &lt;a class="anchor" href="#k8s%e7%9a%84%e8%b0%83%e5%ba%a6%e6%9c%ba%e5%88%b6">#&lt;/a>
&lt;/h2>
&lt;ol>
&lt;li>
&lt;h5 id="1-scheduler工作原理-请求及scheduler调度步骤">
 \1. Scheduler工作原理： 请求及Scheduler调度步骤：
 &lt;a class="anchor" href="#1-scheduler%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86-%e8%af%b7%e6%b1%82%e5%8f%8ascheduler%e8%b0%83%e5%ba%a6%e6%ad%a5%e9%aa%a4">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>
&lt;h5 id="-节点预选predicate排除完全不满足条件的节点如内存大小端口等条件不满足">
 • 节点预选(Predicate)：排除完全不满足条件的节点，如内存大小，端口等条件不满足。
 &lt;a class="anchor" href="#-%e8%8a%82%e7%82%b9%e9%a2%84%e9%80%89predicate%e6%8e%92%e9%99%a4%e5%ae%8c%e5%85%a8%e4%b8%8d%e6%bb%a1%e8%b6%b3%e6%9d%a1%e4%bb%b6%e7%9a%84%e8%8a%82%e7%82%b9%e5%a6%82%e5%86%85%e5%ad%98%e5%a4%a7%e5%b0%8f%e7%ab%af%e5%8f%a3%e7%ad%89%e6%9d%a1%e4%bb%b6%e4%b8%8d%e6%bb%a1%e8%b6%b3">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-节点优先级排序priority根据优先级选出最佳节点">
 • 节点优先级排序(Priority)：根据优先级选出最佳节点
 &lt;a class="anchor" href="#-%e8%8a%82%e7%82%b9%e4%bc%98%e5%85%88%e7%ba%a7%e6%8e%92%e5%ba%8fpriority%e6%a0%b9%e6%8d%ae%e4%bc%98%e5%85%88%e7%ba%a7%e9%80%89%e5%87%ba%e6%9c%80%e4%bd%b3%e8%8a%82%e7%82%b9">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-节点择优select根据优先级选定节点">
 • 节点择优(Select)：根据优先级选定节点
 &lt;a class="anchor" href="#-%e8%8a%82%e7%82%b9%e6%8b%a9%e4%bc%98select%e6%a0%b9%e6%8d%ae%e4%bc%98%e5%85%88%e7%ba%a7%e9%80%89%e5%ae%9a%e8%8a%82%e7%82%b9">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;ol>
&lt;li>
&lt;h5 id="1-具体步骤">
 \1. 具体步骤：
 &lt;a class="anchor" href="#1-%e5%85%b7%e4%bd%93%e6%ad%a5%e9%aa%a4">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>
&lt;h5 id="-首先用户通过-kubernetes-客户端-kubectl-提交创建-pod-的-yaml-的文件向kubernetes-系统发起资源请求该资源请求被提交到">
 • 首先用户通过 Kubernetes 客户端 Kubectl 提交创建 Pod 的 Yaml 的文件，向Kubernetes 系统发起资源请求，该资源请求被提交到
 &lt;a class="anchor" href="#-%e9%a6%96%e5%85%88%e7%94%a8%e6%88%b7%e9%80%9a%e8%bf%87-kubernetes-%e5%ae%a2%e6%88%b7%e7%ab%af-kubectl-%e6%8f%90%e4%ba%a4%e5%88%9b%e5%bb%ba-pod-%e7%9a%84-yaml-%e7%9a%84%e6%96%87%e4%bb%b6%e5%90%91kubernetes-%e7%b3%bb%e7%bb%9f%e5%8f%91%e8%b5%b7%e8%b5%84%e6%ba%90%e8%af%b7%e6%b1%82%e8%af%a5%e8%b5%84%e6%ba%90%e8%af%b7%e6%b1%82%e8%a2%ab%e6%8f%90%e4%ba%a4%e5%88%b0">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-kubernetes-系统中用户通过命令行工具-kubectl-向-kubernetes-集群即-apiserver-用-的方式发送post请求即创建-pod-的请求">
 • Kubernetes 系统中，用户通过命令行工具 Kubectl 向 Kubernetes 集群即 APIServer 用 的方式发送“POST”请求，即创建 Pod 的请求。
 &lt;a class="anchor" href="#-kubernetes-%e7%b3%bb%e7%bb%9f%e4%b8%ad%e7%94%a8%e6%88%b7%e9%80%9a%e8%bf%87%e5%91%bd%e4%bb%a4%e8%a1%8c%e5%b7%a5%e5%85%b7-kubectl-%e5%90%91-kubernetes-%e9%9b%86%e7%be%a4%e5%8d%b3-apiserver-%e7%94%a8-%e7%9a%84%e6%96%b9%e5%bc%8f%e5%8f%91%e9%80%81post%e8%af%b7%e6%b1%82%e5%8d%b3%e5%88%9b%e5%bb%ba-pod-%e7%9a%84%e8%af%b7%e6%b1%82">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-apiserver-接收到请求后把创建-pod-的信息存储到-etcd-中从集群运行那一刻起资源调度系统-scheduler-就会定时去监控-apiserver">
 • APIServer 接收到请求后把创建 Pod 的信息存储到 Etcd 中，从集群运行那一刻起，资源调度系统 Scheduler 就会定时去监控 APIServer
 &lt;a class="anchor" href="#-apiserver-%e6%8e%a5%e6%94%b6%e5%88%b0%e8%af%b7%e6%b1%82%e5%90%8e%e6%8a%8a%e5%88%9b%e5%bb%ba-pod-%e7%9a%84%e4%bf%a1%e6%81%af%e5%ad%98%e5%82%a8%e5%88%b0-etcd-%e4%b8%ad%e4%bb%8e%e9%9b%86%e7%be%a4%e8%bf%90%e8%a1%8c%e9%82%a3%e4%b8%80%e5%88%bb%e8%b5%b7%e8%b5%84%e6%ba%90%e8%b0%83%e5%ba%a6%e7%b3%bb%e7%bb%9f-scheduler-%e5%b0%b1%e4%bc%9a%e5%ae%9a%e6%97%b6%e5%8e%bb%e7%9b%91%e6%8e%a7-apiserver">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-通过-apiserver-得到创建-pod-的信息scheduler-采用-watch-机制一旦-etcd-存储-pod-信息成功便会立即通知apiserver">
 • 通过 APIServer 得到创建 Pod 的信息，Scheduler 采用 watch 机制，一旦 Etcd 存储 Pod 信息成功便会立即通知APIServer，
 &lt;a class="anchor" href="#-%e9%80%9a%e8%bf%87-apiserver-%e5%be%97%e5%88%b0%e5%88%9b%e5%bb%ba-pod-%e7%9a%84%e4%bf%a1%e6%81%afscheduler-%e9%87%87%e7%94%a8-watch-%e6%9c%ba%e5%88%b6%e4%b8%80%e6%97%a6-etcd-%e5%ad%98%e5%82%a8-pod-%e4%bf%a1%e6%81%af%e6%88%90%e5%8a%9f%e4%be%bf%e4%bc%9a%e7%ab%8b%e5%8d%b3%e9%80%9a%e7%9f%a5apiserver">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-apiserver会立即把pod创建的消息通知schedulerscheduler发现-pod-的属性中-dest-node-为空时dest-node便会立即触发调度流程进行调度">
 • APIServer会立即把Pod创建的消息通知Scheduler，Scheduler发现 Pod 的属性中 Dest Node 为空时（Dest Node=””）便会立即触发调度流程进行调度。
 &lt;a class="anchor" href="#-apiserver%e4%bc%9a%e7%ab%8b%e5%8d%b3%e6%8a%8apod%e5%88%9b%e5%bb%ba%e7%9a%84%e6%b6%88%e6%81%af%e9%80%9a%e7%9f%a5schedulerscheduler%e5%8f%91%e7%8e%b0-pod-%e7%9a%84%e5%b1%9e%e6%80%a7%e4%b8%ad-dest-node-%e4%b8%ba%e7%a9%ba%e6%97%b6dest-node%e4%be%bf%e4%bc%9a%e7%ab%8b%e5%8d%b3%e8%a7%a6%e5%8f%91%e8%b0%83%e5%ba%a6%e6%b5%81%e7%a8%8b%e8%bf%9b%e8%a1%8c%e8%b0%83%e5%ba%a6">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-而这一个创建pod对象在调度的过程当中有3个阶段节点预选节点优选节点选定从而筛选出最佳的节点">
 • 而这一个创建Pod对象，在调度的过程当中有3个阶段：节点预选、节点优选、节点选定，从而筛选出最佳的节点
 &lt;a class="anchor" href="#-%e8%80%8c%e8%bf%99%e4%b8%80%e4%b8%aa%e5%88%9b%e5%bb%bapod%e5%af%b9%e8%b1%a1%e5%9c%a8%e8%b0%83%e5%ba%a6%e7%9a%84%e8%bf%87%e7%a8%8b%e5%bd%93%e4%b8%ad%e6%9c%893%e4%b8%aa%e9%98%b6%e6%ae%b5%e8%8a%82%e7%82%b9%e9%a2%84%e9%80%89%e8%8a%82%e7%82%b9%e4%bc%98%e9%80%89%e8%8a%82%e7%82%b9%e9%80%89%e5%ae%9a%e4%bb%8e%e8%80%8c%e7%ad%9b%e9%80%89%e5%87%ba%e6%9c%80%e4%bd%b3%e7%9a%84%e8%8a%82%e7%82%b9">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;ul>
&lt;li>
&lt;h5 id="-节点预选基于一系列的预选规则对每个节点进行检查将那些不符合条件的节点过滤从而完成节点的预选">
 • 节点预选：基于一系列的预选规则对每个节点进行检查，将那些不符合条件的节点过滤，从而完成节点的预选
 &lt;a class="anchor" href="#-%e8%8a%82%e7%82%b9%e9%a2%84%e9%80%89%e5%9f%ba%e4%ba%8e%e4%b8%80%e7%b3%bb%e5%88%97%e7%9a%84%e9%a2%84%e9%80%89%e8%a7%84%e5%88%99%e5%af%b9%e6%af%8f%e4%b8%aa%e8%8a%82%e7%82%b9%e8%bf%9b%e8%a1%8c%e6%a3%80%e6%9f%a5%e5%b0%86%e9%82%a3%e4%ba%9b%e4%b8%8d%e7%ac%a6%e5%90%88%e6%9d%a1%e4%bb%b6%e7%9a%84%e8%8a%82%e7%82%b9%e8%bf%87%e6%bb%a4%e4%bb%8e%e8%80%8c%e5%ae%8c%e6%88%90%e8%8a%82%e7%82%b9%e7%9a%84%e9%a2%84%e9%80%89">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-节点优选对预选出的节点进行优先级排序以便选出最合适运行pod对象的节点">
 • 节点优选：对预选出的节点进行优先级排序，以便选出最合适运行Pod对象的节点
 &lt;a class="anchor" href="#-%e8%8a%82%e7%82%b9%e4%bc%98%e9%80%89%e5%af%b9%e9%a2%84%e9%80%89%e5%87%ba%e7%9a%84%e8%8a%82%e7%82%b9%e8%bf%9b%e8%a1%8c%e4%bc%98%e5%85%88%e7%ba%a7%e6%8e%92%e5%ba%8f%e4%bb%a5%e4%be%bf%e9%80%89%e5%87%ba%e6%9c%80%e5%90%88%e9%80%82%e8%bf%90%e8%a1%8cpod%e5%af%b9%e8%b1%a1%e7%9a%84%e8%8a%82%e7%82%b9">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-节点选定从优先级排序结果中挑选出优先级最高的节点运行pod当这类节点多于1个时则进行随机选择">
 • 节点选定：从优先级排序结果中挑选出优先级最高的节点运行Pod，当这类节点多于1个时，则进行随机选择
 &lt;a class="anchor" href="#-%e8%8a%82%e7%82%b9%e9%80%89%e5%ae%9a%e4%bb%8e%e4%bc%98%e5%85%88%e7%ba%a7%e6%8e%92%e5%ba%8f%e7%bb%93%e6%9e%9c%e4%b8%ad%e6%8c%91%e9%80%89%e5%87%ba%e4%bc%98%e5%85%88%e7%ba%a7%e6%9c%80%e9%ab%98%e7%9a%84%e8%8a%82%e7%82%b9%e8%bf%90%e8%a1%8cpod%e5%bd%93%e8%bf%99%e7%b1%bb%e8%8a%82%e7%82%b9%e5%a4%9a%e4%ba%8e1%e4%b8%aa%e6%97%b6%e5%88%99%e8%bf%9b%e8%a1%8c%e9%9a%8f%e6%9c%ba%e9%80%89%e6%8b%a9">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;ol>
&lt;li>
&lt;h5 id="1-k8s的调用工作方式">
 \1. k8s的调用工作方式
 &lt;a class="anchor" href="#1-k8s%e7%9a%84%e8%b0%83%e7%94%a8%e5%b7%a5%e4%bd%9c%e6%96%b9%e5%bc%8f">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>
&lt;h5 id="-kubernetes调度器作为集群的大脑在如何提高集群的资源利用率保证集群中服务的稳定运行中也会变得越来越重要kubernetes的资源分为两种属性">
 • Kubernetes调度器作为集群的大脑，在如何提高集群的资源利用率、保证集群中服务的稳定运行中也会变得越来越重要Kubernetes的资源分为两种属性。
 &lt;a class="anchor" href="#-kubernetes%e8%b0%83%e5%ba%a6%e5%99%a8%e4%bd%9c%e4%b8%ba%e9%9b%86%e7%be%a4%e7%9a%84%e5%a4%a7%e8%84%91%e5%9c%a8%e5%a6%82%e4%bd%95%e6%8f%90%e9%ab%98%e9%9b%86%e7%be%a4%e7%9a%84%e8%b5%84%e6%ba%90%e5%88%a9%e7%94%a8%e7%8e%87%e4%bf%9d%e8%af%81%e9%9b%86%e7%be%a4%e4%b8%ad%e6%9c%8d%e5%8a%a1%e7%9a%84%e7%a8%b3%e5%ae%9a%e8%bf%90%e8%a1%8c%e4%b8%ad%e4%b9%9f%e4%bc%9a%e5%8f%98%e5%be%97%e8%b6%8a%e6%9d%a5%e8%b6%8a%e9%87%8d%e8%a6%81kubernetes%e7%9a%84%e8%b5%84%e6%ba%90%e5%88%86%e4%b8%ba%e4%b8%a4%e7%a7%8d%e5%b1%9e%e6%80%a7">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;ul>
&lt;li>
&lt;h5 id="-可压缩资源例如cpu循环disk-io带宽都是可以被限制和被回收的对于一个pod来说可以降低这些资源的使用量而不去杀掉pod">
 • 可压缩资源（例如CPU循环，Disk I/O带宽）都是可以被限制和被回收的，对于一个Pod来说可以降低这些资源的使用量而不去杀掉Pod。
 &lt;a class="anchor" href="#-%e5%8f%af%e5%8e%8b%e7%bc%a9%e8%b5%84%e6%ba%90%e4%be%8b%e5%a6%82cpu%e5%be%aa%e7%8e%afdisk-io%e5%b8%a6%e5%ae%bd%e9%83%bd%e6%98%af%e5%8f%af%e4%bb%a5%e8%a2%ab%e9%99%90%e5%88%b6%e5%92%8c%e8%a2%ab%e5%9b%9e%e6%94%b6%e7%9a%84%e5%af%b9%e4%ba%8e%e4%b8%80%e4%b8%aapod%e6%9d%a5%e8%af%b4%e5%8f%af%e4%bb%a5%e9%99%8d%e4%bd%8e%e8%bf%99%e4%ba%9b%e8%b5%84%e6%ba%90%e7%9a%84%e4%bd%bf%e7%94%a8%e9%87%8f%e8%80%8c%e4%b8%8d%e5%8e%bb%e6%9d%80%e6%8e%89pod">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-不可压缩资源例如内存硬盘空间一般来说不杀掉pod就没法回收未来kubernetes会加入更多资源如网络带宽存储iops的支持">
 • 不可压缩资源（例如内存、硬盘空间）一般来说不杀掉Pod就没法回收。未来Kubernetes会加入更多资源，如网络带宽，存储IOPS的支持。
 &lt;a class="anchor" href="#-%e4%b8%8d%e5%8f%af%e5%8e%8b%e7%bc%a9%e8%b5%84%e6%ba%90%e4%be%8b%e5%a6%82%e5%86%85%e5%ad%98%e7%a1%ac%e7%9b%98%e7%a9%ba%e9%97%b4%e4%b8%80%e8%88%ac%e6%9d%a5%e8%af%b4%e4%b8%8d%e6%9d%80%e6%8e%89pod%e5%b0%b1%e6%b2%a1%e6%b3%95%e5%9b%9e%e6%94%b6%e6%9c%aa%e6%9d%a5kubernetes%e4%bc%9a%e5%8a%a0%e5%85%a5%e6%9b%b4%e5%a4%9a%e8%b5%84%e6%ba%90%e5%a6%82%e7%bd%91%e7%bb%9c%e5%b8%a6%e5%ae%bd%e5%ad%98%e5%82%a8iops%e7%9a%84%e6%94%af%e6%8c%81">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="kube-proxy的三种工作模式和原理">
 kube-proxy的三种工作模式和原理
 &lt;a class="anchor" href="#kube-proxy%e7%9a%84%e4%b8%89%e7%a7%8d%e5%b7%a5%e4%bd%9c%e6%a8%a1%e5%bc%8f%e5%92%8c%e5%8e%9f%e7%90%86">#&lt;/a>
&lt;/h2>
&lt;ol>
&lt;li>
&lt;h5 id="1-userspace-模式">
 \1. userspace 模式
 &lt;a class="anchor" href="#1-userspace-%e6%a8%a1%e5%bc%8f">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>
&lt;h5 id="-该模式下kube-proxy会为每一个service创建一个监听端口发向cluster-ip的请求被iptables规则重定向到kube-proxy监听的端口上kube-proxy根据lb算法选择一个提供服务的pod并和其建立链接以将请求转发到pod上">
 • 该模式下kube-proxy会为每一个Service创建一个监听端口。发向Cluster IP的请求被Iptables规则重定向到Kube-proxy监听的端口上，Kube-proxy根据LB算法选择一个提供服务的Pod并和其建立链接，以将请求转发到Pod上。
 &lt;a class="anchor" href="#-%e8%af%a5%e6%a8%a1%e5%bc%8f%e4%b8%8bkube-proxy%e4%bc%9a%e4%b8%ba%e6%af%8f%e4%b8%80%e4%b8%aaservice%e5%88%9b%e5%bb%ba%e4%b8%80%e4%b8%aa%e7%9b%91%e5%90%ac%e7%ab%af%e5%8f%a3%e5%8f%91%e5%90%91cluster-ip%e7%9a%84%e8%af%b7%e6%b1%82%e8%a2%abiptables%e8%a7%84%e5%88%99%e9%87%8d%e5%ae%9a%e5%90%91%e5%88%b0kube-proxy%e7%9b%91%e5%90%ac%e7%9a%84%e7%ab%af%e5%8f%a3%e4%b8%8akube-proxy%e6%a0%b9%e6%8d%aelb%e7%ae%97%e6%b3%95%e9%80%89%e6%8b%a9%e4%b8%80%e4%b8%aa%e6%8f%90%e4%be%9b%e6%9c%8d%e5%8a%a1%e7%9a%84pod%e5%b9%b6%e5%92%8c%e5%85%b6%e5%bb%ba%e7%ab%8b%e9%93%be%e6%8e%a5%e4%bb%a5%e5%b0%86%e8%af%b7%e6%b1%82%e8%bd%ac%e5%8f%91%e5%88%b0pod%e4%b8%8a">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-该模式下kube-proxy充当了一个四层load-balancer的角色由于kube-proxy运行在userspace中在进行转发处理时会增加两次内核和用户空间之间的数据拷贝效率较另外两种模式低一些好处是当后端的pod不可用时kube-proxy可以重试其他pod">
 • 该模式下，Kube-proxy充当了一个四层Load balancer的角色。由于kube-proxy运行在userspace中，在进行转发处理时会增加两次内核和用户空间之间的数据拷贝，效率较另外两种模式低一些；好处是当后端的Pod不可用时，kube-proxy可以重试其他Pod。
 &lt;a class="anchor" href="#-%e8%af%a5%e6%a8%a1%e5%bc%8f%e4%b8%8bkube-proxy%e5%85%85%e5%bd%93%e4%ba%86%e4%b8%80%e4%b8%aa%e5%9b%9b%e5%b1%82load-balancer%e7%9a%84%e8%a7%92%e8%89%b2%e7%94%b1%e4%ba%8ekube-proxy%e8%bf%90%e8%a1%8c%e5%9c%a8userspace%e4%b8%ad%e5%9c%a8%e8%bf%9b%e8%a1%8c%e8%bd%ac%e5%8f%91%e5%a4%84%e7%90%86%e6%97%b6%e4%bc%9a%e5%a2%9e%e5%8a%a0%e4%b8%a4%e6%ac%a1%e5%86%85%e6%a0%b8%e5%92%8c%e7%94%a8%e6%88%b7%e7%a9%ba%e9%97%b4%e4%b9%8b%e9%97%b4%e7%9a%84%e6%95%b0%e6%8d%ae%e6%8b%b7%e8%b4%9d%e6%95%88%e7%8e%87%e8%be%83%e5%8f%a6%e5%a4%96%e4%b8%a4%e7%a7%8d%e6%a8%a1%e5%bc%8f%e4%bd%8e%e4%b8%80%e4%ba%9b%e5%a5%bd%e5%a4%84%e6%98%af%e5%bd%93%e5%90%8e%e7%ab%af%e7%9a%84pod%e4%b8%8d%e5%8f%af%e7%94%a8%e6%97%b6kube-proxy%e5%8f%af%e4%bb%a5%e9%87%8d%e8%af%95%e5%85%b6%e4%bb%96pod">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;ol>
&lt;li>
&lt;h5 id="1-iptables-模式">
 \1. iptables 模式
 &lt;a class="anchor" href="#1-iptables-%e6%a8%a1%e5%bc%8f">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>
&lt;h5 id="-为了避免增加内核和用户空间的数据拷贝操作提高转发效率kube-proxy提供了iptables模式在该模式下kube-proxy为service后端的每个pod创建对应的iptables规则直接将发向cluster-ip的请求重定向到一个pod-ip">
 • 为了避免增加内核和用户空间的数据拷贝操作，提高转发效率，Kube-proxy提供了iptables模式。在该模式下，Kube-proxy为service后端的每个Pod创建对应的iptables规则，直接将发向Cluster IP的请求重定向到一个Pod IP。
 &lt;a class="anchor" href="#-%e4%b8%ba%e4%ba%86%e9%81%bf%e5%85%8d%e5%a2%9e%e5%8a%a0%e5%86%85%e6%a0%b8%e5%92%8c%e7%94%a8%e6%88%b7%e7%a9%ba%e9%97%b4%e7%9a%84%e6%95%b0%e6%8d%ae%e6%8b%b7%e8%b4%9d%e6%93%8d%e4%bd%9c%e6%8f%90%e9%ab%98%e8%bd%ac%e5%8f%91%e6%95%88%e7%8e%87kube-proxy%e6%8f%90%e4%be%9b%e4%ba%86iptables%e6%a8%a1%e5%bc%8f%e5%9c%a8%e8%af%a5%e6%a8%a1%e5%bc%8f%e4%b8%8bkube-proxy%e4%b8%baservice%e5%90%8e%e7%ab%af%e7%9a%84%e6%af%8f%e4%b8%aapod%e5%88%9b%e5%bb%ba%e5%af%b9%e5%ba%94%e7%9a%84iptables%e8%a7%84%e5%88%99%e7%9b%b4%e6%8e%a5%e5%b0%86%e5%8f%91%e5%90%91cluster-ip%e7%9a%84%e8%af%b7%e6%b1%82%e9%87%8d%e5%ae%9a%e5%90%91%e5%88%b0%e4%b8%80%e4%b8%aapod-ip">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-该模式下kube-proxy不承担四层代理的角色只负责创建iptables规则该模式的优点是较userspace模式效率更高但不能提供灵活的lb策略当后端pod不可用时也无法进行重试">
 • 该模式下Kube-proxy不承担四层代理的角色，只负责创建iptables规则。该模式的优点是较userspace模式效率更高，但不能提供灵活的LB策略，当后端Pod不可用时也无法进行重试。
 &lt;a class="anchor" href="#-%e8%af%a5%e6%a8%a1%e5%bc%8f%e4%b8%8bkube-proxy%e4%b8%8d%e6%89%bf%e6%8b%85%e5%9b%9b%e5%b1%82%e4%bb%a3%e7%90%86%e7%9a%84%e8%a7%92%e8%89%b2%e5%8f%aa%e8%b4%9f%e8%b4%a3%e5%88%9b%e5%bb%baiptables%e8%a7%84%e5%88%99%e8%af%a5%e6%a8%a1%e5%bc%8f%e7%9a%84%e4%bc%98%e7%82%b9%e6%98%af%e8%be%83userspace%e6%a8%a1%e5%bc%8f%e6%95%88%e7%8e%87%e6%9b%b4%e9%ab%98%e4%bd%86%e4%b8%8d%e8%83%bd%e6%8f%90%e4%be%9b%e7%81%b5%e6%b4%bb%e7%9a%84lb%e7%ad%96%e7%95%a5%e5%bd%93%e5%90%8e%e7%ab%afpod%e4%b8%8d%e5%8f%af%e7%94%a8%e6%97%b6%e4%b9%9f%e6%97%a0%e6%b3%95%e8%bf%9b%e8%a1%8c%e9%87%8d%e8%af%95">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;ol>
&lt;li>
&lt;h5 id="1-该模式和iptables类似kube-proxy监控pod的变化并创建相应的ipvs-rulesipvs也是在kernel模式下通过netfilter实现的但采用了hash-table来存储规则因此在规则较多的情况下ipvs相对iptables转发效率更高除此以外ipvs支持更多的lb算法如果要设置kube-proxy为ipvs模式必须在操作系统中安装ipvs内核模块">
 \1. 该模式和iptables类似，kube-proxy监控Pod的变化并创建相应的ipvs rules。ipvs也是在kernel模式下通过netfilter实现的，但采用了hash table来存储规则，因此在规则较多的情况下，Ipvs相对iptables转发效率更高。除此以外，ipvs支持更多的LB算法。如果要设置kube-proxy为ipvs模式，必须在操作系统中安装IPVS内核模块。
 &lt;a class="anchor" href="#1-%e8%af%a5%e6%a8%a1%e5%bc%8f%e5%92%8ciptables%e7%b1%bb%e4%bc%bckube-proxy%e7%9b%91%e6%8e%a7pod%e7%9a%84%e5%8f%98%e5%8c%96%e5%b9%b6%e5%88%9b%e5%bb%ba%e7%9b%b8%e5%ba%94%e7%9a%84ipvs-rulesipvs%e4%b9%9f%e6%98%af%e5%9c%a8kernel%e6%a8%a1%e5%bc%8f%e4%b8%8b%e9%80%9a%e8%bf%87netfilter%e5%ae%9e%e7%8e%b0%e7%9a%84%e4%bd%86%e9%87%87%e7%94%a8%e4%ba%86hash-table%e6%9d%a5%e5%ad%98%e5%82%a8%e8%a7%84%e5%88%99%e5%9b%a0%e6%ad%a4%e5%9c%a8%e8%a7%84%e5%88%99%e8%be%83%e5%a4%9a%e7%9a%84%e6%83%85%e5%86%b5%e4%b8%8bipvs%e7%9b%b8%e5%af%b9iptables%e8%bd%ac%e5%8f%91%e6%95%88%e7%8e%87%e6%9b%b4%e9%ab%98%e9%99%a4%e6%ad%a4%e4%bb%a5%e5%a4%96ipvs%e6%94%af%e6%8c%81%e6%9b%b4%e5%a4%9a%e7%9a%84lb%e7%ae%97%e6%b3%95%e5%a6%82%e6%9e%9c%e8%a6%81%e8%ae%be%e7%bd%aekube-proxy%e4%b8%baipvs%e6%a8%a1%e5%bc%8f%e5%bf%85%e9%a1%bb%e5%9c%a8%e6%93%8d%e4%bd%9c%e7%b3%bb%e7%bb%9f%e4%b8%ad%e5%ae%89%e8%a3%85ipvs%e5%86%85%e6%a0%b8%e6%a8%a1%e5%9d%97">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ol></description></item><item><title>2024-04-03 kubekey添加新节点</title><link>https://qq547475331.github.io/docs/kubekey%E6%B7%BB%E5%8A%A0%E6%96%B0%E8%8A%82%E7%82%B9-kubekey-tian-jia-xin-jie-dian/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/kubekey%E6%B7%BB%E5%8A%A0%E6%96%B0%E8%8A%82%E7%82%B9-kubekey-tian-jia-xin-jie-dian/</guid><description>&lt;h1 id="添加新节点">
 添加新节点
 &lt;a class="anchor" href="#%e6%b7%bb%e5%8a%a0%e6%96%b0%e8%8a%82%e7%82%b9">#&lt;/a>
&lt;/h1>
&lt;p>KubeSphere 使用一段时间之后，由于工作负载不断增加，您可能需要水平扩展集群。自 KubeSphere v3.0.0 起，您可以使用全新的安装程序 &lt;a href="https://github.com/kubesphere/kubekey">KubeKey&lt;/a> 将新节点添加到集群。从根本上说，该操作是基于 Kubelet 的注册机制。换言之，新节点将自动加入现有的 Kubernetes 集群。KubeSphere 支持混合环境，这意味着新添加的主机操作系统可以是 CentOS 或者 Ubuntu。&lt;/p>
&lt;p>本教程演示了如何将新节点添加到单节点集群。若要水平扩展多节点集群，操作步骤基本相同。&lt;/p>
&lt;h2 id="准备工作">
 准备工作
 &lt;a class="anchor" href="#%e5%87%86%e5%a4%87%e5%b7%a5%e4%bd%9c">#&lt;/a>
&lt;/h2>
&lt;ul>
&lt;li>您需要一个单节点集群。有关更多信息，请参见&lt;a href="https://kubesphere.com.cn/docs/quick-start/all-in-one-on-linux/">在 Linux 上以 All-in-One 模式安装 KubeSphere&lt;/a>。&lt;/li>
&lt;li>您需要已经&lt;a href="https://kubesphere.com.cn/docs/installing-on-linux/introduction/multioverview/#%e6%ad%a5%e9%aa%a4-2%e4%b8%8b%e8%bd%bd-kubekey">下载了 KubeKey&lt;/a>。&lt;/li>
&lt;/ul>
&lt;p>先执行以下命令以确保您从正确的区域下载 KubeKey。&lt;/p>
&lt;pre tabindex="0">&lt;code>export KKZONE=cn
&lt;/code>&lt;/pre>&lt;p>执行以下命令下载 KubeKey：&lt;/p>
&lt;pre tabindex="0">&lt;code>curl -sfL https://get-kk.kubesphere.io | VERSION=v1.2.1 sh -
&lt;/code>&lt;/pre>&lt;p>备注&lt;/p>
&lt;p>下载 KubeKey 后，如果您将其传输至访问 Googleapis 同样受限的新机器，请您在执行以下步骤之前务必再次执行 &lt;code>export KKZONE=cn&lt;/code> 命令。&lt;/p>
&lt;p>备注&lt;/p>
&lt;p>执行以上命令会下载最新版 KubeKey (v1.2.1)，您可以修改命令中的版本号下载指定版本。&lt;/p>
&lt;h2 id="添加工作节点">
 添加工作节点
 &lt;a class="anchor" href="#%e6%b7%bb%e5%8a%a0%e5%b7%a5%e4%bd%9c%e8%8a%82%e7%82%b9">#&lt;/a>
&lt;/h2>
&lt;p>Ssh分发秘钥&lt;/p>
&lt;pre tabindex="0">&lt;code>ssh-copy-id -i ~/.ssh/id_rsa -p 22 root@192.168.1.108
&lt;/code>&lt;/pre>&lt;ol>
&lt;li>
&lt;p>使用 KubeKey 检索集群信息。以下命令会创建配置文件 (&lt;code>sample.yaml&lt;/code>)。&lt;/p>
&lt;pre tabindex="0">&lt;code>./kk create config3 --from-cluster
&lt;/code>&lt;/pre>&lt;p>备注&lt;/p></description></item><item><title>2024-04-03 Kubernetes API</title><link>https://qq547475331.github.io/docs/kubernetes-api-kubernetesapi/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/kubernetes-api-kubernetesapi/</guid><description>&lt;h2 id="简介">
 简介
 &lt;a class="anchor" href="#%e7%ae%80%e4%bb%8b">#&lt;/a>
&lt;/h2>
&lt;h5 id="kube-apiserver-组件是-k8s-中非常重要的组件每个组件都只能和-kube-apiserver-进行通信kube-apiserver-提供所有的-api">
 kube-apiserver 组件是 k8s 中非常重要的组件，每个组件都只能和 kube-apiserver 进行通信，kube-apiserver 提供所有的 API。
 &lt;a class="anchor" href="#kube-apiserver-%e7%bb%84%e4%bb%b6%e6%98%af-k8s-%e4%b8%ad%e9%9d%9e%e5%b8%b8%e9%87%8d%e8%a6%81%e7%9a%84%e7%bb%84%e4%bb%b6%e6%af%8f%e4%b8%aa%e7%bb%84%e4%bb%b6%e9%83%bd%e5%8f%aa%e8%83%bd%e5%92%8c-kube-apiserver-%e8%bf%9b%e8%a1%8c%e9%80%9a%e4%bf%a1kube-apiserver-%e6%8f%90%e4%be%9b%e6%89%80%e6%9c%89%e7%9a%84-api">#&lt;/a>
&lt;/h5>
&lt;h2 id="资源与-api">
 资源与 API
 &lt;a class="anchor" href="#%e8%b5%84%e6%ba%90%e4%b8%8e-api">#&lt;/a>
&lt;/h2>
&lt;h5 id="在-k8s-中一般都说某某资源并不说接口比如-deploymentservice-等资源这些资源就是-k8s-api-操作的实体最终这些资源都会存储到-etcd-中其实最终就是对-etcd-中的这些资源做-crud">
 在 k8s 中，一般都说某某资源，并不说接口。比如 Deployment，Service 等资源，这些资源就是 k8s api 操作的实体，最终这些资源都会存储到 etcd 中，其实最终就是对 etcd 中的这些资源做 CRUD。
 &lt;a class="anchor" href="#%e5%9c%a8-k8s-%e4%b8%ad%e4%b8%80%e8%88%ac%e9%83%bd%e8%af%b4%e6%9f%90%e6%9f%90%e8%b5%84%e6%ba%90%e5%b9%b6%e4%b8%8d%e8%af%b4%e6%8e%a5%e5%8f%a3%e6%af%94%e5%a6%82-deploymentservice-%e7%ad%89%e8%b5%84%e6%ba%90%e8%bf%99%e4%ba%9b%e8%b5%84%e6%ba%90%e5%b0%b1%e6%98%af-k8s-api-%e6%93%8d%e4%bd%9c%e7%9a%84%e5%ae%9e%e4%bd%93%e6%9c%80%e7%bb%88%e8%bf%99%e4%ba%9b%e8%b5%84%e6%ba%90%e9%83%bd%e4%bc%9a%e5%ad%98%e5%82%a8%e5%88%b0-etcd-%e4%b8%ad%e5%85%b6%e5%ae%9e%e6%9c%80%e7%bb%88%e5%b0%b1%e6%98%af%e5%af%b9-etcd-%e4%b8%ad%e7%9a%84%e8%bf%99%e4%ba%9b%e8%b5%84%e6%ba%90%e5%81%9a-crud">#&lt;/a>
&lt;/h5>
&lt;h3 id="例子">
 例子
 &lt;a class="anchor" href="#%e4%be%8b%e5%ad%90">#&lt;/a>
&lt;/h3>
&lt;h5 id="当我们使用-kubectl-get-deployment查看集群-default-命令空间的-deployment时其实-kubectl-最终是将命令语言转化为-api-发送给-kube-apiserver然后将-kube-apiserver-返回的数据再转成特定格式打印出来">
 当我们使用 kubectl get deployment查看集群 default 命令空间的 deployment时，其实 kubectl 最终是将命令语言转化为 API 发送给 kube-apiserver，然后将 kube-apiserver 返回的数据再转成特定格式打印出来。
 &lt;a class="anchor" href="#%e5%bd%93%e6%88%91%e4%bb%ac%e4%bd%bf%e7%94%a8-kubectl-get-deployment%e6%9f%a5%e7%9c%8b%e9%9b%86%e7%be%a4-default-%e5%91%bd%e4%bb%a4%e7%a9%ba%e9%97%b4%e7%9a%84-deployment%e6%97%b6%e5%85%b6%e5%ae%9e-kubectl-%e6%9c%80%e7%bb%88%e6%98%af%e5%b0%86%e5%91%bd%e4%bb%a4%e8%af%ad%e8%a8%80%e8%bd%ac%e5%8c%96%e4%b8%ba-api-%e5%8f%91%e9%80%81%e7%bb%99-kube-apiserver%e7%84%b6%e5%90%8e%e5%b0%86-kube-apiserver-%e8%bf%94%e5%9b%9e%e7%9a%84%e6%95%b0%e6%8d%ae%e5%86%8d%e8%bd%ac%e6%88%90%e7%89%b9%e5%ae%9a%e6%a0%bc%e5%bc%8f%e6%89%93%e5%8d%b0%e5%87%ba%e6%9d%a5">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>$ kubectl get deployment
--&amp;gt; https://apiserver.cluster.local:6443/apis/apps/v1/namespaces/default/deployments
# apiserver.cluster.local:6443 这个是 kube-apiserver 的访问 url
# apis: 表示下面有多组 api
# apps: 表示 api 组
# v1: 表示 api version
# default: 表示命名空间
# deployments: 表示操作的具体资源类型
&lt;/code>&lt;/pre>&lt;h5 id="下面详细看看一个-url-的设计">
 下面详细看看一个 url 的设计
 &lt;a class="anchor" href="#%e4%b8%8b%e9%9d%a2%e8%af%a6%e7%bb%86%e7%9c%8b%e7%9c%8b%e4%b8%80%e4%b8%aa-url-%e7%9a%84%e8%ae%be%e8%ae%a1">#&lt;/a>
&lt;/h5>
&lt;h3 id="api-组">
 API 组
 &lt;a class="anchor" href="#api-%e7%bb%84">#&lt;/a>
&lt;/h3>
&lt;h5 id="k8s-将每一个-api-都设置组和版本即-groupversion">
 k8s 将每一个 api 都设置组和版本即 groupVersion。
 &lt;a class="anchor" href="#k8s-%e5%b0%86%e6%af%8f%e4%b8%80%e4%b8%aa-api-%e9%83%bd%e8%ae%be%e7%bd%ae%e7%bb%84%e5%92%8c%e7%89%88%e6%9c%ac%e5%8d%b3-groupversion">#&lt;/a>
&lt;/h5>
&lt;h5 id="例如apisappsv1deploymentapis-表示有多组-apiapps-是-groupv1-是-version">
 例如：/apis/apps/v1/deployment，apis 表示有多组 api，apps 是 group，v1 是 version
 &lt;a class="anchor" href="#%e4%be%8b%e5%a6%82apisappsv1deploymentapis-%e8%a1%a8%e7%a4%ba%e6%9c%89%e5%a4%9a%e7%bb%84-apiapps-%e6%98%af-groupv1-%e6%98%af-version">#&lt;/a>
&lt;/h5>
&lt;h5 id="但是-apiv1-这一组-api-例外因为在-k8s-刚开发时并没有预料到后面会发展这么多的-api当时并没有设置-group现在都认为-apiv1-都是核心组可以这么理解-apicorev1这样就和目前所有-api-结构对应上了">
 但是 /api/v1 这一组 API 例外，因为在 k8s 刚开发时，并没有预料到后面会发展这么多的 API，当时并没有设置 group，现在都认为 /api/v1 都是核心组，可以这么理解 /api/core/v1，这样就和目前所有 API 结构对应上了。
 &lt;a class="anchor" href="#%e4%bd%86%e6%98%af-apiv1-%e8%bf%99%e4%b8%80%e7%bb%84-api-%e4%be%8b%e5%a4%96%e5%9b%a0%e4%b8%ba%e5%9c%a8-k8s-%e5%88%9a%e5%bc%80%e5%8f%91%e6%97%b6%e5%b9%b6%e6%b2%a1%e6%9c%89%e9%a2%84%e6%96%99%e5%88%b0%e5%90%8e%e9%9d%a2%e4%bc%9a%e5%8f%91%e5%b1%95%e8%bf%99%e4%b9%88%e5%a4%9a%e7%9a%84-api%e5%bd%93%e6%97%b6%e5%b9%b6%e6%b2%a1%e6%9c%89%e8%ae%be%e7%bd%ae-group%e7%8e%b0%e5%9c%a8%e9%83%bd%e8%ae%a4%e4%b8%ba-apiv1-%e9%83%bd%e6%98%af%e6%a0%b8%e5%bf%83%e7%bb%84%e5%8f%af%e4%bb%a5%e8%bf%99%e4%b9%88%e7%90%86%e8%a7%a3-apicorev1%e8%bf%99%e6%a0%b7%e5%b0%b1%e5%92%8c%e7%9b%ae%e5%89%8d%e6%89%80%e6%9c%89-api-%e7%bb%93%e6%9e%84%e5%af%b9%e5%ba%94%e4%b8%8a%e4%ba%86">#&lt;/a>
&lt;/h5>
&lt;h5 id="例如apiv1serviceapi-表示只有一组-api-及核心组v1-是-version">
 例如：/api/v1/service，api 表示只有一组 api 及核心组，v1 是 version
 &lt;a class="anchor" href="#%e4%be%8b%e5%a6%82apiv1serviceapi-%e8%a1%a8%e7%a4%ba%e5%8f%aa%e6%9c%89%e4%b8%80%e7%bb%84-api-%e5%8f%8a%e6%a0%b8%e5%bf%83%e7%bb%84v1-%e6%98%af-version">#&lt;/a>
&lt;/h5>
&lt;h3 id="api-版本">
 API 版本
 &lt;a class="anchor" href="#api-%e7%89%88%e6%9c%ac">#&lt;/a>
&lt;/h3>
&lt;h5 id="每一个-api-除了有-group还需要拥有-version-属性因为每一个-api-都需要经历多次打磨才能稳定k8s-是这样定义-api-version-的">
 每一个 API 除了有 group，还需要拥有 version 属性，因为每一个 API 都需要经历多次打磨才能稳定，k8s 是这样定义 API version 的。
 &lt;a class="anchor" href="#%e6%af%8f%e4%b8%80%e4%b8%aa-api-%e9%99%a4%e4%ba%86%e6%9c%89-group%e8%bf%98%e9%9c%80%e8%a6%81%e6%8b%a5%e6%9c%89-version-%e5%b1%9e%e6%80%a7%e5%9b%a0%e4%b8%ba%e6%af%8f%e4%b8%80%e4%b8%aa-api-%e9%83%bd%e9%9c%80%e8%a6%81%e7%bb%8f%e5%8e%86%e5%a4%9a%e6%ac%a1%e6%89%93%e7%a3%a8%e6%89%8d%e8%83%bd%e7%a8%b3%e5%ae%9ak8s-%e6%98%af%e8%bf%99%e6%a0%b7%e5%ae%9a%e4%b9%89-api-version-%e7%9a%84">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="-alpha-级别例如-v1alpha1-默认情况下是被禁用的可以随时删除对功能的支持所以要慎用">
 • Alpha 级别，例如 &lt;code>v1alpha1&lt;/code> 默认情况下是被禁用的，可以随时删除对功能的支持，所以要慎用
 &lt;a class="anchor" href="#-alpha-%e7%ba%a7%e5%88%ab%e4%be%8b%e5%a6%82-v1alpha1-%e9%bb%98%e8%ae%a4%e6%83%85%e5%86%b5%e4%b8%8b%e6%98%af%e8%a2%ab%e7%a6%81%e7%94%a8%e7%9a%84%e5%8f%af%e4%bb%a5%e9%9a%8f%e6%97%b6%e5%88%a0%e9%99%a4%e5%af%b9%e5%8a%9f%e8%83%bd%e7%9a%84%e6%94%af%e6%8c%81%e6%89%80%e4%bb%a5%e8%a6%81%e6%85%8e%e7%94%a8">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-beta-级别例如-v2beta1-默认情况下是启用的表示代码已经经过了很好的测试但是对象的语义可能会在随后的版本中以不兼容的方式更改">
 • Beta 级别，例如 &lt;code>v2beta1&lt;/code> 默认情况下是启用的，表示代码已经经过了很好的测试，但是对象的语义可能会在随后的版本中以不兼容的方式更改
 &lt;a class="anchor" href="#-beta-%e7%ba%a7%e5%88%ab%e4%be%8b%e5%a6%82-v2beta1-%e9%bb%98%e8%ae%a4%e6%83%85%e5%86%b5%e4%b8%8b%e6%98%af%e5%90%af%e7%94%a8%e7%9a%84%e8%a1%a8%e7%a4%ba%e4%bb%a3%e7%a0%81%e5%b7%b2%e7%bb%8f%e7%bb%8f%e8%bf%87%e4%ba%86%e5%be%88%e5%a5%bd%e7%9a%84%e6%b5%8b%e8%af%95%e4%bd%86%e6%98%af%e5%af%b9%e8%b1%a1%e7%9a%84%e8%af%ad%e4%b9%89%e5%8f%af%e8%83%bd%e4%bc%9a%e5%9c%a8%e9%9a%8f%e5%90%8e%e7%9a%84%e7%89%88%e6%9c%ac%e4%b8%ad%e4%bb%a5%e4%b8%8d%e5%85%bc%e5%ae%b9%e7%9a%84%e6%96%b9%e5%bc%8f%e6%9b%b4%e6%94%b9">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-稳定级别比如-v1-表示已经是稳定版本了也会出现在后续的很多版本中">
 • 稳定级别，比如 &lt;code>v1&lt;/code> 表示已经是稳定版本了，也会出现在后续的很多版本中。
 &lt;a class="anchor" href="#-%e7%a8%b3%e5%ae%9a%e7%ba%a7%e5%88%ab%e6%af%94%e5%a6%82-v1-%e8%a1%a8%e7%a4%ba%e5%b7%b2%e7%bb%8f%e6%98%af%e7%a8%b3%e5%ae%9a%e7%89%88%e6%9c%ac%e4%ba%86%e4%b9%9f%e4%bc%9a%e5%87%ba%e7%8e%b0%e5%9c%a8%e5%90%8e%e7%bb%ad%e7%9a%84%e5%be%88%e5%a4%9a%e7%89%88%e6%9c%ac%e4%b8%ad">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403191517686.png" alt="image-20240319151739640" />&lt;/p></description></item><item><title>2024-04-03 Kubernetes 源码结构</title><link>https://qq547475331.github.io/docs/kubernetes-%E6%BA%90%E7%A0%81%E7%BB%93%E6%9E%84-kubernetes-yuan-ma-jie-gou/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/kubernetes-%E6%BA%90%E7%A0%81%E7%BB%93%E6%9E%84-kubernetes-yuan-ma-jie-gou/</guid><description>&lt;h2 id="简介">
 简介
 &lt;a class="anchor" href="#%e7%ae%80%e4%bb%8b">#&lt;/a>
&lt;/h2>
&lt;p>Kubernetes 源码非常庞大，其中包括自身核心代码，以及对外提供给开发者的 sdk 等。&lt;/p>
&lt;p>Kubernetes 的源码结构也是按照上述五个组成部分来组织的，这里以 Kubernetes:v1.22.17 讲解，其主要目录结构如下：&lt;/p>
&lt;pre tabindex="0">&lt;code>kubernetes/
 ├── api/
 ├── build/
 ├── CHANGELOG/
 ├── cmd/
 ├── docs/
 ├── hack/
 ├── pkg/
 ├── plugin/
 ├── staging/
 ├── test/
 └── vendor/
&lt;/code>&lt;/pre>&lt;h2 id="目录作用">
 目录作用
 &lt;a class="anchor" href="#%e7%9b%ae%e5%bd%95%e4%bd%9c%e7%94%a8">#&lt;/a>
&lt;/h2>
&lt;h3 id="api">
 &lt;strong>api&lt;/strong>
 &lt;a class="anchor" href="#api">#&lt;/a>
&lt;/h3>
&lt;h5 id="包含-kubernetes-的-api-定义文件如-podservicereplicationcontroller-等但是现在-k8s-的-api-基本都移到-k8sioapi和-k8sioapis-项目下">
 包含 Kubernetes 的 API 定义文件，如 Pod、Service、ReplicationController 等。但是现在 K8s 的 api 基本都移到 k8s.io/api，和 k8s.io/apis 项目下。
 &lt;a class="anchor" href="#%e5%8c%85%e5%90%ab-kubernetes-%e7%9a%84-api-%e5%ae%9a%e4%b9%89%e6%96%87%e4%bb%b6%e5%a6%82-podservicereplicationcontroller-%e7%ad%89%e4%bd%86%e6%98%af%e7%8e%b0%e5%9c%a8-k8s-%e7%9a%84-api-%e5%9f%ba%e6%9c%ac%e9%83%bd%e7%a7%bb%e5%88%b0-k8sioapi%e5%92%8c-k8sioapis-%e9%a1%b9%e7%9b%ae%e4%b8%8b">#&lt;/a>
&lt;/h5>
&lt;h3 id="build">
 &lt;strong>build&lt;/strong>
 &lt;a class="anchor" href="#build">#&lt;/a>
&lt;/h3>
&lt;h5 id="包含-kubernetes-内部组件编译的脚本以及制作-dcoker-镜像的-dockerfile-等">
 包含 Kubernetes 内部组件编译的脚本以及制作 Dcoker 镜像的 Dockerfile 等。
 &lt;a class="anchor" href="#%e5%8c%85%e5%90%ab-kubernetes-%e5%86%85%e9%83%a8%e7%bb%84%e4%bb%b6%e7%bc%96%e8%af%91%e7%9a%84%e8%84%9a%e6%9c%ac%e4%bb%a5%e5%8f%8a%e5%88%b6%e4%bd%9c-dcoker-%e9%95%9c%e5%83%8f%e7%9a%84-dockerfile-%e7%ad%89">#&lt;/a>
&lt;/h5>
&lt;h3 id="changelog">
 &lt;strong>CHANGELOG&lt;/strong>
 &lt;a class="anchor" href="#changelog">#&lt;/a>
&lt;/h3>
&lt;h5 id="本次版本更新的-future-以及修复的-bug-记录">
 本次版本更新的 Future 以及修复的 Bug 记录
 &lt;a class="anchor" href="#%e6%9c%ac%e6%ac%a1%e7%89%88%e6%9c%ac%e6%9b%b4%e6%96%b0%e7%9a%84-future-%e4%bb%a5%e5%8f%8a%e4%bf%ae%e5%a4%8d%e7%9a%84-bug-%e8%ae%b0%e5%bd%95">#&lt;/a>
&lt;/h5>
&lt;h3 id="cmd">
 &lt;strong>cmd&lt;/strong>
 &lt;a class="anchor" href="#cmd">#&lt;/a>
&lt;/h3>
&lt;h5 id="包含-kubernetes-组件启动命令如-kube-apiserverkube-controller-manager-等">
 包含 Kubernetes 组件启动命令，如 kube-apiserver，kube-controller-manager 等
 &lt;a class="anchor" href="#%e5%8c%85%e5%90%ab-kubernetes-%e7%bb%84%e4%bb%b6%e5%90%af%e5%8a%a8%e5%91%bd%e4%bb%a4%e5%a6%82-kube-apiserverkube-controller-manager-%e7%ad%89">#&lt;/a>
&lt;/h5>
&lt;h3 id="docs">
 &lt;strong>docs&lt;/strong>
 &lt;a class="anchor" href="#docs">#&lt;/a>
&lt;/h3>
&lt;h5 id="包含-kubernetes-的文档如开发者指南api-文档等这些文档是用-mkdocs-工具编写的可以生成静态网站供用户参考kubernetes-的文档非常丰富包括了从安装到使用到开发的所有内容对于初学者来说阅读-kubernetes-的官方文档是非常必要的">
 包含 Kubernetes 的文档，如开发者指南、API 文档等。这些文档是用 MkDocs 工具编写的，可以生成静态网站供用户参考。Kubernetes 的文档非常丰富，包括了从安装到使用到开发的所有内容。对于初学者来说，阅读 Kubernetes 的官方文档是非常必要的。
 &lt;a class="anchor" href="#%e5%8c%85%e5%90%ab-kubernetes-%e7%9a%84%e6%96%87%e6%a1%a3%e5%a6%82%e5%bc%80%e5%8f%91%e8%80%85%e6%8c%87%e5%8d%97api-%e6%96%87%e6%a1%a3%e7%ad%89%e8%bf%99%e4%ba%9b%e6%96%87%e6%a1%a3%e6%98%af%e7%94%a8-mkdocs-%e5%b7%a5%e5%85%b7%e7%bc%96%e5%86%99%e7%9a%84%e5%8f%af%e4%bb%a5%e7%94%9f%e6%88%90%e9%9d%99%e6%80%81%e7%bd%91%e7%ab%99%e4%be%9b%e7%94%a8%e6%88%b7%e5%8f%82%e8%80%83kubernetes-%e7%9a%84%e6%96%87%e6%a1%a3%e9%9d%9e%e5%b8%b8%e4%b8%b0%e5%af%8c%e5%8c%85%e6%8b%ac%e4%ba%86%e4%bb%8e%e5%ae%89%e8%a3%85%e5%88%b0%e4%bd%bf%e7%94%a8%e5%88%b0%e5%bc%80%e5%8f%91%e7%9a%84%e6%89%80%e6%9c%89%e5%86%85%e5%ae%b9%e5%af%b9%e4%ba%8e%e5%88%9d%e5%ad%a6%e8%80%85%e6%9d%a5%e8%af%b4%e9%98%85%e8%af%bb-kubernetes-%e7%9a%84%e5%ae%98%e6%96%b9%e6%96%87%e6%a1%a3%e6%98%af%e9%9d%9e%e5%b8%b8%e5%bf%85%e8%a6%81%e7%9a%84">#&lt;/a>
&lt;/h5>
&lt;h3 id="hack">
 &lt;strong>hack&lt;/strong>
 &lt;a class="anchor" href="#hack">#&lt;/a>
&lt;/h3>
&lt;h5 id="包含-kubernetes-的构建和测试脚本这些脚本用于自动化构建测试和发布-kubernetes在这些脚本中包含了大量的构建细节和测试用例这些脚本可以大大提高我们的工作效率同时也可以确保-kubernetes-的代码质量和稳定性">
 包含 Kubernetes 的构建和测试脚本。这些脚本用于自动化构建、测试和发布 Kubernetes。在这些脚本中，包含了大量的构建细节和测试用例。这些脚本可以大大提高我们的工作效率，同时也可以确保 Kubernetes 的代码质量和稳定性。
 &lt;a class="anchor" href="#%e5%8c%85%e5%90%ab-kubernetes-%e7%9a%84%e6%9e%84%e5%bb%ba%e5%92%8c%e6%b5%8b%e8%af%95%e8%84%9a%e6%9c%ac%e8%bf%99%e4%ba%9b%e8%84%9a%e6%9c%ac%e7%94%a8%e4%ba%8e%e8%87%aa%e5%8a%a8%e5%8c%96%e6%9e%84%e5%bb%ba%e6%b5%8b%e8%af%95%e5%92%8c%e5%8f%91%e5%b8%83-kubernetes%e5%9c%a8%e8%bf%99%e4%ba%9b%e8%84%9a%e6%9c%ac%e4%b8%ad%e5%8c%85%e5%90%ab%e4%ba%86%e5%a4%a7%e9%87%8f%e7%9a%84%e6%9e%84%e5%bb%ba%e7%bb%86%e8%8a%82%e5%92%8c%e6%b5%8b%e8%af%95%e7%94%a8%e4%be%8b%e8%bf%99%e4%ba%9b%e8%84%9a%e6%9c%ac%e5%8f%af%e4%bb%a5%e5%a4%a7%e5%a4%a7%e6%8f%90%e9%ab%98%e6%88%91%e4%bb%ac%e7%9a%84%e5%b7%a5%e4%bd%9c%e6%95%88%e7%8e%87%e5%90%8c%e6%97%b6%e4%b9%9f%e5%8f%af%e4%bb%a5%e7%a1%ae%e4%bf%9d-kubernetes-%e7%9a%84%e4%bb%a3%e7%a0%81%e8%b4%a8%e9%87%8f%e5%92%8c%e7%a8%b3%e5%ae%9a%e6%80%a7">#&lt;/a>
&lt;/h5>
&lt;h3 id="pkg">
 &lt;strong>pkg&lt;/strong>
 &lt;a class="anchor" href="#pkg">#&lt;/a>
&lt;/h3>
&lt;h5 id="包含-kubernetes-的核心代码如-api-servercontroller-managerscheduler-等">
 包含 Kubernetes 的核心代码，如 API Server、Controller Manager、Scheduler 等。
 &lt;a class="anchor" href="#%e5%8c%85%e5%90%ab-kubernetes-%e7%9a%84%e6%a0%b8%e5%bf%83%e4%bb%a3%e7%a0%81%e5%a6%82-api-servercontroller-managerscheduler-%e7%ad%89">#&lt;/a>
&lt;/h5>
&lt;h3 id="plugin">
 &lt;strong>plugin&lt;/strong>
 &lt;a class="anchor" href="#plugin">#&lt;/a>
&lt;/h3>
&lt;h5 id="包含-kubernetes-的插件例如存储插件认证插件等它们都可以让-kubernetes-更加灵活和强大">
 包含 Kubernetes 的插件，例如存储插件、认证插件等，它们都可以让 Kubernetes 更加灵活和强大。
 &lt;a class="anchor" href="#%e5%8c%85%e5%90%ab-kubernetes-%e7%9a%84%e6%8f%92%e4%bb%b6%e4%be%8b%e5%a6%82%e5%ad%98%e5%82%a8%e6%8f%92%e4%bb%b6%e8%ae%a4%e8%af%81%e6%8f%92%e4%bb%b6%e7%ad%89%e5%ae%83%e4%bb%ac%e9%83%bd%e5%8f%af%e4%bb%a5%e8%ae%a9-kubernetes-%e6%9b%b4%e5%8a%a0%e7%81%b5%e6%b4%bb%e5%92%8c%e5%bc%ba%e5%a4%a7">#&lt;/a>
&lt;/h5>
&lt;h3 id="test">
 &lt;strong>test&lt;/strong>
 &lt;a class="anchor" href="#test">#&lt;/a>
&lt;/h3>
&lt;h5 id="包含-kubernetes-的测试用例这些测试用例用于测试-kubernetes-的功能是否正常在-kubernetes-的开发过程中测试是非常重要的环节通过测试我们可以发现和解决各种问题确保-kubernetes-的功能正确性和稳定性">
 包含 Kubernetes 的测试用例。这些测试用例用于测试 Kubernetes 的功能是否正常。在 Kubernetes 的开发过程中，测试是非常重要的环节。通过测试，我们可以发现和解决各种问题，确保 Kubernetes 的功能正确性和稳定性。
 &lt;a class="anchor" href="#%e5%8c%85%e5%90%ab-kubernetes-%e7%9a%84%e6%b5%8b%e8%af%95%e7%94%a8%e4%be%8b%e8%bf%99%e4%ba%9b%e6%b5%8b%e8%af%95%e7%94%a8%e4%be%8b%e7%94%a8%e4%ba%8e%e6%b5%8b%e8%af%95-kubernetes-%e7%9a%84%e5%8a%9f%e8%83%bd%e6%98%af%e5%90%a6%e6%ad%a3%e5%b8%b8%e5%9c%a8-kubernetes-%e7%9a%84%e5%bc%80%e5%8f%91%e8%bf%87%e7%a8%8b%e4%b8%ad%e6%b5%8b%e8%af%95%e6%98%af%e9%9d%9e%e5%b8%b8%e9%87%8d%e8%a6%81%e7%9a%84%e7%8e%af%e8%8a%82%e9%80%9a%e8%bf%87%e6%b5%8b%e8%af%95%e6%88%91%e4%bb%ac%e5%8f%af%e4%bb%a5%e5%8f%91%e7%8e%b0%e5%92%8c%e8%a7%a3%e5%86%b3%e5%90%84%e7%a7%8d%e9%97%ae%e9%a2%98%e7%a1%ae%e4%bf%9d-kubernetes-%e7%9a%84%e5%8a%9f%e8%83%bd%e6%ad%a3%e7%a1%ae%e6%80%a7%e5%92%8c%e7%a8%b3%e5%ae%9a%e6%80%a7">#&lt;/a>
&lt;/h5>
&lt;h5 id="vendor">
 &lt;strong>vendor&lt;/strong>
 &lt;a class="anchor" href="#vendor">#&lt;/a>
&lt;/h5>
&lt;h5 id="用于存放-kubernetes-所有依赖的第三方库的代码在编译-kubernetes-源码时需要使用大量的第三方库例如-etcddockerglog-等这些库的源码会被存放在-vendor-目录下它们会被自动下载和编译最终被打包到-kubernetes-的二进制文件中">
 用于存放 Kubernetes 所有依赖的第三方库的代码。在编译 Kubernetes 源码时，需要使用大量的第三方库，例如 &lt;code>etcd&lt;/code>、&lt;code>docker&lt;/code>、&lt;code>glog&lt;/code> 等。这些库的源码会被存放在 &lt;code>vendor&lt;/code> 目录下，它们会被自动下载和编译，最终被打包到 Kubernetes 的二进制文件中。
 &lt;a class="anchor" href="#%e7%94%a8%e4%ba%8e%e5%ad%98%e6%94%be-kubernetes-%e6%89%80%e6%9c%89%e4%be%9d%e8%b5%96%e7%9a%84%e7%ac%ac%e4%b8%89%e6%96%b9%e5%ba%93%e7%9a%84%e4%bb%a3%e7%a0%81%e5%9c%a8%e7%bc%96%e8%af%91-kubernetes-%e6%ba%90%e7%a0%81%e6%97%b6%e9%9c%80%e8%a6%81%e4%bd%bf%e7%94%a8%e5%a4%a7%e9%87%8f%e7%9a%84%e7%ac%ac%e4%b8%89%e6%96%b9%e5%ba%93%e4%be%8b%e5%a6%82-etcddockerglog-%e7%ad%89%e8%bf%99%e4%ba%9b%e5%ba%93%e7%9a%84%e6%ba%90%e7%a0%81%e4%bc%9a%e8%a2%ab%e5%ad%98%e6%94%be%e5%9c%a8-vendor-%e7%9b%ae%e5%bd%95%e4%b8%8b%e5%ae%83%e4%bb%ac%e4%bc%9a%e8%a2%ab%e8%87%aa%e5%8a%a8%e4%b8%8b%e8%bd%bd%e5%92%8c%e7%bc%96%e8%af%91%e6%9c%80%e7%bb%88%e8%a2%ab%e6%89%93%e5%8c%85%e5%88%b0-kubernetes-%e7%9a%84%e4%ba%8c%e8%bf%9b%e5%88%b6%e6%96%87%e4%bb%b6%e4%b8%ad">#&lt;/a>
&lt;/h5>
&lt;h3 id="staging">
 &lt;strong>staging&lt;/strong>
 &lt;a class="anchor" href="#staging">#&lt;/a>
&lt;/h3>
&lt;h5 id="这个目录比较特殊单独拿出来说">
 这个目录比较特殊，单独拿出来说。
 &lt;a class="anchor" href="#%e8%bf%99%e4%b8%aa%e7%9b%ae%e5%bd%95%e6%af%94%e8%be%83%e7%89%b9%e6%ae%8a%e5%8d%95%e7%8b%ac%e6%8b%bf%e5%87%ba%e6%9d%a5%e8%af%b4">#&lt;/a>
&lt;/h5>
&lt;hr>
&lt;h2 id="staging-目录">
 Staging 目录
 &lt;a class="anchor" href="#staging-%e7%9b%ae%e5%bd%95">#&lt;/a>
&lt;/h2>
&lt;h5 id="在-kubernetes-源码中对-kubernetes-项目代码的引用使用的都是-k8sio1">
 在 kubernetes 源码中，对 kubernetes 项目代码的引用使用的都是 k8s.io[1]：
 &lt;a class="anchor" href="#%e5%9c%a8-kubernetes-%e6%ba%90%e7%a0%81%e4%b8%ad%e5%af%b9-kubernetes-%e9%a1%b9%e7%9b%ae%e4%bb%a3%e7%a0%81%e7%9a%84%e5%bc%95%e7%94%a8%e4%bd%bf%e7%94%a8%e7%9a%84%e9%83%bd%e6%98%af-k8sio1">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>package proxy

import (
 &amp;#34;bytes&amp;#34;
 &amp;#34;fmt&amp;#34;

 &amp;#34;github.com/pkg/errors&amp;#34;
 apps &amp;#34;k8s.io/api/apps/v1&amp;#34;
 &amp;#34;k8s.io/api/core/v1&amp;#34;
 rbac &amp;#34;k8s.io/api/rbac/v1&amp;#34;
 metav1 &amp;#34;k8s.io/apimachinery/pkg/apis/meta/v1&amp;#34;
 kuberuntime &amp;#34;k8s.io/apimachinery/pkg/runtime&amp;#34;
 clientset &amp;#34;k8s.io/client-go/kubernetes&amp;#34;
 clientsetscheme &amp;#34;k8s.io/client-go/kubernetes/scheme&amp;#34;
 kubeadmapi &amp;#34;k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm&amp;#34;
 &amp;#34;k8s.io/kubernetes/cmd/kubeadm/app/componentconfigs&amp;#34;
 &amp;#34;k8s.io/kubernetes/cmd/kubeadm/app/constants&amp;#34;
 &amp;#34;k8s.io/kubernetes/cmd/kubeadm/app/images&amp;#34;
 kubeadmutil &amp;#34;k8s.io/kubernetes/cmd/kubeadm/app/util&amp;#34;
 &amp;#34;k8s.io/kubernetes/cmd/kubeadm/app/util/apiclient&amp;#34;
)
&lt;/code>&lt;/pre>&lt;h3 id="主项目代码">
 &lt;strong>主项目代码&lt;/strong>
 &lt;a class="anchor" href="#%e4%b8%bb%e9%a1%b9%e7%9b%ae%e4%bb%a3%e7%a0%81">#&lt;/a>
&lt;/h3>
&lt;h5 id="第一种情况是对主项目代码的引用k8siokubernetes-就是主项目代码的-package-name在-go-modules2-使用的-gomod-文件中定义的">
 第一种情况是对主项目代码的引用。k8s.io/kubernetes 就是主项目代码的 package name，在 go-modules[2] 使用的 go.mod 文件中定义的：
 &lt;a class="anchor" href="#%e7%ac%ac%e4%b8%80%e7%a7%8d%e6%83%85%e5%86%b5%e6%98%af%e5%af%b9%e4%b8%bb%e9%a1%b9%e7%9b%ae%e4%bb%a3%e7%a0%81%e7%9a%84%e5%bc%95%e7%94%a8k8siokubernetes-%e5%b0%b1%e6%98%af%e4%b8%bb%e9%a1%b9%e7%9b%ae%e4%bb%a3%e7%a0%81%e7%9a%84-package-name%e5%9c%a8-go-modules2-%e4%bd%bf%e7%94%a8%e7%9a%84-gomod-%e6%96%87%e4%bb%b6%e4%b8%ad%e5%ae%9a%e4%b9%89%e7%9a%84">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>module k8s.io/kubernetes

go 1.16
&lt;/code>&lt;/pre>&lt;h5 id="因此引用主项目代码需要使用-k8siokubernetes而不是其他项目中经常看到-githubcomxxxx">
 因此引用主项目代码，需要使用 k8s.io/kubernetes，而不是其他项目中经常看到 &lt;code>github.com/xxxx&lt;/code>。
 &lt;a class="anchor" href="#%e5%9b%a0%e6%ad%a4%e5%bc%95%e7%94%a8%e4%b8%bb%e9%a1%b9%e7%9b%ae%e4%bb%a3%e7%a0%81%e9%9c%80%e8%a6%81%e4%bd%bf%e7%94%a8-k8siokubernetes%e8%80%8c%e4%b8%8d%e6%98%af%e5%85%b6%e4%bb%96%e9%a1%b9%e7%9b%ae%e4%b8%ad%e7%bb%8f%e5%b8%b8%e7%9c%8b%e5%88%b0-githubcomxxxx">#&lt;/a>
&lt;/h5>
&lt;h3 id="单独发布的代码">
 &lt;strong>单独发布的代码&lt;/strong>
 &lt;a class="anchor" href="#%e5%8d%95%e7%8b%ac%e5%8f%91%e5%b8%83%e7%9a%84%e4%bb%a3%e7%a0%81">#&lt;/a>
&lt;/h3>
&lt;h5 id="第二种情况是对位于主项目中但是独立发布的代码的引用">
 第二种情况是对位于主项目中但是独立发布的代码的引用。
 &lt;a class="anchor" href="#%e7%ac%ac%e4%ba%8c%e7%a7%8d%e6%83%85%e5%86%b5%e6%98%af%e5%af%b9%e4%bd%8d%e4%ba%8e%e4%b8%bb%e9%a1%b9%e7%9b%ae%e4%b8%ad%e4%bd%86%e6%98%af%e7%8b%ac%e7%ab%8b%e5%8f%91%e5%b8%83%e7%9a%84%e4%bb%a3%e7%a0%81%e7%9a%84%e5%bc%95%e7%94%a8">#&lt;/a>
&lt;/h5>
&lt;h5 id="kubernetes-的一些代码以独立项目的方式发布的譬如kubernetesapi3kubernetesclient-go4-等主要是考虑这些包开发者会经常用到所以将这些包单独拿到-k8s-源码项目之外方便开发者引用这些项目的-package-name-也用同样的方式在-gomod-中定义">
 kubernetes 的一些代码以独立项目的方式发布的，譬如：kubernetes/api[3]、kubernetes/client-go[4] 等，主要是考虑这些包开发者会经常用到，所以将这些包单独拿到 k8s 源码项目之外，方便开发者引用。这些项目的 package name 也用同样的方式在 go.mod 中定义：
 &lt;a class="anchor" href="#kubernetes-%e7%9a%84%e4%b8%80%e4%ba%9b%e4%bb%a3%e7%a0%81%e4%bb%a5%e7%8b%ac%e7%ab%8b%e9%a1%b9%e7%9b%ae%e7%9a%84%e6%96%b9%e5%bc%8f%e5%8f%91%e5%b8%83%e7%9a%84%e8%ad%ac%e5%a6%82kubernetesapi3kubernetesclient-go4-%e7%ad%89%e4%b8%bb%e8%a6%81%e6%98%af%e8%80%83%e8%99%91%e8%bf%99%e4%ba%9b%e5%8c%85%e5%bc%80%e5%8f%91%e8%80%85%e4%bc%9a%e7%bb%8f%e5%b8%b8%e7%94%a8%e5%88%b0%e6%89%80%e4%bb%a5%e5%b0%86%e8%bf%99%e4%ba%9b%e5%8c%85%e5%8d%95%e7%8b%ac%e6%8b%bf%e5%88%b0-k8s-%e6%ba%90%e7%a0%81%e9%a1%b9%e7%9b%ae%e4%b9%8b%e5%a4%96%e6%96%b9%e4%be%bf%e5%bc%80%e5%8f%91%e8%80%85%e5%bc%95%e7%94%a8%e8%bf%99%e4%ba%9b%e9%a1%b9%e7%9b%ae%e7%9a%84-package-name-%e4%b9%9f%e7%94%a8%e5%90%8c%e6%a0%b7%e7%9a%84%e6%96%b9%e5%bc%8f%e5%9c%a8-gomod-%e4%b8%ad%e5%ae%9a%e4%b9%89">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>module k8s.io/api

或者

module k8s.io/client-go
&lt;/code>&lt;/pre>&lt;h5 id="要注意的是这些代码虽然以独立项目发布但是都在-kubernetes-主项目中维护位于目录-kubernetesstaging-这里面的代码代码被定期同步到各个独立项目中">
 要注意的是，这些代码虽然以独立项目发布，但是都在 kubernetes 主项目中维护，位于目录 kubernetes/staging/ ，这里面的代码代码被定期同步到各个独立项目中。
 &lt;a class="anchor" href="#%e8%a6%81%e6%b3%a8%e6%84%8f%e7%9a%84%e6%98%af%e8%bf%99%e4%ba%9b%e4%bb%a3%e7%a0%81%e8%99%bd%e7%84%b6%e4%bb%a5%e7%8b%ac%e7%ab%8b%e9%a1%b9%e7%9b%ae%e5%8f%91%e5%b8%83%e4%bd%86%e6%98%af%e9%83%bd%e5%9c%a8-kubernetes-%e4%b8%bb%e9%a1%b9%e7%9b%ae%e4%b8%ad%e7%bb%b4%e6%8a%a4%e4%bd%8d%e4%ba%8e%e7%9b%ae%e5%bd%95-kubernetesstaging-%e8%bf%99%e9%87%8c%e9%9d%a2%e7%9a%84%e4%bb%a3%e7%a0%81%e4%bb%a3%e7%a0%81%e8%a2%ab%e5%ae%9a%e6%9c%9f%e5%90%8c%e6%ad%a5%e5%88%b0%e5%90%84%e4%b8%aa%e7%8b%ac%e7%ab%8b%e9%a1%b9%e7%9b%ae%e4%b8%ad">#&lt;/a>
&lt;/h5>
&lt;h5 id="kubernetesstaging5-列出了独立发布的代码">
 kubernetes/staging/[5] 列出了独立发布的代码：
 &lt;a class="anchor" href="#kubernetesstaging5-%e5%88%97%e5%87%ba%e4%ba%86%e7%8b%ac%e7%ab%8b%e5%8f%91%e5%b8%83%e7%9a%84%e4%bb%a3%e7%a0%81">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>k8s.io/api
k8s.io/apiextensions-apiserver
k8s.io/apimachinery
k8s.io/apiserver
k8s.io/cli-runtime
k8s.io/client-go
k8s.io/cloud-provider
k8s.io/cluster-bootstrap
k8s.io/code-generator
k8s.io/component-base
k8s.io/cri-api
k8s.io/csi-api
k8s.io/csi-translation-lib
k8s.io/kube-aggregator
k8s.io/kube-controller-manager
k8s.io/kube-proxy
k8s.io/kube-scheduler
k8s.io/kubectl
k8s.io/kubelet
k8s.io/legacy-cloud-providers
k8s.io/metrics
k8s.io/node-api
k8s.io/sample-apiserver
k8s.io/sample-cli-plugin
k8s.io/sample-controller
&lt;/code>&lt;/pre>&lt;h5 id="更需要注意的是kubernetes-主项目引用这些独立发布的代码时引用是位于主项目-staging-目录中的代码而不是独立-repo-中的代码这是因为主项目的-vendor-目录中设置了软链接">
 更需要注意的是，kubernetes 主项目引用这些独立发布的代码时，引用是位于主项目 staging 目录中的代码，而不是独立 repo 中的代码。这是因为主项目的 vendor 目录中设置了软链接。
 &lt;a class="anchor" href="#%e6%9b%b4%e9%9c%80%e8%a6%81%e6%b3%a8%e6%84%8f%e7%9a%84%e6%98%afkubernetes-%e4%b8%bb%e9%a1%b9%e7%9b%ae%e5%bc%95%e7%94%a8%e8%bf%99%e4%ba%9b%e7%8b%ac%e7%ab%8b%e5%8f%91%e5%b8%83%e7%9a%84%e4%bb%a3%e7%a0%81%e6%97%b6%e5%bc%95%e7%94%a8%e6%98%af%e4%bd%8d%e4%ba%8e%e4%b8%bb%e9%a1%b9%e7%9b%ae-staging-%e7%9b%ae%e5%bd%95%e4%b8%ad%e7%9a%84%e4%bb%a3%e7%a0%81%e8%80%8c%e4%b8%8d%e6%98%af%e7%8b%ac%e7%ab%8b-repo-%e4%b8%ad%e7%9a%84%e4%bb%a3%e7%a0%81%e8%bf%99%e6%98%af%e5%9b%a0%e4%b8%ba%e4%b8%bb%e9%a1%b9%e7%9b%ae%e7%9a%84-vendor-%e7%9b%ae%e5%bd%95%e4%b8%ad%e8%ae%be%e7%bd%ae%e4%ba%86%e8%bd%af%e9%93%be%e6%8e%a5">#&lt;/a>
&lt;/h5>
&lt;h5 id="只要单独项目发生了更新例如k8sioapimachinery-就会被自动同步到-kubernetes-源码的-stagingsrck8sioapimachinery-之下">
 只要单独项目发生了更新，例如：k8s.io/apimachinery ，就会被自动同步到 Kubernetes 源码的 staging/src/k8s.io/apimachinery 之下。
 &lt;a class="anchor" href="#%e5%8f%aa%e8%a6%81%e5%8d%95%e7%8b%ac%e9%a1%b9%e7%9b%ae%e5%8f%91%e7%94%9f%e4%ba%86%e6%9b%b4%e6%96%b0%e4%be%8b%e5%a6%82k8sioapimachinery-%e5%b0%b1%e4%bc%9a%e8%a2%ab%e8%87%aa%e5%8a%a8%e5%90%8c%e6%ad%a5%e5%88%b0-kubernetes-%e6%ba%90%e7%a0%81%e7%9a%84-stagingsrck8sioapimachinery-%e4%b9%8b%e4%b8%8b">#&lt;/a>
&lt;/h5>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403191515885.png" alt="image-20240319151528833" />&lt;/p></description></item><item><title>2024-04-03 Kubernetes 证书详解(认证)</title><link>https://qq547475331.github.io/docs/kubernetes-%E8%AF%81%E4%B9%A6%E8%AF%A6%E8%A7%A3%E8%AE%A4%E8%AF%81-kubernetes-zheng-shu-xiang-jie--ren-zheng-/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/kubernetes-%E8%AF%81%E4%B9%A6%E8%AF%A6%E8%A7%A3%E8%AE%A4%E8%AF%81-kubernetes-zheng-shu-xiang-jie--ren-zheng-/</guid><description>&lt;h2 id="k8s-证书介绍">
 K8S 证书介绍
 &lt;a class="anchor" href="#k8s-%e8%af%81%e4%b9%a6%e4%bb%8b%e7%bb%8d">#&lt;/a>
&lt;/h2>
&lt;h5 id="在-kube-apiserver-中提供了很多认证方式其中最常用的就是-tls-认证当然也有-bootstraptokenbasicauth-认证等只要有一个认证通过那么-kube-apiserver-即认为认证通过下面就主要讲解-tls-认证">
 在 Kube-apiserver 中提供了很多认证方式，其中最常用的就是 TLS 认证，当然也有 BootstrapToken，BasicAuth 认证等，只要有一个认证通过，那么 Kube-apiserver 即认为认证通过。下面就主要讲解 TLS 认证。
 &lt;a class="anchor" href="#%e5%9c%a8-kube-apiserver-%e4%b8%ad%e6%8f%90%e4%be%9b%e4%ba%86%e5%be%88%e5%a4%9a%e8%ae%a4%e8%af%81%e6%96%b9%e5%bc%8f%e5%85%b6%e4%b8%ad%e6%9c%80%e5%b8%b8%e7%94%a8%e7%9a%84%e5%b0%b1%e6%98%af-tls-%e8%ae%a4%e8%af%81%e5%bd%93%e7%84%b6%e4%b9%9f%e6%9c%89-bootstraptokenbasicauth-%e8%ae%a4%e8%af%81%e7%ad%89%e5%8f%aa%e8%a6%81%e6%9c%89%e4%b8%80%e4%b8%aa%e8%ae%a4%e8%af%81%e9%80%9a%e8%bf%87%e9%82%a3%e4%b9%88-kube-apiserver-%e5%8d%b3%e8%ae%a4%e4%b8%ba%e8%ae%a4%e8%af%81%e9%80%9a%e8%bf%87%e4%b8%8b%e9%9d%a2%e5%b0%b1%e4%b8%bb%e8%a6%81%e8%ae%b2%e8%a7%a3-tls-%e8%ae%a4%e8%af%81">#&lt;/a>
&lt;/h5>
&lt;h5 id="如果你是使用-kubeadm1-安装的-kubernetes-则会自动生成集群所需的证书但是如果是通过二进制搭建所有的证书是需要自己生成的这里我们说说集群必需的证书">
 如果你是使用 kubeadm[1] 安装的 Kubernetes， 则会自动生成集群所需的证书。但是如果是通过二进制搭建，所有的证书是需要自己生成的，这里我们说说集群必需的证书。
 &lt;a class="anchor" href="#%e5%a6%82%e6%9e%9c%e4%bd%a0%e6%98%af%e4%bd%bf%e7%94%a8-kubeadm1-%e5%ae%89%e8%a3%85%e7%9a%84-kubernetes-%e5%88%99%e4%bc%9a%e8%87%aa%e5%8a%a8%e7%94%9f%e6%88%90%e9%9b%86%e7%be%a4%e6%89%80%e9%9c%80%e7%9a%84%e8%af%81%e4%b9%a6%e4%bd%86%e6%98%af%e5%a6%82%e6%9e%9c%e6%98%af%e9%80%9a%e8%bf%87%e4%ba%8c%e8%bf%9b%e5%88%b6%e6%90%ad%e5%bb%ba%e6%89%80%e6%9c%89%e7%9a%84%e8%af%81%e4%b9%a6%e6%98%af%e9%9c%80%e8%a6%81%e8%87%aa%e5%b7%b1%e7%94%9f%e6%88%90%e7%9a%84%e8%bf%99%e9%87%8c%e6%88%91%e4%bb%ac%e8%af%b4%e8%af%b4%e9%9b%86%e7%be%a4%e5%bf%85%e9%9c%80%e7%9a%84%e8%af%81%e4%b9%a6">#&lt;/a>
&lt;/h5>
&lt;h5 id="在了解-kubernetes-证书之前需要先了解什么是-单向-tls-认证-和-双向-tls-认证">
 在了解 Kubernetes 证书之前，需要先了解什么是 “单向 TLS 认证” 和 “双向 TLS 认证”
 &lt;a class="anchor" href="#%e5%9c%a8%e4%ba%86%e8%a7%a3-kubernetes-%e8%af%81%e4%b9%a6%e4%b9%8b%e5%89%8d%e9%9c%80%e8%a6%81%e5%85%88%e4%ba%86%e8%a7%a3%e4%bb%80%e4%b9%88%e6%98%af-%e5%8d%95%e5%90%91-tls-%e8%ae%a4%e8%af%81-%e5%92%8c-%e5%8f%8c%e5%90%91-tls-%e8%ae%a4%e8%af%81">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="-服务器单向认证只需要服务器端提供证书客户端通过服务器端证书验证服务的身份但服务器并不验证客户端的身份这种情况一般适用于对-internet-开放的服务例如搜索引擎网站任何客户端都可以连接到服务器上进行访问但客户端需要验证服务器的身份以避免连接到伪造的恶意服务器">
 • 服务器单向认证：只需要服务器端提供证书，客户端通过服务器端证书验证服务的身份，但服务器并不验证客户端的身份。这种情况一般适用于对 Internet 开放的服务，例如搜索引擎网站，任何客户端都可以连接到服务器上进行访问，但客户端需要验证服务器的身份，以避免连接到伪造的恶意服务器。
 &lt;a class="anchor" href="#-%e6%9c%8d%e5%8a%a1%e5%99%a8%e5%8d%95%e5%90%91%e8%ae%a4%e8%af%81%e5%8f%aa%e9%9c%80%e8%a6%81%e6%9c%8d%e5%8a%a1%e5%99%a8%e7%ab%af%e6%8f%90%e4%be%9b%e8%af%81%e4%b9%a6%e5%ae%a2%e6%88%b7%e7%ab%af%e9%80%9a%e8%bf%87%e6%9c%8d%e5%8a%a1%e5%99%a8%e7%ab%af%e8%af%81%e4%b9%a6%e9%aa%8c%e8%af%81%e6%9c%8d%e5%8a%a1%e7%9a%84%e8%ba%ab%e4%bb%bd%e4%bd%86%e6%9c%8d%e5%8a%a1%e5%99%a8%e5%b9%b6%e4%b8%8d%e9%aa%8c%e8%af%81%e5%ae%a2%e6%88%b7%e7%ab%af%e7%9a%84%e8%ba%ab%e4%bb%bd%e8%bf%99%e7%a7%8d%e6%83%85%e5%86%b5%e4%b8%80%e8%88%ac%e9%80%82%e7%94%a8%e4%ba%8e%e5%af%b9-internet-%e5%bc%80%e6%94%be%e7%9a%84%e6%9c%8d%e5%8a%a1%e4%be%8b%e5%a6%82%e6%90%9c%e7%b4%a2%e5%bc%95%e6%93%8e%e7%bd%91%e7%ab%99%e4%bb%bb%e4%bd%95%e5%ae%a2%e6%88%b7%e7%ab%af%e9%83%bd%e5%8f%af%e4%bb%a5%e8%bf%9e%e6%8e%a5%e5%88%b0%e6%9c%8d%e5%8a%a1%e5%99%a8%e4%b8%8a%e8%bf%9b%e8%a1%8c%e8%ae%bf%e9%97%ae%e4%bd%86%e5%ae%a2%e6%88%b7%e7%ab%af%e9%9c%80%e8%a6%81%e9%aa%8c%e8%af%81%e6%9c%8d%e5%8a%a1%e5%99%a8%e7%9a%84%e8%ba%ab%e4%bb%bd%e4%bb%a5%e9%81%bf%e5%85%8d%e8%bf%9e%e6%8e%a5%e5%88%b0%e4%bc%aa%e9%80%a0%e7%9a%84%e6%81%b6%e6%84%8f%e6%9c%8d%e5%8a%a1%e5%99%a8">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-双向-tls-认证除了客户端需要验证服务器的证书服务器也要通过客户端证书验证客户端的身份这种情况下服务器提供的是敏感信息只允许特定身份的客户端访问开启服务端验证客户端默认是关闭的需要在服务端开启认证配置">
 • 双向 TLS 认证：除了客户端需要验证服务器的证书，服务器也要通过客户端证书验证客户端的身份。这种情况下服务器提供的是敏感信息，只允许特定身份的客户端访问。开启服务端验证客户端默认是关闭的，需要在服务端开启认证配置。
 &lt;a class="anchor" href="#-%e5%8f%8c%e5%90%91-tls-%e8%ae%a4%e8%af%81%e9%99%a4%e4%ba%86%e5%ae%a2%e6%88%b7%e7%ab%af%e9%9c%80%e8%a6%81%e9%aa%8c%e8%af%81%e6%9c%8d%e5%8a%a1%e5%99%a8%e7%9a%84%e8%af%81%e4%b9%a6%e6%9c%8d%e5%8a%a1%e5%99%a8%e4%b9%9f%e8%a6%81%e9%80%9a%e8%bf%87%e5%ae%a2%e6%88%b7%e7%ab%af%e8%af%81%e4%b9%a6%e9%aa%8c%e8%af%81%e5%ae%a2%e6%88%b7%e7%ab%af%e7%9a%84%e8%ba%ab%e4%bb%bd%e8%bf%99%e7%a7%8d%e6%83%85%e5%86%b5%e4%b8%8b%e6%9c%8d%e5%8a%a1%e5%99%a8%e6%8f%90%e4%be%9b%e7%9a%84%e6%98%af%e6%95%8f%e6%84%9f%e4%bf%a1%e6%81%af%e5%8f%aa%e5%85%81%e8%ae%b8%e7%89%b9%e5%ae%9a%e8%ba%ab%e4%bb%bd%e7%9a%84%e5%ae%a2%e6%88%b7%e7%ab%af%e8%ae%bf%e9%97%ae%e5%bc%80%e5%90%af%e6%9c%8d%e5%8a%a1%e7%ab%af%e9%aa%8c%e8%af%81%e5%ae%a2%e6%88%b7%e7%ab%af%e9%bb%98%e8%ae%a4%e6%98%af%e5%85%b3%e9%97%ad%e7%9a%84%e9%9c%80%e8%a6%81%e5%9c%a8%e6%9c%8d%e5%8a%a1%e7%ab%af%e5%bc%80%e5%90%af%e8%ae%a4%e8%af%81%e9%85%8d%e7%bd%ae">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h5 id="kubernetes-为了安全性都是采用双向认证通常我们使用-kubeadm-在部署-kubernetes-时候kubeadm-会自动生成集群所需要的证书下面我们就这些证书一一给大家进行讲解">
 Kubernetes 为了安全性，都是采用双向认证。通常我们使用 Kubeadm 在部署 Kubernetes 时候，Kubeadm 会自动生成集群所需要的证书，下面我们就这些证书一一给大家进行讲解。
 &lt;a class="anchor" href="#kubernetes-%e4%b8%ba%e4%ba%86%e5%ae%89%e5%85%a8%e6%80%a7%e9%83%bd%e6%98%af%e9%87%87%e7%94%a8%e5%8f%8c%e5%90%91%e8%ae%a4%e8%af%81%e9%80%9a%e5%b8%b8%e6%88%91%e4%bb%ac%e4%bd%bf%e7%94%a8-kubeadm-%e5%9c%a8%e9%83%a8%e7%bd%b2-kubernetes-%e6%97%b6%e5%80%99kubeadm-%e4%bc%9a%e8%87%aa%e5%8a%a8%e7%94%9f%e6%88%90%e9%9b%86%e7%be%a4%e6%89%80%e9%9c%80%e8%a6%81%e7%9a%84%e8%af%81%e4%b9%a6%e4%b8%8b%e9%9d%a2%e6%88%91%e4%bb%ac%e5%b0%b1%e8%bf%99%e4%ba%9b%e8%af%81%e4%b9%a6%e4%b8%80%e4%b8%80%e7%bb%99%e5%a4%a7%e5%ae%b6%e8%bf%9b%e8%a1%8c%e8%ae%b2%e8%a7%a3">#&lt;/a>
&lt;/h5>
&lt;h5 id="这是我们用-kubeadm-搭建完一个集群后在-etckubernetes-目录下所生成的文件">
 这是我们用 Kubeadm 搭建完一个集群后在 &lt;em>&lt;code>/etc/kubernetes&lt;/code>&lt;/em> 目录下所生成的文件
 &lt;a class="anchor" href="#%e8%bf%99%e6%98%af%e6%88%91%e4%bb%ac%e7%94%a8-kubeadm-%e6%90%ad%e5%bb%ba%e5%ae%8c%e4%b8%80%e4%b8%aa%e9%9b%86%e7%be%a4%e5%90%8e%e5%9c%a8-etckubernetes-%e7%9b%ae%e5%bd%95%e4%b8%8b%e6%89%80%e7%94%9f%e6%88%90%e7%9a%84%e6%96%87%e4%bb%b6">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>$ tree kubernetes/
kubernetes/
|-- admin.conf
|-- controller-manager.conf
|-- kubelet.conf
|-- scheduler.conf
|-- manifests
| |-- etcd.yaml
| |-- kube-apiserver.yaml
| |-- kube-controller-manager.yaml
| `-- kube-scheduler.yaml
|-- pki
| |-- apiserver.crt
| |-- apiserver-etcd-client.crt
| |-- apiserver-etcd-client.key
| |-- apiserver.key
| |-- apiserver-kubelet-client.crt
| |-- apiserver-kubelet-client.key
| |-- ca.crt
| |-- ca.key
| |-- etcd
| | |-- ca.crt
| | |-- ca.key
| | |-- healthcheck-client.crt
| | |-- healthcheck-client.key
| | |-- peer.crt
| | |-- peer.key
| | |-- server.crt
| | `-- server.key
| |-- front-proxy-ca.crt
| |-- front-proxy-ca.key
| |-- front-proxy-client.crt
| |-- front-proxy-client.key
| |-- sa.key
| `-- sa.pub
&lt;/code>&lt;/pre>&lt;h5 id="下面我们根据这个-kubernetes-的组件之间通讯图来一一讲解每个证书的作用本文基于-kubernetesv122172">
 下面我们根据这个 Kubernetes 的组件之间通讯图来一一讲解每个证书的作用。本文基于 Kubernetes:v1.22.17[2]
 &lt;a class="anchor" href="#%e4%b8%8b%e9%9d%a2%e6%88%91%e4%bb%ac%e6%a0%b9%e6%8d%ae%e8%bf%99%e4%b8%aa-kubernetes-%e7%9a%84%e7%bb%84%e4%bb%b6%e4%b9%8b%e9%97%b4%e9%80%9a%e8%ae%af%e5%9b%be%e6%9d%a5%e4%b8%80%e4%b8%80%e8%ae%b2%e8%a7%a3%e6%af%8f%e4%b8%aa%e8%af%81%e4%b9%a6%e7%9a%84%e4%bd%9c%e7%94%a8%e6%9c%ac%e6%96%87%e5%9f%ba%e4%ba%8e-kubernetesv122172">#&lt;/a>
&lt;/h5>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403191438271.png" alt="image-20240319143820198" />&lt;/p></description></item><item><title>2024-04-03 Kubernetes 证书详解(鉴权)</title><link>https://qq547475331.github.io/docs/kubernetes-%E8%AF%81%E4%B9%A6%E8%AF%A6%E8%A7%A3%E9%89%B4%E6%9D%83-kubernetes-zheng-shu-xiang-jie--jian-quan-/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/kubernetes-%E8%AF%81%E4%B9%A6%E8%AF%A6%E8%A7%A3%E9%89%B4%E6%9D%83-kubernetes-zheng-shu-xiang-jie--jian-quan-/</guid><description>&lt;h2 id="简介">
 简介
 &lt;a class="anchor" href="#%e7%ae%80%e4%bb%8b">#&lt;/a>
&lt;/h2>
&lt;h5 id="上一篇-kubernetes-证书详解认证-系统分析了-kubernetes-集群中每个证书的作用和证书认证的原理对于-kube-apiserverkubelet-来说它们都能提供-https-服务kube-apiserverkubelet-对于一个请求既要认证也要鉴权在-kube-apiserver-中鉴权也有多种方式">
 上一篇 &lt;a href="http://mp.weixin.qq.com/s?__biz=MzkyNDMyNjAyMg==&amp;amp;mid=2247484214&amp;amp;idx=1&amp;amp;sn=313df9f95fa77ac60abb777d3a72e5c2&amp;amp;chksm=c1d6c2dff6a14bc993cf3ca8990af9f1f403172038d91dc831e49377c10429a9dc3a51f9047a&amp;amp;scene=21#wechat_redirect">Kubernetes 证书详解(认证)&lt;/a> 系统分析了 Kubernetes 集群中每个证书的作用和证书认证的原理。对于 Kube-apiserver，Kubelet 来说，它们都能提供 HTTPS 服务，Kube-apiserver、Kubelet 对于一个请求，既要认证也要鉴权。在 Kube-apiserver 中，鉴权也有多种方式：
 &lt;a class="anchor" href="#%e4%b8%8a%e4%b8%80%e7%af%87-kubernetes-%e8%af%81%e4%b9%a6%e8%af%a6%e8%a7%a3%e8%ae%a4%e8%af%81-%e7%b3%bb%e7%bb%9f%e5%88%86%e6%9e%90%e4%ba%86-kubernetes-%e9%9b%86%e7%be%a4%e4%b8%ad%e6%af%8f%e4%b8%aa%e8%af%81%e4%b9%a6%e7%9a%84%e4%bd%9c%e7%94%a8%e5%92%8c%e8%af%81%e4%b9%a6%e8%ae%a4%e8%af%81%e7%9a%84%e5%8e%9f%e7%90%86%e5%af%b9%e4%ba%8e-kube-apiserverkubelet-%e6%9d%a5%e8%af%b4%e5%ae%83%e4%bb%ac%e9%83%bd%e8%83%bd%e6%8f%90%e4%be%9b-https-%e6%9c%8d%e5%8a%a1kube-apiserverkubelet-%e5%af%b9%e4%ba%8e%e4%b8%80%e4%b8%aa%e8%af%b7%e6%b1%82%e6%97%a2%e8%a6%81%e8%ae%a4%e8%af%81%e4%b9%9f%e8%a6%81%e9%89%b4%e6%9d%83%e5%9c%a8-kube-apiserver-%e4%b8%ad%e9%89%b4%e6%9d%83%e4%b9%9f%e6%9c%89%e5%a4%9a%e7%a7%8d%e6%96%b9%e5%bc%8f">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="-node">
 • Node
 &lt;a class="anchor" href="#-node">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-abac">
 • ABAC
 &lt;a class="anchor" href="#-abac">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-rbac">
 • RBAC
 &lt;a class="anchor" href="#-rbac">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-webhook">
 • Webhook
 &lt;a class="anchor" href="#-webhook">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h5 id="在-tls--rbac-模式下访问-kube-apiserver-有三种方式">
 在 TLS + RBAC 模式下，访问 Kube-apiserver 有三种方式：
 &lt;a class="anchor" href="#%e5%9c%a8-tls--rbac-%e6%a8%a1%e5%bc%8f%e4%b8%8b%e8%ae%bf%e9%97%ae-kube-apiserver-%e6%9c%89%e4%b8%89%e7%a7%8d%e6%96%b9%e5%bc%8f">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="-证书--rbac就是上一篇说到的那些证书">
 • 证书 + RBAC(就是上一篇说到的那些证书)
 &lt;a class="anchor" href="#-%e8%af%81%e4%b9%a6--rbac%e5%b0%b1%e6%98%af%e4%b8%8a%e4%b8%80%e7%af%87%e8%af%b4%e5%88%b0%e7%9a%84%e9%82%a3%e4%ba%9b%e8%af%81%e4%b9%a6">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-node--rbac-kubelet-访问-kube-apiserver-时">
 • Node + RBAC( Kubelet 访问 Kube-apiserver 时)
 &lt;a class="anchor" href="#-node--rbac-kubelet-%e8%ae%bf%e9%97%ae-kube-apiserver-%e6%97%b6">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-serviceaccount--rbac-kubernetes-集群内-pod-访问-kube-apiserver-">
 • ServiceAccount + RBAC( Kubernetes 集群内 Pod 访问 Kube-apiserver ）
 &lt;a class="anchor" href="#-serviceaccount--rbac-kubernetes-%e9%9b%86%e7%be%a4%e5%86%85-pod-%e8%ae%bf%e9%97%ae-kube-apiserver-">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h5 id="关于-rbac-的内容不熟悉的可以参考官网1">
 关于 RBAC 的内容不熟悉的可以参考官网[1]
 &lt;a class="anchor" href="#%e5%85%b3%e4%ba%8e-rbac-%e7%9a%84%e5%86%85%e5%ae%b9%e4%b8%8d%e7%86%9f%e6%82%89%e7%9a%84%e5%8f%af%e4%bb%a5%e5%8f%82%e8%80%83%e5%ae%98%e7%bd%911">#&lt;/a>
&lt;/h5>
&lt;h2 id="k8s-证书的-cno">
 K8S 证书的 CN、O
 &lt;a class="anchor" href="#k8s-%e8%af%81%e4%b9%a6%e7%9a%84-cno">#&lt;/a>
&lt;/h2>
&lt;h5 id="rbac-鉴权需要对-user-或者-group-来绑定相应权限达到效果kubernetes-证书中的-cn-表示-usero-表示-group看一个例子">
 RBAC 鉴权需要对 User 或者 Group 来绑定相应权限达到效果。Kubernetes 证书中的 &lt;em>&lt;code>CN&lt;/code>&lt;/em> 表示 User，&lt;code>O&lt;/code> 表示 Group，看一个例子：
 &lt;a class="anchor" href="#rbac-%e9%89%b4%e6%9d%83%e9%9c%80%e8%a6%81%e5%af%b9-user-%e6%88%96%e8%80%85-group-%e6%9d%a5%e7%bb%91%e5%ae%9a%e7%9b%b8%e5%ba%94%e6%9d%83%e9%99%90%e8%be%be%e5%88%b0%e6%95%88%e6%9e%9ckubernetes-%e8%af%81%e4%b9%a6%e4%b8%ad%e7%9a%84-cn-%e8%a1%a8%e7%a4%ba-usero-%e8%a1%a8%e7%a4%ba-group%e7%9c%8b%e4%b8%80%e4%b8%aa%e4%be%8b%e5%ad%90">#&lt;/a>
&lt;/h5>
&lt;h5 id="用-openssl-命令解析-kubelet-的客户端证书kubelet-访问-kube-apiserver-的时候就会用这个证书来认证鉴权">
 用 &lt;em>&lt;code>openssl&lt;/code>&lt;/em> 命令解析 kubelet 的客户端证书，kubelet 访问 Kube-apiserver 的时候就会用这个证书来认证，鉴权。
 &lt;a class="anchor" href="#%e7%94%a8-openssl-%e5%91%bd%e4%bb%a4%e8%a7%a3%e6%9e%90-kubelet-%e7%9a%84%e5%ae%a2%e6%88%b7%e7%ab%af%e8%af%81%e4%b9%a6kubelet-%e8%ae%bf%e9%97%ae-kube-apiserver-%e7%9a%84%e6%97%b6%e5%80%99%e5%b0%b1%e4%bc%9a%e7%94%a8%e8%bf%99%e4%b8%aa%e8%af%81%e4%b9%a6%e6%9d%a5%e8%ae%a4%e8%af%81%e9%89%b4%e6%9d%83">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>$ openssl x509 -noout -text -in kubelet-client-current.pem 
Certificate:
 Data:
 Version: 3 (0x2)
 Serial Number: 271895513527774644 (0x3c5f7876bd86db4)
 Signature Algorithm: sha256WithRSAEncryption
 Issuer: CN=kubernetes
 Validity
 Not Before: May 10 06:48:30 2023 GMT
 Not After : Apr 16 06:48:37 2123 GMT
 # 看这里
 Subject: O=system:nodes, CN=system:node:master-172-31-97-104
 Subject Public Key Info:
 .......
&lt;/code>&lt;/pre>&lt;h5 id="可以发现-kubelet-的客户端证书的-o-是-systemnodes-是-systemnodemaster-172-31-97-104-所以在-kubernetes-中每个结点的-kubelet-都被赋予-systemnode结点名称-的-user且附属于-systemnodes-的-group">
 可以发现 Kubelet 的客户端证书的 &lt;em>&lt;code>O&lt;/code>&lt;/em> 是 &lt;em>&lt;code>system.nodes&lt;/code>，&lt;code>CN&lt;/code>&lt;/em> 是 &lt;em>&lt;code>system:node:master-172-31-97-104&lt;/code>&lt;/em> ，所以在 Kubernetes 中，每个结点的 Kubelet 都被赋予 &lt;em>&lt;code>system:node:&amp;quot;结点名称“&lt;/code>&lt;/em> 的 User，且附属于 &lt;em>&lt;code>system.nodes&lt;/code>&lt;/em> 的 Group。
 &lt;a class="anchor" href="#%e5%8f%af%e4%bb%a5%e5%8f%91%e7%8e%b0-kubelet-%e7%9a%84%e5%ae%a2%e6%88%b7%e7%ab%af%e8%af%81%e4%b9%a6%e7%9a%84-o-%e6%98%af-systemnodes-%e6%98%af-systemnodemaster-172-31-97-104-%e6%89%80%e4%bb%a5%e5%9c%a8-kubernetes-%e4%b8%ad%e6%af%8f%e4%b8%aa%e7%bb%93%e7%82%b9%e7%9a%84-kubelet-%e9%83%bd%e8%a2%ab%e8%b5%8b%e4%ba%88-systemnode%e7%bb%93%e7%82%b9%e5%90%8d%e7%a7%b0-%e7%9a%84-user%e4%b8%94%e9%99%84%e5%b1%9e%e4%ba%8e-systemnodes-%e7%9a%84-group">#&lt;/a>
&lt;/h5>
&lt;h5 id="kubernetes-rbac-鉴权机制就是利用将权限绑定到-user-或者-group使得-usergroup-拥有对应权限下面就看看-kubernetes-如何-根据证书serviceaccount-鉴权的">
 Kubernetes RBAC 鉴权机制就是利用将权限绑定到 User 或者 Group，使得 User、Group 拥有对应权限，下面就看看 Kubernetes 如何 根据证书、ServiceAccount 鉴权的。
 &lt;a class="anchor" href="#kubernetes-rbac-%e9%89%b4%e6%9d%83%e6%9c%ba%e5%88%b6%e5%b0%b1%e6%98%af%e5%88%a9%e7%94%a8%e5%b0%86%e6%9d%83%e9%99%90%e7%bb%91%e5%ae%9a%e5%88%b0-user-%e6%88%96%e8%80%85-group%e4%bd%bf%e5%be%97-usergroup-%e6%8b%a5%e6%9c%89%e5%af%b9%e5%ba%94%e6%9d%83%e9%99%90%e4%b8%8b%e9%9d%a2%e5%b0%b1%e7%9c%8b%e7%9c%8b-kubernetes-%e5%a6%82%e4%bd%95-%e6%a0%b9%e6%8d%ae%e8%af%81%e4%b9%a6serviceaccount-%e9%89%b4%e6%9d%83%e7%9a%84">#&lt;/a>
&lt;/h5>
&lt;hr>
&lt;h2 id="kubectl">
 Kubectl
 &lt;a class="anchor" href="#kubectl">#&lt;/a>
&lt;/h2>
&lt;h5 id="kubectl-使用-kubeconfig-与-kube-apiserver-进行认证鉴权认证上一篇说过了就是通过-tls-认证这里说鉴权先看看-kubeconfig-的客户端证书-o">
 Kubectl 使用 KubeConfig 与 Kube-apiserver 进行认证、鉴权。认证上一篇说过了，就是通过 TLS 认证。这里说鉴权，先看看 KubeConfig 的客户端证书 &lt;em>&lt;code>O&lt;/code>、&lt;code>CN&lt;/code>&lt;/em>
 &lt;a class="anchor" href="#kubectl-%e4%bd%bf%e7%94%a8-kubeconfig-%e4%b8%8e-kube-apiserver-%e8%bf%9b%e8%a1%8c%e8%ae%a4%e8%af%81%e9%89%b4%e6%9d%83%e8%ae%a4%e8%af%81%e4%b8%8a%e4%b8%80%e7%af%87%e8%af%b4%e8%bf%87%e4%ba%86%e5%b0%b1%e6%98%af%e9%80%9a%e8%bf%87-tls-%e8%ae%a4%e8%af%81%e8%bf%99%e9%87%8c%e8%af%b4%e9%89%b4%e6%9d%83%e5%85%88%e7%9c%8b%e7%9c%8b-kubeconfig-%e7%9a%84%e5%ae%a2%e6%88%b7%e7%ab%af%e8%af%81%e4%b9%a6-o">#&lt;/a>
&lt;/h5>
&lt;h5 id="使用-openssl-命令解析-kubeconfig-中-client-certificate-data-字段查看-kubeconfig-客户端证书的-o">
 使用 &lt;em>&lt;code>openssl&lt;/code>&lt;/em> 命令解析 KubeConfig 中 &lt;em>&lt;code>client-certificate-data&lt;/code>&lt;/em> 字段，查看 KubeConfig 客户端证书的 &lt;em>&lt;code>O&lt;/code>、&lt;code>CN&lt;/code>&lt;/em>
 &lt;a class="anchor" href="#%e4%bd%bf%e7%94%a8-openssl-%e5%91%bd%e4%bb%a4%e8%a7%a3%e6%9e%90-kubeconfig-%e4%b8%ad-client-certificate-data-%e5%ad%97%e6%ae%b5%e6%9f%a5%e7%9c%8b-kubeconfig-%e5%ae%a2%e6%88%b7%e7%ab%af%e8%af%81%e4%b9%a6%e7%9a%84-o">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>$ cat /root/.kube/config | grep client-certificate-data: | sed &amp;#39;s/ client-certificate-data: //g&amp;#39; | base64 -d | openssl x509 -noout -subject
# 结果
subject= /O=system:masters/CN=kubernetes-admin
&lt;/code>&lt;/pre>&lt;h5 id="可以发现-kubeconfig-客户端证书为-kubernetes-admin-user-且属于-systemmasters-group">
 可以发现 KubeConfig 客户端证书为 &lt;em>&lt;code>kubernetes-admin&lt;/code>&lt;/em> User 且属于 &lt;em>&lt;code>system:masters&lt;/code>&lt;/em> Group。
 &lt;a class="anchor" href="#%e5%8f%af%e4%bb%a5%e5%8f%91%e7%8e%b0-kubeconfig-%e5%ae%a2%e6%88%b7%e7%ab%af%e8%af%81%e4%b9%a6%e4%b8%ba-kubernetes-admin-user-%e4%b8%94%e5%b1%9e%e4%ba%8e-systemmasters-group">#&lt;/a>
&lt;/h5>
&lt;h5 id="systemmasters-是-kubernetes-内置的用户组且-kubernetes-集群中也包含许多默认-clusterroleclusterrolebinding其中-cluster-admin-的-clusterrolebinding-就将-cluster-admin-的-clusterrole-绑定到-systemmasters-group这样-kubeconfig-就拥有权限来操作-kube-apiserver-了">
 &lt;em>&lt;code>system:masters&lt;/code>&lt;/em> 是 Kubernetes 内置的用户组，且 Kubernetes 集群中也包含许多&lt;strong>默认&lt;/strong> ClusterRole、ClusterRolebinding，其中 &lt;em>&lt;code>cluster-admin&lt;/code>&lt;/em> 的 ClusterRolebinding 就将 &lt;em>&lt;code>cluster-admin&lt;/code>&lt;/em> 的 ClusterRole 绑定到 &lt;em>&lt;code>system:masters&lt;/code>&lt;/em> Group，这样 KubeConfig 就拥有权限来操作 Kube-apiserver 了。
 &lt;a class="anchor" href="#systemmasters-%e6%98%af-kubernetes-%e5%86%85%e7%bd%ae%e7%9a%84%e7%94%a8%e6%88%b7%e7%bb%84%e4%b8%94-kubernetes-%e9%9b%86%e7%be%a4%e4%b8%ad%e4%b9%9f%e5%8c%85%e5%90%ab%e8%ae%b8%e5%a4%9a%e9%bb%98%e8%ae%a4-clusterroleclusterrolebinding%e5%85%b6%e4%b8%ad-cluster-admin-%e7%9a%84-clusterrolebinding-%e5%b0%b1%e5%b0%86-cluster-admin-%e7%9a%84-clusterrole-%e7%bb%91%e5%ae%9a%e5%88%b0-systemmasters-group%e8%bf%99%e6%a0%b7-kubeconfig-%e5%b0%b1%e6%8b%a5%e6%9c%89%e6%9d%83%e9%99%90%e6%9d%a5%e6%93%8d%e4%bd%9c-kube-apiserver-%e4%ba%86">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code># cluster-admin ClusterRole 拥有所有资源的所有权限
$ kubectl get clusterrole cluster-admin -o yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
 annotations:
 rbac.authorization.kubernetes.io/autoupdate: &amp;#34;true&amp;#34;
 creationTimestamp: &amp;#34;2023-05-10T06:49:27Z&amp;#34;
 labels:
 kubernetes.io/bootstrapping: rbac-defaults
 name: cluster-admin
 resourceVersion: &amp;#34;85&amp;#34;
 uid: 006af228-e6ef-43fa-a73f-ca0c109b13f0
rules:
- apiGroups:
 - &amp;#39;*&amp;#39;
 resources:
 - &amp;#39;*&amp;#39;
 verbs:
 - &amp;#39;*&amp;#39;
- nonResourceURLs:
 - &amp;#39;*&amp;#39;
 verbs:
 - &amp;#39;*&amp;#39;
-------------------------------------------------------------------

# cluster-admin ClusterRoleBinding 将 cluster-admin ClusterRole 绑定到 system:masters Group
$ kubectl get clusterrolebinding cluster-admin -o yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
 annotations:
 rbac.authorization.kubernetes.io/autoupdate: &amp;#34;true&amp;#34;
 creationTimestamp: &amp;#34;2023-05-10T06:49:27Z&amp;#34;
 labels:
 kubernetes.io/bootstrapping: rbac-defaults
 name: cluster-admin
 resourceVersion: &amp;#34;147&amp;#34;
 uid: 980fbdff-6750-4957-b5fa-954a5013b192
roleRef:
 apiGroup: rbac.authorization.k8s.io
 kind: ClusterRole
 # clusterRole name
 name: cluster-admin
subjects:
- apiGroup: rbac.authorization.k8s.io
 kind: Group
 # Group name
 name: system:masters
&lt;/code>&lt;/pre>&lt;h5 id="经过上面的操作kubectl-就可以使用-kubeconfig-来操作-kube-apiserver-了">
 经过上面的操作，Kubectl 就可以使用 KubeConfig 来操作 Kube-apiserver 了。
 &lt;a class="anchor" href="#%e7%bb%8f%e8%bf%87%e4%b8%8a%e9%9d%a2%e7%9a%84%e6%93%8d%e4%bd%9ckubectl-%e5%b0%b1%e5%8f%af%e4%bb%a5%e4%bd%bf%e7%94%a8-kubeconfig-%e6%9d%a5%e6%93%8d%e4%bd%9c-kube-apiserver-%e4%ba%86">#&lt;/a>
&lt;/h5>
&lt;hr>
&lt;h2 id="kube-schedulerkube-controller-manager">
 Kube-scheduler、Kube-controller-manager
 &lt;a class="anchor" href="#kube-schedulerkube-controller-manager">#&lt;/a>
&lt;/h2>
&lt;h5 id="同样-kube-schedulerkube-controller-manager-也是使用各自的-kubeconfig-来认证鉴权但是他们和-kubectl-的-kubeconfig-不属于同一个用户组">
 同样 Kube-scheduler、Kube-controller-manager 也是使用各自的 KubeConfig 来认证、鉴权。但是他们和 Kubectl 的 KubeConfig 不属于同一个用户、组。
 &lt;a class="anchor" href="#%e5%90%8c%e6%a0%b7-kube-schedulerkube-controller-manager-%e4%b9%9f%e6%98%af%e4%bd%bf%e7%94%a8%e5%90%84%e8%87%aa%e7%9a%84-kubeconfig-%e6%9d%a5%e8%ae%a4%e8%af%81%e9%89%b4%e6%9d%83%e4%bd%86%e6%98%af%e4%bb%96%e4%bb%ac%e5%92%8c-kubectl-%e7%9a%84-kubeconfig-%e4%b8%8d%e5%b1%9e%e4%ba%8e%e5%90%8c%e4%b8%80%e4%b8%aa%e7%94%a8%e6%88%b7%e7%bb%84">#&lt;/a>
&lt;/h5>
&lt;h5 id="使用-openssl-命令解析-kube-scheduler-的-kubeconfig-中-client-certificate-data-字段查看-kubeconfig-客户端证书的-o">
 使用 &lt;em>&lt;code>openssl&lt;/code>&lt;/em> 命令解析 Kube-scheduler 的 KubeConfig 中 &lt;em>&lt;code>client-certificate-data&lt;/code>&lt;/em> 字段，查看 KubeConfig 客户端证书的 &lt;em>&lt;code>O&lt;/code>、&lt;code>CN&lt;/code>&lt;/em>
 &lt;a class="anchor" href="#%e4%bd%bf%e7%94%a8-openssl-%e5%91%bd%e4%bb%a4%e8%a7%a3%e6%9e%90-kube-scheduler-%e7%9a%84-kubeconfig-%e4%b8%ad-client-certificate-data-%e5%ad%97%e6%ae%b5%e6%9f%a5%e7%9c%8b-kubeconfig-%e5%ae%a2%e6%88%b7%e7%ab%af%e8%af%81%e4%b9%a6%e7%9a%84-o">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>$ cat /etc/kubernetes/scheduler.conf | grep client-certificate-data: | sed &amp;#39;s/ client-certificate-data: //g&amp;#39; | base64 -d | openssl x509 -noout -subject
# 结果
subject= /CN=system:kube-scheduler
&lt;/code>&lt;/pre>&lt;h5 id="可以发现-kubeconfig-客户端证书为-kubernetes-admin-user-但是不属于某个-group">
 可以发现 KubeConfig 客户端证书为 &lt;em>&lt;code>kubernetes-admin&lt;/code>&lt;/em> User 但是不属于某个 Group。
 &lt;a class="anchor" href="#%e5%8f%af%e4%bb%a5%e5%8f%91%e7%8e%b0-kubeconfig-%e5%ae%a2%e6%88%b7%e7%ab%af%e8%af%81%e4%b9%a6%e4%b8%ba-kubernetes-admin-user-%e4%bd%86%e6%98%af%e4%b8%8d%e5%b1%9e%e4%ba%8e%e6%9f%90%e4%b8%aa-group">#&lt;/a>
&lt;/h5>
&lt;h5 id="systemkube-scheduler-也是-kubernetes-集群内部的设置的用户kubernetes-集群中也存在对应默认的-clusterrole-systemkube-scheduler-和-clusterrolebinding-systemkube-scheduler-">
 &lt;em>&lt;code>system:kube-scheduler&lt;/code>&lt;/em> 也是 Kubernetes 集群内部的设置的用户，Kubernetes 集群中也存在对应默认的 ClusterRole &lt;em>&lt;code>system:kube-scheduler&lt;/code>&lt;/em> 和 ClusterRoleBinding &lt;em>&lt;code>system:kube-scheduler&lt;/code>&lt;/em> 。
 &lt;a class="anchor" href="#systemkube-scheduler-%e4%b9%9f%e6%98%af-kubernetes-%e9%9b%86%e7%be%a4%e5%86%85%e9%83%a8%e7%9a%84%e8%ae%be%e7%bd%ae%e7%9a%84%e7%94%a8%e6%88%b7kubernetes-%e9%9b%86%e7%be%a4%e4%b8%ad%e4%b9%9f%e5%ad%98%e5%9c%a8%e5%af%b9%e5%ba%94%e9%bb%98%e8%ae%a4%e7%9a%84-clusterrole-systemkube-scheduler-%e5%92%8c-clusterrolebinding-systemkube-scheduler-">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code># system:kube-scheduler ClusterRole 拥有细分的权限
$ kubectl get clusterrole system:kube-scheduler -o yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
 annotations:
 rbac.authorization.kubernetes.io/autoupdate: &amp;#34;true&amp;#34;
 creationTimestamp: &amp;#34;2023-05-10T06:49:27Z&amp;#34;
 labels:
 kubernetes.io/bootstrapping: rbac-defaults
 name: system:kube-scheduler
 resourceVersion: &amp;#34;115&amp;#34;
 uid: b1ddf98c-bdb9-4d4d-9af3-9db1c97b038a
rules:
- apiGroups:
 - &amp;#34;&amp;#34;
 - events.k8s.io
 resources:
 - events
 verbs:
 - create
 - patch
 - update
- apiGroups:
 - coordination.k8s.io
 resources:
 - leases
 verbs:
 - create
- apiGroups:
 - coordination.k8s.io
 resourceNames:
 - kube-scheduler
 resources:
 - leases
 verbs:
 - get
 - update
- apiGroups:
 - &amp;#34;&amp;#34;
 resources:
 - endpoints
 verbs:
 - create
 .......
 # 一些更细致的权限
--------------------------------------------------------------------------

# system:kube-scheduler ClusterRoleBinding 将 system:kube-scheduler ClusterRole 绑定到 system:kube-scheduler User
$ kubectl get clusterrolebinding system:kube-scheduler -o yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
 annotations:
 rbac.authorization.kubernetes.io/autoupdate: &amp;#34;true&amp;#34;
 creationTimestamp: &amp;#34;2023-05-10T06:49:27Z&amp;#34;
 labels:
 kubernetes.io/bootstrapping: rbac-defaults
 name: system:kube-scheduler
 resourceVersion: &amp;#34;155&amp;#34;
 uid: a9b8a85e-bb5c-483c-a08e-51822ce84d7f
roleRef:
 apiGroup: rbac.authorization.k8s.io
 kind: ClusterRole
 # ClusterRole name
 name: system:kube-scheduler
subjects:
- apiGroup: rbac.authorization.k8s.io
 kind: User
 # User name
 name: system:kube-scheduler
&lt;/code>&lt;/pre>&lt;h5 id="可以发现-kube-scheduler-权限并不是向-kubectl-那样拥有所有资源的所有操作权限即使是内部组件基于最小权限原则kubernetes-依然会为这两个用户只绑定必要的权限">
 可以发现 Kube-scheduler 权限并不是向 Kubectl 那样拥有所有资源的所有操作权限，即使是内部组件，基于最小权限原则，Kubernetes 依然会为这两个用户只绑定必要的权限。
 &lt;a class="anchor" href="#%e5%8f%af%e4%bb%a5%e5%8f%91%e7%8e%b0-kube-scheduler-%e6%9d%83%e9%99%90%e5%b9%b6%e4%b8%8d%e6%98%af%e5%90%91-kubectl-%e9%82%a3%e6%a0%b7%e6%8b%a5%e6%9c%89%e6%89%80%e6%9c%89%e8%b5%84%e6%ba%90%e7%9a%84%e6%89%80%e6%9c%89%e6%93%8d%e4%bd%9c%e6%9d%83%e9%99%90%e5%8d%b3%e4%bd%bf%e6%98%af%e5%86%85%e9%83%a8%e7%bb%84%e4%bb%b6%e5%9f%ba%e4%ba%8e%e6%9c%80%e5%b0%8f%e6%9d%83%e9%99%90%e5%8e%9f%e5%88%99kubernetes-%e4%be%9d%e7%84%b6%e4%bc%9a%e4%b8%ba%e8%bf%99%e4%b8%a4%e4%b8%aa%e7%94%a8%e6%88%b7%e5%8f%aa%e7%bb%91%e5%ae%9a%e5%bf%85%e8%a6%81%e7%9a%84%e6%9d%83%e9%99%90">#&lt;/a>
&lt;/h5>
&lt;h5 id="kube-controller-manager-和-kube-scheduler-一样只不过-kube-controller-manager-的-user-是-systemkube-controller-manager-其默认的-clusterroleclusterrolebinding-名称不同而已可以根据-kube-scheduler-自行验证">
 Kube-controller-manager 和 Kube-scheduler 一样，只不过 Kube-controller-manager 的 User 是 &lt;em>&lt;code>system:kube-controller-manager&lt;/code>&lt;/em> ，其默认的 ClusterRole、ClusterRoleBinding 名称不同而已，可以根据 Kube-scheduler 自行验证~
 &lt;a class="anchor" href="#kube-controller-manager-%e5%92%8c-kube-scheduler-%e4%b8%80%e6%a0%b7%e5%8f%aa%e4%b8%8d%e8%bf%87-kube-controller-manager-%e7%9a%84-user-%e6%98%af-systemkube-controller-manager-%e5%85%b6%e9%bb%98%e8%ae%a4%e7%9a%84-clusterroleclusterrolebinding-%e5%90%8d%e7%a7%b0%e4%b8%8d%e5%90%8c%e8%80%8c%e5%b7%b2%e5%8f%af%e4%bb%a5%e6%a0%b9%e6%8d%ae-kube-scheduler-%e8%87%aa%e8%a1%8c%e9%aa%8c%e8%af%81">#&lt;/a>
&lt;/h5>
&lt;blockquote>
&lt;p>&lt;code>system:&lt;/code> 前缀是 Kubernetes 保留的关键字，用于描述内置的系统用户和系统用户组，在 Kube-apiserver 启动时，会默认为这些用户绑定好对应的权限，具体参考官网[2]&lt;/p></description></item><item><title>2024-04-03 Linux 性能优化大全</title><link>https://qq547475331.github.io/docs/linux-%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E5%A4%A7%E5%85%A8-linux-xing-neng-you-hua-da-quan/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/linux-%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E5%A4%A7%E5%85%A8-linux-xing-neng-you-hua-da-quan/</guid><description>&lt;p>&lt;strong>性能优化&lt;/strong>&lt;/p>
&lt;h4 id="性能指标">
 &lt;strong>性能指标&lt;/strong>
 &lt;a class="anchor" href="#%e6%80%a7%e8%83%bd%e6%8c%87%e6%a0%87">#&lt;/a>
&lt;/h4>
&lt;h5 id="高并发和响应快对应着性能优化的两个核心指标吞吐和延时">
 高并发和响应快对应着性能优化的两个核心指标：吞吐和延时
 &lt;a class="anchor" href="#%e9%ab%98%e5%b9%b6%e5%8f%91%e5%92%8c%e5%93%8d%e5%ba%94%e5%bf%ab%e5%af%b9%e5%ba%94%e7%9d%80%e6%80%a7%e8%83%bd%e4%bc%98%e5%8c%96%e7%9a%84%e4%b8%a4%e4%b8%aa%e6%a0%b8%e5%bf%83%e6%8c%87%e6%a0%87%e5%90%9e%e5%90%90%e5%92%8c%e5%bb%b6%e6%97%b6">#&lt;/a>
&lt;/h5>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202402261533052.png" alt="image-20240226153330970" />&lt;/p>
&lt;ul>
&lt;li>
&lt;h5 id="应用负载角度直接影响了产品终端的用户体验">
 应用负载角度：直接影响了产品终端的用户体验
 &lt;a class="anchor" href="#%e5%ba%94%e7%94%a8%e8%b4%9f%e8%bd%bd%e8%a7%92%e5%ba%a6%e7%9b%b4%e6%8e%a5%e5%bd%b1%e5%93%8d%e4%ba%86%e4%ba%a7%e5%93%81%e7%bb%88%e7%ab%af%e7%9a%84%e7%94%a8%e6%88%b7%e4%bd%93%e9%aa%8c">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="系统资源角度资源使用率饱和度等">
 系统资源角度：资源使用率、饱和度等
 &lt;a class="anchor" href="#%e7%b3%bb%e7%bb%9f%e8%b5%84%e6%ba%90%e8%a7%92%e5%ba%a6%e8%b5%84%e6%ba%90%e4%bd%bf%e7%94%a8%e7%8e%87%e9%a5%b1%e5%92%8c%e5%ba%a6%e7%ad%89">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h5 id="性能问题的本质就是系统资源已经到达瓶颈但请求的处理还不够快无法支撑更多的请求性能分析实际上就是找出应用或系统的瓶颈设法去避免或缓解它们">
 性能问题的本质就是系统资源已经到达瓶颈，但请求的处理还不够快，无法支撑更多的请求。性能分析实际上就是找出应用或系统的瓶颈，设法去避免或缓解它们。
 &lt;a class="anchor" href="#%e6%80%a7%e8%83%bd%e9%97%ae%e9%a2%98%e7%9a%84%e6%9c%ac%e8%b4%a8%e5%b0%b1%e6%98%af%e7%b3%bb%e7%bb%9f%e8%b5%84%e6%ba%90%e5%b7%b2%e7%bb%8f%e5%88%b0%e8%be%be%e7%93%b6%e9%a2%88%e4%bd%86%e8%af%b7%e6%b1%82%e7%9a%84%e5%a4%84%e7%90%86%e8%bf%98%e4%b8%8d%e5%a4%9f%e5%bf%ab%e6%97%a0%e6%b3%95%e6%94%af%e6%92%91%e6%9b%b4%e5%a4%9a%e7%9a%84%e8%af%b7%e6%b1%82%e6%80%a7%e8%83%bd%e5%88%86%e6%9e%90%e5%ae%9e%e9%99%85%e4%b8%8a%e5%b0%b1%e6%98%af%e6%89%be%e5%87%ba%e5%ba%94%e7%94%a8%e6%88%96%e7%b3%bb%e7%bb%9f%e7%9a%84%e7%93%b6%e9%a2%88%e8%ae%be%e6%b3%95%e5%8e%bb%e9%81%bf%e5%85%8d%e6%88%96%e7%bc%93%e8%a7%a3%e5%ae%83%e4%bb%ac">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="选择指标评估应用程序和系统性能">
 选择指标评估应用程序和系统性能
 &lt;a class="anchor" href="#%e9%80%89%e6%8b%a9%e6%8c%87%e6%a0%87%e8%af%84%e4%bc%b0%e5%ba%94%e7%94%a8%e7%a8%8b%e5%ba%8f%e5%92%8c%e7%b3%bb%e7%bb%9f%e6%80%a7%e8%83%bd">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="为应用程序和系统设置性能目标">
 为应用程序和系统设置性能目标
 &lt;a class="anchor" href="#%e4%b8%ba%e5%ba%94%e7%94%a8%e7%a8%8b%e5%ba%8f%e5%92%8c%e7%b3%bb%e7%bb%9f%e8%ae%be%e7%bd%ae%e6%80%a7%e8%83%bd%e7%9b%ae%e6%a0%87">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="进行性能基准测试">
 进行性能基准测试
 &lt;a class="anchor" href="#%e8%bf%9b%e8%a1%8c%e6%80%a7%e8%83%bd%e5%9f%ba%e5%87%86%e6%b5%8b%e8%af%95">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="性能分析定位瓶颈">
 性能分析定位瓶颈
 &lt;a class="anchor" href="#%e6%80%a7%e8%83%bd%e5%88%86%e6%9e%90%e5%ae%9a%e4%bd%8d%e7%93%b6%e9%a2%88">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="性能监控和告警">
 性能监控和告警
 &lt;a class="anchor" href="#%e6%80%a7%e8%83%bd%e7%9b%91%e6%8e%a7%e5%92%8c%e5%91%8a%e8%ad%a6">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h5 id="对于不同的性能问题要选取不同的性能分析工具下面是常用的linux-performance-tools以及对应分析的性能问题类型">
 对于不同的性能问题要选取不同的性能分析工具。下面是常用的Linux Performance Tools以及对应分析的性能问题类型。
 &lt;a class="anchor" href="#%e5%af%b9%e4%ba%8e%e4%b8%8d%e5%90%8c%e7%9a%84%e6%80%a7%e8%83%bd%e9%97%ae%e9%a2%98%e8%a6%81%e9%80%89%e5%8f%96%e4%b8%8d%e5%90%8c%e7%9a%84%e6%80%a7%e8%83%bd%e5%88%86%e6%9e%90%e5%b7%a5%e5%85%b7%e4%b8%8b%e9%9d%a2%e6%98%af%e5%b8%b8%e7%94%a8%e7%9a%84linux-performance-tools%e4%bb%a5%e5%8f%8a%e5%af%b9%e5%ba%94%e5%88%86%e6%9e%90%e7%9a%84%e6%80%a7%e8%83%bd%e9%97%ae%e9%a2%98%e7%b1%bb%e5%9e%8b">#&lt;/a>
&lt;/h5>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202402261533139.png" alt="image-20240226153358031" />&lt;/p>
&lt;h4 id="到底应该怎么理解平均负载">
 &lt;strong>到底应该怎么理解”平均负载”&lt;/strong>
 &lt;a class="anchor" href="#%e5%88%b0%e5%ba%95%e5%ba%94%e8%af%a5%e6%80%8e%e4%b9%88%e7%90%86%e8%a7%a3%e5%b9%b3%e5%9d%87%e8%b4%9f%e8%bd%bd">#&lt;/a>
&lt;/h4>
&lt;h5 id="平均负载单位时间内系统处于可运行状态和不可中断状态的平均进程数也就是平均活跃进程数它和我们传统意义上理解的cpu使用率并没有直接关系">
 平均负载：单位时间内，系统处于可运行状态和不可中断状态的平均进程数，也就是平均活跃进程数。它和我们传统意义上理解的CPU使用率并没有直接关系。
 &lt;a class="anchor" href="#%e5%b9%b3%e5%9d%87%e8%b4%9f%e8%bd%bd%e5%8d%95%e4%bd%8d%e6%97%b6%e9%97%b4%e5%86%85%e7%b3%bb%e7%bb%9f%e5%a4%84%e4%ba%8e%e5%8f%af%e8%bf%90%e8%a1%8c%e7%8a%b6%e6%80%81%e5%92%8c%e4%b8%8d%e5%8f%af%e4%b8%ad%e6%96%ad%e7%8a%b6%e6%80%81%e7%9a%84%e5%b9%b3%e5%9d%87%e8%bf%9b%e7%a8%8b%e6%95%b0%e4%b9%9f%e5%b0%b1%e6%98%af%e5%b9%b3%e5%9d%87%e6%b4%bb%e8%b7%83%e8%bf%9b%e7%a8%8b%e6%95%b0%e5%ae%83%e5%92%8c%e6%88%91%e4%bb%ac%e4%bc%a0%e7%bb%9f%e6%84%8f%e4%b9%89%e4%b8%8a%e7%90%86%e8%a7%a3%e7%9a%84cpu%e4%bd%bf%e7%94%a8%e7%8e%87%e5%b9%b6%e6%b2%a1%e6%9c%89%e7%9b%b4%e6%8e%a5%e5%85%b3%e7%b3%bb">#&lt;/a>
&lt;/h5>
&lt;h5 id="其中不可中断进程是正处于内核态关键流程中的进程如常见的等待设备的io响应不可中断状态实际上是系统对进程和硬件设备的一种保护机制">
 其中不可中断进程是正处于内核态关键流程中的进程（如常见的等待设备的I/O响应）。不可中断状态实际上是系统对进程和硬件设备的一种保护机制。
 &lt;a class="anchor" href="#%e5%85%b6%e4%b8%ad%e4%b8%8d%e5%8f%af%e4%b8%ad%e6%96%ad%e8%bf%9b%e7%a8%8b%e6%98%af%e6%ad%a3%e5%a4%84%e4%ba%8e%e5%86%85%e6%a0%b8%e6%80%81%e5%85%b3%e9%94%ae%e6%b5%81%e7%a8%8b%e4%b8%ad%e7%9a%84%e8%bf%9b%e7%a8%8b%e5%a6%82%e5%b8%b8%e8%a7%81%e7%9a%84%e7%ad%89%e5%be%85%e8%ae%be%e5%a4%87%e7%9a%84io%e5%93%8d%e5%ba%94%e4%b8%8d%e5%8f%af%e4%b8%ad%e6%96%ad%e7%8a%b6%e6%80%81%e5%ae%9e%e9%99%85%e4%b8%8a%e6%98%af%e7%b3%bb%e7%bb%9f%e5%af%b9%e8%bf%9b%e7%a8%8b%e5%92%8c%e7%a1%ac%e4%bb%b6%e8%ae%be%e5%a4%87%e7%9a%84%e4%b8%80%e7%a7%8d%e4%bf%9d%e6%8a%a4%e6%9c%ba%e5%88%b6">#&lt;/a>
&lt;/h5>
&lt;h4 id="平均负载多少时合理">
 &lt;strong>平均负载多少时合理&lt;/strong>
 &lt;a class="anchor" href="#%e5%b9%b3%e5%9d%87%e8%b4%9f%e8%bd%bd%e5%a4%9a%e5%b0%91%e6%97%b6%e5%90%88%e7%90%86">#&lt;/a>
&lt;/h4>
&lt;h5 id="实际生产环境中将系统的平均负载监控起来根据历史数据判断负载的变化趋势当负载存在明显升高趋势时及时进行分析和调查当然也可以当设置阈值如当平均负载高于cpu数量的70时">
 实际生产环境中将系统的平均负载监控起来，根据历史数据判断负载的变化趋势。当负载存在明显升高趋势时，及时进行分析和调查。当然也可以当设置阈值（如当平均负载高于CPU数量的70%时）
 &lt;a class="anchor" href="#%e5%ae%9e%e9%99%85%e7%94%9f%e4%ba%a7%e7%8e%af%e5%a2%83%e4%b8%ad%e5%b0%86%e7%b3%bb%e7%bb%9f%e7%9a%84%e5%b9%b3%e5%9d%87%e8%b4%9f%e8%bd%bd%e7%9b%91%e6%8e%a7%e8%b5%b7%e6%9d%a5%e6%a0%b9%e6%8d%ae%e5%8e%86%e5%8f%b2%e6%95%b0%e6%8d%ae%e5%88%a4%e6%96%ad%e8%b4%9f%e8%bd%bd%e7%9a%84%e5%8f%98%e5%8c%96%e8%b6%8b%e5%8a%bf%e5%bd%93%e8%b4%9f%e8%bd%bd%e5%ad%98%e5%9c%a8%e6%98%8e%e6%98%be%e5%8d%87%e9%ab%98%e8%b6%8b%e5%8a%bf%e6%97%b6%e5%8f%8a%e6%97%b6%e8%bf%9b%e8%a1%8c%e5%88%86%e6%9e%90%e5%92%8c%e8%b0%83%e6%9f%a5%e5%bd%93%e7%84%b6%e4%b9%9f%e5%8f%af%e4%bb%a5%e5%bd%93%e8%ae%be%e7%bd%ae%e9%98%88%e5%80%bc%e5%a6%82%e5%bd%93%e5%b9%b3%e5%9d%87%e8%b4%9f%e8%bd%bd%e9%ab%98%e4%ba%8ecpu%e6%95%b0%e9%87%8f%e7%9a%8470%e6%97%b6">#&lt;/a>
&lt;/h5>
&lt;h5 id="现实工作中我们会经常混淆平均负载和cpu使用率的概念其实两者并不完全对等">
 现实工作中我们会经常混淆平均负载和CPU使用率的概念，其实两者并不完全对等：
 &lt;a class="anchor" href="#%e7%8e%b0%e5%ae%9e%e5%b7%a5%e4%bd%9c%e4%b8%ad%e6%88%91%e4%bb%ac%e4%bc%9a%e7%bb%8f%e5%b8%b8%e6%b7%b7%e6%b7%86%e5%b9%b3%e5%9d%87%e8%b4%9f%e8%bd%bd%e5%92%8ccpu%e4%bd%bf%e7%94%a8%e7%8e%87%e7%9a%84%e6%a6%82%e5%bf%b5%e5%85%b6%e5%ae%9e%e4%b8%a4%e8%80%85%e5%b9%b6%e4%b8%8d%e5%ae%8c%e5%85%a8%e5%af%b9%e7%ad%89">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="cpu-密集型进程大量-cpu-使用会导致平均负载升高此时两者一致">
 CPU 密集型进程，大量 CPU 使用会导致平均负载升高，此时两者一致
 &lt;a class="anchor" href="#cpu-%e5%af%86%e9%9b%86%e5%9e%8b%e8%bf%9b%e7%a8%8b%e5%a4%a7%e9%87%8f-cpu-%e4%bd%bf%e7%94%a8%e4%bc%9a%e5%af%bc%e8%87%b4%e5%b9%b3%e5%9d%87%e8%b4%9f%e8%bd%bd%e5%8d%87%e9%ab%98%e6%ad%a4%e6%97%b6%e4%b8%a4%e8%80%85%e4%b8%80%e8%87%b4">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="io-密集型进程等待-io-也会导致平均负载升高此时-cpu-使用率并不一定高">
 I/O 密集型进程，等待 I/O 也会导致平均负载升高，此时 CPU 使用率并不一定高
 &lt;a class="anchor" href="#io-%e5%af%86%e9%9b%86%e5%9e%8b%e8%bf%9b%e7%a8%8b%e7%ad%89%e5%be%85-io-%e4%b9%9f%e4%bc%9a%e5%af%bc%e8%87%b4%e5%b9%b3%e5%9d%87%e8%b4%9f%e8%bd%bd%e5%8d%87%e9%ab%98%e6%ad%a4%e6%97%b6-cpu-%e4%bd%bf%e7%94%a8%e7%8e%87%e5%b9%b6%e4%b8%8d%e4%b8%80%e5%ae%9a%e9%ab%98">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="大量等待-cpu-的进程调度会导致平均负载升高此时-cpu-使用率也会比较高">
 大量等待 CPU 的进程调度会导致平均负载升高，此时 CPU 使用率也会比较高
 &lt;a class="anchor" href="#%e5%a4%a7%e9%87%8f%e7%ad%89%e5%be%85-cpu-%e7%9a%84%e8%bf%9b%e7%a8%8b%e8%b0%83%e5%ba%a6%e4%bc%9a%e5%af%bc%e8%87%b4%e5%b9%b3%e5%9d%87%e8%b4%9f%e8%bd%bd%e5%8d%87%e9%ab%98%e6%ad%a4%e6%97%b6-cpu-%e4%bd%bf%e7%94%a8%e7%8e%87%e4%b9%9f%e4%bc%9a%e6%af%94%e8%be%83%e9%ab%98">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h5 id="平均负载高时可能是-cpu-密集型进程导致也可能是-io-繁忙导致具体分析时可以结合-mpstatpidstat-工具辅助分析负载来源">
 平均负载高时可能是 CPU 密集型进程导致，也可能是 I/O 繁忙导致。具体分析时可以结合 mpstat/pidstat 工具辅助分析负载来源。
 &lt;a class="anchor" href="#%e5%b9%b3%e5%9d%87%e8%b4%9f%e8%bd%bd%e9%ab%98%e6%97%b6%e5%8f%af%e8%83%bd%e6%98%af-cpu-%e5%af%86%e9%9b%86%e5%9e%8b%e8%bf%9b%e7%a8%8b%e5%af%bc%e8%87%b4%e4%b9%9f%e5%8f%af%e8%83%bd%e6%98%af-io-%e7%b9%81%e5%bf%99%e5%af%bc%e8%87%b4%e5%85%b7%e4%bd%93%e5%88%86%e6%9e%90%e6%97%b6%e5%8f%af%e4%bb%a5%e7%bb%93%e5%90%88-mpstatpidstat-%e5%b7%a5%e5%85%b7%e8%be%85%e5%8a%a9%e5%88%86%e6%9e%90%e8%b4%9f%e8%bd%bd%e6%9d%a5%e6%ba%90">#&lt;/a>
&lt;/h5>
&lt;h3 id="cpu">
 &lt;strong>CPU&lt;/strong>
 &lt;a class="anchor" href="#cpu">#&lt;/a>
&lt;/h3>
&lt;h4 id="cpu上下文切换上">
 &lt;strong>CPU上下文切换(上)&lt;/strong>
 &lt;a class="anchor" href="#cpu%e4%b8%8a%e4%b8%8b%e6%96%87%e5%88%87%e6%8d%a2%e4%b8%8a">#&lt;/a>
&lt;/h4>
&lt;h5 id="cpu-上下文切换就是把前一个任务的-cpu-上下文cpu-寄存器和-pc保存起来然后加载新任务的上下文到这些寄存器和程序计数器最后再跳转到程序计数器所指的位置运行新任务其中保存下来的上下文会存储在系统内核中待任务重新调度执行时再加载保证原来的任务状态不受影响">
 CPU 上下文切换，就是把前一个任务的 CPU 上下文（CPU 寄存器和 PC）保存起来，然后加载新任务的上下文到这些寄存器和程序计数器，最后再跳转到程序计数器所指的位置，运行新任务。其中，保存下来的上下文会存储在系统内核中，待任务重新调度执行时再加载，保证原来的任务状态不受影响。
 &lt;a class="anchor" href="#cpu-%e4%b8%8a%e4%b8%8b%e6%96%87%e5%88%87%e6%8d%a2%e5%b0%b1%e6%98%af%e6%8a%8a%e5%89%8d%e4%b8%80%e4%b8%aa%e4%bb%bb%e5%8a%a1%e7%9a%84-cpu-%e4%b8%8a%e4%b8%8b%e6%96%87cpu-%e5%af%84%e5%ad%98%e5%99%a8%e5%92%8c-pc%e4%bf%9d%e5%ad%98%e8%b5%b7%e6%9d%a5%e7%84%b6%e5%90%8e%e5%8a%a0%e8%bd%bd%e6%96%b0%e4%bb%bb%e5%8a%a1%e7%9a%84%e4%b8%8a%e4%b8%8b%e6%96%87%e5%88%b0%e8%bf%99%e4%ba%9b%e5%af%84%e5%ad%98%e5%99%a8%e5%92%8c%e7%a8%8b%e5%ba%8f%e8%ae%a1%e6%95%b0%e5%99%a8%e6%9c%80%e5%90%8e%e5%86%8d%e8%b7%b3%e8%bd%ac%e5%88%b0%e7%a8%8b%e5%ba%8f%e8%ae%a1%e6%95%b0%e5%99%a8%e6%89%80%e6%8c%87%e7%9a%84%e4%bd%8d%e7%bd%ae%e8%bf%90%e8%a1%8c%e6%96%b0%e4%bb%bb%e5%8a%a1%e5%85%b6%e4%b8%ad%e4%bf%9d%e5%ad%98%e4%b8%8b%e6%9d%a5%e7%9a%84%e4%b8%8a%e4%b8%8b%e6%96%87%e4%bc%9a%e5%ad%98%e5%82%a8%e5%9c%a8%e7%b3%bb%e7%bb%9f%e5%86%85%e6%a0%b8%e4%b8%ad%e5%be%85%e4%bb%bb%e5%8a%a1%e9%87%8d%e6%96%b0%e8%b0%83%e5%ba%a6%e6%89%a7%e8%a1%8c%e6%97%b6%e5%86%8d%e5%8a%a0%e8%bd%bd%e4%bf%9d%e8%af%81%e5%8e%9f%e6%9d%a5%e7%9a%84%e4%bb%bb%e5%8a%a1%e7%8a%b6%e6%80%81%e4%b8%8d%e5%8f%97%e5%bd%b1%e5%93%8d">#&lt;/a>
&lt;/h5>
&lt;h5 id="按照任务类型cpu-上下文切换分为">
 按照任务类型，CPU 上下文切换分为：
 &lt;a class="anchor" href="#%e6%8c%89%e7%85%a7%e4%bb%bb%e5%8a%a1%e7%b1%bb%e5%9e%8bcpu-%e4%b8%8a%e4%b8%8b%e6%96%87%e5%88%87%e6%8d%a2%e5%88%86%e4%b8%ba">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="进程上下文切换">
 进程上下文切换
 &lt;a class="anchor" href="#%e8%bf%9b%e7%a8%8b%e4%b8%8a%e4%b8%8b%e6%96%87%e5%88%87%e6%8d%a2">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="线程上下文切换">
 线程上下文切换
 &lt;a class="anchor" href="#%e7%ba%bf%e7%a8%8b%e4%b8%8a%e4%b8%8b%e6%96%87%e5%88%87%e6%8d%a2">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="中断上下文切换">
 中断上下文切换
 &lt;a class="anchor" href="#%e4%b8%ad%e6%96%ad%e4%b8%8a%e4%b8%8b%e6%96%87%e5%88%87%e6%8d%a2">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h5 id="进程上下文切换-1">
 &lt;strong>进程上下文切换&lt;/strong>
 &lt;a class="anchor" href="#%e8%bf%9b%e7%a8%8b%e4%b8%8a%e4%b8%8b%e6%96%87%e5%88%87%e6%8d%a2-1">#&lt;/a>
&lt;/h5>
&lt;h5 id="linux-进程按照等级权限将进程的运行空间分为内核空间和用户空间从用户态向内核态转变时需要通过系统调用来完成">
 Linux 进程按照等级权限将进程的运行空间分为内核空间和用户空间。从用户态向内核态转变时需要通过系统调用来完成。
 &lt;a class="anchor" href="#linux-%e8%bf%9b%e7%a8%8b%e6%8c%89%e7%85%a7%e7%ad%89%e7%ba%a7%e6%9d%83%e9%99%90%e5%b0%86%e8%bf%9b%e7%a8%8b%e7%9a%84%e8%bf%90%e8%a1%8c%e7%a9%ba%e9%97%b4%e5%88%86%e4%b8%ba%e5%86%85%e6%a0%b8%e7%a9%ba%e9%97%b4%e5%92%8c%e7%94%a8%e6%88%b7%e7%a9%ba%e9%97%b4%e4%bb%8e%e7%94%a8%e6%88%b7%e6%80%81%e5%90%91%e5%86%85%e6%a0%b8%e6%80%81%e8%bd%ac%e5%8f%98%e6%97%b6%e9%9c%80%e8%a6%81%e9%80%9a%e8%bf%87%e7%b3%bb%e7%bb%9f%e8%b0%83%e7%94%a8%e6%9d%a5%e5%ae%8c%e6%88%90">#&lt;/a>
&lt;/h5>
&lt;h5 id="一次系统调用过程其实进行了两次-cpu-上下文切换">
 一次系统调用过程其实进行了两次 CPU 上下文切换：
 &lt;a class="anchor" href="#%e4%b8%80%e6%ac%a1%e7%b3%bb%e7%bb%9f%e8%b0%83%e7%94%a8%e8%bf%87%e7%a8%8b%e5%85%b6%e5%ae%9e%e8%bf%9b%e8%a1%8c%e4%ba%86%e4%b8%a4%e6%ac%a1-cpu-%e4%b8%8a%e4%b8%8b%e6%96%87%e5%88%87%e6%8d%a2">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="cpu-寄存器中用户态的指令位置先保存起来cpu-寄存器更新为内核态指令的位置跳转到内核态运行内核任务">
 CPU 寄存器中用户态的指令位置先保存起来，CPU 寄存器更新为内核态指令的位置，跳转到内核态运行内核任务；
 &lt;a class="anchor" href="#cpu-%e5%af%84%e5%ad%98%e5%99%a8%e4%b8%ad%e7%94%a8%e6%88%b7%e6%80%81%e7%9a%84%e6%8c%87%e4%bb%a4%e4%bd%8d%e7%bd%ae%e5%85%88%e4%bf%9d%e5%ad%98%e8%b5%b7%e6%9d%a5cpu-%e5%af%84%e5%ad%98%e5%99%a8%e6%9b%b4%e6%96%b0%e4%b8%ba%e5%86%85%e6%a0%b8%e6%80%81%e6%8c%87%e4%bb%a4%e7%9a%84%e4%bd%8d%e7%bd%ae%e8%b7%b3%e8%bd%ac%e5%88%b0%e5%86%85%e6%a0%b8%e6%80%81%e8%bf%90%e8%a1%8c%e5%86%85%e6%a0%b8%e4%bb%bb%e5%8a%a1">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="系统调用结束后cpu-寄存器恢复原来保存的用户态数据再切换到用户空间继续运行">
 系统调用结束后，CPU 寄存器恢复原来保存的用户态数据，再切换到用户空间继续运行。
 &lt;a class="anchor" href="#%e7%b3%bb%e7%bb%9f%e8%b0%83%e7%94%a8%e7%bb%93%e6%9d%9f%e5%90%8ecpu-%e5%af%84%e5%ad%98%e5%99%a8%e6%81%a2%e5%a4%8d%e5%8e%9f%e6%9d%a5%e4%bf%9d%e5%ad%98%e7%9a%84%e7%94%a8%e6%88%b7%e6%80%81%e6%95%b0%e6%8d%ae%e5%86%8d%e5%88%87%e6%8d%a2%e5%88%b0%e7%94%a8%e6%88%b7%e7%a9%ba%e9%97%b4%e7%bb%a7%e7%bb%ad%e8%bf%90%e8%a1%8c">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h5 id="系统调用过程中并不会涉及虚拟内存等进程用户态资源也不会切换进程和传统意义上的进程上下文切换不同因此系统调用通常称为特权模式切换">
 系统调用过程中并不会涉及虚拟内存等进程用户态资源，也不会切换进程。和传统意义上的进程上下文切换不同。因此系统调用通常称为特权模式切换。
 &lt;a class="anchor" href="#%e7%b3%bb%e7%bb%9f%e8%b0%83%e7%94%a8%e8%bf%87%e7%a8%8b%e4%b8%ad%e5%b9%b6%e4%b8%8d%e4%bc%9a%e6%b6%89%e5%8f%8a%e8%99%9a%e6%8b%9f%e5%86%85%e5%ad%98%e7%ad%89%e8%bf%9b%e7%a8%8b%e7%94%a8%e6%88%b7%e6%80%81%e8%b5%84%e6%ba%90%e4%b9%9f%e4%b8%8d%e4%bc%9a%e5%88%87%e6%8d%a2%e8%bf%9b%e7%a8%8b%e5%92%8c%e4%bc%a0%e7%bb%9f%e6%84%8f%e4%b9%89%e4%b8%8a%e7%9a%84%e8%bf%9b%e7%a8%8b%e4%b8%8a%e4%b8%8b%e6%96%87%e5%88%87%e6%8d%a2%e4%b8%8d%e5%90%8c%e5%9b%a0%e6%ad%a4%e7%b3%bb%e7%bb%9f%e8%b0%83%e7%94%a8%e9%80%9a%e5%b8%b8%e7%a7%b0%e4%b8%ba%e7%89%b9%e6%9d%83%e6%a8%a1%e5%bc%8f%e5%88%87%e6%8d%a2">#&lt;/a>
&lt;/h5>
&lt;h5 id="进程是由内核管理和调度的进程上下文切换只能发生在内核态因此相比系统调用来说在保存当前进程的内核状态和cpu寄存器之前需要先把该进程的虚拟内存栈保存下来再加载新进程的内核态后还要刷新进程的虚拟内存和用户栈">
 进程是由内核管理和调度的，进程上下文切换只能发生在内核态。因此相比系统调用来说，在保存当前进程的内核状态和CPU寄存器之前，需要先把该进程的虚拟内存，栈保存下来。再加载新进程的内核态后，还要刷新进程的虚拟内存和用户栈。
 &lt;a class="anchor" href="#%e8%bf%9b%e7%a8%8b%e6%98%af%e7%94%b1%e5%86%85%e6%a0%b8%e7%ae%a1%e7%90%86%e5%92%8c%e8%b0%83%e5%ba%a6%e7%9a%84%e8%bf%9b%e7%a8%8b%e4%b8%8a%e4%b8%8b%e6%96%87%e5%88%87%e6%8d%a2%e5%8f%aa%e8%83%bd%e5%8f%91%e7%94%9f%e5%9c%a8%e5%86%85%e6%a0%b8%e6%80%81%e5%9b%a0%e6%ad%a4%e7%9b%b8%e6%af%94%e7%b3%bb%e7%bb%9f%e8%b0%83%e7%94%a8%e6%9d%a5%e8%af%b4%e5%9c%a8%e4%bf%9d%e5%ad%98%e5%bd%93%e5%89%8d%e8%bf%9b%e7%a8%8b%e7%9a%84%e5%86%85%e6%a0%b8%e7%8a%b6%e6%80%81%e5%92%8ccpu%e5%af%84%e5%ad%98%e5%99%a8%e4%b9%8b%e5%89%8d%e9%9c%80%e8%a6%81%e5%85%88%e6%8a%8a%e8%af%a5%e8%bf%9b%e7%a8%8b%e7%9a%84%e8%99%9a%e6%8b%9f%e5%86%85%e5%ad%98%e6%a0%88%e4%bf%9d%e5%ad%98%e4%b8%8b%e6%9d%a5%e5%86%8d%e5%8a%a0%e8%bd%bd%e6%96%b0%e8%bf%9b%e7%a8%8b%e7%9a%84%e5%86%85%e6%a0%b8%e6%80%81%e5%90%8e%e8%bf%98%e8%a6%81%e5%88%b7%e6%96%b0%e8%bf%9b%e7%a8%8b%e7%9a%84%e8%99%9a%e6%8b%9f%e5%86%85%e5%ad%98%e5%92%8c%e7%94%a8%e6%88%b7%e6%a0%88">#&lt;/a>
&lt;/h5>
&lt;h5 id="进程只有在调度到cpu上运行时才需要切换上下文有以下几种场景cpu时间片轮流分配系统资源不足导致进程挂起进程通过sleep函数主动挂起高优先级进程抢占时间片硬件中断时cpu上的进程被挂起转而执行内核中的中断服务">
 进程只有在调度到CPU上运行时才需要切换上下文，有以下几种场景：CPU时间片轮流分配，系统资源不足导致进程挂起，进程通过sleep函数主动挂起，高优先级进程抢占时间片，硬件中断时CPU上的进程被挂起转而执行内核中的中断服务。
 &lt;a class="anchor" href="#%e8%bf%9b%e7%a8%8b%e5%8f%aa%e6%9c%89%e5%9c%a8%e8%b0%83%e5%ba%a6%e5%88%b0cpu%e4%b8%8a%e8%bf%90%e8%a1%8c%e6%97%b6%e6%89%8d%e9%9c%80%e8%a6%81%e5%88%87%e6%8d%a2%e4%b8%8a%e4%b8%8b%e6%96%87%e6%9c%89%e4%bb%a5%e4%b8%8b%e5%87%a0%e7%a7%8d%e5%9c%ba%e6%99%afcpu%e6%97%b6%e9%97%b4%e7%89%87%e8%bd%ae%e6%b5%81%e5%88%86%e9%85%8d%e7%b3%bb%e7%bb%9f%e8%b5%84%e6%ba%90%e4%b8%8d%e8%b6%b3%e5%af%bc%e8%87%b4%e8%bf%9b%e7%a8%8b%e6%8c%82%e8%b5%b7%e8%bf%9b%e7%a8%8b%e9%80%9a%e8%bf%87sleep%e5%87%bd%e6%95%b0%e4%b8%bb%e5%8a%a8%e6%8c%82%e8%b5%b7%e9%ab%98%e4%bc%98%e5%85%88%e7%ba%a7%e8%bf%9b%e7%a8%8b%e6%8a%a2%e5%8d%a0%e6%97%b6%e9%97%b4%e7%89%87%e7%a1%ac%e4%bb%b6%e4%b8%ad%e6%96%ad%e6%97%b6cpu%e4%b8%8a%e7%9a%84%e8%bf%9b%e7%a8%8b%e8%a2%ab%e6%8c%82%e8%b5%b7%e8%bd%ac%e8%80%8c%e6%89%a7%e8%a1%8c%e5%86%85%e6%a0%b8%e4%b8%ad%e7%9a%84%e4%b8%ad%e6%96%ad%e6%9c%8d%e5%8a%a1">#&lt;/a>
&lt;/h5>
&lt;h5 id="线程上下文切换-1">
 &lt;strong>线程上下文切换&lt;/strong>
 &lt;a class="anchor" href="#%e7%ba%bf%e7%a8%8b%e4%b8%8a%e4%b8%8b%e6%96%87%e5%88%87%e6%8d%a2-1">#&lt;/a>
&lt;/h5>
&lt;h5 id="线程上下文切换分为两种">
 线程上下文切换分为两种：
 &lt;a class="anchor" href="#%e7%ba%bf%e7%a8%8b%e4%b8%8a%e4%b8%8b%e6%96%87%e5%88%87%e6%8d%a2%e5%88%86%e4%b8%ba%e4%b8%a4%e7%a7%8d">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="前后线程同属于一个进程切换时虚拟内存资源不变只需要切换线程的私有数据寄存器等">
 前后线程同属于一个进程，切换时虚拟内存资源不变，只需要切换线程的私有数据，寄存器等；
 &lt;a class="anchor" href="#%e5%89%8d%e5%90%8e%e7%ba%bf%e7%a8%8b%e5%90%8c%e5%b1%9e%e4%ba%8e%e4%b8%80%e4%b8%aa%e8%bf%9b%e7%a8%8b%e5%88%87%e6%8d%a2%e6%97%b6%e8%99%9a%e6%8b%9f%e5%86%85%e5%ad%98%e8%b5%84%e6%ba%90%e4%b8%8d%e5%8f%98%e5%8f%aa%e9%9c%80%e8%a6%81%e5%88%87%e6%8d%a2%e7%ba%bf%e7%a8%8b%e7%9a%84%e7%a7%81%e6%9c%89%e6%95%b0%e6%8d%ae%e5%af%84%e5%ad%98%e5%99%a8%e7%ad%89">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="前后线程属于不同进程与进程上下文切换相同">
 前后线程属于不同进程，与进程上下文切换相同。
 &lt;a class="anchor" href="#%e5%89%8d%e5%90%8e%e7%ba%bf%e7%a8%8b%e5%b1%9e%e4%ba%8e%e4%b8%8d%e5%90%8c%e8%bf%9b%e7%a8%8b%e4%b8%8e%e8%bf%9b%e7%a8%8b%e4%b8%8a%e4%b8%8b%e6%96%87%e5%88%87%e6%8d%a2%e7%9b%b8%e5%90%8c">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h5 id="同进程的线程切换消耗资源较少这也是多线程的优势">
 同进程的线程切换消耗资源较少，这也是多线程的优势。
 &lt;a class="anchor" href="#%e5%90%8c%e8%bf%9b%e7%a8%8b%e7%9a%84%e7%ba%bf%e7%a8%8b%e5%88%87%e6%8d%a2%e6%b6%88%e8%80%97%e8%b5%84%e6%ba%90%e8%be%83%e5%b0%91%e8%bf%99%e4%b9%9f%e6%98%af%e5%a4%9a%e7%ba%bf%e7%a8%8b%e7%9a%84%e4%bc%98%e5%8a%bf">#&lt;/a>
&lt;/h5>
&lt;h5 id="中断上下文切换-1">
 &lt;strong>中断上下文切换&lt;/strong>
 &lt;a class="anchor" href="#%e4%b8%ad%e6%96%ad%e4%b8%8a%e4%b8%8b%e6%96%87%e5%88%87%e6%8d%a2-1">#&lt;/a>
&lt;/h5>
&lt;h5 id="中断上下文切换并不涉及到进程的用户态因此中断上下文只包括内核态中断服务程序执行所必须的状态cpu寄存器内核堆栈硬件中断参数等">
 中断上下文切换并不涉及到进程的用户态，因此中断上下文只包括内核态中断服务程序执行所必须的状态（CPU寄存器，内核堆栈，硬件中断参数等）。
 &lt;a class="anchor" href="#%e4%b8%ad%e6%96%ad%e4%b8%8a%e4%b8%8b%e6%96%87%e5%88%87%e6%8d%a2%e5%b9%b6%e4%b8%8d%e6%b6%89%e5%8f%8a%e5%88%b0%e8%bf%9b%e7%a8%8b%e7%9a%84%e7%94%a8%e6%88%b7%e6%80%81%e5%9b%a0%e6%ad%a4%e4%b8%ad%e6%96%ad%e4%b8%8a%e4%b8%8b%e6%96%87%e5%8f%aa%e5%8c%85%e6%8b%ac%e5%86%85%e6%a0%b8%e6%80%81%e4%b8%ad%e6%96%ad%e6%9c%8d%e5%8a%a1%e7%a8%8b%e5%ba%8f%e6%89%a7%e8%a1%8c%e6%89%80%e5%bf%85%e9%a1%bb%e7%9a%84%e7%8a%b6%e6%80%81cpu%e5%af%84%e5%ad%98%e5%99%a8%e5%86%85%e6%a0%b8%e5%a0%86%e6%a0%88%e7%a1%ac%e4%bb%b6%e4%b8%ad%e6%96%ad%e5%8f%82%e6%95%b0%e7%ad%89">#&lt;/a>
&lt;/h5>
&lt;h5 id="中断处理优先级比进程高所以中断上下文切换和进程上下文切换不会同时发生">
 中断处理优先级比进程高，所以中断上下文切换和进程上下文切换不会同时发生
 &lt;a class="anchor" href="#%e4%b8%ad%e6%96%ad%e5%a4%84%e7%90%86%e4%bc%98%e5%85%88%e7%ba%a7%e6%af%94%e8%bf%9b%e7%a8%8b%e9%ab%98%e6%89%80%e4%bb%a5%e4%b8%ad%e6%96%ad%e4%b8%8a%e4%b8%8b%e6%96%87%e5%88%87%e6%8d%a2%e5%92%8c%e8%bf%9b%e7%a8%8b%e4%b8%8a%e4%b8%8b%e6%96%87%e5%88%87%e6%8d%a2%e4%b8%8d%e4%bc%9a%e5%90%8c%e6%97%b6%e5%8f%91%e7%94%9f">#&lt;/a>
&lt;/h5>
&lt;h4 id="cpu上下文切换下">
 &lt;strong>CPU上下文切换(下)&lt;/strong>
 &lt;a class="anchor" href="#cpu%e4%b8%8a%e4%b8%8b%e6%96%87%e5%88%87%e6%8d%a2%e4%b8%8b">#&lt;/a>
&lt;/h4>
&lt;p>通过 vmstat 可以查看系统总体的上下文切换情况&lt;/p></description></item><item><title>2024-04-03 MetalLB L2 原理</title><link>https://qq547475331.github.io/docs/metallb-l2-%E5%8E%9F%E7%90%86-metallbl2-yuan-li/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/metallb-l2-%E5%8E%9F%E7%90%86-metallbl2-yuan-li/</guid><description>&lt;h1 id="穷人版-lb--metallb-l2-原理">
 穷人版 LB &amp;ndash; MetalLB L2 原理
 &lt;a class="anchor" href="#%e7%a9%b7%e4%ba%ba%e7%89%88-lb--metallb-l2-%e5%8e%9f%e7%90%86">#&lt;/a>
&lt;/h1>
&lt;h1 id="lb-的作用">
 LB 的作用
 &lt;a class="anchor" href="#lb-%e7%9a%84%e4%bd%9c%e7%94%a8">#&lt;/a>
&lt;/h1>
&lt;h5 id="在-kubernetes-集群中要想把服务暴露给别人访问无外乎是使用-service-nodeport-或者-hostnetwork-网络模式这种方式会使得主机端口随着服务的数量增加而增加显然不建议使用">
 在 Kubernetes 集群中，要想把服务暴露给别人访问，无外乎是使用 Service &lt;strong>NodePort&lt;/strong> 或者 &lt;strong>hostNetwork&lt;/strong> 网络模式。这种方式会使得主机端口随着服务的数量增加而增加，显然不建议使用。
 &lt;a class="anchor" href="#%e5%9c%a8-kubernetes-%e9%9b%86%e7%be%a4%e4%b8%ad%e8%a6%81%e6%83%b3%e6%8a%8a%e6%9c%8d%e5%8a%a1%e6%9a%b4%e9%9c%b2%e7%bb%99%e5%88%ab%e4%ba%ba%e8%ae%bf%e9%97%ae%e6%97%a0%e5%a4%96%e4%b9%8e%e6%98%af%e4%bd%bf%e7%94%a8-service-nodeport-%e6%88%96%e8%80%85-hostnetwork-%e7%bd%91%e7%bb%9c%e6%a8%a1%e5%bc%8f%e8%bf%99%e7%a7%8d%e6%96%b9%e5%bc%8f%e4%bc%9a%e4%bd%bf%e5%be%97%e4%b8%bb%e6%9c%ba%e7%ab%af%e5%8f%a3%e9%9a%8f%e7%9d%80%e6%9c%8d%e5%8a%a1%e7%9a%84%e6%95%b0%e9%87%8f%e5%a2%9e%e5%8a%a0%e8%80%8c%e5%a2%9e%e5%8a%a0%e6%98%be%e7%84%b6%e4%b8%8d%e5%bb%ba%e8%ae%ae%e4%bd%bf%e7%94%a8">#&lt;/a>
&lt;/h5>
&lt;h5 id="那么可以使用网关来转发到后端服务这样只需要开启网关的-service-nodeport-即可显然网关承载了入口流量网关就需要实现高可用一般会在网关前面部署一个-lb-流量经过-lb-负载到多个网关示例那么可能又有人会这个-lb-又怎么高可用">
 那么可以使用网关来转发到后端服务，这样只需要开启网关的 Service &lt;strong>NodePort&lt;/strong> 即可，显然网关承载了入口流量，网关就需要实现高可用。一般会在网关前面部署一个 LB ，流量经过 LB 负载到多个网关示例。那么可能又有人会，这个 LB 又怎么高可用？
 &lt;a class="anchor" href="#%e9%82%a3%e4%b9%88%e5%8f%af%e4%bb%a5%e4%bd%bf%e7%94%a8%e7%bd%91%e5%85%b3%e6%9d%a5%e8%bd%ac%e5%8f%91%e5%88%b0%e5%90%8e%e7%ab%af%e6%9c%8d%e5%8a%a1%e8%bf%99%e6%a0%b7%e5%8f%aa%e9%9c%80%e8%a6%81%e5%bc%80%e5%90%af%e7%bd%91%e5%85%b3%e7%9a%84-service-nodeport-%e5%8d%b3%e5%8f%af%e6%98%be%e7%84%b6%e7%bd%91%e5%85%b3%e6%89%bf%e8%bd%bd%e4%ba%86%e5%85%a5%e5%8f%a3%e6%b5%81%e9%87%8f%e7%bd%91%e5%85%b3%e5%b0%b1%e9%9c%80%e8%a6%81%e5%ae%9e%e7%8e%b0%e9%ab%98%e5%8f%af%e7%94%a8%e4%b8%80%e8%88%ac%e4%bc%9a%e5%9c%a8%e7%bd%91%e5%85%b3%e5%89%8d%e9%9d%a2%e9%83%a8%e7%bd%b2%e4%b8%80%e4%b8%aa-lb-%e6%b5%81%e9%87%8f%e7%bb%8f%e8%bf%87-lb-%e8%b4%9f%e8%bd%bd%e5%88%b0%e5%a4%9a%e4%b8%aa%e7%bd%91%e5%85%b3%e7%a4%ba%e4%be%8b%e9%82%a3%e4%b9%88%e5%8f%af%e8%83%bd%e5%8f%88%e6%9c%89%e4%ba%ba%e4%bc%9a%e8%bf%99%e4%b8%aa-lb-%e5%8f%88%e6%80%8e%e4%b9%88%e9%ab%98%e5%8f%af%e7%94%a8">#&lt;/a>
&lt;/h5>
&lt;h5 id="公有云厂商直接提供-lb对于自行搭建的-kubetnetes-集群就需要一个乞丐版的-lb-来实现开源的-lb-也有很多这里介绍-metallb1metallb-实现了所说的负载均衡功能同时自身也是高可用的">
 公有云厂商直接提供 LB，对于自行搭建的 Kubetnetes 集群，就需要一个乞丐版的 LB 来实现，开源的 LB 也有很多，这里介绍 Metallb[1]，MetalLB 实现了所说的负载均衡功能，同时自身也是高可用的。
 &lt;a class="anchor" href="#%e5%85%ac%e6%9c%89%e4%ba%91%e5%8e%82%e5%95%86%e7%9b%b4%e6%8e%a5%e6%8f%90%e4%be%9b-lb%e5%af%b9%e4%ba%8e%e8%87%aa%e8%a1%8c%e6%90%ad%e5%bb%ba%e7%9a%84-kubetnetes-%e9%9b%86%e7%be%a4%e5%b0%b1%e9%9c%80%e8%a6%81%e4%b8%80%e4%b8%aa%e4%b9%9e%e4%b8%90%e7%89%88%e7%9a%84-lb-%e6%9d%a5%e5%ae%9e%e7%8e%b0%e5%bc%80%e6%ba%90%e7%9a%84-lb-%e4%b9%9f%e6%9c%89%e5%be%88%e5%a4%9a%e8%bf%99%e9%87%8c%e4%bb%8b%e7%bb%8d-metallb1metallb-%e5%ae%9e%e7%8e%b0%e4%ba%86%e6%89%80%e8%af%b4%e7%9a%84%e8%b4%9f%e8%bd%bd%e5%9d%87%e8%a1%a1%e5%8a%9f%e8%83%bd%e5%90%8c%e6%97%b6%e8%87%aa%e8%ba%ab%e4%b9%9f%e6%98%af%e9%ab%98%e5%8f%af%e7%94%a8%e7%9a%84">#&lt;/a>
&lt;/h5>
&lt;h1 id="使用-metallb">
 使用 Metallb
 &lt;a class="anchor" href="#%e4%bd%bf%e7%94%a8-metallb">#&lt;/a>
&lt;/h1>
&lt;h5 id="metallb-提供了自行搭建-kubernetes-集群负载均衡的功能类似于公有云一样来体验负载均衡">
 Metallb 提供了自行搭建 Kubernetes 集群负载均衡的功能，类似于公有云一样来体验负载均衡。
 &lt;a class="anchor" href="#metallb-%e6%8f%90%e4%be%9b%e4%ba%86%e8%87%aa%e8%a1%8c%e6%90%ad%e5%bb%ba-kubernetes-%e9%9b%86%e7%be%a4%e8%b4%9f%e8%bd%bd%e5%9d%87%e8%a1%a1%e7%9a%84%e5%8a%9f%e8%83%bd%e7%b1%bb%e4%bc%bc%e4%ba%8e%e5%85%ac%e6%9c%89%e4%ba%91%e4%b8%80%e6%a0%b7%e6%9d%a5%e4%bd%93%e9%aa%8c%e8%b4%9f%e8%bd%bd%e5%9d%87%e8%a1%a1">#&lt;/a>
&lt;/h5>
&lt;h5 id="metallb-提供两种模式bgpl2两种模式各有优缺点本文主要讲解-l2-模式">
 Metallb 提供两种模式：BGP、L2。两种模式各有优缺点，本文主要讲解 &lt;strong>L2&lt;/strong> 模式。
 &lt;a class="anchor" href="#metallb-%e6%8f%90%e4%be%9b%e4%b8%a4%e7%a7%8d%e6%a8%a1%e5%bc%8fbgpl2%e4%b8%a4%e7%a7%8d%e6%a8%a1%e5%bc%8f%e5%90%84%e6%9c%89%e4%bc%98%e7%bc%ba%e7%82%b9%e6%9c%ac%e6%96%87%e4%b8%bb%e8%a6%81%e8%ae%b2%e8%a7%a3-l2-%e6%a8%a1%e5%bc%8f">#&lt;/a>
&lt;/h5>
&lt;h3 id="依赖">
 依赖
 &lt;a class="anchor" href="#%e4%be%9d%e8%b5%96">#&lt;/a>
&lt;/h3>
&lt;h5 id="在-kubernetes-集群中使用-metallb-需要满足以下需求">
 在 Kubernetes 集群中使用 Metallb 需要满足以下需求：
 &lt;a class="anchor" href="#%e5%9c%a8-kubernetes-%e9%9b%86%e7%be%a4%e4%b8%ad%e4%bd%bf%e7%94%a8-metallb-%e9%9c%80%e8%a6%81%e6%bb%a1%e8%b6%b3%e4%bb%a5%e4%b8%8b%e9%9c%80%e6%b1%82">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="-kubernetes-版本--v113">
 • Kubernetes 版本 ≥ &lt;code>v1.13&lt;/code>
 &lt;a class="anchor" href="#-kubernetes-%e7%89%88%e6%9c%ac--v113">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-提供-ip-地址段或者多个-ip-给-metallb-分配">
 • 提供 IP 地址段或者多个 IP 给 Metallb 分配
 &lt;a class="anchor" href="#-%e6%8f%90%e4%be%9b-ip-%e5%9c%b0%e5%9d%80%e6%ae%b5%e6%88%96%e8%80%85%e5%a4%9a%e4%b8%aa-ip-%e7%bb%99-metallb-%e5%88%86%e9%85%8d">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-如果使用-l2-模式主机上需要放行-7946-端口">
 • 如果使用 L2 模式，主机上需要放行 7946 端口
 &lt;a class="anchor" href="#-%e5%a6%82%e6%9e%9c%e4%bd%bf%e7%94%a8-l2-%e6%a8%a1%e5%bc%8f%e4%b8%bb%e6%9c%ba%e4%b8%8a%e9%9c%80%e8%a6%81%e6%94%be%e8%a1%8c-7946-%e7%ab%af%e5%8f%a3">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h3 id="部署">
 部署
 &lt;a class="anchor" href="#%e9%83%a8%e7%bd%b2">#&lt;/a>
&lt;/h3>
&lt;h5 id="修改-kube-proxy-配置">
 修改 &lt;strong>Kube-proxy&lt;/strong> 配置
 &lt;a class="anchor" href="#%e4%bf%ae%e6%94%b9-kube-proxy-%e9%85%8d%e7%bd%ae">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>$ kubectl edit configmap -n kube-system kube-proxy

apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
mode: &amp;#34;ipvs&amp;#34;
ipvs:
 strictARP: true
&lt;/code>&lt;/pre>&lt;h5 id="通过-manifests-安装当然也可以通过-helm-或者-operator-方式安装可参考官网2">
 通过 manifests 安装，当然也可以通过 helm 或者 operator 方式安装，可参考官网[2]
 &lt;a class="anchor" href="#%e9%80%9a%e8%bf%87-manifests-%e5%ae%89%e8%a3%85%e5%bd%93%e7%84%b6%e4%b9%9f%e5%8f%af%e4%bb%a5%e9%80%9a%e8%bf%87-helm-%e6%88%96%e8%80%85-operator-%e6%96%b9%e5%bc%8f%e5%ae%89%e8%a3%85%e5%8f%af%e5%8f%82%e8%80%83%e5%ae%98%e7%bd%912">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>$ kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.13.12/config/manifests/metallb-native.yaml
&lt;/code>&lt;/pre>&lt;h5 id="部署成功后集群中在-metallb-system-namespace-下会存在名为-controller-的-deployment-和-speaker-的-daemonset">
 部署成功后，集群中在 &lt;code>metallb-system&lt;/code> namespace 下会存在名为 &lt;strong>Controller&lt;/strong> 的 deployment 和 &lt;strong>Speaker&lt;/strong> 的 daemonSet
 &lt;a class="anchor" href="#%e9%83%a8%e7%bd%b2%e6%88%90%e5%8a%9f%e5%90%8e%e9%9b%86%e7%be%a4%e4%b8%ad%e5%9c%a8-metallb-system-namespace-%e4%b8%8b%e4%bc%9a%e5%ad%98%e5%9c%a8%e5%90%8d%e4%b8%ba-controller-%e7%9a%84-deployment-%e5%92%8c-speaker-%e7%9a%84-daemonset">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="-metallb-systemcontroller-deployment该-controller-用于-watch-集群中使用-loadbalancer-类型的-service-并为其分配-external-ip">
 • &lt;code>metallb-system/controller&lt;/code> deployment，该 Controller 用于 watch 集群中使用 &lt;strong>LoadBalancer&lt;/strong> 类型的 service 并为其分配 &lt;code>EXTERNAL-IP&lt;/code>
 &lt;a class="anchor" href="#-metallb-systemcontroller-deployment%e8%af%a5-controller-%e7%94%a8%e4%ba%8e-watch-%e9%9b%86%e7%be%a4%e4%b8%ad%e4%bd%bf%e7%94%a8-loadbalancer-%e7%b1%bb%e5%9e%8b%e7%9a%84-service-%e5%b9%b6%e4%b8%ba%e5%85%b6%e5%88%86%e9%85%8d-external-ip">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-metallb-systemspeaker-daemonset该组件在每个节点都会运行使用-hostnetwork-网络模式可以保证上面分配的-ip-可访问">
 • &lt;code>metallb-system/speaker&lt;/code> daemonset，该组件在每个节点都会运行，使用 &lt;strong>hostNetwork&lt;/strong> 网络模式，可以保证上面分配的 IP 可访问。
 &lt;a class="anchor" href="#-metallb-systemspeaker-daemonset%e8%af%a5%e7%bb%84%e4%bb%b6%e5%9c%a8%e6%af%8f%e4%b8%aa%e8%8a%82%e7%82%b9%e9%83%bd%e4%bc%9a%e8%bf%90%e8%a1%8c%e4%bd%bf%e7%94%a8-hostnetwork-%e7%bd%91%e7%bb%9c%e6%a8%a1%e5%bc%8f%e5%8f%af%e4%bb%a5%e4%bf%9d%e8%af%81%e4%b8%8a%e9%9d%a2%e5%88%86%e9%85%8d%e7%9a%84-ip-%e5%8f%af%e8%ae%bf%e9%97%ae">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>$ kubectl get pods -n metallb-system -o wide
NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES
controller-679f6b6cb5-l5k2z 1/1 Running 0 46h 100.80.192.27 node-172-30-91-215 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
speaker-6qxxj 1/1 Running 0 46h 172.30.91.17 node-172-30-91-17 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
speaker-chb8g 1/1 Running 0 46h 172.30.91.215 node-172-30-91-215 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
speaker-kw9vd 1/1 Running 0 46h 172.30.91.166 master-172-30-91-166 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
&lt;/code>&lt;/pre>&lt;h3 id="配置">
 配置
 &lt;a class="anchor" href="#%e9%85%8d%e7%bd%ae">#&lt;/a>
&lt;/h3>
&lt;h5 id="上面只是部署了-metallb要想使用-metallb-l2-模式还需要一些配置才能正常使用">
 上面只是部署了 Metallb，要想使用 Metallb L2 模式还需要一些配置，才能正常使用。
 &lt;a class="anchor" href="#%e4%b8%8a%e9%9d%a2%e5%8f%aa%e6%98%af%e9%83%a8%e7%bd%b2%e4%ba%86-metallb%e8%a6%81%e6%83%b3%e4%bd%bf%e7%94%a8-metallb-l2-%e6%a8%a1%e5%bc%8f%e8%bf%98%e9%9c%80%e8%a6%81%e4%b8%80%e4%ba%9b%e9%85%8d%e7%bd%ae%e6%89%8d%e8%83%bd%e6%ad%a3%e5%b8%b8%e4%bd%bf%e7%94%a8">#&lt;/a>
&lt;/h5>
&lt;h5 id="上面说到-metallb-会自动分配-ip-给集群中的-loadbalancer-service所以就需要提供地址池给-metallb通过创建-ipaddresspool-cr">
 上面说到 Metallb 会自动分配 IP 给集群中的 &lt;strong>LoadBalancer&lt;/strong> service，所以就需要提供地址池给 Metallb，通过创建 &lt;code>IPAddressPool&lt;/code> CR
 &lt;a class="anchor" href="#%e4%b8%8a%e9%9d%a2%e8%af%b4%e5%88%b0-metallb-%e4%bc%9a%e8%87%aa%e5%8a%a8%e5%88%86%e9%85%8d-ip-%e7%bb%99%e9%9b%86%e7%be%a4%e4%b8%ad%e7%9a%84-loadbalancer-service%e6%89%80%e4%bb%a5%e5%b0%b1%e9%9c%80%e8%a6%81%e6%8f%90%e4%be%9b%e5%9c%b0%e5%9d%80%e6%b1%a0%e7%bb%99-metallb%e9%80%9a%e8%bf%87%e5%88%9b%e5%bb%ba-ipaddresspool-cr">#&lt;/a>
&lt;/h5>
&lt;h5 id="地址池可以支持-ip-网段也支持-ip-组">
 地址池可以支持 IP 网段，也支持 IP 组
 &lt;a class="anchor" href="#%e5%9c%b0%e5%9d%80%e6%b1%a0%e5%8f%af%e4%bb%a5%e6%94%af%e6%8c%81-ip-%e7%bd%91%e6%ae%b5%e4%b9%9f%e6%94%af%e6%8c%81-ip-%e7%bb%84">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code># 地址池配置多个 IP
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
 name: subnet-91
 namespace: metallb-system
spec:
 addresses:
 - 172.30.91.2/32
 - 172.30.91.3/32
 - 172.30.91.4/32

# 地址池配置 IP 网络段

apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
 name: subnet-91
 namespace: metallb-system
spec:
 addresses:
 - 172.30.91.0/24
&lt;/code>&lt;/pre>&lt;h5 id="上面配置创建成功后可通过命令查看">
 上面配置创建成功后，可通过命令查看
 &lt;a class="anchor" href="#%e4%b8%8a%e9%9d%a2%e9%85%8d%e7%bd%ae%e5%88%9b%e5%bb%ba%e6%88%90%e5%8a%9f%e5%90%8e%e5%8f%af%e9%80%9a%e8%bf%87%e5%91%bd%e4%bb%a4%e6%9f%a5%e7%9c%8b">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>$ kubectl get IPAddressPool -n metallb-system
NAME AUTO ASSIGN AVOID BUGGY IPS ADDRESSES
subnet-91 true false [&amp;#34;172.30.91.2/32&amp;#34;,&amp;#34;172.30.91.3/32&amp;#34;,&amp;#34;172.30.91.4/32&amp;#34;]
&lt;/code>&lt;/pre>&lt;blockquote>
&lt;p>1、以上配置的 IP 需要路由可达或者和 K8S 集群节点同属一个网段&lt;/p></description></item><item><title>2024-04-03 prometheus企业级监控使用总结</title><link>https://qq547475331.github.io/docs/prometheus%E4%BC%81%E4%B8%9A%E7%BA%A7%E7%9B%91%E6%8E%A7%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93-prometheus-qi-ye-ji-jian-kong-shi-yong-zong-jie/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/prometheus%E4%BC%81%E4%B8%9A%E7%BA%A7%E7%9B%91%E6%8E%A7%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93-prometheus-qi-ye-ji-jian-kong-shi-yong-zong-jie/</guid><description>&lt;h2 id="一监控概念误区">
 &lt;strong>一、监控概念&amp;amp;误区&lt;/strong>
 &lt;a class="anchor" href="#%e4%b8%80%e7%9b%91%e6%8e%a7%e6%a6%82%e5%bf%b5%e8%af%af%e5%8c%ba">#&lt;/a>
&lt;/h2>
&lt;p>监控是管理基础设施和业务的核心工具，监控应该和应用程序一起构建和部署，没有监控，将无法了解你的系统运行环境，进行故障诊断，也无法阻止提供系统性的性能、成本和状态等信息。&lt;/p>
&lt;p>误区：要尽量避免进行机械式的监控、不够准确的监控、静态和监控、不频繁的监控、缺少自动化或自服务。&lt;/p>
&lt;h2 id="二黑盒监控白盒监控">
 &lt;strong>二、黑盒监控&amp;amp;白盒监控&lt;/strong>
 &lt;a class="anchor" href="#%e4%ba%8c%e9%bb%91%e7%9b%92%e7%9b%91%e6%8e%a7%e7%99%bd%e7%9b%92%e7%9b%91%e6%8e%a7">#&lt;/a>
&lt;/h2>
&lt;p>1、黑盒监控&lt;/p>
&lt;ul>
&lt;li>应用程序或主机是从外部观察的，因此，这种方法可能相当有限。检查是为了评估被观察的系统是否以已知的方式响应探测。&lt;/li>
&lt;li>例子：&lt;/li>
&lt;/ul>
&lt;p>1）主机是否相应PING的请求&lt;/p>
&lt;p>2）特定的TCP端口是否打开&lt;/p>
&lt;p>3）应用程序在接受到特定的HTTP请求时，是否使用正确的数据和状态代码进行响应&lt;/p>
&lt;p>4）特定应用程序的进程是否在其主机中运行&lt;/p>
&lt;h3 id="2白盒监控">
 2、白盒监控
 &lt;a class="anchor" href="#2%e7%99%bd%e7%9b%92%e7%9b%91%e6%8e%a7">#&lt;/a>
&lt;/h3>
&lt;p>系统在被测对象表面显示其内部状态和临界段的性能数据。这种类型的自省可能非常强大，因为它暴露了内部操作，显示不同内部组件的健康状况，否则很难甚至不可能确定。这种数据处理通常以胰腺癌方式进行处理：&lt;/p>
&lt;p>&lt;strong>1）通过日志导出&lt;/strong>：到目前为止。这是也是在广泛引入库之前，应用程序是如何暴露其内部工作的最常见的情况，例如：可以处理 HTTP 服务器的访问日志来监视请求率、延迟和错误百分比；&lt;/p>
&lt;p>&lt;strong>2）以结构化的事件输出&lt;/strong>：这种方法类似于日志记录，但不是将数据写入磁盘，而是直接将数据发送到处理系统进行分析和聚合。&lt;/p>
&lt;p>&lt;strong>3）以聚合的方式保存在内存中&lt;/strong>：这种格式的数据可以驻留在端点中，也可以直接从命令行工具中读取。这种方法的例子有/metrics with Prometheus metrics、HAProxy 的 stats 页面或 varnishstats 命令行工具。&lt;/p>
&lt;h2 id="三度量指标">
 &lt;strong>三、度量指标&lt;/strong>
 &lt;a class="anchor" href="#%e4%b8%89%e5%ba%a6%e9%87%8f%e6%8c%87%e6%a0%87">#&lt;/a>
&lt;/h2>
&lt;p>度量指标有监控系统执行的过程通常可以分为两种方式：push（监控系统去服务进行拉取）、pull（被监控的服务自动往监控系统进行推送）【站在客户的角度】&lt;/p>
&lt;ul>
&lt;li>Push VS Pull&lt;/li>
&lt;li>测量什么：&lt;/li>
&lt;/ul>
&lt;p>谷歌提出应该监控的四个指标：&lt;/p>
&lt;ul>
&lt;li>延迟：服务请求所需的时间&lt;/li>
&lt;li>流量：正在发出的请求的数量&lt;/li>
&lt;li>错误：求失败的比率&lt;/li>
&lt;li>饱和：未处理的工作量，通常在队列中&lt;/li>
&lt;/ul>
&lt;p>Brendan 的方法更关注于及其他声明对于每个资源（CPU、磁盘、网络接口等等），应该监视以下指标：&lt;/p>
&lt;ul>
&lt;li>利用率：以资源繁忙的百分比来衡量&lt;/li>
&lt;li>饱和：资源无法处理的工作量，通常会排队&lt;/li>
&lt;li>错误：发生的错误数量&lt;/li>
&lt;/ul>
&lt;p>汤姆威尔基的红色方法：更侧重于服务级别方法，而不是底层系统本身。显然，这种才领略对于见识服务很有用，对于预测外部客户的体验也很有价值。如果服务的错误率增加，那么就可以合理地假设这些错误将直接或间接地影响客户的体验。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>速率：转换成每秒请求数&lt;/p>
&lt;/li>
&lt;li>
&lt;p>错误：每秒失败请求的数量&lt;/p>
&lt;/li>
&lt;li>
&lt;p>持久性：这些请求所花费的时间&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="四prometheus">
 &lt;strong>四、Prometheus&lt;/strong>
 &lt;a class="anchor" href="#%e5%9b%9bprometheus">#&lt;/a>
&lt;/h2>
&lt;p>**
**&lt;/p>
&lt;h3 id="1介绍架构">
 1、介绍&amp;amp;架构
 &lt;a class="anchor" href="#1%e4%bb%8b%e7%bb%8d%e6%9e%b6%e6%9e%84">#&lt;/a>
&lt;/h3>
&lt;p>Prometheus 是一个开源系统监控和警报工具包，将其监控的指标进行收集并存储为时间序列数据，即指标信息与记录时的时间戳以及称为标签的可选键值对一起存储。很多公司用来监控 K8s集群。&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202402221119644.png" alt="image-20240222111955521" />&lt;/p>
&lt;h3 id="2-合适不合适场景">
 2. 合适&amp;amp;不合适场景
 &lt;a class="anchor" href="#2-%e5%90%88%e9%80%82%e4%b8%8d%e5%90%88%e9%80%82%e5%9c%ba%e6%99%af">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>合适场景&lt;/strong>：Prometheus 可以很好地记录任何数字时间序列，它既适合以机器为中心的监控，也适合监控高度动态的面向服务的架构。在微服务的世界中，他对多维数据收集的查询的支持是一个特殊的优势。专为可靠性而设计，是在中断期间可以使用的系统，可让你快速诊断问题。每个Prometheus服务器都是独立的，不依赖于网络存储或其他远程服务。当你的基础设施的其他部分损坏时，你可以依赖他，并且你无需设置大量基础设施即可使用&lt;/li>
&lt;li>&lt;strong>不合适场景&lt;/strong>：你需要100%准确性，例如按请求计费。这时候Prometheus就不太适合，你最好使用其他系统来收集和分析数据以进行计费。&lt;/li>
&lt;/ul>
&lt;h3 id="3-数据模型">
 3. 数据模型
 &lt;a class="anchor" href="#3-%e6%95%b0%e6%8d%ae%e6%a8%a1%e5%9e%8b">#&lt;/a>
&lt;/h3>
&lt;p>因为监控数量极大，所以使用了时间序列数据存储（就是带时间戳和值的）&lt;/p></description></item><item><title>2024-04-03 ssl证书自签发</title><link>https://qq547475331.github.io/docs/ssl%E8%AF%81%E4%B9%A6%E8%87%AA%E7%AD%BE%E5%8F%91-ssl-zheng-shu-zi-qian-fa/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/ssl%E8%AF%81%E4%B9%A6%E8%87%AA%E7%AD%BE%E5%8F%91-ssl-zheng-shu-zi-qian-fa/</guid><description>&lt;p>一.相关介绍
1.生成步骤&lt;/p>
&lt;h5 id="1生成私钥private-key使用-openssl-工具生成一个私钥文件用于加密和解密传输的数据">
 （1）生成私钥（Private Key）：使用 OpenSSL 工具生成一个私钥文件，用于加密和解密传输的数据。
 &lt;a class="anchor" href="#1%e7%94%9f%e6%88%90%e7%a7%81%e9%92%a5private-key%e4%bd%bf%e7%94%a8-openssl-%e5%b7%a5%e5%85%b7%e7%94%9f%e6%88%90%e4%b8%80%e4%b8%aa%e7%a7%81%e9%92%a5%e6%96%87%e4%bb%b6%e7%94%a8%e4%ba%8e%e5%8a%a0%e5%af%86%e5%92%8c%e8%a7%a3%e5%af%86%e4%bc%a0%e8%be%93%e7%9a%84%e6%95%b0%e6%8d%ae">#&lt;/a>
&lt;/h5>
&lt;h5 id="2生成证书签名请求certificate-signing-requestcsr使用-openssl-工具生成一个-csr-文件其中包含你的服务器公钥和相关的信息以便用于生成证书">
 （2）生成证书签名请求（Certificate Signing Request，CSR）：使用 OpenSSL 工具生成一个 CSR 文件，其中包含你的服务器公钥和相关的信息，以便用于生成证书。
 &lt;a class="anchor" href="#2%e7%94%9f%e6%88%90%e8%af%81%e4%b9%a6%e7%ad%be%e5%90%8d%e8%af%b7%e6%b1%82certificate-signing-requestcsr%e4%bd%bf%e7%94%a8-openssl-%e5%b7%a5%e5%85%b7%e7%94%9f%e6%88%90%e4%b8%80%e4%b8%aa-csr-%e6%96%87%e4%bb%b6%e5%85%b6%e4%b8%ad%e5%8c%85%e5%90%ab%e4%bd%a0%e7%9a%84%e6%9c%8d%e5%8a%a1%e5%99%a8%e5%85%ac%e9%92%a5%e5%92%8c%e7%9b%b8%e5%85%b3%e7%9a%84%e4%bf%a1%e6%81%af%e4%bb%a5%e4%be%bf%e7%94%a8%e4%ba%8e%e7%94%9f%e6%88%90%e8%af%81%e4%b9%a6">#&lt;/a>
&lt;/h5>
&lt;h5 id="3自签名证书生成使用-openssl-工具根据-csr-文件和私钥生成自签名的-ssl-证书文件">
 （3）自签名证书生成：使用 OpenSSL 工具根据 CSR 文件和私钥生成自签名的 SSL 证书文件。
 &lt;a class="anchor" href="#3%e8%87%aa%e7%ad%be%e5%90%8d%e8%af%81%e4%b9%a6%e7%94%9f%e6%88%90%e4%bd%bf%e7%94%a8-openssl-%e5%b7%a5%e5%85%b7%e6%a0%b9%e6%8d%ae-csr-%e6%96%87%e4%bb%b6%e5%92%8c%e7%a7%81%e9%92%a5%e7%94%9f%e6%88%90%e8%87%aa%e7%ad%be%e5%90%8d%e7%9a%84-ssl-%e8%af%81%e4%b9%a6%e6%96%87%e4%bb%b6">#&lt;/a>
&lt;/h5>
&lt;h5 id="4nginx-配置修改在-nginx-配置文件中进行相应的修改包括指定-ssl-证书文件路径私钥文件路径以及其他相关的-ssl-配置项">
 （4）Nginx 配置修改：在 Nginx 配置文件中进行相应的修改，包括指定 SSL 证书文件路径、私钥文件路径以及其他相关的 SSL 配置项。
 &lt;a class="anchor" href="#4nginx-%e9%85%8d%e7%bd%ae%e4%bf%ae%e6%94%b9%e5%9c%a8-nginx-%e9%85%8d%e7%bd%ae%e6%96%87%e4%bb%b6%e4%b8%ad%e8%bf%9b%e8%a1%8c%e7%9b%b8%e5%ba%94%e7%9a%84%e4%bf%ae%e6%94%b9%e5%8c%85%e6%8b%ac%e6%8c%87%e5%ae%9a-ssl-%e8%af%81%e4%b9%a6%e6%96%87%e4%bb%b6%e8%b7%af%e5%be%84%e7%a7%81%e9%92%a5%e6%96%87%e4%bb%b6%e8%b7%af%e5%be%84%e4%bb%a5%e5%8f%8a%e5%85%b6%e4%bb%96%e7%9b%b8%e5%85%b3%e7%9a%84-ssl-%e9%85%8d%e7%bd%ae%e9%a1%b9">#&lt;/a>
&lt;/h5>
&lt;h5 id="总nginx-就可以使用自签名-ssl-证书来启用-https实现加密和安全的通信需要注意的是自签名-ssl-证书不会受到公信任的证书颁发机构certificate-authority认可因此浏览器会显示安全警告在生产环境中建议使用由受信任的证书颁发机构签发的证书来获得更高的安全性和可信度">
 总：Nginx 就可以使用自签名 SSL 证书来启用 HTTPS，实现加密和安全的通信。需要注意的是，自签名 SSL 证书不会受到公信任的证书颁发机构（Certificate Authority）认可，因此浏览器会显示安全警告。在生产环境中，建议使用由受信任的证书颁发机构签发的证书来获得更高的安全性和可信度。
 &lt;a class="anchor" href="#%e6%80%bbnginx-%e5%b0%b1%e5%8f%af%e4%bb%a5%e4%bd%bf%e7%94%a8%e8%87%aa%e7%ad%be%e5%90%8d-ssl-%e8%af%81%e4%b9%a6%e6%9d%a5%e5%90%af%e7%94%a8-https%e5%ae%9e%e7%8e%b0%e5%8a%a0%e5%af%86%e5%92%8c%e5%ae%89%e5%85%a8%e7%9a%84%e9%80%9a%e4%bf%a1%e9%9c%80%e8%a6%81%e6%b3%a8%e6%84%8f%e7%9a%84%e6%98%af%e8%87%aa%e7%ad%be%e5%90%8d-ssl-%e8%af%81%e4%b9%a6%e4%b8%8d%e4%bc%9a%e5%8f%97%e5%88%b0%e5%85%ac%e4%bf%a1%e4%bb%bb%e7%9a%84%e8%af%81%e4%b9%a6%e9%a2%81%e5%8f%91%e6%9c%ba%e6%9e%84certificate-authority%e8%ae%a4%e5%8f%af%e5%9b%a0%e6%ad%a4%e6%b5%8f%e8%a7%88%e5%99%a8%e4%bc%9a%e6%98%be%e7%a4%ba%e5%ae%89%e5%85%a8%e8%ad%a6%e5%91%8a%e5%9c%a8%e7%94%9f%e4%ba%a7%e7%8e%af%e5%a2%83%e4%b8%ad%e5%bb%ba%e8%ae%ae%e4%bd%bf%e7%94%a8%e7%94%b1%e5%8f%97%e4%bf%a1%e4%bb%bb%e7%9a%84%e8%af%81%e4%b9%a6%e9%a2%81%e5%8f%91%e6%9c%ba%e6%9e%84%e7%ad%be%e5%8f%91%e7%9a%84%e8%af%81%e4%b9%a6%e6%9d%a5%e8%8e%b7%e5%be%97%e6%9b%b4%e9%ab%98%e7%9a%84%e5%ae%89%e5%85%a8%e6%80%a7%e5%92%8c%e5%8f%af%e4%bf%a1%e5%ba%a6">#&lt;/a>
&lt;/h5>
&lt;p>1
2.相关名词介绍&lt;/p>
&lt;h5 id="1私钥key生成使用-openssl-工具的-genrsa-命令生成私钥文件其中私钥是用于加密和解密数据的关键">
 （1）私钥（Key）生成：使用 OpenSSL 工具的 “genrsa” 命令生成私钥文件，其中私钥是用于加密和解密数据的关键。
 &lt;a class="anchor" href="#1%e7%a7%81%e9%92%a5key%e7%94%9f%e6%88%90%e4%bd%bf%e7%94%a8-openssl-%e5%b7%a5%e5%85%b7%e7%9a%84-genrsa-%e5%91%bd%e4%bb%a4%e7%94%9f%e6%88%90%e7%a7%81%e9%92%a5%e6%96%87%e4%bb%b6%e5%85%b6%e4%b8%ad%e7%a7%81%e9%92%a5%e6%98%af%e7%94%a8%e4%ba%8e%e5%8a%a0%e5%af%86%e5%92%8c%e8%a7%a3%e5%af%86%e6%95%b0%e6%8d%ae%e7%9a%84%e5%85%b3%e9%94%ae">#&lt;/a>
&lt;/h5>
&lt;h5 id="2公钥csr生成使用私钥文件生成证书签名请求certificate-signing-requestcsr其中包含公钥和其他相关信息这个公钥将被用于生成证书并在浏览器连接时进行身份验证">
 （2）公钥（CSR）生成：使用私钥文件生成证书签名请求（Certificate Signing Request，CSR），其中包含公钥和其他相关信息。这个公钥将被用于生成证书，并在浏览器连接时进行身份验证。
 &lt;a class="anchor" href="#2%e5%85%ac%e9%92%a5csr%e7%94%9f%e6%88%90%e4%bd%bf%e7%94%a8%e7%a7%81%e9%92%a5%e6%96%87%e4%bb%b6%e7%94%9f%e6%88%90%e8%af%81%e4%b9%a6%e7%ad%be%e5%90%8d%e8%af%b7%e6%b1%82certificate-signing-requestcsr%e5%85%b6%e4%b8%ad%e5%8c%85%e5%90%ab%e5%85%ac%e9%92%a5%e5%92%8c%e5%85%b6%e4%bb%96%e7%9b%b8%e5%85%b3%e4%bf%a1%e6%81%af%e8%bf%99%e4%b8%aa%e5%85%ac%e9%92%a5%e5%b0%86%e8%a2%ab%e7%94%a8%e4%ba%8e%e7%94%9f%e6%88%90%e8%af%81%e4%b9%a6%e5%b9%b6%e5%9c%a8%e6%b5%8f%e8%a7%88%e5%99%a8%e8%bf%9e%e6%8e%a5%e6%97%b6%e8%bf%9b%e8%a1%8c%e8%ba%ab%e4%bb%bd%e9%aa%8c%e8%af%81">#&lt;/a>
&lt;/h5>
&lt;h5 id="3证书crt生成证书由公钥csr和签名组成签名可以是自签名的也可以是由受信任的证书颁发机构ca签名的通过使用私钥key与公钥csr进行签名最终生成证书crt文件">
 （3）证书（CRT）生成：证书由公钥（CSR）和签名组成。签名可以是自签名的，也可以是由受信任的证书颁发机构（CA）签名的。通过使用私钥（Key）与公钥（CSR）进行签名，最终生成证书（CRT）文件。
 &lt;a class="anchor" href="#3%e8%af%81%e4%b9%a6crt%e7%94%9f%e6%88%90%e8%af%81%e4%b9%a6%e7%94%b1%e5%85%ac%e9%92%a5csr%e5%92%8c%e7%ad%be%e5%90%8d%e7%bb%84%e6%88%90%e7%ad%be%e5%90%8d%e5%8f%af%e4%bb%a5%e6%98%af%e8%87%aa%e7%ad%be%e5%90%8d%e7%9a%84%e4%b9%9f%e5%8f%af%e4%bb%a5%e6%98%af%e7%94%b1%e5%8f%97%e4%bf%a1%e4%bb%bb%e7%9a%84%e8%af%81%e4%b9%a6%e9%a2%81%e5%8f%91%e6%9c%ba%e6%9e%84ca%e7%ad%be%e5%90%8d%e7%9a%84%e9%80%9a%e8%bf%87%e4%bd%bf%e7%94%a8%e7%a7%81%e9%92%a5key%e4%b8%8e%e5%85%ac%e9%92%a5csr%e8%bf%9b%e8%a1%8c%e7%ad%be%e5%90%8d%e6%9c%80%e7%bb%88%e7%94%9f%e6%88%90%e8%af%81%e4%b9%a6crt%e6%96%87%e4%bb%b6">#&lt;/a>
&lt;/h5>
&lt;h5 id="4服务器证书servercrt生成的证书文件就是服务器证书通常命名为-servercrt">
 （4）服务器证书（server.crt）：生成的证书文件就是服务器证书，通常命名为 “server.crt”。
 &lt;a class="anchor" href="#4%e6%9c%8d%e5%8a%a1%e5%99%a8%e8%af%81%e4%b9%a6servercrt%e7%94%9f%e6%88%90%e7%9a%84%e8%af%81%e4%b9%a6%e6%96%87%e4%bb%b6%e5%b0%b1%e6%98%af%e6%9c%8d%e5%8a%a1%e5%99%a8%e8%af%81%e4%b9%a6%e9%80%9a%e5%b8%b8%e5%91%bd%e5%90%8d%e4%b8%ba-servercrt">#&lt;/a>
&lt;/h5>
&lt;h5 id="5签名过程签名是使用私钥key与公钥csr进行证书生成的过程私钥用于对公钥进行签名以确保证书的完整性和身份验证">
 （5）签名过程：签名是使用私钥（Key）与公钥（CSR）进行证书生成的过程。私钥用于对公钥进行签名，以确保证书的完整性和身份验证。
 &lt;a class="anchor" href="#5%e7%ad%be%e5%90%8d%e8%bf%87%e7%a8%8b%e7%ad%be%e5%90%8d%e6%98%af%e4%bd%bf%e7%94%a8%e7%a7%81%e9%92%a5key%e4%b8%8e%e5%85%ac%e9%92%a5csr%e8%bf%9b%e8%a1%8c%e8%af%81%e4%b9%a6%e7%94%9f%e6%88%90%e7%9a%84%e8%bf%87%e7%a8%8b%e7%a7%81%e9%92%a5%e7%94%a8%e4%ba%8e%e5%af%b9%e5%85%ac%e9%92%a5%e8%bf%9b%e8%a1%8c%e7%ad%be%e5%90%8d%e4%bb%a5%e7%a1%ae%e4%bf%9d%e8%af%81%e4%b9%a6%e7%9a%84%e5%ae%8c%e6%95%b4%e6%80%a7%e5%92%8c%e8%ba%ab%e4%bb%bd%e9%aa%8c%e8%af%81">#&lt;/a>
&lt;/h5>
&lt;h5 id="二nginx中实现自签名ssl证书生成与配置">
 二.Nginx中实现自签名SSL证书生成与配置
 &lt;a class="anchor" href="#%e4%ba%8cnginx%e4%b8%ad%e5%ae%9e%e7%8e%b0%e8%87%aa%e7%ad%be%e5%90%8dssl%e8%af%81%e4%b9%a6%e7%94%9f%e6%88%90%e4%b8%8e%e9%85%8d%e7%bd%ae">#&lt;/a>
&lt;/h5>
&lt;h5 id="1私钥生成">
 1.私钥生成
 &lt;a class="anchor" href="#1%e7%a7%81%e9%92%a5%e7%94%9f%e6%88%90">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>#关闭防火墙及安全机制
systemctl stop firewalld.service 
setenforce 0
&lt;/code>&lt;/pre>&lt;h5 id="在root用户的家目录下执行">
 #在root用户的家目录下执行
 &lt;a class="anchor" href="#%e5%9c%a8root%e7%94%a8%e6%88%b7%e7%9a%84%e5%ae%b6%e7%9b%ae%e5%bd%95%e4%b8%8b%e6%89%a7%e8%a1%8c">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>cd ~
#使用ssl生成私钥名为 server.key
openssl genrsa -des3 -out server.key 1024

openssl genrsa -des3 -out server.key 2048
#回车，输入自定义的密码文本，此处设置为12345
#输入两次
&lt;/code>&lt;/pre>&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403081005897.png" alt="image-20240308100510817" />&lt;/p></description></item><item><title>2024-04-03 两张图全面理解K8S原理</title><link>https://qq547475331.github.io/docs/%E4%B8%A4%E5%BC%A0%E5%9B%BE%E5%85%A8%E9%9D%A2%E7%90%86%E8%A7%A3k8s%E5%8E%9F%E7%90%86-liang-zhang-tu-quan-mian-li-jie-k8s-yuan-li/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/%E4%B8%A4%E5%BC%A0%E5%9B%BE%E5%85%A8%E9%9D%A2%E7%90%86%E8%A7%A3k8s%E5%8E%9F%E7%90%86-liang-zhang-tu-quan-mian-li-jie-k8s-yuan-li/</guid><description>&lt;h1 id="kubernetes-简介">
 Kubernetes 简介
 &lt;a class="anchor" href="#kubernetes-%e7%ae%80%e4%bb%8b">#&lt;/a>
&lt;/h1>
&lt;p>&lt;strong>Kubernetes&lt;/strong> 源于希腊语，意为“舵手”。k8s 缩写是因为 k 和 s 之间有八个字符的原因。它是 Google 在 2015 开源的容器调度编排的平台。&lt;/p>
&lt;p>&lt;strong>Kubernetes&lt;/strong> 作为一款优秀的容器编排工具，拥有非常精妙的架构设计。&lt;/p>
&lt;h1 id="kubernetes-架构">
 Kubernetes 架构
 &lt;a class="anchor" href="#kubernetes-%e6%9e%b6%e6%9e%84">#&lt;/a>
&lt;/h1>
&lt;h5 id="kubernetes-是一个-master--worker-的架构master-可理解为-kubernetes-的控制面worker-理解为-kubernetes-的数据面">
 &lt;strong>Kubernetes&lt;/strong> 是一个 &lt;strong>Master + Worker&lt;/strong> 的架构，&lt;strong>Master&lt;/strong> 可理解为 Kubernetes 的控制面，&lt;strong>Worker&lt;/strong> 理解为 Kubernetes 的数据面。
 &lt;a class="anchor" href="#kubernetes-%e6%98%af%e4%b8%80%e4%b8%aa-master--worker-%e7%9a%84%e6%9e%b6%e6%9e%84master-%e5%8f%af%e7%90%86%e8%a7%a3%e4%b8%ba-kubernetes-%e7%9a%84%e6%8e%a7%e5%88%b6%e9%9d%a2worker-%e7%90%86%e8%a7%a3%e4%b8%ba-kubernetes-%e7%9a%84%e6%95%b0%e6%8d%ae%e9%9d%a2">#&lt;/a>
&lt;/h5>
&lt;h5 id="master-节点一般只运行-kubernetes-控制组件是整个集群的大脑一般不运行业务容器">
 Master 节点一般只运行 Kubernetes 控制组件，是整个集群的大脑，一般不运行业务容器。
 &lt;a class="anchor" href="#master-%e8%8a%82%e7%82%b9%e4%b8%80%e8%88%ac%e5%8f%aa%e8%bf%90%e8%a1%8c-kubernetes-%e6%8e%a7%e5%88%b6%e7%bb%84%e4%bb%b6%e6%98%af%e6%95%b4%e4%b8%aa%e9%9b%86%e7%be%a4%e7%9a%84%e5%a4%a7%e8%84%91%e4%b8%80%e8%88%ac%e4%b8%8d%e8%bf%90%e8%a1%8c%e4%b8%9a%e5%8a%a1%e5%ae%b9%e5%99%a8">#&lt;/a>
&lt;/h5>
&lt;h5 id="worker-节点是运行业务容器的节点">
 Worker 节点是运行业务容器的节点。
 &lt;a class="anchor" href="#worker-%e8%8a%82%e7%82%b9%e6%98%af%e8%bf%90%e8%a1%8c%e4%b8%9a%e5%8a%a1%e5%ae%b9%e5%99%a8%e7%9a%84%e8%8a%82%e7%82%b9">#&lt;/a>
&lt;/h5>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403191453702.png" alt="image-20240319145332645" />&lt;/p>
&lt;h3 id="master">
 Master
 &lt;a class="anchor" href="#master">#&lt;/a>
&lt;/h3>
&lt;h5 id="kubernetes-master-节点需要运行以下组件">
 Kubernetes &lt;strong>Master&lt;/strong> 节点需要运行以下组件：
 &lt;a class="anchor" href="#kubernetes-master-%e8%8a%82%e7%82%b9%e9%9c%80%e8%a6%81%e8%bf%90%e8%a1%8c%e4%bb%a5%e4%b8%8b%e7%bb%84%e4%bb%b6">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="-kube-apiserverkube-apiserver-是-kubernetes-的-api-入口是集群流量入口">
 • **Kube-apiserver：**Kube-apiserver 是 Kubernetes 的 API 入口，是集群流量入口；
 &lt;a class="anchor" href="#-kube-apiserverkube-apiserver-%e6%98%af-kubernetes-%e7%9a%84-api-%e5%85%a5%e5%8f%a3%e6%98%af%e9%9b%86%e7%be%a4%e6%b5%81%e9%87%8f%e5%85%a5%e5%8f%a3">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-kube-controller-managerkube-controller-manager-包含很多个-controller用于调谐集群中的资源">
 • **Kube-controller-manager：**Kube-controller-manager 包含很多个 &lt;strong>Controller&lt;/strong>，用于调谐集群中的资源；
 &lt;a class="anchor" href="#-kube-controller-managerkube-controller-manager-%e5%8c%85%e5%90%ab%e5%be%88%e5%a4%9a%e4%b8%aa-controller%e7%94%a8%e4%ba%8e%e8%b0%83%e8%b0%90%e9%9b%86%e7%be%a4%e4%b8%ad%e7%9a%84%e8%b5%84%e6%ba%90">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-kube-schedulerkube-scheduler-是集群中默认调度器给-pod-选择最优节点调度">
 • **Kube-scheduler：**Kube-scheduler 是集群中默认调度器，给 Pod 选择最优节点调度；
 &lt;a class="anchor" href="#-kube-schedulerkube-scheduler-%e6%98%af%e9%9b%86%e7%be%a4%e4%b8%ad%e9%bb%98%e8%ae%a4%e8%b0%83%e5%ba%a6%e5%99%a8%e7%bb%99-pod-%e9%80%89%e6%8b%a9%e6%9c%80%e4%bc%98%e8%8a%82%e7%82%b9%e8%b0%83%e5%ba%a6">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-etcdkuebernetes-的后端存储集群中所有可持久化数据都存储在-etcd-中">
 • **Etcd：**Kuebernetes 的后端存储，集群中所有可持久化数据都存储在 Etcd 中。
 &lt;a class="anchor" href="#-etcdkuebernetes-%e7%9a%84%e5%90%8e%e7%ab%af%e5%ad%98%e5%82%a8%e9%9b%86%e7%be%a4%e4%b8%ad%e6%89%80%e6%9c%89%e5%8f%af%e6%8c%81%e4%b9%85%e5%8c%96%e6%95%b0%e6%8d%ae%e9%83%bd%e5%ad%98%e5%82%a8%e5%9c%a8-etcd-%e4%b8%ad">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;h5 id="kubernetes-中采用声明式-api即-api-不需要关心具体功能实现只需关心最终状态一个-api-对应一个-controller具体功能由-controller-实现同时调谐至预期状态">
 Kubernetes 中采用&lt;strong>声明式 API&lt;/strong>，即 API 不需要关心具体功能实现，只需关心最终状态。一个 API 对应一个 Controller，具体功能由 Controller 实现同时调谐至预期状态。
 &lt;a class="anchor" href="#kubernetes-%e4%b8%ad%e9%87%87%e7%94%a8%e5%a3%b0%e6%98%8e%e5%bc%8f-api%e5%8d%b3-api-%e4%b8%8d%e9%9c%80%e8%a6%81%e5%85%b3%e5%bf%83%e5%85%b7%e4%bd%93%e5%8a%9f%e8%83%bd%e5%ae%9e%e7%8e%b0%e5%8f%aa%e9%9c%80%e5%85%b3%e5%bf%83%e6%9c%80%e7%bb%88%e7%8a%b6%e6%80%81%e4%b8%80%e4%b8%aa-api-%e5%af%b9%e5%ba%94%e4%b8%80%e4%b8%aa-controller%e5%85%b7%e4%bd%93%e5%8a%9f%e8%83%bd%e7%94%b1-controller-%e5%ae%9e%e7%8e%b0%e5%90%8c%e6%97%b6%e8%b0%83%e8%b0%90%e8%87%b3%e9%a2%84%e6%9c%9f%e7%8a%b6%e6%80%81">#&lt;/a>
&lt;/h5>&lt;/blockquote>
&lt;h5 id="master-节点的高可用一般取决于-etcdetcd-高可用推荐三节点或者五节点所以-master-节点通常为三个或者五个如果接入外部-etcd-集群那么-master-节点可以是偶数个">
 Master 节点的&lt;strong>高可用&lt;/strong>一般取决于 Etcd，Etcd 高可用推荐&lt;strong>三节点或者五节点&lt;/strong>，所以 Master 节点通常为三个或者五个，如果接入&lt;strong>外部 Etcd 集群&lt;/strong>，那么 Master 节点可以是偶数个。
 &lt;a class="anchor" href="#master-%e8%8a%82%e7%82%b9%e7%9a%84%e9%ab%98%e5%8f%af%e7%94%a8%e4%b8%80%e8%88%ac%e5%8f%96%e5%86%b3%e4%ba%8e-etcdetcd-%e9%ab%98%e5%8f%af%e7%94%a8%e6%8e%a8%e8%8d%90%e4%b8%89%e8%8a%82%e7%82%b9%e6%88%96%e8%80%85%e4%ba%94%e8%8a%82%e7%82%b9%e6%89%80%e4%bb%a5-master-%e8%8a%82%e7%82%b9%e9%80%9a%e5%b8%b8%e4%b8%ba%e4%b8%89%e4%b8%aa%e6%88%96%e8%80%85%e4%ba%94%e4%b8%aa%e5%a6%82%e6%9e%9c%e6%8e%a5%e5%85%a5%e5%a4%96%e9%83%a8-etcd-%e9%9b%86%e7%be%a4%e9%82%a3%e4%b9%88-master-%e8%8a%82%e7%82%b9%e5%8f%af%e4%bb%a5%e6%98%af%e5%81%b6%e6%95%b0%e4%b8%aa">#&lt;/a>
&lt;/h5>
&lt;h5 id="上图-kubernetes-使用内部-etcd即-etcd-与-master-节点个数一致部署在-kubernetes-集群中">
 上图 Kubernetes 使用内部 Etcd，即 Etcd 与 Master 节点个数一致，部署在 Kubernetes 集群中。
 &lt;a class="anchor" href="#%e4%b8%8a%e5%9b%be-kubernetes-%e4%bd%bf%e7%94%a8%e5%86%85%e9%83%a8-etcd%e5%8d%b3-etcd-%e4%b8%8e-master-%e8%8a%82%e7%82%b9%e4%b8%aa%e6%95%b0%e4%b8%80%e8%87%b4%e9%83%a8%e7%bd%b2%e5%9c%a8-kubernetes-%e9%9b%86%e7%be%a4%e4%b8%ad">#&lt;/a>
&lt;/h5>
&lt;blockquote>
&lt;h5 id="etcd-集群一般要求三个五个类似奇数个实例etcd-集群选举机制要求集群中半数以上的实例投票选举如果集群是两个实例那么一个实例宕机剩下一个实例没有办法选举同样四个实例和三个实例实际上效果是一样的">
 Etcd 集群一般要求三个、五个类似奇数个实例，Etcd 集群选举机制要求集群中&lt;strong>半数以上&lt;/strong>的实例投票选举，如果集群是两个实例，那么一个实例宕机，剩下一个实例没有办法选举。同样四个实例和三个实例实际上效果是一样的。
 &lt;a class="anchor" href="#etcd-%e9%9b%86%e7%be%a4%e4%b8%80%e8%88%ac%e8%a6%81%e6%b1%82%e4%b8%89%e4%b8%aa%e4%ba%94%e4%b8%aa%e7%b1%bb%e4%bc%bc%e5%a5%87%e6%95%b0%e4%b8%aa%e5%ae%9e%e4%be%8betcd-%e9%9b%86%e7%be%a4%e9%80%89%e4%b8%be%e6%9c%ba%e5%88%b6%e8%a6%81%e6%b1%82%e9%9b%86%e7%be%a4%e4%b8%ad%e5%8d%8a%e6%95%b0%e4%bb%a5%e4%b8%8a%e7%9a%84%e5%ae%9e%e4%be%8b%e6%8a%95%e7%a5%a8%e9%80%89%e4%b8%be%e5%a6%82%e6%9e%9c%e9%9b%86%e7%be%a4%e6%98%af%e4%b8%a4%e4%b8%aa%e5%ae%9e%e4%be%8b%e9%82%a3%e4%b9%88%e4%b8%80%e4%b8%aa%e5%ae%9e%e4%be%8b%e5%ae%95%e6%9c%ba%e5%89%a9%e4%b8%8b%e4%b8%80%e4%b8%aa%e5%ae%9e%e4%be%8b%e6%b2%a1%e6%9c%89%e5%8a%9e%e6%b3%95%e9%80%89%e4%b8%be%e5%90%8c%e6%a0%b7%e5%9b%9b%e4%b8%aa%e5%ae%9e%e4%be%8b%e5%92%8c%e4%b8%89%e4%b8%aa%e5%ae%9e%e4%be%8b%e5%ae%9e%e9%99%85%e4%b8%8a%e6%95%88%e6%9e%9c%e6%98%af%e4%b8%80%e6%a0%b7%e7%9a%84">#&lt;/a>
&lt;/h5>&lt;/blockquote>
&lt;h3 id="worker">
 Worker
 &lt;a class="anchor" href="#worker">#&lt;/a>
&lt;/h3>
&lt;h5 id="kubernetes-worker-节点作为容器运行节点需要部署以下组件">
 Kubernetes &lt;strong>Worker&lt;/strong> 节点作为容器运行节点，需要部署以下组件：
 &lt;a class="anchor" href="#kubernetes-worker-%e8%8a%82%e7%82%b9%e4%bd%9c%e4%b8%ba%e5%ae%b9%e5%99%a8%e8%bf%90%e8%a1%8c%e8%8a%82%e7%82%b9%e9%9c%80%e8%a6%81%e9%83%a8%e7%bd%b2%e4%bb%a5%e4%b8%8b%e7%bb%84%e4%bb%b6">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="-cri容器运行时管理容器生命周期">
 • **CRI：**容器运行时，管理容器生命周期；
 &lt;a class="anchor" href="#-cri%e5%ae%b9%e5%99%a8%e8%bf%90%e8%a1%8c%e6%97%b6%e7%ae%a1%e7%90%86%e5%ae%b9%e5%99%a8%e7%94%9f%e5%91%bd%e5%91%a8%e6%9c%9f">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-kubelet管理-pod-生命周期pod-是-kubernetes-中最小调度单元">
 • **Kubelet：**管理 &lt;strong>Pod&lt;/strong> 生命周期，Pod 是 Kubernetes 中最小调度单元；
 &lt;a class="anchor" href="#-kubelet%e7%ae%a1%e7%90%86-pod-%e7%94%9f%e5%91%bd%e5%91%a8%e6%9c%9fpod-%e6%98%af-kubernetes-%e4%b8%ad%e6%9c%80%e5%b0%8f%e8%b0%83%e5%ba%a6%e5%8d%95%e5%85%83">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-cni容器网络接口实现-kubernetes-中-pod-间网络联通">
 • **CNI：**容器网络接口，实现 Kubernetes 中 Pod 间网络联通；
 &lt;a class="anchor" href="#-cni%e5%ae%b9%e5%99%a8%e7%bd%91%e7%bb%9c%e6%8e%a5%e5%8f%a3%e5%ae%9e%e7%8e%b0-kubernetes-%e4%b8%ad-pod-%e9%97%b4%e7%bd%91%e7%bb%9c%e8%81%94%e9%80%9a">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-csi容器存储接口屏蔽底层存储实现方便用户使用第三方存储">
 • **CSI：**容器存储接口，屏蔽底层存储实现，方便用户使用第三方存储；
 &lt;a class="anchor" href="#-csi%e5%ae%b9%e5%99%a8%e5%ad%98%e5%82%a8%e6%8e%a5%e5%8f%a3%e5%b1%8f%e8%94%bd%e5%ba%95%e5%b1%82%e5%ad%98%e5%82%a8%e5%ae%9e%e7%8e%b0%e6%96%b9%e4%be%bf%e7%94%a8%e6%88%b7%e4%bd%bf%e7%94%a8%e7%ac%ac%e4%b8%89%e6%96%b9%e5%ad%98%e5%82%a8">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-kube-proxy该组件主要实现多组-pod-的负载均衡">
 • **Kube-proxy：**该组件主要实现多组 Pod 的负载均衡；
 &lt;a class="anchor" href="#-kube-proxy%e8%af%a5%e7%bb%84%e4%bb%b6%e4%b8%bb%e8%a6%81%e5%ae%9e%e7%8e%b0%e5%a4%9a%e7%bb%84-pod-%e7%9a%84%e8%b4%9f%e8%bd%bd%e5%9d%87%e8%a1%a1">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;h5 id="为什么-kubernetes-需要在容器上之上抽象一个-pod-资源呢大部分情况是一个-pod-对应一个容器有的场景就需要一个-pod-对应多个容器例如日志收集场景每个-pod-都会包含一个业务容器和一个日志收集容器将这两个容器放在一个-pod-里可用共享日志-volume">
 为什么 Kubernetes 需要在容器上之上抽象一个 Pod 资源呢？大部分情况是一个 Pod 对应一个容器，有的场景就需要一个 Pod 对应多个容器，例如日志收集场景，每个 Pod 都会包含一个业务容器和一个日志收集容器，将这两个容器放在一个 Pod 里可用共享日志 Volume。
 &lt;a class="anchor" href="#%e4%b8%ba%e4%bb%80%e4%b9%88-kubernetes-%e9%9c%80%e8%a6%81%e5%9c%a8%e5%ae%b9%e5%99%a8%e4%b8%8a%e4%b9%8b%e4%b8%8a%e6%8a%bd%e8%b1%a1%e4%b8%80%e4%b8%aa-pod-%e8%b5%84%e6%ba%90%e5%91%a2%e5%a4%a7%e9%83%a8%e5%88%86%e6%83%85%e5%86%b5%e6%98%af%e4%b8%80%e4%b8%aa-pod-%e5%af%b9%e5%ba%94%e4%b8%80%e4%b8%aa%e5%ae%b9%e5%99%a8%e6%9c%89%e7%9a%84%e5%9c%ba%e6%99%af%e5%b0%b1%e9%9c%80%e8%a6%81%e4%b8%80%e4%b8%aa-pod-%e5%af%b9%e5%ba%94%e5%a4%9a%e4%b8%aa%e5%ae%b9%e5%99%a8%e4%be%8b%e5%a6%82%e6%97%a5%e5%bf%97%e6%94%b6%e9%9b%86%e5%9c%ba%e6%99%af%e6%af%8f%e4%b8%aa-pod-%e9%83%bd%e4%bc%9a%e5%8c%85%e5%90%ab%e4%b8%80%e4%b8%aa%e4%b8%9a%e5%8a%a1%e5%ae%b9%e5%99%a8%e5%92%8c%e4%b8%80%e4%b8%aa%e6%97%a5%e5%bf%97%e6%94%b6%e9%9b%86%e5%ae%b9%e5%99%a8%e5%b0%86%e8%bf%99%e4%b8%a4%e4%b8%aa%e5%ae%b9%e5%99%a8%e6%94%be%e5%9c%a8%e4%b8%80%e4%b8%aa-pod-%e9%87%8c%e5%8f%af%e7%94%a8%e5%85%b1%e4%ba%ab%e6%97%a5%e5%bf%97-volume">#&lt;/a>
&lt;/h5>&lt;/blockquote>
&lt;h5 id="worker-节点的-kubelet-需要注册到集群中就需要每个-worker-节点的-kubelet-能够连接-master-节点的-kube-apiserver如果集群中-master-采用高可用部署就会存在多个-master那么-worker-节点的-kubelet-就需要同时连接所有的-kube-apiserver-保证高可用实现这种高可用的方式有很多种例如-haproxy--keepalived-nginxenvoy-等上图就是-lb-组件就代表这些实现负载-kube-apsierver-的组件">
 Worker 节点的 Kubelet 需要&lt;strong>注册&lt;/strong>到集群中，就需要每个 Worker 节点的 Kubelet 能够连接 Master 节点的 Kube-apiserver。如果集群中 Master 采用高可用部署，就会存在多个 Master，那么 Worker 节点的 Kubelet 就需要同时连接&lt;strong>所有的 Kube-apiserver&lt;/strong> 保证高可用。实现这种高可用的方式有很多种，例如 &lt;code>Haproxy + Keepalived 、Nginx、Envoy&lt;/code> 等。上图就是 &lt;strong>LB 组件&lt;/strong>就代表这些实现负载 Kube-apsierver 的组件。
 &lt;a class="anchor" href="#worker-%e8%8a%82%e7%82%b9%e7%9a%84-kubelet-%e9%9c%80%e8%a6%81%e6%b3%a8%e5%86%8c%e5%88%b0%e9%9b%86%e7%be%a4%e4%b8%ad%e5%b0%b1%e9%9c%80%e8%a6%81%e6%af%8f%e4%b8%aa-worker-%e8%8a%82%e7%82%b9%e7%9a%84-kubelet-%e8%83%bd%e5%a4%9f%e8%bf%9e%e6%8e%a5-master-%e8%8a%82%e7%82%b9%e7%9a%84-kube-apiserver%e5%a6%82%e6%9e%9c%e9%9b%86%e7%be%a4%e4%b8%ad-master-%e9%87%87%e7%94%a8%e9%ab%98%e5%8f%af%e7%94%a8%e9%83%a8%e7%bd%b2%e5%b0%b1%e4%bc%9a%e5%ad%98%e5%9c%a8%e5%a4%9a%e4%b8%aa-master%e9%82%a3%e4%b9%88-worker-%e8%8a%82%e7%82%b9%e7%9a%84-kubelet-%e5%b0%b1%e9%9c%80%e8%a6%81%e5%90%8c%e6%97%b6%e8%bf%9e%e6%8e%a5%e6%89%80%e6%9c%89%e7%9a%84-kube-apiserver-%e4%bf%9d%e8%af%81%e9%ab%98%e5%8f%af%e7%94%a8%e5%ae%9e%e7%8e%b0%e8%bf%99%e7%a7%8d%e9%ab%98%e5%8f%af%e7%94%a8%e7%9a%84%e6%96%b9%e5%bc%8f%e6%9c%89%e5%be%88%e5%a4%9a%e7%a7%8d%e4%be%8b%e5%a6%82-haproxy--keepalived-nginxenvoy-%e7%ad%89%e4%b8%8a%e5%9b%be%e5%b0%b1%e6%98%af-lb-%e7%bb%84%e4%bb%b6%e5%b0%b1%e4%bb%a3%e8%a1%a8%e8%bf%99%e4%ba%9b%e5%ae%9e%e7%8e%b0%e8%b4%9f%e8%bd%bd-kube-apsierver-%e7%9a%84%e7%bb%84%e4%bb%b6">#&lt;/a>
&lt;/h5>
&lt;h1 id="创建一个-pod-需要经历哪些流程">
 创建一个 Pod 需要经历哪些流程
 &lt;a class="anchor" href="#%e5%88%9b%e5%bb%ba%e4%b8%80%e4%b8%aa-pod-%e9%9c%80%e8%a6%81%e7%bb%8f%e5%8e%86%e5%93%aa%e4%ba%9b%e6%b5%81%e7%a8%8b">#&lt;/a>
&lt;/h1>
&lt;h5 id="当用户创建一个-deployment-的时候kubernetes-中各组件的工作流程是如何的">
 当用户创建一个 &lt;strong>Deployment&lt;/strong> 的时候，Kubernetes 中各组件的&lt;strong>工作流程&lt;/strong>是如何的？
 &lt;a class="anchor" href="#%e5%bd%93%e7%94%a8%e6%88%b7%e5%88%9b%e5%bb%ba%e4%b8%80%e4%b8%aa-deployment-%e7%9a%84%e6%97%b6%e5%80%99kubernetes-%e4%b8%ad%e5%90%84%e7%bb%84%e4%bb%b6%e7%9a%84%e5%b7%a5%e4%bd%9c%e6%b5%81%e7%a8%8b%e6%98%af%e5%a6%82%e4%bd%95%e7%9a%84">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="-用户通过-kubectl-创建一个-deployment请求会发给-kube-apiserver">
 • 用户通过 &lt;code>kubectl&lt;/code> 创建一个 &lt;strong>Deployment&lt;/strong>，请求会发给 &lt;strong>Kube-apiserver&lt;/strong>；
 &lt;a class="anchor" href="#-%e7%94%a8%e6%88%b7%e9%80%9a%e8%bf%87-kubectl-%e5%88%9b%e5%bb%ba%e4%b8%80%e4%b8%aa-deployment%e8%af%b7%e6%b1%82%e4%bc%9a%e5%8f%91%e7%bb%99-kube-apiserver">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-kube-apiserver-会将-deployment-的描述信息写入-etcd-中kube-apiserver-将请求结果返回给用户">
 • &lt;strong>Kube-apiserver&lt;/strong> 会将 &lt;strong>Deployment&lt;/strong> 的描述信息写入 &lt;strong>Etcd&lt;/strong> 中，&lt;strong>Kube-apiserver&lt;/strong> 将请求结果返回给用户；
 &lt;a class="anchor" href="#-kube-apiserver-%e4%bc%9a%e5%b0%86-deployment-%e7%9a%84%e6%8f%8f%e8%bf%b0%e4%bf%a1%e6%81%af%e5%86%99%e5%85%a5-etcd-%e4%b8%adkube-apiserver-%e5%b0%86%e8%af%b7%e6%b1%82%e7%bb%93%e6%9e%9c%e8%bf%94%e5%9b%9e%e7%bb%99%e7%94%a8%e6%88%b7">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;p>• &lt;strong>Kube-controller-manager&lt;/strong> 的 &lt;strong>Deployment Controller&lt;/strong> 从 &lt;strong>Kube-apiserver&lt;/strong> &lt;code>Watch&lt;/code> 到 &lt;strong>Deployment&lt;/strong> 的创建事件，并创建一个 &lt;strong>ReplicaSet&lt;/strong>；&lt;/p></description></item><item><title>2024-04-03 二进制部署K8S加节点操作</title><link>https://qq547475331.github.io/docs/%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2k8s%E5%8A%A0%E8%8A%82%E7%82%B9%E6%93%8D%E4%BD%9C-er-jin-zhi-bu-shu-k8s-jia-jie-dian-cao-zuo/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2k8s%E5%8A%A0%E8%8A%82%E7%82%B9%E6%93%8D%E4%BD%9C-er-jin-zhi-bu-shu-k8s-jia-jie-dian-cao-zuo/</guid><description>&lt;h4 id="安装docker">
 安装docker
 &lt;a class="anchor" href="#%e5%ae%89%e8%a3%85docker">#&lt;/a>
&lt;/h4>
&lt;pre tabindex="0">&lt;code>#拷贝另一台work节点的二进制文件 到/usr/bin/下
- name: copy docker to /usr/bin
 copy: src={{ item }} dest=/usr/bin/{{ item }} mode=755
 with_items:
 - docker
 - dockerd
 - docker-init
 - docker-proxy
 - containerd
 - containerd-shim
 - containerd-shim-runc-v2
 - ctr
 - runc

#创建docker目录 /etc/docker
- name: make config dir &amp;#34;/etc/docker&amp;#34;
 file: dest=/etc/docker mode=755 state=directory


#拷贝另一台work节点的daemon.json文件到/etc/docker/daemon.json
- name: copy config file docker to /etc/docker
 template: src=daemon.json dest=/etc/docker/daemon.json

#拷贝另一台work节点的docker.service文件到/usr/lib/systemd/system/docker.service
- name: copy docker.service to /usr/lib/systemd/system
 copy: src=docker.service dest=/usr/lib/systemd/system/docker.service

#拷贝另一台work节点的containerd.service文件到/usr/lib/systemd/system/containerd.service
- name: copy containerd.service to /usr/lib/systemd/system
 copy: src=containerd.service dest=/usr/lib/systemd/system/containerd.service

#拷贝另一台work节点的docker.socket文件到/usr/lib/systemd/system/docker.socket
- name: copy docker.socket to /usr/lib/systemd/system
 copy: src=docker.socket dest=/usr/lib/systemd/system/docker.socket



#启动并检查服务状态 systemctl daemon-reload
- name: systemctl daemon-reload
 command: systemctl daemon-reload

#重启docker systemctl restart docker
- name: systemctl daemon-reload
 command: systemctl restart docker


#检查docker状态 systemctl status docker
- name: check docker service started ok
 command: systemctl status docker
&lt;/code>&lt;/pre>&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202402281915931.png" alt="image-20240228191521865" />&lt;/p></description></item><item><title>2024-04-03 使用kubernees leases 轻松实现leader election</title><link>https://qq547475331.github.io/docs/%E4%BD%BF%E7%94%A8kubernees-leases-%E8%BD%BB%E6%9D%BE%E5%AE%9E%E7%8E%B0leader-election-shi-yong-kuberneesleases-qing-song-shi-xian-leaderelection/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/%E4%BD%BF%E7%94%A8kubernees-leases-%E8%BD%BB%E6%9D%BE%E5%AE%9E%E7%8E%B0leader-election-shi-yong-kuberneesleases-qing-song-shi-xian-leaderelection/</guid><description>&lt;h5 id="假如你正在创建一个数据复制服务该服务只需要一个实例来处理数据以保持正确的顺序或者考虑开发一个多人在线游戏在这种游戏中同步所有参与者之间的游戏状态至关重要这些类型的服务通常需要单个实例专门处理特定任务并且还需要备用副本在当前实例失败时能够迅速接管工作负载">
 假如你正在创建一个数据复制服务，该服务只需要一个实例来处理数据以保持正确的顺序。或者考虑开发一个多人在线游戏，在这种游戏中，同步所有参与者之间的游戏状态至关重要。这些类型的服务通常需要单个实例专门处理特定任务，并且还需要备用副本在当前实例失败时能够迅速接管工作负载。
 &lt;a class="anchor" href="#%e5%81%87%e5%a6%82%e4%bd%a0%e6%ad%a3%e5%9c%a8%e5%88%9b%e5%bb%ba%e4%b8%80%e4%b8%aa%e6%95%b0%e6%8d%ae%e5%a4%8d%e5%88%b6%e6%9c%8d%e5%8a%a1%e8%af%a5%e6%9c%8d%e5%8a%a1%e5%8f%aa%e9%9c%80%e8%a6%81%e4%b8%80%e4%b8%aa%e5%ae%9e%e4%be%8b%e6%9d%a5%e5%a4%84%e7%90%86%e6%95%b0%e6%8d%ae%e4%bb%a5%e4%bf%9d%e6%8c%81%e6%ad%a3%e7%a1%ae%e7%9a%84%e9%a1%ba%e5%ba%8f%e6%88%96%e8%80%85%e8%80%83%e8%99%91%e5%bc%80%e5%8f%91%e4%b8%80%e4%b8%aa%e5%a4%9a%e4%ba%ba%e5%9c%a8%e7%ba%bf%e6%b8%b8%e6%88%8f%e5%9c%a8%e8%bf%99%e7%a7%8d%e6%b8%b8%e6%88%8f%e4%b8%ad%e5%90%8c%e6%ad%a5%e6%89%80%e6%9c%89%e5%8f%82%e4%b8%8e%e8%80%85%e4%b9%8b%e9%97%b4%e7%9a%84%e6%b8%b8%e6%88%8f%e7%8a%b6%e6%80%81%e8%87%b3%e5%85%b3%e9%87%8d%e8%a6%81%e8%bf%99%e4%ba%9b%e7%b1%bb%e5%9e%8b%e7%9a%84%e6%9c%8d%e5%8a%a1%e9%80%9a%e5%b8%b8%e9%9c%80%e8%a6%81%e5%8d%95%e4%b8%aa%e5%ae%9e%e4%be%8b%e4%b8%93%e9%97%a8%e5%a4%84%e7%90%86%e7%89%b9%e5%ae%9a%e4%bb%bb%e5%8a%a1%e5%b9%b6%e4%b8%94%e8%bf%98%e9%9c%80%e8%a6%81%e5%a4%87%e7%94%a8%e5%89%af%e6%9c%ac%e5%9c%a8%e5%bd%93%e5%89%8d%e5%ae%9e%e4%be%8b%e5%a4%b1%e8%b4%a5%e6%97%b6%e8%83%bd%e5%a4%9f%e8%bf%85%e9%80%9f%e6%8e%a5%e7%ae%a1%e5%b7%a5%e4%bd%9c%e8%b4%9f%e8%bd%bd">#&lt;/a>
&lt;/h5>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202402261502546.png" alt="image-20240226150259482" />&lt;/p>
&lt;h5 id="解决此类挑战常用的技术之一是-leader-election它通过使组集群中的节点参与投票或共识过程来选择单个-leader-实例然后该leader-承担需要集中协调的责任如果选举失败则其余节点会自动重新运行选举过程以选择新的-leader确保系统连续性和容错性">
 解决此类挑战常用的技术之一是 &lt;code>leader election&lt;/code>。它通过使组/集群中的节点参与投票或共识过程来选择单个 Leader 实例。然后该Leader 承担需要集中协调的责任。如果选举失败，则其余节点会自动重新运行选举过程以选择新的 Leader，确保系统连续性和容错性。
 &lt;a class="anchor" href="#%e8%a7%a3%e5%86%b3%e6%ad%a4%e7%b1%bb%e6%8c%91%e6%88%98%e5%b8%b8%e7%94%a8%e7%9a%84%e6%8a%80%e6%9c%af%e4%b9%8b%e4%b8%80%e6%98%af-leader-election%e5%ae%83%e9%80%9a%e8%bf%87%e4%bd%bf%e7%bb%84%e9%9b%86%e7%be%a4%e4%b8%ad%e7%9a%84%e8%8a%82%e7%82%b9%e5%8f%82%e4%b8%8e%e6%8a%95%e7%a5%a8%e6%88%96%e5%85%b1%e8%af%86%e8%bf%87%e7%a8%8b%e6%9d%a5%e9%80%89%e6%8b%a9%e5%8d%95%e4%b8%aa-leader-%e5%ae%9e%e4%be%8b%e7%84%b6%e5%90%8e%e8%af%a5leader-%e6%89%bf%e6%8b%85%e9%9c%80%e8%a6%81%e9%9b%86%e4%b8%ad%e5%8d%8f%e8%b0%83%e7%9a%84%e8%b4%a3%e4%bb%bb%e5%a6%82%e6%9e%9c%e9%80%89%e4%b8%be%e5%a4%b1%e8%b4%a5%e5%88%99%e5%85%b6%e4%bd%99%e8%8a%82%e7%82%b9%e4%bc%9a%e8%87%aa%e5%8a%a8%e9%87%8d%e6%96%b0%e8%bf%90%e8%a1%8c%e9%80%89%e4%b8%be%e8%bf%87%e7%a8%8b%e4%bb%a5%e9%80%89%e6%8b%a9%e6%96%b0%e7%9a%84-leader%e7%a1%ae%e4%bf%9d%e7%b3%bb%e7%bb%9f%e8%bf%9e%e7%bb%ad%e6%80%a7%e5%92%8c%e5%ae%b9%e9%94%99%e6%80%a7">#&lt;/a>
&lt;/h5>
&lt;h2 id="实现的复杂性">
 &lt;strong>实现的复杂性&lt;/strong>
 &lt;a class="anchor" href="#%e5%ae%9e%e7%8e%b0%e7%9a%84%e5%a4%8d%e6%9d%82%e6%80%a7">#&lt;/a>
&lt;/h2>
&lt;h5 id="在一个组中达成所有节点对-leader-的共识对于实现-leader-选举至关重要">
 在一个组中达成所有节点对 Leader 的共识，对于实现 Leader 选举至关重要。
 &lt;a class="anchor" href="#%e5%9c%a8%e4%b8%80%e4%b8%aa%e7%bb%84%e4%b8%ad%e8%be%be%e6%88%90%e6%89%80%e6%9c%89%e8%8a%82%e7%82%b9%e5%af%b9-leader-%e7%9a%84%e5%85%b1%e8%af%86%e5%af%b9%e4%ba%8e%e5%ae%9e%e7%8e%b0-leader-%e9%80%89%e4%b8%be%e8%87%b3%e5%85%b3%e9%87%8d%e8%a6%81">#&lt;/a>
&lt;/h5>
&lt;h5 id="基本的实现可以利用锁服务来确定领导权">
 基本的实现可以利用&lt;strong>锁&lt;/strong>服务来确定领导权。
 &lt;a class="anchor" href="#%e5%9f%ba%e6%9c%ac%e7%9a%84%e5%ae%9e%e7%8e%b0%e5%8f%af%e4%bb%a5%e5%88%a9%e7%94%a8%e9%94%81%e6%9c%8d%e5%8a%a1%e6%9d%a5%e7%a1%ae%e5%ae%9a%e9%a2%86%e5%af%bc%e6%9d%83">#&lt;/a>
&lt;/h5>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202402261503413.png" alt="image-20240226150312356" />&lt;/p>
&lt;h5 id="在这种情况下每个实例都尝试获取一个共享锁锁服务确保只有一个实例在任何给定时间持有该锁有效地使该实例成为-leader其他副本不断尝试获取锁从而准备好进行无缝故障转移以防当前的-leader-变得不可用">
 在这种情况下，每个实例都尝试获取一个共享锁。锁服务确保只有一个实例在任何给定时间持有该锁，有效地使该实例成为 Leader。其他副本不断尝试获取锁，从而准备好进行无缝故障转移，以防当前的 Leader 变得不可用。
 &lt;a class="anchor" href="#%e5%9c%a8%e8%bf%99%e7%a7%8d%e6%83%85%e5%86%b5%e4%b8%8b%e6%af%8f%e4%b8%aa%e5%ae%9e%e4%be%8b%e9%83%bd%e5%b0%9d%e8%af%95%e8%8e%b7%e5%8f%96%e4%b8%80%e4%b8%aa%e5%85%b1%e4%ba%ab%e9%94%81%e9%94%81%e6%9c%8d%e5%8a%a1%e7%a1%ae%e4%bf%9d%e5%8f%aa%e6%9c%89%e4%b8%80%e4%b8%aa%e5%ae%9e%e4%be%8b%e5%9c%a8%e4%bb%bb%e4%bd%95%e7%bb%99%e5%ae%9a%e6%97%b6%e9%97%b4%e6%8c%81%e6%9c%89%e8%af%a5%e9%94%81%e6%9c%89%e6%95%88%e5%9c%b0%e4%bd%bf%e8%af%a5%e5%ae%9e%e4%be%8b%e6%88%90%e4%b8%ba-leader%e5%85%b6%e4%bb%96%e5%89%af%e6%9c%ac%e4%b8%8d%e6%96%ad%e5%b0%9d%e8%af%95%e8%8e%b7%e5%8f%96%e9%94%81%e4%bb%8e%e8%80%8c%e5%87%86%e5%a4%87%e5%a5%bd%e8%bf%9b%e8%a1%8c%e6%97%a0%e7%bc%9d%e6%95%85%e9%9a%9c%e8%bd%ac%e7%a7%bb%e4%bb%a5%e9%98%b2%e5%bd%93%e5%89%8d%e7%9a%84-leader-%e5%8f%98%e5%be%97%e4%b8%8d%e5%8f%af%e7%94%a8">#&lt;/a>
&lt;/h5>
&lt;h5 id="然而在某些情况下会出现挑战例如当领导节点失效和无响应时并且持有锁一段时间直到该实例被回收这种情况可能会导致传入请求因超时而失败并且由于备用副本无法承担领导职责而增加正在进行任务的延迟为了缓解这个问题通常建议锁服务在锁上实施过期机制通常称为基于-ttl-的租约">
 然而，在某些情况下会出现挑战，例如当领导节点失效和无响应时，并且持有锁一段时间直到该实例被回收。这种情况可能会导致传入请求因超时而失败，并且由于备用副本无法承担领导职责而增加正在进行任务的延迟。为了缓解这个问题，通常建议锁服务在锁上实施过期机制，通常称为基于 TTL 的租约。
 &lt;a class="anchor" href="#%e7%84%b6%e8%80%8c%e5%9c%a8%e6%9f%90%e4%ba%9b%e6%83%85%e5%86%b5%e4%b8%8b%e4%bc%9a%e5%87%ba%e7%8e%b0%e6%8c%91%e6%88%98%e4%be%8b%e5%a6%82%e5%bd%93%e9%a2%86%e5%af%bc%e8%8a%82%e7%82%b9%e5%a4%b1%e6%95%88%e5%92%8c%e6%97%a0%e5%93%8d%e5%ba%94%e6%97%b6%e5%b9%b6%e4%b8%94%e6%8c%81%e6%9c%89%e9%94%81%e4%b8%80%e6%ae%b5%e6%97%b6%e9%97%b4%e7%9b%b4%e5%88%b0%e8%af%a5%e5%ae%9e%e4%be%8b%e8%a2%ab%e5%9b%9e%e6%94%b6%e8%bf%99%e7%a7%8d%e6%83%85%e5%86%b5%e5%8f%af%e8%83%bd%e4%bc%9a%e5%af%bc%e8%87%b4%e4%bc%a0%e5%85%a5%e8%af%b7%e6%b1%82%e5%9b%a0%e8%b6%85%e6%97%b6%e8%80%8c%e5%a4%b1%e8%b4%a5%e5%b9%b6%e4%b8%94%e7%94%b1%e4%ba%8e%e5%a4%87%e7%94%a8%e5%89%af%e6%9c%ac%e6%97%a0%e6%b3%95%e6%89%bf%e6%8b%85%e9%a2%86%e5%af%bc%e8%81%8c%e8%b4%a3%e8%80%8c%e5%a2%9e%e5%8a%a0%e6%ad%a3%e5%9c%a8%e8%bf%9b%e8%a1%8c%e4%bb%bb%e5%8a%a1%e7%9a%84%e5%bb%b6%e8%bf%9f%e4%b8%ba%e4%ba%86%e7%bc%93%e8%a7%a3%e8%bf%99%e4%b8%aa%e9%97%ae%e9%a2%98%e9%80%9a%e5%b8%b8%e5%bb%ba%e8%ae%ae%e9%94%81%e6%9c%8d%e5%8a%a1%e5%9c%a8%e9%94%81%e4%b8%8a%e5%ae%9e%e6%96%bd%e8%bf%87%e6%9c%9f%e6%9c%ba%e5%88%b6%e9%80%9a%e5%b8%b8%e7%a7%b0%e4%b8%ba%e5%9f%ba%e4%ba%8e-ttl-%e7%9a%84%e7%a7%9f%e7%ba%a6">#&lt;/a>
&lt;/h5>
&lt;h3 id="问题1失效的-leader-实例">
 &lt;strong>问题1：失效的 Leader 实例&lt;/strong>
 &lt;a class="anchor" href="#%e9%97%ae%e9%a2%981%e5%a4%b1%e6%95%88%e7%9a%84-leader-%e5%ae%9e%e4%be%8b">#&lt;/a>
&lt;/h3>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202402261503620.png" alt="image-20240226150333561" />&lt;/p>
&lt;h5 id="此外锁服务必须符合一定的健壮性标准一方面它不能作为单个实例运行因为这将使其成为单点故障另一方面如果它使用多个实例以提供高可用性则必须设计能够抵御网络分区问题否则可能导致一个棘手的情况发生在不同网络分区中的两个副本都认为自己已经获得了锁并因此承担领导者角色">
 此外，锁服务必须符合一定的健壮性标准。一方面，它不能作为单个实例运行，因为这将使其成为单点故障。另一方面，如果它使用多个实例以提供高可用性，则必须设计能够抵御网络分区问题。否则可能导致一个棘手的情况发生：在不同网络分区中的两个副本都认为自己已经获得了锁，并因此承担领导者角色。
 &lt;a class="anchor" href="#%e6%ad%a4%e5%a4%96%e9%94%81%e6%9c%8d%e5%8a%a1%e5%bf%85%e9%a1%bb%e7%ac%a6%e5%90%88%e4%b8%80%e5%ae%9a%e7%9a%84%e5%81%a5%e5%a3%ae%e6%80%a7%e6%a0%87%e5%87%86%e4%b8%80%e6%96%b9%e9%9d%a2%e5%ae%83%e4%b8%8d%e8%83%bd%e4%bd%9c%e4%b8%ba%e5%8d%95%e4%b8%aa%e5%ae%9e%e4%be%8b%e8%bf%90%e8%a1%8c%e5%9b%a0%e4%b8%ba%e8%bf%99%e5%b0%86%e4%bd%bf%e5%85%b6%e6%88%90%e4%b8%ba%e5%8d%95%e7%82%b9%e6%95%85%e9%9a%9c%e5%8f%a6%e4%b8%80%e6%96%b9%e9%9d%a2%e5%a6%82%e6%9e%9c%e5%ae%83%e4%bd%bf%e7%94%a8%e5%a4%9a%e4%b8%aa%e5%ae%9e%e4%be%8b%e4%bb%a5%e6%8f%90%e4%be%9b%e9%ab%98%e5%8f%af%e7%94%a8%e6%80%a7%e5%88%99%e5%bf%85%e9%a1%bb%e8%ae%be%e8%ae%a1%e8%83%bd%e5%a4%9f%e6%8a%b5%e5%be%a1%e7%bd%91%e7%bb%9c%e5%88%86%e5%8c%ba%e9%97%ae%e9%a2%98%e5%90%a6%e5%88%99%e5%8f%af%e8%83%bd%e5%af%bc%e8%87%b4%e4%b8%80%e4%b8%aa%e6%a3%98%e6%89%8b%e7%9a%84%e6%83%85%e5%86%b5%e5%8f%91%e7%94%9f%e5%9c%a8%e4%b8%8d%e5%90%8c%e7%bd%91%e7%bb%9c%e5%88%86%e5%8c%ba%e4%b8%ad%e7%9a%84%e4%b8%a4%e4%b8%aa%e5%89%af%e6%9c%ac%e9%83%bd%e8%ae%a4%e4%b8%ba%e8%87%aa%e5%b7%b1%e5%b7%b2%e7%bb%8f%e8%8e%b7%e5%be%97%e4%ba%86%e9%94%81%e5%b9%b6%e5%9b%a0%e6%ad%a4%e6%89%bf%e6%8b%85%e9%a2%86%e5%af%bc%e8%80%85%e8%a7%92%e8%89%b2">#&lt;/a>
&lt;/h5>
&lt;h3 id="问题2脑裂">
 &lt;strong>问题2：脑裂&lt;/strong>
 &lt;a class="anchor" href="#%e9%97%ae%e9%a2%982%e8%84%91%e8%a3%82">#&lt;/a>
&lt;/h3>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202402261503732.png" alt="image-20240226150349667" />&lt;/p>
&lt;h2 id="基于仲裁的系统成为首选解决方案">
 &lt;strong>基于仲裁的系统成为首选解决方案&lt;/strong>
 &lt;a class="anchor" href="#%e5%9f%ba%e4%ba%8e%e4%bb%b2%e8%a3%81%e7%9a%84%e7%b3%bb%e7%bb%9f%e6%88%90%e4%b8%ba%e9%a6%96%e9%80%89%e8%a7%a3%e5%86%b3%e6%96%b9%e6%a1%88">#&lt;/a>
&lt;/h2>
&lt;h5 id="为了解决网络分区的核心问题像-zookeeper被-kafka-使用和-etcd被-kubernetes-利用这样的基于仲裁的系统通过内置的-ttl-功能应运而生将自己确立为领导者选举的最佳解决方案zookeeper-原子广播zab协议和-etcd-采用的-raft-协议都确保只有在大多数节点认可时才会考虑任何操作是有效的在发生网络分区时只有一个分区可以包含大多数节点有效地防止了出现多个领导者的可能性">
 为了解决网络分区的核心问题，像 ZooKeeper（被 Kafka 使用）和 etcd（被 Kubernetes 利用）这样的基于仲裁的系统通过内置的 TTL 功能应运而生，将自己确立为领导者选举的最佳解决方案。ZooKeeper 原子广播（ZAB）协议和 etcd 采用的 RAFT 协议都确保只有在大多数节点认可时才会考虑任何操作是有效的。在发生网络分区时，只有一个分区可以包含大多数节点，有效地防止了出现多个领导者的可能性。
 &lt;a class="anchor" href="#%e4%b8%ba%e4%ba%86%e8%a7%a3%e5%86%b3%e7%bd%91%e7%bb%9c%e5%88%86%e5%8c%ba%e7%9a%84%e6%a0%b8%e5%bf%83%e9%97%ae%e9%a2%98%e5%83%8f-zookeeper%e8%a2%ab-kafka-%e4%bd%bf%e7%94%a8%e5%92%8c-etcd%e8%a2%ab-kubernetes-%e5%88%a9%e7%94%a8%e8%bf%99%e6%a0%b7%e7%9a%84%e5%9f%ba%e4%ba%8e%e4%bb%b2%e8%a3%81%e7%9a%84%e7%b3%bb%e7%bb%9f%e9%80%9a%e8%bf%87%e5%86%85%e7%bd%ae%e7%9a%84-ttl-%e5%8a%9f%e8%83%bd%e5%ba%94%e8%bf%90%e8%80%8c%e7%94%9f%e5%b0%86%e8%87%aa%e5%b7%b1%e7%a1%ae%e7%ab%8b%e4%b8%ba%e9%a2%86%e5%af%bc%e8%80%85%e9%80%89%e4%b8%be%e7%9a%84%e6%9c%80%e4%bd%b3%e8%a7%a3%e5%86%b3%e6%96%b9%e6%a1%88zookeeper-%e5%8e%9f%e5%ad%90%e5%b9%bf%e6%92%adzab%e5%8d%8f%e8%ae%ae%e5%92%8c-etcd-%e9%87%87%e7%94%a8%e7%9a%84-raft-%e5%8d%8f%e8%ae%ae%e9%83%bd%e7%a1%ae%e4%bf%9d%e5%8f%aa%e6%9c%89%e5%9c%a8%e5%a4%a7%e5%a4%9a%e6%95%b0%e8%8a%82%e7%82%b9%e8%ae%a4%e5%8f%af%e6%97%b6%e6%89%8d%e4%bc%9a%e8%80%83%e8%99%91%e4%bb%bb%e4%bd%95%e6%93%8d%e4%bd%9c%e6%98%af%e6%9c%89%e6%95%88%e7%9a%84%e5%9c%a8%e5%8f%91%e7%94%9f%e7%bd%91%e7%bb%9c%e5%88%86%e5%8c%ba%e6%97%b6%e5%8f%aa%e6%9c%89%e4%b8%80%e4%b8%aa%e5%88%86%e5%8c%ba%e5%8f%af%e4%bb%a5%e5%8c%85%e5%90%ab%e5%a4%a7%e5%a4%9a%e6%95%b0%e8%8a%82%e7%82%b9%e6%9c%89%e6%95%88%e5%9c%b0%e9%98%b2%e6%ad%a2%e4%ba%86%e5%87%ba%e7%8e%b0%e5%a4%9a%e4%b8%aa%e9%a2%86%e5%af%bc%e8%80%85%e7%9a%84%e5%8f%af%e8%83%bd%e6%80%a7">#&lt;/a>
&lt;/h5>
&lt;h5 id="然而无论是-zookeeper-还是-etcd-都不具备成本效益分布式系统固有复杂性带来了运维挑战为小规模服务部署这样的集群可能太重并且额外开销可能超过应用程序本身的运维成本">
 然而，无论是 ZooKeeper 还是 etcd 都不具备成本效益。分布式系统固有复杂性带来了运维挑战。为小规模服务部署这样的集群可能太重，并且额外开销可能超过应用程序本身的运维成本。
 &lt;a class="anchor" href="#%e7%84%b6%e8%80%8c%e6%97%a0%e8%ae%ba%e6%98%af-zookeeper-%e8%bf%98%e6%98%af-etcd-%e9%83%bd%e4%b8%8d%e5%85%b7%e5%a4%87%e6%88%90%e6%9c%ac%e6%95%88%e7%9b%8a%e5%88%86%e5%b8%83%e5%bc%8f%e7%b3%bb%e7%bb%9f%e5%9b%ba%e6%9c%89%e5%a4%8d%e6%9d%82%e6%80%a7%e5%b8%a6%e6%9d%a5%e4%ba%86%e8%bf%90%e7%bb%b4%e6%8c%91%e6%88%98%e4%b8%ba%e5%b0%8f%e8%a7%84%e6%a8%a1%e6%9c%8d%e5%8a%a1%e9%83%a8%e7%bd%b2%e8%bf%99%e6%a0%b7%e7%9a%84%e9%9b%86%e7%be%a4%e5%8f%af%e8%83%bd%e5%a4%aa%e9%87%8d%e5%b9%b6%e4%b8%94%e9%a2%9d%e5%a4%96%e5%bc%80%e9%94%80%e5%8f%af%e8%83%bd%e8%b6%85%e8%bf%87%e5%ba%94%e7%94%a8%e7%a8%8b%e5%ba%8f%e6%9c%ac%e8%ba%ab%e7%9a%84%e8%bf%90%e7%bb%b4%e6%88%90%e6%9c%ac">#&lt;/a>
&lt;/h5>
&lt;h2 id="k8s-leases">
 &lt;strong>K8s Leases&lt;/strong>
 &lt;a class="anchor" href="#k8s-leases">#&lt;/a>
&lt;/h2>
&lt;h5 id="由于成本问题可靠的领导者选举只适用于大规模系统吗未必如果你正在使用-kubernetes-基础设施则可以通过使用-kubernetes-leases-来简单地实现-etcd-级别的领导者选举--无需直接与-etcd-api-交互">
 由于成本问题，可靠的领导者选举只适用于大规模系统吗？未必，如果你正在使用 Kubernetes 基础设施，则可以通过使用 Kubernetes Leases 来简单地实现 etcd 级别的领导者选举 —— 无需直接与 Etcd API 交互。
 &lt;a class="anchor" href="#%e7%94%b1%e4%ba%8e%e6%88%90%e6%9c%ac%e9%97%ae%e9%a2%98%e5%8f%af%e9%9d%a0%e7%9a%84%e9%a2%86%e5%af%bc%e8%80%85%e9%80%89%e4%b8%be%e5%8f%aa%e9%80%82%e7%94%a8%e4%ba%8e%e5%a4%a7%e8%a7%84%e6%a8%a1%e7%b3%bb%e7%bb%9f%e5%90%97%e6%9c%aa%e5%bf%85%e5%a6%82%e6%9e%9c%e4%bd%a0%e6%ad%a3%e5%9c%a8%e4%bd%bf%e7%94%a8-kubernetes-%e5%9f%ba%e7%a1%80%e8%ae%be%e6%96%bd%e5%88%99%e5%8f%af%e4%bb%a5%e9%80%9a%e8%bf%87%e4%bd%bf%e7%94%a8-kubernetes-leases-%e6%9d%a5%e7%ae%80%e5%8d%95%e5%9c%b0%e5%ae%9e%e7%8e%b0-etcd-%e7%ba%a7%e5%88%ab%e7%9a%84%e9%a2%86%e5%af%bc%e8%80%85%e9%80%89%e4%b8%be--%e6%97%a0%e9%9c%80%e7%9b%b4%e6%8e%a5%e4%b8%8e-etcd-api-%e4%ba%a4%e4%ba%92">#&lt;/a>
&lt;/h5>
&lt;h3 id="为什么很强大">
 &lt;strong>为什么很强大&lt;/strong>
 &lt;a class="anchor" href="#%e4%b8%ba%e4%bb%80%e4%b9%88%e5%be%88%e5%bc%ba%e5%a4%a7">#&lt;/a>
&lt;/h3>
&lt;h5 id="kubernetes-leases-之所以强大部分原因在于-kubernetes-本身通常将-etcd-作为其所有-api对象包括租约的存储这意味着-etcd-的高可用性一致性和容错特性自然地可用于-kubernetes-leases但是kubernetes-api-添加了额外的功能层比如通过资源版本控制实现乐观并发控制这也有助于使租约成为在-kubernetes-环境中进行领导者选举时一个强大的选择">
 Kubernetes Leases 之所以强大，部分原因在于 Kubernetes 本身通常将 etcd 作为其所有 API对象（包括租约）的存储。这意味着 etcd 的高可用性、一致性和容错特性自然地可用于 Kubernetes Leases。但是，Kubernetes API 添加了额外的功能层，比如通过资源版本控制实现乐观并发控制，这也有助于使租约成为在 Kubernetes 环境中进行领导者选举时一个强大的选择。
 &lt;a class="anchor" href="#kubernetes-leases-%e4%b9%8b%e6%89%80%e4%bb%a5%e5%bc%ba%e5%a4%a7%e9%83%a8%e5%88%86%e5%8e%9f%e5%9b%a0%e5%9c%a8%e4%ba%8e-kubernetes-%e6%9c%ac%e8%ba%ab%e9%80%9a%e5%b8%b8%e5%b0%86-etcd-%e4%bd%9c%e4%b8%ba%e5%85%b6%e6%89%80%e6%9c%89-api%e5%af%b9%e8%b1%a1%e5%8c%85%e6%8b%ac%e7%a7%9f%e7%ba%a6%e7%9a%84%e5%ad%98%e5%82%a8%e8%bf%99%e6%84%8f%e5%91%b3%e7%9d%80-etcd-%e7%9a%84%e9%ab%98%e5%8f%af%e7%94%a8%e6%80%a7%e4%b8%80%e8%87%b4%e6%80%a7%e5%92%8c%e5%ae%b9%e9%94%99%e7%89%b9%e6%80%a7%e8%87%aa%e7%84%b6%e5%9c%b0%e5%8f%af%e7%94%a8%e4%ba%8e-kubernetes-leases%e4%bd%86%e6%98%afkubernetes-api-%e6%b7%bb%e5%8a%a0%e4%ba%86%e9%a2%9d%e5%a4%96%e7%9a%84%e5%8a%9f%e8%83%bd%e5%b1%82%e6%af%94%e5%a6%82%e9%80%9a%e8%bf%87%e8%b5%84%e6%ba%90%e7%89%88%e6%9c%ac%e6%8e%a7%e5%88%b6%e5%ae%9e%e7%8e%b0%e4%b9%90%e8%a7%82%e5%b9%b6%e5%8f%91%e6%8e%a7%e5%88%b6%e8%bf%99%e4%b9%9f%e6%9c%89%e5%8a%a9%e4%ba%8e%e4%bd%bf%e7%a7%9f%e7%ba%a6%e6%88%90%e4%b8%ba%e5%9c%a8-kubernetes-%e7%8e%af%e5%a2%83%e4%b8%ad%e8%bf%9b%e8%a1%8c%e9%a2%86%e5%af%bc%e8%80%85%e9%80%89%e4%b8%be%e6%97%b6%e4%b8%80%e4%b8%aa%e5%bc%ba%e5%a4%a7%e7%9a%84%e9%80%89%e6%8b%a9">#&lt;/a>
&lt;/h5>
&lt;h3 id="工作原理">
 &lt;strong>工作原理&lt;/strong>
 &lt;a class="anchor" href="#%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86">#&lt;/a>
&lt;/h3>
&lt;h5 id="在-kubernetes-中可以使用-lease-对象来促进领导者选举竞争领导权的候选人要么创建要么更新-lease-对象并将其标识符设置在-holderidentity-字段中领导者持续续订该-lease-以保持其角色如果领导者未能在-lease-到期前续订则其他候选人会尝试获取它首个成功更新-lease-的候选人将成为新领导者确保平稳故障转移所有候选人都会监视-lease-对象以跟踪领导变更">
 在 Kubernetes 中，可以使用 Lease 对象来促进领导者选举。竞争领导权的候选人要么创建要么更新 Lease 对象，并将其标识符设置在 &lt;code>holderIdentity&lt;/code> 字段中。领导者持续“续订”该 Lease 以保持其角色。如果领导者未能在 Lease 到期前续订，则其他候选人会尝试获取它。首个成功更新 Lease 的候选人将成为新领导者，确保平稳故障转移。所有候选人都会监视 Lease 对象以跟踪领导变更。
 &lt;a class="anchor" href="#%e5%9c%a8-kubernetes-%e4%b8%ad%e5%8f%af%e4%bb%a5%e4%bd%bf%e7%94%a8-lease-%e5%af%b9%e8%b1%a1%e6%9d%a5%e4%bf%83%e8%bf%9b%e9%a2%86%e5%af%bc%e8%80%85%e9%80%89%e4%b8%be%e7%ab%9e%e4%ba%89%e9%a2%86%e5%af%bc%e6%9d%83%e7%9a%84%e5%80%99%e9%80%89%e4%ba%ba%e8%a6%81%e4%b9%88%e5%88%9b%e5%bb%ba%e8%a6%81%e4%b9%88%e6%9b%b4%e6%96%b0-lease-%e5%af%b9%e8%b1%a1%e5%b9%b6%e5%b0%86%e5%85%b6%e6%a0%87%e8%af%86%e7%ac%a6%e8%ae%be%e7%bd%ae%e5%9c%a8-holderidentity-%e5%ad%97%e6%ae%b5%e4%b8%ad%e9%a2%86%e5%af%bc%e8%80%85%e6%8c%81%e7%bb%ad%e7%bb%ad%e8%ae%a2%e8%af%a5-lease-%e4%bb%a5%e4%bf%9d%e6%8c%81%e5%85%b6%e8%a7%92%e8%89%b2%e5%a6%82%e6%9e%9c%e9%a2%86%e5%af%bc%e8%80%85%e6%9c%aa%e8%83%bd%e5%9c%a8-lease-%e5%88%b0%e6%9c%9f%e5%89%8d%e7%bb%ad%e8%ae%a2%e5%88%99%e5%85%b6%e4%bb%96%e5%80%99%e9%80%89%e4%ba%ba%e4%bc%9a%e5%b0%9d%e8%af%95%e8%8e%b7%e5%8f%96%e5%ae%83%e9%a6%96%e4%b8%aa%e6%88%90%e5%8a%9f%e6%9b%b4%e6%96%b0-lease-%e7%9a%84%e5%80%99%e9%80%89%e4%ba%ba%e5%b0%86%e6%88%90%e4%b8%ba%e6%96%b0%e9%a2%86%e5%af%bc%e8%80%85%e7%a1%ae%e4%bf%9d%e5%b9%b3%e7%a8%b3%e6%95%85%e9%9a%9c%e8%bd%ac%e7%a7%bb%e6%89%80%e6%9c%89%e5%80%99%e9%80%89%e4%ba%ba%e9%83%bd%e4%bc%9a%e7%9b%91%e8%a7%86-lease-%e5%af%b9%e8%b1%a1%e4%bb%a5%e8%b7%9f%e8%b8%aa%e9%a2%86%e5%af%bc%e5%8f%98%e6%9b%b4">#&lt;/a>
&lt;/h5>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202402261504213.png" alt="image-20240226150408146" />&lt;/p></description></item><item><title>2024-04-03 大规模并发下如何加快 Pod 启动速度</title><link>https://qq547475331.github.io/docs/%E5%A4%A7%E8%A7%84%E6%A8%A1%E5%B9%B6%E5%8F%91%E4%B8%8B%E5%A6%82%E4%BD%95%E5%8A%A0%E5%BF%AB-pod-%E5%90%AF%E5%8A%A8%E9%80%9F%E5%BA%A6-da-gui-mo-bing-fa-xia-ru-he-jia-kuai-pod-qi-dong-su-du/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/%E5%A4%A7%E8%A7%84%E6%A8%A1%E5%B9%B6%E5%8F%91%E4%B8%8B%E5%A6%82%E4%BD%95%E5%8A%A0%E5%BF%AB-pod-%E5%90%AF%E5%8A%A8%E9%80%9F%E5%BA%A6-da-gui-mo-bing-fa-xia-ru-he-jia-kuai-pod-qi-dong-su-du/</guid><description>&lt;h5 id="容器化的应用在真正能够运行业务逻辑前需要经过镜像拉取镜像解压为容器运行时提供联合文件系统容器启动业务初始化等多个步骤其中容器镜像拉取是所有环节中最耗时的">
 容器化的应用在真正能够运行业务逻辑前，需要经过镜像拉取、镜像解压、为容器运行时提供联合文件系统、容器启动、业务初始化等多个步骤，其中&lt;strong>容器镜像拉取&lt;/strong>是所有环节中最耗时的。
 &lt;a class="anchor" href="#%e5%ae%b9%e5%99%a8%e5%8c%96%e7%9a%84%e5%ba%94%e7%94%a8%e5%9c%a8%e7%9c%9f%e6%ad%a3%e8%83%bd%e5%a4%9f%e8%bf%90%e8%a1%8c%e4%b8%9a%e5%8a%a1%e9%80%bb%e8%be%91%e5%89%8d%e9%9c%80%e8%a6%81%e7%bb%8f%e8%bf%87%e9%95%9c%e5%83%8f%e6%8b%89%e5%8f%96%e9%95%9c%e5%83%8f%e8%a7%a3%e5%8e%8b%e4%b8%ba%e5%ae%b9%e5%99%a8%e8%bf%90%e8%a1%8c%e6%97%b6%e6%8f%90%e4%be%9b%e8%81%94%e5%90%88%e6%96%87%e4%bb%b6%e7%b3%bb%e7%bb%9f%e5%ae%b9%e5%99%a8%e5%90%af%e5%8a%a8%e4%b8%9a%e5%8a%a1%e5%88%9d%e5%a7%8b%e5%8c%96%e7%ad%89%e5%a4%9a%e4%b8%aa%e6%ad%a5%e9%aa%a4%e5%85%b6%e4%b8%ad%e5%ae%b9%e5%99%a8%e9%95%9c%e5%83%8f%e6%8b%89%e5%8f%96%e6%98%af%e6%89%80%e6%9c%89%e7%8e%af%e8%8a%82%e4%b8%ad%e6%9c%80%e8%80%97%e6%97%b6%e7%9a%84">#&lt;/a>
&lt;/h5>
&lt;h5 id="在大规模集群下镜像拉取如果耗时过久对于流量突发场景会影响业务的弹性效率对于大数据ai-的场景会影响任务的吞吐性能为缓解对于弹性效率的影响研发团队需要提前更多时间预置节点扩容业务容器针对吞吐性能影响需要扩大集群的规模然而上述操作都会在无形中造成成本上升因此镜像拉取的优化一直是容器服务重点优化的方向之一">
 在大规模集群下，镜像拉取如果耗时过久，对于流量突发场景，会影响业务的弹性效率；对于大数据、AI 的场景，会影响任务的吞吐性能。为缓解对于弹性效率的影响，研发团队需要提前更多时间预置节点，扩容业务容器；针对吞吐性能影响，需要扩大集群的规模。然而上述操作都会在无形中造成成本上升。因此镜像拉取的优化一直是容器服务重点优化的方向之一。
 &lt;a class="anchor" href="#%e5%9c%a8%e5%a4%a7%e8%a7%84%e6%a8%a1%e9%9b%86%e7%be%a4%e4%b8%8b%e9%95%9c%e5%83%8f%e6%8b%89%e5%8f%96%e5%a6%82%e6%9e%9c%e8%80%97%e6%97%b6%e8%bf%87%e4%b9%85%e5%af%b9%e4%ba%8e%e6%b5%81%e9%87%8f%e7%aa%81%e5%8f%91%e5%9c%ba%e6%99%af%e4%bc%9a%e5%bd%b1%e5%93%8d%e4%b8%9a%e5%8a%a1%e7%9a%84%e5%bc%b9%e6%80%a7%e6%95%88%e7%8e%87%e5%af%b9%e4%ba%8e%e5%a4%a7%e6%95%b0%e6%8d%aeai-%e7%9a%84%e5%9c%ba%e6%99%af%e4%bc%9a%e5%bd%b1%e5%93%8d%e4%bb%bb%e5%8a%a1%e7%9a%84%e5%90%9e%e5%90%90%e6%80%a7%e8%83%bd%e4%b8%ba%e7%bc%93%e8%a7%a3%e5%af%b9%e4%ba%8e%e5%bc%b9%e6%80%a7%e6%95%88%e7%8e%87%e7%9a%84%e5%bd%b1%e5%93%8d%e7%a0%94%e5%8f%91%e5%9b%a2%e9%98%9f%e9%9c%80%e8%a6%81%e6%8f%90%e5%89%8d%e6%9b%b4%e5%a4%9a%e6%97%b6%e9%97%b4%e9%a2%84%e7%bd%ae%e8%8a%82%e7%82%b9%e6%89%a9%e5%ae%b9%e4%b8%9a%e5%8a%a1%e5%ae%b9%e5%99%a8%e9%92%88%e5%af%b9%e5%90%9e%e5%90%90%e6%80%a7%e8%83%bd%e5%bd%b1%e5%93%8d%e9%9c%80%e8%a6%81%e6%89%a9%e5%a4%a7%e9%9b%86%e7%be%a4%e7%9a%84%e8%a7%84%e6%a8%a1%e7%84%b6%e8%80%8c%e4%b8%8a%e8%bf%b0%e6%93%8d%e4%bd%9c%e9%83%bd%e4%bc%9a%e5%9c%a8%e6%97%a0%e5%bd%a2%e4%b8%ad%e9%80%a0%e6%88%90%e6%88%90%e6%9c%ac%e4%b8%8a%e5%8d%87%e5%9b%a0%e6%ad%a4%e9%95%9c%e5%83%8f%e6%8b%89%e5%8f%96%e7%9a%84%e4%bc%98%e5%8c%96%e4%b8%80%e7%9b%b4%e6%98%af%e5%ae%b9%e5%99%a8%e6%9c%8d%e5%8a%a1%e9%87%8d%e7%82%b9%e4%bc%98%e5%8c%96%e7%9a%84%e6%96%b9%e5%90%91%e4%b9%8b%e4%b8%80">#&lt;/a>
&lt;/h5>
&lt;h5 id="火山引擎容器服务-vkevolcengine-kubernetes-engine深度融合新一代云原生技术可提供高性能高可靠极致弹性的企业级容器管理能力在服务企业客户的过程中为了进一步提升镜像拉取效率帮助以-aigc-为代表的企业敏捷高效地落地-ai-技术容器服务-vke-结合对镜像拉取环节问题的分析从三个不同角度对镜像拉取进行了一系列优化">
 火山引擎容器服务 VKE（Volcengine Kubernetes Engine）深度融合新一代云原生技术，可提供高性能、高可靠、极致弹性的企业级容器管理能力。在服务企业客户的过程中，为了进一步提升镜像拉取效率，帮助以 AIGC 为代表的企业敏捷、高效地落地 AI 技术，容器服务 VKE 结合对镜像拉取环节问题的分析，从三个不同角度对镜像拉取进行了一系列优化：
 &lt;a class="anchor" href="#%e7%81%ab%e5%b1%b1%e5%bc%95%e6%93%8e%e5%ae%b9%e5%99%a8%e6%9c%8d%e5%8a%a1-vkevolcengine-kubernetes-engine%e6%b7%b1%e5%ba%a6%e8%9e%8d%e5%90%88%e6%96%b0%e4%b8%80%e4%bb%a3%e4%ba%91%e5%8e%9f%e7%94%9f%e6%8a%80%e6%9c%af%e5%8f%af%e6%8f%90%e4%be%9b%e9%ab%98%e6%80%a7%e8%83%bd%e9%ab%98%e5%8f%af%e9%9d%a0%e6%9e%81%e8%87%b4%e5%bc%b9%e6%80%a7%e7%9a%84%e4%bc%81%e4%b8%9a%e7%ba%a7%e5%ae%b9%e5%99%a8%e7%ae%a1%e7%90%86%e8%83%bd%e5%8a%9b%e5%9c%a8%e6%9c%8d%e5%8a%a1%e4%bc%81%e4%b8%9a%e5%ae%a2%e6%88%b7%e7%9a%84%e8%bf%87%e7%a8%8b%e4%b8%ad%e4%b8%ba%e4%ba%86%e8%bf%9b%e4%b8%80%e6%ad%a5%e6%8f%90%e5%8d%87%e9%95%9c%e5%83%8f%e6%8b%89%e5%8f%96%e6%95%88%e7%8e%87%e5%b8%ae%e5%8a%a9%e4%bb%a5-aigc-%e4%b8%ba%e4%bb%a3%e8%a1%a8%e7%9a%84%e4%bc%81%e4%b8%9a%e6%95%8f%e6%8d%b7%e9%ab%98%e6%95%88%e5%9c%b0%e8%90%bd%e5%9c%b0-ai-%e6%8a%80%e6%9c%af%e5%ae%b9%e5%99%a8%e6%9c%8d%e5%8a%a1-vke-%e7%bb%93%e5%90%88%e5%af%b9%e9%95%9c%e5%83%8f%e6%8b%89%e5%8f%96%e7%8e%af%e8%8a%82%e9%97%ae%e9%a2%98%e7%9a%84%e5%88%86%e6%9e%90%e4%bb%8e%e4%b8%89%e4%b8%aa%e4%b8%8d%e5%90%8c%e8%a7%92%e5%ba%a6%e5%af%b9%e9%95%9c%e5%83%8f%e6%8b%89%e5%8f%96%e8%bf%9b%e8%a1%8c%e4%ba%86%e4%b8%80%e7%b3%bb%e5%88%97%e4%bc%98%e5%8c%96">#&lt;/a>
&lt;/h5>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202402261446296.png" alt="image-20240226144644234" />&lt;/p>
&lt;p>&lt;strong>P2P 加速&lt;/strong>&lt;/p>
&lt;p>在大镜像场景下，火山引擎容器服务 VKE 基于开源项目 Dragonfly，推出了 P2P 加速方案，来规避镜像仓库 CR 带宽有限的问题。&lt;/p>
&lt;p>&lt;strong>P2P 加速原理&lt;/strong>&lt;/p>
&lt;p>Dragonfly 有如下组件：&lt;/p>
&lt;ul>
&lt;li>Manager：维护每个 P2P 集群之间的关系，动态配置管理。&lt;/li>
&lt;li>Scheduler：为下载节点选择最优的下载父节点，控制异常 Peer 的回源。&lt;/li>
&lt;li>Peer：Dragonfly 网络中的一个节点，也就是用户提出文件下载请求的计算机或服务器。&lt;/li>
&lt;/ul>
&lt;p>火山引擎容器服务 VKE 实现了对 Manager 和 Scheduler 的托管化改造，无需用户额外管理。VKE 中 P2P 组件的工作流程如下图所示：&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202402261447588.png" alt="image-20240226144714536" />&lt;/p>
&lt;ul>
&lt;li>当一个 Peer（例如，Peer A）需要拉取镜像时，它会首先与 Manager 节点进行通信。&lt;/li>
&lt;li>Manager 会检查所有在线的 Peer 的列表，考虑到各种因素（如网络连通性等）。&lt;/li>
&lt;li>选择合适的 Peer 作为 Parent Peer。如果没有可供选择的 Parent Peer，Manager 会带领 Peer 直接从源服务器获取镜像。&lt;/li>
&lt;li>Manager 把找到的 Parent Peer 信息发送给发起请求的 Peer A，包括每个 Parent Peer 的地址和服务端口。&lt;/li>
&lt;li>Peer A 根据从 Manager 接收到的 Parent Peer 信息，从其它的 Peer 中下载镜像数据。&lt;/li>
&lt;li>Scheduler 模块会持续监控整个文件下载过程。如果发现 Parent Peer 下载速度过慢或者出现错误的情况，它将重新从 Manager 获取新的 Parent Peer 进行下载。&lt;/li>
&lt;li>当获取整个镜像后，Peer A 就成为了该镜像的一个分发节点，所有的镜像数据都会直接从一个 Peer 传输到另一个 Peer。&lt;/li>
&lt;/ul>
&lt;p>如果此时有另一个 Peer（例如 Peer B）也需要同样的镜像，那么当 Dragonfly 收到 Peer B 的请求时，同样经过 Manager、Scheduler 的处理，最后会从已经保存了该镜像的 Peer A 那里拉取数据，从而实现了 P2P 的镜像分发。&lt;/p></description></item><item><title>2024-04-03 如何使用tekton快速搭建CI/CD平台</title><link>https://qq547475331.github.io/docs/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8tekton%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BAcicd%E5%B9%B3%E5%8F%B0-ru-he-shi-yong-tekton-kuai-su-da-jian-cicd-ping-tai/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8tekton%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BAcicd%E5%B9%B3%E5%8F%B0-ru-he-shi-yong-tekton-kuai-su-da-jian-cicd-ping-tai/</guid><description>&lt;p>由于组织内部的人事调整，我接手了一套 CICD 平台。为了帮助团队成员尽快熟悉这套系统，今天我将详细讲解 CI 和 CD 的概念，并介绍这套系统底层关键组件 Tekon 的基本知识。最后，我将通过一个 Golang 项目作为示例，向大家演示如何使用 Tekton 从零开始快速构建自己的 CICD 平台，实现自动化的流程。&lt;/p>
&lt;h2 id="cicd">
 &lt;strong>CICD&lt;/strong>
 &lt;a class="anchor" href="#cicd">#&lt;/a>
&lt;/h2>
&lt;h5 id="ci-和-cd-是软件开发中常接触到的两个术语分别代表持续集成continuous-integration和持续交付continuous-delivery或持续部署continuous-deployment">
 CI 和 CD 是软件开发中常接触到的两个术语，分别代表持续集成（Continuous Integration）和持续交付（Continuous Delivery）或持续部署（Continuous Deployment）。
 &lt;a class="anchor" href="#ci-%e5%92%8c-cd-%e6%98%af%e8%bd%af%e4%bb%b6%e5%bc%80%e5%8f%91%e4%b8%ad%e5%b8%b8%e6%8e%a5%e8%a7%a6%e5%88%b0%e7%9a%84%e4%b8%a4%e4%b8%aa%e6%9c%af%e8%af%ad%e5%88%86%e5%88%ab%e4%bb%a3%e8%a1%a8%e6%8c%81%e7%bb%ad%e9%9b%86%e6%88%90continuous-integration%e5%92%8c%e6%8c%81%e7%bb%ad%e4%ba%a4%e4%bb%98continuous-delivery%e6%88%96%e6%8c%81%e7%bb%ad%e9%83%a8%e7%bd%b2continuous-deployment">#&lt;/a>
&lt;/h5>
&lt;h5 id="cicd-在敏捷开发和-devops-扮演着重要的角色通过自动化的脚本-cicd-可以减少从代码开发到部署期间人工操作次数从而加快软件交付速度提高软件质量促进团队协作因此cicd-是实现软件持续高质量交付的关键流程">
 CICD 在敏捷开发和 DevOps 扮演着重要的角色，通过自动化的脚本 CICD 可以减少从代码开发到部署期间人工操作次数，从而加快软件交付速度、提高软件质量、促进团队协作。因此，CICD 是实现软件持续高质量交付的关键流程。
 &lt;a class="anchor" href="#cicd-%e5%9c%a8%e6%95%8f%e6%8d%b7%e5%bc%80%e5%8f%91%e5%92%8c-devops-%e6%89%ae%e6%bc%94%e7%9d%80%e9%87%8d%e8%a6%81%e7%9a%84%e8%a7%92%e8%89%b2%e9%80%9a%e8%bf%87%e8%87%aa%e5%8a%a8%e5%8c%96%e7%9a%84%e8%84%9a%e6%9c%ac-cicd-%e5%8f%af%e4%bb%a5%e5%87%8f%e5%b0%91%e4%bb%8e%e4%bb%a3%e7%a0%81%e5%bc%80%e5%8f%91%e5%88%b0%e9%83%a8%e7%bd%b2%e6%9c%9f%e9%97%b4%e4%ba%ba%e5%b7%a5%e6%93%8d%e4%bd%9c%e6%ac%a1%e6%95%b0%e4%bb%8e%e8%80%8c%e5%8a%a0%e5%bf%ab%e8%bd%af%e4%bb%b6%e4%ba%a4%e4%bb%98%e9%80%9f%e5%ba%a6%e6%8f%90%e9%ab%98%e8%bd%af%e4%bb%b6%e8%b4%a8%e9%87%8f%e4%bf%83%e8%bf%9b%e5%9b%a2%e9%98%9f%e5%8d%8f%e4%bd%9c%e5%9b%a0%e6%ad%a4cicd-%e6%98%af%e5%ae%9e%e7%8e%b0%e8%bd%af%e4%bb%b6%e6%8c%81%e7%bb%ad%e9%ab%98%e8%b4%a8%e9%87%8f%e4%ba%a4%e4%bb%98%e7%9a%84%e5%85%b3%e9%94%ae%e6%b5%81%e7%a8%8b">#&lt;/a>
&lt;/h5>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202402221146639.png" alt="image-20240222114650519" />&lt;/p>
&lt;p>&lt;strong>持续集成&lt;/strong>&lt;/p>
&lt;p>持续集成的概念早在上个世纪 90 年代就开始出现，经过多年的完善和推广，现在已经被广泛认可和使用。其基本概念是：持续集成是一种软件开发实践，团队成员频繁将他们的工作成果集成在一起；每次提交后，自动触发运行一次包含自动化验证集的构建任务，以便能尽早发现集成问题。&lt;/p>
&lt;p>持续集成的核心原则包括频繁提交代码、自动化构建和测试、快速反馈、团队合作等。&lt;/p>
&lt;p>通过频繁将代码集成到主干分支中，减少代码集成造成的延期和风险，通过自动化构建和测试进行功能测试、单元测试、集成测试等操作快速发现问题，并及时向开发人员反馈测试结果和推进问题修复。在这个过程中，各团队成员之间需要加强沟通和合作，协同开发和解决问题。&lt;/p>
&lt;p>在持续集成中，为了确保功能分支的代码可以顺利集成到主干，团队成员可以采用六步提交法：&lt;/p>
&lt;ol>
&lt;li>检出代码前先更新代码库，确保代码库中的最新代码与本地代码相同步；&lt;/li>
&lt;li>在个人工作区对代码进行修改；&lt;/li>
&lt;li>在本地进行构建和测试，确保代码符合修改的要求；&lt;/li>
&lt;li>将主干分支合并到个人工作区后，再次进行构建和测试，防止开发期间代码与他人冲突；&lt;/li>
&lt;li>将修改后的代码推送到主干分支；&lt;/li>
&lt;li>在主干分支进行构建和测试，确保主干分支的代码质量和准确性；&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202402221147813.png" alt="image-20240222114702744" />&lt;/p>
&lt;h3 id="持续交付持续部署">
 &lt;strong>持续交付&amp;amp;&amp;amp;持续部署&lt;/strong>
 &lt;a class="anchor" href="#%e6%8c%81%e7%bb%ad%e4%ba%a4%e4%bb%98%e6%8c%81%e7%bb%ad%e9%83%a8%e7%bd%b2">#&lt;/a>
&lt;/h3>
&lt;p>持续交付和持续部署是在持续集成的基础上更进一步的自动化。&lt;/p>
&lt;p>持续交付是指在开发流程中，将代码持续地构建、测试和打包，并尽可能快地交付到生产环境前的预生产环境中。&lt;/p>
&lt;p>持续部署是指在持续交付的基础上，将代码在通过预生产环境的测试后，自动部署到生产环境中，与持续交付相比，持续部署不需要人工干预。&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202402221147297.png" alt="image-20240222114717137" />&lt;/p>
&lt;p>那么，如何在企业内快速搭建自己的 CICD 平台呢？Tekton 给我们提供了一种解决方案。&lt;/p>
&lt;h2 id="tekton">
 &lt;strong>Tekton&lt;/strong>
 &lt;a class="anchor" href="#tekton">#&lt;/a>
&lt;/h2>
&lt;p>Tekton 是一款基于 Kubernetes 实现的 CICD 开源框架，它提供了丰富的组件来满足各种构建、测试和部署的场景。我们可以直接使用 Tekton 来构建自己的 CICD 流程，也可以基于它进行二次开发搭建满足企业内部定制化需求的 CICD 平台。&lt;/p></description></item><item><title>2024-04-03 如何调试 crash 容器的网络</title><link>https://qq547475331.github.io/docs/%E5%A6%82%E4%BD%95%E8%B0%83%E8%AF%95-crash-%E5%AE%B9%E5%99%A8%E7%9A%84%E7%BD%91%E7%BB%9C-ru-he-diao-shi-crash-rong-qi-de-wang-luo/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/%E5%A6%82%E4%BD%95%E8%B0%83%E8%AF%95-crash-%E5%AE%B9%E5%99%A8%E7%9A%84%E7%BD%91%E7%BB%9C-ru-he-diao-shi-crash-rong-qi-de-wang-luo/</guid><description>&lt;h2 id="背景">
 背景
 &lt;a class="anchor" href="#%e8%83%8c%e6%99%af">#&lt;/a>
&lt;/h2>
&lt;h5 id="现在大部分的公司业务基本都已经容器化甚至-k8s-化的情况下当容器pod-运行异常时无非是看看其日志和一些-k8s-event-信息当容器pod-内部出现网络访问失败时或者其他一些问题时通常会进入容器pod-内部通过一些网络工具来进行调试那么问题来了一般容器内是不会安装太多的调试工具基本都是最小化的操作系统所以有的时候根本没办法调试">
 现在大部分的公司业务基本都已经容器化，甚至 K8S 化的情况下，当容器、Pod 运行异常时，无非是看看其日志和一些 K8S event 信息，当容器、Pod 内部出现网络访问失败时，或者其他一些问题时。通常会进入容器、Pod 内部通过一些网络工具来进行调试，那么问题来了。一般容器内是不会安装太多的调试工具，基本都是最小化的操作系统，所以有的时候根本没办法调试。
 &lt;a class="anchor" href="#%e7%8e%b0%e5%9c%a8%e5%a4%a7%e9%83%a8%e5%88%86%e7%9a%84%e5%85%ac%e5%8f%b8%e4%b8%9a%e5%8a%a1%e5%9f%ba%e6%9c%ac%e9%83%bd%e5%b7%b2%e7%bb%8f%e5%ae%b9%e5%99%a8%e5%8c%96%e7%94%9a%e8%87%b3-k8s-%e5%8c%96%e7%9a%84%e6%83%85%e5%86%b5%e4%b8%8b%e5%bd%93%e5%ae%b9%e5%99%a8pod-%e8%bf%90%e8%a1%8c%e5%bc%82%e5%b8%b8%e6%97%b6%e6%97%a0%e9%9d%9e%e6%98%af%e7%9c%8b%e7%9c%8b%e5%85%b6%e6%97%a5%e5%bf%97%e5%92%8c%e4%b8%80%e4%ba%9b-k8s-event-%e4%bf%a1%e6%81%af%e5%bd%93%e5%ae%b9%e5%99%a8pod-%e5%86%85%e9%83%a8%e5%87%ba%e7%8e%b0%e7%bd%91%e7%bb%9c%e8%ae%bf%e9%97%ae%e5%a4%b1%e8%b4%a5%e6%97%b6%e6%88%96%e8%80%85%e5%85%b6%e4%bb%96%e4%b8%80%e4%ba%9b%e9%97%ae%e9%a2%98%e6%97%b6%e9%80%9a%e5%b8%b8%e4%bc%9a%e8%bf%9b%e5%85%a5%e5%ae%b9%e5%99%a8pod-%e5%86%85%e9%83%a8%e9%80%9a%e8%bf%87%e4%b8%80%e4%ba%9b%e7%bd%91%e7%bb%9c%e5%b7%a5%e5%85%b7%e6%9d%a5%e8%bf%9b%e8%a1%8c%e8%b0%83%e8%af%95%e9%82%a3%e4%b9%88%e9%97%ae%e9%a2%98%e6%9d%a5%e4%ba%86%e4%b8%80%e8%88%ac%e5%ae%b9%e5%99%a8%e5%86%85%e6%98%af%e4%b8%8d%e4%bc%9a%e5%ae%89%e8%a3%85%e5%a4%aa%e5%a4%9a%e7%9a%84%e8%b0%83%e8%af%95%e5%b7%a5%e5%85%b7%e5%9f%ba%e6%9c%ac%e9%83%bd%e6%98%af%e6%9c%80%e5%b0%8f%e5%8c%96%e7%9a%84%e6%93%8d%e4%bd%9c%e7%b3%bb%e7%bb%9f%e6%89%80%e4%bb%a5%e6%9c%89%e7%9a%84%e6%97%b6%e5%80%99%e6%a0%b9%e6%9c%ac%e6%b2%a1%e5%8a%9e%e6%b3%95%e8%b0%83%e8%af%95">#&lt;/a>
&lt;/h5>
&lt;h5 id="可以有以下几种方式解决">
 可以有以下几种方式解决：
 &lt;a class="anchor" href="#%e5%8f%af%e4%bb%a5%e6%9c%89%e4%bb%a5%e4%b8%8b%e5%87%a0%e7%a7%8d%e6%96%b9%e5%bc%8f%e8%a7%a3%e5%86%b3">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="-就在制作镜像时安装一些常用的工具比如ippingtelnetsstcpdump-等命令方便后期使用但是这个就违背了容器化的最小化镜像原则本身上容器就是实现更轻便更快速地启动业务安装一些列工具就增加了镜像大小">
 • 就在制作镜像时，安装一些常用的工具，比如：&lt;code>ip&lt;/code>、&lt;code>ping&lt;/code>、&lt;code>telnet&lt;/code>、&lt;code>ss&lt;/code>、&lt;code>tcpdump&lt;/code> 等命令，方便后期使用，但是这个就违背了容器化的最小化镜像原则。本身上容器就是实现更轻便、更快速地启动业务，安装一些列工具就增加了镜像大小。
 &lt;a class="anchor" href="#-%e5%b0%b1%e5%9c%a8%e5%88%b6%e4%bd%9c%e9%95%9c%e5%83%8f%e6%97%b6%e5%ae%89%e8%a3%85%e4%b8%80%e4%ba%9b%e5%b8%b8%e7%94%a8%e7%9a%84%e5%b7%a5%e5%85%b7%e6%af%94%e5%a6%82ippingtelnetsstcpdump-%e7%ad%89%e5%91%bd%e4%bb%a4%e6%96%b9%e4%be%bf%e5%90%8e%e6%9c%9f%e4%bd%bf%e7%94%a8%e4%bd%86%e6%98%af%e8%bf%99%e4%b8%aa%e5%b0%b1%e8%bf%9d%e8%83%8c%e4%ba%86%e5%ae%b9%e5%99%a8%e5%8c%96%e7%9a%84%e6%9c%80%e5%b0%8f%e5%8c%96%e9%95%9c%e5%83%8f%e5%8e%9f%e5%88%99%e6%9c%ac%e8%ba%ab%e4%b8%8a%e5%ae%b9%e5%99%a8%e5%b0%b1%e6%98%af%e5%ae%9e%e7%8e%b0%e6%9b%b4%e8%bd%bb%e4%be%bf%e6%9b%b4%e5%bf%ab%e9%80%9f%e5%9c%b0%e5%90%af%e5%8a%a8%e4%b8%9a%e5%8a%a1%e5%ae%89%e8%a3%85%e4%b8%80%e4%ba%9b%e5%88%97%e5%b7%a5%e5%85%b7%e5%b0%b1%e5%a2%9e%e5%8a%a0%e4%ba%86%e9%95%9c%e5%83%8f%e5%a4%a7%e5%b0%8f">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-在容器pod-内直接安装所需的命令但是安装过程可能会比较困难有的时候容器内操作系统连-yumrpm-等安装工具都没有编译安装显然太浪费时间同时有可能离线环境没-yum-等特殊情况都会使得安装非常麻烦">
 • 在容器、Pod 内直接安装所需的命令，但是安装过程可能会比较困难，有的时候容器内操作系统连 &lt;code>yum&lt;/code>、&lt;code>rpm&lt;/code> 等安装工具都没有，编译安装显然太浪费时间。同时有可能离线环境，没 &lt;code>yum&lt;/code> 等特殊情况，都会使得安装非常麻烦。
 &lt;a class="anchor" href="#-%e5%9c%a8%e5%ae%b9%e5%99%a8pod-%e5%86%85%e7%9b%b4%e6%8e%a5%e5%ae%89%e8%a3%85%e6%89%80%e9%9c%80%e7%9a%84%e5%91%bd%e4%bb%a4%e4%bd%86%e6%98%af%e5%ae%89%e8%a3%85%e8%bf%87%e7%a8%8b%e5%8f%af%e8%83%bd%e4%bc%9a%e6%af%94%e8%be%83%e5%9b%b0%e9%9a%be%e6%9c%89%e7%9a%84%e6%97%b6%e5%80%99%e5%ae%b9%e5%99%a8%e5%86%85%e6%93%8d%e4%bd%9c%e7%b3%bb%e7%bb%9f%e8%bf%9e-yumrpm-%e7%ad%89%e5%ae%89%e8%a3%85%e5%b7%a5%e5%85%b7%e9%83%bd%e6%b2%a1%e6%9c%89%e7%bc%96%e8%af%91%e5%ae%89%e8%a3%85%e6%98%be%e7%84%b6%e5%a4%aa%e6%b5%aa%e8%b4%b9%e6%97%b6%e9%97%b4%e5%90%8c%e6%97%b6%e6%9c%89%e5%8f%af%e8%83%bd%e7%a6%bb%e7%ba%bf%e7%8e%af%e5%a2%83%e6%b2%a1-yum-%e7%ad%89%e7%89%b9%e6%ae%8a%e6%83%85%e5%86%b5%e9%83%bd%e4%bc%9a%e4%bd%bf%e5%be%97%e5%ae%89%e8%a3%85%e9%9d%9e%e5%b8%b8%e9%ba%bb%e7%83%a6">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-利用容器的运行原理使用-nsenter-来进入容器的对应-namespace-进行调试">
 • 利用容器的运行原理，使用 &lt;code>nsenter&lt;/code> 来进入容器的对应 &lt;code>namespace&lt;/code> 进行调试。
 &lt;a class="anchor" href="#-%e5%88%a9%e7%94%a8%e5%ae%b9%e5%99%a8%e7%9a%84%e8%bf%90%e8%a1%8c%e5%8e%9f%e7%90%86%e4%bd%bf%e7%94%a8-nsenter-%e6%9d%a5%e8%bf%9b%e5%85%a5%e5%ae%b9%e5%99%a8%e7%9a%84%e5%af%b9%e5%ba%94-namespace-%e8%bf%9b%e8%a1%8c%e8%b0%83%e8%af%95">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h5 id="所以显然第一第二两种方法不方便且不现实所以看看第三种方法是如何操作的">
 所以，显然第一、第二两种方法不方便且不现实，所以看看第三种方法是如何操作的。
 &lt;a class="anchor" href="#%e6%89%80%e4%bb%a5%e6%98%be%e7%84%b6%e7%ac%ac%e4%b8%80%e7%ac%ac%e4%ba%8c%e4%b8%a4%e7%a7%8d%e6%96%b9%e6%b3%95%e4%b8%8d%e6%96%b9%e4%be%bf%e4%b8%94%e4%b8%8d%e7%8e%b0%e5%ae%9e%e6%89%80%e4%bb%a5%e7%9c%8b%e7%9c%8b%e7%ac%ac%e4%b8%89%e7%a7%8d%e6%96%b9%e6%b3%95%e6%98%af%e5%a6%82%e4%bd%95%e6%93%8d%e4%bd%9c%e7%9a%84">#&lt;/a>
&lt;/h5>
&lt;hr>
&lt;h2 id="容器原理">
 容器原理
 &lt;a class="anchor" href="#%e5%ae%b9%e5%99%a8%e5%8e%9f%e7%90%86">#&lt;/a>
&lt;/h2>
&lt;h5 id="在介绍-nsenter-之前先见到说下容器的原理当我们用-docker-run-启动一个容器时实际上底层做的就是创建一个进程以及对应的-network-namespacemount-namespaceuts-namespaceipc-namespacepid-namspaceuser-namespace然后将这个进程加入到这些到命令空间同时给划分对应的-cgroup最后使用-chroot-将容器的文件系统切换为根目录这样就实现了这个进程与-root-namespace-的多维度隔离使得进入容器内就像是进入一个新的操作系统">
 在介绍 &lt;code>nsenter&lt;/code> 之前，先见到说下容器的原理，当我们用 &lt;code>docker run&lt;/code> 启动一个容器时，实际上底层做的就是创建一个进程以及对应的 network namespace、mount namespace、uts namespace、ipc namespace、pid namspace、user namespace，然后将这个进程加入到这些到命令空间，同时给划分对应的 cgroup，最后使用 &lt;code>chroot&lt;/code> 将容器的文件系统切换为根目录。这样就实现了这个进程与 &lt;code>root namespace&lt;/code> 的多维度隔离，使得进入容器内就像是进入一个新的操作系统。
 &lt;a class="anchor" href="#%e5%9c%a8%e4%bb%8b%e7%bb%8d-nsenter-%e4%b9%8b%e5%89%8d%e5%85%88%e8%a7%81%e5%88%b0%e8%af%b4%e4%b8%8b%e5%ae%b9%e5%99%a8%e7%9a%84%e5%8e%9f%e7%90%86%e5%bd%93%e6%88%91%e4%bb%ac%e7%94%a8-docker-run-%e5%90%af%e5%8a%a8%e4%b8%80%e4%b8%aa%e5%ae%b9%e5%99%a8%e6%97%b6%e5%ae%9e%e9%99%85%e4%b8%8a%e5%ba%95%e5%b1%82%e5%81%9a%e7%9a%84%e5%b0%b1%e6%98%af%e5%88%9b%e5%bb%ba%e4%b8%80%e4%b8%aa%e8%bf%9b%e7%a8%8b%e4%bb%a5%e5%8f%8a%e5%af%b9%e5%ba%94%e7%9a%84-network-namespacemount-namespaceuts-namespaceipc-namespacepid-namspaceuser-namespace%e7%84%b6%e5%90%8e%e5%b0%86%e8%bf%99%e4%b8%aa%e8%bf%9b%e7%a8%8b%e5%8a%a0%e5%85%a5%e5%88%b0%e8%bf%99%e4%ba%9b%e5%88%b0%e5%91%bd%e4%bb%a4%e7%a9%ba%e9%97%b4%e5%90%8c%e6%97%b6%e7%bb%99%e5%88%92%e5%88%86%e5%af%b9%e5%ba%94%e7%9a%84-cgroup%e6%9c%80%e5%90%8e%e4%bd%bf%e7%94%a8-chroot-%e5%b0%86%e5%ae%b9%e5%99%a8%e7%9a%84%e6%96%87%e4%bb%b6%e7%b3%bb%e7%bb%9f%e5%88%87%e6%8d%a2%e4%b8%ba%e6%a0%b9%e7%9b%ae%e5%bd%95%e8%bf%99%e6%a0%b7%e5%b0%b1%e5%ae%9e%e7%8e%b0%e4%ba%86%e8%bf%99%e4%b8%aa%e8%bf%9b%e7%a8%8b%e4%b8%8e-root-namespace-%e7%9a%84%e5%a4%9a%e7%bb%b4%e5%ba%a6%e9%9a%94%e7%a6%bb%e4%bd%bf%e5%be%97%e8%bf%9b%e5%85%a5%e5%ae%b9%e5%99%a8%e5%86%85%e5%b0%b1%e5%83%8f%e6%98%af%e8%bf%9b%e5%85%a5%e4%b8%80%e4%b8%aa%e6%96%b0%e7%9a%84%e6%93%8d%e4%bd%9c%e7%b3%bb%e7%bb%9f">#&lt;/a>
&lt;/h5>
&lt;h5 id="所以说容器就是一个进程只不过他都加入到不同的命名空间下了">
 所以说容器就是一个进程，只不过他都加入到不同的命名空间下了。
 &lt;a class="anchor" href="#%e6%89%80%e4%bb%a5%e8%af%b4%e5%ae%b9%e5%99%a8%e5%b0%b1%e6%98%af%e4%b8%80%e4%b8%aa%e8%bf%9b%e7%a8%8b%e5%8f%aa%e4%b8%8d%e8%bf%87%e4%bb%96%e9%83%bd%e5%8a%a0%e5%85%a5%e5%88%b0%e4%b8%8d%e5%90%8c%e7%9a%84%e5%91%bd%e5%90%8d%e7%a9%ba%e9%97%b4%e4%b8%8b%e4%ba%86">#&lt;/a>
&lt;/h5>
&lt;hr>
&lt;h2 id="nsenter-简介">
 nsenter 简介
 &lt;a class="anchor" href="#nsenter-%e7%ae%80%e4%bb%8b">#&lt;/a>
&lt;/h2>
&lt;h5 id="nsenter-是一个-linux-命令行工具作用是可以进入-linux-系统下某个进程的命令空间如-network-namespacemount-namespaceuts-namespaceipc-namespacepid-namspaceuser-namespacecgroup">
 &lt;code>nsenter&lt;/code> 是一个 Linux 命令行工具，作用是可以进入 Linux 系统下某个进程的命令空间，如 network namespace、mount namespace、uts namespace、ipc namespace、pid namspace、user namespace、cgroup。
 &lt;a class="anchor" href="#nsenter-%e6%98%af%e4%b8%80%e4%b8%aa-linux-%e5%91%bd%e4%bb%a4%e8%a1%8c%e5%b7%a5%e5%85%b7%e4%bd%9c%e7%94%a8%e6%98%af%e5%8f%af%e4%bb%a5%e8%bf%9b%e5%85%a5-linux-%e7%b3%bb%e7%bb%9f%e4%b8%8b%e6%9f%90%e4%b8%aa%e8%bf%9b%e7%a8%8b%e7%9a%84%e5%91%bd%e4%bb%a4%e7%a9%ba%e9%97%b4%e5%a6%82-network-namespacemount-namespaceuts-namespaceipc-namespacepid-namspaceuser-namespacecgroup">#&lt;/a>
&lt;/h5>
&lt;h5 id="所以使用-nsenter-调试容器网络可以按照以下步骤操作">
 所以使用 &lt;code>nsenter&lt;/code> 调试容器网络，可以按照以下步骤操作：
 &lt;a class="anchor" href="#%e6%89%80%e4%bb%a5%e4%bd%bf%e7%94%a8-nsenter-%e8%b0%83%e8%af%95%e5%ae%b9%e5%99%a8%e7%bd%91%e7%bb%9c%e5%8f%af%e4%bb%a5%e6%8c%89%e7%85%a7%e4%bb%a5%e4%b8%8b%e6%ad%a5%e9%aa%a4%e6%93%8d%e4%bd%9c">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="-在-root-namespace-下找到容器的-pid也就是这个容器在-root-namespace-下的进程号">
 • 在 &lt;code>root namespace&lt;/code> 下找到容器的 Pid，也就是这个容器在 &lt;code>root namespace&lt;/code> 下的进程号
 &lt;a class="anchor" href="#-%e5%9c%a8-root-namespace-%e4%b8%8b%e6%89%be%e5%88%b0%e5%ae%b9%e5%99%a8%e7%9a%84-pid%e4%b9%9f%e5%b0%b1%e6%98%af%e8%bf%99%e4%b8%aa%e5%ae%b9%e5%99%a8%e5%9c%a8-root-namespace-%e4%b8%8b%e7%9a%84%e8%bf%9b%e7%a8%8b%e5%8f%b7">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-使用-nsenter-进入到该-pid-的-network-namespace-即可这样就保证了当前的环境是容器的网络环境但是文件系统还是在-root-namespace-下以及-useruts-等命名空间都还是在-root-namespace-下所以就可以使用-root-namespace-下的调试命令来进行调试了">
 • 使用 &lt;code>nsenter&lt;/code> 进入到该 Pid 的 &lt;code>network namespace&lt;/code> 即可，这样就保证了当前的环境是容器的网络环境，但是文件系统还是在 &lt;code>root namespace&lt;/code> 下，以及 user、uts 等命名空间都还是在 &lt;code>root namespace&lt;/code> 下。所以就可以使用 &lt;code>root namespace&lt;/code> 下的调试命令来进行调试了。
 &lt;a class="anchor" href="#-%e4%bd%bf%e7%94%a8-nsenter-%e8%bf%9b%e5%85%a5%e5%88%b0%e8%af%a5-pid-%e7%9a%84-network-namespace-%e5%8d%b3%e5%8f%af%e8%bf%99%e6%a0%b7%e5%b0%b1%e4%bf%9d%e8%af%81%e4%ba%86%e5%bd%93%e5%89%8d%e7%9a%84%e7%8e%af%e5%a2%83%e6%98%af%e5%ae%b9%e5%99%a8%e7%9a%84%e7%bd%91%e7%bb%9c%e7%8e%af%e5%a2%83%e4%bd%86%e6%98%af%e6%96%87%e4%bb%b6%e7%b3%bb%e7%bb%9f%e8%bf%98%e6%98%af%e5%9c%a8-root-namespace-%e4%b8%8b%e4%bb%a5%e5%8f%8a-useruts-%e7%ad%89%e5%91%bd%e5%90%8d%e7%a9%ba%e9%97%b4%e9%83%bd%e8%bf%98%e6%98%af%e5%9c%a8-root-namespace-%e4%b8%8b%e6%89%80%e4%bb%a5%e5%b0%b1%e5%8f%af%e4%bb%a5%e4%bd%bf%e7%94%a8-root-namespace-%e4%b8%8b%e7%9a%84%e8%b0%83%e8%af%95%e5%91%bd%e4%bb%a4%e6%9d%a5%e8%bf%9b%e8%a1%8c%e8%b0%83%e8%af%95%e4%ba%86">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h5 id="nsenter-位于-util-linux-包中一般常用的-linux-发行版都已经默认安装如果你的系统没有安装可以使用以下命令进行安装">
 &lt;code>nsenter&lt;/code> 位于 &lt;code>util-linux&lt;/code> 包中，一般常用的 Linux 发行版都已经默认安装。如果你的系统没有安装，可以使用以下命令进行安装：
 &lt;a class="anchor" href="#nsenter-%e4%bd%8d%e4%ba%8e-util-linux-%e5%8c%85%e4%b8%ad%e4%b8%80%e8%88%ac%e5%b8%b8%e7%94%a8%e7%9a%84-linux-%e5%8f%91%e8%a1%8c%e7%89%88%e9%83%bd%e5%b7%b2%e7%bb%8f%e9%bb%98%e8%ae%a4%e5%ae%89%e8%a3%85%e5%a6%82%e6%9e%9c%e4%bd%a0%e7%9a%84%e7%b3%bb%e7%bb%9f%e6%b2%a1%e6%9c%89%e5%ae%89%e8%a3%85%e5%8f%af%e4%bb%a5%e4%bd%bf%e7%94%a8%e4%bb%a5%e4%b8%8b%e5%91%bd%e4%bb%a4%e8%bf%9b%e8%a1%8c%e5%ae%89%e8%a3%85">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code># Centos 
$ yum install util-linux
&lt;/code>&lt;/pre>&lt;h5 id="使用-nsenter-----help-查看-nsenter-用法">
 使用 &lt;code>nsenter - - help&lt;/code> 查看 &lt;code>nsenter&lt;/code> 用法。
 &lt;a class="anchor" href="#%e4%bd%bf%e7%94%a8-nsenter-----help-%e6%9f%a5%e7%9c%8b-nsenter-%e7%94%a8%e6%b3%95">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>$ nsenter --help

用法：
 nsenter [选项] [&amp;lt;程序&amp;gt; [&amp;lt;参数&amp;gt;...]]

以其他程序的名字空间运行某个程序。

选项：
 -a, --all enter all namespaces
 -t, --target &amp;lt;pid&amp;gt; 要获取名字空间的目标进程
 -m, --mount[=&amp;lt;文件&amp;gt;] 进入 mount 名字空间
 -u, --uts[=&amp;lt;文件&amp;gt;] 进入 UTS 名字空间(主机名等)
 -i, --ipc[=&amp;lt;文件&amp;gt;] 进入 System V IPC 名字空间
 -n, --net[=&amp;lt;文件&amp;gt;] 进入网络名字空间
 -p, --pid[=&amp;lt;文件&amp;gt;] 进入 pid 名字空间
 -C, --cgroup[=&amp;lt;文件&amp;gt;] 进入 cgroup 名字空间
 -U, --user[=&amp;lt;文件&amp;gt;] 进入用户名字空间
 -S, --setuid &amp;lt;uid&amp;gt; 设置进入空间中的 uid
 -G, --setgid &amp;lt;gid&amp;gt; 设置进入名字空间中的 gid
 --preserve-credentials 不干涉 uid 或 gid
 -r, --root[=&amp;lt;目录&amp;gt;] 设置根目录
 -w, --wd[=&amp;lt;dir&amp;gt;] 设置工作目录
 -F, --no-fork 执行 &amp;lt;程序&amp;gt; 前不 fork
 -Z, --follow-context 根据 --target PID 设置 SELinux 环境

 -h, --help display this help
 -V, --version display version
&lt;/code>&lt;/pre>&lt;h5 id="上面介绍了-nsenter-的原理下面就实际演示一下下面演示两个场景都是在工作中非常常见的">
 上面介绍了 &lt;code>nsenter&lt;/code> 的原理，下面就实际演示一下，下面演示两个场景，都是在工作中非常常见的。
 &lt;a class="anchor" href="#%e4%b8%8a%e9%9d%a2%e4%bb%8b%e7%bb%8d%e4%ba%86-nsenter-%e7%9a%84%e5%8e%9f%e7%90%86%e4%b8%8b%e9%9d%a2%e5%b0%b1%e5%ae%9e%e9%99%85%e6%bc%94%e7%a4%ba%e4%b8%80%e4%b8%8b%e4%b8%8b%e9%9d%a2%e6%bc%94%e7%a4%ba%e4%b8%a4%e4%b8%aa%e5%9c%ba%e6%99%af%e9%83%bd%e6%98%af%e5%9c%a8%e5%b7%a5%e4%bd%9c%e4%b8%ad%e9%9d%9e%e5%b8%b8%e5%b8%b8%e8%a7%81%e7%9a%84">#&lt;/a>
&lt;/h5>
&lt;hr>
&lt;h2 id="调试容器网络">
 调试容器网络
 &lt;a class="anchor" href="#%e8%b0%83%e8%af%95%e5%ae%b9%e5%99%a8%e7%bd%91%e7%bb%9c">#&lt;/a>
&lt;/h2>
&lt;h5 id="当使用-docker-run-启动一个容器时容器运行无报错即容器不在重启的情况下这种情况直接使用-nsenter-进入可以">
 当使用 &lt;code>docker run&lt;/code> 启动一个容器时，容器运行无报错，即容器不在重启的情况下。这种情况直接使用 &lt;code>nsenter&lt;/code> 进入可以。
 &lt;a class="anchor" href="#%e5%bd%93%e4%bd%bf%e7%94%a8-docker-run-%e5%90%af%e5%8a%a8%e4%b8%80%e4%b8%aa%e5%ae%b9%e5%99%a8%e6%97%b6%e5%ae%b9%e5%99%a8%e8%bf%90%e8%a1%8c%e6%97%a0%e6%8a%a5%e9%94%99%e5%8d%b3%e5%ae%b9%e5%99%a8%e4%b8%8d%e5%9c%a8%e9%87%8d%e5%90%af%e7%9a%84%e6%83%85%e5%86%b5%e4%b8%8b%e8%bf%99%e7%a7%8d%e6%83%85%e5%86%b5%e7%9b%b4%e6%8e%a5%e4%bd%bf%e7%94%a8-nsenter-%e8%bf%9b%e5%85%a5%e5%8f%af%e4%bb%a5">#&lt;/a>
&lt;/h5>
&lt;h5 id="先进入容器内-curl-wwwbaiducom发现容器内没有-curl-命令">
 先进入容器内 &lt;code>curl www.baidu.com&lt;/code>，发现容器内没有 &lt;code>curl&lt;/code> 命令
 &lt;a class="anchor" href="#%e5%85%88%e8%bf%9b%e5%85%a5%e5%ae%b9%e5%99%a8%e5%86%85-curl-wwwbaiducom%e5%8f%91%e7%8e%b0%e5%ae%b9%e5%99%a8%e5%86%85%e6%b2%a1%e6%9c%89-curl-%e5%91%bd%e4%bb%a4">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>$ docker run -it alpine-amd64:3.11 sh
$ curl http://www.baidu.com
sh: curl: not found
&lt;/code>&lt;/pre>&lt;h5 id="下面使用-nsenter-进行调试">
 下面使用 &lt;code>nsenter&lt;/code> 进行调试
 &lt;a class="anchor" href="#%e4%b8%8b%e9%9d%a2%e4%bd%bf%e7%94%a8-nsenter-%e8%bf%9b%e8%a1%8c%e8%b0%83%e8%af%95">#&lt;/a>
&lt;/h5>
&lt;h5 id="1获取容器-pid即-3448">
 1、获取容器 Pid，即 3448
 &lt;a class="anchor" href="#1%e8%8e%b7%e5%8f%96%e5%ae%b9%e5%99%a8-pid%e5%8d%b3-3448">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>$ docker inspect fd9ec0381062 | grep Pid
 &amp;#34;Pid&amp;#34;: 3448,
 &amp;#34;PidMode&amp;#34;: &amp;#34;&amp;#34;,
 &amp;#34;PidsLimit&amp;#34;: null,
&lt;/code>&lt;/pre>&lt;h5 id="2使用-nsenter-进入该-pid-的-network-namespace">
 2、使用 &lt;code>nsenter&lt;/code> 进入该 Pid 的 &lt;code>network namespace&lt;/code>
 &lt;a class="anchor" href="#2%e4%bd%bf%e7%94%a8-nsenter-%e8%bf%9b%e5%85%a5%e8%af%a5-pid-%e7%9a%84-network-namespace">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code># -t 表示目标进程号, -n 表示进入 network namespace
$ nsenter -t 3448 -n
&lt;/code>&lt;/pre>&lt;h5 id="3查看当前的网络环境再使用-curl发现正常返回">
 3、查看当前的网络环境，再使用 &lt;code>curl&lt;/code>，发现正常返回
 &lt;a class="anchor" href="#3%e6%9f%a5%e7%9c%8b%e5%bd%93%e5%89%8d%e7%9a%84%e7%bd%91%e7%bb%9c%e7%8e%af%e5%a2%83%e5%86%8d%e4%bd%bf%e7%94%a8-curl%e5%8f%91%e7%8e%b0%e6%ad%a3%e5%b8%b8%e8%bf%94%e5%9b%9e">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code># 查看当前网络环境，可以确认是容器内的网络
$ ip addr
1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
 inet 127.0.0.1/8 scope host lo
 valid_lft forever preferred_lft forever
12: eth0@if13: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UP group default 
 link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0
 inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0
 valid_lft forever preferred_lft forever
# 再次使用 curl, 发现有命令
$ curl http://www.baidu.com
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;!--STATUS OK--&amp;gt;&amp;lt;html&amp;gt; &amp;lt;head&amp;gt;&amp;lt;meta http-equiv=content-ty......
&lt;/code>&lt;/pre>&lt;hr>
&lt;h2 id="调试-pod-网络">
 调试 Pod 网络
 &lt;a class="anchor" href="#%e8%b0%83%e8%af%95-pod-%e7%bd%91%e7%bb%9c">#&lt;/a>
&lt;/h2>
&lt;h5 id="实际上-pod-网络与容器网络是一样的下面看两个场景">
 实际上 Pod 网络与容器网络是一样的，下面看两个场景
 &lt;a class="anchor" href="#%e5%ae%9e%e9%99%85%e4%b8%8a-pod-%e7%bd%91%e7%bb%9c%e4%b8%8e%e5%ae%b9%e5%99%a8%e7%bd%91%e7%bb%9c%e6%98%af%e4%b8%80%e6%a0%b7%e7%9a%84%e4%b8%8b%e9%9d%a2%e7%9c%8b%e4%b8%a4%e4%b8%aa%e5%9c%ba%e6%99%af">#&lt;/a>
&lt;/h5>
&lt;h3 id="pod-running-状态">
 Pod running 状态
 &lt;a class="anchor" href="#pod-running-%e7%8a%b6%e6%80%81">#&lt;/a>
&lt;/h3>
&lt;h5 id="当一个-pod-运行状态是-running-时其调试方式和上面的-docker-调试方式一样直接进入容器的-network-namespace">
 当一个 Pod 运行状态是 &lt;code>running&lt;/code> 时，其调试方式和上面的 docker 调试方式一样，直接进入容器的 &lt;code>network namespace&lt;/code>。
 &lt;a class="anchor" href="#%e5%bd%93%e4%b8%80%e4%b8%aa-pod-%e8%bf%90%e8%a1%8c%e7%8a%b6%e6%80%81%e6%98%af-running-%e6%97%b6%e5%85%b6%e8%b0%83%e8%af%95%e6%96%b9%e5%bc%8f%e5%92%8c%e4%b8%8a%e9%9d%a2%e7%9a%84-docker-%e8%b0%83%e8%af%95%e6%96%b9%e5%bc%8f%e4%b8%80%e6%a0%b7%e7%9b%b4%e6%8e%a5%e8%bf%9b%e5%85%a5%e5%ae%b9%e5%99%a8%e7%9a%84-network-namespace">#&lt;/a>
&lt;/h5>
&lt;h5 id="1找到-pod-的运行结点即-master-172-31-97-104">
 1、找到 Pod 的运行结点，即 &lt;code>master-172-31-97-104&lt;/code>
 &lt;a class="anchor" href="#1%e6%89%be%e5%88%b0-pod-%e7%9a%84%e8%bf%90%e8%a1%8c%e7%bb%93%e7%82%b9%e5%8d%b3-master-172-31-97-104">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>$ kubectl get pods -A -o wide | grep test-nsenter
NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES
test test-nsenter-7df7d5fff7-5f666 1/1 Running 0 4m21s 100.121.45.129 master-172-31-97-104 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
&lt;/code>&lt;/pre>&lt;h5 id="2去-master-172-31-97-104-结点获取容器-pid会发现有两个-test-nsenter-容器其中一个是业务容器另一个是-k8s-起的-pause-容器用于共享容器网络">
 2、去 &lt;code>master-172-31-97-104&lt;/code> 结点获取容器 &lt;code>Pid&lt;/code>。会发现有两个 &lt;code>test-nsenter&lt;/code> 容器，其中一个是业务容器，另一个是 K8S 起的 &lt;code>pause&lt;/code> 容器用于共享容器网络。
 &lt;a class="anchor" href="#2%e5%8e%bb-master-172-31-97-104-%e7%bb%93%e7%82%b9%e8%8e%b7%e5%8f%96%e5%ae%b9%e5%99%a8-pid%e4%bc%9a%e5%8f%91%e7%8e%b0%e6%9c%89%e4%b8%a4%e4%b8%aa-test-nsenter-%e5%ae%b9%e5%99%a8%e5%85%b6%e4%b8%ad%e4%b8%80%e4%b8%aa%e6%98%af%e4%b8%9a%e5%8a%a1%e5%ae%b9%e5%99%a8%e5%8f%a6%e4%b8%80%e4%b8%aa%e6%98%af-k8s-%e8%b5%b7%e7%9a%84-pause-%e5%ae%b9%e5%99%a8%e7%94%a8%e4%ba%8e%e5%85%b1%e4%ba%ab%e5%ae%b9%e5%99%a8%e7%bd%91%e7%bb%9c">#&lt;/a>
&lt;/h5>
&lt;h5 id="直接获取业务容器的-pid即-48344">
 直接获取业务容器的 &lt;code>Pid&lt;/code>，即 48344
 &lt;a class="anchor" href="#%e7%9b%b4%e6%8e%a5%e8%8e%b7%e5%8f%96%e4%b8%9a%e5%8a%a1%e5%ae%b9%e5%99%a8%e7%9a%84-pid%e5%8d%b3-48344">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>$ docker ps | grep test-nsenter
f5fdbd788a8e test-nsenter:latest &amp;#34;sleep 300&amp;#34; 6 minutes ago Up 6 minutes k8s_test-nsenter-c5577484c-wlndj_test_516c4915-0fa9-4d1f-a4c3-612b1ab02c13_0
b387d915a853 sea.hub:5000/pause:3.5 &amp;#34;/pause&amp;#34; 7 minutes ago Up 7 minutes k8s_POD_test-nsneter-c5577484c-wlndj_test_516c4915-0fa9-4d1f-a4c3-612b1ab02c13_2
$ docker inspect f5fdbd788a8e | grep Pid
 &amp;#34;Pid&amp;#34;: 48344,
 &amp;#34;PidMode&amp;#34;: &amp;#34;&amp;#34;,
 &amp;#34;PidsLimit&amp;#34;: null,
&lt;/code>&lt;/pre>&lt;h5 id="3nsenter-进入该-pid-的-network-namespace-中使用-curl">
 3、&lt;code>nsenter&lt;/code> 进入该 &lt;code>Pid&lt;/code> 的 &lt;code>network namespace&lt;/code> 中，使用 &lt;code>curl&lt;/code>
 &lt;a class="anchor" href="#3nsenter-%e8%bf%9b%e5%85%a5%e8%af%a5-pid-%e7%9a%84-network-namespace-%e4%b8%ad%e4%bd%bf%e7%94%a8-curl">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>$ nsenter -t 48344 -n
# 查看当前网络环境，可以确认是容器内的网络
$ ip addr
1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
 inet 127.0.0.1/8 scope host lo
 valid_lft forever preferred_lft forever
3: eth0@if101: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1480 qdisc noqueue state UP group default 
 link/ether 32:b3:d0:37:ad:e9 brd ff:ff:ff:ff:ff:ff link-netnsid 0
 inet 100.118.171.133/32 scope global eth0
 valid_lft forever preferred_lft forever
4: tunl0@NONE: &amp;lt;NOARP&amp;gt; mtu 1480 qdisc noop state DOWN group default qlen 1000
 link/ipip 0.0.0.0 brd 0.0.0.0
# 再次使用 curl, 发现有命令
$ curl http://www.baidu.com
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;!--STATUS OK--&amp;gt;&amp;lt;html&amp;gt; &amp;lt;head&amp;gt;&amp;lt;meta http-equiv=content-ty......
&lt;/code>&lt;/pre>&lt;h3 id="pod-crashlookbackoff-状态">
 Pod crashLookBackOff 状态
 &lt;a class="anchor" href="#pod-crashlookbackoff-%e7%8a%b6%e6%80%81">#&lt;/a>
&lt;/h3>
&lt;h5 id="上面演示的容器都是-running-状态可以在结点上找到对应-pid但是如果-pod-一直在重启则-pid-一直都在变所以调试也会不断中断">
 上面演示的容器都是 &lt;code>running&lt;/code> 状态，可以在结点上找到对应 &lt;code>Pid&lt;/code>，但是如果 Pod 一直在重启，则 &lt;code>Pid&lt;/code> 一直都在变，所以调试也会不断中断。
 &lt;a class="anchor" href="#%e4%b8%8a%e9%9d%a2%e6%bc%94%e7%a4%ba%e7%9a%84%e5%ae%b9%e5%99%a8%e9%83%bd%e6%98%af-running-%e7%8a%b6%e6%80%81%e5%8f%af%e4%bb%a5%e5%9c%a8%e7%bb%93%e7%82%b9%e4%b8%8a%e6%89%be%e5%88%b0%e5%af%b9%e5%ba%94-pid%e4%bd%86%e6%98%af%e5%a6%82%e6%9e%9c-pod-%e4%b8%80%e7%9b%b4%e5%9c%a8%e9%87%8d%e5%90%af%e5%88%99-pid-%e4%b8%80%e7%9b%b4%e9%83%bd%e5%9c%a8%e5%8f%98%e6%89%80%e4%bb%a5%e8%b0%83%e8%af%95%e4%b9%9f%e4%bc%9a%e4%b8%8d%e6%96%ad%e4%b8%ad%e6%96%ad">#&lt;/a>
&lt;/h5>
&lt;h5 id="所以当-pod-处于crashlookbackoff-状态-时可以进入-pod-pause-容器因为-pause-容器与业务容器是共享网络的而且永远不会重启除非-pod-被删除了">
 所以当 Pod 处于&lt;code>crashLookBackOff&lt;/code> 状态 时，可以进入 Pod &lt;code>Pause&lt;/code> 容器，因为 &lt;code>Pause&lt;/code> 容器与业务容器是共享网络的，而且永远不会重启，除非 Pod 被删除了。
 &lt;a class="anchor" href="#%e6%89%80%e4%bb%a5%e5%bd%93-pod-%e5%a4%84%e4%ba%8ecrashlookbackoff-%e7%8a%b6%e6%80%81-%e6%97%b6%e5%8f%af%e4%bb%a5%e8%bf%9b%e5%85%a5-pod-pause-%e5%ae%b9%e5%99%a8%e5%9b%a0%e4%b8%ba-pause-%e5%ae%b9%e5%99%a8%e4%b8%8e%e4%b8%9a%e5%8a%a1%e5%ae%b9%e5%99%a8%e6%98%af%e5%85%b1%e4%ba%ab%e7%bd%91%e7%bb%9c%e7%9a%84%e8%80%8c%e4%b8%94%e6%b0%b8%e8%bf%9c%e4%b8%8d%e4%bc%9a%e9%87%8d%e5%90%af%e9%99%a4%e9%9d%9e-pod-%e8%a2%ab%e5%88%a0%e9%99%a4%e4%ba%86">#&lt;/a>
&lt;/h5>
&lt;h5 id="1进入-pod-crashlookbackoff-状态-的容器-network-namespace">
 1、进入 Pod &lt;code>crashLookBackOff&lt;/code> 状态 的容器 &lt;code>network namespace&lt;/code>
 &lt;a class="anchor" href="#1%e8%bf%9b%e5%85%a5-pod-crashlookbackoff-%e7%8a%b6%e6%80%81-%e7%9a%84%e5%ae%b9%e5%99%a8-network-namespace">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code># 发现只有pause 容器，因为业务容器一直在重启
$ docker ps|grep test-nsenter
70e8079e82ed sea.hub:5000/pause:3.5 &amp;#34;/pause&amp;#34; 39 seconds ago up 38 seconds k8s_POD_test-nsenter-7cdf977947-thk57_test_9fedbb55-4726-4f13-a669-d5bcb0b19b94_0
# 查看 Pause 容器的 Pid
$ docker inspect 70e8079e82ed |grep Pid
 &amp;#34;Pid&amp;#34;: 14213,
 &amp;#34;PidMode&amp;#34;: &amp;#34;&amp;#34;,
 &amp;#34;PidsLimit&amp;#34;: null, 
# nsenter 进入 Pause 容器的 network namespace
$ nsneter -t 14213 -n 
# 查看 pause 容器的网络, 和 Pod 网络一致
$ ip addr
1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
 inet 127.0.0.1/8 scope host lo
 valid_lft forever preferred_lft forever
2: tunl0@NONE: &amp;lt;NOARP&amp;gt; mtu 1480 qdisc noop state DOWN group default qlen 1000
 link/ipip 0.0.0.0 brd 0.0.0.0
4: eth0@if68: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1480 qdisc noqueue state UP group default 
 link/ether c2:64:99:f0:a6:f1 brd ff:ff:ff:ff:ff:ff link-netnsid 0
 inet 100.121.45.130/32 scope global eth0
 valid_lft forever preferred_lft forever 
# 再次使用 curl, 发现有命令
$ curl http://www.baidu.com
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;!--STATUS OK--&amp;gt;&amp;lt;html&amp;gt; &amp;lt;head&amp;gt;&amp;lt;meta http-equiv=content-ty...... 
&lt;/code>&lt;/pre>&lt;h5 id="所以说当使用-nsenter-调试-pod-网络时不管-pod-状态如何我们直接进入其-pause-容器的-network-namespace-即可">
 所以说，当使用 &lt;code>nsenter&lt;/code> 调试 Pod 网络时，不管 Pod 状态如何，我们直接进入其 &lt;code>Pause&lt;/code> 容器的 &lt;code>network namespace&lt;/code> 即可。
 &lt;a class="anchor" href="#%e6%89%80%e4%bb%a5%e8%af%b4%e5%bd%93%e4%bd%bf%e7%94%a8-nsenter-%e8%b0%83%e8%af%95-pod-%e7%bd%91%e7%bb%9c%e6%97%b6%e4%b8%8d%e7%ae%a1-pod-%e7%8a%b6%e6%80%81%e5%a6%82%e4%bd%95%e6%88%91%e4%bb%ac%e7%9b%b4%e6%8e%a5%e8%bf%9b%e5%85%a5%e5%85%b6-pause-%e5%ae%b9%e5%99%a8%e7%9a%84-network-namespace-%e5%8d%b3%e5%8f%af">#&lt;/a>
&lt;/h5>
&lt;hr>
&lt;h2 id="总结">
 总结
 &lt;a class="anchor" href="#%e6%80%bb%e7%bb%93">#&lt;/a>
&lt;/h2>
&lt;h5 id="nsenter-非常便捷地帮助我们调试容器环境下和-k8s-环境下的网络调试也可以调试其他问题nsenter-使用也非常简单是一个非常好用的调试工具很好地解决了容器镜像缺少命令行工具的问题">
 nsenter 非常便捷地帮助我们调试容器环境下和 K8S 环境下的网络调试，也可以调试其他问题。&lt;code>nsenter&lt;/code> 使用也非常简单，是一个非常好用的调试工具，很好地解决了容器镜像缺少命令行工具的问题。
 &lt;a class="anchor" href="#nsenter-%e9%9d%9e%e5%b8%b8%e4%be%bf%e6%8d%b7%e5%9c%b0%e5%b8%ae%e5%8a%a9%e6%88%91%e4%bb%ac%e8%b0%83%e8%af%95%e5%ae%b9%e5%99%a8%e7%8e%af%e5%a2%83%e4%b8%8b%e5%92%8c-k8s-%e7%8e%af%e5%a2%83%e4%b8%8b%e7%9a%84%e7%bd%91%e7%bb%9c%e8%b0%83%e8%af%95%e4%b9%9f%e5%8f%af%e4%bb%a5%e8%b0%83%e8%af%95%e5%85%b6%e4%bb%96%e9%97%ae%e9%a2%98nsenter-%e4%bd%bf%e7%94%a8%e4%b9%9f%e9%9d%9e%e5%b8%b8%e7%ae%80%e5%8d%95%e6%98%af%e4%b8%80%e4%b8%aa%e9%9d%9e%e5%b8%b8%e5%a5%bd%e7%94%a8%e7%9a%84%e8%b0%83%e8%af%95%e5%b7%a5%e5%85%b7%e5%be%88%e5%a5%bd%e5%9c%b0%e8%a7%a3%e5%86%b3%e4%ba%86%e5%ae%b9%e5%99%a8%e9%95%9c%e5%83%8f%e7%bc%ba%e5%b0%91%e5%91%bd%e4%bb%a4%e8%a1%8c%e5%b7%a5%e5%85%b7%e7%9a%84%e9%97%ae%e9%a2%98">#&lt;/a>
&lt;/h5>
&lt;p>除了调试网络，也可以调试容5器的 &lt;code>ipc&lt;/code>、&lt;code>mount&lt;/code> 等，可以根据场景自行演示。&lt;/p></description></item><item><title>2024-04-03 容器中域名解析以及不同dnspolicy对域名解析的影响</title><link>https://qq547475331.github.io/docs/%E5%AE%B9%E5%99%A8%E4%B8%AD%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90%E4%BB%A5%E5%8F%8A%E4%B8%8D%E5%90%8Cdnspolicy%E5%AF%B9%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90%E7%9A%84%E5%BD%B1%E5%93%8D-rong-qi-zhong-yu-ming-jie-xi-yi-ji-bu-tong-dnspolicy-dui-yu-ming-jie-xi-de-ying-xiang/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/%E5%AE%B9%E5%99%A8%E4%B8%AD%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90%E4%BB%A5%E5%8F%8A%E4%B8%8D%E5%90%8Cdnspolicy%E5%AF%B9%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90%E7%9A%84%E5%BD%B1%E5%93%8D-rong-qi-zhong-yu-ming-jie-xi-yi-ji-bu-tong-dnspolicy-dui-yu-ming-jie-xi-de-ying-xiang/</guid><description>&lt;h1 id="一coredns背景">
 一、coreDNS背景
 &lt;a class="anchor" href="#%e4%b8%80coredns%e8%83%8c%e6%99%af">#&lt;/a>
&lt;/h1>
&lt;h5 id="部署在kubernetes集群中的容器业务通过coredns服务解析域名coredns基于caddy框架将整个coredns服务都建立在一个使用go编写的http2-web-服务器caddy上通过插件化链架构以预配置的方式configmap卷挂载内容配置选择需要的插件编译按序执行插件链上的逻辑通过四种方式tcpudpgrpc和https对外直接提供dns服务">
 部署在kubernetes集群中的容器业务通过coreDNS服务解析域名，Coredns基于caddy框架，将整个CoreDNS服务都建立在一个使用Go编写的HTTP/2 Web 服务器Caddy上。通过插件化（链）架构，以预配置的方式（configmap卷挂载内容配置）选择需要的插件编译，按序执行插件链上的逻辑，通过四种方式（TCP、UDP、gRPC和HTTPS）对外直接提供DNS服务。
 &lt;a class="anchor" href="#%e9%83%a8%e7%bd%b2%e5%9c%a8kubernetes%e9%9b%86%e7%be%a4%e4%b8%ad%e7%9a%84%e5%ae%b9%e5%99%a8%e4%b8%9a%e5%8a%a1%e9%80%9a%e8%bf%87coredns%e6%9c%8d%e5%8a%a1%e8%a7%a3%e6%9e%90%e5%9f%9f%e5%90%8dcoredns%e5%9f%ba%e4%ba%8ecaddy%e6%a1%86%e6%9e%b6%e5%b0%86%e6%95%b4%e4%b8%aacoredns%e6%9c%8d%e5%8a%a1%e9%83%bd%e5%bb%ba%e7%ab%8b%e5%9c%a8%e4%b8%80%e4%b8%aa%e4%bd%bf%e7%94%a8go%e7%bc%96%e5%86%99%e7%9a%84http2-web-%e6%9c%8d%e5%8a%a1%e5%99%a8caddy%e4%b8%8a%e9%80%9a%e8%bf%87%e6%8f%92%e4%bb%b6%e5%8c%96%e9%93%be%e6%9e%b6%e6%9e%84%e4%bb%a5%e9%a2%84%e9%85%8d%e7%bd%ae%e7%9a%84%e6%96%b9%e5%bc%8fconfigmap%e5%8d%b7%e6%8c%82%e8%bd%bd%e5%86%85%e5%ae%b9%e9%85%8d%e7%bd%ae%e9%80%89%e6%8b%a9%e9%9c%80%e8%a6%81%e7%9a%84%e6%8f%92%e4%bb%b6%e7%bc%96%e8%af%91%e6%8c%89%e5%ba%8f%e6%89%a7%e8%a1%8c%e6%8f%92%e4%bb%b6%e9%93%be%e4%b8%8a%e7%9a%84%e9%80%bb%e8%be%91%e9%80%9a%e8%bf%87%e5%9b%9b%e7%a7%8d%e6%96%b9%e5%bc%8ftcpudpgrpc%e5%92%8chttps%e5%af%b9%e5%a4%96%e7%9b%b4%e6%8e%a5%e6%8f%90%e4%be%9bdns%e6%9c%8d%e5%8a%a1">#&lt;/a>
&lt;/h5>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041103699.png" alt="image-20240304110320602" />&lt;/p>
&lt;h1 id="二kubelet通过修改容器etcresolvconf文件使得容器中可解析域名">
 二、kubelet通过修改容器/etc/resolv.conf文件使得容器中可解析域名
 &lt;a class="anchor" href="#%e4%ba%8ckubelet%e9%80%9a%e8%bf%87%e4%bf%ae%e6%94%b9%e5%ae%b9%e5%99%a8etcresolvconf%e6%96%87%e4%bb%b6%e4%bd%bf%e5%be%97%e5%ae%b9%e5%99%a8%e4%b8%ad%e5%8f%af%e8%a7%a3%e6%9e%90%e5%9f%9f%e5%90%8d">#&lt;/a>
&lt;/h1>
&lt;h5 id="在kubernetes集群中coredns服务和kube-apiserver通信获取clusterip和servicename的映射关系并且coredns本身通过clusterip默认-xxxx310比如集群clusterip网段为10247xx则coredns对外暴露服务的clusterip为10247310我们知道操作系统域名服务器关键配置文件etcresolvconf中的nameserver字段指定所以只需要使得容器etcresolvconf中-nameserver字段配置为coredns的clusterip地址即可">
 在kubernetes集群中，coreDNS服务和kube-apiserver通信获取clusterip和serviceName的映射关系，并且coreDNS本身通过clusterip（默认 xx.xx.3.10，比如集群clusterip网段为10.247.x.x，则coreDNS对外暴露服务的clusterip为10.247.3.10），我们知道操作系统域名服务器关键配置文件/etc/resolv.conf中的nameserver字段指定，所以只需要使得容器/etc/resolv.conf中 nameserver字段配置为coreDNS的clusterip地址即可。
 &lt;a class="anchor" href="#%e5%9c%a8kubernetes%e9%9b%86%e7%be%a4%e4%b8%adcoredns%e6%9c%8d%e5%8a%a1%e5%92%8ckube-apiserver%e9%80%9a%e4%bf%a1%e8%8e%b7%e5%8f%96clusterip%e5%92%8cservicename%e7%9a%84%e6%98%a0%e5%b0%84%e5%85%b3%e7%b3%bb%e5%b9%b6%e4%b8%94coredns%e6%9c%ac%e8%ba%ab%e9%80%9a%e8%bf%87clusterip%e9%bb%98%e8%ae%a4-xxxx310%e6%af%94%e5%a6%82%e9%9b%86%e7%be%a4clusterip%e7%bd%91%e6%ae%b5%e4%b8%ba10247xx%e5%88%99coredns%e5%af%b9%e5%a4%96%e6%9a%b4%e9%9c%b2%e6%9c%8d%e5%8a%a1%e7%9a%84clusterip%e4%b8%ba10247310%e6%88%91%e4%bb%ac%e7%9f%a5%e9%81%93%e6%93%8d%e4%bd%9c%e7%b3%bb%e7%bb%9f%e5%9f%9f%e5%90%8d%e6%9c%8d%e5%8a%a1%e5%99%a8%e5%85%b3%e9%94%ae%e9%85%8d%e7%bd%ae%e6%96%87%e4%bb%b6etcresolvconf%e4%b8%ad%e7%9a%84nameserver%e5%ad%97%e6%ae%b5%e6%8c%87%e5%ae%9a%e6%89%80%e4%bb%a5%e5%8f%aa%e9%9c%80%e8%a6%81%e4%bd%bf%e5%be%97%e5%ae%b9%e5%99%a8etcresolvconf%e4%b8%ad-nameserver%e5%ad%97%e6%ae%b5%e9%85%8d%e7%bd%ae%e4%b8%bacoredns%e7%9a%84clusterip%e5%9c%b0%e5%9d%80%e5%8d%b3%e5%8f%af">#&lt;/a>
&lt;/h5>
&lt;h5 id="那么谁来完成容器etcresolvconf的修改和如何修改kubelet负责拉起容器启动参数中cluster-dns字段对应值就是该集群coredns的clusterip地址kubelet在拉起容器中根据pod的dnspolicy选项把该值修改注入到容器中">
 那么谁来完成容器/etc/resolv.conf的修改和如何修改？kubelet负责拉起容器，启动参数中&amp;ndash;cluster-dns字段对应值就是该集群coreDNS的clusterip地址，kubelet在拉起容器中，根据Pod的dnsPolicy选项，把该值修改注入到容器中。
 &lt;a class="anchor" href="#%e9%82%a3%e4%b9%88%e8%b0%81%e6%9d%a5%e5%ae%8c%e6%88%90%e5%ae%b9%e5%99%a8etcresolvconf%e7%9a%84%e4%bf%ae%e6%94%b9%e5%92%8c%e5%a6%82%e4%bd%95%e4%bf%ae%e6%94%b9kubelet%e8%b4%9f%e8%b4%a3%e6%8b%89%e8%b5%b7%e5%ae%b9%e5%99%a8%e5%90%af%e5%8a%a8%e5%8f%82%e6%95%b0%e4%b8%adcluster-dns%e5%ad%97%e6%ae%b5%e5%af%b9%e5%ba%94%e5%80%bc%e5%b0%b1%e6%98%af%e8%af%a5%e9%9b%86%e7%be%a4coredns%e7%9a%84clusterip%e5%9c%b0%e5%9d%80kubelet%e5%9c%a8%e6%8b%89%e8%b5%b7%e5%ae%b9%e5%99%a8%e4%b8%ad%e6%a0%b9%e6%8d%aepod%e7%9a%84dnspolicy%e9%80%89%e9%a1%b9%e6%8a%8a%e8%af%a5%e5%80%bc%e4%bf%ae%e6%94%b9%e6%b3%a8%e5%85%a5%e5%88%b0%e5%ae%b9%e5%99%a8%e4%b8%ad">#&lt;/a>
&lt;/h5>
&lt;h1 id="三pod不同dnspolicy对容器etcresolvconf的影响">
 三、Pod不同dnsPolicy对容器/etc/resolv.conf的影响
 &lt;a class="anchor" href="#%e4%b8%89pod%e4%b8%8d%e5%90%8cdnspolicy%e5%af%b9%e5%ae%b9%e5%99%a8etcresolvconf%e7%9a%84%e5%bd%b1%e5%93%8d">#&lt;/a>
&lt;/h1>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041103954.png" alt="image-20240304110337886" />&lt;/p>
&lt;ul>
&lt;li>Default：如果dnsPolicy被设置为“Default”，则名称解析nameserver配置将从pod运行的节点/etc/resolv.conf继承。&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code># 节点/etc/resolv.conf配置
nameserver X.X.X.X
nameserver X.X.X.Y
options ndots:5 timeout:2 single-request-reopen
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>ClusterFirst：如果dnsPolicy被设置为“ClusterFirst”，则使用集群coredns的service 地址作为Pod内/etc/resolv.conf中nameserver配置。&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>nameserver 10.247.3.10 
search default.svc.cluster.local svc.cluster.local cluster.local 
options ndots:5 timeout:2 single-request-reopen
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>
&lt;p>ClusterFirstWithHostNet：对于使用hostNetwork网络模式运行的Pod，需明确设置其DNS策略“ClusterFirstWithHostNet”，否则 hostNetwork + ClusterFirst实际效果 = Default&lt;/p>
&lt;pre tabindex="0">&lt;code>ClusterFirstWithHostNet 是 Kubernetes 中的一个 DNS 策略。当 Pod 使用 hostNetwork 模式运行时（即 hostNetwork: true），这个策略指示 Pod 优先使用 Kubernetes 环境的 DNS 服务（如 CoreDNS 提供的域名解析服务）进行域名解析。如果 Kubernetes 环境的 DNS 服务无法解析某个域名，那么该请求会被转发到从宿主机继承的 DNS 服务器上进行解析。

这个策略确保了 Pod 在使用宿主机的网络命名空间时，仍然能够利用 Kubernetes 提供的 DNS 服务进行域名解析，从而保持了与 Kubernetes 集群中其他服务的连通性。这对于需要在 Pod 内访问集群内部服务或跨命名空间通信的场景非常有用。

请注意，为了使用 ClusterFirstWithHostNet 策略，您需要在 Pod 的规格中显式设置 dnsPolicy 字段为 ClusterFirstWithHostNet，并且还需要将 hostNetwork 设置为 true 以启用 hostNetwork 模式。如果不设置 dnsPolicy: ClusterFirstWithHostNet，Pod 默认会使用所在宿主主机使用的 DNS，这可能会导致容器内无法通过 service name 访问 Kubernetes 集群中的其他 Pod。
&lt;/code>&lt;/pre>&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>nameserver 10.247.3.10 
search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5 timeout:2 single-request-reopen
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>None：它允许用户自定义Pod内/etc/resolv.conf配置，忽略Kubernetes环境中默认的DNS设置。应使用dnsConfigPod规范中的字段提供所有DNS设置 。&lt;/li>
&lt;/ul>
&lt;p>/etc/resolv.conf相关配置说明&lt;/p></description></item><item><title>2024-04-03 容器内的 1 号进程</title><link>https://qq547475331.github.io/docs/%E5%AE%B9%E5%99%A8%E5%86%85%E7%9A%84-1-%E5%8F%B7%E8%BF%9B%E7%A8%8B-rong-qi-nei-de-1-hao-jin-cheng/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/%E5%AE%B9%E5%99%A8%E5%86%85%E7%9A%84-1-%E5%8F%B7%E8%BF%9B%E7%A8%8B-rong-qi-nei-de-1-hao-jin-cheng/</guid><description>&lt;h2 id="简介">
 简介
 &lt;a class="anchor" href="#%e7%ae%80%e4%bb%8b">#&lt;/a>
&lt;/h2>
&lt;h5 id="在-linux-系统中系统启动的时候先是执行内核态的代码然后在内核中调用-1-号进程的代码从内核态切换到用户态内核执行的第一个用户态程序就是-1-号进程">
 在 Linux 系统中，系统启动的时候先是执行内核态的代码，然后在内核中调用 &lt;strong>1 号进程&lt;/strong>的代码，从内核态切换到用户态。内核执行的第一个用户态程序就是 &lt;strong>1 号进程。&lt;/strong>
 &lt;a class="anchor" href="#%e5%9c%a8-linux-%e7%b3%bb%e7%bb%9f%e4%b8%ad%e7%b3%bb%e7%bb%9f%e5%90%af%e5%8a%a8%e7%9a%84%e6%97%b6%e5%80%99%e5%85%88%e6%98%af%e6%89%a7%e8%a1%8c%e5%86%85%e6%a0%b8%e6%80%81%e7%9a%84%e4%bb%a3%e7%a0%81%e7%84%b6%e5%90%8e%e5%9c%a8%e5%86%85%e6%a0%b8%e4%b8%ad%e8%b0%83%e7%94%a8-1-%e5%8f%b7%e8%bf%9b%e7%a8%8b%e7%9a%84%e4%bb%a3%e7%a0%81%e4%bb%8e%e5%86%85%e6%a0%b8%e6%80%81%e5%88%87%e6%8d%a2%e5%88%b0%e7%94%a8%e6%88%b7%e6%80%81%e5%86%85%e6%a0%b8%e6%89%a7%e8%a1%8c%e7%9a%84%e7%ac%ac%e4%b8%80%e4%b8%aa%e7%94%a8%e6%88%b7%e6%80%81%e7%a8%8b%e5%ba%8f%e5%b0%b1%e6%98%af-1-%e5%8f%b7%e8%bf%9b%e7%a8%8b">#&lt;/a>
&lt;/h5>
&lt;h5 id="目前主流的-linux-发行版无论是-redhat-系的还是-debian-系的都会把-sbininit-作为符号链接指向-systemdsystemd-是目前最流行的-linux-init-进程在它之前还有-sysvinitupstart-等-linux-init-进程">
 目前主流的 Linux 发行版，无论是 RedHat 系的还是 Debian 系的，都会把 /sbin/init 作为符号链接指向 Systemd。Systemd 是目前最流行的 Linux init 进程，在它之前还有 SysVinit、UpStart 等 Linux init 进程。
 &lt;a class="anchor" href="#%e7%9b%ae%e5%89%8d%e4%b8%bb%e6%b5%81%e7%9a%84-linux-%e5%8f%91%e8%a1%8c%e7%89%88%e6%97%a0%e8%ae%ba%e6%98%af-redhat-%e7%b3%bb%e7%9a%84%e8%bf%98%e6%98%af-debian-%e7%b3%bb%e7%9a%84%e9%83%bd%e4%bc%9a%e6%8a%8a-sbininit-%e4%bd%9c%e4%b8%ba%e7%ac%a6%e5%8f%b7%e9%93%be%e6%8e%a5%e6%8c%87%e5%90%91-systemdsystemd-%e6%98%af%e7%9b%ae%e5%89%8d%e6%9c%80%e6%b5%81%e8%a1%8c%e7%9a%84-linux-init-%e8%bf%9b%e7%a8%8b%e5%9c%a8%e5%ae%83%e4%b9%8b%e5%89%8d%e8%bf%98%e6%9c%89-sysvinitupstart-%e7%ad%89-linux-init-%e8%bf%9b%e7%a8%8b">#&lt;/a>
&lt;/h5>
&lt;h5 id="同样在容器中也有-1-号进程的概念一旦容器建立了自己的-pid-namespace进程命名空间这个-namespace-里的进程号也是从-1-开始标记的">
 同样在&lt;strong>容器中&lt;/strong>也有 1 号进程的概念，一旦容器建立了自己的 **Pid Namespace（进程命名空间)，**这个 Namespace 里的进程号也是从 1 开始标记的。
 &lt;a class="anchor" href="#%e5%90%8c%e6%a0%b7%e5%9c%a8%e5%ae%b9%e5%99%a8%e4%b8%ad%e4%b9%9f%e6%9c%89-1-%e5%8f%b7%e8%bf%9b%e7%a8%8b%e7%9a%84%e6%a6%82%e5%bf%b5%e4%b8%80%e6%97%a6%e5%ae%b9%e5%99%a8%e5%bb%ba%e7%ab%8b%e4%ba%86%e8%87%aa%e5%b7%b1%e7%9a%84-pid-namespace%e8%bf%9b%e7%a8%8b%e5%91%bd%e5%90%8d%e7%a9%ba%e9%97%b4%e8%bf%99%e4%b8%aa-namespace-%e9%87%8c%e7%9a%84%e8%bf%9b%e7%a8%8b%e5%8f%b7%e4%b9%9f%e6%98%af%e4%bb%8e-1-%e5%bc%80%e5%a7%8b%e6%a0%87%e8%ae%b0%e7%9a%84">#&lt;/a>
&lt;/h5>
&lt;h5 id="1-号进程是第一个用户态的进程由它直接或者间接创建了容器中的其他进程">
 1 号进程是第一个用户态的进程，由它直接或者间接创建了容器中的其他进程。
 &lt;a class="anchor" href="#1-%e5%8f%b7%e8%bf%9b%e7%a8%8b%e6%98%af%e7%ac%ac%e4%b8%80%e4%b8%aa%e7%94%a8%e6%88%b7%e6%80%81%e7%9a%84%e8%bf%9b%e7%a8%8b%e7%94%b1%e5%ae%83%e7%9b%b4%e6%8e%a5%e6%88%96%e8%80%85%e9%97%b4%e6%8e%a5%e5%88%9b%e5%bb%ba%e4%ba%86%e5%ae%b9%e5%99%a8%e4%b8%ad%e7%9a%84%e5%85%b6%e4%bb%96%e8%bf%9b%e7%a8%8b">#&lt;/a>
&lt;/h5>
&lt;h2 id="为什么杀不掉容器中-1号进程">
 为什么杀不掉容器中 1号进程
 &lt;a class="anchor" href="#%e4%b8%ba%e4%bb%80%e4%b9%88%e6%9d%80%e4%b8%8d%e6%8e%89%e5%ae%b9%e5%99%a8%e4%b8%ad-1%e5%8f%b7%e8%bf%9b%e7%a8%8b">#&lt;/a>
&lt;/h2>
&lt;h5 id="通过几个实际的示例来说明">
 通过几个实际的示例来说明
 &lt;a class="anchor" href="#%e9%80%9a%e8%bf%87%e5%87%a0%e4%b8%aa%e5%ae%9e%e9%99%85%e7%9a%84%e7%a4%ba%e4%be%8b%e6%9d%a5%e8%af%b4%e6%98%8e">#&lt;/a>
&lt;/h5>
&lt;h5 id="下面示例统一用如下-dockerfile启动一个休闲-600s-的容器">
 下面示例统一用如下 Dockerfile，启动一个休闲 &lt;code>600s&lt;/code> 的容器。
 &lt;a class="anchor" href="#%e4%b8%8b%e9%9d%a2%e7%a4%ba%e4%be%8b%e7%bb%9f%e4%b8%80%e7%94%a8%e5%a6%82%e4%b8%8b-dockerfile%e5%90%af%e5%8a%a8%e4%b8%80%e4%b8%aa%e4%bc%91%e9%97%b2-600s-%e7%9a%84%e5%ae%b9%e5%99%a8">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>FROM ubuntu
ENTRYPOINT [&amp;#34;sleep&amp;#34;, &amp;#34;600&amp;#34;]
&lt;/code>&lt;/pre>&lt;h3 id="现象">
 现象
 &lt;a class="anchor" href="#%e7%8e%b0%e8%b1%a1">#&lt;/a>
&lt;/h3>
&lt;p>&lt;strong>示例一&lt;/strong>&lt;/p>
&lt;h5 id="在容器中使用-kill--9kill--15-杀死-1-号进程">
 在容器中使用 &lt;code>kill -9，kill -15&lt;/code> 杀死 1 号进程
 &lt;a class="anchor" href="#%e5%9c%a8%e5%ae%b9%e5%99%a8%e4%b8%ad%e4%bd%bf%e7%94%a8-kill--9kill--15-%e6%9d%80%e6%ad%bb-1-%e5%8f%b7%e8%bf%9b%e7%a8%8b">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>$ docker run --name test -d test-init:v1
$ docker exec -it test bash
[root@4db3c6f1766b /]# ps -ef
UID PID PPID C STIME TTY TIME CMD
root 1 0 0 01:38 ? 00:00:00 sleep 600
root 6 0 2 01:39 pts/0 00:00:00 bash
root 13 6 0 01:39 pts/0 00:00:00 ps -ef

[root@5cc69036b7b2 /]# kill 1
[root@5cc69036b7b2 /]# kill -9 1
[root@5cc69036b7b2 /]# ps -ef
UID PID PPID C STIME TTY TIME CMD
root 1 0 0 01:38 ? 00:00:00 sleep 600
root 6 0 2 01:39 pts/0 00:00:00 bash
root 13 6 0 01:39 pts/0 00:00:00 ps -ef
&lt;/code>&lt;/pre>&lt;h5 id="当我们完成前面的操作就会发现无论运行-kill-1-对应-linux-中的-sigterm-信号-还是-kill--9-1对应-linux-中的-sigkill-信号都无法让进程终止那么问题来了这两个常常用来终止进程的信号都对容器中的-1号进程不起作用">
 当我们完成前面的操作，就会发现无论运行 &lt;code>kill 1&lt;/code> (对应 Linux 中的 &lt;code>SIGTERM&lt;/code> 信号) 还是 &lt;code>kill -9 1&lt;/code>(对应 Linux 中的 &lt;code>SIGKILL&lt;/code> 信号)，都无法让进程终止。那么问题来了，这两个常常用来终止进程的信号，都对容器中的 1号进程不起作用。
 &lt;a class="anchor" href="#%e5%bd%93%e6%88%91%e4%bb%ac%e5%ae%8c%e6%88%90%e5%89%8d%e9%9d%a2%e7%9a%84%e6%93%8d%e4%bd%9c%e5%b0%b1%e4%bc%9a%e5%8f%91%e7%8e%b0%e6%97%a0%e8%ae%ba%e8%bf%90%e8%a1%8c-kill-1-%e5%af%b9%e5%ba%94-linux-%e4%b8%ad%e7%9a%84-sigterm-%e4%bf%a1%e5%8f%b7-%e8%bf%98%e6%98%af-kill--9-1%e5%af%b9%e5%ba%94-linux-%e4%b8%ad%e7%9a%84-sigkill-%e4%bf%a1%e5%8f%b7%e9%83%bd%e6%97%a0%e6%b3%95%e8%ae%a9%e8%bf%9b%e7%a8%8b%e7%bb%88%e6%ad%a2%e9%82%a3%e4%b9%88%e9%97%ae%e9%a2%98%e6%9d%a5%e4%ba%86%e8%bf%99%e4%b8%a4%e4%b8%aa%e5%b8%b8%e5%b8%b8%e7%94%a8%e6%9d%a5%e7%bb%88%e6%ad%a2%e8%bf%9b%e7%a8%8b%e7%9a%84%e4%bf%a1%e5%8f%b7%e9%83%bd%e5%af%b9%e5%ae%b9%e5%99%a8%e4%b8%ad%e7%9a%84-1%e5%8f%b7%e8%bf%9b%e7%a8%8b%e4%b8%8d%e8%b5%b7%e4%bd%9c%e7%94%a8">#&lt;/a>
&lt;/h5>
&lt;p>&lt;strong>示例二&lt;/strong>&lt;/p></description></item><item><title>2024-04-03 容器原理</title><link>https://qq547475331.github.io/docs/%E5%AE%B9%E5%99%A8%E5%8E%9F%E7%90%86-rong-qi-yuan-li/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/%E5%AE%B9%E5%99%A8%E5%8E%9F%E7%90%86-rong-qi-yuan-li/</guid><description>&lt;h2 id="简介">
 简介
 &lt;a class="anchor" href="#%e7%ae%80%e4%bb%8b">#&lt;/a>
&lt;/h2>
&lt;h5 id="当在任何一个能够运行-cri-dockercontainered-等-的机器上使用-docker-run-image-id-启动一个容器后从使用者的角度来看容器和一台独立的机器或者虚拟机几乎没有什么区别使得我们在容器里就像操作虚拟机一样将服务运行在容器中这样你的机器上能够非常多的容器且容器之间都有独立的运行资源网络文件系统等相互隔离">
 当在任何一个能够运行 CRI (Docker、Containered 等) 的机器上，使用 &lt;code>docker run &amp;lt;image-id&amp;gt;&lt;/code> 启动一个容器后。从使用者的角度来看，容器和一台独立的机器或者虚拟机几乎没有什么区别。使得我们在容器里就像操作虚拟机一样将服务运行在容器中，这样你的机器上能够非常多的容器，且容器之间都有独立的运行资源，网络、文件系统等相互隔离。
 &lt;a class="anchor" href="#%e5%bd%93%e5%9c%a8%e4%bb%bb%e4%bd%95%e4%b8%80%e4%b8%aa%e8%83%bd%e5%a4%9f%e8%bf%90%e8%a1%8c-cri-dockercontainered-%e7%ad%89-%e7%9a%84%e6%9c%ba%e5%99%a8%e4%b8%8a%e4%bd%bf%e7%94%a8-docker-run-image-id-%e5%90%af%e5%8a%a8%e4%b8%80%e4%b8%aa%e5%ae%b9%e5%99%a8%e5%90%8e%e4%bb%8e%e4%bd%bf%e7%94%a8%e8%80%85%e7%9a%84%e8%a7%92%e5%ba%a6%e6%9d%a5%e7%9c%8b%e5%ae%b9%e5%99%a8%e5%92%8c%e4%b8%80%e5%8f%b0%e7%8b%ac%e7%ab%8b%e7%9a%84%e6%9c%ba%e5%99%a8%e6%88%96%e8%80%85%e8%99%9a%e6%8b%9f%e6%9c%ba%e5%87%a0%e4%b9%8e%e6%b2%a1%e6%9c%89%e4%bb%80%e4%b9%88%e5%8c%ba%e5%88%ab%e4%bd%bf%e5%be%97%e6%88%91%e4%bb%ac%e5%9c%a8%e5%ae%b9%e5%99%a8%e9%87%8c%e5%b0%b1%e5%83%8f%e6%93%8d%e4%bd%9c%e8%99%9a%e6%8b%9f%e6%9c%ba%e4%b8%80%e6%a0%b7%e5%b0%86%e6%9c%8d%e5%8a%a1%e8%bf%90%e8%a1%8c%e5%9c%a8%e5%ae%b9%e5%99%a8%e4%b8%ad%e8%bf%99%e6%a0%b7%e4%bd%a0%e7%9a%84%e6%9c%ba%e5%99%a8%e4%b8%8a%e8%83%bd%e5%a4%9f%e9%9d%9e%e5%b8%b8%e5%a4%9a%e7%9a%84%e5%ae%b9%e5%99%a8%e4%b8%94%e5%ae%b9%e5%99%a8%e4%b9%8b%e9%97%b4%e9%83%bd%e6%9c%89%e7%8b%ac%e7%ab%8b%e7%9a%84%e8%bf%90%e8%a1%8c%e8%b5%84%e6%ba%90%e7%bd%91%e7%bb%9c%e6%96%87%e4%bb%b6%e7%b3%bb%e7%bb%9f%e7%ad%89%e7%9b%b8%e4%ba%92%e9%9a%94%e7%a6%bb">#&lt;/a>
&lt;/h5>
&lt;h5 id="但是容器和虚拟机相比却没有各种复杂的硬件虚拟层没有独立的-linux-内核容器所有的进程调度内存访问文件的读写都直接跑在宿主机的内核之上这是怎么做到的呢">
 但是容器和虚拟机相比，却没有各种复杂的硬件虚拟层，没有独立的 Linux 内核。容器所有的进程调度，内存访问，文件的读写都直接跑在宿主机的内核之上，这是怎么做到的呢？
 &lt;a class="anchor" href="#%e4%bd%86%e6%98%af%e5%ae%b9%e5%99%a8%e5%92%8c%e8%99%9a%e6%8b%9f%e6%9c%ba%e7%9b%b8%e6%af%94%e5%8d%b4%e6%b2%a1%e6%9c%89%e5%90%84%e7%a7%8d%e5%a4%8d%e6%9d%82%e7%9a%84%e7%a1%ac%e4%bb%b6%e8%99%9a%e6%8b%9f%e5%b1%82%e6%b2%a1%e6%9c%89%e7%8b%ac%e7%ab%8b%e7%9a%84-linux-%e5%86%85%e6%a0%b8%e5%ae%b9%e5%99%a8%e6%89%80%e6%9c%89%e7%9a%84%e8%bf%9b%e7%a8%8b%e8%b0%83%e5%ba%a6%e5%86%85%e5%ad%98%e8%ae%bf%e9%97%ae%e6%96%87%e4%bb%b6%e7%9a%84%e8%af%bb%e5%86%99%e9%83%bd%e7%9b%b4%e6%8e%a5%e8%b7%91%e5%9c%a8%e5%ae%bf%e4%b8%bb%e6%9c%ba%e7%9a%84%e5%86%85%e6%a0%b8%e4%b9%8b%e4%b8%8a%e8%bf%99%e6%98%af%e6%80%8e%e4%b9%88%e5%81%9a%e5%88%b0%e7%9a%84%e5%91%a2">#&lt;/a>
&lt;/h5>
&lt;h5 id="linux-内核的-namespace-和-cgroups-功能可以让程序在一个资源可控的独立隔离环境中运行这个就是容器了">
 Linux 内核的 &lt;strong>Namespace&lt;/strong> 和 &lt;strong>Cgroups&lt;/strong> 功能可以让程序在一个资源可控的独立（隔离）环境中运行，这个就是容器了。
 &lt;a class="anchor" href="#linux-%e5%86%85%e6%a0%b8%e7%9a%84-namespace-%e5%92%8c-cgroups-%e5%8a%9f%e8%83%bd%e5%8f%af%e4%bb%a5%e8%ae%a9%e7%a8%8b%e5%ba%8f%e5%9c%a8%e4%b8%80%e4%b8%aa%e8%b5%84%e6%ba%90%e5%8f%af%e6%8e%a7%e7%9a%84%e7%8b%ac%e7%ab%8b%e9%9a%94%e7%a6%bb%e7%8e%af%e5%a2%83%e4%b8%ad%e8%bf%90%e8%a1%8c%e8%bf%99%e4%b8%aa%e5%b0%b1%e6%98%af%e5%ae%b9%e5%99%a8%e4%ba%86">#&lt;/a>
&lt;/h5>
&lt;h2 id="用户视角的容器">
 &lt;strong>用户视角的容器&lt;/strong>
 &lt;a class="anchor" href="#%e7%94%a8%e6%88%b7%e8%a7%86%e8%a7%92%e7%9a%84%e5%ae%b9%e5%99%a8">#&lt;/a>
&lt;/h2>
&lt;h5 id="上面说到用户使用容器就跟使用虚拟机几乎5没什么区别也就是说">
 上面说到用户使用容器就跟使用虚拟机几乎5没什么区别，也就是说：
 &lt;a class="anchor" href="#%e4%b8%8a%e9%9d%a2%e8%af%b4%e5%88%b0%e7%94%a8%e6%88%b7%e4%bd%bf%e7%94%a8%e5%ae%b9%e5%99%a8%e5%b0%b1%e8%b7%9f%e4%bd%bf%e7%94%a8%e8%99%9a%e6%8b%9f%e6%9c%ba%e5%87%a0%e4%b9%8e5%e6%b2%a1%e4%bb%80%e4%b9%88%e5%8c%ba%e5%88%ab%e4%b9%9f%e5%b0%b1%e6%98%af%e8%af%b4">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="-容器的文件系统是独立的也就是容器之间文件系统是隔离的">
 • 容器的文件系统是独立的，也就是容器之间文件系统是隔离的
 &lt;a class="anchor" href="#-%e5%ae%b9%e5%99%a8%e7%9a%84%e6%96%87%e4%bb%b6%e7%b3%bb%e7%bb%9f%e6%98%af%e7%8b%ac%e7%ab%8b%e7%9a%84%e4%b9%9f%e5%b0%b1%e6%98%af%e5%ae%b9%e5%99%a8%e4%b9%8b%e9%97%b4%e6%96%87%e4%bb%b6%e7%b3%bb%e7%bb%9f%e6%98%af%e9%9a%94%e7%a6%bb%e7%9a%84">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-容器网络是独立的默认容器网络有单独网络协议栈">
 • 容器网络是独立的，默认容器网络有单独网络协议栈
 &lt;a class="anchor" href="#-%e5%ae%b9%e5%99%a8%e7%bd%91%e7%bb%9c%e6%98%af%e7%8b%ac%e7%ab%8b%e7%9a%84%e9%bb%98%e8%ae%a4%e5%ae%b9%e5%99%a8%e7%bd%91%e7%bb%9c%e6%9c%89%e5%8d%95%e7%8b%ac%e7%bd%91%e7%bb%9c%e5%8d%8f%e8%ae%ae%e6%a0%88">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-容器进程间的通信是隔离的">
 • 容器进程间的通信是隔离的
 &lt;a class="anchor" href="#-%e5%ae%b9%e5%99%a8%e8%bf%9b%e7%a8%8b%e9%97%b4%e7%9a%84%e9%80%9a%e4%bf%a1%e6%98%af%e9%9a%94%e7%a6%bb%e7%9a%84">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-容器的用户用户组也是隔离的">
 • 容器的用户、用户组也是隔离的
 &lt;a class="anchor" href="#-%e5%ae%b9%e5%99%a8%e7%9a%84%e7%94%a8%e6%88%b7%e7%94%a8%e6%88%b7%e7%bb%84%e4%b9%9f%e6%98%af%e9%9a%94%e7%a6%bb%e7%9a%84">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-容器内进程-pid-独立">
 • 容器内进程 PID 独立
 &lt;a class="anchor" href="#-%e5%ae%b9%e5%99%a8%e5%86%85%e8%bf%9b%e7%a8%8b-pid-%e7%8b%ac%e7%ab%8b">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-容器的主机名独立">
 • 容器的主机名独立
 &lt;a class="anchor" href="#-%e5%ae%b9%e5%99%a8%e7%9a%84%e4%b8%bb%e6%9c%ba%e5%90%8d%e7%8b%ac%e7%ab%8b">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-资源-cpumemory-等--隔离容器之间资源隔离">
 • 资源( CPU、Memory 等 ) 隔离，容器之间资源隔离
 &lt;a class="anchor" href="#-%e8%b5%84%e6%ba%90-cpumemory-%e7%ad%89--%e9%9a%94%e7%a6%bb%e5%ae%b9%e5%99%a8%e4%b9%8b%e9%97%b4%e8%b5%84%e6%ba%90%e9%9a%94%e7%a6%bb">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h5 id="对于一个操作系统如果以上这些条件都实现那么该操作系统在用户视角下就完完全全是一个全新独立的容器就是利用-linux-namespacecgroup-技术来实现的">
 对于一个操作系统，如果以上这些条件都实现，那么该操作系统在用户视角下就完完全全是一个全新、独立的。容器就是利用 &lt;strong>Linux Namespace、Cgroup&lt;/strong> 技术来实现的。
 &lt;a class="anchor" href="#%e5%af%b9%e4%ba%8e%e4%b8%80%e4%b8%aa%e6%93%8d%e4%bd%9c%e7%b3%bb%e7%bb%9f%e5%a6%82%e6%9e%9c%e4%bb%a5%e4%b8%8a%e8%bf%99%e4%ba%9b%e6%9d%a1%e4%bb%b6%e9%83%bd%e5%ae%9e%e7%8e%b0%e9%82%a3%e4%b9%88%e8%af%a5%e6%93%8d%e4%bd%9c%e7%b3%bb%e7%bb%9f%e5%9c%a8%e7%94%a8%e6%88%b7%e8%a7%86%e8%a7%92%e4%b8%8b%e5%b0%b1%e5%ae%8c%e5%ae%8c%e5%85%a8%e5%85%a8%e6%98%af%e4%b8%80%e4%b8%aa%e5%85%a8%e6%96%b0%e7%8b%ac%e7%ab%8b%e7%9a%84%e5%ae%b9%e5%99%a8%e5%b0%b1%e6%98%af%e5%88%a9%e7%94%a8-linux-namespacecgroup-%e6%8a%80%e6%9c%af%e6%9d%a5%e5%ae%9e%e7%8e%b0%e7%9a%84">#&lt;/a>
&lt;/h5>
&lt;h5 id="当使用-docker-run--d-centoshttpd-24-centos7latest-启动一个-httpd-服务的容器使用-ls--l-procpidns-在宿主机查看该进程的-namespace-信息">
 当使用 &lt;code>docker run -d centos/httpd-24-centos7:latest&lt;/code> 启动一个 httpd 服务的容器，使用 &lt;code>ls -l /proc/&amp;lt;pid&amp;gt;/ns/&lt;/code> 在宿主机查看该进程的 &lt;strong>Namespace&lt;/strong> 信息。
 &lt;a class="anchor" href="#%e5%bd%93%e4%bd%bf%e7%94%a8-docker-run--d-centoshttpd-24-centos7latest-%e5%90%af%e5%8a%a8%e4%b8%80%e4%b8%aa-httpd-%e6%9c%8d%e5%8a%a1%e7%9a%84%e5%ae%b9%e5%99%a8%e4%bd%bf%e7%94%a8-ls--l-procpidns-%e5%9c%a8%e5%ae%bf%e4%b8%bb%e6%9c%ba%e6%9f%a5%e7%9c%8b%e8%af%a5%e8%bf%9b%e7%a8%8b%e7%9a%84-namespace-%e4%bf%a1%e6%81%af">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>ls -l /proc/126013/ns/
lrwxrwxrwx. 1 1001 root 0 10月 30 11:33 ipc -&amp;gt; ipc:[4026535315]
lrwxrwxrwx. 1 1001 root 0 10月 30 11:33 mnt -&amp;gt; mnt:[4026535313]
lrwxrwxrwx. 1 1001 root 0 10月 30 11:33 net -&amp;gt; net:[4026535319]
lrwxrwxrwx. 1 1001 root 0 10月 30 11:33 pid -&amp;gt; pid:[4026535316]
lrwxrwxrwx. 1 1001 root 0 10月 30 15:27 user -&amp;gt; user:[4026531837]
lrwxrwxrwx. 1 1001 root 0 10月 30 11:33 uts -&amp;gt; uts:[4026535314]
&lt;/code>&lt;/pre>&lt;h5 id="发现该进程对应的-ipcmntnetpiduseruts-都指向了单独的-namespace">
 发现该进程对应的 ipc、mnt、net、pid、user、uts 都指向了单独的 namespace。
 &lt;a class="anchor" href="#%e5%8f%91%e7%8e%b0%e8%af%a5%e8%bf%9b%e7%a8%8b%e5%af%b9%e5%ba%94%e7%9a%84-ipcmntnetpiduseruts-%e9%83%bd%e6%8c%87%e5%90%91%e4%ba%86%e5%8d%95%e7%8b%ac%e7%9a%84-namespace">#&lt;/a>
&lt;/h5>
&lt;hr>
&lt;h2 id="namespace">
 Namespace
 &lt;a class="anchor" href="#namespace">#&lt;/a>
&lt;/h2>
&lt;h5 id="linux-内核-namespace-类型有很多支持-cgroupipcnetworkmountpidtimeuseruts下面看看是如何使用这些-namespace-隔离技术运行一个容器">
 Linux 内核 &lt;strong>Namespace&lt;/strong> 类型有很多，支持 **cgroup/ipc/network/mount/pid/time/user/uts，**下面看看是如何使用这些 Namespace 隔离技术运行一个容器。
 &lt;a class="anchor" href="#linux-%e5%86%85%e6%a0%b8-namespace-%e7%b1%bb%e5%9e%8b%e6%9c%89%e5%be%88%e5%a4%9a%e6%94%af%e6%8c%81-cgroupipcnetworkmountpidtimeuseruts%e4%b8%8b%e9%9d%a2%e7%9c%8b%e7%9c%8b%e6%98%af%e5%a6%82%e4%bd%95%e4%bd%bf%e7%94%a8%e8%bf%99%e4%ba%9b-namespace-%e9%9a%94%e7%a6%bb%e6%8a%80%e6%9c%af%e8%bf%90%e8%a1%8c%e4%b8%80%e4%b8%aa%e5%ae%b9%e5%99%a8">#&lt;/a>
&lt;/h5>
&lt;h3 id="pid-namespace">
 PID Namespace
 &lt;a class="anchor" href="#pid-namespace">#&lt;/a>
&lt;/h3>
&lt;h5 id="当使用-docker-exec-624d08ddaf99-ps--ef-获取一个容器的所有进程看到了该容器只运行了五个-httpd-进程">
 当使用 &lt;code>docker exec 624d08ddaf99 ps -ef&lt;/code> 获取一个容器的所有进程，看到了该容器只运行了五个 httpd 进程。
 &lt;a class="anchor" href="#%e5%bd%93%e4%bd%bf%e7%94%a8-docker-exec-624d08ddaf99-ps--ef-%e8%8e%b7%e5%8f%96%e4%b8%80%e4%b8%aa%e5%ae%b9%e5%99%a8%e7%9a%84%e6%89%80%e6%9c%89%e8%bf%9b%e7%a8%8b%e7%9c%8b%e5%88%b0%e4%ba%86%e8%af%a5%e5%ae%b9%e5%99%a8%e5%8f%aa%e8%bf%90%e8%a1%8c%e4%ba%86%e4%ba%94%e4%b8%aa-httpd-%e8%bf%9b%e7%a8%8b">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code># docker exec c5a9ff78d9c1 ps -ef

UID PID PPID C STIME TTY TIME CMD
root 1 0 0 01:59 ? 00:00:00 /sbin/httpd -D FOREGROUND
apache 6 1 0 01:59 ? 00:00:00 /sbin/httpd -D FOREGROUND
apache 7 1 0 01:59 ? 00:00:00 /sbin/httpd -D FOREGROUND
apache 8 1 0 01:59 ? 00:00:00 /sbin/httpd -D FOREGROUND
apache 9 1 0 01:59 ? 00:00:00 /sbin/httpd -D FOREGROUND
&lt;/code>&lt;/pre>&lt;h5 id="直接获取该容器所在的宿主机上的-httpd-进程同样可以看到这-5-个进程">
 直接获取该容器所在的宿主机上的 httpd 进程，同样可以看到这 5 个进程。
 &lt;a class="anchor" href="#%e7%9b%b4%e6%8e%a5%e8%8e%b7%e5%8f%96%e8%af%a5%e5%ae%b9%e5%99%a8%e6%89%80%e5%9c%a8%e7%9a%84%e5%ae%bf%e4%b8%bb%e6%9c%ba%e4%b8%8a%e7%9a%84-httpd-%e8%bf%9b%e7%a8%8b%e5%90%8c%e6%a0%b7%e5%8f%af%e4%bb%a5%e7%9c%8b%e5%88%b0%e8%bf%99-5-%e4%b8%aa%e8%bf%9b%e7%a8%8b">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code># ps -ef | grep httpd

UID PID PPID C STIME TTY TIME CMD
1001 126013 125976 0 11:33 ? 00:00:00 httpd -D FOREGROUND
1001 126100 126013 0 11:33 ? 00:00:01 httpd -D FOREGROUND
1001 126105 126013 0 11:33 ? 00:00:01 httpd -D FOREGROUND
1001 126119 126013 0 11:33 ? 00:00:01 httpd -D FOREGROUND
1001 126134 126013 0 11:33 ? 00:00:01 httpd -D FOREGROUND
1001 126140 126013 0 11:33 ? 00:00:01 httpd -D FOREGROUND
&lt;/code>&lt;/pre>&lt;h5 id="对比两组结果发现两组数据的-pid-不一样linux-在创建容器的时候就会建出一个-pid-namespace该-pid-namespace-下所有所有进程编号都从1-开始其实和-linux-宿主机一样">
 对比两组结果，发现两组数据的 &lt;strong>PID&lt;/strong> 不一样。Linux 在创建容器的时候，就会建出一个 &lt;strong>PID Namespace&lt;/strong>，该 &lt;strong>PID Namespace&lt;/strong> 下所有所有进程编号都从1 开始。其实和 Linux 宿主机一样，
 &lt;a class="anchor" href="#%e5%af%b9%e6%af%94%e4%b8%a4%e7%bb%84%e7%bb%93%e6%9e%9c%e5%8f%91%e7%8e%b0%e4%b8%a4%e7%bb%84%e6%95%b0%e6%8d%ae%e7%9a%84-pid-%e4%b8%8d%e4%b8%80%e6%a0%b7linux-%e5%9c%a8%e5%88%9b%e5%bb%ba%e5%ae%b9%e5%99%a8%e7%9a%84%e6%97%b6%e5%80%99%e5%b0%b1%e4%bc%9a%e5%bb%ba%e5%87%ba%e4%b8%80%e4%b8%aa-pid-namespace%e8%af%a5-pid-namespace-%e4%b8%8b%e6%89%80%e6%9c%89%e6%89%80%e6%9c%89%e8%bf%9b%e7%a8%8b%e7%bc%96%e5%8f%b7%e9%83%bd%e4%bb%8e1-%e5%bc%80%e5%a7%8b%e5%85%b6%e5%ae%9e%e5%92%8c-linux-%e5%ae%bf%e4%b8%bb%e6%9c%ba%e4%b8%80%e6%a0%b7">#&lt;/a>
&lt;/h5>
&lt;h5 id="linux-开机时就会启动一个-1-号进程该进程是所有进程的父进程同时在这个-pid-namesapce-中只能看到该-namespace-下的进程看不到其他-namespace-下的进程">
 Linux 开机时就会启动一个 1 号进程，该进程是所有进程的父进程。同时在这个 &lt;strong>PID Namesapce&lt;/strong> 中只能看到该 Namespace 下的进程，看不到其他 Namespace 下的进程。
 &lt;a class="anchor" href="#linux-%e5%bc%80%e6%9c%ba%e6%97%b6%e5%b0%b1%e4%bc%9a%e5%90%af%e5%8a%a8%e4%b8%80%e4%b8%aa-1-%e5%8f%b7%e8%bf%9b%e7%a8%8b%e8%af%a5%e8%bf%9b%e7%a8%8b%e6%98%af%e6%89%80%e6%9c%89%e8%bf%9b%e7%a8%8b%e7%9a%84%e7%88%b6%e8%bf%9b%e7%a8%8b%e5%90%8c%e6%97%b6%e5%9c%a8%e8%bf%99%e4%b8%aa-pid-namesapce-%e4%b8%ad%e5%8f%aa%e8%83%bd%e7%9c%8b%e5%88%b0%e8%af%a5-namespace-%e4%b8%8b%e7%9a%84%e8%bf%9b%e7%a8%8b%e7%9c%8b%e4%b8%8d%e5%88%b0%e5%85%b6%e4%bb%96-namespace-%e4%b8%8b%e7%9a%84%e8%bf%9b%e7%a8%8b">#&lt;/a>
&lt;/h5>
&lt;h5 id="那么这个-httpd-服务对应的宿主机-pid-和容器-pid-的关系是什么样的这两个-pid-对应着都是同一个-httpd-服务的某个进程">
 那么这个 httpd 服务对应的&lt;strong>宿主机 PID&lt;/strong> 和&lt;strong>容器 PID&lt;/strong> 的关系是什么样的，这两个 PID 对应着都是同一个 httpd 服务的某个进程。
 &lt;a class="anchor" href="#%e9%82%a3%e4%b9%88%e8%bf%99%e4%b8%aa-httpd-%e6%9c%8d%e5%8a%a1%e5%af%b9%e5%ba%94%e7%9a%84%e5%ae%bf%e4%b8%bb%e6%9c%ba-pid-%e5%92%8c%e5%ae%b9%e5%99%a8-pid-%e7%9a%84%e5%85%b3%e7%b3%bb%e6%98%af%e4%bb%80%e4%b9%88%e6%a0%b7%e7%9a%84%e8%bf%99%e4%b8%a4%e4%b8%aa-pid-%e5%af%b9%e5%ba%94%e7%9d%80%e9%83%bd%e6%98%af%e5%90%8c%e4%b8%80%e4%b8%aa-httpd-%e6%9c%8d%e5%8a%a1%e7%9a%84%e6%9f%90%e4%b8%aa%e8%bf%9b%e7%a8%8b">#&lt;/a>
&lt;/h5>
&lt;h5 id="可以通过-docker-inspect-container-id--grep-pid-查看这个容器对应宿主机上的-pid会发现这个-pid-126013-就是在宿主机上使用-ps--ef-grep-httpd-查到的-httpd-进程一样只不过这个-httpd-服务有多个进程126013-是所有子进程的父进程">
 可以通过 &lt;code>docker inspect &amp;lt;container id&amp;gt; | grep Pid&lt;/code> 查看这个容器对应&lt;code>宿主机上的 PID&lt;/code>。会发现这个 &lt;code>&amp;quot;Pid&amp;quot;: 126013&lt;/code> 就是在宿主机上使用 &lt;code>ps -ef |grep httpd&lt;/code> 查到的 httpd 进程一样，只不过这个 httpd 服务有多个进程，126013 是所有子进程的父进程。
 &lt;a class="anchor" href="#%e5%8f%af%e4%bb%a5%e9%80%9a%e8%bf%87-docker-inspect-container-id--grep-pid-%e6%9f%a5%e7%9c%8b%e8%bf%99%e4%b8%aa%e5%ae%b9%e5%99%a8%e5%af%b9%e5%ba%94%e5%ae%bf%e4%b8%bb%e6%9c%ba%e4%b8%8a%e7%9a%84-pid%e4%bc%9a%e5%8f%91%e7%8e%b0%e8%bf%99%e4%b8%aa-pid-126013-%e5%b0%b1%e6%98%af%e5%9c%a8%e5%ae%bf%e4%b8%bb%e6%9c%ba%e4%b8%8a%e4%bd%bf%e7%94%a8-ps--ef-grep-httpd-%e6%9f%a5%e5%88%b0%e7%9a%84-httpd-%e8%bf%9b%e7%a8%8b%e4%b8%80%e6%a0%b7%e5%8f%aa%e4%b8%8d%e8%bf%87%e8%bf%99%e4%b8%aa-httpd-%e6%9c%8d%e5%8a%a1%e6%9c%89%e5%a4%9a%e4%b8%aa%e8%bf%9b%e7%a8%8b126013-%e6%98%af%e6%89%80%e6%9c%89%e5%ad%90%e8%bf%9b%e7%a8%8b%e7%9a%84%e7%88%b6%e8%bf%9b%e7%a8%8b">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code># docker inspect 5d22ea980dc8|grep Pid
 &amp;#34;Pid&amp;#34;: 126013,

# ps -ef | grep httpd
UID PID PPID C STIME TTY TIME CMD
1001 126013 125976 0 11:33 ? 00:00:00 httpd -D FOREGROUND
&lt;/code>&lt;/pre>&lt;h5 id="这也就是说如果有另外一个容器那么它也有自己的一个-pid-namespace而这两个-pid-namespace-之间是不能看到对方的进程的这里就体现出了-namespace-的作用相互隔离">
 这也就是说，如果有另外一个容器，那么它也有自己的一个 &lt;strong>PID Namespace&lt;/strong>，而这两个 &lt;strong>PID Namespace&lt;/strong> 之间是不能看到对方的进程的，这里就体现出了 Namespace 的作用：&lt;strong>相互隔离&lt;/strong>。
 &lt;a class="anchor" href="#%e8%bf%99%e4%b9%9f%e5%b0%b1%e6%98%af%e8%af%b4%e5%a6%82%e6%9e%9c%e6%9c%89%e5%8f%a6%e5%a4%96%e4%b8%80%e4%b8%aa%e5%ae%b9%e5%99%a8%e9%82%a3%e4%b9%88%e5%ae%83%e4%b9%9f%e6%9c%89%e8%87%aa%e5%b7%b1%e7%9a%84%e4%b8%80%e4%b8%aa-pid-namespace%e8%80%8c%e8%bf%99%e4%b8%a4%e4%b8%aa-pid-namespace-%e4%b9%8b%e9%97%b4%e6%98%af%e4%b8%8d%e8%83%bd%e7%9c%8b%e5%88%b0%e5%af%b9%e6%96%b9%e7%9a%84%e8%bf%9b%e7%a8%8b%e7%9a%84%e8%bf%99%e9%87%8c%e5%b0%b1%e4%bd%93%e7%8e%b0%e5%87%ba%e4%ba%86-namespace-%e7%9a%84%e4%bd%9c%e7%94%a8%e7%9b%b8%e4%ba%92%e9%9a%94%e7%a6%bb">#&lt;/a>
&lt;/h5>
&lt;h5 id="当然这只是隔离了服务的进程下面看看-network-namespace">
 当然这只是隔离了服务的进程，下面看看 &lt;strong>Network Namespace&lt;/strong>。
 &lt;a class="anchor" href="#%e5%bd%93%e7%84%b6%e8%bf%99%e5%8f%aa%e6%98%af%e9%9a%94%e7%a6%bb%e4%ba%86%e6%9c%8d%e5%8a%a1%e7%9a%84%e8%bf%9b%e7%a8%8b%e4%b8%8b%e9%9d%a2%e7%9c%8b%e7%9c%8b-network-namespace">#&lt;/a>
&lt;/h5>
&lt;h3 id="network-namespace">
 Network Namespace
 &lt;a class="anchor" href="#network-namespace">#&lt;/a>
&lt;/h3>
&lt;h5 id="如果启动容器时不使用-nethost-即容器使用宿主机的网络那么-linux-就会给这个容器创建单独的-network-namespace在这个-network-namespace-中都有一套独立的网络接口">
 如果启动容器时不使用 &lt;code>—net=host&lt;/code> ，即容器使用宿主机的网络，那么 Linux 就会给这个容器创建单独的 &lt;strong>Network Namespace&lt;/strong>。在这个 &lt;strong>Network Namespace&lt;/strong> 中都有一套独立的网络接口。
 &lt;a class="anchor" href="#%e5%a6%82%e6%9e%9c%e5%90%af%e5%8a%a8%e5%ae%b9%e5%99%a8%e6%97%b6%e4%b8%8d%e4%bd%bf%e7%94%a8-nethost-%e5%8d%b3%e5%ae%b9%e5%99%a8%e4%bd%bf%e7%94%a8%e5%ae%bf%e4%b8%bb%e6%9c%ba%e7%9a%84%e7%bd%91%e7%bb%9c%e9%82%a3%e4%b9%88-linux-%e5%b0%b1%e4%bc%9a%e7%bb%99%e8%bf%99%e4%b8%aa%e5%ae%b9%e5%99%a8%e5%88%9b%e5%bb%ba%e5%8d%95%e7%8b%ac%e7%9a%84-network-namespace%e5%9c%a8%e8%bf%99%e4%b8%aa-network-namespace-%e4%b8%ad%e9%83%bd%e6%9c%89%e4%b8%80%e5%a5%97%e7%8b%ac%e7%ab%8b%e7%9a%84%e7%bd%91%e7%bb%9c%e6%8e%a5%e5%8f%a3">#&lt;/a>
&lt;/h5>
&lt;p>比如查看容器的网络接口，这里的 lo，eth0，还有独立的 TCP/IP 的协议栈配置。&lt;/p></description></item><item><title>2024-04-03 容器的文件系统 OverlayFS 原理</title><link>https://qq547475331.github.io/docs/%E5%AE%B9%E5%99%A8%E7%9A%84%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E4%B8%80-overlayfs-%E5%8E%9F%E7%90%86-rong-qi-de-wen-jian-xi-tong--yi-overlayfs-yuan-li/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/%E5%AE%B9%E5%99%A8%E7%9A%84%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E4%B8%80-overlayfs-%E5%8E%9F%E7%90%86-rong-qi-de-wen-jian-xi-tong--yi-overlayfs-yuan-li/</guid><description>&lt;h2 id="容器文件系统">
 容器文件系统
 &lt;a class="anchor" href="#%e5%ae%b9%e5%99%a8%e6%96%87%e4%bb%b6%e7%b3%bb%e7%bb%9f">#&lt;/a>
&lt;/h2>
&lt;h5 id="之前文章-容器原理-说到容器的核心技术是由-linux-namespace--cgroups-实现的其中在说到-mount-namespace-中说到使用-docker-exec-进入某个容器后可以看到一个崭新的文件系统就类似于-ssh-到某个虚拟机或者物理机上实际上就是利用-linux-mount-namespace-隔离实现的当创建一个容器时linux-系统上就会创建一个-对应的-mount-namespace那么这个-mount-namespace-就是该容器的文件系统">
 之前文章 &lt;a href="http://mp.weixin.qq.com/s?__biz=MzkyNDMyNjAyMg==&amp;amp;mid=2247484030&amp;amp;idx=1&amp;amp;sn=43bd06072aa327ded21d6513b891d84d&amp;amp;chksm=c1d6c397f6a14a81cbef67203522f0a5201492b161d1faf205d7d271b2e6afe64f1a2b710391&amp;amp;scene=21#wechat_redirect">容器原理&lt;/a> 说到容器的核心技术是由 Linux &lt;strong>Namespace + Cgroups&lt;/strong> 实现的。其中在说到 &lt;strong>Mount Namespace&lt;/strong> 中说到，使用 &lt;code>docker exec&lt;/code> 进入某个容器后。可以看到一个崭新的文件系统，就类似于 &lt;code>ssh&lt;/code> 到某个虚拟机或者物理机上，实际上就是利用 Linux &lt;strong>Mount Namespace&lt;/strong> 隔离实现的。当创建一个容器时，Linux 系统上就会创建一个 对应的 &lt;strong>Mount Namespace&lt;/strong>，那么这个 &lt;strong>Mount Namespace&lt;/strong> 就是该容器的文件系统。
 &lt;a class="anchor" href="#%e4%b9%8b%e5%89%8d%e6%96%87%e7%ab%a0-%e5%ae%b9%e5%99%a8%e5%8e%9f%e7%90%86-%e8%af%b4%e5%88%b0%e5%ae%b9%e5%99%a8%e7%9a%84%e6%a0%b8%e5%bf%83%e6%8a%80%e6%9c%af%e6%98%af%e7%94%b1-linux-namespace--cgroups-%e5%ae%9e%e7%8e%b0%e7%9a%84%e5%85%b6%e4%b8%ad%e5%9c%a8%e8%af%b4%e5%88%b0-mount-namespace-%e4%b8%ad%e8%af%b4%e5%88%b0%e4%bd%bf%e7%94%a8-docker-exec-%e8%bf%9b%e5%85%a5%e6%9f%90%e4%b8%aa%e5%ae%b9%e5%99%a8%e5%90%8e%e5%8f%af%e4%bb%a5%e7%9c%8b%e5%88%b0%e4%b8%80%e4%b8%aa%e5%b4%ad%e6%96%b0%e7%9a%84%e6%96%87%e4%bb%b6%e7%b3%bb%e7%bb%9f%e5%b0%b1%e7%b1%bb%e4%bc%bc%e4%ba%8e-ssh-%e5%88%b0%e6%9f%90%e4%b8%aa%e8%99%9a%e6%8b%9f%e6%9c%ba%e6%88%96%e8%80%85%e7%89%a9%e7%90%86%e6%9c%ba%e4%b8%8a%e5%ae%9e%e9%99%85%e4%b8%8a%e5%b0%b1%e6%98%af%e5%88%a9%e7%94%a8-linux-mount-namespace-%e9%9a%94%e7%a6%bb%e5%ae%9e%e7%8e%b0%e7%9a%84%e5%bd%93%e5%88%9b%e5%bb%ba%e4%b8%80%e4%b8%aa%e5%ae%b9%e5%99%a8%e6%97%b6linux-%e7%b3%bb%e7%bb%9f%e4%b8%8a%e5%b0%b1%e4%bc%9a%e5%88%9b%e5%bb%ba%e4%b8%80%e4%b8%aa-%e5%af%b9%e5%ba%94%e7%9a%84-mount-namespace%e9%82%a3%e4%b9%88%e8%bf%99%e4%b8%aa-mount-namespace-%e5%b0%b1%e6%98%af%e8%af%a5%e5%ae%b9%e5%99%a8%e7%9a%84%e6%96%87%e4%bb%b6%e7%b3%bb%e7%bb%9f">#&lt;/a>
&lt;/h5>
&lt;h5 id="容器的文件系统在用户视角和宿主机的文件系统没有什么区别但是其背后的原理却不同在容器里运行-df-命令可以看到在容器中根目录的文件系统类型是-overlay它不是我们在普通-linux-节点上看到的-ext4-或者-xfs-之类常见的文件系统">
 容器的文件系统在用户视角和宿主机的文件系统没有什么区别，但是其背后的原理却不同。在容器里运行 &lt;code>df&lt;/code> 命令，可以看到在容器中&lt;code>根目录(/)&lt;/code>的文件系统类型是 &lt;strong>overlay&lt;/strong>，它不是我们在普通 Linux 节点上看到的 Ext4 或者 XFS 之类常见的文件系统。
 &lt;a class="anchor" href="#%e5%ae%b9%e5%99%a8%e7%9a%84%e6%96%87%e4%bb%b6%e7%b3%bb%e7%bb%9f%e5%9c%a8%e7%94%a8%e6%88%b7%e8%a7%86%e8%a7%92%e5%92%8c%e5%ae%bf%e4%b8%bb%e6%9c%ba%e7%9a%84%e6%96%87%e4%bb%b6%e7%b3%bb%e7%bb%9f%e6%b2%a1%e6%9c%89%e4%bb%80%e4%b9%88%e5%8c%ba%e5%88%ab%e4%bd%86%e6%98%af%e5%85%b6%e8%83%8c%e5%90%8e%e7%9a%84%e5%8e%9f%e7%90%86%e5%8d%b4%e4%b8%8d%e5%90%8c%e5%9c%a8%e5%ae%b9%e5%99%a8%e9%87%8c%e8%bf%90%e8%a1%8c-df-%e5%91%bd%e4%bb%a4%e5%8f%af%e4%bb%a5%e7%9c%8b%e5%88%b0%e5%9c%a8%e5%ae%b9%e5%99%a8%e4%b8%ad%e6%a0%b9%e7%9b%ae%e5%bd%95%e7%9a%84%e6%96%87%e4%bb%b6%e7%b3%bb%e7%bb%9f%e7%b1%bb%e5%9e%8b%e6%98%af-overlay%e5%ae%83%e4%b8%8d%e6%98%af%e6%88%91%e4%bb%ac%e5%9c%a8%e6%99%ae%e9%80%9a-linux-%e8%8a%82%e7%82%b9%e4%b8%8a%e7%9c%8b%e5%88%b0%e7%9a%84-ext4-%e6%88%96%e8%80%85-xfs-%e4%b9%8b%e7%b1%bb%e5%b8%b8%e8%a7%81%e7%9a%84%e6%96%87%e4%bb%b6%e7%b3%bb%e7%bb%9f">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>root@624d08ddaf99:~# df -h
Filesystem Size Used Avail Use% Mounted on
overlay 558G 70G 488G 13% /
&lt;/code>&lt;/pre>&lt;hr>
&lt;h2 id="为什么需要-overlayfs">
 为什么需要 OverlayFS
 &lt;a class="anchor" href="#%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81-overlayfs">#&lt;/a>
&lt;/h2>
&lt;h5 id="那么这个-overlayfs-是一个怎样的文件系统容器为什么使用这种文件系统">
 那么这个 &lt;strong>OverlayFS&lt;/strong> 是一个怎样的文件系统，容器为什么使用这种文件系统？
 &lt;a class="anchor" href="#%e9%82%a3%e4%b9%88%e8%bf%99%e4%b8%aa-overlayfs-%e6%98%af%e4%b8%80%e4%b8%aa%e6%80%8e%e6%a0%b7%e7%9a%84%e6%96%87%e4%bb%b6%e7%b3%bb%e7%bb%9f%e5%ae%b9%e5%99%a8%e4%b8%ba%e4%bb%80%e4%b9%88%e4%bd%bf%e7%94%a8%e8%bf%99%e7%a7%8d%e6%96%87%e4%bb%b6%e7%b3%bb%e7%bb%9f">#&lt;/a>
&lt;/h5>
&lt;h5 id="我们知道虚拟机需要-iso-镜像才能启动那么每个容器也需要一个镜像才能启动然后这个镜像会包含容器运行的二进制文件库文件配置文件其他的依赖的文件等全部打包成一个-镜像文件如果使用正常的-ext4-或者-xfs-文件系统的话那么每次启动一个容器就需要把一个镜像文件下载并存储在宿主机上">
 我们知道虚拟机需要 iso 镜像才能启动，那么每个容器也需要一个镜像才能启动。然后这个镜像会包含容器运行的二进制文件、库文件、配置文件，其他的依赖的文件等全部打包成一个 镜像文件。如果使用正常的 EXT4 或者 XFS 文件系统的话，那么每次启动一个容器，就需要把一个镜像文件下载并存储在宿主机上。
 &lt;a class="anchor" href="#%e6%88%91%e4%bb%ac%e7%9f%a5%e9%81%93%e8%99%9a%e6%8b%9f%e6%9c%ba%e9%9c%80%e8%a6%81-iso-%e9%95%9c%e5%83%8f%e6%89%8d%e8%83%bd%e5%90%af%e5%8a%a8%e9%82%a3%e4%b9%88%e6%af%8f%e4%b8%aa%e5%ae%b9%e5%99%a8%e4%b9%9f%e9%9c%80%e8%a6%81%e4%b8%80%e4%b8%aa%e9%95%9c%e5%83%8f%e6%89%8d%e8%83%bd%e5%90%af%e5%8a%a8%e7%84%b6%e5%90%8e%e8%bf%99%e4%b8%aa%e9%95%9c%e5%83%8f%e4%bc%9a%e5%8c%85%e5%90%ab%e5%ae%b9%e5%99%a8%e8%bf%90%e8%a1%8c%e7%9a%84%e4%ba%8c%e8%bf%9b%e5%88%b6%e6%96%87%e4%bb%b6%e5%ba%93%e6%96%87%e4%bb%b6%e9%85%8d%e7%bd%ae%e6%96%87%e4%bb%b6%e5%85%b6%e4%bb%96%e7%9a%84%e4%be%9d%e8%b5%96%e7%9a%84%e6%96%87%e4%bb%b6%e7%ad%89%e5%85%a8%e9%83%a8%e6%89%93%e5%8c%85%e6%88%90%e4%b8%80%e4%b8%aa-%e9%95%9c%e5%83%8f%e6%96%87%e4%bb%b6%e5%a6%82%e6%9e%9c%e4%bd%bf%e7%94%a8%e6%ad%a3%e5%b8%b8%e7%9a%84-ext4-%e6%88%96%e8%80%85-xfs-%e6%96%87%e4%bb%b6%e7%b3%bb%e7%bb%9f%e7%9a%84%e8%af%9d%e9%82%a3%e4%b9%88%e6%af%8f%e6%ac%a1%e5%90%af%e5%8a%a8%e4%b8%80%e4%b8%aa%e5%ae%b9%e5%99%a8%e5%b0%b1%e9%9c%80%e8%a6%81%e6%8a%8a%e4%b8%80%e4%b8%aa%e9%95%9c%e5%83%8f%e6%96%87%e4%bb%b6%e4%b8%8b%e8%bd%bd%e5%b9%b6%e5%ad%98%e5%82%a8%e5%9c%a8%e5%ae%bf%e4%b8%bb%e6%9c%ba%e4%b8%8a">#&lt;/a>
&lt;/h5>
&lt;h5 id="比如假设一个镜像文件的大小是-500mb那么-100-个容器的话就需要下载-500mb100-50gb-的文件并且占用-50gb-的磁盘空间然而在这-50gb-的磁盘空间中大部分数据都是重复的因为大部分数据都是镜像的-base-镜像而且这些-base-镜像基本都是只读的在容器运行时不会变动">
 比如：假设一个镜像文件的大小是 500MB，那么 100 个容器的话，就需要下载 &lt;code>500MB*100= 50GB&lt;/code> 的文件，并且占用 50GB 的磁盘空间。然而在这 50GB 的磁盘空间中，大部分数据都是重复的，因为大部分数据都是镜像的 base 镜像，而且这些 base 镜像基本都是只读的，在容器运行时不会变动。
 &lt;a class="anchor" href="#%e6%af%94%e5%a6%82%e5%81%87%e8%ae%be%e4%b8%80%e4%b8%aa%e9%95%9c%e5%83%8f%e6%96%87%e4%bb%b6%e7%9a%84%e5%a4%a7%e5%b0%8f%e6%98%af-500mb%e9%82%a3%e4%b9%88-100-%e4%b8%aa%e5%ae%b9%e5%99%a8%e7%9a%84%e8%af%9d%e5%b0%b1%e9%9c%80%e8%a6%81%e4%b8%8b%e8%bd%bd-500mb100-50gb-%e7%9a%84%e6%96%87%e4%bb%b6%e5%b9%b6%e4%b8%94%e5%8d%a0%e7%94%a8-50gb-%e7%9a%84%e7%a3%81%e7%9b%98%e7%a9%ba%e9%97%b4%e7%84%b6%e8%80%8c%e5%9c%a8%e8%bf%99-50gb-%e7%9a%84%e7%a3%81%e7%9b%98%e7%a9%ba%e9%97%b4%e4%b8%ad%e5%a4%a7%e9%83%a8%e5%88%86%e6%95%b0%e6%8d%ae%e9%83%bd%e6%98%af%e9%87%8d%e5%a4%8d%e7%9a%84%e5%9b%a0%e4%b8%ba%e5%a4%a7%e9%83%a8%e5%88%86%e6%95%b0%e6%8d%ae%e9%83%bd%e6%98%af%e9%95%9c%e5%83%8f%e7%9a%84-base-%e9%95%9c%e5%83%8f%e8%80%8c%e4%b8%94%e8%bf%99%e4%ba%9b-base-%e9%95%9c%e5%83%8f%e5%9f%ba%e6%9c%ac%e9%83%bd%e6%98%af%e5%8f%aa%e8%af%bb%e7%9a%84%e5%9c%a8%e5%ae%b9%e5%99%a8%e8%bf%90%e8%a1%8c%e6%97%b6%e4%b8%8d%e4%bc%9a%e5%8f%98%e5%8a%a8">#&lt;/a>
&lt;/h5>
&lt;h5 id="如说这-100-个容器镜像都是基于-ubuntu1804-的每个容器镜像只是额外复制了-50mb-左右自己的应用程序到-ubuntu-1804-里那么就是说在总共-50gb-的数据里有-90-的数据是冗余的">
 如说这 100 个容器镜像都是基于 ubuntu:18.04 的，每个容器镜像只是额外复制了 50MB 左右自己的应用程序到 ubuntu: 18.04 里，那么就是说在总共 50GB 的数据里，有 90% 的数据是冗余的。
 &lt;a class="anchor" href="#%e5%a6%82%e8%af%b4%e8%bf%99-100-%e4%b8%aa%e5%ae%b9%e5%99%a8%e9%95%9c%e5%83%8f%e9%83%bd%e6%98%af%e5%9f%ba%e4%ba%8e-ubuntu1804-%e7%9a%84%e6%af%8f%e4%b8%aa%e5%ae%b9%e5%99%a8%e9%95%9c%e5%83%8f%e5%8f%aa%e6%98%af%e9%a2%9d%e5%a4%96%e5%a4%8d%e5%88%b6%e4%ba%86-50mb-%e5%b7%a6%e5%8f%b3%e8%87%aa%e5%b7%b1%e7%9a%84%e5%ba%94%e7%94%a8%e7%a8%8b%e5%ba%8f%e5%88%b0-ubuntu-1804-%e9%87%8c%e9%82%a3%e4%b9%88%e5%b0%b1%e6%98%af%e8%af%b4%e5%9c%a8%e6%80%bb%e5%85%b1-50gb-%e7%9a%84%e6%95%b0%e6%8d%ae%e9%87%8c%e6%9c%89-90-%e7%9a%84%e6%95%b0%e6%8d%ae%e6%98%af%e5%86%97%e4%bd%99%e7%9a%84">#&lt;/a>
&lt;/h5>
&lt;h5 id="所以有没有一种文件系统可以达到用户的应用程序可以重复利用-ubuntu1804-也就是能够有效地减少磁盘上冗余的镜像数据同时减少冗余的镜像数据在网络上的传输这类的文件系统被称为-unionfs下图就是-unionfs-解决问题的实现">
 所以有没有一种文件系统可以达到用户的应用程序可以重复利用 ubuntu:18.04 ？也就是能够有效地减少磁盘上冗余的镜像数据，同时减少冗余的镜像数据在网络上的传输。这类的文件系统被称为 &lt;strong>UnionFS&lt;/strong>。下图就是 &lt;strong>UnionFS&lt;/strong> 解决问题的实现。
 &lt;a class="anchor" href="#%e6%89%80%e4%bb%a5%e6%9c%89%e6%b2%a1%e6%9c%89%e4%b8%80%e7%a7%8d%e6%96%87%e4%bb%b6%e7%b3%bb%e7%bb%9f%e5%8f%af%e4%bb%a5%e8%be%be%e5%88%b0%e7%94%a8%e6%88%b7%e7%9a%84%e5%ba%94%e7%94%a8%e7%a8%8b%e5%ba%8f%e5%8f%af%e4%bb%a5%e9%87%8d%e5%a4%8d%e5%88%a9%e7%94%a8-ubuntu1804-%e4%b9%9f%e5%b0%b1%e6%98%af%e8%83%bd%e5%a4%9f%e6%9c%89%e6%95%88%e5%9c%b0%e5%87%8f%e5%b0%91%e7%a3%81%e7%9b%98%e4%b8%8a%e5%86%97%e4%bd%99%e7%9a%84%e9%95%9c%e5%83%8f%e6%95%b0%e6%8d%ae%e5%90%8c%e6%97%b6%e5%87%8f%e5%b0%91%e5%86%97%e4%bd%99%e7%9a%84%e9%95%9c%e5%83%8f%e6%95%b0%e6%8d%ae%e5%9c%a8%e7%bd%91%e7%bb%9c%e4%b8%8a%e7%9a%84%e4%bc%a0%e8%be%93%e8%bf%99%e7%b1%bb%e7%9a%84%e6%96%87%e4%bb%b6%e7%b3%bb%e7%bb%9f%e8%a2%ab%e7%a7%b0%e4%b8%ba-unionfs%e4%b8%8b%e5%9b%be%e5%b0%b1%e6%98%af-unionfs-%e8%a7%a3%e5%86%b3%e9%97%ae%e9%a2%98%e7%9a%84%e5%ae%9e%e7%8e%b0">#&lt;/a>
&lt;/h5>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403191550850.png" alt="image-20240319155030778" />&lt;/p></description></item><item><title>2024-04-03 容器网络原理</title><link>https://qq547475331.github.io/docs/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E5%8E%9F%E7%90%86-rong-qi-wang-luo-yuan-li/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E5%8E%9F%E7%90%86-rong-qi-wang-luo-yuan-li/</guid><description>&lt;h1 id="容器网络">
 容器网络
 &lt;a class="anchor" href="#%e5%ae%b9%e5%99%a8%e7%bd%91%e7%bb%9c">#&lt;/a>
&lt;/h1>
&lt;h5 id="前文容器原理一文说到容器在网络上容器使用-network-namespace-实现对网络资源的隔离被隔离的进程只能看到当前-namespace-里的网络栈">
 前文&lt;a href="http://mp.weixin.qq.com/s?__biz=MzkyNDMyNjAyMg==&amp;amp;mid=2247484030&amp;amp;idx=1&amp;amp;sn=43bd06072aa327ded21d6513b891d84d&amp;amp;chksm=c1d6c397f6a14a81cbef67203522f0a5201492b161d1faf205d7d271b2e6afe64f1a2b710391&amp;amp;scene=21#wechat_redirect">容器原理&lt;/a>一文说到容器在网络上，容器使用 &lt;code>Network Namespace&lt;/code> 实现对网络资源的隔离，被隔离的进程只能看到当前 &lt;code>Namespace&lt;/code> 里的&lt;strong>网络栈。&lt;/strong>
 &lt;a class="anchor" href="#%e5%89%8d%e6%96%87%e5%ae%b9%e5%99%a8%e5%8e%9f%e7%90%86%e4%b8%80%e6%96%87%e8%af%b4%e5%88%b0%e5%ae%b9%e5%99%a8%e5%9c%a8%e7%bd%91%e7%bb%9c%e4%b8%8a%e5%ae%b9%e5%99%a8%e4%bd%bf%e7%94%a8-network-namespace-%e5%ae%9e%e7%8e%b0%e5%af%b9%e7%bd%91%e7%bb%9c%e8%b5%84%e6%ba%90%e7%9a%84%e9%9a%94%e7%a6%bb%e8%a2%ab%e9%9a%94%e7%a6%bb%e7%9a%84%e8%bf%9b%e7%a8%8b%e5%8f%aa%e8%83%bd%e7%9c%8b%e5%88%b0%e5%bd%93%e5%89%8d-namespace-%e9%87%8c%e7%9a%84%e7%bd%91%e7%bb%9c%e6%a0%88">#&lt;/a>
&lt;/h5>
&lt;h5 id="网络栈包括了网卡network-interface回环设备loopback-device路由表routing-table和-iptables-规则linux-宿主机的网络通过网络栈来实现同样容器的网络也是通过网络栈实现">
 &lt;strong>网络栈&lt;/strong>包括了：网卡（Network Interface）、回环设备（Loopback Device）、路由表（Routing Table）和 iptables 规则。Linux 宿主机的网络通过网络栈来实现，同样容器的网络也是通过网络栈实现。
 &lt;a class="anchor" href="#%e7%bd%91%e7%bb%9c%e6%a0%88%e5%8c%85%e6%8b%ac%e4%ba%86%e7%bd%91%e5%8d%a1network-interface%e5%9b%9e%e7%8e%af%e8%ae%be%e5%a4%87loopback-device%e8%b7%af%e7%94%b1%e8%a1%a8routing-table%e5%92%8c-iptables-%e8%a7%84%e5%88%99linux-%e5%ae%bf%e4%b8%bb%e6%9c%ba%e7%9a%84%e7%bd%91%e7%bb%9c%e9%80%9a%e8%bf%87%e7%bd%91%e7%bb%9c%e6%a0%88%e6%9d%a5%e5%ae%9e%e7%8e%b0%e5%90%8c%e6%a0%b7%e5%ae%b9%e5%99%a8%e7%9a%84%e7%bd%91%e7%bb%9c%e4%b9%9f%e6%98%af%e9%80%9a%e8%bf%87%e7%bd%91%e7%bb%9c%e6%a0%88%e5%ae%9e%e7%8e%b0">#&lt;/a>
&lt;/h5>
&lt;h5 id="容器网络需要解决哪些问题">
 容器网络需要解决哪些问题：
 &lt;a class="anchor" href="#%e5%ae%b9%e5%99%a8%e7%bd%91%e7%bb%9c%e9%9c%80%e8%a6%81%e8%a7%a3%e5%86%b3%e5%93%aa%e4%ba%9b%e9%97%ae%e9%a2%98">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="-容器-ip-的分配">
 • 容器 IP 的分配；
 &lt;a class="anchor" href="#-%e5%ae%b9%e5%99%a8-ip-%e7%9a%84%e5%88%86%e9%85%8d">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-容器之间的互相访问这里只考虑单节点容器间访问跨节点可考虑-kubernetes">
 • 容器之间的互相访问(这里只考虑单节点容器间访问，跨节点可考虑 Kubernetes)；
 &lt;a class="anchor" href="#-%e5%ae%b9%e5%99%a8%e4%b9%8b%e9%97%b4%e7%9a%84%e4%ba%92%e7%9b%b8%e8%ae%bf%e9%97%ae%e8%bf%99%e9%87%8c%e5%8f%aa%e8%80%83%e8%99%91%e5%8d%95%e8%8a%82%e7%82%b9%e5%ae%b9%e5%99%a8%e9%97%b4%e8%ae%bf%e9%97%ae%e8%b7%a8%e8%8a%82%e7%82%b9%e5%8f%af%e8%80%83%e8%99%91-kubernetes">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-容器如何访问主机外部网络">
 • 容器如何访问主机外部网络；
 &lt;a class="anchor" href="#-%e5%ae%b9%e5%99%a8%e5%a6%82%e4%bd%95%e8%ae%bf%e9%97%ae%e4%b8%bb%e6%9c%ba%e5%a4%96%e9%83%a8%e7%bd%91%e7%bb%9c">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-外部网络如何访问到容器内部">
 • 外部网络如何访问到容器内部。
 &lt;a class="anchor" href="#-%e5%a4%96%e9%83%a8%e7%bd%91%e7%bb%9c%e5%a6%82%e4%bd%95%e8%ae%bf%e9%97%ae%e5%88%b0%e5%ae%b9%e5%99%a8%e5%86%85%e9%83%a8">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h5 id="下面带着以上四个问题来看看容器网络的实现原理">
 下面带着以上四个问题来看看容器网络的实现原理。
 &lt;a class="anchor" href="#%e4%b8%8b%e9%9d%a2%e5%b8%a6%e7%9d%80%e4%bb%a5%e4%b8%8a%e5%9b%9b%e4%b8%aa%e9%97%ae%e9%a2%98%e6%9d%a5%e7%9c%8b%e7%9c%8b%e5%ae%b9%e5%99%a8%e7%bd%91%e7%bb%9c%e7%9a%84%e5%ae%9e%e7%8e%b0%e5%8e%9f%e7%90%86">#&lt;/a>
&lt;/h5>
&lt;h1 id="linux-虚拟网络技术">
 Linux 虚拟网络技术
 &lt;a class="anchor" href="#linux-%e8%99%9a%e6%8b%9f%e7%bd%91%e7%bb%9c%e6%8a%80%e6%9c%af">#&lt;/a>
&lt;/h1>
&lt;h5 id="一般网络设备包括交换机路由器网桥等这些网络设备会存在多个网卡或者端口那么-linux-不仅仅可以作为网络设备同时还可以实现虚拟网络设备例如网桥虚拟网卡对等那么-network-namespace-就是利用-linux-虚拟网络技术路由iptables-等技术来实现的下面看看常用的-veth-pair-和-bridge">
 一般网络设备包括，交换机，路由器，网桥等，这些网络设备会存在多个网卡或者端口，那么 Linux 不仅仅可以作为网络设备，同时还可以实现虚拟网络设备，例如：网桥，虚拟网卡对等，那么 &lt;code>Network Namespace&lt;/code> 就是利用 Linux 虚拟网络技术、路由、iptables 等技术来实现的。下面看看常用的 &lt;strong>veth pair&lt;/strong> 和 &lt;strong>bridge&lt;/strong>。
 &lt;a class="anchor" href="#%e4%b8%80%e8%88%ac%e7%bd%91%e7%bb%9c%e8%ae%be%e5%a4%87%e5%8c%85%e6%8b%ac%e4%ba%a4%e6%8d%a2%e6%9c%ba%e8%b7%af%e7%94%b1%e5%99%a8%e7%bd%91%e6%a1%a5%e7%ad%89%e8%bf%99%e4%ba%9b%e7%bd%91%e7%bb%9c%e8%ae%be%e5%a4%87%e4%bc%9a%e5%ad%98%e5%9c%a8%e5%a4%9a%e4%b8%aa%e7%bd%91%e5%8d%a1%e6%88%96%e8%80%85%e7%ab%af%e5%8f%a3%e9%82%a3%e4%b9%88-linux-%e4%b8%8d%e4%bb%85%e4%bb%85%e5%8f%af%e4%bb%a5%e4%bd%9c%e4%b8%ba%e7%bd%91%e7%bb%9c%e8%ae%be%e5%a4%87%e5%90%8c%e6%97%b6%e8%bf%98%e5%8f%af%e4%bb%a5%e5%ae%9e%e7%8e%b0%e8%99%9a%e6%8b%9f%e7%bd%91%e7%bb%9c%e8%ae%be%e5%a4%87%e4%be%8b%e5%a6%82%e7%bd%91%e6%a1%a5%e8%99%9a%e6%8b%9f%e7%bd%91%e5%8d%a1%e5%af%b9%e7%ad%89%e9%82%a3%e4%b9%88-network-namespace-%e5%b0%b1%e6%98%af%e5%88%a9%e7%94%a8-linux-%e8%99%9a%e6%8b%9f%e7%bd%91%e7%bb%9c%e6%8a%80%e6%9c%af%e8%b7%af%e7%94%b1iptables-%e7%ad%89%e6%8a%80%e6%9c%af%e6%9d%a5%e5%ae%9e%e7%8e%b0%e7%9a%84%e4%b8%8b%e9%9d%a2%e7%9c%8b%e7%9c%8b%e5%b8%b8%e7%94%a8%e7%9a%84-veth-pair-%e5%92%8c-bridge">#&lt;/a>
&lt;/h5>
&lt;h1 id="linux-veth-pair">
 Linux veth pair
 &lt;a class="anchor" href="#linux-veth-pair">#&lt;/a>
&lt;/h1>
&lt;h5 id="veth-pair-是成对出现的一种虚拟网络设备接口一端连着网络协议栈一端彼此相连veth-pair-总是成对出现的从一端进入的数据包将会在另一端出现我们可以把-veth-pair-看成一条网线两端连接的两张以太网卡只要将-veth-pair-每一段分别接入不同的-namespace那么这两个-namespace-就可以实现互通了">
 &lt;strong>veth pair&lt;/strong> 是成对出现的一种虚拟网络设备接口，一端连着网络协议栈，一端彼此相连。&lt;strong>veth pair&lt;/strong> 总是成对出现的，从一端进入的数据包将会在另一端出现。我们可以把 &lt;strong>veth pair&lt;/strong> 看成一条网线两端连接的两张以太网卡。只要将 &lt;strong>veth pair&lt;/strong> 每一段分别接入不同的 &lt;code>Namespace&lt;/code>，那么这两个 &lt;code>Namespace&lt;/code> 就可以实现互通了。
 &lt;a class="anchor" href="#veth-pair-%e6%98%af%e6%88%90%e5%af%b9%e5%87%ba%e7%8e%b0%e7%9a%84%e4%b8%80%e7%a7%8d%e8%99%9a%e6%8b%9f%e7%bd%91%e7%bb%9c%e8%ae%be%e5%a4%87%e6%8e%a5%e5%8f%a3%e4%b8%80%e7%ab%af%e8%bf%9e%e7%9d%80%e7%bd%91%e7%bb%9c%e5%8d%8f%e8%ae%ae%e6%a0%88%e4%b8%80%e7%ab%af%e5%bd%bc%e6%ad%a4%e7%9b%b8%e8%bf%9eveth-pair-%e6%80%bb%e6%98%af%e6%88%90%e5%af%b9%e5%87%ba%e7%8e%b0%e7%9a%84%e4%bb%8e%e4%b8%80%e7%ab%af%e8%bf%9b%e5%85%a5%e7%9a%84%e6%95%b0%e6%8d%ae%e5%8c%85%e5%b0%86%e4%bc%9a%e5%9c%a8%e5%8f%a6%e4%b8%80%e7%ab%af%e5%87%ba%e7%8e%b0%e6%88%91%e4%bb%ac%e5%8f%af%e4%bb%a5%e6%8a%8a-veth-pair-%e7%9c%8b%e6%88%90%e4%b8%80%e6%9d%a1%e7%bd%91%e7%ba%bf%e4%b8%a4%e7%ab%af%e8%bf%9e%e6%8e%a5%e7%9a%84%e4%b8%a4%e5%bc%a0%e4%bb%a5%e5%a4%aa%e7%bd%91%e5%8d%a1%e5%8f%aa%e8%a6%81%e5%b0%86-veth-pair-%e6%af%8f%e4%b8%80%e6%ae%b5%e5%88%86%e5%88%ab%e6%8e%a5%e5%85%a5%e4%b8%8d%e5%90%8c%e7%9a%84-namespace%e9%82%a3%e4%b9%88%e8%bf%99%e4%b8%a4%e4%b8%aa-namespace-%e5%b0%b1%e5%8f%af%e4%bb%a5%e5%ae%9e%e7%8e%b0%e4%ba%92%e9%80%9a%e4%ba%86">#&lt;/a>
&lt;/h5>
&lt;blockquote>
&lt;p>Linux 即使在同一个主机上创建的两个 &lt;code>Network Namespace&lt;/code>，相互之间缺省也是不能进行网络通信的。&lt;/p></description></item><item><title>2024-04-03 搞懂K8S鉴权</title><link>https://qq547475331.github.io/docs/%E6%90%9E%E6%87%82k8s%E9%89%B4%E6%9D%83-gao-dong-k8s-jian-quan/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/%E6%90%9E%E6%87%82k8s%E9%89%B4%E6%9D%83-gao-dong-k8s-jian-quan/</guid><description>&lt;h2 id="前言">
 &lt;strong>前言&lt;/strong>
 &lt;a class="anchor" href="#%e5%89%8d%e8%a8%80">#&lt;/a>
&lt;/h2>
&lt;p>本文介绍K8s中的鉴权模块。对其4种鉴权模式都进行了概述讲解。结合例子着重对大家日常中使用最多的RBAC鉴权模式进行了说明。&lt;/p>
&lt;h2 id="鉴权概述">
 &lt;strong>鉴权概述&lt;/strong>
 &lt;a class="anchor" href="#%e9%89%b4%e6%9d%83%e6%a6%82%e8%bf%b0">#&lt;/a>
&lt;/h2>
&lt;p>《搞懂K8s认证》中，我们提到不论是通过kubectl客户端还是REST请求访问K8s集群，最终都需要经过API Server来进行资源的操作并通过Etcd。整个过程如下图1所示，可以分成4个阶段：&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202402221152527.png" alt="image-20240222115206444" />图1 K8s API请求访问过程&lt;/p>
&lt;p>请求发起方进行K8s API请求，经过&lt;code>Authentication&lt;/code>（认证）、&lt;code>Authorization&lt;/code>（鉴权）、&lt;code>AdmissionControl&lt;/code>（准入控制）三个阶段的校验，最后把请求转化为对K8s对象的变更操作持久化至&lt;code>etcd&lt;/code>中。&lt;/p>
&lt;p>其中认证主要解决的是请求来源能否访问的问题。即通过了认证，那么可以认为它是一个合法的请求对象。那么如何去决定请求对象能访问哪些资源以及对这些资源能进行哪些操作，便是鉴权所要完成的事情了。&lt;/p>
&lt;p>鉴权的最终目的，是区分请求对象，限定操作的影响范围，让其使用最小的权限完成自己所要进行操作，从而进一步保证安全。权限控制的划分方式有许多种，K8s中提供了4种鉴权模式，分别为&lt;strong>Node、ABAC、RBAC&lt;/strong>和&lt;strong>Webhook&lt;/strong>。&lt;/p>
&lt;p>默认情况下，我们可以从&lt;code>/etc/kubernates/manifests/kube-apiserver.yaml&lt;/code>文件中查看&lt;code>apiserver&lt;/code>启动时认证模式，片段如图2所示：&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202402221152166.png" alt="image-20240222115225063" />&lt;/p>
&lt;p>图2 kube-apiserver的认证参数&lt;/p>
&lt;p>其中可以使用的参数如表1所示：&lt;/p>
&lt;p>表1 鉴权模块参数标识表&lt;/p>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th style="text-align: left">参数配置&lt;/th>
 &lt;th style="text-align: left">含义&lt;/th>
 &lt;th style="text-align: left">一般的使用场景&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td style="text-align: left">&amp;ndash;authorization-mode=ABAC&lt;/td>
 &lt;td style="text-align: left">使用基于属性的访问控制(ABAC)&lt;/td>
 &lt;td style="text-align: left">根据用户的用户名或者组名来控制其对集群资源的访问权限，适用于较小的组织或开发团队&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">&amp;ndash;authorization-mode=RBAC&lt;/td>
 &lt;td style="text-align: left">使用基于角色的访问控制(RBAC)&lt;/td>
 &lt;td style="text-align: left">自定义ServiceAccount，绑定资源根据角色来控制资源的访问权限，适用于较大型的组织或者开发运维团队&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">&amp;ndash;authorization-mode=Webhook&lt;/td>
 &lt;td style="text-align: left">使用HTTP回调模式，允许你使用远程REST端点管理鉴权&lt;/td>
 &lt;td style="text-align: left">将鉴权角色交给外部服务进行处理，根据自身需求，定制和扩展鉴权策略，如自定义Webhook鉴权模块对跨云平台的应用进行集中的的访问控制&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">&amp;ndash;authorization-mode=Node&lt;/td>
 &lt;td style="text-align: left">针对kubelet发出的API请求执行鉴权&lt;/td>
 &lt;td style="text-align: left">验证节点身份，确保只有经过身份验证且具有所需权限的Node才能连接到K8s集群&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">&amp;ndash;authorization-mode=AlwaysDeny&lt;/td>
 &lt;td style="text-align: left">阻止所有请求&lt;/td>
 &lt;td style="text-align: left">一般仅用作测试&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">&amp;ndash;authorization-mode=AlwaysAllow&lt;/td>
 &lt;td style="text-align: left">允许所有请求&lt;/td>
 &lt;td style="text-align: left">不需要API请求进行鉴权的场景&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;p>如图2所示，可以同时配置多个鉴权模块（多个模块之间使用逗号分隔），排在靠前的模块优先执行，任何模式允许或拒接请求，则立即返回该决定，并不会与其他鉴权模块协商。&lt;/p>
&lt;h2 id="node鉴权">
 &lt;strong>Node鉴权&lt;/strong>
 &lt;a class="anchor" href="#node%e9%89%b4%e6%9d%83">#&lt;/a>
&lt;/h2>
&lt;p>Node鉴权是一种特殊用途的鉴权模式，旨在对&lt;code>kubelet&lt;/code>发出API请求进行授权。Node鉴权允许&lt;code>kubelet&lt;/code>执行API的操作分成读和写两部分。读取操作控制范围为：&lt;code>services、endpoints、nodes、pods&lt;/code>以及绑定到kubelet节点 &lt;code>Pod&lt;/code>相关的&lt;code>secret、configmap、pvc&lt;/code>和持久卷。写入操作的范围主要是节点和节点状态、Pod和Pod状态以及事件，若要限制&lt;code>kubelet&lt;/code>只能修改自己的节点，则还需要在&lt;code>Apiserver&lt;/code>启动时，开启&lt;code>NodeRestriction&lt;/code>准入插件（见图2第二个红框）。&lt;/p>
&lt;p>开启Node鉴权模块后，&lt;code>kubellet&lt;/code>为了获取授权，必须使用一个特定规则的凭据，如图3所示：&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202402221152052.png" alt="image-20240222115246974" />&lt;/p>
&lt;p>图3 kubelet的证书凭据&lt;/p>
&lt;p>从图中我们看到，&lt;code>kubelet&lt;/code>使用了一个证书凭据，其中&lt;code>O=system:nodes&lt;/code>表示其所在组，&lt;code>CN=system:node:paas-cnp-k8s-kce-01&lt;/code>表示其用户名，满足了Node鉴权模块要求的组名必须为&lt;code>system:nodes&lt;/code>，用户名必须为&lt;code>system:node:&amp;lt;nodeName&amp;gt;&lt;/code>的要求。其中&lt;code>&amp;lt;nodeName&amp;gt;&lt;/code>默认由&lt;code>hostname&lt;/code>或&lt;code>kubelet --hostname-override&lt;/code>选项提供指定，其必须与&lt;code>kubelet&lt;/code>提供的主机名称精确匹配。&lt;/p>
&lt;p>&lt;code>system:nodes&lt;/code>是K8s的内置用户组，我们可以通过其默认的&lt;code>ClusterRoleBinding&lt;/code>，如图4所示：&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202402221153564.png" alt="image-20240222115305481" />&lt;/p>
&lt;p>图4 system:nodes的ClusterRoleBinding内容&lt;/p>
&lt;p>我们可以发现，它指示指向了&lt;code>system:node&lt;/code>这个&lt;code>ClusterRole&lt;/code>，并没有subjects的内容，即其没有绑定&lt;code>system:node:paas-cnp-k8s-kce-01&lt;/code>用户也没有绑定&lt;code>system:nodes&lt;/code>组。其正是因为K8s基于 Node鉴权模块来限制kubelet只能读取和修改本节点上的资源，并不是使用 RBAC来鉴权（涉及到部分RBAC的内容，下文会进行详解）。&lt;/p>
&lt;h2 id="abac鉴权">
 &lt;strong>ABAC鉴权&lt;/strong>
 &lt;a class="anchor" href="#abac%e9%89%b4%e6%9d%83">#&lt;/a>
&lt;/h2>
&lt;p>基于属性的访问控制，K8s中可以表述将访问策略授予用户或者组。与RBAC不同的点在于，其策略是由任何类型的属性（用户属性、资源属性、对象、环境等）进行描述。&lt;/p></description></item><item><title>2024-04-03 文学的故乡</title><link>https://qq547475331.github.io/docs/%E6%96%87%E5%AD%A6%E7%9A%84%E6%95%85%E4%B9%A1-wen-xue-de-gu-xiang/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/%E6%96%87%E5%AD%A6%E7%9A%84%E6%95%85%E4%B9%A1-wen-xue-de-gu-xiang/</guid><description>&lt;p>奇怪，央视怎么啥神仙阵容都能搞到&lt;/p>
&lt;p>仿佛那里有一个童年时的自己，在那里思考的是成年的自己，风景像桥梁一样将两者相连，我认为这就是文学的作用。有一个叫莫言的少年，在这片风景中悲伤过，痛苦过，害怕过，真爱过。&lt;/p>
&lt;p>常有人问Sir。&lt;/p>
&lt;p>想提高电影审美品味，有没有特别招数。&lt;/p>
&lt;p>有是有，就怕你嫌Sir暴露年龄——&lt;/p>
&lt;p>看文学作品。&lt;/p>
&lt;p>&lt;strong>因为有时分电影之高下的，并不是故事，而是“叙事”。&lt;/strong>&lt;/p>
&lt;h5 id="好比杜琪峰最喜欢自己的柔道龙虎榜sir也好喜欢可很多观众觉得也就so-so啊不就是一个小故事吗">
 好比杜琪峰最喜欢自己的《柔道龙虎榜》（Sir也好喜欢），可很多观众觉得，也就so so啊，不就是一个小故事吗？
 &lt;a class="anchor" href="#%e5%a5%bd%e6%af%94%e6%9d%9c%e7%90%aa%e5%b3%b0%e6%9c%80%e5%96%9c%e6%ac%a2%e8%87%aa%e5%b7%b1%e7%9a%84%e6%9f%94%e9%81%93%e9%be%99%e8%99%8e%e6%a6%9csir%e4%b9%9f%e5%a5%bd%e5%96%9c%e6%ac%a2%e5%8f%af%e5%be%88%e5%a4%9a%e8%a7%82%e4%bc%97%e8%a7%89%e5%be%97%e4%b9%9f%e5%b0%b1so-so%e5%95%8a%e4%b8%8d%e5%b0%b1%e6%98%af%e4%b8%80%e4%b8%aa%e5%b0%8f%e6%95%85%e4%ba%8b%e5%90%97">#&lt;/a>
&lt;/h5>
&lt;h5 id="对它的高级之处就是杜氏叙事而好的电影叙事能引出故事本身所没有的哲学和美学体会用人话说就是像诗哦好有想象哦等等">
 对，它的高级之处，就是杜氏叙事。而好的电影叙事，能引出故事本身所没有的哲学和美学体会，用人话说就是……“像诗哦、好有想象哦”等等。
 &lt;a class="anchor" href="#%e5%af%b9%e5%ae%83%e7%9a%84%e9%ab%98%e7%ba%a7%e4%b9%8b%e5%a4%84%e5%b0%b1%e6%98%af%e6%9d%9c%e6%b0%8f%e5%8f%99%e4%ba%8b%e8%80%8c%e5%a5%bd%e7%9a%84%e7%94%b5%e5%bd%b1%e5%8f%99%e4%ba%8b%e8%83%bd%e5%bc%95%e5%87%ba%e6%95%85%e4%ba%8b%e6%9c%ac%e8%ba%ab%e6%89%80%e6%b2%a1%e6%9c%89%e7%9a%84%e5%93%b2%e5%ad%a6%e5%92%8c%e7%be%8e%e5%ad%a6%e4%bd%93%e4%bc%9a%e7%94%a8%e4%ba%ba%e8%af%9d%e8%af%b4%e5%b0%b1%e6%98%af%e5%83%8f%e8%af%97%e5%93%a6%e5%a5%bd%e6%9c%89%e6%83%b3%e8%b1%a1%e5%93%a6%e7%ad%89%e7%ad%89">#&lt;/a>
&lt;/h5>
&lt;h5 id="但叙事这门手艺从哪来">
 但叙事这门手艺，从哪来？
 &lt;a class="anchor" href="#%e4%bd%86%e5%8f%99%e4%ba%8b%e8%bf%99%e9%97%a8%e6%89%8b%e8%89%ba%e4%bb%8e%e5%93%aa%e6%9d%a5">#&lt;/a>
&lt;/h5>
&lt;h5 id="最早就从文学来">
 最早，就从文学来。
 &lt;a class="anchor" href="#%e6%9c%80%e6%97%a9%e5%b0%b1%e4%bb%8e%e6%96%87%e5%ad%a6%e6%9d%a5">#&lt;/a>
&lt;/h5>
&lt;h5 id="你说你太忙啦没什么时间好的sir理解那么">
 你说你太忙啦没什么时间，好的Sir理解，那么。
 &lt;a class="anchor" href="#%e4%bd%a0%e8%af%b4%e4%bd%a0%e5%a4%aa%e5%bf%99%e5%95%a6%e6%b2%a1%e4%bb%80%e4%b9%88%e6%97%b6%e9%97%b4%e5%a5%bd%e7%9a%84sir%e7%90%86%e8%a7%a3%e9%82%a3%e4%b9%88">#&lt;/a>
&lt;/h5>
&lt;h5 id="看一部纪录片的时间总有吧">
 看一部纪录片的时间总有吧？
 &lt;a class="anchor" href="#%e7%9c%8b%e4%b8%80%e9%83%a8%e7%ba%aa%e5%bd%95%e7%89%87%e7%9a%84%e6%97%b6%e9%97%b4%e6%80%bb%e6%9c%89%e5%90%a7">#&lt;/a>
&lt;/h5>
&lt;h5 id="这部片里记录了中国几位顶级作家比如贾平凹莫言等你也别说乡土题材你不感冒">
 这部片里记录了中国几位顶级作家，比如贾平凹、莫言等……你也别说乡土题材你不感冒。
 &lt;a class="anchor" href="#%e8%bf%99%e9%83%a8%e7%89%87%e9%87%8c%e8%ae%b0%e5%bd%95%e4%ba%86%e4%b8%ad%e5%9b%bd%e5%87%a0%e4%bd%8d%e9%a1%b6%e7%ba%a7%e4%bd%9c%e5%ae%b6%e6%af%94%e5%a6%82%e8%b4%be%e5%b9%b3%e5%87%b9%e8%8e%ab%e8%a8%80%e7%ad%89%e4%bd%a0%e4%b9%9f%e5%88%ab%e8%af%b4%e4%b9%a1%e5%9c%9f%e9%a2%98%e6%9d%90%e4%bd%a0%e4%b8%8d%e6%84%9f%e5%86%92">#&lt;/a>
&lt;/h5>
&lt;h5 id="就算你是城市青年你也是城市这片土地长大的">
 就算你是城市青年，你也是城市这片土地长大的。
 &lt;a class="anchor" href="#%e5%b0%b1%e7%ae%97%e4%bd%a0%e6%98%af%e5%9f%8e%e5%b8%82%e9%9d%92%e5%b9%b4%e4%bd%a0%e4%b9%9f%e6%98%af%e5%9f%8e%e5%b8%82%e8%bf%99%e7%89%87%e5%9c%9f%e5%9c%b0%e9%95%bf%e5%a4%a7%e7%9a%84">#&lt;/a>
&lt;/h5>
&lt;h5 id="好比伍迪艾伦这种纽约老小子也是纽约那片土地养大的">
 好比伍迪·艾伦这种纽约（老）小子，也是纽约那片土地养大的。
 &lt;a class="anchor" href="#%e5%a5%bd%e6%af%94%e4%bc%8d%e8%bf%aa%e8%89%be%e4%bc%a6%e8%bf%99%e7%a7%8d%e7%ba%bd%e7%ba%a6%e8%80%81%e5%b0%8f%e5%ad%90%e4%b9%9f%e6%98%af%e7%ba%bd%e7%ba%a6%e9%82%a3%e7%89%87%e5%9c%9f%e5%9c%b0%e5%85%bb%e5%a4%a7%e7%9a%84">#&lt;/a>
&lt;/h5>
&lt;p>懂了吧？&lt;/p>
&lt;p>**叙事不是时尚穿搭，没有什么土和洋。&lt;/p>
&lt;p>**&lt;/p>
&lt;h5 id="而这部纪录片起码可以为你的电影审美开一个绝对不土的好头">
 而这部纪录片，起码可以为你的电影审美，开一个绝对不土的好头——
 &lt;a class="anchor" href="#%e8%80%8c%e8%bf%99%e9%83%a8%e7%ba%aa%e5%bd%95%e7%89%87%e8%b5%b7%e7%a0%81%e5%8f%af%e4%bb%a5%e4%b8%ba%e4%bd%a0%e7%9a%84%e7%94%b5%e5%bd%b1%e5%ae%a1%e7%be%8e%e5%bc%80%e4%b8%80%e4%b8%aa%e7%bb%9d%e5%af%b9%e4%b8%8d%e5%9c%9f%e7%9a%84%e5%a5%bd%e5%a4%b4">#&lt;/a>
&lt;/h5>
&lt;p>&lt;strong>文学的故乡&lt;/strong>&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403201024638.png" alt="image-20240320102418539" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403201024819.png" alt="image-20240320102433770" />&lt;/p>
&lt;h5 id="sir先来一个假设">
 Sir先来一个假设。
 &lt;a class="anchor" href="#sir%e5%85%88%e6%9d%a5%e4%b8%80%e4%b8%aa%e5%81%87%e8%ae%be">#&lt;/a>
&lt;/h5>
&lt;h5 id="假如时光倒流你有一次这样的机会">
 假如时光倒流，你有一次这样的机会：
 &lt;a class="anchor" href="#%e5%81%87%e5%a6%82%e6%97%b6%e5%85%89%e5%80%92%e6%b5%81%e4%bd%a0%e6%9c%89%e4%b8%80%e6%ac%a1%e8%bf%99%e6%a0%b7%e7%9a%84%e6%9c%ba%e4%bc%9a">#&lt;/a>
&lt;/h5>
&lt;h5 id="鲁迅先生带你游逛他的家乡指给你看他和闰土一起捕过鸟的地方一起偷过西瓜的田地甚至如果闰土还活着他还会让闰土给你签个名">
 鲁迅先生带你游逛他的家乡，指给你看他和闰土一起捕过鸟的地方，一起偷过西瓜的田地&amp;hellip;&amp;hellip;.甚至如果闰土还活着，他还会让闰土给你签个名；
 &lt;a class="anchor" href="#%e9%b2%81%e8%bf%85%e5%85%88%e7%94%9f%e5%b8%a6%e4%bd%a0%e6%b8%b8%e9%80%9b%e4%bb%96%e7%9a%84%e5%ae%b6%e4%b9%a1%e6%8c%87%e7%bb%99%e4%bd%a0%e7%9c%8b%e4%bb%96%e5%92%8c%e9%97%b0%e5%9c%9f%e4%b8%80%e8%b5%b7%e6%8d%95%e8%bf%87%e9%b8%9f%e7%9a%84%e5%9c%b0%e6%96%b9%e4%b8%80%e8%b5%b7%e5%81%b7%e8%bf%87%e8%a5%bf%e7%93%9c%e7%9a%84%e7%94%b0%e5%9c%b0%e7%94%9a%e8%87%b3%e5%a6%82%e6%9e%9c%e9%97%b0%e5%9c%9f%e8%bf%98%e6%b4%bb%e7%9d%80%e4%bb%96%e8%bf%98%e4%bc%9a%e8%ae%a9%e9%97%b0%e5%9c%9f%e7%bb%99%e4%bd%a0%e7%ad%be%e4%b8%aa%e5%90%8d">#&lt;/a>
&lt;/h5>
&lt;h5 id="又或者鲁迅路线一转准备带你去上海滩走走不是现在的上海滩是100年前那个逛逛他上海租界的家说说他和妻子的日常如果恰好遇上萧红来此借宿哇你又有了一张合影">
 又或者鲁迅路线一转，准备带你去上海滩走走。不是现在的上海滩，是100年前那个，逛逛他上海租界的家，说说他和妻子的日常。如果恰好遇上萧红来此借宿，哇，你又有了一张合影；
 &lt;a class="anchor" href="#%e5%8f%88%e6%88%96%e8%80%85%e9%b2%81%e8%bf%85%e8%b7%af%e7%ba%bf%e4%b8%80%e8%bd%ac%e5%87%86%e5%a4%87%e5%b8%a6%e4%bd%a0%e5%8e%bb%e4%b8%8a%e6%b5%b7%e6%bb%a9%e8%b5%b0%e8%b5%b0%e4%b8%8d%e6%98%af%e7%8e%b0%e5%9c%a8%e7%9a%84%e4%b8%8a%e6%b5%b7%e6%bb%a9%e6%98%af100%e5%b9%b4%e5%89%8d%e9%82%a3%e4%b8%aa%e9%80%9b%e9%80%9b%e4%bb%96%e4%b8%8a%e6%b5%b7%e7%a7%9f%e7%95%8c%e7%9a%84%e5%ae%b6%e8%af%b4%e8%af%b4%e4%bb%96%e5%92%8c%e5%a6%bb%e5%ad%90%e7%9a%84%e6%97%a5%e5%b8%b8%e5%a6%82%e6%9e%9c%e6%81%b0%e5%a5%bd%e9%81%87%e4%b8%8a%e8%90%a7%e7%ba%a2%e6%9d%a5%e6%ad%a4%e5%80%9f%e5%ae%bf%e5%93%87%e4%bd%a0%e5%8f%88%e6%9c%89%e4%ba%86%e4%b8%80%e5%bc%a0%e5%90%88%e5%bd%b1">#&lt;/a>
&lt;/h5>
&lt;h5 id="你去不去免费保证没有旅游点强买强卖">
 你去不去？（免费，保证没有旅游点强买强卖）
 &lt;a class="anchor" href="#%e4%bd%a0%e5%8e%bb%e4%b8%8d%e5%8e%bb%e5%85%8d%e8%b4%b9%e4%bf%9d%e8%af%81%e6%b2%a1%e6%9c%89%e6%97%85%e6%b8%b8%e7%82%b9%e5%bc%ba%e4%b9%b0%e5%bc%ba%e5%8d%96">#&lt;/a>
&lt;/h5>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403201024022.png" alt="image-20240320102456901" />&lt;/p>
&lt;h5 id="-电影黄金时代">
 △ 电影《黄金时代》
 &lt;a class="anchor" href="#-%e7%94%b5%e5%bd%b1%e9%bb%84%e9%87%91%e6%97%b6%e4%bb%a3">#&lt;/a>
&lt;/h5>
&lt;h5 id="行了冷静">
 行了，冷静。
 &lt;a class="anchor" href="#%e8%a1%8c%e4%ba%86%e5%86%b7%e9%9d%99">#&lt;/a>
&lt;/h5>
&lt;h5 id="想去也没有">
 想去，也没有。
 &lt;a class="anchor" href="#%e6%83%b3%e5%8e%bb%e4%b9%9f%e6%b2%a1%e6%9c%89">#&lt;/a>
&lt;/h5>
&lt;h5 id="因为遗憾的是">
 因为遗憾的是。
 &lt;a class="anchor" href="#%e5%9b%a0%e4%b8%ba%e9%81%97%e6%86%be%e7%9a%84%e6%98%af">#&lt;/a>
&lt;/h5>
&lt;h5 id="鲁迅生前并没留下任何录像资料">
 鲁迅生前，并没留下任何录像资料。
 &lt;a class="anchor" href="#%e9%b2%81%e8%bf%85%e7%94%9f%e5%89%8d%e5%b9%b6%e6%b2%a1%e7%95%99%e4%b8%8b%e4%bb%bb%e4%bd%95%e5%bd%95%e5%83%8f%e8%b5%84%e6%96%99">#&lt;/a>
&lt;/h5>
&lt;h5 id="文学的故乡的导演张同道他也有同样的遗憾">
 《文学的故乡》的导演张同道，他也有同样的遗憾——
 &lt;a class="anchor" href="#%e6%96%87%e5%ad%a6%e7%9a%84%e6%95%85%e4%b9%a1%e7%9a%84%e5%af%bc%e6%bc%94%e5%bc%a0%e5%90%8c%e9%81%93%e4%bb%96%e4%b9%9f%e6%9c%89%e5%90%8c%e6%a0%b7%e7%9a%84%e9%81%97%e6%86%be">#&lt;/a>
&lt;/h5>
&lt;h5 id="先生去世之后有电影人拍鲁迅先生的葬礼可就是没人在先生健康的时候去拍一拍他的生活纪实有不可替代的力量多大的明星去扮演他也不是他">
 “先生去世之后，有电影人拍鲁迅先生的葬礼，可就是没人在先生健康的时候去拍一拍他的生活。纪实有不可替代的力量，多大的明星去扮演他，也不是他。”
 &lt;a class="anchor" href="#%e5%85%88%e7%94%9f%e5%8e%bb%e4%b8%96%e4%b9%8b%e5%90%8e%e6%9c%89%e7%94%b5%e5%bd%b1%e4%ba%ba%e6%8b%8d%e9%b2%81%e8%bf%85%e5%85%88%e7%94%9f%e7%9a%84%e8%91%ac%e7%a4%bc%e5%8f%af%e5%b0%b1%e6%98%af%e6%b2%a1%e4%ba%ba%e5%9c%a8%e5%85%88%e7%94%9f%e5%81%a5%e5%ba%b7%e7%9a%84%e6%97%b6%e5%80%99%e5%8e%bb%e6%8b%8d%e4%b8%80%e6%8b%8d%e4%bb%96%e7%9a%84%e7%94%9f%e6%b4%bb%e7%ba%aa%e5%ae%9e%e6%9c%89%e4%b8%8d%e5%8f%af%e6%9b%bf%e4%bb%a3%e7%9a%84%e5%8a%9b%e9%87%8f%e5%a4%9a%e5%a4%a7%e7%9a%84%e6%98%8e%e6%98%9f%e5%8e%bb%e6%89%ae%e6%bc%94%e4%bb%96%e4%b9%9f%e4%b8%8d%e6%98%af%e4%bb%96">#&lt;/a>
&lt;/h5>
&lt;h5 id="所以">
 所以。
 &lt;a class="anchor" href="#%e6%89%80%e4%bb%a5">#&lt;/a>
&lt;/h5>
&lt;h5 id="为了防止再有这种类似的遗憾他制作了这部纪录片">
 为了防止再有这种类似的遗憾，他制作了这部纪录片。
 &lt;a class="anchor" href="#%e4%b8%ba%e4%ba%86%e9%98%b2%e6%ad%a2%e5%86%8d%e6%9c%89%e8%bf%99%e7%a7%8d%e7%b1%bb%e4%bc%bc%e7%9a%84%e9%81%97%e6%86%be%e4%bb%96%e5%88%b6%e4%bd%9c%e4%ba%86%e8%bf%99%e9%83%a8%e7%ba%aa%e5%bd%95%e7%89%87">#&lt;/a>
&lt;/h5>
&lt;h5 id="一是纪实录像对于历史记载的意义重大">
 一是，纪实录像对于历史记载的意义重大。
 &lt;a class="anchor" href="#%e4%b8%80%e6%98%af%e7%ba%aa%e5%ae%9e%e5%bd%95%e5%83%8f%e5%af%b9%e4%ba%8e%e5%8e%86%e5%8f%b2%e8%ae%b0%e8%bd%bd%e7%9a%84%e6%84%8f%e4%b9%89%e9%87%8d%e5%a4%a7">#&lt;/a>
&lt;/h5>
&lt;p>二是，&lt;strong>作为普通人，普通读者，终于有机会见到文字之外的一个鲜活的人。&lt;/strong>&lt;/p></description></item><item><title>2024-04-03 极大提高工作效率的 Linux 命令</title><link>https://qq547475331.github.io/docs/%E6%9E%81%E5%A4%A7%E6%8F%90%E9%AB%98%E5%B7%A5%E4%BD%9C%E6%95%88%E7%8E%87%E7%9A%84-linux-%E5%91%BD%E4%BB%A4-ji-da-ti-gao-gong-zuo-xiao-lv-de-linux-ming-ling/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/%E6%9E%81%E5%A4%A7%E6%8F%90%E9%AB%98%E5%B7%A5%E4%BD%9C%E6%95%88%E7%8E%87%E7%9A%84-linux-%E5%91%BD%E4%BB%A4-ji-da-ti-gao-gong-zuo-xiao-lv-de-linux-ming-ling/</guid><description>&lt;p>作为一名软件开发人员，掌握 Linux 命令是必不可少的技能。即使你使用 Windows 或 macOS，你总会遇到需要使用 Linux 命令的场合。例如，大多数 Docker 镜像都基于 Linux 系统。要进行 DevOps 工作，你需要熟悉Linux，至少要了解一些常用命令。&lt;/p>
&lt;p>在本文中，我将介绍一些我每天使用的命令。如果你是Linux的新手，或者想要更新、提高或加强对 Linux 命令的了解，本文对你可能会有所帮助。不过，本文不会重点介绍像 cd 或 ls 这样的基础命令，而是介绍一些从实践中学到的更高级的命令。&lt;/p>
&lt;h2 id="自定义bash提示符">
 &lt;strong>自定义bash提示符&lt;/strong>
 &lt;a class="anchor" href="#%e8%87%aa%e5%ae%9a%e4%b9%89bash%e6%8f%90%e7%a4%ba%e7%ac%a6">#&lt;/a>
&lt;/h2>
&lt;p>嗯，这个主题本身可以是一篇冗长的文章。不过，我们不需要学习所有的内容。在大多数情况下，我们只需要修改 PS1 变量，该变量指定在每个命令之前显示的内容。如果不修改它，提示符将显示路径，当我们深入到一个文件夹中时，这是非常不方便的。我更喜欢在提示符中只显示用户名和当前文件夹，可以通过以下命令设置：&lt;/p>
&lt;pre tabindex="0">&lt;code>linuxmi@linuxmi:~/www.linuxmi.com$ export PS1=&amp;#34;[\u@\W]\$&amp;#34;
&lt;/code>&lt;/pre>&lt;p>``&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403221412868.png" alt="image-20240322141202821" />&lt;/p>
&lt;p>这里，\u表示当前用户名，\W表示当前工作目录。[、]和@保持原样显示。对于$，如果当前用户不是root，则显示$，否则显示#。更多代码可以在这里找到。&lt;/p>
&lt;p>请注意，如果您希望更改持久化，这个命令和下面显示的命令应添加到 ~/.bashrc中。&lt;/p>
&lt;p>&lt;strong>查找文件或文件夹&lt;/strong>&lt;/p>
&lt;p>查找文件或文件夹是一个非常常见的需求，可以使用find命令来实现：&lt;/p>
&lt;pre tabindex="0">&lt;code># 查找文件：
find ~ -type f -name data-model.ts

# 查找文件夹：
find ~ -type d -name angular15
&lt;/code>&lt;/pre>&lt;h5 id="请注意第一个参数是要查找的目标文件或文件夹的路径如果未指定类型默认为文件">
 请注意，第一个参数是要查找的目标文件或文件夹的路径。如果未指定类型，默认为文件。
 &lt;a class="anchor" href="#%e8%af%b7%e6%b3%a8%e6%84%8f%e7%ac%ac%e4%b8%80%e4%b8%aa%e5%8f%82%e6%95%b0%e6%98%af%e8%a6%81%e6%9f%a5%e6%89%be%e7%9a%84%e7%9b%ae%e6%a0%87%e6%96%87%e4%bb%b6%e6%88%96%e6%96%87%e4%bb%b6%e5%a4%b9%e7%9a%84%e8%b7%af%e5%be%84%e5%a6%82%e6%9e%9c%e6%9c%aa%e6%8c%87%e5%ae%9a%e7%b1%bb%e5%9e%8b%e9%bb%98%e8%ae%a4%e4%b8%ba%e6%96%87%e4%bb%b6">#&lt;/a>
&lt;/h5>
&lt;h5 id="这似乎有点太简单了-让我们来做一些稍微复杂一点的事情让我们找一些文件并统计每个文件的行数">
 这似乎有点太简单了 😏。让我们来做一些稍微复杂一点的事情。让我们找一些文件，并统计每个文件的行数：
 &lt;a class="anchor" href="#%e8%bf%99%e4%bc%bc%e4%b9%8e%e6%9c%89%e7%82%b9%e5%a4%aa%e7%ae%80%e5%8d%95%e4%ba%86-%e8%ae%a9%e6%88%91%e4%bb%ac%e6%9d%a5%e5%81%9a%e4%b8%80%e4%ba%9b%e7%a8%8d%e5%be%ae%e5%a4%8d%e6%9d%82%e4%b8%80%e7%82%b9%e7%9a%84%e4%ba%8b%e6%83%85%e8%ae%a9%e6%88%91%e4%bb%ac%e6%89%be%e4%b8%80%e4%ba%9b%e6%96%87%e4%bb%b6%e5%b9%b6%e7%bb%9f%e8%ae%a1%e6%af%8f%e4%b8%aa%e6%96%87%e4%bb%b6%e7%9a%84%e8%a1%8c%e6%95%b0">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>#查找所有.html文件并计算每个文件的行数：
find src/app/ -name &amp;#34;*.html&amp;#34; -exec wc -l {} \;
&lt;/code>&lt;/pre>&lt;p>``&lt;/p>
&lt;h5 id="请注意这里的魔法语法-exec-指定了要针对每个找到的文件运行的命令花括号--是文件的占位符而转义的分号--用于防止-shell-解释该命令我们也可以使用加号--来实现同样的效果">
 请注意这里的魔法语法。-exec 指定了要针对每个找到的文件运行的命令。花括号 {} 是文件的占位符，而转义的分号 ; 用于防止 shell 解释该命令。我们也可以使用加号 + 来实现同样的效果：
 &lt;a class="anchor" href="#%e8%af%b7%e6%b3%a8%e6%84%8f%e8%bf%99%e9%87%8c%e7%9a%84%e9%ad%94%e6%b3%95%e8%af%ad%e6%b3%95-exec-%e6%8c%87%e5%ae%9a%e4%ba%86%e8%a6%81%e9%92%88%e5%af%b9%e6%af%8f%e4%b8%aa%e6%89%be%e5%88%b0%e7%9a%84%e6%96%87%e4%bb%b6%e8%bf%90%e8%a1%8c%e7%9a%84%e5%91%bd%e4%bb%a4%e8%8a%b1%e6%8b%ac%e5%8f%b7--%e6%98%af%e6%96%87%e4%bb%b6%e7%9a%84%e5%8d%a0%e4%bd%8d%e7%ac%a6%e8%80%8c%e8%bd%ac%e4%b9%89%e7%9a%84%e5%88%86%e5%8f%b7--%e7%94%a8%e4%ba%8e%e9%98%b2%e6%ad%a2-shell-%e8%a7%a3%e9%87%8a%e8%af%a5%e5%91%bd%e4%bb%a4%e6%88%91%e4%bb%ac%e4%b9%9f%e5%8f%af%e4%bb%a5%e4%bd%bf%e7%94%a8%e5%8a%a0%e5%8f%b7--%e6%9d%a5%e5%ae%9e%e7%8e%b0%e5%90%8c%e6%a0%b7%e7%9a%84%e6%95%88%e6%9e%9c">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>find src/app/ -name &amp;#34;*.html&amp;#34; -exec wc -l {} +
&lt;/code>&lt;/pre>&lt;p>``&lt;/p></description></item><item><title>2024-04-03 流量何处来何处去</title><link>https://qq547475331.github.io/docs/%E6%B5%81%E9%87%8F%E4%BD%95%E5%A4%84%E6%9D%A5%E4%BD%95%E5%A4%84%E5%8E%BB-liu-liang-he-chu-lai-he-chu-qu/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/%E6%B5%81%E9%87%8F%E4%BD%95%E5%A4%84%E6%9D%A5%E4%BD%95%E5%A4%84%E5%8E%BB-liu-liang-he-chu-lai-he-chu-qu/</guid><description>&lt;h2 id="kubernetes-网络概述">
 Kubernetes 网络概述
 &lt;a class="anchor" href="#kubernetes-%e7%bd%91%e7%bb%9c%e6%a6%82%e8%bf%b0">#&lt;/a>
&lt;/h2>
&lt;p>在 Kubernetes 集群中，每个容器都在一个 Pod 中运行，并且 Pod 由 Kubernetes 控制平面调度到节点（物理机或虚拟机）上。Kubernetes 网络为 Pod 之间、服务之间以及外部资源之间的通信提供了一种方式。&lt;/p>
&lt;p>Kubernetes 使用扁平网络模型，所有 Pod 都可以直接相互通信，无论它们运行在哪个节点上。为了实现这一点，Kubernetes 设置了一个跨越集群中所有节点的虚拟网络，并为每个 Pod 分配了该网络中唯一的 IP 地址。&lt;/p>
&lt;p>你是否想知道 Kubernetes 中的 API 流量去了哪里？&lt;/p>
&lt;p>仔细想想，你在 Kubernetes 集群上执行的任何操作都是 API 调用。无论你是创建新的 Pod 还是列出 Ingress Controller，这都是 API 调用。&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403191124802.png" alt="image-20240319112419761" />Kubeshark&lt;/p>
&lt;p>在这篇博文中，你将了解如何利用 Kubeshark（适用于 Kubernetes 的 API 流量查看器）提供对 Kubernetes 内部网络的实时协议级可见性&amp;mdash;捕获、剖析和监控所有(跨容器、pod、节点和集群)进出的流量和有效负载。&lt;/p>
&lt;h1 id="什么是-wireshark">
 什么是 Wireshark？
 &lt;a class="anchor" href="#%e4%bb%80%e4%b9%88%e6%98%af-wireshark">#&lt;/a>
&lt;/h1>
&lt;p>在深入了解 Kubeshark 之前，有必要简单介绍一下 Wireshark。为什么？因为当你看到谈论 Kubeshark 时，你会看到它与 Wireshark 的比较。&lt;/p>
&lt;h5 id="wireshark是一款免费的开源数据包分析器它用于网络故障排除分析wireshark是跨平台的并使用qt工具来实现其用户界面使用pcap来捕获数据包它可以在-linuxmacosbsdsolaris其他一些类-unix-操作系统和-microsoft-windows-上运行还有一个基于终端非-gui的版本称为-tsharkwireshark-与-tcpdump-非常相似但具有图形化以及集成的排序和过滤选项">
 &lt;strong>Wireshark&lt;/strong>是一款免费的开源数据包分析器。它用于网络故障排除、分析。Wireshark是跨平台的，并使用Qt工具来实现其用户界面，使用pcap来捕获数据包；它可以在 Linux、macOS、BSD、Solaris、其他一些类 Unix 操作系统和 Microsoft Windows 上运行。还有一个基于终端（非 GUI）的版本，称为 TShark。Wireshark 与 tcpdump 非常相似，但具有图形化以及集成的排序和过滤选项。
 &lt;a class="anchor" href="#wireshark%e6%98%af%e4%b8%80%e6%ac%be%e5%85%8d%e8%b4%b9%e7%9a%84%e5%bc%80%e6%ba%90%e6%95%b0%e6%8d%ae%e5%8c%85%e5%88%86%e6%9e%90%e5%99%a8%e5%ae%83%e7%94%a8%e4%ba%8e%e7%bd%91%e7%bb%9c%e6%95%85%e9%9a%9c%e6%8e%92%e9%99%a4%e5%88%86%e6%9e%90wireshark%e6%98%af%e8%b7%a8%e5%b9%b3%e5%8f%b0%e7%9a%84%e5%b9%b6%e4%bd%bf%e7%94%a8qt%e5%b7%a5%e5%85%b7%e6%9d%a5%e5%ae%9e%e7%8e%b0%e5%85%b6%e7%94%a8%e6%88%b7%e7%95%8c%e9%9d%a2%e4%bd%bf%e7%94%a8pcap%e6%9d%a5%e6%8d%95%e8%8e%b7%e6%95%b0%e6%8d%ae%e5%8c%85%e5%ae%83%e5%8f%af%e4%bb%a5%e5%9c%a8-linuxmacosbsdsolaris%e5%85%b6%e4%bb%96%e4%b8%80%e4%ba%9b%e7%b1%bb-unix-%e6%93%8d%e4%bd%9c%e7%b3%bb%e7%bb%9f%e5%92%8c-microsoft-windows-%e4%b8%8a%e8%bf%90%e8%a1%8c%e8%bf%98%e6%9c%89%e4%b8%80%e4%b8%aa%e5%9f%ba%e4%ba%8e%e7%bb%88%e7%ab%af%e9%9d%9e-gui%e7%9a%84%e7%89%88%e6%9c%ac%e7%a7%b0%e4%b8%ba-tsharkwireshark-%e4%b8%8e-tcpdump-%e9%9d%9e%e5%b8%b8%e7%9b%b8%e4%bc%bc%e4%bd%86%e5%85%b7%e6%9c%89%e5%9b%be%e5%bd%a2%e5%8c%96%e4%bb%a5%e5%8f%8a%e9%9b%86%e6%88%90%e7%9a%84%e6%8e%92%e5%ba%8f%e5%92%8c%e8%bf%87%e6%bb%a4%e9%80%89%e9%a1%b9">#&lt;/a>
&lt;/h5>
&lt;h5 id="tcpdumpwireshark-使我们能够在微观层面上可视化和理解网络中发生的情况想象一下如果这样的事情在-kubernetes-中是可能的你就可以看到工作负载pod或服务帐户间相互交互时到底会发生什么">
 TCPDump/Wireshark 使我们能够在微观层面上可视化和理解网络中发生的情况。想象一下，如果这样的事情在 Kubernetes 中是可能的，你就可以看到工作负载、Pod或服务帐户间相互交互时到底会发生什么。
 &lt;a class="anchor" href="#tcpdumpwireshark-%e4%bd%bf%e6%88%91%e4%bb%ac%e8%83%bd%e5%a4%9f%e5%9c%a8%e5%be%ae%e8%a7%82%e5%b1%82%e9%9d%a2%e4%b8%8a%e5%8f%af%e8%a7%86%e5%8c%96%e5%92%8c%e7%90%86%e8%a7%a3%e7%bd%91%e7%bb%9c%e4%b8%ad%e5%8f%91%e7%94%9f%e7%9a%84%e6%83%85%e5%86%b5%e6%83%b3%e8%b1%a1%e4%b8%80%e4%b8%8b%e5%a6%82%e6%9e%9c%e8%bf%99%e6%a0%b7%e7%9a%84%e4%ba%8b%e6%83%85%e5%9c%a8-kubernetes-%e4%b8%ad%e6%98%af%e5%8f%af%e8%83%bd%e7%9a%84%e4%bd%a0%e5%b0%b1%e5%8f%af%e4%bb%a5%e7%9c%8b%e5%88%b0%e5%b7%a5%e4%bd%9c%e8%b4%9f%e8%bd%bdpod%e6%88%96%e6%9c%8d%e5%8a%a1%e5%b8%90%e6%88%b7%e9%97%b4%e7%9b%b8%e4%ba%92%e4%ba%a4%e4%ba%92%e6%97%b6%e5%88%b0%e5%ba%95%e4%bc%9a%e5%8f%91%e7%94%9f%e4%bb%80%e4%b9%88">#&lt;/a>
&lt;/h5>
&lt;h5 id="kubeshark-与-wireshark-非常相似会展示每次调用时发生的情况不同之处在于-wireshark-查看数据包kubeshark-查看-api-调用">
 Kubeshark 与 Wireshark 非常相似，会展示每次调用时发生的情况。不同之处在于 Wireshark 查看数据包。Kubeshark 查看 API 调用。
 &lt;a class="anchor" href="#kubeshark-%e4%b8%8e-wireshark-%e9%9d%9e%e5%b8%b8%e7%9b%b8%e4%bc%bc%e4%bc%9a%e5%b1%95%e7%a4%ba%e6%af%8f%e6%ac%a1%e8%b0%83%e7%94%a8%e6%97%b6%e5%8f%91%e7%94%9f%e7%9a%84%e6%83%85%e5%86%b5%e4%b8%8d%e5%90%8c%e4%b9%8b%e5%a4%84%e5%9c%a8%e4%ba%8e-wireshark-%e6%9f%a5%e7%9c%8b%e6%95%b0%e6%8d%ae%e5%8c%85kubeshark-%e6%9f%a5%e7%9c%8b-api-%e8%b0%83%e7%94%a8">#&lt;/a>
&lt;/h5>
&lt;h1 id="api-调用">
 API 调用？
 &lt;a class="anchor" href="#api-%e8%b0%83%e7%94%a8">#&lt;/a>
&lt;/h1>
&lt;h5 id="在-kubernetes-中你可能看起来没有进行-api-调用你运行一些看起来像的命令kubectl-get-pods并认为它只是一个命令但该命令实际上是在进行-api-调用当你运行时kubectl-get-pods你实际上正在做的是对-kubernetes-api-服务器执行get请求">
 在 Kubernetes 中，你可能看起来没有进行 API 调用。你运行一些看起来像的命令&lt;code>kubectl get pods&lt;/code>，并认为它只是一个命令，但该命令实际上是在进行 API 调用。当你运行时&lt;code>kubectl get pods&lt;/code>，你实际上正在做的是对 Kubernetes API 服务器执行&lt;code>GET&lt;/code>请求。
 &lt;a class="anchor" href="#%e5%9c%a8-kubernetes-%e4%b8%ad%e4%bd%a0%e5%8f%af%e8%83%bd%e7%9c%8b%e8%b5%b7%e6%9d%a5%e6%b2%a1%e6%9c%89%e8%bf%9b%e8%a1%8c-api-%e8%b0%83%e7%94%a8%e4%bd%a0%e8%bf%90%e8%a1%8c%e4%b8%80%e4%ba%9b%e7%9c%8b%e8%b5%b7%e6%9d%a5%e5%83%8f%e7%9a%84%e5%91%bd%e4%bb%a4kubectl-get-pods%e5%b9%b6%e8%ae%a4%e4%b8%ba%e5%ae%83%e5%8f%aa%e6%98%af%e4%b8%80%e4%b8%aa%e5%91%bd%e4%bb%a4%e4%bd%86%e8%af%a5%e5%91%bd%e4%bb%a4%e5%ae%9e%e9%99%85%e4%b8%8a%e6%98%af%e5%9c%a8%e8%bf%9b%e8%a1%8c-api-%e8%b0%83%e7%94%a8%e5%bd%93%e4%bd%a0%e8%bf%90%e8%a1%8c%e6%97%b6kubectl-get-pods%e4%bd%a0%e5%ae%9e%e9%99%85%e4%b8%8a%e6%ad%a3%e5%9c%a8%e5%81%9a%e7%9a%84%e6%98%af%e5%af%b9-kubernetes-api-%e6%9c%8d%e5%8a%a1%e5%99%a8%e6%89%a7%e8%a1%8cget%e8%af%b7%e6%b1%82">#&lt;/a>
&lt;/h5>
&lt;h5 id="无论你是在创建新的-kubernetes-资源例如部署或服务还是在检索有关-kubernetes-资源的信息你都在进行-api-调用每当你与-kubernetes-交互时你都在进行-api-调用">
 无论你是在创建新的 Kubernetes 资源（例如部署或服务），还是在检索有关 Kubernetes 资源的信息，你都在进行 API 调用。每当你与 Kubernetes 交互时，你都在进行 API 调用。
 &lt;a class="anchor" href="#%e6%97%a0%e8%ae%ba%e4%bd%a0%e6%98%af%e5%9c%a8%e5%88%9b%e5%bb%ba%e6%96%b0%e7%9a%84-kubernetes-%e8%b5%84%e6%ba%90%e4%be%8b%e5%a6%82%e9%83%a8%e7%bd%b2%e6%88%96%e6%9c%8d%e5%8a%a1%e8%bf%98%e6%98%af%e5%9c%a8%e6%a3%80%e7%b4%a2%e6%9c%89%e5%85%b3-kubernetes-%e8%b5%84%e6%ba%90%e7%9a%84%e4%bf%a1%e6%81%af%e4%bd%a0%e9%83%bd%e5%9c%a8%e8%bf%9b%e8%a1%8c-api-%e8%b0%83%e7%94%a8%e6%af%8f%e5%bd%93%e4%bd%a0%e4%b8%8e-kubernetes-%e4%ba%a4%e4%ba%92%e6%97%b6%e4%bd%a0%e9%83%bd%e5%9c%a8%e8%bf%9b%e8%a1%8c-api-%e8%b0%83%e7%94%a8">#&lt;/a>
&lt;/h5>
&lt;h5 id="正如你可以想象的那样有大量流量流向-api-服务器假如我们能够监控流量是多么有意义并且对于故障排除特别有帮助">
 正如你可以想象的那样，有大量流量流向 API 服务器。假如，我们能够监控流量是多么有意义，并且对于故障排除特别有帮助。
 &lt;a class="anchor" href="#%e6%ad%a3%e5%a6%82%e4%bd%a0%e5%8f%af%e4%bb%a5%e6%83%b3%e8%b1%a1%e7%9a%84%e9%82%a3%e6%a0%b7%e6%9c%89%e5%a4%a7%e9%87%8f%e6%b5%81%e9%87%8f%e6%b5%81%e5%90%91-api-%e6%9c%8d%e5%8a%a1%e5%99%a8%e5%81%87%e5%a6%82%e6%88%91%e4%bb%ac%e8%83%bd%e5%a4%9f%e7%9b%91%e6%8e%a7%e6%b5%81%e9%87%8f%e6%98%af%e5%a4%9a%e4%b9%88%e6%9c%89%e6%84%8f%e4%b9%89%e5%b9%b6%e4%b8%94%e5%af%b9%e4%ba%8e%e6%95%85%e9%9a%9c%e6%8e%92%e9%99%a4%e7%89%b9%e5%88%ab%e6%9c%89%e5%b8%ae%e5%8a%a9">#&lt;/a>
&lt;/h5>
&lt;h1 id="为什么选择-kubeshark">
 为什么选择 Kubeshark？
 &lt;a class="anchor" href="#%e4%b8%ba%e4%bb%80%e4%b9%88%e9%80%89%e6%8b%a9-kubeshark">#&lt;/a>
&lt;/h1>
&lt;h5 id="kubeshark-填补了-kubernetes-长期以来的空白以可视化方式查看-api-调用">
 Kubeshark 填补了 Kubernetes 长期以来的空白——以可视化方式查看 API 调用。
 &lt;a class="anchor" href="#kubeshark-%e5%a1%ab%e8%a1%a5%e4%ba%86-kubernetes-%e9%95%bf%e6%9c%9f%e4%bb%a5%e6%9d%a5%e7%9a%84%e7%a9%ba%e7%99%bd%e4%bb%a5%e5%8f%af%e8%a7%86%e5%8c%96%e6%96%b9%e5%bc%8f%e6%9f%a5%e7%9c%8b-api-%e8%b0%83%e7%94%a8">#&lt;/a>
&lt;/h5>
&lt;h5 id="你能看到在没有-kubeshark-的情况下完成的-api-调用吗绝对地">
 你能看到在没有 Kubeshark 的情况下完成的 API 调用吗？绝对地。
 &lt;a class="anchor" href="#%e4%bd%a0%e8%83%bd%e7%9c%8b%e5%88%b0%e5%9c%a8%e6%b2%a1%e6%9c%89-kubeshark-%e7%9a%84%e6%83%85%e5%86%b5%e4%b8%8b%e5%ae%8c%e6%88%90%e7%9a%84-api-%e8%b0%83%e7%94%a8%e5%90%97%e7%bb%9d%e5%af%b9%e5%9c%b0">#&lt;/a>
&lt;/h5>
&lt;h5 id="下面的命令将向你展示用于检索-pod-信息的-api-调用">
 下面的命令将向你展示用于检索 Pod 信息的 API 调用。
 &lt;a class="anchor" href="#%e4%b8%8b%e9%9d%a2%e7%9a%84%e5%91%bd%e4%bb%a4%e5%b0%86%e5%90%91%e4%bd%a0%e5%b1%95%e7%a4%ba%e7%94%a8%e4%ba%8e%e6%a3%80%e7%b4%a2-pod-%e4%bf%a1%e6%81%af%e7%9a%84-api-%e8%b0%83%e7%94%a8">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>kubectl get pods -v=8
&lt;/code>&lt;/pre>&lt;p>你会看到类似于下面显示 API 调用的输出。&lt;/p></description></item><item><title>2024-04-03 清理残留的calico网络插件</title><link>https://qq547475331.github.io/docs/%E6%B8%85%E7%90%86%E6%AE%8B%E7%95%99%E7%9A%84calico%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6-qing-li-can-liu-de-calico-wang-luo/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/%E6%B8%85%E7%90%86%E6%AE%8B%E7%95%99%E7%9A%84calico%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6-qing-li-can-liu-de-calico-wang-luo/</guid><description>&lt;p>#停止docker kubelet kube-proxy服务
systemctl stop docker
systemctl stop kubelet
systemctl stop kube-proxy&lt;/p>
&lt;h1 id="删除残留路径">
 删除残留路径
 &lt;a class="anchor" href="#%e5%88%a0%e9%99%a4%e6%ae%8b%e7%95%99%e8%b7%af%e5%be%84">#&lt;/a>
&lt;/h1>
&lt;p>rm -rf /etc/cni
rm -rf /opt/cni
rm -rf /run/calico
rm -rf /var/lib/calico
rm -rf /var/lib/cni
rm -rf /var/run/calico&lt;/p>
&lt;h1 id="清理网络接口">
 清理网络接口
 &lt;a class="anchor" href="#%e6%b8%85%e7%90%86%e7%bd%91%e7%bb%9c%e6%8e%a5%e5%8f%a3">#&lt;/a>
&lt;/h1>
&lt;p>ip link delete vxlan.calico
ip link delete calia09f92a66fa@if3&lt;/p>
&lt;p>iptables -F &amp;amp;&amp;amp; iptables -t nat -F&lt;/p>
&lt;p>systemctl start docker
systemctl start kubelet
systemctl start kube-proxy&lt;/p>
&lt;p>systemctl status docker
systemctl status kubelet
systemctl status kube-proxy&lt;/p></description></item><item><title>2024-04-03 磁盘数据恢复</title><link>https://qq547475331.github.io/docs/%E7%A3%81%E7%9B%98%E6%95%B0%E6%8D%AE%E6%81%A2%E5%A4%8D-ci-pan-shu-ju-hui-fu/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/%E7%A3%81%E7%9B%98%E6%95%B0%E6%8D%AE%E6%81%A2%E5%A4%8D-ci-pan-shu-ju-hui-fu/</guid><description>&lt;h2 id="1背景">
 1.背景
 &lt;a class="anchor" href="#1%e8%83%8c%e6%99%af">#&lt;/a>
&lt;/h2>
&lt;p>如果使用rm -rf 误删除数据，我们怎么恢复误删除的数据呢？&lt;/p>
&lt;h2 id="2场景分析">
 2.场景分析
 &lt;a class="anchor" href="#2%e5%9c%ba%e6%99%af%e5%88%86%e6%9e%90">#&lt;/a>
&lt;/h2>
&lt;p>删除数据主要有两种场景：&lt;/p>
&lt;ul>
&lt;li>场景一 在执行rm -rf 删除文件时，该文件正在被进程使用。&lt;/li>
&lt;li>场景二 这个文件没有被进程占用，从而被误删除&lt;/li>
&lt;/ul>
&lt;h5 id="在场景一对于进程正在使用的场景数据可以恢复是因为linux里每个文件都有2个link计数器i_count和i_nlink">
 在&lt;strong>场景一&lt;/strong>，对于进程正在使用的场景，数据可以恢复是因为Linux里每个文件都有2个link计数器：i_count和i_nlink
 &lt;a class="anchor" href="#%e5%9c%a8%e5%9c%ba%e6%99%af%e4%b8%80%e5%af%b9%e4%ba%8e%e8%bf%9b%e7%a8%8b%e6%ad%a3%e5%9c%a8%e4%bd%bf%e7%94%a8%e7%9a%84%e5%9c%ba%e6%99%af%e6%95%b0%e6%8d%ae%e5%8f%af%e4%bb%a5%e6%81%a2%e5%a4%8d%e6%98%af%e5%9b%a0%e4%b8%balinux%e9%87%8c%e6%af%8f%e4%b8%aa%e6%96%87%e4%bb%b6%e9%83%bd%e6%9c%892%e4%b8%aalink%e8%ae%a1%e6%95%b0%e5%99%a8i_count%e5%92%8ci_nlink">#&lt;/a>
&lt;/h5>
&lt;h5 id="i_count-当一个文件被一个进程引用时它的数值会加1也就是说它记录的是文件被进程引用的次数">
 i_count: 当一个文件被一个进程引用时，它的数值会加1，也就是说它记录的是文件被进程引用的次数。
 &lt;a class="anchor" href="#i_count-%e5%bd%93%e4%b8%80%e4%b8%aa%e6%96%87%e4%bb%b6%e8%a2%ab%e4%b8%80%e4%b8%aa%e8%bf%9b%e7%a8%8b%e5%bc%95%e7%94%a8%e6%97%b6%e5%ae%83%e7%9a%84%e6%95%b0%e5%80%bc%e4%bc%9a%e5%8a%a01%e4%b9%9f%e5%b0%b1%e6%98%af%e8%af%b4%e5%ae%83%e8%ae%b0%e5%bd%95%e7%9a%84%e6%98%af%e6%96%87%e4%bb%b6%e8%a2%ab%e8%bf%9b%e7%a8%8b%e5%bc%95%e7%94%a8%e7%9a%84%e6%ac%a1%e6%95%b0">#&lt;/a>
&lt;/h5>
&lt;h5 id="i_nlink-记录文件产生硬链接的个数">
 i_nlink: 记录文件产生硬链接的个数。
 &lt;a class="anchor" href="#i_nlink-%e8%ae%b0%e5%bd%95%e6%96%87%e4%bb%b6%e4%ba%a7%e7%94%9f%e7%a1%ac%e9%93%be%e6%8e%a5%e7%9a%84%e4%b8%aa%e6%95%b0">#&lt;/a>
&lt;/h5>
&lt;h5 id="linux系统只有在两个数值都清零的时候文件才被系统认为是删除的假设此时删除文件有进程在使用那么i_conunt数值不为0">
 Linux系统只有在两个数值都清零的时候，文件才被系统认为是删除的，假设此时删除文件有进程在使用，那么i_conunt数值不为0。
 &lt;a class="anchor" href="#linux%e7%b3%bb%e7%bb%9f%e5%8f%aa%e6%9c%89%e5%9c%a8%e4%b8%a4%e4%b8%aa%e6%95%b0%e5%80%bc%e9%83%bd%e6%b8%85%e9%9b%b6%e7%9a%84%e6%97%b6%e5%80%99%e6%96%87%e4%bb%b6%e6%89%8d%e8%a2%ab%e7%b3%bb%e7%bb%9f%e8%ae%a4%e4%b8%ba%e6%98%af%e5%88%a0%e9%99%a4%e7%9a%84%e5%81%87%e8%ae%be%e6%ad%a4%e6%97%b6%e5%88%a0%e9%99%a4%e6%96%87%e4%bb%b6%e6%9c%89%e8%bf%9b%e7%a8%8b%e5%9c%a8%e4%bd%bf%e7%94%a8%e9%82%a3%e4%b9%88i_conunt%e6%95%b0%e5%80%bc%e4%b8%8d%e4%b8%ba0">#&lt;/a>
&lt;/h5>
&lt;h5 id="在场景二此时i_count和i_nlink都为0inode连接信息已经被删除了我们就需要通过存放文件的block单元做数据块的数据找回">
 在&lt;strong>场景二&lt;/strong>，此时i_count和i_nlink都为0，inode连接信息已经被删除了，我们就需要通过存放文件的block单元，做数据块的数据找回。
 &lt;a class="anchor" href="#%e5%9c%a8%e5%9c%ba%e6%99%af%e4%ba%8c%e6%ad%a4%e6%97%b6i_count%e5%92%8ci_nlink%e9%83%bd%e4%b8%ba0inode%e8%bf%9e%e6%8e%a5%e4%bf%a1%e6%81%af%e5%b7%b2%e7%bb%8f%e8%a2%ab%e5%88%a0%e9%99%a4%e4%ba%86%e6%88%91%e4%bb%ac%e5%b0%b1%e9%9c%80%e8%a6%81%e9%80%9a%e8%bf%87%e5%ad%98%e6%94%be%e6%96%87%e4%bb%b6%e7%9a%84block%e5%8d%95%e5%85%83%e5%81%9a%e6%95%b0%e6%8d%ae%e5%9d%97%e7%9a%84%e6%95%b0%e6%8d%ae%e6%89%be%e5%9b%9e">#&lt;/a>
&lt;/h5>
&lt;h5 id="找回数据需要依赖两个关键的参数">
 找回数据需要依赖两个关键的参数：
 &lt;a class="anchor" href="#%e6%89%be%e5%9b%9e%e6%95%b0%e6%8d%ae%e9%9c%80%e8%a6%81%e4%be%9d%e8%b5%96%e4%b8%a4%e4%b8%aa%e5%85%b3%e9%94%ae%e7%9a%84%e5%8f%82%e6%95%b0">#&lt;/a>
&lt;/h5>
&lt;h5 id="inode-用于存放文件的相关元数据它的元数据里会有一个类似于索引的值能够索引到后面具体存放数据的block单元">
 inode: 用于存放文件的相关元数据，它的元数据里会有一个类似于索引的值，能够索引到后面具体存放数据的block单元
 &lt;a class="anchor" href="#inode-%e7%94%a8%e4%ba%8e%e5%ad%98%e6%94%be%e6%96%87%e4%bb%b6%e7%9a%84%e7%9b%b8%e5%85%b3%e5%85%83%e6%95%b0%e6%8d%ae%e5%ae%83%e7%9a%84%e5%85%83%e6%95%b0%e6%8d%ae%e9%87%8c%e4%bc%9a%e6%9c%89%e4%b8%80%e4%b8%aa%e7%b1%bb%e4%bc%bc%e4%ba%8e%e7%b4%a2%e5%bc%95%e7%9a%84%e5%80%bc%e8%83%bd%e5%a4%9f%e7%b4%a2%e5%bc%95%e5%88%b0%e5%90%8e%e9%9d%a2%e5%85%b7%e4%bd%93%e5%ad%98%e6%94%be%e6%95%b0%e6%8d%ae%e7%9a%84block%e5%8d%95%e5%85%83">#&lt;/a>
&lt;/h5>
&lt;h5 id="block-是一个数据块用来实际存放数据">
 block: 是一个数据块，用来实际存放数据
 &lt;a class="anchor" href="#block-%e6%98%af%e4%b8%80%e4%b8%aa%e6%95%b0%e6%8d%ae%e5%9d%97%e7%94%a8%e6%9d%a5%e5%ae%9e%e9%99%85%e5%ad%98%e6%94%be%e6%95%b0%e6%8d%ae">#&lt;/a>
&lt;/h5>
&lt;h5 id="所以理论上可以通过block块找回数据因为上面保存着真实的数据">
 所以理论上可以通过block块找回数据，因为上面保存着真实的数据。
 &lt;a class="anchor" href="#%e6%89%80%e4%bb%a5%e7%90%86%e8%ae%ba%e4%b8%8a%e5%8f%af%e4%bb%a5%e9%80%9a%e8%bf%87block%e5%9d%97%e6%89%be%e5%9b%9e%e6%95%b0%e6%8d%ae%e5%9b%a0%e4%b8%ba%e4%b8%8a%e9%9d%a2%e4%bf%9d%e5%ad%98%e7%9d%80%e7%9c%9f%e5%ae%9e%e7%9a%84%e6%95%b0%e6%8d%ae">#&lt;/a>
&lt;/h5>
&lt;h5 id="风险如果有进程在在不断地往磁盘写数据时需要申请新的block块如果操作系统分配已删除的block块时新写入的数据就会覆盖原来的数据此时就会造成数据真正丢失">
 &lt;strong>风险&lt;/strong>：如果有进程在在不断地往磁盘写数据时，需要申请新的block块，如果操作系统分配已删除的block块时，新写入的数据就会覆盖原来的数据，此时就会造成数据真正丢失。
 &lt;a class="anchor" href="#%e9%a3%8e%e9%99%a9%e5%a6%82%e6%9e%9c%e6%9c%89%e8%bf%9b%e7%a8%8b%e5%9c%a8%e5%9c%a8%e4%b8%8d%e6%96%ad%e5%9c%b0%e5%be%80%e7%a3%81%e7%9b%98%e5%86%99%e6%95%b0%e6%8d%ae%e6%97%b6%e9%9c%80%e8%a6%81%e7%94%b3%e8%af%b7%e6%96%b0%e7%9a%84block%e5%9d%97%e5%a6%82%e6%9e%9c%e6%93%8d%e4%bd%9c%e7%b3%bb%e7%bb%9f%e5%88%86%e9%85%8d%e5%b7%b2%e5%88%a0%e9%99%a4%e7%9a%84block%e5%9d%97%e6%97%b6%e6%96%b0%e5%86%99%e5%85%a5%e7%9a%84%e6%95%b0%e6%8d%ae%e5%b0%b1%e4%bc%9a%e8%a6%86%e7%9b%96%e5%8e%9f%e6%9d%a5%e7%9a%84%e6%95%b0%e6%8d%ae%e6%ad%a4%e6%97%b6%e5%b0%b1%e4%bc%9a%e9%80%a0%e6%88%90%e6%95%b0%e6%8d%ae%e7%9c%9f%e6%ad%a3%e4%b8%a2%e5%a4%b1">#&lt;/a>
&lt;/h5>
&lt;h5 id="在这种情况下应该第一时间umount目录所在的磁盘或者不对磁盘进行任何写入以保证理论数据还存在磁盘上那么还可以通过相关分析找回数据">
 在这种情况下，应该第一时间umount目录所在的磁盘，或者不对磁盘进行任何写入，以保证理论数据还存在磁盘上，那么还可以通过相关分析找回数据。
 &lt;a class="anchor" href="#%e5%9c%a8%e8%bf%99%e7%a7%8d%e6%83%85%e5%86%b5%e4%b8%8b%e5%ba%94%e8%af%a5%e7%ac%ac%e4%b8%80%e6%97%b6%e9%97%b4umount%e7%9b%ae%e5%bd%95%e6%89%80%e5%9c%a8%e7%9a%84%e7%a3%81%e7%9b%98%e6%88%96%e8%80%85%e4%b8%8d%e5%af%b9%e7%a3%81%e7%9b%98%e8%bf%9b%e8%a1%8c%e4%bb%bb%e4%bd%95%e5%86%99%e5%85%a5%e4%bb%a5%e4%bf%9d%e8%af%81%e7%90%86%e8%ae%ba%e6%95%b0%e6%8d%ae%e8%bf%98%e5%ad%98%e5%9c%a8%e7%a3%81%e7%9b%98%e4%b8%8a%e9%82%a3%e4%b9%88%e8%bf%98%e5%8f%af%e4%bb%a5%e9%80%9a%e8%bf%87%e7%9b%b8%e5%85%b3%e5%88%86%e6%9e%90%e6%89%be%e5%9b%9e%e6%95%b0%e6%8d%ae">#&lt;/a>
&lt;/h5>
&lt;h2 id="3场景一演示">
 3.场景一演示
 &lt;a class="anchor" href="#3%e5%9c%ba%e6%99%af%e4%b8%80%e6%bc%94%e7%a4%ba">#&lt;/a>
&lt;/h2>
&lt;p>&lt;strong>场景一搭建：&lt;/strong>&lt;/p>
&lt;p>首先准备测试的文件aaa.txt，内容为运维贼船公众号的域名，在终端一中执行：&lt;/p>
&lt;pre tabindex="0">&lt;code># 创建测试文件夹
mkdir /tmp/test
# 创建测试文件
echo &amp;#34;aaa.al&amp;#34; &amp;gt; /tmp/test/aaa.txt
&lt;/code>&lt;/pre>&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202402281046554.png" alt="image-20240228104651503" />&lt;/p>
&lt;p>创建完文件后，使用进程打开它并保持占用，这里用tail -f来模拟：&lt;/p></description></item><item><title>2024-04-03 离线安装kubephere</title><link>https://qq547475331.github.io/docs/%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85kubephere-li-xian-an-zhuang-kubephere/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85kubephere-li-xian-an-zhuang-kubephere/</guid><description>&lt;h1 id="离线安装">
 离线安装
 &lt;a class="anchor" href="#%e7%a6%bb%e7%ba%bf%e5%ae%89%e8%a3%85">#&lt;/a>
&lt;/h1>
&lt;p>离线安装几乎与在线安装相同，不同之处是您必须创建一个本地仓库来托管 Docker 镜像。本教程演示了如何在离线环境中将 KubeSphere 安装到 Kubernetes 上。&lt;/p>
&lt;p>开始下方步骤之前，请先参阅&lt;a href="https://www.kubesphere.io/zh/docs/v3.4/installing-on-kubernetes/introduction/prerequisites/">准备工作&lt;/a>。&lt;/p>
&lt;h2 id="步骤-1准备一个私有镜像仓库">
 步骤 1：准备一个私有镜像仓库
 &lt;a class="anchor" href="#%e6%ad%a5%e9%aa%a4-1%e5%87%86%e5%a4%87%e4%b8%80%e4%b8%aa%e7%a7%81%e6%9c%89%e9%95%9c%e5%83%8f%e4%bb%93%e5%ba%93">#&lt;/a>
&lt;/h2>
&lt;p>您可以使用 Harbor 或者其他任意私有镜像仓库。本教程以 Docker 仓库作为示例，并使用&lt;a href="https://docs.docker.com/registry/insecure/#use-self-signed-certificates">自签名证书&lt;/a>（如果您有自己的私有镜像仓库，可以跳过这一步）。&lt;/p>
&lt;h3 id="使用自签名证书">
 使用自签名证书
 &lt;a class="anchor" href="#%e4%bd%bf%e7%94%a8%e8%87%aa%e7%ad%be%e5%90%8d%e8%af%81%e4%b9%a6">#&lt;/a>
&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>执行以下命令生成您自己的证书：&lt;/p>
&lt;pre tabindex="0">&lt;code>mkdir -p certs
&lt;/code>&lt;/pre>&lt;pre tabindex="0">&lt;code>openssl req \
-newkey rsa:4096 -nodes -sha256 -keyout certs/domain.key \
-x509 -days 36500 -out certs/domain.crt
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;h5 id="当您生成自己的证书时请确保在字段-common-name-中指定一个域名例如本示例中该字段被指定为-dockerhubkubekeylocal">
 当您生成自己的证书时，请确保在字段 &lt;code>Common Name&lt;/code> 中指定一个域名。例如，本示例中该字段被指定为 &lt;code>dockerhub.kubekey.local&lt;/code>。
 &lt;a class="anchor" href="#%e5%bd%93%e6%82%a8%e7%94%9f%e6%88%90%e8%87%aa%e5%b7%b1%e7%9a%84%e8%af%81%e4%b9%a6%e6%97%b6%e8%af%b7%e7%a1%ae%e4%bf%9d%e5%9c%a8%e5%ad%97%e6%ae%b5-common-name-%e4%b8%ad%e6%8c%87%e5%ae%9a%e4%b8%80%e4%b8%aa%e5%9f%9f%e5%90%8d%e4%be%8b%e5%a6%82%e6%9c%ac%e7%a4%ba%e4%be%8b%e4%b8%ad%e8%af%a5%e5%ad%97%e6%ae%b5%e8%a2%ab%e6%8c%87%e5%ae%9a%e4%b8%ba-dockerhubkubekeylocal">#&lt;/a>
&lt;/h5>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403071628547.png" alt="image-20240307162801481" />&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="启动-docker-仓库">
 启动 Docker 仓库
 &lt;a class="anchor" href="#%e5%90%af%e5%8a%a8-docker-%e4%bb%93%e5%ba%93">#&lt;/a>
&lt;/h3>
&lt;h5 id="执行以下命令启动-docker-仓库">
 执行以下命令启动 Docker 仓库：
 &lt;a class="anchor" href="#%e6%89%a7%e8%a1%8c%e4%bb%a5%e4%b8%8b%e5%91%bd%e4%bb%a4%e5%90%af%e5%8a%a8-docker-%e4%bb%93%e5%ba%93">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>docker run -d \
 --restart=always \
 --name registry \
 -v &amp;#34;$(pwd)&amp;#34;/certs:/certs \
 -v /mnt/registry:/var/lib/registry \
 -e REGISTRY_HTTP_ADDR=0.0.0.0:443 \
 -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt \
 -e REGISTRY_HTTP_TLS_KEY=/certs/domain.key \
 -p 443:443 \
 registry:2
&lt;/code>&lt;/pre>&lt;h5 id="备注">
 备注
 &lt;a class="anchor" href="#%e5%a4%87%e6%b3%a8">#&lt;/a>
&lt;/h5>
&lt;h5 id="docker-使用-varlibdocker-作为默认路径来存储所有-docker-相关文件包括镜像建议您添加附加存储卷分别给-varlibdocker-和-mntregistry-挂载至少-100g请参见-fdisk-的参考命令">
 Docker 使用 &lt;code>/var/lib/docker&lt;/code> 作为默认路径来存储所有 Docker 相关文件（包括镜像）。建议您添加附加存储卷，分别给 &lt;code>/var/lib/docker&lt;/code> 和 &lt;code>/mnt/registry&lt;/code> 挂载至少 &lt;strong>100G&lt;/strong>。请参见 &lt;a href="https://www.computerhope.com/unix/fdisk.htm">fdisk&lt;/a> 的参考命令。
 &lt;a class="anchor" href="#docker-%e4%bd%bf%e7%94%a8-varlibdocker-%e4%bd%9c%e4%b8%ba%e9%bb%98%e8%ae%a4%e8%b7%af%e5%be%84%e6%9d%a5%e5%ad%98%e5%82%a8%e6%89%80%e6%9c%89-docker-%e7%9b%b8%e5%85%b3%e6%96%87%e4%bb%b6%e5%8c%85%e6%8b%ac%e9%95%9c%e5%83%8f%e5%bb%ba%e8%ae%ae%e6%82%a8%e6%b7%bb%e5%8a%a0%e9%99%84%e5%8a%a0%e5%ad%98%e5%82%a8%e5%8d%b7%e5%88%86%e5%88%ab%e7%bb%99-varlibdocker-%e5%92%8c-mntregistry-%e6%8c%82%e8%bd%bd%e8%87%b3%e5%b0%91-100g%e8%af%b7%e5%8f%82%e8%a7%81-fdisk-%e7%9a%84%e5%8f%82%e8%80%83%e5%91%bd%e4%bb%a4">#&lt;/a>
&lt;/h5>
&lt;h3 id="配置仓库">
 配置仓库
 &lt;a class="anchor" href="#%e9%85%8d%e7%bd%ae%e4%bb%93%e5%ba%93">#&lt;/a>
&lt;/h3>
&lt;ol>
&lt;li>
&lt;h5 id="在-etchosts-中添加一个条目将主机名即仓库域名在本示例中是-dockerhubkubekeylocal映射到您机器的私有-ip-地址如下所示">
 在 &lt;code>/etc/hosts&lt;/code> 中添加一个条目，将主机名（即仓库域名；在本示例中是 &lt;code>dockerhub.kubekey.local&lt;/code>）映射到您机器的私有 IP 地址，如下所示。
 &lt;a class="anchor" href="#%e5%9c%a8-etchosts-%e4%b8%ad%e6%b7%bb%e5%8a%a0%e4%b8%80%e4%b8%aa%e6%9d%a1%e7%9b%ae%e5%b0%86%e4%b8%bb%e6%9c%ba%e5%90%8d%e5%8d%b3%e4%bb%93%e5%ba%93%e5%9f%9f%e5%90%8d%e5%9c%a8%e6%9c%ac%e7%a4%ba%e4%be%8b%e4%b8%ad%e6%98%af-dockerhubkubekeylocal%e6%98%a0%e5%b0%84%e5%88%b0%e6%82%a8%e6%9c%ba%e5%99%a8%e7%9a%84%e7%a7%81%e6%9c%89-ip-%e5%9c%b0%e5%9d%80%e5%a6%82%e4%b8%8b%e6%89%80%e7%a4%ba">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code># docker registry
192.168.0.2 dockerhub.kubekey.local
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;h5 id="执行以下命令复制证书到指定目录并使-docker-信任该证书">
 执行以下命令，复制证书到指定目录，并使 Docker 信任该证书。
 &lt;a class="anchor" href="#%e6%89%a7%e8%a1%8c%e4%bb%a5%e4%b8%8b%e5%91%bd%e4%bb%a4%e5%a4%8d%e5%88%b6%e8%af%81%e4%b9%a6%e5%88%b0%e6%8c%87%e5%ae%9a%e7%9b%ae%e5%bd%95%e5%b9%b6%e4%bd%bf-docker-%e4%bf%a1%e4%bb%bb%e8%af%a5%e8%af%81%e4%b9%a6">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>mkdir -p /etc/docker/certs.d/dockerhub.kubekey.local
&lt;/code>&lt;/pre>&lt;pre tabindex="0">&lt;code>cp certs/domain.crt /etc/docker/certs.d/dockerhub.kubekey.local/ca.crt
&lt;/code>&lt;/pre>&lt;h5 id="备注-1">
 备注
 &lt;a class="anchor" href="#%e5%a4%87%e6%b3%a8-1">#&lt;/a>
&lt;/h5>
&lt;h5 id="证书的路径与域名相关联当您复制路径时如果与上面设置的路径不同请使用实际域名">
 证书的路径与域名相关联。当您复制路径时，如果与上面设置的路径不同，请使用实际域名。
 &lt;a class="anchor" href="#%e8%af%81%e4%b9%a6%e7%9a%84%e8%b7%af%e5%be%84%e4%b8%8e%e5%9f%9f%e5%90%8d%e7%9b%b8%e5%85%b3%e8%81%94%e5%bd%93%e6%82%a8%e5%a4%8d%e5%88%b6%e8%b7%af%e5%be%84%e6%97%b6%e5%a6%82%e6%9e%9c%e4%b8%8e%e4%b8%8a%e9%9d%a2%e8%ae%be%e7%bd%ae%e7%9a%84%e8%b7%af%e5%be%84%e4%b8%8d%e5%90%8c%e8%af%b7%e4%bd%bf%e7%94%a8%e5%ae%9e%e9%99%85%e5%9f%9f%e5%90%8d">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="要验证私有仓库是否有效您可以先复制一个镜像到您的本地机器然后使用-docker-push-和-docker-pull-来测试">
 要验证私有仓库是否有效，您可以先复制一个镜像到您的本地机器，然后使用 &lt;code>docker push&lt;/code> 和 &lt;code>docker pull&lt;/code> 来测试。
 &lt;a class="anchor" href="#%e8%a6%81%e9%aa%8c%e8%af%81%e7%a7%81%e6%9c%89%e4%bb%93%e5%ba%93%e6%98%af%e5%90%a6%e6%9c%89%e6%95%88%e6%82%a8%e5%8f%af%e4%bb%a5%e5%85%88%e5%a4%8d%e5%88%b6%e4%b8%80%e4%b8%aa%e9%95%9c%e5%83%8f%e5%88%b0%e6%82%a8%e7%9a%84%e6%9c%ac%e5%9c%b0%e6%9c%ba%e5%99%a8%e7%84%b6%e5%90%8e%e4%bd%bf%e7%94%a8-docker-push-%e5%92%8c-docker-pull-%e6%9d%a5%e6%b5%8b%e8%af%95">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ol>
&lt;h2 id="步骤-2准备安装镜像">
 步骤 2：准备安装镜像
 &lt;a class="anchor" href="#%e6%ad%a5%e9%aa%a4-2%e5%87%86%e5%a4%87%e5%ae%89%e8%a3%85%e9%95%9c%e5%83%8f">#&lt;/a>
&lt;/h2>
&lt;h5 id="当您在离线环境中安装-kubesphere-时需要事先准备一个包含所有必需镜像的镜像包">
 当您在离线环境中安装 KubeSphere 时，需要事先准备一个包含所有必需镜像的镜像包。
 &lt;a class="anchor" href="#%e5%bd%93%e6%82%a8%e5%9c%a8%e7%a6%bb%e7%ba%bf%e7%8e%af%e5%a2%83%e4%b8%ad%e5%ae%89%e8%a3%85-kubesphere-%e6%97%b6%e9%9c%80%e8%a6%81%e4%ba%8b%e5%85%88%e5%87%86%e5%a4%87%e4%b8%80%e4%b8%aa%e5%8c%85%e5%90%ab%e6%89%80%e6%9c%89%e5%bf%85%e9%9c%80%e9%95%9c%e5%83%8f%e7%9a%84%e9%95%9c%e5%83%8f%e5%8c%85">#&lt;/a>
&lt;/h5>
&lt;ol>
&lt;li>
&lt;h5 id="使用以下命令从能够访问互联网的机器上下载镜像清单文件-images-listtxt">
 使用以下命令从能够访问互联网的机器上下载镜像清单文件 &lt;code>images-list.txt&lt;/code>：
 &lt;a class="anchor" href="#%e4%bd%bf%e7%94%a8%e4%bb%a5%e4%b8%8b%e5%91%bd%e4%bb%a4%e4%bb%8e%e8%83%bd%e5%a4%9f%e8%ae%bf%e9%97%ae%e4%ba%92%e8%81%94%e7%bd%91%e7%9a%84%e6%9c%ba%e5%99%a8%e4%b8%8a%e4%b8%8b%e8%bd%bd%e9%95%9c%e5%83%8f%e6%b8%85%e5%8d%95%e6%96%87%e4%bb%b6-images-listtxt">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>curl -L -O https://github.com/kubesphere/ks-installer/releases/download/v3.4.1/images-list.txt
&lt;/code>&lt;/pre>&lt;p>备注&lt;/p></description></item><item><title>2024-04-03 自动屏蔽IP攻击</title><link>https://qq547475331.github.io/docs/%E8%87%AA%E5%8A%A8%E5%B1%8F%E8%94%BDip%E6%94%BB%E5%87%BB-zi-dong-ping-bi-ip-gong-ji/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/%E8%87%AA%E5%8A%A8%E5%B1%8F%E8%94%BDip%E6%94%BB%E5%87%BB-zi-dong-ping-bi-ip-gong-ji/</guid><description>&lt;h5 id="在当今的数字世界中服务器安全性成为了每个企业或个人都必须面对的重要议题特别是当面对如dos拒绝服务攻击等网络威胁时一个有效的防范措施显得尤为关键本文将探讨如何通过自动屏蔽攻击ip来增强您的服务器抵御dos攻击的能力并提供一个实用的bash脚本作为参考">
 在当今的数字世界中，服务器安全性成为了每个企业或个人都必须面对的重要议题。特别是当面对如DoS（拒绝服务）攻击等网络威胁时，一个有效的防范措施显得尤为关键。本文将探讨如何通过自动屏蔽攻击IP来增强您的服务器抵御DoS攻击的能力，并提供一个实用的Bash脚本作为参考。
 &lt;a class="anchor" href="#%e5%9c%a8%e5%bd%93%e4%bb%8a%e7%9a%84%e6%95%b0%e5%ad%97%e4%b8%96%e7%95%8c%e4%b8%ad%e6%9c%8d%e5%8a%a1%e5%99%a8%e5%ae%89%e5%85%a8%e6%80%a7%e6%88%90%e4%b8%ba%e4%ba%86%e6%af%8f%e4%b8%aa%e4%bc%81%e4%b8%9a%e6%88%96%e4%b8%aa%e4%ba%ba%e9%83%bd%e5%bf%85%e9%a1%bb%e9%9d%a2%e5%af%b9%e7%9a%84%e9%87%8d%e8%a6%81%e8%ae%ae%e9%a2%98%e7%89%b9%e5%88%ab%e6%98%af%e5%bd%93%e9%9d%a2%e5%af%b9%e5%a6%82dos%e6%8b%92%e7%bb%9d%e6%9c%8d%e5%8a%a1%e6%94%bb%e5%87%bb%e7%ad%89%e7%bd%91%e7%bb%9c%e5%a8%81%e8%83%81%e6%97%b6%e4%b8%80%e4%b8%aa%e6%9c%89%e6%95%88%e7%9a%84%e9%98%b2%e8%8c%83%e6%8e%aa%e6%96%bd%e6%98%be%e5%be%97%e5%b0%a4%e4%b8%ba%e5%85%b3%e9%94%ae%e6%9c%ac%e6%96%87%e5%b0%86%e6%8e%a2%e8%ae%a8%e5%a6%82%e4%bd%95%e9%80%9a%e8%bf%87%e8%87%aa%e5%8a%a8%e5%b1%8f%e8%94%bd%e6%94%bb%e5%87%bbip%e6%9d%a5%e5%a2%9e%e5%bc%ba%e6%82%a8%e7%9a%84%e6%9c%8d%e5%8a%a1%e5%99%a8%e6%8a%b5%e5%be%a1dos%e6%94%bb%e5%87%bb%e7%9a%84%e8%83%bd%e5%8a%9b%e5%b9%b6%e6%8f%90%e4%be%9b%e4%b8%80%e4%b8%aa%e5%ae%9e%e7%94%a8%e7%9a%84bash%e8%84%9a%e6%9c%ac%e4%bd%9c%e4%b8%ba%e5%8f%82%e8%80%83">#&lt;/a>
&lt;/h5>
&lt;p>&lt;strong>理解DoS攻击&lt;/strong>&lt;/p>
&lt;h5 id="dos攻击是一种恶意行为通过向目标服务器发送大量无用的请求耗尽其资源使合法用户无法访问服务这种攻击不仅影响服务器的正常运行还可能导致数据丢失系统崩溃等严重后果">
 DoS攻击是一种恶意行为，通过向目标服务器发送大量无用的请求，耗尽其资源，使合法用户无法访问服务。这种攻击不仅影响服务器的正常运行，还可能导致数据丢失、系统崩溃等严重后果。
 &lt;a class="anchor" href="#dos%e6%94%bb%e5%87%bb%e6%98%af%e4%b8%80%e7%a7%8d%e6%81%b6%e6%84%8f%e8%a1%8c%e4%b8%ba%e9%80%9a%e8%bf%87%e5%90%91%e7%9b%ae%e6%a0%87%e6%9c%8d%e5%8a%a1%e5%99%a8%e5%8f%91%e9%80%81%e5%a4%a7%e9%87%8f%e6%97%a0%e7%94%a8%e7%9a%84%e8%af%b7%e6%b1%82%e8%80%97%e5%b0%bd%e5%85%b6%e8%b5%84%e6%ba%90%e4%bd%bf%e5%90%88%e6%b3%95%e7%94%a8%e6%88%b7%e6%97%a0%e6%b3%95%e8%ae%bf%e9%97%ae%e6%9c%8d%e5%8a%a1%e8%bf%99%e7%a7%8d%e6%94%bb%e5%87%bb%e4%b8%8d%e4%bb%85%e5%bd%b1%e5%93%8d%e6%9c%8d%e5%8a%a1%e5%99%a8%e7%9a%84%e6%ad%a3%e5%b8%b8%e8%bf%90%e8%a1%8c%e8%bf%98%e5%8f%af%e8%83%bd%e5%af%bc%e8%87%b4%e6%95%b0%e6%8d%ae%e4%b8%a2%e5%a4%b1%e7%b3%bb%e7%bb%9f%e5%b4%a9%e6%ba%83%e7%ad%89%e4%b8%a5%e9%87%8d%e5%90%8e%e6%9e%9c">#&lt;/a>
&lt;/h5>
&lt;p>&lt;strong>自动屏蔽攻击IP的策略&lt;/strong>&lt;/p>
&lt;h5 id="为了有效应对dos攻击我们需要一个能够实时监控服务器流量并自动识别和屏蔽攻击ip的系统以下是实现这一目标的基本步骤">
 为了有效应对DoS攻击，我们需要一个能够实时监控服务器流量，并自动识别和屏蔽攻击IP的系统。以下是实现这一目标的基本步骤：
 &lt;a class="anchor" href="#%e4%b8%ba%e4%ba%86%e6%9c%89%e6%95%88%e5%ba%94%e5%af%b9dos%e6%94%bb%e5%87%bb%e6%88%91%e4%bb%ac%e9%9c%80%e8%a6%81%e4%b8%80%e4%b8%aa%e8%83%bd%e5%a4%9f%e5%ae%9e%e6%97%b6%e7%9b%91%e6%8e%a7%e6%9c%8d%e5%8a%a1%e5%99%a8%e6%b5%81%e9%87%8f%e5%b9%b6%e8%87%aa%e5%8a%a8%e8%af%86%e5%88%ab%e5%92%8c%e5%b1%8f%e8%94%bd%e6%94%bb%e5%87%bbip%e7%9a%84%e7%b3%bb%e7%bb%9f%e4%bb%a5%e4%b8%8b%e6%98%af%e5%ae%9e%e7%8e%b0%e8%bf%99%e4%b8%80%e7%9b%ae%e6%a0%87%e7%9a%84%e5%9f%ba%e6%9c%ac%e6%ad%a5%e9%aa%a4">#&lt;/a>
&lt;/h5>
&lt;ol>
&lt;li>
&lt;h5 id="监控服务器日志通过分析nginx等服务器的访问日志我们可以识别出异常流量模式如短时间内来自同一ip的大量请求">
 &lt;strong>监控服务器日志&lt;/strong>：通过分析Nginx等服务器的访问日志，我们可以识别出异常流量模式，如短时间内来自同一IP的大量请求。
 &lt;a class="anchor" href="#%e7%9b%91%e6%8e%a7%e6%9c%8d%e5%8a%a1%e5%99%a8%e6%97%a5%e5%bf%97%e9%80%9a%e8%bf%87%e5%88%86%e6%9e%90nginx%e7%ad%89%e6%9c%8d%e5%8a%a1%e5%99%a8%e7%9a%84%e8%ae%bf%e9%97%ae%e6%97%a5%e5%bf%97%e6%88%91%e4%bb%ac%e5%8f%af%e4%bb%a5%e8%af%86%e5%88%ab%e5%87%ba%e5%bc%82%e5%b8%b8%e6%b5%81%e9%87%8f%e6%a8%a1%e5%bc%8f%e5%a6%82%e7%9f%ad%e6%97%b6%e9%97%b4%e5%86%85%e6%9d%a5%e8%87%aa%e5%90%8c%e4%b8%80ip%e7%9a%84%e5%a4%a7%e9%87%8f%e8%af%b7%e6%b1%82">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="识别攻击ip使用脚本或工具对日志数据进行处理提取出那些行为异常的ip地址">
 &lt;strong>识别攻击IP&lt;/strong>：使用脚本或工具对日志数据进行处理，提取出那些行为异常的IP地址。
 &lt;a class="anchor" href="#%e8%af%86%e5%88%ab%e6%94%bb%e5%87%bbip%e4%bd%bf%e7%94%a8%e8%84%9a%e6%9c%ac%e6%88%96%e5%b7%a5%e5%85%b7%e5%af%b9%e6%97%a5%e5%bf%97%e6%95%b0%e6%8d%ae%e8%bf%9b%e8%a1%8c%e5%a4%84%e7%90%86%e6%8f%90%e5%8f%96%e5%87%ba%e9%82%a3%e4%ba%9b%e8%a1%8c%e4%b8%ba%e5%bc%82%e5%b8%b8%e7%9a%84ip%e5%9c%b0%e5%9d%80">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="自动屏蔽一旦发现攻击ip立即使用iptables等防火墙工具将其加入黑名单阻止其进一步的访问">
 &lt;strong>自动屏蔽&lt;/strong>：一旦发现攻击IP，立即使用iptables等防火墙工具将其加入黑名单，阻止其进一步的访问。
 &lt;a class="anchor" href="#%e8%87%aa%e5%8a%a8%e5%b1%8f%e8%94%bd%e4%b8%80%e6%97%a6%e5%8f%91%e7%8e%b0%e6%94%bb%e5%87%bbip%e7%ab%8b%e5%8d%b3%e4%bd%bf%e7%94%a8iptables%e7%ad%89%e9%98%b2%e7%81%ab%e5%a2%99%e5%b7%a5%e5%85%b7%e5%b0%86%e5%85%b6%e5%8a%a0%e5%85%a5%e9%bb%91%e5%90%8d%e5%8d%95%e9%98%bb%e6%ad%a2%e5%85%b6%e8%bf%9b%e4%b8%80%e6%ad%a5%e7%9a%84%e8%ae%bf%e9%97%ae">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="记录和报告将屏蔽的ip地址和时间记录在日志文件中以便后续分析和报告">
 &lt;strong>记录和报告&lt;/strong>：将屏蔽的IP地址和时间记录在日志文件中，以便后续分析和报告。
 &lt;a class="anchor" href="#%e8%ae%b0%e5%bd%95%e5%92%8c%e6%8a%a5%e5%91%8a%e5%b0%86%e5%b1%8f%e8%94%bd%e7%9a%84ip%e5%9c%b0%e5%9d%80%e5%92%8c%e6%97%b6%e9%97%b4%e8%ae%b0%e5%bd%95%e5%9c%a8%e6%97%a5%e5%bf%97%e6%96%87%e4%bb%b6%e4%b8%ad%e4%bb%a5%e4%be%bf%e5%90%8e%e7%bb%ad%e5%88%86%e6%9e%90%e5%92%8c%e6%8a%a5%e5%91%8a">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>实用Bash脚本&lt;/strong>&lt;/p>
&lt;h5 id="下面是一个基于上述策略的bash脚本示例请注意此脚本应在具有相应权限的服务器上运行并且可能需要根据您的实际环境进行调整">
 下面是一个基于上述策略的Bash脚本示例。请注意，此脚本应在具有相应权限的服务器上运行，并且可能需要根据您的实际环境进行调整。
 &lt;a class="anchor" href="#%e4%b8%8b%e9%9d%a2%e6%98%af%e4%b8%80%e4%b8%aa%e5%9f%ba%e4%ba%8e%e4%b8%8a%e8%bf%b0%e7%ad%96%e7%95%a5%e7%9a%84bash%e8%84%9a%e6%9c%ac%e7%a4%ba%e4%be%8b%e8%af%b7%e6%b3%a8%e6%84%8f%e6%ad%a4%e8%84%9a%e6%9c%ac%e5%ba%94%e5%9c%a8%e5%85%b7%e6%9c%89%e7%9b%b8%e5%ba%94%e6%9d%83%e9%99%90%e7%9a%84%e6%9c%8d%e5%8a%a1%e5%99%a8%e4%b8%8a%e8%bf%90%e8%a1%8c%e5%b9%b6%e4%b8%94%e5%8f%af%e8%83%bd%e9%9c%80%e8%a6%81%e6%a0%b9%e6%8d%ae%e6%82%a8%e7%9a%84%e5%ae%9e%e9%99%85%e7%8e%af%e5%a2%83%e8%bf%9b%e8%a1%8c%e8%b0%83%e6%95%b4">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>#!/bin/bash 

# 定义日志文件路径和临时黑名单文件路径 
LOG_FILE=/usr/local/nginx/logs/demo2.access.log 
BLACKLIST_FILE=/tmp/blacklist.txt 
DROP_LOG=/tmp/drop_ip.log 
 
# 获取当前时间并格式化为日期 
DATE=$(date +&amp;#34;%d/%b/%Y:%H:%M&amp;#34;) 
 
# 从日志文件中提取异常IP地址（这里假设每分钟超过10个请求的IP为异常） 
tail -n 5000 $LOG_FILE | grep &amp;#34;$DATE&amp;#34; | awk &amp;#39;{print $1, $4}&amp;#39; | sort -n | uniq -c | awk &amp;#39;$1 &amp;gt; 10 {print $2}&amp;#39; &amp;gt; $BLACKLIST_FILE 
 
# 读取黑名单文件，并屏蔽这些IP地址 
while IFS= read -r IP; do 
 if ! iptables -vnL INPUT | grep -q &amp;#34;$IP&amp;#34;; then 
 iptables -I INPUT -s &amp;#34;$IP&amp;#34; -j DROP 
 echo &amp;#34;$(date +&amp;#39;%F_%T&amp;#39;) $IP&amp;#34; &amp;gt;&amp;gt; $DROP_LOG 
 echo &amp;#34;Blocked IP: $IP&amp;#34; 
 fi 
done &amp;lt; $BLACKLIST_FILE 
 
# （可选）清理过时的iptables规则，避免规则过多导致性能下降 
# 这里可以根据需要添加逻辑来定期清理旧的屏蔽规则
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>脚本首先定义了所需的文件路径和日期格式。&lt;/li>
&lt;li>使用&lt;code>tail&lt;/code>和&lt;code>grep&lt;/code>命令结合当前时间从Nginx访问日志中提取最近的访问记录。&lt;/li>
&lt;li>通过&lt;code>awk&lt;/code>和&lt;code>sort&lt;/code>命令处理这些记录，识别出每分钟请求次数超过10次的IP地址，并将它们写入临时黑名单文件。&lt;/li>
&lt;li>脚本接着读取黑名单文件中的每个IP地址，并使用&lt;code>iptables&lt;/code>命令将它们加入INPUT链的DROP目标中，从而屏蔽这些IP的访问。同时，将屏蔽操作记录在日志文件中。&lt;/li>
&lt;li>最后，您可以根据需要添加额外的逻辑来定期清理iptables中的旧规则，以保持防火墙的性能。&lt;/li>
&lt;/ul>
&lt;p>请注意，此脚本是一个基本示例，可能需要根据您的服务器配置、日志格式和安全策略进行定制。在实际使用中，还应考虑脚本的性能影响、错误处理和日志管理等方面的问题。&lt;/p></description></item><item><title>2024-04-03 面试用 Golang 手撸 LRU</title><link>https://qq547475331.github.io/docs/%E9%9D%A2%E8%AF%95%E7%94%A8-golang-%E6%89%8B%E6%92%B8-lru-mian-shi-yong-golang-shou-lu-lru/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/%E9%9D%A2%E8%AF%95%E7%94%A8-golang-%E6%89%8B%E6%92%B8-lru-mian-shi-yong-golang-shou-lu-lru/</guid><description>&lt;h5 id="上周参加一个云原生-devops-开发的面试第一轮面试问一些技能项目相关问题最后留了-20-分要求用-golang-实现-lru">
 上周参加一个云原生 DevOps 开发的面试，第一轮面试问一些技能、项目相关问题，最后留了 20 分要求用 Golang 实现 &lt;strong>&lt;code>LRU&lt;/code>&lt;/strong>。
 &lt;a class="anchor" href="#%e4%b8%8a%e5%91%a8%e5%8f%82%e5%8a%a0%e4%b8%80%e4%b8%aa%e4%ba%91%e5%8e%9f%e7%94%9f-devops-%e5%bc%80%e5%8f%91%e7%9a%84%e9%9d%a2%e8%af%95%e7%ac%ac%e4%b8%80%e8%bd%ae%e9%9d%a2%e8%af%95%e9%97%ae%e4%b8%80%e4%ba%9b%e6%8a%80%e8%83%bd%e9%a1%b9%e7%9b%ae%e7%9b%b8%e5%85%b3%e9%97%ae%e9%a2%98%e6%9c%80%e5%90%8e%e7%95%99%e4%ba%86-20-%e5%88%86%e8%a6%81%e6%b1%82%e7%94%a8-golang-%e5%ae%9e%e7%8e%b0-lru">#&lt;/a>
&lt;/h5>
&lt;h5 id="过程大概用了半个多小时大概写出来了-80-一面勉强过了写篇文章记录下加深印象">
 过程大概用了半个多小时，大概写出来了 80 %，一面勉强过了。写篇文章记录下，加深印象。
 &lt;a class="anchor" href="#%e8%bf%87%e7%a8%8b%e5%a4%a7%e6%a6%82%e7%94%a8%e4%ba%86%e5%8d%8a%e4%b8%aa%e5%a4%9a%e5%b0%8f%e6%97%b6%e5%a4%a7%e6%a6%82%e5%86%99%e5%87%ba%e6%9d%a5%e4%ba%86-80-%e4%b8%80%e9%9d%a2%e5%8b%89%e5%bc%ba%e8%bf%87%e4%ba%86%e5%86%99%e7%af%87%e6%96%87%e7%ab%a0%e8%ae%b0%e5%bd%95%e4%b8%8b%e5%8a%a0%e6%b7%b1%e5%8d%b0%e8%b1%a1">#&lt;/a>
&lt;/h5>
&lt;hr>
&lt;h2 id="lru-是什么">
 &lt;strong>LRU 是什么&lt;/strong>
 &lt;a class="anchor" href="#lru-%e6%98%af%e4%bb%80%e4%b9%88">#&lt;/a>
&lt;/h2>
&lt;h5 id="lruleast-recently-used最近最少使用是一种常见的缓存淘汰算法当缓存满时淘汰最近最久未使用的元素">
 &lt;strong>&lt;code>LRU&lt;/code>&lt;/strong>（&lt;strong>Least recently used&lt;/strong>，最近最少使用)，是一种常见的缓存淘汰算法，当缓存满时，淘汰最近最久未使用的元素。
 &lt;a class="anchor" href="#lruleast-recently-used%e6%9c%80%e8%bf%91%e6%9c%80%e5%b0%91%e4%bd%bf%e7%94%a8%e6%98%af%e4%b8%80%e7%a7%8d%e5%b8%b8%e8%a7%81%e7%9a%84%e7%bc%93%e5%ad%98%e6%b7%98%e6%b1%b0%e7%ae%97%e6%b3%95%e5%bd%93%e7%bc%93%e5%ad%98%e6%bb%a1%e6%97%b6%e6%b7%98%e6%b1%b0%e6%9c%80%e8%bf%91%e6%9c%80%e4%b9%85%e6%9c%aa%e4%bd%bf%e7%94%a8%e7%9a%84%e5%85%83%e7%b4%a0">#&lt;/a>
&lt;/h5>
&lt;h5 id="其基本思想是如果一个数据在最近一段时间没有被访问到那么可以认为在将来它被访问的可能性也很小因此当缓存满时最久未被访问的数据最先被淘汰具体做法是将最近使用的元素存放到靠近缓存顶部的位置当一个新条目被访问时lru-将它放置到缓存的顶部当缓存满时较早之前访问的条目将从缓存底部被移除">
 其基本思想是如果一个数据在最近一段时间没有被访问到，那么可以认为在将来它被访问的可能性也很小。因此，当缓存满时，最久未被访问的数据最先被淘汰。具体做法是将最近使用的元素存放到靠近缓存顶部的位置，当一个新条目被访问时，&lt;strong>&lt;code>LRU&lt;/code>&lt;/strong> 将它放置到缓存的顶部。当缓存满时，较早之前访问的条目将从缓存底部被移除。
 &lt;a class="anchor" href="#%e5%85%b6%e5%9f%ba%e6%9c%ac%e6%80%9d%e6%83%b3%e6%98%af%e5%a6%82%e6%9e%9c%e4%b8%80%e4%b8%aa%e6%95%b0%e6%8d%ae%e5%9c%a8%e6%9c%80%e8%bf%91%e4%b8%80%e6%ae%b5%e6%97%b6%e9%97%b4%e6%b2%a1%e6%9c%89%e8%a2%ab%e8%ae%bf%e9%97%ae%e5%88%b0%e9%82%a3%e4%b9%88%e5%8f%af%e4%bb%a5%e8%ae%a4%e4%b8%ba%e5%9c%a8%e5%b0%86%e6%9d%a5%e5%ae%83%e8%a2%ab%e8%ae%bf%e9%97%ae%e7%9a%84%e5%8f%af%e8%83%bd%e6%80%a7%e4%b9%9f%e5%be%88%e5%b0%8f%e5%9b%a0%e6%ad%a4%e5%bd%93%e7%bc%93%e5%ad%98%e6%bb%a1%e6%97%b6%e6%9c%80%e4%b9%85%e6%9c%aa%e8%a2%ab%e8%ae%bf%e9%97%ae%e7%9a%84%e6%95%b0%e6%8d%ae%e6%9c%80%e5%85%88%e8%a2%ab%e6%b7%98%e6%b1%b0%e5%85%b7%e4%bd%93%e5%81%9a%e6%b3%95%e6%98%af%e5%b0%86%e6%9c%80%e8%bf%91%e4%bd%bf%e7%94%a8%e7%9a%84%e5%85%83%e7%b4%a0%e5%ad%98%e6%94%be%e5%88%b0%e9%9d%a0%e8%bf%91%e7%bc%93%e5%ad%98%e9%a1%b6%e9%83%a8%e7%9a%84%e4%bd%8d%e7%bd%ae%e5%bd%93%e4%b8%80%e4%b8%aa%e6%96%b0%e6%9d%a1%e7%9b%ae%e8%a2%ab%e8%ae%bf%e9%97%ae%e6%97%b6lru-%e5%b0%86%e5%ae%83%e6%94%be%e7%bd%ae%e5%88%b0%e7%bc%93%e5%ad%98%e7%9a%84%e9%a1%b6%e9%83%a8%e5%bd%93%e7%bc%93%e5%ad%98%e6%bb%a1%e6%97%b6%e8%be%83%e6%97%a9%e4%b9%8b%e5%89%8d%e8%ae%bf%e9%97%ae%e7%9a%84%e6%9d%a1%e7%9b%ae%e5%b0%86%e4%bb%8e%e7%bc%93%e5%ad%98%e5%ba%95%e9%83%a8%e8%a2%ab%e7%a7%bb%e9%99%a4">#&lt;/a>
&lt;/h5>
&lt;h5 id="lru-底层用-h-来缓存数据">
 &lt;strong>&lt;code>LRU&lt;/code>&lt;/strong> 底层用 &lt;strong>H&lt;code>a&lt;/code>****&lt;code>sh Map&lt;/code>&lt;/strong> 来缓存数据。
 &lt;a class="anchor" href="#lru-%e5%ba%95%e5%b1%82%e7%94%a8-h-%e6%9d%a5%e7%bc%93%e5%ad%98%e6%95%b0%e6%8d%ae">#&lt;/a>
&lt;/h5>
&lt;hr>
&lt;h2 id="手撸-lru">
 &lt;strong>手撸 LRU&lt;/strong>
 &lt;a class="anchor" href="#%e6%89%8b%e6%92%b8-lru">#&lt;/a>
&lt;/h2>
&lt;h5 id="面试手撸-lru-很刺激">
 面试手撸 &lt;strong>&lt;code>LRU&lt;/code>&lt;/strong> 很刺激~
 &lt;a class="anchor" href="#%e9%9d%a2%e8%af%95%e6%89%8b%e6%92%b8-lru-%e5%be%88%e5%88%ba%e6%bf%80">#&lt;/a>
&lt;/h5>
&lt;h3 id="题目要求">
 &lt;strong>题目要求&lt;/strong>
 &lt;a class="anchor" href="#%e9%a2%98%e7%9b%ae%e8%a6%81%e6%b1%82">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>
&lt;h5 id="lru-缓存-int-类型的值且缓存具有容量限制">
 **&lt;code>LRU &lt;/code>**缓存 &lt;strong>&lt;code>int&lt;/code>&lt;/strong> 类型的值，且缓存具有容量限制；
 &lt;a class="anchor" href="#lru-%e7%bc%93%e5%ad%98-int-%e7%b1%bb%e5%9e%8b%e7%9a%84%e5%80%bc%e4%b8%94%e7%bc%93%e5%ad%98%e5%85%b7%e6%9c%89%e5%ae%b9%e9%87%8f%e9%99%90%e5%88%b6">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="实现-lru-getkey-int-方法如果关键字-key-存在于缓存中则返回关键字的值否则返回--1">
 实现 &lt;strong>&lt;code>LRU&lt;/code>&lt;/strong> &lt;strong>&lt;code>Get(key int)&lt;/code>&lt;/strong> 方法，如果关键字 &lt;strong>&lt;code>key&lt;/code>&lt;/strong> 存在于缓存中，则返回关键字的值，否则返回 -1；
 &lt;a class="anchor" href="#%e5%ae%9e%e7%8e%b0-lru-getkey-int-%e6%96%b9%e6%b3%95%e5%a6%82%e6%9e%9c%e5%85%b3%e9%94%ae%e5%ad%97-key-%e5%ad%98%e5%9c%a8%e4%ba%8e%e7%bc%93%e5%ad%98%e4%b8%ad%e5%88%99%e8%bf%94%e5%9b%9e%e5%85%b3%e9%94%ae%e5%ad%97%e7%9a%84%e5%80%bc%e5%90%a6%e5%88%99%e8%bf%94%e5%9b%9e--1">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="实现-lru-putkey-intvalue-int-方法如果关键字-key-存在于缓存中则变更其数据值-value如果不存在则向缓存中插入该-keyvalue如果插入操作导致导致-key-数量超过缓存容量则应该逐出最久未使用的-key">
 实现 &lt;strong>&lt;code>LRU&lt;/code>&lt;/strong> &lt;strong>&lt;code>Put(key int，value int)&lt;/code>&lt;/strong> 方法，如果关键字 &lt;strong>&lt;code>key&lt;/code>&lt;/strong> 存在于缓存中，则变更其数据值 &lt;strong>&lt;code>value&lt;/code>&lt;/strong>；如果不存在，则向缓存中插入该 &lt;strong>&lt;code>key&lt;/code>&lt;/strong>，&lt;strong>&lt;code>value&lt;/code>&lt;/strong>。如果插入操作导致导致 &lt;strong>&lt;code>key&lt;/code>&lt;/strong> 数量超过缓存容量，则应该逐出最久未使用的 &lt;strong>&lt;code>key&lt;/code>&lt;/strong>；
 &lt;a class="anchor" href="#%e5%ae%9e%e7%8e%b0-lru-putkey-intvalue-int-%e6%96%b9%e6%b3%95%e5%a6%82%e6%9e%9c%e5%85%b3%e9%94%ae%e5%ad%97-key-%e5%ad%98%e5%9c%a8%e4%ba%8e%e7%bc%93%e5%ad%98%e4%b8%ad%e5%88%99%e5%8f%98%e6%9b%b4%e5%85%b6%e6%95%b0%e6%8d%ae%e5%80%bc-value%e5%a6%82%e6%9e%9c%e4%b8%8d%e5%ad%98%e5%9c%a8%e5%88%99%e5%90%91%e7%bc%93%e5%ad%98%e4%b8%ad%e6%8f%92%e5%85%a5%e8%af%a5-keyvalue%e5%a6%82%e6%9e%9c%e6%8f%92%e5%85%a5%e6%93%8d%e4%bd%9c%e5%af%bc%e8%87%b4%e5%af%bc%e8%87%b4-key-%e6%95%b0%e9%87%8f%e8%b6%85%e8%bf%87%e7%bc%93%e5%ad%98%e5%ae%b9%e9%87%8f%e5%88%99%e5%ba%94%e8%af%a5%e9%80%90%e5%87%ba%e6%9c%80%e4%b9%85%e6%9c%aa%e4%bd%bf%e7%94%a8%e7%9a%84-key">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="getput-以-o1-的时间复杂度运行">
 &lt;strong>&lt;code>Get&lt;/code>&lt;/strong>、&lt;strong>&lt;code>Put&lt;/code>&lt;/strong> 以 O(1) 的时间复杂度运行。
 &lt;a class="anchor" href="#getput-%e4%bb%a5-o1-%e7%9a%84%e6%97%b6%e9%97%b4%e5%a4%8d%e6%9d%82%e5%ba%a6%e8%bf%90%e8%a1%8c">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h3 id="图解-lru">
 &lt;strong>图解 LRU&lt;/strong>
 &lt;a class="anchor" href="#%e5%9b%be%e8%a7%a3-lru">#&lt;/a>
&lt;/h3>
&lt;h5 id="1首先要想缓存数据通过-hash-map-效率高操作方便定义当前缓存的数量和最大容量">
 1、首先要想缓存数据，通过 &lt;strong>&lt;code>hash map&lt;/code>&lt;/strong> ，效率高，操作方便；定义当前缓存的数量和最大容量
 &lt;a class="anchor" href="#1%e9%a6%96%e5%85%88%e8%a6%81%e6%83%b3%e7%bc%93%e5%ad%98%e6%95%b0%e6%8d%ae%e9%80%9a%e8%bf%87-hash-map-%e6%95%88%e7%8e%87%e9%ab%98%e6%93%8d%e4%bd%9c%e6%96%b9%e4%be%bf%e5%ae%9a%e4%b9%89%e5%bd%93%e5%89%8d%e7%bc%93%e5%ad%98%e7%9a%84%e6%95%b0%e9%87%8f%e5%92%8c%e6%9c%80%e5%a4%a7%e5%ae%b9%e9%87%8f">#&lt;/a>
&lt;/h5>
&lt;h5 id="2因为需要保证最久未使用的数据在缓存满的时候将其删除所以就需要一个数据结构能辅助完成这个逻辑">
 2、因为需要保证最久未使用的数据在缓存满的时候将其删除，所以就需要一个数据结构能辅助完成这个逻辑。
 &lt;a class="anchor" href="#2%e5%9b%a0%e4%b8%ba%e9%9c%80%e8%a6%81%e4%bf%9d%e8%af%81%e6%9c%80%e4%b9%85%e6%9c%aa%e4%bd%bf%e7%94%a8%e7%9a%84%e6%95%b0%e6%8d%ae%e5%9c%a8%e7%bc%93%e5%ad%98%e6%bb%a1%e7%9a%84%e6%97%b6%e5%80%99%e5%b0%86%e5%85%b6%e5%88%a0%e9%99%a4%e6%89%80%e4%bb%a5%e5%b0%b1%e9%9c%80%e8%a6%81%e4%b8%80%e4%b8%aa%e6%95%b0%e6%8d%ae%e7%bb%93%e6%9e%84%e8%83%bd%e8%be%85%e5%8a%a9%e5%ae%8c%e6%88%90%e8%bf%99%e4%b8%aa%e9%80%bb%e8%be%91">#&lt;/a>
&lt;/h5>
&lt;h5 id="所以说使用链表这种结构可以方便删除结点新增结点但由于最久未使用的结点在尾结点通过单链表不方便操作所以双链表会更加方便操作尾结点">
 所以说使用链表这种结构可以方便删除结点，新增结点，但由于最久未使用的结点在尾结点，通过单链表不方便操作，所以双链表会更加方便操作尾结点。
 &lt;a class="anchor" href="#%e6%89%80%e4%bb%a5%e8%af%b4%e4%bd%bf%e7%94%a8%e9%93%be%e8%a1%a8%e8%bf%99%e7%a7%8d%e7%bb%93%e6%9e%84%e5%8f%af%e4%bb%a5%e6%96%b9%e4%be%bf%e5%88%a0%e9%99%a4%e7%bb%93%e7%82%b9%e6%96%b0%e5%a2%9e%e7%bb%93%e7%82%b9%e4%bd%86%e7%94%b1%e4%ba%8e%e6%9c%80%e4%b9%85%e6%9c%aa%e4%bd%bf%e7%94%a8%e7%9a%84%e7%bb%93%e7%82%b9%e5%9c%a8%e5%b0%be%e7%bb%93%e7%82%b9%e9%80%9a%e8%bf%87%e5%8d%95%e9%93%be%e8%a1%a8%e4%b8%8d%e6%96%b9%e4%be%bf%e6%93%8d%e4%bd%9c%e6%89%80%e4%bb%a5%e5%8f%8c%e9%93%be%e8%a1%a8%e4%bc%9a%e6%9b%b4%e5%8a%a0%e6%96%b9%e4%be%bf%e6%93%8d%e4%bd%9c%e5%b0%be%e7%bb%93%e7%82%b9">#&lt;/a>
&lt;/h5>
&lt;h5 id="所以这里利用双链表数据结构headtail-两个指针不存数据这样保证每个结点操作逻辑一致">
 所以这里利用双链表数据结构，&lt;strong>&lt;code>head、tail&lt;/code>&lt;/strong> 两个指针不存数据，这样保证每个结点操作逻辑一致。
 &lt;a class="anchor" href="#%e6%89%80%e4%bb%a5%e8%bf%99%e9%87%8c%e5%88%a9%e7%94%a8%e5%8f%8c%e9%93%be%e8%a1%a8%e6%95%b0%e6%8d%ae%e7%bb%93%e6%9e%84headtail-%e4%b8%a4%e4%b8%aa%e6%8c%87%e9%92%88%e4%b8%8d%e5%ad%98%e6%95%b0%e6%8d%ae%e8%bf%99%e6%a0%b7%e4%bf%9d%e8%af%81%e6%af%8f%e4%b8%aa%e7%bb%93%e7%82%b9%e6%93%8d%e4%bd%9c%e9%80%bb%e8%be%91%e4%b8%80%e8%87%b4">#&lt;/a>
&lt;/h5>
&lt;h5 id="然后上面的-map-的-key-为用户传入的-keymap-的-value-为双链表的中-node即通过-key-来获取链表中的-nodenode-里存有用户传的-value">
 然后上面的 &lt;strong>&lt;code>map&lt;/code>&lt;/strong> 的 &lt;strong>&lt;code>key&lt;/code>&lt;/strong> 为用户传入的 &lt;strong>&lt;code>key&lt;/code>&lt;/strong>，&lt;strong>&lt;code>map&lt;/code>&lt;/strong> 的 &lt;strong>&lt;code>value&lt;/code>&lt;/strong> 为双链表的中 &lt;strong>&lt;code>node&lt;/code>&lt;/strong>，即通过 &lt;strong>&lt;code>key&lt;/code>&lt;/strong> 来获取链表中的 &lt;strong>&lt;code>node&lt;/code>&lt;/strong>，&lt;strong>&lt;code>node&lt;/code>&lt;/strong> 里存有用户传的 &lt;strong>&lt;code>value&lt;/code>&lt;/strong>
 &lt;a class="anchor" href="#%e7%84%b6%e5%90%8e%e4%b8%8a%e9%9d%a2%e7%9a%84-map-%e7%9a%84-key-%e4%b8%ba%e7%94%a8%e6%88%b7%e4%bc%a0%e5%85%a5%e7%9a%84-keymap-%e7%9a%84-value-%e4%b8%ba%e5%8f%8c%e9%93%be%e8%a1%a8%e7%9a%84%e4%b8%ad-node%e5%8d%b3%e9%80%9a%e8%bf%87-key-%e6%9d%a5%e8%8e%b7%e5%8f%96%e9%93%be%e8%a1%a8%e4%b8%ad%e7%9a%84-nodenode-%e9%87%8c%e5%ad%98%e6%9c%89%e7%94%a8%e6%88%b7%e4%bc%a0%e7%9a%84-value">#&lt;/a>
&lt;/h5>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403191521386.png" alt="image-20240319152100340" />&lt;/p></description></item><item><title>2024-04-07 彻悟容器网络</title><link>https://qq547475331.github.io/docs/%E5%BD%BB%E6%82%9F%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C-che-wu-rong-qi-wang-luo/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/%E5%BD%BB%E6%82%9F%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C-che-wu-rong-qi-wang-luo/</guid><description>&lt;p>&lt;strong>01&lt;/strong>&lt;/p>
&lt;p>&lt;strong>背景&lt;/strong>&lt;/p>
&lt;p>&lt;strong>容器网络为何出现&lt;/strong>&lt;/p>
&lt;p>&lt;strong>在一个汽车发动机的生产车间中，汽车发动机的各个组件会存在一定的顺序进行组装，这就要求有直接关系的组件必须知道下一个组件的具体位置。当一个汽车发动机组装完成后，距离最后成品汽车，还差了很多部件，比如底盘，车身等。此时，需要将发动机发往一个装配中心进行组合安装，这样我们就必须知道装配中心的地理位置。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>这些位置在容器中可以理解为 IP 地址，容器网络便是如此。在上面这个例子里，即描述了容器网络本节点内部互通场景，又描述了跨节点的通信场景。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>随着云计算的发展，应用间通信从物理机网络，虚拟机网络，发展到目前的容器网络。由于容器不同于物理机、虚拟机，容器可以被理解为一个标准的，轻量级的，便携的，独立的集装箱，集装箱之间相互隔离，都使用自己的环境和资源。但随着越来越复杂环境变化，容器在运行中会需要容器间或者容器与集群外部之间的信息传输，这时候容器就要在网络层面拥有一个名字（即 IP 地址），由此容器网络就应运而生。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>再从技术的角度来谈容器网络的由来，首先要从容器本质说起，它是由以下几点来实现的：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>cgroup：实现资源的可配额&lt;/strong>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>overlay fs：实现文件系统的安全性和便携性&lt;/strong>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>namespace：实现资源的隔离性&lt;/strong>&lt;/p>
&lt;/li>
&lt;li>
&lt;ul>
&lt;li>&lt;strong>IPC ：System V IPC 和 POSIX 消息队列&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Network：网络设备、网络协议栈、网络端口等&lt;/strong>&lt;/li>
&lt;li>&lt;strong>PID：进程&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Mount：挂载点&lt;/strong>&lt;/li>
&lt;li>&lt;strong>UTS：主机名和域名&lt;/strong>&lt;/li>
&lt;li>&lt;strong>USR：用户和用户组&lt;/strong>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>由于主机与容器、容器与容器间的网络栈并不相通，也没有统一的控制面，导致容器间无法直接的感知。为了解决这个问题，本文中我们要讨论的容器网络出现了，再配合不同的网络虚拟化技术带来了多样化的容器网络方案。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>02&lt;/strong>&lt;/p>
&lt;p>&lt;strong>容器网络的基本要求&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>IP-per-Pod，每个 Pod 都拥有一个独立 IP 地址，Pod 内所有容器共享一个网络命名空间&lt;/strong>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>集群内所有 Pod 都在一个直接连通的扁平网络中，可通过 IP 直接访问&lt;/strong>&lt;/p>
&lt;/li>
&lt;li>
&lt;ul>
&lt;li>&lt;strong>所有容器之间无需 NAT 就可以直接互相访问&lt;/strong>&lt;/li>
&lt;li>&lt;strong>所有 Node 和所有容器之间无需 NAT 就可以直接互相访问&lt;/strong>&lt;/li>
&lt;li>&lt;strong>容器自己看到的 IP 跟其他容器看到的一样&lt;/strong>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Service cluster IP 尽可在集群内部访问，外部请求需要通过 NodePort、LoadBalance 或者 Ingress 来访问&lt;/strong>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>02&lt;/strong>&lt;/p>
&lt;p>&lt;strong>网络插件介绍&lt;/strong>&lt;/p>
&lt;p>&lt;em>&lt;strong>Aliware&lt;/strong>&lt;/em>&lt;/p>
&lt;p>&lt;strong>01&lt;/strong>&lt;/p>
&lt;p>&lt;strong>网络插件概述&lt;/strong>&lt;/p>
&lt;p>&lt;strong>容器和该容器所在的宿主机是分隔的两地，如果需要连通就得建立一座桥梁，但由于容器侧还没有名字，就无法建立桥梁，这时候就先需要给容器侧命名，这样才能成功建立起桥梁。网络插件就是起到给容器侧命名和建立桥梁的功能。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>即网络插件将网络接口插入容器网络命名空间（例如，veth 对的一端），并在主机上进行任何必要的改变（例如将 veth 的另一端连接到网桥）。然后通过调用适当的 IPAM 插件（IP 地址管理插件）分配给接口一个空闲的 IP 地址，并设置与此 IP 地址相对应的路由规则。&lt;/strong>&lt;/p></description></item><item><title>2024-04-09 K8S 调度器 scheduler_one.go 源码解读</title><link>https://qq547475331.github.io/docs/k8s-%E8%B0%83%E5%BA%A6%E5%99%A8-scheduler_onego-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-diao-du-qi-scheduleronego-yuan-ma-jie-du/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/k8s-%E8%B0%83%E5%BA%A6%E5%99%A8-scheduler_onego-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-diao-du-qi-scheduleronego-yuan-ma-jie-du/</guid><description>&lt;pre tabindex="0">&lt;code>/*
Copyright 2014 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the &amp;#34;License&amp;#34;);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

 http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &amp;#34;AS IS&amp;#34; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package scheduler

import (
	&amp;#34;container/heap&amp;#34;
	&amp;#34;context&amp;#34;
	&amp;#34;errors&amp;#34;
	&amp;#34;fmt&amp;#34;
	&amp;#34;math/rand&amp;#34;
	&amp;#34;strconv&amp;#34;
	&amp;#34;sync&amp;#34;
	&amp;#34;sync/atomic&amp;#34;
	&amp;#34;time&amp;#34;

	v1 &amp;#34;k8s.io/api/core/v1&amp;#34;
	metav1 &amp;#34;k8s.io/apimachinery/pkg/apis/meta/v1&amp;#34;
	utilruntime &amp;#34;k8s.io/apimachinery/pkg/util/runtime&amp;#34;
	&amp;#34;k8s.io/apimachinery/pkg/util/sets&amp;#34;
	clientset &amp;#34;k8s.io/client-go/kubernetes&amp;#34;
	extenderv1 &amp;#34;k8s.io/kube-scheduler/extender/v1&amp;#34;
	podutil &amp;#34;k8s.io/kubernetes/pkg/api/v1/pod&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/apis/core/validation&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/scheduler/framework&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/scheduler/framework/parallelize&amp;#34;
	internalqueue &amp;#34;k8s.io/kubernetes/pkg/scheduler/internal/queue&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/scheduler/metrics&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/scheduler/util&amp;#34;
	utiltrace &amp;#34;k8s.io/utils/trace&amp;#34;
)

const (
	// Percentage of plugin metrics to be sampled.
	pluginMetricsSamplePercent = 10
	// minFeasibleNodesToFind is the minimum number of nodes that would be scored
	// in each scheduling cycle. This is a semi-arbitrary value to ensure that a
	// certain minimum of nodes are checked for feasibility. This in turn helps
	// ensure a minimum level of spreading.
	minFeasibleNodesToFind = 100
	// minFeasibleNodesPercentageToFind is the minimum percentage of nodes that
	// would be scored in each scheduling cycle. This is a semi-arbitrary value
	// to ensure that a certain minimum of nodes are checked for feasibility.
	// This in turn helps ensure a minimum level of spreading.
	minFeasibleNodesPercentageToFind = 5
	// numberOfHighestScoredNodesToReport is the number of node scores
	// to be included in ScheduleResult.
	numberOfHighestScoredNodesToReport = 3
)

//这段代码定义了四个常量，分别是：
//- pluginMetricsSamplePercent：插件指标的采样百分比，值为10。
//- minFeasibleNodesToFind：每个调度周期中要评分的最小节点数，值为100。
//- minFeasibleNodesPercentageToFind：每个调度周期中要评分的最小节点百分比，值为5。
//- numberOfHighestScoredNodesToReport：要在调度结果中报告的最高评分节点数，值为3。

// ScheduleOne does the entire scheduling workflow for a single pod. It is serialized on the scheduling algorithm&amp;#39;s host fitting.
func (sched *Scheduler) ScheduleOne(ctx context.Context) {
	logger := klog.FromContext(ctx)
	podInfo, err := sched.NextPod(logger)
	if err != nil {
		logger.Error(err, &amp;#34;Error while retrieving next pod from scheduling queue&amp;#34;)
		return
	}
	// pod could be nil when schedulerQueue is closed
	if podInfo == nil || podInfo.Pod == nil {
		return
	}
	//该函数是一个Go函数，名为ScheduleOne，它为单个pod执行整个调度工作流程。
	//它在调度算法的主机适应性上进行序列化。函数首先从上下文中获取logger，然后使用sched.NextPod(logger)方法获取下一个要调度的pod信息。
	//如果获取过程中出现错误，则记录错误信息并返回。
	//如果获取到的pod信息为空或pod为空，则直接返回。

	pod := podInfo.Pod
	// TODO(knelasevero): Remove duplicated keys from log entry calls
	// When contextualized logging hits GA
	// https://github.com/kubernetes/kubernetes/issues/111672
	logger = klog.LoggerWithValues(logger, &amp;#34;pod&amp;#34;, klog.KObj(pod))
	ctx = klog.NewContext(ctx, logger)
	logger.V(4).Info(&amp;#34;About to try and schedule pod&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod))

	fwk, err := sched.frameworkForPod(pod)
	if err != nil {
		// This shouldn&amp;#39;t happen, because we only accept for scheduling the pods
		// which specify a scheduler name that matches one of the profiles.
		logger.Error(err, &amp;#34;Error occurred&amp;#34;)
		return
	}
	if sched.skipPodSchedule(ctx, fwk, pod) {
		return
	}
	//这段Go代码中的函数主要包括两部分逻辑。
	//首先，通过klog.LoggerWithValues和klog.NewContext函数为日志记录器logger添加了pod信息，并创建了一个新的上下文ctx。
	//然后使用logger.V(4).Info记录了一条日志，表示即将尝试调度Pod。
	//接下来，调用sched.frameworkForPod(pod)方法获取与Pod相匹配的调度框架fwk。
	//如果获取发生错误，则使用logger.Error记录错误日志并返回。
	//如果获取成功，则调用sched.skipPodSchedule(ctx, fwk, pod)判断是否跳过Pod的调度。
	//如果返回true，则表示跳过调度，函数直接返回。

	logger.V(3).Info(&amp;#34;Attempting to schedule pod&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod))

	// Synchronously attempt to find a fit for the pod.
	start := time.Now()
	state := framework.NewCycleState()
	state.SetRecordPluginMetrics(rand.Intn(100) &amp;lt; pluginMetricsSamplePercent)

	// Initialize an empty podsToActivate struct, which will be filled up by plugins or stay empty.
	podsToActivate := framework.NewPodsToActivate()
	state.Write(framework.PodsToActivateKey, podsToActivate)

	schedulingCycleCtx, cancel := context.WithCancel(ctx)
	defer cancel()

	scheduleResult, assumedPodInfo, status := sched.schedulingCycle(schedulingCycleCtx, state, fwk, podInfo, start, podsToActivate)
	if !status.IsSuccess() {
		sched.FailureHandler(schedulingCycleCtx, fwk, assumedPodInfo, status, scheduleResult.nominatingInfo, start)
		return
	}
	//这段Go代码是一个调度Pod的函数。它首先使用logger.V(3).Info记录尝试调度Pod的日志。
	//然后，它同步地尝试为Pod找到一个合适的节点。
	//在开始调度之前，它创建了一个新的CycleState对象，并设置了记录插件指标的标志。
	//接着，它初始化了一个空的PodsToActivate对象，并将其写入到CycleState中。
	//然后，它创建了一个可取消的上下文对象，并在函数结束时取消它。
	//最后，它调用schedulingCycle函数进行调度，并根据调度结果处理成功或失败的情况。

	// bind the pod to its host asynchronously (we can do this b/c of the assumption step above).
	go func() {
		bindingCycleCtx, cancel := context.WithCancel(ctx)
		defer cancel()

		metrics.Goroutines.WithLabelValues(metrics.Binding).Inc()
		defer metrics.Goroutines.WithLabelValues(metrics.Binding).Dec()

		status := sched.bindingCycle(bindingCycleCtx, state, fwk, scheduleResult, assumedPodInfo, start, podsToActivate)
		if !status.IsSuccess() {
			sched.handleBindingCycleError(bindingCycleCtx, state, fwk, assumedPodInfo, start, scheduleResult, status)
			return
		}
		// Usually, DonePod is called inside the scheduling queue,
		// but in this case, we need to call it here because this Pod won&amp;#39;t go back to the scheduling queue.
		sched.SchedulingQueue.Done(assumedPodInfo.Pod.UID)
	}()
}

//这个go函数是用于将Pod绑定到其宿主机的异步操作。
//它首先创建一个可取消的上下文，然后增加一个goroutine计数器，并在defer语句中减少该计数器。
//接着，它调用sched.bindingCycle方法来执行绑定周期操作，并根据操作结果处理错误。
//如果操作成功，则调用sched.SchedulingQueue.Done方法来标记Pod绑定完成。

var clearNominatedNode = &amp;amp;framework.NominatingInfo{NominatingMode: framework.ModeOverride, NominatedNodeName: &amp;#34;&amp;#34;}

// schedulingCycle tries to schedule a single Pod.
func (sched *Scheduler) schedulingCycle(
	ctx context.Context,
	state *framework.CycleState,
	fwk framework.Framework,
	podInfo *framework.QueuedPodInfo,
	start time.Time,
	podsToActivate *framework.PodsToActivate,
) (ScheduleResult, *framework.QueuedPodInfo, *framework.Status) {
	logger := klog.FromContext(ctx)
	pod := podInfo.Pod
	scheduleResult, err := sched.SchedulePod(ctx, fwk, state, pod)
	if err != nil {
		defer func() {
			metrics.SchedulingAlgorithmLatency.Observe(metrics.SinceInSeconds(start))
		}()
		if err == ErrNoNodesAvailable {
			status := framework.NewStatus(framework.UnschedulableAndUnresolvable).WithError(err)
			return ScheduleResult{nominatingInfo: clearNominatedNode}, podInfo, status
		}
		//该函数是一个Go语言函数，定义在一个名为Scheduler的结构体中，名为schedulingCycle。
		//该函数尝试调度一个Pod。
		//函数参数包括上下文ctx、状态state、框架fwk、待调度的Pod信息podInfo、开始时间start、待激活的PodspodsToActivate。
		//函数返回一个ScheduleResult结构体、一个*framework.QueuedPodInfo指针和一个*framework.Status指针。
		//函数首先从上下文中获取日志记录器logger，然后调用sched.SchedulePod方法尝试调度Pod。
		//如果调度成功，函数将返回调度结果、podInfo和nil。
		//如果调度失败且错误为ErrNoNodesAvailable，函数将记录调度算法的延迟指标，并返回一个带有UnschedulableAndUnresolvable状态和错误信息的ScheduleResult结构体、podInfo和status。

		fitError, ok := err.(*framework.FitError)
		if !ok {
			logger.Error(err, &amp;#34;Error selecting node for pod&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod))
			return ScheduleResult{nominatingInfo: clearNominatedNode}, podInfo, framework.AsStatus(err)
		}

		// SchedulePod() may have failed because the pod would not fit on any host, so we try to
		// preempt, with the expectation that the next time the pod is tried for scheduling it
		// will fit due to the preemption. It is also possible that a different pod will schedule
		// into the resources that were preempted, but this is harmless.

		if !fwk.HasPostFilterPlugins() {
			logger.V(3).Info(&amp;#34;No PostFilter plugins are registered, so no preemption will be performed&amp;#34;)
			return ScheduleResult{}, podInfo, framework.NewStatus(framework.Unschedulable).WithError(err)
		}
		//这段Go代码是处理调度过程中出现错误的逻辑。
		//首先，它会判断错误类型是否为*framework.FitError，如果不是，则记录错误日志并返回一个空的ScheduleResult，
		//同时将错误包装成framework.AsStatus(err)。
		//如果错误类型是*framework.FitError，则会尝试进行预占操作，以期望在下一次调度时，该Pod能够适应。
		//然后它会检查是否有注册了PostFilter插件，如果没有，则记录日志并返回一个包含framework.Unschedulable状态的ScheduleResult，
		//同时将错误包装成framework.NewStatus(framework.Unschedulable).WithError(err)。

		// Run PostFilter plugins to attempt to make the pod schedulable in a future scheduling cycle.
		result, status := fwk.RunPostFilterPlugins(ctx, state, pod, fitError.Diagnosis.NodeToStatusMap)
		msg := status.Message()
		fitError.Diagnosis.PostFilterMsg = msg
		if status.Code() == framework.Error {
			logger.Error(nil, &amp;#34;Status after running PostFilter plugins for pod&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod), &amp;#34;status&amp;#34;, msg)
		} else {
			logger.V(5).Info(&amp;#34;Status after running PostFilter plugins for pod&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod), &amp;#34;status&amp;#34;, msg)
		}

		var nominatingInfo *framework.NominatingInfo
		if result != nil {
			nominatingInfo = result.NominatingInfo
		}
		return ScheduleResult{nominatingInfo: nominatingInfo}, podInfo, framework.NewStatus(framework.Unschedulable).WithError(err)
	}
	//该函数主要执行调度过程中的后过滤器插件，以尝试使Pod在未来的调度周期中可调度。具体步骤如下：
	//1. 运行后过滤器插件，并获取运行结果和状态。
	//2. 设置诊断信息中的后过滤器消息。
	//3. 如果状态为错误，则记录错误日志；否则记录信息日志。
	//4. 如果运行结果不为空，则获取其中的提名信息。
	//5. 返回提名信息和Pod信息，以及一个不可调度的状态。
	//总结：该函数主要负责在调度过程中执行后过滤器插件，并处理运行结果和状态，最终返回提名信息和不可调度的状态。

	metrics.SchedulingAlgorithmLatency.Observe(metrics.SinceInSeconds(start))
	// Tell the cache to assume that a pod now is running on a given node, even though it hasn&amp;#39;t been bound yet.
	// This allows us to keep scheduling without waiting on binding to occur.
	assumedPodInfo := podInfo.DeepCopy()
	assumedPod := assumedPodInfo.Pod
	// assume modifies `assumedPod` by setting NodeName=scheduleResult.SuggestedHost
	err = sched.assume(logger, assumedPod, scheduleResult.SuggestedHost)
	if err != nil {
		// This is most probably result of a BUG in retrying logic.
		// We report an error here so that pod scheduling can be retried.
		// This relies on the fact that Error will check if the pod has been bound
		// to a node and if so will not add it back to the unscheduled pods queue
		// (otherwise this would cause an infinite loop).
		return ScheduleResult{nominatingInfo: clearNominatedNode}, assumedPodInfo, framework.AsStatus(err)
	}
	//这段Go代码主要实现了以下功能：
	//1. 记录调度算法的延迟指标。
	//2. 假设Pod已经运行在给定的节点上，即使它还没有被绑定。这允许我们在不等待绑定发生的情况下继续调度。
	//3. 尝试将Pod的节点名设置为推荐的主机名，并记录错误信息。如果出现错误，则返回一个清除了提名节点信息的ScheduleResult对象和假设的Pod信息，以及将错误转换为状态对象。
	//这段代码中的关键函数包括： - metrics.SchedulingAlgorithmLatency.Observe(metrics.SinceInSeconds(start))：记录调度算法的延迟指标，
	//其中start是调度开始的时间点。
	//- sched.assume(logger, assumedPod, scheduleResult.SuggestedHost)：假设Pod已经运行在给定的节点上，并将Pod的节点名设置为推荐的主机名。如果出现错误，则返回错误信息。

	// Run the Reserve method of reserve plugins.
	if sts := fwk.RunReservePluginsReserve(ctx, state, assumedPod, scheduleResult.SuggestedHost); !sts.IsSuccess() {
		// trigger un-reserve to clean up state associated with the reserved Pod
		fwk.RunReservePluginsUnreserve(ctx, state, assumedPod, scheduleResult.SuggestedHost)
		if forgetErr := sched.Cache.ForgetPod(logger, assumedPod); forgetErr != nil {
			logger.Error(forgetErr, &amp;#34;Scheduler cache ForgetPod failed&amp;#34;)
		}

		if sts.IsRejected() {
			fitErr := &amp;amp;framework.FitError{
				NumAllNodes: 1,
				Pod: pod,
				Diagnosis: framework.Diagnosis{
					NodeToStatusMap: framework.NodeToStatusMap{scheduleResult.SuggestedHost: sts},
				},
			}
			fitErr.Diagnosis.AddPluginStatus(sts)
			return ScheduleResult{nominatingInfo: clearNominatedNode}, assumedPodInfo, framework.NewStatus(sts.Code()).WithError(fitErr)
		}
		return ScheduleResult{nominatingInfo: clearNominatedNode}, assumedPodInfo, sts
	}
	//该Go函数主要执行以下操作：
	//1. 调用fwk.RunReservePluginsReserve方法运行reserve插件的Reserve方法。
	//2. 如果Reserve方法执行失败，则触发un-reserve操作以清理与预留的Pod相关的状态。
	//3. 如果un-reserve操作成功，则从调度器缓存中忘记Pod，并记录忘记操作失败的错误（如果有）。
	//4. 如果Reserve方法被拒绝，则创建一个FitError对象并返回。 Markdown格式输出如下：
	//1. 调用fwk.RunReservePluginsReserve方法运行reserve插件的Reserve方法。
	//2. 如果Reserve方法执行失败，则触发un-reserve操作以清理与预留的Pod相关的状态。
	//- 调用fwk.RunReservePluginsUnreserve方法执行un-reserve操作。
	//- 如果忘记Pod操作失败，则记录错误。
	//3. 如果Reserve方法被拒绝，则创建一个FitError对象并返回。

	// Run &amp;#34;permit&amp;#34; plugins.
	runPermitStatus := fwk.RunPermitPlugins(ctx, state, assumedPod, scheduleResult.SuggestedHost)
	if !runPermitStatus.IsWait() &amp;amp;&amp;amp; !runPermitStatus.IsSuccess() {
		// trigger un-reserve to clean up state associated with the reserved Pod
		fwk.RunReservePluginsUnreserve(ctx, state, assumedPod, scheduleResult.SuggestedHost)
		if forgetErr := sched.Cache.ForgetPod(logger, assumedPod); forgetErr != nil {
			logger.Error(forgetErr, &amp;#34;Scheduler cache ForgetPod failed&amp;#34;)
		}
		//这段Go代码中的函数是用于运行框架中的&amp;#34;permit&amp;#34;插件，并在必要时进行清理操作。
		//首先，该函数通过调用fwk.RunPermitPlugins来运行&amp;#34;permit&amp;#34;插件，并获取运行状态。
		//如果运行状态不是等待状态且不是成功状态，则需要进行清理操作。
		//接下来，该函数调用fwk.RunReservePluginsUnreserve来触发取消预留操作，以清理与预留的Pod相关的状态。
		//然后，该函数调用sched.Cache.ForgetPod来从调度器缓存中忘记Pod，并记录错误信息。
		//总之，该函数的主要功能是运行&amp;#34;permit&amp;#34;插件，并在必要时进行清理操作，包括取消预留和忘记Pod。

		if runPermitStatus.IsRejected() {
			fitErr := &amp;amp;framework.FitError{
				NumAllNodes: 1,
				Pod: pod,
				Diagnosis: framework.Diagnosis{
					NodeToStatusMap: framework.NodeToStatusMap{scheduleResult.SuggestedHost: runPermitStatus},
				},
			}
			fitErr.Diagnosis.AddPluginStatus(runPermitStatus)
			return ScheduleResult{nominatingInfo: clearNominatedNode}, assumedPodInfo, framework.NewStatus(runPermitStatus.Code()).WithError(fitErr)
		}

		return ScheduleResult{nominatingInfo: clearNominatedNode}, assumedPodInfo, runPermitStatus
	}
	//这段Go代码是 Kubernetes 调度器中的片段，用于处理调度结果。
	//- 如果 runPermitStatus.IsRejected() 返回 true，表示调度被拒绝，则创建一个 framework.FitError 错误对象，
	//记录节点状态和错误信息，并返回一个空的 ScheduleResult 和 assumedPodInfo，以及带有错误信息的 framework.Status。
	//- 如果 runPermitStatus.IsRejected() 返回 false，表示调度成功，
	//则直接返回一个带有 clearNominatedNode 的 ScheduleResult 和 assumedPodInfo，以及 runPermitStatus。
	//这段代码的主要作用是根据 runPermitStatus 的状态来决定调度是否成功，并返回相应的结果。

	// At the end of a successful scheduling cycle, pop and move up Pods if needed.
	if len(podsToActivate.Map) != 0 {
		sched.SchedulingQueue.Activate(logger, podsToActivate.Map)
		// Clear the entries after activation.
		podsToActivate.Map = make(map[string]*v1.Pod)
	}

	return scheduleResult, assumedPodInfo, nil
}

//这段Go代码是调度器在一个成功的调度周期结束时，检查是否有需要激活的Pods，如果有，则将其激活并清空激活列表。
//具体来说：
//1. 如果podsToActivate.Map不为空，即有待激活的Pods，则调用sched.SchedulingQueue.Activate方法将这些Pods激活。
//2. 激活后，清空podsToActivate.Map，即清空待激活Pods的列表。
//3. 返回调度结果scheduleResult、已假设的Pod信息assumedPodInfo和nil错误。
//这段代码的作用是确保在调度周期结束时，所有需要激活的Pods都被正确处理，并为下一个调度周期做准备。

// bindingCycle tries to bind an assumed Pod.
func (sched *Scheduler) bindingCycle(
	ctx context.Context,
	state *framework.CycleState,
	fwk framework.Framework,
	scheduleResult ScheduleResult,
	assumedPodInfo *framework.QueuedPodInfo,
	start time.Time,
	podsToActivate *framework.PodsToActivate) *framework.Status {
	logger := klog.FromContext(ctx)

	assumedPod := assumedPodInfo.Pod
	//该函数是Scheduler的一个方法，用于尝试绑定一个假设的Pod。
	//它通过传入上下文、状态、框架、调度结果、假设的Pod信息、开始时间和待激活的Pods，返回一个框架状态。
	//具体流程包括：从上下文中获取日志记录器；
	//使用假设的Pod信息获取Pod；
	//调用绑定函数进行绑定操作；根据绑定结果返回相应的框架状态。

	// Run &amp;#34;permit&amp;#34; plugins.
	if status := fwk.WaitOnPermit(ctx, assumedPod); !status.IsSuccess() {
		if status.IsRejected() {
			fitErr := &amp;amp;framework.FitError{
				NumAllNodes: 1,
				Pod: assumedPodInfo.Pod,
				Diagnosis: framework.Diagnosis{
					NodeToStatusMap: framework.NodeToStatusMap{scheduleResult.SuggestedHost: status},
					UnschedulablePlugins: sets.New(status.Plugin()),
				},
			}
			return framework.NewStatus(status.Code()).WithError(fitErr)
		}
		return status
	}
	//该函数用于运行&amp;#34;permit&amp;#34;插件，并根据插件的执行结果进行相应的处理。
	//如果插件执行失败并且被拒绝，则创建并返回一个FitError错误；否则返回插件的执行状态。

	// Run &amp;#34;prebind&amp;#34; plugins.
	if status := fwk.RunPreBindPlugins(ctx, state, assumedPod, scheduleResult.SuggestedHost); !status.IsSuccess() {
		return status
	}
	//该函数的作用是运行&amp;#34;prebind&amp;#34;插件。它首先通过调用fwk.RunPreBindPlugins方法来执行&amp;#34;prebind&amp;#34;插件，
	//并将上下文、状态、假设的Pod和建议的主机作为参数传递给该方法。
	//如果运行插件后的状态不成功，则函数会直接返回该状态。

	// Run &amp;#34;bind&amp;#34; plugins.
	if status := sched.bind(ctx, fwk, assumedPod, scheduleResult.SuggestedHost, state); !status.IsSuccess() {
		return status
	}

	// Calculating nodeResourceString can be heavy. Avoid it if klog verbosity is below 2.
	logger.V(2).Info(&amp;#34;Successfully bound pod to node&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(assumedPod), &amp;#34;node&amp;#34;, scheduleResult.SuggestedHost, &amp;#34;evaluatedNodes&amp;#34;, scheduleResult.EvaluatedNodes, &amp;#34;feasibleNodes&amp;#34;, scheduleResult.FeasibleNodes)
	metrics.PodScheduled(fwk.ProfileName(), metrics.SinceInSeconds(start))
	metrics.PodSchedulingAttempts.Observe(float64(assumedPodInfo.Attempts))
	if assumedPodInfo.InitialAttemptTimestamp != nil {
		metrics.PodSchedulingDuration.WithLabelValues(getAttemptsLabel(assumedPodInfo)).Observe(metrics.SinceInSeconds(*assumedPodInfo.InitialAttemptTimestamp))
		metrics.PodSchedulingSLIDuration.WithLabelValues(getAttemptsLabel(assumedPodInfo)).Observe(metrics.SinceInSeconds(*assumedPodInfo.InitialAttemptTimestamp))
	}
	//这段Go代码中的函数主要执行以下操作：
	//1. 调用sched.bind()函数运行名为&amp;#34;bind&amp;#34;的插件，并检查返回的状态是否成功。如果不成功，则返回该状态。
	//2. 如果日志的详细程度高于等于2，则记录一条成功将Pod绑定到节点的日志，同时记录一些指标，如评估的节点数和可行节点数。
	//3. 更新Pod相关的指标，例如记录Pod被调度的次数和调度尝试的持续时间。
	//4. 如果Pod有初始尝试时间，则记录Pod调度的持续时间和SLI（服务级别指标）持续时间。
	//这段代码的主要目的是在调度Pod后执行一些后续操作，包括记录日志和更新指标。

	// Run &amp;#34;postbind&amp;#34; plugins.
	fwk.RunPostBindPlugins(ctx, state, assumedPod, scheduleResult.SuggestedHost)

	// At the end of a successful binding cycle, move up Pods if needed.
	if len(podsToActivate.Map) != 0 {
		sched.SchedulingQueue.Activate(logger, podsToActivate.Map)
		// Unlike the logic in schedulingCycle(), we don&amp;#39;t bother deleting the entries
		// as `podsToActivate.Map` is no longer consumed.
	}

	return nil
}

//这段Go代码是 Kubernetes 调度器中的一个函数片段，主要执行以下两个操作：
//1. 运行 &amp;#34;postbind&amp;#34; 插件：fwk.RunPostBindPlugins(ctx, state, assumedPod, scheduleResult.SuggestedHost)
//这行代码会调用所有注册的 &amp;#34;postbind&amp;#34; 插件。这些插件是在绑定周期成功结束后运行的，用于执行一些额外的操作，例如更新 Pod 的状态或记录日志。
//2. 激活就绪的 Pod：如果 podsToActivate.Map 不为空，即有待激活的 Pod，
//则调用 sched.SchedulingQueue.Activate(logger, podsToActivate.Map) 来激活这些 Pod。这会将这些 Pod 加入到调度队列中，
//以便它们可以被调度到合适的节点上运行。与调度周期中的逻辑不同，这里不需要删除条目，因为 podsToActivate.Map 不再被使用。
//这段代码的主要目的是在成功完成绑定周期后，执行必要的后处理操作，并激活就绪的 Pod。

func (sched *Scheduler) handleBindingCycleError(
	ctx context.Context,
	state *framework.CycleState,
	fwk framework.Framework,
	podInfo *framework.QueuedPodInfo,
	start time.Time,
	scheduleResult ScheduleResult,
	status *framework.Status) {
	logger := klog.FromContext(ctx)
	//该函数是Scheduler的一个方法，用于处理绑定周期错误。
	//它通过记录日志来记录错误信息，其中包括上下文、框架状态、队列中的Pod信息、开始时间、调度结果和状态等。

	assumedPod := podInfo.Pod
	// trigger un-reserve plugins to clean up state associated with the reserved Pod
	fwk.RunReservePluginsUnreserve(ctx, state, assumedPod, scheduleResult.SuggestedHost)
	if forgetErr := sched.Cache.ForgetPod(logger, assumedPod); forgetErr != nil {
		logger.Error(forgetErr, &amp;#34;scheduler cache ForgetPod failed&amp;#34;)
	} else {
		// &amp;#34;Forget&amp;#34;ing an assumed Pod in binding cycle should be treated as a PodDelete event,
		// as the assumed Pod had occupied a certain amount of resources in scheduler cache.
		//
		// Avoid moving the assumed Pod itself as it&amp;#39;s always Unschedulable.
		// It&amp;#39;s intentional to &amp;#34;defer&amp;#34; this operation; otherwise MoveAllToActiveOrBackoffQueue() would
		// add this event to in-flight events and thus move the assumed pod to backoffQ anyways if the plugins don&amp;#39;t have appropriate QueueingHint.
		if status.IsRejected() {
			defer sched.SchedulingQueue.MoveAllToActiveOrBackoffQueue(logger, internalqueue.AssignedPodDelete, assumedPod, nil, func(pod *v1.Pod) bool {
				return assumedPod.UID != pod.UID
			})
		} else {
			sched.SchedulingQueue.MoveAllToActiveOrBackoffQueue(logger, internalqueue.AssignedPodDelete, assumedPod, nil, nil)
		}
	}
	//该函数主要完成以下功能：
	//1. 调用fwk.RunReservePluginsUnreserve方法，触发未预留插件清理与预留Pod相关联的状态。
	//2. 忘记缓存中的Pod，如果忘记失败，则记录错误日志。
	//3. 如果Pod被拒绝，则将除被假设的Pod本身以外的所有Pod移动到活动队列或退避队列；否则，将所有Pod移动到活动队列或退避队列。

	sched.FailureHandler(ctx, fwk, podInfo, status, clearNominatedNode, start)
}

//第一个函数sched.FailureHandler(ctx, fwk, podInfo, status, clearNominatedNode, start)是一个处理调度失败的函数。
//它接受多个参数，包括上下文、框架、Pod信息、状态、是否清除提名节点以及开始时间，用于处理Pod调度失败的情况。

func (sched *Scheduler) frameworkForPod(pod *v1.Pod) (framework.Framework, error) {
	fwk, ok := sched.Profiles[pod.Spec.SchedulerName]
	if !ok {
		return nil, fmt.Errorf(&amp;#34;profile not found for scheduler name %q&amp;#34;, pod.Spec.SchedulerName)
	}
	return fwk, nil
}

//第二个函数(sched *Scheduler) frameworkForPod(pod *v1.Pod) (framework.Framework, error)是一个根据Pod获取对应调度框架的函数。
//它接受一个Pod作为参数，通过Pod的Spec.SchedulerName来查找对应的调度框架。
//如果找到，则返回该框架；如果找不到，则返回一个错误。

// skipPodSchedule returns true if we could skip scheduling the pod for specified cases.
func (sched *Scheduler) skipPodSchedule(ctx context.Context, fwk framework.Framework, pod *v1.Pod) bool {
	// Case 1: pod is being deleted.
	if pod.DeletionTimestamp != nil {
		fwk.EventRecorder().Eventf(pod, nil, v1.EventTypeWarning, &amp;#34;FailedScheduling&amp;#34;, &amp;#34;Scheduling&amp;#34;, &amp;#34;skip schedule deleting pod: %v/%v&amp;#34;, pod.Namespace, pod.Name)
		klog.FromContext(ctx).V(3).Info(&amp;#34;Skip schedule deleting pod&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod))
		return true
	}
	//该函数是一个Go语言函数，名为skipPodSchedule，它属于Scheduler类型。函数的主要功能是判断是否可以跳过指定Pod的调度。
	//具体来说，函数首先检查Pod的DeletionTimestamp是否为nil，如果是，则记录事件和日志，并返回true，表示可以跳过调度。
	//否则，函数不进行任何操作，返回false。

	// Case 2: pod that has been assumed could be skipped.
	// An assumed pod can be added again to the scheduling queue if it got an update event
	// during its previous scheduling cycle but before getting assumed.
	isAssumed, err := sched.Cache.IsAssumedPod(pod)
	if err != nil {
		// TODO(91633): pass ctx into a revised HandleError
		utilruntime.HandleError(fmt.Errorf(&amp;#34;failed to check whether pod %s/%s is assumed: %v&amp;#34;, pod.Namespace, pod.Name, err))
		return false
	}
	return isAssumed
}

//该函数用于判断一个Pod是否已被调度器假设（Assumed）。
//- 首先，它调用sched.Cache.IsAssumedPod(pod)方法来检查Pod是否已被假设。
//- 如果检查过程中出现错误，会通过utilruntime.HandleError方法记录错误信息，并返回false。
//- 如果检查没有错误，则直接返回检查结果。
//这个函数的主要作用是在调度Pod时，判断该Pod是否需要重新进入调度队列。
//如果Pod在上一次调度周期中更新了事件，但在被假设之前，它可能会被再次添加到调度队列中。

// schedulePod tries to schedule the given pod to one of the nodes in the node list.
// If it succeeds, it will return the name of the node.
// If it fails, it will return a FitError with reasons.
func (sched *Scheduler) schedulePod(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) (result ScheduleResult, err error) {
	trace := utiltrace.New(&amp;#34;Scheduling&amp;#34;, utiltrace.Field{Key: &amp;#34;namespace&amp;#34;, Value: pod.Namespace}, utiltrace.Field{Key: &amp;#34;name&amp;#34;, Value: pod.Name})
	defer trace.LogIfLong(100 * time.Millisecond)
	if err := sched.Cache.UpdateSnapshot(klog.FromContext(ctx), sched.nodeInfoSnapshot); err != nil {
		return result, err
	}
	trace.Step(&amp;#34;Snapshotting scheduler cache and node infos done&amp;#34;)
	//该函数是一个调度函数，用于尝试将给定的Pod调度到节点列表中的一个节点上。
	//如果成功，它将返回节点的名称；如果失败，它将返回一个FitError错误，其中包含原因。
	//函数首先创建一个utiltrace对象用于记录跟踪信息，并在函数退出时记录跟踪信息的时长。
	//然后，它通过调用sched.Cache.UpdateSnapshot函数更新调度程序的缓存和节点信息快照。如果更新出现错误，函数将返回错误。
	//最后，函数通过调用trace.Step记录一个跟踪步骤，表示快照节点信息和调度程序缓存的操作已经完成。

	if sched.nodeInfoSnapshot.NumNodes() == 0 {
		return result, ErrNoNodesAvailable
	}

	feasibleNodes, diagnosis, err := sched.findNodesThatFitPod(ctx, fwk, state, pod)
	if err != nil {
		return result, err
	}
	trace.Step(&amp;#34;Computing predicates done&amp;#34;)

	if len(feasibleNodes) == 0 {
		return result, &amp;amp;framework.FitError{
			Pod: pod,
			NumAllNodes: sched.nodeInfoSnapshot.NumNodes(),
			Diagnosis: diagnosis,
		}
	}
	//这段Go代码是关于调度器在一个集群中为一个Pod寻找合适节点的逻辑。
	//首先，它检查当前集群中是否有可用节点，如果没有则返回错误ErrNoNodesAvailable。
	//接下来，它调用sched.findNodesThatFitPod函数来找到能够容纳该Pod的节点。如果该函数返回错误，则直接返回错误。
	//如果找到了合适的节点，代码会继续执行
	//；如果没有找到合适的节点，则返回一个FitError错误，其中包含了Pod信息、集群中节点的数量以及诊断信息。

	// When only one node after predicate, just use it.
	if len(feasibleNodes) == 1 {
		return ScheduleResult{
			SuggestedHost: feasibleNodes[0].Node().Name,
			EvaluatedNodes: 1 + len(diagnosis.NodeToStatusMap),
			FeasibleNodes: 1,
		}, nil
	}
	//该函数是一个Go语言函数片段，用于在满足一定条件时返回一个ScheduleResult结构体实例。
	//首先，函数通过判断feasibleNodes切片的长度是否为1来确定是否满足某种条件。
	//如果满足条件，即feasibleNodes长度为1，
	//则创建并返回一个ScheduleResult结构体实例，
	//其中SuggestedHost字段被设置为feasibleNodes[0].Node().Name，EvaluatedNodes字段被设置为1 + len(diagnosis.NodeToStatusMap)，
	//FeasibleNodes字段被设置为1。
	//这个函数的主要作用是在找到唯一一个符合条件的节点时，生成一个调度结果，建议将任务调度到该节点上，并统计评估的节点数和可行节点数。

	priorityList, err := prioritizeNodes(ctx, sched.Extenders, fwk, state, pod, feasibleNodes)
	if err != nil {
		return result, err
	}

	host, _, err := selectHost(priorityList, numberOfHighestScoredNodesToReport)
	trace.Step(&amp;#34;Prioritizing done&amp;#34;)

	return ScheduleResult{
		SuggestedHost: host,
		EvaluatedNodes: len(feasibleNodes) + len(diagnosis.NodeToStatusMap),
		FeasibleNodes: len(feasibleNodes),
	}, err
}

//这段Go代码是一个调度过程中的一部分，用于根据一系列规则对节点进行优先级排序，并最终选择一个最适合运行Pod的节点。
//首先，函数调用prioritizeNodes来对可行节点进行优先级排序，得到一个优先级列表priorityList。如果在这个过程中出现错误，则会返回一个错误结果。
//接下来，函数调用selectHost来从优先级列表中选择一个最适合运行Pod的节点。选择节点的过程会考虑到节点的得分和其他因素。
//选择完成后，会记录一个调度过程的步骤。
//最后，函数返回一个ScheduleResult结构体实例，其中包含了建议的节点主机名host、评估的节点数和可行节点数。
//如果在调度过程中出现错误，
//则会将错误一起返回。
//这个函数的主要作用是在给定的一组可行节点中，根据预定的规则和策略选择一个最优节点来运行Pod，并生成相应的调度结果。

// Filters the nodes to find the ones that fit the pod based on the framework
// filter plugins and filter extenders.
func (sched *Scheduler) findNodesThatFitPod(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) ([]*framework.NodeInfo, framework.Diagnosis, error) {
	logger := klog.FromContext(ctx)
	diagnosis := framework.Diagnosis{
		NodeToStatusMap: make(framework.NodeToStatusMap),
	}
	//这段Go代码定义了一个名为findNodesThatFitPod的函数，它属于Scheduler类型。该函数的功能是筛选出适合运行给定Pod的节点。
	//函数的输入参数包括：
	//- ctx：上下文对象，用于控制函数执行的生命周期。
	//- fwk：一个实现了Framework接口的对象，用于执行筛选插件和扩展程序。
	//- state：一个CycleState对象，包含了调度过程中的状态信息。
	//- pod：一个指向v1.Pod对象的指针，表示需要调度的Pod。
	//函数的输出结果包括：
	//- 一个包含所有适合运行Pod的节点信息的切片。
	//- 一个Diagnosis对象，包含了在筛选过程中收集到的诊断信息。
	//- 一个错误对象，如果在筛选过程中发生错误，则会返回该错误。
	//在函数内部，它首先创建了一个logger对象，用于记录日志信息。
	//然后创建了一个diagnosis对象，并初始化了它的NodeToStatusMap字段。
	//接下来，函数调用fwk.Filter方法来筛选出适合运行Pod的节点。这个方法会根据框架中的筛选插件和扩展程序来判断节点是否适合运行Pod。
	//如果节点不适合，则会在diagnosis.NodeToStatusMap中记录下该节点不适合的原因。
	//最后，函数返回筛选出的节点信息切片、诊断信息和错误对象（如果有）。

	allNodes, err := sched.nodeInfoSnapshot.NodeInfos().List()
	if err != nil {
		return nil, diagnosis, err
	}
	// Run &amp;#34;prefilter&amp;#34; plugins.
	preRes, s := fwk.RunPreFilterPlugins(ctx, state, pod)
	if !s.IsSuccess() {
		if !s.IsRejected() {
			return nil, diagnosis, s.AsError()
		}
		// All nodes in NodeToStatusMap will have the same status so that they can be handled in the preemption.
		// Some non trivial refactoring is needed to avoid this copy.
		for _, n := range allNodes {
			diagnosis.NodeToStatusMap[n.Node().Name] = s
		}
		//该Go函数主要实现了以下功能：
		//1. 调用sched.nodeInfoSnapshot.NodeInfos().List()获取所有节点的信息。
		//2. 运行&amp;#34;prefilter&amp;#34;插件，通过fwk.RunPreFilterPlugins(ctx, state, pod)获取插件运行结果。
		//3. 如果插件运行结果不成功且未被拒绝，则将所有节点的状态更新为该插件的运行结果状态，并返回错误信息。
		//具体分析如下：
		//- 首先，函数会尝试获取集群中所有节点的信息，并将结果保存在allNodes变量中。如果获取节点信息时出现错误，函数会立即返回错误信息。
		//- 接下来，函数会运行&amp;#34;prefilter&amp;#34;插件，并将运行结果保存在preRes变量中。如果插件运行结果不成功（即未通过插件的验证），
		//则会根据插件的运行结果状态进行处理。
		//- 如果插件运行结果状态既不是成功也不是被拒绝，则函数会将所有节点的状态更新为该插件的运行结果状态，
		//并将该状态保存在diagnosis.NodeToStatusMap中。这样做的目的是为了在后续的预删除操作中，能够统一处理所有节点的状态。
		//- 最后，如果插件运行结果状态是被拒绝的，则函数会直接返回错误信息。
		//总之，该函数的主要作用是在调度过程中运行&amp;#34;prefilter&amp;#34;插件，并根据插件的运行结果来更新节点的状态或返回错误信息。

		// Record the messages from PreFilter in Diagnosis.PreFilterMsg.
		msg := s.Message()
		diagnosis.PreFilterMsg = msg
		logger.V(5).Info(&amp;#34;Status after running PreFilter plugins for pod&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod), &amp;#34;status&amp;#34;, msg)
		diagnosis.AddPluginStatus(s)
		return nil, diagnosis, nil
	}
	//该函数主要功能是记录PreFilter插件运行后的内容。
	//1. 首先获取消息msg := s.Message()；
	//2. 然后将该消息记录到diagnosis.PreFilterMsg中；
	//3. 使用logger.V(5).Info记录日志，包括pod信息和状态消息；
	//4. 最后将插件状态添加到diagnosis中，并返回nil, diagnosis, nil。

	// &amp;#34;NominatedNodeName&amp;#34; can potentially be set in a previous scheduling cycle as a result of preemption.
	// This node is likely the only candidate that will fit the pod, and hence we try it first before iterating over all nodes.
	if len(pod.Status.NominatedNodeName) &amp;gt; 0 {
		feasibleNodes, err := sched.evaluateNominatedNode(ctx, pod, fwk, state, diagnosis)
		if err != nil {
			logger.Error(err, &amp;#34;Evaluation failed on nominated node&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod), &amp;#34;node&amp;#34;, pod.Status.NominatedNodeName)
		}
		// Nominated node passes all the filters, scheduler is good to assign this node to the pod.
		if len(feasibleNodes) != 0 {
			return feasibleNodes, diagnosis, nil
		}
	}
	//这段Go代码的功能是在调度Pod之前，优先尝试在上一次调度周期中因抢占而被提名的节点上是否可以放置该Pod。
	//如果提名的节点通过了所有的过滤器，则将该节点分配给Pod。

	nodes := allNodes
	if !preRes.AllNodes() {
		nodes = make([]*framework.NodeInfo, 0, len(preRes.NodeNames))
		for _, n := range allNodes {
			if !preRes.NodeNames.Has(n.Node().Name) {
				// We consider Nodes that are filtered out by PreFilterResult as rejected via UnschedulableAndUnresolvable.
				// We have to record them in NodeToStatusMap so that they won&amp;#39;t be considered as candidates in the preemption.
				diagnosis.NodeToStatusMap[n.Node().Name] = framework.NewStatus(framework.UnschedulableAndUnresolvable, &amp;#34;node is filtered out by the prefilter result&amp;#34;)
				continue
			}
			nodes = append(nodes, n)
		}
	}
	feasibleNodes, err := sched.findNodesThatPassFilters(ctx, fwk, state, pod, &amp;amp;diagnosis, nodes)
	// always try to update the sched.nextStartNodeIndex regardless of whether an error has occurred
	// this is helpful to make sure that all the nodes have a chance to be searched
	processedNodes := len(feasibleNodes) + len(diagnosis.NodeToStatusMap)
	sched.nextStartNodeIndex = (sched.nextStartNodeIndex + processedNodes) % len(nodes)
	if err != nil {
		return nil, diagnosis, err
	}
	//该函数是Go语言编写的，用于在给定节点列表中找到满足特定条件的节点。
	//首先，函数会检查preRes.AllNodes()是否为真，如果不为真，则遍历allNodes列表，
	//将不在preRes.NodeNames中的节点过滤掉，并将过滤掉的节点标记为&amp;#34;rejected via UnschedulableAndUnresolvable&amp;#34;。
	//接下来，函数调用sched.findNodesThatPassFilters方法来找到通过特定过滤器的节点，并将结果保存在feasibleNodes变量中。
	//最后，函数更新sched.nextStartNodeIndex属性，并返回结果。

	feasibleNodesAfterExtender, err := findNodesThatPassExtenders(ctx, sched.Extenders, pod, feasibleNodes, diagnosis.NodeToStatusMap)
	if err != nil {
		return nil, diagnosis, err
	}
	if len(feasibleNodesAfterExtender) != len(feasibleNodes) {
		// Extenders filtered out some nodes.
		//
		// Extender doesn&amp;#39;t support any kind of requeueing feature like EnqueueExtensions in the scheduling framework.
		// When Extenders reject some Nodes and the pod ends up being unschedulable,
		// we put framework.ExtenderName to pInfo.UnschedulablePlugins.
		// This Pod will be requeued from unschedulable pod pool to activeQ/backoffQ
		// by any kind of cluster events.
		// https://github.com/kubernetes/kubernetes/issues/122019
		if diagnosis.UnschedulablePlugins == nil {
			diagnosis.UnschedulablePlugins = sets.New[string]()
		}
		diagnosis.UnschedulablePlugins.Insert(framework.ExtenderName)
	}

	return feasibleNodesAfterExtender, diagnosis, nil
}

//这段Go代码的功能是在给定的节点列表中，通过调用一组扩展程序来找到满足特定条件的节点。
//首先，函数调用findNodesThatPassExtenders方法，该方法会遍历给定的扩展程序列表，
//并将通过扩展程序过滤的节点保存在feasibleNodesAfterExtender变量中。
//如果feasibleNodesAfterExtender的长度与feasibleNodes的长度不相等，则说明有节点被扩展程序过滤掉了。
//在这种情况下，函数会将framework.ExtenderName添加到diagnosis.UnschedulablePlugins集合中，以表示该扩展程序导致了节点被拒绝。
//最后，函数返回feasibleNodesAfterExtender、diagnosis和nil作为结果。

func (sched *Scheduler) evaluateNominatedNode(ctx context.Context, pod *v1.Pod, fwk framework.Framework, state *framework.CycleState, diagnosis framework.Diagnosis) ([]*framework.NodeInfo, error) {
	nnn := pod.Status.NominatedNodeName
	nodeInfo, err := sched.nodeInfoSnapshot.Get(nnn)
	if err != nil {
		return nil, err
	}
	node := []*framework.NodeInfo{nodeInfo}
	feasibleNodes, err := sched.findNodesThatPassFilters(ctx, fwk, state, pod, &amp;amp;diagnosis, node)
	if err != nil {
		return nil, err
	}

	feasibleNodes, err = findNodesThatPassExtenders(ctx, sched.Extenders, pod, feasibleNodes, diagnosis.NodeToStatusMap)
	if err != nil {
		return nil, err
	}

	return feasibleNodes, nil
}

//该函数是Scheduler的一个方法，用于评估提名的节点是否适合放置Pod。
//它首先通过NominatedNodeName获取节点信息，然后调用findNodesThatPassFilters方法过滤不满足条件的节点，
//并调用findNodesThatPassExtenders方法进一步过滤节点。最终返回满足条件的节点列表。

// hasScoring checks if scoring nodes is configured.
func (sched *Scheduler) hasScoring(fwk framework.Framework) bool {
	if fwk.HasScorePlugins() {
		return true
	}
	for _, extender := range sched.Extenders {
		if extender.IsPrioritizer() {
			return true
		}
	}
	return false
}

//该函数用于判断调度器中是否配置了评分节点。
//首先检查给定的框架是否有评分插件，如果有，则返回true。
//如果没有，则遍历调度器的扩展程序，如果某个扩展程序是优先级判断器，则返回true。
//最后，如果没有找到评分插件或优先级判断器，则返回false。

// hasExtenderFilters checks if any extenders filter nodes.
func (sched *Scheduler) hasExtenderFilters() bool {
	for _, extender := range sched.Extenders {
		if extender.IsFilter() {
			return true
		}
	}
	return false
}

//该函数用于判断调度器中是否存在扩展程序过滤节点。
//它遍历调度器的扩展程序列表，如果找到任何一个具有过滤功能的扩展程序，则返回true，表示存在扩展程序过滤节点。
//如果没有找到任何具有过滤功能的扩展程序，则返回false。

// findNodesThatPassFilters finds the nodes that fit the filter plugins.
func (sched *Scheduler) findNodesThatPassFilters(
	ctx context.Context,
	fwk framework.Framework,
	state *framework.CycleState,
	pod *v1.Pod,
	diagnosis *framework.Diagnosis,
	nodes []*framework.NodeInfo) ([]*framework.NodeInfo, error) {
	numAllNodes := len(nodes)
	numNodesToFind := sched.numFeasibleNodesToFind(fwk.PercentageOfNodesToScore(), int32(numAllNodes))
	if !sched.hasExtenderFilters() &amp;amp;&amp;amp; !sched.hasScoring(fwk) {
		numNodesToFind = 1
	}
	//该函数是Scheduler的一个方法，用于查找符合过滤条件的节点。
	//它根据传入的过滤插件、调度框架、调度状态、Pod和诊断信息，从给定的节点列表中找到符合条件的节点。
	//函数首先计算出需要查找的节点数量，根据是否有扩展器过滤器和评分器来决定最终的查找数量。
	//如果没有扩展器过滤器和评分器，则只需要找到一个符合条件的节点。
	//函数返回最终找到的符合条件的节点列表和可能的错误。

	// Create feasible list with enough space to avoid growing it
	// and allow assigning.
	feasibleNodes := make([]*framework.NodeInfo, numNodesToFind)

	if !fwk.HasFilterPlugins() {
		for i := range feasibleNodes {
			feasibleNodes[i] = nodes[(sched.nextStartNodeIndex+i)%numAllNodes]
		}
		return feasibleNodes, nil
	}
	//这段代码是Scheduler的一个方法，用于创建一个可行节点列表，列表的长度为numNodesToFind，以避免增长并允许分配。
	//如果fwk没有过滤插件，则将nodes中的节点按顺序填充到feasibleNodes中，并返回该列表。

	errCh := parallelize.NewErrorChannel()
	var feasibleNodesLen int32
	ctx, cancel := context.WithCancel(ctx)
	defer cancel()

	type nodeStatus struct {
		node string
		status *framework.Status
	}
	result := make([]*nodeStatus, numAllNodes)
	checkNode := func(i int) {
		// We check the nodes starting from where we left off in the previous scheduling cycle,
		// this is to make sure all nodes have the same chance of being examined across pods.
		nodeInfo := nodes[(sched.nextStartNodeIndex+i)%numAllNodes]
		status := fwk.RunFilterPluginsWithNominatedPods(ctx, state, pod, nodeInfo)
		if status.Code() == framework.Error {
			errCh.SendErrorWithCancel(status.AsError(), cancel)
			return
		}
		if status.IsSuccess() {
			length := atomic.AddInt32(&amp;amp;feasibleNodesLen, 1)
			if length &amp;gt; numNodesToFind {
				cancel()
				atomic.AddInt32(&amp;amp;feasibleNodesLen, -1)
			} else {
				feasibleNodes[length-1] = nodeInfo
			}
		} else {
			result[i] = &amp;amp;nodeStatus{node: nodeInfo.Node().Name, status: status}
		}
	}
	//这段Go代码中的函数是一个并发执行节点检查的函数。
	//它通过调用parallelize.NewErrorChannel()创建了一个错误通道errCh，用于收集并发执行过程中的错误信息。
	//函数使用context.WithCancel(ctx)创建了一个可取消的上下文ctx，并在函数结束时通过defer cancel()取消该上下文，
	//以确保所有并发操作都被正确终止。 函数内部定义了一个nodeStatus结构体，用于存储节点的名称和状态。
	//result是一个用于存储检查结果的切片，其长度为numAllNodes。checkNode是一个闭包函数，用于检查节点是否适合放置一个Pod。
	//它通过计算索引，从上一次调度周期中未检查的节点开始检查。
	//在检查节点时，它会调用fwk.RunFilterPluginsWithNominatedPods来运行过滤插件，
	//并根据插件的返回状态来更新feasibleNodesLen和feasibleNodes，或者将节点的状态存储到result中。
	//如果节点检查过程中出现错误，会通过错误通道发送错误信息并取消上下文。
	//总之，这个函数的作用是并发地检查一组节点，并收集检查结果和错误信息。

	beginCheckNode := time.Now()
	statusCode := framework.Success
	defer func() {
		// We record Filter extension point latency here instead of in framework.go because framework.RunFilterPlugins
		// function is called for each node, whereas we want to have an overall latency for all nodes per scheduling cycle.
		// Note that this latency also includes latency for `addNominatedPods`, which calls framework.RunPreFilterAddPod.
		metrics.FrameworkExtensionPointDuration.WithLabelValues(metrics.Filter, statusCode.String(), fwk.ProfileName()).Observe(metrics.SinceInSeconds(beginCheckNode))
	}()

	// Stops searching for more nodes once the configured number of feasible nodes
	// are found.
	fwk.Parallelizer().Until(ctx, numAllNodes, checkNode, metrics.Filter)
	feasibleNodes = feasibleNodes[:feasibleNodesLen]
	for _, item := range result {
		if item == nil {
			continue
		}
		diagnosis.NodeToStatusMap[item.node] = item.status
		diagnosis.AddPluginStatus(item.status)
	}
	if err := errCh.ReceiveError(); err != nil {
		statusCode = framework.Error
		return feasibleNodes, err
	}
	return feasibleNodes, nil
}

//这段Go代码中的函数是用于在调度周期内并行检查多个节点是否适合放置Pod的函数。
//它首先记录了过滤扩展点的延迟时间，并在函数退出时更新相关指标。
//然后使用fwk.Parallelizer().Until方法并行执行checkNode函数，该函数会对每个节点进行检查，
//并根据检查结果更新feasibleNodes和diagnosis.NodeToStatusMap。
//最后，该函数会从错误通道接收错误信息，并根据错误信息更新状态码并返回结果。

// numFeasibleNodesToFind returns the number of feasible nodes that once found, the scheduler stops
// its search for more feasible nodes.
func (sched *Scheduler) numFeasibleNodesToFind(percentageOfNodesToScore *int32, numAllNodes int32) (numNodes int32) {
	if numAllNodes &amp;lt; minFeasibleNodesToFind {
		return numAllNodes
	}

	// Use profile percentageOfNodesToScore if it&amp;#39;s set. Otherwise, use global percentageOfNodesToScore.
	var percentage int32
	if percentageOfNodesToScore != nil {
		percentage = *percentageOfNodesToScore
	} else {
		percentage = sched.percentageOfNodesToScore
	}

	if percentage == 0 {
		percentage = int32(50) - numAllNodes/125
		if percentage &amp;lt; minFeasibleNodesPercentageToFind {
			percentage = minFeasibleNodesPercentageToFind
		}
	}

	numNodes = numAllNodes * percentage / 100
	if numNodes &amp;lt; minFeasibleNodesToFind {
		return minFeasibleNodesToFind
	}

	return numNodes
}

//该函数是Go语言编写的，用于计算调度器需要找到的可行节点的数量。
//函数首先判断所有节点的数量是否小于最小可行节点数，如果是，则直接返回所有节点数。
//然后根据传入的节点评分百分比或全局百分比计算需要找到的节点数。
//如果百分比为0，则根据节点总数计算一个默认的百分比。
//最后，返回计算得到的可行节点数，如果小于最小可行节点数，则返回最小可行节点数。

func findNodesThatPassExtenders(ctx context.Context, extenders []framework.Extender, pod *v1.Pod, feasibleNodes []*framework.NodeInfo, statuses framework.NodeToStatusMap) ([]*framework.NodeInfo, error) {
	logger := klog.FromContext(ctx)
	// Extenders are called sequentially.
	// Nodes in original feasibleNodes can be excluded in one extender, and pass on to the next
	// extender in a decreasing manner.
	for _, extender := range extenders {
		if len(feasibleNodes) == 0 {
			break
		}
		if !extender.IsInterested(pod) {
			continue
		}
		//该函数的功能是通过调用一系列的extenders来筛选出能够运行pod的节点。
		//它会依次调用每个extender，并将筛选后的节点传递给下一个extender。
		//如果某个extender对pod不感兴趣，则会跳过该extender。

		// Status of failed nodes in failedAndUnresolvableMap will be added or overwritten in &amp;lt;statuses&amp;gt;,
		// so that the scheduler framework can respect the UnschedulableAndUnresolvable status for
		// particular nodes, and this may eventually improve preemption efficiency.
		// Note: users are recommended to configure the extenders that may return UnschedulableAndUnresolvable
		// status ahead of others.
		feasibleList, failedMap, failedAndUnresolvableMap, err := extender.Filter(pod, feasibleNodes)
		if err != nil {
			if extender.IsIgnorable() {
				logger.Info(&amp;#34;Skipping extender as it returned error and has ignorable flag set&amp;#34;, &amp;#34;extender&amp;#34;, extender, &amp;#34;err&amp;#34;, err)
				continue
			}
			return nil, err
		}
		//这段Go代码是调用一个名为extender.Filter的函数，该函数用于过滤出适合放置Pod的节点，并更新节点的状态。
		//函数的返回值包括可行节点列表feasibleList、失败节点映射failedMap、失败且不可解决节点映射failedAndUnresolvableMap和错误信息err。
		//如果extender.Filter函数返回错误，且该错误可被忽略（通过extender.IsIgnorable()判断），则会打印日志信息并继续执行；否则，直接返回错误。

		for failedNodeName, failedMsg := range failedAndUnresolvableMap {
			var aggregatedReasons []string
			if _, found := statuses[failedNodeName]; found {
				aggregatedReasons = statuses[failedNodeName].Reasons()
			}
			aggregatedReasons = append(aggregatedReasons, failedMsg)
			statuses[failedNodeName] = framework.NewStatus(framework.UnschedulableAndUnresolvable, aggregatedReasons...)
		}
		//这段Go代码遍历failedAndUnresolvableMap，对于每个失败且不可解决的节点，获取其已有的状态信息（如果存在），
		//将当前失败消息添加到状态原因列表中，并更新节点的状态为UnschedulableAndUnresolvable。
		//这里使用了framework.NewStatus函数创建新的状态对象。

		for failedNodeName, failedMsg := range failedMap {
			if _, found := failedAndUnresolvableMap[failedNodeName]; found {
				// failedAndUnresolvableMap takes precedence over failedMap
				// note that this only happens if the extender returns the node in both maps
				continue
			}
			if _, found := statuses[failedNodeName]; !found {
				statuses[failedNodeName] = framework.NewStatus(framework.Unschedulable, failedMsg)
			} else {
				statuses[failedNodeName].AppendReason(failedMsg)
			}
		}

		feasibleNodes = feasibleList
	}
	return feasibleNodes, nil
}

//这段Go代码是一个for循环，用于遍历failedMap，并根据条件对failedAndUnresolvableMap和statuses进行操作。
//接着将feasibleList赋值给feasibleNodes，并最终返回feasibleNodes和nil。
//具体来说：
//- 遍历failedMap中的每个元素，其中failedNodeName为键，failedMsg为值。
//- 判断failedAndUnresolvableMap中是否存在键为failedNodeName的元素，如果存在则跳过当前循环。
//- 判断statuses中是否存在键为failedNodeName的元素，如果存在则将failedMsg追加为该元素的reason，
//如果不存在则创建一个新的framework.Status对象，并将其添加到statuses中。
//最后，将feasibleList赋值给feasibleNodes，并返回feasibleNodes和nil。

// prioritizeNodes prioritizes the nodes by running the score plugins,
// which return a score for each node from the call to RunScorePlugins().
// The scores from each plugin are added together to make the score for that node, then
// any extenders are run as well.
// All scores are finally combined (added) to get the total weighted scores of all nodes
func prioritizeNodes(
	ctx context.Context,
	extenders []framework.Extender,
	fwk framework.Framework,
	state *framework.CycleState,
	pod *v1.Pod,
	nodes []*framework.NodeInfo,
) ([]framework.NodePluginScores, error) {
	logger := klog.FromContext(ctx)
	// If no priority configs are provided, then all nodes will have a score of one.
	// This is required to generate the priority list in the required format
	if len(extenders) == 0 &amp;amp;&amp;amp; !fwk.HasScorePlugins() {
		result := make([]framework.NodePluginScores, 0, len(nodes))
		for i := range nodes {
			result = append(result, framework.NodePluginScores{
				Name: nodes[i].Node().Name,
				TotalScore: 1,
			})
		}
		return result, nil
	} //该函数的作用是通过运行评分插件来优先级排序节点。
	// 首先，它根据调用RunScorePlugins()方法从每个插件返回的分数计算每个节点的分数
	// 然后，它运行任何扩展程序。最后，将所有分数相加得到节点的总权重分数。
	//如果未提供优先级配置，则所有节点的得分为1。

	// Run PreScore plugins.
	preScoreStatus := fwk.RunPreScorePlugins(ctx, state, pod, nodes)
	if !preScoreStatus.IsSuccess() {
		return nil, preScoreStatus.AsError()
	}
	//该函数运行PreScore插件，通过fwk.RunPreScorePlugins方法执行。如果运行成功，则继续执行后续代码；如果运行失败，则返回错误信息。

	// Run the Score plugins.
	nodesScores, scoreStatus := fwk.RunScorePlugins(ctx, state, pod, nodes)
	if !scoreStatus.IsSuccess() {
		return nil, scoreStatus.AsError()
	}

	// Additional details logged at level 10 if enabled.
	loggerVTen := logger.V(10)
	if loggerVTen.Enabled() {
		for _, nodeScore := range nodesScores {
			for _, pluginScore := range nodeScore.Scores {
				loggerVTen.Info(&amp;#34;Plugin scored node for pod&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod), &amp;#34;plugin&amp;#34;, pluginScore.Name, &amp;#34;node&amp;#34;, nodeScore.Name, &amp;#34;score&amp;#34;, pluginScore.Score)
			}
		}
	}
	//该函数运行Score插件，通过fwk.RunScorePlugins方法为每个节点计算一个得分。
	//如果运行成功，则继续执行后续代码；如果运行失败，则返回错误信息。如果日志级别10被启用，则会记录每个插件为每个节点打分的详细信息。

	if len(extenders) != 0 &amp;amp;&amp;amp; nodes != nil {
		// allNodeExtendersScores has all extenders scores for all nodes.
		// It is keyed with node name.
		allNodeExtendersScores := make(map[string]*framework.NodePluginScores, len(nodes))
		var mu sync.Mutex
		var wg sync.WaitGroup
		for i := range extenders {
			if !extenders[i].IsInterested(pod) {
				continue
			}
			wg.Add(1)
			go func(extIndex int) {
				metrics.Goroutines.WithLabelValues(metrics.PrioritizingExtender).Inc()
				defer func() {
					metrics.Goroutines.WithLabelValues(metrics.PrioritizingExtender).Dec()
					wg.Done()
				}()
				prioritizedList, weight, err := extenders[extIndex].Prioritize(pod, nodes)
				if err != nil {
					// Prioritization errors from extender can be ignored, let k8s/other extenders determine the priorities
					logger.V(5).Info(&amp;#34;Failed to run extender&amp;#39;s priority function. No score given by this extender.&amp;#34;, &amp;#34;error&amp;#34;, err, &amp;#34;pod&amp;#34;, klog.KObj(pod), &amp;#34;extender&amp;#34;, extenders[extIndex].Name())
					return
				}
				mu.Lock()
				defer mu.Unlock()
				for i := range *prioritizedList {
					nodename := (*prioritizedList)[i].Host
					score := (*prioritizedList)[i].Score
					if loggerVTen.Enabled() {
						loggerVTen.Info(&amp;#34;Extender scored node for pod&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod), &amp;#34;extender&amp;#34;, extenders[extIndex].Name(), &amp;#34;node&amp;#34;, nodename, &amp;#34;score&amp;#34;, score)
					}
					//这段Go代码的功能是使用一组扩展器(extendenders)对一组节点(nodes)进行优先级排序。
					//首先，它会遍历所有扩展器，并对每个感兴趣的扩展器启动一个goroutine来调用其优先级函数。
					//扩展器的优先级函数返回一个加权优先级列表(prioritizedList)和一个权重(weight)。
					//然后，这段代码会将每个扩展器对每个节点的得分存储在一个映射(allNodeExtendersScores)中，以便后续使用。

					// MaxExtenderPriority may diverge from the max priority used in the scheduler and defined by MaxNodeScore,
					// therefore we need to scale the score returned by extenders to the score range used by the scheduler.
					finalscore := score * weight * (framework.MaxNodeScore / extenderv1.MaxExtenderPriority)

					if allNodeExtendersScores[nodename] == nil {
						allNodeExtendersScores[nodename] = &amp;amp;framework.NodePluginScores{
							Name: nodename,
							Scores: make([]framework.PluginScore, 0, len(extenders)),
						}
					}
					allNodeExtendersScores[nodename].Scores = append(allNodeExtendersScores[nodename].Scores, framework.PluginScore{
						Name: extenders[extIndex].Name(),
						Score: finalscore,
					})
					allNodeExtendersScores[nodename].TotalScore += finalscore
				}
			}(i)
		}
		// wait for all go routines to finish
		wg.Wait()
		for i := range nodesScores {
			if score, ok := allNodeExtendersScores[nodes[i].Node().Name]; ok {
				nodesScores[i].Scores = append(nodesScores[i].Scores, score.Scores...)
				nodesScores[i].TotalScore += score.TotalScore
			}
		}
	}

	if loggerVTen.Enabled() {
		for i := range nodesScores {
			loggerVTen.Info(&amp;#34;Calculated node&amp;#39;s final score for pod&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod), &amp;#34;node&amp;#34;, nodesScores[i].Name, &amp;#34;score&amp;#34;, nodesScores[i].TotalScore)
		}
	}
	return nodesScores, nil
}

var errEmptyPriorityList = errors.New(&amp;#34;empty priorityList&amp;#34;)

// selectHost takes a prioritized list of nodes and then picks one
// in a reservoir sampling manner from the nodes that had the highest score.
// It also returns the top {count} Nodes,
// and the top of the list will be always the selected host.
func selectHost(nodeScoreList []framework.NodePluginScores, count int) (string, []framework.NodePluginScores, error) {
	if len(nodeScoreList) == 0 {
		return &amp;#34;&amp;#34;, nil, errEmptyPriorityList
	}

	var h nodeScoreHeap = nodeScoreList
	heap.Init(&amp;amp;h)
	cntOfMaxScore := 1
	selectedIndex := 0
	// The top of the heap is the NodeScoreResult with the highest score.
	sortedNodeScoreList := make([]framework.NodePluginScores, 0, count)
	sortedNodeScoreList = append(sortedNodeScoreList, heap.Pop(&amp;amp;h).(framework.NodePluginScores))

	// This for-loop will continue until all Nodes with the highest scores get checked for a reservoir sampling,
	// and sortedNodeScoreList gets (count - 1) elements.
	for ns := heap.Pop(&amp;amp;h).(framework.NodePluginScores); ; ns = heap.Pop(&amp;amp;h).(framework.NodePluginScores) {
		if ns.TotalScore != sortedNodeScoreList[0].TotalScore &amp;amp;&amp;amp; len(sortedNodeScoreList) == count {
			break
		}

		if ns.TotalScore == sortedNodeScoreList[0].TotalScore {
			cntOfMaxScore++
			if rand.Intn(cntOfMaxScore) == 0 {
				// Replace the candidate with probability of 1/cntOfMaxScore
				selectedIndex = cntOfMaxScore - 1
			}
		}

		sortedNodeScoreList = append(sortedNodeScoreList, ns)

		if h.Len() == 0 {
			break
		}
	}

	if selectedIndex != 0 {
		// replace the first one with selected one
		previous := sortedNodeScoreList[0]
		sortedNodeScoreList[0] = sortedNodeScoreList[selectedIndex]
		sortedNodeScoreList[selectedIndex] = previous
	}

	if len(sortedNodeScoreList) &amp;gt; count {
		sortedNodeScoreList = sortedNodeScoreList[:count]
	}

	return sortedNodeScoreList[0].Name, sortedNodeScoreList, nil
}

// nodeScoreHeap is a heap of framework.NodePluginScores.
type nodeScoreHeap []framework.NodePluginScores

// nodeScoreHeap implements heap.Interface.
var _ heap.Interface = &amp;amp;nodeScoreHeap{}

func (h nodeScoreHeap) Len() int { return len(h) }
func (h nodeScoreHeap) Less(i, j int) bool { return h[i].TotalScore &amp;gt; h[j].TotalScore }
func (h nodeScoreHeap) Swap(i, j int) { h[i], h[j] = h[j], h[i] }

func (h *nodeScoreHeap) Push(x interface{}) {
	*h = append(*h, x.(framework.NodePluginScores))
}

func (h *nodeScoreHeap) Pop() interface{} {
	old := *h
	n := len(old)
	x := old[n-1]
	*h = old[0 : n-1]
	return x
}

// assume signals to the cache that a pod is already in the cache, so that binding can be asynchronous.
// assume modifies `assumed`.
func (sched *Scheduler) assume(logger klog.Logger, assumed *v1.Pod, host string) error {
	// Optimistically assume that the binding will succeed and send it to apiserver
	// in the background.
	// If the binding fails, scheduler will release resources allocated to assumed pod
	// immediately.
	assumed.Spec.NodeName = host

	if err := sched.Cache.AssumePod(logger, assumed); err != nil {
		logger.Error(err, &amp;#34;Scheduler cache AssumePod failed&amp;#34;)
		return err
	}
	// if &amp;#34;assumed&amp;#34; is a nominated pod, we should remove it from internal cache
	if sched.SchedulingQueue != nil {
		sched.SchedulingQueue.DeleteNominatedPodIfExists(assumed)
	}

	return nil
}

// bind binds a pod to a given node defined in a binding object.
// The precedence for binding is: (1) extenders and (2) framework plugins.
// We expect this to run asynchronously, so we handle binding metrics internally.
func (sched *Scheduler) bind(ctx context.Context, fwk framework.Framework, assumed *v1.Pod, targetNode string, state *framework.CycleState) (status *framework.Status) {
	logger := klog.FromContext(ctx)
	defer func() {
		sched.finishBinding(logger, fwk, assumed, targetNode, status)
	}()

	bound, err := sched.extendersBinding(logger, assumed, targetNode)
	if bound {
		return framework.AsStatus(err)
	}
	return fwk.RunBindPlugins(ctx, state, assumed, targetNode)
}

// TODO(#87159): Move this to a Plugin.
func (sched *Scheduler) extendersBinding(logger klog.Logger, pod *v1.Pod, node string) (bool, error) {
	for _, extender := range sched.Extenders {
		if !extender.IsBinder() || !extender.IsInterested(pod) {
			continue
		}
		err := extender.Bind(&amp;amp;v1.Binding{
			ObjectMeta: metav1.ObjectMeta{Namespace: pod.Namespace, Name: pod.Name, UID: pod.UID},
			Target: v1.ObjectReference{Kind: &amp;#34;Node&amp;#34;, Name: node},
		})
		if err != nil &amp;amp;&amp;amp; extender.IsIgnorable() {
			logger.Info(&amp;#34;Skipping extender in bind as it returned error and has ignorable flag set&amp;#34;, &amp;#34;extender&amp;#34;, extender, &amp;#34;err&amp;#34;, err)
			continue
		}
		return true, err
	}
	return false, nil
}

func (sched *Scheduler) finishBinding(logger klog.Logger, fwk framework.Framework, assumed *v1.Pod, targetNode string, status *framework.Status) {
	if finErr := sched.Cache.FinishBinding(logger, assumed); finErr != nil {
		logger.Error(finErr, &amp;#34;Scheduler cache FinishBinding failed&amp;#34;)
	}
	if !status.IsSuccess() {
		logger.V(1).Info(&amp;#34;Failed to bind pod&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(assumed))
		return
	}

	fwk.EventRecorder().Eventf(assumed, nil, v1.EventTypeNormal, &amp;#34;Scheduled&amp;#34;, &amp;#34;Binding&amp;#34;, &amp;#34;Successfully assigned %v/%v to %v&amp;#34;, assumed.Namespace, assumed.Name, targetNode)
}

func getAttemptsLabel(p *framework.QueuedPodInfo) string {
	// We breakdown the pod scheduling duration by attempts capped to a limit
	// to avoid ending up with a high cardinality metric.
	if p.Attempts &amp;gt;= 15 {
		return &amp;#34;15+&amp;#34;
	}
	return strconv.Itoa(p.Attempts)
}

// handleSchedulingFailure records an event for the pod that indicates the
// pod has failed to schedule. Also, update the pod condition and nominated node name if set.
func (sched *Scheduler) handleSchedulingFailure(ctx context.Context, fwk framework.Framework, podInfo *framework.QueuedPodInfo, status *framework.Status, nominatingInfo *framework.NominatingInfo, start time.Time) {
	calledDone := false
	defer func() {
		if !calledDone {
			// Basically, AddUnschedulableIfNotPresent calls DonePod internally.
			// But, AddUnschedulableIfNotPresent isn&amp;#39;t called in some corner cases.
			// Here, we call DonePod explicitly to avoid leaking the pod.
			sched.SchedulingQueue.Done(podInfo.Pod.UID)
		}
	}()
	//该函数是Go语言编写的，用于处理调度失败的情况。
	//它会记录一个表示Pod调度失败的事件，并更新Pod的状态和提名节点名（如果已设置）。函数主要包含以下几点内容：
	//1. 定义了一个名为handleSchedulingFailure的函数，它接受多个参数，
	//包括上下文ctx、框架fwk、排队的Pod信息podInfo、状态status、提名信息nominatingInfo和开始时间start。
	//2. 在函数内部，定义了一个名为calledDone的布尔变量，用于标记是否已经调用了DonePod方法。
	//3. 使用defer语句定义了一个匿名函数，该函数会在handleSchedulingFailure函数退出时执行。
	//匿名函数中，通过判断calledDone是否已被标记为true来决定是否需要显式调用sched.SchedulingQueue.Done(podInfo.Pod.UID)方法，
	//以避免泄露Pod。
	//4. 函数的主体部分包括记录调度失败事件和更新Pod状态和提名节点名的逻辑，这部分内容在给定的代码片段中没有展示出来。

	logger := klog.FromContext(ctx)
	reason := v1.PodReasonSchedulerError
	if status.IsRejected() {
		reason = v1.PodReasonUnschedulable
	}

	switch reason {
	case v1.PodReasonUnschedulable:
		metrics.PodUnschedulable(fwk.ProfileName(), metrics.SinceInSeconds(start))
	case v1.PodReasonSchedulerError:
		metrics.PodScheduleError(fwk.ProfileName(), metrics.SinceInSeconds(start))
	}
	//该函数主要通过klog从上下文ctx中获取logger，根据status的状态来设置Pod失败的原因，然后根据不同的失败原因来记录相应的metrics。
	//具体步骤如下：
	//1. 从上下文ctx中获取logger。
	//2. 初始化reason为SchedulerError。
	//3. 如果status被拒绝，则将reason更新为Unschedulable。
	//4. 根据reason的不同，记录相应的metrics：
	//- 如果reason为Unschedulable，则调用PodUnschedulable方法，并传入fwk的ProfileName和start与当前时间的秒数差作为参数。
	//- 如果reason为SchedulerError，则调用PodScheduleError方法，并传入fwk的ProfileName和start与当前时间的秒数差作为参数。

	pod := podInfo.Pod
	err := status.AsError()
	errMsg := status.Message()

	if err == ErrNoNodesAvailable {
		logger.V(2).Info(&amp;#34;Unable to schedule pod; no nodes are registered to the cluster; waiting&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod))
	} else if fitError, ok := err.(*framework.FitError); ok { // Inject UnschedulablePlugins to PodInfo, which will be used later for moving Pods between queues efficiently.
		podInfo.UnschedulablePlugins = fitError.Diagnosis.UnschedulablePlugins
		podInfo.PendingPlugins = fitError.Diagnosis.PendingPlugins
		logger.V(2).Info(&amp;#34;Unable to schedule pod; no fit; waiting&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod), &amp;#34;err&amp;#34;, errMsg)
	} else {
		logger.Error(err, &amp;#34;Error scheduling pod; retrying&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod))
	}
	//这段Go代码主要处理了三种不同类型的错误，并根据错误类型记录日志或更新podInfo对象的属性。
	//首先，如果错误为ErrNoNodesAvailable，则会记录一条日志，指示无法调度pod，因为没有可用的节点，并等待。
	//其次，如果错误类型为*framework.FitError，则将该错误的诊断信息（UnschedulablePlugins和PendingPlugins）注入到podInfo对象中，
	//以供后续使用，然后记录一条日志，指示无法调度pod，因为没有合适的节点，并等待。
	//最后，如果错误类型不是上述两种类型，则记录一条错误日志，指示调度pod时出错，并重试。
	//总的来说，这段代码主要是根据不同的错误类型进行错误处理，并通过日志记录相关信息。

	// Check if the Pod exists in informer cache.
	podLister := fwk.SharedInformerFactory().Core().V1().Pods().Lister()
	cachedPod, e := podLister.Pods(pod.Namespace).Get(pod.Name)
	if e != nil {
		logger.Info(&amp;#34;Pod doesn&amp;#39;t exist in informer cache&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod), &amp;#34;err&amp;#34;, e)
		// We need to call DonePod here because we don&amp;#39;t call AddUnschedulableIfNotPresent in this case.
	} else {
		// In the case of extender, the pod may have been bound successfully, but timed out returning its response to the scheduler.
		// It could result in the live version to carry .spec.nodeName, and that&amp;#39;s inconsistent with the internal-queued version.
		if len(cachedPod.Spec.NodeName) != 0 {
			logger.Info(&amp;#34;Pod has been assigned to node. Abort adding it back to queue.&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod), &amp;#34;node&amp;#34;, cachedPod.Spec.NodeName)
			// We need to call DonePod here because we don&amp;#39;t call AddUnschedulableIfNotPresent in this case.
		} else {
			// As &amp;lt;cachedPod&amp;gt; is from SharedInformer, we need to do a DeepCopy() here.
			// ignore this err since apiserver doesn&amp;#39;t properly validate affinity terms
			// and we can&amp;#39;t fix the validation for backwards compatibility.
			podInfo.PodInfo, _ = framework.NewPodInfo(cachedPod.DeepCopy())
			if err := sched.SchedulingQueue.AddUnschedulableIfNotPresent(logger, podInfo, sched.SchedulingQueue.SchedulingCycle()); err != nil {
				logger.Error(err, &amp;#34;Error occurred&amp;#34;)
			}
			calledDone = true
		}
	}
	//该函数用于检查Pod是否存在于informer缓存中。
	//如果存在，则根据Pod的状态进行不同的处理。如果Pod已经被分配到节点上，则不再将其添加到队列中；
	//否则，将Pod信息添加到队列中，以便进行调度。
	//如果Pod不存在于缓存中，则需要调用DonePod函数。

	// Update the scheduling queue with the nominated pod information. Without
	// this, there would be a race condition between the next scheduling cycle
	// and the time the scheduler receives a Pod Update for the nominated pod.
	// Here we check for nil only for tests.
	if sched.SchedulingQueue != nil {
		logger := klog.FromContext(ctx)
		sched.SchedulingQueue.AddNominatedPod(logger, podInfo.PodInfo, nominatingInfo)
	}

	if err == nil {
		// Only tests can reach here.
		return
	}

	msg := truncateMessage(errMsg)
	fwk.EventRecorder().Eventf(pod, nil, v1.EventTypeWarning, &amp;#34;FailedScheduling&amp;#34;, &amp;#34;Scheduling&amp;#34;, msg)
	if err := updatePod(ctx, sched.client, pod, &amp;amp;v1.PodCondition{
		Type: v1.PodScheduled,
		Status: v1.ConditionFalse,
		Reason: reason,
		Message: errMsg,
	}, nominatingInfo); err != nil {
		klog.FromContext(ctx).Error(err, &amp;#34;Error updating pod&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod))
	}
}

//这段Go代码主要功能是更新调度队列中的提名Pod信息，避免调度周期和调度器接收提名Pod更新之间的竞态条件。
//具体来说：
//- 首先检查sched.SchedulingQueue是否为nil，仅在测试中会为nil。
//- 若不为nil，则使用logger记录日志，并调用AddNominatedPod方法将提名Pod信息加入调度队列。
//- 若没有错误发生，仅在测试中会执行，直接返回。
//- 截断错误消息，并使用EventRecorder记录事件，事件类型为FailedScheduling，事件原因为msg。
//- 调用updatePod方法更新Pod的条件，将PodScheduled状态设置为ConditionFalse，原因设置为reason，消息设置为errMsg，
//并将提名信息传递给updatePod方法。
//- 若更新Pod时发生错误，记录错误日志。

// truncateMessage truncates a message if it hits the NoteLengthLimit.
func truncateMessage(message string) string {
	max := validation.NoteLengthLimit
	if len(message) &amp;lt;= max {
		return message
	}
	suffix := &amp;#34; ...&amp;#34;
	return message[:max-len(suffix)] + suffix
}

//该函数用于截断字符串，如果给定的消息长度超过了validation.NoteLengthLimit规定的最大长度，则在末尾添加&amp;#34;...&amp;#34;并返回截断后的消息字符串。
//如果给定的消息长度小于等于最大长度，则直接返回原消息字符串。

//该函数用于截断字符串，如果给定的字符串长度超过了validation.NoteLengthLimit规定的最大长度，则在字符串末尾添加&amp;#34;...&amp;#34;并返回截断后的字符串；
//如果给定的字符串长度不超过最大长度，则直接返回原字符串。

func updatePod(ctx context.Context, client clientset.Interface, pod *v1.Pod, condition *v1.PodCondition, nominatingInfo *framework.NominatingInfo) error {
	logger := klog.FromContext(ctx)
	logger.V(3).Info(&amp;#34;Updating pod condition&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod), &amp;#34;conditionType&amp;#34;, condition.Type, &amp;#34;conditionStatus&amp;#34;, condition.Status, &amp;#34;conditionReason&amp;#34;, condition.Reason)
	podStatusCopy := pod.Status.DeepCopy()
	// NominatedNodeName is updated only if we are trying to set it, and the value is
	// different from the existing one.
	nnnNeedsUpdate := nominatingInfo.Mode() == framework.ModeOverride &amp;amp;&amp;amp; pod.Status.NominatedNodeName != nominatingInfo.NominatedNodeName
	if !podutil.UpdatePodCondition(podStatusCopy, condition) &amp;amp;&amp;amp; !nnnNeedsUpdate {
		return nil
	}
	if nnnNeedsUpdate {
		podStatusCopy.NominatedNodeName = nominatingInfo.NominatedNodeName
	}
	return util.PatchPodStatus(ctx, client, pod, podStatusCopy)
}

//该函数用于更新Pod的条件状态，如果满足一定条件，则更新Pod的NominatedNodeName字段，
//并通过PatchPodStatus函数将更新后的Pod状态应用到实际的Pod对象中。
//- 首先，从上下文中获取日志记录器，并输出相关日志信息。
//- 然后，创建一个Pod状态的深拷贝。
//- 接着，判断是否需要更新NominatedNodeName字段，
//只有当nominatingInfo的模式为ModeOverride且当前NominatedNodeName与nominatingInfo中的NominatedNodeName不相同时，才需要更新。
//- 如果不需要更新Pod的条件状态和NominatedNodeName字段，则直接返回。
//- 如果需要更新NominatedNodeName字段，则将其赋值给podStatusCopy中的NominatedNodeName字段。
//- 最后，调用PatchPodStatus函数将podStatusCopy中的更新应用到实际的Pod对象中，并返回操作结果。
//请注意，该函数中涉及到的一些参数和函数的具体实现和用途可能需要结合具体的上下文和代码来理解。
&lt;/code>&lt;/pre></description></item><item><title>2024-04-09 K8S控制器之 deployment_controller.go源码解读</title><link>https://qq547475331.github.io/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8B-deployment_controllergo%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-deploymentcontrollergo-yuan-ma-jie-du/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8B-deployment_controllergo%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-deploymentcontrollergo-yuan-ma-jie-du/</guid><description>&lt;pre tabindex="0">&lt;code>/*
Copyright 2015 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the &amp;#34;License&amp;#34;);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

 http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &amp;#34;AS IS&amp;#34; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

// Package deployment contains all the logic for handling Kubernetes Deployments.
// It implements a set of strategies (rolling, recreate) for deploying an application,
// the means to rollback to previous versions, proportional scaling for mitigating
// risk, cleanup policy, and other useful features of Deployments.
package deployment

import (
	&amp;#34;context&amp;#34;
	&amp;#34;fmt&amp;#34;
	&amp;#34;reflect&amp;#34;
	&amp;#34;time&amp;#34;

	apps &amp;#34;k8s.io/api/apps/v1&amp;#34;
	v1 &amp;#34;k8s.io/api/core/v1&amp;#34;
	&amp;#34;k8s.io/apimachinery/pkg/api/errors&amp;#34;
	metav1 &amp;#34;k8s.io/apimachinery/pkg/apis/meta/v1&amp;#34;
	&amp;#34;k8s.io/apimachinery/pkg/labels&amp;#34;
	&amp;#34;k8s.io/apimachinery/pkg/types&amp;#34;
	utilruntime &amp;#34;k8s.io/apimachinery/pkg/util/runtime&amp;#34;
	&amp;#34;k8s.io/apimachinery/pkg/util/wait&amp;#34;
	appsinformers &amp;#34;k8s.io/client-go/informers/apps/v1&amp;#34;
	coreinformers &amp;#34;k8s.io/client-go/informers/core/v1&amp;#34;
	clientset &amp;#34;k8s.io/client-go/kubernetes&amp;#34;
	&amp;#34;k8s.io/client-go/kubernetes/scheme&amp;#34;
	v1core &amp;#34;k8s.io/client-go/kubernetes/typed/core/v1&amp;#34;
	appslisters &amp;#34;k8s.io/client-go/listers/apps/v1&amp;#34;
	corelisters &amp;#34;k8s.io/client-go/listers/core/v1&amp;#34;
	&amp;#34;k8s.io/client-go/tools/cache&amp;#34;
	&amp;#34;k8s.io/client-go/tools/record&amp;#34;
	&amp;#34;k8s.io/client-go/util/workqueue&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/controller&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/controller/deployment/util&amp;#34;
)

const (
	// maxRetries is the number of times a deployment will be retried before it is dropped out of the queue.
	// With the current rate-limiter in use (5ms*2^(maxRetries-1)) the following numbers represent the times
	// a deployment is going to be requeued:
	//
	// 5ms, 10ms, 20ms, 40ms, 80ms, 160ms, 320ms, 640ms, 1.3s, 2.6s, 5.1s, 10.2s, 20.4s, 41s, 82s
	maxRetries = 15
)

// controllerKind contains the schema.GroupVersionKind for this controller type.
var controllerKind = apps.SchemeGroupVersion.WithKind(&amp;#34;Deployment&amp;#34;)

// DeploymentController is responsible for synchronizing Deployment objects stored
// in the system with actual running replica sets and pods.
type DeploymentController struct {
	// rsControl is used for adopting/releasing replica sets.
	rsControl controller.RSControlInterface
	client clientset.Interface

	eventBroadcaster record.EventBroadcaster
	eventRecorder record.EventRecorder

	// To allow injection of syncDeployment for testing.
	syncHandler func(ctx context.Context, dKey string) error
	// used for unit testing
	enqueueDeployment func(deployment *apps.Deployment)

	// dLister can list/get deployments from the shared informer&amp;#39;s store
	dLister appslisters.DeploymentLister
	// rsLister can list/get replica sets from the shared informer&amp;#39;s store
	rsLister appslisters.ReplicaSetLister
	// podLister can list/get pods from the shared informer&amp;#39;s store
	podLister corelisters.PodLister

	// dListerSynced returns true if the Deployment store has been synced at least once.
	// Added as a member to the struct to allow injection for testing.
	dListerSynced cache.InformerSynced
	// rsListerSynced returns true if the ReplicaSet store has been synced at least once.
	// Added as a member to the struct to allow injection for testing.
	rsListerSynced cache.InformerSynced
	// podListerSynced returns true if the pod store has been synced at least once.
	// Added as a member to the struct to allow injection for testing.
	podListerSynced cache.InformerSynced

	// Deployments that need to be synced
	queue workqueue.RateLimitingInterface
}

// NewDeploymentController creates a new DeploymentController.
func NewDeploymentController(ctx context.Context, dInformer appsinformers.DeploymentInformer, rsInformer appsinformers.ReplicaSetInformer, podInformer coreinformers.PodInformer, client clientset.Interface) (*DeploymentController, error) {
	eventBroadcaster := record.NewBroadcaster(record.WithContext(ctx))
	logger := klog.FromContext(ctx)
	dc := &amp;amp;DeploymentController{
		client: client,
		eventBroadcaster: eventBroadcaster,
		eventRecorder: eventBroadcaster.NewRecorder(scheme.Scheme, v1.EventSource{Component: &amp;#34;deployment-controller&amp;#34;}),
		queue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), &amp;#34;deployment&amp;#34;),
	}
	dc.rsControl = controller.RealRSControl{
		KubeClient: client,
		Recorder: dc.eventRecorder,
	}
	//该函数用于创建一个新的DeploymentController。
	//- 接收上下文ctx以及多个informer和client作为参数。
	//- 创建一个新的事件广播器eventBroadcaster，并从上下文ctx中获取日志记录器logger 。
	//- 初始化一个DeploymentController结构体实例dc，包括client、eventBroadcaster、eventRecorder、queue和rsControl字段。
	//- dc.rsControl使用controller.RealRSControl结构体初始化，包括KubeClient和Recorder字段。
	//- 返回创建的dc实例和可能出现的错误。

	dInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
		AddFunc: func(obj interface{}) {
			dc.addDeployment(logger, obj)
		},
		UpdateFunc: func(oldObj, newObj interface{}) {
			dc.updateDeployment(logger, oldObj, newObj)
		},
		// This will enter the sync loop and no-op, because the deployment has been deleted from the store.
		DeleteFunc: func(obj interface{}) {
			dc.deleteDeployment(logger, obj)
		},
	})
	rsInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
		AddFunc: func(obj interface{}) {
			dc.addReplicaSet(logger, obj)
		},
		UpdateFunc: func(oldObj, newObj interface{}) {
			dc.updateReplicaSet(logger, oldObj, newObj)
		},
		DeleteFunc: func(obj interface{}) {
			dc.deleteReplicaSet(logger, obj)
		},
	})
	podInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
		DeleteFunc: func(obj interface{}) {
			dc.deletePod(logger, obj)
		},
	})
	//这段代码定义了三个事件处理器，分别用于处理deployment、replicaSet和pod的添加、更新和删除事件。
	//每个事件处理器都调用了对应的方法，例如对于deployment的添加事件，调用了dc.addDeployment(logger, obj)方法。
	//这些方法的具体实现可以根据具体业务逻辑进行编写。

	dc.syncHandler = dc.syncDeployment
	dc.enqueueDeployment = dc.enqueue

	dc.dLister = dInformer.Lister()
	dc.rsLister = rsInformer.Lister()
	dc.podLister = podInformer.Lister()
	dc.dListerSynced = dInformer.Informer().HasSynced
	dc.rsListerSynced = rsInformer.Informer().HasSynced
	dc.podListerSynced = podInformer.Informer().HasSynced
	return dc, nil
}

//这段代码是Go语言中的函数，主要功能是设置和初始化一个名为dc的对象，并返回该对象和nil。
//- 首先，将dc.syncHandler设置为dc.syncDeployment，将dc.enqueueDeployment设置为dc.enqueue。
//- 然后，通过dInformer、rsInformer和podInformer的Lister()方法，分别将dc.dLister、dc.rsLister和dc.podLister设置为相应的列表器。
//- 接着，通过dInformer、rsInformer和podInformer的Informer().HasSynced方法，分别将dc.dListerSynced、dc.rsListerSynced和dc.podListerSynced设置为相应的同步状态检查函数。
//- 最后，返回设置好的dc对象和nil。 这段代码主要涉及到对象的设置和初始化操作，使用了多个列表器和同步状态检查函数来管理不同资源的信息。

// Run begins watching and syncing.
func (dc *DeploymentController) Run(ctx context.Context, workers int) {
	defer utilruntime.HandleCrash()

	// Start events processing pipeline.
	dc.eventBroadcaster.StartStructuredLogging(3)
	dc.eventBroadcaster.StartRecordingToSink(&amp;amp;v1core.EventSinkImpl{Interface: dc.client.CoreV1().Events(&amp;#34;&amp;#34;)})
	defer dc.eventBroadcaster.Shutdown()

	defer dc.queue.ShutDown()

	logger := klog.FromContext(ctx)
	logger.Info(&amp;#34;Starting controller&amp;#34;, &amp;#34;controller&amp;#34;, &amp;#34;deployment&amp;#34;)
	defer logger.Info(&amp;#34;Shutting down controller&amp;#34;, &amp;#34;controller&amp;#34;, &amp;#34;deployment&amp;#34;)

	if !cache.WaitForNamedCacheSync(&amp;#34;deployment&amp;#34;, ctx.Done(), dc.dListerSynced, dc.rsListerSynced, dc.podListerSynced) {
		return
	}

	for i := 0; i &amp;lt; workers; i++ {
		go wait.UntilWithContext(ctx, dc.worker, time.Second)
	}

	&amp;lt;-ctx.Done()
}

//该函数是DeploymentController类型的Run方法，用于启动部署控制器的监视和同步操作。
//- 首先，函数通过defer语句句柄处理崩溃情况。
//- 然后，启动事件处理管道，设置日志记录级别和目标。
//- 接着，通过defer语句关闭事件处理管道。
//- 然后，记录日志信息，表示控制器开始启动。
//- 接着，使用cache.WaitForNamedCacheSync函数等待缓存同步完成。
//- 然后，通过循环创建多个goroutine，并调用dc.worker函数进行工作。
//- 最后，等待上下文完成，并返回。
//该函数的主要功能是启动部署控制器，并使其开始监视和同步操作。

func (dc *DeploymentController) addDeployment(logger klog.Logger, obj interface{}) {
	d := obj.(*apps.Deployment)
	logger.V(4).Info(&amp;#34;Adding deployment&amp;#34;, &amp;#34;deployment&amp;#34;, klog.KObj(d))
	dc.enqueueDeployment(d)
}

func (dc *DeploymentController) updateDeployment(logger klog.Logger, old, cur interface{}) {
	oldD := old.(*apps.Deployment)
	curD := cur.(*apps.Deployment)
	logger.V(4).Info(&amp;#34;Updating deployment&amp;#34;, &amp;#34;deployment&amp;#34;, klog.KObj(oldD))
	dc.enqueueDeployment(curD)
}

//这两个函数都是DeploymentController的方法，用于处理Deployment的添加和更新事件。
//- addDeployment方法接收一个logger和一个obj参数，其中obj是通过类型断言转换为*apps.Deployment类型的。
//该方法首先使用logger记录添加deployment的日志信息，然后调用dc.enqueueDeployment方法将该deployment加入队列中，以便后续处理。
//- updateDeployment方法与addDeployment方法类似，但它接收的是旧的和新的Deployment对象。该方法使用logger记录更新deployment的日志信息，
//并将新的Deployment对象加入队列中进行处理。

func (dc *DeploymentController) deleteDeployment(logger klog.Logger, obj interface{}) {
	d, ok := obj.(*apps.Deployment)
	if !ok {
		tombstone, ok := obj.(cache.DeletedFinalStateUnknown)
		if !ok {
			utilruntime.HandleError(fmt.Errorf(&amp;#34;couldn&amp;#39;t get object from tombstone %#v&amp;#34;, obj))
			return
		}
		d, ok = tombstone.Obj.(*apps.Deployment)
		if !ok {
			utilruntime.HandleError(fmt.Errorf(&amp;#34;tombstone contained object that is not a Deployment %#v&amp;#34;, obj))
			return
		}
	}
	logger.V(4).Info(&amp;#34;Deleting deployment&amp;#34;, &amp;#34;deployment&amp;#34;, klog.KObj(d))
	dc.enqueueDeployment(d)
}

//该函数是一个Go语言函数，它定义了一个名为deleteDeployment的方法，该方法接受一个logger和一个obj参数，并没有任何返回值。
//该方法主要用于从一个部署（Deployment）中删除对象。
//首先，函数会尝试将obj参数断言为apps.Deployment类型，并检查断言是否成功。如果断言失败，
//则会尝试将obj参数断言为cache.DeletedFinalStateUnknown类型。如果这个断言也失败了，函数会记录一个错误信息并返回。
//如果断言成功，则会尝试从tombstone中获取对象，并再次断言该对象是否为apps.Deployment类型。如果断言失败，则会记录一个错误信息并返回。
//如果成功断言出对象为apps.Deployment类型，则会使用logger记录一条信息，表示正在删除该部署，
//并调用dc.enqueueDeployment方法将该部署加入队列，以便进一步处理。

// addReplicaSet enqueues the deployment that manages a ReplicaSet when the ReplicaSet is created.
func (dc *DeploymentController) addReplicaSet(logger klog.Logger, obj interface{}) {
	rs := obj.(*apps.ReplicaSet)

	if rs.DeletionTimestamp != nil {
		// On a restart of the controller manager, it&amp;#39;s possible for an object to
		// show up in a state that is already pending deletion.
		dc.deleteReplicaSet(logger, rs)
		return
	}
	// If it has a ControllerRef, that&amp;#39;s all that matters.
	if controllerRef := metav1.GetControllerOf(rs); controllerRef != nil {
		d := dc.resolveControllerRef(rs.Namespace, controllerRef)
		if d == nil {
			return
		}
		logger.V(4).Info(&amp;#34;ReplicaSet added&amp;#34;, &amp;#34;replicaSet&amp;#34;, klog.KObj(rs))
		dc.enqueueDeployment(d)
		return
	}
	//该函数是Go语言编写的，用于在创建ReplicaSet时，将管理该ReplicaSet的Deployment加入队列中。
	//函数首先判断传入的obj对象是否为ReplicaSet类型，并通过判断ReplicaSet的DeletionTimestamp是否为空来确定是否需要删除该ReplicaSet。
	//如果ReplicaSet有ControllerRef，则通过resolveControllerRef函数解析出对应的Deployment，并将其加入队列中。

	// Otherwise, it&amp;#39;s an orphan. Get a list of all matching Deployments and sync
	// them to see if anyone wants to adopt it.
	ds := dc.getDeploymentsForReplicaSet(logger, rs)
	if len(ds) == 0 {
		return
	}
	logger.V(4).Info(&amp;#34;Orphan ReplicaSet added&amp;#34;, &amp;#34;replicaSet&amp;#34;, klog.KObj(rs))
	for _, d := range ds {
		dc.enqueueDeployment(d)
	}
}

//该函数主要处理孤儿ReplicaSet的情况。
//它会获取与该ReplicaSet匹配的所有Deployment列表，并尝试同步这些Deployment，看是否有Deployment愿意采用该ReplicaSet。
//如果找到了对应的Deployment，则将其加入到队列中以便进一步处理。

// getDeploymentsForReplicaSet returns a list of Deployments that potentially
// match a ReplicaSet.
func (dc *DeploymentController) getDeploymentsForReplicaSet(logger klog.Logger, rs *apps.ReplicaSet) []*apps.Deployment {
	deployments, err := util.GetDeploymentsForReplicaSet(dc.dLister, rs)
	if err != nil || len(deployments) == 0 {
		return nil
	}
	// Because all ReplicaSet&amp;#39;s belonging to a deployment should have a unique label key,
	// there should never be more than one deployment returned by the above method.
	// If that happens we should probably dynamically repair the situation by ultimately
	// trying to clean up one of the controllers, for now we just return the older one
	if len(deployments) &amp;gt; 1 {
		// ControllerRef will ensure we don&amp;#39;t do anything crazy, but more than one
		// item in this list nevertheless constitutes user error.
		logger.V(4).Info(&amp;#34;user error! more than one deployment is selecting replica set&amp;#34;,
			&amp;#34;replicaSet&amp;#34;, klog.KObj(rs), &amp;#34;labels&amp;#34;, rs.Labels, &amp;#34;deployment&amp;#34;, klog.KObj(deployments[0]))
	}
	return deployments
}

//函数用于获取与给定ReplicaSet匹配的所有Deployment列表。
//它首先调用util.GetDeploymentsForReplicaSet函数来获取匹配的Deployment列表，如果出现错误或列表为空，则返回nil。
//如果获取的Deployment列表长度大于1，则记录错误日志，并返回列表中的第一个Deployment。
//在正常情况下，返回获取的Deployment列表。

// updateReplicaSet figures out what deployment(s) manage a ReplicaSet when the ReplicaSet
// is updated and wake them up. If the anything of the ReplicaSets have changed, we need to
// awaken both the old and new deployments. old and cur must be *apps.ReplicaSet
// types.
func (dc *DeploymentController) updateReplicaSet(logger klog.Logger, old, cur interface{}) {
	curRS := cur.(*apps.ReplicaSet)
	oldRS := old.(*apps.ReplicaSet)
	if curRS.ResourceVersion == oldRS.ResourceVersion {
		// Periodic resync will send update events for all known replica sets.
		// Two different versions of the same replica set will always have different RVs.
		return
	}
	//该函数用于在更新ReplicaSet时，确定由哪个部署管理该ReplicaSet，并唤醒它们。
	//如果ReplicaSet的任何内容发生变化，需要唤醒旧的和新的部署。old和cur必须是指向apps.ReplicaSet类型的指针。
	//函数首先将传入的old和cur参数转换为*apps.ReplicaSet类型。
	//然后，它比较两个ReplicaSet的ResourceVersion字段。
	//如果它们相等，则表示这是周期性同步发送的更新事件，而对于同一ReplicaSet的两个不同版本，它们的ResourceVersion总会有不同的值。
	//在这种情况下，函数直接返回，不做任何处理。

	curControllerRef := metav1.GetControllerOf(curRS)
	oldControllerRef := metav1.GetControllerOf(oldRS)
	controllerRefChanged := !reflect.DeepEqual(curControllerRef, oldControllerRef)
	if controllerRefChanged &amp;amp;&amp;amp; oldControllerRef != nil {
		// The ControllerRef was changed. Sync the old controller, if any.
		if d := dc.resolveControllerRef(oldRS.Namespace, oldControllerRef); d != nil {
			dc.enqueueDeployment(d)
		}
	}
	//这段Go代码主要关注于检查和处理两个ReplicaSet（curRS和oldRS）的ControllerRef是否发生变化，并根据变化情况同步旧的控制器。
	//1. 首先，通过metav1.GetControllerOf函数获取curRS和oldRS的ControllerRef。
	//2. 然后，使用reflect.DeepEqual函数比较curControllerRef和oldControllerRef是否相等。
	//3. 如果两个ControllerRef不相等且oldControllerRef不为nil，则认为ControllerRef发生了变化。
	//4. 当ControllerRef发生变化时，需要同步旧的控制器。通过dc.resolveControllerRef函数解析oldRS的命名空间和ControllerRef，
	//得到对应的Deployment。
	//5. 最后，如果解析成功（d!=nil），则将该Deployment加入到调度队列中，以便进一步处理。

	// If it has a ControllerRef, that&amp;#39;s all that matters.
	if curControllerRef != nil {
		d := dc.resolveControllerRef(curRS.Namespace, curControllerRef)
		if d == nil {
			return
		}
		logger.V(4).Info(&amp;#34;ReplicaSet updated&amp;#34;, &amp;#34;replicaSet&amp;#34;, klog.KObj(curRS))
		dc.enqueueDeployment(d)
		return
	}
	//这个Go函数主要检查当前的curControllerRef是否为nil。
	//如果不为nil，则通过dc.resolveControllerRef方法解析curControllerRef，并检查解析结果是否为nil。
	//如果不为nil，则记录日志并使用dc.enqueueDeployment方法将解析结果加入到队列中。

	// Otherwise, it&amp;#39;s an orphan. If anything changed, sync matching controllers
	// to see if anyone wants to adopt it now.
	labelChanged := !reflect.DeepEqual(curRS.Labels, oldRS.Labels)
	if labelChanged || controllerRefChanged {
		ds := dc.getDeploymentsForReplicaSet(logger, curRS)
		if len(ds) == 0 {
			return
		}
		logger.V(4).Info(&amp;#34;Orphan ReplicaSet updated&amp;#34;, &amp;#34;replicaSet&amp;#34;, klog.KObj(curRS))
		for _, d := range ds {
			dc.enqueueDeployment(d)
		}
	}
}

//这段Go代码是处理孤儿ReplicaSet的逻辑。
//如果ReplicaSet的标签或控制器引用发生了变化，它会尝试同步匹配的控制器，以查看是否有控制器愿意现在采用它。具体步骤如下：
//1. 检查当前ReplicaSet的标签和旧ReplicaSet的标签是否相等，如果不相等，则标记标签发生变化；
//2. 检查控制器引用是否发生变化；
//3. 如果标签发生变化或控制器引用发生变化，则获取当前ReplicaSet对应的部署列表；
//4. 如果部署列表为空，则直接返回；
//5. 输出日志信息，表示孤儿ReplicaSet已更新；
//6. 遍历部署列表，将每个部署对象加入到队列中，以便进一步处理。

// deleteReplicaSet enqueues the deployment that manages a ReplicaSet when
// the ReplicaSet is deleted. obj could be an *apps.ReplicaSet, or
// a DeletionFinalStateUnknown marker item.
func (dc *DeploymentController) deleteReplicaSet(logger klog.Logger, obj interface{}) {
	rs, ok := obj.(*apps.ReplicaSet)
	//该函数是Go语言编写的，属于DeploymentController类型的一个方法，方法名为deleteReplicaSet。
	//该方法接收一个logger和一个obj参数，其中logger用于记录日志，obj是一个接口类型，
	//可以是*apps.ReplicaSet类型或者DeletionFinalStateUnknown标记项。
	//方法的主要功能是从obj中解析出*apps.ReplicaSet类型的rs，并判断解析是否成功。

	// When a delete is dropped, the relist will notice a pod in the store not
	// in the list, leading to the insertion of a tombstone object which contains
	// the deleted key/value. Note that this value might be stale. If the ReplicaSet
	// changed labels the new deployment will not be woken up till the periodic resync.
	if !ok {
		tombstone, ok := obj.(cache.DeletedFinalStateUnknown)
		if !ok {
			utilruntime.HandleError(fmt.Errorf(&amp;#34;couldn&amp;#39;t get object from tombstone %#v&amp;#34;, obj))
			return
		}
		rs, ok = tombstone.Obj.(*apps.ReplicaSet)
		if !ok {
			utilruntime.HandleError(fmt.Errorf(&amp;#34;tombstone contained object that is not a ReplicaSet %#v&amp;#34;, obj))
			return
		}
	}
	//这段Go代码是处理从存储中获取对象时，如果对象已被删除的情况。
	//如果获取对象失败，会检查对象是否是一个 tombstone 对象（即已被删除的对象）。
	//如果是 tombstone 对象，则尝试从 tombstone 中恢复被删除的对象。如果恢复失败，则记录错误信息。

	controllerRef := metav1.GetControllerOf(rs)
	if controllerRef == nil {
		// No controller should care about orphans being deleted.
		return
	}
	d := dc.resolveControllerRef(rs.Namespace, controllerRef)
	if d == nil {
		return
	}
	logger.V(4).Info(&amp;#34;ReplicaSet deleted&amp;#34;, &amp;#34;replicaSet&amp;#34;, klog.KObj(rs))
	dc.enqueueDeployment(d)
}

//该函数主要实现当一个ReplicaSet被删除时，检查是否有对应的Deployment控制器，并将该Deployment加入到队列中以便进一步处理。具体流程如下：
//1. 通过metav1.GetControllerOf(rs)获取ReplicaSet的控制器引用；
//2. 如果控制器引用为空，则表示没有对应的Deployment控制器，直接返回；
//3. 调用dc.resolveControllerRef(rs.Namespace, controllerRef)解析控制器引用，获取对应的Deployment对象；
//4. 如果解析失败或返回的Deployment对象为空，则直接返回；
//5. 使用logger.V(4).Info记录日志，表示ReplicaSet已被删除；
//6. 调用dc.enqueueDeployment(d)将对应的Deployment对象加入到队列中，以便进一步处理。

// deletePod will enqueue a Recreate Deployment once all of its pods have stopped running.
func (dc *DeploymentController) deletePod(logger klog.Logger, obj interface{}) {
	pod, ok := obj.(*v1.Pod)
	//该函数是一个Go语言函数，名为deletePod，它属于DeploymentController类型。
	//函数通过传入的logger和obj参数，将一个Recreate Deployment加入队列，但只有当该Deployment的所有Pod都停止运行时才会执行。
	//函数首先尝试将obj参数断言为*v1.Pod类型，并检查断言是否成功。

	// When a delete is dropped, the relist will notice a pod in the store not
	// in the list, leading to the insertion of a tombstone object which contains
	// the deleted key/value. Note that this value might be stale. If the Pod
	// changed labels the new deployment will not be woken up till the periodic resync.
	if !ok {
		tombstone, ok := obj.(cache.DeletedFinalStateUnknown)
		if !ok {
			utilruntime.HandleError(fmt.Errorf(&amp;#34;couldn&amp;#39;t get object from tombstone %#v&amp;#34;, obj))
			return
		}
		pod, ok = tombstone.Obj.(*v1.Pod)
		if !ok {
			utilruntime.HandleError(fmt.Errorf(&amp;#34;tombstone contained object that is not a pod %#v&amp;#34;, obj))
			return
		}
	}
	d := dc.getDeploymentForPod(logger, pod)
	if d == nil {
		return
	}
	logger.V(4).Info(&amp;#34;Pod deleted&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod))
	if d.Spec.Strategy.Type == apps.RecreateDeploymentStrategyType {
		// Sync if this Deployment now has no more Pods.
		rsList, err := util.ListReplicaSets(d, util.RsListFromClient(dc.client.AppsV1()))
		if err != nil {
			return
		}
		podMap, err := dc.getPodMapForDeployment(d, rsList)
		if err != nil {
			return
		}
		numPods := 0
		for _, podList := range podMap {
			numPods += len(podList)
		}
		if numPods == 0 {
			dc.enqueueDeployment(d)
		}
	}
}

//这段Go代码是用于处理Kubernetes中Pod删除事件的逻辑。
//- 当一个Pod被删除时，如果控制器（例如Deployment）没有及时收到该事件，就会在存储中注意到这个Pod不在列表中，
//从而插入一个包含删除的键值对的墓碑对象（tombstone object）。
//- 如果墓碑对象中的对象不是Pod类型，函数会记录错误并返回。
//- 函数会尝试获取与该Pod关联的Deployment对象，如果获取不到则直接返回。
//- 如果该Deployment的策略是Recreate类型，则会检查该Deployment下是否已经没有Pod了。
//- 如果没有Pod了，则将该Deployment加入到队列中，以便进一步处理。
//这段代码的主要目的是确保在Pod被删除时，与之关联的Deployment能够及时地进行同步和更新。

func (dc *DeploymentController) enqueue(deployment *apps.Deployment) {
	key, err := controller.KeyFunc(deployment)
	if err != nil {
		utilruntime.HandleError(fmt.Errorf(&amp;#34;couldn&amp;#39;t get key for object %#v: %v&amp;#34;, deployment, err))
		return
	}

	dc.queue.Add(key)
}

//该函数是一个Go语言的方法，定义在一个名为DeploymentController的结构体类型上。
//方法名为enqueue，它接收一个参数deployment，类型为*apps.Deployment，表示一个Kubernetes部署对象的指针。
//该方法的主要功能是将给定的部署对象加入到一个队列中，以便后续处理。具体步骤如下：
//1. 调用controller.KeyFunc(deployment)函数，尝试获取部署对象的键值（通常是一个字符串），用于在队列中唯一标识该对象。
//2. 如果获取键值时出现错误，通过utilruntime.HandleError函数处理错误，并打印错误信息。然后直接返回，不将对象加入队列。
//3. 如果成功获取了键值，将其添加到dc.queue（一个队列对象）中，以便后续处理。
//总结：该方法用于将一个Kubernetes部署对象加入到队列中，以便后续进行处理。

func (dc *DeploymentController) enqueueRateLimited(deployment *apps.Deployment) {
	key, err := controller.KeyFunc(deployment)
	if err != nil {
		utilruntime.HandleError(fmt.Errorf(&amp;#34;couldn&amp;#39;t get key for object %#v: %v&amp;#34;, deployment, err))
		return
	}

	dc.queue.AddRateLimited(key)
}

//该函数用于将指定的部署对象加入到队列中，并对其进行速率限制。
//首先，通过调用controller.KeyFunc方法获取该部署对象的键值，如果获取失败，则通过utilruntime.HandleError方法记录错误信息并返回。
//如果获取成功，则调用dc.queue.AddRateLimited方法将键值加入到队列中，并对其进行速率限制。

// enqueueAfter will enqueue a deployment after the provided amount of time.
func (dc *DeploymentController) enqueueAfter(deployment *apps.Deployment, after time.Duration) {
	key, err := controller.KeyFunc(deployment)
	if err != nil {
		utilruntime.HandleError(fmt.Errorf(&amp;#34;couldn&amp;#39;t get key for object %#v: %v&amp;#34;, deployment, err))
		return
	}

	dc.queue.AddAfter(key, after)
}

//该函数将一个部署对象加入到队列中，但会在指定的时间延迟后才执行。
//首先，函数通过调用controller.KeyFunc方法获取部署对象的键值，如果获取失败，则通过utilruntime.HandleError方法记录错误信息并返回。
//接着，函数使用dc.queue.AddAfter方法将键值加入到队列中，并指定延迟执行的时间。

// getDeploymentForPod returns the deployment managing the given Pod.
func (dc *DeploymentController) getDeploymentForPod(logger klog.Logger, pod *v1.Pod) *apps.Deployment {
	// Find the owning replica set
	var rs *apps.ReplicaSet
	var err error
	controllerRef := metav1.GetControllerOf(pod)
	if controllerRef == nil {
		// No controller owns this Pod.
		return nil
	}
	if controllerRef.Kind != apps.SchemeGroupVersion.WithKind(&amp;#34;ReplicaSet&amp;#34;).Kind {
		// Not a pod owned by a replica set.
		return nil
	}
	rs, err = dc.rsLister.ReplicaSets(pod.Namespace).Get(controllerRef.Name)
	if err != nil || rs.UID != controllerRef.UID {
		logger.V(4).Info(&amp;#34;Cannot get replicaset for pod&amp;#34;, &amp;#34;ownerReference&amp;#34;, controllerRef.Name, &amp;#34;pod&amp;#34;, klog.KObj(pod), &amp;#34;err&amp;#34;, err)
		return nil
	}
	//该函数用于获取管理给定Pod的Deployment。
	//- 首先，它查找拥有该Pod的ReplicaSet。
	//- 如果找不到拥有者的Pod，则返回nil。 -
	//如果Pod的拥有者不是ReplicaSet，则返回nil。
	//- 然后，尝试根据Pod的拥有者名称获取ReplicaSet。
	//- 如果获取失败或获取到的ReplicaSet的UID与拥有者的UID不匹配，则返回nil。
	//- 最后，返回获取到的Deployment。

	// Now find the Deployment that owns that ReplicaSet.
	controllerRef = metav1.GetControllerOf(rs)
	if controllerRef == nil {
		return nil
	}
	return dc.resolveControllerRef(rs.Namespace, controllerRef)
}

//这个函数的作用是通过 ReplicaSet 的 controllerRef 找到对应的 Deployment。
//首先通过 metav1.GetControllerOf(rs) 获取到 controllerRef，
//如果 controllerRef 为空则返回 nil，否则调用 dc.resolveControllerRef(rs.Namespace, controllerRef) 解析并返回对应的 Deployment。

// resolveControllerRef returns the controller referenced by a ControllerRef,
// or nil if the ControllerRef could not be resolved to a matching controller
// of the correct Kind.
func (dc *DeploymentController) resolveControllerRef(namespace string, controllerRef *metav1.OwnerReference) *apps.Deployment {
	// We can&amp;#39;t look up by UID, so look up by Name and then verify UID.
	// Don&amp;#39;t even try to look up by Name if it&amp;#39;s the wrong Kind.
	if controllerRef.Kind != controllerKind.Kind {
		return nil
	}
	d, err := dc.dLister.Deployments(namespace).Get(controllerRef.Name)
	if err != nil {
		return nil
	}
	if d.UID != controllerRef.UID {
		// The controller we found with this Name is not the same one that the
		// ControllerRef points to.
		return nil
	}
	return d
}

//该函数是一个Go语言函数，用于解析一个ControllerRef引用所指向的控制器，如果无法解析出正确的控制器，则返回nil。
//函数接受三个参数： - namespace：字符串类型，表示命名空间。
//- controllerRef：指向metav1.OwnerReference类型的指针。
//函数返回一个指向apps.Deployment类型的指针。
//函数的主要步骤如下：
//1. 首先，检查controllerRef的Kind属性是否与controllerKind.Kind相等，如果不相等，则直接返回nil。
//2. 如果controllerRef的Kind属性与controllerKind.Kind相等，则通过dc.dLister.Deployments(namespace).Get(controllerRef.Name)获取具有相同名称的部署对象。
//3. 如果获取部署对象时出现错误，则返回nil。
//4. 最后，检查获取到的部署对象的UID属性是否与controllerRef的UID属性相等，如果不相等，则返回nil，否则返回该部署对象。

// worker runs a worker thread that just dequeues items, processes them, and marks them done.
// It enforces that the syncHandler is never invoked concurrently with the same key.
func (dc *DeploymentController) worker(ctx context.Context) {
	for dc.processNextWorkItem(ctx) {
	}
}

func (dc *DeploymentController) processNextWorkItem(ctx context.Context) bool {
	key, quit := dc.queue.Get()
	if quit {
		return false
	}
	defer dc.queue.Done(key)

	err := dc.syncHandler(ctx, key.(string))
	dc.handleErr(ctx, err, key)

	return true
}

//这段Go代码定义了两个函数：worker 和 processNextWorkItem，它们用于在DeploymentController中处理工作队列中的项。
//1. worker函数是一个无限循环，它不断地调用processNextWorkItem函数来处理队列中的下一个工作项，直到没有更多工作项需要处理为止。
//该函数接受一个context.Context参数，用于控制函数的取消或超时。
//2. processNextWorkItem函数从队列中获取下一个工作项的键，并调用syncHandler函数来处理该工作项。
//如果处理成功，它会标记该工作项为已完成。
//该函数也接受一个context.Context参数，并在处理完成后对其进行取消操作。
//该函数返回一个布尔值，表示是否成功处理了工作项。
//这段代码的主要目的是在DeploymentController中使用工作队列并发地处理工作项，并确保相同的键不会被并发处理。

func (dc *DeploymentController) handleErr(ctx context.Context, err error, key interface{}) {
	logger := klog.FromContext(ctx)
	if err == nil || errors.HasStatusCause(err, v1.NamespaceTerminatingCause) {
		dc.queue.Forget(key)
		return
	}
	ns, name, keyErr := cache.SplitMetaNamespaceKey(key.(string))
	if keyErr != nil {
		logger.Error(err, &amp;#34;Failed to split meta namespace cache key&amp;#34;, &amp;#34;cacheKey&amp;#34;, key)
	}

	if dc.queue.NumRequeues(key) &amp;lt; maxRetries {
		logger.V(2).Info(&amp;#34;Error syncing deployment&amp;#34;, &amp;#34;deployment&amp;#34;, klog.KRef(ns, name), &amp;#34;err&amp;#34;, err)
		dc.queue.AddRateLimited(key)
		return
	}

	utilruntime.HandleError(err)
	logger.V(2).Info(&amp;#34;Dropping deployment out of the queue&amp;#34;, &amp;#34;deployment&amp;#34;, klog.KRef(ns, name), &amp;#34;err&amp;#34;, err)
	dc.queue.Forget(key)
}

//该函数是Go语言中的一个处理错误的函数，它属于DeploymentController类型。
//函数通过传入的上下文、错误和键来执行错误处理逻辑。
//首先，函数从上下文中获取日志记录器。如果错误为nil或错误的原因是命名空间终止，则将键从队列中忘记并返回。
//接下来，函数尝试将键拆分为命名空间和名称，并检查拆分是否成功。如果拆分失败，则记录错误信息。
//如果队列中键的重试次数小于最大重试次数，则记录错误信息，并将键添加到速率限制队列中。
//如果以上条件都不满足，则处理错误，并记录信息，将键从队列中忘记。

// getReplicaSetsForDeployment uses ControllerRefManager to reconcile
// ControllerRef by adopting and orphaning.
// It returns the list of ReplicaSets that this Deployment should manage.
func (dc *DeploymentController) getReplicaSetsForDeployment(ctx context.Context, d *apps.Deployment) ([]*apps.ReplicaSet, error) {
	// List all ReplicaSets to find those we own but that no longer match our
	// selector. They will be orphaned by ClaimReplicaSets().
	rsList, err := dc.rsLister.ReplicaSets(d.Namespace).List(labels.Everything())
	if err != nil {
		return nil, err
	}
	deploymentSelector, err := metav1.LabelSelectorAsSelector(d.Spec.Selector)
	if err != nil {
		return nil, fmt.Errorf(&amp;#34;deployment %s/%s has invalid label selector: %v&amp;#34;, d.Namespace, d.Name, err)
	}
	// If any adoptions are attempted, we should first recheck for deletion with
	// an uncached quorum read sometime after listing ReplicaSets (see #42639).
	canAdoptFunc := controller.RecheckDeletionTimestamp(func(ctx context.Context) (metav1.Object, error) {
		fresh, err := dc.client.AppsV1().Deployments(d.Namespace).Get(ctx, d.Name, metav1.GetOptions{})
		if err != nil {
			return nil, err
		}
		if fresh.UID != d.UID {
			return nil, fmt.Errorf(&amp;#34;original Deployment %v/%v is gone: got uid %v, wanted %v&amp;#34;, d.Namespace, d.Name, fresh.UID, d.UID)
		}
		return fresh, nil
	})
	cm := controller.NewReplicaSetControllerRefManager(dc.rsControl, d, deploymentSelector, controllerKind, canAdoptFunc)
	return cm.ClaimReplicaSets(ctx, rsList)
}

//该函数用于通过ControllerRefManager来协调ControllerRef，实现采用和孤儿化操作。
//它返回一个列表，其中包含该Deployment应管理的ReplicaSets。 函数首先列出所有ReplicaSets，以找到我们拥有但不再匹配我们选择器的那些。
//它们将通过ClaimReplicaSets()函数被孤儿化。
//然后，函数将根据Deployment的规范选择器生成一个标签选择器。如果任何收养尝试都进行了，
//函数将首先在列出ReplicaSets后重新检查删除时间戳（请参阅#42639）。
//最后，函数创建一个ReplicaSetControllerRefManager，并使用ClaimReplicaSets函数来声明应管理的ReplicaSets列表

// getPodMapForDeployment returns the Pods managed by a Deployment.
//
// It returns a map from ReplicaSet UID to a list of Pods controlled by that RS,
// according to the Pod&amp;#39;s ControllerRef.
// NOTE: The pod pointers returned by this method point the pod objects in the cache and thus
// shouldn&amp;#39;t be modified in any way.
func (dc *DeploymentController) getPodMapForDeployment(d *apps.Deployment, rsList []*apps.ReplicaSet) (map[types.UID][]*v1.Pod, error) {
	// Get all Pods that potentially belong to this Deployment.
	selector, err := metav1.LabelSelectorAsSelector(d.Spec.Selector)
	if err != nil {
		return nil, err
	}
	pods, err := dc.podLister.Pods(d.Namespace).List(selector)
	if err != nil {
		return nil, err
	}
	// Group Pods by their controller (if it&amp;#39;s in rsList).
	podMap := make(map[types.UID][]*v1.Pod, len(rsList))
	for _, rs := range rsList {
		podMap[rs.UID] = []*v1.Pod{}
	}
	for _, pod := range pods {
		// Do not ignore inactive Pods because Recreate Deployments need to verify that no
		// Pods from older versions are running before spinning up new Pods.
		controllerRef := metav1.GetControllerOf(pod)
		if controllerRef == nil {
			continue
		}
		// Only append if we care about this UID.
		if _, ok := podMap[controllerRef.UID]; ok {
			podMap[controllerRef.UID] = append(podMap[controllerRef.UID], pod)
		}
	}
	return podMap, nil
}

//该函数用于返回由Deployment管理的Pods的映射。
//它根据Pod的ControllerRef将Pod分组为其控制器（如果在rsList中）。
//函数首先根据Deployment的规范选择器获取所有可能属于该Deployment的Pods 。
//然后，它遍历这些Pods，并通过其ControllerRef将它们分组到podMap中。
//函数返回一个映射，其中键是ReplicaSet的UID，值是由该RS控制的Pod列表。
//注意，该函数返回的Pod指针指向缓存中的Pod对象，因此不应以任何方式修改它们。

// syncDeployment will sync the deployment with the given key.
// This function is not meant to be invoked concurrently with the same key.
func (dc *DeploymentController) syncDeployment(ctx context.Context, key string) error {
	logger := klog.FromContext(ctx)
	namespace, name, err := cache.SplitMetaNamespaceKey(key)
	if err != nil {
		logger.Error(err, &amp;#34;Failed to split meta namespace cache key&amp;#34;, &amp;#34;cacheKey&amp;#34;, key)
		return err
	}
	//该函数是一个Go语言函数，名为syncDeployment，属于DeploymentController结构体。
	//它接收一个ctx context.Context参数和一个key string参数，并返回一个error类型值。函数主要用于同步指定键值的部署信息。
	//函数首先从上下文中获取日志记录器，然后使用cache.SplitMetaNamespaceKey函数将键值拆分为命名空间和名称。
	//如果拆分过程中出现错误，则记录错误日志并返回该错误。

	startTime := time.Now()
	logger.V(4).Info(&amp;#34;Started syncing deployment&amp;#34;, &amp;#34;deployment&amp;#34;, klog.KRef(namespace, name), &amp;#34;startTime&amp;#34;, startTime)
	defer func() {
		logger.V(4).Info(&amp;#34;Finished syncing deployment&amp;#34;, &amp;#34;deployment&amp;#34;, klog.KRef(namespace, name), &amp;#34;duration&amp;#34;, time.Since(startTime))
	}()
	//这段Go代码主要实现了在开始和结束同步部署时记录日志的功能。
	//- 首先，通过time.Now()获取当前时间作为开始时间，并使用logger.V(4).Info记录开始同步部署的日志，其中包含了部署的名称、命名空间和开始时间。
	//- 然后，使用defer关键字定义了一个匿名函数，在函数执行结束后会自动执行该函数。
	//该匿名函数使用logger.V(4).Info记录结束同步部署的日志，其中包含了部署的名称、命名空间和同步部署所花费的时间。
	//通过这种方式，可以在日志中方便地查看部署的同步状态和耗时。

	deployment, err := dc.dLister.Deployments(namespace).Get(name)
	if errors.IsNotFound(err) {
		logger.V(2).Info(&amp;#34;Deployment has been deleted&amp;#34;, &amp;#34;deployment&amp;#34;, klog.KRef(namespace, name))
		return nil
	}
	if err != nil {
		return err
	}
	//该函数通过调用dc.dLister.Deployments(namespace).Get(name)获取指定命名空间中名为name的部署对象。
	//如果该部署对象不存在，则记录日志并返回nil；如果存在其他错误，则直接返回错误。

	// Deep-copy otherwise we are mutating our cache.
	// TODO: Deep-copy only when needed.
	d := deployment.DeepCopy()

	everything := metav1.LabelSelector{}
	if reflect.DeepEqual(d.Spec.Selector, &amp;amp;everything) {
		dc.eventRecorder.Eventf(d, v1.EventTypeWarning, &amp;#34;SelectingAll&amp;#34;, &amp;#34;This deployment is selecting all pods. A non-empty selector is required.&amp;#34;)
		if d.Status.ObservedGeneration &amp;lt; d.Generation {
			d.Status.ObservedGeneration = d.Generation
			dc.client.AppsV1().Deployments(d.Namespace).UpdateStatus(ctx, d, metav1.UpdateOptions{})
		}
		return nil
	}
	//这段Go代码中的函数主要功能是对一个deployment对象进行深拷贝，并检查其Selector是否等于一个空的LabelSelector对象。
	//如果是，则记录一条警告事件并更新deployment的状态。

	// List ReplicaSets owned by this Deployment, while reconciling ControllerRef
	// through adoption/orphaning.
	rsList, err := dc.getReplicaSetsForDeployment(ctx, d)
	if err != nil {
		return err
	}
	// List all Pods owned by this Deployment, grouped by their ReplicaSet.
	// Current uses of the podMap are:
	//
	// * check if a Pod is labeled correctly with the pod-template-hash label.
	// * check that no old Pods are running in the middle of Recreate Deployments.
	podMap, err := dc.getPodMapForDeployment(d, rsList)
	if err != nil {
		return err
	}

	if d.DeletionTimestamp != nil {
		return dc.syncStatusOnly(ctx, d, rsList)
	}

	// Update deployment conditions with an Unknown condition when pausing/resuming
	// a deployment. In this way, we can be sure that we won&amp;#39;t timeout when a user
	// resumes a Deployment with a set progressDeadlineSeconds.
	if err = dc.checkPausedConditions(ctx, d); err != nil {
		return err
	}

	if d.Spec.Paused {
		return dc.sync(ctx, d, rsList)
	}

	// rollback is not re-entrant in case the underlying replica sets are updated with a new
	// revision so we should ensure that we won&amp;#39;t proceed to update replica sets until we
	// make sure that the deployment has cleaned up its rollback spec in subsequent enqueues.
	if getRollbackTo(d) != nil {
		return dc.rollback(ctx, d, rsList)
	}

	scalingEvent, err := dc.isScalingEvent(ctx, d, rsList)
	if err != nil {
		return err
	}
	if scalingEvent {
		return dc.sync(ctx, d, rsList)
	}

	switch d.Spec.Strategy.Type {
	case apps.RecreateDeploymentStrategyType:
		return dc.rolloutRecreate(ctx, d, rsList, podMap)
	case apps.RollingUpdateDeploymentStrategyType:
		return dc.rolloutRolling(ctx, d, rsList)
	}
	return fmt.Errorf(&amp;#34;unexpected deployment strategy type: %s&amp;#34;, d.Spec.Strategy.Type)
}

//该函数主要负责处理Deployment的更新和同步逻辑。
//1. 首先，函数会获取该Deployment所拥有的ReplicaSet列表和Pod的映射关系。
//2. 如果该Deployment已被删除，则只同步状态。
//3. 检查是否暂停，若暂停则只进行同步操作。
//4. 如果需要回滚，则执行回滚操作。
//5. 检测是否为缩放事件，若是则进行同步操作。
//6. 根据Deployment的策略类型（Recreate或RollingUpdate），执行相应的更新操作。
//7. 如果遇到意外的部署策略类型，返回错误。
&lt;/code>&lt;/pre></description></item><item><title>2024-04-09 K8S控制器之 progress.go 进度 源码解读</title><link>https://qq547475331.github.io/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8B-progressgo-%E8%BF%9B%E5%BA%A6-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-progressgo-jin-du-yuan-ma-jie-du/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8B-progressgo-%E8%BF%9B%E5%BA%A6-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-progressgo-jin-du-yuan-ma-jie-du/</guid><description>&lt;pre tabindex="0">&lt;code>/*
Copyright 2016 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the &amp;#34;License&amp;#34;);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

 http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &amp;#34;AS IS&amp;#34; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package deployment

import (
	&amp;#34;context&amp;#34;
	&amp;#34;fmt&amp;#34;
	&amp;#34;reflect&amp;#34;
	&amp;#34;time&amp;#34;

	apps &amp;#34;k8s.io/api/apps/v1&amp;#34;
	&amp;#34;k8s.io/api/core/v1&amp;#34;
	metav1 &amp;#34;k8s.io/apimachinery/pkg/apis/meta/v1&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/controller/deployment/util&amp;#34;
)

// syncRolloutStatus updates the status of a deployment during a rollout. There are
// cases this helper will run that cannot be prevented from the scaling detection,
// for example a resync of the deployment after it was scaled up. In those cases,
// we shouldn&amp;#39;t try to estimate any progress.
func (dc *DeploymentController) syncRolloutStatus(ctx context.Context, allRSs []*apps.ReplicaSet, newRS *apps.ReplicaSet, d *apps.Deployment) error {
	newStatus := calculateStatus(allRSs, newRS, d)

	// If there is no progressDeadlineSeconds set, remove any Progressing condition.
	if !util.HasProgressDeadline(d) {
		util.RemoveDeploymentCondition(&amp;amp;newStatus, apps.DeploymentProgressing)
	}
	//该函数用于更新部署期间的部署状态。在某些情况下，无法从缩放检测中防止此帮助程序运行，例如在部署缩放后进行同步。
	//在这些情况下，不应尝试估计任何进度。
	//函数首先计算所有复制集、新复制集和部署的状态，然后如果未设置进度截止时间，则删除任何正在进行的条件。

	// If there is only one replica set that is active then that means we are not running
	// a new rollout and this is a resync where we don&amp;#39;t need to estimate any progress.
	// In such a case, we should simply not estimate any progress for this deployment.
	currentCond := util.GetDeploymentCondition(d.Status, apps.DeploymentProgressing)
	isCompleteDeployment := newStatus.Replicas == newStatus.UpdatedReplicas &amp;amp;&amp;amp; currentCond != nil &amp;amp;&amp;amp; currentCond.Reason == util.NewRSAvailableReason
	// Check for progress only if there is a progress deadline set and the latest rollout
	// hasn&amp;#39;t completed yet.
	if util.HasProgressDeadline(d) &amp;amp;&amp;amp; !isCompleteDeployment {
		switch {
		case util.DeploymentComplete(d, &amp;amp;newStatus):
			// Update the deployment conditions with a message for the new replica set that
			// was successfully deployed. If the condition already exists, we ignore this update.
			msg := fmt.Sprintf(&amp;#34;Deployment %q has successfully progressed.&amp;#34;, d.Name)
			if newRS != nil {
				msg = fmt.Sprintf(&amp;#34;ReplicaSet %q has successfully progressed.&amp;#34;, newRS.Name)
			}
			condition := util.NewDeploymentCondition(apps.DeploymentProgressing, v1.ConditionTrue, util.NewRSAvailableReason, msg)
			util.SetDeploymentCondition(&amp;amp;newStatus, *condition)

		case util.DeploymentProgressing(d, &amp;amp;newStatus):
			// If there is any progress made, continue by not checking if the deployment failed. This
			// behavior emulates the rolling updater progressDeadline check.
			msg := fmt.Sprintf(&amp;#34;Deployment %q is progressing.&amp;#34;, d.Name)
			if newRS != nil {
				msg = fmt.Sprintf(&amp;#34;ReplicaSet %q is progressing.&amp;#34;, newRS.Name)
			}
			condition := util.NewDeploymentCondition(apps.DeploymentProgressing, v1.ConditionTrue, util.ReplicaSetUpdatedReason, msg)
			//这段Go代码是用于检查和更新Kubernetes部署（Deployment）的进度状态的。具体来说，它根据以下条件来判断是否需要更新部署的进度状态：
			//1. 如果当前只有一个活动的副本集（replica set），则认为这不是一个新的滚动发布（rollout），而是一个重新同步（resync），
			//在这种情况下，不需要估计任何进度，因此直接不估计该部署的进度。
			//2. 如果设置了进度截止时间（progress deadline）并且最新滚动发布尚未完成，则检查部署的进度。
			//- 如果部署已完成，则更新部署状态，设置进度状态为成功，并添加相应的消息。
			//- 如果部署正在进展中，则更新部署状态，设置进度状态为正在进展，并添加相应的消息。
			//这段代码通过调用util包中的一系列函数来实现对部署进度的检查和更新，其中涉及到对部署条件（condition）的获取、设置和更新等操作。

			// Update the current Progressing condition or add a new one if it doesn&amp;#39;t exist.
			// If a Progressing condition with status=true already exists, we should update
			// everything but lastTransitionTime. SetDeploymentCondition already does that but
			// it also is not updating conditions when the reason of the new condition is the
			// same as the old. The Progressing condition is a special case because we want to
			// update with the same reason and change just lastUpdateTime iff we notice any
			// progress. That&amp;#39;s why we handle it here.
			if currentCond != nil {
				if currentCond.Status == v1.ConditionTrue {
					condition.LastTransitionTime = currentCond.LastTransitionTime
				}
				util.RemoveDeploymentCondition(&amp;amp;newStatus, apps.DeploymentProgressing)
			}
			util.SetDeploymentCondition(&amp;amp;newStatus, *condition)

		case util.DeploymentTimedOut(ctx, d, &amp;amp;newStatus):
			// Update the deployment with a timeout condition. If the condition already exists,
			// we ignore this update.
			msg := fmt.Sprintf(&amp;#34;Deployment %q has timed out progressing.&amp;#34;, d.Name)
			if newRS != nil {
				msg = fmt.Sprintf(&amp;#34;ReplicaSet %q has timed out progressing.&amp;#34;, newRS.Name)
			}
			condition := util.NewDeploymentCondition(apps.DeploymentProgressing, v1.ConditionFalse, util.TimedOutReason, msg)
			util.SetDeploymentCondition(&amp;amp;newStatus, *condition)
		}
		//该代码片段是Go语言编写的，它包含两个分支，分别处理不同的条件。
		//第一个分支更新或添加一个正在进行的条件。如果已经存在一个状态为true的正在进行的条件，那么只更新除lastTransitionTime之外的所有内容。
		//这通过调用util.RemoveDeploymentCondition和util.SetDeploymentCondition函数来实现。
		//第二个分支处理超时条件。如果部署或新ReplicaSet超时进行，则更新部署的条件为超时状态，并忽略已经存在的超时条件。
		//这通过调用util.NewDeploymentCondition和util.SetDeploymentCondition函数来实现。

	}

	// Move failure conditions of all replica sets in deployment conditions. For now,
	// only one failure condition is returned from getReplicaFailures.
	if replicaFailureCond := dc.getReplicaFailures(allRSs, newRS); len(replicaFailureCond) &amp;gt; 0 {
		// There will be only one ReplicaFailure condition on the replica set.
		util.SetDeploymentCondition(&amp;amp;newStatus, replicaFailureCond[0])
	} else {
		util.RemoveDeploymentCondition(&amp;amp;newStatus, apps.DeploymentReplicaFailure)
	}

	// Do not update if there is nothing new to add.
	if reflect.DeepEqual(d.Status, newStatus) {
		// Requeue the deployment if required.
		dc.requeueStuckDeployment(ctx, d, newStatus)
		return nil
	}

	newDeployment := d
	newDeployment.Status = newStatus
	_, err := dc.client.AppsV1().Deployments(newDeployment.Namespace).UpdateStatus(ctx, newDeployment, metav1.UpdateOptions{})
	return err
}

//该函数是用于更新部署（deployment）在滚动更新过程中的状态。
//它根据给定的所有复制集（replica set）和新的复制集的信息，计算出新的部署状态。
//然后，根据是否有进度截止时间，以及部署是否已完成，来检查和更新部署的进度状态。
//此外，它还会将复制集的失败条件转移到部署的条件中。
//最后，如果状态有更新，它会将新的部署状态更新到Kubernetes API中。

// getReplicaFailures will convert replica failure conditions from replica sets
// to deployment conditions.
func (dc *DeploymentController) getReplicaFailures(allRSs []*apps.ReplicaSet, newRS *apps.ReplicaSet) []apps.DeploymentCondition {
	var conditions []apps.DeploymentCondition
	if newRS != nil {
		for _, c := range newRS.Status.Conditions {
			if c.Type != apps.ReplicaSetReplicaFailure {
				continue
			}
			conditions = append(conditions, util.ReplicaSetToDeploymentCondition(c))
		}
	}

	// Return failures for the new replica set over failures from old replica sets.
	if len(conditions) &amp;gt; 0 {
		return conditions
	}

	for i := range allRSs {
		rs := allRSs[i]
		if rs == nil {
			continue
		}

		for _, c := range rs.Status.Conditions {
			if c.Type != apps.ReplicaSetReplicaFailure {
				continue
			}
			conditions = append(conditions, util.ReplicaSetToDeploymentCondition(c))
		}
	}
	return conditions
}

//该函数是用于将副本集的副本失败条件转换为部署条件的。
//函数首先检查新副本集（newRS）的状态条件，将除ReplicaSetReplicaFailure类型外的条件添加到conditions切片中。
//如果conditions切片不为空，则直接返回。
//否则，遍历所有旧副本集（allRSs），将除ReplicaSetReplicaFailure类型外的条件添加到conditions切片中。
//最后返回conditions切片。

// used for unit testing
var nowFn = func() time.Time { return time.Now() }

// requeueStuckDeployment checks whether the provided deployment needs to be synced for a progress
// check. It returns the time after the deployment will be requeued for the progress check, 0 if it
// will be requeued now, or -1 if it does not need to be requeued.
func (dc *DeploymentController) requeueStuckDeployment(ctx context.Context, d *apps.Deployment, newStatus apps.DeploymentStatus) time.Duration {
	logger := klog.FromContext(ctx)
	currentCond := util.GetDeploymentCondition(d.Status, apps.DeploymentProgressing)
	// Can&amp;#39;t estimate progress if there is no deadline in the spec or progressing condition in the current status.
	if !util.HasProgressDeadline(d) || currentCond == nil {
		return time.Duration(-1)
	}
	// No need to estimate progress if the rollout is complete or already timed out.
	if util.DeploymentComplete(d, &amp;amp;newStatus) || currentCond.Reason == util.TimedOutReason {
		return time.Duration(-1)
	}
	// If there is no sign of progress at this point then there is a high chance that the
	// deployment is stuck. We should resync this deployment at some point in the future[1]
	// and check whether it has timed out. We definitely need this, otherwise we depend on the
	// controller resync interval. See https://github.com/kubernetes/kubernetes/issues/34458.
	//
	// [1] ProgressingCondition.LastUpdatedTime + progressDeadlineSeconds - time.Now()
	//
	// For example, if a Deployment updated its Progressing condition 3 minutes ago and has a
	// deadline of 10 minutes, it would need to be resynced for a progress check after 7 minutes.
	//
	// lastUpdated: 			00:00:00
	// now: 					00:03:00
	// progressDeadlineSeconds: 600 (10 minutes)
	//
	// lastUpdated + progressDeadlineSeconds - now =&amp;gt; 00:00:00 + 00:10:00 - 00:03:00 =&amp;gt; 07:00
	after := currentCond.LastUpdateTime.Time.Add(time.Duration(*d.Spec.ProgressDeadlineSeconds) * time.Second).Sub(nowFn())
	// If the remaining time is less than a second, then requeue the deployment immediately.
	// Make it ratelimited so we stay on the safe side, eventually the Deployment should
	// transition either to a Complete or to a TimedOut condition.
	if after &amp;lt; time.Second {
		logger.V(4).Info(&amp;#34;Queueing up deployment for a progress check now&amp;#34;, &amp;#34;deployment&amp;#34;, klog.KObj(d))
		dc.enqueueRateLimited(d)
		return time.Duration(0)
	}
	logger.V(4).Info(&amp;#34;Queueing up deployment for a progress check&amp;#34;, &amp;#34;deployment&amp;#34;, klog.KObj(d), &amp;#34;queueAfter&amp;#34;, int(after.Seconds()))
	// Add a second to avoid milliseconds skew in AddAfter.
	// See https://github.com/kubernetes/kubernetes/issues/39785#issuecomment-279959133 for more info.
	dc.enqueueAfter(d, after+time.Second)
	return after
}

//该函数用于检查提供的部署是否需要同步进行进度检查。
//它返回部署重新排队进行进度检查的时间，如果需要立即重新排队则返回0，如果不需要重新排队则返回-1。
//函数首先从上下文中获取日志记录器，然后获取部署的当前状态和新的状态。
//如果部署的规范中没有截止日期或当前状态中没有进行中的条件，则无法估计进度，函数将返回-1。
//如果部署已经完成或已经超时，则无需估计进度，函数将返回-1。
//如果此时没有进度的迹象，则部署可能卡住了。函数将在未来的某个时间点重新同步此部署，并检查是否超时。
//这需要通过调用enqueueRateLimited方法将部署加入到队列中，并立即返回0。
//否则，函数将计算重新同步部署的时间，并通过调用enqueueAfter方法将部署加入到队列中，并返回计算的时间。
//如果计算的时间小于1秒，则函数将立即重新排队，并通过调用enqueueRateLimited方法将部署加入到队列中，并返回0。
//最后，函数将相关的日志信息记录下来。
&lt;/code>&lt;/pre></description></item><item><title>2024-04-09 K8S控制器之 rolling.go 滚动更新 源码解读</title><link>https://qq547475331.github.io/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8B-rollinggo-%E6%BB%9A%E5%8A%A8%E6%9B%B4%E6%96%B0-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-rollinggo-gun-dong-geng-xin-yuan-ma-jie-du/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8B-rollinggo-%E6%BB%9A%E5%8A%A8%E6%9B%B4%E6%96%B0-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-rollinggo-gun-dong-geng-xin-yuan-ma-jie-du/</guid><description>&lt;pre tabindex="0">&lt;code>/*
Copyright 2016 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the &amp;#34;License&amp;#34;);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

 http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &amp;#34;AS IS&amp;#34; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package deployment

import (
	&amp;#34;context&amp;#34;
	&amp;#34;fmt&amp;#34;
	&amp;#34;sort&amp;#34;

	apps &amp;#34;k8s.io/api/apps/v1&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/controller&amp;#34;
	deploymentutil &amp;#34;k8s.io/kubernetes/pkg/controller/deployment/util&amp;#34;
)

// rolloutRolling implements the logic for rolling a new replica set.
func (dc *DeploymentController) rolloutRolling(ctx context.Context, d *apps.Deployment, rsList []*apps.ReplicaSet) error {
	newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(ctx, d, rsList, true)
	if err != nil {
		return err
	}
	allRSs := append(oldRSs, newRS)

	// Scale up, if we can.
	scaledUp, err := dc.reconcileNewReplicaSet(ctx, allRSs, newRS, d)
	if err != nil {
		return err
	}
	if scaledUp {
		// Update DeploymentStatus
		return dc.syncRolloutStatus(ctx, allRSs, newRS, d)
	}

	// Scale down, if we can.
	scaledDown, err := dc.reconcileOldReplicaSets(ctx, allRSs, controller.FilterActiveReplicaSets(oldRSs), newRS, d)
	if err != nil {
		return err
	}
	if scaledDown {
		// Update DeploymentStatus
		return dc.syncRolloutStatus(ctx, allRSs, newRS, d)
	}

	if deploymentutil.DeploymentComplete(d, &amp;amp;d.Status) {
		if err := dc.cleanupDeployment(ctx, oldRSs, d); err != nil {
			return err
		}
	}

	// Sync deployment status
	return dc.syncRolloutStatus(ctx, allRSs, newRS, d)
}

//该函数实现了滚动更新一个新的副本集的逻辑。具体步骤如下：
//1. 调用getAllReplicaSetsAndSyncRevision函数获取所有的副本集并同步修订版本。
//2. 将获取到的所有副本集添加到allRSs切片中。
//3. 调用reconcileNewReplicaSet函数尝试增加新副本集的副本数，如果成功则更新部署状态。
//4. 如果上一步成功，则调用syncRolloutStatus函数更新部署状态。
//5. 否则，调用reconcileOldReplicaSets函数尝试减少旧副本集的副本数，如果成功则更新部署状态。
//6. 如果上一步成功，则再次调用syncRolloutStatus函数更新部署状态。
//7. 检查部署是否完成，如果完成则调用cleanupDeployment函数清理旧的副本集。
//8. 最后，调用syncRolloutStatus函数更新部署状态。

func (dc *DeploymentController) reconcileNewReplicaSet(ctx context.Context, allRSs []*apps.ReplicaSet, newRS *apps.ReplicaSet, deployment *apps.Deployment) (bool, error) {
	if *(newRS.Spec.Replicas) == *(deployment.Spec.Replicas) {
		// Scaling not required.
		return false, nil
	}
	if *(newRS.Spec.Replicas) &amp;gt; *(deployment.Spec.Replicas) {
		// Scale down.
		scaled, _, err := dc.scaleReplicaSetAndRecordEvent(ctx, newRS, *(deployment.Spec.Replicas), deployment)
		return scaled, err
	}
	newReplicasCount, err := deploymentutil.NewRSNewReplicas(deployment, allRSs, newRS)
	if err != nil {
		return false, err
	}
	scaled, _, err := dc.scaleReplicaSetAndRecordEvent(ctx, newRS, newReplicasCount, deployment)
	return scaled, err
}

//该函数是一个用于协调新 ReplicaSet 的函数，它会根据新 ReplicaSet 和 Deployment 的规格来决定是否需要进行缩放操作。
//具体流程如下：
//1. 首先，函数会比较新 ReplicaSet 和 Deployment 规格中的副本数。如果它们相等，说明不需要进行缩放操作，函数直接返回 false 和 nil。
//2. 如果新 ReplicaSet 的副本数大于 Deployment 的副本数，说明需要进行缩放操作。
//函数会调用 dc.scaleReplicaSetAndRecordEvent 方法来缩小新 ReplicaSet 的规模，并返回缩放结果和可能的错误。
//3. 如果新 ReplicaSet 的副本数小于或等于 Deployment 的副本数，并且存在其他 ReplicaSet，
//那么函数会调用 deploymentutil.NewRSNewReplicas 方法来计算新 ReplicaSet 应该拥有的副本数。
//4. 最后，函数会调用 dc.scaleReplicaSetAndRecordEvent 方法来调整新 ReplicaSet 的规模，并返回缩放结果和可能的错误。

func (dc *DeploymentController) reconcileOldReplicaSets(ctx context.Context, allRSs []*apps.ReplicaSet, oldRSs []*apps.ReplicaSet, newRS *apps.ReplicaSet, deployment *apps.Deployment) (bool, error) {
	logger := klog.FromContext(ctx)
	oldPodsCount := deploymentutil.GetReplicaCountForReplicaSets(oldRSs)
	if oldPodsCount == 0 {
		// Can&amp;#39;t scale down further
		return false, nil
	}
	allPodsCount := deploymentutil.GetReplicaCountForReplicaSets(allRSs)
	logger.V(4).Info(&amp;#34;New replica set&amp;#34;, &amp;#34;replicaSet&amp;#34;, klog.KObj(newRS), &amp;#34;availableReplicas&amp;#34;, newRS.Status.AvailableReplicas)
	maxUnavailable := deploymentutil.MaxUnavailable(*deployment)
	//该函数是一个Go语言函数，它属于DeploymentController类型。
	//函数名为reconcileOldReplicaSets，它接受五个参数：ctx上下文对象、allRSs所有ReplicaSet对象的切片、
	//oldRSs旧的ReplicaSet对象的切片、newRS新的ReplicaSet对象和deployment Deployment对象。
	//函数返回两个值：一个布尔值和一个错误对象。
	//函数主要功能是协调旧的ReplicaSet对象的副本数量，以确保与Deployment对象的期望副本数量一致。
	//它首先通过deploymentutil.GetReplicaCountForReplicaSets函数获取旧的ReplicaSet对象的副本数量，并检查是否为0。
	//如果是0，则意味着无法进一步缩容，函数直接返回false和nil。
	//接下来，函数通过同样的方式获取所有ReplicaSet对象的副本数量，并使用logger.V(4).Info记录相关信息。
	//然后，函数调用deploymentutil.MaxUnavailable函数获取最大不可用副本数。
	//函数的后续代码逻辑为，根据最大不可用副本数和新的ReplicaSet对象的可用副本数，判断是否需要缩容旧的ReplicaSet对象的副本数量。
	//如果需要缩容，则进行相应的操作。
	//最后，函数返回一个布尔值和一个错误对象，表示是否成功协调旧的ReplicaSet对象的副本数量。

	// Check if we can scale down. We can scale down in the following 2 cases:
	// * Some old replica sets have unhealthy replicas, we could safely scale down those unhealthy replicas since that won&amp;#39;t further
	// increase unavailability.
	// * New replica set has scaled up and it&amp;#39;s replicas becomes ready, then we can scale down old replica sets in a further step.
	//
	// maxScaledDown := allPodsCount - minAvailable - newReplicaSetPodsUnavailable
	// take into account not only maxUnavailable and any surge pods that have been created, but also unavailable pods from
	// the newRS, so that the unavailable pods from the newRS would not make us scale down old replica sets in a further
	// step(that will increase unavailability).
	//
	// Concrete example:
	//
	// * 10 replicas
	// * 2 maxUnavailable (absolute number, not percent)
	// * 3 maxSurge (absolute number, not percent)
	//
	// case 1:
	// * Deployment is updated, newRS is created with 3 replicas, oldRS is scaled down to 8, and newRS is scaled up to 5.
	// * The new replica set pods crashloop and never become available.
	// * allPodsCount is 13. minAvailable is 8. newRSPodsUnavailable is 5.
	// * A node fails and causes one of the oldRS pods to become unavailable. However, 13 - 8 - 5 = 0, so the oldRS won&amp;#39;t be scaled down.
	// * The user notices the crashloop and does kubectl rollout undo to rollback.
	// * newRSPodsUnavailable is 1, since we rolled back to the good replica set, so maxScaledDown = 13 - 8 - 1 = 4. 4 of the crashlooping pods will be scaled down.
	// * The total number of pods will then be 9 and the newRS can be scaled up to 10.
	//
	// case 2:
	// Same example, but pushing a new pod template instead of rolling back (aka &amp;#34;roll over&amp;#34;):
	// * The new replica set created must start with 0 replicas because allPodsCount is already at 13.
	// * However, newRSPodsUnavailable would also be 0, so the 2 old replica sets could be scaled down by 5 (13 - 8 - 0), which would then
	// allow the new replica set to be scaled up by 5.
	minAvailable := *(deployment.Spec.Replicas) - maxUnavailable
	newRSUnavailablePodCount := *(newRS.Spec.Replicas) - newRS.Status.AvailableReplicas
	maxScaledDown := allPodsCount - minAvailable - newRSUnavailablePodCount
	if maxScaledDown &amp;lt;= 0 {
		return false, nil
	}
	//该函数用于判断是否可以进行缩容操作。缩容可以在以下两种情况下进行：
	//1. 一些旧的副本集存在不健康的副本，可以安全地缩容这些不健康的副本，因为这不会进一步增加不可用性。
	//2. 新的副本集已经扩容并且其副本变为就绪状态，那么可以在进一步的步骤中缩容旧的副本集。
	//函数首先根据给定的参数计算出最小可用副本数minAvailable和新副本集的不可用副本数newRSUnavailablePodCount，
	//然后通过公式maxScaledDown = allPodsCount - minAvailable - newRSUnavailablePodCount 计算出最大可缩容数maxScaledDown。
	//如果maxScaledDown小于等于0，则返回false，表示不能进行缩容操作。

	// Clean up unhealthy replicas first, otherwise unhealthy replicas will block deployment
	// and cause timeout. See https://github.com/kubernetes/kubernetes/issues/16737
	oldRSs, cleanupCount, err := dc.cleanupUnhealthyReplicas(ctx, oldRSs, deployment, maxScaledDown)
	if err != nil {
		return false, nil
	}
	logger.V(4).Info(&amp;#34;Cleaned up unhealthy replicas from old RSes&amp;#34;, &amp;#34;count&amp;#34;, cleanupCount)

	// Scale down old replica sets, need check maxUnavailable to ensure we can scale down
	allRSs = append(oldRSs, newRS)
	scaledDownCount, err := dc.scaleDownOldReplicaSetsForRollingUpdate(ctx, allRSs, oldRSs, deployment)
	if err != nil {
		return false, nil
	}
	logger.V(4).Info(&amp;#34;Scaled down old RSes&amp;#34;, &amp;#34;deployment&amp;#34;, klog.KObj(deployment), &amp;#34;count&amp;#34;, scaledDownCount)

	totalScaledDown := cleanupCount + scaledDownCount
	return totalScaledDown &amp;gt; 0, nil
}

//该函数用于清理不健康的副本，并进行滚动更新时的缩容操作。首先清理不健康的副本，以避免阻塞部署并导致超时。然后将旧的副本集缩容，
//同时检查最大不可用副本数以确保可以进行缩容操作。函数返回一个布尔值，表示是否进行了缩容操作。

// cleanupUnhealthyReplicas will scale down old replica sets with unhealthy replicas, so that all unhealthy replicas will be deleted.
func (dc *DeploymentController) cleanupUnhealthyReplicas(ctx context.Context, oldRSs []*apps.ReplicaSet, deployment *apps.Deployment, maxCleanupCount int32) ([]*apps.ReplicaSet, int32, error) {
	logger := klog.FromContext(ctx)
	sort.Sort(controller.ReplicaSetsByCreationTimestamp(oldRSs))
	// Safely scale down all old replica sets with unhealthy replicas. Replica set will sort the pods in the order
	// such that not-ready &amp;lt; ready, unscheduled &amp;lt; scheduled, and pending &amp;lt; running. This ensures that unhealthy replicas will
	// been deleted first and won&amp;#39;t increase unavailability.
	//该函数用于清理不健康的副本集中的副本，以便所有不健康的副本都将被删除。
	//函数首先通过创建时间对旧的副本集进行排序，然后安全地缩小所有具有不健康副本的旧副本集的规模。
	//副本集将按照以下顺序对Pod进行排序：未就绪&amp;lt;就绪，未调度&amp;lt;已调度，等待&amp;lt;运行。
	//这确保了不健康的副本将首先被删除，并且不会增加不可用性。

	totalScaledDown := int32(0)
	for i, targetRS := range oldRSs {
		if totalScaledDown &amp;gt;= maxCleanupCount {
			break
		}
		if *(targetRS.Spec.Replicas) == 0 {
			// cannot scale down this replica set.
			continue
		}
		logger.V(4).Info(&amp;#34;Found available pods in old RS&amp;#34;, &amp;#34;replicaSet&amp;#34;, klog.KObj(targetRS), &amp;#34;availableReplicas&amp;#34;, targetRS.Status.AvailableReplicas)
		if *(targetRS.Spec.Replicas) == targetRS.Status.AvailableReplicas {
			// no unhealthy replicas found, no scaling required.
			continue
		}
		//该Go函数是一个for循环，用于遍历一组旧的ReplicaSet（oldRSs），并在满足条件的情况下对它们进行缩容操作。
		//具体功能如下：
		//1. 初始化一个整型变量totalScaledDown为0，用于记录已缩容的ReplicaSet数量。
		//2. 遍历oldRSs中的每个ReplicaSet，使用索引i和目标ReplicaSet targetRS。
		//3. 如果totalScaledDown大于等于最大清理数量maxCleanupCount，则跳出循环。
		//4. 如果目标ReplicaSet的副本数量Spec.Replicas为0，则无法进行缩容操作，继续下一个循环。
		//5. 使用logger记录日志信息，表示在目标ReplicaSet中找到了可用的Pod。
		//6. 如果目标ReplicaSet的副本数量Spec.Replicas等于其可用副本数量Status.AvailableReplicas，则无需进行缩容操作，继续下一个循环。
		//7. 如果以上条件均不满足，则进行缩容操作，并更新totalScaledDown的值。
		//综上所述，该函数的功能是在满足条件的情况下对一组旧的ReplicaSet进行缩容操作，并记录已缩容的ReplicaSet数量。

		scaledDownCount := min(maxCleanupCount-totalScaledDown, *(targetRS.Spec.Replicas)-targetRS.Status.AvailableReplicas)
		newReplicasCount := *(targetRS.Spec.Replicas) - scaledDownCount
		if newReplicasCount &amp;gt; *(targetRS.Spec.Replicas) {
			return nil, 0, fmt.Errorf(&amp;#34;when cleaning up unhealthy replicas, got invalid request to scale down %s/%s %d -&amp;gt; %d&amp;#34;, targetRS.Namespace, targetRS.Name, *(targetRS.Spec.Replicas), newReplicasCount)
		}
		_, updatedOldRS, err := dc.scaleReplicaSetAndRecordEvent(ctx, targetRS, newReplicasCount, deployment)
		if err != nil {
			return nil, totalScaledDown, err
		}
		totalScaledDown += scaledDownCount
		oldRSs[i] = updatedOldRS
	}
	return oldRSs, totalScaledDown, nil
}

//该函数用于清理不健康的副本，并将其数量减少到目标副本数量。
//它首先计算清理数量，然后尝试将目标副本数量减少到该数量。如果减少后的数量大于目标副本数量，则返回错误。
//然后，它会更新旧的副本集，并将其添加到一个切片中。
//最后，它返回更新后的旧副本集和清理的总数量。

// scaleDownOldReplicaSetsForRollingUpdate scales down old replica sets when deployment strategy is &amp;#34;RollingUpdate&amp;#34;.
// Need check maxUnavailable to ensure availability
func (dc *DeploymentController) scaleDownOldReplicaSetsForRollingUpdate(ctx context.Context, allRSs []*apps.ReplicaSet, oldRSs []*apps.ReplicaSet, deployment *apps.Deployment) (int32, error) {
	logger := klog.FromContext(ctx)
	maxUnavailable := deploymentutil.MaxUnavailable(*deployment)

	// Check if we can scale down.
	minAvailable := *(deployment.Spec.Replicas) - maxUnavailable
	// Find the number of available pods.
	availablePodCount := deploymentutil.GetAvailableReplicaCountForReplicaSets(allRSs)
	if availablePodCount &amp;lt;= minAvailable {
		// Cannot scale down.
		return 0, nil
	}
	logger.V(4).Info(&amp;#34;Found available pods in deployment, scaling down old RSes&amp;#34;, &amp;#34;deployment&amp;#34;, klog.KObj(deployment), &amp;#34;availableReplicas&amp;#34;, availablePodCount)
	//该函数用于在RollingUpdate策略下滚动更新时，将旧的ReplicaSet缩容。它需要检查最大不可用副本数以确保可用性。
	//函数首先从上下文中获取日志记录器，并计算最大不可用副本数。
	//然后，它检查可用Pod的数量是否大于等于最小可用副本数，如果小于最小可用副本数则不进行缩容操作，返回0。
	//否则，函数记录日志并继续缩容旧的ReplicaSet。

	sort.Sort(controller.ReplicaSetsByCreationTimestamp(oldRSs))

	totalScaledDown := int32(0)
	totalScaleDownCount := availablePodCount - minAvailable
	for _, targetRS := range oldRSs {
		if totalScaledDown &amp;gt;= totalScaleDownCount {
			// No further scaling required.
			break
		}
		if *(targetRS.Spec.Replicas) == 0 {
			// cannot scale down this ReplicaSet.
			continue
		}
		// Scale down.
		scaleDownCount := min(*(targetRS.Spec.Replicas), totalScaleDownCount-totalScaledDown)
		newReplicasCount := *(targetRS.Spec.Replicas) - scaleDownCount
		if newReplicasCount &amp;gt; *(targetRS.Spec.Replicas) {
			return 0, fmt.Errorf(&amp;#34;when scaling down old RS, got invalid request to scale down %s/%s %d -&amp;gt; %d&amp;#34;, targetRS.Namespace, targetRS.Name, *(targetRS.Spec.Replicas), newReplicasCount)
		}
		_, _, err := dc.scaleReplicaSetAndRecordEvent(ctx, targetRS, newReplicasCount, deployment)
		if err != nil {
			return totalScaledDown, err
		}

		totalScaledDown += scaleDownCount
	}
	//这段代码是用于缩容旧的ReplicaSet的。它首先通过sort.Sort方法将旧的ReplicaSet按照创建时间排序。
	//然后，它计算出需要缩容的总副本数，并遍历每个旧的ReplicaSet。
	//如果已经缩容的副本数达到了需要缩容的总副本数，就停止缩容操作。
	//对于每个需要缩容的ReplicaSet，它计算出可以缩容的副本数，并更新其副本数。
	//如果更新后的副本数大于原来的副本数，就会返回错误。如果更新成功，就将缩容的副本数累加到totalScaledDown中。
	//最后，函数返回缩容的总副本数和可能发生的错误。

	return totalScaledDown, nil
}

//该函数用于在Deployment策略为RollingUpdate时，缩放旧的副本集。
//它会检查最大不可用值（maxUnavailable）以确保可用性。
//函数首先计算出最小可用值，然后查找可用Pod的数量。
//如果可用Pod数量小于或等于最小可用值，则不会进行缩放。
//然后，函数会对旧的副本集进行排序，并迭代每个副本集。对于每个副本集，函数会计算出可以缩放的数量，并将其与副本集当前的副本数进行比较。
//如果计算出的缩放数量大于当前副本数，则函数会返回错误。
//最后，函数会缩放每个副本集，并记录缩放事件。函数返回缩放的总数量和可能的错误。
&lt;/code>&lt;/pre></description></item><item><title>2024-04-09 K8S控制器之 scheduler.go 调度器 源码解读</title><link>https://qq547475331.github.io/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8B-schedulergo-%E8%B0%83%E5%BA%A6%E5%99%A8-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-schedulergo-diao-du-qi-yuan-ma-jie-du/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8B-schedulergo-%E8%B0%83%E5%BA%A6%E5%99%A8-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-schedulergo-diao-du-qi-yuan-ma-jie-du/</guid><description>&lt;pre tabindex="0">&lt;code>/*
Copyright 2014 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the &amp;#34;License&amp;#34;);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

 http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &amp;#34;AS IS&amp;#34; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package scheduler

import (
	&amp;#34;context&amp;#34;
	&amp;#34;errors&amp;#34;
	&amp;#34;fmt&amp;#34;
	&amp;#34;time&amp;#34;

	v1 &amp;#34;k8s.io/api/core/v1&amp;#34;
	&amp;#34;k8s.io/apimachinery/pkg/api/meta&amp;#34;
	metav1 &amp;#34;k8s.io/apimachinery/pkg/apis/meta/v1&amp;#34;
	&amp;#34;k8s.io/apimachinery/pkg/util/wait&amp;#34;
	utilfeature &amp;#34;k8s.io/apiserver/pkg/util/feature&amp;#34;
	&amp;#34;k8s.io/client-go/dynamic/dynamicinformer&amp;#34;
	&amp;#34;k8s.io/client-go/informers&amp;#34;
	coreinformers &amp;#34;k8s.io/client-go/informers/core/v1&amp;#34;
	clientset &amp;#34;k8s.io/client-go/kubernetes&amp;#34;
	restclient &amp;#34;k8s.io/client-go/rest&amp;#34;
	&amp;#34;k8s.io/client-go/tools/cache&amp;#34;
	configv1 &amp;#34;k8s.io/kube-scheduler/config/v1&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/features&amp;#34;
	schedulerapi &amp;#34;k8s.io/kubernetes/pkg/scheduler/apis/config&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/scheduler/apis/config/scheme&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/scheduler/framework&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/scheduler/framework/parallelize&amp;#34;
	frameworkplugins &amp;#34;k8s.io/kubernetes/pkg/scheduler/framework/plugins&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/scheduler/framework/plugins/noderesources&amp;#34;
	frameworkruntime &amp;#34;k8s.io/kubernetes/pkg/scheduler/framework/runtime&amp;#34;
	internalcache &amp;#34;k8s.io/kubernetes/pkg/scheduler/internal/cache&amp;#34;
	cachedebugger &amp;#34;k8s.io/kubernetes/pkg/scheduler/internal/cache/debugger&amp;#34;
	internalqueue &amp;#34;k8s.io/kubernetes/pkg/scheduler/internal/queue&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/scheduler/metrics&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/scheduler/profile&amp;#34;
)

const (
	// Duration the scheduler will wait before expiring an assumed pod.
	// See issue #106361 for more details about this parameter and its value.
	durationToExpireAssumedPod time.Duration = 0
)

// ErrNoNodesAvailable is used to describe the error that no nodes available to schedule pods.
var ErrNoNodesAvailable = fmt.Errorf(&amp;#34;no nodes available to schedule pods&amp;#34;)

// Scheduler watches for new unscheduled pods. It attempts to find
// nodes that they fit on and writes bindings back to the api server.
type Scheduler struct {
	// It is expected that changes made via Cache will be observed
	// by NodeLister and Algorithm.
	Cache internalcache.Cache

	Extenders []framework.Extender

	// NextPod should be a function that blocks until the next pod
	// is available. We don&amp;#39;t use a channel for this, because scheduling
	// a pod may take some amount of time and we don&amp;#39;t want pods to get
	// stale while they sit in a channel.
	NextPod func(logger klog.Logger) (*framework.QueuedPodInfo, error)

	// FailureHandler is called upon a scheduling failure.
	FailureHandler FailureHandlerFn

	// SchedulePod tries to schedule the given pod to one of the nodes in the node list.
	// Return a struct of ScheduleResult with the name of suggested host on success,
	// otherwise will return a FitError with reasons.
	SchedulePod func(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) (ScheduleResult, error)

	// Close this to shut down the scheduler.
	StopEverything &amp;lt;-chan struct{}

	// SchedulingQueue holds pods to be scheduled
	SchedulingQueue internalqueue.SchedulingQueue

	// Profiles are the scheduling profiles.
	Profiles profile.Map

	client clientset.Interface

	nodeInfoSnapshot *internalcache.Snapshot

	percentageOfNodesToScore int32

	nextStartNodeIndex int

	// logger *must* be initialized when creating a Scheduler,
	// otherwise logging functions will access a nil sink and
	// panic.
	logger klog.Logger

	// registeredHandlers contains the registrations of all handlers. It&amp;#39;s used to check if all handlers have finished syncing before the scheduling cycles start.
	registeredHandlers []cache.ResourceEventHandlerRegistration
}

//该Go代码定义了一个名为Scheduler的结构体，用于管理Pod的调度过程。
//它包含多个字段，用于配置调度器的行为，
//例如Cache、Extenders、NextPod、FailureHandler、SchedulePod、StopEverything、SchedulingQueue、Profiles、
//client、nodeInfoSnapshot、percentageOfNodesToScore、nextStartNodeIndex、logger和registeredHandlers。
//这些字段的作用如下：
//- Cache：用于存储集群的状态信息， 以便Scheduler能够快速访问。
//- Extenders：是一组扩展程序，可以自定义Pod的调度逻辑。
//- NextPod：是一个函数，用于获取下一个待调度的Pod。 -
//FailureHandler：是一个函数，用于处理调度失败的情况。
//- SchedulePod：是一个函数，用于尝试将给定的Pod调度到节点列表中的一个节点上。
//成功时返回建议的主机名，失败时返回FitError错误。
//- StopEverything：是一个通道，用于关闭Scheduler。
//- SchedulingQueue：用于存储待调度的Pod。
//- Profiles：是一组调度配置文件，用于定义不同的调度策略。
//- client：是用于与Kubernetes API服务器交互的客户端。
//- nodeInfoSnapshot：是集群节点的快照，包含节点的状态信息。
//- percentageOfNodesToScore：用于指定参与评分的节点百分比。
//- nextStartNodeIndex：用于指定下一个开始调度的节点索引。
//- logger：用于记录日志信息。
//- registeredHandlers：包含所有处理器的注册信息，用于检查处理器是否已同步完成。
//总的来说，这个Scheduler结构体定义了一个调度器所需的各种配置和功能，
//包括节点和Pod的缓存、扩展程序、日志记录、失败处理、调度逻辑等。它提供了一种灵活的方式来定制和管理Pod的调度过程。

func (sched *Scheduler) applyDefaultHandlers() {
	sched.SchedulePod = sched.schedulePod
	sched.FailureHandler = sched.handleSchedulingFailure
}

//该函数为一个go语言函数，作用是为Scheduler结构体实例sched设置默认的处理函数。
//具体操作是将sched.schedulePod赋值给sched.SchedulePod，将sched.handleSchedulingFailure赋值给sched.FailureHandler。

type schedulerOptions struct {
	componentConfigVersion string
	kubeConfig *restclient.Config
	// Overridden by profile level percentageOfNodesToScore if set in v1.
	percentageOfNodesToScore int32
	podInitialBackoffSeconds int64
	podMaxBackoffSeconds int64
	podMaxInUnschedulablePodsDuration time.Duration
	// Contains out-of-tree plugins to be merged with the in-tree registry.
	frameworkOutOfTreeRegistry frameworkruntime.Registry
	profiles []schedulerapi.KubeSchedulerProfile
	extenders []schedulerapi.Extender
	frameworkCapturer FrameworkCapturer
	parallelism int32
	applyDefaultProfile bool
}

//该代码定义了一个名为schedulerOptions的结构体，用于配置调度器的参数。
//其中包括了组件配置版本、kubeconfig配置、节点打分百分比、Pod初始退避时间、Pod最大退避时间、
//Pod在不可调度状态下的最大持续时间、外部插件注册表、调度器配置文件、扩展器、框架捕获器和并行度等参数。

// Option configures a Scheduler
type Option func(*schedulerOptions)

// ScheduleResult represents the result of scheduling a pod.
type ScheduleResult struct {
	// Name of the selected node.
	SuggestedHost string
	// The number of nodes the scheduler evaluated the pod against in the filtering
	// phase and beyond.
	// Note that it contains the number of nodes that filtered out by PreFilterResult.
	EvaluatedNodes int
	// The number of nodes out of the evaluated ones that fit the pod.
	FeasibleNodes int
	// The nominating info for scheduling cycle.
	nominatingInfo *framework.NominatingInfo
}

//这段代码定义了Go语言中的两个类型和一个函数：
//1. Option 是一个函数类型，其参数为一个 *schedulerOptions 指针，用于配置一个 Scheduler。
//2. ScheduleResult 是一个结构体类型，代表调度 Pod 的结果。它包含以下字段：
//- SuggestedHost：被选中的节点名称。
//- EvaluatedNodes：在过滤阶段及之后对 Pod 进行评估的节点数量。
//- FeasibleNodes：在评估节点中适合 Pod 的节点数量。
//- nominatingInfo：提名信息，用于记录调度周期的提名情况。
//这段代码没有定义函数的具体实现，因此无法对其复杂度进行评估。

// WithComponentConfigVersion sets the component config version to the
// KubeSchedulerConfiguration version used. The string should be the full
// scheme group/version of the external type we converted from (for example
// &amp;#34;kubescheduler.config.k8s.io/v1&amp;#34;)
func WithComponentConfigVersion(apiVersion string) Option {
	return func(o *schedulerOptions) {
		o.componentConfigVersion = apiVersion
	}
}

// WithKubeConfig sets the kube config for Scheduler.
func WithKubeConfig(cfg *restclient.Config) Option {
	return func(o *schedulerOptions) {
		o.kubeConfig = cfg
	}
}

// WithProfiles sets profiles for Scheduler. By default, there is one profile
// with the name &amp;#34;default-scheduler&amp;#34;.
func WithProfiles(p ...schedulerapi.KubeSchedulerProfile) Option {
	return func(o *schedulerOptions) {
		o.profiles = p
		o.applyDefaultProfile = false
	}
}

// WithParallelism sets the parallelism for all scheduler algorithms. Default is 16.
func WithParallelism(threads int32) Option {
	return func(o *schedulerOptions) {
		o.parallelism = threads
	}
}

// WithPercentageOfNodesToScore sets percentageOfNodesToScore for Scheduler.
// The default value of 0 will use an adaptive percentage: 50 - (num of nodes)/125.
func WithPercentageOfNodesToScore(percentageOfNodesToScore *int32) Option {
	return func(o *schedulerOptions) {
		if percentageOfNodesToScore != nil {
			o.percentageOfNodesToScore = *percentageOfNodesToScore
		}
	}
}

//这些函数是Go语言中的函数，用于设置调度器（scheduler）的配置选项。
//- WithComponentConfigVersion 函数用于设置组件配置的版本。
//它接受一个字符串参数 apiVersion，该参数应该是外部类型转换而来的完整方案组/版本（例如 &amp;#34;kubescheduler.config.k8s.io/v1&amp;#34;）。
//- WithKubeConfig 函数用于设置调度器的kube配置。
//- WithProfiles 函数用于设置调度器的配置文件。默认情况下，有一个名为 &amp;#34;default-scheduler&amp;#34; 的配置文件。
//- WithParallelism 函数用于设置所有调度算法的并行度。默认值为16。
//- WithPercentageOfNodesToScore 函数用于设置Scheduler的 percentageOfNodesToScore。
//默认值为0，使用自适应百分比：50 - (节点数)/125。

// WithFrameworkOutOfTreeRegistry sets the registry for out-of-tree plugins. Those plugins
// will be appended to the default registry.
func WithFrameworkOutOfTreeRegistry(registry frameworkruntime.Registry) Option {
	return func(o *schedulerOptions) {
		o.frameworkOutOfTreeRegistry = registry
	}
}

// WithPodInitialBackoffSeconds sets podInitialBackoffSeconds for Scheduler, the default value is 1
func WithPodInitialBackoffSeconds(podInitialBackoffSeconds int64) Option {
	return func(o *schedulerOptions) {
		o.podInitialBackoffSeconds = podInitialBackoffSeconds
	}
}

// WithPodMaxBackoffSeconds sets podMaxBackoffSeconds for Scheduler, the default value is 10
func WithPodMaxBackoffSeconds(podMaxBackoffSeconds int64) Option {
	return func(o *schedulerOptions) {
		o.podMaxBackoffSeconds = podMaxBackoffSeconds
	}
}

// WithPodMaxInUnschedulablePodsDuration sets podMaxInUnschedulablePodsDuration for PriorityQueue.
func WithPodMaxInUnschedulablePodsDuration(duration time.Duration) Option {
	return func(o *schedulerOptions) {
		o.podMaxInUnschedulablePodsDuration = duration
	}
}

//这些函数是Go语言中的函数，用于设置调度器的配置选项。
//- WithFrameworkOutOfTreeRegistry 函数用于设置外部插件的注册表，将这些插件追加到默认的注册表中。
//- WithPodInitialBackoffSeconds 函数用于设置 Scheduler 的 podInitialBackoffSeconds，其默认值为 1。
//- WithPodMaxBackoffSeconds 函数用于设置 Scheduler 的 podMaxBackoffSeconds，其默认值为 10。
//- WithPodMaxInUnschedulablePodsDuration 函数用于设置 PriorityQueue 的 podMaxInUnschedulablePodsDuration。

// WithExtenders sets extenders for the Scheduler
func WithExtenders(e ...schedulerapi.Extender) Option {
	return func(o *schedulerOptions) {
		o.extenders = e
	}
}

//该函数为Go语言中的函数，名为WithExtenders，接收一个变长参数e，类型为schedulerapi.Extender的切片。
//函数返回一个Option类型的函数，该函数接收一个schedulerOptions类型的指针o，将e赋值给o.extenders。

// FrameworkCapturer is used for registering a notify function in building framework.
type FrameworkCapturer func(schedulerapi.KubeSchedulerProfile)

// WithBuildFrameworkCapturer sets a notify function for getting buildFramework details.
func WithBuildFrameworkCapturer(fc FrameworkCapturer) Option {
	return func(o *schedulerOptions) {
		o.frameworkCapturer = fc
	}
}

//该函数是一个名为WithBuildFrameworkCapturer的函数，
//它接收一个FrameworkCapturer类型的参数fc，并返回一个Option类型的函数。
//返回的函数将传入的fc赋值给o.frameworkCapturer。

var defaultSchedulerOptions = schedulerOptions{
	percentageOfNodesToScore: schedulerapi.DefaultPercentageOfNodesToScore,
	podInitialBackoffSeconds: int64(internalqueue.DefaultPodInitialBackoffDuration.Seconds()),
	podMaxBackoffSeconds: int64(internalqueue.DefaultPodMaxBackoffDuration.Seconds()),
	podMaxInUnschedulablePodsDuration: internalqueue.DefaultPodMaxInUnschedulablePodsDuration,
	parallelism: int32(parallelize.DefaultParallelism),
	// Ideally we would statically set the default profile here, but we can&amp;#39;t because
	// creating the default profile may require testing feature gates, which may get
	// set dynamically in tests. Therefore, we delay creating it until New is actually
	// invoked.
	applyDefaultProfile: true,
}

//这段Go代码定义了一个名为defaultSchedulerOptions的变量，它是一个schedulerOptions类型的结构体。
//这个结构体用于设置调度器的默认选项，包括以下字段：
//- percentageOfNodesToScore：表示要进行打分的节点的百分比，默认值为schedulerapi.DefaultPercentageOfNodesToScore。
//- podInitialBackoffSeconds：表示Pod初始退避时间的秒数，默认值为internalqueue.DefaultPodInitialBackoffDuration的秒数。
//- podMaxBackoffSeconds：表示Pod最大退避时间的秒数，默认值为internalqueue.DefaultPodMaxBackoffDuration的秒数。
//- podMaxInUnschedulablePodsDuration：表示Pod在不可调度状态下允许的最大持续时间，
//默认值为internalqueue.DefaultPodMaxInUnschedulablePodsDuration。
//- parallelism：表示并行处理任务的数量，默认值为parallelize.DefaultParallelism。
//- applyDefaultProfile：表示是否应用默认的调度配置文件，默认值为true。
//这些选项用于配置调度器的行为，例如决定多少节点需要进行打分、设置Pod的退避策略等。

// New returns a Scheduler
func New(ctx context.Context,
	client clientset.Interface,
	informerFactory informers.SharedInformerFactory,
	dynInformerFactory dynamicinformer.DynamicSharedInformerFactory,
	recorderFactory profile.RecorderFactory,
	opts ...Option) (*Scheduler, error) {

	logger := klog.FromContext(ctx)
	stopEverything := ctx.Done()

	options := defaultSchedulerOptions
	for _, opt := range opts {
		opt(&amp;amp;options)
	}
	//该函数名为New，返回一个Scheduler类型指针和一个错误类型。
	//函数参数包括上下文ctx、客户端接口client、共享informer工厂informerFactory、动态共享informer工厂dynInformerFactory、
	//记录器工厂recorderFactory以及可选参数opts。
	//函数首先从上下文中获取日志记录器logger和停止信号stopEverything。
	//然后定义默认的调度器选项options，并遍历opts对options进行配置。
	//最后返回一个Scheduler实例和错误类型。

	if options.applyDefaultProfile {
		var versionedCfg configv1.KubeSchedulerConfiguration
		scheme.Scheme.Default(&amp;amp;versionedCfg)
		cfg := schedulerapi.KubeSchedulerConfiguration{}
		if err := scheme.Scheme.Convert(&amp;amp;versionedCfg, &amp;amp;cfg, nil); err != nil {
			return nil, err
		}
		options.profiles = cfg.Profiles
	}
	//这段Go代码主要功能是应用默认配置到调度器配置中。
	//首先，它创建了一个configv1.KubeSchedulerConfiguration类型的变量versionedCfg，
	//并使用scheme.Scheme.Default函数为其应用默认值。
	//接着，它创建了一个schedulerapi.KubeSchedulerConfiguration类型的变量cfg，
	//并将versionedCfg中的值通过scheme.Scheme.Convert函数转换并赋值给cfg。
	//最后，将cfg.Profiles赋值给options.profiles。

	registry := frameworkplugins.NewInTreeRegistry()
	if err := registry.Merge(options.frameworkOutOfTreeRegistry); err != nil {
		return nil, err
	}

	metrics.Register()

	extenders, err := buildExtenders(logger, options.extenders, options.profiles)
	if err != nil {
		return nil, fmt.Errorf(&amp;#34;couldn&amp;#39;t build extenders: %w&amp;#34;, err)
	}

	podLister := informerFactory.Core().V1().Pods().Lister()
	nodeLister := informerFactory.Core().V1().Nodes().Lister()

	snapshot := internalcache.NewEmptySnapshot()
	metricsRecorder := metrics.NewMetricsAsyncRecorder(1000, time.Second, stopEverything)

	profiles, err := profile.NewMap(ctx, options.profiles, registry, recorderFactory,
		frameworkruntime.WithComponentConfigVersion(options.componentConfigVersion),
		frameworkruntime.WithClientSet(client),
		frameworkruntime.WithKubeConfig(options.kubeConfig),
		frameworkruntime.WithInformerFactory(informerFactory),
		frameworkruntime.WithSnapshotSharedLister(snapshot),
		frameworkruntime.WithCaptureProfile(frameworkruntime.CaptureProfile(options.frameworkCapturer)),
		frameworkruntime.WithParallelism(int(options.parallelism)),
		frameworkruntime.WithExtenders(extenders),
		frameworkruntime.WithMetricsRecorder(metricsRecorder),
	)
	if err != nil {
		return nil, fmt.Errorf(&amp;#34;initializing profiles: %v&amp;#34;, err)
	}
	//这段Go代码的功能是初始化一个调度器配置。
	//1. 首先创建一个in-tree注册表registry。
	//2. 将options.frameworkOutOfTreeRegistry合并到registry中，如果合并失败则返回错误。
	//3. 注册指标收集。
	//4. 构建扩展程序extenders，如果构建失败则返回错误。
	//5. 获取Pod和Node的列表器。
	//6. 创建一个空的快照snapshot。
	//7. 创建一个异步指标记录器metricsRecorder。
	//8. 使用给定的参数初始化调度器配置profiles，如果初始化失败则返回错误。
	//其中，buildExtenders函数用于构建扩展程序，informFactory是一个informers工厂，用于创建Pod和Node的列表器。internalcache.NewEmptySnapshot()创建一个空的快照，metrics.NewMetricsAsyncRecorder创建一个异步指标记录器。profile.NewMap用于初始化调度器配置。

	if len(profiles) == 0 {
		return nil, errors.New(&amp;#34;at least one profile is required&amp;#34;)
	}

	preEnqueuePluginMap := make(map[string][]framework.PreEnqueuePlugin)
	queueingHintsPerProfile := make(internalqueue.QueueingHintMapPerProfile)
	for profileName, profile := range profiles {
		preEnqueuePluginMap[profileName] = profile.PreEnqueuePlugins()
		queueingHintsPerProfile[profileName] = buildQueueingHintMap(profile.EnqueueExtensions())
	}

	podQueue := internalqueue.NewSchedulingQueue(
		profiles[options.profiles[0].SchedulerName].QueueSortFunc(),
		informerFactory,
		internalqueue.WithPodInitialBackoffDuration(time.Duration(options.podInitialBackoffSeconds)*time.Second),
		internalqueue.WithPodMaxBackoffDuration(time.Duration(options.podMaxBackoffSeconds)*time.Second),
		internalqueue.WithPodLister(podLister),
		internalqueue.WithPodMaxInUnschedulablePodsDuration(options.podMaxInUnschedulablePodsDuration),
		internalqueue.WithPreEnqueuePluginMap(preEnqueuePluginMap),
		internalqueue.WithQueueingHintMapPerProfile(queueingHintsPerProfile),
		internalqueue.WithPluginMetricsSamplePercent(pluginMetricsSamplePercent),
		internalqueue.WithMetricsRecorder(*metricsRecorder),
	)

	for _, fwk := range profiles {
		fwk.SetPodNominator(podQueue)
	}

	schedulerCache := internalcache.New(ctx, durationToExpireAssumedPod)

	// Setup cache debugger.
	debugger := cachedebugger.New(nodeLister, podLister, schedulerCache, podQueue)
	debugger.ListenForSignal(ctx)

	sched := &amp;amp;Scheduler{
		Cache: schedulerCache,
		client: client,
		nodeInfoSnapshot: snapshot,
		percentageOfNodesToScore: options.percentageOfNodesToScore,
		Extenders: extenders,
		StopEverything: stopEverything,
		SchedulingQueue: podQueue,
		Profiles: profiles,
		logger: logger,
	}
	sched.NextPod = podQueue.Pop
	sched.applyDefaultHandlers()

	if err = addAllEventHandlers(sched, informerFactory, dynInformerFactory, unionedGVKs(queueingHintsPerProfile)); err != nil {
		return nil, fmt.Errorf(&amp;#34;adding event handlers: %w&amp;#34;, err)
	}

	return sched, nil
}

//该函数主要实现了以下功能：
//1. 检查传入的profiles是否为空，如果为空则返回错误。
//2. 根据profiles创建preEnqueuePluginMap和queueingHintsPerProfile。
//3. 使用profiles中指定的SchedulerName创建一个SchedulingQueue对象，该对象用于管理待调度的Pod。
//4. 为每个profile设置PodNominator。
//5. 创建一个schedulerCache对象，用于缓存节点和Pod的信息。
//6. 创建一个debugger对象，用于调试缓存。
//7. 创建一个Scheduler对象，并设置其属性。
//8. 设置Scheduler的NextPod方法。
//9. 应用默认的处理程序。
//10. 添加所有事件处理程序。
//综上所述，该函数的主要功能是创建一个Scheduler对象，并对其进行初始化。

// defaultQueueingHintFn is the default queueing hint function.
// It always returns Queue as the queueing hint.
var defaultQueueingHintFn = func(_ klog.Logger, _ *v1.Pod, _, _ interface{}) (framework.QueueingHint, error) {
	return framework.Queue, nil
}

func buildQueueingHintMap(es []framework.EnqueueExtensions) internalqueue.QueueingHintMap {
	queueingHintMap := make(internalqueue.QueueingHintMap)
	for _, e := range es {
		events := e.EventsToRegister()
		//该函数的功能是构建一个队列提示映射（QueueingHintMap），它将事件注册到队列中。
		//1. 函数接收一个[]framework.EnqueueExtensions参数，它是一个调度器扩展点的集合，这些扩展点可以注册事件。
		//2. 创建一个空的internalqueue.QueueingHintMap用于存储队列提示映射。
		//3. 遍历扩展点集合es中的每个扩展点e。
		//4. 调用扩展点e的EventsToRegister()方法，获取该扩展点需要注册的事件。
		//5. 将事件添加到队列提示映射queueingHintMap中。
		//最终，函数返回构建完成的队列提示映射queueingHintMap。

		// This will happen when plugin registers with empty events, it&amp;#39;s usually the case a pod
		// will become reschedulable only for self-update, e.g. schedulingGates plugin, the pod
		// will enter into the activeQ via priorityQueue.Update().
		if len(events) == 0 {
			continue
		}

		// Note: Rarely, a plugin implements EnqueueExtensions but returns nil.
		// We treat it as: the plugin is not interested in any event, and hence pod failed by that plugin
		// cannot be moved by any regular cluster event.
		// So, we can just ignore such EventsToRegister here.

		registerNodeAdded := false
		registerNodeTaintUpdated := false
		for _, event := range events {
			fn := event.QueueingHintFn
			if fn == nil || !utilfeature.DefaultFeatureGate.Enabled(features.SchedulerQueueingHints) {
				fn = defaultQueueingHintFn
			}

			if event.Event.Resource == framework.Node {
				if event.Event.ActionType&amp;amp;framework.Add != 0 {
					registerNodeAdded = true
				}
				if event.Event.ActionType&amp;amp;framework.UpdateNodeTaint != 0 {
					registerNodeTaintUpdated = true
				}
			}
			//这段Go代码中的函数是一个循环，用于遍历一组事件（events），
			//并根据这些事件的类型来更新两个布尔变量registerNodeAdded和registerNodeTaintUpdated。
			//具体来说，函数首先检查事件是否启用了调度队列提示功能
			//（通过utilfeature.DefaultFeatureGate.Enabled(features.SchedulerQueueingHints)来判断），
			//如果没有启用，则使用默认的队列提示函数defaultQueueingHintFn
			//。然后，如果事件资源类型为framework.Node，并且事件动作类型包含framework.Add或framework.UpdateNodeTaint，
			//则分别将registerNodeAdded和registerNodeTaintUpdated设置为true。
			//这段代码的主要目的是为了根据事件的类型来决定是否需要注册节点添加或节点污点更新的操作。

			queueingHintMap[event.Event] = append(queueingHintMap[event.Event], &amp;amp;internalqueue.QueueingHintFunction{
				PluginName: e.Name(),
				QueueingHintFn: fn,
			})
		}
		if registerNodeAdded &amp;amp;&amp;amp; !registerNodeTaintUpdated {
			// Temporally fix for the issue https://github.com/kubernetes/kubernetes/issues/109437
			// NodeAdded QueueingHint isn&amp;#39;t always called because of preCheck.
			// It&amp;#39;s definitely not something expected for plugin developers,
			// and registering UpdateNodeTaint event is the only mitigation for now.
			//
			// So, here registers UpdateNodeTaint event for plugins that has NodeAdded event, but don&amp;#39;t have UpdateNodeTaint event.
			// It has a bad impact for the requeuing efficiency though, a lot better than some Pods being stuch in the
			// unschedulable pod pool.
			// This behavior will be removed when we remove the preCheck feature.
			// See: https://github.com/kubernetes/kubernetes/issues/110175
			queueingHintMap[framework.ClusterEvent{Resource: framework.Node, ActionType: framework.UpdateNodeTaint}] =
				append(queueingHintMap[framework.ClusterEvent{Resource: framework.Node, ActionType: framework.UpdateNodeTaint}],
					&amp;amp;internalqueue.QueueingHintFunction{
						PluginName: e.Name(),
						QueueingHintFn: defaultQueueingHintFn,
					},
				)
		}
	}
	return queueingHintMap
}

//这段Go代码是一个函数，它根据传入的事件列表和注册选项生成一个队列提示映射。
//该映射将事件与相应的队列提示函数进行关联。
//如果注册了NodeAdded事件但未注册UpdateNodeTaint事件，则会临时修复一个Kubernetes问题，
//通过为这些插件注册UpdateNodeTaint事件来避免某些Pod被卡在不可调度的Pod池中。
//该函数返回生成的队列提示映射。

// Run begins watching and scheduling. It starts scheduling and blocked until the context is done.
func (sched *Scheduler) Run(ctx context.Context) {
	logger := klog.FromContext(ctx)
	sched.SchedulingQueue.Run(logger)

	// We need to start scheduleOne loop in a dedicated goroutine,
	// because scheduleOne function hangs on getting the next item
	// from the SchedulingQueue.
	// If there are no new pods to schedule, it will be hanging there
	// and if done in this goroutine it will be blocking closing
	// SchedulingQueue, in effect causing a deadlock on shutdown.
	go wait.UntilWithContext(ctx, sched.ScheduleOne, 0)

	&amp;lt;-ctx.Done()
	sched.SchedulingQueue.Close()

	// If the plugins satisfy the io.Closer interface, they are closed.
	err := sched.Profiles.Close()
	if err != nil {
		logger.Error(err, &amp;#34;Failed to close plugins&amp;#34;)
	}
}

//该函数是Scheduler类型的Run方法，用于开始监控和调度。
//它启动调度并阻塞，直到上下文完成。具体步骤如下：
//1. 从上下文中获取日志记录器logger。
//2. 调用SchedulingQueue的Run方法，开始监控调度队列。
//3. 在一个独立的goroutine中启动scheduleOne循环，因为scheduleOne函数会在从SchedulingQueue获取下一个项目时挂起。
//如果没有任何新Pod需要调度，它将一直挂起。
//如果在当前goroutine中执行，它将阻塞关闭SchedulingQueue，从而在关闭时导致死锁。
//4. 等待上下文完成。
//5. 调用SchedulingQueue的Close方法，关闭调度队列。
//6. 尝试关闭满足io.Closer接口的插件。如果关闭失败，记录错误日志。

// NewInformerFactory creates a SharedInformerFactory and initializes a scheduler specific
// in-place podInformer.
func NewInformerFactory(cs clientset.Interface, resyncPeriod time.Duration) informers.SharedInformerFactory {
	informerFactory := informers.NewSharedInformerFactory(cs, resyncPeriod)
	informerFactory.InformerFor(&amp;amp;v1.Pod{}, newPodInformer)
	return informerFactory
}

//该函数创建一个SharedInformerFactory，并为特定的调度程序初始化一个就地podInformer。
//函数接收一个clientset.Interface类型和一个时间周期作为参数，返回一个SharedInformerFactory。
//内部通过调用informers.NewSharedInformerFactory创建一个新的SharedInformerFactory实例，
//并使用InformerFor方法为v1.Pod类型创建一个新的podInformer。
//最后返回创建的SharedInformerFactory实例。

func buildExtenders(logger klog.Logger, extenders []schedulerapi.Extender, profiles []schedulerapi.KubeSchedulerProfile) ([]framework.Extender, error) {
	var fExtenders []framework.Extender
	if len(extenders) == 0 {
		return nil, nil
	}

	var ignoredExtendedResources []string
	var ignorableExtenders []framework.Extender
	for i := range extenders {
		logger.V(2).Info(&amp;#34;Creating extender&amp;#34;, &amp;#34;extender&amp;#34;, extenders[i])
		extender, err := NewHTTPExtender(&amp;amp;extenders[i])
		if err != nil {
			return nil, err
		}
		if !extender.IsIgnorable() {
			fExtenders = append(fExtenders, extender)
		} else {
			ignorableExtenders = append(ignorableExtenders, extender)
		}
		for _, r := range extenders[i].ManagedResources {
			if r.IgnoredByScheduler {
				ignoredExtendedResources = append(ignoredExtendedResources, r.Name)
			}
		}
	}
	// place ignorable extenders to the tail of extenders
	fExtenders = append(fExtenders, ignorableExtenders...)
	//该函数主要负责根据传入的extenders参数构建一个framework.Extender类型的切片fExtenders。具体流程如下：
	//1. 首先判断extenders切片的长度是否为0，如果是则直接返回nil和nil。
	//2. 初始化两个切片ignoredExtendedResources和ignorableExtenders，分别用于存储被忽略的扩展资源名称和可忽略的扩展器。
	//3. 遍历extenders切片，对每个扩展器进行如下操作：
	//- 使用klog.Logger记录日志信息。
	//- 调用NewHTTPExtender函数创建一个新的HTTPExtender对象。
	//- 如果该扩展器不可忽略，则将其添加到fExtenders切片中。
	//- 如果该扩展器可忽略，则将其添加到ignorableExtenders切片中。
	//- 遍历该扩展器的ManagedResources字段，将被忽略的资源名称添加到ignoredExtendedResources切片中。
	//4. 将ignorableExtenders切片追加到fExtenders切片的末尾。
	//最终，函数返回构建好的fExtenders切片和可能出现的错误。

	// If there are any extended resources found from the Extenders, append them to the pluginConfig for each profile.
	// This should only have an effect on ComponentConfig, where it is possible to configure Extenders and
	// plugin args (and in which case the extender ignored resources take precedence).
	if len(ignoredExtendedResources) == 0 {
		return fExtenders, nil
	}

	for i := range profiles {
		prof := &amp;amp;profiles[i]
		var found = false
		for k := range prof.PluginConfig {
			if prof.PluginConfig[k].Name == noderesources.Name {
				// Update the existing args
				pc := &amp;amp;prof.PluginConfig[k]
				args, ok := pc.Args.(*schedulerapi.NodeResourcesFitArgs)
				if !ok {
					return nil, fmt.Errorf(&amp;#34;want args to be of type NodeResourcesFitArgs, got %T&amp;#34;, pc.Args)
				}
				args.IgnoredResources = ignoredExtendedResources
				found = true
				break
			}
		}
		if !found {
			return nil, fmt.Errorf(&amp;#34;can&amp;#39;t find NodeResourcesFitArgs in plugin config&amp;#34;)
		}
	}
	return fExtenders, nil
}

//该函数首先检查ignoredExtendedResources是否为空，如果为空则直接返回fExtenders和nil。
//如果不为空，则遍历profiles中的每一个元素，然后遍历该元素的PluginConfig。
//当找到PluginConfig中的Name等于noderesources.Name时，将ignoredExtendedResources赋值给Args的IgnoredResources字段，
//并将found设为true。 如果遍历完所有PluginConfig后仍未找到符合条件的元素，则返回nil和错误信息。
//最后返回fExtenders和nil。

type FailureHandlerFn func(ctx context.Context, fwk framework.Framework, podInfo *framework.QueuedPodInfo, status *framework.Status, nominatingInfo *framework.NominatingInfo, start time.Time)

func unionedGVKs(queueingHintsPerProfile internalqueue.QueueingHintMapPerProfile) map[framework.GVK]framework.ActionType {
	gvkMap := make(map[framework.GVK]framework.ActionType)
	for _, queueingHints := range queueingHintsPerProfile {
		for evt := range queueingHints {
			if _, ok := gvkMap[evt.Resource]; ok {
				gvkMap[evt.Resource] |= evt.ActionType
			} else {
				gvkMap[evt.Resource] = evt.ActionType
			}
		}
	}
	return gvkMap
}

//该函数用于将一个internalqueue.QueueingHintMapPerProfile类型的参数转换为一个map[framework.GVK]framework.ActionType类型的结果。
//具体实现过程为：遍历输入参数中的每个queueingHints，再遍历queueingHints中的每个事件evt，将evt.Resource作为键，
//evt.ActionType作为值存入gvkMap中。如果gvkMap中已经存在该键，则将该键对应的值与evt.ActionType进行按位或运算后再存入gvkMap中。
//最后返回gvkMap作为结果。

// newPodInformer creates a shared index informer that returns only non-terminal pods.
// The PodInformer allows indexers to be added, but note that only non-conflict indexers are allowed.
func newPodInformer(cs clientset.Interface, resyncPeriod time.Duration) cache.SharedIndexInformer {
	selector := fmt.Sprintf(&amp;#34;status.phase!=%v,status.phase!=%v&amp;#34;, v1.PodSucceeded, v1.PodFailed)
	tweakListOptions := func(options *metav1.ListOptions) {
		options.FieldSelector = selector
	}
	informer := coreinformers.NewFilteredPodInformer(cs, metav1.NamespaceAll, resyncPeriod, cache.Indexers{}, tweakListOptions)

	// Dropping `.metadata.managedFields` to improve memory usage.
	// The Extract workflow (i.e. `ExtractPod`) should be unused.
	trim := func(obj interface{}) (interface{}, error) {
		if accessor, err := meta.Accessor(obj); err == nil {
			accessor.SetManagedFields(nil)
		}
		return obj, nil
	}
	informer.SetTransform(trim)
	return informer
}

//该函数创建一个共享索引 informer，用于返回非终止状态的 pod。
//它通过设置筛选条件，使得 informer 只能获取到状态不是 &amp;#34;Succeeded&amp;#34; 或 &amp;#34;Failed&amp;#34; 的 pod。
//此外，函数还通过设置 transform 函数来删除 pod 的 .metadata.managedFields 字段，以减少内存使用。
//该函数返回一个经过筛选和转换的 pod informer 实例。
&lt;/code>&lt;/pre></description></item><item><title>2024-04-09 K8S控制器之recreate.go 重建 源码解读</title><link>https://qq547475331.github.io/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Brecreatego-%E9%87%8D%E5%BB%BA-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-recreatego-zhong-jian-yuan-ma-jie-du/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Brecreatego-%E9%87%8D%E5%BB%BA-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-recreatego-zhong-jian-yuan-ma-jie-du/</guid><description>&lt;pre tabindex="0">&lt;code>/*
Copyright 2016 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the &amp;#34;License&amp;#34;);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

 http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &amp;#34;AS IS&amp;#34; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package deployment

import (
	&amp;#34;context&amp;#34;
	apps &amp;#34;k8s.io/api/apps/v1&amp;#34;
	v1 &amp;#34;k8s.io/api/core/v1&amp;#34;
	&amp;#34;k8s.io/apimachinery/pkg/types&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/controller&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/controller/deployment/util&amp;#34;
)

// rolloutRecreate implements the logic for recreating a replica set.
func (dc *DeploymentController) rolloutRecreate(ctx context.Context, d *apps.Deployment, rsList []*apps.ReplicaSet, podMap map[types.UID][]*v1.Pod) error {
	// Don&amp;#39;t create a new RS if not already existed, so that we avoid scaling up before scaling down.
	newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(ctx, d, rsList, false)
	if err != nil {
		return err
	}
	allRSs := append(oldRSs, newRS)
	activeOldRSs := controller.FilterActiveReplicaSets(oldRSs)

	// scale down old replica sets.
	scaledDown, err := dc.scaleDownOldReplicaSetsForRecreate(ctx, activeOldRSs, d)
	if err != nil {
		return err
	}
	if scaledDown {
		// Update DeploymentStatus.
		return dc.syncRolloutStatus(ctx, allRSs, newRS, d)
	}

	// Do not process a deployment when it has old pods running.
	if oldPodsRunning(newRS, oldRSs, podMap) {
		return dc.syncRolloutStatus(ctx, allRSs, newRS, d)
	}

	// If we need to create a new RS, create it now.
	if newRS == nil {
		newRS, oldRSs, err = dc.getAllReplicaSetsAndSyncRevision(ctx, d, rsList, true)
		if err != nil {
			return err
		}
		allRSs = append(oldRSs, newRS)
	}

	// scale up new replica set.
	if _, err := dc.scaleUpNewReplicaSetForRecreate(ctx, newRS, d); err != nil {
		return err
	}

	if util.DeploymentComplete(d, &amp;amp;d.Status) {
		if err := dc.cleanupDeployment(ctx, oldRSs, d); err != nil {
			return err
		}
	}

	// Sync deployment status.
	return dc.syncRolloutStatus(ctx, allRSs, newRS, d)
}

//该函数实现了Deployment控制器中重新创建副本集的逻辑。具体步骤如下：
//1. 调用getAllReplicaSetsAndSyncRevision函数获取当前所有的副本集，并同步修订版本。如果已有新的副本集，则不再创建新的副本集，以避免先扩大规模再缩小规模。
//2. 过滤出活动的旧副本集。
//3. 调用scaleDownOldReplicaSetsForRecreate函数缩小旧副本集的规模。如果成功缩小规模，则更新Deployment状态。
//4. 如果存在正在运行的旧Pods，则只更新Deployment状态。
//5. 如果需要创建新的副本集，则调用getAllReplicaSetsAndSyncRevision函数创建新的副本集，并更新allRSs。
//6. 调用scaleUpNewReplicaSetForRecreate函数扩大新副本集的规模。
//7. 如果Deployment已完成，则调用cleanupDeployment函数清理旧的副本集。
//8. 最后，调用syncRolloutStatus函数更新Deployment状态。

// scaleDownOldReplicaSetsForRecreate scales down old replica sets when deployment strategy is &amp;#34;Recreate&amp;#34;.
func (dc *DeploymentController) scaleDownOldReplicaSetsForRecreate(ctx context.Context, oldRSs []*apps.ReplicaSet, deployment *apps.Deployment) (bool, error) {
	scaled := false
	for i := range oldRSs {
		rs := oldRSs[i]
		// Scaling not required.
		if *(rs.Spec.Replicas) == 0 {
			continue
		}
		scaledRS, updatedRS, err := dc.scaleReplicaSetAndRecordEvent(ctx, rs, 0, deployment)
		if err != nil {
			return false, err
		}
		if scaledRS {
			oldRSs[i] = updatedRS
			scaled = true
		}
	}
	return scaled, nil
}

//该函数用于在Recreate部署策略下，将旧的ReplicaSet缩容为0。
//它遍历旧的ReplicaSet列表，对每个非0副本数的ReplicaSet调用scaleReplicaSetAndRecordEvent函数进行缩容，并记录事件。
//如果有任何一个ReplicaSet成功缩容，则函数返回true，否则返回false。

// oldPodsRunning returns whether there are old pods running or any of the old ReplicaSets thinks that it runs pods.
func oldPodsRunning(newRS *apps.ReplicaSet, oldRSs []*apps.ReplicaSet, podMap map[types.UID][]*v1.Pod) bool {
	if oldPods := util.GetActualReplicaCountForReplicaSets(oldRSs); oldPods &amp;gt; 0 {
		return true
	}
	for rsUID, podList := range podMap {
		// If the pods belong to the new ReplicaSet, ignore.
		if newRS != nil &amp;amp;&amp;amp; newRS.UID == rsUID {
			continue
		}
		for _, pod := range podList {
			switch pod.Status.Phase {
			case v1.PodFailed, v1.PodSucceeded:
				// Don&amp;#39;t count pods in terminal state.
				continue
			case v1.PodUnknown:
				// v1.PodUnknown is a deprecated status.
				// This logic is kept for backward compatibility.
				// This used to happen in situation like when the node is temporarily disconnected from the cluster.
				// If we can&amp;#39;t be sure that the pod is not running, we have to count it.
				return true
			default:
				// Pod is not in terminal phase.
				return true
			}
		}
	}
	return false
}

//该函数用于判断是否有旧的Pod正在运行或任何一个旧的ReplicaSet认为它正在运行Pod。
//函数首先通过util.GetActualReplicaCountForReplicaSets函数获取旧的ReplicaSet的实际副本数，如果大于0，则返回true。
//然后，函数遍历podMap，检查每个Pod的状态，如果Pod属于新的ReplicaSet，则忽略；
//如果Pod处于失败或成功状态，则继续；如果Pod状态未知，则为了向后兼容，返回true；
//否则，返回true。
//如果遍历结束后没有返回true，则返回false。

// scaleUpNewReplicaSetForRecreate scales up new replica set when deployment strategy is &amp;#34;Recreate&amp;#34;.
func (dc *DeploymentController) scaleUpNewReplicaSetForRecreate(ctx context.Context, newRS *apps.ReplicaSet, deployment *apps.Deployment) (bool, error) {
	scaled, _, err := dc.scaleReplicaSetAndRecordEvent(ctx, newRS, *(deployment.Spec.Replicas), deployment)
	return scaled, err
}

//该函数用于在Recreate策略下，增加新副本集的副本数。
//函数内部调用了scaleReplicaSetAndRecordEvent函数，传入新副本集、部署的期望副本数和部署本身，该函数会尝试增加新副本集的副本数，并记录相应的事件。
//函数返回一个布尔值和一个错误，表示是否成功增加副本数以及是否发生了错误。
&lt;/code>&lt;/pre></description></item><item><title>2024-04-09 K8S控制器之rollback.go 回滚 源码解读</title><link>https://qq547475331.github.io/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Brollbackgo-%E5%9B%9E%E6%BB%9A-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-rollbackgo-hui-gun-yuan-ma-jie-du/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Brollbackgo-%E5%9B%9E%E6%BB%9A-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-rollbackgo-hui-gun-yuan-ma-jie-du/</guid><description>&lt;pre tabindex="0">&lt;code>/*
Copyright 2016 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the &amp;#34;License&amp;#34;);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

 http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &amp;#34;AS IS&amp;#34; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package deployment

import (
	&amp;#34;context&amp;#34;
	&amp;#34;fmt&amp;#34;
	&amp;#34;strconv&amp;#34;

	apps &amp;#34;k8s.io/api/apps/v1&amp;#34;
	v1 &amp;#34;k8s.io/api/core/v1&amp;#34;
	extensions &amp;#34;k8s.io/api/extensions/v1beta1&amp;#34;
	metav1 &amp;#34;k8s.io/apimachinery/pkg/apis/meta/v1&amp;#34;
	deploymentutil &amp;#34;k8s.io/kubernetes/pkg/controller/deployment/util&amp;#34;
)

// rollback the deployment to the specified revision. In any case cleanup the rollback spec.
func (dc *DeploymentController) rollback(ctx context.Context, d *apps.Deployment, rsList []*apps.ReplicaSet) error {
	logger := klog.FromContext(ctx)
	newRS, allOldRSs, err := dc.getAllReplicaSetsAndSyncRevision(ctx, d, rsList, true)
	if err != nil {
		return err
	}
	//该函数用于回滚部署到指定的修订版本，并在任何情况下清理回滚规范。
	//函数首先从上下文中获取日志记录器，
	//然后通过调用getAllReplicaSetsAndSyncRevision方法获取新的副本集、所有旧的副本集，并同步修订版本。
	//如果获取过程中出现错误，则返回该错误。

	allRSs := append(allOldRSs, newRS)
	rollbackTo := getRollbackTo(d)
	// If rollback revision is 0, rollback to the last revision
	if rollbackTo.Revision == 0 {
		if rollbackTo.Revision = deploymentutil.LastRevision(logger, allRSs); rollbackTo.Revision == 0 {
			// If we still can&amp;#39;t find the last revision, gives up rollback
			dc.emitRollbackWarningEvent(d, deploymentutil.RollbackRevisionNotFound, &amp;#34;Unable to find last revision.&amp;#34;)
			// Gives up rollback
			return dc.updateDeploymentAndClearRollbackTo(ctx, d)
		}
	}
	for _, rs := range allRSs {
		v, err := deploymentutil.Revision(rs)
		if err != nil {
			logger.V(4).Info(&amp;#34;Unable to extract revision from deployment&amp;#39;s replica set&amp;#34;, &amp;#34;replicaSet&amp;#34;, klog.KObj(rs), &amp;#34;err&amp;#34;, err)
			continue
		}
		if v == rollbackTo.Revision {
			logger.V(4).Info(&amp;#34;Found replica set with desired revision&amp;#34;, &amp;#34;replicaSet&amp;#34;, klog.KObj(rs), &amp;#34;revision&amp;#34;, v)
			// rollback by copying podTemplate.Spec from the replica set
			// revision number will be incremented during the next getAllReplicaSetsAndSyncRevision call
			// no-op if the spec matches current deployment&amp;#39;s podTemplate.Spec
			performedRollback, err := dc.rollbackToTemplate(ctx, d, rs)
			if performedRollback &amp;amp;&amp;amp; err == nil {
				dc.emitRollbackNormalEvent(d, fmt.Sprintf(&amp;#34;Rolled back deployment %q to revision %d&amp;#34;, d.Name, rollbackTo.Revision))
			}
			return err
			//该函数是一个Go语言函数，它实现了回滚（rollback）部署的逻辑。
			//函数首先将一个新副本集（newRS）添加到所有旧副本集（allOldRSs）中，然后根据提供的参数d获取回滚到的修订版本号（revision number） 。
			//如果回滚到的修订版本号为0，则将其回滚到最后一个修订版本号。
			//如果无法找到最后一个修订版本号，则放弃回滚并发出警告事件。
			//然后，函数遍历所有副本集，并尝试从每个副本集中提取修订版本号。
			//如果找到与回滚到的修订版本号匹配的副本集，则通过复制该副本集的podTemplate.Spec来执行回滚操作。
			//如果回滚操作成功，则发出正常事件。函数的最后返回值是可能产生的错误。

		}
	}
	dc.emitRollbackWarningEvent(d, deploymentutil.RollbackRevisionNotFound, &amp;#34;Unable to find the revision to rollback to.&amp;#34;)
	// Gives up rollback
	return dc.updateDeploymentAndClearRollbackTo(ctx, d)
}

//该函数是DeploymentController的一个方法，用于回滚部署到指定的修订版本。
//它首先尝试获取所有复制集并同步修订版本，然后根据回滚到的修订版本执行回滚操作。
//如果回滚修订版本为0，则回滚到最后一个修订版本。
//它遍历所有复制集，找到具有所需修订版本的复制集，并从该复制集复制podTemplate.Spec来执行回滚。
//如果找不到要回滚的修订版本，则放弃回滚并清除回滚信息。

// rollbackToTemplate compares the templates of the provided deployment and replica set and
// updates the deployment with the replica set template in case they are different. It also
// cleans up the rollback spec so subsequent requeues of the deployment won&amp;#39;t end up in here.
func (dc *DeploymentController) rollbackToTemplate(ctx context.Context, d *apps.Deployment, rs *apps.ReplicaSet) (bool, error) {
	logger := klog.FromContext(ctx)
	performedRollback := false
	if !deploymentutil.EqualIgnoreHash(&amp;amp;d.Spec.Template, &amp;amp;rs.Spec.Template) {
		logger.V(4).Info(&amp;#34;Rolling back deployment to old template spec&amp;#34;, &amp;#34;deployment&amp;#34;, klog.KObj(d), &amp;#34;templateSpec&amp;#34;, rs.Spec.Template.Spec)
		deploymentutil.SetFromReplicaSetTemplate(d, rs.Spec.Template)
		// set RS (the old RS we&amp;#39;ll rolling back to) annotations back to the deployment;
		// otherwise, the deployment&amp;#39;s current annotations (should be the same as current new RS) will be copied to the RS after the rollback.
		//
		// For example,
		// A Deployment has old RS1 with annotation {change-cause:create}, and new RS2 {change-cause:edit}.
		// Note that both annotations are copied from Deployment, and the Deployment should be annotated {change-cause:edit} as well.
		// Now, rollback Deployment to RS1, we should update Deployment&amp;#39;s pod-template and also copy annotation from RS1.
		// Deployment is now annotated {change-cause:create}, and we have new RS1 {change-cause:create}, old RS2 {change-cause:edit}.
		//
		// If we don&amp;#39;t copy the annotations back from RS to deployment on rollback, the Deployment will stay as {change-cause:edit},
		// and new RS1 becomes {change-cause:edit} (copied from deployment after rollback), old RS2 {change-cause:edit}, which is not correct.
		deploymentutil.SetDeploymentAnnotationsTo(d, rs)
		performedRollback = true
	} else {
		logger.V(4).Info(&amp;#34;Rolling back to a revision that contains the same template as current deployment, skipping rollback...&amp;#34;, &amp;#34;deployment&amp;#34;, klog.KObj(d))
		eventMsg := fmt.Sprintf(&amp;#34;The rollback revision contains the same template as current deployment %q&amp;#34;, d.Name)
		dc.emitRollbackWarningEvent(d, deploymentutil.RollbackTemplateUnchanged, eventMsg)
	}

	return performedRollback, dc.updateDeploymentAndClearRollbackTo(ctx, d)
}

//该函数用于将部署（deployment）的模板（template）与复制集（replica set）的模板进行比较，
//并在两者不相同的情况下，将部署的模板更新为复制集的模板。
//同时，它还会清理回滚规格，以避免后续重新排队导致回滚。
//函数返回一个布尔值和一个错误，表示是否执行了回滚操作和操作是否出错。
//具体步骤如下：
//1. 获取日志记录器。
//2. 初始化执行了回滚操作的标志为false。
//3. 如果部署的模板与复制集的模板不相同，则执行回滚操作：
//- 记录回滚操作的日志。
//- 将部署的模板设置为复制集的模板。
//- 将复制集的注解设置回部署；否则，部署的当前注解（应与当前新复制集相同）将被复制到回滚后的复制集。
//4. 如果部署的模板与复制集的模板相同，则记录跳过回滚的日志，并发出回滚警告事件。
//5. 返回执行了回滚操作的标志和更新部署并清除回滚规格的结果。

func (dc *DeploymentController) emitRollbackWarningEvent(d *apps.Deployment, reason, message string) {
	dc.eventRecorder.Eventf(d, v1.EventTypeWarning, reason, message)
}

func (dc *DeploymentController) emitRollbackNormalEvent(d *apps.Deployment, message string) {
	dc.eventRecorder.Eventf(d, v1.EventTypeNormal, deploymentutil.RollbackDone, message)
}

// updateDeploymentAndClearRollbackTo sets .spec.rollbackTo to nil and update the input deployment
// It is assumed that the caller will have updated the deployment template appropriately (in case
// we want to rollback).
func (dc *DeploymentController) updateDeploymentAndClearRollbackTo(ctx context.Context, d *apps.Deployment) error {
	logger := klog.FromContext(ctx)
	logger.V(4).Info(&amp;#34;Cleans up rollbackTo of deployment&amp;#34;, &amp;#34;deployment&amp;#34;, klog.KObj(d))
	setRollbackTo(d, nil)
	_, err := dc.client.AppsV1().Deployments(d.Namespace).Update(ctx, d, metav1.UpdateOptions{})
	return err
}

//该函数是Go语言编写的，用于更新输入的Deployment对象，并将其.spec.rollbackTo设置为nil。
//函数首先从上下文中获取日志记录器，然后使用日志记录器记录清理Deployment的rollbackTo的信息。
//接下来，它调用setRollbackTo函数将rollbackTo设置为nil。
//最后，它使用client来更新Deployment对象，并返回可能的错误。

// TODO: Remove this when extensions/v1beta1 and apps/v1beta1 Deployment are dropped.
func getRollbackTo(d *apps.Deployment) *extensions.RollbackConfig {
	// Extract the annotation used for round-tripping the deprecated RollbackTo field.
	revision := d.Annotations[apps.DeprecatedRollbackTo]
	if revision == &amp;#34;&amp;#34; {
		return nil
	}
	revision64, err := strconv.ParseInt(revision, 10, 64)
	if err != nil {
		// If it&amp;#39;s invalid, ignore it.
		return nil
	}
	return &amp;amp;extensions.RollbackConfig{
		Revision: revision64,
	}
}

//该函数用于获取一个代表回滚配置的RollbackConfig对象。
//它从传入的Deployment对象的注解中提取回滚版本号（revision），并将其转换为RollbackConfig对象。
//如果注解不存在或格式无效，则返回nil。

// TODO: Remove this when extensions/v1beta1 and apps/v1beta1 Deployment are dropped.
func setRollbackTo(d *apps.Deployment, rollbackTo *extensions.RollbackConfig) {
	if rollbackTo == nil {
		delete(d.Annotations, apps.DeprecatedRollbackTo)
		return
	}
	if d.Annotations == nil {
		d.Annotations = make(map[string]string)
	}
	d.Annotations[apps.DeprecatedRollbackTo] = strconv.FormatInt(rollbackTo.Revision, 10)
}

//该函数用于设置Go语言中的Deployment对象的回滚配置。
//它根据传入的rollbackTo参数，将回滚的版本信息保存在Deployment的注解(annotations)中。
//如果rollbackTo为nil，则删除该注解；
//否则，创建或更新该注解，值为回滚的修订版本号。
&lt;/code>&lt;/pre></description></item><item><title>2024-04-09 K8S控制器之sync.go 同步 源码解读</title><link>https://qq547475331.github.io/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Bsyncgo-%E5%90%8C%E6%AD%A5-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-syncgo-tong-bu-yuan-ma-jie-du/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Bsyncgo-%E5%90%8C%E6%AD%A5-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-syncgo-tong-bu-yuan-ma-jie-du/</guid><description>&lt;pre tabindex="0">&lt;code>/*
Copyright 2016 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the &amp;#34;License&amp;#34;);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

 http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &amp;#34;AS IS&amp;#34; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package deployment

import (
	&amp;#34;context&amp;#34;
	&amp;#34;fmt&amp;#34;
	&amp;#34;reflect&amp;#34;
	&amp;#34;sort&amp;#34;
	&amp;#34;strconv&amp;#34;

	apps &amp;#34;k8s.io/api/apps/v1&amp;#34;
	v1 &amp;#34;k8s.io/api/core/v1&amp;#34;
	&amp;#34;k8s.io/apimachinery/pkg/api/errors&amp;#34;
	metav1 &amp;#34;k8s.io/apimachinery/pkg/apis/meta/v1&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/controller&amp;#34;
	deploymentutil &amp;#34;k8s.io/kubernetes/pkg/controller/deployment/util&amp;#34;
	labelsutil &amp;#34;k8s.io/kubernetes/pkg/util/labels&amp;#34;
)

// syncStatusOnly only updates Deployments Status and doesn&amp;#39;t take any mutating actions.
func (dc *DeploymentController) syncStatusOnly(ctx context.Context, d *apps.Deployment, rsList []*apps.ReplicaSet) error {
	newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(ctx, d, rsList, false)
	if err != nil {
		return err
	}

	allRSs := append(oldRSs, newRS)
	return dc.syncDeploymentStatus(ctx, allRSs, newRS, d)
}

//该函数用于同步更新Deployment的状态，而不进行任何变更操作。
//它通过调用getAllReplicaSetsAndSyncRevision方法获取所有ReplicaSet的列表，并将新旧ReplicaSet区分开来。
//然后，它将所有ReplicaSet附加到旧ReplicaSet列表中，并调用syncDeploymentStatus方法来同步更新Deployment的状态。
//如果在获取ReplicaSet列表时出现错误，则返回该错误。

// sync is responsible for reconciling deployments on scaling events or when they
// are paused.
func (dc *DeploymentController) sync(ctx context.Context, d *apps.Deployment, rsList []*apps.ReplicaSet) error {
	newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(ctx, d, rsList, false)
	if err != nil {
		return err
	}
	if err := dc.scale(ctx, d, newRS, oldRSs); err != nil {
		// If we get an error while trying to scale, the deployment will be requeued
		// so we can abort this resync
		return err
	}

	// Clean up the deployment when it&amp;#39;s paused and no rollback is in flight.
	if d.Spec.Paused &amp;amp;&amp;amp; getRollbackTo(d) == nil {
		if err := dc.cleanupDeployment(ctx, oldRSs, d); err != nil {
			return err
		}
	}

	allRSs := append(oldRSs, newRS)
	return dc.syncDeploymentStatus(ctx, allRSs, newRS, d)
}

//该函数是一个Go语言函数，定义在DeploymentController结构体中，名为sync。它负责在缩放事件或暂停时协调部署。
//函数使用context.Context作为上下文，接收一个*apps.Deployment类型的参数d和一个[]*apps.ReplicaSet类型的参数rsList，返回一个error类型的值。
//函数主要执行以下操作：
//1. 调用getAllReplicaSetsAndSyncRevision方法获取所有复制集并同步修订版本，返回新复制集、旧复制集和错误（如果有）。
//2. 如果scale方法调用失败，则返回错误，以便重新排队处理。
//3. 如果部署被暂停并且没有进行中的回滚，则尝试清理部署。
//4. 将旧复制集和新复制集合并为一个列表，并调用syncDeploymentStatus方法来同步部署状态。
//最后，函数返回可能的错误。

// checkPausedConditions checks if the given deployment is paused or not and adds an appropriate condition.
// These conditions are needed so that we won&amp;#39;t accidentally report lack of progress for resumed deployments
// that were paused for longer than progressDeadlineSeconds.
func (dc *DeploymentController) checkPausedConditions(ctx context.Context, d *apps.Deployment) error {
	if !deploymentutil.HasProgressDeadline(d) {
		return nil
	}
	cond := deploymentutil.GetDeploymentCondition(d.Status, apps.DeploymentProgressing)
	if cond != nil &amp;amp;&amp;amp; cond.Reason == deploymentutil.TimedOutReason {
		// If we have reported lack of progress, do not overwrite it with a paused condition.
		return nil
	}
	pausedCondExists := cond != nil &amp;amp;&amp;amp; cond.Reason == deploymentutil.PausedDeployReason

	needsUpdate := false
	if d.Spec.Paused &amp;amp;&amp;amp; !pausedCondExists {
		condition := deploymentutil.NewDeploymentCondition(apps.DeploymentProgressing, v1.ConditionUnknown, deploymentutil.PausedDeployReason, &amp;#34;Deployment is paused&amp;#34;)
		deploymentutil.SetDeploymentCondition(&amp;amp;d.Status, *condition)
		needsUpdate = true
	} else if !d.Spec.Paused &amp;amp;&amp;amp; pausedCondExists {
		condition := deploymentutil.NewDeploymentCondition(apps.DeploymentProgressing, v1.ConditionUnknown, deploymentutil.ResumedDeployReason, &amp;#34;Deployment is resumed&amp;#34;)
		deploymentutil.SetDeploymentCondition(&amp;amp;d.Status, *condition)
		needsUpdate = true
	}

	if !needsUpdate {
		return nil
	}

	var err error
	_, err = dc.client.AppsV1().Deployments(d.Namespace).UpdateStatus(ctx, d, metav1.UpdateOptions{})
	return err
}

//该函数用于检查给定的部署是否暂停，并添加适当的条件。
//这些条件是必要的，以避免意外报告恢复的部署缺乏进展，这些部署被暂停的时间超过了progressDeadlineSeconds。
//函数首先检查部署是否具有进度截止时间，如果没有，则返回nil。
//然后获取部署的状态，如果状态中存在DeploymentProgressing条件且原因等于TimedOutReason，则不添加暂停条件。
//接下来，函数检查部署是否暂停以及是否存在暂停条件。
//如果部署已暂停但不存在暂停条件，则创建一个新的DeploymentProgressing条件并设置其原因为PausedDeployReason。
//如果部署未暂停但存在暂停条件，则创建一个新的DeploymentProgressing条件并设置其原因为ResumedDeployReason。
//最后，如果条件有更新，则使用client更新部署的状态。函数返回更新状态时可能发生的错误。

// getAllReplicaSetsAndSyncRevision returns all the replica sets for the provided deployment (new and all old), with new RS&amp;#39;s and deployment&amp;#39;s revision updated.
//
// rsList should come from getReplicaSetsForDeployment(d).
//
// 1. Get all old RSes this deployment targets, and calculate the max revision number among them (maxOldV).
// 2. Get new RS this deployment targets (whose pod template matches deployment&amp;#39;s), and update new RS&amp;#39;s revision number to (maxOldV + 1),
// only if its revision number is smaller than (maxOldV + 1). If this step failed, we&amp;#39;ll update it in the next deployment sync loop.
// 3. Copy new RS&amp;#39;s revision number to deployment (update deployment&amp;#39;s revision). If this step failed, we&amp;#39;ll update it in the next deployment sync loop.
//
// Note that currently the deployment controller is using caches to avoid querying the server for reads.
// This may lead to stale reads of replica sets, thus incorrect deployment status.
func (dc *DeploymentController) getAllReplicaSetsAndSyncRevision(ctx context.Context, d *apps.Deployment, rsList []*apps.ReplicaSet, createIfNotExisted bool) (*apps.ReplicaSet, []*apps.ReplicaSet, error) {
	_, allOldRSs := deploymentutil.FindOldReplicaSets(d, rsList)

	// Get new replica set with the updated revision number
	newRS, err := dc.getNewReplicaSet(ctx, d, rsList, allOldRSs, createIfNotExisted)
	if err != nil {
		return nil, nil, err
	}

	return newRS, allOldRSs, nil
}

const (
	// limit revision history length to 100 element (~2000 chars)
	maxRevHistoryLengthInChars = 2000
)

// Returns a replica set that matches the intent of the given deployment. Returns nil if the new replica set doesn&amp;#39;t exist yet.
// 1. Get existing new RS (the RS that the given deployment targets, whose pod template is the same as deployment&amp;#39;s).
// 2. If there&amp;#39;s existing new RS, update its revision number if it&amp;#39;s smaller than (maxOldRevision + 1), where maxOldRevision is the max revision number among all old RSes.
// 3. If there&amp;#39;s no existing new RS and createIfNotExisted is true, create one with appropriate revision number (maxOldRevision + 1) and replicas.
// Note that the pod-template-hash will be added to adopted RSes and pods.
func (dc *DeploymentController) getNewReplicaSet(ctx context.Context, d *apps.Deployment, rsList, oldRSs []*apps.ReplicaSet, createIfNotExisted bool) (*apps.ReplicaSet, error) {
	logger := klog.FromContext(ctx)
	existingNewRS := deploymentutil.FindNewReplicaSet(d, rsList)

	// Calculate the max revision number among all old RSes
	maxOldRevision := deploymentutil.MaxRevision(logger, oldRSs)
	// Calculate revision number for this new replica set
	newRevision := strconv.FormatInt(maxOldRevision+1, 10)

	// Latest replica set exists. We need to sync its annotations (includes copying all but
	// annotationsToSkip from the parent deployment, and update revision, desiredReplicas,
	// and maxReplicas) and also update the revision annotation in the deployment with the
	// latest revision.
	if existingNewRS != nil {
		rsCopy := existingNewRS.DeepCopy()

		// Set existing new replica set&amp;#39;s annotation
		annotationsUpdated := deploymentutil.SetNewReplicaSetAnnotations(ctx, d, rsCopy, newRevision, true, maxRevHistoryLengthInChars)
		minReadySecondsNeedsUpdate := rsCopy.Spec.MinReadySeconds != d.Spec.MinReadySeconds
		if annotationsUpdated || minReadySecondsNeedsUpdate {
			rsCopy.Spec.MinReadySeconds = d.Spec.MinReadySeconds
			return dc.client.AppsV1().ReplicaSets(rsCopy.ObjectMeta.Namespace).Update(ctx, rsCopy, metav1.UpdateOptions{})
		}

		// Should use the revision in existingNewRS&amp;#39;s annotation, since it set by before
		needsUpdate := deploymentutil.SetDeploymentRevision(d, rsCopy.Annotations[deploymentutil.RevisionAnnotation])
		// If no other Progressing condition has been recorded and we need to estimate the progress
		// of this deployment then it is likely that old users started caring about progress. In that
		// case we need to take into account the first time we noticed their new replica set.
		cond := deploymentutil.GetDeploymentCondition(d.Status, apps.DeploymentProgressing)
		if deploymentutil.HasProgressDeadline(d) &amp;amp;&amp;amp; cond == nil {
			msg := fmt.Sprintf(&amp;#34;Found new replica set %q&amp;#34;, rsCopy.Name)
			condition := deploymentutil.NewDeploymentCondition(apps.DeploymentProgressing, v1.ConditionTrue, deploymentutil.FoundNewRSReason, msg)
			deploymentutil.SetDeploymentCondition(&amp;amp;d.Status, *condition)
			needsUpdate = true
		}

		if needsUpdate {
			var err error
			if _, err = dc.client.AppsV1().Deployments(d.Namespace).UpdateStatus(ctx, d, metav1.UpdateOptions{}); err != nil {
				return nil, err
			}
		}
		return rsCopy, nil
	}

	if !createIfNotExisted {
		return nil, nil
	}

	// new ReplicaSet does not exist, create one.
	newRSTemplate := *d.Spec.Template.DeepCopy()
	podTemplateSpecHash := controller.ComputeHash(&amp;amp;newRSTemplate, d.Status.CollisionCount)
	newRSTemplate.Labels = labelsutil.CloneAndAddLabel(d.Spec.Template.Labels, apps.DefaultDeploymentUniqueLabelKey, podTemplateSpecHash)
	// Add podTemplateHash label to selector.
	newRSSelector := labelsutil.CloneSelectorAndAddLabel(d.Spec.Selector, apps.DefaultDeploymentUniqueLabelKey, podTemplateSpecHash)

	// Create new ReplicaSet
	newRS := apps.ReplicaSet{
		ObjectMeta: metav1.ObjectMeta{
			// Make the name deterministic, to ensure idempotence
			Name: d.Name + &amp;#34;-&amp;#34; + podTemplateSpecHash,
			Namespace: d.Namespace,
			OwnerReferences: []metav1.OwnerReference{*metav1.NewControllerRef(d, controllerKind)},
			Labels: newRSTemplate.Labels,
		},
		Spec: apps.ReplicaSetSpec{
			Replicas: new(int32),
			MinReadySeconds: d.Spec.MinReadySeconds,
			Selector: newRSSelector,
			Template: newRSTemplate,
		},
	}
	allRSs := append(oldRSs, &amp;amp;newRS)
	newReplicasCount, err := deploymentutil.NewRSNewReplicas(d, allRSs, &amp;amp;newRS)
	if err != nil {
		return nil, err
	}

	*(newRS.Spec.Replicas) = newReplicasCount
	// Set new replica set&amp;#39;s annotation
	deploymentutil.SetNewReplicaSetAnnotations(ctx, d, &amp;amp;newRS, newRevision, false, maxRevHistoryLengthInChars)
	// Create the new ReplicaSet. If it already exists, then we need to check for possible
	// hash collisions. If there is any other error, we need to report it in the status of
	// the Deployment.
	alreadyExists := false
	createdRS, err := dc.client.AppsV1().ReplicaSets(d.Namespace).Create(ctx, &amp;amp;newRS, metav1.CreateOptions{})
	switch {
	// We may end up hitting this due to a slow cache or a fast resync of the Deployment.
	case errors.IsAlreadyExists(err):
		alreadyExists = true

		// Fetch a copy of the ReplicaSet.
		rs, rsErr := dc.rsLister.ReplicaSets(newRS.Namespace).Get(newRS.Name)
		if rsErr != nil {
			return nil, rsErr
		}

		// If the Deployment owns the ReplicaSet and the ReplicaSet&amp;#39;s PodTemplateSpec is semantically
		// deep equal to the PodTemplateSpec of the Deployment, it&amp;#39;s the Deployment&amp;#39;s new ReplicaSet.
		// Otherwise, this is a hash collision and we need to increment the collisionCount field in
		// the status of the Deployment and requeue to try the creation in the next sync.
		controllerRef := metav1.GetControllerOf(rs)
		if controllerRef != nil &amp;amp;&amp;amp; controllerRef.UID == d.UID &amp;amp;&amp;amp; deploymentutil.EqualIgnoreHash(&amp;amp;d.Spec.Template, &amp;amp;rs.Spec.Template) {
			createdRS = rs
			err = nil
			break
		}

		// Matching ReplicaSet is not equal - increment the collisionCount in the DeploymentStatus
		// and requeue the Deployment.
		if d.Status.CollisionCount == nil {
			d.Status.CollisionCount = new(int32)
		}
		preCollisionCount := *d.Status.CollisionCount
		*d.Status.CollisionCount++
		// Update the collisionCount for the Deployment and let it requeue by returning the original
		// error.
		_, dErr := dc.client.AppsV1().Deployments(d.Namespace).UpdateStatus(ctx, d, metav1.UpdateOptions{})
		if dErr == nil {
			logger.V(2).Info(&amp;#34;Found a hash collision for deployment - bumping collisionCount to resolve it&amp;#34;, &amp;#34;deployment&amp;#34;, klog.KObj(d), &amp;#34;oldCollisionCount&amp;#34;, preCollisionCount, &amp;#34;newCollisionCount&amp;#34;, *d.Status.CollisionCount)
		}
		return nil, err
	case errors.HasStatusCause(err, v1.NamespaceTerminatingCause):
		// if the namespace is terminating, all subsequent creates will fail and we can safely do nothing
		return nil, err
	case err != nil:
		msg := fmt.Sprintf(&amp;#34;Failed to create new replica set %q: %v&amp;#34;, newRS.Name, err)
		if deploymentutil.HasProgressDeadline(d) {
			cond := deploymentutil.NewDeploymentCondition(apps.DeploymentProgressing, v1.ConditionFalse, deploymentutil.FailedRSCreateReason, msg)
			deploymentutil.SetDeploymentCondition(&amp;amp;d.Status, *cond)
			// We don&amp;#39;t really care about this error at this point, since we have a bigger issue to report.
			// TODO: Identify which errors are permanent and switch DeploymentIsFailed to take into account
			// these reasons as well. Related issue: https://github.com/kubernetes/kubernetes/issues/18568
			_, _ = dc.client.AppsV1().Deployments(d.Namespace).UpdateStatus(ctx, d, metav1.UpdateOptions{})
		}
		dc.eventRecorder.Eventf(d, v1.EventTypeWarning, deploymentutil.FailedRSCreateReason, msg)
		return nil, err
	}
	if !alreadyExists &amp;amp;&amp;amp; newReplicasCount &amp;gt; 0 {
		dc.eventRecorder.Eventf(d, v1.EventTypeNormal, &amp;#34;ScalingReplicaSet&amp;#34;, &amp;#34;Scaled up replica set %s to %d&amp;#34;, createdRS.Name, newReplicasCount)
	}

	needsUpdate := deploymentutil.SetDeploymentRevision(d, newRevision)
	if !alreadyExists &amp;amp;&amp;amp; deploymentutil.HasProgressDeadline(d) {
		msg := fmt.Sprintf(&amp;#34;Created new replica set %q&amp;#34;, createdRS.Name)
		condition := deploymentutil.NewDeploymentCondition(apps.DeploymentProgressing, v1.ConditionTrue, deploymentutil.NewReplicaSetReason, msg)
		deploymentutil.SetDeploymentCondition(&amp;amp;d.Status, *condition)
		needsUpdate = true
	}
	if needsUpdate {
		_, err = dc.client.AppsV1().Deployments(d.Namespace).UpdateStatus(ctx, d, metav1.UpdateOptions{})
	}
	return createdRS, err
}

//该函数是Go语言编写的，用于获取与给定部署相匹配的复制集（ReplicaSet）。
//如果该新的复制集不存在，且createIfNotExisted参数为true，则会创建一个新的复制集。
//函数首先查找与给定部署目标相匹配的现有新的复制集（即具有与部署相同的Pod模板）。
//然后，它会计算所有旧复制集的最大修订号（revision number），并为新的复制集计算一个适当的修订号。
//如果存在现有的新复制集，函数将更新其修订号和注释，并可能更新部署的状态。
//如果不存在新的复制集且createIfNotExisted为true，则函数将创建一个新的复制集，并更新部署的状态。
//该函数使用了context.Context来控制函数执行的上下文，使用了apps.Deployment、apps.ReplicaSet等结构体来表示部署和复制集的信息，
//使用了client来与Kubernetes API进行交互。
//函数的具体逻辑包括：
//1. 查找与给定部署相匹配的新的复制集。
//2. 计算所有旧复制集的最大修订号。
//3. 如果存在现有的新复制集，更新其修订号和注释，并可能更新部署的状态。
//4. 如果不存在新的复制集且createIfNotExisted为true，创建一个新的复制集，并更新部署的状态。
//该函数涉及的操作包括查找、更新和创建复制集，以及更新部署的状态。

// scale scales proportionally in order to mitigate risk. Otherwise, scaling up can increase the size
// of the new replica set and scaling down can decrease the sizes of the old ones, both of which would
// have the effect of hastening the rollout progress, which could produce a higher proportion of unavailable
// replicas in the event of a problem with the rolled out template. Should run only on scaling events or
// when a deployment is paused and not during the normal rollout process.
func (dc *DeploymentController) scale(ctx context.Context, deployment *apps.Deployment, newRS *apps.ReplicaSet, oldRSs []*apps.ReplicaSet) error {
	// If there is only one active replica set then we should scale that up to the full count of the
	// deployment. If there is no active replica set, then we should scale up the newest replica set.
	if activeOrLatest := deploymentutil.FindActiveOrLatest(newRS, oldRSs); activeOrLatest != nil {
		if *(activeOrLatest.Spec.Replicas) == *(deployment.Spec.Replicas) {
			return nil
		}
		_, _, err := dc.scaleReplicaSetAndRecordEvent(ctx, activeOrLatest, *(deployment.Spec.Replicas), deployment)
		return err
	}

	// If the new replica set is saturated, old replica sets should be fully scaled down.
	// This case handles replica set adoption during a saturated new replica set.
	if deploymentutil.IsSaturated(deployment, newRS) {
		for _, old := range controller.FilterActiveReplicaSets(oldRSs) {
			if _, _, err := dc.scaleReplicaSetAndRecordEvent(ctx, old, 0, deployment); err != nil {
				return err
			}
		}
		return nil
	}

	// There are old replica sets with pods and the new replica set is not saturated.
	// We need to proportionally scale all replica sets (new and old) in case of a
	// rolling deployment.
	if deploymentutil.IsRollingUpdate(deployment) {
		allRSs := controller.FilterActiveReplicaSets(append(oldRSs, newRS))
		allRSsReplicas := deploymentutil.GetReplicaCountForReplicaSets(allRSs)

		allowedSize := int32(0)
		if *(deployment.Spec.Replicas) &amp;gt; 0 {
			allowedSize = *(deployment.Spec.Replicas) + deploymentutil.MaxSurge(*deployment)
		}

		// Number of additional replicas that can be either added or removed from the total
		// replicas count. These replicas should be distributed proportionally to the active
		// replica sets.
		deploymentReplicasToAdd := allowedSize - allRSsReplicas

		// The additional replicas should be distributed proportionally amongst the active
		// replica sets from the larger to the smaller in size replica set. Scaling direction
		// drives what happens in case we are trying to scale replica sets of the same size.
		// In such a case when scaling up, we should scale up newer replica sets first, and
		// when scaling down, we should scale down older replica sets first.
		var scalingOperation string
		switch {
		case deploymentReplicasToAdd &amp;gt; 0:
			sort.Sort(controller.ReplicaSetsBySizeNewer(allRSs))
			scalingOperation = &amp;#34;up&amp;#34;

		case deploymentReplicasToAdd &amp;lt; 0:
			sort.Sort(controller.ReplicaSetsBySizeOlder(allRSs))
			scalingOperation = &amp;#34;down&amp;#34;
		}

		// Iterate over all active replica sets and estimate proportions for each of them.
		// The absolute value of deploymentReplicasAdded should never exceed the absolute
		// value of deploymentReplicasToAdd.
		deploymentReplicasAdded := int32(0)
		nameToSize := make(map[string]int32)
		logger := klog.FromContext(ctx)
		for i := range allRSs {
			rs := allRSs[i]

			// Estimate proportions if we have replicas to add, otherwise simply populate
			// nameToSize with the current sizes for each replica set.
			if deploymentReplicasToAdd != 0 {
				proportion := deploymentutil.GetProportion(logger, rs, *deployment, deploymentReplicasToAdd, deploymentReplicasAdded)

				nameToSize[rs.Name] = *(rs.Spec.Replicas) + proportion
				deploymentReplicasAdded += proportion
			} else {
				nameToSize[rs.Name] = *(rs.Spec.Replicas)
			}
		}

		// Update all replica sets
		for i := range allRSs {
			rs := allRSs[i]

			// Add/remove any leftovers to the largest replica set.
			if i == 0 &amp;amp;&amp;amp; deploymentReplicasToAdd != 0 {
				leftover := deploymentReplicasToAdd - deploymentReplicasAdded
				nameToSize[rs.Name] = nameToSize[rs.Name] + leftover
				if nameToSize[rs.Name] &amp;lt; 0 {
					nameToSize[rs.Name] = 0
				}
			}

			// TODO: Use transactions when we have them.
			if _, _, err := dc.scaleReplicaSet(ctx, rs, nameToSize[rs.Name], deployment, scalingOperation); err != nil {
				// Return as soon as we fail, the deployment is requeued
				return err
			}
		}
	}
	return nil
}

//该函数是一个Go语言函数，名为scale，属于DeploymentController类型。
//它接收四个参数：ctx是一个上下文对象，deployment是一个Deployment指针，newRS是一个ReplicaSet指针，oldRSs是一个ReplicaSet指针数组。
//函数返回一个错误。
//该函数用于根据给定的部署（deployment）和新的副本集（newRS）来比例地调整副本集的大小，以减轻风险。
//函数首先检查是否有活跃或最新的副本集需要进行大小调整，如果有，则将其调整到部署的指定大小。
//如果新的副本集已饱和，则将旧的副本集完全缩放 down。
//如果部署是滚动更新类型，则按比例缩放所有副本集（新旧副本集）。
//在滚动更新的情况下，函数会根据部署的规格（deployment.Spec.Replicas）和最大突增值（MaxSurge）计算出可以添加或移除的额外副本数量，
//并将这些副本按比例分配给所有活跃的副本集。
//在缩放过程中，函数会根据副本集的大小进行排序，并根据缩放操作的类型（上/下）选择合适的排序方式。
//函数会迭代所有活跃的副本集，并根据比例计算出每个副本集应该增加或减少的副本数量。
//最后，函数会更新所有副本集的大小，并在遇到错误时返回错误。
//总结： 该函数用于根据给定的部署和新的副本集来比例地调整副本集的大小，以减轻风险。
//它会根据不同的条件来判断如何缩放副本集，并将缩放操作应用于所有活跃的副本集。
//该函数在滚动更新的情况下，会按比例分配额外的副本数量给所有活跃的副本集。

func (dc *DeploymentController) scaleReplicaSetAndRecordEvent(ctx context.Context, rs *apps.ReplicaSet, newScale int32, deployment *apps.Deployment) (bool, *apps.ReplicaSet, error) {
	// No need to scale
	if *(rs.Spec.Replicas) == newScale {
		return false, rs, nil
	}
	var scalingOperation string
	if *(rs.Spec.Replicas) &amp;lt; newScale {
		scalingOperation = &amp;#34;up&amp;#34;
	} else {
		scalingOperation = &amp;#34;down&amp;#34;
	}
	scaled, newRS, err := dc.scaleReplicaSet(ctx, rs, newScale, deployment, scalingOperation)
	return scaled, newRS, err
}

//该函数是一个Go语言函数，名为scaleReplicaSetAndRecordEvent，属于DeploymentController类型。
//它接收四个参数：ctx是一个上下文对象，rs是一个ReplicaSet指针，newScale是一个int32类型的变量，deployment是一个Deployment指针。
//函数返回三个值：一个布尔值，一个ReplicaSet指针和一个错误。
//函数首先检查当前ReplicaSet的副本数量是否已经等于目标副本数量newScale，如果是，则直接返回不进行任何操作。
//如果需要进行缩放操作，则根据目标副本数量与当前副本数量的关系，确定是向上扩展还是向下缩小。
//接着，函数调用dc.scaleReplicaSet方法来执行实际的缩放操作，并将缩放操作的结果返回。
//最后，函数返回缩放操作是否成功、新的ReplicaSet指针以及可能的错误信息。
//总结： 该函数用于根据给定的目标副本数量对ReplicaSet进行缩放操作，并记录相关事件。
//它会检查当前副本数量是否已经等于目标数量，如果是则不进行操作；否则，根据目标数量与当前数量的关系确定是向上扩展还是向下缩小，并执行相应的缩放操作。

func (dc *DeploymentController) scaleReplicaSet(ctx context.Context, rs *apps.ReplicaSet, newScale int32, deployment *apps.Deployment, scalingOperation string) (bool, *apps.ReplicaSet, error) {

	sizeNeedsUpdate := *(rs.Spec.Replicas) != newScale

	annotationsNeedUpdate := deploymentutil.ReplicasAnnotationsNeedUpdate(rs, *(deployment.Spec.Replicas), *(deployment.Spec.Replicas)+deploymentutil.MaxSurge(*deployment))

	scaled := false
	var err error
	if sizeNeedsUpdate || annotationsNeedUpdate {
		oldScale := *(rs.Spec.Replicas)
		rsCopy := rs.DeepCopy()
		*(rsCopy.Spec.Replicas) = newScale
		deploymentutil.SetReplicasAnnotations(rsCopy, *(deployment.Spec.Replicas), *(deployment.Spec.Replicas)+deploymentutil.MaxSurge(*deployment))
		rs, err = dc.client.AppsV1().ReplicaSets(rsCopy.Namespace).Update(ctx, rsCopy, metav1.UpdateOptions{})
		if err == nil &amp;amp;&amp;amp; sizeNeedsUpdate {
			scaled = true
			dc.eventRecorder.Eventf(deployment, v1.EventTypeNormal, &amp;#34;ScalingReplicaSet&amp;#34;, &amp;#34;Scaled %s replica set %s to %d from %d&amp;#34;, scalingOperation, rs.Name, newScale, oldScale)
		}
	}
	return scaled, rs, err
}

//该函数用于缩放副本集的副本数量。
//它首先检查副本集的当前副本数量是否与目标副本数量不同，以及副本集的注解是否需要更新。
//如果任一条件为真，则创建一个副本集的深拷贝，并更新其副本数量和注解。
//然后，使用更新后的副本集调用客户端的Update方法来更新副本集。
//如果更新成功并且副本数量发生了变化，则记录一个事件表示副本集已被缩放。
//函数返回一个布尔值表示副本集是否被缩放，更新后的副本集对象和可能出现的错误。

// cleanupDeployment is responsible for cleaning up a deployment ie. retains all but the latest N old replica sets
// where N=d.Spec.RevisionHistoryLimit. Old replica sets are older versions of the podtemplate of a deployment kept
// around by default 1) for historical reasons and 2) for the ability to rollback a deployment.
func (dc *DeploymentController) cleanupDeployment(ctx context.Context, oldRSs []*apps.ReplicaSet, deployment *apps.Deployment) error {
	logger := klog.FromContext(ctx)
	if !deploymentutil.HasRevisionHistoryLimit(deployment) {
		return nil
	}

	// Avoid deleting replica set with deletion timestamp set
	aliveFilter := func(rs *apps.ReplicaSet) bool {
		return rs != nil &amp;amp;&amp;amp; rs.ObjectMeta.DeletionTimestamp == nil
	}
	cleanableRSes := controller.FilterReplicaSets(oldRSs, aliveFilter)

	diff := int32(len(cleanableRSes)) - *deployment.Spec.RevisionHistoryLimit
	if diff &amp;lt;= 0 {
		return nil
	}

	sort.Sort(deploymentutil.ReplicaSetsByRevision(cleanableRSes))
	logger.V(4).Info(&amp;#34;Looking to cleanup old replica sets for deployment&amp;#34;, &amp;#34;deployment&amp;#34;, klog.KObj(deployment))

	for i := int32(0); i &amp;lt; diff; i++ {
		rs := cleanableRSes[i]
		// Avoid delete replica set with non-zero replica counts
		if rs.Status.Replicas != 0 || *(rs.Spec.Replicas) != 0 || rs.Generation &amp;gt; rs.Status.ObservedGeneration || rs.DeletionTimestamp != nil {
			continue
		}
		logger.V(4).Info(&amp;#34;Trying to cleanup replica set for deployment&amp;#34;, &amp;#34;replicaSet&amp;#34;, klog.KObj(rs), &amp;#34;deployment&amp;#34;, klog.KObj(deployment))
		if err := dc.client.AppsV1().ReplicaSets(rs.Namespace).Delete(ctx, rs.Name, metav1.DeleteOptions{}); err != nil &amp;amp;&amp;amp; !errors.IsNotFound(err) {
			// Return error instead of aggregating and continuing DELETEs on the theory
			// that we may be overloading the api server.
			return err
		}
	}

	return nil
}

//该函数用于清理部署，即保留最新N个旧的副本集，其中N等于d.Spec.RevisionHistoryLimit。
//旧的副本集是部署的pod模板的旧版本，保留它们的原因有：
//1）历史原因；
//2）为了能够回滚部署。函数首先检查部署是否设置了修订历史限制，如果没有设置，则直接返回。
//然后，过滤掉具有删除时间戳的副本集，并计算需要清理的副本集数量。
//如果需要清理的副本集数量小于等于0，则直接返回。
//接下来，按修订版本对可清理的副本集进行排序，并尝试清理旧的副本集。
//对于每个需要清理的副本集，如果其副本数量不为零，或者期望的副本数量不为零，或者其生成代数大于观察到的生成代数，或者具有删除时间戳，则跳过清理。
//最后，如果清理过程中发生错误，则返回错误。

// syncDeploymentStatus checks if the status is up-to-date and sync it if necessary
func (dc *DeploymentController) syncDeploymentStatus(ctx context.Context, allRSs []*apps.ReplicaSet, newRS *apps.ReplicaSet, d *apps.Deployment) error {
	newStatus := calculateStatus(allRSs, newRS, d)

	if reflect.DeepEqual(d.Status, newStatus) {
		return nil
	}

	newDeployment := d
	newDeployment.Status = newStatus
	_, err := dc.client.AppsV1().Deployments(newDeployment.Namespace).UpdateStatus(ctx, newDeployment, metav1.UpdateOptions{})
	return err
}

//该函数是一个Go语言函数，用于检查部署（Deployment）的状态是否是最新的，如果不是最新的，则将其同步到最新的状态。
//函数定义在DeploymentController结构体中。
//函数接受四个参数：
//- ctx：上下文对象，用于控制函数执行期间的流程。
//- allRSs：一个包含所有副本集（ReplicaSet）的切片。
//- newRS：一个新的副本集。
//- d：一个部署对象。
//函数首先调用calculateStatus函数来计算最新的状态。
//然后，它将检查当前部署的状态是否与计算出的最新状态相等。
//如果相等，函数将直接返回，不做任何操作。
//如果不相等，函数将更新部署的状态为最新状态，并调用UpdateStatus方法将其更新到Kubernetes集群中。
//最后，函数返回可能发生的错误。

// calculateStatus calculates the latest status for the provided deployment by looking into the provided replica sets.
func calculateStatus(allRSs []*apps.ReplicaSet, newRS *apps.ReplicaSet, deployment *apps.Deployment) apps.DeploymentStatus {
	availableReplicas := deploymentutil.GetAvailableReplicaCountForReplicaSets(allRSs)
	totalReplicas := deploymentutil.GetReplicaCountForReplicaSets(allRSs)
	unavailableReplicas := totalReplicas - availableReplicas
	// If unavailableReplicas is negative, then that means the Deployment has more available replicas running than
	// desired, e.g. whenever it scales down. In such a case we should simply default unavailableReplicas to zero.
	if unavailableReplicas &amp;lt; 0 {
		unavailableReplicas = 0
	}

	status := apps.DeploymentStatus{
		// TODO: Ensure that if we start retrying status updates, we won&amp;#39;t pick up a new Generation value.
		ObservedGeneration: deployment.Generation,
		Replicas: deploymentutil.GetActualReplicaCountForReplicaSets(allRSs),
		UpdatedReplicas: deploymentutil.GetActualReplicaCountForReplicaSets([]*apps.ReplicaSet{newRS}),
		ReadyReplicas: deploymentutil.GetReadyReplicaCountForReplicaSets(allRSs),
		AvailableReplicas: availableReplicas,
		UnavailableReplicas: unavailableReplicas,
		CollisionCount: deployment.Status.CollisionCount,
	}

	// Copy conditions one by one so we won&amp;#39;t mutate the original object.
	conditions := deployment.Status.Conditions
	for i := range conditions {
		status.Conditions = append(status.Conditions, conditions[i])
	}

	if availableReplicas &amp;gt;= *(deployment.Spec.Replicas)-deploymentutil.MaxUnavailable(*deployment) {
		minAvailability := deploymentutil.NewDeploymentCondition(apps.DeploymentAvailable, v1.ConditionTrue, deploymentutil.MinimumReplicasAvailable, &amp;#34;Deployment has minimum availability.&amp;#34;)
		deploymentutil.SetDeploymentCondition(&amp;amp;status, *minAvailability)
	} else {
		noMinAvailability := deploymentutil.NewDeploymentCondition(apps.DeploymentAvailable, v1.ConditionFalse, deploymentutil.MinimumReplicasUnavailable, &amp;#34;Deployment does not have minimum availability.&amp;#34;)
		deploymentutil.SetDeploymentCondition(&amp;amp;status, *noMinAvailability)
	}

	return status
}

// isScalingEvent checks whether the provided deployment has been updated with a scaling event
// by looking at the desired-replicas annotation in the active replica sets of the deployment.
//
// rsList should come from getReplicaSetsForDeployment(d).
func (dc *DeploymentController) isScalingEvent(ctx context.Context, d *apps.Deployment, rsList []*apps.ReplicaSet) (bool, error) {
	newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(ctx, d, rsList, false)
	if err != nil {
		return false, err
	}
	allRSs := append(oldRSs, newRS)
	logger := klog.FromContext(ctx)
	for _, rs := range controller.FilterActiveReplicaSets(allRSs) {
		desired, ok := deploymentutil.GetDesiredReplicasAnnotation(logger, rs)
		if !ok {
			continue
		}
		if desired != *(d.Spec.Replicas) {
			return true, nil
		}
	}
	return false, nil
}

//该函数用于检查提供的deployment是否已通过调整活动replica set的数量来进行更新。
//它通过查看deployment的desired-replicas注解来判断
//。函数首先获取deployment的所有replica set并同步修订版本号，然后遍历所有活动的replica set，
//检查其desired-replicas注解与deployment的期望副本数是否一致，如果一致则返回false，否则返回true。
&lt;/code>&lt;/pre></description></item><item><title>2024-04-09 K8S调度器 extender.go 源码解读</title><link>https://qq547475331.github.io/docs/k8s%E8%B0%83%E5%BA%A6%E5%99%A8-extendergo-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-diao-du-qi-extendergo-yuan-ma-jie-du/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/k8s%E8%B0%83%E5%BA%A6%E5%99%A8-extendergo-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-diao-du-qi-extendergo-yuan-ma-jie-du/</guid><description>&lt;pre tabindex="0">&lt;code>/*
Copyright 2015 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the &amp;#34;License&amp;#34;);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

 http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &amp;#34;AS IS&amp;#34; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package scheduler

import (
	&amp;#34;bytes&amp;#34;
	&amp;#34;encoding/json&amp;#34;
	&amp;#34;fmt&amp;#34;
	&amp;#34;net/http&amp;#34;
	&amp;#34;strings&amp;#34;
	&amp;#34;time&amp;#34;

	v1 &amp;#34;k8s.io/api/core/v1&amp;#34;
	utilnet &amp;#34;k8s.io/apimachinery/pkg/util/net&amp;#34;
	&amp;#34;k8s.io/apimachinery/pkg/util/sets&amp;#34;
	restclient &amp;#34;k8s.io/client-go/rest&amp;#34;
	extenderv1 &amp;#34;k8s.io/kube-scheduler/extender/v1&amp;#34;
	schedulerapi &amp;#34;k8s.io/kubernetes/pkg/scheduler/apis/config&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/scheduler/framework&amp;#34;
)

const (
	// DefaultExtenderTimeout defines the default extender timeout in second.
	DefaultExtenderTimeout = 5 * time.Second
)

// HTTPExtender implements the Extender interface.
type HTTPExtender struct {
	extenderURL string
	preemptVerb string
	filterVerb string
	prioritizeVerb string
	bindVerb string
	weight int64
	client *http.Client
	nodeCacheCapable bool
	managedResources sets.Set[string]
	ignorable bool
}

//这段代码定义了一个名为HTTPExtender的结构体，它实现了Extender接口。
//HTTPExtender表示一个HTTP扩展器，用于扩展Kubernetes调度器的功能。
//它通过发送HTTP请求与Kubernetes调度器进行交互，以实现自定义的调度逻辑。
//该结构体包含以下字段： - extenderURL：扩展器的URL，用于向扩展器发送HTTP请求。
//- preemptVerb：抢占操作的动词，表示扩展器在抢占过程中执行的操作。
//- filterVerb：过滤操作的动词，表示扩展器在过滤节点时执行的操作。
//- prioritizeVerb：优先级排序操作的动词，表示扩展器在对节点进行优先级排序时执行的操作。
//- bindVerb：绑定操作的动词，表示扩展器在绑定Pod到节点时执行的操作。
//- weight：扩展器的权重，用于在优先级排序时对多个扩展器的结果进行加权计算。
//- client：用于发送HTTP请求的HTTP客户端。
//- nodeCacheCapable：表示扩展器是否支持节点缓存。
//如果为true，调度器会将节点信息缓存在本地，以减少对扩展器的请求次数。
//- managedResources：表示扩展器管理的资源集合。
//调度器会将这些资源的调度委托给扩展器处理。
//- ignorable：表示扩展器是否可以被忽略。如果为true，当扩展器出现错误时，调度器可以忽略该错误并继续进行调度。
//此外，代码还定义了一个常量DefaultExtenderTimeout，它表示默认的扩展器超时时间，单位为秒。

func makeTransport(config *schedulerapi.Extender) (http.RoundTripper, error) {
	var cfg restclient.Config
	if config.TLSConfig != nil {
		cfg.TLSClientConfig.Insecure = config.TLSConfig.Insecure
		cfg.TLSClientConfig.ServerName = config.TLSConfig.ServerName
		cfg.TLSClientConfig.CertFile = config.TLSConfig.CertFile
		cfg.TLSClientConfig.KeyFile = config.TLSConfig.KeyFile
		cfg.TLSClientConfig.CAFile = config.TLSConfig.CAFile
		cfg.TLSClientConfig.CertData = config.TLSConfig.CertData
		cfg.TLSClientConfig.KeyData = config.TLSConfig.KeyData
		cfg.TLSClientConfig.CAData = config.TLSConfig.CAData
	}
	if config.EnableHTTPS {
		hasCA := len(cfg.CAFile) &amp;gt; 0 || len(cfg.CAData) &amp;gt; 0
		if !hasCA {
			cfg.Insecure = true
		}
	}
	tlsConfig, err := restclient.TLSConfigFor(&amp;amp;cfg)
	if err != nil {
		return nil, err
	}
	if tlsConfig != nil {
		return utilnet.SetTransportDefaults(&amp;amp;http.Transport{
			TLSClientConfig: tlsConfig,
		}), nil
	}
	return utilnet.SetTransportDefaults(&amp;amp;http.Transport{}), nil
}

//该函数根据给定的schedulerapi.Extender配置创建一个http.RoundTripper传输对象。
//它首先将config.TLSConfig中的配置项复制到cfg.TLSClientConfig中，然后根据config.EnableHTTPS的值设置cfg.Insecure。
//接下来，它通过调用restclient.TLSConfigFor(&amp;amp;cfg)来获取TLS配置，并将其设置到http.Transport中。
//最后，返回设置了默认值的http.Transport对象。如果在过程中出现错误，则返回错误。

// NewHTTPExtender creates an HTTPExtender object.
func NewHTTPExtender(config *schedulerapi.Extender) (framework.Extender, error) {
	if config.HTTPTimeout.Duration.Nanoseconds() == 0 {
		config.HTTPTimeout.Duration = time.Duration(DefaultExtenderTimeout)
	}

	transport, err := makeTransport(config)
	if err != nil {
		return nil, err
	}
	client := &amp;amp;http.Client{
		Transport: transport,
		Timeout: config.HTTPTimeout.Duration,
	}
	managedResources := sets.New[string]()
	for _, r := range config.ManagedResources {
		managedResources.Insert(string(r.Name))
	}
	return &amp;amp;HTTPExtender{
		extenderURL: config.URLPrefix,
		preemptVerb: config.PreemptVerb,
		filterVerb: config.FilterVerb,
		prioritizeVerb: config.PrioritizeVerb,
		bindVerb: config.BindVerb,
		weight: config.Weight,
		client: client,
		nodeCacheCapable: config.NodeCacheCapable,
		managedResources: managedResources,
		ignorable: config.Ignorable,
	}, nil
}

//该函数用于创建一个HTTPExtender对象。它根据传入的config参数初始化HTTPExtender对象的属性，并返回该对象。
//其中，makeTransport函数用于创建一个http.Transport对象，http.Client对象则使用该Transport对象以及config参数中的HTTP超时时间进行初始化。
//此外，该函数还对config.ManagedResources进行遍历，将其中的Name字段转换为string类型，并插入到managedResources集合中。
//最后，该函数返回一个初始化完成的HTTPExtender对象。

// Name returns extenderURL to identify the extender.
func (h *HTTPExtender) Name() string {
	return h.extenderURL
}

// IsIgnorable returns true indicates scheduling should not fail when this extender
// is unavailable
func (h *HTTPExtender) IsIgnorable() bool {
	return h.ignorable
}

// SupportsPreemption returns true if an extender supports preemption.
// An extender should have preempt verb defined and enabled its own node cache.
func (h *HTTPExtender) SupportsPreemption() bool {
	return len(h.preemptVerb) &amp;gt; 0
}

//这段代码定义了一个名为HTTPExtender的结构体及其三个方法。
//1. Name()方法返回extenderURL，用于标识扩展程序。
//2. IsIgnorable()方法返回一个布尔值，指示当此扩展程序不可用时，调度是否不应失败。如果ignorable字段为true，则返回true。
//3. SupportsPreemption()方法返回一个布尔值，表示扩展程序是否支持抢占。
//如果preemptVerb字段的长度大于0，则返回true。
//这意味着扩展程序应该定义抢占动词并启用自己的节点缓存。

// ProcessPreemption returns filtered candidate nodes and victims after running preemption logic in extender.
func (h *HTTPExtender) ProcessPreemption(
	pod *v1.Pod,
	nodeNameToVictims map[string]*extenderv1.Victims,
	nodeInfos framework.NodeInfoLister,
) (map[string]*extenderv1.Victims, error) {
	var (
		result extenderv1.ExtenderPreemptionResult
		args *extenderv1.ExtenderPreemptionArgs
	)

	if !h.SupportsPreemption() {
		return nil, fmt.Errorf(&amp;#34;preempt verb is not defined for extender %v but run into ProcessPreemption&amp;#34;, h.extenderURL)
	}

	if h.nodeCacheCapable {
		// If extender has cached node info, pass NodeNameToMetaVictims in args.
		nodeNameToMetaVictims := convertToMetaVictims(nodeNameToVictims)
		args = &amp;amp;extenderv1.ExtenderPreemptionArgs{
			Pod: pod,
			NodeNameToMetaVictims: nodeNameToMetaVictims,
		}
	} else {
		args = &amp;amp;extenderv1.ExtenderPreemptionArgs{
			Pod: pod,
			NodeNameToVictims: nodeNameToVictims,
		}
	}

	if err := h.send(h.preemptVerb, args, &amp;amp;result); err != nil {
		return nil, err
	}

	// Extender will always return NodeNameToMetaVictims.
	// So let&amp;#39;s convert it to NodeNameToVictims by using &amp;lt;nodeInfos&amp;gt;.
	newNodeNameToVictims, err := h.convertToVictims(result.NodeNameToMetaVictims, nodeInfos)
	if err != nil {
		return nil, err
	}
	// Do not override &amp;lt;nodeNameToVictims&amp;gt;.
	return newNodeNameToVictims, nil
}

//该函数是用于处理抢占逻辑的HTTP扩展器方法。
//它根据传入的Pod和节点信息，通过调用扩展器的抢占逻辑，返回过滤后的候选节点和受害者。
//- 首先，函数检查扩展器是否支持抢占。如果不支持，则返回错误。
//- 接下来，根据扩展器是否具有缓存的节点信息，构建不同的ExtenderPreemptionArgs对象，其中包含Pod和节点信息。
//- 然后，通过调用扩展器的指定动词（preempt），将ExtenderPreemptionArgs发送给扩展器，并将结果存储在ExtenderPreemptionResult中。
//- 最后，将扩展器返回的NodeNameToMetaVictims转换为NodeNameToVictims，并返回新的NodeNameToVictims对象。
//该函数返回的地图map[string]*extenderv1.Victims表示了每个节点的受害者列表，其中每个受害者包含了被抢占的Pod列表。

// convertToVictims converts &amp;#34;nodeNameToMetaVictims&amp;#34; from object identifiers,
// such as UIDs and names, to object pointers.
func (h *HTTPExtender) convertToVictims(
	nodeNameToMetaVictims map[string]*extenderv1.MetaVictims,
	nodeInfos framework.NodeInfoLister,
) (map[string]*extenderv1.Victims, error) {
	nodeNameToVictims := map[string]*extenderv1.Victims{}
	for nodeName, metaVictims := range nodeNameToMetaVictims {
		nodeInfo, err := nodeInfos.Get(nodeName)
		if err != nil {
			return nil, err
		}
		victims := &amp;amp;extenderv1.Victims{
			Pods: []*v1.Pod{},
			NumPDBViolations: metaVictims.NumPDBViolations,
		}
		for _, metaPod := range metaVictims.Pods {
			pod, err := h.convertPodUIDToPod(metaPod, nodeInfo)
			if err != nil {
				return nil, err
			}
			victims.Pods = append(victims.Pods, pod)
		}
		nodeNameToVictims[nodeName] = victims
	}
	return nodeNameToVictims, nil
}

//该函数是一个Go语言函数，它将从对象标识符（如UID和名称）组成的&amp;#34;nodeNameToMetaVictims&amp;#34;映射转换为对象指针组成的&amp;#34;nodeNameToVictims&amp;#34;映射。
//该函数使用给定的节点信息获取节点名称对应的受害者信息，并将其转换为包含Pod信息和PDB违规数量的Victims对象。
//具体来说，它遍历输入的映射，获取每个节点的MetaVictims对象，并通过节点信息获取节点的Pod信息。
//然后，它将MetaPod对象转换为Pod对象，并将其添加到Victims对象的Pod列表中。
//最后，它将转换后的Victims对象添加到输出映射中，并返回该映射。如果在转换过程中发生错误，函数将返回错误。

// convertPodUIDToPod returns v1.Pod object for given MetaPod and node info.
// The v1.Pod object is restored by nodeInfo.Pods().
// It returns an error if there&amp;#39;s cache inconsistency between default scheduler
// and extender, i.e. when the pod is not found in nodeInfo.Pods.
func (h *HTTPExtender) convertPodUIDToPod(
	metaPod *extenderv1.MetaPod,
	nodeInfo *framework.NodeInfo) (*v1.Pod, error) {
	for _, p := range nodeInfo.Pods {
		if string(p.Pod.UID) == metaPod.UID {
			return p.Pod, nil
		}
	}
	return nil, fmt.Errorf(&amp;#34;extender: %v claims to preempt pod (UID: %v) on node: %v, but the pod is not found on that node&amp;#34;,
		h.extenderURL, metaPod, nodeInfo.Node().Name)
}

// convertToMetaVictims converts from struct type to meta types.
func convertToMetaVictims(
	nodeNameToVictims map[string]*extenderv1.Victims,
) map[string]*extenderv1.MetaVictims {
	nodeNameToMetaVictims := map[string]*extenderv1.MetaVictims{}
	for node, victims := range nodeNameToVictims {
		metaVictims := &amp;amp;extenderv1.MetaVictims{
			Pods: []*extenderv1.MetaPod{},
			NumPDBViolations: victims.NumPDBViolations,
		}
		for _, pod := range victims.Pods {
			metaPod := &amp;amp;extenderv1.MetaPod{
				UID: string(pod.UID),
			}
			metaVictims.Pods = append(metaVictims.Pods, metaPod)
		}
		nodeNameToMetaVictims[node] = metaVictims
	}
	return nodeNameToMetaVictims
}

//该函数的功能是将从结构体类型转换为元类型。
//它接收一个nodeNameToVictims参数，该参数是一个map，其中键是节点名称，值是*extenderv1.Victims类型的指针。
//函数创建一个空的nodeNameToMetaVictims映射，然后遍历nodeNameToVictims中的每个元素。
//对于每个节点，它创建一个新的extenderv1.MetaVictims实例，并将NumPDBViolations从旧的victims实例复制到新的metaVictims实例中。
//然后，它遍历旧的victims实例中的每个Pod，为每个Pod创建一个新的extenderv1.MetaPod实例，并将UID从旧的pod实例复制到新的metaPod实例中。
//最后，它将新的metaVictims实例添加到nodeNameToMetaVictims映射中，并返回该映射。

// Filter based on extender implemented predicate functions. The filtered list is
// expected to be a subset of the supplied list; otherwise the function returns an error.
// The failedNodes and failedAndUnresolvableNodes optionally contains the list
// of failed nodes and failure reasons, except nodes in the latter are
// unresolvable.
func (h *HTTPExtender) Filter(
	pod *v1.Pod,
	nodes []*framework.NodeInfo,
) (filteredList []*framework.NodeInfo, failedNodes, failedAndUnresolvableNodes extenderv1.FailedNodesMap, err error) {
	var (
		result extenderv1.ExtenderFilterResult
		nodeList *v1.NodeList
		nodeNames *[]string
		nodeResult []*framework.NodeInfo
		args *extenderv1.ExtenderArgs
	)
	fromNodeName := make(map[string]*framework.NodeInfo)
	for _, n := range nodes {
		fromNodeName[n.Node().Name] = n
	}

	if h.filterVerb == &amp;#34;&amp;#34; {
		return nodes, extenderv1.FailedNodesMap{}, extenderv1.FailedNodesMap{}, nil
	}

	if h.nodeCacheCapable {
		nodeNameSlice := make([]string, 0, len(nodes))
		for _, node := range nodes {
			nodeNameSlice = append(nodeNameSlice, node.Node().Name)
		}
		nodeNames = &amp;amp;nodeNameSlice
	} else {
		nodeList = &amp;amp;v1.NodeList{}
		for _, node := range nodes {
			nodeList.Items = append(nodeList.Items, *node.Node())
		}
	}

	args = &amp;amp;extenderv1.ExtenderArgs{
		Pod: pod,
		Nodes: nodeList,
		NodeNames: nodeNames,
	}

	if err := h.send(h.filterVerb, args, &amp;amp;result); err != nil {
		return nil, nil, nil, err
	}
	if result.Error != &amp;#34;&amp;#34; {
		return nil, nil, nil, fmt.Errorf(result.Error)
	}

	if h.nodeCacheCapable &amp;amp;&amp;amp; result.NodeNames != nil {
		nodeResult = make([]*framework.NodeInfo, len(*result.NodeNames))
		for i, nodeName := range *result.NodeNames {
			if n, ok := fromNodeName[nodeName]; ok {
				nodeResult[i] = n
			} else {
				return nil, nil, nil, fmt.Errorf(
					&amp;#34;extender %q claims a filtered node %q which is not found in the input node list&amp;#34;,
					h.extenderURL, nodeName)
			}
		}
	} else if result.Nodes != nil {
		nodeResult = make([]*framework.NodeInfo, len(result.Nodes.Items))
		for i := range result.Nodes.Items {
			nodeResult[i] = framework.NewNodeInfo()
			nodeResult[i].SetNode(&amp;amp;result.Nodes.Items[i])
		}
	}

	return nodeResult, result.FailedNodes, result.FailedAndUnresolvableNodes, nil
}

//该函数是一个过滤函数，基于扩展器实现的谓词函数对节点进行过滤。
//函数的输入是一个Pod对象和一个节点信息数组，输出是过滤后的节点信息数组、失败的节点信息映射、不可解析的节点信息映射和错误信息。
//函数首先通过遍历输入节点信息数组，创建一个从节点名称到节点信息的映射。
//然后根据扩展器的过滤动词，决定使用节点名称列表还是节点列表作为输入参数，构建ExtenderArgs对象，并调用扩展器的过滤方法。
//如果过滤方法返回错误，函数直接返回错误。如果过滤方法成功，函数根据返回结果构建过滤后的节点信息数组，并返回。
//如果扩展器声明了一个节点被过滤，但是在输入节点信息数组中找不到该节点，则函数返回错误。
//如果扩展器返回的结果中包含失败的节点和不可解析的节点信息，则将其添加到相应的映射中。
//总之，该函数通过调用扩展器的过滤方法，对节点进行过滤，并返回过滤后的节点信息数组和相关错误信息。

// Prioritize based on extender implemented priority functions. Weight*priority is added
// up for each such priority function. The returned score is added to the score computed
// by Kubernetes scheduler. The total score is used to do the host selection.
func (h *HTTPExtender) Prioritize(pod *v1.Pod, nodes []*framework.NodeInfo) (*extenderv1.HostPriorityList, int64, error) {
	var (
		result extenderv1.HostPriorityList
		nodeList *v1.NodeList
		nodeNames *[]string
		args *extenderv1.ExtenderArgs
	)

	if h.prioritizeVerb == &amp;#34;&amp;#34; {
		result := extenderv1.HostPriorityList{}
		for _, node := range nodes {
			result = append(result, extenderv1.HostPriority{Host: node.Node().Name, Score: 0})
		}
		return &amp;amp;result, 0, nil
	}

	if h.nodeCacheCapable {
		nodeNameSlice := make([]string, 0, len(nodes))
		for _, node := range nodes {
			nodeNameSlice = append(nodeNameSlice, node.Node().Name)
		}
		nodeNames = &amp;amp;nodeNameSlice
	} else {
		nodeList = &amp;amp;v1.NodeList{}
		for _, node := range nodes {
			nodeList.Items = append(nodeList.Items, *node.Node())
		}
	}

	args = &amp;amp;extenderv1.ExtenderArgs{
		Pod: pod,
		Nodes: nodeList,
		NodeNames: nodeNames,
	}

	if err := h.send(h.prioritizeVerb, args, &amp;amp;result); err != nil {
		return nil, 0, err
	}
	return &amp;amp;result, h.weight, nil
}

//该函数是一个Go语言函数，定义在HTTPExtender结构体中，用于根据扩展程序实现的优先级函数对节点进行优先级排序。
//该函数将权重乘以优先级的总和添加到每个此类优先级函数中。
//返回的分数将添加到Kubernetes调度程序计算的分数中。总
//分数用于进行主机选择。
//函数接受一个*v1.Pod类型的pod参数，一个[]*framework.NodeInfo类型的nodes参数，以及一个error类型的结果参数。
//函数首先定义了一些局部变量，包括一个extenderv1.HostPriorityList类型的result变量，
//一个*v1.NodeList类型的nodeList变量，一个*[]string类型的nodeNames变量，以及一个*extenderv1.ExtenderArgs类型的args变量。
//如果h.prioritizeVerb为空字符串，则将每个节点的分数设置为0，并返回结果。
//如果h.nodeCacheCapable为true，则将节点名称添加到nodeNameSlice切片中，并将其赋值给nodeNames变量。
//否则，将节点对象追加到nodeList的Items字段中。
//接下来，将pod、nodeList和nodeNames赋值给args的相应字段。
//最后，调用h.send方法，将h.prioritizeVerb、args和&amp;amp;result作为参数传递，并检查是否有错误发生。
//如果有错误，则返回错误。否则，返回结果和权重。

// Bind delegates the action of binding a pod to a node to the extender.
func (h *HTTPExtender) Bind(binding *v1.Binding) error {
	var result extenderv1.ExtenderBindingResult
	if !h.IsBinder() {
		// This shouldn&amp;#39;t happen as this extender wouldn&amp;#39;t have become a Binder.
		return fmt.Errorf(&amp;#34;unexpected empty bindVerb in extender&amp;#34;)
	}
	req := &amp;amp;extenderv1.ExtenderBindingArgs{
		PodName: binding.Name,
		PodNamespace: binding.Namespace,
		PodUID: binding.UID,
		Node: binding.Target.Name,
	}
	if err := h.send(h.bindVerb, req, &amp;amp;result); err != nil {
		return err
	}
	if result.Error != &amp;#34;&amp;#34; {
		return fmt.Errorf(result.Error)
	}
	return nil
}

// IsBinder returns whether this extender is configured for the Bind method.
func (h *HTTPExtender) IsBinder() bool {
	return h.bindVerb != &amp;#34;&amp;#34;
}

// IsPrioritizer returns whether this extender is configured for the Prioritize method.
func (h *HTTPExtender) IsPrioritizer() bool {
	return h.prioritizeVerb != &amp;#34;&amp;#34;
}

// IsFilter returns whether this extender is configured for the Filter method.
func (h *HTTPExtender) IsFilter() bool {
	return h.filterVerb != &amp;#34;&amp;#34;
}

//这段代码定义了一个名为HTTPExtender的结构体及其相关方法。
//这个结构体用于委托将Pod绑定到节点的操作给扩展器。
//- Bind方法用于将Pod绑定到节点
//。它首先检查当前扩展器是否配置为绑定扩展器，如果不是，则返回错误。
//然后创建一个ExtenderBindingArgs请求，包含Pod的名称、命名空间、UID以及目标节点的名称，并通过调用send方法将请求发送给扩展器。
//如果发送请求或处理结果出现错误，则返回相应的错误。
//- IsBinder方法用于判断当前扩展器是否配置了绑定方法。如果bindVerb不为空，则表示配置了绑定方法。
//- IsPrioritizer方法用于判断当前扩展器是否配置了优先级方法。如果prioritizeVerb不为空，则表示配置了优先级方法。
//- IsFilter方法用于判断当前扩展器是否配置了过滤方法。如果filterVerb不为空，则表示配置了过滤方法。

// Helper function to send messages to the extender
func (h *HTTPExtender) send(action string, args interface{}, result interface{}) error {
	out, err := json.Marshal(args)
	if err != nil {
		return err
	}

	url := strings.TrimRight(h.extenderURL, &amp;#34;/&amp;#34;) + &amp;#34;/&amp;#34; + action

	req, err := http.NewRequest(&amp;#34;POST&amp;#34;, url, bytes.NewReader(out))
	if err != nil {
		return err
	}

	req.Header.Set(&amp;#34;Content-Type&amp;#34;, &amp;#34;application/json&amp;#34;)

	resp, err := h.client.Do(req)
	if err != nil {
		return err
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		return fmt.Errorf(&amp;#34;failed %v with extender at URL %v, code %v&amp;#34;, action, url, resp.StatusCode)
	}

	return json.NewDecoder(resp.Body).Decode(result)
}

//该函数是一个发送消息给扩展程序的辅助函数。
//它使用HTTP POST请求将动作、参数和结果发送到指定的扩展程序URL。
//具体步骤如下：
//1. 将参数args转换为JSON格式。
//2. 构建请求URL，通过拼接扩展程序URL和动作字符串。
//3. 创建HTTP POST请求，设置请求头的Content-Type为application/json，并将JSON数据作为请求体。
//4. 发送请求并获取响应。
//5. 检查响应状态码，如果不是200 OK，则返回错误。
//6. 将响应体解码为结果参数result。 如果在上述过程中发生错误，则返回相应的错误。

// IsInterested returns true if at least one extended resource requested by
// this pod is managed by this extender.
func (h *HTTPExtender) IsInterested(pod *v1.Pod) bool {
	if h.managedResources.Len() == 0 {
		return true
	}
	if h.hasManagedResources(pod.Spec.Containers) {
		return true
	}
	if h.hasManagedResources(pod.Spec.InitContainers) {
		return true
	}
	return false
}

//该函数是一个Go语言函数，名为IsInterested，它属于HTTPExtender类型。
//函数用于判断给定的Pod是否至少请求了一个由该extender管理的扩展资源。
//函数返回一个布尔值，如果Pod请求了至少一个由该extender管理的扩展资源，则返回true，否则返回false。
//函数主要包含以下两个步骤：
//1. 首先，函数检查HTTPExtender的managedResources列表是否为空。如果为空，则表示该extender管理所有资源，因此直接返回true。
//2. 如果managedResources不为空，函数会分别检查Pod的Spec.Containers和Spec.InitContainers字段中是否包含由该extender管理的扩展资源。
//如果存在至少一个由extender管理的扩展资源，则返回true。如果两个字段中都不包含由extender管理的扩展资源，则返回false。
//总结：该函数用于判断给定的Pod是否请求了由该extender管理的扩展资源，根据管理资源列表和Pod的容器配置进行匹配判断。

func (h *HTTPExtender) hasManagedResources(containers []v1.Container) bool {
	for i := range containers {
		container := &amp;amp;containers[i]
		for resourceName := range container.Resources.Requests {
			if h.managedResources.Has(string(resourceName)) {
				return true
			}
		}
		for resourceName := range container.Resources.Limits {
			if h.managedResources.Has(string(resourceName)) {
				return true
			}
		}
	}
	return false
}

//该函数用于判断给定的容器列表中是否包含有管理资源。
//具体实现为遍历容器列表，再遍历容器的资源请求和限制，
//若存在管理资源则返回true，否则返回false。
&lt;/code>&lt;/pre></description></item><item><title>2024-04-10 K8S控制器之stateful_pod_control.go源码解读</title><link>https://qq547475331.github.io/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Bstateful_pod_controlgo%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-statefulpodcontrolgo-yuan-ma-jie-du/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Bstateful_pod_controlgo%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-statefulpodcontrolgo-yuan-ma-jie-du/</guid><description>&lt;pre tabindex="0">&lt;code>/*
Copyright 2016 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the &amp;#34;License&amp;#34;);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

 http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &amp;#34;AS IS&amp;#34; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package statefulset

import (
	&amp;#34;context&amp;#34;
	&amp;#34;fmt&amp;#34;
	&amp;#34;strings&amp;#34;

	apps &amp;#34;k8s.io/api/apps/v1&amp;#34;
	v1 &amp;#34;k8s.io/api/core/v1&amp;#34;
	apierrors &amp;#34;k8s.io/apimachinery/pkg/api/errors&amp;#34;
	metav1 &amp;#34;k8s.io/apimachinery/pkg/apis/meta/v1&amp;#34;
	errorutils &amp;#34;k8s.io/apimachinery/pkg/util/errors&amp;#34;
	utilruntime &amp;#34;k8s.io/apimachinery/pkg/util/runtime&amp;#34;
	utilfeature &amp;#34;k8s.io/apiserver/pkg/util/feature&amp;#34;
	clientset &amp;#34;k8s.io/client-go/kubernetes&amp;#34;
	corelisters &amp;#34;k8s.io/client-go/listers/core/v1&amp;#34;
	&amp;#34;k8s.io/client-go/tools/record&amp;#34;
	&amp;#34;k8s.io/client-go/util/retry&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/features&amp;#34;
)

// StatefulPodControlObjectManager abstracts the manipulation of Pods and PVCs. The real controller implements this
// with a clientset for writes and listers for reads; for tests we provide stubs.
type StatefulPodControlObjectManager interface {
	CreatePod(ctx context.Context, pod *v1.Pod) error
	GetPod(namespace, podName string) (*v1.Pod, error)
	UpdatePod(pod *v1.Pod) error
	DeletePod(pod *v1.Pod) error
	CreateClaim(claim *v1.PersistentVolumeClaim) error
	GetClaim(namespace, claimName string) (*v1.PersistentVolumeClaim, error)
	UpdateClaim(claim *v1.PersistentVolumeClaim) error
}

// 这是一个StatefulPodControlObjectManager接口，定义了对有状态Pod和持久卷声明的操作方法，包括创建、获取、更新和删除Pod以及创建、获取、更新持久卷声明。
// StatefulPodControl defines the interface that StatefulSetController uses to create, update, and delete Pods,
// and to update the Status of a StatefulSet. It follows the design paradigms used for PodControl, but its
// implementation provides for PVC creation, ordered Pod creation, ordered Pod termination, and Pod identity enforcement.
// Manipulation of objects is provided through objectMgr, which allows the k8s API to be mocked out for testing.
type StatefulPodControl struct {
	objectMgr StatefulPodControlObjectManager
	recorder record.EventRecorder
}

//该代码定义了一个名为StatefulPodControl的结构体，它是一个接口，用于创建、更新和删除Pods，以及更新StatefulSet的状态。
//它采用了与PodControl相同的设计模式，但其实现提供了PVC创建、有序Pod创建、有序Pod终止和Pod身份强制执行等功能。
//通过objectMgr对象提供对对象的操作，允许在测试中模拟k8s API。

// NewStatefulPodControl constructs a StatefulPodControl using a realStatefulPodControlObjectManager with the given
// clientset, listers and EventRecorder.
func NewStatefulPodControl(
	client clientset.Interface,
	podLister corelisters.PodLister,
	claimLister corelisters.PersistentVolumeClaimLister,
	recorder record.EventRecorder,
) *StatefulPodControl {
	return &amp;amp;StatefulPodControl{&amp;amp;realStatefulPodControlObjectManager{client, podLister, claimLister}, recorder}
}

// 该函数用于构造一个StatefulPodControl对象，使用给定的clientset、listers和EventRecorder。
// 函数通过传入的参数创建一个realStatefulPodControlObjectManager对象，并将其与传入的EventRecorder对象一起封装到StatefulPodControl对象中，最后返回该对象。
// NewStatefulPodControlFromManager creates a StatefulPodControl using the given StatefulPodControlObjectManager and recorder.
func NewStatefulPodControlFromManager(om StatefulPodControlObjectManager, recorder record.EventRecorder) *StatefulPodControl {
	return &amp;amp;StatefulPodControl{om, recorder}
}

// 该函数使用给定的StatefulPodControlObjectManager和record.EventRecorder创建一个StatefulPodControl，并返回其指针。
// realStatefulPodControlObjectManager uses a clientset.Interface and listers.
type realStatefulPodControlObjectManager struct {
	client clientset.Interface
	podLister corelisters.PodLister
	claimLister corelisters.PersistentVolumeClaimLister
}

// 该代码定义了一个名为realStatefulPodControlObjectManager的结构体，它使用了clientset.Interface和listers。
// 结构体中有三个字段：client、podLister和claimLister，分别用于存储客户端接口、Pod列表和持久卷申领列表。
func (om *realStatefulPodControlObjectManager) CreatePod(ctx context.Context, pod *v1.Pod) error {
	_, err := om.client.CoreV1().Pods(pod.Namespace).Create(ctx, pod, metav1.CreateOptions{})
	return err
}

// 该函数是一个Go语言函数，它使用了上下文（Context）来控制请求的超时和取消。
// 该函数的功能是在指定的命名空间中创建一个Pod，并返回创建操作的结果（成功或错误）。
// 具体来说，该函数的实现步骤如下：
// 1. 使用om.client获取CoreV1接口，该接口提供了对Kubernetes集群中Pod资源的操作方法。
// 2. 调用Create方法，在指定的命名空间中创建Pod，并传入上下文（Context）和Pod对象。
// 3. 返回创建操作的结果，即错误信息（Error）。
// 需要注意的是，该函数中使用了context.Context参数，它允许调用者控制请求的超时和取消。在实际应用中，这非常有用，比如在处理大量请求或需要及时响应的场景中。
func (om *realStatefulPodControlObjectManager) GetPod(namespace, podName string) (*v1.Pod, error) {
	return om.podLister.Pods(namespace).Get(podName)
}

// 该函数是一个Go语言的方法，定义在一个名为realStatefulPodControlObjectManager的结构体类型上。该方法的功能是在指定的命名空间
// (namespace)中获取指定名称(podName)的Pod对象，并返回该Pod对象及其可能发生的错误。 具体实现上，
// 该方法通过调用om.podLister.Pods(namespace)来获取指定命名空间下的所有Pods的列表器，
// 然后进一步调用Get(podName)方法来获取指定名称的Pod对象。如果获取成功，则将该Pod对象返回；
// 如果获取失败，则将发生的错误返回。
func (om *realStatefulPodControlObjectManager) UpdatePod(pod *v1.Pod) error {
	_, err := om.client.CoreV1().Pods(pod.Namespace).Update(context.TODO(), pod, metav1.UpdateOptions{})
	return err
}

// 该函数用于更新指定命名空间下的Pod。函数接收一个指向v1.Pod类型的指针作为参数，通过调用om.client.CoreV1().Pods
// (pod.Namespace).Update()方法，将该Pod对象更新至Kubernetes集群中。若更新成功，则返回nil；若更新失败，
// 则返回相应的错误信息。
func (om *realStatefulPodControlObjectManager) DeletePod(pod *v1.Pod) error {
	return om.client.CoreV1().Pods(pod.Namespace).Delete(context.TODO(), pod.Name, metav1.DeleteOptions{})
}

func (om *realStatefulPodControlObjectManager) CreateClaim(claim *v1.PersistentVolumeClaim) error {
	_, err := om.client.CoreV1().PersistentVolumeClaims(claim.Namespace).Create(context.TODO(), claim, metav1.CreateOptions{})
	return err
}

// 该函数用于删除指定命名空间下的Pod。函数接收一个指向v1.Pod类型的指针作为参数，通过调用om.client.CoreV1().Pods
// (pod.Namespace).Delete()方法，根据传入的Pod的Namespace和Name属性，在Kubernetes集群中删除该Pod实例。
// 若删除成功，则返回nil；否则，返回相应的错误信息。
func (om *realStatefulPodControlObjectManager) GetClaim(namespace, claimName string) (*v1.PersistentVolumeClaim, error) {
	return om.claimLister.PersistentVolumeClaims(namespace).Get(claimName)
}

//该函数用于获取指定命名空间下名为claimName的PersistentVolumeClaim资源。函数接收两个字符串参数：namespace（命名空间）和claimName
//（PersistentVolumeClaim名称）。它通过调用om.claimLister.PersistentVolumeClaims(namespace).Get(claimName)从缓存列表中获取指定的
//PersistentVolumeClaim对象，并将其以*v1.PersistentVolumeClaim类型返回。如果获取成功，则返回PersistentVolumeClaim实例及其nil错误；
//如果未能找到对应资源，则返回nil及可能的错误信息。

func (om *realStatefulPodControlObjectManager) UpdateClaim(claim *v1.PersistentVolumeClaim) error {
	_, err := om.client.CoreV1().PersistentVolumeClaims(claim.Namespace).Update(context.TODO(), claim, metav1.UpdateOptions{})
	return err
}

//该函数用于更新指定命名空间下的PersistentVolumeClaim资源。函数接收一个指向v1.PersistentVolumeClaim类型的指针作为参数。
//通过调用om.client.CoreV1().PersistentVolumeClaims(claim.Namespace).Update()方法，根据传入的PersistentVolumeClaim对象的Namespace属性，
//在Kubernetes集群中更新该PersistentVolumeClaim实例。若更新操作成功，则返回nil；若出现错误，则返回相应的错误信息。

func (spc *StatefulPodControl) CreateStatefulPod(ctx context.Context, set *apps.StatefulSet, pod *v1.Pod) error {
	// Create the Pod&amp;#39;s PVCs prior to creating the Pod
	if err := spc.createPersistentVolumeClaims(set, pod); err != nil {
		spc.recordPodEvent(&amp;#34;create&amp;#34;, set, pod, err)
		return err
	}
	// If we created the PVCs attempt to create the Pod
	err := spc.objectMgr.CreatePod(ctx, pod)
	// sink already exists errors
	if apierrors.IsAlreadyExists(err) {
		return err
	}
	if utilfeature.DefaultFeatureGate.Enabled(features.StatefulSetAutoDeletePVC) {
		// Set PVC policy as much as is possible at this point.
		if err := spc.UpdatePodClaimForRetentionPolicy(ctx, set, pod); err != nil {
			spc.recordPodEvent(&amp;#34;update&amp;#34;, set, pod, err)
			return err
		}
	}
	spc.recordPodEvent(&amp;#34;create&amp;#34;, set, pod, err)
	return err
}

// 该函数用于创建有状态的Pod。在创建Pod之前，它会先创建Pod所需的PVC（持久卷声明）。如果创建PVC时出现错误，则会记录Pod事件并返回错误。
// 接下来，它会尝试创建Pod，如果Pod已存在，则直接返回错误。如果启用了StatefulSetAutoDeletePVC功能，则会更新Pod的PVC保留策略。
// 最后，它会记录Pod的创建事件并返回可能的错误
func (spc *StatefulPodControl) UpdateStatefulPod(ctx context.Context, set *apps.StatefulSet, pod *v1.Pod) error {
	attemptedUpdate := false
	err := retry.RetryOnConflict(retry.DefaultBackoff, func() error {
		// assume the Pod is consistent
		consistent := true
		// if the Pod does not conform to its identity, update the identity and dirty the Pod
		if !identityMatches(set, pod) {
			updateIdentity(set, pod)
			consistent = false
		}
		//该函数是一个Go语言函数，它用于更新一个有状态的Pod（StatefulPod）的状态。函数的参数包括一个上下文对象ctx、一个StatefulSet对象set和一个Pod对象pod。
		//函数通过使用retry.RetryOnConflict方法，在遇到冲突时进行重试，以更新Pod的状态。在重试的过程中，
		//函数会检查Pod是否符合其身份要求和StatefulSet的存储要求。如果不符合，函数会相应地更新Pod的身份和PVCs，并标记Pod为“dirty”。
		//如果在更新过程中出现错误，则会返回错误信息。
		// if the Pod does not conform to the StatefulSet&amp;#39;s storage requirements, update the Pod&amp;#39;s PVC&amp;#39;s,
		// dirty the Pod, and create any missing PVCs
		if !storageMatches(set, pod) {
			updateStorage(set, pod)
			consistent = false
			if err := spc.createPersistentVolumeClaims(set, pod); err != nil {
				spc.recordPodEvent(&amp;#34;update&amp;#34;, set, pod, err)
				return err
			}
		}
		//该函数主要检查Pod是否符合StatefulSet的存储要求，如果不符合，则更新Pod的PVCs，标记Pod为dirty，并创建任何缺失的PVCs。
		//具体流程如下： 1. 检查Pod是否符合StatefulSet的存储要求，调用storageMatches(set, pod)函数。
		//2. 如果Pod不符合存储要求，则执行以下操作： - 调用updateStorage(set, pod)函数更新Pod的PVCs。 - 将consistent标记为false。 -
		//调用spc.createPersistentVolumeClaims(set, pod)函数创建任何缺失的PVCs，如果创建失败，则记录Pod事件并返回错误。
		//3. 如果Pod符合存储要求，则继续后续流程。
		if utilfeature.DefaultFeatureGate.Enabled(features.StatefulSetAutoDeletePVC) {
			// if the Pod&amp;#39;s PVCs are not consistent with the StatefulSet&amp;#39;s PVC deletion policy, update the PVC
			// and dirty the pod.
			if match, err := spc.ClaimsMatchRetentionPolicy(ctx, set, pod); err != nil {
				spc.recordPodEvent(&amp;#34;update&amp;#34;, set, pod, err)
				return err
			} else if !match {
				if err := spc.UpdatePodClaimForRetentionPolicy(ctx, set, pod); err != nil {
					spc.recordPodEvent(&amp;#34;update&amp;#34;, set, pod, err)
					return err
				}
				consistent = false
			}
		}
		//这段Go代码是关于StatefulSet自动删除PVC的逻辑。 首先，检查是否启用了StatefulSetAutoDeletePVC功能门。
		//如果启用了，则进一步判断Pod的PVC是否与StatefulSet的PVC删除策略一致。如果不一致，则更新PVC并标记Pod为dirty。
		//如果匹配失败，则调用UpdatePodClaimForRetentionPolicy函数来更新Pod的PVC，并将consistent标记为false。
		//在更新过程中，如果出现错误，则记录Pod事件并返回错误。
		// if the Pod is not dirty, do nothing
		if consistent {
			return nil
		}

		attemptedUpdate = true
		// commit the update, retrying on conflicts

		updateErr := spc.objectMgr.UpdatePod(pod)
		if updateErr == nil {
			return nil
		}

		if updated, err := spc.objectMgr.GetPod(set.Namespace, pod.Name); err == nil {
			// make a copy so we don&amp;#39;t mutate the shared cache
			pod = updated.DeepCopy()
		} else {
			utilruntime.HandleError(fmt.Errorf(&amp;#34;error getting updated Pod %s/%s: %w&amp;#34;, set.Namespace, pod.Name, err))
		}
		//这段Go代码主要做的是更新Pod的操作。具体流程如下：
		//1. 首先判断consistent是否为true，如果是，则直接返回nil。
		//2. 设置attemptedUpdate为true，表示已经尝试更新Pod。
		//3. 调用spc.objectMgr.UpdatePod(pod)方法更新Pod，如果更新成功，则直接返回nil。
		//4. 如果更新失败，则尝试获取最新的Pod信息。
		//5. 如果获取成功，则将获取到的Pod深拷贝一份，防止对共享缓存的污染。
		//6. 如果获取失败，则打印错误信息。 整体来说，这段代码的逻辑比较简单，主要是通过调用UpdatePod方法更新Pod，如果更新失败则尝试重新获取Pod信息。
		return updateErr
	})
	if attemptedUpdate {
		spc.recordPodEvent(&amp;#34;update&amp;#34;, set, pod, err)
	}
	return err
}

// 这段Go代码是一个函数片段，它在一个匿名函数中执行了某种更新操作，并通过参数set和pod记录了事件。如果更新操作尝试过，
// 它会通过spc.recordPodEvent方法记录一个名为&amp;#34;update&amp;#34;的事件。最后，该函数返回一个错误值err。
func (spc *StatefulPodControl) DeleteStatefulPod(set *apps.StatefulSet, pod *v1.Pod) error {
	err := spc.objectMgr.DeletePod(pod)
	spc.recordPodEvent(&amp;#34;delete&amp;#34;, set, pod, err)
	return err
}

// 此函数用于删除一个有状态的Pod。它通过调用spc.objectMgr.DeletePod(pod)来删除指定的Pod，并记录事件。最后返回删除操作的错误（如果有）。
// ClaimsMatchRetentionPolicy returns false if the PVCs for pod are not consistent with set&amp;#39;s PVC deletion policy.
// An error is returned if something is not consistent. This is expected if the pod is being otherwise updated,
// but a problem otherwise (see usage of this method in UpdateStatefulPod).
func (spc *StatefulPodControl) ClaimsMatchRetentionPolicy(ctx context.Context, set *apps.StatefulSet, pod *v1.Pod) (bool, error) {
	logger := klog.FromContext(ctx)
	ordinal := getOrdinal(pod)
	templates := set.Spec.VolumeClaimTemplates
	for i := range templates {
		claimName := getPersistentVolumeClaimName(set, &amp;amp;templates[i], ordinal)
		claim, err := spc.objectMgr.GetClaim(set.Namespace, claimName)
		switch {
		case apierrors.IsNotFound(err):
			klog.FromContext(ctx).V(4).Info(&amp;#34;Expected claim missing, continuing to pick up in next iteration&amp;#34;, &amp;#34;PVC&amp;#34;, klog.KObj(claim))
		case err != nil:
			return false, fmt.Errorf(&amp;#34;Could not retrieve claim %s for %s when checking PVC deletion policy&amp;#34;, claimName, pod.Name)
		default:
			if !claimOwnerMatchesSetAndPod(logger, claim, set, pod) {
				return false, nil
			}
		}
	}
	return true, nil
}

//该函数用于检查Pod的PVC是否符合StatefulSet的PVC删除策略。
//它遍历StatefulSet的VolumeClaimTemplates，并根据模板生成PVC名称。
//然后它尝试获取该PVC，根据获取结果进行判断：
//- 如果PVC不存在，则记录日志并继续下一次迭代。
//- 如果获取PVC出现错误，则返回错误。
//- 如果PVC存在但其owner不是StatefulSet和Pod，则返回false。
//如果所有PVC都符合要求，则返回true。

// UpdatePodClaimForRetentionPolicy updates the PVCs used by pod to match the PVC deletion policy of set.
func (spc *StatefulPodControl) UpdatePodClaimForRetentionPolicy(ctx context.Context, set *apps.StatefulSet, pod *v1.Pod) error {
	logger := klog.FromContext(ctx)
	ordinal := getOrdinal(pod)
	templates := set.Spec.VolumeClaimTemplates
	for i := range templates {
		claimName := getPersistentVolumeClaimName(set, &amp;amp;templates[i], ordinal)
		claim, err := spc.objectMgr.GetClaim(set.Namespace, claimName)
		switch {
		case apierrors.IsNotFound(err):
			logger.V(4).Info(&amp;#34;Expected claim missing, continuing to pick up in next iteration&amp;#34;, &amp;#34;PVC&amp;#34;, klog.KObj(claim))
		case err != nil:
			return fmt.Errorf(&amp;#34;Could not retrieve claim %s not found for %s when checking PVC deletion policy: %w&amp;#34;, claimName, pod.Name, err)
		default:
			if !claimOwnerMatchesSetAndPod(logger, claim, set, pod) {
				claim = claim.DeepCopy() // Make a copy so we don&amp;#39;t mutate the shared cache.
				needsUpdate := updateClaimOwnerRefForSetAndPod(logger, claim, set, pod)
				if needsUpdate {
					err := spc.objectMgr.UpdateClaim(claim)
					if err != nil {
						return fmt.Errorf(&amp;#34;Could not update claim %s for delete policy ownerRefs: %w&amp;#34;, claimName, err)
					}
				}
			}
		}
	}
	return nil
}

//该函数用于更新StatefulSet中的Pod的PersistentVolumeClaim（PVC）的所有者引用，以确保它们与Pod和StatefulSet正确关联。
//它遍历StatefulSet的VolumeClaimTemplates，并根据Pod的序号生成相应的PVC名称。然后，它尝试获取该PVC，根据获取结果进行不同的处理：
//- 如果PVC不存在，则记录一条日志，并继续处理下一个PVC。
//- 如果获取PVC时出现错误，则返回错误。
//- 如果PVC存在但其所有者引用与Pod和StatefulSet不匹配，则创建PVC的深拷贝，并更新其所有者引用。
//如果更新后的PVC与原始PVC不同，则将其更新到Kubernetes集群中。 最终，如果所有PVC都已正确处理，则函数返回nil。

// PodClaimIsStale returns true for a stale PVC that should block pod creation. If the scaling
// policy is deletion, and a PVC has an ownerRef that does not match the pod, the PVC is stale. This
// includes pods whose UID has not been created.
func (spc *StatefulPodControl) PodClaimIsStale(set *apps.StatefulSet, pod *v1.Pod) (bool, error) {
	policy := getPersistentVolumeClaimRetentionPolicy(set)
	if policy.WhenScaled == apps.RetainPersistentVolumeClaimRetentionPolicyType {
		// PVCs are meant to be reused and so can&amp;#39;t be stale.
		return false, nil
	}
	for _, claim := range getPersistentVolumeClaims(set, pod) {
		pvc, err := spc.objectMgr.GetClaim(claim.Namespace, claim.Name)
		switch {
		case apierrors.IsNotFound(err):
			// If the claim doesn&amp;#39;t exist yet, it can&amp;#39;t be stale.
			continue
		case err != nil:
			return false, err
		case err == nil:
			// A claim is stale if it doesn&amp;#39;t match the pod&amp;#39;s UID, including if the pod has no UID.
			if hasStaleOwnerRef(pvc, pod) {
				return true, nil
			}
		}
	}
	return false, nil
}

// SstatefulSet和Pod对应的PersistentVolumeClaim该函数用于判断Pod所使用的PVC是否为旧的（stale），如果为旧的则会阻止Pod的创建。
// 在StatefulSet缩容策略为删除时，如果PVC的ownerRef与Pod不匹配，则认为PVC是旧的。
// 函数首先根据StatefulSet的设置获取PVC的保留策略，如果策略为保留（Retain），则PVC会被重用，不会被认为是旧的。
// 然后遍历获取与Pod相关的PVC，通过查询PVC的详细信息判断PVC是否与Pod的UID匹配，如果不匹配则认为PVC是旧的。
// 如果查询PVC时发生错误，则返回错误信息。最后，如果没有发现旧的PVC，则返回false。
// recordPodEvent records an event for verb applied to a Pod in a StatefulSet. If err is nil the generated event will
// have a reason of v1.EventTypeNormal. If err is not nil the generated event will have a reason of v1.EventTypeWarning.
func (spc *StatefulPodControl) recordPodEvent(verb string, set *apps.StatefulSet, pod *v1.Pod, err error) {
	if err == nil {
		reason := fmt.Sprintf(&amp;#34;Successful%s&amp;#34;, strings.Title(verb))
		message := fmt.Sprintf(&amp;#34;%s Pod %s in StatefulSet %s successful&amp;#34;,
			strings.ToLower(verb), pod.Name, set.Name)
		spc.recorder.Event(set, v1.EventTypeNormal, reason, message)
	} else {
		reason := fmt.Sprintf(&amp;#34;Failed%s&amp;#34;, strings.Title(verb))
		message := fmt.Sprintf(&amp;#34;%s Pod %s in StatefulSet %s failed error: %s&amp;#34;,
			strings.ToLower(verb), pod.Name, set.Name, err)
		spc.recorder.Event(set, v1.EventTypeWarning, reason, message)
	}
}

// 该函数是一个名为recordPodEvent的方法，它属于StatefulPodControl类型。
// 该方法用于记录关于Pod的事件，该Pod属于一个StatefulSet。根据err参数的值，生成的事件会有不同的原因，
// 如果err为nil，则事件原因为v1.EventTypeNormal，否则为v1.EventTypeWarning。
// 根据verb参数，会生成成功或失败的事件消息，并通过spc.recorder.Event方法记录事件。
// recordClaimEvent records an event for verb applied to the PersistentVolumeClaim of a Pod in a StatefulSet. If err is
// nil the generated event will have a reason of v1.EventTypeNormal. If err is not nil the generated event will have a
// reason of v1.EventTypeWarning.
func (spc *StatefulPodControl) recordClaimEvent(verb string, set *apps.StatefulSet, pod *v1.Pod, claim *v1.PersistentVolumeClaim, err error) {
	if err == nil {
		reason := fmt.Sprintf(&amp;#34;Successful%s&amp;#34;, strings.Title(verb))
		message := fmt.Sprintf(&amp;#34;%s Claim %s Pod %s in StatefulSet %s success&amp;#34;,
			strings.ToLower(verb), claim.Name, pod.Name, set.Name)
		spc.recorder.Event(set, v1.EventTypeNormal, reason, message)
	} else {
		reason := fmt.Sprintf(&amp;#34;Failed%s&amp;#34;, strings.Title(verb))
		message := fmt.Sprintf(&amp;#34;%s Claim %s for Pod %s in StatefulSet %s failed error: %s&amp;#34;,
			strings.ToLower(verb), claim.Name, pod.Name, set.Name, err)
		spc.recorder.Event(set, v1.EventTypeWarning, reason, message)
	}
}

// 该函数名为recordClaimEvent，用于记录StatefulSet中Pod的PersistentVolumeClaim的事件。
// 根据err是否为nil，生成的事件会有不同的原因，如果err为nil，则事件原因为v1.EventTypeNormal，否则为v1.EventTypeWarning。
// 函数通过spc.recorder.Event方法记录事件。
// createMissingPersistentVolumeClaims creates all of the required PersistentVolumeClaims for pod, and updates its retention policy
func (spc *StatefulPodControl) createMissingPersistentVolumeClaims(ctx context.Context, set *apps.StatefulSet, pod *v1.Pod) error {
	if err := spc.createPersistentVolumeClaims(set, pod); err != nil {
		return err
	}

	if utilfeature.DefaultFeatureGate.Enabled(features.StatefulSetAutoDeletePVC) {
		// Set PVC policy as much as is possible at this point.
		if err := spc.UpdatePodClaimForRetentionPolicy(ctx, set, pod); err != nil {
			spc.recordPodEvent(&amp;#34;update&amp;#34;, set, pod, err)
			return err
		}
	}
	return nil
}

//该函数是一个Go语言函数，用于创建缺失的PersistentVolumeClaims（PVCs）并更新其保留策略，适用于StatefulPodControl类型。
//函数首先调用createPersistentVolumeClaims函数创建所需的PVCs，如果创建失败，则返回错误。
//接下来，如果启用了StatefulSetAutoDeletePVC功能，则调用UpdatePodClaimForRetentionPolicy函数来更新PVC的保留策略。
//如果更新失败，则记录事件并返回错误。最后，如果上述步骤都成功，则返回nil。

// createPersistentVolumeClaims creates all of the required PersistentVolumeClaims for pod, which must be a member of
// set. If all of the claims for Pod are successfully created, the returned error is nil. If creation fails, this method
// may be called again until no error is returned, indicating the PersistentVolumeClaims for pod are consistent with
// set&amp;#39;s Spec.
func (spc *StatefulPodControl) createPersistentVolumeClaims(set *apps.StatefulSet, pod *v1.Pod) error {
	var errs []error
	for _, claim := range getPersistentVolumeClaims(set, pod) {
		pvc, err := spc.objectMgr.GetClaim(claim.Namespace, claim.Name)
		switch {
		case apierrors.IsNotFound(err):
			err := spc.objectMgr.CreateClaim(&amp;amp;claim)
			if err != nil {
				errs = append(errs, fmt.Errorf(&amp;#34;failed to create PVC %s: %s&amp;#34;, claim.Name, err))
			}
			if err == nil || !apierrors.IsAlreadyExists(err) {
				spc.recordClaimEvent(&amp;#34;create&amp;#34;, set, pod, &amp;amp;claim, err)
			}
		case err != nil:
			errs = append(errs, fmt.Errorf(&amp;#34;failed to retrieve PVC %s: %s&amp;#34;, claim.Name, err))
			spc.recordClaimEvent(&amp;#34;create&amp;#34;, set, pod, &amp;amp;claim, err)
		default:
			if pvc.DeletionTimestamp != nil {
				errs = append(errs, fmt.Errorf(&amp;#34;pvc %s is being deleted&amp;#34;, claim.Name))
			}
		}
		// TODO: Check resource requirements and accessmodes, update if necessary
	}
	return errorutils.NewAggregate(errs)
}

//该函数用于创建StatefulSet中的PersistentVolumeClaims（PVCs）。
//1. 遍历获取需要创建的PVCs。
//2. 对于每个PVC，先尝试从objectMgr中获取，根据获取结果进行不同处理：
//- 如果未找到，则创建该PVC，并记录事件。
//- 如果获取发生错误，则将错误信息记录并返回。
//- 如果已存在该PVC，检查其DeletionTimestamp是否为空，若非空则将错误信息记录并返回。
//3. 返回所有错误的聚合。 注意：该函数未完成，最后还有一行TODO注释，提示需要检查资源需求和访问模式并进行更新。
&lt;/code>&lt;/pre></description></item><item><title>2024-04-10 K8S控制器之stateful_set_control.go源码解读</title><link>https://qq547475331.github.io/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Bstateful_set_controlgo%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-statefulsetcontrolgo-yuan-ma-jie-du/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Bstateful_set_controlgo%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-statefulsetcontrolgo-yuan-ma-jie-du/</guid><description>&lt;pre tabindex="0">&lt;code>/*
Copyright 2016 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the &amp;#34;License&amp;#34;);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

 http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &amp;#34;AS IS&amp;#34; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package statefulset

import (
	&amp;#34;context&amp;#34;
	&amp;#34;sort&amp;#34;
	&amp;#34;sync&amp;#34;

	apps &amp;#34;k8s.io/api/apps/v1&amp;#34;
	v1 &amp;#34;k8s.io/api/core/v1&amp;#34;
	&amp;#34;k8s.io/apimachinery/pkg/api/errors&amp;#34;
	metav1 &amp;#34;k8s.io/apimachinery/pkg/apis/meta/v1&amp;#34;
	utilerrors &amp;#34;k8s.io/apimachinery/pkg/util/errors&amp;#34;
	utilfeature &amp;#34;k8s.io/apiserver/pkg/util/feature&amp;#34;
	&amp;#34;k8s.io/client-go/tools/record&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/controller/history&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/features&amp;#34;
)

// Realistic value for maximum in-flight requests when processing in parallel mode.
const MaxBatchSize = 500

// StatefulSetControl implements the control logic for updating StatefulSets and their children Pods. It is implemented
// as an interface to allow for extensions that provide different semantics. Currently, there is only one implementation.
type StatefulSetControlInterface interface {
	// UpdateStatefulSet implements the control logic for Pod creation, update, and deletion, and
	// persistent volume creation, update, and deletion.
	// If an implementation returns a non-nil error, the invocation will be retried using a rate-limited strategy.
	// Implementors should sink any errors that they do not wish to trigger a retry, and they may feel free to
	// exit exceptionally at any point provided they wish the update to be re-run at a later point in time.
	UpdateStatefulSet(ctx context.Context, set *apps.StatefulSet, pods []*v1.Pod) (*apps.StatefulSetStatus, error)
	// ListRevisions returns a array of the ControllerRevisions that represent the revisions of set. If the returned
	// error is nil, the returns slice of ControllerRevisions is valid.
	ListRevisions(set *apps.StatefulSet) ([]*apps.ControllerRevision, error)
	// AdoptOrphanRevisions adopts any orphaned ControllerRevisions that match set&amp;#39;s Selector. If all adoptions are
	// successful the returned error is nil.
	AdoptOrphanRevisions(set *apps.StatefulSet, revisions []*apps.ControllerRevision) error
}

//这段代码定义了一个名为StatefulSetControlInterface的接口，其中包含了三个方法：
//1. UpdateStatefulSet：用于更新StatefulSet及其子Pods的控制逻辑。实现此方法时，如果返回非空错误，调用将使用速率限制策略进行重试。
//实现者应将任何不希望触发重试的错误沉入，并且可以在任何时间点异常退出，只要希望稍后重新运行更新即可。
//2. ListRevisions：返回代表set修订版本的ControllerRevision数组。如果返回的错误为nil，则返回的ControllerRevision切片是有效的。
//3. AdoptOrphanRevisions：采用与set的Selector匹配的任何孤儿ControllerRevision。如果所有采用都成功，则返回的错误为nil。
//注意：代码中还定义了一个常量MaxBatchSize，其值为500。

// NewDefaultStatefulSetControl returns a new instance of the default implementation StatefulSetControlInterface that
// implements the documented semantics for StatefulSets. podControl is the PodControlInterface used to create, update,
// and delete Pods and to create PersistentVolumeClaims. statusUpdater is the StatefulSetStatusUpdaterInterface used
// to update the status of StatefulSets. You should use an instance returned from NewRealStatefulPodControl() for any
// scenario other than testing.
func NewDefaultStatefulSetControl(
	podControl *StatefulPodControl,
	statusUpdater StatefulSetStatusUpdaterInterface,
	controllerHistory history.Interface,
	recorder record.EventRecorder) StatefulSetControlInterface {
	return &amp;amp;defaultStatefulSetControl{podControl, statusUpdater, controllerHistory, recorder}
}

// 该函数返回一个实现了StatefulSetControlInterface接口的defaultStatefulSetControl实例。
// 参数包括podControl用于创建、更新和删除Pod以及创建PersistentVolumeClaims；
// statusUpdater用于更新StatefulSets的状态；
// controllerHistory用于管理StatefulSet的控制器历史记录；
// recorder用于记录事件。
// 除了测试场景外，应使用NewRealStatefulPodControl()返回的实例作为podControl参数。
type defaultStatefulSetControl struct {
	podControl *StatefulPodControl
	statusUpdater StatefulSetStatusUpdaterInterface
	controllerHistory history.Interface
	recorder record.EventRecorder
}

//该代码定义了一个名为defaultStatefulSetControl的结构体，它有四个字段：
//1. podControl：类型为*StatefulPodControl，用于控制Pod的操作。
//2. statusUpdater：类型为StatefulSetStatusUpdaterInterface，用于更新StatefulSet的状态。
//3. controllerHistory：类型为history.Interface，用于维护控制器的历史记录。
//4. recorder：类型为record.EventRecorder，用于记录事件。 这个结构体主要用于管理和控制StatefulSet的生命周期。

// UpdateStatefulSet executes the core logic loop for a stateful set, applying the predictable and
// consistent monotonic update strategy by default - scale up proceeds in ordinal order, no new pod
// is created while any pod is unhealthy, and pods are terminated in descending order. The burst
// strategy allows these constraints to be relaxed - pods will be created and deleted eagerly and
// in no particular order. Clients using the burst strategy should be careful to ensure they
// understand the consistency implications of having unpredictable numbers of pods available.
func (ssc *defaultStatefulSetControl) UpdateStatefulSet(ctx context.Context, set *apps.StatefulSet, pods []*v1.Pod) (*apps.StatefulSetStatus, error) {
	set = set.DeepCopy() // set is modified when a new revision is created in performUpdate. Make a copy now to avoid mutation errors.

	// list all revisions and sort them
	revisions, err := ssc.ListRevisions(set)
	//该函数是defaultStatefulSetControl类型的UpdateStatefulSet方法，用于更新StatefulSet对象的状态。
	//函数首先对传入的StatefulSet对象进行深拷贝，以避免更新过程中出现突变错误。
	//然后通过调用ssc的ListRevisions方法，列出并排序该StatefulSet的所有修订版本。
	//接下来，函数会根据传入的Pod对象列表，更新StatefulSet的状态信息，并返回更新后的StatefulSet状态以及可能出现的错误。
	if err != nil {
		return nil, err
	}
	history.SortControllerRevisions(revisions)
	//该函数首先检查err是否为nil，如果不为nil，则返回nil和err。然后调用history.SortControllerRevisions函数对revisions进行排序。
	currentRevision, updateRevision, status, err := ssc.performUpdate(ctx, set, pods, revisions)
	if err != nil {
		errs := []error{err}
		if agg, ok := err.(utilerrors.Aggregate); ok {
			errs = agg.Errors()
		}
		return nil, utilerrors.NewAggregate(append(errs, ssc.truncateHistory(set, pods, revisions, currentRevision, updateRevision)))
	}
	//该函数调用ssc.performUpdate执行更新操作，并根据返回的结果进行错误处理和历史记录截断。
	//如果performUpdate返回错误，函数会将错误添加到一个错误切片中，并判断错误是否为utilerrors.Aggregate类型，
	//如果是，则将其中的错误提取出来合并到错误切片中。
	//最后，函数会调用ssc.truncateHistory截断历史记录，并将其错误与之前收集的错误合并后返回。
	// maintain the set&amp;#39;s revision history limit
	return status, ssc.truncateHistory(set, pods, revisions, currentRevision, updateRevision)
	//该函数调用了ssc.truncateHistory方法，用于维护集合的修订历史记录限制。传入参数包括集合、Pods、修订版本号、当前修订版本号和更新修订版本号。返回值为操作状态。
}

//该函数是用于更新一个StatefulSet的状态的核心逻辑循环。
//它执行默认的、可预测的、一致的单调更新策略：扩展按顺序进行，当任何Pod不健康时不会创建新的Pod，Pods按降序终止。
//突发策略允许放松这些约束——Pod将被积极地创建和删除，且没有特定的顺序。
//使用突发策略的客户端应该小心确保他们了解具有不可预测的Pod数量的一致性影响。
//函数首先对传入的StatefulSet进行深拷贝，然后列出并排序所有修订版本。
//然后执行performUpdate函数进行更新操作，根据更新结果维护StatefulSet的修订版本历史记录限制。

func (ssc *defaultStatefulSetControl) performUpdate(
	ctx context.Context, set *apps.StatefulSet, pods []*v1.Pod, revisions []*apps.ControllerRevision) (*apps.ControllerRevision, *apps.ControllerRevision, *apps.StatefulSetStatus, error) {
	var currentStatus *apps.StatefulSetStatus
	logger := klog.FromContext(ctx)
	//该函数是一个Go语言函数，它定义了一个名为performUpdate的方法，该方法属于defaultStatefulSetControl类型。
	//这个方法接收四个参数：
	//ctx是一个上下文对象，
	//set是一个指向StatefulSet类型的指针，
	//pods是一个指向Pod类型的切片，
	//revisions是一个指向ControllerRevision类型的切片。
	//该方法返回四个值：一个指向ControllerRevision类型的指针，一个指向ControllerRevision类型的指针，
	//一个指向StatefulSetStatus类型的指针，以及一个错误类型。 在函数体内部，定义了一个名为currentStatus的变量，
	//它是一个指向StatefulSetStatus类型的指针。
	//然后，通过FromContext方法从ctx中获取了一个日志记录器对象，并将其赋值给名为logger的变量。
	//这个函数的主要功能是在状态fulSet的更新过程中执行一些操作，并返回更新后的状态fulSet的状态信息和其他相关数据。
	//但是，根据给定的代码片段，无法确定该函数的完整逻辑和具体操作。
	// get the current, and update revisions
	currentRevision, updateRevision, collisionCount, err := ssc.getStatefulSetRevisions(set, revisions)
	if err != nil {
		return currentRevision, updateRevision, currentStatus, err
	}
	//该函数用于获取当前状态集的修订版本，并更新修订版本。
	//函数接收一个状态集和修订版本作为参数，返回当前修订版本、更新修订版本、冲突计数和错误信息。
	//如果发生错误，函数会返回当前修订版本、更新修订版本、当前状态和错误信息。
	// perform the main update function and get the status
	currentStatus, err = ssc.updateStatefulSet(ctx, set, currentRevision, updateRevision, collisionCount, pods)
	if err != nil &amp;amp;&amp;amp; currentStatus == nil {
		return currentRevision, updateRevision, nil, err
	}
	//该函数主要调用ssc.updateStatefulSet方法来更新StatefulSet的状态，并获取更新后的状态信息。如果更新出错且没有返回新的状态信息，则返回更新前的状态信息和错误。
	// make sure to update the latest status even if there is an error with non-nil currentStatus
	statusErr := ssc.updateStatefulSetStatus(ctx, set, currentStatus)
	if statusErr == nil {
		logger.V(4).Info(&amp;#34;Updated status&amp;#34;, &amp;#34;statefulSet&amp;#34;, klog.KObj(set),
			&amp;#34;replicas&amp;#34;, currentStatus.Replicas,
			&amp;#34;readyReplicas&amp;#34;, currentStatus.ReadyReplicas,
			&amp;#34;currentReplicas&amp;#34;, currentStatus.CurrentReplicas,
			&amp;#34;updatedReplicas&amp;#34;, currentStatus.UpdatedReplicas)
	}
	//该函数用于更新StatefulSet的状态。
	//首先调用ssc.updateStatefulSetStatus方法更新状态，然后根据返回的错误值进行判断。
	//如果更新状态时没有错误，就打印日志信息，包括StatefulSet的名称和各种状态值（Replicas、ReadyReplicas、CurrentReplicas、UpdatedReplicas）。
	switch {
	case err != nil &amp;amp;&amp;amp; statusErr != nil:
		logger.Error(statusErr, &amp;#34;Could not update status&amp;#34;, &amp;#34;statefulSet&amp;#34;, klog.KObj(set))
		return currentRevision, updateRevision, currentStatus, err
	case err != nil:
		return currentRevision, updateRevision, currentStatus, err
	case statusErr != nil:
		return currentRevision, updateRevision, currentStatus, statusErr
	}
	//这个Go函数通过switch语句根据err和statusErr的值来决定返回的结果。
	//如果err和statusErr都不为nil，则记录错误日志并返回当前修订版号、更新修订版号、当前状态和err；
	//如果只有err不为nil，则直接返回当前修订版号、更新修订版号、当前状态和err；
	//如果只有statusErr不为nil，则直接返回当前修订版号、更新修订版号、当前状态和statusErr。
	logger.V(4).Info(&amp;#34;StatefulSet revisions&amp;#34;, &amp;#34;statefulSet&amp;#34;, klog.KObj(set),
		&amp;#34;currentRevision&amp;#34;, currentStatus.CurrentRevision,
		&amp;#34;updateRevision&amp;#34;, currentStatus.UpdateRevision)

	return currentRevision, updateRevision, currentStatus, nil
	//该函数是一个日志输出函数，通过调用logger.V(4).Info方法输出一条包含多个参数的日志信息。
	//其中，参数StatefulSet revisions是日志的主题，statefulSet、currentRevision、updateRevision是日志的附加信息。
	//函数返回currentRevision、updateRevision、currentStatus以及一个nil错误。
}

func (ssc *defaultStatefulSetControl) ListRevisions(set *apps.StatefulSet) ([]*apps.ControllerRevision, error) {
	selector, err := metav1.LabelSelectorAsSelector(set.Spec.Selector)
	if err != nil {
		return nil, err
	}
	return ssc.controllerHistory.ListControllerRevisions(set, selector)
}

// 该函数是defaultStatefulSetControl类型的成员方法，用于列出给定StatefulSet的所有ControllerRevision。
// 1. 首先，函数将StatefulSet的selector转换为LabelSelector类型。
// 2. 然后，调用controllerHistory的ListControllerRevisions方法，传入StatefulSet和selector，返回所有的ControllerRevision。
// 3. 如果转换selector时出现错误，函数将返回nil和错误信息。 返回值为 []*apps.ControllerRevision，即ControllerRevision的切片。
func (ssc *defaultStatefulSetControl) AdoptOrphanRevisions(
	set *apps.StatefulSet,
	revisions []*apps.ControllerRevision) error {
	for i := range revisions {
		adopted, err := ssc.controllerHistory.AdoptControllerRevision(set, controllerKind, revisions[i])
		if err != nil {
			return err
		}
		revisions[i] = adopted
	}
	return nil
}

//该函数是一个Go语言函数，它属于defaultStatefulSetControl类型的一个方法。
//函数名为AdoptOrphanRevisions，它接受一个StatefulSet类型的指针set和一个ControllerRevision类型的切片revisions作为参数，返回一个error类型的值。
//该函数的作用是领养孤儿ControllerRevision（即没有StatefulSet关联的ControllerRevision），
//通过调用ssc.controllerHistory的AdoptControllerRevision方法，将孤儿ControllerRevision与给定的StatefulSet关联起来。
//函数会遍历revisions切片中的每个ControllerRevision，尝试将其领养，并将领养后的ControllerRevision更新到revisions切片中对应的位置。
//如果领养过程中出现错误，则会立即返回错误。如果成功领养所有孤儿ControllerRevision，则函数返回nil。

// truncateHistory truncates any non-live ControllerRevisions in revisions from set&amp;#39;s history. The UpdateRevision and
// CurrentRevision in set&amp;#39;s Status are considered to be live. Any revisions associated with the Pods in pods are also
// considered to be live. Non-live revisions are deleted, starting with the revision with the lowest Revision, until
// only RevisionHistoryLimit revisions remain. If the returned error is nil the operation was successful. This method
// expects that revisions is sorted when supplied.
func (ssc *defaultStatefulSetControl) truncateHistory(
	set *apps.StatefulSet,
	pods []*v1.Pod,
	revisions []*apps.ControllerRevision,
	current *apps.ControllerRevision,
	update *apps.ControllerRevision) error {
	history := make([]*apps.ControllerRevision, 0, len(revisions))
	// mark all live revisions
	live := map[string]bool{}
	//该函数用于截断StatefulSet历史记录中任何非活动的ControllerRevisions。
	//函数会从set的历史记录中删除非活动的修订版本，从修订版本号最低的开始，直到只剩下RevisionHistoryLimit个修订版本为止。
	//函数会将UpdateRevision和CurrentRevision视为活动的修订版本，同时将与Pods关联的修订版本也视为活动的修订版本。
	//函数期望传入的修订版本已经按照顺序排序。
	//函数返回一个错误，如果返回的错误为nil，则表示操作成功。
	//函数首先创建一个空的ControllerRevision指针切片history，用于存储活动的修订版本。
	//然后创建一个map类型的live变量，用于标记活动的修订版本。
	//接下来，函数遍历传入的修订版本切片revisions，将活动的修订版本添加到history切片中，并在live变量中标记该修订版本为活动的。
	//最后，函数检查history切片的长度是否超过了RevisionHistoryLimit，
	//如果是，则删除history切片中修订版本号最低的修订版本，直到history切片的长度等于RevisionHistoryLimit。
	//如果在截断历史记录的过程中出现错误，则返回该错误。
	if current != nil {
		live[current.Name] = true
	}
	if update != nil {
		live[update.Name] = true
	}
	for i := range pods {
		live[getPodRevision(pods[i])] = true
	}
	// collect live revisions and historic revisions
	for i := range revisions {
		if !live[revisions[i].Name] {
			history = append(history, revisions[i])
		}
	}
	//该Go函数主要实现了以下几个功能：
	//1. 将当前非空对象的名称添加到live映射中，并将其值设置为true。
	//2. 如果update非空，将update对象的名称添加到live映射中，并将其值设置为true。
	//3. 遍历pods切片，将每个Pod的修订版本名称添加到live映射中，并将其值设置为true。
	//4. 遍历revisions切片，将不在live映射中的修订版本对象添加到history切片中。
	//这段代码的主要目的是为了收集活跃的修订版本和历史修订版本，并将它们分别存储在live和history映射和切片中。
	historyLen := len(history)
	historyLimit := int(*set.Spec.RevisionHistoryLimit)
	if historyLen &amp;lt;= historyLimit {
		return nil
	}
	// delete any non-live history to maintain the revision limit.
	history = history[:(historyLen - historyLimit)]
	for i := 0; i &amp;lt; len(history); i++ {
		if err := ssc.controllerHistory.DeleteControllerRevision(history[i]); err != nil {
			return err
		}
	}
	return nil
}

//该函数用于根据设定的修订历史限制，删除非活跃的历史记录，以保持修订限制。
//首先，获取历史记录的长度并将其与修订历史限制进行比较。
//如果历史记录的长度小于等于修订历史限制，则无需进行删除操作，直接返回nil。
//如果历史记录的长度大于修订历史限制，则通过切片操作将超出部分的历史记录删除，并遍历剩余的历史记录，调用DeleteControllerRevision方法将其逐个删除。
//如果删除过程中出现错误，则返回错误。最后，如果删除成功，则返回nil。

// getStatefulSetRevisions returns the current and update ControllerRevisions for set. It also
// returns a collision count that records the number of name collisions set saw when creating
// new ControllerRevisions. This count is incremented on every name collision and is used in
// building the ControllerRevision names for name collision avoidance. This method may create
// a new revision, or modify the Revision of an existing revision if an update to set is detected.
// This method expects that revisions is sorted when supplied.
func (ssc *defaultStatefulSetControl) getStatefulSetRevisions(
	set *apps.StatefulSet,
	revisions []*apps.ControllerRevision) (*apps.ControllerRevision, *apps.ControllerRevision, int32, error) {
	var currentRevision, updateRevision *apps.ControllerRevision

	revisionCount := len(revisions)
	history.SortControllerRevisions(revisions)
	//该函数用于获取StatefulSet的当前和更新的ControllerRevisions，并返回一个记录创建新ControllerRevisions时遇到的名称冲突次数的碰撞计数。
	//如果检测到StatefulSet的更新，该函数可能会创建新修订版或修改现有修订版的修订号。函数期望传入的修订版已排序。
	// Use a local copy of set.Status.CollisionCount to avoid modifying set.Status directly.
	// This copy is returned so the value gets carried over to set.Status in updateStatefulSet.
	var collisionCount int32
	if set.Status.CollisionCount != nil {
		collisionCount = *set.Status.CollisionCount
	}
	//这段Go代码定义了一个局部变量collisionCount，并根据set.Status.CollisionCount的值进行初始化。
	//其主要作用是避免直接修改set.Status的值，通过返回这个局部变量的值，将其传递给updateStatefulSet函数，以更新set.Status中的CollisionCount字段。
	//详细解释如下： 1. var collisionCount int32：定义了一个名为collisionCount的整型变量，用于存储set.Status.CollisionCount的值。
	//2. if set.Status.CollisionCount != nil：判断set.Status.CollisionCount是否非空，即是否已经初始化。
	//3. collisionCount = *set.Status.CollisionCount：如果set.Status.CollisionCount非空，将其值赋给collisionCount变量。
	//这里使用了指针解引用操作*，因为set.Status.CollisionCount可能是一个指针。
	//这段代码的主要目的是在不直接修改set.Status的情况下，获取并传递set.Status.CollisionCount的值。
	//通过使用局部变量的方式，可以确保在updateStatefulSet函数中更新set.Status时，collisionCount的值不会丢失。

	// create a new revision from the current set
	updateRevision, err := newRevision(set, nextRevision(revisions), &amp;amp;collisionCount)
	if err != nil {
		return nil, nil, collisionCount, err
	}
	//这段Go代码中的函数创建了一个新的修订版本，使用当前的版本集。
	//它首先调用nextRevision(revisions)来获取下一个修订版本号，然后调用newRevision(set, revision, &amp;amp;collisionCount)来创建新的修订版本。
	//如果创建过程中出现错误，则返回nil、nil、collisionCount和错误信息。

	// find any equivalent revisions
	equalRevisions := history.FindEqualRevisions(revisions, updateRevision)
	equalCount := len(equalRevisions)
	//该代码片段是用Go编写的。它定义了一个函数，该函数查找与给定修订版本相等的任何修订版本，并返回相等修订版本的数量。
	//具体而言，该函数通过调用history.FindEqualRevisions方法，将给定的修订版本列表revisions和更新的修订版本updateRevision作为参数传递。
	//FindEqualRevisions方法返回一个包含与updateRevision相等的所有修订版本的切片。
	//然后，代码通过使用len函数计算equalRevisions切片的长度，从而获取相等修订版本的数量。
	//总结一下，这段代码的主要功能是查找与指定更新修订版本相等的修订版本的数量。
	if equalCount &amp;gt; 0 &amp;amp;&amp;amp; history.EqualRevision(revisions[revisionCount-1], equalRevisions[equalCount-1]) {
		// if the equivalent revision is immediately prior the update revision has not changed
		updateRevision = revisions[revisionCount-1]
	} else if equalCount &amp;gt; 0 {
		// if the equivalent revision is not immediately prior we will roll back by incrementing the
		// Revision of the equivalent revision
		updateRevision, err = ssc.controllerHistory.UpdateControllerRevision(
			equalRevisions[equalCount-1],
			updateRevision.Revision)
		if err != nil {
			return nil, nil, collisionCount, err
		}
	} else {
		//if there is no equivalent revision we create a new one
		updateRevision, err = ssc.controllerHistory.CreateControllerRevision(set, updateRevision, &amp;amp;collisionCount)
		if err != nil {
			return nil, nil, collisionCount, err
		}
	}
	//这段Go代码中的函数片段是一个条件语句，用于根据不同的条件更新或创建控制器修订版本。
	//具体功能如下：
	//- 首先，它检查是否存在等效的修订版本（equalCount大于0）并且该修订版本是否立即位于更新修订版本之前。
	//如果是，则将更新修订版本设置为最新修订版本。
	//- 如果存在等效的修订版本但不立即位于更新修订版本之前，则通过调用ssc.controllerHistory.UpdateControllerRevision方法来回滚到该等效修订版本，
	//并更新修订版本号。如果回滚过程中出现错误，则返回错误信息。
	//- 如果不存在等效的修订版本，则通过调用ssc.controllerHistory.CreateControllerRevision方法创建一个新的控制器修订版本。
	//如果创建过程中出现错误，则返回错误信息。 总之，这段代码根据条件选择更新或创建控制器修订版本，并可能涉及到回滚到等效的修订版本或创建新修订版本的操作。

	// attempt to find the revision that corresponds to the current revision
	for i := range revisions {
		if revisions[i].Name == set.Status.CurrentRevision {
			currentRevision = revisions[i]
			break
		}
	}
	//该函数是一个for循环，通过遍历revisions切片，查找并设置与set.Status.CurrentRevision名称相匹配的修订版本。
	// if the current revision is nil we initialize the history by setting it to the update revision
	if currentRevision == nil {
		currentRevision = updateRevision
	}

	return currentRevision, updateRevision, collisionCount, nil
}

// 这个函数是一个简单的条件判断语句，判断当前的currentRevision是否为nil，
// 如果是，则将其初始化为updateRevision。
// 最后返回更新后的currentRevision、updateRevision、collisionCount和nil。
func slowStartBatch(initialBatchSize int, remaining int, fn func(int) (bool, error)) (int, error) {
	successes := 0
	j := 0
	for batchSize := min(remaining, initialBatchSize); batchSize &amp;gt; 0; batchSize = min(min(2*batchSize, remaining), MaxBatchSize) {
		errCh := make(chan error, batchSize)
		var wg sync.WaitGroup
		wg.Add(batchSize)
		for i := 0; i &amp;lt; batchSize; i++ {
			go func(k int) {
				defer wg.Done()
				// Ignore the first parameter - relevant for monotonic only.
				if _, err := fn(k); err != nil {
					errCh &amp;lt;- err
				}
			}(j)
			j++
		}
		wg.Wait()
		successes += batchSize - len(errCh)
		close(errCh)
		if len(errCh) &amp;gt; 0 {
			errs := make([]error, 0)
			for err := range errCh {
				errs = append(errs, err)
			}
			return successes, utilerrors.NewAggregate(errs)
		}
		remaining -= batchSize
	}
	return successes, nil
}

// 该函数是一个并发执行函数的批处理工具。它通过调用提供的函数fn来并发执行批处理，并根据执行结果进行错误处理和统计。
// - initialBatchSize：初始批处理大小。
// - remaining：剩余需要处理的数量。
// - fn：一个函数，接收一个整数参数并返回一个布尔值和一个错误。
// 该函数将被并发调用以执行批处理任务。
// 函数主要逻辑如下：
// 1. 初始化成功执行数successes和内部计数器j。
// 2. 使用min函数计算当前批处理大小batchSize，并进入循环，直到batchSize为0或处理完成。
// 3. 创建一个错误通道errCh，用于收集并发执行中的错误。
// 4. 使用sync.WaitGroup来等待所有并发执行完成。
// 5. 并发调用fn函数，将j作为参数，递增j。
// 6. 等待所有并发执行完成，并统计成功执行数。
// 7. 如果存在错误，将错误收集到errs中并返回。
// 8. 更新剩余需要处理的数量。
// 9. 返回成功执行数和错误。
// 该函数通过控制批处理的大小和并发执行，实现了一个自适应的并发批处理逻辑，可以根据执行情况动态调整批处理的大小。
type replicaStatus struct {
	replicas int32
	readyReplicas int32
	availableReplicas int32
	currentReplicas int32
	updatedReplicas int32
}

// 该代码定义了一个名为replicaStatus的结构体类型，
// 它有5个字段：
// - replicas表示副本的总数；
// - readyReplicas表示准备就绪的副本数量；
// - availableReplicas表示可用的副本数量；
// - currentReplicas表示当前存在的副本数量；
// - updatedReplicas表示已更新的副本数量。
func computeReplicaStatus(pods []*v1.Pod, minReadySeconds int32, currentRevision, updateRevision *apps.ControllerRevision) replicaStatus {
	status := replicaStatus{}
	for _, pod := range pods {
		if isCreated(pod) {
			status.replicas++
		}
		//该函数用于计算副本状态。它接收一个Pod列表、最小就绪秒数、当前修订版和更新修订版作为参数，并返回一个副本状态。
		//函数遍历Pod列表，并对每个Pod进行如下操作：
		//1. 如果Pod被创建，则将副本数量加1。
		//最后，函数返回计算得到的副本状态。
		// count the number of running and ready replicas
		if isRunningAndReady(pod) {
			status.readyReplicas++
			// count the number of running and available replicas
			if isRunningAndAvailable(pod, minReadySeconds) {
				status.availableReplicas++
			}

		}
		//这段Go代码中的函数用于增加就绪副本数和可用副本数的计数。
		//具体来说：
		//- isRunningAndReady(pod) 函数检查 Pod 是否处于运行且就绪状态，如果是，则将 status.readyReplicas 增加 1。
		//- isRunningAndAvailable(pod, minReadySeconds) 函数检查 Pod 是否处于运行且可用状态，
		//其中 minReadySeconds 参数表示最小就绪时间（单位：秒）。
		//如果 Pod 在最小就绪时间内一直保持就绪状态，则将 status.availableReplicas 增加 1。
		//这段代码的主要目的是统计 Kubernetes 中某个 Pod 的就绪副本数和可用副本数，以便于对 Pod 的状态进行管理和监控。
		// count the number of current and update replicas
		if isCreated(pod) &amp;amp;&amp;amp; !isTerminating(pod) {
			revision := getPodRevision(pod)
			if revision == currentRevision.Name {
				status.currentReplicas++
			}
			if revision == updateRevision.Name {
				status.updatedReplicas++
			}
		}
	}
	return status
}

// 这段Go代码中的函数逻辑如下：
// - 首先判断Pod是否已经创建并且没有处于终止状态；
// - 如果满足条件，获取Pod的修订版本号；
// - 判断该修订版本号是否与当前修订版本名相同，若相同则当前副本数加一；
// - 判断该修订版本号是否与更新修订版本名相同，若相同则更新副本数加一；
// - 最后返回更新后的状态信息。 这段代码主要是用来统计Pod的当前副本数和更新后的副本数。
func updateStatus(status *apps.StatefulSetStatus, minReadySeconds int32, currentRevision, updateRevision *apps.ControllerRevision, podLists ...[]*v1.Pod) {
	status.Replicas = 0
	status.ReadyReplicas = 0
	status.AvailableReplicas = 0
	status.CurrentReplicas = 0
	status.UpdatedReplicas = 0
	for _, list := range podLists {
		replicaStatus := computeReplicaStatus(list, minReadySeconds, currentRevision, updateRevision)
		status.Replicas += replicaStatus.replicas
		status.ReadyReplicas += replicaStatus.readyReplicas
		status.AvailableReplicas += replicaStatus.availableReplicas
		status.CurrentReplicas += replicaStatus.currentReplicas
		status.UpdatedReplicas += replicaStatus.updatedReplicas
	}
}

// 该函数用于更新StatefulSet的状态信息。
// 它接收一个指向StatefulSetStatus的指针、minReadySeconds参数以及currentRevision和updateRevision两个ControllerRevision指针，
// 还有可变长度的podLists参数。
// 函数首先将状态信息中的五个计数器重置为0，然后遍历podLists中的每个Pod列表，
// 通过调用computeReplicaStatus函数计算每个列表的副本状态，并将计算结果累加到状态信息中
func (ssc *defaultStatefulSetControl) processReplica(
	ctx context.Context,
	set *apps.StatefulSet,
	currentRevision *apps.ControllerRevision,
	updateRevision *apps.ControllerRevision,
	currentSet *apps.StatefulSet,
	updateSet *apps.StatefulSet,
	monotonic bool,
	replicas []*v1.Pod,
	i int) (bool, error) {
	logger := klog.FromContext(ctx)
	//该函数是一个Go语言函数，它定义了一个用于处理StatefulSet副本的过程。
	//函数接受多个参数，包括上下文、StatefulSet对象、两个ControllerRevision对象、两个StatefulSet对象、一个布尔值、一个Pod对象数组和一个整数。
	//函数返回一个布尔值和一个错误对象。 Markdown格式输出：
	//该函数是一个Go语言函数，定义了一个处理StatefulSet副本的过程。它接受以下参数：
	//- ctx：上下文对象。
	//- set：StatefulSet对象。
	//- currentRevision：当前的ControllerRevision对象。
	//- updateRevision：要更新的ControllerRevision对象。
	//- currentSet：当前的StatefulSet对象。
	//- updateSet：要更新的StatefulSet对象。
	//- monotonic：一个布尔值，表示是否为单调递增。
	//- replicas：一个Pod对象数组。
	//- i：一个整数。
	//函数返回一个布尔值和一个错误对象。
	// Delete and recreate pods which finished running.
	//
	// Note that pods with phase Succeeded will also trigger this event. This is
	// because final pod phase of evicted or otherwise forcibly stopped pods
	// (e.g. terminated on node reboot) is determined by the exit code of the
	// container, not by the reason for pod termination. We should restart the pod
	// regardless of the exit code.
	if isFailed(replicas[i]) || isSucceeded(replicas[i]) {
		if isFailed(replicas[i]) {
			ssc.recorder.Eventf(set, v1.EventTypeWarning, &amp;#34;RecreatingFailedPod&amp;#34;,
				&amp;#34;StatefulSet %s/%s is recreating failed Pod %s&amp;#34;,
				set.Namespace,
				set.Name,
				replicas[i].Name)
			//这段Go代码是一个函数的一部分，它的功能是删除已经运行完成的Pod，并重新创建它们。
			//注意，这个函数还会处理那些成功结束的Pod（即状态为Succeeded的Pod），
			//因为被驱逐或强制停止的Pod的最终状态是由容器的退出码决定的，而不是由Pod终止的原因决定的。
			//因此，无论退出码如何，都应该重新启动Pod。
			//函数中使用了isFailed和isSucceeded函数来判断Pod的状态，
			//如果状态为Failed或Succeeded，则会使用ssc.recorder.Eventf函数记录一个事件，说明StatefulSet正在重新创建失败或成功的Pod。
		} else {
			ssc.recorder.Eventf(set, v1.EventTypeNormal, &amp;#34;RecreatingTerminatedPod&amp;#34;,
				&amp;#34;StatefulSet %s/%s is recreating terminated Pod %s&amp;#34;,
				set.Namespace,
				set.Name,
				replicas[i].Name)
			//这段Go代码是Kubernetes中StatefulSet控制器的一部分，用于处理有状态副本集（StatefulSet）中已终止的Pod的重建事件。
			//在else块中，代码通过ssc.recorder.Eventf方法记录了一个事件，
			//事件类型为v1.EventTypeNormal，事件名为&amp;#34;RecreatingTerminatedPod&amp;#34;，
			//事件消息为&amp;#34;StatefulSet %s/%s is recreating terminated Pod %s&amp;#34;，
			//其中包含了StatefulSet的命名空间、名称以及需要重建的Pod的名称。
			//这个函数的作用是通过事件机制通知用户，某个StatefulSet正在重建已终止的Pod。
		}
		if err := ssc.podControl.DeleteStatefulPod(set, replicas[i]); err != nil {
			return true, err
		}
		replicaOrd := i + getStartOrdinal(set)
		replicas[i] = newVersionedStatefulSetPod(
			currentSet,
			updateSet,
			currentRevision.Name,
			updateRevision.Name,
			replicaOrd)
		//该函数的功能是在StatefulSet中删除一个Pod，并创建一个新的Pod来替代它。
		//函数首先尝试删除指定的Pod，如果删除成功，则根据当前的StatefulSet和更新后的StatefulSet信息创建一个新的Pod，并将其添加到replicas列表中。
	}
	//该函数用于删除并重新创建已经完成运行的Pods。
	//注意，即使Pod的阶段为Succeeded，也会触发此事件。
	//这是因为被驱逐或以其他方式强制停止的Pod的最终Pod阶段（例如在节点重新启动时终止）由容器的退出代码决定，而不是由Pod终止的原因决定。
	//无论退出代码如何，我们都应该重新启动Pod。
	//函数首先检查每个Pod是否失败或成功，如果是，则记录事件并尝试删除该Pod。
	//如果删除成功，则创建一个新的Pod来替代它。
	// If we find a Pod that has not been created we create the Pod
	if !isCreated(replicas[i]) {
		if utilfeature.DefaultFeatureGate.Enabled(features.StatefulSetAutoDeletePVC) {
			if isStale, err := ssc.podControl.PodClaimIsStale(set, replicas[i]); err != nil {
				return true, err
			} else if isStale {
				// If a pod has a stale PVC, no more work can be done this round.
				return true, err
			}
		}
		if err := ssc.podControl.CreateStatefulPod(ctx, set, replicas[i]); err != nil {
			return true, err
		}
		if monotonic {
			// if the set does not allow bursting, return immediately
			return true, nil
		}
	}
	//这段Go代码是一个条件判断语句的集合，用于处理Pod的创建。
	//具体来说，它首先检查一个Pod是否已经被创建，如果没有，则根据特定的条件进行进一步的处理。
	//如果启用了StatefulSetAutoDeletePVC特性，并且Pod的PersistentVolumeClaim（PVC）是陈旧的，则函数会提前返回。
	//如果创建Pod失败，函数也会返回。如果该集合不允许突发（monotonic）行为，函数会在创建Pod后立即返回。
	// If the Pod is in pending state then trigger PVC creation to create missing PVCs
	if isPending(replicas[i]) {
		logger.V(4).Info(
			&amp;#34;StatefulSet is triggering PVC creation for pending Pod&amp;#34;,
			&amp;#34;statefulSet&amp;#34;, klog.KObj(set), &amp;#34;pod&amp;#34;, klog.KObj(replicas[i]))
		if err := ssc.podControl.createMissingPersistentVolumeClaims(ctx, set, replicas[i]); err != nil {
			return true, err
		}
	}
	//该函数用于检查StatefulSet中的Pod是否处于pending状态，
	//如果是，则触发PVC（Persistent Volume Claim）的创建，以创建缺失的PVC。
	//具体实现中，通过调用podControl的createMissingPersistentVolumeClaims方法来创建缺失的PVC，
	//如果创建失败，则返回true和错误信息。

	// If we find a Pod that is currently terminating, we must wait until graceful deletion
	// completes before we continue to make progress.
	if isTerminating(replicas[i]) &amp;amp;&amp;amp; monotonic {
		logger.V(4).Info(&amp;#34;StatefulSet is waiting for Pod to Terminate&amp;#34;,
			&amp;#34;statefulSet&amp;#34;, klog.KObj(set), &amp;#34;pod&amp;#34;, klog.KObj(replicas[i]))
		return true, nil
	}
	//该函数是一个条件判断语句，判断某个Pod是否正在终止状态，并且是否需要等待其优雅删除完成。如果满足条件，则返回true和nil。

	// If we have a Pod that has been created but is not running and ready we can not make progress.
	// We must ensure that all for each Pod, when we create it, all of its predecessors, with respect to its
	// ordinal, are Running and Ready.
	if !isRunningAndReady(replicas[i]) &amp;amp;&amp;amp; monotonic {
		logger.V(4).Info(&amp;#34;StatefulSet is waiting for Pod to be Running and Ready&amp;#34;,
			&amp;#34;statefulSet&amp;#34;, klog.KObj(set), &amp;#34;pod&amp;#34;, klog.KObj(replicas[i]))
		return true, nil
	}
	//该函数是Go语言编写的，用于判断一个Pod是否处于运行和就绪状态。
	//如果Pod没有运行且未准备就绪，并且设置为单调递增模式，则函数会返回true和nil。
	//这段代码主要用于StatefulSet等待Pod达到运行和就绪状态的情况。

	// If we have a Pod that has been created but is not available we can not make progress.
	// We must ensure that all for each Pod, when we create it, all of its predecessors, with respect to its
	// ordinal, are Available.
	if !isRunningAndAvailable(replicas[i], set.Spec.MinReadySeconds) &amp;amp;&amp;amp; monotonic {
		logger.V(4).Info(&amp;#34;StatefulSet is waiting for Pod to be Available&amp;#34;,
			&amp;#34;statefulSet&amp;#34;, klog.KObj(set), &amp;#34;pod&amp;#34;, klog.KObj(replicas[i]))
		return true, nil
	}
	//该函数是Go语言编写的，用于判断StatefulSet中的Pod是否可用。
	//如果Pod未运行或不可用，并且StatefulSet的monotonic属性为true，则函数会返回true，否则返回false。
	//函数通过调用isRunningAndAvailable函数来判断Pod的状态。

	// Enforce the StatefulSet invariants
	retentionMatch := true
	if utilfeature.DefaultFeatureGate.Enabled(features.StatefulSetAutoDeletePVC) {
		var err error
		retentionMatch, err = ssc.podControl.ClaimsMatchRetentionPolicy(ctx, updateSet, replicas[i])
		// An error is expected if the pod is not yet fully updated, and so return is treated as matching.
		if err != nil {
			retentionMatch = true
		}
	}
	//这段Go代码是用于检查StatefulSet的保留策略是否匹配的函数。
	//- 首先，它检查是否启用了StatefulSet的自动删除PVC的功能。
	//- 如果启用了，它会调用ssc.podControl.ClaimsMatchRetentionPolicy方法来检查当前的Pod是否匹配保留策略。
	//- 如果检查过程中出现错误，则将retentionMatch设置为true，表示匹配保留策略。
	//- 最后，retentionMatch的值将用于决定是否保留或删除StatefulSet的PVC。

	if identityMatches(set, replicas[i]) &amp;amp;&amp;amp; storageMatches(set, replicas[i]) &amp;amp;&amp;amp; retentionMatch {
		return false, nil
	}
	//该函数是用于判断某个条件是否满足，如果满足则返回false和nil。
	//具体判断条件为：identityMatches函数返回值为true，并且storageMatches函数返回值为true，并且retentionMatch的值为true。
	//其中，identityMatches函数用于判断某个set是否与replicas[i]相等；
	//storageMatches函数用于判断某个set是否与replicas[i]的存储相匹配；
	//retentionMatch是一个布尔值，用于判断某个条件是否满足。
	//如果以上条件都满足，则函数返回false和nil。

	// Make a deep copy so we don&amp;#39;t mutate the shared cache
	replica := replicas[i].DeepCopy()
	if err := ssc.podControl.UpdateStatefulPod(ctx, updateSet, replica); err != nil {
		return true, err
	}
	//这个函数的功能是通过调用DeepCopy方法创建replicas[i]的深拷贝，并将拷贝赋值给replica变量。
	//然后，它调用ssc.podControl.UpdateStatefulPod方法来更新状态fulPod，如果更新失败，则返回true和错误信息。
	return false, nil
}

func (ssc *defaultStatefulSetControl) processCondemned(ctx context.Context, set *apps.StatefulSet, firstUnhealthyPod *v1.Pod, monotonic bool, condemned []*v1.Pod, i int) (bool, error) {
	logger := klog.FromContext(ctx)
	if isTerminating(condemned[i]) {
		// if we are in monotonic mode, block and wait for terminating pods to expire
		if monotonic {
			logger.V(4).Info(&amp;#34;StatefulSet is waiting for Pod to Terminate prior to scale down&amp;#34;,
				&amp;#34;statefulSet&amp;#34;, klog.KObj(set), &amp;#34;pod&amp;#34;, klog.KObj(condemned[i]))
			return true, nil
		}
		return false, nil
	}
	//该函数是defaultStatefulSetControl类型的processCondemned方法，
	//用于处理StatefulSet中的废弃Pod。函数根据是否为单调模式，决定是否等待废弃Pod终止。
	//如果处于单调模式且当前Pod为终止状态，则函数会等待Pod终止。如果当前Pod不是终止状态，则函数直接返回。

	// if we are in monotonic mode and the condemned target is not the first unhealthy Pod block
	if !isRunningAndReady(condemned[i]) &amp;amp;&amp;amp; monotonic &amp;amp;&amp;amp; condemned[i] != firstUnhealthyPod {
		logger.V(4).Info(&amp;#34;StatefulSet is waiting for Pod to be Running and Ready prior to scale down&amp;#34;,
			&amp;#34;statefulSet&amp;#34;, klog.KObj(set), &amp;#34;pod&amp;#34;, klog.KObj(firstUnhealthyPod))
		return true, nil
	}
	//该函数片段是一个条件判断语句，
	//主要功能是在特定条件下，判断是否需要等待某个Pod变为Running和Ready状态后再进行缩容操作。
	//具体来说，它包含了以下几个要点：
	//1. 判断当前Pod是否处于运行且就绪状态：通过调用isRunningAndReady(condemned[i])函数来判断当前Pod是否满足运行且就绪的条件。
	//2. 判断是否处于单调模式：通过检查monotonic变量的值来判断当前是否处于单调模式。
	//3. 判断当前Pod是否为第一个不健康的Pod：通过比较condemned[i]和firstUnhealthyPod的值来判断当前Pod是否为第一个不健康的Pod。
	//如果满足上述条件，即当前Pod不运行且不就绪，当前处于单调模式，且当前Pod不是第一个不健康的Pod，则该函数会返回true和nil，
	//表示需要等待Pod变为Running和Ready状态后再进行缩容操作。

	// if we are in monotonic mode and the condemned target is not the first unhealthy Pod, block.
	if !isRunningAndAvailable(condemned[i], set.Spec.MinReadySeconds) &amp;amp;&amp;amp; monotonic &amp;amp;&amp;amp; condemned[i] != firstUnhealthyPod {
		logger.V(4).Info(&amp;#34;StatefulSet is waiting for Pod to be Available prior to scale down&amp;#34;,
			&amp;#34;statefulSet&amp;#34;, klog.KObj(set), &amp;#34;pod&amp;#34;, klog.KObj(firstUnhealthyPod))
		return true, nil
	}
	//该函数是一段Go语言代码，它是一个条件判断语句。
	//该函数主要功能是在特定条件下阻止某个操作的执行。
	//具体来说，函数首先判断某个Pod（被谴责的目标）是否处于运行和可用状态，然后判断是否处于单调模式，并且被谴责的目标不是第一个不健康的Pod。
	//如果满足这些条件，函数会返回true，表示需要等待Pod变为可用状态后再进行缩容操作。否则，函数返回nil，表示可以进行缩容操作。

	logger.V(2).Info(&amp;#34;Pod of StatefulSet is terminating for scale down&amp;#34;,
		&amp;#34;statefulSet&amp;#34;, klog.KObj(set), &amp;#34;pod&amp;#34;, klog.KObj(condemned[i]))
	return true, ssc.podControl.DeleteStatefulPod(set, condemned[i])
}

// 该函数是一个日志记录和操作Pod的函数。
// 它首先使用logger.V(2).Info记录一条日志信息，表示某个StatefulSet的Pod正在因为缩容而终止。
// 日志中包含了StatefulSet和Pod的相关信息。
// 然后，函数通过调用ssc.podControl.DeleteStatefulPod方法来删除指定的Pod，并返回true表示删除成功。
func runForAll(pods []*v1.Pod, fn func(i int) (bool, error), monotonic bool) (bool, error) {
	if monotonic {
		for i := range pods {
			if shouldExit, err := fn(i); shouldExit || err != nil {
				return true, err
			}
		}
	} else {
		if _, err := slowStartBatch(1, len(pods), fn); err != nil {
			return true, err
		}
	}
	return false, nil
}

//该函数runForAll接受三个参数：
//一个pods的切片，一个函数fn，和一个布尔值monotonic。
//根据monotonic的值，函数会以不同的方式调用fn函数。
//如果monotonic为true，则会顺序遍历pods切片，并依次调用fn函数，传入当前索引i作为参数。
//如果fn函数返回true或出现错误，则会立即返回true和错误。
//如果monotonic为false，则会调用slowStartBatch函数，以一定的延迟并发地调用fn函数。
//最终，如果所有fn函数调用都成功完成，则返回false和nil。

// updateStatefulSet performs the update function for a StatefulSet. This method creates, updates, and deletes Pods in
// the set in order to conform the system to the target state for the set. The target state always contains
// set.Spec.Replicas Pods with a Ready Condition. If the UpdateStrategy.Type for the set is
// RollingUpdateStatefulSetStrategyType then all Pods in the set must be at set.Status.CurrentRevision.
// If the UpdateStrategy.Type for the set is OnDeleteStatefulSetStrategyType, the target state implies nothing about
// the revisions of Pods in the set. If the UpdateStrategy.Type for the set is PartitionStatefulSetStrategyType, then
// all Pods with ordinal less than UpdateStrategy.Partition.Ordinal must be at Status.CurrentRevision and all other
// Pods must be at Status.UpdateRevision. If the returned error is nil, the returned StatefulSetStatus is valid and the
// update must be recorded. If the error is not nil, the method should be retried until successful.
func (ssc *defaultStatefulSetControl) updateStatefulSet(
	ctx context.Context,
	set *apps.StatefulSet,
	currentRevision *apps.ControllerRevision,
	updateRevision *apps.ControllerRevision,
	collisionCount int32,
	pods []*v1.Pod) (*apps.StatefulSetStatus, error) {
	logger := klog.FromContext(ctx)
	//该函数是用于更新StatefulSet的状态的。
	//它会根据目标状态来创建、更新和删除Pod，以使系统符合StatefulSet的目标状态。
	//目标状态始终包含具有Ready Condition的set.Spec.Replicas Pods。
	//如果StatefulSet的UpdateStrategy.Type为RollingUpdateStatefulSetStrategyType，
	//则集合中的所有Pod必须处于set.Status.CurrentRevision。
	//如果UpdateStrategy.Type为OnDeleteStatefulSetStrategyType，
	//则目标状态与集合中Pod的修订版本无关。
	//如果UpdateStrategy.Type为PartitionStatefulSetStrategyType，
	//则所有序号小于UpdateStrategy.Partition.Ordinal的Pod必须处于Status.CurrentRevision，
	//而所有其他Pod必须处于Status.UpdateRevision。
	//如果返回的错误为nil，则返回的StatefulSetStatus是有效的，并且必须记录更新。
	//如果错误不为nil，则应重试该方法，直到成功。
	// get the current and update revisions of the set.
	currentSet, err := ApplyRevision(set, currentRevision)
	if err != nil {
		return nil, err
	}
	updateSet, err := ApplyRevision(set, updateRevision)
	if err != nil {
		return nil, err
	}
	//这段Go代码中的函数功能是：根据给定的集合和两个修订版本号，分别获取当前修订版本对应的集合内容和更新修订版本对应的集合内容。
	//具体描述如下：
	//1. 首先，函数使用ApplyRevision函数和给定的集合以及当前修订版本号currentRevision，获取当前修订版本对应的集合内容，并将结果赋值给currentSet变量。
	//如果出现错误，则返回nil和错误信息。
	//2. 然后，函数使用ApplyRevision函数和给定的集合以及更新修订版本号updateRevision，获取更新修订版本对应的集合内容，并将结果赋值给updateSet变量。
	//如果出现错误，则返回nil和错误信息。
	//需要注意的是，该函数的返回值类型为(interface{}, error)，
	//其中interface{}表示返回的内容可以是任意类型，error表示返回的错误信息。

	// set the generation, and revisions in the returned status
	status := apps.StatefulSetStatus{}
	status.ObservedGeneration = set.Generation
	status.CurrentRevision = currentRevision.Name
	status.UpdateRevision = updateRevision.Name
	status.CollisionCount = new(int32)
	*status.CollisionCount = collisionCount
	//该Go函数主要实现了设置StatefulSetStatus结构体中ObservedGeneration、CurrentRevision、UpdateRevision和CollisionCount字段的值。
	//- 首先，创建了一个空的StatefulSetStatus结构体变量status。
	//- 然后，将set.Generation的值赋给status.ObservedGeneration字段。
	//- 接着，将currentRevision.Name的值赋给status.CurrentRevision字段。
	//- 再将updateRevision.Name的值赋给status.UpdateRevision字段。
	//- 最后，创建一个新的int32类型指针变量status.CollisionCount并将其值设为collisionCount。
	//总结来说，该函数的作用是通过给StatefulSetStatus结构体的字段赋值，来初始化或更新一个StatefulSet的状态信息。

	updateStatus(&amp;amp;status, set.Spec.MinReadySeconds, currentRevision, updateRevision, pods)
	//该函数用于更新状态。根据参数设置的最小就绪秒数、当前修订版本、更新修订版本和Pods，来更新状态对象status。

	replicaCount := int(*set.Spec.Replicas)
	// slice that will contain all Pods such that getStartOrdinal(set) &amp;lt;= getOrdinal(pod) &amp;lt;= getEndOrdinal(set)
	replicas := make([]*v1.Pod, replicaCount)
	// slice that will contain all Pods such that getOrdinal(pod) &amp;lt; getStartOrdinal(set) OR getOrdinal(pod) &amp;gt; getEndOrdinal(set)
	condemned := make([]*v1.Pod, 0, len(pods))
	unhealthy := 0
	var firstUnhealthyPod *v1.Pod

	// First we partition pods into two lists valid replicas and condemned Pods
	for _, pod := range pods {
		if podInOrdinalRange(pod, set) {
			// if the ordinal of the pod is within the range of the current number of replicas,
			// insert it at the indirection of its ordinal
			replicas[getOrdinal(pod)-getStartOrdinal(set)] = pod
		} else if getOrdinal(pod) &amp;gt;= 0 {
			// if the ordinal is valid, but not within the range add it to the condemned list
			condemned = append(condemned, pod)
		}
		// If the ordinal could not be parsed (ord &amp;lt; 0), ignore the Pod.
	}
	//该函数根据给定的Pod集合和Deployment规格，
	//将Pod分为两类：有效副本和废弃Pod。
	//有效副本是指其序号在当前副本数量范围内且序号有效的Pod，
	//而废弃Pod是指序号不在当前副本数量范围内但序号有效的Pod。
	//函数通过遍历Pod集合，根据Pod的序号与当前副本数量范围的比较，将Pod分别加入到有效副本和废弃Pod的切片中。

	// for any empty indices in the sequence [0,set.Spec.Replicas) create a new Pod at the correct revision
	for ord := getStartOrdinal(set); ord &amp;lt;= getEndOrdinal(set); ord++ {
		replicaIdx := ord - getStartOrdinal(set)
		if replicas[replicaIdx] == nil {
			replicas[replicaIdx] = newVersionedStatefulSetPod(
				currentSet,
				updateSet,
				currentRevision.Name,
				updateRevision.Name, ord)
		}
	}
	//该Go函数的功能是在有空缺索引的位置上创建新的Pod。
	//它根据set的规格，从起始序号到结束序号遍历，如果在replicas数组中对应索引位置上的Pod为空，则创建一个新的有正确修订版本的Pod，并将其赋值给该索引位置。

	// sort the condemned Pods by their ordinals
	sort.Sort(descendingOrdinal(condemned))
	//该函数是一个排序函数，用于将一个名为condemned的Pod切片按照其序号降序排序。
	//其中，descendingOrdinal是一个自定义的排序类型，实现了sort.Interface接口的Sort方法，通过比较Pod的序号来实现降序排序。

	// find the first unhealthy Pod
	for i := range replicas {
		if !isHealthy(replicas[i]) {
			unhealthy++
			if firstUnhealthyPod == nil {
				firstUnhealthyPod = replicas[i]
			}
		}
	}
	//该函数用于遍历一组Pod副本，查找第一个不健康的状态。
	//具体操作为循环遍历replicas切片，通过调用isHealthy函数判断每个副本的健康状态。
	//如果不健康，则将unhealthy计数加1，并且如果firstUnhealthyPod为空，则将其赋值为当前副本。

	// or the first unhealthy condemned Pod (condemned are sorted in descending order for ease of use)
	for i := len(condemned) - 1; i &amp;gt;= 0; i-- {
		if !isHealthy(condemned[i]) {
			unhealthy++
			if firstUnhealthyPod == nil {
				firstUnhealthyPod = condemned[i]
			}
		}
	}
	//该Go代码片段是一个for循环，遍历一个名为condemned的切片。
	//循环从condemned切片的最后一个元素开始，倒序遍历。
	//在每次迭代中，它调用isHealthy函数来检查当前Pod的健康状况。
	//如果Pod不健康，则将unhealthy计数器加1，并检查是否是第一个不健康的Pod。
	//如果是，则将其赋值给firstUnhealthyPod变量。
	//该循环的主要目的是找到第一个不健康的Pod，并统计不健康Pod的数量。

	if unhealthy &amp;gt; 0 {
		logger.V(4).Info(&amp;#34;StatefulSet has unhealthy Pods&amp;#34;, &amp;#34;statefulSet&amp;#34;, klog.KObj(set), &amp;#34;unhealthyReplicas&amp;#34;, unhealthy, &amp;#34;pod&amp;#34;, klog.KObj(firstUnhealthyPod))
	}
	//这段Go代码是一个条件判断语句，判断变量unhealthy是否大于0，如果大于0，则调用logger.V(4).Info方法输出一条日志信息。
	//日志信息的内容包括：&amp;#34;StatefulSet has unhealthy Pods&amp;#34;，&amp;#34;statefulSet&amp;#34;，&amp;#34;unhealthyReplicas&amp;#34;，&amp;#34;pod&amp;#34;。
	//其中，&amp;#34;statefulSet&amp;#34;和&amp;#34;pod&amp;#34;是通过klog.KObj方法获取的对象的字符串表示形式。

	// If the StatefulSet is being deleted, don&amp;#39;t do anything other than updating
	// status.
	if set.DeletionTimestamp != nil {
		return &amp;amp;status, nil
	}
	//该函数用于判断一个StatefulSet对象是否正在被删除。如果是，则只更新状态信息，不做其他操作。
	//函数通过检查StatefulSet对象的DeletionTimestamp字段是否为nil来判断是否正在被删除。
	//如果DeletionTimestamp不为nil，则说明该对象正在被删除，函数会返回当前状态信息和nil。
	//否则，函数会继续执行其他操作。

	monotonic := !allowsBurst(set)

	// First, process each living replica. Exit if we run into an error or something blocking in monotonic mode.
	processReplicaFn := func(i int) (bool, error) {
		return ssc.processReplica(ctx, set, currentRevision, updateRevision, currentSet, updateSet, monotonic, replicas, i)
	}
	//这段Go代码定义了一个函数和一个内部函数。
	//函数monotonic := !allowsBurst(set)根据set参数的值计算一个布尔值monotonic。
	//allowsBurst是一个外部函数，其作用是判断是否允许突发（burst），
	//这里通过取反操作得到monotonic，即不允许突发。
	//内部函数processReplicaFn := func(i int) (bool, error) {...}定义了一个闭包函数，
	//用于处理每个存活的副本。该函数接受一个整数参数i，返回一个布尔值和一个错误。
	//在函数体中，调用了ssc.processReplica函数来处理副本，
	//该函数接受多个参数，包括上下文、集合、当前修订版本、更新修订版本、当前集合、更新集合、是否单调以及副本列表和副本索引。
	//该内部函数的主要作用是遍历副本并逐个处理它们，如果在单调模式下遇到错误或阻塞，则退出处理。

	if shouldExit, err := runForAll(replicas, processReplicaFn, monotonic); shouldExit || err != nil {
		updateStatus(&amp;amp;status, set.Spec.MinReadySeconds, currentRevision, updateRevision, replicas, condemned)
		return &amp;amp;status, err
	}
	//该函数中，runForAll()函数执行processReplicaFn函数操作replicas，并返回一个shouldExit标志和一个错误err。
	//如果shouldExit为真或err不为nil，则更新状态并返回。

	// Fix pod claims for condemned pods, if necessary.
	if utilfeature.DefaultFeatureGate.Enabled(features.StatefulSetAutoDeletePVC) {
		fixPodClaim := func(i int) (bool, error) {
			if matchPolicy, err := ssc.podControl.ClaimsMatchRetentionPolicy(ctx, updateSet, condemned[i]); err != nil {
				return true, err
			} else if !matchPolicy {
				if err := ssc.podControl.UpdatePodClaimForRetentionPolicy(ctx, updateSet, condemned[i]); err != nil {
					return true, err
				}
			}
			return false, nil
		}
		if shouldExit, err := runForAll(condemned, fixPodClaim, monotonic); shouldExit || err != nil {
			updateStatus(&amp;amp;status, set.Spec.MinReadySeconds, currentRevision, updateRevision, replicas, condemned)
			return &amp;amp;status, err
		}
	}
	//这段Go代码是用于修复有状态副本集（StatefulSet）中被废弃的Pod的声明（Claim）的。
	//如果启用了features.StatefulSetAutoDeletePVC特性，并且ssc.podControl.ClaimsMatchRetentionPolicy函数返回错误或者返回的匹配策略为false，
	//则会调用ssc.podControl.UpdatePodClaimForRetentionPolicy函数来更新Pod的声明。
	//runForAll函数会遍历所有被废弃的Pod，并调用fixPodClaim函数进行修复。
	//如果runForAll函数返回需要退出或者发生了错误，则会更新副本集的状态并返回。

	// At this point, in monotonic mode all of the current Replicas are Running, Ready and Available,
	// and we can consider termination.
	// We will wait for all predecessors to be Running and Ready prior to attempting a deletion.
	// We will terminate Pods in a monotonically decreasing order.
	// Note that we do not resurrect Pods in this interval. Also note that scaling will take precedence over
	// updates.
	processCondemnedFn := func(i int) (bool, error) {
		return ssc.processCondemned(ctx, set, firstUnhealthyPod, monotonic, condemned, i)
	}
	if shouldExit, err := runForAll(condemned, processCondemnedFn, monotonic); shouldExit || err != nil {
		updateStatus(&amp;amp;status, set.Spec.MinReadySeconds, currentRevision, updateRevision, replicas, condemned)
		return &amp;amp;status, err
	}
	//这段Go代码中的函数是一个处理被宣判终止的Pod的函数。
	//在函数中，它会按照单调递减的顺序终止Pods，并且在终止Pod之前会等待所有前置Pod都处于Running和Ready状态。
	//函数会调用ssc.processCondemned来处理每个被宣判终止的Pod。
	//如果函数执行完毕或者出现错误，它会更新状态并返回该状态和错误信息。

	updateStatus(&amp;amp;status, set.Spec.MinReadySeconds, currentRevision, updateRevision, replicas, condemned)

	// for the OnDelete strategy we short circuit. Pods will be updated when they are manually deleted.
	if set.Spec.UpdateStrategy.Type == apps.OnDeleteStatefulSetStrategyType {
		return &amp;amp;status, nil
	}

	if utilfeature.DefaultFeatureGate.Enabled(features.MaxUnavailableStatefulSet) {
		return updateStatefulSetAfterInvariantEstablished(ctx,
			ssc,
			set,
			replicas,
			updateRevision,
			status,
		)
	}
	//该函数用于更新状态fulset的状态。
	//根据输入参数的不同，函数会有不同的行为。
	//如果状态fulset的更新策略为OnDelete，则函数会直接返回当前状态。
	//如果启用了MaxUnavailableStatefulSet特性，则会调用updateStatefulSetAfterInvariantEstablished函数进行进一步的状态更新。

	// we compute the minimum ordinal of the target sequence for a destructive update based on the strategy.
	updateMin := 0
	if set.Spec.UpdateStrategy.RollingUpdate != nil {
		updateMin = int(*set.Spec.UpdateStrategy.RollingUpdate.Partition)
	}
	// we terminate the Pod with the largest ordinal that does not match the update revision.
	for target := len(replicas) - 1; target &amp;gt;= updateMin; target-- {

		// delete the Pod if it is not already terminating and does not match the update revision.
		if getPodRevision(replicas[target]) != updateRevision.Name &amp;amp;&amp;amp; !isTerminating(replicas[target]) {
			logger.V(2).Info(&amp;#34;Pod of StatefulSet is terminating for update&amp;#34;,
				&amp;#34;statefulSet&amp;#34;, klog.KObj(set), &amp;#34;pod&amp;#34;, klog.KObj(replicas[target]))
			if err := ssc.podControl.DeleteStatefulPod(set, replicas[target]); err != nil {
				if !errors.IsNotFound(err) {
					return &amp;amp;status, err
				}
			}
			status.CurrentReplicas--
			return &amp;amp;status, err
		}
		//该Go函数用于根据策略计算目标序列的最小序数值，用于破坏性更新。
		//接着，函数会终止与更新修订版本不匹配的最大序号的Pod。
		//具体流程如下：
		//1. 初始化updateMin为0。
		//2. 如果set.Spec.UpdateStrategy.RollingUpdate不为nil，则将updateMin设置为*set.Spec.UpdateStrategy.RollingUpdate.Partition的整数值。
		//3. 从replicas的最后一个元素开始，迭代到updateMin（包括updateMin）。
		//4. 如果当前Pod的修订版本与更新修订版本不匹配且未终止，则记录日志，并尝试删除该Pod。若删除成功，将status.CurrentReplicas减1，
		//并返回status和错误信息err。
		//5. 若删除Pod时出现错误且错误类型不是NotFound，则返回status和该错误。
		//注意：函数中的replicas是Pod的副本列表，updateRevision.Name是更新的修订版本名称，ssc.podControl.DeleteStatefulPod是删除Pod的方法，logger.V(2).Info是记录日志的方法。
		// wait for unhealthy Pods on update
		if !isHealthy(replicas[target]) {
			logger.V(4).Info(&amp;#34;StatefulSet is waiting for Pod to update&amp;#34;,
				&amp;#34;statefulSet&amp;#34;, klog.KObj(set), &amp;#34;pod&amp;#34;, klog.KObj(replicas[target]))
			return &amp;amp;status, nil
		}

	}
	return &amp;amp;status, nil
}

//这段Go代码是一个条件判断语句，其主要功能是检查StatefulSet中的Pod是否健康。
//具体来说，函数首先检查replicas[target]是否健康，如果不健康，则记录日志并返回当前状态status。
//如果replicas[target]健康，则直接返回当前状态status。

func updateStatefulSetAfterInvariantEstablished(
	ctx context.Context,
	ssc *defaultStatefulSetControl,
	set *apps.StatefulSet,
	replicas []*v1.Pod,
	updateRevision *apps.ControllerRevision,
	status apps.StatefulSetStatus,
) (*apps.StatefulSetStatus, error) {
	//该函数是在建立不变性后更新StatefulSet的状态。
	//它接收上下文、StatefulSet控制对象、StatefulSet实例、Pod实例数组、控制器修订版本和StatefulSet状态作为参数，
	//并返回更新后的StatefulSet状态和错误信息。
	//函数的主要逻辑是更新StatefulSet的状态，包括当前修订版本、复制集和条件等，并返回更新后的状态。

	logger := klog.FromContext(ctx)
	replicaCount := int(*set.Spec.Replicas)
	//该代码片段是用Go语言编写的，其中定义了两个变量logger和replicaCount。
	//- logger := klog.FromContext(ctx)：该行代码从ctx上下文中获取了一个名为logger的变量。
	//该变量很可能是用于日志记录的一种日志器对象。
	//klog是一个用于日志记录的Go库，FromContext函数可能是用于从上下文中提取特定的日志器对象。
	//- replicaCount := int(*set.Spec.Replicas)：该行代码定义了一个名为replicaCount的整型变量，
	//并将其初始化为set.Spec.Replicas指针指向的值的整型表示。根据变量的命名，该变量可能是用于存储某个集合的副本数量。
	//set的类型和Spec的定义没有给出，因此无法确定set.Spec.Replicas的具体类型和含义。
	//但是根据常规的命名习惯，Spec通常表示某个对象的规格说明，而Replicas则通常表示副本数量。
	//因此，可以推测set.Spec.Replicas可能是一个指针，指向一个表示副本数量的整数值。通过将其转换为int类型，可以将该值用于后续的逻辑处理或计算。

	// we compute the minimum ordinal of the target sequence for a destructive update based on the strategy.
	updateMin := 0
	maxUnavailable := 1
	if set.Spec.UpdateStrategy.RollingUpdate != nil {
		updateMin = int(*set.Spec.UpdateStrategy.RollingUpdate.Partition)

		// if the feature was enabled and then later disabled, MaxUnavailable may have a value
		// more than 1. Ignore the passed in value and Use maxUnavailable as 1 to enforce
		// expected behavior when feature gate is not enabled.
		var err error
		maxUnavailable, err = getStatefulSetMaxUnavailable(set.Spec.UpdateStrategy.RollingUpdate.MaxUnavailable, replicaCount)
		if err != nil {
			return &amp;amp;status, err
		}
	}
	//这段Go代码中的函数主要作用是根据给定的策略计算目标序列的最小序数值，用于破坏性更新。
	//函数首先将更新的最小序数值初始化为0，将最大不可用数值初始化为1。
	//然后，如果设置了更新策略的RollingUpdate属性，将更新的最小序数值设置为RollingUpdate中的Partition属性的值。
	//接着，根据RollingUpdate中的MaxUnavailable属性和副本数量，获取最大不可用数值，并返回错误信息。

	// Collect all targets in the range between getStartOrdinal(set) and getEndOrdinal(set). Count any targets in that range
	// that are unhealthy i.e. terminated or not running and ready as unavailable). Select the
	// (MaxUnavailable - Unavailable) Pods, in order with respect to their ordinal for termination. Delete
	// those pods and count the successful deletions. Update the status with the correct number of deletions.
	unavailablePods := 0
	for target := len(replicas) - 1; target &amp;gt;= 0; target-- {
		if !isHealthy(replicas[target]) {
			unavailablePods++
		}
	}
	//该函数用于收集在指定范围内的所有目标，并计算其中不健康的目标数量。
	//然后，它选择最多不可用的Pods，并按照它们的序号进行终止。
	//函数会删除这些Pods，并统计成功删除的数量，最后更新状态信息。
	//函数使用一个循环遍历所有的副本，并通过isHealthy函数判断每个副本的健康状况，如果不健康则增加不可用Pods的计数。

	if unavailablePods &amp;gt;= maxUnavailable {
		logger.V(2).Info(&amp;#34;StatefulSet found unavailablePods, more than or equal to allowed maxUnavailable&amp;#34;,
			&amp;#34;statefulSet&amp;#34;, klog.KObj(set),
			&amp;#34;unavailablePods&amp;#34;, unavailablePods,
			&amp;#34;maxUnavailable&amp;#34;, maxUnavailable)
		return &amp;amp;status, nil
	}
	//该函数是一个条件判断语句，判断unavailablePods是否大于等于maxUnavailable。如果是，则通过logger记录信息并返回status和nil。

	// Now we need to delete MaxUnavailable- unavailablePods
	// start deleting one by one starting from the highest ordinal first
	podsToDelete := maxUnavailable - unavailablePods
	//这个Go函数的功能是计算需要删除的Pod数量。它根据MaxUnavailable和unavailablePods的差值，确定需要删除的Pod数量，并将结果存储在podsToDelete变量中。

	deletedPods := 0
	for target := len(replicas) - 1; target &amp;gt;= updateMin &amp;amp;&amp;amp; deletedPods &amp;lt; podsToDelete; target-- {

		// delete the Pod if it is healthy and the revision doesnt match the target
		if getPodRevision(replicas[target]) != updateRevision.Name &amp;amp;&amp;amp; !isTerminating(replicas[target]) {
			// delete the Pod if it is healthy and the revision doesnt match the target
			logger.V(2).Info(&amp;#34;StatefulSet terminating Pod for update&amp;#34;,
				&amp;#34;statefulSet&amp;#34;, klog.KObj(set),
				&amp;#34;pod&amp;#34;, klog.KObj(replicas[target]))
			if err := ssc.podControl.DeleteStatefulPod(set, replicas[target]); err != nil {
				if !errors.IsNotFound(err) {
					return &amp;amp;status, err
				}
			}
			deletedPods++
			status.CurrentReplicas--
		}
	}
	return &amp;amp;status, nil
}

//该函数用于删除StatefulSet中的一部分Pod。
//它根据指定的条件遍历replicas列表，并删除满足条件的Pod。
//具体而言，它从replicas列表的末尾开始遍历，
//如果Pod的修订版本号与目标修订版本号不匹配且Pod未终止，则删除该Pod。
//删除Pod时，如果出现错误（除找不到Pod的错误外），则返回错误信息。函数返回更新后的状态信息和nil。

// updateStatefulSetStatus updates set&amp;#39;s Status to be equal to status. If status indicates a complete update, it is
// mutated to indicate completion. If status is semantically equivalent to set&amp;#39;s Status no update is performed. If the
// returned error is nil, the update is successful.
func (ssc *defaultStatefulSetControl) updateStatefulSetStatus(
	ctx context.Context,
	set *apps.StatefulSet,
	status *apps.StatefulSetStatus) error {
	// complete any in progress rolling update if necessary
	completeRollingUpdate(set, status)

	// if the status is not inconsistent do not perform an update
	if !inconsistentStatus(set, status) {
		return nil
	}

	// copy set and update its status
	set = set.DeepCopy()
	if err := ssc.statusUpdater.UpdateStatefulSetStatus(ctx, set, status); err != nil {
		return err
	}

	return nil
}

var _ StatefulSetControlInterface = &amp;amp;defaultStatefulSetControl{}

//该函数用于更新StatefulSet的状态。
//它首先完成任何正在进行的滚动更新，然后检查状态是否一致，如果不一致则进行更新。
//更新时会创建StatefulSet的深拷贝，并调用statusUpdater的UpdateStatefulSetStatus方法进行更新。
//如果更新成功，则返回nil；否则返回错误。
&lt;/code>&lt;/pre></description></item><item><title>2024-04-10 K8S控制器之stateful_set_status_update.go源码解读</title><link>https://qq547475331.github.io/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Bstateful_set_status_updatego%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-statefulsetstatusupdatego-yuan-ma-jie-du/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Bstateful_set_status_updatego%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-statefulsetstatusupdatego-yuan-ma-jie-du/</guid><description>&lt;pre tabindex="0">&lt;code>/*
Copyright 2017 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the &amp;#34;License&amp;#34;);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

 http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &amp;#34;AS IS&amp;#34; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package statefulset

import (
	&amp;#34;context&amp;#34;
	&amp;#34;fmt&amp;#34;

	apps &amp;#34;k8s.io/api/apps/v1&amp;#34;
	metav1 &amp;#34;k8s.io/apimachinery/pkg/apis/meta/v1&amp;#34;
	utilruntime &amp;#34;k8s.io/apimachinery/pkg/util/runtime&amp;#34;
	clientset &amp;#34;k8s.io/client-go/kubernetes&amp;#34;
	appslisters &amp;#34;k8s.io/client-go/listers/apps/v1&amp;#34;
	&amp;#34;k8s.io/client-go/util/retry&amp;#34;
)

// StatefulSetStatusUpdaterInterface is an interface used to update the StatefulSetStatus associated with a StatefulSet.
// For any use other than testing, clients should create an instance using NewRealStatefulSetStatusUpdater.
type StatefulSetStatusUpdaterInterface interface {
	// UpdateStatefulSetStatus sets the set&amp;#39;s Status to status. Implementations are required to retry on conflicts,
	// but fail on other errors. If the returned error is nil set&amp;#39;s Status has been successfully set to status.
	UpdateStatefulSetStatus(ctx context.Context, set *apps.StatefulSet, status *apps.StatefulSetStatus) error
}

//这段代码定义了一个名为StatefulSetStatusUpdaterInterface的接口，其中包含一个方法UpdateStatefulSetStatus。
//该方法用于更新与StatefulSet关联的StatefulSetStatus。
//接口的实现需要在发生冲突时进行重试，但在其他错误情况下失败。
//如果返回的错误为nil，则表示set的状态已成功设置为status。

// NewRealStatefulSetStatusUpdater returns a StatefulSetStatusUpdaterInterface that updates the Status of a StatefulSet,
// using the supplied client and setLister.
func NewRealStatefulSetStatusUpdater(
	client clientset.Interface,
	setLister appslisters.StatefulSetLister) StatefulSetStatusUpdaterInterface {
	return &amp;amp;realStatefulSetStatusUpdater{client, setLister}
}

//该函数返回一个StatefulSetStatusUpdaterInterface接口，
//用于更新StatefulSet的状态。
//它接受一个clientset.Interface类型的client和一个appslisters.StatefulSetLister类型的setLister作为参数，
//并将它们封装到realStatefulSetStatusUpdater结构体中，然后返回该结构体的指针。

type realStatefulSetStatusUpdater struct {
	client clientset.Interface
	setLister appslisters.StatefulSetLister
}

func (ssu *realStatefulSetStatusUpdater) UpdateStatefulSetStatus(
	ctx context.Context,
	set *apps.StatefulSet,
	status *apps.StatefulSetStatus) error {
	// don&amp;#39;t wait due to limited number of clients, but backoff after the default number of steps
	return retry.RetryOnConflict(retry.DefaultRetry, func() error {
		set.Status = *status
		// TODO: This context.TODO should use a real context once we have RetryOnConflictWithContext
		_, updateErr := ssu.client.AppsV1().StatefulSets(set.Namespace).UpdateStatus(context.TODO(), set, metav1.UpdateOptions{})
		if updateErr == nil {
			return nil
		}
		if updated, err := ssu.setLister.StatefulSets(set.Namespace).Get(set.Name); err == nil {
			// make a copy so we don&amp;#39;t mutate the shared cache
			set = updated.DeepCopy()
		} else {
			utilruntime.HandleError(fmt.Errorf(&amp;#34;error getting updated StatefulSet %s/%s from lister: %v&amp;#34;, set.Namespace, set.Name, err))
		}

		return updateErr
	})
}

//这段Go代码定义了一个realStatefulSetStatusUpdater结构体和一个UpdateStatefulSetStatus方法。
//realStatefulSetStatusUpdater结构体包含两个字段：client和setLister，分别用于与Kubernetes API通信和获取StatefulSet列表。
//UpdateStatefulSetStatus方法用于更新指定StatefulSet的状态。它使用retry.RetryOnConflict函数进行重试，
//以处理并发更新冲突的情况。在重试的回调函数中，将新状态赋值给set.Status，
//然后调用UpdateStatus方法更新StatefulSet的状态。
//如果更新失败，将尝试从setLister获取最新的StatefulSet副本，并再次尝试更新。
//如果获取最新StatefulSet时出现错误，将记录错误信息。
//总结来说，这个函数的作用是更新指定StatefulSet的状态，处理并发更新冲突，并在更新失败时进行重试。

var _ StatefulSetStatusUpdaterInterface = &amp;amp;realStatefulSetStatusUpdater{}
&lt;/code>&lt;/pre></description></item><item><title>2024-04-10 K8S控制器之stateful_set.go源码解读</title><link>https://qq547475331.github.io/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Bstateful_setgo%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-statefulsetgo-yuan-ma-jie-du/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Bstateful_setgo%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-statefulsetgo-yuan-ma-jie-du/</guid><description>&lt;pre tabindex="0">&lt;code>/*
Copyright 2016 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the &amp;#34;License&amp;#34;);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

 http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &amp;#34;AS IS&amp;#34; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package statefulset

import (
	&amp;#34;context&amp;#34;
	&amp;#34;fmt&amp;#34;
	&amp;#34;reflect&amp;#34;
	&amp;#34;time&amp;#34;

	apps &amp;#34;k8s.io/api/apps/v1&amp;#34;
	v1 &amp;#34;k8s.io/api/core/v1&amp;#34;
	&amp;#34;k8s.io/apimachinery/pkg/api/errors&amp;#34;
	metav1 &amp;#34;k8s.io/apimachinery/pkg/apis/meta/v1&amp;#34;
	&amp;#34;k8s.io/apimachinery/pkg/labels&amp;#34;
	utilruntime &amp;#34;k8s.io/apimachinery/pkg/util/runtime&amp;#34;
	&amp;#34;k8s.io/apimachinery/pkg/util/wait&amp;#34;
	appsinformers &amp;#34;k8s.io/client-go/informers/apps/v1&amp;#34;
	coreinformers &amp;#34;k8s.io/client-go/informers/core/v1&amp;#34;
	clientset &amp;#34;k8s.io/client-go/kubernetes&amp;#34;
	&amp;#34;k8s.io/client-go/kubernetes/scheme&amp;#34;
	v1core &amp;#34;k8s.io/client-go/kubernetes/typed/core/v1&amp;#34;
	appslisters &amp;#34;k8s.io/client-go/listers/apps/v1&amp;#34;
	corelisters &amp;#34;k8s.io/client-go/listers/core/v1&amp;#34;
	&amp;#34;k8s.io/client-go/tools/cache&amp;#34;
	&amp;#34;k8s.io/client-go/tools/record&amp;#34;
	&amp;#34;k8s.io/client-go/util/workqueue&amp;#34;
	podutil &amp;#34;k8s.io/kubernetes/pkg/api/v1/pod&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/controller&amp;#34;
	&amp;#34;k8s.io/kubernetes/pkg/controller/history&amp;#34;
)

// controllerKind contains the schema.GroupVersionKind for this controller type.
var controllerKind = apps.SchemeGroupVersion.WithKind(&amp;#34;StatefulSet&amp;#34;)

// StatefulSetController controls statefulsets.
type StatefulSetController struct {
	// client interface
	kubeClient clientset.Interface
	// control returns an interface capable of syncing a stateful set.
	// Abstracted out for testing.
	control StatefulSetControlInterface
	// podControl is used for patching pods.
	podControl controller.PodControlInterface
	// podLister is able to list/get pods from a shared informer&amp;#39;s store
	podLister corelisters.PodLister
	// podListerSynced returns true if the pod shared informer has synced at least once
	podListerSynced cache.InformerSynced
	// setLister is able to list/get stateful sets from a shared informer&amp;#39;s store
	setLister appslisters.StatefulSetLister
	// setListerSynced returns true if the stateful set shared informer has synced at least once
	setListerSynced cache.InformerSynced
	// pvcListerSynced returns true if the pvc shared informer has synced at least once
	pvcListerSynced cache.InformerSynced
	// revListerSynced returns true if the rev shared informer has synced at least once
	revListerSynced cache.InformerSynced
	// StatefulSets that need to be synced.
	queue workqueue.RateLimitingInterface
	// eventBroadcaster is the core of event processing pipeline.
	eventBroadcaster record.EventBroadcaster
}

// 这段Go代码定义了一个名为StatefulSetController的结构体，用于控制StatefulSets。
// 它包含了一系列的成员变量，包括client接口、控制StatefulSet同步的接口、用于patching pods的接口、以及一系列的Lister和Synced函数，
// 用于从共享informer的存储中获取Pods、StatefulSets、PVCs等信息。
// 此外，它还定义了一个工作队列和一个事件广播器。这个结构体主要用于管理和同步Kubernetes中的StatefulSets。
// NewStatefulSetController creates a new statefulset controller.
func NewStatefulSetController(
	ctx context.Context,
	podInformer coreinformers.PodInformer,
	setInformer appsinformers.StatefulSetInformer,
	pvcInformer coreinformers.PersistentVolumeClaimInformer,
	revInformer appsinformers.ControllerRevisionInformer,
	kubeClient clientset.Interface,
) *StatefulSetController {
	logger := klog.FromContext(ctx)
	eventBroadcaster := record.NewBroadcaster(record.WithContext(ctx))
	recorder := eventBroadcaster.NewRecorder(scheme.Scheme, v1.EventSource{Component: &amp;#34;statefulset-controller&amp;#34;})
	ssc := &amp;amp;StatefulSetController{
		kubeClient: kubeClient,
		control: NewDefaultStatefulSetControl(
			NewStatefulPodControl(
				kubeClient,
				podInformer.Lister(),
				pvcInformer.Lister(),
				recorder),
			NewRealStatefulSetStatusUpdater(kubeClient, setInformer.Lister()),
			history.NewHistory(kubeClient, revInformer.Lister()),
			recorder,
		),
		pvcListerSynced: pvcInformer.Informer().HasSynced,
		revListerSynced: revInformer.Informer().HasSynced,
		queue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), &amp;#34;statefulset&amp;#34;),
		podControl: controller.RealPodControl{KubeClient: kubeClient, Recorder: recorder},

		eventBroadcaster: eventBroadcaster,
	}
	//该函数用于创建一个新的StatefulSet控制器。它接收多个参数，包括上下文、各种Informer、客户端接口等。
	//函数内部通过调用NewDefaultStatefulSetControl函数和其他函数，初始化了StatefulSetController结构体的各个字段，
	//并返回一个指向StatefulSetController结构体的指针。
	podInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
		// lookup the statefulset and enqueue
		AddFunc: func(obj interface{}) {
			ssc.addPod(logger, obj)
		},
		// lookup current and old statefulset if labels changed
		UpdateFunc: func(oldObj, newObj interface{}) {
			ssc.updatePod(logger, oldObj, newObj)
		},
		// lookup statefulset accounting for deletion tombstones
		DeleteFunc: func(obj interface{}) {
			ssc.deletePod(logger, obj)
		},
	})
	//这段代码定义了一个Go函数，它为podInformer的Informer添加了一个事件处理程序。
	//该事件处理程序使用cache.ResourceEventHandlerFuncs结构体中的AddFunc、UpdateFunc和DeleteFunc来处理资源的添加、更新和删除事件。
	//- AddFunc函数在资源添加时调用ssc.addPod(logger, obj)来添加Pod。
	//- UpdateFunc函数在资源更新时调用ssc.updatePod(logger, oldObj, newObj)来更新Pod。
	//- DeleteFunc函数在资源删除时调用ssc.deletePod(logger, obj)来删除Pod。
	//这些函数通过传入的logger和相应的资源对象来执行具体的操作。
	ssc.podLister = podInformer.Lister()
	ssc.podListerSynced = podInformer.Informer().HasSynced

	setInformer.Informer().AddEventHandler(
		cache.ResourceEventHandlerFuncs{
			AddFunc: ssc.enqueueStatefulSet,
			UpdateFunc: func(old, cur interface{}) {
				oldPS := old.(*apps.StatefulSet)
				curPS := cur.(*apps.StatefulSet)
				if oldPS.Status.Replicas != curPS.Status.Replicas {
					logger.V(4).Info(&amp;#34;Observed updated replica count for StatefulSet&amp;#34;, &amp;#34;statefulSet&amp;#34;, klog.KObj(curPS), &amp;#34;oldReplicas&amp;#34;, oldPS.Status.Replicas, &amp;#34;newReplicas&amp;#34;, curPS.Status.Replicas)
				}
				ssc.enqueueStatefulSet(cur)
			},
			DeleteFunc: ssc.enqueueStatefulSet,
		},
	)
	//这段代码主要涉及到了Kubernetes Informer机制的使用，用于监听StatefulSet资源的变化，并对变化事件进行处理。
	//1. 首先，ssc.podLister = podInformer.Lister()和ssc.podListerSynced = podInformer.Informer().HasSynced
	//两行代码是将pod的Lister和是否同步完成的函数赋值给ssc对象的对应字段。Lister是用来从Informer的cache中获取资源列表和单个资源的，
	//HasSynced则是用来判断Informer是否已经完成初始的资源同步。
	//2. 接下来，setInformer.Informer().AddEventHandler(...)这行代码是给setInformer添加了一个事件处理函数。
	//其中，cache.ResourceEventHandlerFuncs是一个包含了资源添加、更新和删除事件处理函数的结构体。
	//- AddFunc: ssc.enqueueStatefulSet表示当有StatefulSet资源被添加时，调用ssc.enqueueStatefulSet函数来处理该事件。
	//- UpdateFunc表示当有StatefulSet资源被更新时，会先判断其副本数是否发生了变化，如果有变化则打印日志并调用ssc.enqueueStatefulSet函数来处理该事件。
	//- DeleteFunc: ssc.enqueueStatefulSet表示当有StatefulSet资源被删除时，调用ssc.enqueueStatefulSet函数来处理该事件。
	//综上所述，这段代码通过使用Kubernetes Informer机制监听StatefulSet资源的变化，并调用ssc.enqueueStatefulSet函数来处理这些变化事件。
	ssc.setLister = setInformer.Lister()
	ssc.setListerSynced = setInformer.Informer().HasSynced

	// TODO: Watch volumes
	return ssc
}

// 这个Go函数主要设置了ssc对象的setLister和setListerSynced属性。
// setLister是通过setInformer.Lister()方法设置的，它用于从存储中获取设置列表。setListerSynced是通过setInformer.Informer().HasSynced方法设置的，
// 它用于检查设置的同步状态。最后，该函数返回ssc对象。
// Run runs the statefulset controller.
func (ssc *StatefulSetController) Run(ctx context.Context, workers int) {
	defer utilruntime.HandleCrash()

	// Start events processing pipeline.
	ssc.eventBroadcaster.StartStructuredLogging(3)
	ssc.eventBroadcaster.StartRecordingToSink(&amp;amp;v1core.EventSinkImpl{Interface: ssc.kubeClient.CoreV1().Events(&amp;#34;&amp;#34;)})
	defer ssc.eventBroadcaster.Shutdown()

	defer ssc.queue.ShutDown()

	logger := klog.FromContext(ctx)
	logger.Info(&amp;#34;Starting stateful set controller&amp;#34;)
	defer logger.Info(&amp;#34;Shutting down statefulset controller&amp;#34;)
	//该函数是StatefulSetController类型的Run方法，用于启动statefulset控制器。
	//函数首先处理崩溃情况，然后开启事件处理管道，记录事件并将其发送到事件接收器。
	//之后，函数通过调用queue的ShutDown方法来关闭队列。函数在启动和关闭控制器时分别记录日志信息。
	if !cache.WaitForNamedCacheSync(&amp;#34;stateful set&amp;#34;, ctx.Done(), ssc.podListerSynced, ssc.setListerSynced, ssc.pvcListerSynced, ssc.revListerSynced) {
		return
	}

	for i := 0; i &amp;lt; workers; i++ {
		go wait.UntilWithContext(ctx, ssc.worker, time.Second)
	}

	&amp;lt;-ctx.Done()
}

// 这段Go代码主要实现了以下功能：
// - 等待多个缓存同步完成：通过调用cache.WaitForNamedCacheSync方法，
// 等待podListerSynced、setListerSynced、pvcListerSynced和revListerSynced四个缓存同步完成。
// 如果在指定的上下文ctx被取消或超时之前，所有缓存都已同步，则继续执行后续代码；否则直接返回。
// - 启动多个工作协程：使用for循环和go关键字，启动workers个工作协程，并在每个协程中定期执行ssc.worker函数，其间隔时间为1秒。
// 这些工作协程会一直运行，直到上下文ctx被取消或超时。
// - 等待上下文取消或超时：通过读取ctx.Done()通道，阻塞当前协程，等待上下文ctx被取消或超时。
// 一旦上下文被取消或超时，整个函数执行结束。 这段代码通常用于在启动时等待多个缓存同步完成，并启动多个工作协程来处理后续任务。
// 通过上下文ctx来控制整个函数的运行时长，并能够在缓存同步或工作协程运行过程中随时取消操作。
// addPod adds the statefulset for the pod to the sync queue
func (ssc *StatefulSetController) addPod(logger klog.Logger, obj interface{}) {
	pod := obj.(*v1.Pod)

	if pod.DeletionTimestamp != nil {
		// on a restart of the controller manager, it&amp;#39;s possible a new pod shows up in a state that
		// is already pending deletion. Prevent the pod from being a creation observation.
		ssc.deletePod(logger, pod)
		return
	}

	// If it has a ControllerRef, that&amp;#39;s all that matters.
	if controllerRef := metav1.GetControllerOf(pod); controllerRef != nil {
		set := ssc.resolveControllerRef(pod.Namespace, controllerRef)
		if set == nil {
			return
		}
		logger.V(4).Info(&amp;#34;Pod created with labels&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod), &amp;#34;labels&amp;#34;, pod.Labels)
		ssc.enqueueStatefulSet(set)
		return
	}
	//该函数是StatefulSetController类型的一个方法，用于将Pod的状态添加到同步队列中。
	//首先，它检查Pod是否已被标记为删除，如果是，则调用deletePod方法删除Pod，并返回。
	//然后，它检查Pod是否有ControllerRef，如果有，则通过resolveControllerRef方法解析ControllerRef，
	//并将解析结果传递给enqueueStatefulSet方法，以将StatefulSet添加到同步队列中。
	// Otherwise, it&amp;#39;s an orphan. Get a list of all matching controllers and sync
	// them to see if anyone wants to adopt it.
	sets := ssc.getStatefulSetsForPod(pod)
	if len(sets) == 0 {
		return
	}
	logger.V(4).Info(&amp;#34;Orphan Pod created with labels&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod), &amp;#34;labels&amp;#34;, pod.Labels)
	for _, set := range sets {
		ssc.enqueueStatefulSet(set)
	}
}

// 该函数用于处理一个无归属的Pod（孤儿Pod），通过获取与该Pod匹配的所有StatefulSet控制器，并将它们同步，以查看是否有控制器愿意“收养”这个Pod。
// 首先，函数会调用ssc.getStatefulSetsForPod(pod)方法来获取与Pod匹配的所有StatefulSet集合。
// 如果这个集合的长度为0，说明没有找到匹配的控制器，那么函数直接返回。
// 否则，函数会使用logger记录一条日志信息，以4级日志级别记录“创建了一个孤儿Pod，并列出了Pod的标签”。
// 然后，函数会遍历这个匹配的StatefulSet集合，对每个集合调用ssc.enqueueStatefulSet(set)方法，将其加入到同步队列中，以便后续进行同步处理。
// updatePod adds the statefulset for the current and old pods to the sync queue.
func (ssc *StatefulSetController) updatePod(logger klog.Logger, old, cur interface{}) {
	curPod := cur.(*v1.Pod)
	oldPod := old.(*v1.Pod)
	if curPod.ResourceVersion == oldPod.ResourceVersion {
		// In the event of a re-list we may receive update events for all known pods.
		// Two different versions of the same pod will always have different RVs.
		return
	}

	labelChanged := !reflect.DeepEqual(curPod.Labels, oldPod.Labels)

	curControllerRef := metav1.GetControllerOf(curPod)
	oldControllerRef := metav1.GetControllerOf(oldPod)
	controllerRefChanged := !reflect.DeepEqual(curControllerRef, oldControllerRef)
	if controllerRefChanged &amp;amp;&amp;amp; oldControllerRef != nil {
		// The ControllerRef was changed. Sync the old controller, if any.
		if set := ssc.resolveControllerRef(oldPod.Namespace, oldControllerRef); set != nil {
			ssc.enqueueStatefulSet(set)
		}
	}
	//该函数用于将当前和旧的Pod的状态fulset添加到同步队列中。
	//函数首先将传入的old和cur参数转换为v1.Pod类型，并检查它们的ResourceVersion是否相同。
	//如果相同，则表示这是由于重新列表而导致的更新事件，函数将直接返回。
	//然后，函数会检查Pod的标签是否发生变化，并检查ControllerRef是否发生变化。
	//如果ControllerRef发生变化且旧的ControllerRef不为空，则函数将解析旧的ControllerRef，并将对应的StatefulSet添加到同步队列中。
	// If it has a ControllerRef, that&amp;#39;s all that matters.
	if curControllerRef != nil {
		set := ssc.resolveControllerRef(curPod.Namespace, curControllerRef)
		if set == nil {
			return
		}
		logger.V(4).Info(&amp;#34;Pod objectMeta updated&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(curPod), &amp;#34;oldObjectMeta&amp;#34;, oldPod.ObjectMeta, &amp;#34;newObjectMeta&amp;#34;, curPod.ObjectMeta)
		ssc.enqueueStatefulSet(set)
		// TODO: MinReadySeconds in the Pod will generate an Available condition to be added in
		// the Pod status which in turn will trigger a requeue of the owning replica set thus
		// having its status updated with the newly available replica.
		if !podutil.IsPodReady(oldPod) &amp;amp;&amp;amp; podutil.IsPodReady(curPod) &amp;amp;&amp;amp; set.Spec.MinReadySeconds &amp;gt; 0 {
			logger.V(2).Info(&amp;#34;StatefulSet will be enqueued after minReadySeconds for availability check&amp;#34;, &amp;#34;statefulSet&amp;#34;, klog.KObj(set), &amp;#34;minReadySeconds&amp;#34;, set.Spec.MinReadySeconds)
			// Add a second to avoid milliseconds skew in AddAfter.
			// See https://github.com/kubernetes/kubernetes/issues/39785#issuecomment-279959133 for more info.
			ssc.enqueueSSAfter(set, (time.Duration(set.Spec.MinReadySeconds)*time.Second)+time.Second)
		}
		return
	}
	//该Go函数是一个处理Pod对象元数据更新的函数。
	//它首先检查当前Pod是否有ControllerRef，如果有，则通过resolveControllerRef方法解析其引用的Controller，并将其加入到队列中以进行后续处理。
	//如果Pod满足特定条件（例如，从不可用状态变为可用状态，并且其所属的StatefulSet的MinReadySeconds大于0），
	//则会将StatefulSet加入到队列中， 以便在MinReadySeconds秒后进行可用性检查。
	// Otherwise, it&amp;#39;s an orphan. If anything changed, sync matching controllers
	// to see if anyone wants to adopt it now.
	if labelChanged || controllerRefChanged {
		sets := ssc.getStatefulSetsForPod(curPod)
		if len(sets) == 0 {
			return
		}
		logger.V(4).Info(&amp;#34;Orphan Pod objectMeta updated&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(curPod), &amp;#34;oldObjectMeta&amp;#34;, oldPod.ObjectMeta, &amp;#34;newObjectMeta&amp;#34;, curPod.ObjectMeta)
		for _, set := range sets {
			ssc.enqueueStatefulSet(set)
		}
	}
}

// 这段Go代码是处理Pod对象的元数据更新的逻辑。
// 如果Pod的标签或控制器引用发生了变化，它会检查是否有StatefulSet控制器可以“收养”这个Pod。
// 如果有，则将这个StatefulSet控制器加入到队列中，以便进一步处理。
// deletePod enqueues the statefulset for the pod accounting for deletion tombstones.
func (ssc *StatefulSetController) deletePod(logger klog.Logger, obj interface{}) {
	pod, ok := obj.(*v1.Pod)

	// When a delete is dropped, the relist will notice a pod in the store not
	// in the list, leading to the insertion of a tombstone object which contains
	// the deleted key/value. Note that this value might be stale.
	if !ok {
		tombstone, ok := obj.(cache.DeletedFinalStateUnknown)
		if !ok {
			utilruntime.HandleError(fmt.Errorf(&amp;#34;couldn&amp;#39;t get object from tombstone %+v&amp;#34;, obj))
			return
		}
		pod, ok = tombstone.Obj.(*v1.Pod)
		if !ok {
			utilruntime.HandleError(fmt.Errorf(&amp;#34;tombstone contained object that is not a pod %+v&amp;#34;, obj))
			return
		}
	}
	//该函数是一个Go语言函数，名为deletePod，它属于StatefulSetController类型。
	//该函数的功能是将删除操作对应的状态fulset加入到队列中，并考虑到删除标记（tombstones）。
	//函数参数： - logger：klog.Logger类型，用于记录日志。
	//- obj：interface{}类型，表示要删除的Pod对象。
	//函数流程： 1. 尝试将obj对象转换为*v1.Pod类型。
	//2. 如果转换失败，则尝试将obj对象转换为cache.DeletedFinalStateUnknown类型。
	//3. 如果转换成功，则从tombstone对象中获取被删除的Pod对象。
	//4. 如果获取失败，则记录错误信息并返回。
	//5. 如果成功获取到Pod对象，则将其加入到队列中，以供后续处理。
	//该函数主要处理了两种情况：直接删除Pod对象和通过tombstone对象删除Pod。
	//在处理过程中，函数会进行类型断言来确保对象类型正确，并在出现异常时记录错误信息。
	controllerRef := metav1.GetControllerOf(pod)
	if controllerRef == nil {
		// No controller should care about orphans being deleted.
		return
	}
	set := ssc.resolveControllerRef(pod.Namespace, controllerRef)
	if set == nil {
		return
	}
	logger.V(4).Info(&amp;#34;Pod deleted.&amp;#34;, &amp;#34;pod&amp;#34;, klog.KObj(pod), &amp;#34;caller&amp;#34;, utilruntime.GetCaller())
	ssc.enqueueStatefulSet(set)
}

// 该函数是一个Go语言函数，名为deletePod，它属于StatefulSetController类型。
// 该函数的功能是在删除Pod时，检查是否有对应的控制器（StatefulSet）对该Pod有管理关系，
// 如果有，则将该StatefulSet加入到队列中，以触发相应的更新操作。
// 函数流程：
// 1. 通过metav1.GetControllerOf函数获取Pod的控制器引用（controllerRef）。
// 2. 如果controllerRef为nil，表示该Pod没有对应的控制器，则直接返回。
// 3. 调用ssc.resolveControllerRef函数解析controllerRef，获取对应的StatefulSet对象（set）。
// 4. 如果set为nil，表示无法解析到对应的StatefulSet，则直接返回。
// 5. 记录日志信息，表示Pod已被删除。
// 6. 将set加入到队列中，以触发StatefulSet的更新操作。
// 该函数主要通过controllerRef来确定Pod所属的StatefulSet，并在删除Pod时，通知对应的StatefulSet进行相应的更新操作。
// 如果无法确定Pod的控制器或者无法解析到对应的StatefulSet，则函数直接返回，不做任何处理。
// getPodsForStatefulSet returns the Pods that a given StatefulSet should manage.
// It also reconciles ControllerRef by adopting/orphaning.
//
// NOTE: Returned Pods are pointers to objects from the cache.
// If you need to modify one, you need to copy it first.
func (ssc *StatefulSetController) getPodsForStatefulSet(ctx context.Context, set *apps.StatefulSet, selector labels.Selector) ([]*v1.Pod, error) {
	// List all pods to include the pods that don&amp;#39;t match the selector anymore but
	// has a ControllerRef pointing to this StatefulSet.
	pods, err := ssc.podLister.Pods(set.Namespace).List(labels.Everything())
	if err != nil {
		return nil, err
	}

	filter := func(pod *v1.Pod) bool {
		// Only claim if it matches our StatefulSet name. Otherwise release/ignore.
		return isMemberOf(set, pod)
	}

	cm := controller.NewPodControllerRefManager(ssc.podControl, set, selector, controllerKind, ssc.canAdoptFunc(ctx, set))
	return cm.ClaimPods(ctx, pods, filter)
}

// 该函数用于获取指定StatefulSet应该管理的Pods，并通过adopting/orphaning来协调ControllerRef。
// 函数首先从缓存中列出所有Pods，包括不再匹配选择器但具有指向该StatefulSet的ControllerRef的Pods。
// 然后通过isMemberOf函数筛选出属于该StatefulSet的Pods。
// 最后，使用PodControllerRefManager的ClaimPods方法来处理ControllerRef的采用/孤儿化，并返回属于该StatefulSet的Pods的指针。
// 注意：返回的Pods是来自缓存的对象指针，如果需要修改，则需要先复制。
// If any adoptions are attempted, we should first recheck for deletion with
// an uncached quorum read sometime after listing Pods/ControllerRevisions (see #42639).
func (ssc *StatefulSetController) canAdoptFunc(ctx context.Context, set *apps.StatefulSet) func(ctx2 context.Context) error {
	return controller.RecheckDeletionTimestamp(func(ctx context.Context) (metav1.Object, error) {
		fresh, err := ssc.kubeClient.AppsV1().StatefulSets(set.Namespace).Get(ctx, set.Name, metav1.GetOptions{})
		if err != nil {
			return nil, err
		}
		if fresh.UID != set.UID {
			return nil, fmt.Errorf(&amp;#34;original StatefulSet %v/%v is gone: got uid %v, wanted %v&amp;#34;, set.Namespace, set.Name, fresh.UID, set.UID)
		}
		return fresh, nil
	})
}

// 该函数是一个生成函数，返回一个函数，用于检查是否有资格收养（adopt）某个StatefulSet。
// 收养的条件是：在尝试收养之后，需要再次使用未缓存的多数读取（uncached quorum read）检查该StatefulSet是否已被删除。
// 函数内部通过调用ssc.kubeClient.AppsV1().StatefulSets(set.Namespace).Get()来获取最新的StatefulSet信息，并与原有信息进行对比，
// 如果UID一致，则返回最新的StatefulSet对象；否则返回错误信息。
// adoptOrphanRevisions adopts any orphaned ControllerRevisions matched by set&amp;#39;s Selector.
func (ssc *StatefulSetController) adoptOrphanRevisions(ctx context.Context, set *apps.StatefulSet) error {
	revisions, err := ssc.control.ListRevisions(set)
	if err != nil {
		return err
	}
	orphanRevisions := make([]*apps.ControllerRevision, 0)
	for i := range revisions {
		if metav1.GetControllerOf(revisions[i]) == nil {
			orphanRevisions = append(orphanRevisions, revisions[i])
		}
	}
	if len(orphanRevisions) &amp;gt; 0 {
		canAdoptErr := ssc.canAdoptFunc(ctx, set)(ctx)
		if canAdoptErr != nil {
			return fmt.Errorf(&amp;#34;can&amp;#39;t adopt ControllerRevisions: %v&amp;#34;, canAdoptErr)
		}
		return ssc.control.AdoptOrphanRevisions(set, orphanRevisions)
	}
	return nil
}

// 该函数用于收养被控制器创建但没有被任何StatefulSet认领的ControllerRevision对象。具体流程如下：
// 1. 调用ssc.control.ListRevisions(set)获取与给定StatefulSet相关联的所有ControllerRevision对象。
// 2. 遍历所有ControllerRevision对象，如果其metav1.GetControllerOf字段为空，则将其加入orphanRevisions列表。
// 3. 如果orphanRevisions列表非空，则调用ssc.canAdoptFunc(ctx, set)(ctx)判断是否可以收养这些孤儿Revision。
// 4. 如果可以收养，则调用ssc.control.AdoptOrphanRevisions(set, orphanRevisions)进行收养操作。
// 5. 如果收养过程中出现错误，则返回错误信息；否则返回nil表示成功。
// getStatefulSetsForPod returns a list of StatefulSets that potentially match
// a given pod.
func (ssc *StatefulSetController) getStatefulSetsForPod(pod *v1.Pod) []*apps.StatefulSet {
	sets, err := ssc.setLister.GetPodStatefulSets(pod)
	if err != nil {
		return nil
	}
	// More than one set is selecting the same Pod
	if len(sets) &amp;gt; 1 {
		// ControllerRef will ensure we don&amp;#39;t do anything crazy, but more than one
		// item in this list nevertheless constitutes user error.
		setNames := []string{}
		for _, s := range sets {
			setNames = append(setNames, s.Name)
		}
		utilruntime.HandleError(
			fmt.Errorf(
				&amp;#34;user error: more than one StatefulSet is selecting pods with labels: %+v. Sets: %v&amp;#34;,
				pod.Labels, setNames))
	}
	return sets
}

// 该函数用于获取与给定Pod相匹配的所有StatefulSet列表。
// - 首先，函数通过调用ssc.setLister.GetPodStatefulSets(pod)方法获取选择该Pod的所有StatefulSet。
// - 如果获取过程中出现错误，则直接返回nil。
// - 如果存在多个StatefulSet选择同一个Pod（即sets长度大于1），则将其视为用户错误。
// 函数会记录错误信息，并返回所有匹配的StatefulSet列表。
// 注意：在存在多个StatefulSet选择同一个Pod时，通过ControllerRef可以确保不会发生疯狂的行为。
// 但是多个StatefulSet出现在列表中仍然被认为是用户错误。
// resolveControllerRef returns the controller referenced by a ControllerRef,
// or nil if the ControllerRef could not be resolved to a matching controller
// of the correct Kind.
func (ssc *StatefulSetController) resolveControllerRef(namespace string, controllerRef *metav1.OwnerReference) *apps.StatefulSet {
	// We can&amp;#39;t look up by UID, so look up by Name and then verify UID.
	// Don&amp;#39;t even try to look up by Name if it&amp;#39;s the wrong Kind.
	if controllerRef.Kind != controllerKind.Kind {
		return nil
	}
	set, err := ssc.setLister.StatefulSets(namespace).Get(controllerRef.Name)
	if err != nil {
		return nil
	}
	if set.UID != controllerRef.UID {
		// The controller we found with this Name is not the same one that the
		// ControllerRef points to.
		return nil
	}
	return set
}

// 该函数用于解析ControllerRef引用的控制器，如果引用无法解析为正确的类型，则返回nil。
// 具体步骤如下： 1. 检查controllerRef的类型是否与期望的类型相匹配，如果不匹配则直接返回nil。
// 2. 通过名称查找StatefulSet控制器，如果查找失败则返回nil。
// 3. 验证找到的StatefulSet控制器的UID是否与controllerRef的UID相同，如果不同则返回nil。
// 4. 如果以上验证都通过，则返回找到的StatefulSet控制器。
// enqueueStatefulSet enqueues the given statefulset in the work queue.
func (ssc *StatefulSetController) enqueueStatefulSet(obj interface{}) {
	key, err := controller.KeyFunc(obj)
	if err != nil {
		utilruntime.HandleError(fmt.Errorf(&amp;#34;couldn&amp;#39;t get key for object %+v: %v&amp;#34;, obj, err))
		return
	}
	ssc.queue.Add(key)
}

// 该函数用于将给定的状态fulset对象加入工作队列中。
// 函数首先通过controller.KeyFunc方法获取对象的键值，如果获取失败，则通过utilruntime.HandleError方法记录错误信息并返回。
// 最后，将获取到的键值加入到ssc.queue队列中。
// enqueueStatefulSet enqueues the given statefulset in the work queue after given time
func (ssc *StatefulSetController) enqueueSSAfter(ss *apps.StatefulSet, duration time.Duration) {
	key, err := controller.KeyFunc(ss)
	if err != nil {
		utilruntime.HandleError(fmt.Errorf(&amp;#34;couldn&amp;#39;t get key for object %#v: %v&amp;#34;, ss, err))
		return
	}
	ssc.queue.AddAfter(key, duration)
}

// 该函数将给定的状态集（StatefulSet）在指定时间后加入到工作队列中。
// 首先，函数通过调用controller.KeyFunc(ss)方法获取状态集的键值。
// 如果获取键值时出现错误，则使用utilruntime.HandleError()方法处理错误并返回。
// 最后，将键值和指定时间间隔添加到ssc.queue队列中，以便后续处理。
// processNextWorkItem dequeues items, processes them, and marks them done. It enforces that the syncHandler is never
// invoked concurrently with the same key.
func (ssc *StatefulSetController) processNextWorkItem(ctx context.Context) bool {
	key, quit := ssc.queue.Get()
	if quit {
		return false
	}
	defer ssc.queue.Done(key)
	if err := ssc.sync(ctx, key.(string)); err != nil {
		utilruntime.HandleError(fmt.Errorf(&amp;#34;error syncing StatefulSet %v, requeuing: %v&amp;#34;, key.(string), err))
		ssc.queue.AddRateLimited(key)
	} else {
		ssc.queue.Forget(key)
	}
	return true
}

// 该函数是一个处理工作项的函数，它从队列中获取工作项，处理它，并将其标记为完成。
// 它确保同步处理程序永远不会使用相同的键并发执行。
// 函数返回一个布尔值，表示是否继续处理下一个工作项。
// worker runs a worker goroutine that invokes processNextWorkItem until the controller&amp;#39;s queue is closed
func (ssc *StatefulSetController) worker(ctx context.Context) {
	for ssc.processNextWorkItem(ctx) {
	}
}

// 该函数是一个goroutine，用于不断调用processNextWorkItem处理工作项，直到控制器的队列关闭。
// sync syncs the given statefulset.
func (ssc *StatefulSetController) sync(ctx context.Context, key string) error {
	startTime := time.Now()
	logger := klog.FromContext(ctx)
	defer func() {
		logger.V(4).Info(&amp;#34;Finished syncing statefulset&amp;#34;, &amp;#34;key&amp;#34;, key, &amp;#34;time&amp;#34;, time.Since(startTime))
	}()

	namespace, name, err := cache.SplitMetaNamespaceKey(key)
	//该函数是一个Go语言函数，它定义在名为StatefulSetController的结构体中，使用sync作为函数名。
	//该函数接收两个参数：ctx context.Context和key string，并返回一个error类型的值。
	//函数主要功能如下：
	//1. 记录开始时间：在函数开始处，通过调用time.Now()记录当前时间，用于后续计算函数执行时间。
	//2. 获取日志记录器：通过从传入的ctx参数中获取日志记录器，用于记录函数执行的日志信息。
	//3. 延迟执行日志记录：使用defer关键字定义延迟执行的函数，该函数会在函数返回前执行。
	//延迟函数主要记录函数执行结束时间，并通过日志记录器记录函数执行时间及关键参数信息。
	//4. 分解键值：通过调用cache.SplitMetaNamespaceKey(key)函数，将传入的键值key分解为命名空间namespace和名称name两个部分。
	//整体而言，该函数的主要作用是同步处理某个键值对应的StatefulSet对象，具体处理逻辑在该函数的后续代码中实现。
	//该函数通过记录函数执行时间、获取日志记录器以及分解键值等操作，为后续处理提供了必要信息。
	if err != nil {
		return err
	}
	set, err := ssc.setLister.StatefulSets(namespace).Get(name)
	//该函数主要执行以下操作：
	//1. 检查err是否为nil，如果不为nil，则直接返回错误。
	//2. 调用ssc.setLister.StatefulSets(namespace).Get(name)获取指定命名空间中的状态集，并将其赋值给set变量。
	//3. 返回set和err，其中set为获取的状态集，err为执行过程中可能出现的错误。
	if errors.IsNotFound(err) {
		logger.Info(&amp;#34;StatefulSet has been deleted&amp;#34;, &amp;#34;key&amp;#34;, key)
		return nil
	}
	//这段Go代码主要进行错误判断和日志记录。具体功能如下：
	//- 首先，它检查错误变量err是否为NotFound错误（即该错误表示某个资源未找到）。
	//- 如果是NotFound错误，它会通过logger.Info方法记录一条日志信息，指示某个StatefulSet已被删除，同时在日志中包含key参数的值。
	//- 最后，函数返回nil，表示处理完成且无需进一步处理。
	//这段代码的作用是在遇到特定类型的错误时，记录一条相关日志信息，并终止进一步的错误处理流程。
	if err != nil {
		utilruntime.HandleError(fmt.Errorf(&amp;#34;unable to retrieve StatefulSet %v from store: %v&amp;#34;, key, err))
		return err
	}
	//这段Go代码是一个错误处理的示例。
	//如果变量err不为nil，则会使用utilruntime.HandleError()函数处理错误，
	//该函数会将一个格式化后的错误信息打印出来。然后函数会返回err。这段代码的作用是在发生错误时记录错误信息并返回错误。
	selector, err := metav1.LabelSelectorAsSelector(set.Spec.Selector)
	if err != nil {
		utilruntime.HandleError(fmt.Errorf(&amp;#34;error converting StatefulSet %v selector: %v&amp;#34;, key, err))
		// This is a non-transient error, so don&amp;#39;t retry.
		return nil
	}
	//这段Go代码主要实现了将StatefulSet的标签选择器转换为Selector的功能。
	//具体来说，它首先通过metav1.LabelSelectorAsSelector函数将set.Spec.Selector转换为Selector类型，
	//如果转换过程中出现错误，则通过utilruntime.HandleError函数记录错误信息，并返回nil。
	//需要注意的是，这里的错误被认为是非瞬时错误，因此不会进行重试。
	if err := ssc.adoptOrphanRevisions(ctx, set); err != nil {
		return err
	}
	//这个函数调用了ssc.adoptOrphanRevisions方法，该方法的作用是在给定的上下文ctx中采用孤儿修订版本。如果方法执行出错，则会返回错误信息err。
	pods, err := ssc.getPodsForStatefulSet(ctx, set, selector)
	if err != nil {
		return err
	}
	//该函数尝试通过调用ssc.getPodsForStatefulSet方法获取与指定StatefulSet和选择器相关联的Pods。如果获取过程中出现错误，则将错误返回。
	return ssc.syncStatefulSet(ctx, set, pods)
	//该函数是Go语言编写的，用于同步更新StatefulSet（有状态副本集）的状态。
	//- ctx是一个上下文对象，用于控制函数执行的生命周期。
	//- set是一个StatefulSet对象，表示要更新的有状态副本集。
	//- pods是一个Pod对象的列表，表示有状态副本集中的Pods。
	//函数内部会根据传入的有状态副本集和Pods对象，进行一系列操作来更新副本集的状态，使其与实际的Pods状态保持一致。具体操作细节可以根据函数实现来确定。
}

// syncStatefulSet syncs a tuple of (statefulset, []*v1.Pod).
func (ssc *StatefulSetController) syncStatefulSet(ctx context.Context, set *apps.StatefulSet, pods []*v1.Pod) error {
	logger := klog.FromContext(ctx)
	logger.V(4).Info(&amp;#34;Syncing StatefulSet with pods&amp;#34;, &amp;#34;statefulSet&amp;#34;, klog.KObj(set), &amp;#34;pods&amp;#34;, len(pods))
	var status *apps.StatefulSetStatus
	var err error
	status, err = ssc.control.UpdateStatefulSet(ctx, set, pods)
	if err != nil {
		return err
	}
	logger.V(4).Info(&amp;#34;Successfully synced StatefulSet&amp;#34;, &amp;#34;statefulSet&amp;#34;, klog.KObj(set))
	// One more sync to handle the clock skew. This is also helping in requeuing right after status update
	if set.Spec.MinReadySeconds &amp;gt; 0 &amp;amp;&amp;amp; status != nil &amp;amp;&amp;amp; status.AvailableReplicas != *set.Spec.Replicas {
		ssc.enqueueSSAfter(set, time.Duration(set.Spec.MinReadySeconds)*time.Second)
	}

	return nil
}

//该函数是一个用于同步StatefulSet和其关联Pods的函数。
//它通过调用ssc.control.UpdateStatefulSet方法来更新StatefulSet的状态，
//并在更新完成后进行额外的同步操作来处理时钟偏移。
//如果StatefulSet的MinReadySeconds大于0，并且更新后的状态的AvailableReplicas与Spec.Replicas不相等，则会重新排队等待处理。
&lt;/code>&lt;/pre></description></item><item><title>2024-04-16 K8S如何获得 IP</title><link>https://qq547475331.github.io/docs/k8s%E5%A6%82%E4%BD%95%E8%8E%B7%E5%BE%97-ip-k8s-ru-he-huo-de-ip/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/k8s%E5%A6%82%E4%BD%95%E8%8E%B7%E5%BE%97-ip-k8s-ru-he-huo-de-ip/</guid><description>&lt;p>&lt;strong>Kubernetes 网络模型的核心要求之一是每个 Pod 都应该有自己的 IP 地址，并且集群中的每个 Pod 都应该能够使用这个 IP 地址与其进行通信。有多个网络提供商（flannel、calico、canal 等）实现了这种网络模型。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>在 kubernetes 中设置网络有多种方法，容器运行时也有多种选项。在这篇文章中，我将使用 Flannel作为网络提供程序， 使用 Containerd作为容器运行时。&lt;/strong>&lt;/p>
&lt;h2 id="背景概念">
 &lt;strong>背景概念&lt;/strong>
 &lt;a class="anchor" href="#%e8%83%8c%e6%99%af%e6%a6%82%e5%bf%b5">#&lt;/a>
&lt;/h2>
&lt;p>&lt;strong>容器网络：非常简短的概述&lt;/strong>&lt;/p>
&lt;p>&lt;strong>有一些非常好的帖子解释了容器网络的工作原理。对于上下文，我将在这里使用涉及 Linux 桥接网络和数据包封装的单一方法进行非常高层次的概述。在这里跳过细节。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>同一主机上的容器&lt;/strong>&lt;/p>
&lt;p>&lt;strong>在同一主机上运行的容器可以通过其 IP 地址相互通信的方式之一是通过 Linux 桥。在 kubernetes（和 docker）世界中， 创建了veth（虚拟以太网）设备来实现此目的。该 veth 设备的一端插入容器网络命名空间，另一端连接到 主机网络上的Linux 桥。同一主机上的所有容器都将这一 veth 对的一端连接到 linux 网桥，并且它们可以通过网桥使用其 IP 地址相互通信。Linux 网桥还分配有一个 IP 地址，并充当从 pod 发往不同节点的出口流量的网关。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202404161650509.png" alt="image-20240416165053458" />&lt;/strong>&lt;/p>
&lt;p>&lt;strong>不同主机上的容器&lt;/strong>&lt;/p>
&lt;p>&lt;strong>在不同主机上运行的容器可以通过其 IP 地址相互通信的方法之一是使用数据包封装。Flannel 通过 vxlan支持此功能，它将原始数据包包装在 UDP 数据包中并将其发送到目的地。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>在 kubernetes 集群中，flannel 在每个节点上创建一个 vxlan 设备和一些路由表条目。发往不同主机上的容器的每个数据包都会经过 vxlan 设备并封装在 UDP 数据包中。在目的地，检索封装的数据包并将数据包路由到目标 Pod。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202404161651472.png" alt="image-20240416165109426" />&lt;/strong>&lt;/p></description></item><item><title>2024-04-16 如何为K8S保驾护航</title><link>https://qq547475331.github.io/docs/%E5%A6%82%E4%BD%95%E4%B8%BAk8s%E4%BF%9D%E9%A9%BE%E6%8A%A4%E8%88%AA-ru-he-wei-k8s-bao-jia-hu-hang/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/%E5%A6%82%E4%BD%95%E4%B8%BAk8s%E4%BF%9D%E9%A9%BE%E6%8A%A4%E8%88%AA-ru-he-wei-k8s-bao-jia-hu-hang/</guid><description>&lt;p>&lt;strong>随着Kubernetes的不断发展，技术不断成熟，越来越多的公司选择把自家的应用部署到Kubernetes中。但是把应用部署到Kubernetes中就完事了吗？显然不是，应用容器化只是万里长征的第一步，如何让应用安心、稳定的运行才是后续的所有工作。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>这里主要从以下几个方面来进行整理，对于大部分公司足够使用。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202404161704871.png" alt="image-20240416170405824" />&lt;/strong>&lt;/p>
&lt;h2 id="node">
 &lt;strong>Node&lt;/strong>
 &lt;a class="anchor" href="#node">#&lt;/a>
&lt;/h2>
&lt;p>&lt;strong>Node可以是物理主机，也可以是云主机，它是Kubernetes的载体。在很多时候我们并不太关心Node怎么样了，除非其异常。但是作为运维人员，我们最不希望的就是异常，对于Node也是一样。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>Node节点并不需要做太多太复杂的操作，主要如下：&lt;/strong>&lt;/p>
&lt;p>&lt;strong>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202404161704585.png" alt="image-20240416170423538" />&lt;/strong>&lt;/p>
&lt;h4 id="内核升级">
 &lt;strong>&amp;gt;内核升级&lt;/strong>
 &lt;a class="anchor" href="#%e5%86%85%e6%a0%b8%e5%8d%87%e7%ba%a7">#&lt;/a>
&lt;/h4>
&lt;p>&lt;strong>对于大部分企业，CentOS系统还是首选，默认情况下，7系列系统默认版本是3.10，该版本的内核在Kubernetes社区有很多已知的Bug，所以对节点来说，升级内核是必须的，或者企业可以选择Ubuntu作为底层操作系统。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>升级内核的步骤如下（简单的升级方式）：&lt;/strong>&lt;/p>
&lt;pre tabindex="0">&lt;code>wget https://elrepo.org/linux/kernel/el7/x86_64/RPMS/kernel-lt-5.4.86-1.el7.elrepo.x86_64.rpm
rpm -ivh kernel-lt-5.4.86-1.el7.elrepo.x86_64.rpm
cat /boot/grub2/grub.cfg | grep menuentry
grub2-set-default &amp;#39;CentOS Linux (5.4.86-1.el7.elrepo.x86_64) 7 (Core)&amp;#39;
grub2-editenv list
grub2-mkconfig -o /boot/grub2/grub.cfg
reboot
&lt;/code>&lt;/pre>&lt;h4 id="软件更新">
 &lt;strong>&amp;gt;软件更新&lt;/strong>
 &lt;a class="anchor" href="#%e8%bd%af%e4%bb%b6%e6%9b%b4%e6%96%b0">#&lt;/a>
&lt;/h4>
&lt;p>&lt;strong>对于大部分人来说，更新软件在很多情况下是不做的，因为害怕兼容问题。不过在实际生产中，对于已知有高危漏洞的软件，我们还需要对其进行更新，这个可以针对处理。&lt;/strong>&lt;/p>
&lt;h4 id="优化docker配置文件">
 &lt;strong>&amp;gt;优化Docker配置文件&lt;/strong>
 &lt;a class="anchor" href="#%e4%bc%98%e5%8c%96docker%e9%85%8d%e7%bd%ae%e6%96%87%e4%bb%b6">#&lt;/a>
&lt;/h4>
&lt;p>&lt;strong>对于Docker的配置文件，主要优化的就是日志驱动、保留日志大小以及镜像加速等，其他的配置根据情况而定，如下：&lt;/strong>&lt;/p>
&lt;pre tabindex="0">&lt;code>cat &amp;gt; /etc/docker/daemon.json &amp;lt;&amp;lt; EOF
{
 &amp;#34;exec-opts&amp;#34;: [&amp;#34;native.cgroupdriver=systemd&amp;#34;],
 &amp;#34;log-driver&amp;#34;: &amp;#34;json-file&amp;#34;,
 &amp;#34;log-opts&amp;#34;: {
 &amp;#34;max-size&amp;#34;: &amp;#34;100m&amp;#34;,
 &amp;#34;max-file&amp;#34;: &amp;#34;10&amp;#34;
 },
 &amp;#34;bip&amp;#34;: &amp;#34;169.254.123.1/24&amp;#34;,
 &amp;#34;oom-score-adjust&amp;#34;: -1000,
 &amp;#34;registry-mirrors&amp;#34;: [&amp;#34;https://pqbap4ya.mirror.aliyuncs.com&amp;#34;],
 &amp;#34;storage-driver&amp;#34;: &amp;#34;overlay2&amp;#34;,
 &amp;#34;storage-opts&amp;#34;:[&amp;#34;overlay2.override_kernel_check=true&amp;#34;],
 &amp;#34;live-restore&amp;#34;: true
}
EOF
&lt;/code>&lt;/pre>&lt;h4 id="优化kubelet参数">
 &lt;strong>&amp;gt;优化kubelet参数&lt;/strong>
 &lt;a class="anchor" href="#%e4%bc%98%e5%8c%96kubelet%e5%8f%82%e6%95%b0">#&lt;/a>
&lt;/h4>
&lt;p>&lt;strong>对于K8S来讲，kubelet是每个Node的组长，负责Node的&amp;quot;饮食起居&amp;quot;，这里对它的参数配置主要如下：&lt;/strong>&lt;/p></description></item><item><title>2024-06-28 使用cloudflare(CF)搭建dockerhub代理</title><link>https://qq547475331.github.io/docs/%E4%BD%BF%E7%94%A8cloudflarecf%E6%90%AD%E5%BB%BAdockerhub%E4%BB%A3%E7%90%86-shi-yong-cloudflarecf-da-jian-dockerhub-dai-li/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/%E4%BD%BF%E7%94%A8cloudflarecf%E6%90%AD%E5%BB%BAdockerhub%E4%BB%A3%E7%90%86-shi-yong-cloudflarecf-da-jian-dockerhub-dai-li/</guid><description>&lt;h1 id="使用cloudflarecf搭建dockerhub代理">
 使用cloudflare(CF)搭建dockerhub代理
 &lt;a class="anchor" href="#%e4%bd%bf%e7%94%a8cloudflarecf%e6%90%ad%e5%bb%badockerhub%e4%bb%a3%e7%90%86">#&lt;/a>
&lt;/h1>
&lt;h2 id="前言">
 前言
 &lt;a class="anchor" href="#%e5%89%8d%e8%a8%80">#&lt;/a>
&lt;/h2>
&lt;p>目前国内docker所有域名都被屏蔽，造成一些玩docker的用户很是苦恼，更换阿里云的镜像加速但镜像也没dockerhub那么多，有些好用的工具一直拉不下来，自己搭建dockerhub镜像站又耗时还得购买海外服务器，非常不划算。本文按照B站一个大佬的方法为此我撰写一篇文章用最简单和最清晰的思路。&lt;/p>
&lt;h3 id="准备环境">
 准备环境
 &lt;a class="anchor" href="#%e5%87%86%e5%a4%87%e7%8e%af%e5%a2%83">#&lt;/a>
&lt;/h3>
&lt;ol>
&lt;li>&lt;a href="https://link.zhihu.com/?target=https%3A//dash.cloudflare.com/">注册cloudflare账户&lt;/a>（必须）&lt;/li>
&lt;li>&lt;a href="https://link.zhihu.com/?target=http%3A//github.com/">注册github账户&lt;/a>（必须）&lt;/li>
&lt;li>购买域名并绑定在cloudflare域下（可选）&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>虽然cf默认给你分配一个免费的域名，但是这个域名我试了一下解析非常慢，建议挂自己域名。&lt;/p>&lt;/blockquote>
&lt;p>关于如何购买并绑定到cf本文不再赘述，网上有非常多的教程可以自行搜索。&lt;/p>
&lt;h2 id="一克隆github项目到自己的库">
 一、克隆github项目到自己的库
 &lt;a class="anchor" href="#%e4%b8%80%e5%85%8b%e9%9a%86github%e9%a1%b9%e7%9b%ae%e5%88%b0%e8%87%aa%e5%b7%b1%e7%9a%84%e5%ba%93">#&lt;/a>
&lt;/h2>
&lt;ol>
&lt;li>&lt;a href="https://link.zhihu.com/?target=https%3A//github.com/cmliu/CF-Workers-docker.io">访问此网站&lt;/a>&lt;/li>
&lt;li>克隆到自己仓库&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202406281032303.png" alt="image-20240628103226223" />&lt;/p>
&lt;p>由于我这里已经克隆过了，大家没有克隆过的点击加号会显示的页面直接点击右下角即可。&lt;/p>
&lt;h2 id="二部署到cloudflare">
 二、部署到cloudflare
 &lt;a class="anchor" href="#%e4%ba%8c%e9%83%a8%e7%bd%b2%e5%88%b0cloudflare">#&lt;/a>
&lt;/h2>
&lt;ol>
&lt;li>绑定github&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202406281032311.png" alt="image-20240628103241264" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202406281032112.png" alt="image-20240628103254054" />&lt;/p>
&lt;p>\2. 部署源码&lt;/p>
&lt;p>选择仓库后一直下一步过程无需选择其它，直接点到此页面&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202406281033257.png" alt="image-20240628103313187" />&lt;/p>
&lt;p>至此部署环节已完成，参考&lt;a href="https://link.zhihu.com/?target=https%3A//www.bilibili.com/video/BV1H442197oQ/%3Fshare_source%3Dcopy_web%26vd_source%3D6fdda38be5eb9fcf9f074fd04e9bf9ae">此视频&lt;/a>编写&lt;/p>
&lt;h2 id="如何使用">
 如何使用
 &lt;a class="anchor" href="#%e5%a6%82%e4%bd%95%e4%bd%bf%e7%94%a8">#&lt;/a>
&lt;/h2>
&lt;ol>
&lt;li>可以在拉取镜像名称前加入此域名，例如 https://&amp;lt;域名&amp;gt;/镜像名:lates&lt;/li>
&lt;li>可以参考下面方法换源永久实现&lt;/li>
&lt;/ol>
&lt;h2 id="三更换docker源">
 三、更换docker源
 &lt;a class="anchor" href="#%e4%b8%89%e6%9b%b4%e6%8d%a2docker%e6%ba%90">#&lt;/a>
&lt;/h2>
&lt;p>可以参考我这篇文章&lt;/p>
&lt;h2 id="四需要注意">
 四、需要注意
 &lt;a class="anchor" href="#%e5%9b%9b%e9%9c%80%e8%a6%81%e6%b3%a8%e6%84%8f">#&lt;/a>
&lt;/h2>
&lt;ol>
&lt;li>由于是cloudflare网络，国内访问比较缓慢，所以下载速度肯定没有国内镜像站快，但是绝大部分还是可以成功下载的。&lt;/li>
&lt;li>此方案并不是完全免费，cloudlfare每个人免费配额为10000次请求，如果是你自己使用完全足够，每天能拉几百次。但是如果公开到网上可能就会到达上限，因此不要随意将此域名公开到网上以防被刷量。我们可以在概览页面查看请求量&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202406281033038.png" alt="image-20240628103344988" />&lt;/p>
&lt;h3 id="增加dns解析cloudsxtop到刚刚cl自动生成的域名">
 增加dns解析cloudsx.top到刚刚cl自动生成的域名
 &lt;a class="anchor" href="#%e5%a2%9e%e5%8a%a0dns%e8%a7%a3%e6%9e%90cloudsxtop%e5%88%b0%e5%88%9a%e5%88%9acl%e8%87%aa%e5%8a%a8%e7%94%9f%e6%88%90%e7%9a%84%e5%9f%9f%e5%90%8d">#&lt;/a>
&lt;/h3>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202406281034921.png" alt="image-20240628103440840" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202406281035343.png" alt="image-20240628103501247" />&lt;/p>
&lt;h3 id="cloudsxtop代理的dockerhub页面">
 &lt;strong>cloudsx.top代理的dockerhub页面&lt;/strong>
 &lt;a class="anchor" href="#cloudsxtop%e4%bb%a3%e7%90%86%e7%9a%84dockerhub%e9%a1%b5%e9%9d%a2">#&lt;/a>
&lt;/h3>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202406281035929.png" alt="image-20240628103524829" />&lt;/p>
&lt;h3 id="拉镜像">
 拉镜像
 &lt;a class="anchor" href="#%e6%8b%89%e9%95%9c%e5%83%8f">#&lt;/a>
&lt;/h3>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202406281037685.png" alt="image-20240628103729619" />&lt;/p></description></item><item><title>2024-07-05 K8S之ingress-nginx原理及配置</title><link>https://qq547475331.github.io/docs/k8s%E4%B9%8Bingress-nginx%E5%8E%9F%E7%90%86%E5%8F%8A%E9%85%8D%E7%BD%AE-k8s-zhi-ingress-nginx-yuan-li-ji-pei-zhi/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/k8s%E4%B9%8Bingress-nginx%E5%8E%9F%E7%90%86%E5%8F%8A%E9%85%8D%E7%BD%AE-k8s-zhi-ingress-nginx-yuan-li-ji-pei-zhi/</guid><description>&lt;p>前言
在Kubernetes中，服务和Pod的IP地址仅可以在集群网络内部使用，对于集群外的应用是不可见的。为了使外部的应用能够访问集群内的服务，在Kubernetes中目前提供了以下几种方案：&lt;/p>
&lt;pre tabindex="0">&lt;code>* NodePort
* LoadBalancer
* Ingress
&lt;/code>&lt;/pre>&lt;p>在之前的博文中介绍过NodePort，简单来说，就是通过service这种资源对象，为后端pod提供一个统一的访问接口，然后将service的统一访问接口映射到群集节点上，最终实现client通过映射到群集节点上的端口访问到后端pod提供的服务。&lt;/p>
&lt;p>但是，这种方式有一个弊端，就是当新生成一个pod服务就需要创建对应的service将其映射到节点端口，当运行的pod过多时，我们节点暴露给client端的端口也会随之增加，这样我们整个k8s群集的危险系数就会增加，因为我们在搭建群集之处，官方明确指出，必须关闭firewalld防火墙及清空iptables规则，现在我们又暴露了那么多端口给client，安全系数可想而知。&lt;/p>
&lt;p>一、Ingress-nginx介绍
1、Ingress-nginx组成&lt;/p>
&lt;pre tabindex="0">&lt;code>* ingress-nginx-controller：根据用户编写的ingress规则（创建的ingress的yaml文件），
 动态的去更改nginx服务的配置文件，并且reload重载使其生效（是自动化的，通过lua脚本来实现）；
* ingress资源对象：将Nginx的配置抽象成一个Ingress对象，每添加一个新的Service资
 源对象只需写一个新的Ingress规则的yaml文件即可（或修改已存在的ingress规则的yaml文件）
&lt;/code>&lt;/pre>&lt;p>2、Ingress-nginx可以解决什么问题？&lt;/p>
&lt;pre tabindex="0">&lt;code>1)动态配置服务
　　如果按照传统方式, 当新增加一个服务时, 我们可能需要在流量入口加一个反向代理指
	向我们新的k8s服务. 而如果用了Ingress-nginx, 只需要配置好这个服务, 当服务启动
	时, 会自动注册到Ingress的中, 不需要而外的操作。
2)减少不必要的端口映射
　　配置过k8s的都清楚, 第一步是要关闭防火墙的, 主要原因是k8s的很多服务会
	以NodePort方式映射出去, 这样就相当于给宿主机打了很多孔, 既不安全也不优雅. 
	而Ingress可以避免这个问题, 除了Ingress自身服务可能需要映射出去, 其他服务都不要
	用NodePort方式
&lt;/code>&lt;/pre>&lt;p>3、Ingress-nginx工作原理&lt;/p>
&lt;pre tabindex="0">&lt;code>1）ingress controller通过和kubernetes api交互，动态的去感知集群中ingress规则变化，
2）然后读取它，按照自定义的规则，规则就是写明了哪个域名对应哪个service，生成一段nginx配置，
3）再写到nginx-ingress-controller的pod里，这个Ingress controller的pod里运行着一个Nginx服务，控制器会把生成的nginx配置写入/etc/nginx.conf文件中，
4）然后reload一下使配置生效。以此达到域名分别配置和动态更新的问题。
&lt;/code>&lt;/pre>&lt;p>2、创建namespace（也可跳过，使用默认的default名称空间也可以，但需要删除下面所有yaml文件中关于自定义的名称空间的配置字段）&lt;/p>
&lt;pre tabindex="0">&lt;code>[root@master ~]# kubectl create ns test-ns //创建名称空间test-ns
[root@master ~]# kubectl get ns //确认创建成功
&lt;/code>&lt;/pre>&lt;p>3、创建Deployment、Service资源对象
1）创建httpd服务及其service与之关联&lt;/p>
&lt;pre tabindex="0">&lt;code>---
apiVersion: apps/v1
kind: Deployment
metadata:
 annotations: {}
 labels:
 app: httpd01
 k8s.kuboard.cn/name: web01
 name: web01
 namespace: test-ns
spec:
 progressDeadlineSeconds: 600
 replicas: 3
 revisionHistoryLimit: 10
 selector:
 matchLabels:
 app: httpd01
 k8s.kuboard.cn/name: web01
 strategy:
 rollingUpdate:
 maxSurge: 25%
 maxUnavailable: 25%
 type: RollingUpdate
 template:
 metadata:
 creationTimestamp: null
 labels:
 app: httpd01
 k8s.kuboard.cn/name: web01
 spec:
 containers:
 - image: &amp;#39;192.168.0.140:881/library/httpd:latest&amp;#39;
 imagePullPolicy: IfNotPresent
 name: httpd
 resources: {}
 terminationMessagePath: /dev/termination-log
 terminationMessagePolicy: File
 dnsPolicy: ClusterFirst
 restartPolicy: Always
 schedulerName: default-scheduler
 securityContext: {}
 terminationGracePeriodSeconds: 30
---
apiVersion: v1
kind: Service
metadata:
 annotations: {}
 name: httpd-svc
 namespace: test-ns
spec:
 internalTrafficPolicy: Cluster
 ipFamilies:
 - IPv4
 ipFamilyPolicy: SingleStack
 ports:
 - name: httpd-svc
 port: 80
 protocol: TCP
 targetPort: 80
 selector:
 app: httpd01
 sessionAffinity: None
 type: ClusterIP
&lt;/code>&lt;/pre>&lt;p>2）创建tomcat服务及其service&lt;/p></description></item><item><title>2024-07-22 OpenKruise详细解释以及原地升级及全链路灰度发布方案</title><link>https://qq547475331.github.io/docs/openkruise%E8%AF%A6%E7%BB%86%E8%A7%A3%E9%87%8A%E4%BB%A5%E5%8F%8A%E5%8E%9F%E5%9C%B0%E5%8D%87%E7%BA%A7%E5%8F%8A%E5%85%A8%E9%93%BE%E8%B7%AF%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83%E6%96%B9%E6%A1%88-openkruise-xiang-xi-jie-shi-yi-ji-yuan-de-sheng-ji-ji-quan-lian-lu-hui-du-fa-bu-fang-an/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/openkruise%E8%AF%A6%E7%BB%86%E8%A7%A3%E9%87%8A%E4%BB%A5%E5%8F%8A%E5%8E%9F%E5%9C%B0%E5%8D%87%E7%BA%A7%E5%8F%8A%E5%85%A8%E9%93%BE%E8%B7%AF%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83%E6%96%B9%E6%A1%88-openkruise-xiang-xi-jie-shi-yi-ji-yuan-de-sheng-ji-ji-quan-lian-lu-hui-du-fa-bu-fang-an/</guid><description>&lt;p>&lt;strong>OpenKruise简介&lt;/strong>
&lt;strong>OpenKruise来源&lt;/strong>
&lt;strong>它是由阿里巴巴集团的阿里云团队维护和开发的，并且在2018年将其贡献给了云原生计算基金会（CNCF），成为了CNCF的孵化项目。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>OpenKruise是什么？&lt;/strong>
&lt;strong>OpenKruise 是一个基于 Kubernetes 的扩展项目，旨在增强和扩展 Kubernetes 的原生能力，以更好地支持大规模应用的管理和运维。它通过提供一系列自定义控制器和自定义资源（CRD），帮助用户在 Kubernetes 集群中更加灵活、高效地管理容器化应用。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>核心组件有什么？&lt;/strong>
&lt;strong>1.CloneSet:&lt;/strong>
&lt;strong>功能:&lt;/strong>
&lt;strong>用于管理一组具有相同模板的 Pod。类似于 Kubernetes 的 Deployment，但提供了更多高级特性，如灰度发布、并行和顺序更新策略、最大不可用副本数等。&lt;/strong>
&lt;strong>用途:&lt;/strong>
&lt;strong>适用于需要复杂更新策略和高可用性的应用场景。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>2.SidecarSet:&lt;/strong>
&lt;strong>功能:&lt;/strong>
&lt;strong>用于管理 Sidecar 容器。可以动态地将 Sidecar 容器注入到指定的 Pod 中，而不需要修改 Pod 的模板。&lt;/strong>
&lt;strong>用途:&lt;/strong>
&lt;strong>适用于需要在多个应用 Pod 中添加统一的辅助容器，如日志收集、监控代理等。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>3.StatefulSet:&lt;/strong>
&lt;strong>功能:&lt;/strong>
&lt;strong>扩展了 Kubernetes 的 StatefulSet 功能。支持有状态应用的管理，提供了更灵活的更新和扩展策略。&lt;/strong>
&lt;strong>用途:&lt;/strong>
&lt;strong>适用于有状态应用，如数据库、缓存服务等。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>4.Advanced DaemonSet:&lt;/strong>
&lt;strong>功能:&lt;/strong>
&lt;strong>提供了比原生 Kubernetes DaemonSet 更加灵活的功能，如灰度发布、并行和顺序更新策略等。&lt;/strong>
&lt;strong>用途:&lt;/strong>
&lt;strong>适用于需要在每个节点上运行一个副本的应用，如监控代理、日志收集代理等。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>5.BroadcastJob:&lt;/strong>
&lt;strong>功能:&lt;/strong>
&lt;strong>类似于 Kubernetes 的 Job，但用于在集群中所有或部分节点上运行一次性任务。&lt;/strong>
&lt;strong>用途:&lt;/strong>
&lt;strong>适用于需要在每个节点上执行一次性任务的场景，如节点初始化、数据分发等。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>6.ImagePullJob:&lt;/strong>
&lt;strong>功能:&lt;/strong>
&lt;strong>用于在集群的所有或部分节点上预拉取镜像，以减少 Pod 启动时的延迟。&lt;/strong>
&lt;strong>用途:&lt;/strong>
&lt;strong>适用于需要快速启动大量 Pod 的场景，如批量部署、大规模弹性扩容等。&lt;/strong>&lt;/p></description></item><item><title>2024-08-02 33款gitops与devops主流系统</title><link>https://qq547475331.github.io/docs/33%E6%AC%BEgitops%E4%B8%8Edevops%E4%B8%BB%E6%B5%81%E7%B3%BB%E7%BB%9F-33-kuan-gitops-yu-devops-zhu-liu-xi-tong/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/33%E6%AC%BEgitops%E4%B8%8Edevops%E4%B8%BB%E6%B5%81%E7%B3%BB%E7%BB%9F-33-kuan-gitops-yu-devops-zhu-liu-xi-tong/</guid><description>&lt;p>为了帮助大家开启 GitOps 之旅，文介绍了 30 多种工具，如果你要想应用 GitOps，建议你使用这些工具。&lt;/p>
&lt;p>GitOps 借鉴了 DevOps 方法论的自动化方面，是一种旨在通过软件开发和部署来简化基础设施管理和云操作的方法。虽然许多人认为 GitOps 可以替代 DevOps，但事实并非如此，该方法仅专注于实现 DevOps 方法论中的自动化这一个方面。&lt;/p>
&lt;p>具体来说，GitOps 使用 Git 拉取（pull）请求来自动化基础设施配置和软件部署，所有这些都是为了使 CI/CD 变得更加高效。&lt;/p>
&lt;p>GitOps 将 Git 作为应用程序开发和云基础设施的唯一事实源；采用声明式语句来简化配置和部署。&lt;/p>
&lt;p>GitOps 统一了许多关键任务，比如云集群（特别是运行在云中的容器）的部署、管理和监控，并允许开发人员对他们的应用程序部署管道有更多的控制。由于 Git 可用于基础设施即代码（IaC）和应用程序开发，因此它是该方法的理想事实存储库。&lt;/p>
&lt;p>1GitOps 的好处&lt;/p>
&lt;p>对于那些使用该方法的人来说，GitOps 提供了一些关键的优势，首先是更精细的 CI/CD 管道本身。该方法充分利用了云原生应用程序和可伸缩云基础设施的优势，而没有引入常见的复杂性。&lt;/p>
&lt;p>其他好处还包括：&lt;/p>
&lt;ul>
&lt;li>更高的可靠性，这是由 Git 的原生特性决定的。如果新代码导致了错误，你可以回滚部署并使用 Git 的跟踪机制恢复到该应用程序的任何版本。这也会使云基础设施更加健壮。&lt;/li>
&lt;li>提高了稳定性，尤其是在管理 Kubernetes 集群时。一切都是可跟踪的，并且集群配置中的变更也可以在需要时恢复。将以 Git 作为事实源，自动创建审计日志。&lt;/li>
&lt;li>更高的生产效率，使得开发人员能够更加关注代码的质量，而不是管道本身。一旦将新的代码提交给 Git，一切都将完全自动化，此外还可以利用其他自动化工具。&lt;/li>
&lt;li>最大程度的一致性，特别是在整个流程中使用相同的方法来进行端到端的管理时。GitOps 简化了应用程序、Kubernetes 附属组件以及 Kubernetes 基础设施的所有工作。&lt;/li>
&lt;/ul>
&lt;p>许多观点认为，GitOps 将持续交付与云原生优势和 IaC 结合起来，提供了这两个领域的最佳服务。GitOps 最佳实践还标准化了端到端的管道，你可以将该方法与任何现有管道进行集成，而无需进行大的更改。只要使用合适的工具即可完成这项工作。&lt;/p>
&lt;p>2GitOps 工具&lt;/p>
&lt;p>说到适合这项工作的工具，有无数工具可以帮助你将 GitOps 方法与现有工作流进行集成。一些支持 GitOps 的工具是非常流行的，甚至可以在现有的管道中使用它们。如果你想加入 GitOps，这里有一些我们推荐的工具可以帮助你进行入门学习。&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202408011513585.png" alt="image-20240801151359516" />&lt;/p>
&lt;p>1、 Kubernetes&lt;/p></description></item><item><title>2024-08-02 dockerfile定制专属镜像</title><link>https://qq547475331.github.io/docs/dockerfile%E5%AE%9A%E5%88%B6%E4%B8%93%E5%B1%9E%E9%95%9C%E5%83%8F-dockerfile-ding-zhi-zhuan-shu-jing-xiang/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/dockerfile%E5%AE%9A%E5%88%B6%E4%B8%93%E5%B1%9E%E9%95%9C%E5%83%8F-dockerfile-ding-zhi-zhuan-shu-jing-xiang/</guid><description>&lt;h2 id="前言">
 前言
 &lt;a class="anchor" href="#%e5%89%8d%e8%a8%80">#&lt;/a>
&lt;/h2>
&lt;p>大家好，本文是对 Docker 自定义镜像的详细讲解，讲解了如何进行构建自己的 Docker 镜像以及 Dockerfile 的操作指令。希望对大家有所帮助~&lt;/p>
&lt;h2 id="一使用-dockerfile-定制镜像">
 一、使用 Dockerfile 定制镜像
 &lt;a class="anchor" href="#%e4%b8%80%e4%bd%bf%e7%94%a8-dockerfile-%e5%ae%9a%e5%88%b6%e9%95%9c%e5%83%8f">#&lt;/a>
&lt;/h2>
&lt;h3 id="11dockerfile-定制镜像">
 &lt;strong>1.1、Dockerfile 定制镜像&lt;/strong>
 &lt;a class="anchor" href="#11dockerfile-%e5%ae%9a%e5%88%b6%e9%95%9c%e5%83%8f">#&lt;/a>
&lt;/h3>
&lt;p>镜像的定制实际上就是定制每一层所添加的配置、文件。如果我们可以把每一层修改、安装、构建、操作的命令都写入一个脚本，用这个脚本来构建、定制镜像，无法重复的问题、镜像构建透明性的问题、体积的问题就都会解决。这个脚本就是 &lt;code>Dockerfile&lt;/code>。&lt;/p>
&lt;p>&lt;code>Dockerfile&lt;/code> 是一个文本文件，其内包含了一条条的指令(Instruction)，每一条指令构建一层，因此每一条指令的内容，就是描述该层应当如何构建。&lt;/p>
&lt;p>以 &lt;code>nginx&lt;/code> 镜像为例，这次我们使用 &lt;code>Dockerfile&lt;/code> 来定制。&lt;/p>
&lt;p>在一个空白目录中，建立一个文本文件，并命名为 &lt;code>Dockerfile&lt;/code>：&lt;/p>
&lt;pre tabindex="0">&lt;code>$ mkdir mynginx
$ cd mynginx
$ touch Dockerfile
&lt;/code>&lt;/pre>&lt;p>其内容为：&lt;/p>
&lt;pre tabindex="0">&lt;code>FROM nginx
RUN echo &amp;#39;&amp;lt;h1&amp;gt;Hello, Docker!&amp;lt;/h1&amp;gt;&amp;#39; &amp;gt; /usr/share/nginx/html/index.html
&lt;/code>&lt;/pre>&lt;p>这个 &lt;code>Dockerfile&lt;/code> 很简单，一共就两行。涉及到了两条指令，&lt;code>FROM&lt;/code> 和 &lt;code>RUN&lt;/code>。&lt;/p>
&lt;h3 id="12from-指定基础镜像">
 &lt;strong>1.2、FROM 指定基础镜像&lt;/strong>
 &lt;a class="anchor" href="#12from-%e6%8c%87%e5%ae%9a%e5%9f%ba%e7%a1%80%e9%95%9c%e5%83%8f">#&lt;/a>
&lt;/h3>
&lt;p>所谓定制镜像，那一定是以一个镜像为基础，在其上进行定制。就像我们之前运行了一个 &lt;code>nginx&lt;/code> 镜像的容器，再进行修改一样，基础镜像是必须指定的。而 &lt;code>FROM&lt;/code> 就是指定&lt;strong>基础镜像&lt;/strong>，因此一个 &lt;code>Dockerfile&lt;/code> 中 &lt;code>FROM&lt;/code>是必备的指令，并且必须是第一条指令。&lt;/p>
&lt;p>在 &lt;code>Docker Store&lt;/code> 上有非常多的高质量的官方镜像，有可以直接拿来使用的服务类的镜像，如 &lt;code>nginx&lt;/code>、&lt;code>redis&lt;/code>、&lt;code>mongo&lt;/code>、&lt;code>mysql&lt;/code>、&lt;code>httpd&lt;/code>、&lt;code>php&lt;/code>、&lt;code>tomcat&lt;/code> 等；也有一些方便开发、构建、运行各种语言应用的镜像，如&lt;code>node&lt;/code>、&lt;code>openjdk&lt;/code>、&lt;code>python&lt;/code>、&lt;code>ruby&lt;/code>、&lt;code>golang&lt;/code>等。可以在其中寻找一个最符合我们最终目标的镜像为基础镜像进行定制。&lt;/p>
&lt;p>如果没有找到对应服务的镜像，官方镜像中还提供了一些更为基础的操作系统镜像，如&lt;code>ubuntu&lt;/code>、&lt;code>debian&lt;/code>、&lt;code>centos&lt;/code>、&lt;code>fedora&lt;/code>、&lt;code>alpine&lt;/code> 等，这些操作系统的软件库为我们提供了更广阔的扩展空间。&lt;/p></description></item><item><title>2024-08-02 godel-scheduler</title><link>https://qq547475331.github.io/docs/godel-scheduler-godel-scheduler/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/godel-scheduler-godel-scheduler/</guid><description>&lt;h1 id="local-gödel-environment-setup-with-kind">
 Local Gödel Environment Setup with KIND
 &lt;a class="anchor" href="#local-g%c3%b6del-environment-setup-with-kind">#&lt;/a>
&lt;/h1>
&lt;p>使用KIND设置本地哥德尔环境&lt;/p>
&lt;p>This guide will walk you through how to set up the Gödel Unified Scheduling system.&lt;/p>
&lt;p>本指南将指导您如何设置哥德尔统一调度系统。&lt;/p>
&lt;h2 id="one-step-cluster-bootstrap--installation">
 One-Step Cluster Bootstrap &amp;amp; Installation
 &lt;a class="anchor" href="#one-step-cluster-bootstrap--installation">#&lt;/a>
&lt;/h2>
&lt;p>一步式集群引导和安装&lt;/p>
&lt;p>We provided a quick way to help you try Gödel on your local machine, which will set up a kind cluster locally and deploy necessary crds, clusterrole and rolebindings&lt;/p>
&lt;p>我们提供了一种快速的方法来帮助您在本地计算机上尝试Gödel，它将在本地设置一个类集群，并部署必要的crd、clusterrole和rolebindings&lt;/p>
&lt;h3 id="prerequisites">
 Prerequisites
 &lt;a class="anchor" href="#prerequisites">#&lt;/a>
&lt;/h3>
&lt;p>先决条件&lt;/p>
&lt;p>Please make sure the following dependencies are installed.&lt;/p></description></item><item><title>2024-08-02 istio-ingress-gateway</title><link>https://qq547475331.github.io/docs/istio-ingress-gateway-istio-ingress-gateway/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/istio-ingress-gateway-istio-ingress-gateway/</guid><description>&lt;p>&lt;strong>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202408011004700.png" alt="image-20240801100451585" />&lt;/strong>&lt;/p>
&lt;p>&lt;em>&lt;strong>Istio 服务网格中的网关&lt;/strong>&lt;/em>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>使用网关为网格来管理入站和出站流量，可以让用户指定要进入或离开网格的流量。&lt;/strong>&lt;/li>
&lt;li>&lt;strong>使用网关为网格来管理入站和出站流量，可以让用户指定要进入或离开网格的流量。&lt;/strong>&lt;/li>
&lt;li>&lt;strong>网关配置被用于运行在网格内独立 Envoy 代理中，而不是服务工作负载的应用 Sidecar 代理。&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>&lt;a href="https://links.jianshu.com/go?to=https%3A%2F%2Fpreliminary.istio.io%2Fzh%2Fdocs%2Freference%2Fconfig%2Fnetworking%2Fgateway%2F">&lt;code>Gateway&lt;/code>&lt;/a> 用于为 HTTP / TCP 流量配置负载均衡器，并不管该负载均衡器将在哪里运行。网格中可以存在任意数量的 &lt;a href="https://links.jianshu.com/go?to=https%3A%2F%2Fpreliminary.istio.io%2Fzh%2Fdocs%2Freference%2Fconfig%2Fnetworking%2Fgateway%2F">&lt;code>Gateway&lt;/code>&lt;/a>，并且多个不同的 &lt;a href="https://links.jianshu.com/go?to=https%3A%2F%2Fpreliminary.istio.io%2Fzh%2Fdocs%2Freference%2Fconfig%2Fnetworking%2Fgateway%2F">&lt;code>Gateway&lt;/code>&lt;/a> 实现可以共存。实际上，通过在配置中指定一组工作负载（Pod）标签，可以将 Gateway 配置绑定到特定的工作负载，从而允许用户通过编写简单的 Gateway Controller 来重用现成的网络设备。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>&lt;code>Gateway&lt;/code> 只用于配置 L4-L6 功能（例如，对外公开的端口，TLS 配置），所有主流的 L7 代理均以统一的方式实现了这些功能。然后，通过在 &lt;code>Gateway&lt;/code> 上绑定 &lt;code>VirtualService&lt;/code> 的方式，可以使用标准的 Istio 规则来控制进入 &lt;code>Gateway&lt;/code> 的 HTTP 和 TCP 流量。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>例如，下面这个简单的 &lt;code>Gateway&lt;/code> 配置了一个 Load Balancer，以允许访问 host &lt;code>bookinfo.com&lt;/code> 的 https 外部流量进入网格中：&lt;/strong>&lt;/p>
&lt;pre tabindex="0">&lt;code>apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
 name: bookinfo-gateway
spec:
 selector:
 app: my-ingress-gateway
 servers:
 - port:
 number: 443
 name: https
 protocol: HTTPS
 hosts:
 - bookinfo.com
 tls:
 mode: SIMPLE
 serverCertificate: /tmp/tls.crt
 privateKey: /tmp/tls.key
&lt;/code>&lt;/pre>&lt;p>&lt;strong>要为进入上面的 Gateway 的流量配置相应的路由，必须为同一个 host 定义一个 &lt;code>VirtualService&lt;/code>（在下一节中描述），并使用配置中的 &lt;code>gateways&lt;/code> 字段绑定到前面定义的 &lt;code>Gateway&lt;/code> 上：&lt;/strong>&lt;/p></description></item><item><title>2024-08-02 istio部署</title><link>https://qq547475331.github.io/docs/istio%E9%83%A8%E7%BD%B2-istio-bu-shu/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/istio%E9%83%A8%E7%BD%B2-istio-bu-shu/</guid><description>&lt;pre tabindex="0">&lt;code>[root@master1 istio-1.22.3]# istioctl install --set profile=demo -y

The Kubernetes version v1.23.6 is not supported by Istio 1.22.3. The minimum supported Kubernetes version is 1.26.
Proceeding with the installation, but you might experience problems. See https://istio.io/latest/docs/releases/supported-releases/ for a list of supported versions.

2024-07-30T09:29:52.253698Z error klog an error occurred forwarding 40586 -&amp;gt; 15014: error forwarding port 15014 to pod 8835fe7129746ef5df21abaf44c2d302f16012ae2c6e03c1103c5b8d8aff680c, uid : unable to do port forwarding: socat not found
2024-07-30T09:29:52.254140Z error port forward failed: lost connection to pod
The default revision has been updated to point to this installation.
✔ Istio core installed
✔ Istiod installed
✔ Egress gateways installed
✔ Ingress gateways installed
✔ Installation complete Made this installation the default for injection and validation.
[root@master1 istio-1.22.3]#
&lt;/code>&lt;/pre>&lt;pre tabindex="0">&lt;code>quay.io/kiali/kiali:v1.63
jimmidyson/configmap-reload:v0.5.0
&lt;/code>&lt;/pre>&lt;pre tabindex="0">&lt;code>#原镜像
quay.io/kiali/kiali:v1.63

#转换后镜像
anjia0532/quay.kiali.kiali:v1.63


#下载并重命名镜像
docker pull anjia0532/quay.kiali.kiali:v1.63 

docker tag anjia0532/quay.kiali.kiali:v1.63 quay.io/kiali/kiali:v1.63

docker images | grep $(echo quay.io/kiali/kiali:v1.63 |awk -F&amp;#39;:&amp;#39; &amp;#39;{print $1}&amp;#39;)


docker tag cloudsx.top/anjia0532/quay.kiali.kiali:v1.63 192.168.0.140:881//kiali/kiali:v1.63
&lt;/code>&lt;/pre>&lt;h3 id="istio快速入门">
 Istio快速入门
 &lt;a class="anchor" href="#istio%e5%bf%ab%e9%80%9f%e5%85%a5%e9%97%a8">#&lt;/a>
&lt;/h3>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202407311104774.png" alt="image-20240731110412591" />&lt;/p></description></item><item><title>2024-08-02 K8S的最后一块拼图</title><link>https://qq547475331.github.io/docs/k8s%E7%9A%84%E6%9C%80%E5%90%8E%E4%B8%80%E5%9D%97%E6%8B%BC%E5%9B%BE-dbpaas-k8s-de-zui-hou-yi-kuai-pin-tu-dbpaas/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/k8s%E7%9A%84%E6%9C%80%E5%90%8E%E4%B8%80%E5%9D%97%E6%8B%BC%E5%9B%BE-dbpaas-k8s-de-zui-hou-yi-kuai-pin-tu-dbpaas/</guid><description>&lt;p>&lt;strong>K8s 的发展使得私有云跟公共云之间的技术差不断的缩小，不管是在私有云还是公共云，大家今天都在基于 K8s 去开发 PaaS 系统。而 K8s 作为构建 PaaS 的基础，其全景图里还缺最后一块“拼图”——dbPaaS。作为一个云数据库行业干了十几年的资深从业者，以及一个云数据库初创公司的创始人，在本文中，我将结合近年来数据库和云计算的发展方向，以及我们在技术和工程上的实践，分享一些看法。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>私有化部署，数据库发展的方向&lt;/strong>&lt;/p>
&lt;p>&lt;strong>近年来, 数据库整个领域主要在以下三个方向上发展: 公共云全托管，Serverless 和私有化部署。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>公共云全托管，包括各家云厂商的 RDS，以及每家云厂商自研的数据库，如 PolarDB、Aurora 等等，都属于这个板块。大部分公共云用户都是用这个服务，这个板块比较成熟稳定，跟着公共云的规模一起缓慢增长。公共云的主要的优势是弹性，有更多弹性需求的用户都在被吸引到公有云的 Serverless 产品上。而海外很多初创的公司，像 CockroachDB、Neon、 PlanetScale，包括中国的初创公司 TiDB 都在往 Serverless，无服务器数据库这个方向发展。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>另外一个方向就是私有化部署。有报告说中国公共云的服务器大概只占服务器总数的 5% ，线下部署的比例是非常高的。大型的互联网公司、央国企、银行都是用私有云。国内的信创数据库基本上都是在做这个方向。本质上来讲，企业选择什么数据库是由企业的业务场景来决定的，很少会因为选择一个数据库而倒推上面的云和架构。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202408011446554.png" alt="image-20240801144604472" />&lt;/strong>&lt;/p>
&lt;p>&lt;strong>公有云还是私有云？K8s 正在统一云的操作系统&lt;/strong>&lt;/p>
&lt;p>&lt;strong>企业对云的选择和使用是一直在变化的。选择私有云有很多原因，内因比如说合规、成本控制、自主可控，外因主要是 K8s。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202408011446303.png" alt="image-20240801144654236" />&lt;/strong>&lt;/p>
&lt;p>&lt;strong>第一个变化是中国的私有云正在从 OpenStack 全面进化到 K8s。过去的私有云，是用 OpenStack 管理物理机生产虚拟机。如果客户要用容器和 K8s 再在虚拟机上去搭上面的容器。但是最近，大家都开始用 K8s 去管理物理机，业务最好是能够全部容器化、云原生化。如果你要用虚拟机，可以在 K8s 上管理，例如用 kubevirt 这样的技术在 Pod 里面跑虚拟机，底层是 K8s，VM 在上层，这是一个挺大的变化。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>第二个变化是 K8s 的发展使得私有云跟公共云之间的技术差逐步变窄。公共云和私有云上的 K8s 技术差别并不大。过去云厂商主要的技术壁垒都在于我怎么把几百万台虚拟机运维好，用规模优势来降低成本，但是这样的技术在 K8s 面前就像马奇诺防线一样被绕过了。公共云和私有云今天都得站在怎么把容器做好，怎么把 K8s 生态做好这同一条起跑线上。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>第三个趋势是，不管运行在私有云还是公共云，大家都在基于 K8s 去开发 PaaS 系统。K8s 作为一个多云的，可以一统公共云、私有云的容器操作系统，可以屏蔽底层 IaaS 的差异，让上层通过 Pod、Service、PVC 等抽象来操作 IaaS 资源。调度器、服务发现、配置管理、API Server 等等这些开发一个 PaaS 所需要依赖的基础能力，K8s 都提供了，不仅可以节约很多开发成本，而且 K8s 在设计和工程上都做的更好。比如说，K8s 的 API 是声明式 API，这个就是个挺先进的设计。公共云厂商做的比较早，在设计 API 的时候还没有这个理念，所以 API 都是过程式的。但后来的系统，比方说 Terraform，就用的声明式 API，所以开发者使用起来更简单，书写起来更容易理解，也更不容易出错。在 K8s 上做 PaaS 就可以利用好这些后发优势。&lt;/strong>&lt;/p></description></item><item><title>2024-08-02 k8s背后service是如何工作的</title><link>https://qq547475331.github.io/docs/k8s%E8%83%8C%E5%90%8Eservice%E6%98%AF%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C%E7%9A%84-k8s-bei-hou-service-shi-ru-he-gong-zuo-de/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/k8s%E8%83%8C%E5%90%8Eservice%E6%98%AF%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C%E7%9A%84-k8s-bei-hou-service-shi-ru-he-gong-zuo-de/</guid><description>&lt;p>&lt;code>kube-proxy&lt;/code> 是 Kubernetes 集群中负责服务发现和负载均衡的组件之一。它是一个网络代理，运行在每个节点上, 用于 service 资源的负载均衡。它有两种模式：&lt;code>iptables&lt;/code> 和 &lt;code>ipvs&lt;/code>。&lt;/p>
&lt;h2 id="iptables">
 &lt;strong>iptables&lt;/strong>
 &lt;a class="anchor" href="#iptables">#&lt;/a>
&lt;/h2>
&lt;p>iptables 是 Linux 系统中的一个用户空间实用程序，用于配置内核的网络包过滤和网络地址转换（NAT）规则。它是 Linux 内核中的 netfilter 框架的一部分，并负责在网络包进入、转发或离开计算机时进行筛选和处理。其主要功能和用途包括：&lt;/p>
&lt;ol>
&lt;li>防火墙：iptables 提供了强大的防火墙功能，可以根据不同的规则来过滤和拒绝不需要的网络包。管理员可以创建自定义的规则集，允许或拒绝从特定 IP 地址、端口或协议的数据包。&lt;/li>
&lt;li>NAT（网络地址转换）：iptables 支持 NAT 功能，可以用来将私有网络中的计算机与外部网络连接。例如，它可以在一个 NAT 路由器上将内部网络的多个设备映射到单个外部 IP 地址。&lt;/li>
&lt;li>端口转发：iptables 可以将特定的端口流量从一个网络接口转发到另一个接口或目标 IP 地址，通常用于内部网络的服务公开。&lt;/li>
&lt;li>负载均衡：它也可以通过 DNAT（目标网络地址转换）功能将流量转发给多个内部服务器，实现简单的负载均衡。&lt;/li>
&lt;/ol>
&lt;p>iptables 是通过链（chains）和表（tables）来组织规则的。每个链由一组规则组成，当网络数据包经过时，这些规则会逐一执行。常用的表包括：&lt;/p>
&lt;ul>
&lt;li>filter 表：用于包过滤，是最常用的表。&lt;/li>
&lt;li>nat 表：用于网络地址转换。&lt;/li>
&lt;li>mangle 表：用于修改数据包的 IP 层字段。&lt;/li>
&lt;li>raw 表：用于绕过连接跟踪。&lt;/li>
&lt;/ul>
&lt;p>链的流向为：&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202408011443634.png" alt="image-20240801144356564" />&lt;/p>
&lt;p>所以，根据上图，我们能够想象出某些常用场景中，报文的流向：&lt;/p>
&lt;p>到本机某进程的报文：&lt;code>PREROUTING&lt;/code> –&amp;gt; &lt;code>INPUT&lt;/code>&lt;/p>
&lt;p>由本机转发的报文：&lt;code>PREROUTING&lt;/code> –&amp;gt; &lt;code>FORWARD&lt;/code> –&amp;gt; &lt;code>POSTROUTING&lt;/code>&lt;/p>
&lt;p>由本机的某进程发出报文（通常为响应报文）：&lt;code>OUTPUT&lt;/code> –&amp;gt; &lt;code>POSTROUTING&lt;/code>&lt;/p>
&lt;p>尽管在某些情况下配置 iptables 规则可能复杂，但它提供了高度的灵活性和强大的功能，使其成为 Linux 网络安全的重要组成部分。&lt;/p></description></item><item><title>2024-08-02 K8S面试题</title><link>https://qq547475331.github.io/docs/k8s%E9%9D%A2%E8%AF%95%E9%A2%98-k8s-mian-shi-ti/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/k8s%E9%9D%A2%E8%AF%95%E9%A2%98-k8s-mian-shi-ti/</guid><description>&lt;h2 id="简述etcd及其特点">
 &lt;strong>简述ETCD及其特点？&lt;/strong>
 &lt;a class="anchor" href="#%e7%ae%80%e8%bf%b0etcd%e5%8f%8a%e5%85%b6%e7%89%b9%e7%82%b9">#&lt;/a>
&lt;/h2>
&lt;p>&lt;strong>etcd 是 CoreOS 团队发起的开源项目，是一个管理配置信息和服务发现（service discovery）的项目，它的目标是构建一个高可用的分布式键值（key-value）数据库，基于 Go 语言实现。&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>特点：&lt;/strong>&lt;/p>
&lt;/li>
&lt;li>
&lt;ul>
&lt;li>&lt;strong>简单：支持 REST 风格的 HTTP+JSON API&lt;/strong>&lt;/li>
&lt;li>&lt;strong>安全：支持 HTTPS 方式的访问&lt;/strong>&lt;/li>
&lt;li>&lt;strong>快速：支持并发 1k/s 的写操作&lt;/strong>&lt;/li>
&lt;li>&lt;strong>可靠：支持分布式结构，基于 Raft 的一致性算法，Raft 是一套通过选举主节点来实现分布式系统一致性的算法。&lt;/strong>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="简述etcd适应的场景">
 &lt;strong>简述ETCD适应的场景？&lt;/strong>
 &lt;a class="anchor" href="#%e7%ae%80%e8%bf%b0etcd%e9%80%82%e5%ba%94%e7%9a%84%e5%9c%ba%e6%99%af">#&lt;/a>
&lt;/h2>
&lt;p>&lt;strong>etcd基于其优秀的特点，可广泛的应用于以下场景：&lt;/strong>&lt;/p>
&lt;p>&lt;strong>服务发现(Service Discovery)：服务发现主要解决在同一个分布式集群中的进程或服务，要如何才能找到对方并建立连接。本质上来说，服务发现就是想要了解集群中是否有进程在监听udp或tcp端口，并且通过名字就可以查找和连接。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>消息发布与订阅：在分布式系统中，最适用的一种组件间通信方式就是消息发布与订阅。即构建一个配置共享中心，数据提供者在这个配置中心发布消息，而消息使用者则订阅他们关心的主题，一旦主题有消息发布，就会实时通知订阅者。通过这种方式可以做到分布式系统配置的集中式管理与动态更新。应用中用到的一些配置信息放到etcd上进行集中管理。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>负载均衡：在分布式系统中，为了保证服务的高可用以及数据的一致性，通常都会把数据和服务部署多份，以此达到对等服务，即使其中的某一个服务失效了，也不影响使用。etcd本身分布式架构存储的信息访问支持负载均衡。etcd集群化以后，每个etcd的核心节点都可以处理用户的请求。所以，把数据量小但是访问频繁的消息数据直接存储到etcd中也可以实现负载均衡的效果。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>分布式通知与协调：与消息发布和订阅类似，都用到了etcd中的Watcher机制，通过注册与异步通知机制，实现分布式环境下不同系统之间的通知与协调，从而对数据变更做到实时处理。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>分布式锁：因为etcd使用Raft算法保持了数据的强一致性，某次操作存储到集群中的值必然是全局一致的，所以很容易实现分布式锁。锁服务有两种使用方式，一是保持独占，二是控制时序。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>集群监控与Leader竞选：通过etcd来进行监控实现起来非常简单并且实时性强。&lt;/strong>&lt;/p>
&lt;h2 id="简述什么是kubernetes">
 &lt;strong>简述什么是Kubernetes？&lt;/strong>
 &lt;a class="anchor" href="#%e7%ae%80%e8%bf%b0%e4%bb%80%e4%b9%88%e6%98%afkubernetes">#&lt;/a>
&lt;/h2>
&lt;p>&lt;strong>Kubernetes是一个全新的基于容器技术的分布式系统支撑平台。是Google开源的容器集群管理系统（谷歌内部:Borg）。在Docker技术的基础上，为容器化的应用提供部署运行、资源调度、服务发现和动态伸缩等一系列完整功能，提高了大规模容器集群管理的便捷性。并且具有完备的集群管理能力，多层次的安全防护和准入机制、多租户应用支撑能力、透明的服务注册和发现机制、內建智能负载均衡器、强大的故障发现和自我修复能力、服务滚动升级和在线扩容能力、可扩展的资源自动调度机制以及多粒度的资源配额管理能力。&lt;/strong>&lt;/p>
&lt;h2 id="简述kubernetes和docker的关系">
 &lt;strong>简述Kubernetes和Docker的关系？&lt;/strong>
 &lt;a class="anchor" href="#%e7%ae%80%e8%bf%b0kubernetes%e5%92%8cdocker%e7%9a%84%e5%85%b3%e7%b3%bb">#&lt;/a>
&lt;/h2>
&lt;p>&lt;strong>Docker 提供容器的生命周期管理和，Docker 镜像构建运行时容器。它的主要优点是将将软件/应用程序运行所需的设置和依赖项打包到一个容器中，从而实现了可移植性等优点。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>Kubernetes 用于关联和编排在多个主机上运行的容器。&lt;/strong>&lt;/p>
&lt;h2 id="简述kubernetes中什么是minikubekubectlkubelet">
 &lt;strong>简述Kubernetes中什么是Minikube、Kubectl、Kubelet？&lt;/strong>
 &lt;a class="anchor" href="#%e7%ae%80%e8%bf%b0kubernetes%e4%b8%ad%e4%bb%80%e4%b9%88%e6%98%afminikubekubectlkubelet">#&lt;/a>
&lt;/h2>
&lt;p>&lt;strong>Minikube 是一种可以在本地轻松运行一个单节点 Kubernetes 群集的工具。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>Kubectl 是一个命令行工具，可以使用该工具控制Kubernetes集群管理器，如检查群集资源，创建、删除和更新组件，查看应用程序。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>Kubelet 是一个代理服务，它在每个节点上运行，并使从服务器与主服务器通信。&lt;/strong>&lt;/p>
&lt;h2 id="简述kubernetes常见的部署方式">
 &lt;strong>简述Kubernetes常见的部署方式？&lt;/strong>
 &lt;a class="anchor" href="#%e7%ae%80%e8%bf%b0kubernetes%e5%b8%b8%e8%a7%81%e7%9a%84%e9%83%a8%e7%bd%b2%e6%96%b9%e5%bc%8f">#&lt;/a>
&lt;/h2>
&lt;p>&lt;strong>常见的Kubernetes部署方式有：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>kubeadm：也是推荐的一种部署方式；&lt;/strong>&lt;/li>
&lt;li>&lt;strong>二进制：网页上很多教程，未来我也会写一个&lt;/strong>&lt;/li>
&lt;li>&lt;strong>minikube：在本地轻松运行一个单节点 Kubernetes 群集的工具。&lt;/strong>&lt;/li>
&lt;li>&lt;/li>
&lt;/ul>
&lt;h2 id="简述kubernetes如何实现集群管理">
 &lt;strong>简述Kubernetes如何实现集群管理？&lt;/strong>
 &lt;a class="anchor" href="#%e7%ae%80%e8%bf%b0kubernetes%e5%a6%82%e4%bd%95%e5%ae%9e%e7%8e%b0%e9%9b%86%e7%be%a4%e7%ae%a1%e7%90%86">#&lt;/a>
&lt;/h2>
&lt;p>&lt;strong>在集群管理方面，Kubernetes将集群中的机器划分为一个Master节点和一群工作节点Node。其中，在Master节点运行着集群管理相关的一组进程kube-apiserver、kube-controller-manager和kube-scheduler，这些进程实现了整个集群的资源管理、Pod调度、弹性伸缩、安全控制、系统监控和纠错等管理能力，并且都是全自动完成的。&lt;/strong>&lt;/p></description></item><item><title>2024-08-02 kruise原地升级解析</title><link>https://qq547475331.github.io/docs/kruise%E5%8E%9F%E5%9C%B0%E5%8D%87%E7%BA%A7%E8%A7%A3%E6%9E%90-kruise-yuan-de-sheng-ji-jie-xi/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/kruise%E5%8E%9F%E5%9C%B0%E5%8D%87%E7%BA%A7%E8%A7%A3%E6%9E%90-kruise-yuan-de-sheng-ji-jie-xi/</guid><description>&lt;p>&lt;strong>从源码解析Kruise原地升级原理&lt;/strong>
&lt;strong>本文从源码的角度分析 Kruise 原地升级相关功能的实现。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>本篇Kruise版本为v1.5.2。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>Kruise项目地址: &lt;a href="https://github.com/openkruise/kruise">https://github.com/openkruise/kruise&lt;/a>&lt;/strong>&lt;/p>
&lt;p>&lt;strong>更多云原生、K8S相关文章请点击【专栏】查看！&lt;/strong>&lt;/p>
&lt;p>&lt;strong>原地升级的概念&lt;/strong>
&lt;strong>当我们使用deployment等Workload， 我们更改镜像版本时，k8s会删除原有pod进行重建，重建后pod的相关属性都有可能会变化， 比如uid、node、ipd等。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>原地升级的目的就是保持pod的相关属性不变，只更改镜像版本。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>下面的测试可以帮助理解kubelet的原地升级功能。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>测试一： 修改deployment镜像版本&lt;/strong>
&lt;strong>比如当前deployment使用nginx作为镜像, 且有一个pod实例：&lt;/strong>&lt;/p>
&lt;pre tabindex="0">&lt;code>~|⇒ kubectl get deployment test -o jsonpath=&amp;#34;{.spec.template.spec.containers[0]}&amp;#34;
{&amp;#34;image&amp;#34;:&amp;#34;nginx&amp;#34;,&amp;#34;imagePullPolicy&amp;#34;:&amp;#34;Always&amp;#34;,&amp;#34;name&amp;#34;:&amp;#34;nginx&amp;#34;,&amp;#34;resources&amp;#34;:{},&amp;#34;terminationMessagePath&amp;#34;:&amp;#34;/dev/termination-log&amp;#34;,&amp;#34;terminationMessagePolicy&amp;#34;:&amp;#34;File&amp;#34;}
~|⇒ kubectl get pod
NAME READY STATUS RESTARTS AGE
test-5746d4c59f-nwc6q 1/1 Running 0 10m
web-0 1/1 Running 1 (71m ago) 18d
&lt;/code>&lt;/pre>&lt;p>&lt;strong>修改镜像版本后， pod会被重建：&lt;/strong>&lt;/p>
&lt;pre tabindex="0">&lt;code>~|⇒ kubectl edit deployment test
deployment.apps/test edited
~|⇒ kubectl get pod
NAME READY STATUS RESTARTS AGE
test-5746d4c59f-nwc6q 1/1 Running 0 11m
test-674d57777c-8qc7c 0/1 ContainerCreating 0 2s
web-0 1/1 Running 1 (72m ago) 18d
~|⇒ kubectl get pod
NAME READY STATUS RESTARTS AGE
test-674d57777c-8qc7c 1/1 Running 0 42s
&lt;/code>&lt;/pre>&lt;blockquote>
&lt;p>&lt;strong>可以看到，pod被重建后，pod的名称（以及其他属性）发生了变化。&lt;/strong>&lt;/p></description></item><item><title>2024-08-02 kubewharf</title><link>https://qq547475331.github.io/docs/kubewharf-kubewharf/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/kubewharf-kubewharf/</guid><description>&lt;p>深入云原生—基于 KubeWharf 深度剖析场景与解读。我们需要先了解一下 KubeWharf，可能很多人都感觉到有点陌生吧，下面我们来一起学习！&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202408011629456.png" alt="image-20240801162956360" />&lt;/p>
&lt;h1 id="一kubewharf-详解">
 🌰一.KubeWharf 详解
 &lt;a class="anchor" href="#%e4%b8%80kubewharf-%e8%af%a6%e8%a7%a3">#&lt;/a>
&lt;/h1>
&lt;p>KubeWharf 是字节跳动基础架构团队在对 Kubernetes 进行了大规模应用和不断优化增强之后的技术结晶。这是一套以 Kubernetes 为基础构建的分布式操作系统，由一组云原生组件构成，专注于提高系统的可扩展性、功能性、稳定性、可观测性、安全性等，以支持大规模多租集群、在离线混部、存储和机器学习云原生化等场景。&lt;/p>
&lt;p>KubeWharf 由以下项目组成：&lt;/p>
&lt;p>● KubeBrain 是一个高性能的 Kubernetes 元数据系统，用于存储和管理 Kubernetes 集群的元数据。&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202408011630078.png" alt="image-20240801163017988" />&lt;/p>
&lt;p>KubeZoo 是一个轻量级的 Kubernetes 多租户网关，用于在 Kubernetes 集群之间进行安全隔离。&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202408011630860.png" alt="image-20240801163030753" />&lt;/p>
&lt;p>● KubeGateway 是一个 Kubernetes 网关，用于在本地和云端 Kubernetes 集群之间进行通信。&lt;/p>
&lt;p>● Godel Scheduler 是一个高性能的 Kubernetes 调度器，用于在 Kubernetes 集群中智能地调度容器。&lt;/p>
&lt;p>KubeWharf 是一个分布式操作系统，由字节跳动基础架构团队在应用和优化增强 Kubernetes 之后创建。这个系统是一套以 Kubernetes 为基础构建的分布式操作系统，由一组云原生组件构成，专注于提高系统的可扩展性、功能性、稳定性、可观测性、安全性等，以支持大规模多租集群、在离线混部、存储和机器学习云原生化等场景。&lt;/p>
&lt;h1 id="二一个真实的-kubewharf-应用场景">
 二.一个真实的 KubeWharf 应用场景
 &lt;a class="anchor" href="#%e4%ba%8c%e4%b8%80%e4%b8%aa%e7%9c%9f%e5%ae%9e%e7%9a%84-kubewharf-%e5%ba%94%e7%94%a8%e5%9c%ba%e6%99%af">#&lt;/a>
&lt;/h1>
&lt;p>这是一个真实的应用场景哈，因为涉及业务，这里只能脱敏化的描述了。&lt;/p>
&lt;p>一家大型互联网公司某某 X 需要构建一个可扩展、稳定、安全的 Kubernetes 集群，用于支持其在线业务和离线业务。&lt;/p>
&lt;p>需求：&lt;/p>
&lt;p>● 需要支持数万个节点。&lt;/p></description></item><item><title>2024-08-02 linux awk文本处理器 8个案例</title><link>https://qq547475331.github.io/docs/linux-awk%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E5%99%A8-8%E4%B8%AA%E6%A1%88%E4%BE%8B-linuxawk-wen-ben-chu-li-qi-8-ge-an-li/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/linux-awk%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E5%99%A8-8%E4%B8%AA%E6%A1%88%E4%BE%8B-linuxawk-wen-ben-chu-li-qi-8-ge-an-li/</guid><description>&lt;p>AWK是Linux下一个非常强大的文本处理工具，能够分析各种复杂文件。&lt;/p>
&lt;p>以下是8个常见的AWK文本处理案例，可助你提升AWK技能！&lt;/p>
&lt;p>&lt;strong>案例一：分析Nginx访问日志&lt;/strong>&lt;/p>
&lt;pre tabindex="0">&lt;code>统计访问IP次数：
awk &amp;#39;{a[$1]++}END{for(v in a)print v,a[v]}&amp;#39; access.log

统计访问IP次数：
awk &amp;#39;{a[$1]++}END{for(v in a)print v,a[v]}&amp;#39; access.log

统计访问访问大于100次的IP：
awk &amp;#39;{a[$1]++}END{for(v in a){if(a[v]&amp;gt;100)print v,a[v]}}&amp;#39; access.log

统计访问IP次数并排序取前10：
awk &amp;#39;{a[$1]++}END{for(v in a)print v,a[v]|&amp;#34;sort -k2 -nr |head -10&amp;#34;}&amp;#39; access.log

统计时间段访问最多的IP：
awk &amp;#39;$4&amp;gt;=&amp;#34;[02/Jan/2017:00:02:00&amp;#34; &amp;amp;&amp;amp; $4&amp;lt;=&amp;#34;[02/Jan/2017:00:03:00&amp;#34;{a[$1]++}END{for(v in a)print v,a[v]}&amp;#39; access.log

统计上一分钟访问量：
date=$(date -d &amp;#39;-1 minute&amp;#39; +%d/%d/%Y:%H:%M)
awk -vdate=$date &amp;#39;$4~date{c++}END{print c}&amp;#39; access.log

统计访问最多的10个页面：
awk &amp;#39;{a[$7]++}END{for(v in a)print v,a[v]|&amp;#34;sort -k1 -nr|head -n10&amp;#34;}&amp;#39; access.log

统计每个URL数量和返回内容总大小:
awk &amp;#39;{a[$7]++;size[$7]+=$10}END{for(v in a)print a[v],v,size[v]}&amp;#39; access.log

统计每个IP访问状态码数量：
awk &amp;#39;{a[$1&amp;#34; &amp;#34;$9]++}END{for(v in a)print v,a[v]}&amp;#39; access.log

统计访问IP是404状态次数：
awk &amp;#39;{if($9~/404/)a[$1&amp;#34; &amp;#34;$9]++}END{for(i in a)print v,a[v]}&amp;#39; access.log
&lt;/code>&lt;/pre>&lt;p>&lt;strong>案例二：两个文件差异对比&lt;/strong>&lt;/p></description></item><item><title>2024-08-02 linux系统性能优化 七个实战经验</title><link>https://qq547475331.github.io/docs/linux%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-%E4%B8%83%E4%B8%AA%E5%AE%9E%E6%88%98%E7%BB%8F%E9%AA%8C-linux-xi-tong-xing-neng-you-hua-qi-ge-shi-zhan-jing-yan/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/linux%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-%E4%B8%83%E4%B8%AA%E5%AE%9E%E6%88%98%E7%BB%8F%E9%AA%8C-linux-xi-tong-xing-neng-you-hua-qi-ge-shi-zhan-jing-yan/</guid><description>&lt;h1 id="linux系统性能优化七个实战经验">
 Linux系统性能优化：七个实战经验
 &lt;a class="anchor" href="#linux%e7%b3%bb%e7%bb%9f%e6%80%a7%e8%83%bd%e4%bc%98%e5%8c%96%e4%b8%83%e4%b8%aa%e5%ae%9e%e6%88%98%e7%bb%8f%e9%aa%8c">#&lt;/a>
&lt;/h1>
&lt;p>Linux系统的性能是指操作系统完成任务的有效性、稳定性和响应速度。Linux系统管理员可能经常会遇到系统不稳定、响应速度慢等问题，例如在Linux上搭建了一个web服务，经常出现网页无法打开、打开速度慢等现象，而遇到这些问题，就有人会抱怨Linux系统不好，其实这些都是表面现象。&lt;/p>
&lt;p>操作系统完成一个任务时，与系统自身设置、网络拓朴结构、路由设备、路由策略、接入设备、物理线路等多个方面都密切相关，任何一个环节出现问题，都会影响整个系统的性能。因此当Linux应用出现问题时，应当从应用程序、操作系统、服务器硬件、网络环境等方面综合排查，定位问题出现在哪个部分，然后集中解决。&lt;/p>
&lt;p>随着容器时代的普及和AI技术的颠覆，面对越来越复杂的业务和架构，再加上企业的降本增效已提上了日程，因此对Linux的高性能、可靠性提出了更高的要求，Linux性能优化成为运维人员的必备的核心技能。&lt;/p>
&lt;p>例如，主机CPU使用率过高报警，登录Linux上去top完之后，却不知道怎么进一步定位，到底是系统CPU资源太少，还是应用程序导致的问题？这些Linux性能问题一直困扰着我们，哪怕工作多年的资深工程师也不例外。&lt;/p>
&lt;h1 id="本文根据社区探讨整理出企业linux系统性能优化的7个实战经验来自社区专家和会员分享希望对大家有所帮助">
 本文根据社区探讨，整理出&lt;strong>企业Linux系统性能优化的7个实战经验&lt;/strong>，来自社区专家和会员分享，希望对大家有所帮助。
 &lt;a class="anchor" href="#%e6%9c%ac%e6%96%87%e6%a0%b9%e6%8d%ae%e7%a4%be%e5%8c%ba%e6%8e%a2%e8%ae%a8%e6%95%b4%e7%90%86%e5%87%ba%e4%bc%81%e4%b8%9alinux%e7%b3%bb%e7%bb%9f%e6%80%a7%e8%83%bd%e4%bc%98%e5%8c%96%e7%9a%847%e4%b8%aa%e5%ae%9e%e6%88%98%e7%bb%8f%e9%aa%8c%e6%9d%a5%e8%87%aa%e7%a4%be%e5%8c%ba%e4%b8%93%e5%ae%b6%e5%92%8c%e4%bc%9a%e5%91%98%e5%88%86%e4%ba%ab%e5%b8%8c%e6%9c%9b%e5%af%b9%e5%a4%a7%e5%ae%b6%e6%9c%89%e6%89%80%e5%b8%ae%e5%8a%a9">#&lt;/a>
&lt;/h1>
&lt;h1 id="">
 
 &lt;a class="anchor" href="#">#&lt;/a>
&lt;/h1>
&lt;h2 id="1影响linux系统性能的因素一般有哪些">
 &lt;strong>1、影响Linux系统性能的因素一般有哪些？&lt;/strong>
 &lt;a class="anchor" href="#1%e5%bd%b1%e5%93%8dlinux%e7%b3%bb%e7%bb%9f%e6%80%a7%e8%83%bd%e7%9a%84%e5%9b%a0%e7%b4%a0%e4%b8%80%e8%88%ac%e6%9c%89%e5%93%aa%e4%ba%9b">#&lt;/a>
&lt;/h2>
&lt;hr>
&lt;h2 id="zhaoxiaoyong081-平安科技-资深工程师">
 @zhaoxiaoyong081 平安科技 资深工程师：
 &lt;a class="anchor" href="#zhaoxiaoyong081-%e5%b9%b3%e5%ae%89%e7%a7%91%e6%8a%80-%e8%b5%84%e6%b7%b1%e5%b7%a5%e7%a8%8b%e5%b8%88">#&lt;/a>
&lt;/h2>
&lt;p>Linux系统的性能受多个因素的影响。以下是一些常见的影响Linux系统性能的因素：&lt;/p>
&lt;ul>
&lt;li>CPU负载：CPU的利用率和负载水平对系统性能有直接影响。高CPU负载可能导致进程响应变慢、延迟增加和系统变得不稳定。&lt;/li>
&lt;li>内存使用：内存是系统运行的关键资源。当系统内存不足时，可能会导致进程被终止、交换分区使用过多以及系统性能下降。&lt;/li>
&lt;li>磁盘I/O：磁盘I/O性能是影响系统响应时间和吞吐量的重要因素。高磁盘I/O负载可能导致延迟增加、响应变慢和系统性能下降。&lt;/li>
&lt;li>网络负载：网络流量的增加和网络延迟会对系统性能产生影响。高网络负载可能导致网络延迟增加、响应变慢和系统资源竞争。&lt;/li>
&lt;li>进程调度：Linux系统使用进程调度器来管理和分配CPU资源。调度算法的选择和配置会影响进程的优先级和执行顺序，从而影响系统的响应能力和负载均衡。&lt;/li>
&lt;li>文件系统性能：文件系统的选择和配置对磁盘I/O性能有影响。不同的文件系统可能在性能方面有所差异，适当的文件系统选项和调整可以改善系统性能。&lt;/li>
&lt;li>内核参数：Linux内核有许多可调整的参数，可以影响系统的性能和行为。例如，TCP/IP参数、内存管理参数、文件系统缓存等。适当的内核参数调整可以改善系统的性能和资源利用率。&lt;/li>
&lt;li>资源限制和配额：在多用户环境中，资源限制和配额的设置可以控制每个用户或进程可使用的资源量。适当的资源管理可以避免某些进程耗尽系统资源而导致性能问题。&lt;/li>
&lt;/ul>
&lt;p>这些因素之间相互关联，对系统性能产生综合影响。为了优化Linux系统性能，需要综合考虑并适当调整这些因素，以满足特定的需求和使用情况。&lt;/p>
&lt;h2 id="2工作中有没有快速排除故障的办法">
 &lt;strong>2、工作中有没有快速排除故障的办法？&lt;/strong>
 &lt;a class="anchor" href="#2%e5%b7%a5%e4%bd%9c%e4%b8%ad%e6%9c%89%e6%b2%a1%e6%9c%89%e5%bf%ab%e9%80%9f%e6%8e%92%e9%99%a4%e6%95%85%e9%9a%9c%e7%9a%84%e5%8a%9e%e6%b3%95">#&lt;/a>
&lt;/h2>
&lt;h2 id="">
 
 &lt;a class="anchor" href="#">#&lt;/a>
&lt;/h2>
&lt;hr>
&lt;h2 id="zhaoxiaoyong081-平安科技-资深工程师-1">
 @zhaoxiaoyong081 平安科技 资深工程师：
 &lt;a class="anchor" href="#zhaoxiaoyong081-%e5%b9%b3%e5%ae%89%e7%a7%91%e6%8a%80-%e8%b5%84%e6%b7%b1%e5%b7%a5%e7%a8%8b%e5%b8%88-1">#&lt;/a>
&lt;/h2>
&lt;p>1.CPU 性能分析&lt;/p>
&lt;p>利用 top、vmstat、pidstat、strace 以及 perf 等几个最常见的工具，获取 CPU 性能指标后，再结合进程与 CPU 的工作原理，就可以迅速定位出 CPU 性能瓶颈的来源。&lt;/p>
&lt;p>比如说，当你收到系统的用户 CPU 使用率过高告警时，从监控系统中直接查询到，导致 CPU 使用率过高的进程；然后再登录到进程所在的 Linux 服务器中，分析该进程的行为。你可以使用 strace，查看进程的系统调用汇总；也可以使用 perf 等工具，找出进程的热点函数；甚至还可以使用动态追踪的方法，来观察进程的当前执行过程，直到确定瓶颈的根源。&lt;/p>
&lt;p>2.内存性能分析&lt;/p>
&lt;p>可以通过 free 和 vmstat 输出的性能指标，确认内存瓶颈；然后，再根据内存问题的类型，进一步分析内存的使用、分配、泄漏以及缓存等，最后找出问题的来源。&lt;/p>
&lt;p>比如说，当你收到内存不足的告警时，首先可以从监控系统中。找出占用内存最多的几个进程。然后，再根据这些进程的内存占用历史，观察是否存在内存泄漏问题。确定出最可疑的进程后，再登录到进程所在的 Linux 服务器中，分析该进程的内存空间或者内存分配，最后弄清楚进程为什么会占用大量内存。&lt;/p></description></item><item><title>2024-08-02 linux运维工程师50个常见面试题</title><link>https://qq547475331.github.io/docs/linux%E8%BF%90%E7%BB%B4%E5%B7%A5%E7%A8%8B%E5%B8%8850%E4%B8%AA%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98-linux-yun-wei-gong-cheng-shi-50-ge-chang-jian-mian-shi-ti/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/linux%E8%BF%90%E7%BB%B4%E5%B7%A5%E7%A8%8B%E5%B8%8850%E4%B8%AA%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98-linux-yun-wei-gong-cheng-shi-50-ge-chang-jian-mian-shi-ti/</guid><description>&lt;p>&lt;strong>1、请简述OSI七层网络模型有哪些层及各自的含义?&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>物理层：底层数据传输，比如网线、网卡标准&lt;/strong>&lt;/li>
&lt;li>&lt;strong>数据链路层：定义数据的基本格式，如何传输，如何标识。比如网卡MAC地址&lt;/strong>&lt;/li>
&lt;li>&lt;strong>网络层：定义IP编码，定义路由功能，比如不同设备的数据转发&lt;/strong>&lt;/li>
&lt;li>&lt;strong>传输层：端到端传输数据的基本功能，比如TCP、UDP&lt;/strong>&lt;/li>
&lt;li>&lt;strong>会话层：控制应用程序之间会话能力，比如不同软件数据分发给不停软件&lt;/strong>&lt;/li>
&lt;li>&lt;strong>表示层：数据格式标识，基本压缩加密功能。&lt;/strong>&lt;/li>
&lt;li>&lt;strong>应用层：各种应用软件，包括 Web 应用。&lt;/strong>&lt;/li>
&lt;/ul>
&lt;h3 id="2在linux的lvm分区格式下请简述给根分区磁盘扩容的步骤">
 &lt;strong>2、在Linux的LVM分区格式下，请简述给根分区磁盘扩容的步骤?&lt;/strong>
 &lt;a class="anchor" href="#2%e5%9c%a8linux%e7%9a%84lvm%e5%88%86%e5%8c%ba%e6%a0%bc%e5%bc%8f%e4%b8%8b%e8%af%b7%e7%ae%80%e8%bf%b0%e7%bb%99%e6%a0%b9%e5%88%86%e5%8c%ba%e7%a3%81%e7%9b%98%e6%89%a9%e5%ae%b9%e7%9a%84%e6%ad%a5%e9%aa%a4">#&lt;/a>
&lt;/h3>
&lt;p>&lt;strong>这个分3种&lt;/strong>&lt;/p>
&lt;p>&lt;strong>第一种方法:&lt;/strong>&lt;/p>
&lt;pre tabindex="0">&lt;code>growpart /dev/vda 1
resize2fs /dev/vda1 
&lt;/code>&lt;/pre>&lt;p>&lt;strong>第二种方法:&lt;/strong>&lt;/p>
&lt;pre tabindex="0">&lt;code>partpeobe /dev/sda
resize2fs /dev/vda1 
&lt;/code>&lt;/pre>&lt;p>&lt;strong>第三种方法:&lt;/strong>&lt;/p>
&lt;pre tabindex="0">&lt;code>fdisk /dev/sdb # n p 1 1 回车 回车 t 8e w
pvcreate /dev/sdb1
vgextend datavg /dev/sdb1
lvextend -r -L +100%free /dev/mapper/datavg-lv01
&lt;/code>&lt;/pre>&lt;h3 id="3讲述一下tomcat800580098080三个端口的含义">
 &lt;strong>3、讲述一下Tomcat8005、8009、8080三个端口的含义？&lt;/strong>
 &lt;a class="anchor" href="#3%e8%ae%b2%e8%bf%b0%e4%b8%80%e4%b8%8btomcat800580098080%e4%b8%89%e4%b8%aa%e7%ab%af%e5%8f%a3%e7%9a%84%e5%90%ab%e4%b9%89">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>8005 关闭时使用&lt;/strong>&lt;/li>
&lt;li>&lt;strong>8009为AJP端口，即容器使用，如Apache能通过AJP协议访问Tomcat的8009端口来实现功能&lt;/strong>&lt;/li>
&lt;li>&lt;strong>8080 一般应用使用&lt;/strong>&lt;/li>
&lt;/ul>
&lt;h3 id="4简述dns进行域名解析的过程">
 &lt;strong>4、简述DNS进行域名解析的过程？&lt;/strong>
 &lt;a class="anchor" href="#4%e7%ae%80%e8%bf%b0dns%e8%bf%9b%e8%a1%8c%e5%9f%9f%e5%90%8d%e8%a7%a3%e6%9e%90%e7%9a%84%e8%bf%87%e7%a8%8b">#&lt;/a>
&lt;/h3>
&lt;p>&lt;strong>迭代查询（返回最优结果）、递归查询（本地找DNS）用户要访问 &lt;a href="https://www.baidu.com">www.baidu.com&lt;/a>，会先找本机的host文件，再找本地设置的DNS服务器，如果也没有找到，就去网络中找根服务器，根服务器反馈结果，说只能提供一级域名服务器.cn，就去找一级域名服务器，一级域名服务器说只能提供二级域名服务器.com.cn,就去找二级域名服务器，二级域服务器只能提供三级域名服务器.baidu.com.cn，就去找三级域名服务器，三级域名服务器正好有这个网站www.baidu.com，然后发给请求的服务器，保存一份之后，再发给客户端。&lt;/strong>&lt;/p>
&lt;h3 id="5讲一下keepalived的工作原理">
 &lt;strong>5、讲一下Keepalived的工作原理？&lt;/strong>
 &lt;a class="anchor" href="#5%e8%ae%b2%e4%b8%80%e4%b8%8bkeepalived%e7%9a%84%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86">#&lt;/a>
&lt;/h3>
&lt;p>&lt;strong>在一个虚拟路由器中，只有作为MASTER的VRRP(虚拟路由冗余协议)路由器会一直发送VRRP通告信息，BACKUP不会抢占MASTER，除非它的优先级更高。当MASTER不可用时(BACKUP收不到通告信息)多台BACKUP中优先级最高的这台会被抢占为MASTER。这种抢占是非常快速的(&amp;lt;1s)，以保证服务的连续性由于安全性考虑，VRRP包使用了加密协议进行加密。BACKUP不会发送通告信息，只会接收通告信息。&lt;/strong>&lt;/p>
&lt;h3 id="6lvsnginxhaproxy有什么区别工作中你怎么选择">
 &lt;strong>6、LVS、Nginx、HAproxy有什么区别？工作中你怎么选择？&lt;/strong>
 &lt;a class="anchor" href="#6lvsnginxhaproxy%e6%9c%89%e4%bb%80%e4%b9%88%e5%8c%ba%e5%88%ab%e5%b7%a5%e4%bd%9c%e4%b8%ad%e4%bd%a0%e6%80%8e%e4%b9%88%e9%80%89%e6%8b%a9">#&lt;/a>
&lt;/h3>
&lt;p>&lt;strong>LVS：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>抗负载能力强、工作在第4层仅作分发之用，没有流量的产生，这个特点也决定了它在负载均衡软件里的性能最强的；无流量，同时保证了均衡器IO的性能不会受到大流量的影响；&lt;/strong>&lt;/li>
&lt;li>&lt;strong>工作稳定，自身有完整的双机热备方案，如LVS+Keepalived和LVS+Heartbeat；&lt;/strong>&lt;/li>
&lt;li>&lt;strong>应用范围比较广，可以对所有应用做负载均衡；&lt;/strong>&lt;/li>
&lt;li>&lt;strong>配置简单，因为没有可太多配置的东西，所以并不需要太多接触，大大减少了人为出错的几率；&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>LVS的缺点：&lt;/strong>&lt;/p></description></item><item><title>2024-08-02 netctl检测集群pod间连通性</title><link>https://qq547475331.github.io/docs/netctl%E6%A3%80%E6%B5%8B%E9%9B%86%E7%BE%A4pod%E9%97%B4%E8%BF%9E%E9%80%9A%E6%80%A7-netctl-jian-ce-ji-qun-pod-jian-lian-tong-xing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/netctl%E6%A3%80%E6%B5%8B%E9%9B%86%E7%BE%A4pod%E9%97%B4%E8%BF%9E%E9%80%9A%E6%80%A7-netctl-jian-ce-ji-qun-pod-jian-lian-tong-xing/</guid><description>&lt;h1 id="netdoctor">
 NetDoctor
 &lt;a class="anchor" href="#netdoctor">#&lt;/a>
&lt;/h1>
&lt;blockquote>
&lt;p>&lt;a href="README.md">English&lt;/a> | 中文&lt;/p>&lt;/blockquote>
&lt;h2 id="介绍">
 介绍
 &lt;a class="anchor" href="#%e4%bb%8b%e7%bb%8d">#&lt;/a>
&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>Kubernetes集群投入使用后集群网络可能会存在种种的连通性问题，因此我们希望可以有一个验收工具，在完成部署后检查集群的网络连通性是否正常。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>另一方面，Kosmos是一个跨集群的解决方案，在Kosmos管理多个集群之前，需要先检查各个集群的容器网络自身是否存在问题，部署完成后也需要验证跨集群的网络是否已经被Kosmos连通。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>出于以上两个方面，我们设计了&lt;code>NetDoctor&lt;/code>工具用于解决Kubernetes集群遇到的网络问题。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="架构">
 架构
 &lt;a class="anchor" href="#%e6%9e%b6%e6%9e%84">#&lt;/a>
&lt;/h2>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202407151645896.png" alt="image-20240715164552817" />&lt;/p>
&lt;h2 id="先决条件">
 先决条件
 &lt;a class="anchor" href="#%e5%85%88%e5%86%b3%e6%9d%a1%e4%bb%b6">#&lt;/a>
&lt;/h2>
&lt;ul>
&lt;li>&lt;code>go&lt;/code> 版本 v1.15+&lt;/li>
&lt;li>&lt;code>kubernetes&lt;/code> 版本 v1.16+&lt;/li>
&lt;/ul>
&lt;h2 id="快速开始">
 快速开始
 &lt;a class="anchor" href="#%e5%bf%ab%e9%80%9f%e5%bc%80%e5%a7%8b">#&lt;/a>
&lt;/h2>
&lt;h3 id="netctl工具">
 netctl工具
 &lt;a class="anchor" href="#netctl%e5%b7%a5%e5%85%b7">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>NetDoctor提供配套工具&lt;code>netctl&lt;/code>，您可以方便的通过命令行去进行Kubernetes集群的网络连通性检查。&lt;/li>
&lt;/ul>
&lt;h4 id="从制品库获取">
 从制品库获取
 &lt;a class="anchor" href="#%e4%bb%8e%e5%88%b6%e5%93%81%e5%ba%93%e8%8e%b7%e5%8f%96">#&lt;/a>
&lt;/h4>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>wget https://github.com/kosmos-io/netdoctor/releases/download/v0.0.1/netctl-linux-amd64 
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mv netctl-linux-amd64 netctl
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="从源码编译">
 从源码编译
 &lt;a class="anchor" href="#%e4%bb%8e%e6%ba%90%e7%a0%81%e7%bc%96%e8%af%91">#&lt;/a>
&lt;/h4>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 下载项目源码&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ git clone https://github.com/kosmos-io/netdoctor.git
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 执行后netctl会输出至./netdoctor/_output/bin/linux/amd64目录&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ make netctl
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="netctl命令">
 netctl命令
 &lt;a class="anchor" href="#netctl%e5%91%bd%e4%bb%a4">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>&lt;code>netctl init&lt;/code>命令用于在当前目录生成网络检查需要的配置文件&lt;code>config.json&lt;/code>，示例如下：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ netctl init
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>I0205 16:27:26.258964 &lt;span style="color:#ae81ff">2765415&lt;/span> init.go:69&lt;span style="color:#f92672">]&lt;/span> write opts success
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ cat config.json
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;namespace&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;kosmos-system&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;version&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;v0.2.0&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;protocol&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;tcp&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;podWaitTime&amp;#34;&lt;/span>: 30,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;port&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;8889&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;maxNum&amp;#34;&lt;/span>: 3,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;cmdTimeout&amp;#34;&lt;/span>: 10,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;srcKubeConfig&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;~/.kube/config&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;srcImageRepository&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;ghcr.io/kosmos-io&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>&lt;code>netctl check&lt;/code>命令会读取&lt;code>config.json&lt;/code>，然后创建一个名为&lt;code>Floater&lt;/code>的&lt;code>DaemonSet&lt;/code>以及相关联的一些资源，之后会获取所有的&lt;code>Floater&lt;/code>的&lt;code>IP&lt;/code>信息，然后依次进入到&lt;code>Pod&lt;/code>中执行&lt;code>Ping&lt;/code>或者&lt;code>Curl&lt;/code>命令。需要注意的是，这个操作是并发执行的，并发度根据&lt;code>config.json&lt;/code>中的&lt;code>maxNum&lt;/code>参数动态变化。&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ netctl check
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>I0205 16:34:06.147671 &lt;span style="color:#ae81ff">2769373&lt;/span> check.go:61&lt;span style="color:#f92672">]&lt;/span> use config from file!!!!!!
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>I0205 16:34:06.148619 &lt;span style="color:#ae81ff">2769373&lt;/span> floater.go:73&lt;span style="color:#f92672">]&lt;/span> create Clusterlink floater, namespace: kosmos-system
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>I0205 16:34:06.157582 &lt;span style="color:#ae81ff">2769373&lt;/span> floater.go:83&lt;span style="color:#f92672">]&lt;/span> create Clusterlink floater, apply RBAC
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>I0205 16:34:06.167799 &lt;span style="color:#ae81ff">2769373&lt;/span> floater.go:94&lt;span style="color:#f92672">]&lt;/span> create Clusterlink floater, version: v0.2.0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>I0205 16:34:09.178566 &lt;span style="color:#ae81ff">2769373&lt;/span> verify.go:79&lt;span style="color:#f92672">]&lt;/span> pod: clusterlink-floater-9dzsg is ready. status: Running
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>I0205 16:34:09.179593 &lt;span style="color:#ae81ff">2769373&lt;/span> verify.go:79&lt;span style="color:#f92672">]&lt;/span> pod: clusterlink-floater-cscdh is ready. status: Running
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Do check... 100% &lt;span style="color:#f92672">[================================================================================]&lt;/span> &lt;span style="color:#f92672">[&lt;/span>0s&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>+-----+----------------+----------------+-----------+-----------+
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| S/N | SRC NODE NAME | DST NODE NAME | TARGET IP | RESULT |
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>+-----+----------------+----------------+-----------+-----------+
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| &lt;span style="color:#ae81ff">1&lt;/span> | ecs-net-dr-001 | ecs-net-dr-001 | 10.0.1.86 | SUCCESSED |
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| &lt;span style="color:#ae81ff">2&lt;/span> | ecs-net-dr-002 | ecs-net-dr-002 | 10.0.2.29 | SUCCESSED |
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>+-----+----------------+----------------+-----------+-----------+
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>+-----+----------------+----------------+-----------+-----------+-------------------------------+
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| S/N | SRC NODE NAME | DST NODE NAME | TARGET IP | RESULT | LOG |
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>+-----+----------------+----------------+-----------+-----------+-------------------------------+
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| &lt;span style="color:#ae81ff">1&lt;/span> | ecs-net-dr-002 | ecs-net-dr-001 | 10.0.1.86 | EXCEPTION |exec error: unable to upgrade |
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| &lt;span style="color:#ae81ff">2&lt;/span> | ecs-net-dr-001 | ecs-net-dr-002 | 10.0.2.29 | EXCEPTION |connection: container not......|
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>+-----+----------------+----------------+-----------+-----------+-------------------------------+
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>I0205 16:34:09.280220 &lt;span style="color:#ae81ff">2769373&lt;/span> &lt;span style="color:#66d9ef">do&lt;/span>.go:93&lt;span style="color:#f92672">]&lt;/span> write opts success
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>在&lt;code>check&lt;/code>命令执行的过程中，会有进度条显示校验进度。命令执行完成后，会打印检查结果，并将结果保存在文件&lt;code>resume.json&lt;/code>中。&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;Status&amp;#34;&lt;/span>: 0,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;ResultStr&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;exec error: unable to upgrade connection: container not found (\&amp;#34;floater\&amp;#34;), stderr: &amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;srcNodeName&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;ecs-sealos-001&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;dstNodeName&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;ecs-sealos-002&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;targetIP&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;10.0.2.29&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;Status&amp;#34;&lt;/span>: 0,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;ResultStr&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;exec error: command terminated with exit code 7, stderr % Total % Received % Xferd Average Speed Time Time Time Current\n Dload Upload Total Spent Left Speed\n\r 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0\r 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0\ncurl: (7) Failed to connect to 10.0.0.36 port 8889 after 0 ms: Couldn&amp;#39;t connect to server\n&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;srcNodeName&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;ecs-sealos-002&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;dstNodeName&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;ecs-sealos-001&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;targetIP&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;10.0.0.36&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>如果需要检查Kosmos集群联邦中任意两个集群之间的网络连通性，则可以在配置文件&lt;code>config.json&lt;/code>增加参数&lt;code>dstKubeConfig&lt;/code>和&lt;code>dstImageRepository&lt;/code>，这样就可以检查两个集群之间网络连通性了。&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ vim config.json
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;namespace&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;kosmos-system&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;version&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;v0.2.0&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;protocol&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;tcp&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;podWaitTime&amp;#34;&lt;/span>: 30,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;port&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;8889&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;maxNum&amp;#34;&lt;/span>: 3,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;cmdTimeout&amp;#34;&lt;/span>: 10,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;srcKubeConfig&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;~/.kube/src-config&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;srcImageRepository&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;ghcr.io/kosmos-io&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;dstKubeConfig&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;~/.kube/dst-config&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;dstImageRepository&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;ghcr.io/kosmos-io&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>&lt;code>netctl resume&lt;/code>命令用于复测时只检验第一次检查时有问题的集群节点。因为线上环境节点数量很多，单次检查可能会需要比较长的时间才能生成结果，所以我们希望仅对前一次检查异常的节点进行复测。&lt;code>resume&lt;/code>命令因此被开发，该命令会读取&lt;code>resume.json&lt;/code>文件，并对前一次异常的节点进行再次检查，我们可以重复执行此命令至没有异常的结果后再执行全量检查。&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ netctl resume
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>I0205 16:34:06.147671 &lt;span style="color:#ae81ff">2769373&lt;/span> check.go:61&lt;span style="color:#f92672">]&lt;/span> use config from file!!!!!!
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>I0205 16:34:06.148619 &lt;span style="color:#ae81ff">2769373&lt;/span> floater.go:73&lt;span style="color:#f92672">]&lt;/span> create Clusterlink floater, namespace: kosmos-system
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>I0205 16:34:06.157582 &lt;span style="color:#ae81ff">2769373&lt;/span> floater.go:83&lt;span style="color:#f92672">]&lt;/span> create Clusterlink floater, apply RBAC
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>I0205 16:34:06.167799 &lt;span style="color:#ae81ff">2769373&lt;/span> floater.go:94&lt;span style="color:#f92672">]&lt;/span> create Clusterlink floater, version: v0.2.0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>I0205 16:34:09.178566 &lt;span style="color:#ae81ff">2769373&lt;/span> verify.go:79&lt;span style="color:#f92672">]&lt;/span> pod: clusterlink-floater-9dzsg is ready. status: Running
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>I0205 16:34:09.179593 &lt;span style="color:#ae81ff">2769373&lt;/span> verify.go:79&lt;span style="color:#f92672">]&lt;/span> pod: clusterlink-floater-cscdh is ready. status: Running
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Do check... 100% &lt;span style="color:#f92672">[================================================================================]&lt;/span> &lt;span style="color:#f92672">[&lt;/span>0s&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>+-----+----------------+----------------+-----------+-----------+
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| S/N | SRC NODE NAME | DST NODE NAME | TARGET IP | RESULT |
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>+-----+----------------+----------------+-----------+-----------+
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| &lt;span style="color:#ae81ff">1&lt;/span> | ecs-net-dr-002 | ecs-net-dr-001 | 10.0.1.86 | SUCCESSED |
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| &lt;span style="color:#ae81ff">2&lt;/span> | ecs-net-dr-001 | ecs-net-dr-002 | 10.0.2.29 | SUCCESSED |
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>+-----+----------------+----------------+-----------+-----------+
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>&lt;code>netctl clean&lt;/code>命令用于清理&lt;code>NetDoctor&lt;/code>创建的所有资源。&lt;/li>
&lt;/ul>
&lt;h2 id="贡献代码">
 贡献代码
 &lt;a class="anchor" href="#%e8%b4%a1%e7%8c%ae%e4%bb%a3%e7%a0%81">#&lt;/a>
&lt;/h2>
&lt;ul>
&lt;li>我们欢迎任何形式的帮助，包括但不限定于完善文档、提出问题、修复 Bug 和增加特性。&lt;/li>
&lt;/ul>
&lt;h2 id="联系我们">
 联系我们
 &lt;a class="anchor" href="#%e8%81%94%e7%b3%bb%e6%88%91%e4%bb%ac">#&lt;/a>
&lt;/h2>
&lt;ul>
&lt;li>如果您在使用过程中遇到了任何问题，欢迎提交 &lt;a href="https://github.com/kosmos-io/netdoctor/issues">Issue&lt;/a> 进行反馈。&lt;/li>
&lt;li>您也可以扫描&lt;a href="./docs/img/kosmos-WeChatIMG.png">WeChat&lt;/a>加入技术交流群。&lt;/li>
&lt;/ul></description></item><item><title>2024-08-02 nginx如何解决惊群效应</title><link>https://qq547475331.github.io/docs/nginx%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E6%83%8A%E7%BE%A4%E6%95%88%E5%BA%94-nginx-ru-he-jie-jue-jing-qun-xiao-ying/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/nginx%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E6%83%8A%E7%BE%A4%E6%95%88%E5%BA%94-nginx-ru-he-jie-jue-jing-qun-xiao-ying/</guid><description>&lt;h2 id="前置知识">
 &lt;strong>前置知识&lt;/strong>
 &lt;a class="anchor" href="#%e5%89%8d%e7%bd%ae%e7%9f%a5%e8%af%86">#&lt;/a>
&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>linux 网络处理的基本方法：bind listen accept&lt;/strong>&lt;/li>
&lt;li>&lt;strong>epoll 的基本方法：epoll_create epoll_ctl epoll_wait&lt;/strong>&lt;/li>
&lt;/ul>
&lt;h2 id="什么是惊群效应">
 &lt;strong>什么是惊群效应？&lt;/strong>
 &lt;a class="anchor" href="#%e4%bb%80%e4%b9%88%e6%98%af%e6%83%8a%e7%be%a4%e6%95%88%e5%ba%94">#&lt;/a>
&lt;/h2>
&lt;p>&lt;strong>第一次听到的这个名词的时候觉得很是有趣，不知道是个什么意思，总觉得又是奇怪的中文翻译导致的。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>复杂的说（来源于网络）TLDR;&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>惊群效应（thundering herd）是指多进程（多线程）在同时阻塞等待同一个事件的时候（休眠状态），如果等待的这个事件发生，那么他就会唤醒等待的所有进程（或者线程），但是最终却只能有一个进程（线程）获得这个时间的“控制权”，对该事件进行处理，而其他进程（线程）获取“控制权”失败，只能重新进入休眠状态，这种现象和性能浪费就叫做惊群效应。&lt;/strong>&lt;/p>&lt;/blockquote>
&lt;p>&lt;strong>简单的讲（我的大白话）&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>有一道雷打下来，把很多人都吵醒了，但只有其中一个人去收衣服了。也就是：有一个请求过来了，把很多进程都唤醒了，但只有其中一个能最终处理。&lt;/strong>&lt;/p>&lt;/blockquote>
&lt;h3 id="原因问题">
 &lt;strong>原因&amp;amp;问题&lt;/strong>
 &lt;a class="anchor" href="#%e5%8e%9f%e5%9b%a0%e9%97%ae%e9%a2%98">#&lt;/a>
&lt;/h3>
&lt;p>&lt;strong>说起来其实也简单，多数时候为了提高应用的请求处理能力，会使用多进程（多线程）去监听请求，当请求来时，因为都有能力处理，所以就都被唤醒了。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>而问题就是，最终还是只能有一个进程能来处理。当请求多了，不停地唤醒、休眠、唤醒、休眠，做了很多的无用功，上下文切换又累，对吧。那怎么解决这个问题呢？下面就是今天要看的重点，我们看看 nginx 是如何解决这个问题的。&lt;/strong>&lt;/p>
&lt;h2 id="nginx-架构">
 &lt;strong>Nginx 架构&lt;/strong>
 &lt;a class="anchor" href="#nginx-%e6%9e%b6%e6%9e%84">#&lt;/a>
&lt;/h2>
&lt;p>&lt;strong>第一点我们需要了解 nginx 大致的架构是怎么样的。nginx 将进程分为 master 和 worker 两类，非常常见的一种 M-S 策略，也就是 master 负责统筹管理 worker，当然它也负责如：启动、读取配置文件，监听处理各种信号等工作。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202408011508824.png" alt="image-20240801150852757" />&lt;/strong>&lt;/p>
&lt;p>&lt;strong>但是，第一个要注意的问题就出现了，master 的工作有且只有这些，对于请求来说它是不管的，就如同图中所示，请求是直接被 worker 处理的。如此一来，请求应该被哪个 worker 处理呢？worker 内部又是如何处理请求的呢？&lt;/strong>&lt;/p>
&lt;h2 id="nginx-使用-epoll">
 &lt;strong>Nginx 使用 epoll&lt;/strong>
 &lt;a class="anchor" href="#nginx-%e4%bd%bf%e7%94%a8-epoll">#&lt;/a>
&lt;/h2>
&lt;p>&lt;strong>接下来我们就要知道 nginx 是如何使用 epoll 来处理请求的。下面可能会涉及到一些源码的内容，但不用担心，你不需要全部理解，只需要知道它们的作用就可以了。顺便我会简单描述一下我是如何去找到这些源码的位置的。&lt;/strong>&lt;/p>
&lt;h3 id="master-的工作">
 &lt;strong>Master 的工作&lt;/strong>
 &lt;a class="anchor" href="#master-%e7%9a%84%e5%b7%a5%e4%bd%9c">#&lt;/a>
&lt;/h3>
&lt;p>&lt;strong>其实 Master 并不是毫无作为，至少端口是它来占的。https://github.com/nginx/nginx/blob/b489ba83e9be446923facfe1a2fe392be3095d1f/src/core/ngx_connection.c#L407C13-L407C13&lt;/strong>&lt;/p></description></item><item><title>2024-08-02 pixie</title><link>https://qq547475331.github.io/docs/pixie-pixie/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/pixie-pixie/</guid><description>&lt;p>&lt;strong>Pixie是基于Ebpf技术构建的一套可观测性平台，Pixie默认已经集成了许多可观测性工具，例如：我们可以清晰的通过Pixie观测到K8S内部的流量情况、DNS解析时延、TCP丢包、掉包等，同时我们还可以通过Pixie多K8S内Namespace、Pod等进行监控，当然大名鼎鼎的Cilium（一款K8S的网络插件）也可以做到，但是如果您的K8S集群不是使用到Cilium的话，使用Pixie作为K8S可观测性平台是非常不错的选择。&lt;/strong>&lt;/p>
&lt;pre>&lt;code> Pixie官方主要给出了两种搭建模式，第一种是Community Cloud for Pixie,第二种是通过Self-Hosted模式进行搭建，如果您的K8S集群在公有云，那么选择Community Cloud的模式是一种快捷、简单的方式，在这种模式下用户只需要部署Pixie采集端，而无需单独部署Pixie Cloud(用于管理部署在各个K8S集群的Pixie采集端)，Pixie Cloud由社区来提供，但是这种方式对于我们国内很多系统来说是不适用的，它要求K8S集群能够直连到Pixie社区提供的Cloud上去，这意味着K8S需要与公网打通，这往往不符合安全要求，于使只能采用第二种模式来进行部署，将Pixie Cloud和Pixie采集端部署到私有的集群上面。以下是笔者在实验环境搭建的过程。

 首先我的实验环境主要由三台虚拟机组成，基于Kubernetes 1.23.2版本搭建了一套集群，集群信息如下，这里要注意Pixie要求K8S的版本需要高于1.16，同时集群中主机需要使用Cgroup v1版本，笔者尝试过Cgroup v2，但是Pixie无法正常运行。
&lt;/code>&lt;/pre>
&lt;p>&lt;strong>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202407311709383.png" alt="image-20240731170928295" />&lt;/strong>&lt;/p>
&lt;p>&lt;strong>接下来我们可以按照官网的步骤开始搭建，这里贴出官网地址，大家可以参考：&lt;/strong>&lt;/p>
&lt;pre tabindex="0">&lt;code>https://docs.px.dev/installing-pixie/install-guides/self-hosted-pixie/
&lt;/code>&lt;/pre>&lt;p>&lt;strong>1.第一步：按照官网要求，我们需要首先使用Git将Pixie源码切到本地，这里没有特殊的说明&lt;/strong>&lt;/p>
&lt;pre tabindex="0">&lt;code>git clone https://github.com/pixie-io/pixie.git
cd pixie
&lt;/code>&lt;/pre>&lt;p>&lt;strong>2.第二步：安装mkcert，具体如何安装这里不详细写出来，各位可以参考官方文档&lt;/strong>&lt;/p>
&lt;pre tabindex="0">&lt;code>https://github.com/FiloSottile/mkcert#installation
&lt;/code>&lt;/pre>&lt;p>&lt;strong>这个mkcert是作用是生成本地的CA证书，之后Pixie Cloud和客户端之间需要通过SSL来进行通信，这里没有什么坑，安装即可。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>3.第三步：生成CA证书，这里也没什么好说的，往下走&lt;/strong>&lt;/p>
&lt;pre tabindex="0">&lt;code>mkcert -install
&lt;/code>&lt;/pre>&lt;p>&lt;strong>4.第四步：创建名为plc的命名空间，这个命名空间是用于Pixie Cloud的服务&lt;/strong>&lt;/p>
&lt;pre tabindex="0">&lt;code>kubectl create namespace plc
&lt;/code>&lt;/pre>&lt;p>&lt;strong>5.第五步：创建Pixie Cloud中所需要的secrets资源，包括一些密钥之类的，这一步一定确保已经在命名空间中创建，如果这里缺失了，后面Pixie Cloud的部分服务就会无法启动，创建以后可以通过Kubectl 进行查看下。&lt;/strong>&lt;/p>
&lt;pre tabindex="0">&lt;code>./scripts/create_cloud_secrets.sh
&lt;/code>&lt;/pre>&lt;p>&lt;strong>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202407311710718.png" alt="image-20240731171035646" />&lt;/strong>&lt;/p>
&lt;p>&lt;strong>6.第六步：安装kustomize，这个工具是用于生成安装Pixie Cloud的YAML文件的，这步没有坑，安装方法如下:&lt;/strong>&lt;/p>
&lt;pre tabindex="0">&lt;code>https://kubectl.docs.kubernetes.io/installation/kustomize/
&lt;/code>&lt;/pre>&lt;p>&lt;strong>7.第七步：安装Pixie Cloud的依赖，这一步会安装Pixie Cloud服务依赖的一些中间件之类的，比如es,postgres,nat,stan等。注意这些服务都需要挂在数据卷，在部署之前需要提前把需要的数据卷给创建好，因为笔者这里使用的是本地环境，于使创建的是宿主机数据卷，服务在启动的时候会自动去Bound数据卷，数据卷的大小需要查看依赖服务的PVC的要求来创建。&lt;/strong>&lt;/p>
&lt;pre tabindex="0">&lt;code>kustomize build k8s/cloud_deps/public/ | kubectl apply -f - --namespace=plc

kustomize build k8s/cloud_deps/public/ | kubectl delete -f - --namespace=plc
&lt;/code>&lt;/pre>&lt;p>&lt;strong>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202407311711916.png" alt="image-20240731171120820" />&lt;/strong>&lt;/p>
&lt;p>&lt;strong>8.第八步：安装Pixie Cloud，这里就是等待Pull镜像和安装成功，可以通过查看plc命名空间下容器状态来观察：&lt;/strong>&lt;/p>
&lt;pre tabindex="0">&lt;code> kustomize build k8s/cloud/public/ | kubectl apply -f - --namespace=plc
 
 kustomize build k8s/cloud/public/ | kubectl delete -f - --namespace=plc
&lt;/code>&lt;/pre>&lt;p>&lt;strong>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202407311711213.png" alt="image-20240731171139155" />&lt;/strong>&lt;/p></description></item><item><title>2024-08-02 prometheus-stack</title><link>https://qq547475331.github.io/docs/prometheus-stack-prometheus-stack/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/prometheus-stack-prometheus-stack/</guid><description>&lt;pre tabindex="0">&lt;code>image:
 registry: quay.io
 repository: thanos/thanos
 tag: v0.35.1
 sha: &amp;#34;&amp;#34;


quay.io/thanos/thanos:v0.35.1



#原镜像
quay.io/thanos/thanos:v0.35.1

#转换后镜像
anjia0532/quay.thanos.thanos:v0.35.1


#下载并重命名镜像
docker pull anjia0532/quay.thanos.thanos:v0.35.1 

docker tag anjia0532/quay.thanos.thanos:v0.35.1 quay.io/thanos/thanos:v0.35.1

docker images | grep $(echo quay.io/thanos/thanos:v0.35.1 |awk -F&amp;#39;:&amp;#39; &amp;#39;{print $1}&amp;#39;)


docker pull cloudsx.top/anjia0532/quay.thanos.thanos:v0.35.1


docker tag cloudsx.top/anjia0532/quay.thanos.thanos:v0.35.1 192.168.0.140:881/thanos/thanos:v0.35.1
&lt;/code>&lt;/pre>&lt;pre tabindex="0">&lt;code>image:
 registry: quay.io
 repository: prometheus/alertmanager
 tag: v0.27.0
 sha: &amp;#34;&amp;#34;
 
 quay.io/prometheus/alertmanager:v0.27.0 
 
 
#原镜像
quay.io/prometheus/alertmanager:v0.27.0

#转换后镜像
anjia0532/quay.prometheus.alertmanager:v0.27.0


#下载并重命名镜像
docker pull anjia0532/quay.prometheus.alertmanager:v0.27.0 

docker tag anjia0532/quay.prometheus.alertmanager:v0.27.0 quay.io/prometheus/alertmanager:v0.27.0

docker images | grep $(echo quay.io/prometheus/alertmanager:v0.27.0 |awk -F&amp;#39;:&amp;#39; &amp;#39;{print $1}&amp;#39;)



docker pull cloudsx.top/anjia0532/quay.prometheus.alertmanager:v0.27.0 

docker tag cloudsx.top/anjia0532/quay.prometheus.alertmanager:v0.27.0 192.168.0.140:881/prometheus/alertmanager:v0.27.0

docker push 192.168.0.140:881/prometheus/alertmanager:v0.27.0
&lt;/code>&lt;/pre>&lt;pre tabindex="0">&lt;code>image:
 registry: quay.io
 repository: prometheus-operator/admission-webhook
 # if not set appVersion field from Chart.yaml is used
 tag: &amp;#34;&amp;#34;
 sha: &amp;#34;&amp;#34;
 pullPolicy: IfNotPresent
 
quay.io/prometheus-operator/admission-webhook:v0.75.1 



#原镜像
quay.io/prometheus-operator/admission-webhook:v0.75.1

#转换后镜像
anjia0532/quay.prometheus-operator.admission-webhook:v0.75.1


#下载并重命名镜像
docker pull anjia0532/quay.prometheus-operator.admission-webhook:v0.75.1 

docker tag anjia0532/quay.prometheus-operator.admission-webhook:v0.75.1 quay.io/prometheus-operator/admission-webhook:v0.75.1

docker images | grep $(echo quay.io/prometheus-operator/admission-webhook:v0.75.1 |awk -F&amp;#39;:&amp;#39; &amp;#39;{print $1}&amp;#39;)


docker pull cloudsx.top/anjia0532/quay.prometheus-operator.admission-webhook:v0.75.1 

docker tag cloudsx.top/anjia0532/quay.prometheus-operator.admission-webhook:v0.75.1 192.168.0.140:881/prometheus-operator/admission-webhook:v0.75.1

docker push 192.168.0.140:881/prometheus-operator/admission-webhook:v0.75.1
&lt;/code>&lt;/pre>&lt;pre tabindex="0">&lt;code>image:
 registry: registry.k8s.io
 repository: ingress-nginx/kube-webhook-certgen
 tag: v20221220-controller-v1.5.1-58-g787ea74b6
 sha: &amp;#34;&amp;#34;

registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20221220-controller-v1.5.1-58-g787ea74b6




#原镜像
registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20221220-controller-v1.5.1-58-g787ea74b6

#转换后镜像
anjia0532/google-containers.ingress-nginx.kube-webhook-certgen:v20221220-controller-v1.5.1-58-g787ea74b6


#下载并重命名镜像
docker pull anjia0532/google-containers.ingress-nginx.kube-webhook-certgen:v20221220-controller-v1.5.1-58-g787ea74b6 

docker tag anjia0532/google-containers.ingress-nginx.kube-webhook-certgen:v20221220-controller-v1.5.1-58-g787ea74b6 registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20221220-controller-v1.5.1-58-g787ea74b6

docker images | grep $(echo registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20221220-controller-v1.5.1-58-g787ea74b6 |awk -F&amp;#39;:&amp;#39; &amp;#39;{print $1}&amp;#39;)



docker pull cloudsx.top/anjia0532/google-containers.ingress-nginx.kube-webhook-certgen:v20221220-controller-v1.5.1-58-g787ea74b6


docker tag cloudsx.top/anjia0532/google-containers.ingress-nginx.kube-webhook-certgen:v20221220-controller-v1.5.1-58-g787ea74b6 192.168.0.140:881/ingress-nginx/kube-webhook-certgen:v20221220-controller-v1.5.1-58-g787ea74b6

docker push 192.168.0.140:881/ingress-nginx/kube-webhook-certgen:v20221220-controller-v1.5.1-58-g787ea74b6
&lt;/code>&lt;/pre>&lt;pre tabindex="0">&lt;code>image:
 registry: quay.io
 repository: prometheus-operator/prometheus-operator
 # if not set appVersion field from Chart.yaml is used
 tag: &amp;#34;&amp;#34;
 sha: &amp;#34;&amp;#34;
 pullPolicy: IfNotPresent
 
quay.io/prometheus-operator/prometheus-operator:v0.75.1 


#原镜像
quay.io/prometheus-operator/prometheus-operator:v0.75.1

#转换后镜像
anjia0532/quay.prometheus-operator.prometheus-operator:v0.75.1


#下载并重命名镜像
docker pull anjia0532/quay.prometheus-operator.prometheus-operator:v0.75.1 

docker tag anjia0532/quay.prometheus-operator.prometheus-operator:v0.75.1 quay.io/prometheus-operator/prometheus-operator:v0.75.1

docker images | grep $(echo quay.io/prometheus-operator/prometheus-operator:v0.75.1 |awk -F&amp;#39;:&amp;#39; &amp;#39;{print $1}&amp;#39;)


docker pull cloudsx.top/anjia0532/quay.prometheus-operator.prometheus-operator:v0.75.1 


docker tag cloudsx.top/anjia0532/quay.prometheus-operator.prometheus-operator:v0.75.1 192.168.0.140:881/prometheus-operator/prometheus-operator:v0.75.1

docker push 192.168.0.140:881/prometheus-operator/prometheus-operator:v0.75.1
&lt;/code>&lt;/pre>&lt;pre tabindex="0">&lt;code>image:
 registry: quay.io
 repository: prometheus-operator/prometheus-config-reloader
 # if not set appVersion field from Chart.yaml is used
 tag: &amp;#34;&amp;#34;
 sha: &amp;#34;&amp;#34;
 
quay.io/prometheus-operator/prometheus-config-reloader:v0.75.1 




#原镜像
quay.io/prometheus-operator/prometheus-config-reloader:v0.75.1

#转换后镜像
anjia0532/quay.prometheus-operator.prometheus-config-reloader:v0.75.1


#下载并重命名镜像
docker pull anjia0532/quay.prometheus-operator.prometheus-config-reloader:v0.75.1 

docker tag anjia0532/quay.prometheus-operator.prometheus-config-reloader:v0.75.1 quay.io/prometheus-operator/prometheus-config-reloader:v0.75.1

docker images | grep $(echo quay.io/prometheus-operator/prometheus-config-reloader:v0.75.1 |awk -F&amp;#39;:&amp;#39; &amp;#39;{print $1}&amp;#39;)


docker pull cloudsx.top/anjia0532/quay.prometheus-operator.prometheus-config-reloader:v0.75.1 


docker tag cloudsx.top/anjia0532/quay.prometheus-operator.prometheus-config-reloader:v0.75.1 192.168.0.140:881/prometheus-operator/prometheus-config-reloader:v0.75.1

docker push 192.168.0.140:881/prometheus-operator/prometheus-config-reloader:v0.75.1
&lt;/code>&lt;/pre>&lt;pre tabindex="0">&lt;code>image:
 registry: quay.io
 repository: prometheus/prometheus
 tag: v2.53.1
 sha: &amp;#34;&amp;#34;

quay.io/prometheus/prometheus:v2.53.1



#原镜像
quay.io/prometheus/prometheus:v2.53.1

#转换后镜像
anjia0532/quay.prometheus.prometheus:v2.53.1


#下载并重命名镜像
docker pull anjia0532/quay.prometheus.prometheus:v2.53.1 

docker tag anjia0532/quay.prometheus.prometheus:v2.53.1 quay.io/prometheus/prometheus:v2.53.1

docker images | grep $(echo quay.io/prometheus/prometheus:v2.53.1 |awk -F&amp;#39;:&amp;#39; &amp;#39;{print $1}&amp;#39;)


docker pull cloudsx.top/anjia0532/quay.prometheus.prometheus:v2.53.1

docker tag cloudsx.top/anjia0532/quay.prometheus.prometheus:v2.53.1 192.168.0.140:881/prometheus/prometheus:v2.53.1
&lt;/code>&lt;/pre>&lt;pre tabindex="0">&lt;code>grafana
adminPassword: prom-operator
&lt;/code>&lt;/pre>&lt;h3 id="安装prometheus">
 安装prometheus
 &lt;a class="anchor" href="#%e5%ae%89%e8%a3%85prometheus">#&lt;/a>
&lt;/h3>
&lt;pre tabindex="0">&lt;code>helm install promethues -f my-value.yaml ./ 


helm install grafana ./ 
&lt;/code>&lt;/pre>&lt;pre tabindex="0">&lt;code>#原镜像
quay.io/prometheus/node-exporter:v1.8.1

#转换后镜像
anjia0532/quay.prometheus.node-exporter:v1.8.1


#下载并重命名镜像
docker pull anjia0532/quay.prometheus.node-exporter:v1.8.1 

docker tag anjia0532/quay.prometheus.node-exporter:v1.8.1 quay.io/prometheus/node-exporter:v1.8.1

docker images | grep $(echo quay.io/prometheus/node-exporter:v1.8.1 |awk -F&amp;#39;:&amp;#39; &amp;#39;{print $1}&amp;#39;)


docker pull cloudsx.top/anjia0532/quay.prometheus.node-exporter:v1.8.1 

docker tag cloudsx.top/anjia0532/quay.prometheus.node-exporter:v1.8.1 192.168.0.140:881/prometheus/node-exporter:v1.8.1

docker push 192.168.0.140:881/prometheus/node-exporter:v1.8.1
&lt;/code>&lt;/pre>&lt;pre tabindex="0">&lt;code>#原镜像
quay.io/kiwigrid/k8s-sidecar:1.26.1

#转换后镜像
anjia0532/quay.kiwigrid.k8s-sidecar:1.26.1


#下载并重命名镜像
docker pull anjia0532/quay.kiwigrid.k8s-sidecar:1.26.1 

docker tag anjia0532/quay.kiwigrid.k8s-sidecar:1.26.1 quay.io/kiwigrid/k8s-sidecar:1.26.1

docker images | grep $(echo quay.io/kiwigrid/k8s-sidecar:1.26.1 |awk -F&amp;#39;:&amp;#39; &amp;#39;{print $1}&amp;#39;)


docker pull cloudsx.top/anjia0532/quay.kiwigrid.k8s-sidecar:1.26.1 
docker tag cloudsx.top/anjia0532/quay.kiwigrid.k8s-sidecar:1.26.1 192.168.0.140:881/kiwigrid/k8s-sidecar:1.26.1

docker push 192.168.0.140:881/kiwigrid/k8s-sidecar:1.26.1
&lt;/code>&lt;/pre>&lt;pre tabindex="0">&lt;code>#原镜像
registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.12.0

#转换后镜像
anjia0532/google-containers.kube-state-metrics.kube-state-metrics:v2.12.0


#下载并重命名镜像
docker pull anjia0532/google-containers.kube-state-metrics.kube-state-metrics:v2.12.0 

docker tag anjia0532/google-containers.kube-state-metrics.kube-state-metrics:v2.12.0 registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.12.0

docker images | grep $(echo registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.12.0 |awk -F&amp;#39;:&amp;#39; &amp;#39;{print $1}&amp;#39;)


docker pull cloudsx.top/anjia0532/google-containers.kube-state-metrics.kube-state-metrics:v2.12.0 

docker tag cloudsx.top/anjia0532/google-containers.kube-state-metrics.kube-state-metrics:v2.12.0 192.168.0.140:881/kube-state-metrics/kube-state-metrics:v2.12.0

docker push 192.168.0.140:881/kube-state-metrics/kube-state-metrics:v2.12.0
&lt;/code>&lt;/pre>&lt;pre tabindex="0">&lt;code> image:
 # -- The Docker registry
 registry: docker.io
 repository: library/busybox
 tag: &amp;#34;1.31.1&amp;#34;
 sha: &amp;#34;&amp;#34;
 pullPolicy: IfNotPresent
 
 
 
 docker.io/library/busybox:1.31.1 
&lt;/code>&lt;/pre>&lt;pre tabindex="0">&lt;code>sidecar:
 image:
 # -- The Docker registry
 registry: quay.io
 repository: kiwigrid/k8s-sidecar
 tag: 1.26.1
 sha: &amp;#34;&amp;#34;
 imagePullPolicy: IfNotPresent
 resources: {}

quay.io/kiwigrid/k8s-sidecar:1.26.1


#原镜像
quay.io/kiwigrid/k8s-sidecar:1.26.1

#转换后镜像
anjia0532/quay.kiwigrid.k8s-sidecar:1.26.1


#下载并重命名镜像
docker pull anjia0532/quay.kiwigrid.k8s-sidecar:1.26.1 

docker tag anjia0532/quay.kiwigrid.k8s-sidecar:1.26.1 quay.io/kiwigrid/k8s-sidecar:1.26.1

docker images | grep $(echo quay.io/kiwigrid/k8s-sidecar:1.26.1 |awk -F&amp;#39;:&amp;#39; &amp;#39;{print $1}&amp;#39;)
&lt;/code>&lt;/pre>&lt;pre tabindex="0">&lt;code> image:
 # -- The Docker registry
 registry: docker.io
 # image-renderer Image repository
 repository: grafana/grafana-image-renderer
 # image-renderer Image tag
 tag: latest
 # image-renderer Image sha (optional)
 sha: &amp;#34;&amp;#34;
 # image-renderer ImagePullPolicy
 pullPolicy: Always
 # extra environment variables

docker.io/grafana/grafana-image-renderer:latest
&lt;/code>&lt;/pre>&lt;pre tabindex="0">&lt;code>image:
 # -- The Docker registry
 registry: docker.io
 # -- Docker image repository
 repository: grafana/grafana
 # Overrides the Grafana image tag whose default is the chart appVersion
 tag: &amp;#34;&amp;#34;
 sha: &amp;#34;&amp;#34;
 pullPolicy: IfNotPresent

docker.io/grafana/grafana:11.1.0
&lt;/code>&lt;/pre>&lt;pre tabindex="0">&lt;code> registry: docker.io
 repository: curlimages/curl
 tag: 7.85.0
 sha: &amp;#34;&amp;#34;
 pullPolicy: IfNotPresent

docker.io/curlimages/curl:7.85.0
&lt;/code>&lt;/pre>&lt;pre tabindex="0">&lt;code> image:
 # -- The Docker registry
 registry: docker.io
 repository: bats/bats
 tag: &amp;#34;v1.4.1&amp;#34;
 imagePullPolicy: IfNotPresent
 
 
docker.io/bats/bats:v1.4.1 
&lt;/code>&lt;/pre>&lt;pre tabindex="0">&lt;code> image:
 # -- The Docker registry
 registry: docker.io
 # image-renderer Image repository
 repository: grafana/grafana-image-renderer
 # image-renderer Image tag
 tag: latest
 # image-renderer Image sha (optional)
 sha: &amp;#34;&amp;#34;
 # image-renderer ImagePullPolicy
 pullPolicy: Always

docker.io/grafana/grafana-image-renderer:latest
&lt;/code>&lt;/pre>&lt;pre tabindex="0">&lt;code>docker.io/grafana/grafana:11.1.0
docker.io/library/busybox:1.31.1


docker pull cloudsx.top/grafana/grafana:11.1.0

docker pull cloudsx.top/library/busybox:1.31.1

docker tag cloudsx.top/grafana/grafana:11.1.0 192.168.0.140:881/grafana/grafana:11.1.0

docker tag cloudsx.top/library/busybox:1.31.1 192.168.0.140:881/library/busybox:1.31.1
&lt;/code>&lt;/pre></description></item><item><title>2024-08-02 TEG与istio集成</title><link>https://qq547475331.github.io/docs/teg%E4%B8%8Eistio%E9%9B%86%E6%88%90-teg-yu-istio-ji-cheng/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/teg%E4%B8%8Eistio%E9%9B%86%E6%88%90-teg-yu-istio-ji-cheng/</guid><description>&lt;p>介绍 Tetrate Enterprise Gateway 及与 Istio 集成：云原生应用的全面网关解决方案&lt;/p>
&lt;p>深入了解 Tetrate Enterprise Gateway (TEG) 及其如何与 Istio 服务网格集成 —— 一种基于 Envoy 的企业级网关解决方案，包括它的架构、基本功能以及如何在 Kubernetes 中使用 TEG 来暴露和管理应用。&lt;/p>
&lt;h2 id="teg-简介">
 TEG 简介
 &lt;a class="anchor" href="#teg-%e7%ae%80%e4%bb%8b">#&lt;/a>
&lt;/h2>
&lt;p>Tetrate Enterprise Gateway（TEG）是基于 &lt;a href="https://gateway.envoyproxy.io/">Envoy Gateway&lt;/a> (EG) 的企业级解决方案，专门针对 Envoy Proxy 设计，通过 &lt;a href="https://gateway-api.sigs.k8s.io/">Kubernetes Gateway API&lt;/a> 提供更易于消费的 Envoy 代理配置和管理包。TEG 结合了 Kubernetes Gateway API 的特性，支持在 Kubernetes 中轻松暴露服务和应用程序。&lt;/p>
&lt;p>TEG 相对于 Envoy Gateway 的主要新增特性包括：&lt;/p>
&lt;ol>
&lt;li>&lt;strong>全局速率限制（Rate Limiting）&lt;/strong>：TEG 支持基于 IP 5-tuple、请求头等进行流量控制，需要通过 Redis 实例管理。&lt;/li>
&lt;li>&lt;strong>WAF 功能（Web Application Firewall）&lt;/strong>：TEG 提供了与 &lt;code>mod_security&lt;/code> 兼容的 WAF 功能，增强了安全防护能力。&lt;/li>
&lt;li>&lt;strong>OIDC/OAuth2认证&lt;/strong>：支持在网关级别进行 OIDC/OAuth2 认证，应用程序可以按路由配置认证方式。&lt;/li>
&lt;li>&lt;strong>使用 Kubernetes Gateway API&lt;/strong>：相较于其他 API，Kubernetes Gateway API 的设计更加现代，结合了众多 Ingress 实现的经验，将网关的配置与流量的路由分离，使平台所有者可以管理网关，而应用团队可以掌控流量路由。&lt;/li>
&lt;/ol>
&lt;p>TEG 将 Envoy 的高级网络流量处理能力带入 Kubernetes 环境，提供了一种简化的方法来部署和管理负载平衡、API 网关功能、安全控制等，同时支持现代的、开放的应用程序暴露 API，如 Kubernetes Gateway API。这些特性使 TEG 成为一个功能丰富、易于管理的企业级网关解决方案。&lt;/p></description></item><item><title>2024-08-02 史上最牛jenkins pipeline流水线详解</title><link>https://qq547475331.github.io/docs/%E5%8F%B2%E4%B8%8A%E6%9C%80%E7%89%9Bjenkins-pipeline%E6%B5%81%E6%B0%B4%E7%BA%BF%E8%AF%A6%E8%A7%A3-shi-shang-zui-niu-jenkinspipeline-liu-shui-xian-xiang-jie/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/%E5%8F%B2%E4%B8%8A%E6%9C%80%E7%89%9Bjenkins-pipeline%E6%B5%81%E6%B0%B4%E7%BA%BF%E8%AF%A6%E8%A7%A3-shi-shang-zui-niu-jenkinspipeline-liu-shui-xian-xiang-jie/</guid><description>&lt;h1 id="一什么是流水线">
 一、什么是流水线
 &lt;a class="anchor" href="#%e4%b8%80%e4%bb%80%e4%b9%88%e6%98%af%e6%b5%81%e6%b0%b4%e7%ba%bf">#&lt;/a>
&lt;/h1>
&lt;p>jenkins 有 2 种流水线分为声明式流水线与脚本化流水线，脚本化流水线是 jenkins 旧版本使用的流水线脚本，新版本 Jenkins 推荐使用声明式流水线。文档只介绍声明流水线。&lt;/p>
&lt;h2 id="11-声明式流水线">
 1.1 声明式流水线
 &lt;a class="anchor" href="#11-%e5%a3%b0%e6%98%8e%e5%bc%8f%e6%b5%81%e6%b0%b4%e7%ba%bf">#&lt;/a>
&lt;/h2>
&lt;p>在声明式流水线语法中，流水线过程定义在 Pipeline{}中，Pipeline 块定义了整个流水线中完成的所有工作，比如&lt;/p>
&lt;p>&lt;strong>参数说明&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>agent any：在任何可用的代理上执行流水线或它的任何阶段，也就是执行流水线过程的位置，也可以指定到具体的节点&lt;/li>
&lt;li>stage：定义流水线的执行过程（相当于一个阶段），比如下文所示的 Build、Test、Deploy， 但是这个名字是根据实际情况进行定义的，并非固定的名字&lt;/li>
&lt;li>steps：执行某阶段具体的步骤。&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>//Jenkinsfile (Declarative Pipeline)
pipeline {
 agent any
 stages {
 stage(&amp;#39;Build&amp;#39;) {
 steps {
 echo &amp;#39;Build&amp;#39;
 }
 }
 stage(&amp;#39;Test&amp;#39;) {
 steps {
 echo &amp;#39;Test&amp;#39;
 }
 }
 stage(&amp;#39;Deploy&amp;#39;) {
 steps {
 echo &amp;#39;Deploy&amp;#39;
 }
 }
 }
}
&lt;/code>&lt;/pre>&lt;h2 id="12-脚本化流水线">
 1.2 脚本化流水线
 &lt;a class="anchor" href="#12-%e8%84%9a%e6%9c%ac%e5%8c%96%e6%b5%81%e6%b0%b4%e7%ba%bf">#&lt;/a>
&lt;/h2>
&lt;p>在脚本化流水线语法中，会有一个或多个 Node（节点）块在整个流水线中执行核心工作&lt;/p>
&lt;p>&lt;strong>参数说明&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>node：在任何可用的代理上执行流水线或它的任何阶段，也可以指定到具体的节点&lt;/li>
&lt;li>stage：和声明式的含义一致，定义流水线的阶段。Stage 块在脚本化流水线语法中是可选的，然而在脚本化流水线中实现 stage 块，可以清楚地在 Jenkins UI 界面中显示每个 stage 的任务子集。&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>//Jenkinsfile (Scripted Pipeline)
node {
 stage(&amp;#39;Build&amp;#39;) {
 echo &amp;#39;Build&amp;#39;
 }
 stage(&amp;#39;Test&amp;#39;) {
 echo &amp;#39;Test&amp;#39;
 }
 stage(&amp;#39;Deploy&amp;#39;) {
 echo &amp;#39;Deploy&amp;#39;
 }
}
&lt;/code>&lt;/pre>&lt;h1 id="二声明式流水线">
 二、声明式流水线
 &lt;a class="anchor" href="#%e4%ba%8c%e5%a3%b0%e6%98%8e%e5%bc%8f%e6%b5%81%e6%b0%b4%e7%ba%bf">#&lt;/a>
&lt;/h1>
&lt;p>声明式流水线必须包含在一个 Pipeline 块中，比如是一个 Pipeline 块的格式&lt;/p></description></item><item><title>2024-08-02 大厂总结nginx高并发优化笔记</title><link>https://qq547475331.github.io/docs/%E5%A4%A7%E5%8E%82%E6%80%BB%E7%BB%93nginx%E9%AB%98%E5%B9%B6%E5%8F%91%E4%BC%98%E5%8C%96%E7%AC%94%E8%AE%B0-da-chang-zong-jie-nginx-gao-bing-fa-you-hua-bi-ji/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/%E5%A4%A7%E5%8E%82%E6%80%BB%E7%BB%93nginx%E9%AB%98%E5%B9%B6%E5%8F%91%E4%BC%98%E5%8C%96%E7%AC%94%E8%AE%B0-da-chang-zong-jie-nginx-gao-bing-fa-you-hua-bi-ji/</guid><description>&lt;p>在日常的运维工作中，经常会用到nginx服务，也时常会碰到nginx因高并发导致的性能瓶颈问题。这里简单梳理下nginx性能优化的配置。&lt;/p>
&lt;p>&lt;strong>一、Nginx配置中比较重要的优化项&lt;/strong>&lt;/p>
&lt;p>1）nginx进程数，建议按照cpu数目来指定，一般跟cpu核数相同或为它的倍数。
worker_processes 8;
2）为每个进程分配cpu，上例中将8个进程分配到8个cpu，当然可以写多个，或者将一个进程分配到多个cpu。
worker_cpu_affinity 00000001 00000010 00000100 00001000 00010000 00100000 01000000 10000000;
3）下面这个指令是指当一个nginx进程打开的最多文件描述符数目，理论值应该是系统的最多打开文件数（ulimit -n）与nginx进程数相除，但是nginx分配请求并不是那么均匀，所以最好与ulimit -n的值保持一致。
worker_rlimit_nofile 65535;
4）使用epoll的I/O模型，用这个模型来高效处理异步事件
use epoll;
5）每个进程允许的最多连接数，理论上每台nginx服务器的最大连接数为worker_processes*worker_connections。
worker_connections 65535;
6）http连接超时时间，默认是60s，功能是使客户端到服务器端的连接在设定的时间内持续有效，当出现对服务器的后继请求时，该功能避免了建立或者重新建立连接。切记这个参数也不能设置过大！否则会导致许多无效的http连接占据着nginx的连接数，终nginx崩溃！
keepalive_timeout 60;
7）客户端请求头部的缓冲区大小，这个可以根据你的系统分页大小来设置，一般一个请求的头部大小不会超过1k，不过由于一般系统分页都要大于1k，所以这里设置为分页大小。分页大小可以用命令getconf PAGESIZE取得。
client_header_buffer_size 4k;
8）下面这个参数将为打开文件指定缓存，默认是没有启用的，max指定缓存数量，建议和打开文件数一致，inactive是指经过多长时间文件没被请求后删除缓存。
open_file_cache max=102400 inactive=20s;
9）下面这个是指多长时间检查一次缓存的有效信息。
open_file_cache_valid 30s;
10）open_file_cache指令中的inactive参数时间内文件的最少使用次数，如果超过这个数字，文件描述符一直是在缓存中打开的，如上例，如果有一个文件在inactive时间内一次没被使用，它将被移除。
open_file_cache_min_uses 1;&lt;/p>
&lt;p>11）隐藏响应头中的有关操作系统和web server（Nginx）版本号的信息，这样对于安全性是有好处的。
server_tokens off;
12）可以让sendfile()发挥作用。sendfile()可以在磁盘和TCP socket之间互相拷贝数据(或任意两个文件描述符)。Pre-sendfile是传送数据之前在用户空间申请数据缓冲区。之后用read()将数据从文件拷贝到这个缓冲区，write()将缓冲区数据写入网络。sendfile()是立即将数据从磁盘读到OS缓存。因为这种拷贝是在内核完成的，sendfile()要比组合read()和write()以及打开关闭丢弃缓冲更加有效(更多有关于sendfile)。
sendfile on;
13）告诉nginx在一个数据包里发送所有头文件，而不一个接一个的发送。就是说数据包不会马上传送出去，等到数据包最大时，一次性的传输出去，这样有助于解决网络堵塞。
tcp_nopush on;
14）告诉nginx不要缓存数据，而是一段一段的发送&amp;ndash;当需要及时发送数据时，就应该给应用设置这个属性，这样发送一小块数据信息时就不能立即得到返回值。
tcp_nodelay on;
比如：
http {
server_tokens off;
sendfile on;
tcp_nopush on;
tcp_nodelay on;
&amp;hellip;&amp;hellip;
}
15）客户端请求头部的缓冲区大小，这个可以根据系统分页大小来设置，一般一个请求头的大小不会超过1k，不过由于一般系统分页都要大于1k，所以这里设置为分页大小。
client_header_buffer_size 4k;
客户端请求头部的缓冲区大小，这个可以根据系统分页大小来设置，一般一个请求头的大小不会超过1k，不过由于一般系统分页都要大于1k，所以这里设置为分页大小。
分页大小可以用命令getconf PAGESIZE取得。
[root@test-huanqiu ~]# getconf PAGESIZE
4096
但也有client_header_buffer_size超过4k的情况，但是client_header_buffer_size该值必须设置为“系统分页大小”的整倍数。
16）为打开文件指定缓存，默认是没有启用的，max 指定缓存数量，建议和打开文件数一致，inactive 是指经过多长时间文件没被请求后删除缓存。
open_file_cache max=65535 inactive=60s;
17）open_file_cache 指令中的inactive 参数时间内文件的最少使用次数，如果超过这个数字，文件描述符一直是在缓存中打开的，如上例，如果有一个文件在inactive 时间内一次没被使用，它将被移除。
open_file_cache_min_uses 1;
18）指定多长时间检查一次缓存的有效信息。
open_file_cache_valid 80s;&lt;/p></description></item><item><title>2024-08-02 常见linux运维面试题</title><link>https://qq547475331.github.io/docs/%E5%B8%B8%E8%A7%81linux%E8%BF%90%E7%BB%B4%E9%9D%A2%E8%AF%95%E9%A2%98-chang-jian-linux-yun-wei-mian-shi-ti/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/%E5%B8%B8%E8%A7%81linux%E8%BF%90%E7%BB%B4%E9%9D%A2%E8%AF%95%E9%A2%98-chang-jian-linux-yun-wei-mian-shi-ti/</guid><description>&lt;p>下面是一名运维人员求职数十家公司总结的 Linux运维面试题，给大家参考下~&lt;/p>
&lt;p>&lt;strong>1、现在给你三百台服务器，你怎么对他们进行管理？&lt;/strong>&lt;/p>
&lt;p>管理3百台服务器的方式：
1）设定跳板机，使用统一账号登录，便于安全与登录的考量。
2）使用 salt、ansiable、puppet 进行系统的统一调度与配置的统一管理。
3）建立简单的服务器的系统、配置、应用的 cmdb 信息管理。便于查阅每台服务器上的各种信息记录。&lt;/p>
&lt;p>&lt;strong>2、简述 raid0 raid1 raid5 三种工作模式的工作原理及特点&lt;/strong>&lt;/p>
&lt;p>RAID，可以把硬盘整合成一个大磁盘，还可以在大磁盘上再分区，放数据
还有一个大功能，多块盘放在一起可以有冗余（备份）
RAID整合方式有很多，常用的：0 1 5 10&lt;/p>
&lt;p>&lt;strong>RAID 0，可以是一块盘和 N 个盘组合&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>优点&lt;/strong>读写快，是 RAID 中最好的&lt;/li>
&lt;li>&lt;strong>缺点&lt;/strong>：没有冗余，一块坏了数据就全没有了&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>RAID 1，只能2块盘，盘的大小可以不一样，以小的为准。&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>10G+10G只有10G，另一个做备份。它有100%的冗余，&lt;/li>
&lt;li>缺点：浪费资源，成本高&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>RAID 5 ，3块盘，容量计算10 *（n-1），损失一块盘&lt;/strong>&lt;/p>
&lt;p>特点，读写性能一般，读还好一点，写不好&lt;/p>
&lt;ul>
&lt;li>冗余从好到坏：RAID1 RAID10 RAID 5 RAID0&lt;/li>
&lt;li>性能从好到坏：RAID0 RAID10 RAID5 RAID1&lt;/li>
&lt;li>成本从低到高：RAID0 RAID5 RAID1 RAID10&lt;/li>
&lt;/ul>
&lt;p>单台服务器：很重要盘不多，系统盘，RAID1
数据库服务器：主库：RAID10 从库 RAID5\RAID0（为了维护成本，RAID10）
WEB服务器，如果没有太多的数据的话，RAID5,RAID0（单盘）
有多台，监控、应用服务器，RAID0 RAID5&lt;/p>
&lt;p>我们会根据数据的存储和访问的需求，去匹配对应的RAID级别&lt;/p>
&lt;p>&lt;strong>3、LVS、Nginx、HAproxy 有什么区别？工作中你怎么选择？&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>LVS&lt;/strong>：是基于四层的转发&lt;/li>
&lt;li>&lt;strong>HAproxy&lt;/strong>：是基于四层和七层的转发，是专业的代理服务器&lt;/li>
&lt;li>&lt;strong>Nginx&lt;/strong>：是WEB服务器，缓存服务器，又是反向代理服务器，可以做七层的转发&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>区别：LVS由于是基于四层的转发所以只能做端口的转发，而基于URL的、基于目录的这种转发LVS就做不了。&lt;/p>&lt;/blockquote>
&lt;p>工作选择：&lt;/p>
&lt;ul>
&lt;li>HAproxy 和 Nginx 由于可以做七层的转发，所以 URL 和目录的转发都可以做&lt;/li>
&lt;li>在很大并发量的时候我们就要选择LVS，像中小型公司的话并发量没那么大&lt;/li>
&lt;li>选择 HAproxy 或者 Nginx 足已，由于 HAproxy 由是专业的代理服务器&lt;/li>
&lt;li>配置简单，所以中小型企业推荐使用 HAproxy&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>4、Squid、Varinsh 和 Nginx 有什么区别，工作中你怎么选择？&lt;/strong>&lt;/p></description></item><item><title>2024-08-02 我只想做技术 走技术路线</title><link>https://qq547475331.github.io/docs/%E6%88%91%E5%8F%AA%E6%83%B3%E5%81%9A%E6%8A%80%E6%9C%AF-%E8%B5%B0%E6%8A%80%E6%9C%AF%E8%B7%AF%E7%BA%BF-wo-zhi-xiang-zuo-ji-shu-zou-ji-shu-lu-xian/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/%E6%88%91%E5%8F%AA%E6%83%B3%E5%81%9A%E6%8A%80%E6%9C%AF-%E8%B5%B0%E6%8A%80%E6%9C%AF%E8%B7%AF%E7%BA%BF-wo-zhi-xiang-zuo-ji-shu-zou-ji-shu-lu-xian/</guid><description>&lt;p>&lt;strong>“我对技术感兴趣，我只想做技术，走技术路线。”&lt;/strong>&lt;/p>
&lt;p>&lt;strong>这句话是不是很听过呢？请不要误会，这句话本身没有问题，但是说出这句话的软件工程师中，十有八九有一个误区，就是高估了技术本身在个人职业生涯中，起作用的占比。而我，曾经是其中之一。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>我多次听到来自家长这样的评论，说是他/她的孩子不善言辞，缺乏沟通技巧，然而罗辑思维缜密，对于软件技术充满热情，想做一个出色的程序员。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>我觉得，这样的言论，既对又不对。对的是，如果刚踏入职场，相当一个优秀的程序员，我觉得上述的优缺点，都很 “契合”，或者说，也许只需要学好技术，把确定的任务做完、做好，就能找得到工作的褒奖，就能胜任岗位。但是，随着你继续在职业生涯的道路上向上突破，基本坚定地走着技术通道（譬如我），上述能力的缺失，而造成的 “短板效应”，会越来越明显。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>个中的原因，其实并不难理解。软件工程师，不是搞学术，而是搞工程，而工程能力，是一个非常复杂的多面体。比如提到的沟通合作的能力，是根本就绕不过去的。职场这些年来，我已经无数次看到那些所谓的技术 “牛人”，有着优秀的独立问题分析与解决的技能，却总是在贡献与晋升方面迟人一步，甚至多年过去，也难以在职业生涯的路线上，迈入更高的台阶。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>除了沟通，再举一个例子，管理能力。我知道很多铁了心要做技术的程序员，也包括曾经的我在内，是对 “管理” 这个词有着不由自主的排斥。总对公司那些技术优秀的工程师，却 “转管理” 嗤之以鼻。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>某些总失偏颇的媒体也报道，“国外”（他们口中，中国以外的世界仿佛没有太大的分别，统一以 “国外” 概括之）五十岁的工程师还在写代码，六十岁的程序员还在发光发热……我理解这样的想法，但是这里被忽略掉的一个事实是，即便在那样技术通道被极大地保护的环境里，即便继续走坚持技术路线，坚持写代码，随着职位的升高，管理的工作就是不可避免地越来越多的，这是软件之所以为 “工程”、而非 “工学” 的一个特性。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>换言之，“沟通” 也好，“管理” 也罢，即便对于技术人来说，也都是逃不掉的。你要跨团队合作，你要带着一票人一起攻克项目，团队能达成的事情，在重要性上往往要盖过自己在努力啃着的特定的问题。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>当然，工程这个多面体中，除了 “硬技术”，必备的 “软能力” 有很多，我只是拿这两个方面举了例子。那么，再说一条我认为比较重要的能力吧——对于实际问题、模糊问题的挖掘和剖析能力。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>事实上，这一条也是我参与的软件工程师面试中，非常重要的一条考察项。实际问题总是模糊的，先要把问题搞清楚，抽象成一个软件可解的问题，再使用各种软件的工具（代码）来解决问题。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>你看，这里面有两个步骤，而在我们所熟悉的教育体系中，后者被极大地强化——算法、数据结构、库、平台……这些涉及的技能领域，都在后者这个 “解决一个已经经过抽象了的问题” 上面；而对于前一步骤的学习，往往是不足的。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>什么才是 “模糊的实际问题”？随便举个例子：&lt;/strong>&lt;/p>
&lt;p>&lt;strong>我们公司内部有大约 100 个服务（service），提供不同的功能，分别由不同团队维护。现在需要你带领一个小团队，来设计实现一套监控系统，统一监控管理这些服务的健康状况。你打算怎么做？&lt;/strong>&lt;/p>
&lt;p>&lt;strong>这就是一个非常模糊的实际问题，不是一个算法问题，也不是一个传统意义上的系统设计问题。但这是一个真正的、实际的 “软件问题”。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>最后，我想说的是。日常的工作中，也许我们一猛子下去扎得很深，但是我们还是需要时不时地跳出每天琐碎的条条框框，站高一点审视一下，自己在职业生涯路线中的位置，尽量带着技术人的决心和热情，但不要带着技术人的封闭与迂腐。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>作者介绍&lt;/strong>&lt;/p>
&lt;p>&lt;strong>熊燚，笔名四火，现于西雅图 Oracle 任首席软件工程师一职，负责研发云基础设施的分布式工作流引擎。曾先后任职于华为、亚马逊，做过多种类型的研发工作，从大小网站到高可用服务，从数据平台到可视化系统，他带领团队攻克过数个项目难关，在全栈之路上具有丰富的实战经验。&lt;/strong>&lt;/p></description></item><item><title>2024-08-02 搭个日志手机系统不香吗</title><link>https://qq547475331.github.io/docs/%E6%90%AD%E4%B8%AA%E6%97%A5%E5%BF%97%E6%89%8B%E6%9C%BA%E7%B3%BB%E7%BB%9F%E4%B8%8D%E9%A6%99%E5%90%97-da-ge-ri-zhi-shou-ji-xi-tong-bu-xiang-ma/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/%E6%90%AD%E4%B8%AA%E6%97%A5%E5%BF%97%E6%89%8B%E6%9C%BA%E7%B3%BB%E7%BB%9F%E4%B8%8D%E9%A6%99%E5%90%97-da-ge-ri-zhi-shou-ji-xi-tong-bu-xiang-ma/</guid><description>&lt;p>&lt;strong>一、ELK日志系统&lt;/strong>&lt;/p>
&lt;p>经典的ELK架构或现被称为Elastic Stack。Elastic Stack架构为Elasticsearch + Logstash + Kibana + Beats的组合：&lt;/p>
&lt;ul>
&lt;li>Beats负责日志的采集&lt;/li>
&lt;li>Logstash负责做日志的聚合和处理&lt;/li>
&lt;li>ES作为日志的存储和搜索系统&lt;/li>
&lt;li>Kibana作为可视化前端展示&lt;/li>
&lt;/ul>
&lt;p>整体架构图：&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202408011451836.png" alt="image-20240801145108735" />&lt;/p>
&lt;p>&lt;strong>二、EFK日志系统&lt;/strong>&lt;/p>
&lt;p>容器化场景中，尤其k8s环境，用户经常使用EFK架构。F代表Fluent Bit，一个开源多平台的日志处理器和转发器。Fluent Bit可以：&lt;/p>
&lt;ul>
&lt;li>让用户从不同来源收集数据/日志&lt;/li>
&lt;li>统一并发到多个目的地&lt;/li>
&lt;li>完全兼容Docker和k8s环境&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202408011451760.png" alt="image-20240801145121668" />&lt;/p>
&lt;p>&lt;strong>三、PLG日志系统&lt;/strong>&lt;/p>
&lt;p>&lt;strong>1.Prometheus+k8s日志系统&lt;/strong>&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202408011451628.png" alt="image-20240801145132549" />&lt;/p>
&lt;p>&lt;strong>2.PLG&lt;/strong>&lt;/p>
&lt;p>Grafana Labs提供的另一个日志解决方案PLG逐渐流行。PLG架构即Promtail + Loki + Grafana的组合：&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202408011451705.png" alt="image-20240801145143614" />&lt;/p>
&lt;p>Grafana，开源的可视化和分析软件，允许用户查询、可视化、警告和探索监控指标。Grafana主要提供时间序列数据的仪表板解决方案，支持超过数十种数据源。&lt;/p>
&lt;p>Grafana Loki是一组可以组成一个功能齐全的日志堆栈组件，与其它日志系统不同，Loki只建立日志标签的索引而不索引原始日志消息，而是为日志数据设置一组标签，即Loki运营成本更低，效率还提高几个数量级。&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202408011451968.png" alt="image-20240801145153889" />&lt;/p>
&lt;p>&lt;strong>3.Loki设计理念&lt;/strong>&lt;/p>
&lt;p>Prometheus启发，可实现可水平扩展、高可用的多租户日志系统。Loki整体架构由不同组件协同完成日志收集、索引、存储等。&lt;/p>
&lt;p>各组件如下，Loki’s Architecture深入了解。Loki就是like Prometheus, but for logs。&lt;/p>
&lt;p>Promtail是一个日志收集的代理，会将本地日志内容发到一个Loki实例，它通常部署到需要监视应用程序的每台机器/容器上。Promtail主要是用来发现目标、将标签附加到日志流以及将日志推送到Loki。截止到目前，Promtail可以跟踪两个来源的日志：本地日志文件和systemd日志（仅支持AMD64架构）。&lt;/p>
&lt;p>&lt;strong>四、PLG V.S ELK&lt;/strong>&lt;/p>
&lt;p>&lt;strong>1.ES V.S Loki&lt;/strong>&lt;/p>
&lt;p>ELK/EFK架构确实强，经多年实际环境验证。存储在ES中的日志通常以非结构化JSON对象形式存储在磁盘，且ES为每个对象都建索引，以便全文搜索，然后用户可特定查询语言搜索这些日志数据。&lt;/p>
&lt;p>而Loki数据存储解耦：&lt;/p>
&lt;ul>
&lt;li>既可在磁盘存储&lt;/li>
&lt;li>也可用如Amazon S3云存储系统&lt;/li>
&lt;/ul>
&lt;p>Loki日志带有一组标签名和值，只有标签对被索引，这种权衡使它比完整索引操作成本更低，但针对基于内容的查询，需通过LogQL再单独查询。&lt;/p>
&lt;p>&lt;strong>2.Fluentd V.S Promtail&lt;/strong>&lt;/p>
&lt;p>相比Fluentd，Promtail专为Loki定制，它可为运行在同一节点的k8s Pods做服务发现，从指定文件夹读取日志。Loki类似Prometheus的标签方式。因此，当与Prometheus部署在同一环境，因为相同的服务发现机制，来自Promtail的日志通常具有与应用程序指标相同的标签，统一标签管理。&lt;/p></description></item><item><title>2024-08-02 是技术大神还是基础架构部的祸害</title><link>https://qq547475331.github.io/docs/%E6%98%AF%E6%8A%80%E6%9C%AF%E5%A4%A7%E7%A5%9E%E8%BF%98%E6%98%AF%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84%E9%83%A8%E7%9A%84%E7%A5%B8%E5%AE%B3-shi-ji-shu-da-shen-hai-shi-ji-chu-jia-gou-bu-de-huo-hai/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/%E6%98%AF%E6%8A%80%E6%9C%AF%E5%A4%A7%E7%A5%9E%E8%BF%98%E6%98%AF%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84%E9%83%A8%E7%9A%84%E7%A5%B8%E5%AE%B3-shi-ji-shu-da-shen-hai-shi-ji-chu-jia-gou-bu-de-huo-hai/</guid><description>&lt;p>这两天马工关于基础架构部的文章「&lt;a href="https://mp.weixin.qq.com/s?__biz=MzkwODMyMDE2NQ==&amp;amp;mid=2247484429&amp;amp;idx=1&amp;amp;sn=e8c96fc5d2bd5058dfcfb58592488422&amp;amp;scene=21#wechat_redirect">基础架构部，还有必要吗？&lt;/a>」进一步在朋友圈疯转。严格来说，这篇文章更应该被看作前一篇关于多云的文章「&lt;a href="http://mp.weixin.qq.com/s?__biz=MzkwODMyMDE2NQ==&amp;amp;mid=2247484418&amp;amp;idx=1&amp;amp;sn=c7eb6c850f8c9d2a53ffb4462bdca459&amp;amp;chksm=c0ca88c8f7bd01de66c36b8a35f767840a321515bd9867aa0ac03fd3d401d29223e5a2886fef&amp;amp;scene=21#wechat_redirect">多云战略：无奈的现实，危险的选择&lt;/a>」的姊妹篇。&lt;/p>
&lt;p>恰逢这两天笔者也在思考关于「云上安全」的一些话题，其中一部分核心观点其实和这篇文章多少有些类似。之前我们在文章&lt;a href="http://mp.weixin.qq.com/s?__biz=Mzg4MDA5NzM4Nw==&amp;amp;mid=2247486521&amp;amp;idx=1&amp;amp;sn=8c0c8b19c983af9271e902e19f6220c7&amp;amp;chksm=cf7b2d3df80ca42be6d4309ed027cc065186d76321f70439e1dd1ec7e455cd0737095ab0395e&amp;amp;scene=21#wechat_redirect">为什么汽车行业正在全面拥抱公有云&lt;/a>中从企业决策者视角详细讲述企业上云动机，借此讨论，再展开多聊几句关于用户如何使用云的一些拙见。&lt;/p>
&lt;p>&lt;strong>「云优先」的思维方式&lt;/strong>&lt;/p>
&lt;hr>
&lt;p>马工这篇文章和他这两年的观点是一脉相承的，核心观点只有一个：&lt;strong>云正在不经意之间改变着组织架构的构建形态，甚至重新定义职业能力象限。&lt;/strong>&lt;/p>
&lt;p>只是文章整体论证多少有些武断，马工通过例举了基础架构部的各种**「无能」**事迹，甚至不惜将团队妖魔化，去论证团队已经没有存在的必要，笔者认为仅凭借这点是不足以服众的，毕竟组织架构和团队能力这两者本身就要分开来看。&lt;/p>
&lt;p>但本文并无意纠结于基础架构部到底有没有核心技术，事实上，笔者认为这个话题的讨论重点也根本不是**「基础架构部的存在是否利大于弊」&lt;strong>。真正在讨论的是，今天我们所处的云时代，行业正不断走向成熟，云早已不再只是资源托管平台。作为一种全新的生产工具，在带来新的生产力之后，在&lt;/strong>「新的生产关系下，企业组织架构该如何构建」**。&lt;/p>
&lt;p>笔者观点甚至可能比马工更悲观：在今天，但凡企业组织形态没有基于云的特性去做架构思考和设计，即使达到了马工在文章中所讲述的两点：具备「&lt;strong>优秀的技术&lt;/strong>」和「&lt;strong>引导使用优秀技术的能力&lt;/strong>」，最终的结果大部分情况都是令人失望的。关于这点我从令辉兄的另一篇文章&lt;a href="https://mp.weixin.qq.com/s?__biz=Mzg2MDYzOTUzMA==&amp;amp;mid=2247483762&amp;amp;idx=1&amp;amp;sn=a3fade1bfe2be2b86de08c1c1f780728&amp;amp;scene=21#wechat_redirect">基础架构部不死，只是慢慢消逝&lt;/a>一文中感受到了同样的情感，&lt;strong>老兵不死，他们只是逐渐凋零。&lt;/strong>&lt;/p>
&lt;p>基础架构部未来会消失吗？我认为不会。在过去，马车用于运输，但今天却常见于马戏表演，演变成了一种娱乐活动。马车消失了吗？没有，只是它的数量和存在的目的都变了。&lt;/p>
&lt;p>这两年在和不同行业客户打交道的过程中，总是会遇到各种用云过程中的各种怪相。&lt;/p>
&lt;p>例如用户虽然使用公有云，但是团队完全按照数据中心时代去构建，从工作职能划分再到技术选型，最好是在迁移上云过程中，连IP地址都不要发生任何变化。&lt;/p>
&lt;p>甚至于一些用户会提需求说，我们希望能让我云上的架构看起来和线下一样&amp;hellip;&amp;hellip;&lt;/p>
&lt;p>再比如，一些客户会希望利用公有云去实现K8s架构下的应用及数据冷备&amp;hellip;&amp;hellip;&lt;/p>
&lt;p>再比如，我此前在多篇文章中所提到的，非要在毫无标准可言的多个云平台之间，基于美好但不切实际的多云管理平台去实现跨云迁移。&lt;/p>
&lt;p>种种这些，许多人只是惯性的将系统从云下部署到了云上，却又很难讲清应用究竟是否真正的上云，以及和没有上云有什么区别。&lt;/p>
&lt;p>前两年「Cloud Native」这个词很火，时而被形容架构，时而形容产品，笔者认为在此之前，「Cloud Native」更应该被视作为用户在设计应用及平台架构时的用云观。&lt;/p>
&lt;p>这两天也看了不少留言和文章极力证明自己团队的技术价值，从而试图论证基础架构有存在的必要，实际上都没有太大的实质意义。&lt;/p>
&lt;p>**
**&lt;/p>
&lt;p>&lt;strong>「自服务」的去中心化组织&lt;/strong>&lt;/p>
&lt;hr>
&lt;p>其实如果记录下时间轴，我们会看到最早是网络工程师和操作系统管理员、这两年讨论更多的DBA，然后再到基础架构部。而这个顺序刚好是伴随着云计算行业的不断成熟，从IaaS基础设施层，再到PaaS一层层往上走的。&lt;/p>
&lt;p>在国外，由于用户和云厂商之间所形成的技术共识，这个结果的发生的比国内要早几年，从这两年的行业趋势来看，国内目前也朝着这个方向在发展。&lt;/p>
&lt;p>如果你所在的组织并没有使用，也永远不会使用公有云，大可不必将马工的文章放在心上，至少眼下你所面临的处境，生产工具和生产关系的矛盾并没有发生。&lt;/p>
&lt;p>但如果你所在的组织未来计划上云，或是已经用云，从技术选型再到人员能力矩阵的重构，这个时间或早或晚，但一定会在某一天发生。这也就意味着今天许多团队在云上或云下自建的许多基础设施平台的工作，在未来可能都是负价值的。&lt;/p>
&lt;p>为什么在云架构下，传统技术架构的划分已不再适用？&lt;/p>
&lt;p>&lt;strong>架构的不可持续性&lt;/strong>&lt;/p>
&lt;p>过去我们在做基础架构工作时，最常见不过的便是技术山头下的标准化问题。永远不要挑战人性，只要在企业内部团队不是一家独大，大家就一定都想自建一套技术标准和产品体系，最后的结果大概率无法达成共识。大部分情况下，基础架构都成为了为特定业务服务的配套团队。&lt;/p>
&lt;p>而上面这种已经算是比较好的技术归宿，事实证明「&lt;strong>以“大神”为中心&lt;/strong>」构建的技术平台永远都是不可持续的。更多的情况是当业务系统接入之后，因人员变动缺乏维护的基础组件，留给业务团队的只能是一地鸡毛。&lt;/p>
&lt;p>这种试错和重构所带来的额外技术成本，几乎成为了企业在基础架构发展过程中的阿格琉斯之踵。但基于云优先理念，情况可能完全不同，不同业务部门基于同一套云托管服务很容易在团队之间达成技术共识，在标准的平台界面上各方都是一致的，同时也能够基于各自的实际业务场景，选择不同的技术实现方式。而在过去以资源为中心的技术架构形态之下，这种情况是不存在的。&lt;/p>
&lt;p>因为上述的诸多原因，从这两年的云上趋势来看，头部玩家也在逐渐拥抱云上PaaS。从跨越时间周期更长远的视角来看技术演进之路，基于云平台技术架构的可持续性是经得起时间验证的。&lt;/p>
&lt;p>&lt;strong>DevOps挑战&lt;/strong>&lt;/p>
&lt;p>另一方面，笔者并不想要挑战基础架构的技术能力，但事实上基于云仍会比技术自建要高效的多。&lt;/p>
&lt;p>过去十余年，国内在实践DevOps工程的过程中，其组织形态往往是运维来主导，因此自动化便成为了首要业务关键指标。但自动化其实只是过程，经过这些年的尝试，我们很容易发现DevOps的最大挑战其实并不能通过自动化彻底解决，甚至可以说是占比极小的一部分。除非今天你所面临的业务场景足够简单，同时业务又很少发生变化，否则真实世界一定会有大量的corner case是需要依靠人工变更去完成的。&lt;/p>
&lt;p>在人员架构复杂的大型组织，DevOps真正要解决的问题是&lt;strong>跨职能团队之间的信息衰减&lt;/strong>，而想要达到消除信息衰减，关键在于如何将组织形态尽可能地&lt;strong>扁平化&lt;/strong>和&lt;strong>去中心化&lt;/strong>。因此许多时候，组织寄希望于靠一套自动化工具平台去解决跨职能团队的协同问题，效果往往是很差的。&lt;/p>
&lt;p>还有比将所有基础架构完全交由云厂商管理，由研发人员所主导**「开发自服务」**更优雅的DevOps工程实践吗？&lt;/p>
&lt;p>由于部门墙的存在，企业内部的许多团队被无形的保护了起来，以至于很多事情组织并不是以最高效的方式去运作的，而在另一些主营业务是云上代运维服务提供商的企业，在真实商业环境你死我活的竞争下，他们能够更快察觉到开发自服务所带来的全新业务挑战。&lt;/p>
&lt;p>曾经，当软件提供商给客户开发交付一套核心系统时，他们需要和企业内的运维、基础架构等团队一起协同；如今，他们完全可以基于Serverless、IaC等工程实践，彻底屏蔽并绕开组织内的所有人，拿走项目中最大的那块蛋糕。最后，还可以正大光明的讲述一个在技术领先性的架构理念下如何提升人效的故事。&lt;/p>
&lt;p>马工一直在讲国内云用户其实不怎么懂云，仅仅将其作为资源托管平台的IDC2.0。其实关于这点笔者是有信心的，从最近这两年和客户在各种场景和项目中的交流来看，仍然有越来越多的客户是在正视这些现象的，从资源运维到平台治理，从ClickOps到IaC，这种趋势的转变更多的只是时间问题。&lt;/p>
&lt;p>&lt;strong>写在最后&lt;/strong>&lt;/p>
&lt;hr>
&lt;p>回到本文最初的讨论，基础架构团队在未来仍然会存在，但他们的数量必然也会像更多企业的DBA一样被极大的减少，从企业的技术建设者转向云实践下的工程布道者。&lt;/p>
&lt;p>云计算作为技术平权的开放平台，在研发流程中极大的消除了不同管理角色的工作职能，但生产关系的改变同时也在创造新的机会，未来的中心化团队建设重心也将转向从「精细化」管理到「精确化」运营的平台工程的体系建设。&lt;/p>
&lt;p>不仅如此，如果再向上看一步，云并不仅仅是解决了技术人员的统一语言问题，甚至包括财务人员在内的非技术人员同样收益，今天如果要做预算规划，财务人员完全能够基于云账号实现「财务自服务」，最终使得FinOps成为可能。&lt;/p>
&lt;p>如果认为云计算最终只会卷到甲方的，可能真的只是一种错觉。这几年，能够明显感觉到随着行业走向成熟，对于从业人员的能力矩阵也在发生着改变。&lt;/p>
&lt;p>最近这段时间，笔者的大部分精力都放在了技术服务岗的面试工作。在此过程中也发现一些有趣的现象。一些愿意主动投递简历做云技术服务的，大部分是以过往以运维工程师相关背景的技术人员。&lt;/p>
&lt;p>而一些研发背景的人员会觉得云计算和他们的职业成长规划似乎并没有太大的关系，在过往的工作中，他们也很少会真正由他们去创建、配置、操作云服务，有类似经历的其实在行业中不在少数。&lt;/p>
&lt;p>作为普通职场人，每个人都或多或少会面临不同程度的职业危机。许多时候我们总是会苛责自己没有努力做到更好，又或是时常抱怨自己自己没有得到更多的机会，但外部环境的变化从不以人的喜好和意志为转移，绝大多数情况下，普通人能做到的极限往往也只是顺势而为。&lt;/p></description></item><item><title>2024-08-02 构建容器镜像利器buildkit</title><link>https://qq547475331.github.io/docs/%E6%9E%84%E5%BB%BA%E5%AE%B9%E5%99%A8%E9%95%9C%E5%83%8F%E5%88%A9%E5%99%A8buildkit-gou-jian-rong-qi-jing-xiang-li-qi-buildkit/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/%E6%9E%84%E5%BB%BA%E5%AE%B9%E5%99%A8%E9%95%9C%E5%83%8F%E5%88%A9%E5%99%A8buildkit-gou-jian-rong-qi-jing-xiang-li-qi-buildkit/</guid><description>&lt;p>Docker通过读取Dockerfile中的指令自动构建镜像，Dockerfile是一个文本文件，其中依次包含构建给定镜像所需的所有命令。&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202408011444771.png" alt="image-20240801144458689" />&lt;/p>
&lt;p>上面的解释摘自Docker的官方文档并总结了Dockerfile的用途。Dockerfile的使用非常重要，因为它是我们的蓝图，是我们添加到Docker镜像中的层的记录。&lt;/p>
&lt;p>本文，我们将学习如何利用BuildKit功能，这是Docker v18.09上引入的一组增强功能。集成BuildKit将为我们提供更好的性能，存储管理和安全性。&lt;/p>
&lt;h3 id="本文目标">
 本文目标
 &lt;a class="anchor" href="#%e6%9c%ac%e6%96%87%e7%9b%ae%e6%a0%87">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>减少构建时间；&lt;/li>
&lt;li>缩小镜像尺寸；&lt;/li>
&lt;li>获得可维护性；&lt;/li>
&lt;li>获得可重复性；&lt;/li>
&lt;li>了解多阶段Dockerfile;&lt;/li>
&lt;li>了解BuildKit功能。&lt;/li>
&lt;/ul>
&lt;h3 id="先决条件">
 先决条件
 &lt;a class="anchor" href="#%e5%85%88%e5%86%b3%e6%9d%a1%e4%bb%b6">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>Docker概念知识&lt;/li>
&lt;li>已安装Docker（当前使用v19.03）&lt;/li>
&lt;li>一个Java应用程序（在本文中，我使用了一个Jenkins Maven示例应用程序）&lt;/li>
&lt;/ul>
&lt;p>让我们开始吧！&lt;/p>
&lt;h3 id="简单的dockerfile示例">
 简单的Dockerfile示例
 &lt;a class="anchor" href="#%e7%ae%80%e5%8d%95%e7%9a%84dockerfile%e7%a4%ba%e4%be%8b">#&lt;/a>
&lt;/h3>
&lt;p>以下是一个包含Java应用程序的未优化Dockerfile的示例。我们将逐步进行一些优化。&lt;/p>
&lt;pre tabindex="0">&lt;code>FROM debian
COPY . /app
RUN apt-get update
RUN apt-get -y install openjdk-11-jdk ssh emacs
CMD [“java”, “-jar”, “/app/target/my-app-1.0-SNAPSHOT.jar”]
&lt;/code>&lt;/pre>&lt;p>在这里，我们可能会问自己：构建需要多长时间？为了回答这个问题，让我们在本地开发环境上创建该Dockerfile，并让Docker构建镜像。&lt;/p>
&lt;pre tabindex="0">&lt;code># enter your Java app folder
cd simple-java-maven-app-master
# create a Dockerfile
vim Dockerfile
# write content, save and exit
docker pull debian:latest # pull the source image
time docker build --no-cache -t docker-class . # overwrite previous layers
# notice the build time
0,21s user 0,23s system 0% cpu 1:55,17 total
&lt;/code>&lt;/pre>&lt;p>此时，我们的构建需要1m55s。&lt;/p></description></item><item><title>2024-08-02 清理docker镜像</title><link>https://qq547475331.github.io/docs/%E6%B8%85%E7%90%86docker%E9%95%9C%E5%83%8F-qing-li-docker-jing-xiang/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/%E6%B8%85%E7%90%86docker%E9%95%9C%E5%83%8F-qing-li-docker-jing-xiang/</guid><description>&lt;p>&lt;strong>清理docker镜像&lt;/strong>&lt;/p>
&lt;p>&lt;strong>要清理Docker镜像，可以使用以下命令：&lt;/strong>&lt;/p>
&lt;ol>
&lt;li>&lt;strong>删除未使用的Docker镜像：&lt;/strong>&lt;/li>
&lt;/ol>
&lt;pre tabindex="0">&lt;code>docker image prune

docker image prune -a
&lt;/code>&lt;/pre>&lt;ol>
&lt;li>&lt;strong>删除所有停止的容器：&lt;/strong>&lt;/li>
&lt;/ol>
&lt;pre tabindex="0">&lt;code>docker container prune
&lt;/code>&lt;/pre>&lt;ol>
&lt;li>&lt;strong>删除未使用的网络：&lt;/strong>&lt;/li>
&lt;/ol>
&lt;pre tabindex="0">&lt;code>docker network prune
&lt;/code>&lt;/pre>&lt;ol>
&lt;li>&lt;strong>删除未使用的卷：&lt;/strong>&lt;/li>
&lt;/ol>
&lt;pre tabindex="0">&lt;code>docker volume prune
&lt;/code>&lt;/pre>&lt;ol>
&lt;li>&lt;strong>删除未使用的构建缓存：&lt;/strong>&lt;/li>
&lt;/ol>
&lt;pre tabindex="0">&lt;code>docker builder prune
&lt;/code>&lt;/pre>&lt;p>&lt;strong>如果你想删除特定的镜像，可以使用：&lt;/strong>&lt;/p>
&lt;pre tabindex="0">&lt;code>docker rmi [IMAGE_ID_OR_NAME]
&lt;/code>&lt;/pre>&lt;p>&lt;strong>请注意，替换掉&lt;code>[IMAGE_ID_OR_NAME]&lt;/code>为你想删除的镜像ID或名称。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>如果你想删除所有停止的容器、未使用的网络和卷，可以一次性执行：&lt;/strong>&lt;/p>
&lt;pre tabindex="0">&lt;code>docker system prune
&lt;/code>&lt;/pre>&lt;p>&lt;strong>如果你想删除所有未使用的镜像（包括那些被停止的容器所使用的镜像），可以使用：&lt;/strong>&lt;/p>
&lt;pre tabindex="0">&lt;code>docker image prune -a
&lt;/code>&lt;/pre>&lt;p>&lt;strong>这些命令应该帮助你清理Docker环境中不再需要的对象。在运行这些命令之前，请确保你不再需要这些对象，因为这将是不可逆的操作。&lt;/strong>&lt;/p>
&lt;h3 id="清理docker镜像">
 &lt;strong>&lt;a href="http://www.baidu.com/link?url=wtyrDsSQDgyvqSiHDXWMIfXN87CZMj01lVXnYSsyg6N1pZ2tY1bT-u020_8UZf2UR1bwL4HgcCcBvGgD7b5-3bCBTtBqOwuqskuufCg-9j9XZS5XFTNGPgB6ss_zABlSz_UjUv-bkzzz3Jf94qA8nM2OJtDmpzznMq2jllVowvGz5cih1C4TYe_r1niN_pmqdJZSr-0wbDPAn3R1N-v1m28dKZzkPOqQnYt5LtqFpWHFxWLYmlsLaKuCljd6jqxCFrvzzDAIvajiA5dUWuJ1fcb5nvJKX_O7bxu_Yjo7wR07rq_5lDddMIjGUWXzf6Preq6VfVlZMx-jnlAKMKjGbls0frCdXReCkb0FuSBkL4kKUF5XPPsDLGQr980eKoxhqEZKeVo9PK5mJqDVJ8NR10jAusNW6bmpDreOoI4JBK7jVRNNhRZRk8aBSAcSiZpjQ1YREoJ2oXTqL7VwePXQlOqiL7KgUGSV3HYLYNoNzHEi2pZYuWEfeejUBNNUenfx">清理docker镜像&lt;/a>&lt;/strong>
 &lt;a class="anchor" href="#%e6%b8%85%e7%90%86docker%e9%95%9c%e5%83%8f">#&lt;/a>
&lt;/h3>
&lt;p>&lt;strong>清理Docker镜像可以通过多种方法进行，包括删除特定时间前的镜像、按照大小排序、使用脚本自动化、定时自动清理以及删除特定模式的镜像。以下是一些具体的方法：12&lt;/strong>&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>删除特定时间前的镜像：使用&lt;/strong>&lt;/p>
&lt;pre tabindex="0">&lt;code>docker image prune --filter &amp;#34;until=168h&amp;#34;
&lt;/code>&lt;/pre>&lt;p>&lt;strong>命令可以删除所有创建时间超过168小时（7天）的镜像。&lt;/strong>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>按照大小排序：通过&lt;/strong>&lt;/p>
&lt;pre tabindex="0">&lt;code>docker images --format &amp;#39;{{.Size}}\t{{.Repository}}:{{.Tag}}&amp;#39; | sort -hr
&lt;/code>&lt;/pre>&lt;p>&lt;strong>命令，可以直观地看到哪些镜像占用空间较大，然后手动删除。&lt;/strong>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>使用脚本自动化：编写一个简单的脚本来自动化清理过程，例如清理所有未使用的镜像和容器，或者删除所有以特定版本开头的镜像。&lt;/strong>&lt;/p>
&lt;pre tabindex="0">&lt;code>#!/bin/bash
# 清理所有未使用的镜像和容器
docker system prune -af
# 删除所有1.0版本开头的镜像
docker images | grep &amp;#39;1.0&amp;#39; | awk &amp;#39;{print $3}&amp;#39; | xargs docker rmi
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>&lt;strong>定时自动清理：利用cron定时任务，可以设定周期性清理工作，例如每天凌晨两点执行清理脚本。&lt;/strong>&lt;/p></description></item><item><title>2024-08-02 顶级devops工具大盘点</title><link>https://qq547475331.github.io/docs/%E9%A1%B6%E7%BA%A7devops%E5%B7%A5%E5%85%B7%E5%A4%A7%E7%9B%98%E7%82%B9-ding-ji-devops-gong-ju-da-pan-dian/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/%E9%A1%B6%E7%BA%A7devops%E5%B7%A5%E5%85%B7%E5%A4%A7%E7%9B%98%E7%82%B9-ding-ji-devops-gong-ju-da-pan-dian/</guid><description>&lt;p>本篇文章中将介绍一些能够帮助你实现 DevOps 目标的核心技术类别和具体技术。&lt;/p>
&lt;h2 id="关于-devops-及其工具">
 关于 DevOps 及其工具
 &lt;a class="anchor" href="#%e5%85%b3%e4%ba%8e-devops-%e5%8f%8a%e5%85%b6%e5%b7%a5%e5%85%b7">#&lt;/a>
&lt;/h2>
&lt;p>关于 DevOps 及其工具，需要记住：&lt;/p>
&lt;ul>
&lt;li>持续改进是目标；&lt;/li>
&lt;li>DevOps 不是花钱买来的；&lt;/li>
&lt;li>分阶段采用工具。&lt;/li>
&lt;/ul>
&lt;h2 id="计划工具">
 计划工具
 &lt;a class="anchor" href="#%e8%ae%a1%e5%88%92%e5%b7%a5%e5%85%b7">#&lt;/a>
&lt;/h2>
&lt;p>为什么计划工具对于 DevOps 来说很重要？&lt;/p>
&lt;ul>
&lt;li>分享目标；&lt;/li>
&lt;li>透明性；&lt;/li>
&lt;li>赋能。&lt;/li>
&lt;/ul>
&lt;h2 id="计划工具示例">
 计划工具示例
 &lt;a class="anchor" href="#%e8%ae%a1%e5%88%92%e5%b7%a5%e5%85%b7%e7%a4%ba%e4%be%8b">#&lt;/a>
&lt;/h2>
&lt;h6 id="gitlab">
 GitLab
 &lt;a class="anchor" href="#gitlab">#&lt;/a>
&lt;/h6>
&lt;p>GitLab 是一个基于 Web 的 DevOps 生命周期工具。它提供了一个 Git 仓库管理器，具备 wiki、问题跟踪和 CI/CD 管道功能，采用的是 GitLab 公司的开源许可。&lt;/p>
&lt;h6 id="tasktop">
 Tasktop
 &lt;a class="anchor" href="#tasktop">#&lt;/a>
&lt;/h6>
&lt;p>Tasktop 允许将所有这些工具添加到敏捷、ALM、PPM 和 ITSM 中，实现了对整个生命周期前所未有的可见性。&lt;/p>
&lt;h6 id="collabnet-versionone">
 CollabNet VersionOne
 &lt;a class="anchor" href="#collabnet-versionone">#&lt;/a>
&lt;/h6>
&lt;p>VersionOne 支持 Scrum、看板、XP、SAFe 和混合开发方法，并使跨团队、程序、软件组合和企业的计划、跟踪和报告变得更容易。&lt;/p>
&lt;h6 id="pivotal-tracker">
 Pivotal Tracker
 &lt;a class="anchor" href="#pivotal-tracker">#&lt;/a>
&lt;/h6>
&lt;p>敏捷项目管理工具，是开发人员围绕高优先级共享 backlog 进行实时协作的首选工具。&lt;/p>
&lt;h6 id="trello">
 Trello
 &lt;a class="anchor" href="#trello">#&lt;/a>
&lt;/h6>
&lt;p>Trello 是一个基于 Web 的看板风格的清单应用程序，是 Atlassian 的子公司。&lt;/p></description></item><item><title>2024-10-20 使用 Keepalived 和 HAproxy 创建高可用 Kubernetes 集群</title><link>https://qq547475331.github.io/docs/2024-10-20-%E5%88%9B%E5%BB%BA%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2024-10-20-%E5%88%9B%E5%BB%BA%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/</guid><description>&lt;h1 id="使用-keepalived-和-haproxy-创建高可用-kubernetes-集群">
 使用 Keepalived 和 HAproxy 创建高可用 Kubernetes 集群
 &lt;a class="anchor" href="#%e4%bd%bf%e7%94%a8-keepalived-%e5%92%8c-haproxy-%e5%88%9b%e5%bb%ba%e9%ab%98%e5%8f%af%e7%94%a8-kubernetes-%e9%9b%86%e7%be%a4">#&lt;/a>
&lt;/h1>
&lt;p>高可用 Kubernetes 集群能够确保应用程序在运行时不会出现服务中断，这也是生产的需求之一。为此，有很多方法可供选择以实现高可用。&lt;/p>
&lt;p>本教程演示了如何配置 Keepalived 和 HAproxy 使负载均衡、实现高可用。步骤如下：&lt;/p>
&lt;ol>
&lt;li>准备主机。&lt;/li>
&lt;li>配置 Keepalived 和 HAproxy。&lt;/li>
&lt;li>使用 KubeKey 创建 Kubernetes 集群，并安装 KubeSphere。&lt;/li>
&lt;/ol>
&lt;h2 id="集群架构">
 集群架构
 &lt;a class="anchor" href="#%e9%9b%86%e7%be%a4%e6%9e%b6%e6%9e%84">#&lt;/a>
&lt;/h2>
&lt;p>示例集群有三个主节点，三个工作节点，两个用于负载均衡的节点，以及一个虚拟 IP 地址。本示例中的虚拟 IP 地址也可称为“浮动 IP 地址”。这意味着在节点故障的情况下，该 IP 地址可在节点之间漂移，从而实现高可用。&lt;/p>
&lt;p>&lt;img src="https://kubesphere.io/images/docs/v3.x/installing-on-linux/high-availability-configurations/set-up-ha-cluster-using-keepalived-haproxy/architecture-ha-k8s-cluster.png" alt="architecture-ha-k8s-cluster" />&lt;/p>
&lt;p>请注意，在本示例中，Keepalived 和 HAproxy 没有安装在任何主节点上。但您也可以这样做，并同时实现高可用。然而，配置两个用于负载均衡的特定节点（您可以按需增加更多此类节点）会更加安全。这两个节点上只安装 Keepalived 和 HAproxy，以避免与任何 Kubernetes 组件和服务的潜在冲突。&lt;/p>
&lt;h2 id="准备主机">
 准备主机
 &lt;a class="anchor" href="#%e5%87%86%e5%a4%87%e4%b8%bb%e6%9c%ba">#&lt;/a>
&lt;/h2>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th style="text-align: left">IP 地址&lt;/th>
 &lt;th style="text-align: left">主机名&lt;/th>
 &lt;th style="text-align: left">角色&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td style="text-align: left">172.16.0.2&lt;/td>
 &lt;td style="text-align: left">lb1&lt;/td>
 &lt;td style="text-align: left">Keepalived &amp;amp; HAproxy&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">172.16.0.3&lt;/td>
 &lt;td style="text-align: left">lb2&lt;/td>
 &lt;td style="text-align: left">Keepalived &amp;amp; HAproxy&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">172.16.0.4&lt;/td>
 &lt;td style="text-align: left">master1&lt;/td>
 &lt;td style="text-align: left">master, etcd&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">172.16.0.5&lt;/td>
 &lt;td style="text-align: left">master2&lt;/td>
 &lt;td style="text-align: left">master, etcd&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">172.16.0.6&lt;/td>
 &lt;td style="text-align: left">master3&lt;/td>
 &lt;td style="text-align: left">master, etcd&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">172.16.0.7&lt;/td>
 &lt;td style="text-align: left">worker1&lt;/td>
 &lt;td style="text-align: left">worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">172.16.0.8&lt;/td>
 &lt;td style="text-align: left">worker2&lt;/td>
 &lt;td style="text-align: left">worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">172.16.0.9&lt;/td>
 &lt;td style="text-align: left">worker3&lt;/td>
 &lt;td style="text-align: left">worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">172.16.0.10&lt;/td>
 &lt;td style="text-align: left">&lt;/td>
 &lt;td style="text-align: left">虚拟 IP 地址&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;p>有关更多节点、网络、依赖项等要求的信息，请参见&lt;a href="https://kubesphere.io/zh/docs/v3.4/installing-on-linux/introduction/multioverview/#step-1-prepare-linux-hosts">多节点安装&lt;/a>。&lt;/p></description></item><item><title>2024-12-05 kubeasz部署k8s</title><link>https://qq547475331.github.io/docs/2024-12-05-kubeasz%E9%83%A8%E7%BD%B2k8s/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2024-12-05-kubeasz%E9%83%A8%E7%BD%B2k8s/</guid><description>&lt;h2 id="00-集群规划和基础参数设定">
 00-集群规划和基础参数设定
 &lt;a class="anchor" href="#00-%e9%9b%86%e7%be%a4%e8%a7%84%e5%88%92%e5%92%8c%e5%9f%ba%e7%a1%80%e5%8f%82%e6%95%b0%e8%ae%be%e5%ae%9a">#&lt;/a>
&lt;/h2>
&lt;h3 id="ha-architecture">
 HA architecture
 &lt;a class="anchor" href="#ha-architecture">#&lt;/a>
&lt;/h3>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/image-20241204235826382.png" alt="image-20241204235826382" />&lt;/p>
&lt;ul>
&lt;li>注意1：确保各节点时区设置一致、时间同步。 如果你的环境没有提供NTP 时间同步，推荐集成安装&lt;a href="../guide/chrony.md">chrony&lt;/a>&lt;/li>
&lt;li>注意2：确保在干净的系统上开始安装，不要使用曾经装过kubeadm或其他k8s发行版的环境&lt;/li>
&lt;li>注意3：建议操作系统升级到新的稳定内核，请结合阅读&lt;a href="../guide/kernel_upgrade.md">内核升级文档&lt;/a>&lt;/li>
&lt;li>注意4：在公有云上创建多主集群，请结合阅读&lt;a href="kubeasz_on_public_cloud.md">在公有云上部署 kubeasz&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="高可用集群所需节点配置如下">
 高可用集群所需节点配置如下
 &lt;a class="anchor" href="#%e9%ab%98%e5%8f%af%e7%94%a8%e9%9b%86%e7%be%a4%e6%89%80%e9%9c%80%e8%8a%82%e7%82%b9%e9%85%8d%e7%bd%ae%e5%a6%82%e4%b8%8b">#&lt;/a>
&lt;/h2>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th style="text-align: left">角色&lt;/th>
 &lt;th style="text-align: left">数量&lt;/th>
 &lt;th style="text-align: left">描述&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td style="text-align: left">部署节点&lt;/td>
 &lt;td style="text-align: left">1&lt;/td>
 &lt;td style="text-align: left">运行ansible/ezctl命令，一般复用第一个master节点&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">etcd节点&lt;/td>
 &lt;td style="text-align: left">3&lt;/td>
 &lt;td style="text-align: left">注意etcd集群需要1,3,5,&amp;hellip;奇数个节点，一般复用master节点&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">master节点&lt;/td>
 &lt;td style="text-align: left">2&lt;/td>
 &lt;td style="text-align: left">高可用集群至少2个master节点&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">node节点&lt;/td>
 &lt;td style="text-align: left">n&lt;/td>
 &lt;td style="text-align: left">运行应用负载的节点，可根据需要提升机器配置/增加节点数&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;p>机器配置：&lt;/p>
&lt;ul>
&lt;li>master节点：4c/8g内存/50g硬盘&lt;/li>
&lt;li>worker节点：建议8c/32g内存/200g硬盘以上&lt;/li>
&lt;/ul>
&lt;p>注意：默认配置下容器运行时和kubelet会占用/var的磁盘空间，如果磁盘分区特殊，可以设置config.yml中的容器运行时和kubelet数据目录：&lt;code>CONTAINERD_STORAGE_DIR&lt;/code> &lt;code>DOCKER_STORAGE_DIR&lt;/code> &lt;code>KUBELET_ROOT_DIR&lt;/code>&lt;/p>
&lt;p>在 kubeasz 2x 版本，多节点高可用集群安装可以使用2种方式&lt;/p>
&lt;ul>
&lt;li>1.按照本文步骤先规划准备，预先配置节点信息后，直接安装多节点高可用集群&lt;/li>
&lt;li>2.先部署单节点集群 &lt;a href="quickStart.md">AllinOne部署&lt;/a>，然后通过 &lt;a href="../op/op-index.md">节点添加&lt;/a> 扩容成高可用集群&lt;/li>
&lt;/ul>
&lt;h2 id="部署步骤">
 部署步骤
 &lt;a class="anchor" href="#%e9%83%a8%e7%bd%b2%e6%ad%a5%e9%aa%a4">#&lt;/a>
&lt;/h2>
&lt;p>以下示例创建一个4节点的多主高可用集群，文档中命令默认都需要root权限运行。&lt;/p>
&lt;h3 id="1基础系统配置">
 1.基础系统配置
 &lt;a class="anchor" href="#1%e5%9f%ba%e7%a1%80%e7%b3%bb%e7%bb%9f%e9%85%8d%e7%bd%ae">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>2c/4g内存/40g硬盘（该配置仅测试用）&lt;/li>
&lt;li>最小化安装&lt;code>Ubuntu 16.04 server&lt;/code>或者&lt;code>CentOS 7 Minimal&lt;/code>&lt;/li>
&lt;li>配置基础网络、更新源、SSH登录等&lt;/li>
&lt;/ul>
&lt;h3 id="2在每个节点安装依赖工具">
 2.在每个节点安装依赖工具
 &lt;a class="anchor" href="#2%e5%9c%a8%e6%af%8f%e4%b8%aa%e8%8a%82%e7%82%b9%e5%ae%89%e8%a3%85%e4%be%9d%e8%b5%96%e5%b7%a5%e5%85%b7">#&lt;/a>
&lt;/h3>
&lt;p>推荐使用ansible in docker 容器化方式运行，无需安装额外依赖。&lt;/p>
&lt;h3 id="3准备ssh免密登陆">
 3.准备ssh免密登陆
 &lt;a class="anchor" href="#3%e5%87%86%e5%a4%87ssh%e5%85%8d%e5%af%86%e7%99%bb%e9%99%86">#&lt;/a>
&lt;/h3>
&lt;p>配置从部署节点能够ssh免密登陆所有节点，并且设置python软连接&lt;/p></description></item><item><title>2024-12-07 microk8s</title><link>https://qq547475331.github.io/docs/2024-12-07-microk8s/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2024-12-07-microk8s/</guid><description>&lt;p>在mutipass中启动ubuntu虚拟机&lt;/p>
&lt;pre tabindex="0">&lt;code>multipass shell microk8s-vm
&lt;/code>&lt;/pre>&lt;p>&lt;strong>MicroK8s&lt;/strong> 是一个由 &lt;strong>Canonical&lt;/strong>（Ubuntu 的开发商）维护的轻量级 Kubernetes 发行版。它旨在为开发者、运维人员和边缘计算场景提供一个快速、简便的 Kubernetes 部署方式，适用于本地开发、测试环境、边缘设备以及生产场景。&lt;/p>
&lt;hr>
&lt;h3 id="microk8s-的特点">
 &lt;strong>MicroK8s 的特点&lt;/strong>
 &lt;a class="anchor" href="#microk8s-%e7%9a%84%e7%89%b9%e7%82%b9">#&lt;/a>
&lt;/h3>
&lt;ol>
&lt;li>&lt;strong>轻量级&lt;/strong>
&lt;ul>
&lt;li>微内核设计，占用资源少，适合运行在笔记本电脑、工作站、树莓派或其他资源受限的设备上。&lt;/li>
&lt;li>默认禁用许多附加组件，只有基本的 Kubernetes 核心功能。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>易安装&lt;/strong>
&lt;ul>
&lt;li>提供单命令安装，尤其在 Ubuntu 系统上，使用 &lt;code>snap&lt;/code> 包管理器即可快速安装。&lt;/li>
&lt;li>支持多种操作系统，包括 Linux（Ubuntu、CentOS 等）、Windows 和 macOS（通过虚拟化工具）。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>模块化&lt;/strong>
&lt;ul>
&lt;li>提供了多种可选的附加组件（addons），如 &lt;code>dns&lt;/code>、&lt;code>ingress&lt;/code>、&lt;code>storage&lt;/code>、&lt;code>helm&lt;/code> 等，用户可以根据需要启用或禁用这些组件。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>开箱即用&lt;/strong>
&lt;ul>
&lt;li>直接内置了 Kubernetes 集群的核心组件，如 API Server、kubelet 和调度器，用户无需额外配置即可开始使用。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>自动升级&lt;/strong>
&lt;ul>
&lt;li>使用 &lt;code>snap&lt;/code> 管理，支持自动更新到最新版本。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>跨平台&lt;/strong>
&lt;ul>
&lt;li>支持多种硬件架构（如 x86、ARM），适合开发、测试和物联网边缘计算场景。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>兼容性&lt;/strong>
&lt;ul>
&lt;li>完全兼容 Kubernetes API，可以运行大多数 Kubernetes 应用程序。&lt;/li>
&lt;li>支持与 kubectl、Helm 等工具集成。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;hr>
&lt;h3 id="microk8s-的用途">
 &lt;strong>MicroK8s 的用途&lt;/strong>
 &lt;a class="anchor" href="#microk8s-%e7%9a%84%e7%94%a8%e9%80%94">#&lt;/a>
&lt;/h3>
&lt;ol>
&lt;li>&lt;strong>本地开发与测试&lt;/strong>
&lt;ul>
&lt;li>开发人员可以在本地快速启动一个 Kubernetes 集群，用于应用开发、测试和调试。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>学习 Kubernetes&lt;/strong>
&lt;ul>
&lt;li>对于新手来说，MicroK8s 是学习 Kubernetes 的理想工具，因为它简单易用且资源需求低。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>边缘计算&lt;/strong>
&lt;ul>
&lt;li>MicroK8s 轻量级的特性使其适合运行在物联网设备或资源有限的边缘设备上。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>CI/CD 测试环境&lt;/strong>
&lt;ul>
&lt;li>用于快速创建临时 Kubernetes 环境以运行持续集成或自动化测试。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>单节点或小型生产环境&lt;/strong>
&lt;ul>
&lt;li>在资源有限的情况下，可以作为单节点 Kubernetes 集群使用。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;hr>
&lt;h3 id="microk8s-的安装与使用">
 &lt;strong>MicroK8s 的安装与使用&lt;/strong>
 &lt;a class="anchor" href="#microk8s-%e7%9a%84%e5%ae%89%e8%a3%85%e4%b8%8e%e4%bd%bf%e7%94%a8">#&lt;/a>
&lt;/h3>
&lt;h4 id="安装-microk8s">
 &lt;strong>安装 MicroK8s&lt;/strong>
 &lt;a class="anchor" href="#%e5%ae%89%e8%a3%85-microk8s">#&lt;/a>
&lt;/h4>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>在 Ubuntu 上安装&lt;/strong> 使用 &lt;code>snap&lt;/code> 进行安装：&lt;/p></description></item><item><title>2024-12-08 devstack</title><link>https://qq547475331.github.io/docs/2024-12-08-devstack/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2024-12-08-devstack/</guid><description>&lt;h1 id="devstack">
 DevStack
 &lt;a class="anchor" href="#devstack">#&lt;/a>
&lt;/h1>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/logo-blue.png" alt="_images/徽标-蓝色.png" />&lt;/p>
&lt;p>DevStack 是一系列可扩展的脚本，用于根据来自 git master 的所有内容的最新版本快速构建完整的 OpenStack 环境。它可交互地用作开发环境，并作为 OpenStack 项目大部分功能测试的基础。&lt;/p>
&lt;p>源代码位于https://opendev.org/openstack/devstack。&lt;/p>
&lt;p>警告&lt;/p>
&lt;p>DevStack 将在安装过程中对您的系统进行重大更改。仅在专用于此目的的服务器或虚拟机上运行 DevStack。&lt;/p>
&lt;h2 id="快速入门">
 快速入门
 &lt;a class="anchor" href="#%e5%bf%ab%e9%80%9f%e5%85%a5%e9%97%a8">#&lt;/a>
&lt;/h2>
&lt;h3 id="安装-linux">
 安装 Linux
 &lt;a class="anchor" href="#%e5%ae%89%e8%a3%85-linux">#&lt;/a>
&lt;/h3>
&lt;p>从干净且最小限度的 Linux 系统安装开始。DevStack 尝试支持 Ubuntu 的两个最新 LTS 版本：Rocky Linux 9 和 openEuler。&lt;/p>
&lt;p>如果您没有偏好，Ubuntu 22.04（Jammy）是经过最多测试的，并且可能会运行得最顺利。&lt;/p>
&lt;h3 id="添加-stack-用户可选">
 添加 Stack 用户（可选）
 &lt;a class="anchor" href="#%e6%b7%bb%e5%8a%a0-stack-%e7%94%a8%e6%88%b7%e5%8f%af%e9%80%89">#&lt;/a>
&lt;/h3>
&lt;p>DevStack 应该以非 root 用户身份运行，并启用 sudo（通常使用“ubuntu”或“cloud-user”等云镜像的标准登录就可以了）。&lt;/p>
&lt;p>如果你不使用云镜像，你可以创建一个单独的堆栈用户来运行 DevStack&lt;/p>
&lt;pre tabindex="0">&lt;code>$ sudo useradd -s /bin/bash -d /opt/stack -m stack
&lt;/code>&lt;/pre>&lt;p>确保用户的主目录&lt;code>stack&lt;/code>对所有人都具有可执行权限，因为基于 RHEL 的发行版会创建它&lt;code>700&lt;/code>，而 Ubuntu 21.04+&lt;code>750&lt;/code> 可能会在部署期间导致问题。&lt;/p>
&lt;pre tabindex="0">&lt;code>$ sudo chmod +x /opt/stack
&lt;/code>&lt;/pre>&lt;p>由于该用户将对您的系统进行许多更改，因此它应该具有 sudo 权限：&lt;/p></description></item><item><title>2024-12-08 mutilpass操作</title><link>https://qq547475331.github.io/docs/2024-12-08-mutilpass%E6%93%8D%E4%BD%9C/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2024-12-08-mutilpass%E6%93%8D%E4%BD%9C/</guid><description>&lt;p>如果你想销毁一个已经创建的 &lt;code>multipass&lt;/code> 虚拟机（例如 &lt;code>openstack-vm&lt;/code>），可以按照以下步骤操作：&lt;/p>
&lt;h3 id="1-停止虚拟机">
 1. &lt;strong>停止虚拟机&lt;/strong>
 &lt;a class="anchor" href="#1-%e5%81%9c%e6%ad%a2%e8%99%9a%e6%8b%9f%e6%9c%ba">#&lt;/a>
&lt;/h3>
&lt;p>首先，停止虚拟机：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>multipass stop openstack-vm
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>这将停止虚拟机，但不会删除它。如果你只想关闭虚拟机，可以跳过删除步骤。&lt;/p>
&lt;h3 id="2-删除虚拟机">
 2. &lt;strong>删除虚拟机&lt;/strong>
 &lt;a class="anchor" href="#2-%e5%88%a0%e9%99%a4%e8%99%9a%e6%8b%9f%e6%9c%ba">#&lt;/a>
&lt;/h3>
&lt;p>要彻底销毁虚拟机并删除它的所有相关文件，你需要使用 &lt;code>multipass delete&lt;/code> 命令：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>multipass delete openstack-vm
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>这将删除虚拟机，但是文件系统和相关数据可能会被保留。&lt;/p>
&lt;h3 id="3-清理虚拟机数据">
 3. &lt;strong>清理虚拟机数据&lt;/strong>
 &lt;a class="anchor" href="#3-%e6%b8%85%e7%90%86%e8%99%9a%e6%8b%9f%e6%9c%ba%e6%95%b0%e6%8d%ae">#&lt;/a>
&lt;/h3>
&lt;p>如果你希望完全清除虚拟机及其所有相关数据（包括磁盘映像、配置文件等），可以使用 &lt;code>multipass purge&lt;/code> 命令：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>multipass purge
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>这将清除所有已删除虚拟机的所有磁盘和缓存数据。&lt;/p>
&lt;h3 id="完整的操作步骤">
 完整的操作步骤
 &lt;a class="anchor" href="#%e5%ae%8c%e6%95%b4%e7%9a%84%e6%93%8d%e4%bd%9c%e6%ad%a5%e9%aa%a4">#&lt;/a>
&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>停止虚拟机&lt;/strong>：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>multipass stop openstack-vm
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>&lt;strong>删除虚拟机&lt;/strong>：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>multipass delete openstack-vm
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>&lt;strong>清理磁盘和缓存数据&lt;/strong>：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>multipass purge
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ol>
&lt;h3 id="总结">
 总结
 &lt;a class="anchor" href="#%e6%80%bb%e7%bb%93">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>&lt;code>multipass stop &amp;lt;vm-name&amp;gt;&lt;/code>：停止虚拟机。&lt;/li>
&lt;li>&lt;code>multipass delete &amp;lt;vm-name&amp;gt;&lt;/code>：删除虚拟机。&lt;/li>
&lt;li>&lt;code>multipass purge&lt;/code>：清除删除的虚拟机的所有相关数据。&lt;/li>
&lt;/ul>
&lt;p>这些步骤将帮助你完全销毁虚拟机。如果你需要重新创建虚拟机，可以再次使用 &lt;code>multipass launch&lt;/code> 命令进行创建。&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/image-20241208082002247.png" alt="image-20241208082002247" />&lt;/p></description></item><item><title>2024-12-08 nano操作</title><link>https://qq547475331.github.io/docs/2024-12-08-nano%E6%93%8D%E4%BD%9C/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2024-12-08-nano%E6%93%8D%E4%BD%9C/</guid><description>&lt;p>&lt;code>nano&lt;/code> 是一个在 Unix-like 系统中非常流行的文本编辑器，适合新手和快速编辑配置文件。它是一个基于命令行的编辑器，功能简洁且易于使用。下面是一些基本的 &lt;code>nano&lt;/code> 使用方法：&lt;/p>
&lt;h3 id="1-打开文件">
 1. &lt;strong>打开文件&lt;/strong>
 &lt;a class="anchor" href="#1-%e6%89%93%e5%bc%80%e6%96%87%e4%bb%b6">#&lt;/a>
&lt;/h3>
&lt;p>要编辑一个文件，可以在命令行中输入：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>nano &amp;lt;文件路径&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>如果文件不存在，&lt;code>nano&lt;/code> 会创建一个新的文件。例如，要编辑 &lt;code>local.conf&lt;/code> 文件，可以使用：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>nano local.conf
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="2-编辑文件">
 2. &lt;strong>编辑文件&lt;/strong>
 &lt;a class="anchor" href="#2-%e7%bc%96%e8%be%91%e6%96%87%e4%bb%b6">#&lt;/a>
&lt;/h3>
&lt;p>进入 &lt;code>nano&lt;/code> 编辑器后，你可以直接开始输入文本。光标可以用方向键移动，编辑内容就像在普通的文本编辑器中一样。&lt;/p>
&lt;h3 id="3-保存文件">
 3. &lt;strong>保存文件&lt;/strong>
 &lt;a class="anchor" href="#3-%e4%bf%9d%e5%ad%98%e6%96%87%e4%bb%b6">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>按下 &lt;strong>&lt;code>Ctrl + O&lt;/code>&lt;/strong>（即按住 &lt;code>Ctrl&lt;/code> 键，然后按 &lt;code>O&lt;/code> 键）保存当前文件。屏幕底部会显示 &lt;code>File Name to Write: &amp;lt;文件路径&amp;gt;&lt;/code>，表示文件已准备好保存。&lt;/li>
&lt;li>按 &lt;strong>Enter&lt;/strong> 键确认保存文件。&lt;/li>
&lt;/ul>
&lt;h3 id="4-退出-nano">
 4. &lt;strong>退出 nano&lt;/strong>
 &lt;a class="anchor" href="#4-%e9%80%80%e5%87%ba-nano">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>按&lt;/p>
&lt;p>&lt;code>Ctrl + X&lt;/code>&lt;/p>
&lt;p>退出&lt;/p>
&lt;pre tabindex="0">&lt;code>nano
&lt;/code>&lt;/pre>&lt;p>。如果你在退出前修改了文件，&lt;/p>
&lt;pre tabindex="0">&lt;code>nano
&lt;/code>&lt;/pre>&lt;p>会提示你是否保存更改：&lt;/p>
&lt;ul>
&lt;li>如果想保存，按 &lt;strong>Y&lt;/strong>（Yes），然后按 &lt;strong>Enter&lt;/strong>。&lt;/li>
&lt;li>如果不想保存更改，按 &lt;strong>N&lt;/strong>（No）。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="5-基本命令">
 5. &lt;strong>基本命令&lt;/strong>
 &lt;a class="anchor" href="#5-%e5%9f%ba%e6%9c%ac%e5%91%bd%e4%bb%a4">#&lt;/a>
&lt;/h3>
&lt;p>在 &lt;code>nano&lt;/code> 中，很多操作是通过 &lt;strong>Ctrl + 字母键&lt;/strong> 来完成的。下面是一些常见的快捷键：&lt;/p></description></item><item><title>2024-12-08 openstack和kubernetes区别</title><link>https://qq547475331.github.io/docs/2024-12-08-openstack%E5%92%8Ckubernetes%E5%8C%BA%E5%88%AB/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2024-12-08-openstack%E5%92%8Ckubernetes%E5%8C%BA%E5%88%AB/</guid><description>&lt;p>&lt;strong>OpenStack&lt;/strong> 和 &lt;strong>Kubernetes&lt;/strong> 都是现代云计算和容器化技术中的重要组件，但它们关注的领域和解决的问题有所不同。下面是对两者关系的详细解释。&lt;/p>
&lt;h3 id="1-openstack-是什么">
 1. &lt;strong>OpenStack 是什么？&lt;/strong>
 &lt;a class="anchor" href="#1-openstack-%e6%98%af%e4%bb%80%e4%b9%88">#&lt;/a>
&lt;/h3>
&lt;p>&lt;strong>OpenStack&lt;/strong> 是一个开源的云计算平台，旨在提供 IaaS（基础设施即服务），主要用于构建公共或私有云环境。它由一系列的模块组成，用于管理计算、存储、网络、安全等资源。&lt;/p>
&lt;h4 id="openstack-的核心组件">
 OpenStack 的核心组件：
 &lt;a class="anchor" href="#openstack-%e7%9a%84%e6%a0%b8%e5%bf%83%e7%bb%84%e4%bb%b6">#&lt;/a>
&lt;/h4>
&lt;ul>
&lt;li>&lt;strong>Nova&lt;/strong>：计算服务，负责虚拟机（VM）的管理和调度。&lt;/li>
&lt;li>&lt;strong>Neutron&lt;/strong>：网络服务，提供虚拟网络、子网、路由等功能。&lt;/li>
&lt;li>&lt;strong>Cinder&lt;/strong>：块存储服务，用于管理磁盘存储。&lt;/li>
&lt;li>&lt;strong>Glance&lt;/strong>：镜像服务，提供虚拟机镜像的存储和管理。&lt;/li>
&lt;li>&lt;strong>Keystone&lt;/strong>：身份认证服务，管理用户和权限。&lt;/li>
&lt;li>&lt;strong>Horizon&lt;/strong>：Web 控制面板，用于管理 OpenStack 环境。&lt;/li>
&lt;/ul>
&lt;p>OpenStack 主要面向虚拟化资源管理（主要是虚拟机），用于提供基础设施服务，支持创建、管理和销毁虚拟机、虚拟存储和虚拟网络。&lt;/p>
&lt;h3 id="2-kubernetes-是什么">
 2. &lt;strong>Kubernetes 是什么？&lt;/strong>
 &lt;a class="anchor" href="#2-kubernetes-%e6%98%af%e4%bb%80%e4%b9%88">#&lt;/a>
&lt;/h3>
&lt;p>&lt;strong>Kubernetes&lt;/strong> 是一个开源的容器编排平台，用于自动化容器的部署、扩展和管理。它解决了容器化应用程序在大规模运行中的编排问题，特别是多容器环境的管理。&lt;/p>
&lt;h4 id="kubernetes-的核心功能">
 Kubernetes 的核心功能：
 &lt;a class="anchor" href="#kubernetes-%e7%9a%84%e6%a0%b8%e5%bf%83%e5%8a%9f%e8%83%bd">#&lt;/a>
&lt;/h4>
&lt;ul>
&lt;li>&lt;strong>Pod&lt;/strong>：Kubernetes 的最小部署单元，包含一个或多个容器。&lt;/li>
&lt;li>&lt;strong>Service&lt;/strong>：定义了容器之间如何通信，并为容器提供一个稳定的网络入口。&lt;/li>
&lt;li>&lt;strong>Deployment&lt;/strong>：管理应用程序的副本和更新。&lt;/li>
&lt;li>&lt;strong>Node&lt;/strong>：运行容器的工作节点。&lt;/li>
&lt;li>&lt;strong>ReplicaSet&lt;/strong>：确保某个特定数量的 Pod 副本在任何时间点都在运行。&lt;/li>
&lt;li>&lt;strong>ConfigMap/Secret&lt;/strong>：管理应用的配置和敏感信息。&lt;/li>
&lt;/ul>
&lt;p>Kubernetes 主要面向 &lt;strong>容器化&lt;/strong> 的应用部署和管理，尤其适用于微服务架构和分布式应用程序。&lt;/p>
&lt;h3 id="3-openstack-和-kubernetes-的关系">
 3. &lt;strong>OpenStack 和 Kubernetes 的关系&lt;/strong>
 &lt;a class="anchor" href="#3-openstack-%e5%92%8c-kubernetes-%e7%9a%84%e5%85%b3%e7%b3%bb">#&lt;/a>
&lt;/h3>
&lt;p>OpenStack 和 Kubernetes 解决的是不同层次的问题，但它们也可以结合使用，为云环境提供更强大的功能。&lt;/p>
&lt;h4 id="openstack-提供基础设施kubernetes-提供容器编排">
 &lt;strong>OpenStack 提供基础设施，Kubernetes 提供容器编排&lt;/strong>：
 &lt;a class="anchor" href="#openstack-%e6%8f%90%e4%be%9b%e5%9f%ba%e7%a1%80%e8%ae%be%e6%96%bdkubernetes-%e6%8f%90%e4%be%9b%e5%ae%b9%e5%99%a8%e7%bc%96%e6%8e%92">#&lt;/a>
&lt;/h4>
&lt;ul>
&lt;li>&lt;strong>OpenStack&lt;/strong> 提供了虚拟机（VM）、存储、网络等基础设施层面的资源，可以被 Kubernetes 用来运行和管理容器化应用。&lt;/li>
&lt;li>&lt;strong>Kubernetes&lt;/strong> 运行在 OpenStack 提供的计算资源（虚拟机）上，并管理在这些资源上运行的容器化应用。&lt;/li>
&lt;/ul>
&lt;p>可以把 OpenStack 看作是为容器提供运行环境的基础设施层，而 Kubernetes 则是在此基础上提供容器编排和应用管理的功能。&lt;/p></description></item><item><title>2024-12-08 openstack需要几台虚拟机</title><link>https://qq547475331.github.io/docs/2024-12-08-openstack%E9%9C%80%E8%A6%81%E5%87%A0%E5%8F%B0%E8%99%9A%E6%8B%9F%E6%9C%BA/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2024-12-08-openstack%E9%9C%80%E8%A6%81%E5%87%A0%E5%8F%B0%E8%99%9A%E6%8B%9F%E6%9C%BA/</guid><description>&lt;p>使用 &lt;strong>OpenStack Ansible&lt;/strong> 部署 OpenStack 时，通常会按照不同的角色分配到多台虚拟机上，具体的虚拟机数量和分配方式取决于你选择的部署规模和设计架构。对于小规模的测试和实验环境，可以在一台虚拟机上部署所有组件，而对于生产环境，通常会将 OpenStack 的各个组件分布到不同的虚拟机上，以确保更好的性能和高可用性。&lt;/p>
&lt;p>以下是部署 OpenStack 时可能需要的虚拟机和每台虚拟机的角色说明：&lt;/p>
&lt;h3 id="1-控制节点controller-node">
 1. &lt;strong>控制节点（Controller Node）&lt;/strong>
 &lt;a class="anchor" href="#1-%e6%8e%a7%e5%88%b6%e8%8a%82%e7%82%b9controller-node">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>数量&lt;/strong>：1-2 台&lt;/p>
&lt;/li>
&lt;li>
&lt;p>作用&lt;/p>
&lt;p>：&lt;/p>
&lt;ul>
&lt;li>控制节点负责管理 OpenStack 环境的所有核心服务，包括身份认证（Keystone）、图像服务（Glance）、计算服务（Nova）、网络服务（Neutron）、块存储服务（Cinder）、仪表板（Horizon）等。&lt;/li>
&lt;li>控制节点通常部署 OpenStack 的 &lt;strong>管理服务&lt;/strong>，这些服务需要处理来自计算节点和存储节点的请求。&lt;/li>
&lt;li>需要更多的 CPU 和内存资源以处理控制和管理任务，尤其是数据库（如 MySQL）、消息队列（如 RabbitMQ）等需要在控制节点上运行。&lt;/li>
&lt;li>对于生产环境，建议部署两个控制节点以提供高可用性，确保如果一个控制节点失败，另一个节点可以接管管理任务。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="2-计算节点compute-node">
 2. &lt;strong>计算节点（Compute Node）&lt;/strong>
 &lt;a class="anchor" href="#2-%e8%ae%a1%e7%ae%97%e8%8a%82%e7%82%b9compute-node">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>数量&lt;/strong>：至少 1 台（根据需要扩展）&lt;/p>
&lt;/li>
&lt;li>
&lt;p>作用&lt;/p>
&lt;p>：&lt;/p>
&lt;ul>
&lt;li>计算节点负责运行虚拟机实例，提供计算资源。计算节点上会运行 Nova 计算服务，它是 OpenStack 中管理虚拟机生命周期的核心组件。&lt;/li>
&lt;li>计算节点需要安装 KVM（或其他虚拟化技术）来运行虚拟机，并且需要充足的 CPU、内存和存储资源。&lt;/li>
&lt;li>对于生产环境，通常会有多个计算节点，以分散虚拟机负载并提高容错能力。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="3-网络节点network-node">
 3. &lt;strong>网络节点（Network Node）&lt;/strong>
 &lt;a class="anchor" href="#3-%e7%bd%91%e7%bb%9c%e8%8a%82%e7%82%b9network-node">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>数量&lt;/strong>：1 台（可选，取决于网络需求）&lt;/p>
&lt;/li>
&lt;li>
&lt;p>作用&lt;/p>
&lt;p>：&lt;/p>
&lt;ul>
&lt;li>网络节点负责处理 OpenStack 环境中的网络服务，通常会运行 Neutron 网络服务的关键组件（如 L3 路由器、DHCP、VPN 等）。&lt;/li>
&lt;li>如果你需要使用 Neutron 的高级网络功能（如 VLAN、VXLAN、SDN 等），则建议单独设置网络节点。&lt;/li>
&lt;li>在较小的环境中，网络服务通常可以与控制节点合并，但对于大规模部署，最好将网络服务独立出来。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="4-存储节点storage-node">
 4. &lt;strong>存储节点（Storage Node）&lt;/strong>
 &lt;a class="anchor" href="#4-%e5%ad%98%e5%82%a8%e8%8a%82%e7%82%b9storage-node">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>数量&lt;/strong>：1-2 台（根据存储需求）&lt;/p></description></item><item><title>2024-12-08 块存储和对象储存区别</title><link>https://qq547475331.github.io/docs/2024-12-08-%E5%9D%97%E5%AD%98%E5%82%A8%E5%92%8C%E5%AF%B9%E8%B1%A1%E5%82%A8%E5%AD%98%E5%8C%BA%E5%88%AB/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2024-12-08-%E5%9D%97%E5%AD%98%E5%82%A8%E5%92%8C%E5%AF%B9%E8%B1%A1%E5%82%A8%E5%AD%98%E5%8C%BA%E5%88%AB/</guid><description>&lt;p>&lt;strong>块存储&lt;/strong>和&lt;strong>对象存储&lt;/strong>是两种常见的存储方式，它们在存储模型、使用场景、性能和管理方式等方面存在显著差异。理解这两者的区别，有助于选择适合的存储解决方案来满足特定应用和业务需求。&lt;/p>
&lt;h3 id="1-存储模型的区别">
 1. &lt;strong>存储模型的区别&lt;/strong>
 &lt;a class="anchor" href="#1-%e5%ad%98%e5%82%a8%e6%a8%a1%e5%9e%8b%e7%9a%84%e5%8c%ba%e5%88%ab">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>块存储（Block Storage）&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>数据存储方式&lt;/strong>：块存储将数据分割为固定大小的块（通常为 512 字节或 4 KB）。每个块都有一个唯一的地址，计算机可以单独访问这些块。&lt;/li>
&lt;li>&lt;strong>如何工作&lt;/strong>：块存储像硬盘一样提供一个裸存储卷，应用程序和操作系统可以直接访问这些块。通常用作操作系统的磁盘、数据库存储等。&lt;/li>
&lt;li>&lt;strong>使用方式&lt;/strong>：通常用于需要高速访问和低延迟的应用场景，如数据库、高性能应用和虚拟机存储等。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>对象存储（Object Storage）&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>数据存储方式&lt;/strong>：对象存储是将数据以“对象”的形式存储，每个对象由数据、元数据和唯一的标识符（对象ID）组成。对象存储是一个无层次、扁平的存储系统，不像文件系统那样有目录结构。&lt;/li>
&lt;li>&lt;strong>如何工作&lt;/strong>：对象存储通过 RESTful API（如 Amazon S3 API）来访问数据，数据以对象形式存储并通过唯一的 URL 进行检索。每个对象通常是一个文件（如图片、视频、文档等），可以是任意大小。&lt;/li>
&lt;li>&lt;strong>使用方式&lt;/strong>：适用于大量的非结构化数据存储，如备份、日志存储、大数据存储、网站文件存储等。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="2-性能和可扩展性">
 2. &lt;strong>性能和可扩展性&lt;/strong>
 &lt;a class="anchor" href="#2-%e6%80%a7%e8%83%bd%e5%92%8c%e5%8f%af%e6%89%a9%e5%b1%95%e6%80%a7">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>块存储&lt;/p>
&lt;p>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>性能&lt;/strong>：块存储通常提供较低的延迟和较高的吞吐量，因为它直接与操作系统交互，适合需要频繁、快速读取和写入操作的应用（如数据库、高性能计算、虚拟机等）。&lt;/li>
&lt;li>&lt;strong>可扩展性&lt;/strong>：扩展块存储通常需要通过增加磁盘或者在存储阵列中增加更多的存储单元来实现。扩展时可能需要手动干预或配置。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>对象存储&lt;/p>
&lt;p>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>性能&lt;/strong>：对象存储的性能通常不如块存储，因为它是基于 HTTP 协议的 RESTful API 来进行访问，因此延迟会相对较高。不过，适合存储大规模的数据并进行低频访问（如备份、归档等）。&lt;/li>
&lt;li>&lt;strong>可扩展性&lt;/strong>：对象存储天然具备高可扩展性，能够轻松处理 PB 级别的存储需求。它通过分布式架构和数据冗余技术，自动在多个节点之间扩展存储空间，具有更好的横向扩展性。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="3-数据访问方式">
 3. &lt;strong>数据访问方式&lt;/strong>
 &lt;a class="anchor" href="#3-%e6%95%b0%e6%8d%ae%e8%ae%bf%e9%97%ae%e6%96%b9%e5%bc%8f">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>块存储&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>访问方式&lt;/strong>：可以通过操作系统的文件系统直接访问和管理块存储，支持传统的文件系统结构（如 NTFS、EXT4、XFS 等），操作系统对块存储提供低级的读写操作。&lt;/li>
&lt;li>&lt;strong>适用场景&lt;/strong>：适合对存储进行频繁随机读写的应用，比如数据库、虚拟化环境中的虚拟机磁盘等。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>对象存储&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>访问方式&lt;/strong>：对象存储通过 API（如 RESTful API）进行访问，支持通过 Web 请求获取对象数据。数据不需要通过操作系统文件系统进行访问，而是通过唯一的 URL 地址或对象键来检索。&lt;/li>
&lt;li>&lt;strong>适用场景&lt;/strong>：适合存储大量非结构化数据，如视频、图片、音频、备份数据、日志文件、静态网页内容等。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="4-管理和操作">
 4. &lt;strong>管理和操作&lt;/strong>
 &lt;a class="anchor" href="#4-%e7%ae%a1%e7%90%86%e5%92%8c%e6%93%8d%e4%bd%9c">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>块存储&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>管理方式&lt;/strong>：块存储的管理通常更复杂，特别是在大规模部署时。你需要分配、格式化和挂载每个磁盘，进行数据备份、恢复等操作。&lt;/li>
&lt;li>&lt;strong>优点&lt;/strong>：高性能，支持对文件系统、数据库等的直接管理。&lt;/li>
&lt;li>&lt;strong>缺点&lt;/strong>：管理开销较大，扩展时可能较为复杂，需要手动干预或配置。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>对象存储&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>管理方式&lt;/strong>：对象存储的管理相对简单，存储的对象通过唯一标识符（如对象ID）进行访问，自动管理冗余和扩展。通过简单的 API 和 Web 控制面板即可完成操作。&lt;/li>
&lt;li>&lt;strong>优点&lt;/strong>：易于扩展、管理和访问，适合存储大规模、非结构化的数据。&lt;/li>
&lt;li>&lt;strong>缺点&lt;/strong>：不适合低延迟或高频率随机读写操作，因为访问是基于 HTTP 协议，且没有传统文件系统的目录结构。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="5-常见使用场景">
 5. &lt;strong>常见使用场景&lt;/strong>
 &lt;a class="anchor" href="#5-%e5%b8%b8%e8%a7%81%e4%bd%bf%e7%94%a8%e5%9c%ba%e6%99%af">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>块存储&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>数据库存储&lt;/strong>：块存储适用于需要高 I/O 性能的数据库应用，如关系型数据库（MySQL、PostgreSQL）或 NoSQL 数据库（MongoDB、Cassandra）。&lt;/li>
&lt;li>&lt;strong>虚拟化&lt;/strong>：虚拟机存储虚拟硬盘（如 VMware 或 KVM）使用块存储。&lt;/li>
&lt;li>&lt;strong>高性能应用&lt;/strong>：需要快速数据访问的高性能计算（HPC）应用、交易系统等。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>对象存储&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>大数据存储&lt;/strong>：存储海量的非结构化数据，如视频、音频、日志文件、图像等。&lt;/li>
&lt;li>&lt;strong>备份与归档&lt;/strong>：适合大规模备份和长期存储，很多公司将对象存储用于云备份解决方案。&lt;/li>
&lt;li>&lt;strong>内容分发网络（CDN）&lt;/strong>：将静态网站内容、视频流等分发到全球各地的节点，利用对象存储优化分发速度。&lt;/li>
&lt;li>&lt;strong>静态网页存储&lt;/strong>：存储 HTML 文件、图片和其他静态资源。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="6-冗余与容错机制">
 6. &lt;strong>冗余与容错机制&lt;/strong>
 &lt;a class="anchor" href="#6-%e5%86%97%e4%bd%99%e4%b8%8e%e5%ae%b9%e9%94%99%e6%9c%ba%e5%88%b6">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>块存储&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>冗余&lt;/strong>：块存储的冗余通常由存储设备（如 SAN、NAS）提供，可以使用 RAID 等技术确保数据的高可用性和冗余。&lt;/li>
&lt;li>&lt;strong>容错机制&lt;/strong>：块存储支持快速备份和恢复，但如果单台硬盘或存储设备出现故障，可能会影响存储的可用性。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>对象存储&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>冗余&lt;/strong>：对象存储通常内建数据冗余机制，像 AWS S3 等平台自动将数据复制到多个数据中心或节点，以确保数据的持久性和可用性。&lt;/li>
&lt;li>&lt;strong>容错机制&lt;/strong>：对象存储的高可用性和数据冗余通常通过分布式架构自动处理，具备更强的容错能力。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h3 id="7-总结">
 7. &lt;strong>总结&lt;/strong>
 &lt;a class="anchor" href="#7-%e6%80%bb%e7%bb%93">#&lt;/a>
&lt;/h3>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th>特性&lt;/th>
 &lt;th>块存储 (Block Storage)&lt;/th>
 &lt;th>对象存储 (Object Storage)&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td>&lt;strong>存储结构&lt;/strong>&lt;/td>
 &lt;td>数据被分割成固定大小的块&lt;/td>
 &lt;td>数据以对象（包括数据、元数据和ID）存储&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;strong>访问方式&lt;/strong>&lt;/td>
 &lt;td>通过操作系统文件系统直接访问&lt;/td>
 &lt;td>通过 API（如 RESTful API）访问&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;strong>性能&lt;/strong>&lt;/td>
 &lt;td>适合高性能、低延迟的应用&lt;/td>
 &lt;td>适合大规模、非结构化数据的存储和归档&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;strong>管理复杂性&lt;/strong>&lt;/td>
 &lt;td>需要手动管理和配置，管理较为复杂&lt;/td>
 &lt;td>管理简便，自动化较强&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;strong>扩展性&lt;/strong>&lt;/td>
 &lt;td>扩展较为复杂，可能需要人工干预&lt;/td>
 &lt;td>极高的扩展性，适合大规模存储&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;strong>典型应用场景&lt;/strong>&lt;/td>
 &lt;td>数据库、虚拟机、性能要求高的应用&lt;/td>
 &lt;td>大数据存储、备份、归档、静态内容存储&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;strong>冗余与容错&lt;/strong>&lt;/td>
 &lt;td>由存储设备（如 RAID）提供冗余&lt;/td>
 &lt;td>自动数据冗余和容错机制&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;p>总的来说，&lt;strong>块存储&lt;/strong>适用于需要频繁随机读写和高性能的应用，如数据库和虚拟机磁盘；而&lt;strong>对象存储&lt;/strong>适用于大规模的非结构化数据存储，如备份、归档、媒体内容分发等。&lt;/p></description></item><item><title>2024-12-09 docker daemon.json</title><link>https://qq547475331.github.io/docs/2024-12-09-docker-daemon.json/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2024-12-09-docker-daemon.json/</guid><description>&lt;p>在国内使用 Docker 的朋友们，可能都遇到过配置镜像源来加速镜像拉取的操作。然而，最近几个月发现许多曾经常用的国内镜像站（包括各种云服务商和高校镜像站）已经无法使用。因此，本人开始搜索并汇总了目前可用的镜像站和镜像加速地址，并计划定期测试它们的可用性，并更新这个列表。如果您知道新的可用站点，也欢迎随时补充哦！&lt;/p>
&lt;h1 id="docker-镜像加速列表20241202已更新">
 Docker 镜像加速列表（20241202已更新）
 &lt;a class="anchor" href="#docker-%e9%95%9c%e5%83%8f%e5%8a%a0%e9%80%9f%e5%88%97%e8%a1%a820241202%e5%b7%b2%e6%9b%b4%e6%96%b0">#&lt;/a>
&lt;/h1>
&lt;pre tabindex="0">&lt;code>cat &amp;gt; /etc/docker/daemon.json &amp;lt;&amp;lt; EOF
{
 &amp;#34;registry-mirrors&amp;#34;: [&amp;#34;https://docker.m.daocloud.io&amp;#34;,
 &amp;#34;https://hub.roker.org&amp;#34;,
 &amp;#34;https://si7y70hh.mirror.aliyuncs.com&amp;#34;,
 &amp;#34;https://registry.docker-cn.com&amp;#34;,
 &amp;#34;https://hub.geekery.cn&amp;#34;,
 &amp;#34;https://hub.littlediary.cn&amp;#34;,
 &amp;#34;https://docker.rainbond.cc&amp;#34;,
 &amp;#34;https://docker.unsee.tech&amp;#34;,
 &amp;#34;https://hub.crdz.gq&amp;#34;,
 &amp;#34;https://docker.nastool.de&amp;#34;,
 &amp;#34;https://hub.firefly.store&amp;#34;,
 &amp;#34;https://registry.dockermirror.com&amp;#34;,
 &amp;#34;https://docker.1panelproxy.com&amp;#34;,
 &amp;#34;https://rhub.rat.dev&amp;#34;,
 &amp;#34;https://docker.kejilion.pro&amp;#34;,
 &amp;#34;https://dhub.kubesre.xyz&amp;#34;,
 &amp;#34;https://docker.1panel.live&amp;#34;,
 &amp;#34;https://dockerpull.org&amp;#34;,
 &amp;#34;https://docker.udayun.com&amp;#34;,
 &amp;#34;https://docker.hlmirror.com&amp;#34;,
 &amp;#34;https://docker.mirrors.ustc.edu.cn&amp;#34;,
 &amp;#34;http://hub-mirror.c.163.com&amp;#34;,
 &amp;#34;https://dockerhub.azk8s.cn&amp;#34;,
 &amp;#34;https://hub.uuuadc.top&amp;#34;,
 &amp;#34;https://docker.anyhub.us.kg&amp;#34;,
 &amp;#34;https://dockerhub.jobcher.com&amp;#34;,
 &amp;#34;https://dockerhub.icu&amp;#34;,
 &amp;#34;https://cloudsx.top&amp;#34;,
 &amp;#34;https://docker.ckyl.me&amp;#34;,
 &amp;#34;https://docker.awsl9527.cn&amp;#34;,
 &amp;#34;https://docker.chenby.cn&amp;#34;,
 &amp;#34;https://docker.hpcloud.cloud&amp;#34;,
 &amp;#34;https://atomhub.openatom.cn&amp;#34;],
 &amp;#34;exec-opts&amp;#34;: [&amp;#34;native.cgroupdriver=systemd&amp;#34;],
 &amp;#34;insecure-registries&amp;#34;: [&amp;#34;http://192.168.200.250&amp;#34;],
 &amp;#34;log-driver&amp;#34;: &amp;#34;json-file&amp;#34;,
 &amp;#34;log-opts&amp;#34;: {
 &amp;#34;max-size&amp;#34;: &amp;#34;1000m&amp;#34;,&amp;#34;max-file&amp;#34;: &amp;#34;6&amp;#34;
 },
 &amp;#34;storage-driver&amp;#34;: &amp;#34;overlay2&amp;#34;
}
EOF
&lt;/code>&lt;/pre>&lt;blockquote>
&lt;p>请注意！有些镜像站仅提供基础镜像或白名单镜像，如果某个加速地址无法拉取到所需的镜像，可以尝试切换到其他地址。有些代理站点是热心网友自费搭建的，请务必合理使用。如果侵犯了您的权益，请随时联系我，我会及时删除相关信息。感谢您的理解与支持！&lt;/p>&lt;/blockquote>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th>DockerHub 镜像仓库&lt;/th>
 &lt;th>是否正常&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td>&lt;code>hub.geekery.cn&lt;/code>&lt;/td>
 &lt;td>正常&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;code>hub.littlediary.cn&lt;/code>&lt;/td>
 &lt;td>正常&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;code>docker.rainbond.cc&lt;/code>&lt;/td>
 &lt;td>正常&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;code>docker.unsee.tech&lt;/code>&lt;/td>
 &lt;td>正常&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;code>docker.m.daocloud.io&lt;/code>&lt;/td>
 &lt;td>正常&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;code>hub.crdz.gq&lt;/code>&lt;/td>
 &lt;td>正常&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;code>docker.nastool.de&lt;/code>&lt;/td>
 &lt;td>正常&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;code>hub.firefly.store&lt;/code>&lt;/td>
 &lt;td>正常&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;code>registry.dockermirror.com&lt;/code>&lt;/td>
 &lt;td>正常&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;code>docker.1panelproxy.com&lt;/code>&lt;/td>
 &lt;td>正常&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;code>hub.rat.dev&lt;/code>&lt;/td>
 &lt;td>正常&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;code>docker.udayun.com&lt;/code>&lt;/td>
 &lt;td>正常&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;code>docker.kejilion.pro&lt;/code>&lt;/td>
 &lt;td>正常&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;code>dhub.kubesre.xyz&lt;/code>&lt;/td>
 &lt;td>正常&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;code>docker.1panel.live&lt;/code>&lt;/td>
 &lt;td>正常&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;code>dockerpull.org&lt;/code>&lt;/td>
 &lt;td>正常&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;code>docker.hlmirror.com&lt;/code>&lt;/td>
 &lt;td>新增&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;code>hub.xdark.top&lt;/code>&lt;/td>
 &lt;td>失效&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;code>noohub.ru&lt;/code>&lt;/td>
 &lt;td>失效&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;code>freeno.xyz&lt;/code>&lt;/td>
 &lt;td>失效&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;code>docker-cf.registry.cyou&lt;/code>&lt;/td>
 &lt;td>失效&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;code>dockerpull.com&lt;/code>&lt;/td>
 &lt;td>失效&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;code>lynn520.xyz&lt;/code>&lt;/td>
 &lt;td>失效&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;code>hub.yuzuha.cc&lt;/code>&lt;/td>
 &lt;td>失效&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;code>ccr.ccs.tencentyun.com&lt;/code>&lt;/td>
 &lt;td>失效&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;code>docker.chenby.cn&lt;/code>&lt;/td>
 &lt;td>失效&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;code>docker.linkedbus.com&lt;/code>&lt;/td>
 &lt;td>失效&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;code>docker.hlyun.org&lt;/code>&lt;/td>
 &lt;td>失效&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;code>dockerproxy.cn&lt;/code>&lt;/td>
 &lt;td>失效&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;code>docker.registry.cyou&lt;/code>&lt;/td>
 &lt;td>失效&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;code>dockerproxy.com&lt;/code>&lt;/td>
 &lt;td>失效&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;code>ginger20240704.asia&lt;/code>&lt;/td>
 &lt;td>失效&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;code>hub.docker-ttc.xyz&lt;/code>&lt;/td>
 &lt;td>失效&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;code>docker.mrxn.net&lt;/code>&lt;/td>
 &lt;td>失效&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;code>docker.wget.at&lt;/code>&lt;/td>
 &lt;td>失效&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;code>dislabaiot.xyz&lt;/code>&lt;/td>
 &lt;td>失效&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;code>docker.hpcloud.cloud&lt;/code>&lt;/td>
 &lt;td>失效&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;code>atomhub.openatom.cn&lt;/code>&lt;/td>
 &lt;td>失效&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;code>docker.nat.tf&lt;/code>&lt;/td>
 &lt;td>失效&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;code>doublezonline.cloud&lt;/code>&lt;/td>
 &lt;td>失效&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;code>dockerproxy.github.io&lt;/code>&lt;/td>
 &lt;td>失效&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;h1 id="配置方式1临时使用">
 配置方式1：临时使用
 &lt;a class="anchor" href="#%e9%85%8d%e7%bd%ae%e6%96%b9%e5%bc%8f1%e4%b8%b4%e6%97%b6%e4%bd%bf%e7%94%a8">#&lt;/a>
&lt;/h1>
&lt;p>直接使用，直接拿镜像域名拼接上官方镜像名，例如要拉去镜像 &lt;code>istio/distroless&lt;/code>，可以用下面写法（不要带 &lt;code>https://&lt;/code>）&lt;/p></description></item><item><title>2024-12-09 helmchart 部署flask应用</title><link>https://qq547475331.github.io/docs/2024-12-09-helmchart-%E9%83%A8%E7%BD%B2flask%E5%BA%94%E7%94%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2024-12-09-helmchart-%E9%83%A8%E7%BD%B2flask%E5%BA%94%E7%94%A8/</guid><description>&lt;p>要使用 Helm 部署一个 Flask 应用，你需要进行几个步骤，主要包括：&lt;/p>
&lt;ol>
&lt;li>&lt;strong>创建一个 Flask 应用&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>编写 Dockerfile&lt;/strong> 来将 Flask 应用容器化。&lt;/li>
&lt;li>&lt;strong>推送容器镜像&lt;/strong> 到容器镜像仓库。&lt;/li>
&lt;li>&lt;strong>创建 Helm Chart&lt;/strong> 来描述如何部署应用。&lt;/li>
&lt;/ol>
&lt;h3 id="步骤-1-创建一个简单的-flask-应用">
 步骤 1: 创建一个简单的 Flask 应用
 &lt;a class="anchor" href="#%e6%ad%a5%e9%aa%a4-1-%e5%88%9b%e5%bb%ba%e4%b8%80%e4%b8%aa%e7%ae%80%e5%8d%95%e7%9a%84-flask-%e5%ba%94%e7%94%a8">#&lt;/a>
&lt;/h3>
&lt;p>首先，确保你已经创建了一个简单的 Flask 应用。例如：&lt;/p>
&lt;p>&lt;code>app.py&lt;/code>：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">from&lt;/span> flask &lt;span style="color:#f92672">import&lt;/span> Flask
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>app &lt;span style="color:#f92672">=&lt;/span> Flask(__name__)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">@app.route&lt;/span>(&lt;span style="color:#e6db74">&amp;#39;/&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">hello_world&lt;/span>():
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#e6db74">&amp;#34;Hello, World!&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">if&lt;/span> __name__ &lt;span style="color:#f92672">==&lt;/span> &lt;span style="color:#e6db74">&amp;#39;__main__&amp;#39;&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> app&lt;span style="color:#f92672">.&lt;/span>run(host&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;0.0.0.0&amp;#39;&lt;/span>, port&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">5000&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="步骤-2-编写-dockerfile">
 步骤 2: 编写 Dockerfile
 &lt;a class="anchor" href="#%e6%ad%a5%e9%aa%a4-2-%e7%bc%96%e5%86%99-dockerfile">#&lt;/a>
&lt;/h3>
&lt;p>然后，为你的 Flask 应用创建一个 Dockerfile，将 Flask 应用容器化。假设你将 Flask 应用保存为 &lt;code>app.py&lt;/code>。&lt;/p>
&lt;p>&lt;code>Dockerfile&lt;/code>：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-Dockerfile" data-lang="Dockerfile">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 使用官方 Python 镜像&lt;/span>&lt;span style="color:#960050;background-color:#1e0010">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#960050;background-color:#1e0010">&lt;/span>&lt;span style="color:#66d9ef">FROM&lt;/span>&lt;span style="color:#e6db74"> python:3.9-slim&lt;/span>&lt;span style="color:#960050;background-color:#1e0010">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#960050;background-color:#1e0010">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#960050;background-color:#1e0010">&lt;/span>&lt;span style="color:#75715e"># 设置工作目录&lt;/span>&lt;span style="color:#960050;background-color:#1e0010">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#960050;background-color:#1e0010">&lt;/span>&lt;span style="color:#66d9ef">WORKDIR&lt;/span>&lt;span style="color:#e6db74"> /app&lt;/span>&lt;span style="color:#960050;background-color:#1e0010">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#960050;background-color:#1e0010">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#960050;background-color:#1e0010">&lt;/span>&lt;span style="color:#75715e"># 安装 Flask&lt;/span>&lt;span style="color:#960050;background-color:#1e0010">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#960050;background-color:#1e0010">&lt;/span>&lt;span style="color:#66d9ef">COPY&lt;/span> requirements.txt /app/&lt;span style="color:#960050;background-color:#1e0010">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#960050;background-color:#1e0010">&lt;/span>&lt;span style="color:#66d9ef">RUN&lt;/span> pip install --no-cache-dir -r requirements.txt&lt;span style="color:#960050;background-color:#1e0010">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#960050;background-color:#1e0010">&lt;/span>&lt;span style="color:#66d9ef">RUN&lt;/span> pip install flask werkzeug&lt;span style="color:#f92672">==&lt;/span>1.0.1&lt;span style="color:#960050;background-color:#1e0010">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#960050;background-color:#1e0010">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#960050;background-color:#1e0010">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#960050;background-color:#1e0010">&lt;/span>&lt;span style="color:#75715e"># 复制 Flask 应用&lt;/span>&lt;span style="color:#960050;background-color:#1e0010">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#960050;background-color:#1e0010">&lt;/span>&lt;span style="color:#66d9ef">COPY&lt;/span> . /app&lt;span style="color:#960050;background-color:#1e0010">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#960050;background-color:#1e0010">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#960050;background-color:#1e0010">&lt;/span>&lt;span style="color:#75715e"># 运行 Flask 应用&lt;/span>&lt;span style="color:#960050;background-color:#1e0010">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#960050;background-color:#1e0010">&lt;/span>&lt;span style="color:#66d9ef">CMD&lt;/span> [&lt;span style="color:#e6db74">&amp;#34;python&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;app.py&amp;#34;&lt;/span>]&lt;span style="color:#960050;background-color:#1e0010">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>你需要创建 &lt;code>requirements.txt&lt;/code> 文件来列出依赖项：&lt;/p></description></item><item><title>2024-12-09 mutilpass部署openstack devstack形式</title><link>https://qq547475331.github.io/docs/2024-12-08-mutilpass%E9%83%A8%E7%BD%B2openstack/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2024-12-08-mutilpass%E9%83%A8%E7%BD%B2openstack/</guid><description>&lt;p>在 macOS 上通过 &lt;strong>Multipass&lt;/strong> 部署 OpenStack 测试环境是一个非常方便的方式，尤其是使用 &lt;strong>All-in-One&lt;/strong> 部署方式来简化环境配置。All-in-One 部署将 OpenStack 的所有核心组件（如 Nova、Keystone、Glance、Cinder、Neutron 等）部署在一台虚拟机上，适合用于测试和开发环境。&lt;/p>
&lt;h3 id="使用-multipass-部署-openstack">
 使用 Multipass 部署 OpenStack
 &lt;a class="anchor" href="#%e4%bd%bf%e7%94%a8-multipass-%e9%83%a8%e7%bd%b2-openstack">#&lt;/a>
&lt;/h3>
&lt;h4 id="1-准备工作">
 1. &lt;strong>准备工作：&lt;/strong>
 &lt;a class="anchor" href="#1-%e5%87%86%e5%a4%87%e5%b7%a5%e4%bd%9c">#&lt;/a>
&lt;/h4>
&lt;ul>
&lt;li>&lt;strong>安装 Multipass&lt;/strong>：确保你已经安装了 Multipass。你可以通过官网（&lt;a href="https://multipass.run/">Multipass 官方网站&lt;/a>）下载并安装。&lt;/li>
&lt;li>&lt;strong>安装 Ubuntu 虚拟机&lt;/strong>：创建一个 Ubuntu 虚拟机来部署 OpenStack。&lt;/li>
&lt;/ul>
&lt;h4 id="2-创建虚拟机">
 2. &lt;strong>创建虚拟机：&lt;/strong>
 &lt;a class="anchor" href="#2-%e5%88%9b%e5%bb%ba%e8%99%9a%e6%8b%9f%e6%9c%ba">#&lt;/a>
&lt;/h4>
&lt;p>使用 Multipass 创建 Ubuntu 虚拟机时，你可以指定虚拟机的资源配置（如 CPU 核数、内存和硬盘空间）。以下是创建虚拟机的命令：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>multipass launch --name openstack-vm --cpus &lt;span style="color:#ae81ff">4&lt;/span> --mem 8G --disk 40G 22.04
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>&lt;code>--name openstack-vm&lt;/code>：虚拟机的名称。&lt;/li>
&lt;li>&lt;code>--cpus 4&lt;/code>：分配 4 个 CPU 核心。&lt;/li>
&lt;li>&lt;code>--mem 8G&lt;/code>：分配 8GB 内存。&lt;/li>
&lt;li>&lt;code>--disk 40G&lt;/code>：分配 40GB 硬盘空间。&lt;/li>
&lt;li>&lt;code>20.04&lt;/code>：指定使用 Ubuntu 20.04 LTS 版本。&lt;/li>
&lt;/ul>
&lt;p>这将创建一台名为 &lt;code>openstack-vm&lt;/code> 的虚拟机，具有 4 核 CPU、8GB 内存和 40GB 硬盘空间。&lt;/p></description></item><item><title>2024-12-09 openstack ssh连接</title><link>https://qq547475331.github.io/docs/2024-12-09-openstack-ssh%E8%BF%9E%E6%8E%A5/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2024-12-09-openstack-ssh%E8%BF%9E%E6%8E%A5/</guid><description>&lt;h1 id="devstack-网络">
 DevStack 网络
 &lt;a class="anchor" href="#devstack-%e7%bd%91%e7%bb%9c">#&lt;/a>
&lt;/h1>
&lt;p>DevStack 体验的一个重要部分是默认为创建的来宾提供网络连接。这可能不是您特定测试环境的最佳选择，因此本文档尽力解释发生了什么。&lt;/p>
&lt;h2 id="默认值">
 默认值
 &lt;a class="anchor" href="#%e9%bb%98%e8%ae%a4%e5%80%bc">#&lt;/a>
&lt;/h2>
&lt;p>如果您不指定任何配置，您将获得以下内容：&lt;/p>
&lt;ul>
&lt;li>neutron（包括带有 openvswitch 的 l3）&lt;/li>
&lt;li>每个 openstack 项目的私有项目网络&lt;/li>
&lt;li>浮动 IP 范围为 172.24.4.0/24，网关为 172.24.4.1&lt;/li>
&lt;li>演示项目在从 10.0.0.0/22 范围分配的子网上配置了固定 IP&lt;/li>
&lt;li>由 neutron 控制的所有网络的接口&lt;code>br-ex&lt;/code>（不连接到任何物理接口）。&lt;/li>
&lt;li>根据主机的 resolv.conf 为访客提供 DNS 解析&lt;/li>
&lt;li>允许创建的访客路由出去的 IP masq 规则&lt;/li>
&lt;/ul>
&lt;p>这将创建一个与单个主机隔离的环境。访客可以访问外部网络以获取软件包更新。Tempest 测试将在此环境中进行。&lt;/p>
&lt;p>笔记&lt;/p>
&lt;p>默认情况下，所有 OpenStack 环境都有安全组规则，阻止所有发往客户的入站数据包。如果您希望能够 ssh/ping 您创建的客户，则应运行以下命令。&lt;/p>
&lt;pre tabindex="0">&lt;code>ubuntu@openstack-vm:~$ openstack security group list

+--------------------------------------+---------+------------------------+----------------------------------+------+
| ID | Name | Description | Project | Tags |
+--------------------------------------+---------+------------------------+----------------------------------+------+
| 109ac0cd-daf6-481c-9594-867dbbdb129b | default | Default security group | 14278ce7bc604abd91e85b82e78ea45e | [] |
| 73b1a108-a0db-49a3-973f-dc91c030d4dd | default | Default security group | 6175cac8870541d1973a0f1010981180 | [] |
| 75e4b266-40bf-4bd8-a7d5-b2f0b0d6e84d | default | Default security group | a942dd11388440a6b8b9922fb405ad55 | [] |
| 9f07480a-fded-4172-852b-c997653324dc | t1 | | 6175cac8870541d1973a0f1010981180 | [] |
+--------------------------------------+---------+------------------------+----------------------------------+------+
ubuntu@openstack-vm:~$
&lt;/code>&lt;/pre>&lt;p>你可以使用以下命令删除指定的安全组：&lt;/p></description></item><item><title>2024-12-10 docker registrry</title><link>https://qq547475331.github.io/docs/2024-12-10-docker-registrry/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2024-12-10-docker-registrry/</guid><description>&lt;p>在 Ubuntu 上搭建一个轻量级的 Docker 镜像仓库，可以使用 &lt;strong>Harbor&lt;/strong> 或者 &lt;strong>Docker Registry&lt;/strong>。如果你想要一个简单、轻量的仓库，&lt;code>Docker Registry&lt;/code> 是一个非常合适的选择，因为它是 Docker 官方提供的。&lt;/p>
&lt;h3 id="步骤-1安装-docker">
 步骤 1：安装 Docker
 &lt;a class="anchor" href="#%e6%ad%a5%e9%aa%a4-1%e5%ae%89%e8%a3%85-docker">#&lt;/a>
&lt;/h3>
&lt;p>确保你的系统上已安装 Docker，如果还没有，可以通过以下命令安装：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>sudo apt update
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sudo apt install -y docker.io
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="步骤-2使用-docker-registry-搭建镜像仓库">
 步骤 2：使用 Docker Registry 搭建镜像仓库
 &lt;a class="anchor" href="#%e6%ad%a5%e9%aa%a4-2%e4%bd%bf%e7%94%a8-docker-registry-%e6%90%ad%e5%bb%ba%e9%95%9c%e5%83%8f%e4%bb%93%e5%ba%93">#&lt;/a>
&lt;/h3>
&lt;p>Docker Registry 是 Docker 官方提供的开源镜像仓库，使用它可以轻松搭建私有镜像仓库。&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>拉取 Docker Registry 镜像&lt;/strong>： Docker 官方提供了一个 Docker Registry 镜像，运行这个镜像就可以搭建一个私有仓库。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>sudo docker pull registry:2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>&lt;strong>运行 Docker Registry&lt;/strong>： 启动一个 Docker Registry 实例，默认情况下会使用端口 5000。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>sudo docker run -d -p 5000:5000 --name registry registry:2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>这条命令将会拉取并运行一个 &lt;code>registry&lt;/code> 容器，并将本地的 5000 端口映射到容器的 5000 端口。&lt;/p></description></item><item><title>2024-2-22 k8s架构师面试大全</title><link>https://qq547475331.github.io/docs/2024-2-22-k8s%E6%9E%B6%E6%9E%84%E5%B8%88%E9%9D%A2%E8%AF%95%E5%A4%A7%E5%85%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2024-2-22-k8s%E6%9E%B6%E6%9E%84%E5%B8%88%E9%9D%A2%E8%AF%95%E5%A4%A7%E5%85%A8/</guid><description>&lt;p>&lt;strong>一、Kubernetes 基础知识面试题10 道面试题&lt;/strong>&lt;/p>
&lt;p>&lt;strong>1、什么是 Kubernetes？&lt;/strong>&lt;/p>
&lt;h5 id="kubernetes-是一个开源容器管理工具负责容器部署容器扩缩容以及负载平衡它提供了出色的社区并与所有云提供商合作因此我们可以说-kubernetes-不是一个容器化平台而是一个多容器管理解决方案">
 Kubernetes 是一个开源容器管理工具，负责容器部署，容器扩缩容以及负载平衡。它提供了出色的社区，并与所有云提供商合作。因此，我们可以说 Kubernetes 不是一个容器化平台，而是一个多容器管理解决方案。
 &lt;a class="anchor" href="#kubernetes-%e6%98%af%e4%b8%80%e4%b8%aa%e5%bc%80%e6%ba%90%e5%ae%b9%e5%99%a8%e7%ae%a1%e7%90%86%e5%b7%a5%e5%85%b7%e8%b4%9f%e8%b4%a3%e5%ae%b9%e5%99%a8%e9%83%a8%e7%bd%b2%e5%ae%b9%e5%99%a8%e6%89%a9%e7%bc%a9%e5%ae%b9%e4%bb%a5%e5%8f%8a%e8%b4%9f%e8%bd%bd%e5%b9%b3%e8%a1%a1%e5%ae%83%e6%8f%90%e4%be%9b%e4%ba%86%e5%87%ba%e8%89%b2%e7%9a%84%e7%a4%be%e5%8c%ba%e5%b9%b6%e4%b8%8e%e6%89%80%e6%9c%89%e4%ba%91%e6%8f%90%e4%be%9b%e5%95%86%e5%90%88%e4%bd%9c%e5%9b%a0%e6%ad%a4%e6%88%91%e4%bb%ac%e5%8f%af%e4%bb%a5%e8%af%b4-kubernetes-%e4%b8%8d%e6%98%af%e4%b8%80%e4%b8%aa%e5%ae%b9%e5%99%a8%e5%8c%96%e5%b9%b3%e5%8f%b0%e8%80%8c%e6%98%af%e4%b8%80%e4%b8%aa%e5%a4%9a%e5%ae%b9%e5%99%a8%e7%ae%a1%e7%90%86%e8%a7%a3%e5%86%b3%e6%96%b9%e6%a1%88">#&lt;/a>
&lt;/h5>
&lt;p>&lt;strong>2、 Kubernetes 与 docker 什么关系？&lt;/strong>&lt;/p>
&lt;h5 id="docker-提供容器的生命周期管理docker-镜像构建运行时容器但是由于这些单独的容器必须通信因此使用-kubernetes因此我们说-docker-构建容器这些容器通过-kubernetes-相互通信因此可以使用-kubernetes-手动关联和编排在多个主机上运行的容器">
 Docker 提供容器的生命周期管理，Docker 镜像构建运行时容器。但是，由于这些单独的容器必须通信，因此使用 Kubernetes。因此，我们说 Docker 构建容器，这些容器通过 Kubernetes 相互通信。因此，可以使用 Kubernetes 手动关联和编排在多个主机上运行的容器。
 &lt;a class="anchor" href="#docker-%e6%8f%90%e4%be%9b%e5%ae%b9%e5%99%a8%e7%9a%84%e7%94%9f%e5%91%bd%e5%91%a8%e6%9c%9f%e7%ae%a1%e7%90%86docker-%e9%95%9c%e5%83%8f%e6%9e%84%e5%bb%ba%e8%bf%90%e8%a1%8c%e6%97%b6%e5%ae%b9%e5%99%a8%e4%bd%86%e6%98%af%e7%94%b1%e4%ba%8e%e8%bf%99%e4%ba%9b%e5%8d%95%e7%8b%ac%e7%9a%84%e5%ae%b9%e5%99%a8%e5%bf%85%e9%a1%bb%e9%80%9a%e4%bf%a1%e5%9b%a0%e6%ad%a4%e4%bd%bf%e7%94%a8-kubernetes%e5%9b%a0%e6%ad%a4%e6%88%91%e4%bb%ac%e8%af%b4-docker-%e6%9e%84%e5%bb%ba%e5%ae%b9%e5%99%a8%e8%bf%99%e4%ba%9b%e5%ae%b9%e5%99%a8%e9%80%9a%e8%bf%87-kubernetes-%e7%9b%b8%e4%ba%92%e9%80%9a%e4%bf%a1%e5%9b%a0%e6%ad%a4%e5%8f%af%e4%bb%a5%e4%bd%bf%e7%94%a8-kubernetes-%e6%89%8b%e5%8a%a8%e5%85%b3%e8%81%94%e5%92%8c%e7%bc%96%e6%8e%92%e5%9c%a8%e5%a4%9a%e4%b8%aa%e4%b8%bb%e6%9c%ba%e4%b8%8a%e8%bf%90%e8%a1%8c%e7%9a%84%e5%ae%b9%e5%99%a8">#&lt;/a>
&lt;/h5>
&lt;p>&lt;strong>3、Kubernetes 与 Docker Swarm 的区别？&lt;/strong>&lt;/p>
&lt;h5 id="docker-swarm-和-kubernetes-都可以用于类似目的它们都是容器编排工具">
 Docker Swarm 和 Kubernetes 都可以用于类似目的。它们都是容器编排工具。
 &lt;a class="anchor" href="#docker-swarm-%e5%92%8c-kubernetes-%e9%83%bd%e5%8f%af%e4%bb%a5%e7%94%a8%e4%ba%8e%e7%b1%bb%e4%bc%bc%e7%9b%ae%e7%9a%84%e5%ae%83%e4%bb%ac%e9%83%bd%e6%98%af%e5%ae%b9%e5%99%a8%e7%bc%96%e6%8e%92%e5%b7%a5%e5%85%b7">#&lt;/a>
&lt;/h5>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202402221059641.png" alt="image-20240222105935530" />&lt;/p>
&lt;p>&lt;strong>4、在主机和容器上部署应用程序有什么区别？&lt;/strong>&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202402221059626.png" alt="image-20240222105949456" />&lt;/p>
&lt;h5 id="上图左侧架构表示在主机上部署应用程序因此这种架构将具有操作系统然后操作系统将具有内核该内核将在应用程序所需的操作系统上安装各种库因此在这种框架中你可以拥有-n-个应用程序并且所有应用程序将共享该操作系统中存在的库">
 &lt;strong>上图左&lt;/strong>侧架构表示在主机上部署应用程序。因此，这种架构将具有操作系统，然后操作系统将具有内核，该内核将在应用程序所需的操作系统上安装各种库。因此，在这种框架中，你可以拥有 n 个应用程序，并且所有应用程序将共享该操作系统中存在的库。
 &lt;a class="anchor" href="#%e4%b8%8a%e5%9b%be%e5%b7%a6%e4%be%a7%e6%9e%b6%e6%9e%84%e8%a1%a8%e7%a4%ba%e5%9c%a8%e4%b8%bb%e6%9c%ba%e4%b8%8a%e9%83%a8%e7%bd%b2%e5%ba%94%e7%94%a8%e7%a8%8b%e5%ba%8f%e5%9b%a0%e6%ad%a4%e8%bf%99%e7%a7%8d%e6%9e%b6%e6%9e%84%e5%b0%86%e5%85%b7%e6%9c%89%e6%93%8d%e4%bd%9c%e7%b3%bb%e7%bb%9f%e7%84%b6%e5%90%8e%e6%93%8d%e4%bd%9c%e7%b3%bb%e7%bb%9f%e5%b0%86%e5%85%b7%e6%9c%89%e5%86%85%e6%a0%b8%e8%af%a5%e5%86%85%e6%a0%b8%e5%b0%86%e5%9c%a8%e5%ba%94%e7%94%a8%e7%a8%8b%e5%ba%8f%e6%89%80%e9%9c%80%e7%9a%84%e6%93%8d%e4%bd%9c%e7%b3%bb%e7%bb%9f%e4%b8%8a%e5%ae%89%e8%a3%85%e5%90%84%e7%a7%8d%e5%ba%93%e5%9b%a0%e6%ad%a4%e5%9c%a8%e8%bf%99%e7%a7%8d%e6%a1%86%e6%9e%b6%e4%b8%ad%e4%bd%a0%e5%8f%af%e4%bb%a5%e6%8b%a5%e6%9c%89-n-%e4%b8%aa%e5%ba%94%e7%94%a8%e7%a8%8b%e5%ba%8f%e5%b9%b6%e4%b8%94%e6%89%80%e6%9c%89%e5%ba%94%e7%94%a8%e7%a8%8b%e5%ba%8f%e5%b0%86%e5%85%b1%e4%ba%ab%e8%af%a5%e6%93%8d%e4%bd%9c%e7%b3%bb%e7%bb%9f%e4%b8%ad%e5%ad%98%e5%9c%a8%e7%9a%84%e5%ba%93">#&lt;/a>
&lt;/h5>
&lt;h5 id="上图右侧架构是容器中部署应用程序这种架构将有一个内核这是唯一一个在所有应用程序之间唯一共同的东西各个块基本上是容器化的并且这些块与其他应用程序隔离因此应用程序具有与系统其余部分隔离的必要库和二进制文件并且不能被任何其他应用程序侵占">
 &lt;strong>上图右&lt;/strong>侧架构是容器中部署应用程序。这种架构将有一个内核，这是唯一一个在所有应用程序之间唯一共同的东西。各个块基本上是容器化的，并且这些块与其他应用程序隔离。因此，应用程序具有与系统其余部分隔离的必要库和二进制文件，并且不能被任何其他应用程序侵占。
 &lt;a class="anchor" href="#%e4%b8%8a%e5%9b%be%e5%8f%b3%e4%be%a7%e6%9e%b6%e6%9e%84%e6%98%af%e5%ae%b9%e5%99%a8%e4%b8%ad%e9%83%a8%e7%bd%b2%e5%ba%94%e7%94%a8%e7%a8%8b%e5%ba%8f%e8%bf%99%e7%a7%8d%e6%9e%b6%e6%9e%84%e5%b0%86%e6%9c%89%e4%b8%80%e4%b8%aa%e5%86%85%e6%a0%b8%e8%bf%99%e6%98%af%e5%94%af%e4%b8%80%e4%b8%80%e4%b8%aa%e5%9c%a8%e6%89%80%e6%9c%89%e5%ba%94%e7%94%a8%e7%a8%8b%e5%ba%8f%e4%b9%8b%e9%97%b4%e5%94%af%e4%b8%80%e5%85%b1%e5%90%8c%e7%9a%84%e4%b8%9c%e8%a5%bf%e5%90%84%e4%b8%aa%e5%9d%97%e5%9f%ba%e6%9c%ac%e4%b8%8a%e6%98%af%e5%ae%b9%e5%99%a8%e5%8c%96%e7%9a%84%e5%b9%b6%e4%b8%94%e8%bf%99%e4%ba%9b%e5%9d%97%e4%b8%8e%e5%85%b6%e4%bb%96%e5%ba%94%e7%94%a8%e7%a8%8b%e5%ba%8f%e9%9a%94%e7%a6%bb%e5%9b%a0%e6%ad%a4%e5%ba%94%e7%94%a8%e7%a8%8b%e5%ba%8f%e5%85%b7%e6%9c%89%e4%b8%8e%e7%b3%bb%e7%bb%9f%e5%85%b6%e4%bd%99%e9%83%a8%e5%88%86%e9%9a%94%e7%a6%bb%e7%9a%84%e5%bf%85%e8%a6%81%e5%ba%93%e5%92%8c%e4%ba%8c%e8%bf%9b%e5%88%b6%e6%96%87%e4%bb%b6%e5%b9%b6%e4%b8%94%e4%b8%8d%e8%83%bd%e8%a2%ab%e4%bb%bb%e4%bd%95%e5%85%b6%e4%bb%96%e5%ba%94%e7%94%a8%e7%a8%8b%e5%ba%8f%e4%be%b5%e5%8d%a0">#&lt;/a>
&lt;/h5>
&lt;p>&lt;strong>5、Kubernetes 如何简化容器化部署？&lt;/strong>&lt;/p>
&lt;h5 id="跨主机的容器都需要相互通信因此要做到这一点你需要一些能够负载平衡扩展和监控容器的东西由于-kubernetes-与云无关并且可以在任何公共私有提供商上运行因此可以简化容器化部署程序">
 跨主机的容器都需要相互通信。因此，要做到这一点，你需要一些能够负载平衡，扩展和监控容器的东西。由于 Kubernetes 与云无关并且可以在任何公共/私有提供商上运行，因此可以简化容器化部署程序。
 &lt;a class="anchor" href="#%e8%b7%a8%e4%b8%bb%e6%9c%ba%e7%9a%84%e5%ae%b9%e5%99%a8%e9%83%bd%e9%9c%80%e8%a6%81%e7%9b%b8%e4%ba%92%e9%80%9a%e4%bf%a1%e5%9b%a0%e6%ad%a4%e8%a6%81%e5%81%9a%e5%88%b0%e8%bf%99%e4%b8%80%e7%82%b9%e4%bd%a0%e9%9c%80%e8%a6%81%e4%b8%80%e4%ba%9b%e8%83%bd%e5%a4%9f%e8%b4%9f%e8%bd%bd%e5%b9%b3%e8%a1%a1%e6%89%a9%e5%b1%95%e5%92%8c%e7%9b%91%e6%8e%a7%e5%ae%b9%e5%99%a8%e7%9a%84%e4%b8%9c%e8%a5%bf%e7%94%b1%e4%ba%8e-kubernetes-%e4%b8%8e%e4%ba%91%e6%97%a0%e5%85%b3%e5%b9%b6%e4%b8%94%e5%8f%af%e4%bb%a5%e5%9c%a8%e4%bb%bb%e4%bd%95%e5%85%ac%e5%85%b1%e7%a7%81%e6%9c%89%e6%8f%90%e4%be%9b%e5%95%86%e4%b8%8a%e8%bf%90%e8%a1%8c%e5%9b%a0%e6%ad%a4%e5%8f%af%e4%bb%a5%e7%ae%80%e5%8c%96%e5%ae%b9%e5%99%a8%e5%8c%96%e9%83%a8%e7%bd%b2%e7%a8%8b%e5%ba%8f">#&lt;/a>
&lt;/h5>
&lt;p>&lt;strong>6、什么是 kubectl？&lt;/strong>&lt;/p>
&lt;h5 id="kubectl-是一个平台可以使用该平台将命令传递给集群因此它基本上为cli-提供了针对-kubernetes-集群运行命令的方法以及创建和管理-kubernetes组件的各种方法">
 Kubectl 是一个平台，可以使用该平台将命令传递给集群。因此，它基本上为CLI 提供了针对 Kubernetes 集群运行命令的方法，以及创建和管理 Kubernetes组件的各种方法。
 &lt;a class="anchor" href="#kubectl-%e6%98%af%e4%b8%80%e4%b8%aa%e5%b9%b3%e5%8f%b0%e5%8f%af%e4%bb%a5%e4%bd%bf%e7%94%a8%e8%af%a5%e5%b9%b3%e5%8f%b0%e5%b0%86%e5%91%bd%e4%bb%a4%e4%bc%a0%e9%80%92%e7%bb%99%e9%9b%86%e7%be%a4%e5%9b%a0%e6%ad%a4%e5%ae%83%e5%9f%ba%e6%9c%ac%e4%b8%8a%e4%b8%bacli-%e6%8f%90%e4%be%9b%e4%ba%86%e9%92%88%e5%af%b9-kubernetes-%e9%9b%86%e7%be%a4%e8%bf%90%e8%a1%8c%e5%91%bd%e4%bb%a4%e7%9a%84%e6%96%b9%e6%b3%95%e4%bb%a5%e5%8f%8a%e5%88%9b%e5%bb%ba%e5%92%8c%e7%ae%a1%e7%90%86-kubernetes%e7%bb%84%e4%bb%b6%e7%9a%84%e5%90%84%e7%a7%8d%e6%96%b9%e6%b3%95">#&lt;/a>
&lt;/h5>
&lt;p>&lt;strong>7、什么是 kubelet？&lt;/strong>&lt;/p></description></item><item><title>2024-2-22 k8s面试宝典</title><link>https://qq547475331.github.io/docs/2024-2-22-k8s%E9%9D%A2%E8%AF%95%E5%AE%9D%E5%85%B8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2024-2-22-k8s%E9%9D%A2%E8%AF%95%E5%AE%9D%E5%85%B8/</guid><description>&lt;h2 id="创建-pod的主要流程">
 创建 Pod的主要流程?
 &lt;a class="anchor" href="#%e5%88%9b%e5%bb%ba-pod%e7%9a%84%e4%b8%bb%e8%a6%81%e6%b5%81%e7%a8%8b">#&lt;/a>
&lt;/h2>
&lt;ol>
&lt;li>
&lt;h5 id="客户端提交-pod-的配置信息可以是-yaml-文件定义的信息到-kube-apiserver">
 客户端提交 Pod 的配置信息(可以是 yaml 文件定义的信息)到 kube-apiserver.
 &lt;a class="anchor" href="#%e5%ae%a2%e6%88%b7%e7%ab%af%e6%8f%90%e4%ba%a4-pod-%e7%9a%84%e9%85%8d%e7%bd%ae%e4%bf%a1%e6%81%af%e5%8f%af%e4%bb%a5%e6%98%af-yaml-%e6%96%87%e4%bb%b6%e5%ae%9a%e4%b9%89%e7%9a%84%e4%bf%a1%e6%81%af%e5%88%b0-kube-apiserver">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="apiserver-收到指令后通知-controllr-manager-创建一个资源对象">
 Apiserver 收到指令后,通知 controllr-manager 创建一个资源对象
 &lt;a class="anchor" href="#apiserver-%e6%94%b6%e5%88%b0%e6%8c%87%e4%bb%a4%e5%90%8e%e9%80%9a%e7%9f%a5-controllr-manager-%e5%88%9b%e5%bb%ba%e4%b8%80%e4%b8%aa%e8%b5%84%e6%ba%90%e5%af%b9%e8%b1%a1">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="controller-manager-通过-apiserver-将-pod-的配置信息存储到-etcd-数据中心中">
 controller-manager 通过 apiserver 将 pod 的配置信息存储到 ETCD 数据中心中
 &lt;a class="anchor" href="#controller-manager-%e9%80%9a%e8%bf%87-apiserver-%e5%b0%86-pod-%e7%9a%84%e9%85%8d%e7%bd%ae%e4%bf%a1%e6%81%af%e5%ad%98%e5%82%a8%e5%88%b0-etcd-%e6%95%b0%e6%8d%ae%e4%b8%ad%e5%bf%83%e4%b8%ad">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="kube-scheduler-检查到-pod-信息会开始调度预选会先过滤不符合-pod-资源配置要求的节点然后开始调度调优主要是挑选出更适合运行的-pod-节点然后将-pod-的资源配置单发送到-node-节点上的-kubelet-组件上">
 kube-scheduler 检查到 pod 信息会开始调度预选,会先过滤不符合 Pod 资源配置要求的节点,然后开始调度调优,主要是挑选出更适合运行的 pod 节点,然后将 pod 的资源配置单发送到 node 节点上的 kubelet 组件上
 &lt;a class="anchor" href="#kube-scheduler-%e6%a3%80%e6%9f%a5%e5%88%b0-pod-%e4%bf%a1%e6%81%af%e4%bc%9a%e5%bc%80%e5%a7%8b%e8%b0%83%e5%ba%a6%e9%a2%84%e9%80%89%e4%bc%9a%e5%85%88%e8%bf%87%e6%bb%a4%e4%b8%8d%e7%ac%a6%e5%90%88-pod-%e8%b5%84%e6%ba%90%e9%85%8d%e7%bd%ae%e8%a6%81%e6%b1%82%e7%9a%84%e8%8a%82%e7%82%b9%e7%84%b6%e5%90%8e%e5%bc%80%e5%a7%8b%e8%b0%83%e5%ba%a6%e8%b0%83%e4%bc%98%e4%b8%bb%e8%a6%81%e6%98%af%e6%8c%91%e9%80%89%e5%87%ba%e6%9b%b4%e9%80%82%e5%90%88%e8%bf%90%e8%a1%8c%e7%9a%84-pod-%e8%8a%82%e7%82%b9%e7%84%b6%e5%90%8e%e5%b0%86-pod-%e7%9a%84%e8%b5%84%e6%ba%90%e9%85%8d%e7%bd%ae%e5%8d%95%e5%8f%91%e9%80%81%e5%88%b0-node-%e8%8a%82%e7%82%b9%e4%b8%8a%e7%9a%84-kubelet-%e7%bb%84%e4%bb%b6%e4%b8%8a">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="kubelet-根据-scheduler-发来的资源配置单运行-pod运行成功后将-pod-的运行的信息返回-scheduler-scheduler-将返回的-pod-运行状况的信息存储到-etcd-数据中心">
 kubelet 根据 scheduler 发来的资源配置单运行 pod,运行成功后,将 pod 的运行的信息返回 scheduler, scheduler 将返回的 pod 运行状况的信息存储到 etcd 数据中心
 &lt;a class="anchor" href="#kubelet-%e6%a0%b9%e6%8d%ae-scheduler-%e5%8f%91%e6%9d%a5%e7%9a%84%e8%b5%84%e6%ba%90%e9%85%8d%e7%bd%ae%e5%8d%95%e8%bf%90%e8%a1%8c-pod%e8%bf%90%e8%a1%8c%e6%88%90%e5%8a%9f%e5%90%8e%e5%b0%86-pod-%e7%9a%84%e8%bf%90%e8%a1%8c%e7%9a%84%e4%bf%a1%e6%81%af%e8%bf%94%e5%9b%9e-scheduler-scheduler-%e5%b0%86%e8%bf%94%e5%9b%9e%e7%9a%84-pod-%e8%bf%90%e8%a1%8c%e7%8a%b6%e5%86%b5%e7%9a%84%e4%bf%a1%e6%81%af%e5%ad%98%e5%82%a8%e5%88%b0-etcd-%e6%95%b0%e6%8d%ae%e4%b8%ad%e5%bf%83">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ol>
&lt;h2 id="pod-的重启策略">
 Pod 的重启策略
 &lt;a class="anchor" href="#pod-%e7%9a%84%e9%87%8d%e5%90%af%e7%ad%96%e7%95%a5">#&lt;/a>
&lt;/h2>
&lt;ul>
&lt;li>
&lt;h5 id="-pod-重启策略restartpolicy应用于-pod-内的所有容器并且仅再-pod-所处的-node-上由-kubelet-进行判断和重启操作当某个容器异常退出或健康检查失败时kubele-将根据-restartpolicy-的设置来进行相应操作">
 • Pod 重启策略(RestartPolicy)应用于 Pod 内的所有容器,并且仅再 Pod 所处的 Node 上由 Kubelet 进行判断和重启操作.当某个容器异常退出或健康检查失败时,kubele 将根据 RestartPolicy 的设置来进行相应操作
 &lt;a class="anchor" href="#-pod-%e9%87%8d%e5%90%af%e7%ad%96%e7%95%a5restartpolicy%e5%ba%94%e7%94%a8%e4%ba%8e-pod-%e5%86%85%e7%9a%84%e6%89%80%e6%9c%89%e5%ae%b9%e5%99%a8%e5%b9%b6%e4%b8%94%e4%bb%85%e5%86%8d-pod-%e6%89%80%e5%a4%84%e7%9a%84-node-%e4%b8%8a%e7%94%b1-kubelet-%e8%bf%9b%e8%a1%8c%e5%88%a4%e6%96%ad%e5%92%8c%e9%87%8d%e5%90%af%e6%93%8d%e4%bd%9c%e5%bd%93%e6%9f%90%e4%b8%aa%e5%ae%b9%e5%99%a8%e5%bc%82%e5%b8%b8%e9%80%80%e5%87%ba%e6%88%96%e5%81%a5%e5%ba%b7%e6%a3%80%e6%9f%a5%e5%a4%b1%e8%b4%a5%e6%97%b6kubele-%e5%b0%86%e6%a0%b9%e6%8d%ae-restartpolicy-%e7%9a%84%e8%ae%be%e7%bd%ae%e6%9d%a5%e8%bf%9b%e8%a1%8c%e7%9b%b8%e5%ba%94%e6%93%8d%e4%bd%9c">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-pod-的重启策略包括-alwaysonfaliure-和-never默认值为-always">
 • pod 的重启策略包括 Always,OnFaliure 和 Never,默认值为 Always
 &lt;a class="anchor" href="#-pod-%e7%9a%84%e9%87%8d%e5%90%af%e7%ad%96%e7%95%a5%e5%8c%85%e6%8b%ac-alwaysonfaliure-%e5%92%8c-never%e9%bb%98%e8%ae%a4%e5%80%bc%e4%b8%ba-always">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;ul>
&lt;li>
&lt;h5 id="-always-当容器失效时由-kubelet-自动重启该容器">
 • Always: 当容器失效时由 kubelet 自动重启该容器
 &lt;a class="anchor" href="#-always-%e5%bd%93%e5%ae%b9%e5%99%a8%e5%a4%b1%e6%95%88%e6%97%b6%e7%94%b1-kubelet-%e8%87%aa%e5%8a%a8%e9%87%8d%e5%90%af%e8%af%a5%e5%ae%b9%e5%99%a8">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-onfailure当容器终止运行且退出不为-0-时-由-kubelet-自动重启该容器">
 • OnFailure:当容器终止运行且退出不为 0 时, 由 kubelet 自动重启该容器
 &lt;a class="anchor" href="#-onfailure%e5%bd%93%e5%ae%b9%e5%99%a8%e7%bb%88%e6%ad%a2%e8%bf%90%e8%a1%8c%e4%b8%94%e9%80%80%e5%87%ba%e4%b8%8d%e4%b8%ba-0-%e6%97%b6-%e7%94%b1-kubelet-%e8%87%aa%e5%8a%a8%e9%87%8d%e5%90%af%e8%af%a5%e5%ae%b9%e5%99%a8">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-nerve-不论容器运行状态如何kubelet-都不会重启该容器">
 • Nerve: 不论容器运行状态如何,kubelet 都不会重启该容器
 &lt;a class="anchor" href="#-nerve-%e4%b8%8d%e8%ae%ba%e5%ae%b9%e5%99%a8%e8%bf%90%e8%a1%8c%e7%8a%b6%e6%80%81%e5%a6%82%e4%bd%95kubelet-%e9%83%bd%e4%b8%8d%e4%bc%9a%e9%87%8d%e5%90%af%e8%af%a5%e5%ae%b9%e5%99%a8">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;h5 id="-同时-pod-的容器策略与控制方式关联当前可用于管理-pod-的控制器包括-relicatoncontroller">
 • 同时 pod 的容器策略与控制方式关联,当前可用于管理 Pod 的控制器包括 RelicatonController
 &lt;a class="anchor" href="#-%e5%90%8c%e6%97%b6-pod-%e7%9a%84%e5%ae%b9%e5%99%a8%e7%ad%96%e7%95%a5%e4%b8%8e%e6%8e%a7%e5%88%b6%e6%96%b9%e5%bc%8f%e5%85%b3%e8%81%94%e5%bd%93%e5%89%8d%e5%8f%af%e7%94%a8%e4%ba%8e%e7%ae%a1%e7%90%86-pod-%e7%9a%84%e6%8e%a7%e5%88%b6%e5%99%a8%e5%8c%85%e6%8b%ac-relicatoncontroller">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h2 id="pod-的健康检查方式">
 Pod 的健康检查方式
 &lt;a class="anchor" href="#pod-%e7%9a%84%e5%81%a5%e5%ba%b7%e6%a3%80%e6%9f%a5%e6%96%b9%e5%bc%8f">#&lt;/a>
&lt;/h2>
&lt;ul>
&lt;li>
&lt;h5 id="-livenessprobe探针用于判断容器是否存活running状态如果livenessprobe探针探测到容器不健康则kubelet将杀掉该容器并根据容器的重启策略做相应处理若一个容器不包含livenessprobe探针kubelet认为该容器的livenessprobe探针返回值用于是success">
 • LivenessProbe探针：用于判断容器是否存活（running状态），如果LivenessProbe探针探测到容器不健康，则kubelet将杀掉该容器，并根据容器的重启策略做相应处理。若一个容器不包含LivenessProbe探针，kubelet认为该容器的LivenessProbe探针返回值用于是“Success”。
 &lt;a class="anchor" href="#-livenessprobe%e6%8e%a2%e9%92%88%e7%94%a8%e4%ba%8e%e5%88%a4%e6%96%ad%e5%ae%b9%e5%99%a8%e6%98%af%e5%90%a6%e5%ad%98%e6%b4%bbrunning%e7%8a%b6%e6%80%81%e5%a6%82%e6%9e%9clivenessprobe%e6%8e%a2%e9%92%88%e6%8e%a2%e6%b5%8b%e5%88%b0%e5%ae%b9%e5%99%a8%e4%b8%8d%e5%81%a5%e5%ba%b7%e5%88%99kubelet%e5%b0%86%e6%9d%80%e6%8e%89%e8%af%a5%e5%ae%b9%e5%99%a8%e5%b9%b6%e6%a0%b9%e6%8d%ae%e5%ae%b9%e5%99%a8%e7%9a%84%e9%87%8d%e5%90%af%e7%ad%96%e7%95%a5%e5%81%9a%e7%9b%b8%e5%ba%94%e5%a4%84%e7%90%86%e8%8b%a5%e4%b8%80%e4%b8%aa%e5%ae%b9%e5%99%a8%e4%b8%8d%e5%8c%85%e5%90%ablivenessprobe%e6%8e%a2%e9%92%88kubelet%e8%ae%a4%e4%b8%ba%e8%af%a5%e5%ae%b9%e5%99%a8%e7%9a%84livenessprobe%e6%8e%a2%e9%92%88%e8%bf%94%e5%9b%9e%e5%80%bc%e7%94%a8%e4%ba%8e%e6%98%afsuccess">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-readineeprobe探针用于判断容器是否启动完成ready状态如果readinessprobe探针探测到失败则pod的状态将被修改endpoint-controller将从service的endpoint中删除包含该容器所在pod的eenpoint">
 • ReadineeProbe探针：用于判断容器是否启动完成（ready状态）。如果ReadinessProbe探针探测到失败，则Pod的状态将被修改。Endpoint Controller将从Service的Endpoint中删除包含该容器所在Pod的Eenpoint。
 &lt;a class="anchor" href="#-readineeprobe%e6%8e%a2%e9%92%88%e7%94%a8%e4%ba%8e%e5%88%a4%e6%96%ad%e5%ae%b9%e5%99%a8%e6%98%af%e5%90%a6%e5%90%af%e5%8a%a8%e5%ae%8c%e6%88%90ready%e7%8a%b6%e6%80%81%e5%a6%82%e6%9e%9creadinessprobe%e6%8e%a2%e9%92%88%e6%8e%a2%e6%b5%8b%e5%88%b0%e5%a4%b1%e8%b4%a5%e5%88%99pod%e7%9a%84%e7%8a%b6%e6%80%81%e5%b0%86%e8%a2%ab%e4%bf%ae%e6%94%b9endpoint-controller%e5%b0%86%e4%bb%8eservice%e7%9a%84endpoint%e4%b8%ad%e5%88%a0%e9%99%a4%e5%8c%85%e5%90%ab%e8%af%a5%e5%ae%b9%e5%99%a8%e6%89%80%e5%9c%a8pod%e7%9a%84eenpoint">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-startupprobe探针启动检查机制应用一些启动缓慢的业务避免业务长时间启动而被上面两类探针kill掉">
 • startupProbe探针：启动检查机制，应用一些启动缓慢的业务，避免业务长时间启动而被上面两类探针kill掉。
 &lt;a class="anchor" href="#-startupprobe%e6%8e%a2%e9%92%88%e5%90%af%e5%8a%a8%e6%a3%80%e6%9f%a5%e6%9c%ba%e5%88%b6%e5%ba%94%e7%94%a8%e4%b8%80%e4%ba%9b%e5%90%af%e5%8a%a8%e7%bc%93%e6%85%a2%e7%9a%84%e4%b8%9a%e5%8a%a1%e9%81%bf%e5%85%8d%e4%b8%9a%e5%8a%a1%e9%95%bf%e6%97%b6%e9%97%b4%e5%90%af%e5%8a%a8%e8%80%8c%e8%a2%ab%e4%b8%8a%e9%9d%a2%e4%b8%a4%e7%b1%bb%e6%8e%a2%e9%92%88kill%e6%8e%89">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h2 id="pod-探针常见方式">
 Pod 探针常见方式
 &lt;a class="anchor" href="#pod-%e6%8e%a2%e9%92%88%e5%b8%b8%e8%a7%81%e6%96%b9%e5%bc%8f">#&lt;/a>
&lt;/h2>
&lt;ul>
&lt;li>
&lt;h5 id="-execaction在容器内执行一个命令若返回码为0则表明容器健康">
 • ExecAction：在容器内执行一个命令，若返回码为0，则表明容器健康。
 &lt;a class="anchor" href="#-execaction%e5%9c%a8%e5%ae%b9%e5%99%a8%e5%86%85%e6%89%a7%e8%a1%8c%e4%b8%80%e4%b8%aa%e5%91%bd%e4%bb%a4%e8%8b%a5%e8%bf%94%e5%9b%9e%e7%a0%81%e4%b8%ba0%e5%88%99%e8%a1%a8%e6%98%8e%e5%ae%b9%e5%99%a8%e5%81%a5%e5%ba%b7">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-tcpsocketaction通过容器的ip地址和端口号执行tcp检查若能建立tcp连接则表明容器健康">
 • TCPSocketAction：通过容器的IP地址和端口号执行TCP检查，若能建立TCP连接，则表明容器健康。
 &lt;a class="anchor" href="#-tcpsocketaction%e9%80%9a%e8%bf%87%e5%ae%b9%e5%99%a8%e7%9a%84ip%e5%9c%b0%e5%9d%80%e5%92%8c%e7%ab%af%e5%8f%a3%e5%8f%b7%e6%89%a7%e8%a1%8ctcp%e6%a3%80%e6%9f%a5%e8%8b%a5%e8%83%bd%e5%bb%ba%e7%ab%8btcp%e8%bf%9e%e6%8e%a5%e5%88%99%e8%a1%a8%e6%98%8e%e5%ae%b9%e5%99%a8%e5%81%a5%e5%ba%b7">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-httpgetaction通过容器的ip地址端口号及路径调用http-get方法若响应的状态码大于等于200且小于400则表明容器健康">
 • HTTPGetAction：通过容器的IP地址、端口号及路径调用HTTP Get方法，若响应的状态码大于等于200且小于400，则表明容器健康。
 &lt;a class="anchor" href="#-httpgetaction%e9%80%9a%e8%bf%87%e5%ae%b9%e5%99%a8%e7%9a%84ip%e5%9c%b0%e5%9d%80%e7%ab%af%e5%8f%a3%e5%8f%b7%e5%8f%8a%e8%b7%af%e5%be%84%e8%b0%83%e7%94%a8http-get%e6%96%b9%e6%b3%95%e8%8b%a5%e5%93%8d%e5%ba%94%e7%9a%84%e7%8a%b6%e6%80%81%e7%a0%81%e5%a4%a7%e4%ba%8e%e7%ad%89%e4%ba%8e200%e4%b8%94%e5%b0%8f%e4%ba%8e400%e5%88%99%e8%a1%a8%e6%98%8e%e5%ae%b9%e5%99%a8%e5%81%a5%e5%ba%b7">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h2 id="pod-常见的调度方式">
 Pod 常见的调度方式
 &lt;a class="anchor" href="#pod-%e5%b8%b8%e8%a7%81%e7%9a%84%e8%b0%83%e5%ba%a6%e6%96%b9%e5%bc%8f">#&lt;/a>
&lt;/h2>
&lt;ul>
&lt;li>
&lt;h5 id="-deployment或rc该调度策略主要功能就是自动部署一个容器应用的多份副本以及持续监控副本的数量在集群内始终维持用户指定的副本数量">
 • Deployment或RC：该调度策略主要功能就是自动部署一个容器应用的多份副本，以及持续监控副本的数量，在集群内始终维持用户指定的副本数量。
 &lt;a class="anchor" href="#-deployment%e6%88%96rc%e8%af%a5%e8%b0%83%e5%ba%a6%e7%ad%96%e7%95%a5%e4%b8%bb%e8%a6%81%e5%8a%9f%e8%83%bd%e5%b0%b1%e6%98%af%e8%87%aa%e5%8a%a8%e9%83%a8%e7%bd%b2%e4%b8%80%e4%b8%aa%e5%ae%b9%e5%99%a8%e5%ba%94%e7%94%a8%e7%9a%84%e5%a4%9a%e4%bb%bd%e5%89%af%e6%9c%ac%e4%bb%a5%e5%8f%8a%e6%8c%81%e7%bb%ad%e7%9b%91%e6%8e%a7%e5%89%af%e6%9c%ac%e7%9a%84%e6%95%b0%e9%87%8f%e5%9c%a8%e9%9b%86%e7%be%a4%e5%86%85%e5%a7%8b%e7%bb%88%e7%bb%b4%e6%8c%81%e7%94%a8%e6%88%b7%e6%8c%87%e5%ae%9a%e7%9a%84%e5%89%af%e6%9c%ac%e6%95%b0%e9%87%8f">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-nodeselector定向调度当需要手动指定将pod调度到特定node上可以通过node的标签label和pod的nodeselector属性相匹配">
 • NodeSelector：定向调度，当需要手动指定将Pod调度到特定Node上，可以通过Node的标签（Label）和Pod的nodeSelector属性相匹配。
 &lt;a class="anchor" href="#-nodeselector%e5%ae%9a%e5%90%91%e8%b0%83%e5%ba%a6%e5%bd%93%e9%9c%80%e8%a6%81%e6%89%8b%e5%8a%a8%e6%8c%87%e5%ae%9a%e5%b0%86pod%e8%b0%83%e5%ba%a6%e5%88%b0%e7%89%b9%e5%ae%9anode%e4%b8%8a%e5%8f%af%e4%bb%a5%e9%80%9a%e8%bf%87node%e7%9a%84%e6%a0%87%e7%ad%belabel%e5%92%8cpod%e7%9a%84nodeselector%e5%b1%9e%e6%80%a7%e7%9b%b8%e5%8c%b9%e9%85%8d">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-nodeaffinity亲和性调度亲和性调度机制极大的扩展了pod的调度能力目前有两种节点亲和力表达">
 • NodeAffinity亲和性调度：亲和性调度机制极大的扩展了Pod的调度能力，目前有两种节点亲和力表达：
 &lt;a class="anchor" href="#-nodeaffinity%e4%ba%b2%e5%92%8c%e6%80%a7%e8%b0%83%e5%ba%a6%e4%ba%b2%e5%92%8c%e6%80%a7%e8%b0%83%e5%ba%a6%e6%9c%ba%e5%88%b6%e6%9e%81%e5%a4%a7%e7%9a%84%e6%89%a9%e5%b1%95%e4%ba%86pod%e7%9a%84%e8%b0%83%e5%ba%a6%e8%83%bd%e5%8a%9b%e7%9b%ae%e5%89%8d%e6%9c%89%e4%b8%a4%e7%a7%8d%e8%8a%82%e7%82%b9%e4%ba%b2%e5%92%8c%e5%8a%9b%e8%a1%a8%e8%be%be">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-requiredduringschedulingignoredduringexecution硬规则必须满足指定的规则调度器才可以调度pod至node上类似nodeselector语法不同">
 • requiredDuringSchedulingIgnoredDuringExecution：硬规则，必须满足指定的规则，调度器才可以调度Pod至Node上（类似nodeSelector，语法不同）。
 &lt;a class="anchor" href="#-requiredduringschedulingignoredduringexecution%e7%a1%ac%e8%a7%84%e5%88%99%e5%bf%85%e9%a1%bb%e6%bb%a1%e8%b6%b3%e6%8c%87%e5%ae%9a%e7%9a%84%e8%a7%84%e5%88%99%e8%b0%83%e5%ba%a6%e5%99%a8%e6%89%8d%e5%8f%af%e4%bb%a5%e8%b0%83%e5%ba%a6pod%e8%87%b3node%e4%b8%8a%e7%b1%bb%e4%bc%bcnodeselector%e8%af%ad%e6%b3%95%e4%b8%8d%e5%90%8c">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-preferredduringschedulingignoredduringexecution软规则优先调度至满足的node的节点但不强求多个优先级规则还可以设置权重值">
 • preferredDuringSchedulingIgnoredDuringExecution：软规则，优先调度至满足的Node的节点，但不强求，多个优先级规则还可以设置权重值。
 &lt;a class="anchor" href="#-preferredduringschedulingignoredduringexecution%e8%bd%af%e8%a7%84%e5%88%99%e4%bc%98%e5%85%88%e8%b0%83%e5%ba%a6%e8%87%b3%e6%bb%a1%e8%b6%b3%e7%9a%84node%e7%9a%84%e8%8a%82%e7%82%b9%e4%bd%86%e4%b8%8d%e5%bc%ba%e6%b1%82%e5%a4%9a%e4%b8%aa%e4%bc%98%e5%85%88%e7%ba%a7%e8%a7%84%e5%88%99%e8%bf%98%e5%8f%af%e4%bb%a5%e8%ae%be%e7%bd%ae%e6%9d%83%e9%87%8d%e5%80%bc">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-taints和tolerations污点和容忍">
 • Taints和Tolerations（污点和容忍）：
 &lt;a class="anchor" href="#-taints%e5%92%8ctolerations%e6%b1%a1%e7%82%b9%e5%92%8c%e5%ae%b9%e5%bf%8d">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;ul>
&lt;li>
&lt;h5 id="-taint使node拒绝特定pod运行">
 • Taint：使Node拒绝特定Pod运行；
 &lt;a class="anchor" href="#-taint%e4%bd%bfnode%e6%8b%92%e7%bb%9d%e7%89%b9%e5%ae%9apod%e8%bf%90%e8%a1%8c">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-toleration为pod的属性表示pod能容忍运行标注了taint的node">
 • Toleration：为Pod的属性，表示Pod能容忍（运行）标注了Taint的Node。
 &lt;a class="anchor" href="#-toleration%e4%b8%bapod%e7%9a%84%e5%b1%9e%e6%80%a7%e8%a1%a8%e7%a4%bapod%e8%83%bd%e5%ae%b9%e5%bf%8d%e8%bf%90%e8%a1%8c%e6%a0%87%e6%b3%a8%e4%ba%86taint%e7%9a%84node">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="deployment升级策略">
 deployment升级策略?
 &lt;a class="anchor" href="#deployment%e5%8d%87%e7%ba%a7%e7%ad%96%e7%95%a5">#&lt;/a>
&lt;/h2>
&lt;ul>
&lt;li>
&lt;h5 id="-在deployment的定义中可以通过specstrategy指定pod更新的策略目前支持两种策略recreate重建和rollingupdate滚动更新默认值为rollingupdate">
 • 在Deployment的定义中，可以通过spec.strategy指定Pod更新的策略，目前支持两种策略：Recreate（重建）和RollingUpdate（滚动更新），默认值为RollingUpdate。
 &lt;a class="anchor" href="#-%e5%9c%a8deployment%e7%9a%84%e5%ae%9a%e4%b9%89%e4%b8%ad%e5%8f%af%e4%bb%a5%e9%80%9a%e8%bf%87specstrategy%e6%8c%87%e5%ae%9apod%e6%9b%b4%e6%96%b0%e7%9a%84%e7%ad%96%e7%95%a5%e7%9b%ae%e5%89%8d%e6%94%af%e6%8c%81%e4%b8%a4%e7%a7%8d%e7%ad%96%e7%95%a5recreate%e9%87%8d%e5%bb%ba%e5%92%8crollingupdate%e6%bb%9a%e5%8a%a8%e6%9b%b4%e6%96%b0%e9%bb%98%e8%ae%a4%e5%80%bc%e4%b8%barollingupdate">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-recreate设置specstrategytyperecreate表示deployment在更新pod时会先杀掉所有正在运行的pod然后创建新的pod">
 • Recreate：设置spec.strategy.type=Recreate，表示Deployment在更新Pod时，会先杀掉所有正在运行的Pod，然后创建新的Pod。
 &lt;a class="anchor" href="#-recreate%e8%ae%be%e7%bd%aespecstrategytyperecreate%e8%a1%a8%e7%a4%badeployment%e5%9c%a8%e6%9b%b4%e6%96%b0pod%e6%97%b6%e4%bc%9a%e5%85%88%e6%9d%80%e6%8e%89%e6%89%80%e6%9c%89%e6%ad%a3%e5%9c%a8%e8%bf%90%e8%a1%8c%e7%9a%84pod%e7%84%b6%e5%90%8e%e5%88%9b%e5%bb%ba%e6%96%b0%e7%9a%84pod">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-rollingupdate设置specstrategytyperollingupdate表示deployment会以滚动更新的方式来逐个更新pod同时可以通过设置specstrategyrollingupdate下的两个参数maxunavailable和maxsurge来控制滚动更新的过程">
 • RollingUpdate：设置spec.strategy.type=RollingUpdate，表示Deployment会以滚动更新的方式来逐个更新Pod。同时，可以通过设置spec.strategy.rollingUpdate下的两个参数（maxUnavailable和maxSurge）来控制滚动更新的过程
 &lt;a class="anchor" href="#-rollingupdate%e8%ae%be%e7%bd%aespecstrategytyperollingupdate%e8%a1%a8%e7%a4%badeployment%e4%bc%9a%e4%bb%a5%e6%bb%9a%e5%8a%a8%e6%9b%b4%e6%96%b0%e7%9a%84%e6%96%b9%e5%bc%8f%e6%9d%a5%e9%80%90%e4%b8%aa%e6%9b%b4%e6%96%b0pod%e5%90%8c%e6%97%b6%e5%8f%af%e4%bb%a5%e9%80%9a%e8%bf%87%e8%ae%be%e7%bd%aespecstrategyrollingupdate%e4%b8%8b%e7%9a%84%e4%b8%a4%e4%b8%aa%e5%8f%82%e6%95%b0maxunavailable%e5%92%8cmaxsurge%e6%9d%a5%e6%8e%a7%e5%88%b6%e6%bb%9a%e5%8a%a8%e6%9b%b4%e6%96%b0%e7%9a%84%e8%bf%87%e7%a8%8b">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h2 id="kubernetes-service类型">
 Kubernetes Service类型?
 &lt;a class="anchor" href="#kubernetes-service%e7%b1%bb%e5%9e%8b">#&lt;/a>
&lt;/h2>
&lt;h5 id="通过创建service可以为一组具有相同功能的容器应用提供一个统一的入口地址并且将请求负载分发到后端的各个容器应用上其主要类型有">
 通过创建Service，可以为一组具有相同功能的容器应用提供一个统一的入口地址，并且将请求负载分发到后端的各个容器应用上。其主要类型有：
 &lt;a class="anchor" href="#%e9%80%9a%e8%bf%87%e5%88%9b%e5%bb%baservice%e5%8f%af%e4%bb%a5%e4%b8%ba%e4%b8%80%e7%bb%84%e5%85%b7%e6%9c%89%e7%9b%b8%e5%90%8c%e5%8a%9f%e8%83%bd%e7%9a%84%e5%ae%b9%e5%99%a8%e5%ba%94%e7%94%a8%e6%8f%90%e4%be%9b%e4%b8%80%e4%b8%aa%e7%bb%9f%e4%b8%80%e7%9a%84%e5%85%a5%e5%8f%a3%e5%9c%b0%e5%9d%80%e5%b9%b6%e4%b8%94%e5%b0%86%e8%af%b7%e6%b1%82%e8%b4%9f%e8%bd%bd%e5%88%86%e5%8f%91%e5%88%b0%e5%90%8e%e7%ab%af%e7%9a%84%e5%90%84%e4%b8%aa%e5%ae%b9%e5%99%a8%e5%ba%94%e7%94%a8%e4%b8%8a%e5%85%b6%e4%b8%bb%e8%a6%81%e7%b1%bb%e5%9e%8b%e6%9c%89">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="-clusterip虚拟的服务ip地址该地址用于kubernetes集群内部的pod访问在node上kube-proxy通过设置的iptables规则进行转发">
 • ClusterIP：虚拟的服务IP地址，该地址用于Kubernetes集群内部的Pod访问，在Node上kube-proxy通过设置的iptables规则进行转发；
 &lt;a class="anchor" href="#-clusterip%e8%99%9a%e6%8b%9f%e7%9a%84%e6%9c%8d%e5%8a%a1ip%e5%9c%b0%e5%9d%80%e8%af%a5%e5%9c%b0%e5%9d%80%e7%94%a8%e4%ba%8ekubernetes%e9%9b%86%e7%be%a4%e5%86%85%e9%83%a8%e7%9a%84pod%e8%ae%bf%e9%97%ae%e5%9c%a8node%e4%b8%8akube-proxy%e9%80%9a%e8%bf%87%e8%ae%be%e7%bd%ae%e7%9a%84iptables%e8%a7%84%e5%88%99%e8%bf%9b%e8%a1%8c%e8%bd%ac%e5%8f%91">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-nodeport使用宿主机的端口使能够访问各node的外部客户端通过node的ip地址和端口号就能访问服务">
 • NodePort：使用宿主机的端口，使能够访问各Node的外部客户端通过Node的IP地址和端口号就能访问服务；
 &lt;a class="anchor" href="#-nodeport%e4%bd%bf%e7%94%a8%e5%ae%bf%e4%b8%bb%e6%9c%ba%e7%9a%84%e7%ab%af%e5%8f%a3%e4%bd%bf%e8%83%bd%e5%a4%9f%e8%ae%bf%e9%97%ae%e5%90%84node%e7%9a%84%e5%a4%96%e9%83%a8%e5%ae%a2%e6%88%b7%e7%ab%af%e9%80%9a%e8%bf%87node%e7%9a%84ip%e5%9c%b0%e5%9d%80%e5%92%8c%e7%ab%af%e5%8f%a3%e5%8f%b7%e5%b0%b1%e8%83%bd%e8%ae%bf%e9%97%ae%e6%9c%8d%e5%8a%a1">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-loadbalancer使用外接负载均衡器完成到服务的负载分发需要在specstatusloadbalancer字段指定外部负载均衡器的ip地址通常用于公有云">
 • LoadBalancer：使用外接负载均衡器完成到服务的负载分发，需要在spec.status.loadBalancer字段指定外部负载均衡器的IP地址，通常用于公有云。
 &lt;a class="anchor" href="#-loadbalancer%e4%bd%bf%e7%94%a8%e5%a4%96%e6%8e%a5%e8%b4%9f%e8%bd%bd%e5%9d%87%e8%a1%a1%e5%99%a8%e5%ae%8c%e6%88%90%e5%88%b0%e6%9c%8d%e5%8a%a1%e7%9a%84%e8%b4%9f%e8%bd%bd%e5%88%86%e5%8f%91%e9%9c%80%e8%a6%81%e5%9c%a8specstatusloadbalancer%e5%ad%97%e6%ae%b5%e6%8c%87%e5%ae%9a%e5%a4%96%e9%83%a8%e8%b4%9f%e8%bd%bd%e5%9d%87%e8%a1%a1%e5%99%a8%e7%9a%84ip%e5%9c%b0%e5%9d%80%e9%80%9a%e5%b8%b8%e7%94%a8%e4%ba%8e%e5%85%ac%e6%9c%89%e4%ba%91">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h2 id="service分发后端的策略">
 Service分发后端的策略?
 &lt;a class="anchor" href="#service%e5%88%86%e5%8f%91%e5%90%8e%e7%ab%af%e7%9a%84%e7%ad%96%e7%95%a5">#&lt;/a>
&lt;/h2>
&lt;h5 id="service负载分发的策略有roundrobin和sessionaffinity">
 Service负载分发的策略有：RoundRobin和SessionAffinity
 &lt;a class="anchor" href="#service%e8%b4%9f%e8%bd%bd%e5%88%86%e5%8f%91%e7%9a%84%e7%ad%96%e7%95%a5%e6%9c%89roundrobin%e5%92%8csessionaffinity">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="-roundrobin默认为轮询模式即轮询将请求转发到后端的各个pod上">
 • RoundRobin：默认为轮询模式，即轮询将请求转发到后端的各个Pod上。
 &lt;a class="anchor" href="#-roundrobin%e9%bb%98%e8%ae%a4%e4%b8%ba%e8%bd%ae%e8%af%a2%e6%a8%a1%e5%bc%8f%e5%8d%b3%e8%bd%ae%e8%af%a2%e5%b0%86%e8%af%b7%e6%b1%82%e8%bd%ac%e5%8f%91%e5%88%b0%e5%90%8e%e7%ab%af%e7%9a%84%e5%90%84%e4%b8%aapod%e4%b8%8a">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-sessionaffinity基于客户端ip地址进行会话保持的模式即第1次将某个客户端发起的请求转发到后端的某个pod上之后从相同的客户端发起的请求都将被转发到后端相同的pod上">
 • SessionAffinity：基于客户端IP地址进行会话保持的模式，即第1次将某个客户端发起的请求转发到后端的某个Pod上，之后从相同的客户端发起的请求都将被转发到后端相同的Pod上。
 &lt;a class="anchor" href="#-sessionaffinity%e5%9f%ba%e4%ba%8e%e5%ae%a2%e6%88%b7%e7%ab%afip%e5%9c%b0%e5%9d%80%e8%bf%9b%e8%a1%8c%e4%bc%9a%e8%af%9d%e4%bf%9d%e6%8c%81%e7%9a%84%e6%a8%a1%e5%bc%8f%e5%8d%b3%e7%ac%ac1%e6%ac%a1%e5%b0%86%e6%9f%90%e4%b8%aa%e5%ae%a2%e6%88%b7%e7%ab%af%e5%8f%91%e8%b5%b7%e7%9a%84%e8%af%b7%e6%b1%82%e8%bd%ac%e5%8f%91%e5%88%b0%e5%90%8e%e7%ab%af%e7%9a%84%e6%9f%90%e4%b8%aapod%e4%b8%8a%e4%b9%8b%e5%90%8e%e4%bb%8e%e7%9b%b8%e5%90%8c%e7%9a%84%e5%ae%a2%e6%88%b7%e7%ab%af%e5%8f%91%e8%b5%b7%e7%9a%84%e8%af%b7%e6%b1%82%e9%83%bd%e5%b0%86%e8%a2%ab%e8%bd%ac%e5%8f%91%e5%88%b0%e5%90%8e%e7%ab%af%e7%9b%b8%e5%90%8c%e7%9a%84pod%e4%b8%8a">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h2 id="kubernetes外部如何访问集群内的服务">
 Kubernetes外部如何访问集群内的服务?
 &lt;a class="anchor" href="#kubernetes%e5%a4%96%e9%83%a8%e5%a6%82%e4%bd%95%e8%ae%bf%e9%97%ae%e9%9b%86%e7%be%a4%e5%86%85%e7%9a%84%e6%9c%8d%e5%8a%a1">#&lt;/a>
&lt;/h2>
&lt;ul>
&lt;li>
&lt;h5 id="-映射pod到物理机将pod端口号映射到宿主机即在pod中采用hostport方式以使客户端应用能够通过物理机访问容器应用">
 • 映射Pod到物理机：将Pod端口号映射到宿主机，即在Pod中采用hostPort方式，以使客户端应用能够通过物理机访问容器应用。
 &lt;a class="anchor" href="#-%e6%98%a0%e5%b0%84pod%e5%88%b0%e7%89%a9%e7%90%86%e6%9c%ba%e5%b0%86pod%e7%ab%af%e5%8f%a3%e5%8f%b7%e6%98%a0%e5%b0%84%e5%88%b0%e5%ae%bf%e4%b8%bb%e6%9c%ba%e5%8d%b3%e5%9c%a8pod%e4%b8%ad%e9%87%87%e7%94%a8hostport%e6%96%b9%e5%bc%8f%e4%bb%a5%e4%bd%bf%e5%ae%a2%e6%88%b7%e7%ab%af%e5%ba%94%e7%94%a8%e8%83%bd%e5%a4%9f%e9%80%9a%e8%bf%87%e7%89%a9%e7%90%86%e6%9c%ba%e8%ae%bf%e9%97%ae%e5%ae%b9%e5%99%a8%e5%ba%94%e7%94%a8">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-映射service到物理机将service端口号映射到宿主机即在service中采用nodeport方式以使客户端应用能够通过物理机访问容器应用">
 • 映射Service到物理机：将Service端口号映射到宿主机，即在Service中采用nodePort方式，以使客户端应用能够通过物理机访问容器应用。
 &lt;a class="anchor" href="#-%e6%98%a0%e5%b0%84service%e5%88%b0%e7%89%a9%e7%90%86%e6%9c%ba%e5%b0%86service%e7%ab%af%e5%8f%a3%e5%8f%b7%e6%98%a0%e5%b0%84%e5%88%b0%e5%ae%bf%e4%b8%bb%e6%9c%ba%e5%8d%b3%e5%9c%a8service%e4%b8%ad%e9%87%87%e7%94%a8nodeport%e6%96%b9%e5%bc%8f%e4%bb%a5%e4%bd%bf%e5%ae%a2%e6%88%b7%e7%ab%af%e5%ba%94%e7%94%a8%e8%83%bd%e5%a4%9f%e9%80%9a%e8%bf%87%e7%89%a9%e7%90%86%e6%9c%ba%e8%ae%bf%e9%97%ae%e5%ae%b9%e5%99%a8%e5%ba%94%e7%94%a8">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-映射sercie到loadbalancer通过设置loadbalancer映射到云服务商提供的loadbalancer地址这种用法仅用于在公有云服务提供商的云平台上设置service的场景">
 • 映射Sercie到LoadBalancer：通过设置LoadBalancer映射到云服务商提供的LoadBalancer地址。这种用法仅用于在公有云服务提供商的云平台上设置Service的场景。
 &lt;a class="anchor" href="#-%e6%98%a0%e5%b0%84sercie%e5%88%b0loadbalancer%e9%80%9a%e8%bf%87%e8%ae%be%e7%bd%aeloadbalancer%e6%98%a0%e5%b0%84%e5%88%b0%e4%ba%91%e6%9c%8d%e5%8a%a1%e5%95%86%e6%8f%90%e4%be%9b%e7%9a%84loadbalancer%e5%9c%b0%e5%9d%80%e8%bf%99%e7%a7%8d%e7%94%a8%e6%b3%95%e4%bb%85%e7%94%a8%e4%ba%8e%e5%9c%a8%e5%85%ac%e6%9c%89%e4%ba%91%e6%9c%8d%e5%8a%a1%e6%8f%90%e4%be%9b%e5%95%86%e7%9a%84%e4%ba%91%e5%b9%b3%e5%8f%b0%e4%b8%8a%e8%ae%be%e7%bd%aeservice%e7%9a%84%e5%9c%ba%e6%99%af">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h2 id="kubernetes-ingress">
 Kubernetes ingress?
 &lt;a class="anchor" href="#kubernetes-ingress">#&lt;/a>
&lt;/h2>
&lt;ul>
&lt;li>
&lt;h5 id="-kubernetes的ingress资源对象用于将不同url的访问请求转发到后端不同的service以实现http层的业务路由机制">
 • Kubernetes的Ingress资源对象，用于将不同URL的访问请求转发到后端不同的Service，以实现HTTP层的业务路由机制。
 &lt;a class="anchor" href="#-kubernetes%e7%9a%84ingress%e8%b5%84%e6%ba%90%e5%af%b9%e8%b1%a1%e7%94%a8%e4%ba%8e%e5%b0%86%e4%b8%8d%e5%90%8curl%e7%9a%84%e8%ae%bf%e9%97%ae%e8%af%b7%e6%b1%82%e8%bd%ac%e5%8f%91%e5%88%b0%e5%90%8e%e7%ab%af%e4%b8%8d%e5%90%8c%e7%9a%84service%e4%bb%a5%e5%ae%9e%e7%8e%b0http%e5%b1%82%e7%9a%84%e4%b8%9a%e5%8a%a1%e8%b7%af%e7%94%b1%e6%9c%ba%e5%88%b6">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-kubernetes使用了ingress策略和ingress-controller两者结合并实现了一个完整的ingress负载均衡器使用ingress进行负载分发时ingress-controller基于ingress规则将客户端请求直接转发到service对应的后端endpointpod上从而跳过kube-proxy的转发功能kube-proxy不再起作用全过程为ingress-controller--ingress-规则---services">
 • Kubernetes使用了Ingress策略和Ingress Controller，两者结合并实现了一个完整的Ingress负载均衡器。使用Ingress进行负载分发时，Ingress Controller基于Ingress规则将客户端请求直接转发到Service对应的后端Endpoint（Pod）上，从而跳过kube-proxy的转发功能，kube-proxy不再起作用，全过程为：ingress controller + ingress 规则 &amp;mdash;-&amp;gt; services。
 &lt;a class="anchor" href="#-kubernetes%e4%bd%bf%e7%94%a8%e4%ba%86ingress%e7%ad%96%e7%95%a5%e5%92%8cingress-controller%e4%b8%a4%e8%80%85%e7%bb%93%e5%90%88%e5%b9%b6%e5%ae%9e%e7%8e%b0%e4%ba%86%e4%b8%80%e4%b8%aa%e5%ae%8c%e6%95%b4%e7%9a%84ingress%e8%b4%9f%e8%bd%bd%e5%9d%87%e8%a1%a1%e5%99%a8%e4%bd%bf%e7%94%a8ingress%e8%bf%9b%e8%a1%8c%e8%b4%9f%e8%bd%bd%e5%88%86%e5%8f%91%e6%97%b6ingress-controller%e5%9f%ba%e4%ba%8eingress%e8%a7%84%e5%88%99%e5%b0%86%e5%ae%a2%e6%88%b7%e7%ab%af%e8%af%b7%e6%b1%82%e7%9b%b4%e6%8e%a5%e8%bd%ac%e5%8f%91%e5%88%b0service%e5%af%b9%e5%ba%94%e7%9a%84%e5%90%8e%e7%ab%afendpointpod%e4%b8%8a%e4%bb%8e%e8%80%8c%e8%b7%b3%e8%bf%87kube-proxy%e7%9a%84%e8%bd%ac%e5%8f%91%e5%8a%9f%e8%83%bdkube-proxy%e4%b8%8d%e5%86%8d%e8%b5%b7%e4%bd%9c%e7%94%a8%e5%85%a8%e8%bf%87%e7%a8%8b%e4%b8%baingress-controller--ingress-%e8%a7%84%e5%88%99---services">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-同时当ingress-controller提供的是对外服务则实际上实现的是边缘路由器的功能">
 • 同时当Ingress Controller提供的是对外服务，则实际上实现的是边缘路由器的功能。
 &lt;a class="anchor" href="#-%e5%90%8c%e6%97%b6%e5%bd%93ingress-controller%e6%8f%90%e4%be%9b%e7%9a%84%e6%98%af%e5%af%b9%e5%a4%96%e6%9c%8d%e5%8a%a1%e5%88%99%e5%ae%9e%e9%99%85%e4%b8%8a%e5%ae%9e%e7%8e%b0%e7%9a%84%e6%98%af%e8%be%b9%e7%bc%98%e8%b7%af%e7%94%b1%e5%99%a8%e7%9a%84%e5%8a%9f%e8%83%bd">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h2 id="kubernetes镜像的下载策略">
 Kubernetes镜像的下载策略?
 &lt;a class="anchor" href="#kubernetes%e9%95%9c%e5%83%8f%e7%9a%84%e4%b8%8b%e8%bd%bd%e7%ad%96%e7%95%a5">#&lt;/a>
&lt;/h2>
&lt;h5 id="k8s的镜像下载策略有三种alwaysneverifnotpresent">
 K8s的镜像下载策略有三种：Always、Never、IFNotPresent。
 &lt;a class="anchor" href="#k8s%e7%9a%84%e9%95%9c%e5%83%8f%e4%b8%8b%e8%bd%bd%e7%ad%96%e7%95%a5%e6%9c%89%e4%b8%89%e7%a7%8dalwaysneverifnotpresent">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="-always镜像标签为latest时总是从指定的仓库中获取镜像">
 • Always：镜像标签为latest时，总是从指定的仓库中获取镜像。
 &lt;a class="anchor" href="#-always%e9%95%9c%e5%83%8f%e6%a0%87%e7%ad%be%e4%b8%balatest%e6%97%b6%e6%80%bb%e6%98%af%e4%bb%8e%e6%8c%87%e5%ae%9a%e7%9a%84%e4%bb%93%e5%ba%93%e4%b8%ad%e8%8e%b7%e5%8f%96%e9%95%9c%e5%83%8f">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-never禁止从仓库中下载镜像也就是说只能使用本地镜像">
 • Never：禁止从仓库中下载镜像，也就是说只能使用本地镜像。
 &lt;a class="anchor" href="#-never%e7%a6%81%e6%ad%a2%e4%bb%8e%e4%bb%93%e5%ba%93%e4%b8%ad%e4%b8%8b%e8%bd%bd%e9%95%9c%e5%83%8f%e4%b9%9f%e5%b0%b1%e6%98%af%e8%af%b4%e5%8f%aa%e8%83%bd%e4%bd%bf%e7%94%a8%e6%9c%ac%e5%9c%b0%e9%95%9c%e5%83%8f">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-ifnotpresent仅当本地没有对应镜像时才从目标仓库中下载">
 • IfNotPresent：仅当本地没有对应镜像时，才从目标仓库中下载。
 &lt;a class="anchor" href="#-ifnotpresent%e4%bb%85%e5%bd%93%e6%9c%ac%e5%9c%b0%e6%b2%a1%e6%9c%89%e5%af%b9%e5%ba%94%e9%95%9c%e5%83%8f%e6%97%b6%e6%89%8d%e4%bb%8e%e7%9b%ae%e6%a0%87%e4%bb%93%e5%ba%93%e4%b8%ad%e4%b8%8b%e8%bd%bd">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h5 id="默认的镜像下载策略是当镜像标签是latest时默认策略是always当镜像标签是自定义时也就是标签不是latest那么默认策略是ifnotpresent">
 默认的镜像下载策略是：当镜像标签是latest时，默认策略是Always；当镜像标签是自定义时（也就是标签不是latest），那么默认策略是IfNotPresent。
 &lt;a class="anchor" href="#%e9%bb%98%e8%ae%a4%e7%9a%84%e9%95%9c%e5%83%8f%e4%b8%8b%e8%bd%bd%e7%ad%96%e7%95%a5%e6%98%af%e5%bd%93%e9%95%9c%e5%83%8f%e6%a0%87%e7%ad%be%e6%98%aflatest%e6%97%b6%e9%bb%98%e8%ae%a4%e7%ad%96%e7%95%a5%e6%98%afalways%e5%bd%93%e9%95%9c%e5%83%8f%e6%a0%87%e7%ad%be%e6%98%af%e8%87%aa%e5%ae%9a%e4%b9%89%e6%97%b6%e4%b9%9f%e5%b0%b1%e6%98%af%e6%a0%87%e7%ad%be%e4%b8%8d%e6%98%aflatest%e9%82%a3%e4%b9%88%e9%bb%98%e8%ae%a4%e7%ad%96%e7%95%a5%e6%98%afifnotpresent">#&lt;/a>
&lt;/h5>
&lt;h2 id="kubernetes-kubelet的作用">
 Kubernetes kubelet的作用?
 &lt;a class="anchor" href="#kubernetes-kubelet%e7%9a%84%e4%bd%9c%e7%94%a8">#&lt;/a>
&lt;/h2>
&lt;ul>
&lt;li>
&lt;h5 id="-在kubernetes集群中在每个node又称worker上都会启动一个kubelet服务进程该进程用于处理master下发到本节点的任务管理pod及pod中的容器每个kubelet进程都会在api-server上注册节点自身的信息定期向master汇报节点资源的使用情况并通过cadvisor监控容器和节点资源">
 • 在Kubernetes集群中，在每个Node（又称Worker）上都会启动一个kubelet服务进程。该进程用于处理Master下发到本节点的任务，管理Pod及Pod中的容器。每个kubelet进程都会在API Server上注册节点自身的信息，定期向Master汇报节点资源的使用情况，并通过cAdvisor监控容器和节点资源。
 &lt;a class="anchor" href="#-%e5%9c%a8kubernetes%e9%9b%86%e7%be%a4%e4%b8%ad%e5%9c%a8%e6%af%8f%e4%b8%aanode%e5%8f%88%e7%a7%b0worker%e4%b8%8a%e9%83%bd%e4%bc%9a%e5%90%af%e5%8a%a8%e4%b8%80%e4%b8%aakubelet%e6%9c%8d%e5%8a%a1%e8%bf%9b%e7%a8%8b%e8%af%a5%e8%bf%9b%e7%a8%8b%e7%94%a8%e4%ba%8e%e5%a4%84%e7%90%86master%e4%b8%8b%e5%8f%91%e5%88%b0%e6%9c%ac%e8%8a%82%e7%82%b9%e7%9a%84%e4%bb%bb%e5%8a%a1%e7%ae%a1%e7%90%86pod%e5%8f%8apod%e4%b8%ad%e7%9a%84%e5%ae%b9%e5%99%a8%e6%af%8f%e4%b8%aakubelet%e8%bf%9b%e7%a8%8b%e9%83%bd%e4%bc%9a%e5%9c%a8api-server%e4%b8%8a%e6%b3%a8%e5%86%8c%e8%8a%82%e7%82%b9%e8%87%aa%e8%ba%ab%e7%9a%84%e4%bf%a1%e6%81%af%e5%ae%9a%e6%9c%9f%e5%90%91master%e6%b1%87%e6%8a%a5%e8%8a%82%e7%82%b9%e8%b5%84%e6%ba%90%e7%9a%84%e4%bd%bf%e7%94%a8%e6%83%85%e5%86%b5%e5%b9%b6%e9%80%9a%e8%bf%87cadvisor%e7%9b%91%e6%8e%a7%e5%ae%b9%e5%99%a8%e5%92%8c%e8%8a%82%e7%82%b9%e8%b5%84%e6%ba%90">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h2 id="secret有哪些使用方式">
 Secret有哪些使用方式?
 &lt;a class="anchor" href="#secret%e6%9c%89%e5%93%aa%e4%ba%9b%e4%bd%bf%e7%94%a8%e6%96%b9%e5%bc%8f">#&lt;/a>
&lt;/h2>
&lt;ul>
&lt;li>
&lt;h5 id="-创建完secret之后可通过如下三种方式使用">
 • 创建完secret之后，可通过如下三种方式使用：
 &lt;a class="anchor" href="#-%e5%88%9b%e5%bb%ba%e5%ae%8csecret%e4%b9%8b%e5%90%8e%e5%8f%af%e9%80%9a%e8%bf%87%e5%a6%82%e4%b8%8b%e4%b8%89%e7%a7%8d%e6%96%b9%e5%bc%8f%e4%bd%bf%e7%94%a8">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-在创建pod时通过为pod指定service-account来自动使用该secret">
 • 在创建Pod时，通过为Pod指定Service Account来自动使用该Secret。
 &lt;a class="anchor" href="#-%e5%9c%a8%e5%88%9b%e5%bb%bapod%e6%97%b6%e9%80%9a%e8%bf%87%e4%b8%bapod%e6%8c%87%e5%ae%9aservice-account%e6%9d%a5%e8%87%aa%e5%8a%a8%e4%bd%bf%e7%94%a8%e8%af%a5secret">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-通过挂载该secret到pod来使用它">
 • 通过挂载该Secret到Pod来使用它。
 &lt;a class="anchor" href="#-%e9%80%9a%e8%bf%87%e6%8c%82%e8%bd%bd%e8%af%a5secret%e5%88%b0pod%e6%9d%a5%e4%bd%bf%e7%94%a8%e5%ae%83">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-在docker镜像下载时使用通过指定pod的spcimagepullsecrets来引用它">
 • 在Docker镜像下载时使用，通过指定Pod的spc.ImagePullSecrets来引用它。
 &lt;a class="anchor" href="#-%e5%9c%a8docker%e9%95%9c%e5%83%8f%e4%b8%8b%e8%bd%bd%e6%97%b6%e4%bd%bf%e7%94%a8%e9%80%9a%e8%bf%87%e6%8c%87%e5%ae%9apod%e7%9a%84spcimagepullsecrets%e6%9d%a5%e5%bc%95%e7%94%a8%e5%ae%83">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h2 id="kubernetes-cni模型">
 Kubernetes CNI模型?
 &lt;a class="anchor" href="#kubernetes-cni%e6%a8%a1%e5%9e%8b">#&lt;/a>
&lt;/h2>
&lt;ul>
&lt;li>
&lt;h5 id="-cni提供了一种应用容器的插件化网络解决方案定义对容器网络进行操作和配置的规范通过插件的形式对cni接口进行实现cni仅关注在创建容器时分配网络资源和在销毁容器时删除网络资源在cni模型中只涉及两个概念容器和网络">
 • CNI提供了一种应用容器的插件化网络解决方案，定义对容器网络进行操作和配置的规范，通过插件的形式对CNI接口进行实现。CNI仅关注在创建容器时分配网络资源，和在销毁容器时删除网络资源。在CNI模型中只涉及两个概念：容器和网络。
 &lt;a class="anchor" href="#-cni%e6%8f%90%e4%be%9b%e4%ba%86%e4%b8%80%e7%a7%8d%e5%ba%94%e7%94%a8%e5%ae%b9%e5%99%a8%e7%9a%84%e6%8f%92%e4%bb%b6%e5%8c%96%e7%bd%91%e7%bb%9c%e8%a7%a3%e5%86%b3%e6%96%b9%e6%a1%88%e5%ae%9a%e4%b9%89%e5%af%b9%e5%ae%b9%e5%99%a8%e7%bd%91%e7%bb%9c%e8%bf%9b%e8%a1%8c%e6%93%8d%e4%bd%9c%e5%92%8c%e9%85%8d%e7%bd%ae%e7%9a%84%e8%a7%84%e8%8c%83%e9%80%9a%e8%bf%87%e6%8f%92%e4%bb%b6%e7%9a%84%e5%bd%a2%e5%bc%8f%e5%af%b9cni%e6%8e%a5%e5%8f%a3%e8%bf%9b%e8%a1%8c%e5%ae%9e%e7%8e%b0cni%e4%bb%85%e5%85%b3%e6%b3%a8%e5%9c%a8%e5%88%9b%e5%bb%ba%e5%ae%b9%e5%99%a8%e6%97%b6%e5%88%86%e9%85%8d%e7%bd%91%e7%bb%9c%e8%b5%84%e6%ba%90%e5%92%8c%e5%9c%a8%e9%94%80%e6%af%81%e5%ae%b9%e5%99%a8%e6%97%b6%e5%88%a0%e9%99%a4%e7%bd%91%e7%bb%9c%e8%b5%84%e6%ba%90%e5%9c%a8cni%e6%a8%a1%e5%9e%8b%e4%b8%ad%e5%8f%aa%e6%b6%89%e5%8f%8a%e4%b8%a4%e4%b8%aa%e6%a6%82%e5%bf%b5%e5%ae%b9%e5%99%a8%e5%92%8c%e7%bd%91%e7%bb%9c">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-容器container是拥有独立linux网络命名空间的环境例如使用docker或rkt创建的容器容器需要拥有自己的linux网络命名空间这是加入网络的必要条件">
 • 容器（Container）：是拥有独立Linux网络命名空间的环境，例如使用Docker或rkt创建的容器。容器需要拥有自己的Linux网络命名空间，这是加入网络的必要条件。
 &lt;a class="anchor" href="#-%e5%ae%b9%e5%99%a8container%e6%98%af%e6%8b%a5%e6%9c%89%e7%8b%ac%e7%ab%8blinux%e7%bd%91%e7%bb%9c%e5%91%bd%e5%90%8d%e7%a9%ba%e9%97%b4%e7%9a%84%e7%8e%af%e5%a2%83%e4%be%8b%e5%a6%82%e4%bd%bf%e7%94%a8docker%e6%88%96rkt%e5%88%9b%e5%bb%ba%e7%9a%84%e5%ae%b9%e5%99%a8%e5%ae%b9%e5%99%a8%e9%9c%80%e8%a6%81%e6%8b%a5%e6%9c%89%e8%87%aa%e5%b7%b1%e7%9a%84linux%e7%bd%91%e7%bb%9c%e5%91%bd%e5%90%8d%e7%a9%ba%e9%97%b4%e8%bf%99%e6%98%af%e5%8a%a0%e5%85%a5%e7%bd%91%e7%bb%9c%e7%9a%84%e5%bf%85%e8%a6%81%e6%9d%a1%e4%bb%b6">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-网络network表示可以互连的一组实体这些实体拥有各自独立唯一的ip地址可以是容器物理机或者其他网络设备比如路由器等">
 • 网络（Network）：表示可以互连的一组实体，这些实体拥有各自独立、唯一的IP地址，可以是容器、物理机或者其他网络设备（比如路由器）等。
 &lt;a class="anchor" href="#-%e7%bd%91%e7%bb%9cnetwork%e8%a1%a8%e7%a4%ba%e5%8f%af%e4%bb%a5%e4%ba%92%e8%bf%9e%e7%9a%84%e4%b8%80%e7%bb%84%e5%ae%9e%e4%bd%93%e8%bf%99%e4%ba%9b%e5%ae%9e%e4%bd%93%e6%8b%a5%e6%9c%89%e5%90%84%e8%87%aa%e7%8b%ac%e7%ab%8b%e5%94%af%e4%b8%80%e7%9a%84ip%e5%9c%b0%e5%9d%80%e5%8f%af%e4%bb%a5%e6%98%af%e5%ae%b9%e5%99%a8%e7%89%a9%e7%90%86%e6%9c%ba%e6%88%96%e8%80%85%e5%85%b6%e4%bb%96%e7%bd%91%e7%bb%9c%e8%ae%be%e5%a4%87%e6%af%94%e5%a6%82%e8%b7%af%e7%94%b1%e5%99%a8%e7%ad%89">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-对容器网络的设置和操作都通过插件plugin进行具体实现cni插件包括两种类型cni-plugin和ipamip-address-managementplugincni-plugin负责为容器配置网络资源ipam-plugin负责对容器的ip地址进行分配和管理ipam-plugin作为cni-plugin的一部分与cni-plugin协同工作">
 • 对容器网络的设置和操作都通过插件（Plugin）进行具体实现，CNI插件包括两种类型：CNI Plugin和IPAM（IP Address Management）Plugin。CNI Plugin负责为容器配置网络资源，IPAM Plugin负责对容器的IP地址进行分配和管理。IPAM Plugin作为CNI Plugin的一部分，与CNI Plugin协同工作。
 &lt;a class="anchor" href="#-%e5%af%b9%e5%ae%b9%e5%99%a8%e7%bd%91%e7%bb%9c%e7%9a%84%e8%ae%be%e7%bd%ae%e5%92%8c%e6%93%8d%e4%bd%9c%e9%83%bd%e9%80%9a%e8%bf%87%e6%8f%92%e4%bb%b6plugin%e8%bf%9b%e8%a1%8c%e5%85%b7%e4%bd%93%e5%ae%9e%e7%8e%b0cni%e6%8f%92%e4%bb%b6%e5%8c%85%e6%8b%ac%e4%b8%a4%e7%a7%8d%e7%b1%bb%e5%9e%8bcni-plugin%e5%92%8cipamip-address-managementplugincni-plugin%e8%b4%9f%e8%b4%a3%e4%b8%ba%e5%ae%b9%e5%99%a8%e9%85%8d%e7%bd%ae%e7%bd%91%e7%bb%9c%e8%b5%84%e6%ba%90ipam-plugin%e8%b4%9f%e8%b4%a3%e5%af%b9%e5%ae%b9%e5%99%a8%e7%9a%84ip%e5%9c%b0%e5%9d%80%e8%bf%9b%e8%a1%8c%e5%88%86%e9%85%8d%e5%92%8c%e7%ae%a1%e7%90%86ipam-plugin%e4%bd%9c%e4%b8%bacni-plugin%e7%9a%84%e4%b8%80%e9%83%a8%e5%88%86%e4%b8%8ecni-plugin%e5%8d%8f%e5%90%8c%e5%b7%a5%e4%bd%9c">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h2 id="kubernetes-pv和pvc">
 Kubernetes PV和PVC?
 &lt;a class="anchor" href="#kubernetes-pv%e5%92%8cpvc">#&lt;/a>
&lt;/h2>
&lt;ul>
&lt;li>
&lt;h5 id="-pv是对底层网络共享存储的抽象将共享存储定义为一种资源">
 • PV是对底层网络共享存储的抽象，将共享存储定义为一种“资源”。
 &lt;a class="anchor" href="#-pv%e6%98%af%e5%af%b9%e5%ba%95%e5%b1%82%e7%bd%91%e7%bb%9c%e5%85%b1%e4%ba%ab%e5%ad%98%e5%82%a8%e7%9a%84%e6%8a%bd%e8%b1%a1%e5%b0%86%e5%85%b1%e4%ba%ab%e5%ad%98%e5%82%a8%e5%ae%9a%e4%b9%89%e4%b8%ba%e4%b8%80%e7%a7%8d%e8%b5%84%e6%ba%90">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-pvc则是用户对存储资源的一个申请">
 • PVC则是用户对存储资源的一个“申请”。
 &lt;a class="anchor" href="#-pvc%e5%88%99%e6%98%af%e7%94%a8%e6%88%b7%e5%af%b9%e5%ad%98%e5%82%a8%e8%b5%84%e6%ba%90%e7%9a%84%e4%b8%80%e4%b8%aa%e7%94%b3%e8%af%b7">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h2 id="pv生命周期内的阶段">
 PV生命周期内的阶段?
 &lt;a class="anchor" href="#pv%e7%94%9f%e5%91%bd%e5%91%a8%e6%9c%9f%e5%86%85%e7%9a%84%e9%98%b6%e6%ae%b5">#&lt;/a>
&lt;/h2>
&lt;h5 id="某个pv在生命周期中可能处于以下4个阶段phaes之一">
 某个PV在生命周期中可能处于以下4个阶段（Phaes）之一。
 &lt;a class="anchor" href="#%e6%9f%90%e4%b8%aapv%e5%9c%a8%e7%94%9f%e5%91%bd%e5%91%a8%e6%9c%9f%e4%b8%ad%e5%8f%af%e8%83%bd%e5%a4%84%e4%ba%8e%e4%bb%a5%e4%b8%8b4%e4%b8%aa%e9%98%b6%e6%ae%b5phaes%e4%b9%8b%e4%b8%80">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="-available可用状态还未与某个pvc绑定">
 • Available：可用状态，还未与某个PVC绑定。
 &lt;a class="anchor" href="#-available%e5%8f%af%e7%94%a8%e7%8a%b6%e6%80%81%e8%bf%98%e6%9c%aa%e4%b8%8e%e6%9f%90%e4%b8%aapvc%e7%bb%91%e5%ae%9a">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-bound已与某个pvc绑定">
 • Bound：已与某个PVC绑定。
 &lt;a class="anchor" href="#-bound%e5%b7%b2%e4%b8%8e%e6%9f%90%e4%b8%aapvc%e7%bb%91%e5%ae%9a">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-released绑定的pvc已经删除资源已释放但没有被集群回收">
 • Released：绑定的PVC已经删除，资源已释放，但没有被集群回收。
 &lt;a class="anchor" href="#-released%e7%bb%91%e5%ae%9a%e7%9a%84pvc%e5%b7%b2%e7%bb%8f%e5%88%a0%e9%99%a4%e8%b5%84%e6%ba%90%e5%b7%b2%e9%87%8a%e6%94%be%e4%bd%86%e6%b2%a1%e6%9c%89%e8%a2%ab%e9%9b%86%e7%be%a4%e5%9b%9e%e6%94%b6">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-failed自动资源回收失败">
 • Failed：自动资源回收失败。
 &lt;a class="anchor" href="#-failed%e8%87%aa%e5%8a%a8%e8%b5%84%e6%ba%90%e5%9b%9e%e6%94%b6%e5%a4%b1%e8%b4%a5">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h2 id="calico-网络模式">
 calico 网络模式
 &lt;a class="anchor" href="#calico-%e7%bd%91%e7%bb%9c%e6%a8%a1%e5%bc%8f">#&lt;/a>
&lt;/h2>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th>&lt;strong>模式&lt;/strong>&lt;/th>
 &lt;th>&lt;strong>数据包封包&lt;/strong>&lt;/th>
 &lt;th>&lt;strong>优点&lt;/strong>&lt;/th>
 &lt;th>&lt;strong>缺点&lt;/strong>&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td>vxlan&lt;/td>
 &lt;td>封包， 在vxlan设备上将pod发来的数据包源、目的mac替换为本机vxlan网卡和对端节点vxlan网卡的mac。外层udp目的ip地址根据路由和对端vxlan的mac查fdb表获取&lt;/td>
 &lt;td>只要k8s节点间三层互通， 可以跨网段， 对主机网关路由没有特殊要求。各个node节点通过vxlan设备实现基于三层的”二层”互通, 三层即vxlan包封装在udp数据包中， 要求udp在k8s节点间三层可达；二层即vxlan封包的源mac地址和目的mac地址是自己的vxlan设备mac和对端vxlan设备mac。&lt;/td>
 &lt;td>需要进行vxlan的数据包封包和解包会存在一定的性能损耗&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>ipip&lt;/td>
 &lt;td>封包，在tunl0设备上将pod发来的数据包的mac层去掉，留下ip层封包。 外层数据包目的ip地址根据路由得到。&lt;/td>
 &lt;td>只要k8s节点间三层互通， 可以跨网段， 对主机网关路由没有特殊要求。&lt;/td>
 &lt;td>需要进行ipip的数据包封包和解包会存在一定的性能损耗&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>bgp&lt;/td>
 &lt;td>不需要进行数据包封包&lt;/td>
 &lt;td>不用封包解包，通过bgp协议可实现pod网络在主机间的三层可达， k8s节点不跨网段时和flannel的host-gw相似；&lt;/td>
 &lt;td>&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>支持跨网段， 满足复杂的网络架构&lt;/td>
 &lt;td>跨网段时，需要主机网关路由也充当BGP Speaker能够学习到pod子网路由并实现pod子网路由的转发&lt;/td>
 &lt;td>&lt;/td>
 &lt;td>&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;h2 id="fannel三种模式">
 fannel三种模式
 &lt;a class="anchor" href="#fannel%e4%b8%89%e7%a7%8d%e6%a8%a1%e5%bc%8f">#&lt;/a>
&lt;/h2>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th>&lt;strong>fannel三种模式&lt;/strong>&lt;/th>
 &lt;th>&lt;strong>效率&lt;/strong>&lt;/th>
 &lt;th>&lt;strong>calico 模式&lt;/strong>&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td>UDP&lt;/td>
 &lt;td>性能较差，封包解包涉及到多次用户态和内核态交互&lt;/td>
 &lt;td>类似 IPIP&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>VXLAN&lt;/td>
 &lt;td>性能较好，封包解包在内核态实现，内核转发数据，flanneld负责动态配置ARP和FDB（转发数据库）表项更新&lt;/td>
 &lt;td>类似VXLAN&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>host-gw&lt;/td>
 &lt;td>性能最好，不需要再次封包，正常发包，目的容器所在的主机充当网关&lt;/td>
 &lt;td>&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>flanneld 负责主机上路由表的刷新&lt;/td>
 &lt;td>类似 BGP&lt;/td>
 &lt;td>&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;h2 id="你知道的几种cni网络插件并详述其工作原理k8s常用的cni网络插件-calico--flannel简述一下它们的工作原理和区别">
 你知道的几种CNI网络插件，并详述其工作原理。K8s常用的CNI网络插件 （calico &amp;amp;&amp;amp; flannel），简述一下它们的工作原理和区别。
 &lt;a class="anchor" href="#%e4%bd%a0%e7%9f%a5%e9%81%93%e7%9a%84%e5%87%a0%e7%a7%8dcni%e7%bd%91%e7%bb%9c%e6%8f%92%e4%bb%b6%e5%b9%b6%e8%af%a6%e8%bf%b0%e5%85%b6%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86k8s%e5%b8%b8%e7%94%a8%e7%9a%84cni%e7%bd%91%e7%bb%9c%e6%8f%92%e4%bb%b6-calico--flannel%e7%ae%80%e8%bf%b0%e4%b8%80%e4%b8%8b%e5%ae%83%e4%bb%ac%e7%9a%84%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86%e5%92%8c%e5%8c%ba%e5%88%ab">#&lt;/a>
&lt;/h2>
&lt;ol>
&lt;li>
&lt;h5 id="1-calico根据iptables规则进行路由转发并没有进行封包解包的过程这和flannel比起来效率就会快多-calico包括如下重要组件felixetcdbgp-clientbgp-route-reflector下面分别说明一下这些组件">
 \1. calico根据iptables规则进行路由转发，并没有进行封包，解包的过程，这和flannel比起来效率就会快多 calico包括如下重要组件：Felix，etcd，BGP Client，BGP Route Reflector。下面分别说明一下这些组件。
 &lt;a class="anchor" href="#1-calico%e6%a0%b9%e6%8d%aeiptables%e8%a7%84%e5%88%99%e8%bf%9b%e8%a1%8c%e8%b7%af%e7%94%b1%e8%bd%ac%e5%8f%91%e5%b9%b6%e6%b2%a1%e6%9c%89%e8%bf%9b%e8%a1%8c%e5%b0%81%e5%8c%85%e8%a7%a3%e5%8c%85%e7%9a%84%e8%bf%87%e7%a8%8b%e8%bf%99%e5%92%8cflannel%e6%af%94%e8%b5%b7%e6%9d%a5%e6%95%88%e7%8e%87%e5%b0%b1%e4%bc%9a%e5%bf%ab%e5%a4%9a-calico%e5%8c%85%e6%8b%ac%e5%a6%82%e4%b8%8b%e9%87%8d%e8%a6%81%e7%bb%84%e4%bb%b6felixetcdbgp-clientbgp-route-reflector%e4%b8%8b%e9%9d%a2%e5%88%86%e5%88%ab%e8%af%b4%e6%98%8e%e4%b8%80%e4%b8%8b%e8%bf%99%e4%ba%9b%e7%bb%84%e4%bb%b6">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ol>
&lt;h5 id="felix主要负责路由配置以及acls规则的配置以及下发它存在在每个node节点上-etcd分布式键值存储主要负责网络元数据一致性确保calico网络状态的准确性可以与kubernetes共用-bgpclientbird-主要负责把-felix写入-kernel的路由信息分发到当前-calico网络确保-workload间的通信的有效性-bgproute-reflectorbird-大规模部署时使用摒弃所有节点互联的mesh模式通过一个或者多个-bgproute-reflector-来完成集中式的路由分发-通过将整个互联网的可扩展-ip网络原则压缩到数据中心级别calico在每一个计算节点利用-linuxkernel-实现了一个高效的-vrouter来负责数据转发而每个vrouter通过-bgp协议负责把自己上运行的-workload的路由信息向整个calico网络内传播小规模部署可以直接互联大规模下可通过指定的bgproute-reflector-来完成这样保证最终所有的workload之间的数据流量都是通过-ip包的方式完成互联的">
 Felix：主要负责路由配置以及ACLS规则的配置以及下发，它存在在每个node节点上。 etcd：分布式键值存储，主要负责网络元数据一致性，确保Calico网络状态的准确性，可以与kubernetes共用； BGPClient(BIRD), 主要负责把 Felix写入 kernel的路由信息分发到当前 Calico网络，确保 workload间的通信的有效性； BGPRoute Reflector(BIRD), 大规模部署时使用，摒弃所有节点互联的mesh模式，通过一个或者多个 BGPRoute Reflector 来完成集中式的路由分发 通过将整个互联网的可扩展 IP网络原则压缩到数据中心级别，Calico在每一个计算节点利用 Linuxkernel 实现了一个高效的 vRouter来负责数据转发，而每个vRouter通过 BGP协议负责把自己上运行的 workload的路由信息向整个Calico网络内传播，小规模部署可以直接互联，大规模下可通过指定的BGProute reflector 来完成。这样保证最终所有的workload之间的数据流量都是通过 IP包的方式完成互联的。
 &lt;a class="anchor" href="#felix%e4%b8%bb%e8%a6%81%e8%b4%9f%e8%b4%a3%e8%b7%af%e7%94%b1%e9%85%8d%e7%bd%ae%e4%bb%a5%e5%8f%8aacls%e8%a7%84%e5%88%99%e7%9a%84%e9%85%8d%e7%bd%ae%e4%bb%a5%e5%8f%8a%e4%b8%8b%e5%8f%91%e5%ae%83%e5%ad%98%e5%9c%a8%e5%9c%a8%e6%af%8f%e4%b8%aanode%e8%8a%82%e7%82%b9%e4%b8%8a-etcd%e5%88%86%e5%b8%83%e5%bc%8f%e9%94%ae%e5%80%bc%e5%ad%98%e5%82%a8%e4%b8%bb%e8%a6%81%e8%b4%9f%e8%b4%a3%e7%bd%91%e7%bb%9c%e5%85%83%e6%95%b0%e6%8d%ae%e4%b8%80%e8%87%b4%e6%80%a7%e7%a1%ae%e4%bf%9dcalico%e7%bd%91%e7%bb%9c%e7%8a%b6%e6%80%81%e7%9a%84%e5%87%86%e7%a1%ae%e6%80%a7%e5%8f%af%e4%bb%a5%e4%b8%8ekubernetes%e5%85%b1%e7%94%a8-bgpclientbird-%e4%b8%bb%e8%a6%81%e8%b4%9f%e8%b4%a3%e6%8a%8a-felix%e5%86%99%e5%85%a5-kernel%e7%9a%84%e8%b7%af%e7%94%b1%e4%bf%a1%e6%81%af%e5%88%86%e5%8f%91%e5%88%b0%e5%bd%93%e5%89%8d-calico%e7%bd%91%e7%bb%9c%e7%a1%ae%e4%bf%9d-workload%e9%97%b4%e7%9a%84%e9%80%9a%e4%bf%a1%e7%9a%84%e6%9c%89%e6%95%88%e6%80%a7-bgproute-reflectorbird-%e5%a4%a7%e8%a7%84%e6%a8%a1%e9%83%a8%e7%bd%b2%e6%97%b6%e4%bd%bf%e7%94%a8%e6%91%92%e5%bc%83%e6%89%80%e6%9c%89%e8%8a%82%e7%82%b9%e4%ba%92%e8%81%94%e7%9a%84mesh%e6%a8%a1%e5%bc%8f%e9%80%9a%e8%bf%87%e4%b8%80%e4%b8%aa%e6%88%96%e8%80%85%e5%a4%9a%e4%b8%aa-bgproute-reflector-%e6%9d%a5%e5%ae%8c%e6%88%90%e9%9b%86%e4%b8%ad%e5%bc%8f%e7%9a%84%e8%b7%af%e7%94%b1%e5%88%86%e5%8f%91-%e9%80%9a%e8%bf%87%e5%b0%86%e6%95%b4%e4%b8%aa%e4%ba%92%e8%81%94%e7%bd%91%e7%9a%84%e5%8f%af%e6%89%a9%e5%b1%95-ip%e7%bd%91%e7%bb%9c%e5%8e%9f%e5%88%99%e5%8e%8b%e7%bc%a9%e5%88%b0%e6%95%b0%e6%8d%ae%e4%b8%ad%e5%bf%83%e7%ba%a7%e5%88%abcalico%e5%9c%a8%e6%af%8f%e4%b8%80%e4%b8%aa%e8%ae%a1%e7%ae%97%e8%8a%82%e7%82%b9%e5%88%a9%e7%94%a8-linuxkernel-%e5%ae%9e%e7%8e%b0%e4%ba%86%e4%b8%80%e4%b8%aa%e9%ab%98%e6%95%88%e7%9a%84-vrouter%e6%9d%a5%e8%b4%9f%e8%b4%a3%e6%95%b0%e6%8d%ae%e8%bd%ac%e5%8f%91%e8%80%8c%e6%af%8f%e4%b8%aavrouter%e9%80%9a%e8%bf%87-bgp%e5%8d%8f%e8%ae%ae%e8%b4%9f%e8%b4%a3%e6%8a%8a%e8%87%aa%e5%b7%b1%e4%b8%8a%e8%bf%90%e8%a1%8c%e7%9a%84-workload%e7%9a%84%e8%b7%af%e7%94%b1%e4%bf%a1%e6%81%af%e5%90%91%e6%95%b4%e4%b8%aacalico%e7%bd%91%e7%bb%9c%e5%86%85%e4%bc%a0%e6%92%ad%e5%b0%8f%e8%a7%84%e6%a8%a1%e9%83%a8%e7%bd%b2%e5%8f%af%e4%bb%a5%e7%9b%b4%e6%8e%a5%e4%ba%92%e8%81%94%e5%a4%a7%e8%a7%84%e6%a8%a1%e4%b8%8b%e5%8f%af%e9%80%9a%e8%bf%87%e6%8c%87%e5%ae%9a%e7%9a%84bgproute-reflector-%e6%9d%a5%e5%ae%8c%e6%88%90%e8%bf%99%e6%a0%b7%e4%bf%9d%e8%af%81%e6%9c%80%e7%bb%88%e6%89%80%e6%9c%89%e7%9a%84workload%e4%b9%8b%e9%97%b4%e7%9a%84%e6%95%b0%e6%8d%ae%e6%b5%81%e9%87%8f%e9%83%bd%e6%98%af%e9%80%9a%e8%bf%87-ip%e5%8c%85%e7%9a%84%e6%96%b9%e5%bc%8f%e5%ae%8c%e6%88%90%e4%ba%92%e8%81%94%e7%9a%84">#&lt;/a>
&lt;/h5>
&lt;ol>
&lt;li>
&lt;h5 id="1-flannel的工作原理-flannel实质上是一种覆盖网络overlay-network也就是将tcp数据包装在另一种网络包里面进行路由转发和通信目前已经支持udpvxlanaws-vpc和gce路由等数据转发方式">
 \1. Flannel的工作原理： Flannel实质上是一种“覆盖网络(overlay network)”，也就是将TCP数据包装在另一种网络包里面进行路由转发和通信，目前已经支持UDP、VxLAN、AWS VPC和GCE路由等数据转发方式。
 &lt;a class="anchor" href="#1-flannel%e7%9a%84%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86-flannel%e5%ae%9e%e8%b4%a8%e4%b8%8a%e6%98%af%e4%b8%80%e7%a7%8d%e8%a6%86%e7%9b%96%e7%bd%91%e7%bb%9coverlay-network%e4%b9%9f%e5%b0%b1%e6%98%af%e5%b0%86tcp%e6%95%b0%e6%8d%ae%e5%8c%85%e8%a3%85%e5%9c%a8%e5%8f%a6%e4%b8%80%e7%a7%8d%e7%bd%91%e7%bb%9c%e5%8c%85%e9%87%8c%e9%9d%a2%e8%bf%9b%e8%a1%8c%e8%b7%af%e7%94%b1%e8%bd%ac%e5%8f%91%e5%92%8c%e9%80%9a%e4%bf%a1%e7%9b%ae%e5%89%8d%e5%b7%b2%e7%bb%8f%e6%94%af%e6%8c%81udpvxlanaws-vpc%e5%92%8cgce%e8%b7%af%e7%94%b1%e7%ad%89%e6%95%b0%e6%8d%ae%e8%bd%ac%e5%8f%91%e6%96%b9%e5%bc%8f">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ol>
&lt;h5 id="默认的节点间数据通信方式是udp转发-工作原理-数据从源容器中发出后经由所在主机的docker0虚拟网卡转发到flannel0虚拟网卡先可以不经过docker0网卡使用cni模式这是个p2p的虚拟网卡flanneld服务监听在网卡的另外一端-flannel通过etcd服务维护了一张节点间的路由表详细记录了各节点子网网段--源主机的flanneld服务将原本的数据内容udp封装后根据自己的路由表投递给目的节点的flanneld服务数据到达以后被解包然后直接进入目的节点的flannel0虚拟网卡然后被转发到目的主机的docker0虚拟网卡最后就像本机容器通信一下的有docker0路由到达目标容器-flannel在进行路由转发的基础上进行了封包解包的操作这样浪费了cpu的计算资源">
 默认的节点间数据通信方式是UDP转发。 工作原理： 数据从源容器中发出后，经由所在主机的docker0虚拟网卡转发到flannel0虚拟网卡（先可以不经过docker0网卡，使用cni模式），这是个P2P的虚拟网卡，flanneld服务监听在网卡的另外一端。 Flannel通过Etcd服务维护了一张节点间的路由表，详细记录了各节点子网网段 。 源主机的flanneld服务将原本的数据内容UDP封装后根据自己的路由表投递给目的节点的flanneld服务，数据到达以后被解包，然后直接进入目的节点的flannel0虚拟网卡，然后被转发到目的主机的docker0虚拟网卡，最后就像本机容器通信一下的有docker0路由到达目标容器。 flannel在进行路由转发的基础上进行了封包解包的操作，这样浪费了CPU的计算资源。
 &lt;a class="anchor" href="#%e9%bb%98%e8%ae%a4%e7%9a%84%e8%8a%82%e7%82%b9%e9%97%b4%e6%95%b0%e6%8d%ae%e9%80%9a%e4%bf%a1%e6%96%b9%e5%bc%8f%e6%98%afudp%e8%bd%ac%e5%8f%91-%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86-%e6%95%b0%e6%8d%ae%e4%bb%8e%e6%ba%90%e5%ae%b9%e5%99%a8%e4%b8%ad%e5%8f%91%e5%87%ba%e5%90%8e%e7%bb%8f%e7%94%b1%e6%89%80%e5%9c%a8%e4%b8%bb%e6%9c%ba%e7%9a%84docker0%e8%99%9a%e6%8b%9f%e7%bd%91%e5%8d%a1%e8%bd%ac%e5%8f%91%e5%88%b0flannel0%e8%99%9a%e6%8b%9f%e7%bd%91%e5%8d%a1%e5%85%88%e5%8f%af%e4%bb%a5%e4%b8%8d%e7%bb%8f%e8%bf%87docker0%e7%bd%91%e5%8d%a1%e4%bd%bf%e7%94%a8cni%e6%a8%a1%e5%bc%8f%e8%bf%99%e6%98%af%e4%b8%aap2p%e7%9a%84%e8%99%9a%e6%8b%9f%e7%bd%91%e5%8d%a1flanneld%e6%9c%8d%e5%8a%a1%e7%9b%91%e5%90%ac%e5%9c%a8%e7%bd%91%e5%8d%a1%e7%9a%84%e5%8f%a6%e5%a4%96%e4%b8%80%e7%ab%af-flannel%e9%80%9a%e8%bf%87etcd%e6%9c%8d%e5%8a%a1%e7%bb%b4%e6%8a%a4%e4%ba%86%e4%b8%80%e5%bc%a0%e8%8a%82%e7%82%b9%e9%97%b4%e7%9a%84%e8%b7%af%e7%94%b1%e8%a1%a8%e8%af%a6%e7%bb%86%e8%ae%b0%e5%bd%95%e4%ba%86%e5%90%84%e8%8a%82%e7%82%b9%e5%ad%90%e7%bd%91%e7%bd%91%e6%ae%b5--%e6%ba%90%e4%b8%bb%e6%9c%ba%e7%9a%84flanneld%e6%9c%8d%e5%8a%a1%e5%b0%86%e5%8e%9f%e6%9c%ac%e7%9a%84%e6%95%b0%e6%8d%ae%e5%86%85%e5%ae%b9udp%e5%b0%81%e8%a3%85%e5%90%8e%e6%a0%b9%e6%8d%ae%e8%87%aa%e5%b7%b1%e7%9a%84%e8%b7%af%e7%94%b1%e8%a1%a8%e6%8a%95%e9%80%92%e7%bb%99%e7%9b%ae%e7%9a%84%e8%8a%82%e7%82%b9%e7%9a%84flanneld%e6%9c%8d%e5%8a%a1%e6%95%b0%e6%8d%ae%e5%88%b0%e8%be%be%e4%bb%a5%e5%90%8e%e8%a2%ab%e8%a7%a3%e5%8c%85%e7%84%b6%e5%90%8e%e7%9b%b4%e6%8e%a5%e8%bf%9b%e5%85%a5%e7%9b%ae%e7%9a%84%e8%8a%82%e7%82%b9%e7%9a%84flannel0%e8%99%9a%e6%8b%9f%e7%bd%91%e5%8d%a1%e7%84%b6%e5%90%8e%e8%a2%ab%e8%bd%ac%e5%8f%91%e5%88%b0%e7%9b%ae%e7%9a%84%e4%b8%bb%e6%9c%ba%e7%9a%84docker0%e8%99%9a%e6%8b%9f%e7%bd%91%e5%8d%a1%e6%9c%80%e5%90%8e%e5%b0%b1%e5%83%8f%e6%9c%ac%e6%9c%ba%e5%ae%b9%e5%99%a8%e9%80%9a%e4%bf%a1%e4%b8%80%e4%b8%8b%e7%9a%84%e6%9c%89docker0%e8%b7%af%e7%94%b1%e5%88%b0%e8%be%be%e7%9b%ae%e6%a0%87%e5%ae%b9%e5%99%a8-flannel%e5%9c%a8%e8%bf%9b%e8%a1%8c%e8%b7%af%e7%94%b1%e8%bd%ac%e5%8f%91%e7%9a%84%e5%9f%ba%e7%a1%80%e4%b8%8a%e8%bf%9b%e8%a1%8c%e4%ba%86%e5%b0%81%e5%8c%85%e8%a7%a3%e5%8c%85%e7%9a%84%e6%93%8d%e4%bd%9c%e8%bf%99%e6%a0%b7%e6%b5%aa%e8%b4%b9%e4%ba%86cpu%e7%9a%84%e8%ae%a1%e7%ae%97%e8%b5%84%e6%ba%90">#&lt;/a>
&lt;/h5>
&lt;h2 id="worker节点宕机简述pods驱逐流程">
 Worker节点宕机，简述Pods驱逐流程。
 &lt;a class="anchor" href="#worker%e8%8a%82%e7%82%b9%e5%ae%95%e6%9c%ba%e7%ae%80%e8%bf%b0pods%e9%a9%b1%e9%80%90%e6%b5%81%e7%a8%8b">#&lt;/a>
&lt;/h2>
&lt;ol>
&lt;li>
&lt;h5 id="1-在-kubernetes-集群中当节点由于某些原因网络宕机等不能正常工作时会被认定为不可用状态unknown-或者-false-状态当时间超过了-pod-eviction-timeout-值时那么节点上的所有-pod-都会被节点控制器计划删除">
 \1. 在 Kubernetes 集群中，当节点由于某些原因（网络、宕机等）不能正常工作时会被认定为不可用状态（Unknown 或者 False 状态），当时间超过了 pod-eviction-timeout 值时，那么节点上的所有 Pod 都会被节点控制器计划删除。
 &lt;a class="anchor" href="#1-%e5%9c%a8-kubernetes-%e9%9b%86%e7%be%a4%e4%b8%ad%e5%bd%93%e8%8a%82%e7%82%b9%e7%94%b1%e4%ba%8e%e6%9f%90%e4%ba%9b%e5%8e%9f%e5%9b%a0%e7%bd%91%e7%bb%9c%e5%ae%95%e6%9c%ba%e7%ad%89%e4%b8%8d%e8%83%bd%e6%ad%a3%e5%b8%b8%e5%b7%a5%e4%bd%9c%e6%97%b6%e4%bc%9a%e8%a2%ab%e8%ae%a4%e5%ae%9a%e4%b8%ba%e4%b8%8d%e5%8f%af%e7%94%a8%e7%8a%b6%e6%80%81unknown-%e6%88%96%e8%80%85-false-%e7%8a%b6%e6%80%81%e5%bd%93%e6%97%b6%e9%97%b4%e8%b6%85%e8%bf%87%e4%ba%86-pod-eviction-timeout-%e5%80%bc%e6%97%b6%e9%82%a3%e4%b9%88%e8%8a%82%e7%82%b9%e4%b8%8a%e7%9a%84%e6%89%80%e6%9c%89-pod-%e9%83%bd%e4%bc%9a%e8%a2%ab%e8%8a%82%e7%82%b9%e6%8e%a7%e5%88%b6%e5%99%a8%e8%ae%a1%e5%88%92%e5%88%a0%e9%99%a4">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="2-kubernetes-集群中有一个节点生命周期控制器node_lifecycle_controllergo它会与每一个节点上的-kubelet-进行通信以收集各个节点已经节点上容器的相关状态信息当超出一定时间后不能与-kubelet-通信那么就会标记该节点为-unknown-状态并且节点生命周期控制器会自动创建代表状况的污点用于防止调度器调度-pod-到该节点">
 \2. Kubernetes 集群中有一个节点生命周期控制器：node_lifecycle_controller.go。它会与每一个节点上的 kubelet 进行通信，以收集各个节点已经节点上容器的相关状态信息。当超出一定时间后不能与 kubelet 通信，那么就会标记该节点为 Unknown 状态。并且节点生命周期控制器会自动创建代表状况的污点，用于防止调度器调度 pod 到该节点。
 &lt;a class="anchor" href="#2-kubernetes-%e9%9b%86%e7%be%a4%e4%b8%ad%e6%9c%89%e4%b8%80%e4%b8%aa%e8%8a%82%e7%82%b9%e7%94%9f%e5%91%bd%e5%91%a8%e6%9c%9f%e6%8e%a7%e5%88%b6%e5%99%a8node_lifecycle_controllergo%e5%ae%83%e4%bc%9a%e4%b8%8e%e6%af%8f%e4%b8%80%e4%b8%aa%e8%8a%82%e7%82%b9%e4%b8%8a%e7%9a%84-kubelet-%e8%bf%9b%e8%a1%8c%e9%80%9a%e4%bf%a1%e4%bb%a5%e6%94%b6%e9%9b%86%e5%90%84%e4%b8%aa%e8%8a%82%e7%82%b9%e5%b7%b2%e7%bb%8f%e8%8a%82%e7%82%b9%e4%b8%8a%e5%ae%b9%e5%99%a8%e7%9a%84%e7%9b%b8%e5%85%b3%e7%8a%b6%e6%80%81%e4%bf%a1%e6%81%af%e5%bd%93%e8%b6%85%e5%87%ba%e4%b8%80%e5%ae%9a%e6%97%b6%e9%97%b4%e5%90%8e%e4%b8%8d%e8%83%bd%e4%b8%8e-kubelet-%e9%80%9a%e4%bf%a1%e9%82%a3%e4%b9%88%e5%b0%b1%e4%bc%9a%e6%a0%87%e8%ae%b0%e8%af%a5%e8%8a%82%e7%82%b9%e4%b8%ba-unknown-%e7%8a%b6%e6%80%81%e5%b9%b6%e4%b8%94%e8%8a%82%e7%82%b9%e7%94%9f%e5%91%bd%e5%91%a8%e6%9c%9f%e6%8e%a7%e5%88%b6%e5%99%a8%e4%bc%9a%e8%87%aa%e5%8a%a8%e5%88%9b%e5%bb%ba%e4%bb%a3%e8%a1%a8%e7%8a%b6%e5%86%b5%e7%9a%84%e6%b1%a1%e7%82%b9%e7%94%a8%e4%ba%8e%e9%98%b2%e6%ad%a2%e8%b0%83%e5%ba%a6%e5%99%a8%e8%b0%83%e5%ba%a6-pod-%e5%88%b0%e8%af%a5%e8%8a%82%e7%82%b9">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="3-那么-unknown-状态的节点上已经运行的-pod-会怎么处理呢节点上的所有-pod-都会被污点管理器taint_managergo计划删除而在节点被认定为不可用状态到删除节点上的-pod-之间是有一段时间的这段时间被称为容忍度如果在不配置的情况下kubernetes-会自动给-pod-添加一个-key-为-nodekubernetesionot-ready-的容忍度-并配置-tolerationseconds300同样kubernetes-会给-pod-添加一个-key-为-nodekubernetesiounreachable-的容忍度-并配置-tolerationseconds300">
 \3. 那么 Unknown 状态的节点上已经运行的 pod 会怎么处理呢？节点上的所有 Pod 都会被污点管理器（taint_manager.go）计划删除。而在节点被认定为不可用状态到删除节点上的 Pod 之间是有一段时间的，这段时间被称为容忍度。如果在不配置的情况下，Kubernetes 会自动给 Pod 添加一个 key 为 node.kubernetes.io/not-ready 的容忍度 并配置 tolerationSeconds=300，同样，Kubernetes 会给 Pod 添加一个 key 为 node.kubernetes.io/unreachable 的容忍度 并配置 tolerationSeconds=300。
 &lt;a class="anchor" href="#3-%e9%82%a3%e4%b9%88-unknown-%e7%8a%b6%e6%80%81%e7%9a%84%e8%8a%82%e7%82%b9%e4%b8%8a%e5%b7%b2%e7%bb%8f%e8%bf%90%e8%a1%8c%e7%9a%84-pod-%e4%bc%9a%e6%80%8e%e4%b9%88%e5%a4%84%e7%90%86%e5%91%a2%e8%8a%82%e7%82%b9%e4%b8%8a%e7%9a%84%e6%89%80%e6%9c%89-pod-%e9%83%bd%e4%bc%9a%e8%a2%ab%e6%b1%a1%e7%82%b9%e7%ae%a1%e7%90%86%e5%99%a8taint_managergo%e8%ae%a1%e5%88%92%e5%88%a0%e9%99%a4%e8%80%8c%e5%9c%a8%e8%8a%82%e7%82%b9%e8%a2%ab%e8%ae%a4%e5%ae%9a%e4%b8%ba%e4%b8%8d%e5%8f%af%e7%94%a8%e7%8a%b6%e6%80%81%e5%88%b0%e5%88%a0%e9%99%a4%e8%8a%82%e7%82%b9%e4%b8%8a%e7%9a%84-pod-%e4%b9%8b%e9%97%b4%e6%98%af%e6%9c%89%e4%b8%80%e6%ae%b5%e6%97%b6%e9%97%b4%e7%9a%84%e8%bf%99%e6%ae%b5%e6%97%b6%e9%97%b4%e8%a2%ab%e7%a7%b0%e4%b8%ba%e5%ae%b9%e5%bf%8d%e5%ba%a6%e5%a6%82%e6%9e%9c%e5%9c%a8%e4%b8%8d%e9%85%8d%e7%bd%ae%e7%9a%84%e6%83%85%e5%86%b5%e4%b8%8bkubernetes-%e4%bc%9a%e8%87%aa%e5%8a%a8%e7%bb%99-pod-%e6%b7%bb%e5%8a%a0%e4%b8%80%e4%b8%aa-key-%e4%b8%ba-nodekubernetesionot-ready-%e7%9a%84%e5%ae%b9%e5%bf%8d%e5%ba%a6-%e5%b9%b6%e9%85%8d%e7%bd%ae-tolerationseconds300%e5%90%8c%e6%a0%b7kubernetes-%e4%bc%9a%e7%bb%99-pod-%e6%b7%bb%e5%8a%a0%e4%b8%80%e4%b8%aa-key-%e4%b8%ba-nodekubernetesiounreachable-%e7%9a%84%e5%ae%b9%e5%bf%8d%e5%ba%a6-%e5%b9%b6%e9%85%8d%e7%bd%ae-tolerationseconds300">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="4-当到了删除-pod-时污点管理器会创建污点标记事件然后驱逐-pod-这里需要注意的是由于已经不能与-kubelet-通信所以该节点上的-pod-在管理后台看到的是处于灰色标记但是此时如果去获取-pod-的状态其实还是处于-running-状态每种类型的资源都有相应的资源控制器controller例如deployment_controllergostateful_set_controlgo每种控制器都在监听资源变化从而做出相应的动作执行deployment-控制器在监听到-pod-被驱逐后会创建一个新的-pod-出来但是-statefulset-控制器并不会创建出新的-pod原因是因为它可能会违反-statefulset-固有的至多一个的语义可能出现具有相同身份的多个成员这将可能是灾难性的并且可能导致数据丢失">
 \4. 当到了删除 Pod 时，污点管理器会创建污点标记事件，然后驱逐 pod 。这里需要注意的是由于已经不能与 kubelet 通信，所以该节点上的 Pod 在管理后台看到的是处于灰色标记，但是此时如果去获取 pod 的状态其实还是处于 Running 状态。每种类型的资源都有相应的资源控制器（Controller），例如：deployment_controller.go、stateful_set_control.go。每种控制器都在监听资源变化，从而做出相应的动作执行。deployment 控制器在监听到 Pod 被驱逐后会创建一个新的 Pod 出来，但是 Statefulset 控制器并不会创建出新的 Pod，原因是因为它可能会违反 StatefulSet 固有的至多一个的语义，可能出现具有相同身份的多个成员，这将可能是灾难性的，并且可能导致数据丢失。
 &lt;a class="anchor" href="#4-%e5%bd%93%e5%88%b0%e4%ba%86%e5%88%a0%e9%99%a4-pod-%e6%97%b6%e6%b1%a1%e7%82%b9%e7%ae%a1%e7%90%86%e5%99%a8%e4%bc%9a%e5%88%9b%e5%bb%ba%e6%b1%a1%e7%82%b9%e6%a0%87%e8%ae%b0%e4%ba%8b%e4%bb%b6%e7%84%b6%e5%90%8e%e9%a9%b1%e9%80%90-pod-%e8%bf%99%e9%87%8c%e9%9c%80%e8%a6%81%e6%b3%a8%e6%84%8f%e7%9a%84%e6%98%af%e7%94%b1%e4%ba%8e%e5%b7%b2%e7%bb%8f%e4%b8%8d%e8%83%bd%e4%b8%8e-kubelet-%e9%80%9a%e4%bf%a1%e6%89%80%e4%bb%a5%e8%af%a5%e8%8a%82%e7%82%b9%e4%b8%8a%e7%9a%84-pod-%e5%9c%a8%e7%ae%a1%e7%90%86%e5%90%8e%e5%8f%b0%e7%9c%8b%e5%88%b0%e7%9a%84%e6%98%af%e5%a4%84%e4%ba%8e%e7%81%b0%e8%89%b2%e6%a0%87%e8%ae%b0%e4%bd%86%e6%98%af%e6%ad%a4%e6%97%b6%e5%a6%82%e6%9e%9c%e5%8e%bb%e8%8e%b7%e5%8f%96-pod-%e7%9a%84%e7%8a%b6%e6%80%81%e5%85%b6%e5%ae%9e%e8%bf%98%e6%98%af%e5%a4%84%e4%ba%8e-running-%e7%8a%b6%e6%80%81%e6%af%8f%e7%a7%8d%e7%b1%bb%e5%9e%8b%e7%9a%84%e8%b5%84%e6%ba%90%e9%83%bd%e6%9c%89%e7%9b%b8%e5%ba%94%e7%9a%84%e8%b5%84%e6%ba%90%e6%8e%a7%e5%88%b6%e5%99%a8controller%e4%be%8b%e5%a6%82deployment_controllergostateful_set_controlgo%e6%af%8f%e7%a7%8d%e6%8e%a7%e5%88%b6%e5%99%a8%e9%83%bd%e5%9c%a8%e7%9b%91%e5%90%ac%e8%b5%84%e6%ba%90%e5%8f%98%e5%8c%96%e4%bb%8e%e8%80%8c%e5%81%9a%e5%87%ba%e7%9b%b8%e5%ba%94%e7%9a%84%e5%8a%a8%e4%bd%9c%e6%89%a7%e8%a1%8cdeployment-%e6%8e%a7%e5%88%b6%e5%99%a8%e5%9c%a8%e7%9b%91%e5%90%ac%e5%88%b0-pod-%e8%a2%ab%e9%a9%b1%e9%80%90%e5%90%8e%e4%bc%9a%e5%88%9b%e5%bb%ba%e4%b8%80%e4%b8%aa%e6%96%b0%e7%9a%84-pod-%e5%87%ba%e6%9d%a5%e4%bd%86%e6%98%af-statefulset-%e6%8e%a7%e5%88%b6%e5%99%a8%e5%b9%b6%e4%b8%8d%e4%bc%9a%e5%88%9b%e5%bb%ba%e5%87%ba%e6%96%b0%e7%9a%84-pod%e5%8e%9f%e5%9b%a0%e6%98%af%e5%9b%a0%e4%b8%ba%e5%ae%83%e5%8f%af%e8%83%bd%e4%bc%9a%e8%bf%9d%e5%8f%8d-statefulset-%e5%9b%ba%e6%9c%89%e7%9a%84%e8%87%b3%e5%a4%9a%e4%b8%80%e4%b8%aa%e7%9a%84%e8%af%ad%e4%b9%89%e5%8f%af%e8%83%bd%e5%87%ba%e7%8e%b0%e5%85%b7%e6%9c%89%e7%9b%b8%e5%90%8c%e8%ba%ab%e4%bb%bd%e7%9a%84%e5%a4%9a%e4%b8%aa%e6%88%90%e5%91%98%e8%bf%99%e5%b0%86%e5%8f%af%e8%83%bd%e6%98%af%e7%81%be%e9%9a%be%e6%80%a7%e7%9a%84%e5%b9%b6%e4%b8%94%e5%8f%af%e8%83%bd%e5%af%bc%e8%87%b4%e6%95%b0%e6%8d%ae%e4%b8%a2%e5%a4%b1">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ol>
&lt;h2 id="你知道的k8s中几种controller控制器并详述其工作原理">
 你知道的K8s中几种Controller控制器，并详述其工作原理
 &lt;a class="anchor" href="#%e4%bd%a0%e7%9f%a5%e9%81%93%e7%9a%84k8s%e4%b8%ad%e5%87%a0%e7%a7%8dcontroller%e6%8e%a7%e5%88%b6%e5%99%a8%e5%b9%b6%e8%af%a6%e8%bf%b0%e5%85%b6%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86">#&lt;/a>
&lt;/h2>
&lt;ol>
&lt;li>
&lt;h5 id="1-deployment适合无状态的服务部署-适合部署无状态的应用服务用来管理pod和replicaset具有上线部署副本设定滚动更新回滚等功能还可提供声明式更新例如只更新一个新的image">
 \1. deployment：适合无状态的服务部署 适合部署无状态的应用服务，用来管理pod和replicaset，具有上线部署、副本设定、滚动更新、回滚等功能，还可提供声明式更新，例如只更新一个新的Image
 &lt;a class="anchor" href="#1-deployment%e9%80%82%e5%90%88%e6%97%a0%e7%8a%b6%e6%80%81%e7%9a%84%e6%9c%8d%e5%8a%a1%e9%83%a8%e7%bd%b2-%e9%80%82%e5%90%88%e9%83%a8%e7%bd%b2%e6%97%a0%e7%8a%b6%e6%80%81%e7%9a%84%e5%ba%94%e7%94%a8%e6%9c%8d%e5%8a%a1%e7%94%a8%e6%9d%a5%e7%ae%a1%e7%90%86pod%e5%92%8creplicaset%e5%85%b7%e6%9c%89%e4%b8%8a%e7%ba%bf%e9%83%a8%e7%bd%b2%e5%89%af%e6%9c%ac%e8%ae%be%e5%ae%9a%e6%bb%9a%e5%8a%a8%e6%9b%b4%e6%96%b0%e5%9b%9e%e6%bb%9a%e7%ad%89%e5%8a%9f%e8%83%bd%e8%bf%98%e5%8f%af%e6%8f%90%e4%be%9b%e5%a3%b0%e6%98%8e%e5%bc%8f%e6%9b%b4%e6%96%b0%e4%be%8b%e5%a6%82%e5%8f%aa%e6%9b%b4%e6%96%b0%e4%b8%80%e4%b8%aa%e6%96%b0%e7%9a%84image">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>
&lt;h5 id="-编写yaml文件并创建nginx服务pod资源">
 • 编写yaml文件，并创建nginx服务pod资源。
 &lt;a class="anchor" href="#-%e7%bc%96%e5%86%99yaml%e6%96%87%e4%bb%b6%e5%b9%b6%e5%88%9b%e5%bb%banginx%e6%9c%8d%e5%8a%a1pod%e8%b5%84%e6%ba%90">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;ol>
&lt;li>
&lt;h5 id="1-statefullset适合有状态的服务部署-适合部署有状态应用解决pod的独立生命周期保持pod启动顺序和唯一性">
 \1. StatefullSet：适合有状态的服务部署 适合部署有状态应用，解决Pod的独立生命周期，保持Pod启动顺序和唯一性。
 &lt;a class="anchor" href="#1-statefullset%e9%80%82%e5%90%88%e6%9c%89%e7%8a%b6%e6%80%81%e7%9a%84%e6%9c%8d%e5%8a%a1%e9%83%a8%e7%bd%b2-%e9%80%82%e5%90%88%e9%83%a8%e7%bd%b2%e6%9c%89%e7%8a%b6%e6%80%81%e5%ba%94%e7%94%a8%e8%a7%a3%e5%86%b3pod%e7%9a%84%e7%8b%ac%e7%ab%8b%e7%94%9f%e5%91%bd%e5%91%a8%e6%9c%9f%e4%bf%9d%e6%8c%81pod%e5%90%af%e5%8a%a8%e9%a1%ba%e5%ba%8f%e5%92%8c%e5%94%af%e4%b8%80%e6%80%a7">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>
&lt;h5 id="-稳定唯一的网络标识符持久存储例如etcd配置文件节点地址发生变化将无法使用">
 • 稳定，唯一的网络标识符，持久存储（例如：etcd配置文件，节点地址发生变化，将无法使用）
 &lt;a class="anchor" href="#-%e7%a8%b3%e5%ae%9a%e5%94%af%e4%b8%80%e7%9a%84%e7%bd%91%e7%bb%9c%e6%a0%87%e8%af%86%e7%ac%a6%e6%8c%81%e4%b9%85%e5%ad%98%e5%82%a8%e4%be%8b%e5%a6%82etcd%e9%85%8d%e7%bd%ae%e6%96%87%e4%bb%b6%e8%8a%82%e7%82%b9%e5%9c%b0%e5%9d%80%e5%8f%91%e7%94%9f%e5%8f%98%e5%8c%96%e5%b0%86%e6%97%a0%e6%b3%95%e4%bd%bf%e7%94%a8">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-有序优雅的部署和扩展删除和终止例如mysql主从关系先启动主再启动从有序滚动更新">
 • 有序，优雅的部署和扩展、删除和终止（例如：mysql主从关系，先启动主，再启动从）有序，滚动更新
 &lt;a class="anchor" href="#-%e6%9c%89%e5%ba%8f%e4%bc%98%e9%9b%85%e7%9a%84%e9%83%a8%e7%bd%b2%e5%92%8c%e6%89%a9%e5%b1%95%e5%88%a0%e9%99%a4%e5%92%8c%e7%bb%88%e6%ad%a2%e4%be%8b%e5%a6%82mysql%e4%b8%bb%e4%bb%8e%e5%85%b3%e7%b3%bb%e5%85%88%e5%90%af%e5%8a%a8%e4%b8%bb%e5%86%8d%e5%90%af%e5%8a%a8%e4%bb%8e%e6%9c%89%e5%ba%8f%e6%bb%9a%e5%8a%a8%e6%9b%b4%e6%96%b0">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-应用场景例如数据库">
 • 应用场景：例如数据库
 &lt;a class="anchor" href="#-%e5%ba%94%e7%94%a8%e5%9c%ba%e6%99%af%e4%be%8b%e5%a6%82%e6%95%b0%e6%8d%ae%e5%ba%93">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h5 id="无状态服务的特点">
 无状态服务的特点：
 &lt;a class="anchor" href="#%e6%97%a0%e7%8a%b6%e6%80%81%e6%9c%8d%e5%8a%a1%e7%9a%84%e7%89%b9%e7%82%b9">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="-deployment-认为所有的pod都是一样的">
 • deployment 认为所有的pod都是一样的
 &lt;a class="anchor" href="#-deployment-%e8%ae%a4%e4%b8%ba%e6%89%80%e6%9c%89%e7%9a%84pod%e9%83%bd%e6%98%af%e4%b8%80%e6%a0%b7%e7%9a%84">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-不用考虑顺序的要求">
 • 不用考虑顺序的要求
 &lt;a class="anchor" href="#-%e4%b8%8d%e7%94%a8%e8%80%83%e8%99%91%e9%a1%ba%e5%ba%8f%e7%9a%84%e8%a6%81%e6%b1%82">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-不用考虑在哪个node节点上运行">
 • 不用考虑在哪个node节点上运行
 &lt;a class="anchor" href="#-%e4%b8%8d%e7%94%a8%e8%80%83%e8%99%91%e5%9c%a8%e5%93%aa%e4%b8%aanode%e8%8a%82%e7%82%b9%e4%b8%8a%e8%bf%90%e8%a1%8c">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-可以随意扩容和缩容">
 • 可以随意扩容和缩容
 &lt;a class="anchor" href="#-%e5%8f%af%e4%bb%a5%e9%9a%8f%e6%84%8f%e6%89%a9%e5%ae%b9%e5%92%8c%e7%bc%a9%e5%ae%b9">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h5 id="有状态服务的特点">
 有状态服务的特点：
 &lt;a class="anchor" href="#%e6%9c%89%e7%8a%b6%e6%80%81%e6%9c%8d%e5%8a%a1%e7%9a%84%e7%89%b9%e7%82%b9">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="-实例之间有差别每个实例都有自己的独特性元数据不同例如etcdzookeeper">
 • 实例之间有差别，每个实例都有自己的独特性，元数据不同，例如etcd，zookeeper
 &lt;a class="anchor" href="#-%e5%ae%9e%e4%be%8b%e4%b9%8b%e9%97%b4%e6%9c%89%e5%b7%ae%e5%88%ab%e6%af%8f%e4%b8%aa%e5%ae%9e%e4%be%8b%e9%83%bd%e6%9c%89%e8%87%aa%e5%b7%b1%e7%9a%84%e7%8b%ac%e7%89%b9%e6%80%a7%e5%85%83%e6%95%b0%e6%8d%ae%e4%b8%8d%e5%90%8c%e4%be%8b%e5%a6%82etcdzookeeper">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-实例之间不对等的关系以及依靠外部存储的应用">
 • 实例之间不对等的关系，以及依靠外部存储的应用
 &lt;a class="anchor" href="#-%e5%ae%9e%e4%be%8b%e4%b9%8b%e9%97%b4%e4%b8%8d%e5%af%b9%e7%ad%89%e7%9a%84%e5%85%b3%e7%b3%bb%e4%bb%a5%e5%8f%8a%e4%be%9d%e9%9d%a0%e5%a4%96%e9%83%a8%e5%ad%98%e5%82%a8%e7%9a%84%e5%ba%94%e7%94%a8">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-常规的service服务和无头服务的区别">
 • 常规的service服务和无头服务的区别
 &lt;a class="anchor" href="#-%e5%b8%b8%e8%a7%84%e7%9a%84service%e6%9c%8d%e5%8a%a1%e5%92%8c%e6%97%a0%e5%a4%b4%e6%9c%8d%e5%8a%a1%e7%9a%84%e5%8c%ba%e5%88%ab">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-service一组pod访问策略提供cluster-ip群集之间通讯还提供负载均衡和服务发现">
 • service：一组Pod访问策略，提供cluster-IP群集之间通讯，还提供负载均衡和服务发现
 &lt;a class="anchor" href="#-service%e4%b8%80%e7%bb%84pod%e8%ae%bf%e9%97%ae%e7%ad%96%e7%95%a5%e6%8f%90%e4%be%9bcluster-ip%e7%be%a4%e9%9b%86%e4%b9%8b%e9%97%b4%e9%80%9a%e8%ae%af%e8%bf%98%e6%8f%90%e4%be%9b%e8%b4%9f%e8%bd%bd%e5%9d%87%e8%a1%a1%e5%92%8c%e6%9c%8d%e5%8a%a1%e5%8f%91%e7%8e%b0">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-headless-service-无头服务不需要cluster-ip直接绑定具体的pod的ip无头服务经常用于statefulset的有状态部署">
 • Headless service 无头服务，不需要cluster-IP，直接绑定具体的Pod的IP，无头服务经常用于statefulset的有状态部署
 &lt;a class="anchor" href="#-headless-service-%e6%97%a0%e5%a4%b4%e6%9c%8d%e5%8a%a1%e4%b8%8d%e9%9c%80%e8%a6%81cluster-ip%e7%9b%b4%e6%8e%a5%e7%bb%91%e5%ae%9a%e5%85%b7%e4%bd%93%e7%9a%84pod%e7%9a%84ip%e6%97%a0%e5%a4%b4%e6%9c%8d%e5%8a%a1%e7%bb%8f%e5%b8%b8%e7%94%a8%e4%ba%8estatefulset%e7%9a%84%e6%9c%89%e7%8a%b6%e6%80%81%e9%83%a8%e7%bd%b2">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-创建无头服务的service资源和dns资源由于有状态服务的ip地址是动态的所以使用无头服务的时候要绑定dns服务">
 • 创建无头服务的service资源和dns资源，由于有状态服务的IP地址是动态的，所以使用无头服务的时候要绑定dns服务
 &lt;a class="anchor" href="#-%e5%88%9b%e5%bb%ba%e6%97%a0%e5%a4%b4%e6%9c%8d%e5%8a%a1%e7%9a%84service%e8%b5%84%e6%ba%90%e5%92%8cdns%e8%b5%84%e6%ba%90%e7%94%b1%e4%ba%8e%e6%9c%89%e7%8a%b6%e6%80%81%e6%9c%8d%e5%8a%a1%e7%9a%84ip%e5%9c%b0%e5%9d%80%e6%98%af%e5%8a%a8%e6%80%81%e7%9a%84%e6%89%80%e4%bb%a5%e4%bd%bf%e7%94%a8%e6%97%a0%e5%a4%b4%e6%9c%8d%e5%8a%a1%e7%9a%84%e6%97%b6%e5%80%99%e8%a6%81%e7%bb%91%e5%ae%9adns%e6%9c%8d%e5%8a%a1">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;ol>
&lt;li>
&lt;h5 id="1-daemonset一次部署所有的node节点都会部署例如一些典型的应用场景-运行集群存储-daemon例如在每个node上运行-glusterdceph">
 \1. DaemonSet：一次部署，所有的node节点都会部署，例如一些典型的应用场景： 运行集群存储 daemon，例如在每个Node上运行 glusterd、ceph
 &lt;a class="anchor" href="#1-daemonset%e4%b8%80%e6%ac%a1%e9%83%a8%e7%bd%b2%e6%89%80%e6%9c%89%e7%9a%84node%e8%8a%82%e7%82%b9%e9%83%bd%e4%bc%9a%e9%83%a8%e7%bd%b2%e4%be%8b%e5%a6%82%e4%b8%80%e4%ba%9b%e5%85%b8%e5%9e%8b%e7%9a%84%e5%ba%94%e7%94%a8%e5%9c%ba%e6%99%af-%e8%bf%90%e8%a1%8c%e9%9b%86%e7%be%a4%e5%ad%98%e5%82%a8-daemon%e4%be%8b%e5%a6%82%e5%9c%a8%e6%af%8f%e4%b8%aanode%e4%b8%8a%e8%bf%90%e8%a1%8c-glusterdceph">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>
&lt;h5 id="-在每个node上运行日志收集-daemon例如-fluentd-logstash">
 • 在每个Node上运行日志收集 daemon，例如 fluentd、 logstash
 &lt;a class="anchor" href="#-%e5%9c%a8%e6%af%8f%e4%b8%aanode%e4%b8%8a%e8%bf%90%e8%a1%8c%e6%97%a5%e5%bf%97%e6%94%b6%e9%9b%86-daemon%e4%be%8b%e5%a6%82-fluentd-logstash">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-在每个node上运行监控-daemon例如-prometheus-node-exporter">
 • 在每个Node上运行监控 daemon，例如 Prometheus Node Exporter
 &lt;a class="anchor" href="#-%e5%9c%a8%e6%af%8f%e4%b8%aanode%e4%b8%8a%e8%bf%90%e8%a1%8c%e7%9b%91%e6%8e%a7-daemon%e4%be%8b%e5%a6%82-prometheus-node-exporter">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-在每一个node上运行一个pod">
 • 在每一个Node上运行一个Pod
 &lt;a class="anchor" href="#-%e5%9c%a8%e6%af%8f%e4%b8%80%e4%b8%aanode%e4%b8%8a%e8%bf%90%e8%a1%8c%e4%b8%80%e4%b8%aapod">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-新加入的node也同样会自动运行一个pod">
 • 新加入的Node也同样会自动运行一个Pod
 &lt;a class="anchor" href="#-%e6%96%b0%e5%8a%a0%e5%85%a5%e7%9a%84node%e4%b9%9f%e5%90%8c%e6%a0%b7%e4%bc%9a%e8%87%aa%e5%8a%a8%e8%bf%90%e8%a1%8c%e4%b8%80%e4%b8%aapod">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-应用场景监控分布式存储日志收集等">
 • 应用场景：监控，分布式存储，日志收集等
 &lt;a class="anchor" href="#-%e5%ba%94%e7%94%a8%e5%9c%ba%e6%99%af%e7%9b%91%e6%8e%a7%e5%88%86%e5%b8%83%e5%bc%8f%e5%ad%98%e5%82%a8%e6%97%a5%e5%bf%97%e6%94%b6%e9%9b%86%e7%ad%89">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;ol>
&lt;li>
&lt;h5 id="1-job一次性的执行任务">
 \1. Job：一次性的执行任务
 &lt;a class="anchor" href="#1-job%e4%b8%80%e6%ac%a1%e6%80%a7%e7%9a%84%e6%89%a7%e8%a1%8c%e4%bb%bb%e5%8a%a1">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>
&lt;h5 id="-一次性执行任务类似linux中的job">
 • 一次性执行任务，类似Linux中的job
 &lt;a class="anchor" href="#-%e4%b8%80%e6%ac%a1%e6%80%a7%e6%89%a7%e8%a1%8c%e4%bb%bb%e5%8a%a1%e7%b1%bb%e4%bc%bclinux%e4%b8%ad%e7%9a%84job">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-应用场景如离线数据处理视频解码等业务">
 • 应用场景：如离线数据处理，视频解码等业务
 &lt;a class="anchor" href="#-%e5%ba%94%e7%94%a8%e5%9c%ba%e6%99%af%e5%a6%82%e7%a6%bb%e7%ba%bf%e6%95%b0%e6%8d%ae%e5%a4%84%e7%90%86%e8%a7%86%e9%a2%91%e8%a7%a3%e7%a0%81%e7%ad%89%e4%b8%9a%e5%8a%a1">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;ol>
&lt;li>
&lt;h5 id="1-cronjob周期性的执行任务">
 \1. Cronjob：周期性的执行任务
 &lt;a class="anchor" href="#1-cronjob%e5%91%a8%e6%9c%9f%e6%80%a7%e7%9a%84%e6%89%a7%e8%a1%8c%e4%bb%bb%e5%8a%a1">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>
&lt;h5 id="-周期性任务像linux的crontab一样">
 • 周期性任务，像Linux的Crontab一样
 &lt;a class="anchor" href="#-%e5%91%a8%e6%9c%9f%e6%80%a7%e4%bb%bb%e5%8a%a1%e5%83%8flinux%e7%9a%84crontab%e4%b8%80%e6%a0%b7">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-应用场景如通知备份等">
 • 应用场景：如通知，备份等
 &lt;a class="anchor" href="#-%e5%ba%94%e7%94%a8%e5%9c%ba%e6%99%af%e5%a6%82%e9%80%9a%e7%9f%a5%e5%a4%87%e4%bb%bd%e7%ad%89">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-使用cronjob要慎重用完之后要删掉不然会占用很多资源">
 • 使用cronjob要慎重，用完之后要删掉，不然会占用很多资源
 &lt;a class="anchor" href="#-%e4%bd%bf%e7%94%a8cronjob%e8%a6%81%e6%85%8e%e9%87%8d%e7%94%a8%e5%ae%8c%e4%b9%8b%e5%90%8e%e8%a6%81%e5%88%a0%e6%8e%89%e4%b8%8d%e7%84%b6%e4%bc%9a%e5%8d%a0%e7%94%a8%e5%be%88%e5%a4%9a%e8%b5%84%e6%ba%90">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h2 id="ingress-controller的工作机制">
 ingress-controller的工作机制
 &lt;a class="anchor" href="#ingress-controller%e7%9a%84%e5%b7%a5%e4%bd%9c%e6%9c%ba%e5%88%b6">#&lt;/a>
&lt;/h2>
&lt;h5 id="通常情况下service和pod的ip仅可在集群内部访问">
 通常情况下，service和pod的IP仅可在集群内部访问
 &lt;a class="anchor" href="#%e9%80%9a%e5%b8%b8%e6%83%85%e5%86%b5%e4%b8%8bservice%e5%92%8cpod%e7%9a%84ip%e4%bb%85%e5%8f%af%e5%9c%a8%e9%9b%86%e7%be%a4%e5%86%85%e9%83%a8%e8%ae%bf%e9%97%ae">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="-k8s提供了service方式nodeport-来提供对外的服务外部的服务可以通过访问node节点ipnodeport端口来访问集群内部的资源外部的请求先到达service所选中的节点上然后负载均衡到每一个节点上">
 • k8s提供了service方式：NodePort 来提供对外的服务，外部的服务可以通过访问Node节点ip+NodePort端口来访问集群内部的资源，外部的请求先到达service所选中的节点上，然后负载均衡到每一个节点上。
 &lt;a class="anchor" href="#-k8s%e6%8f%90%e4%be%9b%e4%ba%86service%e6%96%b9%e5%bc%8fnodeport-%e6%9d%a5%e6%8f%90%e4%be%9b%e5%af%b9%e5%a4%96%e7%9a%84%e6%9c%8d%e5%8a%a1%e5%a4%96%e9%83%a8%e7%9a%84%e6%9c%8d%e5%8a%a1%e5%8f%af%e4%bb%a5%e9%80%9a%e8%bf%87%e8%ae%bf%e9%97%aenode%e8%8a%82%e7%82%b9ipnodeport%e7%ab%af%e5%8f%a3%e6%9d%a5%e8%ae%bf%e9%97%ae%e9%9b%86%e7%be%a4%e5%86%85%e9%83%a8%e7%9a%84%e8%b5%84%e6%ba%90%e5%a4%96%e9%83%a8%e7%9a%84%e8%af%b7%e6%b1%82%e5%85%88%e5%88%b0%e8%be%beservice%e6%89%80%e9%80%89%e4%b8%ad%e7%9a%84%e8%8a%82%e7%82%b9%e4%b8%8a%e7%84%b6%e5%90%8e%e8%b4%9f%e8%bd%bd%e5%9d%87%e8%a1%a1%e5%88%b0%e6%af%8f%e4%b8%80%e4%b8%aa%e8%8a%82%e7%82%b9%e4%b8%8a">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h5 id="nodeport虽然提供了对外的方式但也有很大弊端">
 NodePort虽然提供了对外的方式但也有很大弊端：
 &lt;a class="anchor" href="#nodeport%e8%99%bd%e7%84%b6%e6%8f%90%e4%be%9b%e4%ba%86%e5%af%b9%e5%a4%96%e7%9a%84%e6%96%b9%e5%bc%8f%e4%bd%86%e4%b9%9f%e6%9c%89%e5%be%88%e5%a4%a7%e5%bc%8a%e7%ab%af">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="-由于service的实现方式user_space-iptebles-3-ipvs方式这三种方式只支持在4层协议通信不支持7层协议因此nodeport不能代理https服务">
 • 由于service的实现方式：user_space 、iptebles、 3 ipvs、方式这三种方式只支持在4层协议通信，不支持7层协议，因此NodePort不能代理https服务。
 &lt;a class="anchor" href="#-%e7%94%b1%e4%ba%8eservice%e7%9a%84%e5%ae%9e%e7%8e%b0%e6%96%b9%e5%bc%8fuser_space-iptebles-3-ipvs%e6%96%b9%e5%bc%8f%e8%bf%99%e4%b8%89%e7%a7%8d%e6%96%b9%e5%bc%8f%e5%8f%aa%e6%94%af%e6%8c%81%e5%9c%a84%e5%b1%82%e5%8d%8f%e8%ae%ae%e9%80%9a%e4%bf%a1%e4%b8%8d%e6%94%af%e6%8c%817%e5%b1%82%e5%8d%8f%e8%ae%ae%e5%9b%a0%e6%ad%a4nodeport%e4%b8%8d%e8%83%bd%e4%bb%a3%e7%90%86https%e6%9c%8d%e5%8a%a1">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-nodeport-需要暴露service所属每个node节点上端口当需求越来越多端口数量过多导致维护成本过高并且集群不好管理">
 • NodePort 需要暴露service所属每个node节点上端口，当需求越来越多，端口数量过多，导致维护成本过高，并且集群不好管理。
 &lt;a class="anchor" href="#-nodeport-%e9%9c%80%e8%a6%81%e6%9a%b4%e9%9c%b2service%e6%89%80%e5%b1%9e%e6%af%8f%e4%b8%aanode%e8%8a%82%e7%82%b9%e4%b8%8a%e7%ab%af%e5%8f%a3%e5%bd%93%e9%9c%80%e6%b1%82%e8%b6%8a%e6%9d%a5%e8%b6%8a%e5%a4%9a%e7%ab%af%e5%8f%a3%e6%95%b0%e9%87%8f%e8%bf%87%e5%a4%9a%e5%af%bc%e8%87%b4%e7%bb%b4%e6%8a%a4%e6%88%90%e6%9c%ac%e8%bf%87%e9%ab%98%e5%b9%b6%e4%b8%94%e9%9b%86%e7%be%a4%e4%b8%8d%e5%a5%bd%e7%ae%a1%e7%90%86">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h5 id="原理">
 原理
 &lt;a class="anchor" href="#%e5%8e%9f%e7%90%86">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="-ingress也是kubernetes-api的标准资源类型之一它其实就是一组基于dns名称host或url路径把请求转发到指定的service资源的规则用于将集群外部的请求流量转发到集群内部完成的服务发布我们需要明白的是ingress资源自身不能进行流量穿透仅仅是一组规则的集合这些集合规则还需要其他功能的辅助比如监听某套接字然后根据这些规则的匹配进行路由转发这些能够为ingress资源监听套接字并将流量转发的组件就是ingress-controller">
 • Ingress也是Kubernetes API的标准资源类型之一，它其实就是一组基于DNS名称（host）或URL路径把请求转发到指定的Service资源的规则。用于将集群外部的请求流量转发到集群内部完成的服务发布。我们需要明白的是，Ingress资源自身不能进行“流量穿透”，仅仅是一组规则的集合，这些集合规则还需要其他功能的辅助，比如监听某套接字，然后根据这些规则的匹配进行路由转发，这些能够为Ingress资源监听套接字并将流量转发的组件就是Ingress Controller。
 &lt;a class="anchor" href="#-ingress%e4%b9%9f%e6%98%afkubernetes-api%e7%9a%84%e6%a0%87%e5%87%86%e8%b5%84%e6%ba%90%e7%b1%bb%e5%9e%8b%e4%b9%8b%e4%b8%80%e5%ae%83%e5%85%b6%e5%ae%9e%e5%b0%b1%e6%98%af%e4%b8%80%e7%bb%84%e5%9f%ba%e4%ba%8edns%e5%90%8d%e7%a7%b0host%e6%88%96url%e8%b7%af%e5%be%84%e6%8a%8a%e8%af%b7%e6%b1%82%e8%bd%ac%e5%8f%91%e5%88%b0%e6%8c%87%e5%ae%9a%e7%9a%84service%e8%b5%84%e6%ba%90%e7%9a%84%e8%a7%84%e5%88%99%e7%94%a8%e4%ba%8e%e5%b0%86%e9%9b%86%e7%be%a4%e5%a4%96%e9%83%a8%e7%9a%84%e8%af%b7%e6%b1%82%e6%b5%81%e9%87%8f%e8%bd%ac%e5%8f%91%e5%88%b0%e9%9b%86%e7%be%a4%e5%86%85%e9%83%a8%e5%ae%8c%e6%88%90%e7%9a%84%e6%9c%8d%e5%8a%a1%e5%8f%91%e5%b8%83%e6%88%91%e4%bb%ac%e9%9c%80%e8%a6%81%e6%98%8e%e7%99%bd%e7%9a%84%e6%98%afingress%e8%b5%84%e6%ba%90%e8%87%aa%e8%ba%ab%e4%b8%8d%e8%83%bd%e8%bf%9b%e8%a1%8c%e6%b5%81%e9%87%8f%e7%a9%bf%e9%80%8f%e4%bb%85%e4%bb%85%e6%98%af%e4%b8%80%e7%bb%84%e8%a7%84%e5%88%99%e7%9a%84%e9%9b%86%e5%90%88%e8%bf%99%e4%ba%9b%e9%9b%86%e5%90%88%e8%a7%84%e5%88%99%e8%bf%98%e9%9c%80%e8%a6%81%e5%85%b6%e4%bb%96%e5%8a%9f%e8%83%bd%e7%9a%84%e8%be%85%e5%8a%a9%e6%af%94%e5%a6%82%e7%9b%91%e5%90%ac%e6%9f%90%e5%a5%97%e6%8e%a5%e5%ad%97%e7%84%b6%e5%90%8e%e6%a0%b9%e6%8d%ae%e8%bf%99%e4%ba%9b%e8%a7%84%e5%88%99%e7%9a%84%e5%8c%b9%e9%85%8d%e8%bf%9b%e8%a1%8c%e8%b7%af%e7%94%b1%e8%bd%ac%e5%8f%91%e8%bf%99%e4%ba%9b%e8%83%bd%e5%a4%9f%e4%b8%baingress%e8%b5%84%e6%ba%90%e7%9b%91%e5%90%ac%e5%a5%97%e6%8e%a5%e5%ad%97%e5%b9%b6%e5%b0%86%e6%b5%81%e9%87%8f%e8%bd%ac%e5%8f%91%e7%9a%84%e7%bb%84%e4%bb%b6%e5%b0%b1%e6%98%afingress-controller">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-ingress-控制器不同于deployment-等pod控制器的是ingress控制器不直接运行为kube-controller-manager的一部分它仅仅是kubernetes集群的一个附件类似于coredns需要在集群上单独部署">
 • Ingress 控制器不同于Deployment 等pod控制器的是，Ingress控制器不直接运行为kube-controller-manager的一部分，它仅仅是Kubernetes集群的一个附件，类似于CoreDNS，需要在集群上单独部署。
 &lt;a class="anchor" href="#-ingress-%e6%8e%a7%e5%88%b6%e5%99%a8%e4%b8%8d%e5%90%8c%e4%ba%8edeployment-%e7%ad%89pod%e6%8e%a7%e5%88%b6%e5%99%a8%e7%9a%84%e6%98%afingress%e6%8e%a7%e5%88%b6%e5%99%a8%e4%b8%8d%e7%9b%b4%e6%8e%a5%e8%bf%90%e8%a1%8c%e4%b8%bakube-controller-manager%e7%9a%84%e4%b8%80%e9%83%a8%e5%88%86%e5%ae%83%e4%bb%85%e4%bb%85%e6%98%afkubernetes%e9%9b%86%e7%be%a4%e7%9a%84%e4%b8%80%e4%b8%aa%e9%99%84%e4%bb%b6%e7%b1%bb%e4%bc%bc%e4%ba%8ecoredns%e9%9c%80%e8%a6%81%e5%9c%a8%e9%9b%86%e7%be%a4%e4%b8%8a%e5%8d%95%e7%8b%ac%e9%83%a8%e7%bd%b2">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-ingress-controller通过监视api-server获取相关ingressserviceendpointsecretnodeconfigmap对象并在程序内部不断循环监视相关service是否有新的endpoints变化一旦发生变化则自动更新nginxconf模板配置并产生新的配置文件进行reload">
 • ingress controller通过监视api server获取相关ingress、service、endpoint、secret、node、configmap对象，并在程序内部不断循环监视相关service是否有新的endpoints变化，一旦发生变化则自动更新nginx.conf模板配置并产生新的配置文件进行reload
 &lt;a class="anchor" href="#-ingress-controller%e9%80%9a%e8%bf%87%e7%9b%91%e8%a7%86api-server%e8%8e%b7%e5%8f%96%e7%9b%b8%e5%85%b3ingressserviceendpointsecretnodeconfigmap%e5%af%b9%e8%b1%a1%e5%b9%b6%e5%9c%a8%e7%a8%8b%e5%ba%8f%e5%86%85%e9%83%a8%e4%b8%8d%e6%96%ad%e5%be%aa%e7%8e%af%e7%9b%91%e8%a7%86%e7%9b%b8%e5%85%b3service%e6%98%af%e5%90%a6%e6%9c%89%e6%96%b0%e7%9a%84endpoints%e5%8f%98%e5%8c%96%e4%b8%80%e6%97%a6%e5%8f%91%e7%94%9f%e5%8f%98%e5%8c%96%e5%88%99%e8%87%aa%e5%8a%a8%e6%9b%b4%e6%96%b0nginxconf%e6%a8%a1%e6%9d%bf%e9%85%8d%e7%bd%ae%e5%b9%b6%e4%ba%a7%e7%94%9f%e6%96%b0%e7%9a%84%e9%85%8d%e7%bd%ae%e6%96%87%e4%bb%b6%e8%bf%9b%e8%a1%8creload">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h2 id="k8s的调度机制">
 k8s的调度机制
 &lt;a class="anchor" href="#k8s%e7%9a%84%e8%b0%83%e5%ba%a6%e6%9c%ba%e5%88%b6">#&lt;/a>
&lt;/h2>
&lt;ol>
&lt;li>
&lt;h5 id="1-scheduler工作原理-请求及scheduler调度步骤">
 \1. Scheduler工作原理： 请求及Scheduler调度步骤：
 &lt;a class="anchor" href="#1-scheduler%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86-%e8%af%b7%e6%b1%82%e5%8f%8ascheduler%e8%b0%83%e5%ba%a6%e6%ad%a5%e9%aa%a4">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>
&lt;h5 id="-节点预选predicate排除完全不满足条件的节点如内存大小端口等条件不满足">
 • 节点预选(Predicate)：排除完全不满足条件的节点，如内存大小，端口等条件不满足。
 &lt;a class="anchor" href="#-%e8%8a%82%e7%82%b9%e9%a2%84%e9%80%89predicate%e6%8e%92%e9%99%a4%e5%ae%8c%e5%85%a8%e4%b8%8d%e6%bb%a1%e8%b6%b3%e6%9d%a1%e4%bb%b6%e7%9a%84%e8%8a%82%e7%82%b9%e5%a6%82%e5%86%85%e5%ad%98%e5%a4%a7%e5%b0%8f%e7%ab%af%e5%8f%a3%e7%ad%89%e6%9d%a1%e4%bb%b6%e4%b8%8d%e6%bb%a1%e8%b6%b3">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-节点优先级排序priority根据优先级选出最佳节点">
 • 节点优先级排序(Priority)：根据优先级选出最佳节点
 &lt;a class="anchor" href="#-%e8%8a%82%e7%82%b9%e4%bc%98%e5%85%88%e7%ba%a7%e6%8e%92%e5%ba%8fpriority%e6%a0%b9%e6%8d%ae%e4%bc%98%e5%85%88%e7%ba%a7%e9%80%89%e5%87%ba%e6%9c%80%e4%bd%b3%e8%8a%82%e7%82%b9">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-节点择优select根据优先级选定节点">
 • 节点择优(Select)：根据优先级选定节点
 &lt;a class="anchor" href="#-%e8%8a%82%e7%82%b9%e6%8b%a9%e4%bc%98select%e6%a0%b9%e6%8d%ae%e4%bc%98%e5%85%88%e7%ba%a7%e9%80%89%e5%ae%9a%e8%8a%82%e7%82%b9">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;ol>
&lt;li>
&lt;h5 id="1-具体步骤">
 \1. 具体步骤：
 &lt;a class="anchor" href="#1-%e5%85%b7%e4%bd%93%e6%ad%a5%e9%aa%a4">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>
&lt;h5 id="-首先用户通过-kubernetes-客户端-kubectl-提交创建-pod-的-yaml-的文件向kubernetes-系统发起资源请求该资源请求被提交到">
 • 首先用户通过 Kubernetes 客户端 Kubectl 提交创建 Pod 的 Yaml 的文件，向Kubernetes 系统发起资源请求，该资源请求被提交到
 &lt;a class="anchor" href="#-%e9%a6%96%e5%85%88%e7%94%a8%e6%88%b7%e9%80%9a%e8%bf%87-kubernetes-%e5%ae%a2%e6%88%b7%e7%ab%af-kubectl-%e6%8f%90%e4%ba%a4%e5%88%9b%e5%bb%ba-pod-%e7%9a%84-yaml-%e7%9a%84%e6%96%87%e4%bb%b6%e5%90%91kubernetes-%e7%b3%bb%e7%bb%9f%e5%8f%91%e8%b5%b7%e8%b5%84%e6%ba%90%e8%af%b7%e6%b1%82%e8%af%a5%e8%b5%84%e6%ba%90%e8%af%b7%e6%b1%82%e8%a2%ab%e6%8f%90%e4%ba%a4%e5%88%b0">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-kubernetes-系统中用户通过命令行工具-kubectl-向-kubernetes-集群即-apiserver-用-的方式发送post请求即创建-pod-的请求">
 • Kubernetes 系统中，用户通过命令行工具 Kubectl 向 Kubernetes 集群即 APIServer 用 的方式发送“POST”请求，即创建 Pod 的请求。
 &lt;a class="anchor" href="#-kubernetes-%e7%b3%bb%e7%bb%9f%e4%b8%ad%e7%94%a8%e6%88%b7%e9%80%9a%e8%bf%87%e5%91%bd%e4%bb%a4%e8%a1%8c%e5%b7%a5%e5%85%b7-kubectl-%e5%90%91-kubernetes-%e9%9b%86%e7%be%a4%e5%8d%b3-apiserver-%e7%94%a8-%e7%9a%84%e6%96%b9%e5%bc%8f%e5%8f%91%e9%80%81post%e8%af%b7%e6%b1%82%e5%8d%b3%e5%88%9b%e5%bb%ba-pod-%e7%9a%84%e8%af%b7%e6%b1%82">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-apiserver-接收到请求后把创建-pod-的信息存储到-etcd-中从集群运行那一刻起资源调度系统-scheduler-就会定时去监控-apiserver">
 • APIServer 接收到请求后把创建 Pod 的信息存储到 Etcd 中，从集群运行那一刻起，资源调度系统 Scheduler 就会定时去监控 APIServer
 &lt;a class="anchor" href="#-apiserver-%e6%8e%a5%e6%94%b6%e5%88%b0%e8%af%b7%e6%b1%82%e5%90%8e%e6%8a%8a%e5%88%9b%e5%bb%ba-pod-%e7%9a%84%e4%bf%a1%e6%81%af%e5%ad%98%e5%82%a8%e5%88%b0-etcd-%e4%b8%ad%e4%bb%8e%e9%9b%86%e7%be%a4%e8%bf%90%e8%a1%8c%e9%82%a3%e4%b8%80%e5%88%bb%e8%b5%b7%e8%b5%84%e6%ba%90%e8%b0%83%e5%ba%a6%e7%b3%bb%e7%bb%9f-scheduler-%e5%b0%b1%e4%bc%9a%e5%ae%9a%e6%97%b6%e5%8e%bb%e7%9b%91%e6%8e%a7-apiserver">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-通过-apiserver-得到创建-pod-的信息scheduler-采用-watch-机制一旦-etcd-存储-pod-信息成功便会立即通知apiserver">
 • 通过 APIServer 得到创建 Pod 的信息，Scheduler 采用 watch 机制，一旦 Etcd 存储 Pod 信息成功便会立即通知APIServer，
 &lt;a class="anchor" href="#-%e9%80%9a%e8%bf%87-apiserver-%e5%be%97%e5%88%b0%e5%88%9b%e5%bb%ba-pod-%e7%9a%84%e4%bf%a1%e6%81%afscheduler-%e9%87%87%e7%94%a8-watch-%e6%9c%ba%e5%88%b6%e4%b8%80%e6%97%a6-etcd-%e5%ad%98%e5%82%a8-pod-%e4%bf%a1%e6%81%af%e6%88%90%e5%8a%9f%e4%be%bf%e4%bc%9a%e7%ab%8b%e5%8d%b3%e9%80%9a%e7%9f%a5apiserver">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-apiserver会立即把pod创建的消息通知schedulerscheduler发现-pod-的属性中-dest-node-为空时dest-node便会立即触发调度流程进行调度">
 • APIServer会立即把Pod创建的消息通知Scheduler，Scheduler发现 Pod 的属性中 Dest Node 为空时（Dest Node=””）便会立即触发调度流程进行调度。
 &lt;a class="anchor" href="#-apiserver%e4%bc%9a%e7%ab%8b%e5%8d%b3%e6%8a%8apod%e5%88%9b%e5%bb%ba%e7%9a%84%e6%b6%88%e6%81%af%e9%80%9a%e7%9f%a5schedulerscheduler%e5%8f%91%e7%8e%b0-pod-%e7%9a%84%e5%b1%9e%e6%80%a7%e4%b8%ad-dest-node-%e4%b8%ba%e7%a9%ba%e6%97%b6dest-node%e4%be%bf%e4%bc%9a%e7%ab%8b%e5%8d%b3%e8%a7%a6%e5%8f%91%e8%b0%83%e5%ba%a6%e6%b5%81%e7%a8%8b%e8%bf%9b%e8%a1%8c%e8%b0%83%e5%ba%a6">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-而这一个创建pod对象在调度的过程当中有3个阶段节点预选节点优选节点选定从而筛选出最佳的节点">
 • 而这一个创建Pod对象，在调度的过程当中有3个阶段：节点预选、节点优选、节点选定，从而筛选出最佳的节点
 &lt;a class="anchor" href="#-%e8%80%8c%e8%bf%99%e4%b8%80%e4%b8%aa%e5%88%9b%e5%bb%bapod%e5%af%b9%e8%b1%a1%e5%9c%a8%e8%b0%83%e5%ba%a6%e7%9a%84%e8%bf%87%e7%a8%8b%e5%bd%93%e4%b8%ad%e6%9c%893%e4%b8%aa%e9%98%b6%e6%ae%b5%e8%8a%82%e7%82%b9%e9%a2%84%e9%80%89%e8%8a%82%e7%82%b9%e4%bc%98%e9%80%89%e8%8a%82%e7%82%b9%e9%80%89%e5%ae%9a%e4%bb%8e%e8%80%8c%e7%ad%9b%e9%80%89%e5%87%ba%e6%9c%80%e4%bd%b3%e7%9a%84%e8%8a%82%e7%82%b9">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;ul>
&lt;li>
&lt;h5 id="-节点预选基于一系列的预选规则对每个节点进行检查将那些不符合条件的节点过滤从而完成节点的预选">
 • 节点预选：基于一系列的预选规则对每个节点进行检查，将那些不符合条件的节点过滤，从而完成节点的预选
 &lt;a class="anchor" href="#-%e8%8a%82%e7%82%b9%e9%a2%84%e9%80%89%e5%9f%ba%e4%ba%8e%e4%b8%80%e7%b3%bb%e5%88%97%e7%9a%84%e9%a2%84%e9%80%89%e8%a7%84%e5%88%99%e5%af%b9%e6%af%8f%e4%b8%aa%e8%8a%82%e7%82%b9%e8%bf%9b%e8%a1%8c%e6%a3%80%e6%9f%a5%e5%b0%86%e9%82%a3%e4%ba%9b%e4%b8%8d%e7%ac%a6%e5%90%88%e6%9d%a1%e4%bb%b6%e7%9a%84%e8%8a%82%e7%82%b9%e8%bf%87%e6%bb%a4%e4%bb%8e%e8%80%8c%e5%ae%8c%e6%88%90%e8%8a%82%e7%82%b9%e7%9a%84%e9%a2%84%e9%80%89">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-节点优选对预选出的节点进行优先级排序以便选出最合适运行pod对象的节点">
 • 节点优选：对预选出的节点进行优先级排序，以便选出最合适运行Pod对象的节点
 &lt;a class="anchor" href="#-%e8%8a%82%e7%82%b9%e4%bc%98%e9%80%89%e5%af%b9%e9%a2%84%e9%80%89%e5%87%ba%e7%9a%84%e8%8a%82%e7%82%b9%e8%bf%9b%e8%a1%8c%e4%bc%98%e5%85%88%e7%ba%a7%e6%8e%92%e5%ba%8f%e4%bb%a5%e4%be%bf%e9%80%89%e5%87%ba%e6%9c%80%e5%90%88%e9%80%82%e8%bf%90%e8%a1%8cpod%e5%af%b9%e8%b1%a1%e7%9a%84%e8%8a%82%e7%82%b9">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-节点选定从优先级排序结果中挑选出优先级最高的节点运行pod当这类节点多于1个时则进行随机选择">
 • 节点选定：从优先级排序结果中挑选出优先级最高的节点运行Pod，当这类节点多于1个时，则进行随机选择
 &lt;a class="anchor" href="#-%e8%8a%82%e7%82%b9%e9%80%89%e5%ae%9a%e4%bb%8e%e4%bc%98%e5%85%88%e7%ba%a7%e6%8e%92%e5%ba%8f%e7%bb%93%e6%9e%9c%e4%b8%ad%e6%8c%91%e9%80%89%e5%87%ba%e4%bc%98%e5%85%88%e7%ba%a7%e6%9c%80%e9%ab%98%e7%9a%84%e8%8a%82%e7%82%b9%e8%bf%90%e8%a1%8cpod%e5%bd%93%e8%bf%99%e7%b1%bb%e8%8a%82%e7%82%b9%e5%a4%9a%e4%ba%8e1%e4%b8%aa%e6%97%b6%e5%88%99%e8%bf%9b%e8%a1%8c%e9%9a%8f%e6%9c%ba%e9%80%89%e6%8b%a9">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;ol>
&lt;li>
&lt;h5 id="1-k8s的调用工作方式">
 \1. k8s的调用工作方式
 &lt;a class="anchor" href="#1-k8s%e7%9a%84%e8%b0%83%e7%94%a8%e5%b7%a5%e4%bd%9c%e6%96%b9%e5%bc%8f">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>
&lt;h5 id="-kubernetes调度器作为集群的大脑在如何提高集群的资源利用率保证集群中服务的稳定运行中也会变得越来越重要kubernetes的资源分为两种属性">
 • Kubernetes调度器作为集群的大脑，在如何提高集群的资源利用率、保证集群中服务的稳定运行中也会变得越来越重要Kubernetes的资源分为两种属性。
 &lt;a class="anchor" href="#-kubernetes%e8%b0%83%e5%ba%a6%e5%99%a8%e4%bd%9c%e4%b8%ba%e9%9b%86%e7%be%a4%e7%9a%84%e5%a4%a7%e8%84%91%e5%9c%a8%e5%a6%82%e4%bd%95%e6%8f%90%e9%ab%98%e9%9b%86%e7%be%a4%e7%9a%84%e8%b5%84%e6%ba%90%e5%88%a9%e7%94%a8%e7%8e%87%e4%bf%9d%e8%af%81%e9%9b%86%e7%be%a4%e4%b8%ad%e6%9c%8d%e5%8a%a1%e7%9a%84%e7%a8%b3%e5%ae%9a%e8%bf%90%e8%a1%8c%e4%b8%ad%e4%b9%9f%e4%bc%9a%e5%8f%98%e5%be%97%e8%b6%8a%e6%9d%a5%e8%b6%8a%e9%87%8d%e8%a6%81kubernetes%e7%9a%84%e8%b5%84%e6%ba%90%e5%88%86%e4%b8%ba%e4%b8%a4%e7%a7%8d%e5%b1%9e%e6%80%a7">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;ul>
&lt;li>
&lt;h5 id="-可压缩资源例如cpu循环disk-io带宽都是可以被限制和被回收的对于一个pod来说可以降低这些资源的使用量而不去杀掉pod">
 • 可压缩资源（例如CPU循环，Disk I/O带宽）都是可以被限制和被回收的，对于一个Pod来说可以降低这些资源的使用量而不去杀掉Pod。
 &lt;a class="anchor" href="#-%e5%8f%af%e5%8e%8b%e7%bc%a9%e8%b5%84%e6%ba%90%e4%be%8b%e5%a6%82cpu%e5%be%aa%e7%8e%afdisk-io%e5%b8%a6%e5%ae%bd%e9%83%bd%e6%98%af%e5%8f%af%e4%bb%a5%e8%a2%ab%e9%99%90%e5%88%b6%e5%92%8c%e8%a2%ab%e5%9b%9e%e6%94%b6%e7%9a%84%e5%af%b9%e4%ba%8e%e4%b8%80%e4%b8%aapod%e6%9d%a5%e8%af%b4%e5%8f%af%e4%bb%a5%e9%99%8d%e4%bd%8e%e8%bf%99%e4%ba%9b%e8%b5%84%e6%ba%90%e7%9a%84%e4%bd%bf%e7%94%a8%e9%87%8f%e8%80%8c%e4%b8%8d%e5%8e%bb%e6%9d%80%e6%8e%89pod">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-不可压缩资源例如内存硬盘空间一般来说不杀掉pod就没法回收未来kubernetes会加入更多资源如网络带宽存储iops的支持">
 • 不可压缩资源（例如内存、硬盘空间）一般来说不杀掉Pod就没法回收。未来Kubernetes会加入更多资源，如网络带宽，存储IOPS的支持。
 &lt;a class="anchor" href="#-%e4%b8%8d%e5%8f%af%e5%8e%8b%e7%bc%a9%e8%b5%84%e6%ba%90%e4%be%8b%e5%a6%82%e5%86%85%e5%ad%98%e7%a1%ac%e7%9b%98%e7%a9%ba%e9%97%b4%e4%b8%80%e8%88%ac%e6%9d%a5%e8%af%b4%e4%b8%8d%e6%9d%80%e6%8e%89pod%e5%b0%b1%e6%b2%a1%e6%b3%95%e5%9b%9e%e6%94%b6%e6%9c%aa%e6%9d%a5kubernetes%e4%bc%9a%e5%8a%a0%e5%85%a5%e6%9b%b4%e5%a4%9a%e8%b5%84%e6%ba%90%e5%a6%82%e7%bd%91%e7%bb%9c%e5%b8%a6%e5%ae%bd%e5%ad%98%e5%82%a8iops%e7%9a%84%e6%94%af%e6%8c%81">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="kube-proxy的三种工作模式和原理">
 kube-proxy的三种工作模式和原理
 &lt;a class="anchor" href="#kube-proxy%e7%9a%84%e4%b8%89%e7%a7%8d%e5%b7%a5%e4%bd%9c%e6%a8%a1%e5%bc%8f%e5%92%8c%e5%8e%9f%e7%90%86">#&lt;/a>
&lt;/h2>
&lt;ol>
&lt;li>
&lt;h5 id="1-userspace-模式">
 \1. userspace 模式
 &lt;a class="anchor" href="#1-userspace-%e6%a8%a1%e5%bc%8f">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>
&lt;h5 id="-该模式下kube-proxy会为每一个service创建一个监听端口发向cluster-ip的请求被iptables规则重定向到kube-proxy监听的端口上kube-proxy根据lb算法选择一个提供服务的pod并和其建立链接以将请求转发到pod上">
 • 该模式下kube-proxy会为每一个Service创建一个监听端口。发向Cluster IP的请求被Iptables规则重定向到Kube-proxy监听的端口上，Kube-proxy根据LB算法选择一个提供服务的Pod并和其建立链接，以将请求转发到Pod上。
 &lt;a class="anchor" href="#-%e8%af%a5%e6%a8%a1%e5%bc%8f%e4%b8%8bkube-proxy%e4%bc%9a%e4%b8%ba%e6%af%8f%e4%b8%80%e4%b8%aaservice%e5%88%9b%e5%bb%ba%e4%b8%80%e4%b8%aa%e7%9b%91%e5%90%ac%e7%ab%af%e5%8f%a3%e5%8f%91%e5%90%91cluster-ip%e7%9a%84%e8%af%b7%e6%b1%82%e8%a2%abiptables%e8%a7%84%e5%88%99%e9%87%8d%e5%ae%9a%e5%90%91%e5%88%b0kube-proxy%e7%9b%91%e5%90%ac%e7%9a%84%e7%ab%af%e5%8f%a3%e4%b8%8akube-proxy%e6%a0%b9%e6%8d%aelb%e7%ae%97%e6%b3%95%e9%80%89%e6%8b%a9%e4%b8%80%e4%b8%aa%e6%8f%90%e4%be%9b%e6%9c%8d%e5%8a%a1%e7%9a%84pod%e5%b9%b6%e5%92%8c%e5%85%b6%e5%bb%ba%e7%ab%8b%e9%93%be%e6%8e%a5%e4%bb%a5%e5%b0%86%e8%af%b7%e6%b1%82%e8%bd%ac%e5%8f%91%e5%88%b0pod%e4%b8%8a">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-该模式下kube-proxy充当了一个四层load-balancer的角色由于kube-proxy运行在userspace中在进行转发处理时会增加两次内核和用户空间之间的数据拷贝效率较另外两种模式低一些好处是当后端的pod不可用时kube-proxy可以重试其他pod">
 • 该模式下，Kube-proxy充当了一个四层Load balancer的角色。由于kube-proxy运行在userspace中，在进行转发处理时会增加两次内核和用户空间之间的数据拷贝，效率较另外两种模式低一些；好处是当后端的Pod不可用时，kube-proxy可以重试其他Pod。
 &lt;a class="anchor" href="#-%e8%af%a5%e6%a8%a1%e5%bc%8f%e4%b8%8bkube-proxy%e5%85%85%e5%bd%93%e4%ba%86%e4%b8%80%e4%b8%aa%e5%9b%9b%e5%b1%82load-balancer%e7%9a%84%e8%a7%92%e8%89%b2%e7%94%b1%e4%ba%8ekube-proxy%e8%bf%90%e8%a1%8c%e5%9c%a8userspace%e4%b8%ad%e5%9c%a8%e8%bf%9b%e8%a1%8c%e8%bd%ac%e5%8f%91%e5%a4%84%e7%90%86%e6%97%b6%e4%bc%9a%e5%a2%9e%e5%8a%a0%e4%b8%a4%e6%ac%a1%e5%86%85%e6%a0%b8%e5%92%8c%e7%94%a8%e6%88%b7%e7%a9%ba%e9%97%b4%e4%b9%8b%e9%97%b4%e7%9a%84%e6%95%b0%e6%8d%ae%e6%8b%b7%e8%b4%9d%e6%95%88%e7%8e%87%e8%be%83%e5%8f%a6%e5%a4%96%e4%b8%a4%e7%a7%8d%e6%a8%a1%e5%bc%8f%e4%bd%8e%e4%b8%80%e4%ba%9b%e5%a5%bd%e5%a4%84%e6%98%af%e5%bd%93%e5%90%8e%e7%ab%af%e7%9a%84pod%e4%b8%8d%e5%8f%af%e7%94%a8%e6%97%b6kube-proxy%e5%8f%af%e4%bb%a5%e9%87%8d%e8%af%95%e5%85%b6%e4%bb%96pod">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;ol>
&lt;li>
&lt;h5 id="1-iptables-模式">
 \1. iptables 模式
 &lt;a class="anchor" href="#1-iptables-%e6%a8%a1%e5%bc%8f">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>
&lt;h5 id="-为了避免增加内核和用户空间的数据拷贝操作提高转发效率kube-proxy提供了iptables模式在该模式下kube-proxy为service后端的每个pod创建对应的iptables规则直接将发向cluster-ip的请求重定向到一个pod-ip">
 • 为了避免增加内核和用户空间的数据拷贝操作，提高转发效率，Kube-proxy提供了iptables模式。在该模式下，Kube-proxy为service后端的每个Pod创建对应的iptables规则，直接将发向Cluster IP的请求重定向到一个Pod IP。
 &lt;a class="anchor" href="#-%e4%b8%ba%e4%ba%86%e9%81%bf%e5%85%8d%e5%a2%9e%e5%8a%a0%e5%86%85%e6%a0%b8%e5%92%8c%e7%94%a8%e6%88%b7%e7%a9%ba%e9%97%b4%e7%9a%84%e6%95%b0%e6%8d%ae%e6%8b%b7%e8%b4%9d%e6%93%8d%e4%bd%9c%e6%8f%90%e9%ab%98%e8%bd%ac%e5%8f%91%e6%95%88%e7%8e%87kube-proxy%e6%8f%90%e4%be%9b%e4%ba%86iptables%e6%a8%a1%e5%bc%8f%e5%9c%a8%e8%af%a5%e6%a8%a1%e5%bc%8f%e4%b8%8bkube-proxy%e4%b8%baservice%e5%90%8e%e7%ab%af%e7%9a%84%e6%af%8f%e4%b8%aapod%e5%88%9b%e5%bb%ba%e5%af%b9%e5%ba%94%e7%9a%84iptables%e8%a7%84%e5%88%99%e7%9b%b4%e6%8e%a5%e5%b0%86%e5%8f%91%e5%90%91cluster-ip%e7%9a%84%e8%af%b7%e6%b1%82%e9%87%8d%e5%ae%9a%e5%90%91%e5%88%b0%e4%b8%80%e4%b8%aapod-ip">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-该模式下kube-proxy不承担四层代理的角色只负责创建iptables规则该模式的优点是较userspace模式效率更高但不能提供灵活的lb策略当后端pod不可用时也无法进行重试">
 • 该模式下Kube-proxy不承担四层代理的角色，只负责创建iptables规则。该模式的优点是较userspace模式效率更高，但不能提供灵活的LB策略，当后端Pod不可用时也无法进行重试。
 &lt;a class="anchor" href="#-%e8%af%a5%e6%a8%a1%e5%bc%8f%e4%b8%8bkube-proxy%e4%b8%8d%e6%89%bf%e6%8b%85%e5%9b%9b%e5%b1%82%e4%bb%a3%e7%90%86%e7%9a%84%e8%a7%92%e8%89%b2%e5%8f%aa%e8%b4%9f%e8%b4%a3%e5%88%9b%e5%bb%baiptables%e8%a7%84%e5%88%99%e8%af%a5%e6%a8%a1%e5%bc%8f%e7%9a%84%e4%bc%98%e7%82%b9%e6%98%af%e8%be%83userspace%e6%a8%a1%e5%bc%8f%e6%95%88%e7%8e%87%e6%9b%b4%e9%ab%98%e4%bd%86%e4%b8%8d%e8%83%bd%e6%8f%90%e4%be%9b%e7%81%b5%e6%b4%bb%e7%9a%84lb%e7%ad%96%e7%95%a5%e5%bd%93%e5%90%8e%e7%ab%afpod%e4%b8%8d%e5%8f%af%e7%94%a8%e6%97%b6%e4%b9%9f%e6%97%a0%e6%b3%95%e8%bf%9b%e8%a1%8c%e9%87%8d%e8%af%95">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;ol>
&lt;li>
&lt;h5 id="1-该模式和iptables类似kube-proxy监控pod的变化并创建相应的ipvs-rulesipvs也是在kernel模式下通过netfilter实现的但采用了hash-table来存储规则因此在规则较多的情况下ipvs相对iptables转发效率更高除此以外ipvs支持更多的lb算法如果要设置kube-proxy为ipvs模式必须在操作系统中安装ipvs内核模块">
 \1. 该模式和iptables类似，kube-proxy监控Pod的变化并创建相应的ipvs rules。ipvs也是在kernel模式下通过netfilter实现的，但采用了hash table来存储规则，因此在规则较多的情况下，Ipvs相对iptables转发效率更高。除此以外，ipvs支持更多的LB算法。如果要设置kube-proxy为ipvs模式，必须在操作系统中安装IPVS内核模块。
 &lt;a class="anchor" href="#1-%e8%af%a5%e6%a8%a1%e5%bc%8f%e5%92%8ciptables%e7%b1%bb%e4%bc%bckube-proxy%e7%9b%91%e6%8e%a7pod%e7%9a%84%e5%8f%98%e5%8c%96%e5%b9%b6%e5%88%9b%e5%bb%ba%e7%9b%b8%e5%ba%94%e7%9a%84ipvs-rulesipvs%e4%b9%9f%e6%98%af%e5%9c%a8kernel%e6%a8%a1%e5%bc%8f%e4%b8%8b%e9%80%9a%e8%bf%87netfilter%e5%ae%9e%e7%8e%b0%e7%9a%84%e4%bd%86%e9%87%87%e7%94%a8%e4%ba%86hash-table%e6%9d%a5%e5%ad%98%e5%82%a8%e8%a7%84%e5%88%99%e5%9b%a0%e6%ad%a4%e5%9c%a8%e8%a7%84%e5%88%99%e8%be%83%e5%a4%9a%e7%9a%84%e6%83%85%e5%86%b5%e4%b8%8bipvs%e7%9b%b8%e5%af%b9iptables%e8%bd%ac%e5%8f%91%e6%95%88%e7%8e%87%e6%9b%b4%e9%ab%98%e9%99%a4%e6%ad%a4%e4%bb%a5%e5%a4%96ipvs%e6%94%af%e6%8c%81%e6%9b%b4%e5%a4%9a%e7%9a%84lb%e7%ae%97%e6%b3%95%e5%a6%82%e6%9e%9c%e8%a6%81%e8%ae%be%e7%bd%aekube-proxy%e4%b8%baipvs%e6%a8%a1%e5%bc%8f%e5%bf%85%e9%a1%bb%e5%9c%a8%e6%93%8d%e4%bd%9c%e7%b3%bb%e7%bb%9f%e4%b8%ad%e5%ae%89%e8%a3%85ipvs%e5%86%85%e6%a0%b8%e6%a8%a1%e5%9d%97">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ol>
&lt;pre tabindex="0">&lt;code>1.提交创建pod的yaml文件到kube-apiserver
2.kube-controller-manager创建资源对象
3.kubecontroller-manager将pod的配置信息存到etcd
4.kube-scheduler将pod调度到合适的节点
5.在该节点的kubelet上创建pod，pod建好后将pod的信息存到etcd
&lt;/code>&lt;/pre></description></item><item><title>2024-2-26 面试</title><link>https://qq547475331.github.io/docs/2024-2-26-%E9%9D%A2%E8%AF%95/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2024-2-26-%E9%9D%A2%E8%AF%95/</guid><description>&lt;h3 id="k8s-有哪些组件">
 k8s 有哪些组件？
 &lt;a class="anchor" href="#k8s-%e6%9c%89%e5%93%aa%e4%ba%9b%e7%bb%84%e4%bb%b6">#&lt;/a>
&lt;/h3>
&lt;h5 id="1etcd保存了集群的状态">
 1、etcd保存了集群的状态
 &lt;a class="anchor" href="#1etcd%e4%bf%9d%e5%ad%98%e4%ba%86%e9%9b%86%e7%be%a4%e7%9a%84%e7%8a%b6%e6%80%81">#&lt;/a>
&lt;/h5>
&lt;h5 id="2apiserver提供了资源操作的唯一入口还提供认证授权访问控制api注册和服务发现等">
 2、apiserver提供了资源操作的唯一入口，还提供认证，授权，访问控制，api注册和服务发现等
 &lt;a class="anchor" href="#2apiserver%e6%8f%90%e4%be%9b%e4%ba%86%e8%b5%84%e6%ba%90%e6%93%8d%e4%bd%9c%e7%9a%84%e5%94%af%e4%b8%80%e5%85%a5%e5%8f%a3%e8%bf%98%e6%8f%90%e4%be%9b%e8%ae%a4%e8%af%81%e6%8e%88%e6%9d%83%e8%ae%bf%e9%97%ae%e6%8e%a7%e5%88%b6api%e6%b3%a8%e5%86%8c%e5%92%8c%e6%9c%8d%e5%8a%a1%e5%8f%91%e7%8e%b0%e7%ad%89">#&lt;/a>
&lt;/h5>
&lt;h5 id="3controller-manager负责维护集群的状态比如自动扩展滚动更新故障检测等">
 3、controller manager负责维护集群的状态，比如自动扩展，滚动更新，故障检测等
 &lt;a class="anchor" href="#3controller-manager%e8%b4%9f%e8%b4%a3%e7%bb%b4%e6%8a%a4%e9%9b%86%e7%be%a4%e7%9a%84%e7%8a%b6%e6%80%81%e6%af%94%e5%a6%82%e8%87%aa%e5%8a%a8%e6%89%a9%e5%b1%95%e6%bb%9a%e5%8a%a8%e6%9b%b4%e6%96%b0%e6%95%85%e9%9a%9c%e6%a3%80%e6%b5%8b%e7%ad%89">#&lt;/a>
&lt;/h5>
&lt;h5 id="4scheduler-负责集群资源的调度安照预期的调度策略将pod调度到相应的node上">
 4、scheduler 负责集群资源的调度，安照预期的调度策略将pod调度到相应的node上
 &lt;a class="anchor" href="#4scheduler-%e8%b4%9f%e8%b4%a3%e9%9b%86%e7%be%a4%e8%b5%84%e6%ba%90%e7%9a%84%e8%b0%83%e5%ba%a6%e5%ae%89%e7%85%a7%e9%a2%84%e6%9c%9f%e7%9a%84%e8%b0%83%e5%ba%a6%e7%ad%96%e7%95%a5%e5%b0%86pod%e8%b0%83%e5%ba%a6%e5%88%b0%e7%9b%b8%e5%ba%94%e7%9a%84node%e4%b8%8a">#&lt;/a>
&lt;/h5>
&lt;h5 id="5kubelet负责管理容器的生命周期还负责共享存储卷和网络的管理">
 5、kubelet负责管理容器的生命周期，还负责共享存储卷和网络的管理
 &lt;a class="anchor" href="#5kubelet%e8%b4%9f%e8%b4%a3%e7%ae%a1%e7%90%86%e5%ae%b9%e5%99%a8%e7%9a%84%e7%94%9f%e5%91%bd%e5%91%a8%e6%9c%9f%e8%bf%98%e8%b4%9f%e8%b4%a3%e5%85%b1%e4%ba%ab%e5%ad%98%e5%82%a8%e5%8d%b7%e5%92%8c%e7%bd%91%e7%bb%9c%e7%9a%84%e7%ae%a1%e7%90%86">#&lt;/a>
&lt;/h5>
&lt;h5 id="6container-runtime-容器运行时负责管理镜像的管理pod和容器的真正运行">
 6、Container runtime 容器运行时，负责管理镜像的管理，pod和容器的真正运行
 &lt;a class="anchor" href="#6container-runtime-%e5%ae%b9%e5%99%a8%e8%bf%90%e8%a1%8c%e6%97%b6%e8%b4%9f%e8%b4%a3%e7%ae%a1%e7%90%86%e9%95%9c%e5%83%8f%e7%9a%84%e7%ae%a1%e7%90%86pod%e5%92%8c%e5%ae%b9%e5%99%a8%e7%9a%84%e7%9c%9f%e6%ad%a3%e8%bf%90%e8%a1%8c">#&lt;/a>
&lt;/h5>
&lt;h5 id="7kube-proxy负责管理集群的服务发现和负载均衡">
 7、kube-proxy负责管理集群的服务发现和负载均衡
 &lt;a class="anchor" href="#7kube-proxy%e8%b4%9f%e8%b4%a3%e7%ae%a1%e7%90%86%e9%9b%86%e7%be%a4%e7%9a%84%e6%9c%8d%e5%8a%a1%e5%8f%91%e7%8e%b0%e5%92%8c%e8%b4%9f%e8%bd%bd%e5%9d%87%e8%a1%a1">#&lt;/a>
&lt;/h5>
&lt;p>Kubernetes（K8s）是一个功能强大的容器编排平台，它由多个组件构成，这些组件共同协作来实现集群的管理、调度、监控等功能。下面是 Kubernetes 主要组件的简要介绍：&lt;/p>
&lt;h3 id="1-kubernetes-核心组件">
 &lt;strong>1. Kubernetes 核心组件&lt;/strong>
 &lt;a class="anchor" href="#1-kubernetes-%e6%a0%b8%e5%bf%83%e7%bb%84%e4%bb%b6">#&lt;/a>
&lt;/h3>
&lt;h4 id="11-api-server-">
 1.1 &lt;strong>API Server (&lt;code>kube-apiserver&lt;/code>)&lt;/strong>
 &lt;a class="anchor" href="#11-api-server-">#&lt;/a>
&lt;/h4>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>职责&lt;/strong>：API Server 是 Kubernetes 控制平面的核心组件，它暴露了 Kubernetes 集群的 REST API，所有的命令和操作（如创建、删除、更新 Pod 等）都通过 API Server 来进行。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>功能&lt;/p>
&lt;p>：&lt;/p>
&lt;ul>
&lt;li>接收来自用户、控制器、调度器等的请求。&lt;/li>
&lt;li>负责验证和处理请求。&lt;/li>
&lt;li>提供 REST API 接口，供客户端和其他组件使用。&lt;/li>
&lt;li>存储集群状态数据（通过与 etcd 交互）。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="12-controller-manager-">
 1.2 &lt;strong>Controller Manager (&lt;code>kube-controller-manager&lt;/code>)&lt;/strong>
 &lt;a class="anchor" href="#12-controller-manager-">#&lt;/a>
&lt;/h4>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>职责&lt;/strong>：Controller Manager 负责运行集群的控制循环（controller loops），确保集群的实际状态与期望状态相一致。&lt;/p></description></item><item><title>2024-3-14 vivo面试</title><link>https://qq547475331.github.io/docs/2025-3-14-vivo%E9%9D%A2%E8%AF%95/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2025-3-14-vivo%E9%9D%A2%E8%AF%95/</guid><description>&lt;h1 id="你们的k8s集群的高可用架构怎么设计的">
 你们的k8s集群的高可用架构怎么设计的？
 &lt;a class="anchor" href="#%e4%bd%a0%e4%bb%ac%e7%9a%84k8s%e9%9b%86%e7%be%a4%e7%9a%84%e9%ab%98%e5%8f%af%e7%94%a8%e6%9e%b6%e6%9e%84%e6%80%8e%e4%b9%88%e8%ae%be%e8%ae%a1%e7%9a%84">#&lt;/a>
&lt;/h1>
&lt;p>5个master节点 剩下是node节点，联邦，2个控制面集群和38运行面集群 组成k8s联邦，控制面k8s集群只跑paas平台的应用，运行面集群跑业务应用，AD BC 分区，多AZ夸机房容灾&lt;/p>
&lt;p>Kubernetes 集群的高可用架构设计对于确保系统的稳定性、可扩展性和容灾能力非常重要。根据你描述的设计架构，以下是详细的分析和展开：&lt;/p>
&lt;h3 id="1-5个-master-节点控制面">
 1. &lt;strong>5个 Master 节点（控制面）&lt;/strong>
 &lt;a class="anchor" href="#1-5%e4%b8%aa-master-%e8%8a%82%e7%82%b9%e6%8e%a7%e5%88%b6%e9%9d%a2">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>多节点设计&lt;/strong>：高可用的 Kubernetes 集群需要至少三个 master 节点来提供冗余，以避免单点故障。你选择了 5 个 master 节点，这样可以进一步增强集群的高可用性，避免任何 2 个节点宕机时集群服务的不可用。&lt;/li>
&lt;li>&lt;strong>控制面职责&lt;/strong>：Kubernetes master 节点负责调度、集群状态管理、API 服务器、控制器管理器等核心功能。将 master 节点分布在不同的物理或虚拟机上，并使用负载均衡器进行流量分配，可以有效避免 master 节点单点故障。&lt;/li>
&lt;li>&lt;strong>负载均衡&lt;/strong>：使用负载均衡器（如 Nginx、HAProxy 或云服务商的负载均衡）对外暴露 Kubernetes API Server，使得所有的客户端（如 &lt;code>kubectl&lt;/code>、应用）能够在多个 master 节点之间负载均衡，避免单一 master 节点的宕机导致不可用。&lt;/li>
&lt;/ul>
&lt;h3 id="2-node-节点工作节点">
 2. &lt;strong>Node 节点（工作节点）&lt;/strong>
 &lt;a class="anchor" href="#2-node-%e8%8a%82%e7%82%b9%e5%b7%a5%e4%bd%9c%e8%8a%82%e7%82%b9">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>运行业务应用&lt;/strong>：Kubernetes 的 node 节点负责运行容器化的业务应用。在该架构下，节点数目通常根据工作负载的需求进行扩展。每个 node 节点运行 &lt;code>kubelet&lt;/code> 和 &lt;code>kube-proxy&lt;/code>，这些组件管理节点的健康和服务发现。&lt;/li>
&lt;li>&lt;strong>自动扩容&lt;/strong>：可以通过 Kubernetes 集群的 HPA（Horizontal Pod Autoscaler）来根据业务需求动态地增加或减少 node 节点的数量，从而应对负载的波动。&lt;/li>
&lt;/ul>
&lt;h3 id="3-联邦架构federation">
 3. &lt;strong>联邦架构（Federation）&lt;/strong>
 &lt;a class="anchor" href="#3-%e8%81%94%e9%82%a6%e6%9e%b6%e6%9e%84federation">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>控制面集群和运行面集群的分离&lt;/strong>：在联邦架构中，控制面集群负责管理和控制集群的整体状态，但不运行具体的业务应用。而运行面集群专注于运行业务应用，负载由控制面集群统一调度。控制面集群通常只运行 PaaS 平台相关的应用，而业务应用则运行在运行面集群中。&lt;/li>
&lt;li>&lt;strong>联邦调度&lt;/strong>：Kubernetes 联邦可以跨多个集群进行调度，支持多个集群中的服务发现、跨集群负载均衡等功能。使用 Kubernetes 联邦可以实现跨地域、跨数据中心的服务扩展，从而保证高可用性。&lt;/li>
&lt;li>&lt;strong>跨区域容灾&lt;/strong>：联邦架构中的多个运行面集群可以部署在不同的可用区域（Availability Zone，AZ）和不同的数据中心。通过合理的设计，可以在一个区域发生故障时，自动将流量切换到另一个区域，从而实现跨区域容灾。&lt;/li>
&lt;/ul>
&lt;h3 id="4-ad-bc-分区">
 4. &lt;strong>AD BC 分区&lt;/strong>
 &lt;a class="anchor" href="#4-ad-bc-%e5%88%86%e5%8c%ba">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>AD 和 BC 是不同的区域分区&lt;/strong>：这里的 AD 和 BC 很可能指的是不同的应用区域分区或者不同的业务功能模块分区。AD 和 BC 代表了两类不同的业务场景，每个分区可能有独立的业务需求和安全隔离要求。&lt;/li>
&lt;li>&lt;strong>分区管理&lt;/strong>：通过为不同的业务模块使用不同的 Kubernetes Namespace，可以在同一集群中实现一定程度的资源隔离、权限管理和网络隔离。&lt;/li>
&lt;/ul>
&lt;h3 id="5-多-az-跨机房容灾设计">
 5. &lt;strong>多 AZ 跨机房容灾设计&lt;/strong>
 &lt;a class="anchor" href="#5-%e5%a4%9a-az-%e8%b7%a8%e6%9c%ba%e6%88%bf%e5%ae%b9%e7%81%be%e8%ae%be%e8%ae%a1">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>跨机房部署&lt;/strong>：通过将 Kubernetes 集群部署在多个数据中心或多个可用区（AZ）之间，实现高可用和灾难恢复。例如，使用多个 master 节点跨机房部署，并利用云服务提供的负载均衡和高可用性网络设计，保证 master 节点能够跨机房访问且高可用。&lt;/li>
&lt;li>&lt;strong>Pod 跨 AZ 部署&lt;/strong>：在 Kubernetes 集群中，Pod 可以跨多个 AZ 部署，通过配置 &lt;code>topologySpreadConstraints&lt;/code> 或使用适当的亲和性和反亲和性策略，确保应用在多个 AZ 中有冗余备份，避免某个 AZ 故障导致整个服务不可用。&lt;/li>
&lt;li>&lt;strong>Persistent Volume 跨 AZ 存储&lt;/strong>：使用跨 AZ 的持久化存储（如云提供商的跨区域存储卷）确保在一个 AZ 故障时，应用的持久化数据能够快速恢复。某些云平台如 AWS EBS 支持跨可用区的数据复制。&lt;/li>
&lt;/ul>
&lt;h3 id="6-健康检查和负载均衡">
 6. &lt;strong>健康检查和负载均衡&lt;/strong>
 &lt;a class="anchor" href="#6-%e5%81%a5%e5%ba%b7%e6%a3%80%e6%9f%a5%e5%92%8c%e8%b4%9f%e8%bd%bd%e5%9d%87%e8%a1%a1">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>健康检查&lt;/strong>：通过存活检测和就绪检测确保每个组件（如 kube-apiserver、etcd、kubelet 等）都处于正常状态。如果某个组件失败，自动进行自愈或重新调度。&lt;/li>
&lt;li>&lt;strong>Pod 调度和负载均衡&lt;/strong>：Kubernetes 调度器会根据集群的负载和容器的资源需求，将 Pod 分配到合适的 Node 节点上，同时利用 &lt;code>Service&lt;/code> 和 &lt;code>Ingress&lt;/code> 对外暴露应用，确保流量能够在不同的 Pod 和节点之间负载均衡。&lt;/li>
&lt;/ul>
&lt;h3 id="7-灾难恢复与容错">
 7. &lt;strong>灾难恢复与容错&lt;/strong>
 &lt;a class="anchor" href="#7-%e7%81%be%e9%9a%be%e6%81%a2%e5%a4%8d%e4%b8%8e%e5%ae%b9%e9%94%99">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Etcd 高可用&lt;/strong>：Etcd 是 Kubernetes 集群的关键组件，存储了集群的所有配置信息和状态。通过将 Etcd 部署为高可用集群（通常是奇数个节点，如 3 个或 5 个节点），确保在任意节点故障时，集群依然能够正常工作。&lt;/li>
&lt;li>&lt;strong>多区域冗余&lt;/strong>：通过跨区域部署 Etcd 节点和 Master 节点，确保在单个区域或数据中心发生故障时，其他区域可以继续提供服务。即使在大规模灾难中，也能最大程度地保持系统的可用性。&lt;/li>
&lt;/ul>
&lt;h3 id="8-备份与恢复">
 8. &lt;strong>备份与恢复&lt;/strong>
 &lt;a class="anchor" href="#8-%e5%a4%87%e4%bb%bd%e4%b8%8e%e6%81%a2%e5%a4%8d">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>定期备份&lt;/strong>：定期备份 Kubernetes 集群的核心组件（如 etcd、kube-apiserver 的配置等），以应对数据丢失或灾难恢复需求。&lt;/li>
&lt;li>&lt;strong>恢复方案&lt;/strong>：测试并制定恢复方案，在发生故障时能够迅速将集群恢复到正常状态。例如，可以使用 Velero 或其他备份工具进行集群备份和恢复。&lt;/li>
&lt;/ul>
&lt;h3 id="总结">
 总结
 &lt;a class="anchor" href="#%e6%80%bb%e7%bb%93">#&lt;/a>
&lt;/h3>
&lt;p>通过这样的高可用架构设计，Kubernetes 集群能够在面对节点故障、区域故障等情况下保持高度的可靠性和可扩展性。联邦架构、跨机房容灾、多 AZ 部署、Etcd 高可用等设计确保了应用在不同的业务需求下能够得到良好的支持，同时具备了强大的容错能力和扩展能力。&lt;/p></description></item><item><title>2024-3-19 两张图全面理解k8s原理</title><link>https://qq547475331.github.io/docs/2024-3-19-%E4%B8%A4%E5%BC%A0%E5%9B%BE%E5%85%A8%E9%9D%A2%E7%90%86%E8%A7%A3k8s%E5%8E%9F%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2024-3-19-%E4%B8%A4%E5%BC%A0%E5%9B%BE%E5%85%A8%E9%9D%A2%E7%90%86%E8%A7%A3k8s%E5%8E%9F%E7%90%86/</guid><description>&lt;h1 id="kubernetes-简介">
 Kubernetes 简介
 &lt;a class="anchor" href="#kubernetes-%e7%ae%80%e4%bb%8b">#&lt;/a>
&lt;/h1>
&lt;p>&lt;strong>Kubernetes&lt;/strong> 源于希腊语，意为“舵手”。k8s 缩写是因为 k 和 s 之间有八个字符的原因。它是 Google 在 2015 开源的容器调度编排的平台。&lt;/p>
&lt;p>&lt;strong>Kubernetes&lt;/strong> 作为一款优秀的容器编排工具，拥有非常精妙的架构设计。&lt;/p>
&lt;h1 id="kubernetes-架构">
 Kubernetes 架构
 &lt;a class="anchor" href="#kubernetes-%e6%9e%b6%e6%9e%84">#&lt;/a>
&lt;/h1>
&lt;h5 id="kubernetes-是一个-master--worker-的架构master-可理解为-kubernetes-的控制面worker-理解为-kubernetes-的数据面">
 &lt;strong>Kubernetes&lt;/strong> 是一个 &lt;strong>Master + Worker&lt;/strong> 的架构，&lt;strong>Master&lt;/strong> 可理解为 Kubernetes 的控制面，&lt;strong>Worker&lt;/strong> 理解为 Kubernetes 的数据面。
 &lt;a class="anchor" href="#kubernetes-%e6%98%af%e4%b8%80%e4%b8%aa-master--worker-%e7%9a%84%e6%9e%b6%e6%9e%84master-%e5%8f%af%e7%90%86%e8%a7%a3%e4%b8%ba-kubernetes-%e7%9a%84%e6%8e%a7%e5%88%b6%e9%9d%a2worker-%e7%90%86%e8%a7%a3%e4%b8%ba-kubernetes-%e7%9a%84%e6%95%b0%e6%8d%ae%e9%9d%a2">#&lt;/a>
&lt;/h5>
&lt;h5 id="master-节点一般只运行-kubernetes-控制组件是整个集群的大脑一般不运行业务容器">
 Master 节点一般只运行 Kubernetes 控制组件，是整个集群的大脑，一般不运行业务容器。
 &lt;a class="anchor" href="#master-%e8%8a%82%e7%82%b9%e4%b8%80%e8%88%ac%e5%8f%aa%e8%bf%90%e8%a1%8c-kubernetes-%e6%8e%a7%e5%88%b6%e7%bb%84%e4%bb%b6%e6%98%af%e6%95%b4%e4%b8%aa%e9%9b%86%e7%be%a4%e7%9a%84%e5%a4%a7%e8%84%91%e4%b8%80%e8%88%ac%e4%b8%8d%e8%bf%90%e8%a1%8c%e4%b8%9a%e5%8a%a1%e5%ae%b9%e5%99%a8">#&lt;/a>
&lt;/h5>
&lt;h5 id="worker-节点是运行业务容器的节点">
 Worker 节点是运行业务容器的节点。
 &lt;a class="anchor" href="#worker-%e8%8a%82%e7%82%b9%e6%98%af%e8%bf%90%e8%a1%8c%e4%b8%9a%e5%8a%a1%e5%ae%b9%e5%99%a8%e7%9a%84%e8%8a%82%e7%82%b9">#&lt;/a>
&lt;/h5>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403191453702.png" alt="image-20240319145332645" />&lt;/p>
&lt;h3 id="master">
 Master
 &lt;a class="anchor" href="#master">#&lt;/a>
&lt;/h3>
&lt;h5 id="kubernetes-master-节点需要运行以下组件">
 Kubernetes &lt;strong>Master&lt;/strong> 节点需要运行以下组件：
 &lt;a class="anchor" href="#kubernetes-master-%e8%8a%82%e7%82%b9%e9%9c%80%e8%a6%81%e8%bf%90%e8%a1%8c%e4%bb%a5%e4%b8%8b%e7%bb%84%e4%bb%b6">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="-kube-apiserverkube-apiserver-是-kubernetes-的-api-入口是集群流量入口">
 • **Kube-apiserver：**Kube-apiserver 是 Kubernetes 的 API 入口，是集群流量入口；
 &lt;a class="anchor" href="#-kube-apiserverkube-apiserver-%e6%98%af-kubernetes-%e7%9a%84-api-%e5%85%a5%e5%8f%a3%e6%98%af%e9%9b%86%e7%be%a4%e6%b5%81%e9%87%8f%e5%85%a5%e5%8f%a3">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-kube-controller-managerkube-controller-manager-包含很多个-controller用于调谐集群中的资源">
 • **Kube-controller-manager：**Kube-controller-manager 包含很多个 &lt;strong>Controller&lt;/strong>，用于调谐集群中的资源；
 &lt;a class="anchor" href="#-kube-controller-managerkube-controller-manager-%e5%8c%85%e5%90%ab%e5%be%88%e5%a4%9a%e4%b8%aa-controller%e7%94%a8%e4%ba%8e%e8%b0%83%e8%b0%90%e9%9b%86%e7%be%a4%e4%b8%ad%e7%9a%84%e8%b5%84%e6%ba%90">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-kube-schedulerkube-scheduler-是集群中默认调度器给-pod-选择最优节点调度">
 • **Kube-scheduler：**Kube-scheduler 是集群中默认调度器，给 Pod 选择最优节点调度；
 &lt;a class="anchor" href="#-kube-schedulerkube-scheduler-%e6%98%af%e9%9b%86%e7%be%a4%e4%b8%ad%e9%bb%98%e8%ae%a4%e8%b0%83%e5%ba%a6%e5%99%a8%e7%bb%99-pod-%e9%80%89%e6%8b%a9%e6%9c%80%e4%bc%98%e8%8a%82%e7%82%b9%e8%b0%83%e5%ba%a6">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-etcdkuebernetes-的后端存储集群中所有可持久化数据都存储在-etcd-中">
 • **Etcd：**Kuebernetes 的后端存储，集群中所有可持久化数据都存储在 Etcd 中。
 &lt;a class="anchor" href="#-etcdkuebernetes-%e7%9a%84%e5%90%8e%e7%ab%af%e5%ad%98%e5%82%a8%e9%9b%86%e7%be%a4%e4%b8%ad%e6%89%80%e6%9c%89%e5%8f%af%e6%8c%81%e4%b9%85%e5%8c%96%e6%95%b0%e6%8d%ae%e9%83%bd%e5%ad%98%e5%82%a8%e5%9c%a8-etcd-%e4%b8%ad">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;h5 id="kubernetes-中采用声明式-api即-api-不需要关心具体功能实现只需关心最终状态一个-api-对应一个-controller具体功能由-controller-实现同时调谐至预期状态">
 Kubernetes 中采用&lt;strong>声明式 API&lt;/strong>，即 API 不需要关心具体功能实现，只需关心最终状态。一个 API 对应一个 Controller，具体功能由 Controller 实现同时调谐至预期状态。
 &lt;a class="anchor" href="#kubernetes-%e4%b8%ad%e9%87%87%e7%94%a8%e5%a3%b0%e6%98%8e%e5%bc%8f-api%e5%8d%b3-api-%e4%b8%8d%e9%9c%80%e8%a6%81%e5%85%b3%e5%bf%83%e5%85%b7%e4%bd%93%e5%8a%9f%e8%83%bd%e5%ae%9e%e7%8e%b0%e5%8f%aa%e9%9c%80%e5%85%b3%e5%bf%83%e6%9c%80%e7%bb%88%e7%8a%b6%e6%80%81%e4%b8%80%e4%b8%aa-api-%e5%af%b9%e5%ba%94%e4%b8%80%e4%b8%aa-controller%e5%85%b7%e4%bd%93%e5%8a%9f%e8%83%bd%e7%94%b1-controller-%e5%ae%9e%e7%8e%b0%e5%90%8c%e6%97%b6%e8%b0%83%e8%b0%90%e8%87%b3%e9%a2%84%e6%9c%9f%e7%8a%b6%e6%80%81">#&lt;/a>
&lt;/h5>&lt;/blockquote>
&lt;h5 id="master-节点的高可用一般取决于-etcdetcd-高可用推荐三节点或者五节点所以-master-节点通常为三个或者五个如果接入外部-etcd-集群那么-master-节点可以是偶数个">
 Master 节点的&lt;strong>高可用&lt;/strong>一般取决于 Etcd，Etcd 高可用推荐&lt;strong>三节点或者五节点&lt;/strong>，所以 Master 节点通常为三个或者五个，如果接入&lt;strong>外部 Etcd 集群&lt;/strong>，那么 Master 节点可以是偶数个。
 &lt;a class="anchor" href="#master-%e8%8a%82%e7%82%b9%e7%9a%84%e9%ab%98%e5%8f%af%e7%94%a8%e4%b8%80%e8%88%ac%e5%8f%96%e5%86%b3%e4%ba%8e-etcdetcd-%e9%ab%98%e5%8f%af%e7%94%a8%e6%8e%a8%e8%8d%90%e4%b8%89%e8%8a%82%e7%82%b9%e6%88%96%e8%80%85%e4%ba%94%e8%8a%82%e7%82%b9%e6%89%80%e4%bb%a5-master-%e8%8a%82%e7%82%b9%e9%80%9a%e5%b8%b8%e4%b8%ba%e4%b8%89%e4%b8%aa%e6%88%96%e8%80%85%e4%ba%94%e4%b8%aa%e5%a6%82%e6%9e%9c%e6%8e%a5%e5%85%a5%e5%a4%96%e9%83%a8-etcd-%e9%9b%86%e7%be%a4%e9%82%a3%e4%b9%88-master-%e8%8a%82%e7%82%b9%e5%8f%af%e4%bb%a5%e6%98%af%e5%81%b6%e6%95%b0%e4%b8%aa">#&lt;/a>
&lt;/h5>
&lt;h5 id="上图-kubernetes-使用内部-etcd即-etcd-与-master-节点个数一致部署在-kubernetes-集群中">
 上图 Kubernetes 使用内部 Etcd，即 Etcd 与 Master 节点个数一致，部署在 Kubernetes 集群中。
 &lt;a class="anchor" href="#%e4%b8%8a%e5%9b%be-kubernetes-%e4%bd%bf%e7%94%a8%e5%86%85%e9%83%a8-etcd%e5%8d%b3-etcd-%e4%b8%8e-master-%e8%8a%82%e7%82%b9%e4%b8%aa%e6%95%b0%e4%b8%80%e8%87%b4%e9%83%a8%e7%bd%b2%e5%9c%a8-kubernetes-%e9%9b%86%e7%be%a4%e4%b8%ad">#&lt;/a>
&lt;/h5>
&lt;blockquote>
&lt;h5 id="etcd-集群一般要求三个五个类似奇数个实例etcd-集群选举机制要求集群中半数以上的实例投票选举如果集群是两个实例那么一个实例宕机剩下一个实例没有办法选举同样四个实例和三个实例实际上效果是一样的">
 Etcd 集群一般要求三个、五个类似奇数个实例，Etcd 集群选举机制要求集群中&lt;strong>半数以上&lt;/strong>的实例投票选举，如果集群是两个实例，那么一个实例宕机，剩下一个实例没有办法选举。同样四个实例和三个实例实际上效果是一样的。
 &lt;a class="anchor" href="#etcd-%e9%9b%86%e7%be%a4%e4%b8%80%e8%88%ac%e8%a6%81%e6%b1%82%e4%b8%89%e4%b8%aa%e4%ba%94%e4%b8%aa%e7%b1%bb%e4%bc%bc%e5%a5%87%e6%95%b0%e4%b8%aa%e5%ae%9e%e4%be%8betcd-%e9%9b%86%e7%be%a4%e9%80%89%e4%b8%be%e6%9c%ba%e5%88%b6%e8%a6%81%e6%b1%82%e9%9b%86%e7%be%a4%e4%b8%ad%e5%8d%8a%e6%95%b0%e4%bb%a5%e4%b8%8a%e7%9a%84%e5%ae%9e%e4%be%8b%e6%8a%95%e7%a5%a8%e9%80%89%e4%b8%be%e5%a6%82%e6%9e%9c%e9%9b%86%e7%be%a4%e6%98%af%e4%b8%a4%e4%b8%aa%e5%ae%9e%e4%be%8b%e9%82%a3%e4%b9%88%e4%b8%80%e4%b8%aa%e5%ae%9e%e4%be%8b%e5%ae%95%e6%9c%ba%e5%89%a9%e4%b8%8b%e4%b8%80%e4%b8%aa%e5%ae%9e%e4%be%8b%e6%b2%a1%e6%9c%89%e5%8a%9e%e6%b3%95%e9%80%89%e4%b8%be%e5%90%8c%e6%a0%b7%e5%9b%9b%e4%b8%aa%e5%ae%9e%e4%be%8b%e5%92%8c%e4%b8%89%e4%b8%aa%e5%ae%9e%e4%be%8b%e5%ae%9e%e9%99%85%e4%b8%8a%e6%95%88%e6%9e%9c%e6%98%af%e4%b8%80%e6%a0%b7%e7%9a%84">#&lt;/a>
&lt;/h5>&lt;/blockquote>
&lt;h3 id="worker">
 Worker
 &lt;a class="anchor" href="#worker">#&lt;/a>
&lt;/h3>
&lt;h5 id="kubernetes-worker-节点作为容器运行节点需要部署以下组件">
 Kubernetes &lt;strong>Worker&lt;/strong> 节点作为容器运行节点，需要部署以下组件：
 &lt;a class="anchor" href="#kubernetes-worker-%e8%8a%82%e7%82%b9%e4%bd%9c%e4%b8%ba%e5%ae%b9%e5%99%a8%e8%bf%90%e8%a1%8c%e8%8a%82%e7%82%b9%e9%9c%80%e8%a6%81%e9%83%a8%e7%bd%b2%e4%bb%a5%e4%b8%8b%e7%bb%84%e4%bb%b6">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="-cri容器运行时管理容器生命周期">
 • **CRI：**容器运行时，管理容器生命周期；
 &lt;a class="anchor" href="#-cri%e5%ae%b9%e5%99%a8%e8%bf%90%e8%a1%8c%e6%97%b6%e7%ae%a1%e7%90%86%e5%ae%b9%e5%99%a8%e7%94%9f%e5%91%bd%e5%91%a8%e6%9c%9f">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-kubelet管理-pod-生命周期pod-是-kubernetes-中最小调度单元">
 • **Kubelet：**管理 &lt;strong>Pod&lt;/strong> 生命周期，Pod 是 Kubernetes 中最小调度单元；
 &lt;a class="anchor" href="#-kubelet%e7%ae%a1%e7%90%86-pod-%e7%94%9f%e5%91%bd%e5%91%a8%e6%9c%9fpod-%e6%98%af-kubernetes-%e4%b8%ad%e6%9c%80%e5%b0%8f%e8%b0%83%e5%ba%a6%e5%8d%95%e5%85%83">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-cni容器网络接口实现-kubernetes-中-pod-间网络联通">
 • **CNI：**容器网络接口，实现 Kubernetes 中 Pod 间网络联通；
 &lt;a class="anchor" href="#-cni%e5%ae%b9%e5%99%a8%e7%bd%91%e7%bb%9c%e6%8e%a5%e5%8f%a3%e5%ae%9e%e7%8e%b0-kubernetes-%e4%b8%ad-pod-%e9%97%b4%e7%bd%91%e7%bb%9c%e8%81%94%e9%80%9a">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-csi容器存储接口屏蔽底层存储实现方便用户使用第三方存储">
 • **CSI：**容器存储接口，屏蔽底层存储实现，方便用户使用第三方存储；
 &lt;a class="anchor" href="#-csi%e5%ae%b9%e5%99%a8%e5%ad%98%e5%82%a8%e6%8e%a5%e5%8f%a3%e5%b1%8f%e8%94%bd%e5%ba%95%e5%b1%82%e5%ad%98%e5%82%a8%e5%ae%9e%e7%8e%b0%e6%96%b9%e4%be%bf%e7%94%a8%e6%88%b7%e4%bd%bf%e7%94%a8%e7%ac%ac%e4%b8%89%e6%96%b9%e5%ad%98%e5%82%a8">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-kube-proxy该组件主要实现多组-pod-的负载均衡">
 • **Kube-proxy：**该组件主要实现多组 Pod 的负载均衡；
 &lt;a class="anchor" href="#-kube-proxy%e8%af%a5%e7%bb%84%e4%bb%b6%e4%b8%bb%e8%a6%81%e5%ae%9e%e7%8e%b0%e5%a4%9a%e7%bb%84-pod-%e7%9a%84%e8%b4%9f%e8%bd%bd%e5%9d%87%e8%a1%a1">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;h5 id="为什么-kubernetes-需要在容器上之上抽象一个-pod-资源呢大部分情况是一个-pod-对应一个容器有的场景就需要一个-pod-对应多个容器例如日志收集场景每个-pod-都会包含一个业务容器和一个日志收集容器将这两个容器放在一个-pod-里可用共享日志-volume">
 为什么 Kubernetes 需要在容器上之上抽象一个 Pod 资源呢？大部分情况是一个 Pod 对应一个容器，有的场景就需要一个 Pod 对应多个容器，例如日志收集场景，每个 Pod 都会包含一个业务容器和一个日志收集容器，将这两个容器放在一个 Pod 里可用共享日志 Volume。
 &lt;a class="anchor" href="#%e4%b8%ba%e4%bb%80%e4%b9%88-kubernetes-%e9%9c%80%e8%a6%81%e5%9c%a8%e5%ae%b9%e5%99%a8%e4%b8%8a%e4%b9%8b%e4%b8%8a%e6%8a%bd%e8%b1%a1%e4%b8%80%e4%b8%aa-pod-%e8%b5%84%e6%ba%90%e5%91%a2%e5%a4%a7%e9%83%a8%e5%88%86%e6%83%85%e5%86%b5%e6%98%af%e4%b8%80%e4%b8%aa-pod-%e5%af%b9%e5%ba%94%e4%b8%80%e4%b8%aa%e5%ae%b9%e5%99%a8%e6%9c%89%e7%9a%84%e5%9c%ba%e6%99%af%e5%b0%b1%e9%9c%80%e8%a6%81%e4%b8%80%e4%b8%aa-pod-%e5%af%b9%e5%ba%94%e5%a4%9a%e4%b8%aa%e5%ae%b9%e5%99%a8%e4%be%8b%e5%a6%82%e6%97%a5%e5%bf%97%e6%94%b6%e9%9b%86%e5%9c%ba%e6%99%af%e6%af%8f%e4%b8%aa-pod-%e9%83%bd%e4%bc%9a%e5%8c%85%e5%90%ab%e4%b8%80%e4%b8%aa%e4%b8%9a%e5%8a%a1%e5%ae%b9%e5%99%a8%e5%92%8c%e4%b8%80%e4%b8%aa%e6%97%a5%e5%bf%97%e6%94%b6%e9%9b%86%e5%ae%b9%e5%99%a8%e5%b0%86%e8%bf%99%e4%b8%a4%e4%b8%aa%e5%ae%b9%e5%99%a8%e6%94%be%e5%9c%a8%e4%b8%80%e4%b8%aa-pod-%e9%87%8c%e5%8f%af%e7%94%a8%e5%85%b1%e4%ba%ab%e6%97%a5%e5%bf%97-volume">#&lt;/a>
&lt;/h5>&lt;/blockquote>
&lt;h5 id="worker-节点的-kubelet-需要注册到集群中就需要每个-worker-节点的-kubelet-能够连接-master-节点的-kube-apiserver如果集群中-master-采用高可用部署就会存在多个-master那么-worker-节点的-kubelet-就需要同时连接所有的-kube-apiserver-保证高可用实现这种高可用的方式有很多种例如-haproxy--keepalived-nginxenvoy-等上图就是-lb-组件就代表这些实现负载-kube-apsierver-的组件">
 Worker 节点的 Kubelet 需要&lt;strong>注册&lt;/strong>到集群中，就需要每个 Worker 节点的 Kubelet 能够连接 Master 节点的 Kube-apiserver。如果集群中 Master 采用高可用部署，就会存在多个 Master，那么 Worker 节点的 Kubelet 就需要同时连接&lt;strong>所有的 Kube-apiserver&lt;/strong> 保证高可用。实现这种高可用的方式有很多种，例如 &lt;code>Haproxy + Keepalived 、Nginx、Envoy&lt;/code> 等。上图就是 &lt;strong>LB 组件&lt;/strong>就代表这些实现负载 Kube-apsierver 的组件。
 &lt;a class="anchor" href="#worker-%e8%8a%82%e7%82%b9%e7%9a%84-kubelet-%e9%9c%80%e8%a6%81%e6%b3%a8%e5%86%8c%e5%88%b0%e9%9b%86%e7%be%a4%e4%b8%ad%e5%b0%b1%e9%9c%80%e8%a6%81%e6%af%8f%e4%b8%aa-worker-%e8%8a%82%e7%82%b9%e7%9a%84-kubelet-%e8%83%bd%e5%a4%9f%e8%bf%9e%e6%8e%a5-master-%e8%8a%82%e7%82%b9%e7%9a%84-kube-apiserver%e5%a6%82%e6%9e%9c%e9%9b%86%e7%be%a4%e4%b8%ad-master-%e9%87%87%e7%94%a8%e9%ab%98%e5%8f%af%e7%94%a8%e9%83%a8%e7%bd%b2%e5%b0%b1%e4%bc%9a%e5%ad%98%e5%9c%a8%e5%a4%9a%e4%b8%aa-master%e9%82%a3%e4%b9%88-worker-%e8%8a%82%e7%82%b9%e7%9a%84-kubelet-%e5%b0%b1%e9%9c%80%e8%a6%81%e5%90%8c%e6%97%b6%e8%bf%9e%e6%8e%a5%e6%89%80%e6%9c%89%e7%9a%84-kube-apiserver-%e4%bf%9d%e8%af%81%e9%ab%98%e5%8f%af%e7%94%a8%e5%ae%9e%e7%8e%b0%e8%bf%99%e7%a7%8d%e9%ab%98%e5%8f%af%e7%94%a8%e7%9a%84%e6%96%b9%e5%bc%8f%e6%9c%89%e5%be%88%e5%a4%9a%e7%a7%8d%e4%be%8b%e5%a6%82-haproxy--keepalived-nginxenvoy-%e7%ad%89%e4%b8%8a%e5%9b%be%e5%b0%b1%e6%98%af-lb-%e7%bb%84%e4%bb%b6%e5%b0%b1%e4%bb%a3%e8%a1%a8%e8%bf%99%e4%ba%9b%e5%ae%9e%e7%8e%b0%e8%b4%9f%e8%bd%bd-kube-apsierver-%e7%9a%84%e7%bb%84%e4%bb%b6">#&lt;/a>
&lt;/h5>
&lt;h1 id="创建一个-pod-需要经历哪些流程">
 创建一个 Pod 需要经历哪些流程
 &lt;a class="anchor" href="#%e5%88%9b%e5%bb%ba%e4%b8%80%e4%b8%aa-pod-%e9%9c%80%e8%a6%81%e7%bb%8f%e5%8e%86%e5%93%aa%e4%ba%9b%e6%b5%81%e7%a8%8b">#&lt;/a>
&lt;/h1>
&lt;h5 id="当用户创建一个-deployment-的时候kubernetes-中各组件的工作流程是如何的">
 当用户创建一个 &lt;strong>Deployment&lt;/strong> 的时候，Kubernetes 中各组件的&lt;strong>工作流程&lt;/strong>是如何的？
 &lt;a class="anchor" href="#%e5%bd%93%e7%94%a8%e6%88%b7%e5%88%9b%e5%bb%ba%e4%b8%80%e4%b8%aa-deployment-%e7%9a%84%e6%97%b6%e5%80%99kubernetes-%e4%b8%ad%e5%90%84%e7%bb%84%e4%bb%b6%e7%9a%84%e5%b7%a5%e4%bd%9c%e6%b5%81%e7%a8%8b%e6%98%af%e5%a6%82%e4%bd%95%e7%9a%84">#&lt;/a>
&lt;/h5>
&lt;ul>
&lt;li>
&lt;h5 id="-用户通过-kubectl-创建一个-deployment请求会发给-kube-apiserver">
 • 用户通过 &lt;code>kubectl&lt;/code> 创建一个 &lt;strong>Deployment&lt;/strong>，请求会发给 &lt;strong>Kube-apiserver&lt;/strong>；
 &lt;a class="anchor" href="#-%e7%94%a8%e6%88%b7%e9%80%9a%e8%bf%87-kubectl-%e5%88%9b%e5%bb%ba%e4%b8%80%e4%b8%aa-deployment%e8%af%b7%e6%b1%82%e4%bc%9a%e5%8f%91%e7%bb%99-kube-apiserver">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="-kube-apiserver-会将-deployment-的描述信息写入-etcd-中kube-apiserver-将请求结果返回给用户">
 • &lt;strong>Kube-apiserver&lt;/strong> 会将 &lt;strong>Deployment&lt;/strong> 的描述信息写入 &lt;strong>Etcd&lt;/strong> 中，&lt;strong>Kube-apiserver&lt;/strong> 将请求结果返回给用户；
 &lt;a class="anchor" href="#-kube-apiserver-%e4%bc%9a%e5%b0%86-deployment-%e7%9a%84%e6%8f%8f%e8%bf%b0%e4%bf%a1%e6%81%af%e5%86%99%e5%85%a5-etcd-%e4%b8%adkube-apiserver-%e5%b0%86%e8%af%b7%e6%b1%82%e7%bb%93%e6%9e%9c%e8%bf%94%e5%9b%9e%e7%bb%99%e7%94%a8%e6%88%b7">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;p>• &lt;strong>Kube-controller-manager&lt;/strong> 的 &lt;strong>Deployment Controller&lt;/strong> 从 &lt;strong>Kube-apiserver&lt;/strong> &lt;code>Watch&lt;/code> 到 &lt;strong>Deployment&lt;/strong> 的创建事件，并创建一个 &lt;strong>ReplicaSet&lt;/strong>；&lt;/p></description></item><item><title>2024-3-4 CNI剖析演进</title><link>https://qq547475331.github.io/docs/2024-3-4-cni%E5%89%96%E6%9E%90%E6%BC%94%E8%BF%9B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2024-3-4-cni%E5%89%96%E6%9E%90%E6%BC%94%E8%BF%9B/</guid><description>&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041014067.png" alt="image-20240304101409913" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041014936.png" alt="image-20240304101422866" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041014097.png" alt="image-20240304101435883" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041014809.png" alt="image-20240304101449734" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041015326.png" alt="image-20240304101505237" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041015443.png" alt="image-20240304101517303" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041015831.png" alt="image-20240304101532753" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041015668.png" alt="image-20240304101551582" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041016109.png" alt="image-20240304101616039" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041016914.png" alt="image-20240304101631832" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041016340.png" alt="image-20240304101648264" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041017525.png" alt="image-20240304101701442" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041017817.png" alt="image-20240304101716735" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041018482.png" alt="image-20240304101812231" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041018902.png" alt="image-20240304101856828" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041019129.png" alt="image-20240304101921866" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041019595.png" alt="image-20240304101946408" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041019969.png" alt="image-20240304101956900" />&lt;/p></description></item><item><title>2024-3-8 面试</title><link>https://qq547475331.github.io/docs/2024-3-8-%E9%9D%A2%E8%AF%950308/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2024-3-8-%E9%9D%A2%E8%AF%950308/</guid><description>&lt;h3 id="根据nginx日志--过滤nginx前十访问量的ip">
 根据nginx日志 过滤nginx前十访问量的ip
 &lt;a class="anchor" href="#%e6%a0%b9%e6%8d%aenginx%e6%97%a5%e5%bf%97--%e8%bf%87%e6%bb%a4nginx%e5%89%8d%e5%8d%81%e8%ae%bf%e9%97%ae%e9%87%8f%e7%9a%84ip">#&lt;/a>
&lt;/h3>
&lt;h5 id="要根据nginx日志过滤出前十访问量的ip你可以使用awksort和head命令组合来完成这个任务假设你的nginx访问日志格式如下这是nginx的默认格式">
 要根据Nginx日志过滤出前十访问量的IP，你可以使用&lt;code>awk&lt;/code>、&lt;code>sort&lt;/code>和&lt;code>head&lt;/code>命令组合来完成这个任务。假设你的Nginx访问日志格式如下（这是Nginx的默认格式）：
 &lt;a class="anchor" href="#%e8%a6%81%e6%a0%b9%e6%8d%aenginx%e6%97%a5%e5%bf%97%e8%bf%87%e6%bb%a4%e5%87%ba%e5%89%8d%e5%8d%81%e8%ae%bf%e9%97%ae%e9%87%8f%e7%9a%84ip%e4%bd%a0%e5%8f%af%e4%bb%a5%e4%bd%bf%e7%94%a8awksort%e5%92%8chead%e5%91%bd%e4%bb%a4%e7%bb%84%e5%90%88%e6%9d%a5%e5%ae%8c%e6%88%90%e8%bf%99%e4%b8%aa%e4%bb%bb%e5%8a%a1%e5%81%87%e8%ae%be%e4%bd%a0%e7%9a%84nginx%e8%ae%bf%e9%97%ae%e6%97%a5%e5%bf%97%e6%a0%bc%e5%bc%8f%e5%a6%82%e4%b8%8b%e8%bf%99%e6%98%afnginx%e7%9a%84%e9%bb%98%e8%ae%a4%e6%a0%bc%e5%bc%8f">#&lt;/a>
&lt;/h5>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>bash复制代码
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>127.0.0.1 - - &lt;span style="color:#f92672">[&lt;/span>10/Oct/2023:14:05:01 +0000&lt;span style="color:#f92672">]&lt;/span> &lt;span style="color:#e6db74">&amp;#34;GET / HTTP/1.1&amp;#34;&lt;/span> &lt;span style="color:#ae81ff">200&lt;/span> &lt;span style="color:#ae81ff">612&lt;/span> &lt;span style="color:#e6db74">&amp;#34;-&amp;#34;&lt;/span> &lt;span style="color:#e6db74">&amp;#34;curl/7.68.0&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h5 id="在这个日志中第一个字段是客户端的ip地址">
 在这个日志中，第一个字段是客户端的IP地址。
 &lt;a class="anchor" href="#%e5%9c%a8%e8%bf%99%e4%b8%aa%e6%97%a5%e5%bf%97%e4%b8%ad%e7%ac%ac%e4%b8%80%e4%b8%aa%e5%ad%97%e6%ae%b5%e6%98%af%e5%ae%a2%e6%88%b7%e7%ab%af%e7%9a%84ip%e5%9c%b0%e5%9d%80">#&lt;/a>
&lt;/h5>
&lt;h5 id="你可以使用以下命令来过滤出前十访问量的ip">
 你可以使用以下命令来过滤出前十访问量的IP：
 &lt;a class="anchor" href="#%e4%bd%a0%e5%8f%af%e4%bb%a5%e4%bd%bf%e7%94%a8%e4%bb%a5%e4%b8%8b%e5%91%bd%e4%bb%a4%e6%9d%a5%e8%bf%87%e6%bb%a4%e5%87%ba%e5%89%8d%e5%8d%81%e8%ae%bf%e9%97%ae%e9%87%8f%e7%9a%84ip">#&lt;/a>
&lt;/h5>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>bash复制代码
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>awk &lt;span style="color:#e6db74">&amp;#39;{print $1}&amp;#39;&lt;/span> access.log | sort | uniq -c | sort -nr | head -n &lt;span style="color:#ae81ff">10&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cat /var/log/nginx/access.log |awk &lt;span style="color:#e6db74">&amp;#39;{print $1}&amp;#39;&lt;/span>|sort|uniq -c |sort -nr|head -n &lt;span style="color:#ae81ff">10&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h5 id="这个命令的解释如下">
 这个命令的解释如下：
 &lt;a class="anchor" href="#%e8%bf%99%e4%b8%aa%e5%91%bd%e4%bb%a4%e7%9a%84%e8%a7%a3%e9%87%8a%e5%a6%82%e4%b8%8b">#&lt;/a>
&lt;/h5>
&lt;ol>
&lt;li>
&lt;h5 id="awk-print-1-accesslog使用awk命令从accesslog文件中提取每行的第一个字段即ip地址">
 &lt;code>awk '{print $1}' access.log&lt;/code>：使用&lt;code>awk&lt;/code>命令从&lt;code>access.log&lt;/code>文件中提取每行的第一个字段（即IP地址）。
 &lt;a class="anchor" href="#awk-print-1-accesslog%e4%bd%bf%e7%94%a8awk%e5%91%bd%e4%bb%a4%e4%bb%8eaccesslog%e6%96%87%e4%bb%b6%e4%b8%ad%e6%8f%90%e5%8f%96%e6%af%8f%e8%a1%8c%e7%9a%84%e7%ac%ac%e4%b8%80%e4%b8%aa%e5%ad%97%e6%ae%b5%e5%8d%b3ip%e5%9c%b0%e5%9d%80">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="sort对提取出的ip地址进行排序">
 &lt;code>sort&lt;/code>：对提取出的IP地址进行排序。
 &lt;a class="anchor" href="#sort%e5%af%b9%e6%8f%90%e5%8f%96%e5%87%ba%e7%9a%84ip%e5%9c%b0%e5%9d%80%e8%bf%9b%e8%a1%8c%e6%8e%92%e5%ba%8f">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="uniq--c统计每个唯一ip地址的出现次数">
 &lt;code>uniq -c&lt;/code>：统计每个唯一IP地址的出现次数。
 &lt;a class="anchor" href="#uniq--c%e7%bb%9f%e8%ae%a1%e6%af%8f%e4%b8%aa%e5%94%af%e4%b8%80ip%e5%9c%b0%e5%9d%80%e7%9a%84%e5%87%ba%e7%8e%b0%e6%ac%a1%e6%95%b0">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="sort--nr按数字进行逆序排序这样访问量最多的ip会排在最前面">
 &lt;code>sort -nr&lt;/code>：按数字进行逆序排序，这样访问量最多的IP会排在最前面。
 &lt;a class="anchor" href="#sort--nr%e6%8c%89%e6%95%b0%e5%ad%97%e8%bf%9b%e8%a1%8c%e9%80%86%e5%ba%8f%e6%8e%92%e5%ba%8f%e8%bf%99%e6%a0%b7%e8%ae%bf%e9%97%ae%e9%87%8f%e6%9c%80%e5%a4%9a%e7%9a%84ip%e4%bc%9a%e6%8e%92%e5%9c%a8%e6%9c%80%e5%89%8d%e9%9d%a2">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="head--n-10只显示前10行即访问量最多的前10个ip">
 &lt;code>head -n 10&lt;/code>：只显示前10行，即访问量最多的前10个IP。
 &lt;a class="anchor" href="#head--n-10%e5%8f%aa%e6%98%be%e7%a4%ba%e5%89%8d10%e8%a1%8c%e5%8d%b3%e8%ae%bf%e9%97%ae%e9%87%8f%e6%9c%80%e5%a4%9a%e7%9a%84%e5%89%8d10%e4%b8%aaip">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ol>
&lt;h5 id="请注意这个命令假设你的nginx日志文件名是accesslog如果你的日志文件名不同请相应地替换文件名此外这个命令也假设你的日志格式与上面所示的默认格式相匹配如果你的日志格式不同你可能需要调整awk命令中的字段选择器">
 请注意，这个命令假设你的Nginx日志文件名是&lt;code>access.log&lt;/code>。如果你的日志文件名不同，请相应地替换文件名。此外，这个命令也假设你的日志格式与上面所示的默认格式相匹配。如果你的日志格式不同，你可能需要调整&lt;code>awk&lt;/code>命令中的字段选择器。
 &lt;a class="anchor" href="#%e8%af%b7%e6%b3%a8%e6%84%8f%e8%bf%99%e4%b8%aa%e5%91%bd%e4%bb%a4%e5%81%87%e8%ae%be%e4%bd%a0%e7%9a%84nginx%e6%97%a5%e5%bf%97%e6%96%87%e4%bb%b6%e5%90%8d%e6%98%afaccesslog%e5%a6%82%e6%9e%9c%e4%bd%a0%e7%9a%84%e6%97%a5%e5%bf%97%e6%96%87%e4%bb%b6%e5%90%8d%e4%b8%8d%e5%90%8c%e8%af%b7%e7%9b%b8%e5%ba%94%e5%9c%b0%e6%9b%bf%e6%8d%a2%e6%96%87%e4%bb%b6%e5%90%8d%e6%ad%a4%e5%a4%96%e8%bf%99%e4%b8%aa%e5%91%bd%e4%bb%a4%e4%b9%9f%e5%81%87%e8%ae%be%e4%bd%a0%e7%9a%84%e6%97%a5%e5%bf%97%e6%a0%bc%e5%bc%8f%e4%b8%8e%e4%b8%8a%e9%9d%a2%e6%89%80%e7%a4%ba%e7%9a%84%e9%bb%98%e8%ae%a4%e6%a0%bc%e5%bc%8f%e7%9b%b8%e5%8c%b9%e9%85%8d%e5%a6%82%e6%9e%9c%e4%bd%a0%e7%9a%84%e6%97%a5%e5%bf%97%e6%a0%bc%e5%bc%8f%e4%b8%8d%e5%90%8c%e4%bd%a0%e5%8f%af%e8%83%bd%e9%9c%80%e8%a6%81%e8%b0%83%e6%95%b4awk%e5%91%bd%e4%bb%a4%e4%b8%ad%e7%9a%84%e5%ad%97%e6%ae%b5%e9%80%89%e6%8b%a9%e5%99%a8">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>cat access.log|awk &amp;#39;{print$1}&amp;#39;|sort|sort -nr|uniq -c|head -n 10
&lt;/code>&lt;/pre>&lt;h3 id="awk的-nf啥意思">
 awk的 $nf啥意思
 &lt;a class="anchor" href="#awk%e7%9a%84-nf%e5%95%a5%e6%84%8f%e6%80%9d">#&lt;/a>
&lt;/h3>
&lt;h5 id="在awk命令中n是一个字段引用其中n是一个数字表示当前行的第n个字段awk默认使用空白字符通常是空格或制表符作为字段分隔符因此123等分别代表每行的第一个第二个第三个字段依此类推">
 在&lt;code>awk&lt;/code>命令中，&lt;code>$n&lt;/code>是一个字段引用，其中&lt;code>n&lt;/code>是一个数字，表示当前行的第&lt;code>n&lt;/code>个字段。&lt;code>awk&lt;/code>默认使用空白字符（通常是空格或制表符）作为字段分隔符，因此&lt;code>$1&lt;/code>、&lt;code>$2&lt;/code>、&lt;code>$3&lt;/code>等分别代表每行的第一个、第二个、第三个字段，依此类推。
 &lt;a class="anchor" href="#%e5%9c%a8awk%e5%91%bd%e4%bb%a4%e4%b8%adn%e6%98%af%e4%b8%80%e4%b8%aa%e5%ad%97%e6%ae%b5%e5%bc%95%e7%94%a8%e5%85%b6%e4%b8%adn%e6%98%af%e4%b8%80%e4%b8%aa%e6%95%b0%e5%ad%97%e8%a1%a8%e7%a4%ba%e5%bd%93%e5%89%8d%e8%a1%8c%e7%9a%84%e7%ac%acn%e4%b8%aa%e5%ad%97%e6%ae%b5awk%e9%bb%98%e8%ae%a4%e4%bd%bf%e7%94%a8%e7%a9%ba%e7%99%bd%e5%ad%97%e7%ac%a6%e9%80%9a%e5%b8%b8%e6%98%af%e7%a9%ba%e6%a0%bc%e6%88%96%e5%88%b6%e8%a1%a8%e7%ac%a6%e4%bd%9c%e4%b8%ba%e5%ad%97%e6%ae%b5%e5%88%86%e9%9a%94%e7%ac%a6%e5%9b%a0%e6%ad%a4123%e7%ad%89%e5%88%86%e5%88%ab%e4%bb%a3%e8%a1%a8%e6%af%8f%e8%a1%8c%e7%9a%84%e7%ac%ac%e4%b8%80%e4%b8%aa%e7%ac%ac%e4%ba%8c%e4%b8%aa%e7%ac%ac%e4%b8%89%e4%b8%aa%e5%ad%97%e6%ae%b5%e4%be%9d%e6%ad%a4%e7%b1%bb%e6%8e%a8">#&lt;/a>
&lt;/h5>
&lt;h5 id="在处理nginx的访问日志时通常每个字段都是由特定的分隔符如空格或特殊字符分隔的例如在nginx的默认日志格式中">
 在处理Nginx的访问日志时，通常每个字段都是由特定的分隔符（如空格或特殊字符）分隔的。例如，在Nginx的默认日志格式中：
 &lt;a class="anchor" href="#%e5%9c%a8%e5%a4%84%e7%90%86nginx%e7%9a%84%e8%ae%bf%e9%97%ae%e6%97%a5%e5%bf%97%e6%97%b6%e9%80%9a%e5%b8%b8%e6%af%8f%e4%b8%aa%e5%ad%97%e6%ae%b5%e9%83%bd%e6%98%af%e7%94%b1%e7%89%b9%e5%ae%9a%e7%9a%84%e5%88%86%e9%9a%94%e7%ac%a6%e5%a6%82%e7%a9%ba%e6%a0%bc%e6%88%96%e7%89%b9%e6%ae%8a%e5%ad%97%e7%ac%a6%e5%88%86%e9%9a%94%e7%9a%84%e4%be%8b%e5%a6%82%e5%9c%a8nginx%e7%9a%84%e9%bb%98%e8%ae%a4%e6%97%a5%e5%bf%97%e6%a0%bc%e5%bc%8f%e4%b8%ad">#&lt;/a>
&lt;/h5>
&lt;pre tabindex="0">&lt;code>复制代码

127.0.0.1 - - [10/Oct/2023:14:05:01 +0000] &amp;#34;GET / HTTP/1.1&amp;#34; 200 612 &amp;#34;-&amp;#34; &amp;#34;curl/7.68.0&amp;#34;
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>
&lt;h5 id="1-通常是客户端的ip地址">
 &lt;code>$1&lt;/code> 通常是客户端的IP地址。
 &lt;a class="anchor" href="#1-%e9%80%9a%e5%b8%b8%e6%98%af%e5%ae%a2%e6%88%b7%e7%ab%af%e7%9a%84ip%e5%9c%b0%e5%9d%80">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="2-通常是身份验证的用户名在这个例子中是--表示没有用户名">
 &lt;code>$2&lt;/code> 通常是身份验证的用户名（在这个例子中是 &lt;code>-&lt;/code>，表示没有用户名）。
 &lt;a class="anchor" href="#2-%e9%80%9a%e5%b8%b8%e6%98%af%e8%ba%ab%e4%bb%bd%e9%aa%8c%e8%af%81%e7%9a%84%e7%94%a8%e6%88%b7%e5%90%8d%e5%9c%a8%e8%bf%99%e4%b8%aa%e4%be%8b%e5%ad%90%e4%b8%ad%e6%98%af--%e8%a1%a8%e7%a4%ba%e6%b2%a1%e6%9c%89%e7%94%a8%e6%88%b7%e5%90%8d">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="3-通常是用户的身份通常也是--">
 &lt;code>$3&lt;/code> 通常是用户的身份（通常也是 &lt;code>-&lt;/code>）。
 &lt;a class="anchor" href="#3-%e9%80%9a%e5%b8%b8%e6%98%af%e7%94%a8%e6%88%b7%e7%9a%84%e8%ba%ab%e4%bb%bd%e9%80%9a%e5%b8%b8%e4%b9%9f%e6%98%af--">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="4-是请求时间">
 &lt;code>$4&lt;/code> 是请求时间。
 &lt;a class="anchor" href="#4-%e6%98%af%e8%af%b7%e6%b1%82%e6%97%b6%e9%97%b4">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="5-是请求行例如-get--http11">
 &lt;code>$5&lt;/code> 是请求行（例如 &lt;code>&amp;quot;GET / HTTP/1.1&amp;quot;&lt;/code>）。
 &lt;a class="anchor" href="#5-%e6%98%af%e8%af%b7%e6%b1%82%e8%a1%8c%e4%be%8b%e5%a6%82-get--http11">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="6-是响应状态码例如-200">
 &lt;code>$6&lt;/code> 是响应状态码（例如 &lt;code>200&lt;/code>）。
 &lt;a class="anchor" href="#6-%e6%98%af%e5%93%8d%e5%ba%94%e7%8a%b6%e6%80%81%e7%a0%81%e4%be%8b%e5%a6%82-200">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="7-是响应体的大小例如-612">
 &lt;code>$7&lt;/code> 是响应体的大小（例如 &lt;code>612&lt;/code>）。
 &lt;a class="anchor" href="#7-%e6%98%af%e5%93%8d%e5%ba%94%e4%bd%93%e7%9a%84%e5%a4%a7%e5%b0%8f%e4%be%8b%e5%a6%82-612">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;li>
&lt;h5 id="后续的字段可能包括引用页用户代理等">
 后续的字段可能包括引用页、用户代理等。
 &lt;a class="anchor" href="#%e5%90%8e%e7%bb%ad%e7%9a%84%e5%ad%97%e6%ae%b5%e5%8f%af%e8%83%bd%e5%8c%85%e6%8b%ac%e5%bc%95%e7%94%a8%e9%a1%b5%e7%94%a8%e6%88%b7%e4%bb%a3%e7%90%86%e7%ad%89">#&lt;/a>
&lt;/h5>
&lt;/li>
&lt;/ul>
&lt;h5 id="当你使用awk-print-1-accesslog时你正在告诉awk打印出每一行的第一个字段即客户端的ip地址">
 当你使用&lt;code>awk '{print $1}' access.log&lt;/code>时，你正在告诉&lt;code>awk&lt;/code>打印出每一行的第一个字段，即客户端的IP地址。
 &lt;a class="anchor" href="#%e5%bd%93%e4%bd%a0%e4%bd%bf%e7%94%a8awk-print-1-accesslog%e6%97%b6%e4%bd%a0%e6%ad%a3%e5%9c%a8%e5%91%8a%e8%af%89awk%e6%89%93%e5%8d%b0%e5%87%ba%e6%af%8f%e4%b8%80%e8%a1%8c%e7%9a%84%e7%ac%ac%e4%b8%80%e4%b8%aa%e5%ad%97%e6%ae%b5%e5%8d%b3%e5%ae%a2%e6%88%b7%e7%ab%af%e7%9a%84ip%e5%9c%b0%e5%9d%80">#&lt;/a>
&lt;/h5>
&lt;h5 id="awk也允许你通过-f选项来指定一个不同的字段分隔符例如如果nginx日志使用不同的分隔符你可以这样指定">
 &lt;code>awk&lt;/code>也允许你通过&lt;code>-F&lt;/code>选项来指定一个不同的字段分隔符。例如，如果Nginx日志使用不同的分隔符，你可以这样指定：
 &lt;a class="anchor" href="#awk%e4%b9%9f%e5%85%81%e8%ae%b8%e4%bd%a0%e9%80%9a%e8%bf%87-f%e9%80%89%e9%a1%b9%e6%9d%a5%e6%8c%87%e5%ae%9a%e4%b8%80%e4%b8%aa%e4%b8%8d%e5%90%8c%e7%9a%84%e5%ad%97%e6%ae%b5%e5%88%86%e9%9a%94%e7%ac%a6%e4%be%8b%e5%a6%82%e5%a6%82%e6%9e%9cnginx%e6%97%a5%e5%bf%97%e4%bd%bf%e7%94%a8%e4%b8%8d%e5%90%8c%e7%9a%84%e5%88%86%e9%9a%94%e7%ac%a6%e4%bd%a0%e5%8f%af%e4%bb%a5%e8%bf%99%e6%a0%b7%e6%8c%87%e5%ae%9a">#&lt;/a>
&lt;/h5>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>bash复制代码
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>awk -F&lt;span style="color:#e6db74">&amp;#39; &amp;#39;&lt;/span> &lt;span style="color:#e6db74">&amp;#39;{print $1}&amp;#39;&lt;/span> access.log
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>这里的&lt;code>-F' '&lt;/code>告诉&lt;code>awk&lt;/code>使用空格作为字段分隔符。但是，请注意，Nginx日志中的时间戳通常包含在方括号内，这可能会影响字段的计数。如果时间戳是一个字段，并且你想跳过它，你可能需要使用更复杂的&lt;code>awk&lt;/code>脚本或者不同的方法来处理日志。&lt;/p></description></item><item><title>2024-4-17 面试总结</title><link>https://qq547475331.github.io/docs/2024-4-17-%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2024-4-17-%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93/</guid><description>&lt;pre tabindex="0">&lt;code>1.查看主机的硬件信息的命令 包括cpu 网卡 显卡这些
2.查看cpu负载情况 load的数值的含义
3.优化做过哪些？内核参数优化，具体讲下
4.nginx上的服务访问不通 怎么排查
5.502和504的区别
6.k8s的组件有哪些
7.kubelet和kube-proxy在主节点上会跑吗
8.k8s的网络插件用过哪些，calico的BGP模式讲下
&lt;/code>&lt;/pre>&lt;p>&lt;strong>1.查看主机的硬件信息的命令 包括cpu 网卡 显卡这些&lt;/strong>&lt;/p>
&lt;pre tabindex="0">&lt;code>CPU信息：

在终端输入：lscpu
显卡信息：
cat /etc/cpuinfo 或 cat /proc/cpuinfo

使用命令：lspci | grep VGA （通常用于PCIe接口的显卡）
或者 lshw -c display 获取更详细的显卡信息
网卡信息：

使用命令：lspci | grep Ethernet （对于PCIe接口的网卡）
或者 ifconfig -a 或 ip addr show 查看网络接口及其IP配置
若要获取更详细的硬件信息，可以使用 ethtool -i [interface_name]，其中 [interface_name] 是你想要查询的网卡名称。
&lt;/code>&lt;/pre>&lt;p>&lt;strong>2.查看cpu负载情况 load的数值的含义&lt;/strong>&lt;/p>
&lt;pre tabindex="0">&lt;code>12:34:56 up 1 day, 3:23, 3 users, load average: 0.23, 0.25, 0.28

其中，“load average”后面的三个数字分别表示过去1分钟、5分钟和15分钟的平均负载。这些数值表示的是在这段时间内，系统中处于运行（包括正在运行和等待运行）状态的平均进程数（或线程数）。

负载数值的含义：

单核CPU情况下，当负载平均值为1.0时，意味着CPU在这段时间里一直忙于处理进程，没有闲置时间，达到了理论上的满载状态。
多核CPU环境中，负载平均值应当参照CPU核心数量。例如，对于一个8核CPU来说，理想的负载应该低于8.0，即所有核心都被充分利用且无进程等待。不过，实际上，由于进程并非总是能够完全并行处理，所以理想的负载水平往往低于实际核心数，一般认为是核心数的0.7倍左右较为健康。
解读负载值：

负载值小于1表明系统整体上是轻载的，CPU有足够的处理能力和时间处理新的请求。
负载值接近或超过CPU核心数，意味着系统可能正面临资源瓶颈，有较多的进程在等待CPU时间片分配，可能会出现性能下降或响应延迟。
长时间持续较高的负载可能意味着系统配置不足或者有进程出现了性能问题，需要进一步排查优化。
&lt;/code>&lt;/pre>&lt;p>&lt;strong>3.优化做过哪些？内核参数优化，具体讲下&lt;/strong>&lt;/p></description></item><item><title>2024-5-1 单master单etcd改造为3master3etcd</title><link>https://qq547475331.github.io/docs/2024-5-14-%E5%8D%95master%E5%8D%95etcd%E6%94%B9%E9%80%A0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2024-5-14-%E5%8D%95master%E5%8D%95etcd%E6%94%B9%E9%80%A0/</guid><description>&lt;pre tabindex="0">&lt;code>备份etcd的数据

1、单master单etcd扩展到3master3etcd节点
2、涉及的 
 master节点的证书
 - apiserver.crt
 - apiserver.csr
 - apiserver.key
 - client.crt
 - client.key
 - etcd_client.crt
 - etcd_client.key
 - front-proxy_bak-ca.crt
 - front-proxy_bak-ca.key
 - front-proxy_bak-ca.srl
 - front-proxy_bak-client.crt
 - front-proxy_bak-client.key
 - front_proxy_ssl.cnf
 - sa.key
 - sa.pub 
 
 - etcd_server.crt
 - etcd_server.key
 - etcd_client.crt
 - etcd_client.key
 
 work节点的证书
 - ca.crt 
 - client.key 
 - client.crt 
 
 需要重新生成，ca.crt和ca.key可以不用重新生成，新的证书需要准备好，老的证书需要备份好。
3、根据ca证书和master_ssl.cnf，etcd_ssl.cnf生成新的master节点和etcd节点证书，根据ca证书和master_ssl.cnf生成work节点的client.crt证书，提前准备好。 master_ssl.cnf包含3个master节点的ip和硬负载vip，etcd_ssl.cnf包含3个etcd节点的ip。
4、拷贝新生成的etcd证书到etcd节点，替换老的证书，重新etcd节点。
5、拷贝新生成的master节点的节点到master节点，替换老的master节点证书，重启master节点的master服务，包括kube-apiserver，kube-scheduler，kube-controller-manager，kubelet，kube-proxy。
6、拷贝新生成的work节点的证书到work节点，替换work节点的证书，并重启node节点的kubelet和kube-proxy服务。
7、验证master节点的相关服务是否正常，包括kube-apiserver，kube-scheduler，kube-controller-manager，kubelet，kube-proxy，etcd服务。node节点的kubelet和kube-proxy服务。


回退，删除新加的2个master节点，将老的备份的证书替换回去，重启master节点和node节点的相关服务
&lt;/code>&lt;/pre>&lt;pre tabindex="0">&lt;code>模拟单master单etcd节点 扩展到 3master3etcd

现状：
master1 192.168.0.61
node1 192.168.0.64

改造后
master1：192.168.0.61
master2：192.168.0.62
master3：192.168.0.63
node1： 192.168.0.64
lb1： 192.168.0.71
lb2： 192.168.0.72
vip： 192.168.0.70
&lt;/code>&lt;/pre>&lt;pre tabindex="0">&lt;code>1、备份etcd数据
2、停止所有节点的kube-apiserver，kube-scheduler，kube-controller-manager，etcd，kubelet，kube-proxy服务
3、修改all.yml里的master节点的vip为真实的vip，hosts文件增加master组和etcd组，node组 2个新增的master节点的ip
4、备份老的证书文件夹下的文件，执行ansible-playbook -i hosts setup.yml，只包含创建证书这一步

在/opt/kubernetes_ssl下重新生成apiserver.crt
apiserver.csr
apiserver.key
ca.crt
ca.key
ca.srl
client.crt
client.csr
client.key
etcd_client.crt
etcd_client.csr
etcd_client.key
etcd_server.crt
etcd_server.csr
etcd_server.key
etcd_ssl.cnf
front-proxy-ca.crt
front-proxy-ca.key
front-proxy-ca.srl
front-proxy-client.crt
front-proxy-client.csr
front-proxy-client.key
front_proxy_ssl.cnf
master_ssl.cnf
sa.key
sa.pub
证书 
&lt;/code>&lt;/pre>&lt;h3 id="单master-单etcd-pod-pvc-sc需要扩容后数据任然存在">
 单master 单etcd pod pvc sc，需要扩容后数据任然存在
 &lt;a class="anchor" href="#%e5%8d%95master-%e5%8d%95etcd-pod-pvc-sc%e9%9c%80%e8%a6%81%e6%89%a9%e5%ae%b9%e5%90%8e%e6%95%b0%e6%8d%ae%e4%bb%bb%e7%84%b6%e5%ad%98%e5%9c%a8">#&lt;/a>
&lt;/h3>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202405141912859.png" alt="image-20240514191208781" />&lt;/p></description></item><item><title>2024-8-1 k8s面试题</title><link>https://qq547475331.github.io/docs/2024-8-1-kubernetes%E9%9D%A2%E8%AF%95%E9%A2%98/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2024-8-1-kubernetes%E9%9D%A2%E8%AF%95%E9%A2%98/</guid><description>&lt;h2 id="简述etcd及其特点">
 &lt;strong>简述ETCD及其特点？&lt;/strong>
 &lt;a class="anchor" href="#%e7%ae%80%e8%bf%b0etcd%e5%8f%8a%e5%85%b6%e7%89%b9%e7%82%b9">#&lt;/a>
&lt;/h2>
&lt;p>&lt;strong>etcd 是 CoreOS 团队发起的开源项目，是一个管理配置信息和服务发现（service discovery）的项目，它的目标是构建一个高可用的分布式键值（key-value）数据库，基于 Go 语言实现。&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>特点：&lt;/strong>&lt;/p>
&lt;/li>
&lt;li>
&lt;ul>
&lt;li>&lt;strong>简单：支持 REST 风格的 HTTP+JSON API&lt;/strong>&lt;/li>
&lt;li>&lt;strong>安全：支持 HTTPS 方式的访问&lt;/strong>&lt;/li>
&lt;li>&lt;strong>快速：支持并发 1k/s 的写操作&lt;/strong>&lt;/li>
&lt;li>&lt;strong>可靠：支持分布式结构，基于 Raft 的一致性算法，Raft 是一套通过选举主节点来实现分布式系统一致性的算法。&lt;/strong>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="简述etcd适应的场景">
 &lt;strong>简述ETCD适应的场景？&lt;/strong>
 &lt;a class="anchor" href="#%e7%ae%80%e8%bf%b0etcd%e9%80%82%e5%ba%94%e7%9a%84%e5%9c%ba%e6%99%af">#&lt;/a>
&lt;/h2>
&lt;p>&lt;strong>etcd基于其优秀的特点，可广泛的应用于以下场景：&lt;/strong>&lt;/p>
&lt;p>&lt;strong>服务发现(Service Discovery)：服务发现主要解决在同一个分布式集群中的进程或服务，要如何才能找到对方并建立连接。本质上来说，服务发现就是想要了解集群中是否有进程在监听udp或tcp端口，并且通过名字就可以查找和连接。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>消息发布与订阅：在分布式系统中，最适用的一种组件间通信方式就是消息发布与订阅。即构建一个配置共享中心，数据提供者在这个配置中心发布消息，而消息使用者则订阅他们关心的主题，一旦主题有消息发布，就会实时通知订阅者。通过这种方式可以做到分布式系统配置的集中式管理与动态更新。应用中用到的一些配置信息放到etcd上进行集中管理。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>负载均衡：在分布式系统中，为了保证服务的高可用以及数据的一致性，通常都会把数据和服务部署多份，以此达到对等服务，即使其中的某一个服务失效了，也不影响使用。etcd本身分布式架构存储的信息访问支持负载均衡。etcd集群化以后，每个etcd的核心节点都可以处理用户的请求。所以，把数据量小但是访问频繁的消息数据直接存储到etcd中也可以实现负载均衡的效果。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>分布式通知与协调：与消息发布和订阅类似，都用到了etcd中的Watcher机制，通过注册与异步通知机制，实现分布式环境下不同系统之间的通知与协调，从而对数据变更做到实时处理。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>分布式锁：因为etcd使用Raft算法保持了数据的强一致性，某次操作存储到集群中的值必然是全局一致的，所以很容易实现分布式锁。锁服务有两种使用方式，一是保持独占，二是控制时序。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>集群监控与Leader竞选：通过etcd来进行监控实现起来非常简单并且实时性强。&lt;/strong>&lt;/p>
&lt;h2 id="简述什么是kubernetes">
 &lt;strong>简述什么是Kubernetes？&lt;/strong>
 &lt;a class="anchor" href="#%e7%ae%80%e8%bf%b0%e4%bb%80%e4%b9%88%e6%98%afkubernetes">#&lt;/a>
&lt;/h2>
&lt;p>&lt;strong>Kubernetes是一个全新的基于容器技术的分布式系统支撑平台。是Google开源的容器集群管理系统（谷歌内部:Borg）。在Docker技术的基础上，为容器化的应用提供部署运行、资源调度、服务发现和动态伸缩等一系列完整功能，提高了大规模容器集群管理的便捷性。并且具有完备的集群管理能力，多层次的安全防护和准入机制、多租户应用支撑能力、透明的服务注册和发现机制、內建智能负载均衡器、强大的故障发现和自我修复能力、服务滚动升级和在线扩容能力、可扩展的资源自动调度机制以及多粒度的资源配额管理能力。&lt;/strong>&lt;/p>
&lt;h2 id="简述kubernetes和docker的关系">
 &lt;strong>简述Kubernetes和Docker的关系？&lt;/strong>
 &lt;a class="anchor" href="#%e7%ae%80%e8%bf%b0kubernetes%e5%92%8cdocker%e7%9a%84%e5%85%b3%e7%b3%bb">#&lt;/a>
&lt;/h2>
&lt;p>&lt;strong>Docker 提供容器的生命周期管理和，Docker 镜像构建运行时容器。它的主要优点是将将软件/应用程序运行所需的设置和依赖项打包到一个容器中，从而实现了可移植性等优点。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>Kubernetes 用于关联和编排在多个主机上运行的容器。&lt;/strong>&lt;/p>
&lt;h2 id="简述kubernetes中什么是minikubekubectlkubelet">
 &lt;strong>简述Kubernetes中什么是Minikube、Kubectl、Kubelet？&lt;/strong>
 &lt;a class="anchor" href="#%e7%ae%80%e8%bf%b0kubernetes%e4%b8%ad%e4%bb%80%e4%b9%88%e6%98%afminikubekubectlkubelet">#&lt;/a>
&lt;/h2>
&lt;p>&lt;strong>Minikube 是一种可以在本地轻松运行一个单节点 Kubernetes 群集的工具。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>Kubectl 是一个命令行工具，可以使用该工具控制Kubernetes集群管理器，如检查群集资源，创建、删除和更新组件，查看应用程序。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>Kubelet 是一个代理服务，它在每个节点上运行，并使从服务器与主服务器通信。&lt;/strong>&lt;/p>
&lt;h2 id="简述kubernetes常见的部署方式">
 &lt;strong>简述Kubernetes常见的部署方式？&lt;/strong>
 &lt;a class="anchor" href="#%e7%ae%80%e8%bf%b0kubernetes%e5%b8%b8%e8%a7%81%e7%9a%84%e9%83%a8%e7%bd%b2%e6%96%b9%e5%bc%8f">#&lt;/a>
&lt;/h2>
&lt;p>&lt;strong>常见的Kubernetes部署方式有：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>kubeadm：也是推荐的一种部署方式；&lt;/strong>&lt;/li>
&lt;li>&lt;strong>二进制：网页上很多教程，未来我也会写一个&lt;/strong>&lt;/li>
&lt;li>&lt;strong>minikube：在本地轻松运行一个单节点 Kubernetes 群集的工具。&lt;/strong>&lt;/li>
&lt;li>&lt;/li>
&lt;/ul>
&lt;h2 id="简述kubernetes如何实现集群管理">
 &lt;strong>简述Kubernetes如何实现集群管理？&lt;/strong>
 &lt;a class="anchor" href="#%e7%ae%80%e8%bf%b0kubernetes%e5%a6%82%e4%bd%95%e5%ae%9e%e7%8e%b0%e9%9b%86%e7%be%a4%e7%ae%a1%e7%90%86">#&lt;/a>
&lt;/h2>
&lt;p>&lt;strong>在集群管理方面，Kubernetes将集群中的机器划分为一个Master节点和一群工作节点Node。其中，在Master节点运行着集群管理相关的一组进程kube-apiserver、kube-controller-manager和kube-scheduler，这些进程实现了整个集群的资源管理、Pod调度、弹性伸缩、安全控制、系统监控和纠错等管理能力，并且都是全自动完成的。&lt;/strong>&lt;/p></description></item><item><title>2024-8-1 linux运维面试题</title><link>https://qq547475331.github.io/docs/2024-8-1-%E5%B8%B8%E8%A7%81linux%E8%BF%90%E7%BB%B4%E9%9D%A2%E8%AF%95%E9%A2%98%E6%89%BE%E5%B7%A5%E4%BD%9C%E7%9A%84%E5%BF%85%E7%9C%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2024-8-1-%E5%B8%B8%E8%A7%81linux%E8%BF%90%E7%BB%B4%E9%9D%A2%E8%AF%95%E9%A2%98%E6%89%BE%E5%B7%A5%E4%BD%9C%E7%9A%84%E5%BF%85%E7%9C%8B/</guid><description>&lt;p>下面是一名运维人员求职数十家公司总结的 Linux运维面试题，给大家参考下~&lt;/p>
&lt;p>&lt;strong>1、现在给你三百台服务器，你怎么对他们进行管理？&lt;/strong>&lt;/p>
&lt;p>管理3百台服务器的方式：
1）设定跳板机，使用统一账号登录，便于安全与登录的考量。
2）使用 salt、ansiable、puppet 进行系统的统一调度与配置的统一管理。
3）建立简单的服务器的系统、配置、应用的 cmdb 信息管理。便于查阅每台服务器上的各种信息记录。&lt;/p>
&lt;p>&lt;strong>2、简述 raid0 raid1 raid5 三种工作模式的工作原理及特点&lt;/strong>&lt;/p>
&lt;p>RAID，可以把硬盘整合成一个大磁盘，还可以在大磁盘上再分区，放数据
还有一个大功能，多块盘放在一起可以有冗余（备份）
RAID整合方式有很多，常用的：0 1 5 10&lt;/p>
&lt;p>&lt;strong>RAID 0，可以是一块盘和 N 个盘组合&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>优点&lt;/strong>读写快，是 RAID 中最好的&lt;/li>
&lt;li>&lt;strong>缺点&lt;/strong>：没有冗余，一块坏了数据就全没有了&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>RAID 1，只能2块盘，盘的大小可以不一样，以小的为准。&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>10G+10G只有10G，另一个做备份。它有100%的冗余，&lt;/li>
&lt;li>缺点：浪费资源，成本高&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>RAID 5 ，3块盘，容量计算10 *（n-1），损失一块盘&lt;/strong>&lt;/p>
&lt;p>特点，读写性能一般，读还好一点，写不好&lt;/p>
&lt;ul>
&lt;li>冗余从好到坏：RAID1 RAID10 RAID 5 RAID0&lt;/li>
&lt;li>性能从好到坏：RAID0 RAID10 RAID5 RAID1&lt;/li>
&lt;li>成本从低到高：RAID0 RAID5 RAID1 RAID10&lt;/li>
&lt;/ul>
&lt;p>单台服务器：很重要盘不多，系统盘，RAID1
数据库服务器：主库：RAID10 从库 RAID5\RAID0（为了维护成本，RAID10）
WEB服务器，如果没有太多的数据的话，RAID5,RAID0（单盘）
有多台，监控、应用服务器，RAID0 RAID5&lt;/p>
&lt;p>我们会根据数据的存储和访问的需求，去匹配对应的RAID级别&lt;/p>
&lt;p>&lt;strong>3、LVS、Nginx、HAproxy 有什么区别？工作中你怎么选择？&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>LVS&lt;/strong>：是基于四层的转发&lt;/li>
&lt;li>&lt;strong>HAproxy&lt;/strong>：是基于四层和七层的转发，是专业的代理服务器&lt;/li>
&lt;li>&lt;strong>Nginx&lt;/strong>：是WEB服务器，缓存服务器，又是反向代理服务器，可以做七层的转发&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>区别：LVS由于是基于四层的转发所以只能做端口的转发，而基于URL的、基于目录的这种转发LVS就做不了。&lt;/p>&lt;/blockquote>
&lt;p>工作选择：&lt;/p>
&lt;ul>
&lt;li>HAproxy 和 Nginx 由于可以做七层的转发，所以 URL 和目录的转发都可以做&lt;/li>
&lt;li>在很大并发量的时候我们就要选择LVS，像中小型公司的话并发量没那么大&lt;/li>
&lt;li>选择 HAproxy 或者 Nginx 足已，由于 HAproxy 由是专业的代理服务器&lt;/li>
&lt;li>配置简单，所以中小型企业推荐使用 HAproxy&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>4、Squid、Varinsh 和 Nginx 有什么区别，工作中你怎么选择？&lt;/strong>&lt;/p></description></item><item><title>2024-8-1 linux面试题</title><link>https://qq547475331.github.io/docs/2024-8-1-linux%E8%BF%90%E7%BB%B4%E5%B7%A5%E7%A8%8B%E5%B8%8850%E4%B8%AA%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2024-8-1-linux%E8%BF%90%E7%BB%B4%E5%B7%A5%E7%A8%8B%E5%B8%8850%E4%B8%AA%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/</guid><description>&lt;p>&lt;strong>1、请简述OSI七层网络模型有哪些层及各自的含义?&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>物理层：底层数据传输，比如网线、网卡标准&lt;/strong>&lt;/li>
&lt;li>&lt;strong>数据链路层：定义数据的基本格式，如何传输，如何标识。比如网卡MAC地址&lt;/strong>&lt;/li>
&lt;li>&lt;strong>网络层：定义IP编码，定义路由功能，比如不同设备的数据转发&lt;/strong>&lt;/li>
&lt;li>&lt;strong>传输层：端到端传输数据的基本功能，比如TCP、UDP&lt;/strong>&lt;/li>
&lt;li>&lt;strong>会话层：控制应用程序之间会话能力，比如不同软件数据分发给不停软件&lt;/strong>&lt;/li>
&lt;li>&lt;strong>表示层：数据格式标识，基本压缩加密功能。&lt;/strong>&lt;/li>
&lt;li>&lt;strong>应用层：各种应用软件，包括 Web 应用。&lt;/strong>&lt;/li>
&lt;/ul>
&lt;h3 id="2在linux的lvm分区格式下请简述给根分区磁盘扩容的步骤">
 &lt;strong>2、在Linux的LVM分区格式下，请简述给根分区磁盘扩容的步骤?&lt;/strong>
 &lt;a class="anchor" href="#2%e5%9c%a8linux%e7%9a%84lvm%e5%88%86%e5%8c%ba%e6%a0%bc%e5%bc%8f%e4%b8%8b%e8%af%b7%e7%ae%80%e8%bf%b0%e7%bb%99%e6%a0%b9%e5%88%86%e5%8c%ba%e7%a3%81%e7%9b%98%e6%89%a9%e5%ae%b9%e7%9a%84%e6%ad%a5%e9%aa%a4">#&lt;/a>
&lt;/h3>
&lt;p>&lt;strong>这个分3种&lt;/strong>&lt;/p>
&lt;p>&lt;strong>第一种方法:&lt;/strong>&lt;/p>
&lt;pre tabindex="0">&lt;code>growpart /dev/vda 1
resize2fs /dev/vda1 
&lt;/code>&lt;/pre>&lt;p>&lt;strong>第二种方法:&lt;/strong>&lt;/p>
&lt;pre tabindex="0">&lt;code>partpeobe /dev/sda
resize2fs /dev/vda1 
&lt;/code>&lt;/pre>&lt;p>&lt;strong>第三种方法:&lt;/strong>&lt;/p>
&lt;pre tabindex="0">&lt;code>fdisk /dev/sdb # n p 1 1 回车 回车 t 8e w
pvcreate /dev/sdb1
vgextend datavg /dev/sdb1
lvextend -r -L +100%free /dev/mapper/datavg-lv01
&lt;/code>&lt;/pre>&lt;h3 id="3讲述一下tomcat800580098080三个端口的含义">
 &lt;strong>3、讲述一下Tomcat8005、8009、8080三个端口的含义？&lt;/strong>
 &lt;a class="anchor" href="#3%e8%ae%b2%e8%bf%b0%e4%b8%80%e4%b8%8btomcat800580098080%e4%b8%89%e4%b8%aa%e7%ab%af%e5%8f%a3%e7%9a%84%e5%90%ab%e4%b9%89">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>8005 关闭时使用&lt;/strong>&lt;/li>
&lt;li>&lt;strong>8009为AJP端口，即容器使用，如Apache能通过AJP协议访问Tomcat的8009端口来实现功能&lt;/strong>&lt;/li>
&lt;li>&lt;strong>8080 一般应用使用&lt;/strong>&lt;/li>
&lt;/ul>
&lt;h3 id="4简述dns进行域名解析的过程">
 &lt;strong>4、简述DNS进行域名解析的过程？&lt;/strong>
 &lt;a class="anchor" href="#4%e7%ae%80%e8%bf%b0dns%e8%bf%9b%e8%a1%8c%e5%9f%9f%e5%90%8d%e8%a7%a3%e6%9e%90%e7%9a%84%e8%bf%87%e7%a8%8b">#&lt;/a>
&lt;/h3>
&lt;p>&lt;strong>迭代查询（返回最优结果）、递归查询（本地找DNS）用户要访问 &lt;a href="https://www.baidu.com">www.baidu.com&lt;/a>，会先找本机的host文件，再找本地设置的DNS服务器，如果也没有找到，就去网络中找根服务器，根服务器反馈结果，说只能提供一级域名服务器.cn，就去找一级域名服务器，一级域名服务器说只能提供二级域名服务器.com.cn,就去找二级域名服务器，二级域服务器只能提供三级域名服务器.baidu.com.cn，就去找三级域名服务器，三级域名服务器正好有这个网站www.baidu.com，然后发给请求的服务器，保存一份之后，再发给客户端。&lt;/strong>&lt;/p>
&lt;h3 id="5讲一下keepalived的工作原理">
 &lt;strong>5、讲一下Keepalived的工作原理？&lt;/strong>
 &lt;a class="anchor" href="#5%e8%ae%b2%e4%b8%80%e4%b8%8bkeepalived%e7%9a%84%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86">#&lt;/a>
&lt;/h3>
&lt;p>&lt;strong>在一个虚拟路由器中，只有作为MASTER的VRRP(虚拟路由冗余协议)路由器会一直发送VRRP通告信息，BACKUP不会抢占MASTER，除非它的优先级更高。当MASTER不可用时(BACKUP收不到通告信息)多台BACKUP中优先级最高的这台会被抢占为MASTER。这种抢占是非常快速的(&amp;lt;1s)，以保证服务的连续性由于安全性考虑，VRRP包使用了加密协议进行加密。BACKUP不会发送通告信息，只会接收通告信息。&lt;/strong>&lt;/p>
&lt;h3 id="6lvsnginxhaproxy有什么区别工作中你怎么选择">
 &lt;strong>6、LVS、Nginx、HAproxy有什么区别？工作中你怎么选择？&lt;/strong>
 &lt;a class="anchor" href="#6lvsnginxhaproxy%e6%9c%89%e4%bb%80%e4%b9%88%e5%8c%ba%e5%88%ab%e5%b7%a5%e4%bd%9c%e4%b8%ad%e4%bd%a0%e6%80%8e%e4%b9%88%e9%80%89%e6%8b%a9">#&lt;/a>
&lt;/h3>
&lt;p>&lt;strong>LVS：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>抗负载能力强、工作在第4层仅作分发之用，没有流量的产生，这个特点也决定了它在负载均衡软件里的性能最强的；无流量，同时保证了均衡器IO的性能不会受到大流量的影响；&lt;/strong>&lt;/li>
&lt;li>&lt;strong>工作稳定，自身有完整的双机热备方案，如LVS+Keepalived和LVS+Heartbeat；&lt;/strong>&lt;/li>
&lt;li>&lt;strong>应用范围比较广，可以对所有应用做负载均衡；&lt;/strong>&lt;/li>
&lt;li>&lt;strong>配置简单，因为没有可太多配置的东西，所以并不需要太多接触，大大减少了人为出错的几率；&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>LVS的缺点：&lt;/strong>&lt;/p></description></item><item><title>2025-1-1 sealos获投</title><link>https://qq547475331.github.io/docs/2025-1-1-sealos%E8%8E%B7%E6%8A%95/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2025-1-1-sealos%E8%8E%B7%E6%8A%95/</guid><description>&lt;p>36氪获悉，「sealos」项目的所属公司「环界云计算」近日已完成总计2000万元的天使轮和天使+轮融资。该公司成立于 2022年3月，其核心产品sealos是一款以 kubernetes 为内核的云操作系统发行版。据介绍，这两轮融资在三个月内相继完成。其中，天使轮融资由奇绩创坛领投，清华信息学院院长李军、中科大微电子康一教授和 coding 创始人张海龙跟投。天使+轮则由高捷资本独家投资。&lt;/p>
&lt;p>“分布式”与“云原生”一直是近些年计算机领域的热点话题。近年来，这一领域涌现出多家创业公司，而 sealos 正是一家服务分布式应用的开发者与用户的公司。&lt;/p>
&lt;p>sealos 致力打造以 Kubernetes 为内核的开源云操作系统发行版，希望借助Kubernetes 的开源与轻便等特性，让用户用云像使用 PC 操作系统一样简单。sealos 创始人方海涛向36氪表示，linux kernel 诞生随后便出现很多发行版让单机操作系统普及，今天 kubernetes 已经成为云内核的事实标准，所以基于 kubernetes 的云操作系统发行版未来也必然会普及。传统的操作系统是运行在一台机器上的，而对如今众多的分布式应用来说，其天生就需要在多台机器上运行。因此，这些应用的开发者需要花大量的精力去管理众多的机器。比如，开发者需要实现多台机器之间的数据备份、流量监控等功能。方海涛认为，开发者需要一个云操作系统，让云操作系统来管理这些底层的服务器资源，这样开发者可以更专注于应用本身。sealos 正是这样一款向下可以对整个数据中心资源抽象池化，向上可以管理各种分布式应用的云操作系统。最终 sealos 希望让云变得简单/便宜/开放/普及。&lt;/p>
&lt;p>并且，分布式应用对用户也有很高的操作门槛，比如让一个 DBA 去操作 kubernetes 显然是一件不太合理的事情，而 sealos 想把分布式应用的用户门槛降低。比如方海涛向36氪展示，sealos拥有类似于 linux desktop 的图形化界面，点击其桌面上的某个图标，就可以运行该图标对应的分布式数据库，这样抽象的设计可以让不同类型的用户都能拥有很好的使用体验，这就是通过云操作系统提供给用户真正想要的能力，这个能力可以是数据库，消息队列，AI 能力，甚至是一个企业级 CRM 软件。整个操作系统所有能力也提供 API 方便开发者调用各种能力。方海涛补充，sealos 通过分布式应用的 Store，来连接云原生应用的开发者与用户，从而改变云服务生产关系。&lt;/p>
&lt;p>讲起云操作系统与各大云平台的关系，方海涛讲到，与云平台不同，sealos 不再采用 IaaS、PaaS、SaaS 三层架构来设计，sealos 采用的是&lt;strong>基于kubernetes的&lt;/strong>“云内核”的架构，一切皆应用的设计理念。Sealos这样的云内核更精简更内聚，更高度抽象，也更方便企业自由组合这些应用，来满足自身场景。单机操作系统也是从分层架构演化成今天的宏内核微内核架构，云计算架构必然也会从分层架构像云内核架构迁移。&lt;/p>
&lt;p>其次，sealos 完全开源开放。这样用户在使用的时候就不用担心厂商绑定的问题，可以到处运行。对于与云厂商的关系，方海涛认为，类似于基于公有云的Snowflake，sealos 与云厂商同样也是以合作为主，sealos 可以通过开源为云厂商引流，达到共赢的目的，sealos cloud 也是可以完全运行在各大公有云平台之上的。&lt;/p>
&lt;p>最后，在 sealos 的设计中，公有云与私有云是一个东西，本质上也没有区别，所以 sealos 可以用同一套代码同时满足公有云和私有云的场景，像很多大B 的私有云需求都要求多租户，计量，甚至计费，因为有很多部门涉及到成本分摊，虽然是私有云但是需求上更贴近公有云，方海涛认为公有云与私有云差异点只在上面安装的应用不同或者一些配置参数不同而已。&lt;/p>
&lt;p>谈到公司已经取得的进展，方海涛讲，sealos Github star 数接近一万，近半年增长超过历史总和，sealos 历史订阅付费客户也超过 4k。 sealos 上的杀手级函数计算应用月增也超过 100%，上线一个月 5000 多开发者跑了 4000 多应用， 注册用户数平均每月有 150% 的增长速度。此外，sealos 创新性地提供集群镜像功能，docker 的镜像(image)一般是单机镜像，对于分布式场景则不够便利，而集群镜像能把单机上的 build ship run 抽象到了集群的维度，实现分布式应用的 build ship run。sealos 上同样可以管理多个自定义 kubernetes 集群，pgsql 数据库集群等，未来会以云原生的方式提供更多客户需要的应用，如对象存储，AI 能力等。&lt;/p></description></item><item><title>2025-1-1 创业点子</title><link>https://qq547475331.github.io/docs/2025-1-1-%E5%88%9B%E4%B8%9A%E7%82%B9%E5%AD%90/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2025-1-1-%E5%88%9B%E4%B8%9A%E7%82%B9%E5%AD%90/</guid><description>&lt;p>伟大的初创企业点子从何而来？&lt;/p>
&lt;p>伟大的初创企业点子从何而来？&lt;/p>
&lt;p>本文属于《菜鸟先飞》系列，是 YC 的常务董事 Michael Seibel 和合伙人 Dalton Caldwell 的对谈节目。在这个系列里，Michael 和 Dalton 会聊“菜鸟”创始人通常会犯的错误和遇到的困难。&lt;/p>
&lt;ol>
&lt;li>伟大的初创企业点子从何而来？&lt;/li>
&lt;li>创业早期怎么和用户交流？&lt;/li>
&lt;li>为什么早期产品不该加很多功能？&lt;/li>
&lt;/ol>
&lt;p>今天我们要谈的是——伟大的创业想法从何而来？我们挑选了 3 家成功的 YC 公司来分析这个问题。这 3 家公司是民宿网站 Airbnb、数字货币交易平台 Coinbase 和互联网支付处理平台 Stripe。在这 3 个案例中，我们看到一些共同答案。&lt;/p>
&lt;h1 id="关于爱彼迎">
 关于爱彼迎
 &lt;a class="anchor" href="#%e5%85%b3%e4%ba%8e%e7%88%b1%e5%bd%bc%e8%bf%8e">#&lt;/a>
&lt;/h1>
&lt;h2 id="时机很关键">
 时机很关键
 &lt;a class="anchor" href="#%e6%97%b6%e6%9c%ba%e5%be%88%e5%85%b3%e9%94%ae">#&lt;/a>
&lt;/h2>
&lt;p>第一，时机对这些公司都很重要。你可能会说，早就有一个巨头在主导着市场。但是，即使是相同的市场上，也总会有新的机会，驱使你去创造出比现有产品好很多的东西。大多数的创业者和投资人认为这些想法是可怕的。因为这个想法，要么太难执行，要么本来就很糟糕。我认为其中最酷的一点是，等创业者真正开始撸起袖子加油干了，这些创业想法都被证明远超预期的成功，往往比创业者开始想的更有前景。&lt;/p>
&lt;p>让我们来谈谈爱彼迎。我认为非常有趣的一件事是，当爱彼迎进入市场的时候，确实早就有类似的产品存在了，但在此之前，没有人想出要做类似的产品。&lt;/p>
&lt;p>如果我和我妈妈聊。她会说，哦，爱彼迎是一个伟大的创业想法，是那些创始人的异想天开。但很早的时候，我已经在使用 &lt;a href="https://weibo.com/ttarticle/p/show?id=2309404357417109463017">VRBO&lt;/a>。在爱彼迎被YC投资、进入市场之前，就有一个非常火爆的公司，叫做&lt;a href="https://baike.baidu.com/item/%e6%b2%99%e5%8f%91%e5%86%b2%e6%b5%aa/8165075?fr=aladdin">Couch Surfing&lt;/a>，这和爱彼迎很相似，他们有类似的创业想法和比较丰富的房屋资源。在当时，爱彼迎算是这个市场的第三位进入者。但使用 Couch Surfing 是如此痛苦，这也是我们和爱彼迎合作感到兴奋的原因之一。&lt;/p>
&lt;p>所以我认为，进入市场的时机非常重要，爱彼迎牢牢把握了核心。他们首先开始干，因为现有的产品不是很好，我认为谈论为什么现有的产品在哪方面不好是很有趣的话题。因为说实话，这很重要。我认为有 2 个基本的事实，可以说明当时市场上存在的产品不是很好。&lt;/p>
&lt;p>第一个问题是——VRBO 不太方便支付，当你和别人共享房屋或住在别人公寓的时候，你仍然不得不通过现金、支票或电汇的方式支付给他们。这会产生非常多的摩擦，因为两个素不相识的人不得不信任对方，来进行交易。&lt;/p>
&lt;p>第二个问题是——VRBO 在向房东收取费用。如果你考虑到这个市场的关键，房东就是其中最重要的部分。如果你能得到最多的房东，你就赢了，因为这样你就有最好的房源库存。但是，如果你在网站上向他们收费，你可能就做错了，所以时机是非常重要的。因为对爱彼迎来说，现有市场上的产品有10倍以上的改进空间，可以让他们来做。&lt;/p>
&lt;h2 id="创业的原因可能是需要解决自己的痛点">
 创业的原因可能是需要解决自己的痛点
 &lt;a class="anchor" href="#%e5%88%9b%e4%b8%9a%e7%9a%84%e5%8e%9f%e5%9b%a0%e5%8f%af%e8%83%bd%e6%98%af%e9%9c%80%e8%a6%81%e8%a7%a3%e5%86%b3%e8%87%aa%e5%b7%b1%e7%9a%84%e7%97%9b%e7%82%b9">#&lt;/a>
&lt;/h2>
&lt;p>我想说的第二件事是，他们正在解决自己的问题，这不是很有趣么？就像大多数人没有意识到，爱彼迎成立的最初原因，是 Joe 和 Nate 需要赚取租金。当时在旧金山有一个大型会议，他们基本上出租了他们的房间来接待别人，这样他们就可以赚到一些钱，这是很有趣的创业原因。我们总是把创业的原因归结为创始人个人。但是，像不能支付租金这个问题，这就像是一个真正痛点，而且这看起来不像是用来度假的房子，我以前在 VRBO 上看到的就是用来度假的。&lt;/p>
&lt;p>当时很有意思，很多人都意识到他们可以利用自己的房产来赚取额外的钱。这不是一个巧合，这是和08年金融危机同时发生的，很多人需要赚取更多的外快来挨过金融危机。他们突然意识到“哦，我现在有一个资产，我可以在上面赚点钱。&lt;/p>
&lt;p>爱彼迎在推出时，并没有意识到改善支付方式的重要性。Brian（爱彼迎创始人）最近在 YC 发表了演讲。他分享了一个故事——当他使用自己的产品时，他忘了带钱付款，房东就认为他是一个骗子。他意识到“哦，该死，我们的网站需要能让人们付款。”就像自己必须解决这个问题一样。所以这很有趣，虽然这些洞察现在看起来很明显。&lt;/p>
&lt;p>但是，在过去的日子里，你知道投资者都认为这是一个愚蠢的事情。你应该看到两个极端，一个是租房子，当有人在房子里租一个房间，主人也在那里一起住。这会产生非常多的问题，是没有人愿意去做的。比如，有陌生人住在我曼哈顿的公寓里。这虽然是一件挺重要的事情，但人们不应该担心这些事情。&lt;/p>
&lt;p>我对这个问题的看法是，人们都认为建立爱彼迎住房社区，并收钱是不好的。它是一个非常像公共的、基于集体的付出、去中心化的美好事物，因而商业化是非常重要的。&lt;/p>
&lt;p>我们所有的同行都认为，赚钱是不好的，商业化是不好的，他们应该做的是尽可能大，让一亿人在社区建立网络并收费，并进行收费的排序。但这是一个可怕的想法，大家先建立网络，然后再收费是不正确的。&lt;/p>
&lt;p>当将商业模式多元化之后，它开始接受用户的直接付款。但这在当时被认为是愚蠢的。所以很明显——如果他们听了大多数投资者的话。我就会说——好吧，Couch Surfing 是免费的。所以，你的计划是建立类似Couch Surfing 的东西。但是我们没有用户，而且刚刚开始，却要为它收钱。这听起来就像是一个自杀式的任务。&lt;/p></description></item><item><title>2025-1-1 创业者交流</title><link>https://qq547475331.github.io/docs/2025-1-1-%E5%88%9B%E4%B8%9A%E8%80%85%E4%BA%A4%E6%B5%81/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2025-1-1-%E5%88%9B%E4%B8%9A%E8%80%85%E4%BA%A4%E6%B5%81/</guid><description>&lt;p>创业早期怎么和用户交流？&lt;/p>
&lt;p>本文属于《菜鸟先飞》系列，是 YC 的常务董事 Michael Seibel 和合伙人 Dalton Caldwell 的对谈节目。在这个系列里，Michael 和 Dalton 会聊“菜鸟”创始人通常会犯的错误和遇到的困难。&lt;/p>
&lt;ol>
&lt;li>伟大的初创企业点子从何而来？&lt;/li>
&lt;li>创业早期怎么和用户交流？&lt;/li>
&lt;li>为什么早期产品不该加很多功能？&lt;/li>
&lt;/ol>
&lt;p>上一篇文章&lt;a href="https://miracleplus.feishu.cn/docx/doxcnBocE02CJym53IOoyxDH87e">《伟大的初创点子从何而来》&lt;/a>，他们举例了 Airbnb、Coinbase 和 Stripe 三家公司。这篇文章更细致地描绘了创业早期的场景：推出产品后，创业者该如何找用户并与他们交流。&lt;/p>
&lt;p>这是我们今天写在最前面的第一个失误：当你还在思考一个点子或产品的时候，尽可能多地拨打用户或潜在用户的电话，每天至少一次，越多越好。&lt;/p>
&lt;p>这看起来很理所当然，但这也是我们自己之前没能做到的。不要独自一人头脑风暴，因为你不知道人们想要什么。作为创始人，我们头脑中有大量关于“人们想要什么”的想法，但当想法暴露在阳光下之后，你会发现一切都错得令人尴尬。&lt;/p>
&lt;p>真用户使用了你花了很久创造出来的东西，最后却将它遗弃，或者根本就不会用它。这些情况很可怕，会让你感到很难堪，所以，和用户交谈就像是打预防针，避免你耗费大把时间做与用户毫无关系的事。&lt;/p>
&lt;h2 id="创业早期怎么做用户调研">
 创业早期怎么做用户调研？
 &lt;a class="anchor" href="#%e5%88%9b%e4%b8%9a%e6%97%a9%e6%9c%9f%e6%80%8e%e4%b9%88%e5%81%9a%e7%94%a8%e6%88%b7%e8%b0%83%e7%a0%94">#&lt;/a>
&lt;/h2>
&lt;h3 id="创业早期用户调查问卷可能无效">
 创业早期，用户调查问卷可能无效
 &lt;a class="anchor" href="#%e5%88%9b%e4%b8%9a%e6%97%a9%e6%9c%9f%e7%94%a8%e6%88%b7%e8%b0%83%e6%9f%a5%e9%97%ae%e5%8d%b7%e5%8f%af%e8%83%bd%e6%97%a0%e6%95%88">#&lt;/a>
&lt;/h3>
&lt;p>好，到该做调查问卷的时间了！——很多人认为，了解客户需求最有效的方式是调查他们。&lt;/p>
&lt;p>以下的说法可能会冒犯到一些人，&lt;strong>但我们不认为在创业想法阶段时用户调查有价值&lt;/strong>。调查的确有效，能帮我获得想要的反馈，但是，我们更应该把它看作一种微观层面的优化或是找方向时的尝试。&lt;/p>
&lt;p>举个例子，比如你有 4 个想法，现在你想从 4 个公司名称中选择一个最好的。这时候做个调查当然很好。然而，当你在寻找一些基本事实，或者试图找到第一个客户验证市场时，就必须要更加严格。&lt;/p>
&lt;ol>
&lt;li>用户调查不会帮助你获得付费用户&lt;/li>
&lt;/ol>
&lt;p>想让用户为你的产品付钱？这事儿很难，门槛可比让用户做问卷调查或真的安装上你的产品要高得多。想想你自己在填写问卷时，写过多少次“哦，这点子/产品听起来非常有用”？但你永远不会为那些产品付一分钱。&lt;/p>
&lt;ol>
&lt;li>无法触达最佳样本&lt;/li>
&lt;/ol>
&lt;p>你的潜在用户中有一小部分人拥有专业知识，尝试过不同的产品，因为这对他们来说确实是一个非常重要的需求。但如果你对每个人都一视同仁，在问卷中设想所有人都面临同样的问题，那么你就永远不会接触到那些拥有专业背景的最佳样本。&lt;/p>
&lt;p>&lt;strong>Airbnb 爱彼迎的例子&lt;/strong>&lt;/p>
&lt;p>和用户交流这方面，我看到最好的例子是早期的Airbnb。&lt;/p>
&lt;p>Airbnb 做了一件有趣的事情来吸引用户：在很早期，两位创始人会挨个儿发邮件给那些把自家房子放到网上出租的房东，推销免费的高清摄影服务。&lt;/p>
&lt;p>那时数码相机还并非广为人知，也没什么人知道 Airbnb。为了能让房主登陆 Airbnb，他们自己出去买了数码相机，自己去现场拍照片。&lt;/p>
&lt;p>直到今天，我仍然记得有他们有一个特别的客户。那天他们拍下了他房子的照片，对方很喜欢，于是问：“你们想坐下来喝点咖啡或者茶吗？”聊天的时候他说：“我其实已经在互联网各种平台上出租我的房子十年了，我有一本笔记，记录了他们过去十年做得不好和做得好的地方，你想看看吗？”&lt;/p>
&lt;p>如果你当时选择的是用电子邮件发问卷，那可能就不会遇到这样一个人。对方只会收到一封电子邮件的调查问卷，上面写着：“你喜欢Airbnb吗？你的建议会帮助我们变得更好。”他可能注册过了Airbnb，但没有发布任何一个房源，甚至没有再使用过 Airbnb。&lt;/p>
&lt;p>现实中，Airbnb 的创始人可能会从用户那里收集到像小山一样多的反馈。想象一下，你当时已经认识了 Airbnb 创始人，如果他们发问卷，调查人们是否愿意租出自己的房间，做了一些用户研究，也许进行了访谈，问受访者：“你想租出你的房子吗？”&lt;/p>
&lt;p>你就会知道，创始人从中可以学到的知识微乎其微，难以说服一个陌生人把房子租给另一个陌生人，为了让业务跑起来，他们也不会试着去了解房东出租过程中发生的一切。&lt;/p>
&lt;p>然而，他们能从摄影服务的例子中学到很多。相比之下，从一份调查报告中，他们只能拿到一点点有限的反馈——好吧，可能都没到“一点点”，甚至是负的。因为创业早期做用户调查问卷可以很危险。&lt;/p>
&lt;p>&lt;strong>Twitch 的例子&lt;/strong>&lt;/p>
&lt;p>除了 Airbnb，我们还有直播平台 Twitch 的故事：为什么团队决定创建一个用户需求的优先级列表。&lt;/p>
&lt;p>当我们从 Justin.TV 向 Twitch 转型的时候，埃米特·希尔（Emmett Shear）和凯文·林恩（kevin lynn）那时领导了团队，他们很重视这个。我们很多年来没有和用户交流过了，他们把这非常放在心上，开始基本上只是采访游戏直播者，并询问他们想要什么。&lt;/p></description></item><item><title>2025-1-1 初创公司</title><link>https://qq547475331.github.io/docs/2025-1-1-%E5%88%9D%E5%88%9B%E5%85%AC%E5%8F%B8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2025-1-1-%E5%88%9D%E5%88%9B%E5%85%AC%E5%8F%B8/</guid><description>&lt;p>&lt;strong>如何在初创公司中取得成功：关键要点总结&lt;/strong>&lt;/p>
&lt;ol>
&lt;li>&lt;strong>优秀产品是关键&lt;/strong>
&lt;ul>
&lt;li>产品必须如此优秀，以至于用户会自发地向朋友推荐。&lt;/li>
&lt;li>优秀产品的标志包括简单易用、满足大需求、且用户能轻松理解并表达产品价值。&lt;/li>
&lt;li>举例：谷歌和脸书的成功始于用户之间的口碑传播。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>选择增长中的市场&lt;/strong>
&lt;ul>
&lt;li>找到正在或即将经历指数增长的市场，而非仅关注当前的市场规模。&lt;/li>
&lt;li>识别真正的趋势（早期用户痴迷）与虚假趋势（表面火爆但用户使用率低）。&lt;/li>
&lt;li>示例：iPhone引领了移动计算的崛起，而虚拟现实在2018年仍属未来潜力领域。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>创始人的特质&lt;/strong>
&lt;ul>
&lt;li>需要一个热情传道者（通常是CEO），负责招聘、销售、筹资等。&lt;/li>
&lt;li>雄心勃勃的愿景吸引人才并增加团队凝聚力。&lt;/li>
&lt;li>创始人需保持动力，避免团队失去方向和势能。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>组建优秀的团队&lt;/strong>
&lt;ul>
&lt;li>招募乐观主义者和具有创造力的人才。&lt;/li>
&lt;li>优秀团队应具备“交给我了”的行动力和“我们总能解决”的精神。&lt;/li>
&lt;li>初创公司需快速决策、灵活调整，善于在不确定中行动并迅速适应。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>竞争优势与商业模式&lt;/strong>
&lt;ul>
&lt;li>随时间积累长期竞争优势（如网络效应、市场垄断）。&lt;/li>
&lt;li>至少拥有一个合理的商业模式，且明确如何获取用户并实现增长。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>初创公司的独特优势&lt;/strong>
&lt;ul>
&lt;li>初创公司能够快速验证看似“坏主意”的创新点子，避免大公司冗长的审批流程。&lt;/li>
&lt;li>在快速变化的市场中，敏捷性和决策速度成为重要优势。&lt;/li>
&lt;li>当大平台发生转型时，初创公司可迅速调整战略并抓住新机遇。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>文化与执行&lt;/strong>
&lt;ul>
&lt;li>提倡“节俭、专注、痴迷和爱”的创业文化。&lt;/li>
&lt;li>创业早期需要全力投入，保持强劲的工作节奏和动力。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>总结&lt;/strong>
&lt;ul>
&lt;li>成功的初创公司能够敏锐识别机会，创造出用户喜爱的产品，组建卓越团队，并在动态环境中快速行动。创业充满挑战，但灵活性和专注能够帮助初创公司战胜大公司并取得长远成功。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;h6 id="">
 
 &lt;a class="anchor" href="#">#&lt;/a>
&lt;/h6>
&lt;p>今天，我要谈谈如何在初创公司中取得成功。显然，20分钟内可以说得更多，但我会尽我所能。 最重要的是，我们试图教初创公司的第一课是，你成功的程度接近于你创造的产品的优秀程度，这个产品是如此优秀，以至于人们会自发地告诉他们的朋友。初创公司总是向我们询问成功的秘诀，他们总是希望相信秘诀是除此之外的什么事，这是因为创造好产品真的很难。如果你能创造一个非常优秀的产品，人们会自发地分享给身边的人，你已经完成了成为真正成功的创业公司所需工作的80%。 如果你想想那些最成功的公司，比如谷歌、脸书。你最初是怎么发现这些公司的？也许是因为你的一个朋友说：“你必须试试这个，它太棒了。”——这就是基准的水平线。人们非常喜欢的东西，他们会告诉他们的朋友。像这样的产品，一个重要指标是简单易懂。如果你不能用几句话解释它，你该怎么办？如果没有人说：“哦，这个产品很有趣。”——那这通常是一个错误的产品，意味着产品思路不清晰或需求不够大。 创业公司需要寻找的另一件事是一个市场。这个市场最好正在经历或者很快就会开始指数级增长。我认为，这实际上是投资者在评估创业公司时犯的最大错误之一。投资者总是问，你的增长率是多少？如果收入增长很快，投资者会原谅今天微薄的收入。但出于某种原因，人们不会这样看待市场。但如果你想想一些知名的创业公司，它们是在增长非常非常快的小市场开始的。 15年前（译者注：本文所指的是2007年之前），iPhone的市场绝对是0美元。它不是很大。我认为如果你只考虑今天可触达的最大市场（Total addressable market），你会犯一个大错。你真正想做的是找到一个每年都会增长的市场，并能够搭上电梯。为了弄清楚这一点，非常重要的是我们学习如何区分真正的趋势和虚假的趋势。真正的趋势是实际将要发生的事情，而虚假的趋势不会发生，或者至少还没有发生。在你对一个新平台下大赌注之前，你要确保它是真实可信的。现在，有一个简单的技巧：真正的趋势是这样的，一个新的技术平台出现了，早期用户痴迷于它，并告诉朋友他们有多喜欢它。虚假趋势是人们可能会购买产品，但不使用或至少使用得不够。 一个真实趋势的例子，就比如我已经提到的iPhone。当iPhone第一次出现时，许多人对此不屑一顾，因为那年他们只卖出了一百万或两百万台。他们说，好吧，这并不重要。但是对于那些有iPhone的人来说，他们每天使用它几个小时，它成为他们生活的核心。他们喜欢它，他们告诉朋友：你必须也买一台试试。我认为当时对关注的人来说，很明显有些事情已经发生了根本性的变化。我们有了一个新的计算平台，它将催生巨大的业务。这是押注移动应用程序的好时机。 一个虚假的趋势，或者至少是截至2018年8月的虚假趋势，我认为是虚拟现实。我确实相信虚拟现实有一天会很大，但是今天，我认识的大多数拥有虚拟现实头戴设备的人很少使用它。因此，尽管很多人谈论它，甚至可能很多人花钱购买，但我认为在你下大赌注之前，要认清一件事，这些设备早期用户的使用频率并不像你想看到的那样高。 创业公司需要的另一件事是，至少有一个热情传道的创始人——通常是首席执行官。创业公司中必须有人是要招聘、销售产品、与媒体交谈、筹集资金的人。这需要一个能让全世界充满热情地了解公司正在努力做什么的人，也就是一个成为公司首席布道者的人。没有这一点很难取得巨大成功。如果做不到这一点，就很难建立一个（好的）团队。有一个雄心勃勃的愿景对此很有帮助。你永远不想浮夸，这会让人反感。但是随着时间的推移，你想让自己变得更加雄心勃勃。如果你有慢慢结构性地拥有一个更大更有野心的愿景，人们会做出相应的回应。一个雄心勃勃的愿景令人兴奋，也会让工作更很有趣。 事实上，我认为在2018年，至少在硅谷，艰难地创业比简单地创业容易。这也许听起来很矛盾，但是雄心勃勃的项目都很有趣。在当前的环境下，筹资可能相对容易，但做其他事情真的很难。创业公司太多了，创办一个很容易。它们听起来都很有前途，以至于人们很难在其中一个创业团队中聚集足够多的人才。如果你正在研究或解决一个问题，你知道，可能会有一定成功的概率，让最初的几个人加入是很容易的。你可以给他们很多股权，但之后会变得很难。为什么第20号员工会加入？为什么你们解决的这个问题对世界很重要？为什么有人应该为你的初创公司工作，而不是去做其他的事情？ 如果你选对了那个重要的问题，就可以很好地解决招人的困难。所以我认为当你创办公司的时候，应该思考，怎么把愿景发展到很多人都参与帮助建设的地步。我认为在当前的环境下，获得人才、分享思想真的很难。但人们常会对有重要意义的初创企业感兴趣。 我们一次又一次注意到一件事，我们最好的创始人对未来有自信和明确的看法，但他们可能是错的。所以我们说，自信和灵活是好的，但是这种你自信的想法是无限期的，这就是“我认为将要发生的”，或者“这就是将要发生的”。保持相对自信，对自己的信念有勇气，成为一个明确的领导者，说，“我们要做这件事，这就是为什么”。 这又回到了“创始人有一个雄心勃勃的愿景”，但是整个创业生态系统倾向于支持那些成功率很低但一旦成功就会很大的公司。我认为如果成功的话，去做一些伟大的事情会吸引最优秀的人。 我不想过多谈论团队。很多显而易见的事情太多人已经说过很多次了，大家都想要找到善于沟通、努力工作并且聪明的人。但我想说一些不明显的事情，我们很少听到人们谈论你需要组建一个什么样的团队。维诺德·科斯拉（Vinod Khosla）说：“你建立的团队就是你建立的公司。”我认为这是真的。我见过的创始人中，我认为只有很少一部分花了足够多的时间在招聘上，比如马克·扎克伯格（Mark Zuckerberg）就是其中之一。除了选择正确的市场和构建一个好用的产品之外，建立一个好的团队是你需要做的最重要的事情。所有成功的创始人都经历了转变，在这个转变中，你从构建产品转向构建公司，构建公司真的是关于团队的。 所以，你需要乐观主义者。整个世界都会告诉你，为什么你这家创业公司将会失败，如果你没有那种内在的信念之火，如果你身边没有说对你说：“你知道吗，我们就要做这件事，不管那些不怀好意的人说什么，我们都会解决这个问题。”（但对于这个问题，它必须是可解决的。）如果你的团队中没有乐观精神，那当世界继续打你的脸时，你会很难成功。你还需要一些能不断创造新点子的人。在我合作过的成功公司里，总有少数人，他们非常擅长想出新的点子。你不需要太多这样的人，因为一家公司是无法执行下去太多新想法的。一家公司里有一些不断产生新想法的人，尽管其中大部分也许不好主意，但事实证明他们在团队中是非常重要。 “我们总能解决”的精神是我最喜欢在早期创业团队中听到的。很多事情都会出现问题，创业公司是能够在瞬息万变的局势中获胜或者取得优势的。所以中心思想是，即使我们的硬件指标看起来一般，即使我们以前没有解决过这个问题，即使感觉这个问题会搞垮这家公司（其实许多问题都会有这种感觉），这时候团队中的精神是：我知道我们拥有我们最需要的人，我们会解决这个问题，我们会完成这个任务。这个信念，非常重要。 另一句我喜欢从早期团队成员那里听到的话是“交给我了”。在大公司里，很多人会说，那不是我的部门应该负责的，是其他人的活儿。当情况很糟糕，你认为会伤害到你的创业公司时，你希望人们站出来说，“交给我了，我来做，不用担心。”你会希望人们更爱采取行动。初创公司，尤其在早期，通常可以通过快速行动来取胜。你永远不会得到你想要的那么多数据。你永远不会有你想要的那么多时间来思考。你希望人们会愿意在永远不够多的数据以及不确定性之中行动。如果他们行动了，但没有成效，他们应该很快适应并尝试其他的。 我们也可以谈谈缺乏经验的好处。许多我们的初创公司做过不可思议的事情，因为没有人告诉他们这很难，或者没有人告诉他们做不到。史蒂夫·瓦兹尼亚克有一句名言，他做过的所有最好的事情都是来自没有任何经验和钱。你知道，这显然并不总是正确的，但是初创公司总是会发生神奇的事情，尤其是在早期，因为他们还没有学会一个道理：能力不足时就不应该去做一些事。 当然，我也认为，作为一个创业公司，每个人都没有经验是行不通的，但你可以比平时更多地押注于没有经验的高潜力人才。这就是团队这个主题的最后一点。作为创始人，你最重要的工作之一就是永远不要失去动力。这有点令人沮丧，因为这意味着在最初的几年里，你永远不能放松，而是脚时刻都踩在油门上。你永远不会真正休息。我们诚实地讲，创业根本不是平衡工作和生活的最佳选择，尤其是在早期。创业公司都靠自己的动力和势能生存。如果你有动力，人们会不断突破他们自以为的极限、交出超水平的成果。如果你失去了动力，情况就很难再恢复。因此，创始人们，你们应该继续确保初创公司保持好的节奏，确保在相对较短和可预测的时间窗口内你的创业公司可以持续获胜。这非常重要，你不对工作失去掌控是取决于创始人的。 我们认为，初创公司需要具备的另一点能力是随着时间推移积累的竞争优势。现在这听起来很明显。我甚至犹豫着要不要讲，但这其实是一个很好的讨论。越来越多的初创公司申请YC时，我们问他们，你们的长期垄断效应是什么？你们的长期竞争优势？这个行业的网络效应在哪里？他们看着我们，好像这是他们第一次听到这个问题。我知道的所有真正伟大的企业都有这个问题的答案。事实上，他们做得越好，就越假装没有这样做。但这是你需要有计划去考虑的事情。 另一件你得有计划的事：至少一个合理的商业模式。你不必在一开始就想清楚。但是当我们问创始人，你将如何赚钱？他们看着我们，好像这是他们第一次被问到这个问题，最近这种事经常发生，可能比你们想象的次数还要多，这是一个坏信号。同样，当我们问一家初创公司，他们将如何发展，如何获得用户，他们看着我们，也好像第一次听到这个问题，这都是不好的信号。 这里是一些不错的想法你可以参考。保罗·布赫海特（Paul Buchheit，一位YC合伙人）曾经花了很多时间研究我们最好的创始人共同的特征，并试图提炼出来。他想出了“节俭”、“专注”、“痴迷”和“爱”。我认为这真的很好，因为我没有太多要补充的。但是，我想，当你作为一名创业者，应该有能力告诉大家自己到底在做什么事。 最后，我想谈谈为什么初创公司能够打败大公司。原因有很多，我将在这里谈谈几个常见的原因。我认为，当你评估创业想法时，发现一些足够有价值的趋势，值得去思考你是否适合做这些。因为大多数时候，初创公司很难打败大公司。这里有一些我们看到它反复发生的领域。我认为一个关键点在于，如果你是一家大公司的产品经理，你想做一些听起来像坏主意但却实际是个好主意的事情，你必须让所有人——从你的老板，有时一直到首席执行官——都同意，所以一个No就可能让你无法继续做下去。 如果你是一家初创公司，你可以去YC路演日。成千上万的投资者中的任何一个都可能说Yes，你可以尝试一下。所以这是一种非常不同的心态。对于那些听起来很糟，但能在路演日得到肯定的点子——对比一下投资人的Yes和大公司的No，初创公司赢面更大。事实上，初创公司通常会在这一类创新想法中击败大公司。所以，我们应该寻找听起来很糟的想法，它没准很不错，在路演日能得到一个肯定，那这个点子肯定比你在大公司得到所有肯定的点子来得好。 创业公司通常能在变化非常快的市场击败大公司。创业公司的巨大优势在于敏捷性和速度。市场变化越快，你需要做出的决策数量就越多，你可以对产品和战略进行的调整就越多。你想最大化地利用这一点，你的竞争对手必须做出相同的决策数量来与你竞争。因为大公司通常会做出比你更糟糕、更慢的决策，所以市场的快速演变会给你更多的机会积累你与大公司竞争的优势。 最后，初创公司通常会在大平台转型的潮流中获胜。继续用iPhone举个例子。在移动应用成为一种新趋势，许多那时候起步的新公司现在都非常有价值。其中一个原因是，大多数大公司至少每年都在以某种节奏工作。当平台发生巨大转变时，他们不擅长做出足够大的战略支点。大公司的“战舰”转得太慢，而初创公司可以说：“哇，今天早上醒来，世界与六个月前有了根本的不同。我们将全力以赴朝着这个新方向前进。”因此这是初创公司通常获胜的方式和领域。创业还有很多其他门道，但是我认为从调整方向上来说，初创公司占得了先机。 总结&lt;/p></description></item><item><title>2025-1-1 大堰河-我的保姆</title><link>https://qq547475331.github.io/docs/2025-1-1-%E5%A4%A7%E5%A0%B0%E6%B2%B3-%E6%88%91%E7%9A%84%E4%BF%9D%E5%A7%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2025-1-1-%E5%A4%A7%E5%A0%B0%E6%B2%B3-%E6%88%91%E7%9A%84%E4%BF%9D%E5%A7%86/</guid><description>&lt;h1 id="大堰河我的保姆">
 大堰河——我的保姆
 &lt;a class="anchor" href="#%e5%a4%a7%e5%a0%b0%e6%b2%b3%e6%88%91%e7%9a%84%e4%bf%9d%e5%a7%86">#&lt;/a>
&lt;/h1>
&lt;p>【作者】艾青 【朝代】现代&lt;/p>
&lt;p>大堰河，是我的保姆。
她的名字就是生她的村庄的名字，
她是童养媳，
大堰河，是我的保姆。
我是地主的儿子；
也是吃了大堰河的奶而长大了的
大堰河的儿子。
大堰河以养育我而养育她的家，
而我，是吃了你的奶而被养育了的，
大堰河啊，我的保姆。
大堰河，今天我看到雪使我想起了你：
你的被雪压着的草盖的坟墓，
你的关闭了的故居檐头的枯死的瓦菲，
你的被典押了的一丈平方的园地，
你的门前的长了青苔的石椅，
大堰河，今天我看到雪使我想起了你。
你用你厚大的手掌把我抱在怀里，抚摸我；
在你搭好了灶火之后，
在你拍去了围裙上的炭灰之后，
在你尝到饭已煮熟了之后，
在你把乌黑的酱碗放到乌黑的桌子上之后，
在你补好了儿子们的为山腰的荆棘扯破的衣服之后，
在你把小儿被柴刀砍伤了的手包好之后，
在你把夫儿们的衬衣上的虱子一颗颗地掐死之后，
在你拿起了今天的第一颗鸡蛋之后，
你用你厚大的手掌把我抱在怀里，抚摸我。
我是地主的儿子，
在我吃光了你大堰河的奶之后，
我被生我的父母领回到自己的家里。
啊，大堰河，你为什么要哭？
我做了生我的父母家里的新客了！
我摸着红漆雕花的家具，
我摸着父母的睡床上金色的花纹，
我呆呆地看着檐头的我不认得的“天伦叙乐”的匾，
我摸着新换上的衣服的丝的和贝壳的纽扣，
我看着母亲怀里的不熟识的妹妹，
我坐着油漆过的安了火钵的炕凳，
我吃着碾了三番的白米的饭，
但，我是这般忸怩（niǔní）不安！因为我
我做了生我的父母家里的新客了。
大堰河，为了生活，
在她流尽了她的乳汁之后，
她就开始用抱过我的两臂劳动了；
她含着笑，洗着我们的衣服，
她含着笑，提着菜篮到村边的结冰的池塘去，
她含着笑，切着冰屑悉索的萝卜，
她含着笑，用手掏着猪吃的麦糟，
她含着笑，扇着炖肉的炉子的火，
她含着笑，背了团箕到广场上去，
晒好那些大豆和小麦，
大堰河，为了生活，
在她流尽了她的乳液之后，
她就用抱过我的两臂，劳动了。
大堰河，深爱着她的乳儿；
在年节里，为了他，忙着切那冬米的糖，
为了他，常悄悄地走到村边的她的家里去，
为了他，走到她的身边叫一声“妈”，
大堰河，把他画的大红大绿的关云长
贴在灶边的墙上，
大堰河，会对她的邻居夸口赞美她的乳儿；
大堰河曾做了一个不能对人说的梦：
在梦里，她吃着她的乳儿的婚酒，
坐在辉煌的结彩的堂上，
而她的娇美的媳妇亲切的叫她“婆婆”
。&amp;hellip;..
大堰河，深爱着她的乳儿！
大堰河，在她的梦没有做醒的时候已死了。
她死时，乳儿不在她的旁侧，
她死时，平时打骂她的丈夫也为她流泪，
五个儿子，个个哭得很悲，
她死时，轻轻地呼着她的乳儿的名字，
大堰河，已死了，
她死时，乳儿不在她的旁侧。
大堰河，含泪的去了！
同着四十几年的人世生活的凌侮，
同着数不尽的奴隶的凄苦，
同着四块钱的棺材和几束稻草，
同着几尺长方的埋棺材的土地，
同着一手把的纸钱的灰，
大堰河，她含泪的去了。
这是大堰河所不知道的：
她的醉酒的丈夫已死去，
大儿做了土匪，
第二个死在炮火的烟里，
第三，第四，第五
在师傅和地主的叱骂声里过着日子。
而我，我是在写着给予这不公道的世界的咒语。
当我经了长长的漂泊回到故土时，
在山腰里，田野上，
兄弟们碰见时，是比六七年前更要亲密！
这，这是为你，静静地睡着的大堰河
所不知道的啊！
大堰河，今天，你的乳儿是在狱里，
写着一首呈给你的赞美诗，
呈给你黄土下紫色的灵魂，
呈给你拥抱过我的直伸着的手，
呈给你吻过我的唇，
呈给你泥黑的温柔的脸颜，
呈给你养育了我的乳房，
呈给你的儿子们，我的兄弟们，
呈给大地上一切的，
我的大堰河般的保姆和她们的儿子，
呈给爱我如爱她自己的儿子般的大堰河。
大堰河，
我是吃了你的奶而长大了的
你的儿子，
我敬你
爱你！
一九三三年一月十四日，雪&lt;/p></description></item><item><title>2025-1-1 早期模式</title><link>https://qq547475331.github.io/docs/2025-1-1-%E6%97%A9%E6%9C%9F%E6%A8%A1%E5%BC%8F/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2025-1-1-%E6%97%A9%E6%9C%9F%E6%A8%A1%E5%BC%8F/</guid><description>&lt;p>为什么早期产品不该加很多功能？&lt;/p>
&lt;p>本文属于《菜鸟先飞》系列，是 YC 的常务董事 Michael Seibel 和合伙人 Dalton Caldwell 的对谈节目。在这个系列里，Michael 和 Dalton 聊到了“菜鸟”创始人通常会犯的错误和遇到的困难。&lt;/p>
&lt;ol>
&lt;li>伟大的初创企业点子从何而来？&lt;/li>
&lt;li>创业早期怎么和用户交流？&lt;/li>
&lt;li>为什么早期产品不该加很多功能？&lt;/li>
&lt;/ol>
&lt;p>在第一篇文章&lt;a href="http://mp.weixin.qq.com/s?__biz=MjM5MzE3NzE1OA==&amp;amp;mid=2247502067&amp;amp;idx=1&amp;amp;sn=2c5d388773f68dba94f2034be9d460bd&amp;amp;chksm=a69983da91ee0acc3791c8fd54572a5d158f5730057252f9859a21f388e8fd7c0437c02cbdad&amp;amp;scene=21#wechat_redirect">《伟大的初创点子从何而来》&lt;/a>，他们举例了 Airbnb、Coinbase 和 Stripe 三家公司，第二篇文章&lt;a href="http://mp.weixin.qq.com/s?__biz=MjM5MzE3NzE1OA==&amp;amp;mid=2247502071&amp;amp;idx=1&amp;amp;sn=cd107db820591dfadd90fe5d53fd917d&amp;amp;chksm=a69983de91ee0ac84dc55889d24fc5fcc124d2439677d99efb1391f5cd758baeb9b6f1115412&amp;amp;scene=21#wechat_redirect">《早期创业者怎么和用户交流？》&lt;/a>更细致地描绘了早期创业者如何找用户交流。&lt;/p>
&lt;p>这一期里两位讨论了三个问题：&lt;/p>
&lt;p>\1. 早期产品该加很多功能吗？&lt;/p>
&lt;p>\2. 如何验证错误？&lt;/p>
&lt;p>\3. 如何看待众筹？&lt;/p>
&lt;h2 id="早期产品该加很多功能吗">
 早期产品该加很多功能吗？
 &lt;a class="anchor" href="#%e6%97%a9%e6%9c%9f%e4%ba%a7%e5%93%81%e8%af%a5%e5%8a%a0%e5%be%88%e5%a4%9a%e5%8a%9f%e8%83%bd%e5%90%97">#&lt;/a>
&lt;/h2>
&lt;p>想象两个产品——一个功能很多但都不好用，一个只有一个功能但好用。前者明显要糟糕得多。&lt;/p>
&lt;p>我们今天的第一个失误：“在我们公司，我们试图在确定一个使用场景是存在且可行之前，就在产品中容纳了多个使用场景。”&lt;/p>
&lt;p>这是非常常见的问题。我们称之为“煮沸海洋”（Boiling the ocean），这意味着，你正在尝试做一些基本上不可能、非常难的事情。你所有的直觉都建立在一个完整产品之上，而不是先让一个用户进入这个精心设计的计划当中。在获得第一个用户之前，你不应该急着就把产品全部建立出来。&lt;/p>
&lt;p>我想每个创始人创立公司时都在他们脑中撒了一个谎，那就是他们知道客户想要什么，也知道如何解决客户遇到的问题。&lt;/p>
&lt;p>事实上，在把产品真正放在用户面前之前，你做得越多，就越让谎言肆无忌惮。一旦你把在产品放在用户面前，你就可以真正了解到你的假设有多少是正确的，有多少只是一个让我开始创办这个公司的借口，但如果你想用它解决用户的问题，可能这是不怎么样的谎言。&lt;/p>
&lt;h3 id="谷歌病googlitis">
 &lt;strong>谷歌病（Googlitis）&lt;/strong>
 &lt;a class="anchor" href="#%e8%b0%b7%e6%ad%8c%e7%97%85googlitis">#&lt;/a>
&lt;/h3>
&lt;p>我发明了一个词“谷歌病”。人们看着今天的公司，却不了解他他们早期看起来什么样。他们只是比照谷歌和自己当前的产品，然后就觉得自己是下一个亚马逊，所以我们要加入亚马逊的一切功能。但实际上，亚马逊开始也就是一个很简单的网站，只卖书，其他什么事情都不做，它甚至当时没有做得很好，哈哈哈。&lt;/p>
&lt;h3 id="opensea-的例子">
 &lt;strong>Opensea 的例子&lt;/strong>
 &lt;a class="anchor" href="#opensea-%e7%9a%84%e4%be%8b%e5%ad%90">#&lt;/a>
&lt;/h3>
&lt;p>让我们来谈谈其他的，你提到 Opensea 是一个很好的例子。&lt;/p>
&lt;p>它一开始也非常简单，现在的 Opensea 已经成为时代潮流，真的很有名，如果你像一个普通人，你就会感觉到 Opensea 不是无缘无故突然产生的庞然大物。&lt;/p>
&lt;p>2018 年在 YC，Opensea 的创始人们开始得很艰难。他们改变了最初的想法，然后转向做 Opensea。他们实际上花了大约一到两年的时间，专注于开发一个简单的网站，它只允许你买卖 nft，只是仅仅能用的水平。&lt;/p>
&lt;p>如今，他们每天、每周都会发布大量功能，但还没有自己的数字钱包。今天你会注意到，如果你使用 Opensea，必须使用第三方钱包还，有其他很多东西他们没开发。&lt;/p>
&lt;p>然而，通过打磨和极度专注于简单的用途，他们做出了人们想要的东西，达成了网络效应，然后繁荣起来了。有一天，我们都醒来发现，Opensea 已经是世界上最热门的公司之一，价值数十亿美元，每个人都想投资。&lt;/p>
&lt;p>但别忘了，在他们默默无闻的两三年里，一直在努力做最简单的事：开发一个只做一件事的产品。后来这个网站唯一的功能是购买 nft 并转移给其他人。在他们添加真正复杂的功能之前，还有很多基础工作要做。&lt;/p>
&lt;p>就像这个故事一样，Opensea 的打法之所以奏效，是因为开始时他们把交易这件事变得很简单，现在才逐渐让产品变复杂。所以，如果有人想与他们竞争，或者也成立一家 nft 初创公司，他们应该先做简单基础的事情，不要试图一开始就模仿其他人做的功能。&lt;/p></description></item><item><title>2025-1-1 要不要创业</title><link>https://qq547475331.github.io/docs/2025-1-1-%E8%A6%81%E4%B8%8D%E8%A6%81%E5%88%9B%E4%B8%9A/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2025-1-1-%E8%A6%81%E4%B8%8D%E8%A6%81%E5%88%9B%E4%B8%9A/</guid><description>&lt;h3 id="一创业者最核心的一种特质韧性">
 一、创业者最核心的一种特质：韧性
 &lt;a class="anchor" href="#%e4%b8%80%e5%88%9b%e4%b8%9a%e8%80%85%e6%9c%80%e6%a0%b8%e5%bf%83%e7%9a%84%e4%b8%80%e7%a7%8d%e7%89%b9%e8%b4%a8%e9%9f%a7%e6%80%a7">#&lt;/a>
&lt;/h3>
&lt;h4 id="一韧性是创业者成功的关键">
 （一）韧性是创业者成功的关键
 &lt;a class="anchor" href="#%e4%b8%80%e9%9f%a7%e6%80%a7%e6%98%af%e5%88%9b%e4%b8%9a%e8%80%85%e6%88%90%e5%8a%9f%e7%9a%84%e5%85%b3%e9%94%ae">#&lt;/a>
&lt;/h4>
&lt;ol>
&lt;li>&lt;strong>创业者类型多样化&lt;/strong>
成功的创业者不仅限于才华横溢的程序员或充满魅力的产品天才。不同类型的人都能通过各自的优势取得成功。&lt;/li>
&lt;li>&lt;strong>韧性的重要性&lt;/strong>
创业初期面临的拒绝和挫折是对个人心理承受能力的巨大考验。韧性帮助创业者坚持下去，获得早期用户和市场认可。&lt;/li>
&lt;/ol>
&lt;h4 id="二如何判断自己是否具备韧性">
 （二）如何判断自己是否具备韧性
 &lt;a class="anchor" href="#%e4%ba%8c%e5%a6%82%e4%bd%95%e5%88%a4%e6%96%ad%e8%87%aa%e5%b7%b1%e6%98%af%e5%90%a6%e5%85%b7%e5%a4%87%e9%9f%a7%e6%80%a7">#&lt;/a>
&lt;/h4>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>自信与韧性并不等同&lt;/strong>
自信的创业者未必能应对创业中的长期挑战，而看似不自信的人可能拥有强大的韧性。&lt;/p>
&lt;p>&lt;strong>案例：Benchling 创始人 Sajith&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>虽初期表现内向、不擅长销售，但他克服重重困难，带领公司成为行业领先者。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>创业动机的灵活性&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>初期动机（如追求财富或好奇心）并不决定成功。&lt;/li>
&lt;li>真正重要的是对解决问题的兴趣和与团队合作的愉悦感。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>应对最坏情况&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>在创业前，评估失败后可能的代价与学习价值，确保自己心理和经济上能承受。&lt;/li>
&lt;li>失败的创业经历同样能提升职业竞争力，为未来打下基础。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;hr>
&lt;h3 id="二关于启动创业的两条建议">
 二、关于启动创业的两条建议
 &lt;a class="anchor" href="#%e4%ba%8c%e5%85%b3%e4%ba%8e%e5%90%af%e5%8a%a8%e5%88%9b%e4%b8%9a%e7%9a%84%e4%b8%a4%e6%9d%a1%e5%bb%ba%e8%ae%ae">#&lt;/a>
&lt;/h3>
&lt;h4 id="一将寻找创业想法和联合创始人结合起来">
 （一）将寻找创业想法和联合创始人结合起来
 &lt;a class="anchor" href="#%e4%b8%80%e5%b0%86%e5%af%bb%e6%89%be%e5%88%9b%e4%b8%9a%e6%83%b3%e6%b3%95%e5%92%8c%e8%81%94%e5%90%88%e5%88%9b%e5%a7%8b%e4%ba%ba%e7%bb%93%e5%90%88%e8%b5%b7%e6%9d%a5">#&lt;/a>
&lt;/h4>
&lt;ol>
&lt;li>&lt;strong>通过讨论激发创意&lt;/strong>
与志趣相投的人共同探讨问题和解决方案，能更高效地产生可行的创业点子。&lt;/li>
&lt;li>&lt;strong>选择环境和人脉&lt;/strong>
&lt;ul>
&lt;li>如果当前圈子无法提供支持，考虑加入初创公司工作，积累经验和潜在合作伙伴。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;h4 id="二通过副业项目开启创业之路">
 （二）通过副业项目开启创业之路
 &lt;a class="anchor" href="#%e4%ba%8c%e9%80%9a%e8%bf%87%e5%89%af%e4%b8%9a%e9%a1%b9%e7%9b%ae%e5%bc%80%e5%90%af%e5%88%9b%e4%b8%9a%e4%b9%8b%e8%b7%af">#&lt;/a>
&lt;/h4>
&lt;ol>
&lt;li>&lt;strong>从副业项目开始尝试&lt;/strong>
&lt;ul>
&lt;li>将点子转化为简单的初版产品，获取早期用户反馈并逐步优化。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>衡量项目潜力&lt;/strong>
&lt;ul>
&lt;li>优先关注少数用户的强烈热爱，而非普遍接受度。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>注重自身感受&lt;/strong>
&lt;ul>
&lt;li>判断是否享受创造过程，与日常工作对比看是否更有激情。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>评估合作关系&lt;/strong>
&lt;ul>
&lt;li>如果与合作者合作愉快且愿意共同创业，这是极为宝贵的创业起点。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;hr>
&lt;h3 id="总结">
 总结
 &lt;a class="anchor" href="#%e6%80%bb%e7%bb%93">#&lt;/a>
&lt;/h3>
&lt;p>创业是一条需要韧性、勇气和策略的旅程。通过正确评估自身特质、动机和环境，并通过实践验证想法，创业者能更好地适应挑战，最大化创业成功的可能性。&lt;/p>
&lt;h1 id="一创业者最核心的一种特质">
 &lt;strong>一、创业者最核心的一种特质&lt;/strong>
 &lt;a class="anchor" href="#%e4%b8%80%e5%88%9b%e4%b8%9a%e8%80%85%e6%9c%80%e6%a0%b8%e5%bf%83%e7%9a%84%e4%b8%80%e7%a7%8d%e7%89%b9%e8%b4%a8">#&lt;/a>
&lt;/h1>
&lt;h2 id="一韧性才是创业者最重要的特质">
 &lt;strong>（一）韧性才是创业者最重要的特质&lt;/strong>
 &lt;a class="anchor" href="#%e4%b8%80%e9%9f%a7%e6%80%a7%e6%89%8d%e6%98%af%e5%88%9b%e4%b8%9a%e8%80%85%e6%9c%80%e9%87%8d%e8%a6%81%e7%9a%84%e7%89%b9%e8%b4%a8">#&lt;/a>
&lt;/h2>
&lt;p>如果你对成功创业者的印象仅基于书籍或电影中对知名创始人的描绘，比如他应该是像《社交网络》中马克·扎克伯格那样，是一名无情而聪明的程序员，或者像许多电影中描绘的史蒂夫·乔布斯那样，是一个充满魅力的产品天才，那么你很可能会误认为，如果你不是这样的人，就很难成为一名成功的创业者。 现在，我不否认，一名才华横溢的程序员或有魅力的产品天才肯定会更容易成为一名成功的创始人。但是我可以说，在与许多成功的创始人合作之后，我发现在创业公司取得成功的人有很多种类型，他们拥有不同的优势，而不仅仅是那些刻板印象中的特点。&lt;/p>
&lt;p>作为一名投资者，我的工作是选择哪些人会成为出色的创始人。然而，即使在经过了15年的创业和投资之后，我对于谁会成为出色的创始人仍然感到惊讶。这意味着作为创始人的你更难确定：自己是否适合做一名创业者。&lt;/p>
&lt;p>你可能会认为，只要简单地看看某个人在学校或工作中表现得有多好，就可以初步预测将来他的创业公司的成功概率。这确实是我在2010年刚开始在 YC 工作并阅读 YC 创业营申请表时的想法。然而，在与更多的创始人合作之后，我意识到这些信号（某个人在学校或工作中表现）并没有我预期的那么重要。&lt;/p>
&lt;p>当你创业时，有时仅仅是为了说服一个用户去尝试你的产品，你都必须付出大量的心血、汗水甚至眼泪，更别说去说服前10个或100个用户了。你必须经历许多拒绝才能达到这个目标。而且因为这是你的创业公司，所有的拒绝都以一种非常个人化的方式影响着你。然而当你在某家公司工作时，被拒绝却并不会有这种感觉。创始人需要很强的韧性才能在初期的困难中坚持下来，获得早期用户。这是我认为创业公司创始人最重要的品质就是韧性（resilience）。具备韧性的人，适合创业，并值得去尝试创建一家公司。&lt;/p>
&lt;h2 id="二如何判断是否有足够的韧性">
 &lt;strong>（二）如何判断是否有足够的韧性&lt;/strong>
 &lt;a class="anchor" href="#%e4%ba%8c%e5%a6%82%e4%bd%95%e5%88%a4%e6%96%ad%e6%98%af%e5%90%a6%e6%9c%89%e8%b6%b3%e5%a4%9f%e7%9a%84%e9%9f%a7%e6%80%a7">#&lt;/a>
&lt;/h2>
&lt;p>如何知道自己是否具备足够的韧性去创业呢？ 1.自信的创业者不一定有韧性 当我刚开始在 YC 工作时，我认为自信可以成为衡量一个人是否有足够韧性的标准。比如在 YC 面试过程中，如果一个创始人表现得很自信，说话非常有说服力、充满能量，那么我们就会认为他很可能是有韧性的创业者。然而，随着我合作的创业者越来越多，我意识到这是错误的观点。 后来的事实证明，一个人最初可能显得非常自信，但当他们的创业公司遇到困难时却可能显得没有韧性。相反，一些看起来很安静、不自信的创业者反而是比较有韧性的创业者，他们的公司最后也成功上市。&lt;/p></description></item><item><title>2025-1-16 CSI剖析演进</title><link>https://qq547475331.github.io/docs/2024-3-4-k8s-csi%E5%89%96%E6%9E%90/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2024-3-4-k8s-csi%E5%89%96%E6%9E%90/</guid><description>&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041031863.png" alt="image-20240304103140771" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041031858.png" alt="image-20240304103156693" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041032808.png" alt="image-20240304103208726" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041032162.png" alt="image-20240304103222095" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041032246.png" alt="image-20240304103234176" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041032383.png" alt="image-20240304103247304" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041032280.png" alt="image-20240304103259103" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041033536.png" alt="image-20240304103317466" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041033181.png" alt="image-20240304103342109" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041034771.png" alt="image-20240304103402521" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041034936.png" alt="image-20240304103428654" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041035425.png" alt="image-20240304103525215" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041035946.png" alt="image-20240304103541858" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041036235.png" alt="image-20240304103600021" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041036393.png" alt="image-20240304103612299" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041036180.png" alt="image-20240304103636086" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041036722.png" alt="image-20240304103656479" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041037060.png" alt="image-20240304103712989" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041038383.png" alt="image-20240304103829153" />&lt;/p></description></item><item><title>2025-1-16 k8s常见故障指南</title><link>https://qq547475331.github.io/docs/2025-1-16-k8s%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E6%8C%87%E5%8D%97/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2025-1-16-k8s%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E6%8C%87%E5%8D%97/</guid><description>&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/image-20250116083047545.png" alt="image-20250116083047545" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/image-20250116083125611.png" alt="image-20250116083125611" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/image-20250116083148777.png" alt="image-20250116083148777" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/image-20250116083210683.png" alt="image-20250116083210683" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/image-20250116083253513.png" alt="image-20250116083253513" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/image-20250116083331901.png" alt="image-20250116083331901" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/image-20250116083400139.png" alt="image-20250116083400139" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/image-20250116083419191.png" alt="" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/image-20250116083501480.png" alt="image-20250116083501480" />&lt;/p>
&lt;pre tabindex="0">&lt;code>kubeadm reset
systemctl stop kubelet
systemctl stop docker
rm -rf /var/lib/cni/
rm -rf /var/lib/kubelet/*
rm -rf /etc/cni/
ifconfig cni0 down
ifconfig flannel.1 down
ifconfig docker0 down
ip link delete cni0
ip link delete flannel.1
systemctl start docker
systemctl start kubelet
把k8s-node1重新加入节点
&lt;/code>&lt;/pre>&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/image-20250116083543817.png" alt="image-20250116083543817" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/image-20250116083603689.png" alt="image-20250116083603689" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/image-20250116083622868.png" alt="image-20250116083622868" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/image-20250116083641120.png" alt="image-20250116083641120" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/image-20250116083702343.png" alt="image-20250116083702343" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/image-20250116083724784.png" alt="image-20250116083724784" />&lt;/p></description></item><item><title>2025-1-16 k8s流量链路剖析</title><link>https://qq547475331.github.io/docs/2024-3-4-k8s%E6%B5%81%E9%87%8F%E9%93%BE%E8%B7%AF%E5%89%96%E6%9E%90/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2024-3-4-k8s%E6%B5%81%E9%87%8F%E9%93%BE%E8%B7%AF%E5%89%96%E6%9E%90/</guid><description>&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041021558.png" alt="image-20240304102137134" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041021997.png" alt="image-20240304102150912" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041022459.png" alt="image-20240304102205296" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041022999.png" alt="image-20240304102225793" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041022638.png" alt="image-20240304102242571" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041022927.png" alt="image-20240304102254845" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041023817.png" alt="image-20240304102312740" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041023599.png" alt="image-20240304102335528" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041024648.png" alt="image-20240304102410438" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041024130.png" alt="image-20240304102426059" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041024501.png" alt="image-20240304102442427" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041025905.png" alt="image-20240304102514776" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041025454.png" alt="image-20240304102530257" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041025640.png" alt="image-20240304102543566" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041025682.png" alt="image-20240304102555618" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041027558.png" alt="image-20240304102704294" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041027144.png" alt="image-20240304102728769" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/picture/202403041027257.png" alt="image-20240304102755026" />&lt;/p></description></item><item><title>2025-2-11 面试2025-02-11</title><link>https://qq547475331.github.io/docs/2025-2-11-%E9%9D%A2%E8%AF%950211/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2025-2-11-%E9%9D%A2%E8%AF%950211/</guid><description>&lt;h2 id="504错误是啥">
 504错误是啥
 &lt;a class="anchor" href="#504%e9%94%99%e8%af%af%e6%98%af%e5%95%a5">#&lt;/a>
&lt;/h2>
&lt;p>504 错误（Gateway Timeout）是一个 HTTP 状态码，表示网关或代理服务器超时。让我详细解释一下：&lt;/p>
&lt;h3 id="主要原因">
 主要原因：
 &lt;a class="anchor" href="#%e4%b8%bb%e8%a6%81%e5%8e%9f%e5%9b%a0">#&lt;/a>
&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>服务器响应超时&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>上游服务器（如应用服务器）没有在预定时间内响应网关/代理服务器&lt;/li>
&lt;li>可能是因为服务器负载过高或处理请求时间过长&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>网络问题&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>网关与上游服务器之间的网络连接存在问题&lt;/li>
&lt;li>网络拥堵或延迟过高&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;h3 id="常见场景">
 常见场景：
 &lt;a class="anchor" href="#%e5%b8%b8%e8%a7%81%e5%9c%ba%e6%99%af">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>使用 Nginx 作为反向代理时，后端服务响应太慢&lt;/li>
&lt;li>数据库查询操作耗时过长&lt;/li>
&lt;li>服务器资源不足（CPU、内存等）&lt;/li>
&lt;li>网络带宽不足&lt;/li>
&lt;/ul>
&lt;h3 id="解决方案">
 解决方案：
 &lt;a class="anchor" href="#%e8%a7%a3%e5%86%b3%e6%96%b9%e6%a1%88">#&lt;/a>
&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>服务器端&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>增加超时设置时间&lt;/li>
&lt;li>优化后端代码性能&lt;/li>
&lt;li>增加服务器资源&lt;/li>
&lt;li>使用缓存减少处理时间&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>配置示例&lt;/strong>
如果使用 Nginx，可以调整超时设置：&lt;/p>
&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-nginx" data-lang="nginx">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">http&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">proxy_connect_timeout&lt;/span> &lt;span style="color:#e6db74">300s&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">proxy_send_timeout&lt;/span> &lt;span style="color:#e6db74">300s&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">proxy_read_timeout&lt;/span> &lt;span style="color:#e6db74">300s&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="3">
&lt;li>&lt;strong>客户端&lt;/strong>
&lt;ul>
&lt;li>实现请求重试机制&lt;/li>
&lt;li>添加适当的错误处理&lt;/li>
&lt;li>考虑使用断路器模式&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;p>504 错误通常是临时性的，如果持续发生，建议检查服务器日志并进行相应优化。&lt;/p>
&lt;h2 id="常见的状态码">
 常见的状态码
 &lt;a class="anchor" href="#%e5%b8%b8%e8%a7%81%e7%9a%84%e7%8a%b6%e6%80%81%e7%a0%81">#&lt;/a>
&lt;/h2>
&lt;p>让我为你介绍最常见的 HTTP 状态码：&lt;/p>
&lt;h3 id="2xx---成功响应">
 2XX - 成功响应
 &lt;a class="anchor" href="#2xx---%e6%88%90%e5%8a%9f%e5%93%8d%e5%ba%94">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>200 OK&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>最常见的成功状态码&lt;/li>
&lt;li>请求成功且正常返回结果&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>201 Created&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>创建资源成功&lt;/li>
&lt;li>通常在 POST 请求后返回&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>204 No Content&lt;/strong>&lt;/p></description></item><item><title>2025-2-12 面试0212</title><link>https://qq547475331.github.io/docs/2025-2-12-%E9%9D%A2%E8%AF%950212/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2025-2-12-%E9%9D%A2%E8%AF%950212/</guid><description>&lt;h2 id="如何做k8s资源的规划">
 如何做k8s资源的规划
 &lt;a class="anchor" href="#%e5%a6%82%e4%bd%95%e5%81%9ak8s%e8%b5%84%e6%ba%90%e7%9a%84%e8%a7%84%e5%88%92">#&lt;/a>
&lt;/h2>
&lt;p>在K8s中进行资源规划是一个重要的任务，我来详细解释一下关键点：&lt;/p>
&lt;h3 id="1-cpu和内存资源规划">
 1. CPU和内存资源规划
 &lt;a class="anchor" href="#1-cpu%e5%92%8c%e5%86%85%e5%ad%98%e8%b5%84%e6%ba%90%e8%a7%84%e5%88%92">#&lt;/a>
&lt;/h3>
&lt;h4 id="11-资源请求requests和限制limits">
 1.1 资源请求(requests)和限制(limits)
 &lt;a class="anchor" href="#11-%e8%b5%84%e6%ba%90%e8%af%b7%e6%b1%82requests%e5%92%8c%e9%99%90%e5%88%b6limits">#&lt;/a>
&lt;/h4>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml:deployment.yaml" data-lang="yaml:deployment.yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">containers&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">app&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">resources&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">requests&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">memory&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;256Mi&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">cpu&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;250m&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">limits&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">memory&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;512Mi&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">cpu&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;500m&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>&lt;strong>requests&lt;/strong>: 容器需要的最小资源量&lt;/li>
&lt;li>&lt;strong>limits&lt;/strong>: 容器能够使用的最大资源量&lt;/li>
&lt;li>CPU单位：1 CPU = 1000m（毫核）&lt;/li>
&lt;li>内存单位：Mi（兆字节）, Gi（吉字节）&lt;/li>
&lt;/ul>
&lt;h3 id="2-命名空间资源配额">
 2. 命名空间资源配额
 &lt;a class="anchor" href="#2-%e5%91%bd%e5%90%8d%e7%a9%ba%e9%97%b4%e8%b5%84%e6%ba%90%e9%85%8d%e9%a2%9d">#&lt;/a>
&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml:quota.yaml" data-lang="yaml:quota.yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">ResourceQuota&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">compute-quota&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">namespace&lt;/span>: &lt;span style="color:#ae81ff">your-namespace&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">hard&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">requests.cpu&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;4&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">requests.memory&lt;/span>: &lt;span style="color:#ae81ff">4Gi&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">limits.cpu&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;8&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">limits.memory&lt;/span>: &lt;span style="color:#ae81ff">8Gi&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">pods&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;10&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="3-资源规划的最佳实践">
 3. 资源规划的最佳实践
 &lt;a class="anchor" href="#3-%e8%b5%84%e6%ba%90%e8%a7%84%e5%88%92%e7%9a%84%e6%9c%80%e4%bd%b3%e5%ae%9e%e8%b7%b5">#&lt;/a>
&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>监控历史数据&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>使用Prometheus + Grafana监控应用实际资源使用情况&lt;/li>
&lt;li>观察峰值和平均值&lt;/li>
&lt;li>留出30-50%的buffer以应对突发流量&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>合理设置requests和limits&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>requests设置为应用正常运行需要的资源量&lt;/li>
&lt;li>limits设置为预期峰值的1.5-2倍&lt;/li>
&lt;li>避免limits设置过低导致OOM&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Pod水平自动扩缩容(HPA)&lt;/strong>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml:hpa.yaml" data-lang="yaml:hpa.yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">autoscaling/v2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">HorizontalPodAutoscaler&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">app-hpa&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">scaleTargetRef&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">apps/v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Deployment&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">your-app&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">minReplicas&lt;/span>: &lt;span style="color:#ae81ff">2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">maxReplicas&lt;/span>: &lt;span style="color:#ae81ff">10&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">metrics&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">type&lt;/span>: &lt;span style="color:#ae81ff">Resource&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">resource&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">cpu&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">target&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">type&lt;/span>: &lt;span style="color:#ae81ff">Utilization&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">averageUtilization&lt;/span>: &lt;span style="color:#ae81ff">70&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="4-节点资源规划">
 4. 节点资源规划
 &lt;a class="anchor" href="#4-%e8%8a%82%e7%82%b9%e8%b5%84%e6%ba%90%e8%a7%84%e5%88%92">#&lt;/a>
&lt;/h3>
&lt;ol>
&lt;li>&lt;strong>节点标签和污点&lt;/strong>&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml:node-label.yaml" data-lang="yaml:node-label.yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">kubectl label nodes node1 workload-type=cpu-intensive&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">kubectl taint nodes node1 dedicated=cpu-intensive:NoSchedule&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="2">
&lt;li>&lt;strong>节点亲和性&lt;/strong>&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml:pod-affinity.yaml" data-lang="yaml:pod-affinity.yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">affinity&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">nodeAffinity&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">requiredDuringSchedulingIgnoredDuringExecution&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">nodeSelectorTerms&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">matchExpressions&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">key&lt;/span>: &lt;span style="color:#ae81ff">workload-type&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">operator&lt;/span>: &lt;span style="color:#ae81ff">In&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">values&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">cpu-intensive&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="5-资源规划检查清单">
 5. 资源规划检查清单
 &lt;a class="anchor" href="#5-%e8%b5%84%e6%ba%90%e8%a7%84%e5%88%92%e6%a3%80%e6%9f%a5%e6%b8%85%e5%8d%95">#&lt;/a>
&lt;/h3>
&lt;ol>
&lt;li>&lt;strong>应用层面&lt;/strong>&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> 确定应用的基准资源需求&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> 设置合适的requests和limits&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> 配置HPA策略&lt;/li>
&lt;/ul>
&lt;ol start="2">
&lt;li>&lt;strong>集群层面&lt;/strong>&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> 设置namespace资源配额&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> 规划节点池和节点规格&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> 配置节点亲和性策略&lt;/li>
&lt;/ul>
&lt;ol start="3">
&lt;li>&lt;strong>监控和优化&lt;/strong>&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> 部署监控系统&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> 定期检查资源使用情况&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> 根据实际情况调整资源配置&lt;/li>
&lt;/ul>
&lt;h3 id="6-注意事项">
 6. 注意事项
 &lt;a class="anchor" href="#6-%e6%b3%a8%e6%84%8f%e4%ba%8b%e9%a1%b9">#&lt;/a>
&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>避免资源过度分配&lt;/strong>&lt;/p></description></item><item><title>2025-2-16 k8s题目</title><link>https://qq547475331.github.io/docs/2025-2-26-k8s%E7%9B%B8%E5%85%B3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2025-2-26-k8s%E7%9B%B8%E5%85%B3/</guid><description>&lt;h2 id="kubernetes">
 Kubernetes
 &lt;a class="anchor" href="#kubernetes">#&lt;/a>
&lt;/h2>
&lt;h2 id="谈谈你对k8s的理解">
 谈谈你对k8s的理解☆
 &lt;a class="anchor" href="#%e8%b0%88%e8%b0%88%e4%bd%a0%e5%af%b9k8s%e7%9a%84%e7%90%86%e8%a7%a3">#&lt;/a>
&lt;/h2>
&lt;p>Kubernetes（简称 K8s）是一个开源的容器编排平台，旨在自动化容器化应用程序的部署、扩展和管理。它最初由 Google 设计，并在2014年开源。K8s 提供了一个集中的平台来管理和运行容器化的应用程序，尤其适合大规模的分布式系统和微服务架构。&lt;/p>
&lt;p>从技术角度看，K8s 主要有以下几个核心组成部分：&lt;/p>
&lt;ol>
&lt;li>&lt;strong>集群（Cluster）&lt;/strong>： K8s 将多个节点组织成一个集群，集群中通常有两类节点：控制平面节点（Control Plane）和工作节点（Worker Node）。控制平面负责集群的管理与调度，而工作节点运行实际的应用程序。&lt;/li>
&lt;li>&lt;strong>Pod&lt;/strong>： Pod 是 Kubernetes 中最小的调度单位，通常包含一个或多个容器，这些容器共享同一个网络、存储和其他资源。Pod 是容器运行的基本单位。&lt;/li>
&lt;li>&lt;strong>Deployment&lt;/strong>： Deployment 用于管理和控制应用程序的副本和版本。它通过定义副本数来保证应用的高可用性，支持滚动更新、回滚等功能。&lt;/li>
&lt;li>&lt;strong>Service&lt;/strong>： Service 用于暴露应用程序的网络接口，并提供负载均衡。通过 Service，用户可以通过稳定的 IP 或 DNS 名称访问一组 Pod。&lt;/li>
&lt;li>&lt;strong>Ingress&lt;/strong>： Ingress 提供 HTTP 和 HTTPS 路由到集群内的服务，可以用于管理外部流量进入集群，支持负载均衡、SSL 终端等功能。&lt;/li>
&lt;li>&lt;strong>ConfigMap 和 Secret&lt;/strong>： ConfigMap 用于管理应用的配置数据，Secret 用于存储敏感数据（如密码、API 密钥等）。这些资源可以被容器在运行时动态加载。&lt;/li>
&lt;li>&lt;strong>Namespace&lt;/strong>： Namespace 提供了对资源的隔离，允许在同一个集群中多个团队或应用之间共享资源而不发生冲突。它相当于虚拟化的逻辑分区。&lt;/li>
&lt;li>&lt;strong>StatefulSet&lt;/strong>： StatefulSet 是一种控制器，用于管理有状态应用，如数据库等。与 Deployment 不同，StatefulSet 保证了 Pod 的稳定性和唯一性，适合需要持久存储和有序部署的应用。&lt;/li>
&lt;li>&lt;strong>Persistent Volume (PV) 和 Persistent Volume Claim (PVC)&lt;/strong>： PV 是集群内的存储资源，而 PVC 是用户对存储资源的请求。通过这种方式，K8s 支持动态存储的创建和销毁。&lt;/li>
&lt;/ol>
&lt;h3 id="关键优势">
 关键优势：
 &lt;a class="anchor" href="#%e5%85%b3%e9%94%ae%e4%bc%98%e5%8a%bf">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>自动化&lt;/strong>：K8s 提供了自动化部署、滚动更新、扩展和恢复功能，极大地简化了应用程序生命周期管理。&lt;/li>
&lt;li>&lt;strong>高可用性&lt;/strong>：K8s 支持容错和高可用性，可以通过多副本部署、健康检查、自动恢复等机制保证应用的可靠性。&lt;/li>
&lt;li>&lt;strong>灵活性&lt;/strong>：通过声明式配置，K8s 让开发者可以专注于应用本身，而不必担心基础设施细节。&lt;/li>
&lt;li>&lt;strong>扩展性&lt;/strong>：K8s 提供了强大的插件和扩展机制，支持各种中间件、监控、日志等功能的集成。&lt;/li>
&lt;/ul>
&lt;h3 id="适用场景">
 适用场景：
 &lt;a class="anchor" href="#%e9%80%82%e7%94%a8%e5%9c%ba%e6%99%af">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>微服务架构&lt;/strong>：K8s 非常适合容器化微服务的管理，能有效地处理服务之间的依赖、负载均衡、容错等。&lt;/li>
&lt;li>&lt;strong>大规模分布式应用&lt;/strong>：由于其自动化和扩展性，K8s 很适合处理大规模、复杂的分布式系统。&lt;/li>
&lt;li>&lt;strong>混合云/多云架构&lt;/strong>：K8s 提供跨平台的支持，使得应用能够在不同的云环境或本地数据中心之间无缝迁移。&lt;/li>
&lt;/ul>
&lt;p>总的来说，Kubernetes 是现代 DevOps 和持续集成/持续交付（CI/CD）的核心技术之一，能够有效地管理容器化应用，使得开发和运维更加高效、灵活和可靠。&lt;/p></description></item><item><title>2025-2-18 面试2025-0218</title><link>https://qq547475331.github.io/docs/2025-2-18-%E9%9D%A2%E8%AF%95/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2025-2-18-%E9%9D%A2%E8%AF%95/</guid><description>&lt;h2 id="pod的request和limit的关系-以及他们在node节点上的pod被驱逐时的优先级">
 pod的request和limit的关系 以及他们在node节点上的pod被驱逐时的优先级
 &lt;a class="anchor" href="#pod%e7%9a%84request%e5%92%8climit%e7%9a%84%e5%85%b3%e7%b3%bb-%e4%bb%a5%e5%8f%8a%e4%bb%96%e4%bb%ac%e5%9c%a8node%e8%8a%82%e7%82%b9%e4%b8%8a%e7%9a%84pod%e8%a2%ab%e9%a9%b1%e9%80%90%e6%97%b6%e7%9a%84%e4%bc%98%e5%85%88%e7%ba%a7">#&lt;/a>
&lt;/h2>
&lt;p>在 Kubernetes 中，&lt;code>request&lt;/code> 和 &lt;code>limit&lt;/code> 是资源调度的关键参数，分别表示容器资源的最小需求和最大使用限制。它们的关系和在节点上的 Pod 被驱逐时的优先级可以分为以下几个方面：&lt;/p>
&lt;h3 id="1-request">
 1. &lt;strong>&lt;code>request&lt;/code> 和 &lt;code>limit&lt;/code> 的关系&lt;/strong>
 &lt;a class="anchor" href="#1-request">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>&lt;code>request&lt;/code>&lt;/strong>：
&lt;ul>
&lt;li>&lt;code>request&lt;/code> 是容器启动时所需的资源量。Kubernetes 调度器根据 &lt;code>request&lt;/code> 来选择合适的节点调度 Pod。当你设置了 &lt;code>request&lt;/code>，Kubernetes 确保该容器至少能使用这么多资源。若没有足够的资源，Pod 将无法被调度到该节点。&lt;/li>
&lt;li>它反映了容器的“需求量”，用于确定调度时是否能找到一个合适的节点。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>&lt;code>limit&lt;/code>&lt;/strong>：
&lt;ul>
&lt;li>&lt;code>limit&lt;/code> 是容器可以使用的最大资源量。容器在运行时使用的资源如果超过 &lt;code>limit&lt;/code>，Kubernetes 会根据策略（如 OOM Killer）限制容器的资源使用。&lt;/li>
&lt;li>它是对容器资源使用的“上限”，防止容器占用过多的资源，影响同一节点上的其他容器。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>&lt;code>request&lt;/code> 和 &lt;code>limit&lt;/code> 的差异&lt;/strong>：
&lt;ul>
&lt;li>&lt;code>request&lt;/code> 用于调度，而 &lt;code>limit&lt;/code> 用于资源使用控制。理想情况下，&lt;code>request&lt;/code> 应该小于或等于 &lt;code>limit&lt;/code>，否则会造成资源浪费或调度错误。&lt;/li>
&lt;li>例如：如果 &lt;code>request&lt;/code> 设置为 512Mi，而 &lt;code>limit&lt;/code> 设置为 1Gi，Pod 将被调度到有足够 512Mi 内存的节点，但在节点上运行时，该容器最多可使用 1Gi 内存。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="2-node-节点上-pod-被驱逐时的优先级">
 2. &lt;strong>Node 节点上 Pod 被驱逐时的优先级&lt;/strong>
 &lt;a class="anchor" href="#2-node-%e8%8a%82%e7%82%b9%e4%b8%8a-pod-%e8%a2%ab%e9%a9%b1%e9%80%90%e6%97%b6%e7%9a%84%e4%bc%98%e5%85%88%e7%ba%a7">#&lt;/a>
&lt;/h3>
&lt;p>当 Kubernetes 节点资源不足时（如内存或 CPU），Pod 可能会被驱逐。Pod 驱逐的决策基于以下几个因素：&lt;/p></description></item><item><title>2025-2-19 面试0219</title><link>https://qq547475331.github.io/docs/2025-2-19-%E9%9D%A2%E8%AF%950219/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2025-2-19-%E9%9D%A2%E8%AF%950219/</guid><description>&lt;h2 id="排查cpu使用率过高-linux">
 排查cpu使用率过高 linux
 &lt;a class="anchor" href="#%e6%8e%92%e6%9f%a5cpu%e4%bd%bf%e7%94%a8%e7%8e%87%e8%bf%87%e9%ab%98-linux">#&lt;/a>
&lt;/h2>
&lt;p>排查 CPU 使用率过高时，通常可以从以下几个方面入手：&lt;/p>
&lt;h3 id="1-查看当前-cpu-使用情况">
 1. &lt;strong>查看当前 CPU 使用情况&lt;/strong>
 &lt;a class="anchor" href="#1-%e6%9f%a5%e7%9c%8b%e5%bd%93%e5%89%8d-cpu-%e4%bd%bf%e7%94%a8%e6%83%85%e5%86%b5">#&lt;/a>
&lt;/h3>
&lt;p>使用 &lt;code>top&lt;/code> 或 &lt;code>htop&lt;/code> 命令查看系统当前的 CPU 使用情况，找到占用 CPU 资源较高的进程。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>top
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>或者如果你有 &lt;code>htop&lt;/code>，可以提供更直观的界面：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>htop
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>这些工具可以显示进程的 CPU 使用率，并且帮助你找到哪个进程占用了过多的 CPU。&lt;/p>
&lt;h3 id="2-使用">
 2. &lt;strong>使用 &lt;code>ps&lt;/code> 查看具体进程&lt;/strong>
 &lt;a class="anchor" href="#2-%e4%bd%bf%e7%94%a8">#&lt;/a>
&lt;/h3>
&lt;p>使用 &lt;code>ps&lt;/code> 命令来列出所有进程并按 CPU 使用率排序：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>ps aux --sort&lt;span style="color:#f92672">=&lt;/span>-%cpu | head -n &lt;span style="color:#ae81ff">10&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>这条命令会列出 CPU 使用率最高的前 10 个进程。&lt;/p>
&lt;h3 id="3-检查系统负载">
 3. &lt;strong>检查系统负载&lt;/strong>
 &lt;a class="anchor" href="#3-%e6%a3%80%e6%9f%a5%e7%b3%bb%e7%bb%9f%e8%b4%9f%e8%bd%bd">#&lt;/a>
&lt;/h3>
&lt;p>使用 &lt;code>uptime&lt;/code> 或 &lt;code>top&lt;/code> 可以查看系统的负载情况。如果负载过高，可能是 CPU 资源被占用过多，导致系统过载。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>uptime
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>或者&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>top
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>系统负载（load average）越高，表示系统的负载越重。通常负载值大于 CPU 核数时，表示系统处于过载状态。&lt;/p></description></item><item><title>2025-2-20 面试0220</title><link>https://qq547475331.github.io/docs/2025-2-20-%E9%9D%A2%E8%AF%950220/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2025-2-20-%E9%9D%A2%E8%AF%950220/</guid><description>&lt;h2 id="delete-pvc时-设置什么字段可以保证pv不被删除">
 delete pvc时 设置什么字段可以保证pv不被删除
 &lt;a class="anchor" href="#delete-pvc%e6%97%b6-%e8%ae%be%e7%bd%ae%e4%bb%80%e4%b9%88%e5%ad%97%e6%ae%b5%e5%8f%af%e4%bb%a5%e4%bf%9d%e8%af%81pv%e4%b8%8d%e8%a2%ab%e5%88%a0%e9%99%a4">#&lt;/a>
&lt;/h2>
&lt;p>在 Kubernetes 中，当你删除 &lt;strong>PersistentVolumeClaim（PVC）&lt;/strong> 时，默认情况下与之关联的 &lt;strong>PersistentVolume（PV）&lt;/strong> 会根据 PV 的 &lt;strong>回收策略&lt;/strong> 被删除。为了确保在删除 PVC 时 &lt;strong>PV 不被删除&lt;/strong>，你需要修改 PV 的 &lt;strong>回收策略（reclaimPolicy）&lt;/strong> 字段。&lt;/p>
&lt;h3 id="步骤">
 步骤
 &lt;a class="anchor" href="#%e6%ad%a5%e9%aa%a4">#&lt;/a>
&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>设置 PV 的回收策略&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>回收策略&lt;/p>
&lt;p>：Kubernetes 支持几种 PV 的回收策略，常见的有：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Retain&lt;/strong>：保持 PV 不删除，手动处理。&lt;/li>
&lt;li>&lt;strong>Recycle&lt;/strong>：回收 PV（已废弃，不推荐使用）。&lt;/li>
&lt;li>&lt;strong>Delete&lt;/strong>：删除 PV（这是默认设置，当 PVC 删除时 PV 会被删除）。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>如果你希望 PVC 删除时 &lt;strong>PV 不被删除&lt;/strong>，需要将 PV 的回收策略设置为 &lt;code>Retain&lt;/code>。这样，当 PVC 删除时，PV 仍然会保留，必须手动清理或者重新配置。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>如何修改回收策略&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>编辑你的 &lt;strong>PersistentVolume（PV）&lt;/strong> 对象，修改 &lt;code>reclaimPolicy&lt;/code> 为 &lt;code>Retain&lt;/code>。&lt;/li>
&lt;/ul>
&lt;p>下面是一个例子：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">PersistentVolume&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">my-pv&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">capacity&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">storage&lt;/span>: &lt;span style="color:#ae81ff">10Gi&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">volumeMode&lt;/span>: &lt;span style="color:#ae81ff">Filesystem&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">accessModes&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">ReadWriteOnce&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">persistentVolumeReclaimPolicy&lt;/span>: &lt;span style="color:#ae81ff">Retain &lt;/span> &lt;span style="color:#75715e"># 设置回收策略为 Retain&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">storageClassName&lt;/span>: &lt;span style="color:#ae81ff">manual&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">hostPath&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">path&lt;/span>: &lt;span style="color:#ae81ff">/mnt/data&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>在这个例子中，&lt;code>persistentVolumeReclaimPolicy: Retain&lt;/code> 确保当与该 PV 关联的 PVC 被删除时，PV 并不会被删除。&lt;/p></description></item><item><title>2025-2-24 0224面试</title><link>https://qq547475331.github.io/docs/2025-2-24-%E9%9D%A2%E8%AF%950224/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2025-2-24-%E9%9D%A2%E8%AF%950224/</guid><description>&lt;h1 id="比较好的监控是怎样的-prometheus-grafana-alertmanager-考虑存储-大规模-2000台机器-考虑告警延迟-考虑长期储存-考虑监控业务的全面性-准确性-告警实时性">
 比较好的监控是怎样的 prometheus grafana alertmanager 考虑存储 大规模 2000台机器 考虑告警延迟 考虑长期储存 考虑监控业务的全面性 准确性 告警实时性
 &lt;a class="anchor" href="#%e6%af%94%e8%be%83%e5%a5%bd%e7%9a%84%e7%9b%91%e6%8e%a7%e6%98%af%e6%80%8e%e6%a0%b7%e7%9a%84-prometheus-grafana-alertmanager-%e8%80%83%e8%99%91%e5%ad%98%e5%82%a8-%e5%a4%a7%e8%a7%84%e6%a8%a1-2000%e5%8f%b0%e6%9c%ba%e5%99%a8-%e8%80%83%e8%99%91%e5%91%8a%e8%ad%a6%e5%bb%b6%e8%bf%9f-%e8%80%83%e8%99%91%e9%95%bf%e6%9c%9f%e5%82%a8%e5%ad%98-%e8%80%83%e8%99%91%e7%9b%91%e6%8e%a7%e4%b8%9a%e5%8a%a1%e7%9a%84%e5%85%a8%e9%9d%a2%e6%80%a7-%e5%87%86%e7%a1%ae%e6%80%a7-%e5%91%8a%e8%ad%a6%e5%ae%9e%e6%97%b6%e6%80%a7">#&lt;/a>
&lt;/h1>
&lt;p>在你的场景中，涉及到大规模的监控部署，2000台机器、长时间的存储、实时告警等多个方面。要构建一个高效、可靠的监控系统，你可以基于 &lt;strong>Prometheus&lt;/strong>、&lt;strong>Grafana&lt;/strong> 和 &lt;strong>Alertmanager&lt;/strong> 这些工具进行设计，但需要优化配置，以满足大规模、低延迟、高可用、长时间存储等需求。&lt;/p>
&lt;h3 id="1-prometheus-架构设计">
 1. &lt;strong>Prometheus 架构设计&lt;/strong>
 &lt;a class="anchor" href="#1-prometheus-%e6%9e%b6%e6%9e%84%e8%ae%be%e8%ae%a1">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>水平扩展&lt;/strong>：单个 Prometheus 实例无法支撑大规模的数据采集和存储，因此需要考虑&lt;strong>水平扩展&lt;/strong>。可以使用多个 Prometheus 实例分担不同的监控目标（例如，按区域或按机器类型分配 Prometheus 实例）。也可以使用 &lt;strong>Prometheus Federation&lt;/strong>（联邦）来将多个 Prometheus 实例的指标数据汇聚到一个主 Prometheus 实例中。&lt;/li>
&lt;li>&lt;strong>存储优化&lt;/strong>：Prometheus 本身的存储是针对短期数据存储优化的，支持时序数据的压缩。但对于长期存储，Prometheus 并不适合将数据保留很长时间。可以使用 &lt;strong>Thanos&lt;/strong> 或 &lt;strong>Cortex&lt;/strong> 来为 Prometheus 提供分布式存储，支持长期数据保留。这样可以跨多个 Prometheus 实例集中存储数据，进行长时间查询。&lt;/li>
&lt;li>&lt;strong>数据分片&lt;/strong>：如果你的监控数据量非常大（如 2000 台机器），你可以通过数据分片（sharding）来分散负载，减少单个 Prometheus 实例的压力。Thanos 和 Cortex 都支持这种分片模型。&lt;/li>
&lt;/ul>
&lt;h3 id="2-grafana-可视化">
 2. &lt;strong>Grafana 可视化&lt;/strong>
 &lt;a class="anchor" href="#2-grafana-%e5%8f%af%e8%a7%86%e5%8c%96">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>仪表盘设计&lt;/strong>：Grafana 适用于展示大规模监控数据。你可以设计多个针对不同层级的仪表盘，例如：硬件监控、应用性能监控、网络流量等。Grafana 允许将数据按需聚合或细分，以帮助从不同维度监控业务指标。&lt;/li>
&lt;li>&lt;strong>分布式查询&lt;/strong>：通过 &lt;strong>Thanos&lt;/strong> 或 &lt;strong>Cortex&lt;/strong> 等分布式存储系统，Grafana 可以访问跨多个 Prometheus 实例的数据，呈现完整的监控视图。&lt;/li>
&lt;li>&lt;strong>查询性能优化&lt;/strong>：随着监控系统规模的扩大，Grafana 查询的数据量增大。你需要优化 Prometheus 的查询性能，避免低效的查询（如不加时间范围的查询），或者使用预计算的聚合数据来减轻查询负担。&lt;/li>
&lt;/ul>
&lt;h3 id="3-alertmanager-告警管理">
 3. &lt;strong>Alertmanager 告警管理&lt;/strong>
 &lt;a class="anchor" href="#3-alertmanager-%e5%91%8a%e8%ad%a6%e7%ae%a1%e7%90%86">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>告警延迟&lt;/strong>：为了确保告警的实时性，Prometheus 的抓取周期（默认 15 秒）需要优化。如果告警的延迟非常关键，可能需要缩短抓取周期或者增加更多 Prometheus 实例，分担告警流量。&lt;/li>
&lt;li>&lt;strong>告警策略设计&lt;/strong>：针对大规模的监控系统，告警的&lt;strong>去重和抑制&lt;/strong>非常重要，避免过多无效告警。可以根据告警的严重性、关联性进行聚合，避免“告警风暴”。Alertmanager 可以进行告警的抑制和分组，减少重复告警。&lt;/li>
&lt;li>&lt;strong>通知渠道&lt;/strong>：Alertmanager 可以与 Slack、邮件、Webhook 等通知渠道集成，根据告警级别不同，选择不同的通知方式（例如，严重告警通过短信，轻微告警通过邮件）。&lt;/li>
&lt;li>&lt;strong>告警灵敏度&lt;/strong>：在大规模系统中，容易产生很多低优先级的告警。可以通过设置 &lt;strong>Prometheus&lt;/strong> 的告警规则，精细化告警条件，确保告警的准确性和有效性。&lt;/li>
&lt;/ul>
&lt;h3 id="4-存储与长期数据保留">
 4. &lt;strong>存储与长期数据保留&lt;/strong>
 &lt;a class="anchor" href="#4-%e5%ad%98%e5%82%a8%e4%b8%8e%e9%95%bf%e6%9c%9f%e6%95%b0%e6%8d%ae%e4%bf%9d%e7%95%99">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>长期存储解决方案&lt;/p></description></item><item><title>2025-2-24 中级运维面试题</title><link>https://qq547475331.github.io/docs/2025-2-24-%E4%B8%AD%E7%BA%A7%E8%BF%90%E7%BB%B4%E9%9D%A2%E8%AF%95%E9%A2%98_%E9%A2%98%E7%9B%AE/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2025-2-24-%E4%B8%AD%E7%BA%A7%E8%BF%90%E7%BB%B4%E9%9D%A2%E8%AF%95%E9%A2%98_%E9%A2%98%E7%9B%AE/</guid><description>&lt;h1 id="一linux">
 一、linux
 &lt;a class="anchor" href="#%e4%b8%80linux">#&lt;/a>
&lt;/h1>
&lt;h1 id="1-linux系统启动流程">
 1. linux系统启动流程
 &lt;a class="anchor" href="#1-linux%e7%b3%bb%e7%bb%9f%e5%90%af%e5%8a%a8%e6%b5%81%e7%a8%8b">#&lt;/a>
&lt;/h1>
&lt;ul>
&lt;li>第一步：开机自检，加载BIOS&lt;/li>
&lt;li>第二步：读取ＭＢＲ&lt;/li>
&lt;li>第三步：Boot Loader　grub引导菜单&lt;/li>
&lt;li>第四步：加载kernel内核&lt;/li>
&lt;li>第五步：init进程依据inittab文件夹来设定运行级别&lt;/li>
&lt;li>第六步：init进程执行rc.sysinit&lt;/li>
&lt;li>第七步：启动内核模块&lt;/li>
&lt;li>第八步：执行不同运行级别的脚本程序&lt;/li>
&lt;li>第九步：执行/etc/rc.d/rc.lo&lt;/li>
&lt;/ul>
&lt;p>Linux 系统启动流程是一个多阶段的过程，涉及多个重要的系统组件。以下是 Linux 系统从开机到完全启动的详细步骤：&lt;/p>
&lt;h3 id="1-加电自检-post">
 1. &lt;strong>加电自检 (POST)&lt;/strong>
 &lt;a class="anchor" href="#1-%e5%8a%a0%e7%94%b5%e8%87%aa%e6%a3%80-post">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>当电脑通电后，首先会执行硬件自检（POST，Power-On Self-Test），由 BIOS 或 UEFI（现代系统通常使用 UEFI）来完成。这一步检查硬件设备是否正常，比如内存、硬盘、显示器等。&lt;/li>
&lt;/ul>
&lt;h3 id="2-加载引导程序-bootloader">
 2. &lt;strong>加载引导程序 (Bootloader)&lt;/strong>
 &lt;a class="anchor" href="#2-%e5%8a%a0%e8%bd%bd%e5%bc%95%e5%af%bc%e7%a8%8b%e5%ba%8f-bootloader">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>BIOS 或 UEFI 会查找并加载存储设备（如硬盘或 SSD）上的引导加载程序（Bootloader）。常见的引导加载程序有 &lt;strong>GRUB&lt;/strong>（GRand Unified Bootloader）和 &lt;strong>LILO&lt;/strong>（Linux Loader）等。&lt;/li>
&lt;li>引导加载程序的作用是加载并启动操作系统。它会显示可用的操作系统列表（如果有多个系统），并允许用户选择要启动的操作系统。&lt;/li>
&lt;/ul>
&lt;h3 id="3-加载内核-kernel">
 3. &lt;strong>加载内核 (Kernel)&lt;/strong>
 &lt;a class="anchor" href="#3-%e5%8a%a0%e8%bd%bd%e5%86%85%e6%a0%b8-kernel">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>选择操作系统后，引导加载程序会加载操作系统的内核（通常是 &lt;code>vmlinuz&lt;/code> 文件）。内核是操作系统的核心，负责管理硬件资源和提供系统服务。&lt;/li>
&lt;li>内核首先会解压并加载到内存中，然后初始化硬件设备（如处理器、内存、硬盘等）。&lt;/li>
&lt;/ul>
&lt;h3 id="4-初始化硬件和挂载根文件系统-root-filesystem">
 4. &lt;strong>初始化硬件和挂载根文件系统 (Root Filesystem)&lt;/strong>
 &lt;a class="anchor" href="#4-%e5%88%9d%e5%a7%8b%e5%8c%96%e7%a1%ac%e4%bb%b6%e5%92%8c%e6%8c%82%e8%bd%bd%e6%a0%b9%e6%96%87%e4%bb%b6%e7%b3%bb%e7%bb%9f-root-filesystem">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>内核在加载完毕后，会开始初始化硬件，包括磁盘驱动程序、输入设备、网络设备等。&lt;/li>
&lt;li>内核通过 &lt;strong>initramfs&lt;/strong> 或 &lt;strong>initrd&lt;/strong>（初始 RAM 文件系统）来帮助加载根文件系统（通常是 &lt;code>/&lt;/code>）到内存中。initramfs 是一个压缩的文件系统镜像，包含必要的驱动程序和工具，确保系统能够挂载根文件系统。&lt;/li>
&lt;/ul>
&lt;h3 id="5-启动-init-进程">
 5. &lt;strong>启动 init 进程&lt;/strong>
 &lt;a class="anchor" href="#5-%e5%90%af%e5%8a%a8-init-%e8%bf%9b%e7%a8%8b">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>内核初始化完成后，会启动系统的第一个用户空间进程，即 &lt;strong>init&lt;/strong> 进程（PID 1）。这是 Linux 系统中最重要的进程，它负责启动和管理系统的其他进程。&lt;/li>
&lt;li>在现代 Linux 系统中，&lt;code>init&lt;/code> 进程通常由 &lt;strong>systemd&lt;/strong> 替代，systemd 是一种系统和服务管理器，负责管理系统启动过程、服务和进程。&lt;/li>
&lt;/ul>
&lt;h3 id="6-运行-init-脚本或服务管理器">
 6. &lt;strong>运行 init 脚本或服务管理器&lt;/strong>
 &lt;a class="anchor" href="#6-%e8%bf%90%e8%a1%8c-init-%e8%84%9a%e6%9c%ac%e6%88%96%e6%9c%8d%e5%8a%a1%e7%ae%a1%e7%90%86%e5%99%a8">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>systemd&lt;/p></description></item><item><title>2025-2-24 高级运维面试题-linux部分</title><link>https://qq547475331.github.io/docs/2025-2-24-%E9%AB%98%E7%BA%A7%E8%BF%90%E7%BB%B4%E9%9D%A2%E8%AF%95%E9%A2%98_ai_linux%E9%83%A8%E5%88%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2025-2-24-%E9%AB%98%E7%BA%A7%E8%BF%90%E7%BB%B4%E9%9D%A2%E8%AF%95%E9%A2%98_ai_linux%E9%83%A8%E5%88%86/</guid><description>&lt;blockquote>
&lt;p>经过本人为期一个半月的不懈努力，累计面试了二十多家公司，共计约五十余场面试，考察的面试题超两百道，积累了许多宝贵的面试经验。现在，我将这些面试题以及我个人的应对心得精心整理成一份面试攻略分享给大家，快来一起测测自己能回答多少道面试题吧。&lt;/p>&lt;/blockquote>
&lt;h1 id="面试基本信息">
 面试基本信息
 &lt;a class="anchor" href="#%e9%9d%a2%e8%af%95%e5%9f%ba%e6%9c%ac%e4%bf%a1%e6%81%af">#&lt;/a>
&lt;/h1>
&lt;p>面试岗位：运维工程师(容器与ES方向)、运维开发工程师、SRE工程师
工作经验：5年
薪资范围：年薪45万左右，base北京
面试时间：7月初-8月中旬
主要公司：&lt;/p>
&lt;ul>
&lt;li>互联网公司：字节、京东、百度、网易、蚂蚁金服、小米、滴滴、去哪儿、猎豹移动、商汤、旷视、智谱华章、马蜂窝、竞技世界&lt;/li>
&lt;li>国企子公司：电信、联通、建行、中石化&lt;/li>
&lt;/ul>
&lt;h1 id="面试题汇总">
 面试题汇总
 &lt;a class="anchor" href="#%e9%9d%a2%e8%af%95%e9%a2%98%e6%b1%87%e6%80%bb">#&lt;/a>
&lt;/h1>
&lt;p>其中☆表示多次出现过的高频面试题，已经按分类整理。&lt;/p>
&lt;h2 id="linux">
 Linux
 &lt;a class="anchor" href="#linux">#&lt;/a>
&lt;/h2>
&lt;h2 id="grep-sed-awk-cut组合使用">
 grep sed awk cut组合使用☆
 &lt;a class="anchor" href="#grep-sed-awk-cut%e7%bb%84%e5%90%88%e4%bd%bf%e7%94%a8">#&lt;/a>
&lt;/h2>
&lt;p>&lt;code>grep&lt;/code>、&lt;code>sed&lt;/code>、&lt;code>awk&lt;/code> 和 &lt;code>cut&lt;/code> 都是 Linux 系统中常用的文本处理工具，通常可以结合使用以实现更复杂的文本处理任务。这里是一些常见的组合使用示例：&lt;/p>
&lt;h3 id="1-使用-grep--cut筛选和提取字段">
 1. 使用 &lt;code>grep&lt;/code> + &lt;code>cut&lt;/code>：筛选和提取字段
 &lt;a class="anchor" href="#1-%e4%bd%bf%e7%94%a8-grep--cut%e7%ad%9b%e9%80%89%e5%92%8c%e6%8f%90%e5%8f%96%e5%ad%97%e6%ae%b5">#&lt;/a>
&lt;/h3>
&lt;p>假设你有一个文件 &lt;code>data.txt&lt;/code>，其内容如下：&lt;/p>
&lt;pre tabindex="0">&lt;code>Name, Age, Department
Alice, 30, HR
Bob, 25, Engineering
Charlie, 35, Marketing
&lt;/code>&lt;/pre>&lt;p>如果你只想提取 &lt;code>Name&lt;/code> 和 &lt;code>Age&lt;/code> 字段，可以使用 &lt;code>grep&lt;/code> 和 &lt;code>cut&lt;/code> 的组合：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>grep -v &lt;span style="color:#e6db74">&amp;#34;Name&amp;#34;&lt;/span> data.txt | cut -d &lt;span style="color:#e6db74">&amp;#39;,&amp;#39;&lt;/span> -f 1,2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>解释：&lt;/p>
&lt;ul>
&lt;li>&lt;code>grep -v &amp;quot;Name&amp;quot;&lt;/code>：去除第一行标题。&lt;/li>
&lt;li>&lt;code>cut -d ',' -f 1,2&lt;/code>：按照逗号分隔，提取第一列和第二列。&lt;/li>
&lt;/ul>
&lt;p>输出：&lt;/p></description></item><item><title>2025-2-25 面试0225</title><link>https://qq547475331.github.io/docs/2025-2-26-%E9%9D%A2%E8%AF%950225/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2025-2-26-%E9%9D%A2%E8%AF%950225/</guid><description>&lt;h2 id="上海外包太平洋保险">
 上海外包太平洋保险
 &lt;a class="anchor" href="#%e4%b8%8a%e6%b5%b7%e5%a4%96%e5%8c%85%e5%a4%aa%e5%b9%b3%e6%b4%8b%e4%bf%9d%e9%99%a9">#&lt;/a>
&lt;/h2>
&lt;h2 id="有完成过ansible比较复杂的role-task的编写吗">
 有完成过ansible比较复杂的role task的编写吗？
 &lt;a class="anchor" href="#%e6%9c%89%e5%ae%8c%e6%88%90%e8%bf%87ansible%e6%af%94%e8%be%83%e5%a4%8d%e6%9d%82%e7%9a%84role-task%e7%9a%84%e7%bc%96%e5%86%99%e5%90%97">#&lt;/a>
&lt;/h2>
&lt;p>是的，基于你的需求描述，我可以帮你了解和完成一些复杂的 Ansible Role 和 Task 编写。以下是一些示例场景：&lt;/p>
&lt;h3 id="1-完整的-kubernetes-集群节点加入-role-编写">
 &lt;strong>1. 完整的 Kubernetes 集群节点加入 Role 编写&lt;/strong>
 &lt;a class="anchor" href="#1-%e5%ae%8c%e6%95%b4%e7%9a%84-kubernetes-%e9%9b%86%e7%be%a4%e8%8a%82%e7%82%b9%e5%8a%a0%e5%85%a5-role-%e7%bc%96%e5%86%99">#&lt;/a>
&lt;/h3>
&lt;p>假设你需要使用 Ansible 创建一个完整的 Kubernetes 节点加入过程，包含以下内容：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>创建目录结构和配置文件&lt;/strong>&lt;/li>
&lt;li>&lt;strong>二进制文件下载和分发&lt;/strong>&lt;/li>
&lt;li>&lt;strong>&lt;code>systemd&lt;/code> 服务文件创建和启用&lt;/strong>&lt;/li>
&lt;li>&lt;strong>配置文件模板化（&lt;code>kubelet&lt;/code>，&lt;code>kube-proxy&lt;/code>，&lt;code>CNI&lt;/code> 插件等）&lt;/strong>&lt;/li>
&lt;li>&lt;strong>节点标签和调度禁用操作&lt;/strong>&lt;/li>
&lt;li>&lt;strong>&lt;code>kubelet&lt;/code>、&lt;code>kube-proxy&lt;/code> 等服务的启动管理&lt;/strong>&lt;/li>
&lt;/ul>
&lt;h3 id="2-ansible-role-示例">
 &lt;strong>2. Ansible Role 示例&lt;/strong>
 &lt;a class="anchor" href="#2-ansible-role-%e7%a4%ba%e4%be%8b">#&lt;/a>
&lt;/h3>
&lt;p>一个复杂的 Role 可能包含以下几个部分：&lt;/p>
&lt;h4 id="tasksmainyml">
 &lt;strong>&lt;code>tasks/main.yml&lt;/code>&lt;/strong>
 &lt;a class="anchor" href="#tasksmainyml">#&lt;/a>
&lt;/h4>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>---
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 安装并配置 Kubernetes 组件（如 kubelet 和 kube-proxy）&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">创建 Kubernetes 相关目录&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">file&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">path&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;{{ item }}&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">state&lt;/span>: &lt;span style="color:#ae81ff">directory&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">owner&lt;/span>: &lt;span style="color:#ae81ff">root&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">group&lt;/span>: &lt;span style="color:#ae81ff">root&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">mode&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;0755&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">with_items&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">/var/lib/kubelet&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">/var/lib/kube-proxy&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">/etc/cni/net.d&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">/opt/cni/bin&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">分发 kubelet 和 kube-proxy 二进制文件&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">copy&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">src&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;{{ base_dir }}/bin/{{ item }}&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">dest&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;{{ bin_dir }}/{{ item }}&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">mode&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;0755&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">with_items&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">kubectl&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">kubelet&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">kube-proxy&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">分发 CNI 插件&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">copy&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">src&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;{{ item }}&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">dest&lt;/span>: &lt;span style="color:#ae81ff">/opt/cni/bin/&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">mode&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;0755&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">with_fileglob&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#e6db74">&amp;#34;{{ base_dir }}/bin/cni-bin/*&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 创建 kubelet 和 kube-proxy 配置文件&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">创建 kubelet 配置文件&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">template&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">src&lt;/span>: &lt;span style="color:#ae81ff">kubelet-config.yaml.j2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">dest&lt;/span>: &lt;span style="color:#ae81ff">/var/lib/kubelet/config.yaml&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">tags&lt;/span>: &lt;span style="color:#ae81ff">configure_kubelet&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">创建 kube-proxy 配置文件&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">template&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">src&lt;/span>: &lt;span style="color:#ae81ff">kube-proxy-config.yaml.j2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">dest&lt;/span>: &lt;span style="color:#ae81ff">/var/lib/kube-proxy/kube-proxy-config.yaml&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">tags&lt;/span>: &lt;span style="color:#ae81ff">configure_kubeproxy&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 创建 kubelet 和 kube-proxy 的 systemd 服务&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">创建 kubelet systemd 服务文件&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">template&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">src&lt;/span>: &lt;span style="color:#ae81ff">kubelet.service.j2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">dest&lt;/span>: &lt;span style="color:#ae81ff">/etc/systemd/system/kubelet.service&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">tags&lt;/span>: &lt;span style="color:#ae81ff">manage_services&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">创建 kube-proxy systemd 服务文件&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">template&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">src&lt;/span>: &lt;span style="color:#ae81ff">kube-proxy.service.j2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">dest&lt;/span>: &lt;span style="color:#ae81ff">/etc/systemd/system/kube-proxy.service&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">tags&lt;/span>: &lt;span style="color:#ae81ff">manage_services&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 启用并启动服务&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">启用 kubelet 服务&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">systemd&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">kubelet&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">enabled&lt;/span>: &lt;span style="color:#66d9ef">yes&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">state&lt;/span>: &lt;span style="color:#ae81ff">started&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">tags&lt;/span>: &lt;span style="color:#ae81ff">manage_services&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">启用 kube-proxy 服务&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">systemd&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">kube-proxy&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">enabled&lt;/span>: &lt;span style="color:#66d9ef">yes&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">state&lt;/span>: &lt;span style="color:#ae81ff">started&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">tags&lt;/span>: &lt;span style="color:#ae81ff">manage_services&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 节点加入 Kubernetes 集群并设置角色标签&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">设置 Kubernetes 节点角色标签&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">shell&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;{{ kubectl_bin }} label node {{ inventory_hostname }} kubernetes.io/role={{ node_role }} --overwrite&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">tags&lt;/span>: &lt;span style="color:#ae81ff">configure_node_role&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">节点调度控制&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">shell&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;{{ kubectl_bin }} cordon {{ inventory_hostname }}&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">when&lt;/span>: &lt;span style="color:#ae81ff">node_role == &amp;#34;master&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">tags&lt;/span>: &lt;span style="color:#ae81ff">configure_node_role&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="templateskubelet-configyamlj2">
 &lt;strong>&lt;code>templates/kubelet-config.yaml.j2&lt;/code>&lt;/strong>
 &lt;a class="anchor" href="#templateskubelet-configyamlj2">#&lt;/a>
&lt;/h4>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">kubelet.config.k8s.io/v1beta1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">KubeletConfiguration&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">address&lt;/span>: &lt;span style="color:#ae81ff">0.0.0.0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">port&lt;/span>: &lt;span style="color:#ae81ff">10250&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">readOnlyPort&lt;/span>: &lt;span style="color:#ae81ff">10255&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">clusterDNS&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - {{ &lt;span style="color:#ae81ff">cluster_dns }}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">clusterDomain&lt;/span>: {{ &lt;span style="color:#ae81ff">cluster_domain }}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">resolvConf&lt;/span>: &lt;span style="color:#ae81ff">/run/systemd/resolve/resolv.conf&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="templateskube-proxy-configyamlj2">
 &lt;strong>&lt;code>templates/kube-proxy-config.yaml.j2&lt;/code>&lt;/strong>
 &lt;a class="anchor" href="#templateskube-proxy-configyamlj2">#&lt;/a>
&lt;/h4>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">kubeproxy.config.k8s.io/v1alpha1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">KubeProxyConfiguration&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">mode&lt;/span>: &lt;span style="color:#ae81ff">iptables&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">clusterCIDR&lt;/span>: {{ &lt;span style="color:#ae81ff">cluster_cidr }}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="templateskubeletservicej2">
 &lt;strong>&lt;code>templates/kubelet.service.j2&lt;/code>&lt;/strong>
 &lt;a class="anchor" href="#templateskubeletservicej2">#&lt;/a>
&lt;/h4>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-ini" data-lang="ini">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">[Unit]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">Description&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">Kubernetes Kubelet&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">Documentation&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">https://kubernetes.io/docs/home/&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">After&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">network.target&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">Requires&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">docker.service&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">[Service]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">ExecStart&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">{{ bin_dir }}/kubelet \
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> --config={{ config_dir }}/kubelet/config.yaml \
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> --kubeconfig={{ kubeconfig_path }}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">Restart&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">always&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">RestartSec&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">10s&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">[Install]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">WantedBy&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">multi-user.target&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="templateskube-proxyservicej2">
 &lt;strong>&lt;code>templates/kube-proxy.service.j2&lt;/code>&lt;/strong>
 &lt;a class="anchor" href="#templateskube-proxyservicej2">#&lt;/a>
&lt;/h4>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-ini" data-lang="ini">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">[Unit]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">Description&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">Kubernetes Kube Proxy&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">After&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">network.target&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">[Service]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">ExecStart&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">{{ bin_dir }}/kube-proxy \
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> --config={{ config_dir }}/kube-proxy/kube-proxy-config.yaml \
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> --kubeconfig={{ kubeconfig_path }}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">Restart&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">always&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">RestartSec&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">10s&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">[Install]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">WantedBy&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">multi-user.target&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="3-role-的配置文件结构">
 &lt;strong>3. Role 的配置文件结构&lt;/strong>
 &lt;a class="anchor" href="#3-role-%e7%9a%84%e9%85%8d%e7%bd%ae%e6%96%87%e4%bb%b6%e7%bb%93%e6%9e%84">#&lt;/a>
&lt;/h3>
&lt;p>可以在 Role 中定义一个清晰的目录结构，包括以下文件：&lt;/p></description></item><item><title>2025-2-28 prometheus 面试题</title><link>https://qq547475331.github.io/docs/2025-2-28-prometheus/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2025-2-28-prometheus/</guid><description>&lt;h2 id="prometheus">
 Prometheus
 &lt;a class="anchor" href="#prometheus">#&lt;/a>
&lt;/h2>
&lt;h2 id="prometheus的工作流程">
 Prometheus的工作流程
 &lt;a class="anchor" href="#prometheus%e7%9a%84%e5%b7%a5%e4%bd%9c%e6%b5%81%e7%a8%8b">#&lt;/a>
&lt;/h2>
&lt;p>Prometheus 的工作流程可以概括为以下几个主要步骤：&lt;/p>
&lt;ol>
&lt;li>&lt;strong>数据抓取（Scraping）&lt;/strong>：
&lt;ul>
&lt;li>Prometheus 会定期从配置好的目标（如应用程序、服务、节点等）抓取数据。这些目标通常通过 HTTP 接口暴露 Prometheus 格式的监控数据（通常是 &lt;code>/metrics&lt;/code> 路径）。&lt;/li>
&lt;li>这些数据包括各类指标，如 CPU 使用率、内存使用情况、请求数量等。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>存储（Storage）&lt;/strong>：
&lt;ul>
&lt;li>抓取到的数据会被存储在 Prometheus 的本地时间序列数据库（TSDB）中。每个时间序列由一个指标名称和一组标签（如 &lt;code>instance&lt;/code>, &lt;code>job&lt;/code>, &lt;code>region&lt;/code> 等）组成。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>查询（Querying）&lt;/strong>：
&lt;ul>
&lt;li>Prometheus 提供了强大的查询语言——PromQL（Prometheus Query Language），可以通过 PromQL 查询已存储的数据。&lt;/li>
&lt;li>Prometheus 可以通过其 Web 界面、API 或 Grafana 进行查询，显示时间序列数据的图表，或者用于告警规则的计算。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>告警（Alerting）&lt;/strong>：
&lt;ul>
&lt;li>Prometheus 可以基于定义的告警规则（通过 PromQL 查询语句）进行告警。例如，当 CPU 使用率超过一定阈值时，触发告警。&lt;/li>
&lt;li>告警规则可以定义在 Prometheus 配置文件中，告警信息可以通过 Alertmanager 发送到各类通知系统（如邮件、Slack、钉钉等）。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>数据可视化（Visualization）&lt;/strong>：
&lt;ul>
&lt;li>Prometheus 本身也提供了简单的图表功能，但通常会与第三方工具（如 Grafana）结合使用，以便提供更丰富的可视化效果。&lt;/li>
&lt;li>Grafana 可以从 Prometheus 查询数据，生成漂亮的仪表板，帮助团队实时监控系统健康状况。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>服务发现与目标自动化（Service Discovery）&lt;/strong>：
&lt;ul>
&lt;li>Prometheus 支持多种服务发现机制，可以自动发现需要抓取数据的目标。常见的有 Kubernetes、Consul 等服务发现工具，也可以使用静态配置。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;p>整体而言，Prometheus 的核心是周期性地抓取目标的监控数据，存储成时间序列，并通过查询、告警和可视化等功能帮助用户监控和维护系统的健康。&lt;/p></description></item><item><title>2025-2-7 k8s组件</title><link>https://qq547475331.github.io/docs/2025-2-7-k8s%E7%BB%84%E4%BB%B6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2025-2-7-k8s%E7%BB%84%E4%BB%B6/</guid><description>&lt;p>&lt;strong>k8s 有哪些组件？&lt;/strong>&lt;/p>
&lt;p>&lt;strong>1、etcd 保存了整个集群的状态；&lt;/strong>&lt;/p>
&lt;p>&lt;strong>2、apiserver 提供了资源操作的唯一入口，并提供认证、授权、访问控制、API 注册和发现等机制；&lt;/strong>&lt;/p>
&lt;p>&lt;strong>3、controller manager 负责维护集群的状态，比如故障检测、自动扩展、滚动更新等；&lt;/strong>&lt;/p>
&lt;p>&lt;strong>4、scheduler 负责资源的调度，按照预定的调度策略将 Pod 调度到相应的机器上；&lt;/strong>&lt;/p>
&lt;p>&lt;strong>5、kubelet 负责维护容器的生命周期，同时也负责 Volume（CVI）和网络（CNI）的管理；&lt;/strong>&lt;/p>
&lt;p>&lt;strong>6、Container runtime 负责镜像管理以及 Pod 和容器的真正运行（CRI）；&lt;/strong>&lt;/p>
&lt;p>&lt;strong>7&lt;/strong>、&lt;strong>kube-proxy&lt;/strong> &lt;strong>负责为&lt;/strong> &lt;strong>Service&lt;/strong> &lt;strong>提供&lt;/strong> &lt;strong>cluster&lt;/strong> &lt;strong>内部的服务发现和负载均衡。&lt;/strong>&lt;/p></description></item><item><title>2025-2-7 美国码农薪酬</title><link>https://qq547475331.github.io/docs/2025-2-7-%E8%AE%A1%E5%88%92/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2025-2-7-%E8%AE%A1%E5%88%92/</guid><description>&lt;p>可如果不分国家，那么美国的CS（计算机）毕业生刚入职的税前收入就可以达到100万人民币（15.4万美元）这一水平。&lt;/p>
&lt;p>无论是中国、美国还是世界其他地区，在过去十年中，码农都是整体收入增长最快、社会认可度提升最大的一类职业。&lt;/p>
&lt;p>码农是软件工程师的一个带有一点点自嘲性质的简称，但码农确实是这个时代极少数可以通过踏踏实实打工就大幅提升自身经济水平并改变命运的一个职业。在我的圈子中，码农是最多的，其中一半在美国，今天我就以自己的见闻体会结合行业的发展聊聊美国码农的生活状态和财务状况。&lt;/p>
&lt;p>&lt;strong>1&lt;/strong>&lt;/p>
&lt;p>&lt;strong>薪酬对比&lt;/strong>&lt;/p>
&lt;p>在互联网深度融入生活工作的时代，码农已经成为各个国家普遍存在的职业，但在待遇方面，不同国家之间待遇差别可谓巨大。&lt;/p>
&lt;p>如果认真比，当码农最香的地方还是美国。&lt;strong>美国的码农的平均工资比同属发达地区的伦敦、新加坡、多伦多的同行要高出一倍左右&lt;/strong>，比其他发展中国家自然高出更多，这也是为什么你会发现清华北大北邮华科等CS牛校的计算机毕业生很大一部分都计划美国留学，而且这个专业的人回国比率是所有专业中最低的一个&amp;hellip;那是因为他们在大洋彼岸赚得实在是太多了。&lt;/p>
&lt;p>根据科技行业薪资统计平台Levels.fyi给的数据，&lt;strong>旧金山湾区和西雅图的软件工程师中位数年包分别为23万美元和20.4万美元&lt;/strong>；而作为对比，多伦多对应的数字是12.2万美元、伦敦是13.5万美元、新加坡是10.7万美元。所以即使是欧洲、加拿大、新加坡这些发达国家的不少IT同行都希望能到美国工作发展。&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/image-20250207213356315.png" alt="image-20250207213356315" />&lt;/p>
&lt;p>旧金山湾区软件工程师中位数年包23万美元&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/image-20250207213419371.png" alt="image-20250207213419371" />&lt;/p>
&lt;p>加拿大多伦多软件工程师中位数年包12.2万美元&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/image-20250207213445886.png" alt="image-20250207213445886" />&lt;/p>
&lt;p>英国伦敦软件工程师中位数年包13.5万美元&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/image-20250207213510807.png" alt="image-20250207213510807" />&lt;/p>
&lt;p>新加坡软件工程师中位数年包10.7万美元&lt;/p>
&lt;p>多说一下，我们用的Levels.fyi是非常可靠的一个平台，不同公司的员工可以验证身份、上传offer、W2表等方式反馈真实的薪酬数据，数据量非常大。不同的公司对于级别划分不同，但整体差异不大，比如一个刚刚大学毕业的计算机毕业生，去谷歌L3级别的年包平均是19.1万美元，去苹果ICT2级别的年包平均是17.2万美元，谷歌的L3和苹果的ICT2对应的都是入门级软件工程师。&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/image-20250207213529745.png" alt="image-20250207213529745" />&lt;/p>
&lt;p>FAAMG各自对软件工程的职级划分和横向对应&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/image-20250207213550980.png" alt="image-20250207213550980" />&lt;/p>
&lt;p>谷歌各级别软件工程师年收入 L3对应191K（19.1万美元）&lt;/p>
&lt;p>&lt;strong>2&lt;/strong>&lt;/p>
&lt;p>&lt;strong>股票是重点&lt;/strong>&lt;/p>
&lt;p>尽管码农的收入都很高，但这不是这个群体致富的最关键因素，一方面美国有着高额的所得税，另一方面在旧金山湾区、西雅图、纽约等城市，物价和房价也都很高。&lt;/p>
&lt;p>美国码农致富的核心是股票，而且职级越高、收入越高的码农，年包中股票的比例就越大。以亚马逊为例，L4（入门级码农）的年包平均16.4万美元，其中Base（基本工资）12.5万美元，Stock（股票）2.2万美元，Bonus（奖励）1.7万美元；而发展到L6（这个级别是大多数中国人可以触及的最大高度）时，平均年包达到33.2万美元，其中股票16万美元；最高的Principal（总管）年包中资本工资只有16.2万美元，但股票多达43.6万美元。&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/image-20250207213615192.png" alt="image-20250207213615192" />&lt;/p>
&lt;p>亚马逊软件工程师薪酬&lt;/p>
&lt;p>纳斯达克综合指数中大部分公司都是科网公司，可以直接理解为美国的“码农指数”。这个大盘指数从2009年3月到现在累计涨了12倍，这说明在这十二年多的时间里，十倍股是遍布整个市场的，否则大盘不可能有这么大的涨幅。&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/image-20250207213638798.png" alt="image-20250207213638798" />&lt;/p>
&lt;p>我们看FAAMG五大科技公司，有四家在过去一轮牛市中涨幅远超十倍，其中苹果48倍、微软18倍、谷歌17倍、亚马逊55倍，另外一家脸书2012年才上市，至今9倍。&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/image-20250207213657427.png" alt="image-20250207213657427" />&lt;/p>
&lt;p>亚马逊、苹果、微软、谷歌股价涨幅对比 2009-2021&lt;/p>
&lt;p>如果一个亚马逊普通员工在2009年初入职，按当时的行情，入职年包大约是10万美元，其中1万美元是股票，假设他一直持有这些股票到了现在，那么仅2009年一年他得到的股票放现在就价值55万美元，如果他一直在亚马逊工作，而且努力上班，不考虑买房也考虑做其他投资，很可能早就已财务自由。&lt;/p>
&lt;p>当然，人性的弱点导致大多数人没有办法看到手上已经上涨十倍以上的股票还不套现，所以老员工只有很少人能一股不卖留到现在。虽然2009年到现在湾区和西雅图的房价也涨了200%以上，但即使加上杠杆效应还是无法跑赢股价，这也解释了为什么美国的房价没有办法炒到房价收入比和租售比到扭曲的程度，因为他们有可替代的资产去满足投资需要。&lt;/p>
&lt;p>&lt;strong>3&lt;/strong>&lt;/p>
&lt;p>&lt;strong>内卷问题&lt;/strong>&lt;/p>
&lt;p>尽管美国大多数码农收入很高、又有股票助推财富增长，但他们并不是都过得有你想象的那么舒服，美国也是发达经济体中内卷最严重的国家。&lt;/p>
&lt;p>码农聚集地常常也是内卷严重的地方，因为这里人多、而且这些人还都非常聪明、非常努力、收入又高。设身处地想想也能理解，一个人年包25万美元可能会感觉很棒，但当你身边所有人年包都是这个水平时，每天还面对年薪百万美元的领导时，你得到的压力就大过成就感了。&lt;/p>
&lt;p>以旧金山湾区为例，这里不仅汇聚了美国自己一批聪明的年轻人，还吸收了来自包括印度、中国等世界各地的无数学霸。在硅谷华人圈子里，你会发现很多人见面或线上讨论的话题越来越多地涉及到股票、买房、学区、私立学校这些概念，这些话题显然很内卷，一听就感觉到压力。&lt;/p>
&lt;p>科技大厂在美国国内各个城市同级别职位的待遇相差不大，所以相同级别的码农在不同地方很可能完全是两种生活水平，比如100万美元在硅谷要买独立屋只能买到个小黑屋，在德州奥斯汀就可以买一个3000呎的漂亮大house，内卷很大程度上取决于地域。&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/image-20250207213721714.png" alt="image-20250207213721714" />&lt;/p>
&lt;p>在湾区的圣何塞 100万美元买的房子是这样的&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/image-20250207213745332.png" alt="image-20250207213745332" />&lt;/p>
&lt;p>在德州的奥斯汀 50万美元可以买到3000呎的新房&lt;/p>
&lt;p>不过财富和人口都是流动的，内卷也是会蔓延的，在奥斯汀周边的码农聚集区Cedar Park，这里环境美、治安好、就业多、房价低，结果过去一年房价暴涨超过50%，在美国这样一个城市化结束、经济中低速增长、而且还有房产税的国家，这么快的房价涨速是很可怕的。&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/image-20250207213825885.png" alt="image-20250207213825885" />&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/image-20250207213844557.png" alt="image-20250207213844557" />&lt;/p>
&lt;p>Cedar Park房价走势&lt;/p>
&lt;p>说到内卷，工作压力问题绕不开，这方面美国码农还是相对轻松一些的，有oncall（随时待命）制度的公司也只是极少数时间oncall，比如亚马逊每周不同的组轮着oncall，每个组一年大概两次，一次半个星期到一个星期。&lt;/p>
&lt;p>美国科技行业不同公司以及相同公司的不同小组、不同工作者之间在加班上的差异非常大。比如脸书和亚马逊属于整体工作强度比较大的，微软、甲骨文属于工作强度相对低的，但在脸书一些成熟产品的小组，也有人是轻松状态；在微软的新兴产品Azure相关的小组，员工们也都很拼。&lt;/p>
&lt;p>&lt;img src="https://picture-base.oss-cn-hangzhou.aliyuncs.com/image-20250207213901329.png" alt="image-20250207213901329" />&lt;/p></description></item><item><title>2025-2-7 美国码农计划</title><link>https://qq547475331.github.io/docs/2025-2-7-%E8%AE%A1%E5%88%922/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2025-2-7-%E8%AE%A1%E5%88%922/</guid><description>&lt;p>我给想润出去的墙内码农几条建议：&lt;/p>
&lt;p>1.大厂在职的，深耕技术直接投简历或转海外岗，不要花冤枉钱和宝贵的时间在教育上；&lt;/p>
&lt;p>2.第一目标应该是美国，第二是加/坡/澳/英/瑞，其他国家（欧/日/韩/港）期望收入远远不如国内；&lt;/p>
&lt;p>3.绝大多数人的首要障碍是英语听力和口语，一定要多用英语熟练讲项目和刷Leetcode。&lt;/p>
&lt;p>如果认真比，当码农最香的地方还是美国。&lt;strong>美国的码农的平均工资比同属发达地区的伦敦、新加坡、多伦多的同行要高出一倍左右&lt;/strong>，比其他发展中国家自然高出更多，这也是为什么你会发现清华北大北邮华科等CS牛校的计算机毕业生很大一部分都计划美国留学，而且这个专业的人回国比率是所有专业中最低的一个&amp;hellip;那是因为他们在大洋彼岸赚得实在是太多了。&lt;/p>
&lt;p>根据科技行业薪资统计平台Levels.fyi给的数据，&lt;strong>旧金山湾区和西雅图的软件工程师中位数年包分别为23万美元和20.4万美元&lt;/strong>；而作为对比，多伦多对应的数字是12.2万美元、伦敦是13.5万美元、新加坡是10.7万美元。所以即使是欧洲、加拿大、新加坡这些发达国家的不少IT同行都希望能到美国工作发展。&lt;/p>
&lt;p>美国码农致富的核心是股票，而且职级越高、收入越高的码农，年包中股票的比例就越大。以亚马逊为例，L4（入门级码农）的年包平均16.4万美元，其中Base（基本工资）12.5万美元，Stock（股票）2.2万美元，Bonus（奖励）1.7万美元；而发展到L6（这个级别是大多数中国人可以触及的最大高度）时，平均年包达到33.2万美元，其中股票16万美元；最高的Principal（总管）年包中资本工资只有16.2万美元，但股票多达43.6万美元。&lt;/p>
&lt;p>中国通缩严重呗，用一个人干两个人活，中国人收入还不涨，不通缩就怪了，不管二百斤超发多少货币，都会往不缺钱的人口袋里流。&lt;/p>
&lt;p>在美国湾区的生活成本，特别是涉及租房和吃饭的开支，通常相当高。根据网络上的信息和最新的经济数据，这里提供一个大致的开支估算：&lt;/p>
&lt;p>### 租房：&lt;/p>
&lt;p>- &lt;strong>一居室公寓&lt;/strong>：&lt;/p>
&lt;p>- 在旧金山市中心，平均租金大约在$3,000 - $3,500 美元之间。&lt;/p>
&lt;p>- 如果在郊区如Oakland或Fremont，租金会略低，约为$2,000 - $3,000 美元。&lt;/p>
&lt;p>- &lt;strong>合租&lt;/strong>：&lt;/p>
&lt;p>- 与他人合租可以大幅降低开支。合租一居室或多居室公寓，个人承担的租金可能会在$1,500 - $2,500 美元之间。&lt;/p>
&lt;p>### 吃饭：&lt;/p>
&lt;p>- &lt;strong>自制餐食&lt;/strong>：&lt;/p>
&lt;p>- 食品杂货的价格在湾区也相对高于美国其他地区。一个单身人士每月的杂货开支可能在$300 - $600 美元之间，视饮食习惯而定。&lt;/p>
&lt;p>- &lt;strong>外出就餐&lt;/strong>：&lt;/p>
&lt;p>- 在湾区吃一顿普通的餐馆饭菜，价格通常在$15 - $30 美元左右，昂贵的餐厅可能更高。如果你经常外出就餐，每月的开支可能达到$400 - $800 美元或更多。&lt;/p>
&lt;p>### 总体开支：&lt;/p>
&lt;p>- &lt;strong>保守估计&lt;/strong>：假设一个人选择合租，注重自制餐食，尽量减少外出就餐：&lt;/p>
&lt;p>- 租房：$1,500&lt;/p>
&lt;p>- 吃饭：$400（杂货）&lt;/p>
&lt;p>- &lt;strong>总计&lt;/strong>：约$1,900 美元每月&lt;/p>
&lt;p>- &lt;strong>中等开支&lt;/strong>：如果租住一居室公寓，并且偶尔外出就餐：&lt;/p>
&lt;p>- 租房：$3,000&lt;/p>
&lt;p>- 吃饭：$600（杂货 + 偶尔外出就餐）&lt;/p>
&lt;p>- &lt;strong>总计&lt;/strong>：约$3,600 美元每月&lt;/p></description></item><item><title>2025-3-12 追觅面试</title><link>https://qq547475331.github.io/docs/2025-3-12-%E8%BF%BD%E8%A7%85%E9%9D%A2%E8%AF%95/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2025-3-12-%E8%BF%BD%E8%A7%85%E9%9D%A2%E8%AF%95/</guid><description>&lt;h1 id="你做了啥什么对团队项目贡献比较大的">
 你做了啥什么，对团队/项目贡献比较大的？
 &lt;a class="anchor" href="#%e4%bd%a0%e5%81%9a%e4%ba%86%e5%95%a5%e4%bb%80%e4%b9%88%e5%af%b9%e5%9b%a2%e9%98%9f%e9%a1%b9%e7%9b%ae%e8%b4%a1%e7%8c%ae%e6%af%94%e8%be%83%e5%a4%a7%e7%9a%84">#&lt;/a>
&lt;/h1>
&lt;p>试点，给后续上生产起到好的示范作用，后续上生产比较顺利。&lt;/p>
&lt;h1 id="pv-pvc-storage-class之间的关系">
 pv pvc storage-class之间的关系？
 &lt;a class="anchor" href="#pv-pvc-storage-class%e4%b9%8b%e9%97%b4%e7%9a%84%e5%85%b3%e7%b3%bb">#&lt;/a>
&lt;/h1>
&lt;p>在 Kubernetes 中，&lt;code>PV&lt;/code>（PersistentVolume）、&lt;code>PVC&lt;/code>（PersistentVolumeClaim）和 &lt;code>StorageClass&lt;/code> 共同用于管理持久化存储，它们的关系如下：&lt;/p>
&lt;hr>
&lt;h2 id="1-关系概述">
 &lt;strong>1. 关系概述&lt;/strong>
 &lt;a class="anchor" href="#1-%e5%85%b3%e7%b3%bb%e6%a6%82%e8%bf%b0">#&lt;/a>
&lt;/h2>
&lt;ul>
&lt;li>&lt;code>StorageClass&lt;/code>：定义存储的&lt;strong>类型&lt;/strong>，决定如何动态创建 &lt;code>PV&lt;/code>。&lt;/li>
&lt;li>&lt;code>PV&lt;/code>（PersistentVolume）：代表集群中的&lt;strong>实际存储&lt;/strong>，可以是静态预先创建的，也可以通过 &lt;code>StorageClass&lt;/code> 动态创建。&lt;/li>
&lt;li>&lt;code>PVC&lt;/code>（PersistentVolumeClaim）：Pod 申请存储时创建的&lt;strong>存储请求&lt;/strong>，Kubernetes 根据 PVC 自动绑定合适的 PV。&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="2-详细流程">
 &lt;strong>2. 详细流程&lt;/strong>
 &lt;a class="anchor" href="#2-%e8%af%a6%e7%bb%86%e6%b5%81%e7%a8%8b">#&lt;/a>
&lt;/h2>
&lt;p>&lt;strong>1️⃣ Pod 申请存储（创建 PVC）&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>用户创建 &lt;code>PVC&lt;/code>，声明需要多大的存储、读写权限等。&lt;/li>
&lt;li>Kubernetes 查找匹配的 &lt;code>PV&lt;/code>，若存在符合要求的 &lt;code>PV&lt;/code>，则直接绑定。&lt;/li>
&lt;li>若 &lt;code>PV&lt;/code> 不匹配，Kubernetes 使用 &lt;code>StorageClass&lt;/code> &lt;strong>动态创建 &lt;code>PV&lt;/code>&lt;/strong>（如果 &lt;code>PVC&lt;/code> 指定了 &lt;code>StorageClass&lt;/code>）。&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>2️⃣ &lt;code>PV&lt;/code> 绑定 &lt;code>PVC&lt;/code>&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>如果 &lt;code>PVC&lt;/code> 申请的存储满足某个 &lt;code>PV&lt;/code> 的要求，就会绑定该 &lt;code>PV&lt;/code>。&lt;/li>
&lt;li>&lt;code>PVC&lt;/code> 绑定 &lt;code>PV&lt;/code> 后，Pod 只能使用该 &lt;code>PV&lt;/code>，直到 &lt;code>PVC&lt;/code> 释放。&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>3️⃣ Pod 使用 &lt;code>PVC&lt;/code>&lt;/strong>&lt;/p></description></item><item><title>2025-3-12 塔赞面试</title><link>https://qq547475331.github.io/docs/2025-3-12-%E5%A1%94%E8%B5%9E%E9%9D%A2%E8%AF%95/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2025-3-12-%E5%A1%94%E8%B5%9E%E9%9D%A2%E8%AF%95/</guid><description>&lt;p>自我介绍&lt;/p>
&lt;p>10分钟内启动5万pod，具体讲下怎么做到的？etcd哪些参数需要调整的？etcd的核心参数？&lt;/p>
&lt;p>有遇到哪些问题？哪些参数有哪些影响？有遇到调完不生效？哪些参数有哪些影响？&lt;/p>
&lt;p>哪块是核心做的？&lt;/p>
&lt;p>双中心 双平面建设？F5 haproxy在流量分发的具体分工？&lt;/p>
&lt;p>F5的vip 后面挂5个master，master节点宕机如何做到无感切换？&lt;/p>
&lt;p>这块的工作是啥？&lt;/p>
&lt;p>内网环境，两个算力中心，跨地域，异地多活，双中心，故障切换，做高可用架构？&lt;/p>
&lt;p>prometheus的oom的问题，怎么解决的？&lt;/p>
&lt;p>二线工作的挑战是什么？&lt;/p>
&lt;p>项目前期的交付做的事情？&lt;/p>
&lt;p>交付和客户和打交道吗？&lt;/p>
&lt;p>碰到难搞的客户怎么办？&lt;/p>
&lt;p>AI的运维经验？&lt;/p>
&lt;p>找的工作的预期？云平台的运维吗？&lt;/p>
&lt;p>有offer了吗？&lt;/p>
&lt;p>未来职业规划？&lt;/p>
&lt;p>最近在学什么东西？具体学的东西？&lt;/p>
&lt;p>有运维相关的东西吗？&lt;/p>
&lt;p>玩过什么大模型？有什么效果？&lt;/p>
&lt;p>ai对运维有什么影响？考虑？&lt;/p>
&lt;p>反问？&lt;/p>
&lt;p> 运维团队几个人？管理规模怎样的？&lt;/p>
&lt;p>日常工作？管理体系？文化氛围&lt;/p>
&lt;p>绩效考核之前做吗?&lt;/p>
&lt;p>汇报三级，周报一次&lt;/p>
&lt;p>出差频率？&lt;/p>
&lt;p>全国各地，一次出差多久？&lt;/p>
&lt;p>上架部署，标注化产品，很少有项目式的&lt;/p></description></item><item><title>2025-3-13 calico三种模式下流量传输</title><link>https://qq547475331.github.io/docs/2025-3-13-calico%E4%B8%89%E7%A7%8D%E6%A8%A1%E5%BC%8F%E4%B8%8B%E6%B5%81%E9%87%8F%E4%BC%A0%E8%BE%93%E8%B7%AF%E5%BE%84%E5%88%86%E6%9E%90/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2025-3-13-calico%E4%B8%89%E7%A7%8D%E6%A8%A1%E5%BC%8F%E4%B8%8B%E6%B5%81%E9%87%8F%E4%BC%A0%E8%BE%93%E8%B7%AF%E5%BE%84%E5%88%86%E6%9E%90/</guid><description>&lt;p>要详细分析 &lt;strong>Calico BGP、IPIP 和 VXLAN 模式&lt;/strong>下不同 &lt;strong>Node 上 Pod 之间的流量传输路径&lt;/strong>，需要关注以下几个关键网络接口的角色和数据流向：&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Pod 内部的 veth 接口&lt;/strong>&lt;/li>
&lt;li>&lt;strong>宿主机的 veth 对应端&lt;/strong>&lt;/li>
&lt;li>&lt;strong>宿主机的 CNI 设备（通常是 &lt;code>cali+&lt;/code> 接口）&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Node 上的物理网络接口（如 &lt;code>eth0&lt;/code>）&lt;/strong>&lt;/li>
&lt;li>&lt;strong>IPIP/VXLAN 设备（如果适用）&lt;/strong>&lt;/li>
&lt;li>&lt;strong>目的地 Node 的解封装流程&lt;/strong>&lt;/li>
&lt;li>&lt;strong>目的 Pod 的 veth 接口&lt;/strong>&lt;/li>
&lt;/ol>
&lt;hr>
&lt;h1 id="一calico-bgp-模式下-pod-之间的流量传输">
 &lt;strong>一、Calico BGP 模式下 Pod 之间的流量传输&lt;/strong>
 &lt;a class="anchor" href="#%e4%b8%80calico-bgp-%e6%a8%a1%e5%bc%8f%e4%b8%8b-pod-%e4%b9%8b%e9%97%b4%e7%9a%84%e6%b5%81%e9%87%8f%e4%bc%a0%e8%be%93">#&lt;/a>
&lt;/h1>
&lt;h2 id="1-网络拓扑">
 &lt;strong>1. 网络拓扑&lt;/strong>
 &lt;a class="anchor" href="#1-%e7%bd%91%e7%bb%9c%e6%8b%93%e6%89%91">#&lt;/a>
&lt;/h2>
&lt;ul>
&lt;li>Node1（192.168.1.1）：
&lt;ul>
&lt;li>PodA（10.1.1.2）&lt;/li>
&lt;li>宿主机 &lt;code>eth0&lt;/code>（192.168.1.1）&lt;/li>
&lt;li>&lt;code>cali+&lt;/code> 接口（连接 PodA）&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Node2（192.168.1.2）：
&lt;ul>
&lt;li>PodB（10.1.2.3）&lt;/li>
&lt;li>宿主机 &lt;code>eth0&lt;/code>（192.168.1.2）&lt;/li>
&lt;li>&lt;code>cali+&lt;/code> 接口（连接 PodB）&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="2-详细流量路径">
 &lt;strong>2. 详细流量路径&lt;/strong>
 &lt;a class="anchor" href="#2-%e8%af%a6%e7%bb%86%e6%b5%81%e9%87%8f%e8%b7%af%e5%be%84">#&lt;/a>
&lt;/h2>
&lt;pre tabindex="0">&lt;code class="language-mermaid" data-lang="mermaid">graph TD
 subgraph Node1[&amp;#34;Node 1 (192.168.1.1)&amp;#34;]
 PodA[&amp;#34;PodA (10.1.1.2)&amp;#34;]
 vethA[&amp;#34;vethA (PodA eth0)&amp;#34;]
 caliA[&amp;#34;caliXXXX (Calico veth)&amp;#34;]
 eth1[&amp;#34;eth0 (宿主机)&amp;#34;]
 end

 subgraph Node2[&amp;#34;Node 2 (192.168.1.2)&amp;#34;]
 PodB[&amp;#34;PodB (10.1.2.3)&amp;#34;]
 vethB[&amp;#34;vethB (PodB eth0)&amp;#34;]
 caliB[&amp;#34;caliYYYY (Calico veth)&amp;#34;]
 eth2[&amp;#34;eth0 (宿主机)&amp;#34;]
 end

 PodA --&amp;gt; vethA --&amp;gt; caliA
 caliA --&amp;gt;|BGP 路由| eth1
 eth1 --&amp;gt;|裸 IP 直接转发| eth2
 eth2 --&amp;gt; caliB --&amp;gt; vethB --&amp;gt; PodB
&lt;/code>&lt;/pre>&lt;h3 id="poda-10112--podb-10123">
 &lt;strong>PodA (10.1.1.2) → PodB (10.1.2.3)&lt;/strong>
 &lt;a class="anchor" href="#poda-10112--podb-10123">#&lt;/a>
&lt;/h3>
&lt;ol>
&lt;li>&lt;strong>PodA 发送数据包&lt;/strong>
&lt;ul>
&lt;li>PodA (10.1.1.2) 向 PodB (10.1.2.3) 发送数据包&lt;/li>
&lt;li>数据包进入 PodA 的 &lt;strong>veth&lt;/strong> 接口 &lt;code>eth0&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>通过 veth pair 进入宿主机&lt;/strong>
&lt;ul>
&lt;li>PodA 的 &lt;code>eth0&lt;/code>（veth 子接口）连接到宿主机上的 &lt;code>caliXXXX&lt;/code> 设备（Calico 创建的 veth 对应端）&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>宿主机查找路由&lt;/strong>
&lt;ul>
&lt;li>&lt;code>caliXXXX&lt;/code> 设备收到数据包，进入宿主机的 &lt;strong>路由表&lt;/strong>&lt;/li>
&lt;li>由于 Calico 通过 &lt;strong>BGP 传播路由&lt;/strong>，Node1 知道 &lt;strong>10.1.2.3 在 Node2&lt;/strong>&lt;/li>
&lt;li>宿主机的 &lt;code>eth0&lt;/code> 直接将数据包&lt;strong>原生 IP 转发&lt;/strong>到 &lt;code>192.168.1.2&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Node2 接收数据&lt;/strong>
&lt;ul>
&lt;li>Node2 的 &lt;code>eth0&lt;/code> 接收到数据包&lt;/li>
&lt;li>根据 &lt;strong>路由表&lt;/strong> 知道 10.1.2.3 在本机，数据包通过 &lt;code>caliXXXX&lt;/code> 进入 PodB 的 &lt;code>eth0&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>PodB 接收数据&lt;/strong>
&lt;ul>
&lt;li>PodB（10.1.2.3）收到数据包，通信完成&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;h3 id="特点">
 &lt;strong>特点&lt;/strong>
 &lt;a class="anchor" href="#%e7%89%b9%e7%82%b9">#&lt;/a>
&lt;/h3>
&lt;p>✅ &lt;strong>优点&lt;/strong>&lt;/p></description></item><item><title>2025-3-13 istio流量分析</title><link>https://qq547475331.github.io/docs/2025-3-13-istio%E6%B5%81%E9%87%8F%E5%88%86%E6%9E%90/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2025-3-13-istio%E6%B5%81%E9%87%8F%E5%88%86%E6%9E%90/</guid><description>&lt;h3 id="istio-流量路径">
 Istio 流量路径
 &lt;a class="anchor" href="#istio-%e6%b5%81%e9%87%8f%e8%b7%af%e5%be%84">#&lt;/a>
&lt;/h3>
&lt;pre tabindex="0">&lt;code class="language-mermaid" data-lang="mermaid">graph TD;
 A[Client Request] --&amp;gt; B[Istio Ingress Gateway]
 B --&amp;gt; C[VirtualService]
 C --&amp;gt; D[DestinationRule]
 D --&amp;gt;|Traffic Routing| E[Sidecar Proxy - Pod1]
 D --&amp;gt;|Traffic Routing| F[Sidecar Proxy - Pod2]
 E --&amp;gt; G[Application - Pod1]
 F --&amp;gt; H[Application - Pod2]
 G --&amp;gt;|Response| E
 H --&amp;gt;|Response| F
 E --&amp;gt;|Return to Gateway| B
 F --&amp;gt;|Return to Gateway| B
 B --&amp;gt;|Send Response to Client| A
&lt;/code>&lt;/pre>&lt;h3 id="说明">
 &lt;strong>说明&lt;/strong>
 &lt;a class="anchor" href="#%e8%af%b4%e6%98%8e">#&lt;/a>
&lt;/h3>
&lt;ol>
&lt;li>&lt;strong>客户端请求 (&lt;code>A&lt;/code>) 进入 &lt;code>Istio Ingress Gateway&lt;/code> (&lt;code>B&lt;/code>)&lt;/strong>：
&lt;ul>
&lt;li>处理外部 HTTP/HTTPS 请求，可能涉及 TLS 终止。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>&lt;code>VirtualService&lt;/code> (&lt;code>C&lt;/code>) 定义流量规则&lt;/strong>：
&lt;ul>
&lt;li>指定 &lt;strong>流量如何路由&lt;/strong>，如&lt;strong>蓝绿部署、金丝雀发布&lt;/strong>等。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>&lt;code>DestinationRule&lt;/code> (&lt;code>D&lt;/code>) 选择目标 Pod&lt;/strong>：
&lt;ul>
&lt;li>可能涉及 &lt;strong>负载均衡、流量分配&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>流量通过 &lt;code>Sidecar Proxy&lt;/code> (&lt;code>E&lt;/code>, &lt;code>F&lt;/code>) 进入 Pod&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>Envoy 代理&lt;/strong> 处理流量，提供安全、流量管理功能。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Pod 内的应用 (&lt;code>G&lt;/code>, &lt;code>H&lt;/code>) 处理请求并返回&lt;/strong>：
&lt;ul>
&lt;li>响应先经过 &lt;code>Sidecar Proxy&lt;/code>，再回到 &lt;code>Ingress Gateway&lt;/code>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>最终响应返回客户端 (&lt;code>A&lt;/code>)&lt;/strong>。&lt;/li>
&lt;/ol>
&lt;h3 id="流量路径解析">
 &lt;strong>流量路径解析&lt;/strong>
 &lt;a class="anchor" href="#%e6%b5%81%e9%87%8f%e8%b7%af%e5%be%84%e8%a7%a3%e6%9e%90">#&lt;/a>
&lt;/h3>
&lt;ol>
&lt;li>&lt;strong>外部请求（Client）&lt;/strong> 访问 &lt;code>example.com&lt;/code>，流量进入 &lt;strong>Istio Ingress Gateway&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>Ingress Gateway&lt;/strong> 解析流量并匹配相应的 &lt;strong>VirtualService&lt;/strong> 规则。&lt;/li>
&lt;li>&lt;strong>VirtualService&lt;/strong> 进行 &lt;strong>流量路由&lt;/strong>（如 &lt;strong>流量镜像、权重分流、请求重写&lt;/strong>）。&lt;/li>
&lt;li>&lt;strong>DestinationRule&lt;/strong> 负责 &lt;strong>负载均衡、熔断、超时重试&lt;/strong>，并将流量分配给不同的 &lt;strong>Sidecar Proxy&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>Sidecar Proxy（Envoy）&lt;/strong> 负责代理流量到目标 &lt;strong>应用 Pod&lt;/strong>，并提供安全、可观测性等能力。&lt;/li>
&lt;li>&lt;strong>应用 Pod&lt;/strong> 处理请求后返回响应，同样经过 &lt;strong>Sidecar Proxy → Ingress Gateway&lt;/strong>，最终返回给 &lt;strong>外部客户端&lt;/strong>。&lt;/li>
&lt;/ol>
&lt;h3 id="适用场景">
 &lt;strong>适用场景&lt;/strong>
 &lt;a class="anchor" href="#%e9%80%82%e7%94%a8%e5%9c%ba%e6%99%af">#&lt;/a>
&lt;/h3>
&lt;p>✅ 适用于 &lt;strong>Istio 外部流量管理&lt;/strong>（如 &lt;code>https://example.com/api&lt;/code>）
✅ 支持 &lt;strong>负载均衡、蓝绿发布、金丝雀部署&lt;/strong>
✅ 适用于 &lt;strong>Ingress Gateway 代理的 Kubernetes 服务&lt;/strong>&lt;/p></description></item><item><title>2025-3-14 火山云迁移工程师面试记录</title><link>https://qq547475331.github.io/docs/2025-3-14-%E7%81%AB%E5%B1%B1%E4%BA%91%E8%BF%81%E7%A7%BB%E5%B7%A5%E7%A8%8B%E5%B8%88%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2025-3-14-%E7%81%AB%E5%B1%B1%E4%BA%91%E8%BF%81%E7%A7%BB%E5%B7%A5%E7%A8%8B%E5%B8%88%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/</guid><description>&lt;h1 id="项目做的哪些工作">
 项目做的哪些工作？
 &lt;a class="anchor" href="#%e9%a1%b9%e7%9b%ae%e5%81%9a%e7%9a%84%e5%93%aa%e4%ba%9b%e5%b7%a5%e4%bd%9c">#&lt;/a>
&lt;/h1>
&lt;h1 id="上云的系统的架构">
 上云的系统的架构？
 &lt;a class="anchor" href="#%e4%b8%8a%e4%ba%91%e7%9a%84%e7%b3%bb%e7%bb%9f%e7%9a%84%e6%9e%b6%e6%9e%84">#&lt;/a>
&lt;/h1>
&lt;h1 id="pass平台是自研的平台吗">
 pass平台是自研的平台吗？
 &lt;a class="anchor" href="#pass%e5%b9%b3%e5%8f%b0%e6%98%af%e8%87%aa%e7%a0%94%e7%9a%84%e5%b9%b3%e5%8f%b0%e5%90%97">#&lt;/a>
&lt;/h1>
&lt;h1 id="应用上云的工作">
 应用上云的工作？
 &lt;a class="anchor" href="#%e5%ba%94%e7%94%a8%e4%b8%8a%e4%ba%91%e7%9a%84%e5%b7%a5%e4%bd%9c">#&lt;/a>
&lt;/h1>
&lt;h1 id="自己搭的k8s">
 自己搭的k8s？
 &lt;a class="anchor" href="#%e8%87%aa%e5%b7%b1%e6%90%ad%e7%9a%84k8s">#&lt;/a>
&lt;/h1>
&lt;h1 id="k8s怎么管理容器的">
 k8s怎么管理容器的？
 &lt;a class="anchor" href="#k8s%e6%80%8e%e4%b9%88%e7%ae%a1%e7%90%86%e5%ae%b9%e5%99%a8%e7%9a%84">#&lt;/a>
&lt;/h1>
&lt;p>Kubernetes (K8s) 通过 &lt;strong>容器编排&lt;/strong> 来管理容器，主要包括以下几个关键机制：&lt;/p>
&lt;h3 id="1-编排与调度">
 1. &lt;strong>编排与调度&lt;/strong>
 &lt;a class="anchor" href="#1-%e7%bc%96%e6%8e%92%e4%b8%8e%e8%b0%83%e5%ba%a6">#&lt;/a>
&lt;/h3>
&lt;p>K8s 通过 &lt;strong>调度器（Scheduler）&lt;/strong> 将 Pod 调度到合适的节点上，依据资源利用率、节点健康状况等进行智能调度。&lt;/p>
&lt;h3 id="2-资源管理">
 2. &lt;strong>资源管理&lt;/strong>
 &lt;a class="anchor" href="#2-%e8%b5%84%e6%ba%90%e7%ae%a1%e7%90%86">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Cgroup&lt;/strong>：限制和管理容器的 CPU、内存等资源使用。&lt;/li>
&lt;li>&lt;strong>Namespace&lt;/strong>：提供资源隔离，如网络、存储、进程等。&lt;/li>
&lt;/ul>
&lt;h3 id="3-生命周期管理">
 3. &lt;strong>生命周期管理&lt;/strong>
 &lt;a class="anchor" href="#3-%e7%94%9f%e5%91%bd%e5%91%a8%e6%9c%9f%e7%ae%a1%e7%90%86">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Pod 及其控制器（Deployment、StatefulSet、DaemonSet 等）&lt;/strong> 负责管理容器的启动、更新、扩缩容。&lt;/li>
&lt;li>&lt;strong>Liveness &amp;amp; Readiness Probe&lt;/strong>：健康检查，确保容器正常运行。&lt;/li>
&lt;/ul>
&lt;h3 id="4-网络管理">
 4. &lt;strong>网络管理&lt;/strong>
 &lt;a class="anchor" href="#4-%e7%bd%91%e7%bb%9c%e7%ae%a1%e7%90%86">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>CNI 插件&lt;/strong>（如 Flannel、Calico）：提供 Pod 之间的通信。&lt;/li>
&lt;li>&lt;strong>Service&lt;/strong> 解决 Pod 动态 IP 问题，实现服务发现和负载均衡。&lt;/li>
&lt;/ul>
&lt;h3 id="5-存储管理">
 5. &lt;strong>存储管理&lt;/strong>
 &lt;a class="anchor" href="#5-%e5%ad%98%e5%82%a8%e7%ae%a1%e7%90%86">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>PV &amp;amp; PVC&lt;/strong>：管理持久化存储，支持 NFS、Ceph、Local Volume 等后端存储。&lt;/li>
&lt;/ul>
&lt;h3 id="6-安全与访问控制">
 6. &lt;strong>安全与访问控制&lt;/strong>
 &lt;a class="anchor" href="#6-%e5%ae%89%e5%85%a8%e4%b8%8e%e8%ae%bf%e9%97%ae%e6%8e%a7%e5%88%b6">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>RBAC&lt;/strong>：基于角色的访问控制，限制权限。&lt;/li>
&lt;li>&lt;strong>NetworkPolicy&lt;/strong>：控制 Pod 之间的流量访问策略。&lt;/li>
&lt;/ul>
&lt;p>通过这些机制，Kubernetes 能够高效、自动化地管理容器的生命周期、资源分配、网络通信和安全策略，实现弹性伸缩和高可用性。&lt;/p></description></item><item><title>2025-3-18 5w pod压测复盘</title><link>https://qq547475331.github.io/docs/2025-3-18-5w-pod%E5%8E%8B%E6%B5%8B%E5%A4%8D%E7%9B%98/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2025-3-18-5w-pod%E5%8E%8B%E6%B5%8B%E5%A4%8D%E7%9B%98/</guid><description>&lt;h1 id="5w-pod-压力测试复盘">
 5w pod 压力测试复盘
 &lt;a class="anchor" href="#5w-pod-%e5%8e%8b%e5%8a%9b%e6%b5%8b%e8%af%95%e5%a4%8d%e7%9b%98">#&lt;/a>
&lt;/h1>
&lt;h2 id="1-问题汇总">
 1. 问题汇总
 &lt;a class="anchor" href="#1-%e9%97%ae%e9%a2%98%e6%b1%87%e6%80%bb">#&lt;/a>
&lt;/h2>
&lt;h3 id="a类阻塞型问题">
 A类阻塞型问题
 &lt;a class="anchor" href="#a%e7%b1%bb%e9%98%bb%e5%a1%9e%e5%9e%8b%e9%97%ae%e9%a2%98">#&lt;/a>
&lt;/h3>
&lt;ol>
&lt;li>&lt;strong>ETCD 容量问题&lt;/strong>：默认 2G 容量远不足以支撑频繁触发的压力测试，导致 &lt;code>etcd: mvcc database space exceeded&lt;/code> 错误。&lt;/li>
&lt;li>&lt;strong>ETCD 容量超限清理策略&lt;/strong>：ETCD 容量超过限额，需要提供自动清理手段和策略。&lt;/li>
&lt;li>&lt;strong>Kubelet Pod 数量超限&lt;/strong>：压力测试创建的 Pod 数量超过单台 Kubelet 默认配置。&lt;/li>
&lt;li>&lt;strong>Watch 接口连接问题&lt;/strong>：压力测试客户端工具与磐基 Watch 接口在高压下频繁断联。&lt;/li>
&lt;li>&lt;strong>List Pod 接口超时&lt;/strong>：paas平台的 &lt;code>List Pod&lt;/code> 接口数据量过大，导致连接超时。&lt;/li>
&lt;/ol>
&lt;h3 id="b类非阻塞型问题">
 B类非阻塞型问题
 &lt;a class="anchor" href="#b%e7%b1%bb%e9%9d%9e%e9%98%bb%e5%a1%9e%e5%9e%8b%e9%97%ae%e9%a2%98">#&lt;/a>
&lt;/h3>
&lt;ol>
&lt;li>&lt;strong>Deployment 数据丢失&lt;/strong>：前台丢失部分 Deployment 创建的数据，导致后台集群中的 Deployment 无法通过接口删除。&lt;/li>
&lt;li>&lt;strong>Pod 数量超标&lt;/strong>：5W 级别压测最终 Pod 数量超过 5W，Running 状态符合 5W，但仍有部分 Pod 处于 Init 状态。&lt;/li>
&lt;li>&lt;strong>EC-Watch 日志报错&lt;/strong>：&lt;code>ec-watch&lt;/code> 访问 &lt;code>ec-resource-server 8080 failed to response&lt;/code> 错误。&lt;/li>
&lt;/ol>
&lt;h2 id="2-问题分析与解决方案">
 2. 问题分析与解决方案
 &lt;a class="anchor" href="#2-%e9%97%ae%e9%a2%98%e5%88%86%e6%9e%90%e4%b8%8e%e8%a7%a3%e5%86%b3%e6%96%b9%e6%a1%88">#&lt;/a>
&lt;/h2>
&lt;h3 id="etcd-部分优化">
 &lt;strong>ETCD 部分优化&lt;/strong>
 &lt;a class="anchor" href="#etcd-%e9%83%a8%e5%88%86%e4%bc%98%e5%8c%96">#&lt;/a>
&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>调整 ETCD 容量至 8G：&lt;/p></description></item><item><title>2025-3-8 k8s创建pod流程图详解</title><link>https://qq547475331.github.io/docs/2025-3-8-k8s%E5%88%9B%E5%BB%BApod-deployment%E6%B5%81%E7%A8%8B%E5%9B%BE%E8%AF%A6%E8%A7%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2025-3-8-k8s%E5%88%9B%E5%BB%BApod-deployment%E6%B5%81%E7%A8%8B%E5%9B%BE%E8%AF%A6%E8%A7%A3/</guid><description>&lt;pre tabindex="0">&lt;code class="language-mermaid" data-lang="mermaid">sequenceDiagram
 participant User as 用户
 participant Kubectl as Kubectl
 participant API as API Server
 participant CM as Controller Manager
 participant Etcd as etcd
 participant Scheduler as 调度器
 participant Kubelet as Kubelet
 participant CRI as 容器运行时
 participant CNI as 容器网络接口
 participant CSI as 容器存储接口

 User -&amp;gt;&amp;gt; Kubectl: 提交 Pod/Deployment 定义
 Kubectl -&amp;gt;&amp;gt; API: 发送创建请求
 API -&amp;gt;&amp;gt; Etcd: 存储资源信息
 
 alt 如果是高级资源(Deployment/ReplicaSet等)
 API -&amp;gt;&amp;gt; CM: 通知新资源创建
 CM -&amp;gt;&amp;gt; API: 创建对应的附属资源(如ReplicaSet/Pod)
 API -&amp;gt;&amp;gt; Etcd: 存储附属资源信息
 end
 
 API -&amp;gt;&amp;gt; Scheduler: 通知新建的未调度Pod
 Scheduler -&amp;gt;&amp;gt; API: 获取未调度的Pod信息
 Scheduler -&amp;gt;&amp;gt; Scheduler: 评估资源并分配节点
 Scheduler -&amp;gt;&amp;gt; API: 更新Pod的节点分配信息
 API -&amp;gt;&amp;gt; Etcd: 存储更新的Pod信息
 
 API -&amp;gt;&amp;gt; Kubelet: 通知分配到该节点的Pod
 Kubelet -&amp;gt;&amp;gt; API: 获取Pod信息
 Kubelet -&amp;gt;&amp;gt; CRI: 拉取并启动容器镜像
 Kubelet -&amp;gt;&amp;gt; CNI: 配置容器网络
 Kubelet -&amp;gt;&amp;gt; CSI: 挂载存储卷
 
 Kubelet -&amp;gt;&amp;gt; API: 更新Pod状态
 API -&amp;gt;&amp;gt; Etcd: 存储更新的Pod状态
 API -&amp;gt;&amp;gt; CM: 通知Pod状态变化
 CM -&amp;gt;&amp;gt; CM: 处理相关控制循环
 
 CM -&amp;gt;&amp;gt; API: 更新相关资源状态(如Deployment状态)
 API -&amp;gt;&amp;gt; Etcd: 存储更新的资源状态
&lt;/code>&lt;/pre>&lt;h1 id="kubernetes-pod-创建流程时序图详解">
 Kubernetes Pod 创建流程时序图详解
 &lt;a class="anchor" href="#kubernetes-pod-%e5%88%9b%e5%bb%ba%e6%b5%81%e7%a8%8b%e6%97%b6%e5%ba%8f%e5%9b%be%e8%af%a6%e8%a7%a3">#&lt;/a>
&lt;/h1>
&lt;p>基于我们之前讨论的 Kubernetes 时序图，以下是每个交互过程的详细解释：&lt;/p></description></item><item><title>2025-3-8 k8s删除pod或deployment的流程图详解</title><link>https://qq547475331.github.io/docs/2025-3-8-k8s%E5%88%A0%E9%99%A4pod-deployment%E7%9A%84%E6%B5%81%E7%A8%8B%E5%9B%BE%E8%AF%A6%E8%A7%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://qq547475331.github.io/docs/2025-3-8-k8s%E5%88%A0%E9%99%A4pod-deployment%E7%9A%84%E6%B5%81%E7%A8%8B%E5%9B%BE%E8%AF%A6%E8%A7%A3/</guid><description>&lt;h1 id="kubernetes-poddeployment-删除流程图">
 Kubernetes Pod/Deployment 删除流程图
 &lt;a class="anchor" href="#kubernetes-poddeployment-%e5%88%a0%e9%99%a4%e6%b5%81%e7%a8%8b%e5%9b%be">#&lt;/a>
&lt;/h1>
&lt;p>下面是 Kubernetes 中删除 Pod 或 Deployment 的完整流程序列图：&lt;/p>
&lt;pre tabindex="0">&lt;code class="language-mermaid" data-lang="mermaid">sequenceDiagram
 participant User as 用户
 participant Kubectl as Kubectl
 participant API as API Server
 participant CM as Controller Manager
 participant Etcd as etcd
 participant Kubelet as Kubelet
 participant CRI as 容器运行时
 participant CNI as 容器网络接口
 participant CSI as 容器存储接口

 User -&amp;gt;&amp;gt; Kubectl: 发起删除请求 (kubectl delete)
 Kubectl -&amp;gt;&amp;gt; API: 发送删除API请求
 
 API -&amp;gt;&amp;gt; API: 添加删除时间戳(deletionTimestamp)
 API -&amp;gt;&amp;gt; API: 设置finalizers(如有)
 API -&amp;gt;&amp;gt; Etcd: 更新资源状态为&amp;#34;正在删除&amp;#34;
 
 alt 如果是Deployment等高级资源
 API -&amp;gt;&amp;gt; CM: 通知资源正在删除
 CM -&amp;gt;&amp;gt; CM: 检查删除策略(级联/非级联)
 
 alt 级联删除(默认)
 CM -&amp;gt;&amp;gt; API: 删除所属的子资源(如ReplicaSet)
 API -&amp;gt;&amp;gt; Etcd: 更新子资源为&amp;#34;正在删除&amp;#34;
 
 CM -&amp;gt;&amp;gt; API: 删除所属的Pod
 API -&amp;gt;&amp;gt; Etcd: 更新Pod为&amp;#34;正在删除&amp;#34;
 end
 end
 
 API -&amp;gt;&amp;gt; Kubelet: 通知节点上的Pod被标记删除
 
 Kubelet -&amp;gt;&amp;gt; Kubelet: 执行Pod终止流程
 Kubelet -&amp;gt;&amp;gt; Kubelet: 执行preStop钩子(如有)
 
 Kubelet -&amp;gt;&amp;gt; CRI: 发送终止信号(SIGTERM)
 Note over CRI: 等待优雅终止期(默认30秒)
 
 alt 容器未在终止期内退出
 Kubelet -&amp;gt;&amp;gt; CRI: 发送强制终止信号(SIGKILL)
 end
 
 Kubelet -&amp;gt;&amp;gt; CRI: 删除容器
 Kubelet -&amp;gt;&amp;gt; CNI: 清理网络资源
 Kubelet -&amp;gt;&amp;gt; CSI: 卸载并清理存储卷
 
 Kubelet -&amp;gt;&amp;gt; API: 移除Pod finalizers(如有)
 Kubelet -&amp;gt;&amp;gt; API: 报告Pod已终止
 
 API -&amp;gt;&amp;gt; Etcd: 从etcd中物理删除Pod数据
 
 alt 如果是高级资源
 CM -&amp;gt;&amp;gt; API: 检查所有子资源是否已删除
 CM -&amp;gt;&amp;gt; API: 移除资源finalizers
 API -&amp;gt;&amp;gt; Etcd: 从etcd中物理删除资源数据
 end
 
 API -&amp;gt;&amp;gt; Kubectl: 返回删除完成状态
 Kubectl -&amp;gt;&amp;gt; User: 显示资源已被删除
&lt;/code>&lt;/pre>&lt;h1 id="kubernetes-poddeployment-删除流程详解">
 Kubernetes Pod/Deployment 删除流程详解
 &lt;a class="anchor" href="#kubernetes-poddeployment-%e5%88%a0%e9%99%a4%e6%b5%81%e7%a8%8b%e8%af%a6%e8%a7%a3">#&lt;/a>
&lt;/h1>
&lt;p>下面是我生成的删除 Pod/Deployment 流程图中每个步骤的详细解释：&lt;/p></description></item></channel></rss>