<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content='/*
Copyright 2016 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package deployment

import (
	"context"
	"fmt"
	"reflect"
	"sort"
	"strconv"

	apps "k8s.io/api/apps/v1"
	v1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/kubernetes/pkg/controller"
	deploymentutil "k8s.io/kubernetes/pkg/controller/deployment/util"
	labelsutil "k8s.io/kubernetes/pkg/util/labels"
)

// syncStatusOnly only updates Deployments Status and doesn&#39;t take any mutating actions.
func (dc *DeploymentController) syncStatusOnly(ctx context.Context, d *apps.Deployment, rsList []*apps.ReplicaSet) error {
	newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(ctx, d, rsList, false)
	if err != nil {
		return err
	}

	allRSs := append(oldRSs, newRS)
	return dc.syncDeploymentStatus(ctx, allRSs, newRS, d)
}

//该函数用于同步更新Deployment的状态，而不进行任何变更操作。
//它通过调用getAllReplicaSetsAndSyncRevision方法获取所有ReplicaSet的列表，并将新旧ReplicaSet区分开来。
//然后，它将所有ReplicaSet附加到旧ReplicaSet列表中，并调用syncDeploymentStatus方法来同步更新Deployment的状态。
//如果在获取ReplicaSet列表时出现错误，则返回该错误。

// sync is responsible for reconciling deployments on scaling events or when they
// are paused.
func (dc *DeploymentController) sync(ctx context.Context, d *apps.Deployment, rsList []*apps.ReplicaSet) error {
	newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(ctx, d, rsList, false)
	if err != nil {
		return err
	}
	if err := dc.scale(ctx, d, newRS, oldRSs); err != nil {
		// If we get an error while trying to scale, the deployment will be requeued
		// so we can abort this resync
		return err
	}

	// Clean up the deployment when it&#39;s paused and no rollback is in flight.
	if d.Spec.Paused && getRollbackTo(d) == nil {
		if err := dc.cleanupDeployment(ctx, oldRSs, d); err != nil {
			return err
		}
	}

	allRSs := append(oldRSs, newRS)
	return dc.syncDeploymentStatus(ctx, allRSs, newRS, d)
}

//该函数是一个Go语言函数，定义在DeploymentController结构体中，名为sync。它负责在缩放事件或暂停时协调部署。
//函数使用context.Context作为上下文，接收一个*apps.Deployment类型的参数d和一个[]*apps.ReplicaSet类型的参数rsList，返回一个error类型的值。
//函数主要执行以下操作：
//1. 调用getAllReplicaSetsAndSyncRevision方法获取所有复制集并同步修订版本，返回新复制集、旧复制集和错误（如果有）。
//2. 如果scale方法调用失败，则返回错误，以便重新排队处理。
//3. 如果部署被暂停并且没有进行中的回滚，则尝试清理部署。
//4. 将旧复制集和新复制集合并为一个列表，并调用syncDeploymentStatus方法来同步部署状态。
//最后，函数返回可能的错误。

// checkPausedConditions checks if the given deployment is paused or not and adds an appropriate condition.
// These conditions are needed so that we won&#39;t accidentally report lack of progress for resumed deployments
// that were paused for longer than progressDeadlineSeconds.
func (dc *DeploymentController) checkPausedConditions(ctx context.Context, d *apps.Deployment) error {
	if !deploymentutil.HasProgressDeadline(d) {
		return nil
	}
	cond := deploymentutil.GetDeploymentCondition(d.Status, apps.DeploymentProgressing)
	if cond != nil && cond.Reason == deploymentutil.TimedOutReason {
		// If we have reported lack of progress, do not overwrite it with a paused condition.
		return nil
	}
	pausedCondExists := cond != nil && cond.Reason == deploymentutil.PausedDeployReason

	needsUpdate := false
	if d.Spec.Paused && !pausedCondExists {
		condition := deploymentutil.NewDeploymentCondition(apps.DeploymentProgressing, v1.ConditionUnknown, deploymentutil.PausedDeployReason, "Deployment is paused")
		deploymentutil.SetDeploymentCondition(&amp;d.Status, *condition)
		needsUpdate = true
	} else if !d.Spec.Paused && pausedCondExists {
		condition := deploymentutil.NewDeploymentCondition(apps.DeploymentProgressing, v1.ConditionUnknown, deploymentutil.ResumedDeployReason, "Deployment is resumed")
		deploymentutil.SetDeploymentCondition(&amp;d.Status, *condition)
		needsUpdate = true
	}

	if !needsUpdate {
		return nil
	}

	var err error
	_, err = dc.client.AppsV1().Deployments(d.Namespace).UpdateStatus(ctx, d, metav1.UpdateOptions{})
	return err
}

//该函数用于检查给定的部署是否暂停，并添加适当的条件。
//这些条件是必要的，以避免意外报告恢复的部署缺乏进展，这些部署被暂停的时间超过了progressDeadlineSeconds。
//函数首先检查部署是否具有进度截止时间，如果没有，则返回nil。
//然后获取部署的状态，如果状态中存在DeploymentProgressing条件且原因等于TimedOutReason，则不添加暂停条件。
//接下来，函数检查部署是否暂停以及是否存在暂停条件。
//如果部署已暂停但不存在暂停条件，则创建一个新的DeploymentProgressing条件并设置其原因为PausedDeployReason。
//如果部署未暂停但存在暂停条件，则创建一个新的DeploymentProgressing条件并设置其原因为ResumedDeployReason。
//最后，如果条件有更新，则使用client更新部署的状态。函数返回更新状态时可能发生的错误。

// getAllReplicaSetsAndSyncRevision returns all the replica sets for the provided deployment (new and all old), with new RS&#39;s and deployment&#39;s revision updated.
//
// rsList should come from getReplicaSetsForDeployment(d).
//
//  1. Get all old RSes this deployment targets, and calculate the max revision number among them (maxOldV).
//  2. Get new RS this deployment targets (whose pod template matches deployment&#39;s), and update new RS&#39;s revision number to (maxOldV + 1),
//     only if its revision number is smaller than (maxOldV + 1). If this step failed, we&#39;ll update it in the next deployment sync loop.
//  3. Copy new RS&#39;s revision number to deployment (update deployment&#39;s revision). If this step failed, we&#39;ll update it in the next deployment sync loop.
//
// Note that currently the deployment controller is using caches to avoid querying the server for reads.
// This may lead to stale reads of replica sets, thus incorrect deployment status.
func (dc *DeploymentController) getAllReplicaSetsAndSyncRevision(ctx context.Context, d *apps.Deployment, rsList []*apps.ReplicaSet, createIfNotExisted bool) (*apps.ReplicaSet, []*apps.ReplicaSet, error) {
	_, allOldRSs := deploymentutil.FindOldReplicaSets(d, rsList)

	// Get new replica set with the updated revision number
	newRS, err := dc.getNewReplicaSet(ctx, d, rsList, allOldRSs, createIfNotExisted)
	if err != nil {
		return nil, nil, err
	}

	return newRS, allOldRSs, nil
}

const (
	// limit revision history length to 100 element (~2000 chars)
	maxRevHistoryLengthInChars = 2000
)

// Returns a replica set that matches the intent of the given deployment. Returns nil if the new replica set doesn&#39;t exist yet.
// 1. Get existing new RS (the RS that the given deployment targets, whose pod template is the same as deployment&#39;s).
// 2. If there&#39;s existing new RS, update its revision number if it&#39;s smaller than (maxOldRevision + 1), where maxOldRevision is the max revision number among all old RSes.
// 3. If there&#39;s no existing new RS and createIfNotExisted is true, create one with appropriate revision number (maxOldRevision + 1) and replicas.
// Note that the pod-template-hash will be added to adopted RSes and pods.
func (dc *DeploymentController) getNewReplicaSet(ctx context.Context, d *apps.Deployment, rsList, oldRSs []*apps.ReplicaSet, createIfNotExisted bool) (*apps.ReplicaSet, error) {
	logger := klog.FromContext(ctx)
	existingNewRS := deploymentutil.FindNewReplicaSet(d, rsList)

	// Calculate the max revision number among all old RSes
	maxOldRevision := deploymentutil.MaxRevision(logger, oldRSs)
	// Calculate revision number for this new replica set
	newRevision := strconv.FormatInt(maxOldRevision+1, 10)

	// Latest replica set exists. We need to sync its annotations (includes copying all but
	// annotationsToSkip from the parent deployment, and update revision, desiredReplicas,
	// and maxReplicas) and also update the revision annotation in the deployment with the
	// latest revision.
	if existingNewRS != nil {
		rsCopy := existingNewRS.DeepCopy()

		// Set existing new replica set&#39;s annotation
		annotationsUpdated := deploymentutil.SetNewReplicaSetAnnotations(ctx, d, rsCopy, newRevision, true, maxRevHistoryLengthInChars)
		minReadySecondsNeedsUpdate := rsCopy.Spec.MinReadySeconds != d.Spec.MinReadySeconds
		if annotationsUpdated || minReadySecondsNeedsUpdate {
			rsCopy.Spec.MinReadySeconds = d.Spec.MinReadySeconds
			return dc.client.AppsV1().ReplicaSets(rsCopy.ObjectMeta.Namespace).Update(ctx, rsCopy, metav1.UpdateOptions{})
		}

		// Should use the revision in existingNewRS&#39;s annotation, since it set by before
		needsUpdate := deploymentutil.SetDeploymentRevision(d, rsCopy.Annotations[deploymentutil.RevisionAnnotation])
		// If no other Progressing condition has been recorded and we need to estimate the progress
		// of this deployment then it is likely that old users started caring about progress. In that
		// case we need to take into account the first time we noticed their new replica set.
		cond := deploymentutil.GetDeploymentCondition(d.Status, apps.DeploymentProgressing)
		if deploymentutil.HasProgressDeadline(d) && cond == nil {
			msg := fmt.Sprintf("Found new replica set %q", rsCopy.Name)
			condition := deploymentutil.NewDeploymentCondition(apps.DeploymentProgressing, v1.ConditionTrue, deploymentutil.FoundNewRSReason, msg)
			deploymentutil.SetDeploymentCondition(&amp;d.Status, *condition)
			needsUpdate = true
		}

		if needsUpdate {
			var err error
			if _, err = dc.client.AppsV1().Deployments(d.Namespace).UpdateStatus(ctx, d, metav1.UpdateOptions{}); err != nil {
				return nil, err
			}
		}
		return rsCopy, nil
	}

	if !createIfNotExisted {
		return nil, nil
	}

	// new ReplicaSet does not exist, create one.
	newRSTemplate := *d.Spec.Template.DeepCopy()
	podTemplateSpecHash := controller.ComputeHash(&amp;newRSTemplate, d.Status.CollisionCount)
	newRSTemplate.Labels = labelsutil.CloneAndAddLabel(d.Spec.Template.Labels, apps.DefaultDeploymentUniqueLabelKey, podTemplateSpecHash)
	// Add podTemplateHash label to selector.
	newRSSelector := labelsutil.CloneSelectorAndAddLabel(d.Spec.Selector, apps.DefaultDeploymentUniqueLabelKey, podTemplateSpecHash)

	// Create new ReplicaSet
	newRS := apps.ReplicaSet{
		ObjectMeta: metav1.ObjectMeta{
			// Make the name deterministic, to ensure idempotence
			Name:            d.Name + "-" + podTemplateSpecHash,
			Namespace:       d.Namespace,
			OwnerReferences: []metav1.OwnerReference{*metav1.NewControllerRef(d, controllerKind)},
			Labels:          newRSTemplate.Labels,
		},
		Spec: apps.ReplicaSetSpec{
			Replicas:        new(int32),
			MinReadySeconds: d.Spec.MinReadySeconds,
			Selector:        newRSSelector,
			Template:        newRSTemplate,
		},
	}
	allRSs := append(oldRSs, &amp;newRS)
	newReplicasCount, err := deploymentutil.NewRSNewReplicas(d, allRSs, &amp;newRS)
	if err != nil {
		return nil, err
	}

	*(newRS.Spec.Replicas) = newReplicasCount
	// Set new replica set&#39;s annotation
	deploymentutil.SetNewReplicaSetAnnotations(ctx, d, &amp;newRS, newRevision, false, maxRevHistoryLengthInChars)
	// Create the new ReplicaSet. If it already exists, then we need to check for possible
	// hash collisions. If there is any other error, we need to report it in the status of
	// the Deployment.
	alreadyExists := false
	createdRS, err := dc.client.AppsV1().ReplicaSets(d.Namespace).Create(ctx, &amp;newRS, metav1.CreateOptions{})
	switch {
	// We may end up hitting this due to a slow cache or a fast resync of the Deployment.
	case errors.IsAlreadyExists(err):
		alreadyExists = true

		// Fetch a copy of the ReplicaSet.
		rs, rsErr := dc.rsLister.ReplicaSets(newRS.Namespace).Get(newRS.Name)
		if rsErr != nil {
			return nil, rsErr
		}

		// If the Deployment owns the ReplicaSet and the ReplicaSet&#39;s PodTemplateSpec is semantically
		// deep equal to the PodTemplateSpec of the Deployment, it&#39;s the Deployment&#39;s new ReplicaSet.
		// Otherwise, this is a hash collision and we need to increment the collisionCount field in
		// the status of the Deployment and requeue to try the creation in the next sync.
		controllerRef := metav1.GetControllerOf(rs)
		if controllerRef != nil && controllerRef.UID == d.UID && deploymentutil.EqualIgnoreHash(&amp;d.Spec.Template, &amp;rs.Spec.Template) {
			createdRS = rs
			err = nil
			break
		}

		// Matching ReplicaSet is not equal - increment the collisionCount in the DeploymentStatus
		// and requeue the Deployment.
		if d.Status.CollisionCount == nil {
			d.Status.CollisionCount = new(int32)
		}
		preCollisionCount := *d.Status.CollisionCount
		*d.Status.CollisionCount++
		// Update the collisionCount for the Deployment and let it requeue by returning the original
		// error.
		_, dErr := dc.client.AppsV1().Deployments(d.Namespace).UpdateStatus(ctx, d, metav1.UpdateOptions{})
		if dErr == nil {
			logger.V(2).Info("Found a hash collision for deployment - bumping collisionCount to resolve it", "deployment", klog.KObj(d), "oldCollisionCount", preCollisionCount, "newCollisionCount", *d.Status.CollisionCount)
		}
		return nil, err
	case errors.HasStatusCause(err, v1.NamespaceTerminatingCause):
		// if the namespace is terminating, all subsequent creates will fail and we can safely do nothing
		return nil, err
	case err != nil:
		msg := fmt.Sprintf("Failed to create new replica set %q: %v", newRS.Name, err)
		if deploymentutil.HasProgressDeadline(d) {
			cond := deploymentutil.NewDeploymentCondition(apps.DeploymentProgressing, v1.ConditionFalse, deploymentutil.FailedRSCreateReason, msg)
			deploymentutil.SetDeploymentCondition(&amp;d.Status, *cond)
			// We don&#39;t really care about this error at this point, since we have a bigger issue to report.
			// TODO: Identify which errors are permanent and switch DeploymentIsFailed to take into account
			// these reasons as well. Related issue: https://github.com/kubernetes/kubernetes/issues/18568
			_, _ = dc.client.AppsV1().Deployments(d.Namespace).UpdateStatus(ctx, d, metav1.UpdateOptions{})
		}
		dc.eventRecorder.Eventf(d, v1.EventTypeWarning, deploymentutil.FailedRSCreateReason, msg)
		return nil, err
	}
	if !alreadyExists && newReplicasCount > 0 {
		dc.eventRecorder.Eventf(d, v1.EventTypeNormal, "ScalingReplicaSet", "Scaled up replica set %s to %d", createdRS.Name, newReplicasCount)
	}

	needsUpdate := deploymentutil.SetDeploymentRevision(d, newRevision)
	if !alreadyExists && deploymentutil.HasProgressDeadline(d) {
		msg := fmt.Sprintf("Created new replica set %q", createdRS.Name)
		condition := deploymentutil.NewDeploymentCondition(apps.DeploymentProgressing, v1.ConditionTrue, deploymentutil.NewReplicaSetReason, msg)
		deploymentutil.SetDeploymentCondition(&amp;d.Status, *condition)
		needsUpdate = true
	}
	if needsUpdate {
		_, err = dc.client.AppsV1().Deployments(d.Namespace).UpdateStatus(ctx, d, metav1.UpdateOptions{})
	}
	return createdRS, err
}

//该函数是Go语言编写的，用于获取与给定部署相匹配的复制集（ReplicaSet）。
//如果该新的复制集不存在，且createIfNotExisted参数为true，则会创建一个新的复制集。
//函数首先查找与给定部署目标相匹配的现有新的复制集（即具有与部署相同的Pod模板）。
//然后，它会计算所有旧复制集的最大修订号（revision number），并为新的复制集计算一个适当的修订号。
//如果存在现有的新复制集，函数将更新其修订号和注释，并可能更新部署的状态。
//如果不存在新的复制集且createIfNotExisted为true，则函数将创建一个新的复制集，并更新部署的状态。
//该函数使用了context.Context来控制函数执行的上下文，使用了apps.Deployment、apps.ReplicaSet等结构体来表示部署和复制集的信息，
//使用了client来与Kubernetes API进行交互。
//函数的具体逻辑包括：
//1. 查找与给定部署相匹配的新的复制集。
//2. 计算所有旧复制集的最大修订号。
//3. 如果存在现有的新复制集，更新其修订号和注释，并可能更新部署的状态。
//4. 如果不存在新的复制集且createIfNotExisted为true，创建一个新的复制集，并更新部署的状态。
//该函数涉及的操作包括查找、更新和创建复制集，以及更新部署的状态。

// scale scales proportionally in order to mitigate risk. Otherwise, scaling up can increase the size
// of the new replica set and scaling down can decrease the sizes of the old ones, both of which would
// have the effect of hastening the rollout progress, which could produce a higher proportion of unavailable
// replicas in the event of a problem with the rolled out template. Should run only on scaling events or
// when a deployment is paused and not during the normal rollout process.
func (dc *DeploymentController) scale(ctx context.Context, deployment *apps.Deployment, newRS *apps.ReplicaSet, oldRSs []*apps.ReplicaSet) error {
	// If there is only one active replica set then we should scale that up to the full count of the
	// deployment. If there is no active replica set, then we should scale up the newest replica set.
	if activeOrLatest := deploymentutil.FindActiveOrLatest(newRS, oldRSs); activeOrLatest != nil {
		if *(activeOrLatest.Spec.Replicas) == *(deployment.Spec.Replicas) {
			return nil
		}
		_, _, err := dc.scaleReplicaSetAndRecordEvent(ctx, activeOrLatest, *(deployment.Spec.Replicas), deployment)
		return err
	}

	// If the new replica set is saturated, old replica sets should be fully scaled down.
	// This case handles replica set adoption during a saturated new replica set.
	if deploymentutil.IsSaturated(deployment, newRS) {
		for _, old := range controller.FilterActiveReplicaSets(oldRSs) {
			if _, _, err := dc.scaleReplicaSetAndRecordEvent(ctx, old, 0, deployment); err != nil {
				return err
			}
		}
		return nil
	}

	// There are old replica sets with pods and the new replica set is not saturated.
	// We need to proportionally scale all replica sets (new and old) in case of a
	// rolling deployment.
	if deploymentutil.IsRollingUpdate(deployment) {
		allRSs := controller.FilterActiveReplicaSets(append(oldRSs, newRS))
		allRSsReplicas := deploymentutil.GetReplicaCountForReplicaSets(allRSs)

		allowedSize := int32(0)
		if *(deployment.Spec.Replicas) > 0 {
			allowedSize = *(deployment.Spec.Replicas) + deploymentutil.MaxSurge(*deployment)
		}

		// Number of additional replicas that can be either added or removed from the total
		// replicas count. These replicas should be distributed proportionally to the active
		// replica sets.
		deploymentReplicasToAdd := allowedSize - allRSsReplicas

		// The additional replicas should be distributed proportionally amongst the active
		// replica sets from the larger to the smaller in size replica set. Scaling direction
		// drives what happens in case we are trying to scale replica sets of the same size.
		// In such a case when scaling up, we should scale up newer replica sets first, and
		// when scaling down, we should scale down older replica sets first.
		var scalingOperation string
		switch {
		case deploymentReplicasToAdd > 0:
			sort.Sort(controller.ReplicaSetsBySizeNewer(allRSs))
			scalingOperation = "up"

		case deploymentReplicasToAdd < 0:
			sort.Sort(controller.ReplicaSetsBySizeOlder(allRSs))
			scalingOperation = "down"
		}

		// Iterate over all active replica sets and estimate proportions for each of them.
		// The absolute value of deploymentReplicasAdded should never exceed the absolute
		// value of deploymentReplicasToAdd.
		deploymentReplicasAdded := int32(0)
		nameToSize := make(map[string]int32)
		logger := klog.FromContext(ctx)
		for i := range allRSs {
			rs := allRSs[i]

			// Estimate proportions if we have replicas to add, otherwise simply populate
			// nameToSize with the current sizes for each replica set.
			if deploymentReplicasToAdd != 0 {
				proportion := deploymentutil.GetProportion(logger, rs, *deployment, deploymentReplicasToAdd, deploymentReplicasAdded)

				nameToSize[rs.Name] = *(rs.Spec.Replicas) + proportion
				deploymentReplicasAdded += proportion
			} else {
				nameToSize[rs.Name] = *(rs.Spec.Replicas)
			}
		}

		// Update all replica sets
		for i := range allRSs {
			rs := allRSs[i]

			// Add/remove any leftovers to the largest replica set.
			if i == 0 && deploymentReplicasToAdd != 0 {
				leftover := deploymentReplicasToAdd - deploymentReplicasAdded
				nameToSize[rs.Name] = nameToSize[rs.Name] + leftover
				if nameToSize[rs.Name] < 0 {
					nameToSize[rs.Name] = 0
				}
			}

			// TODO: Use transactions when we have them.
			if _, _, err := dc.scaleReplicaSet(ctx, rs, nameToSize[rs.Name], deployment, scalingOperation); err != nil {
				// Return as soon as we fail, the deployment is requeued
				return err
			}
		}
	}
	return nil
}

//该函数是一个Go语言函数，名为scale，属于DeploymentController类型。
//它接收四个参数：ctx是一个上下文对象，deployment是一个Deployment指针，newRS是一个ReplicaSet指针，oldRSs是一个ReplicaSet指针数组。
//函数返回一个错误。
//该函数用于根据给定的部署（deployment）和新的副本集（newRS）来比例地调整副本集的大小，以减轻风险。
//函数首先检查是否有活跃或最新的副本集需要进行大小调整，如果有，则将其调整到部署的指定大小。
//如果新的副本集已饱和，则将旧的副本集完全缩放 down。
//如果部署是滚动更新类型，则按比例缩放所有副本集（新旧副本集）。
//在滚动更新的情况下，函数会根据部署的规格（deployment.Spec.Replicas）和最大突增值（MaxSurge）计算出可以添加或移除的额外副本数量，
//并将这些副本按比例分配给所有活跃的副本集。
//在缩放过程中，函数会根据副本集的大小进行排序，并根据缩放操作的类型（上/下）选择合适的排序方式。
//函数会迭代所有活跃的副本集，并根据比例计算出每个副本集应该增加或减少的副本数量。
//最后，函数会更新所有副本集的大小，并在遇到错误时返回错误。
//总结： 该函数用于根据给定的部署和新的副本集来比例地调整副本集的大小，以减轻风险。
//它会根据不同的条件来判断如何缩放副本集，并将缩放操作应用于所有活跃的副本集。
//该函数在滚动更新的情况下，会按比例分配额外的副本数量给所有活跃的副本集。

func (dc *DeploymentController) scaleReplicaSetAndRecordEvent(ctx context.Context, rs *apps.ReplicaSet, newScale int32, deployment *apps.Deployment) (bool, *apps.ReplicaSet, error) {
	// No need to scale
	if *(rs.Spec.Replicas) == newScale {
		return false, rs, nil
	}
	var scalingOperation string
	if *(rs.Spec.Replicas) < newScale {
		scalingOperation = "up"
	} else {
		scalingOperation = "down"
	}
	scaled, newRS, err := dc.scaleReplicaSet(ctx, rs, newScale, deployment, scalingOperation)
	return scaled, newRS, err
}

//该函数是一个Go语言函数，名为scaleReplicaSetAndRecordEvent，属于DeploymentController类型。
//它接收四个参数：ctx是一个上下文对象，rs是一个ReplicaSet指针，newScale是一个int32类型的变量，deployment是一个Deployment指针。
//函数返回三个值：一个布尔值，一个ReplicaSet指针和一个错误。
//函数首先检查当前ReplicaSet的副本数量是否已经等于目标副本数量newScale，如果是，则直接返回不进行任何操作。
//如果需要进行缩放操作，则根据目标副本数量与当前副本数量的关系，确定是向上扩展还是向下缩小。
//接着，函数调用dc.scaleReplicaSet方法来执行实际的缩放操作，并将缩放操作的结果返回。
//最后，函数返回缩放操作是否成功、新的ReplicaSet指针以及可能的错误信息。
//总结： 该函数用于根据给定的目标副本数量对ReplicaSet进行缩放操作，并记录相关事件。
//它会检查当前副本数量是否已经等于目标数量，如果是则不进行操作；否则，根据目标数量与当前数量的关系确定是向上扩展还是向下缩小，并执行相应的缩放操作。

func (dc *DeploymentController) scaleReplicaSet(ctx context.Context, rs *apps.ReplicaSet, newScale int32, deployment *apps.Deployment, scalingOperation string) (bool, *apps.ReplicaSet, error) {

	sizeNeedsUpdate := *(rs.Spec.Replicas) != newScale

	annotationsNeedUpdate := deploymentutil.ReplicasAnnotationsNeedUpdate(rs, *(deployment.Spec.Replicas), *(deployment.Spec.Replicas)+deploymentutil.MaxSurge(*deployment))

	scaled := false
	var err error
	if sizeNeedsUpdate || annotationsNeedUpdate {
		oldScale := *(rs.Spec.Replicas)
		rsCopy := rs.DeepCopy()
		*(rsCopy.Spec.Replicas) = newScale
		deploymentutil.SetReplicasAnnotations(rsCopy, *(deployment.Spec.Replicas), *(deployment.Spec.Replicas)+deploymentutil.MaxSurge(*deployment))
		rs, err = dc.client.AppsV1().ReplicaSets(rsCopy.Namespace).Update(ctx, rsCopy, metav1.UpdateOptions{})
		if err == nil && sizeNeedsUpdate {
			scaled = true
			dc.eventRecorder.Eventf(deployment, v1.EventTypeNormal, "ScalingReplicaSet", "Scaled %s replica set %s to %d from %d", scalingOperation, rs.Name, newScale, oldScale)
		}
	}
	return scaled, rs, err
}

//该函数用于缩放副本集的副本数量。
//它首先检查副本集的当前副本数量是否与目标副本数量不同，以及副本集的注解是否需要更新。
//如果任一条件为真，则创建一个副本集的深拷贝，并更新其副本数量和注解。
//然后，使用更新后的副本集调用客户端的Update方法来更新副本集。
//如果更新成功并且副本数量发生了变化，则记录一个事件表示副本集已被缩放。
//函数返回一个布尔值表示副本集是否被缩放，更新后的副本集对象和可能出现的错误。

// cleanupDeployment is responsible for cleaning up a deployment ie. retains all but the latest N old replica sets
// where N=d.Spec.RevisionHistoryLimit. Old replica sets are older versions of the podtemplate of a deployment kept
// around by default 1) for historical reasons and 2) for the ability to rollback a deployment.
func (dc *DeploymentController) cleanupDeployment(ctx context.Context, oldRSs []*apps.ReplicaSet, deployment *apps.Deployment) error {
	logger := klog.FromContext(ctx)
	if !deploymentutil.HasRevisionHistoryLimit(deployment) {
		return nil
	}

	// Avoid deleting replica set with deletion timestamp set
	aliveFilter := func(rs *apps.ReplicaSet) bool {
		return rs != nil && rs.ObjectMeta.DeletionTimestamp == nil
	}
	cleanableRSes := controller.FilterReplicaSets(oldRSs, aliveFilter)

	diff := int32(len(cleanableRSes)) - *deployment.Spec.RevisionHistoryLimit
	if diff <= 0 {
		return nil
	}

	sort.Sort(deploymentutil.ReplicaSetsByRevision(cleanableRSes))
	logger.V(4).Info("Looking to cleanup old replica sets for deployment", "deployment", klog.KObj(deployment))

	for i := int32(0); i < diff; i++ {
		rs := cleanableRSes[i]
		// Avoid delete replica set with non-zero replica counts
		if rs.Status.Replicas != 0 || *(rs.Spec.Replicas) != 0 || rs.Generation > rs.Status.ObservedGeneration || rs.DeletionTimestamp != nil {
			continue
		}
		logger.V(4).Info("Trying to cleanup replica set for deployment", "replicaSet", klog.KObj(rs), "deployment", klog.KObj(deployment))
		if err := dc.client.AppsV1().ReplicaSets(rs.Namespace).Delete(ctx, rs.Name, metav1.DeleteOptions{}); err != nil && !errors.IsNotFound(err) {
			// Return error instead of aggregating and continuing DELETEs on the theory
			// that we may be overloading the api server.
			return err
		}
	}

	return nil
}

//该函数用于清理部署，即保留最新N个旧的副本集，其中N等于d.Spec.RevisionHistoryLimit。
//旧的副本集是部署的pod模板的旧版本，保留它们的原因有：
//1）历史原因；
//2）为了能够回滚部署。函数首先检查部署是否设置了修订历史限制，如果没有设置，则直接返回。
//然后，过滤掉具有删除时间戳的副本集，并计算需要清理的副本集数量。
//如果需要清理的副本集数量小于等于0，则直接返回。
//接下来，按修订版本对可清理的副本集进行排序，并尝试清理旧的副本集。
//对于每个需要清理的副本集，如果其副本数量不为零，或者期望的副本数量不为零，或者其生成代数大于观察到的生成代数，或者具有删除时间戳，则跳过清理。
//最后，如果清理过程中发生错误，则返回错误。

// syncDeploymentStatus checks if the status is up-to-date and sync it if necessary
func (dc *DeploymentController) syncDeploymentStatus(ctx context.Context, allRSs []*apps.ReplicaSet, newRS *apps.ReplicaSet, d *apps.Deployment) error {
	newStatus := calculateStatus(allRSs, newRS, d)

	if reflect.DeepEqual(d.Status, newStatus) {
		return nil
	}

	newDeployment := d
	newDeployment.Status = newStatus
	_, err := dc.client.AppsV1().Deployments(newDeployment.Namespace).UpdateStatus(ctx, newDeployment, metav1.UpdateOptions{})
	return err
}

//该函数是一个Go语言函数，用于检查部署（Deployment）的状态是否是最新的，如果不是最新的，则将其同步到最新的状态。
//函数定义在DeploymentController结构体中。
//函数接受四个参数：
//- ctx：上下文对象，用于控制函数执行期间的流程。
//- allRSs：一个包含所有副本集（ReplicaSet）的切片。
//- newRS：一个新的副本集。
//- d：一个部署对象。
//函数首先调用calculateStatus函数来计算最新的状态。
//然后，它将检查当前部署的状态是否与计算出的最新状态相等。
//如果相等，函数将直接返回，不做任何操作。
//如果不相等，函数将更新部署的状态为最新状态，并调用UpdateStatus方法将其更新到Kubernetes集群中。
//最后，函数返回可能发生的错误。

// calculateStatus calculates the latest status for the provided deployment by looking into the provided replica sets.
func calculateStatus(allRSs []*apps.ReplicaSet, newRS *apps.ReplicaSet, deployment *apps.Deployment) apps.DeploymentStatus {
	availableReplicas := deploymentutil.GetAvailableReplicaCountForReplicaSets(allRSs)
	totalReplicas := deploymentutil.GetReplicaCountForReplicaSets(allRSs)
	unavailableReplicas := totalReplicas - availableReplicas
	// If unavailableReplicas is negative, then that means the Deployment has more available replicas running than
	// desired, e.g. whenever it scales down. In such a case we should simply default unavailableReplicas to zero.
	if unavailableReplicas < 0 {
		unavailableReplicas = 0
	}

	status := apps.DeploymentStatus{
		// TODO: Ensure that if we start retrying status updates, we won&#39;t pick up a new Generation value.
		ObservedGeneration:  deployment.Generation,
		Replicas:            deploymentutil.GetActualReplicaCountForReplicaSets(allRSs),
		UpdatedReplicas:     deploymentutil.GetActualReplicaCountForReplicaSets([]*apps.ReplicaSet{newRS}),
		ReadyReplicas:       deploymentutil.GetReadyReplicaCountForReplicaSets(allRSs),
		AvailableReplicas:   availableReplicas,
		UnavailableReplicas: unavailableReplicas,
		CollisionCount:      deployment.Status.CollisionCount,
	}

	// Copy conditions one by one so we won&#39;t mutate the original object.
	conditions := deployment.Status.Conditions
	for i := range conditions {
		status.Conditions = append(status.Conditions, conditions[i])
	}

	if availableReplicas >= *(deployment.Spec.Replicas)-deploymentutil.MaxUnavailable(*deployment) {
		minAvailability := deploymentutil.NewDeploymentCondition(apps.DeploymentAvailable, v1.ConditionTrue, deploymentutil.MinimumReplicasAvailable, "Deployment has minimum availability.")
		deploymentutil.SetDeploymentCondition(&amp;status, *minAvailability)
	} else {
		noMinAvailability := deploymentutil.NewDeploymentCondition(apps.DeploymentAvailable, v1.ConditionFalse, deploymentutil.MinimumReplicasUnavailable, "Deployment does not have minimum availability.")
		deploymentutil.SetDeploymentCondition(&amp;status, *noMinAvailability)
	}

	return status
}

// isScalingEvent checks whether the provided deployment has been updated with a scaling event
// by looking at the desired-replicas annotation in the active replica sets of the deployment.
//
// rsList should come from getReplicaSetsForDeployment(d).
func (dc *DeploymentController) isScalingEvent(ctx context.Context, d *apps.Deployment, rsList []*apps.ReplicaSet) (bool, error) {
	newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(ctx, d, rsList, false)
	if err != nil {
		return false, err
	}
	allRSs := append(oldRSs, newRS)
	logger := klog.FromContext(ctx)
	for _, rs := range controller.FilterActiveReplicaSets(allRSs) {
		desired, ok := deploymentutil.GetDesiredReplicasAnnotation(logger, rs)
		if !ok {
			continue
		}
		if desired != *(d.Spec.Replicas) {
			return true, nil
		}
	}
	return false, nil
}

//该函数用于检查提供的deployment是否已通过调整活动replica set的数量来进行更新。
//它通过查看deployment的desired-replicas注解来判断
//。函数首先获取deployment的所有replica set并同步修订版本号，然后遍历所有活动的replica set，
//检查其desired-replicas注解与deployment的期望副本数是否一致，如果一致则返回false，否则返回true。
'><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://qq547475331.github.io/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Bsyncgo-%E5%90%8C%E6%AD%A5-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-syncgo-tong-bu-yuan-ma-jie-du/"><meta property="og:site_name" content="Guichen's Blog"><meta property="og:title" content="2024-04-09 K8S控制器之sync.go 同步 源码解读"><meta property="og:description" content='/* Copyright 2016 The Kubernetes Authors. Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */ package deployment import ( "context" "fmt" "reflect" "sort" "strconv" apps "k8s.io/api/apps/v1" v1 "k8s.io/api/core/v1" "k8s.io/apimachinery/pkg/api/errors" metav1 "k8s.io/apimachinery/pkg/apis/meta/v1" "k8s.io/kubernetes/pkg/controller" deploymentutil "k8s.io/kubernetes/pkg/controller/deployment/util" labelsutil "k8s.io/kubernetes/pkg/util/labels" ) // syncStatusOnly only updates Deployments Status and doesn&#39;t take any mutating actions. func (dc *DeploymentController) syncStatusOnly(ctx context.Context, d *apps.Deployment, rsList []*apps.ReplicaSet) error { newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(ctx, d, rsList, false) if err != nil { return err } allRSs := append(oldRSs, newRS) return dc.syncDeploymentStatus(ctx, allRSs, newRS, d) } //该函数用于同步更新Deployment的状态，而不进行任何变更操作。 //它通过调用getAllReplicaSetsAndSyncRevision方法获取所有ReplicaSet的列表，并将新旧ReplicaSet区分开来。 //然后，它将所有ReplicaSet附加到旧ReplicaSet列表中，并调用syncDeploymentStatus方法来同步更新Deployment的状态。 //如果在获取ReplicaSet列表时出现错误，则返回该错误。 // sync is responsible for reconciling deployments on scaling events or when they // are paused. func (dc *DeploymentController) sync(ctx context.Context, d *apps.Deployment, rsList []*apps.ReplicaSet) error { newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(ctx, d, rsList, false) if err != nil { return err } if err := dc.scale(ctx, d, newRS, oldRSs); err != nil { // If we get an error while trying to scale, the deployment will be requeued // so we can abort this resync return err } // Clean up the deployment when it&#39;s paused and no rollback is in flight. if d.Spec.Paused && getRollbackTo(d) == nil { if err := dc.cleanupDeployment(ctx, oldRSs, d); err != nil { return err } } allRSs := append(oldRSs, newRS) return dc.syncDeploymentStatus(ctx, allRSs, newRS, d) } //该函数是一个Go语言函数，定义在DeploymentController结构体中，名为sync。它负责在缩放事件或暂停时协调部署。 //函数使用context.Context作为上下文，接收一个*apps.Deployment类型的参数d和一个[]*apps.ReplicaSet类型的参数rsList，返回一个error类型的值。 //函数主要执行以下操作： //1. 调用getAllReplicaSetsAndSyncRevision方法获取所有复制集并同步修订版本，返回新复制集、旧复制集和错误（如果有）。 //2. 如果scale方法调用失败，则返回错误，以便重新排队处理。 //3. 如果部署被暂停并且没有进行中的回滚，则尝试清理部署。 //4. 将旧复制集和新复制集合并为一个列表，并调用syncDeploymentStatus方法来同步部署状态。 //最后，函数返回可能的错误。 // checkPausedConditions checks if the given deployment is paused or not and adds an appropriate condition. // These conditions are needed so that we won&#39;t accidentally report lack of progress for resumed deployments // that were paused for longer than progressDeadlineSeconds. func (dc *DeploymentController) checkPausedConditions(ctx context.Context, d *apps.Deployment) error { if !deploymentutil.HasProgressDeadline(d) { return nil } cond := deploymentutil.GetDeploymentCondition(d.Status, apps.DeploymentProgressing) if cond != nil && cond.Reason == deploymentutil.TimedOutReason { // If we have reported lack of progress, do not overwrite it with a paused condition. return nil } pausedCondExists := cond != nil && cond.Reason == deploymentutil.PausedDeployReason needsUpdate := false if d.Spec.Paused && !pausedCondExists { condition := deploymentutil.NewDeploymentCondition(apps.DeploymentProgressing, v1.ConditionUnknown, deploymentutil.PausedDeployReason, "Deployment is paused") deploymentutil.SetDeploymentCondition(&amp;d.Status, *condition) needsUpdate = true } else if !d.Spec.Paused && pausedCondExists { condition := deploymentutil.NewDeploymentCondition(apps.DeploymentProgressing, v1.ConditionUnknown, deploymentutil.ResumedDeployReason, "Deployment is resumed") deploymentutil.SetDeploymentCondition(&amp;d.Status, *condition) needsUpdate = true } if !needsUpdate { return nil } var err error _, err = dc.client.AppsV1().Deployments(d.Namespace).UpdateStatus(ctx, d, metav1.UpdateOptions{}) return err } //该函数用于检查给定的部署是否暂停，并添加适当的条件。 //这些条件是必要的，以避免意外报告恢复的部署缺乏进展，这些部署被暂停的时间超过了progressDeadlineSeconds。 //函数首先检查部署是否具有进度截止时间，如果没有，则返回nil。 //然后获取部署的状态，如果状态中存在DeploymentProgressing条件且原因等于TimedOutReason，则不添加暂停条件。 //接下来，函数检查部署是否暂停以及是否存在暂停条件。 //如果部署已暂停但不存在暂停条件，则创建一个新的DeploymentProgressing条件并设置其原因为PausedDeployReason。 //如果部署未暂停但存在暂停条件，则创建一个新的DeploymentProgressing条件并设置其原因为ResumedDeployReason。 //最后，如果条件有更新，则使用client更新部署的状态。函数返回更新状态时可能发生的错误。 // getAllReplicaSetsAndSyncRevision returns all the replica sets for the provided deployment (new and all old), with new RS&#39;s and deployment&#39;s revision updated. // // rsList should come from getReplicaSetsForDeployment(d). // // 1. Get all old RSes this deployment targets, and calculate the max revision number among them (maxOldV). // 2. Get new RS this deployment targets (whose pod template matches deployment&#39;s), and update new RS&#39;s revision number to (maxOldV + 1), // only if its revision number is smaller than (maxOldV + 1). If this step failed, we&#39;ll update it in the next deployment sync loop. // 3. Copy new RS&#39;s revision number to deployment (update deployment&#39;s revision). If this step failed, we&#39;ll update it in the next deployment sync loop. // // Note that currently the deployment controller is using caches to avoid querying the server for reads. // This may lead to stale reads of replica sets, thus incorrect deployment status. func (dc *DeploymentController) getAllReplicaSetsAndSyncRevision(ctx context.Context, d *apps.Deployment, rsList []*apps.ReplicaSet, createIfNotExisted bool) (*apps.ReplicaSet, []*apps.ReplicaSet, error) { _, allOldRSs := deploymentutil.FindOldReplicaSets(d, rsList) // Get new replica set with the updated revision number newRS, err := dc.getNewReplicaSet(ctx, d, rsList, allOldRSs, createIfNotExisted) if err != nil { return nil, nil, err } return newRS, allOldRSs, nil } const ( // limit revision history length to 100 element (~2000 chars) maxRevHistoryLengthInChars = 2000 ) // Returns a replica set that matches the intent of the given deployment. Returns nil if the new replica set doesn&#39;t exist yet. // 1. Get existing new RS (the RS that the given deployment targets, whose pod template is the same as deployment&#39;s). // 2. If there&#39;s existing new RS, update its revision number if it&#39;s smaller than (maxOldRevision + 1), where maxOldRevision is the max revision number among all old RSes. // 3. If there&#39;s no existing new RS and createIfNotExisted is true, create one with appropriate revision number (maxOldRevision + 1) and replicas. // Note that the pod-template-hash will be added to adopted RSes and pods. func (dc *DeploymentController) getNewReplicaSet(ctx context.Context, d *apps.Deployment, rsList, oldRSs []*apps.ReplicaSet, createIfNotExisted bool) (*apps.ReplicaSet, error) { logger := klog.FromContext(ctx) existingNewRS := deploymentutil.FindNewReplicaSet(d, rsList) // Calculate the max revision number among all old RSes maxOldRevision := deploymentutil.MaxRevision(logger, oldRSs) // Calculate revision number for this new replica set newRevision := strconv.FormatInt(maxOldRevision+1, 10) // Latest replica set exists. We need to sync its annotations (includes copying all but // annotationsToSkip from the parent deployment, and update revision, desiredReplicas, // and maxReplicas) and also update the revision annotation in the deployment with the // latest revision. if existingNewRS != nil { rsCopy := existingNewRS.DeepCopy() // Set existing new replica set&#39;s annotation annotationsUpdated := deploymentutil.SetNewReplicaSetAnnotations(ctx, d, rsCopy, newRevision, true, maxRevHistoryLengthInChars) minReadySecondsNeedsUpdate := rsCopy.Spec.MinReadySeconds != d.Spec.MinReadySeconds if annotationsUpdated || minReadySecondsNeedsUpdate { rsCopy.Spec.MinReadySeconds = d.Spec.MinReadySeconds return dc.client.AppsV1().ReplicaSets(rsCopy.ObjectMeta.Namespace).Update(ctx, rsCopy, metav1.UpdateOptions{}) } // Should use the revision in existingNewRS&#39;s annotation, since it set by before needsUpdate := deploymentutil.SetDeploymentRevision(d, rsCopy.Annotations[deploymentutil.RevisionAnnotation]) // If no other Progressing condition has been recorded and we need to estimate the progress // of this deployment then it is likely that old users started caring about progress. In that // case we need to take into account the first time we noticed their new replica set. cond := deploymentutil.GetDeploymentCondition(d.Status, apps.DeploymentProgressing) if deploymentutil.HasProgressDeadline(d) && cond == nil { msg := fmt.Sprintf("Found new replica set %q", rsCopy.Name) condition := deploymentutil.NewDeploymentCondition(apps.DeploymentProgressing, v1.ConditionTrue, deploymentutil.FoundNewRSReason, msg) deploymentutil.SetDeploymentCondition(&amp;d.Status, *condition) needsUpdate = true } if needsUpdate { var err error if _, err = dc.client.AppsV1().Deployments(d.Namespace).UpdateStatus(ctx, d, metav1.UpdateOptions{}); err != nil { return nil, err } } return rsCopy, nil } if !createIfNotExisted { return nil, nil } // new ReplicaSet does not exist, create one. newRSTemplate := *d.Spec.Template.DeepCopy() podTemplateSpecHash := controller.ComputeHash(&amp;newRSTemplate, d.Status.CollisionCount) newRSTemplate.Labels = labelsutil.CloneAndAddLabel(d.Spec.Template.Labels, apps.DefaultDeploymentUniqueLabelKey, podTemplateSpecHash) // Add podTemplateHash label to selector. newRSSelector := labelsutil.CloneSelectorAndAddLabel(d.Spec.Selector, apps.DefaultDeploymentUniqueLabelKey, podTemplateSpecHash) // Create new ReplicaSet newRS := apps.ReplicaSet{ ObjectMeta: metav1.ObjectMeta{ // Make the name deterministic, to ensure idempotence Name: d.Name + "-" + podTemplateSpecHash, Namespace: d.Namespace, OwnerReferences: []metav1.OwnerReference{*metav1.NewControllerRef(d, controllerKind)}, Labels: newRSTemplate.Labels, }, Spec: apps.ReplicaSetSpec{ Replicas: new(int32), MinReadySeconds: d.Spec.MinReadySeconds, Selector: newRSSelector, Template: newRSTemplate, }, } allRSs := append(oldRSs, &amp;newRS) newReplicasCount, err := deploymentutil.NewRSNewReplicas(d, allRSs, &amp;newRS) if err != nil { return nil, err } *(newRS.Spec.Replicas) = newReplicasCount // Set new replica set&#39;s annotation deploymentutil.SetNewReplicaSetAnnotations(ctx, d, &amp;newRS, newRevision, false, maxRevHistoryLengthInChars) // Create the new ReplicaSet. If it already exists, then we need to check for possible // hash collisions. If there is any other error, we need to report it in the status of // the Deployment. alreadyExists := false createdRS, err := dc.client.AppsV1().ReplicaSets(d.Namespace).Create(ctx, &amp;newRS, metav1.CreateOptions{}) switch { // We may end up hitting this due to a slow cache or a fast resync of the Deployment. case errors.IsAlreadyExists(err): alreadyExists = true // Fetch a copy of the ReplicaSet. rs, rsErr := dc.rsLister.ReplicaSets(newRS.Namespace).Get(newRS.Name) if rsErr != nil { return nil, rsErr } // If the Deployment owns the ReplicaSet and the ReplicaSet&#39;s PodTemplateSpec is semantically // deep equal to the PodTemplateSpec of the Deployment, it&#39;s the Deployment&#39;s new ReplicaSet. // Otherwise, this is a hash collision and we need to increment the collisionCount field in // the status of the Deployment and requeue to try the creation in the next sync. controllerRef := metav1.GetControllerOf(rs) if controllerRef != nil && controllerRef.UID == d.UID && deploymentutil.EqualIgnoreHash(&amp;d.Spec.Template, &amp;rs.Spec.Template) { createdRS = rs err = nil break } // Matching ReplicaSet is not equal - increment the collisionCount in the DeploymentStatus // and requeue the Deployment. if d.Status.CollisionCount == nil { d.Status.CollisionCount = new(int32) } preCollisionCount := *d.Status.CollisionCount *d.Status.CollisionCount++ // Update the collisionCount for the Deployment and let it requeue by returning the original // error. _, dErr := dc.client.AppsV1().Deployments(d.Namespace).UpdateStatus(ctx, d, metav1.UpdateOptions{}) if dErr == nil { logger.V(2).Info("Found a hash collision for deployment - bumping collisionCount to resolve it", "deployment", klog.KObj(d), "oldCollisionCount", preCollisionCount, "newCollisionCount", *d.Status.CollisionCount) } return nil, err case errors.HasStatusCause(err, v1.NamespaceTerminatingCause): // if the namespace is terminating, all subsequent creates will fail and we can safely do nothing return nil, err case err != nil: msg := fmt.Sprintf("Failed to create new replica set %q: %v", newRS.Name, err) if deploymentutil.HasProgressDeadline(d) { cond := deploymentutil.NewDeploymentCondition(apps.DeploymentProgressing, v1.ConditionFalse, deploymentutil.FailedRSCreateReason, msg) deploymentutil.SetDeploymentCondition(&amp;d.Status, *cond) // We don&#39;t really care about this error at this point, since we have a bigger issue to report. // TODO: Identify which errors are permanent and switch DeploymentIsFailed to take into account // these reasons as well. Related issue: https://github.com/kubernetes/kubernetes/issues/18568 _, _ = dc.client.AppsV1().Deployments(d.Namespace).UpdateStatus(ctx, d, metav1.UpdateOptions{}) } dc.eventRecorder.Eventf(d, v1.EventTypeWarning, deploymentutil.FailedRSCreateReason, msg) return nil, err } if !alreadyExists && newReplicasCount > 0 { dc.eventRecorder.Eventf(d, v1.EventTypeNormal, "ScalingReplicaSet", "Scaled up replica set %s to %d", createdRS.Name, newReplicasCount) } needsUpdate := deploymentutil.SetDeploymentRevision(d, newRevision) if !alreadyExists && deploymentutil.HasProgressDeadline(d) { msg := fmt.Sprintf("Created new replica set %q", createdRS.Name) condition := deploymentutil.NewDeploymentCondition(apps.DeploymentProgressing, v1.ConditionTrue, deploymentutil.NewReplicaSetReason, msg) deploymentutil.SetDeploymentCondition(&amp;d.Status, *condition) needsUpdate = true } if needsUpdate { _, err = dc.client.AppsV1().Deployments(d.Namespace).UpdateStatus(ctx, d, metav1.UpdateOptions{}) } return createdRS, err } //该函数是Go语言编写的，用于获取与给定部署相匹配的复制集（ReplicaSet）。 //如果该新的复制集不存在，且createIfNotExisted参数为true，则会创建一个新的复制集。 //函数首先查找与给定部署目标相匹配的现有新的复制集（即具有与部署相同的Pod模板）。 //然后，它会计算所有旧复制集的最大修订号（revision number），并为新的复制集计算一个适当的修订号。 //如果存在现有的新复制集，函数将更新其修订号和注释，并可能更新部署的状态。 //如果不存在新的复制集且createIfNotExisted为true，则函数将创建一个新的复制集，并更新部署的状态。 //该函数使用了context.Context来控制函数执行的上下文，使用了apps.Deployment、apps.ReplicaSet等结构体来表示部署和复制集的信息， //使用了client来与Kubernetes API进行交互。 //函数的具体逻辑包括： //1. 查找与给定部署相匹配的新的复制集。 //2. 计算所有旧复制集的最大修订号。 //3. 如果存在现有的新复制集，更新其修订号和注释，并可能更新部署的状态。 //4. 如果不存在新的复制集且createIfNotExisted为true，创建一个新的复制集，并更新部署的状态。 //该函数涉及的操作包括查找、更新和创建复制集，以及更新部署的状态。 // scale scales proportionally in order to mitigate risk. Otherwise, scaling up can increase the size // of the new replica set and scaling down can decrease the sizes of the old ones, both of which would // have the effect of hastening the rollout progress, which could produce a higher proportion of unavailable // replicas in the event of a problem with the rolled out template. Should run only on scaling events or // when a deployment is paused and not during the normal rollout process. func (dc *DeploymentController) scale(ctx context.Context, deployment *apps.Deployment, newRS *apps.ReplicaSet, oldRSs []*apps.ReplicaSet) error { // If there is only one active replica set then we should scale that up to the full count of the // deployment. If there is no active replica set, then we should scale up the newest replica set. if activeOrLatest := deploymentutil.FindActiveOrLatest(newRS, oldRSs); activeOrLatest != nil { if *(activeOrLatest.Spec.Replicas) == *(deployment.Spec.Replicas) { return nil } _, _, err := dc.scaleReplicaSetAndRecordEvent(ctx, activeOrLatest, *(deployment.Spec.Replicas), deployment) return err } // If the new replica set is saturated, old replica sets should be fully scaled down. // This case handles replica set adoption during a saturated new replica set. if deploymentutil.IsSaturated(deployment, newRS) { for _, old := range controller.FilterActiveReplicaSets(oldRSs) { if _, _, err := dc.scaleReplicaSetAndRecordEvent(ctx, old, 0, deployment); err != nil { return err } } return nil } // There are old replica sets with pods and the new replica set is not saturated. // We need to proportionally scale all replica sets (new and old) in case of a // rolling deployment. if deploymentutil.IsRollingUpdate(deployment) { allRSs := controller.FilterActiveReplicaSets(append(oldRSs, newRS)) allRSsReplicas := deploymentutil.GetReplicaCountForReplicaSets(allRSs) allowedSize := int32(0) if *(deployment.Spec.Replicas) > 0 { allowedSize = *(deployment.Spec.Replicas) + deploymentutil.MaxSurge(*deployment) } // Number of additional replicas that can be either added or removed from the total // replicas count. These replicas should be distributed proportionally to the active // replica sets. deploymentReplicasToAdd := allowedSize - allRSsReplicas // The additional replicas should be distributed proportionally amongst the active // replica sets from the larger to the smaller in size replica set. Scaling direction // drives what happens in case we are trying to scale replica sets of the same size. // In such a case when scaling up, we should scale up newer replica sets first, and // when scaling down, we should scale down older replica sets first. var scalingOperation string switch { case deploymentReplicasToAdd > 0: sort.Sort(controller.ReplicaSetsBySizeNewer(allRSs)) scalingOperation = "up" case deploymentReplicasToAdd < 0: sort.Sort(controller.ReplicaSetsBySizeOlder(allRSs)) scalingOperation = "down" } // Iterate over all active replica sets and estimate proportions for each of them. // The absolute value of deploymentReplicasAdded should never exceed the absolute // value of deploymentReplicasToAdd. deploymentReplicasAdded := int32(0) nameToSize := make(map[string]int32) logger := klog.FromContext(ctx) for i := range allRSs { rs := allRSs[i] // Estimate proportions if we have replicas to add, otherwise simply populate // nameToSize with the current sizes for each replica set. if deploymentReplicasToAdd != 0 { proportion := deploymentutil.GetProportion(logger, rs, *deployment, deploymentReplicasToAdd, deploymentReplicasAdded) nameToSize[rs.Name] = *(rs.Spec.Replicas) + proportion deploymentReplicasAdded += proportion } else { nameToSize[rs.Name] = *(rs.Spec.Replicas) } } // Update all replica sets for i := range allRSs { rs := allRSs[i] // Add/remove any leftovers to the largest replica set. if i == 0 && deploymentReplicasToAdd != 0 { leftover := deploymentReplicasToAdd - deploymentReplicasAdded nameToSize[rs.Name] = nameToSize[rs.Name] + leftover if nameToSize[rs.Name] < 0 { nameToSize[rs.Name] = 0 } } // TODO: Use transactions when we have them. if _, _, err := dc.scaleReplicaSet(ctx, rs, nameToSize[rs.Name], deployment, scalingOperation); err != nil { // Return as soon as we fail, the deployment is requeued return err } } } return nil } //该函数是一个Go语言函数，名为scale，属于DeploymentController类型。 //它接收四个参数：ctx是一个上下文对象，deployment是一个Deployment指针，newRS是一个ReplicaSet指针，oldRSs是一个ReplicaSet指针数组。 //函数返回一个错误。 //该函数用于根据给定的部署（deployment）和新的副本集（newRS）来比例地调整副本集的大小，以减轻风险。 //函数首先检查是否有活跃或最新的副本集需要进行大小调整，如果有，则将其调整到部署的指定大小。 //如果新的副本集已饱和，则将旧的副本集完全缩放 down。 //如果部署是滚动更新类型，则按比例缩放所有副本集（新旧副本集）。 //在滚动更新的情况下，函数会根据部署的规格（deployment.Spec.Replicas）和最大突增值（MaxSurge）计算出可以添加或移除的额外副本数量， //并将这些副本按比例分配给所有活跃的副本集。 //在缩放过程中，函数会根据副本集的大小进行排序，并根据缩放操作的类型（上/下）选择合适的排序方式。 //函数会迭代所有活跃的副本集，并根据比例计算出每个副本集应该增加或减少的副本数量。 //最后，函数会更新所有副本集的大小，并在遇到错误时返回错误。 //总结： 该函数用于根据给定的部署和新的副本集来比例地调整副本集的大小，以减轻风险。 //它会根据不同的条件来判断如何缩放副本集，并将缩放操作应用于所有活跃的副本集。 //该函数在滚动更新的情况下，会按比例分配额外的副本数量给所有活跃的副本集。 func (dc *DeploymentController) scaleReplicaSetAndRecordEvent(ctx context.Context, rs *apps.ReplicaSet, newScale int32, deployment *apps.Deployment) (bool, *apps.ReplicaSet, error) { // No need to scale if *(rs.Spec.Replicas) == newScale { return false, rs, nil } var scalingOperation string if *(rs.Spec.Replicas) < newScale { scalingOperation = "up" } else { scalingOperation = "down" } scaled, newRS, err := dc.scaleReplicaSet(ctx, rs, newScale, deployment, scalingOperation) return scaled, newRS, err } //该函数是一个Go语言函数，名为scaleReplicaSetAndRecordEvent，属于DeploymentController类型。 //它接收四个参数：ctx是一个上下文对象，rs是一个ReplicaSet指针，newScale是一个int32类型的变量，deployment是一个Deployment指针。 //函数返回三个值：一个布尔值，一个ReplicaSet指针和一个错误。 //函数首先检查当前ReplicaSet的副本数量是否已经等于目标副本数量newScale，如果是，则直接返回不进行任何操作。 //如果需要进行缩放操作，则根据目标副本数量与当前副本数量的关系，确定是向上扩展还是向下缩小。 //接着，函数调用dc.scaleReplicaSet方法来执行实际的缩放操作，并将缩放操作的结果返回。 //最后，函数返回缩放操作是否成功、新的ReplicaSet指针以及可能的错误信息。 //总结： 该函数用于根据给定的目标副本数量对ReplicaSet进行缩放操作，并记录相关事件。 //它会检查当前副本数量是否已经等于目标数量，如果是则不进行操作；否则，根据目标数量与当前数量的关系确定是向上扩展还是向下缩小，并执行相应的缩放操作。 func (dc *DeploymentController) scaleReplicaSet(ctx context.Context, rs *apps.ReplicaSet, newScale int32, deployment *apps.Deployment, scalingOperation string) (bool, *apps.ReplicaSet, error) { sizeNeedsUpdate := *(rs.Spec.Replicas) != newScale annotationsNeedUpdate := deploymentutil.ReplicasAnnotationsNeedUpdate(rs, *(deployment.Spec.Replicas), *(deployment.Spec.Replicas)+deploymentutil.MaxSurge(*deployment)) scaled := false var err error if sizeNeedsUpdate || annotationsNeedUpdate { oldScale := *(rs.Spec.Replicas) rsCopy := rs.DeepCopy() *(rsCopy.Spec.Replicas) = newScale deploymentutil.SetReplicasAnnotations(rsCopy, *(deployment.Spec.Replicas), *(deployment.Spec.Replicas)+deploymentutil.MaxSurge(*deployment)) rs, err = dc.client.AppsV1().ReplicaSets(rsCopy.Namespace).Update(ctx, rsCopy, metav1.UpdateOptions{}) if err == nil && sizeNeedsUpdate { scaled = true dc.eventRecorder.Eventf(deployment, v1.EventTypeNormal, "ScalingReplicaSet", "Scaled %s replica set %s to %d from %d", scalingOperation, rs.Name, newScale, oldScale) } } return scaled, rs, err } //该函数用于缩放副本集的副本数量。 //它首先检查副本集的当前副本数量是否与目标副本数量不同，以及副本集的注解是否需要更新。 //如果任一条件为真，则创建一个副本集的深拷贝，并更新其副本数量和注解。 //然后，使用更新后的副本集调用客户端的Update方法来更新副本集。 //如果更新成功并且副本数量发生了变化，则记录一个事件表示副本集已被缩放。 //函数返回一个布尔值表示副本集是否被缩放，更新后的副本集对象和可能出现的错误。 // cleanupDeployment is responsible for cleaning up a deployment ie. retains all but the latest N old replica sets // where N=d.Spec.RevisionHistoryLimit. Old replica sets are older versions of the podtemplate of a deployment kept // around by default 1) for historical reasons and 2) for the ability to rollback a deployment. func (dc *DeploymentController) cleanupDeployment(ctx context.Context, oldRSs []*apps.ReplicaSet, deployment *apps.Deployment) error { logger := klog.FromContext(ctx) if !deploymentutil.HasRevisionHistoryLimit(deployment) { return nil } // Avoid deleting replica set with deletion timestamp set aliveFilter := func(rs *apps.ReplicaSet) bool { return rs != nil && rs.ObjectMeta.DeletionTimestamp == nil } cleanableRSes := controller.FilterReplicaSets(oldRSs, aliveFilter) diff := int32(len(cleanableRSes)) - *deployment.Spec.RevisionHistoryLimit if diff <= 0 { return nil } sort.Sort(deploymentutil.ReplicaSetsByRevision(cleanableRSes)) logger.V(4).Info("Looking to cleanup old replica sets for deployment", "deployment", klog.KObj(deployment)) for i := int32(0); i < diff; i++ { rs := cleanableRSes[i] // Avoid delete replica set with non-zero replica counts if rs.Status.Replicas != 0 || *(rs.Spec.Replicas) != 0 || rs.Generation > rs.Status.ObservedGeneration || rs.DeletionTimestamp != nil { continue } logger.V(4).Info("Trying to cleanup replica set for deployment", "replicaSet", klog.KObj(rs), "deployment", klog.KObj(deployment)) if err := dc.client.AppsV1().ReplicaSets(rs.Namespace).Delete(ctx, rs.Name, metav1.DeleteOptions{}); err != nil && !errors.IsNotFound(err) { // Return error instead of aggregating and continuing DELETEs on the theory // that we may be overloading the api server. return err } } return nil } //该函数用于清理部署，即保留最新N个旧的副本集，其中N等于d.Spec.RevisionHistoryLimit。 //旧的副本集是部署的pod模板的旧版本，保留它们的原因有： //1）历史原因； //2）为了能够回滚部署。函数首先检查部署是否设置了修订历史限制，如果没有设置，则直接返回。 //然后，过滤掉具有删除时间戳的副本集，并计算需要清理的副本集数量。 //如果需要清理的副本集数量小于等于0，则直接返回。 //接下来，按修订版本对可清理的副本集进行排序，并尝试清理旧的副本集。 //对于每个需要清理的副本集，如果其副本数量不为零，或者期望的副本数量不为零，或者其生成代数大于观察到的生成代数，或者具有删除时间戳，则跳过清理。 //最后，如果清理过程中发生错误，则返回错误。 // syncDeploymentStatus checks if the status is up-to-date and sync it if necessary func (dc *DeploymentController) syncDeploymentStatus(ctx context.Context, allRSs []*apps.ReplicaSet, newRS *apps.ReplicaSet, d *apps.Deployment) error { newStatus := calculateStatus(allRSs, newRS, d) if reflect.DeepEqual(d.Status, newStatus) { return nil } newDeployment := d newDeployment.Status = newStatus _, err := dc.client.AppsV1().Deployments(newDeployment.Namespace).UpdateStatus(ctx, newDeployment, metav1.UpdateOptions{}) return err } //该函数是一个Go语言函数，用于检查部署（Deployment）的状态是否是最新的，如果不是最新的，则将其同步到最新的状态。 //函数定义在DeploymentController结构体中。 //函数接受四个参数： //- ctx：上下文对象，用于控制函数执行期间的流程。 //- allRSs：一个包含所有副本集（ReplicaSet）的切片。 //- newRS：一个新的副本集。 //- d：一个部署对象。 //函数首先调用calculateStatus函数来计算最新的状态。 //然后，它将检查当前部署的状态是否与计算出的最新状态相等。 //如果相等，函数将直接返回，不做任何操作。 //如果不相等，函数将更新部署的状态为最新状态，并调用UpdateStatus方法将其更新到Kubernetes集群中。 //最后，函数返回可能发生的错误。 // calculateStatus calculates the latest status for the provided deployment by looking into the provided replica sets. func calculateStatus(allRSs []*apps.ReplicaSet, newRS *apps.ReplicaSet, deployment *apps.Deployment) apps.DeploymentStatus { availableReplicas := deploymentutil.GetAvailableReplicaCountForReplicaSets(allRSs) totalReplicas := deploymentutil.GetReplicaCountForReplicaSets(allRSs) unavailableReplicas := totalReplicas - availableReplicas // If unavailableReplicas is negative, then that means the Deployment has more available replicas running than // desired, e.g. whenever it scales down. In such a case we should simply default unavailableReplicas to zero. if unavailableReplicas < 0 { unavailableReplicas = 0 } status := apps.DeploymentStatus{ // TODO: Ensure that if we start retrying status updates, we won&#39;t pick up a new Generation value. ObservedGeneration: deployment.Generation, Replicas: deploymentutil.GetActualReplicaCountForReplicaSets(allRSs), UpdatedReplicas: deploymentutil.GetActualReplicaCountForReplicaSets([]*apps.ReplicaSet{newRS}), ReadyReplicas: deploymentutil.GetReadyReplicaCountForReplicaSets(allRSs), AvailableReplicas: availableReplicas, UnavailableReplicas: unavailableReplicas, CollisionCount: deployment.Status.CollisionCount, } // Copy conditions one by one so we won&#39;t mutate the original object. conditions := deployment.Status.Conditions for i := range conditions { status.Conditions = append(status.Conditions, conditions[i]) } if availableReplicas >= *(deployment.Spec.Replicas)-deploymentutil.MaxUnavailable(*deployment) { minAvailability := deploymentutil.NewDeploymentCondition(apps.DeploymentAvailable, v1.ConditionTrue, deploymentutil.MinimumReplicasAvailable, "Deployment has minimum availability.") deploymentutil.SetDeploymentCondition(&amp;status, *minAvailability) } else { noMinAvailability := deploymentutil.NewDeploymentCondition(apps.DeploymentAvailable, v1.ConditionFalse, deploymentutil.MinimumReplicasUnavailable, "Deployment does not have minimum availability.") deploymentutil.SetDeploymentCondition(&amp;status, *noMinAvailability) } return status } // isScalingEvent checks whether the provided deployment has been updated with a scaling event // by looking at the desired-replicas annotation in the active replica sets of the deployment. // // rsList should come from getReplicaSetsForDeployment(d). func (dc *DeploymentController) isScalingEvent(ctx context.Context, d *apps.Deployment, rsList []*apps.ReplicaSet) (bool, error) { newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(ctx, d, rsList, false) if err != nil { return false, err } allRSs := append(oldRSs, newRS) logger := klog.FromContext(ctx) for _, rs := range controller.FilterActiveReplicaSets(allRSs) { desired, ok := deploymentutil.GetDesiredReplicasAnnotation(logger, rs) if !ok { continue } if desired != *(d.Spec.Replicas) { return true, nil } } return false, nil } //该函数用于检查提供的deployment是否已通过调整活动replica set的数量来进行更新。 //它通过查看deployment的desired-replicas注解来判断 //。函数首先获取deployment的所有replica set并同步修订版本号，然后遍历所有活动的replica set， //检查其desired-replicas注解与deployment的期望副本数是否一致，如果一致则返回false，否则返回true。'><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><title>2024-04-09 K8S控制器之sync.go 同步 源码解读 | Guichen's Blog</title>
<link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://qq547475331.github.io/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Bsyncgo-%E5%90%8C%E6%AD%A5-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-syncgo-tong-bu-yuan-ma-jie-du/><link rel=stylesheet href=/book.min.6c8b9d2a1fc95075ed7da46ca81060b39add8fff6741ac51259f768929281e2c.css integrity="sha256-bIudKh/JUHXtfaRsqBBgs5rdj/9nQaxRJZ92iSkoHiw=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.9b03c2d39fa09197c1a6934f1c803e970e02db0937daf518ecfb6afc42bf5919.js integrity="sha256-mwPC05+gkZfBppNPHIA+lw4C2wk32vUY7Ptq/EK/WRk=" crossorigin=anonymous></script></head><script src=https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.2.3/mermaid.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){mermaid.initialize({startOnLoad:!0});let e=document.querySelectorAll("pre > code.language-mermaid");e.forEach(e=>{let t=document.createElement("div");t.classList.add("mermaid"),t.innerHTML=e.innerText,e.parentNode.replaceWith(t)}),mermaid.init(void 0,document.querySelectorAll(".mermaid"))})</script><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>Guichen's Blog</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><a href=/docs/2025-11-30-kubernetes%E8%AE%BE%E8%AE%A1%E7%90%86%E5%BF%B5/>2025-11-30 Kubernetes设计理念</a></li><li><a href=/docs/2025-10-27-reconciler%E6%A8%A1%E5%BC%8F/>2025-10-27 informer模式3</a></li><li><a href=/docs/2025-10-23-informer3/>2025-10-23 informer模式3</a></li><li><a href=/docs/2025-10-23-informer2/>2025-10-23 informer模式2</a></li><li><a href=/docs/2025-10-23-informer/>2025-10-23 informer模式</a></li><li><a href=/docs/2025-9-29-vercel%E5%88%9B%E5%A7%8B%E4%BA%BA%E5%AF%B9%E8%AF%9D/>2025-09-29 Vercel创始人对话</a></li><li><a href=/docs/2025-9-28-spec-workflow-mcp/>2025-09-28 spec-workflow-mcp</a></li><li><a href=/docs/2025-9-28-chrome-mcp-tools/>2025-09-28 chrome-devtools-mcp</a></li><li><a href=/docs/2025-9-19-%E7%90%86%E8%A7%A3/>2025-09-19 理解</a></li><li><a href=/docs/2025-9-10-%E6%8F%90%E7%A4%BA%E8%AF%8D/>2025-09-10 提示词</a></li><li><a href=/docs/2025-9-9-music/>2025-09-09 music资源</a></li><li><a href=/docs/2025-8-29-%E8%A1%A8%E5%8D%95%E5%88%B0%E9%9B%86%E7%BE%A4/>2025-08-28 表单到集群</a></li><li><a href=/docs/2025-6-27-geminicli/>2025-06-27 geminicli</a></li><li><a href=/docs/2025-6-23-ingress-nginx-contrller-%E5%88%86%E6%9E%90/>2025-06-23 ingress nginx contrller 内存使用过高分析</a></li><li><a href=/docs/2025-6-20-oom/>2025-06-20 oom排查思路</a></li><li><a href=/docs/2025-6-16-fire%E8%A7%84%E5%88%99/>2025-06-16 Cursor RIPER-5规则</a></li><li><a href=/docs/2025-6-12-karmada/>2025-06-12 karmada介绍</a></li><li><a href=/docs/2025-6-12-flutter%E8%A7%84%E5%88%99/>2025-06-12 flutter规则</a></li><li><a href=/docs/2025-6-10-%E7%8B%AC%E7%AB%8B%E5%BC%80%E5%8F%91/>2025-06-10 独立开发</a></li><li><a href=/docs/2025-5-21-ingress%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/>2025-05-21 主Ingress副本变为0后报503问题分析</a></li><li><a href=/docs/2025-5-7-%E6%8E%A5%E5%8D%95app/>2025-05-07 接单app设计</a></li><li><a href=/docs/2025-5-7-%E5%A5%BD%E5%BF%83%E6%80%81-app/>2025-05-07 好心态app</a></li><li><a href=/docs/2025-4-28-cursor-agent-%E6%8F%90%E7%A4%BA%E5%99%A8/>2025-04-28 cursor agent 提示器</a></li><li><a href=/docs/2025-4-20-%E6%80%A7%E5%90%8C%E6%84%8Fapp/>2025-04-20 性同意App</a></li><li><a href=/docs/2025-4-16-%E8%87%AA%E7%A0%94k8s%E5%B9%B3%E5%8F%B0/>2025-04-16 自研k8s平台</a></li><li><a href=/docs/2025-4-16-sleep%E7%9D%A1%E7%9C%A0%E5%BA%94%E7%94%A8/>2025-04-16 sleep睡眠应用</a></li><li><a href=/docs/2025-4-16-paas%E8%AE%BE%E8%AE%A1/>2025-04-16 paas开发记录</a></li><li><a href=/docs/2025-4-16-cursoe-free-vip/>2025-04-16 Cursor Free VIP</a></li><li><a href=/docs/2025-4-16-boss%E7%9B%B4%E8%81%98%E8%87%AA%E5%8A%A8%E6%8A%95%E9%80%92/>2025-04-16 BOSS直聘自动投递</a></li><li><a href=/docs/2025-4-14-github%E6%8E%A8%E9%80%81/>2025-04-14 github推送</a></li><li><a href=/docs/2025-3-30-metallb/>2025-03-30 metallb</a></li><li><a href=/docs/2025-3-24-%E8%87%AA%E6%88%91%E4%BB%8B%E7%BB%8D/>2025-03-24 自我介绍</a></li><li><a href=/docs/2025-3-20-victoriametrics-%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84/>2025-03-20 victoriametrics高可用架构</a></li><li><a href=/docs/2025-3-20-victoriametrics%E6%9E%B6%E6%9E%84/>2025-03-20 victoriametrics 架构</a></li><li><a href=/docs/2025-3-20-victoriametrics%E5%92%8Cthanos%E5%AF%B9%E6%AF%94/>2025-03-20 VictoriaMetrics 和 Thanos 对比</a></li><li><a href=/docs/2025-3-20-thanos%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84/>2025-03-20 thanos高可用架构</a></li><li><a href=/docs/2025-3-20-thanos%E6%9E%B6%E6%9E%84/>2025-03-20 thanos架构</a></li><li><a href=/docs/2025-3-18-5w-pod%E5%8E%8B%E6%B5%8B%E5%A4%8D%E7%9B%98/>2025-03-18 5w pod压测复盘</a></li><li><a href=/docs/2025-3-14-%E7%81%AB%E5%B1%B1%E4%BA%91%E8%BF%81%E7%A7%BB%E5%B7%A5%E7%A8%8B%E5%B8%88%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/>2025-03-14 火山云迁移工程师面试记录</a></li><li><a href=/docs/2025-3-14-vivo%E9%9D%A2%E8%AF%95/>2025-03-14 vivo面试</a></li><li><a href=/docs/2025-3-13-istio%E6%B5%81%E9%87%8F%E5%88%86%E6%9E%90/>2025-03-13 istio流量分析</a></li><li><a href=/docs/2025-3-13-calico%E4%B8%89%E7%A7%8D%E6%A8%A1%E5%BC%8F%E4%B8%8B%E6%B5%81%E9%87%8F%E4%BC%A0%E8%BE%93%E8%B7%AF%E5%BE%84%E5%88%86%E6%9E%90/>2025-03-13 calico三种模式下流量传输</a></li><li><a href=/docs/2025-3-12-%E5%A1%94%E8%B5%9E%E9%9D%A2%E8%AF%95/>2025-03-12 塔赞面试</a></li><li><a href=/docs/2025-3-12-%E8%BF%BD%E8%A7%85%E9%9D%A2%E8%AF%95/>2025-03-12 追觅面试</a></li><li><a href=/docs/2025-3-8-k8s%E5%88%A0%E9%99%A4pod-deployment%E7%9A%84%E6%B5%81%E7%A8%8B%E5%9B%BE%E8%AF%A6%E8%A7%A3/>2025-03-08 k8s删除pod或deployment的流程图详解</a></li><li><a href=/docs/2025-3-8-k8s%E5%88%9B%E5%BB%BApod-deployment%E6%B5%81%E7%A8%8B%E5%9B%BE%E8%AF%A6%E8%A7%A3/>2025-03-08 k8s创建pod流程图详解</a></li><li><a href=/docs/2025-2-28-prometheus%E9%A2%98%E7%9B%AE/>2025-02-28 prometheus面试题</a></li><li><a href=/docs/2025-2-26-%E9%9D%A2%E8%AF%950225/>2025-02-25 面试0225</a></li><li><a href=/docs/2025-2-24-%E9%AB%98%E7%BA%A7%E8%BF%90%E7%BB%B4%E9%9D%A2%E8%AF%95%E9%A2%98_ai_linux%E9%83%A8%E5%88%86/>2025-02-24 高级运维面试题-linux部分</a></li><li><a href=/docs/2025-2-24-%E4%B8%AD%E7%BA%A7%E8%BF%90%E7%BB%B4%E9%9D%A2%E8%AF%95%E9%A2%98_%E9%A2%98%E7%9B%AE/>2025-02-24 中级运维面试题</a></li><li><a href=/docs/2025-2-24-%E9%9D%A2%E8%AF%950224/>2025-02-24 0224面试</a></li><li><a href=/docs/2025-2-20-%E9%9D%A2%E8%AF%950220/>2025-02-20 面试0220</a></li><li><a href=/docs/2025-2-19-%E9%9D%A2%E8%AF%950219/>2025-02-19 面试0219</a></li><li><a href=/docs/2025-2-18-%E9%9D%A2%E8%AF%95/>2025-02-18 面试2025-02-18</a></li><li><a href=/docs/2025-2-26-k8s%E7%9B%B8%E5%85%B3/>2025-02-16 k8s题目</a></li><li><a href=/docs/2025-2-12-%E9%9D%A2%E8%AF%950212/>2025-02-12 面试0212</a></li><li><a href=/docs/2025-2-11-%E9%9D%A2%E8%AF%950211/>2025-02-11 面试2025-02-11</a></li><li><a href=/docs/2025-2-7-%E8%AE%A1%E5%88%922/>2025-02-07 美国码农计划</a></li><li><a href=/docs/2025-2-7-%E8%AE%A1%E5%88%92/>2025-02-07 美国码农薪酬</a></li><li><a href=/docs/2025-2-7-k8s%E7%BB%84%E4%BB%B6/>2025-02-07 k8s组件</a></li><li><a href=/docs/2025-1-16-k8s%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E6%8C%87%E5%8D%97/>2025-01-16 k8s常见故障指南</a></li><li><a href=/docs/2025-1-1-%E8%A6%81%E4%B8%8D%E8%A6%81%E5%88%9B%E4%B8%9A/>2025-01-01 要不要创业</a></li><li><a href=/docs/2025-1-1-%E6%97%A9%E6%9C%9F%E6%A8%A1%E5%BC%8F/>2025-01-01 早期模式</a></li><li><a href=/docs/2025-1-1-%E5%A4%A7%E5%A0%B0%E6%B2%B3-%E6%88%91%E7%9A%84%E4%BF%9D%E5%A7%86/>2025-01-01 大堰河-我的保姆</a></li><li><a href=/docs/2025-1-1-%E5%88%9D%E5%88%9B%E5%85%AC%E5%8F%B8/>2025-01-01 初创公司</a></li><li><a href=/docs/2025-1-1-%E5%88%9B%E4%B8%9A%E8%80%85%E4%BA%A4%E6%B5%81/>2025-01-01 创业者交流</a></li><li><a href=/docs/2025-1-1-%E5%88%9B%E4%B8%9A%E7%82%B9%E5%AD%90/>2025-01-01 创业点子</a></li><li><a href=/docs/2025-1-1-sealos%E8%8E%B7%E6%8A%95/>2025-01-01 sealos获投</a></li><li><a href=/docs/2024-12-10-docker-registrry/>2024-12-10 docker registrry</a></li><li><a href=/docs/2024-12-09-openstack-ssh%E8%BF%9E%E6%8E%A5/>2024-12-09 openstack ssh连接</a></li><li><a href=/docs/2024-12-08-mutilpass%E9%83%A8%E7%BD%B2openstack/>2024-12-09 mutilpass部署openstack devstack形式</a></li><li><a href=/docs/2024-12-09-helmchart-%E9%83%A8%E7%BD%B2flask%E5%BA%94%E7%94%A8/>2024-12-09 helmchart 部署flask应用</a></li><li><a href=/docs/2024-12-09-docker-daemon.json/>2024-12-09 docker daemon.json</a></li><li><a href=/docs/2024-12-08-%E5%9D%97%E5%AD%98%E5%82%A8%E5%92%8C%E5%AF%B9%E8%B1%A1%E5%82%A8%E5%AD%98%E5%8C%BA%E5%88%AB/>2024-12-08 块存储和对象储存区别</a></li><li><a href=/docs/2024-12-08-openstack%E9%9C%80%E8%A6%81%E5%87%A0%E5%8F%B0%E8%99%9A%E6%8B%9F%E6%9C%BA/>2024-12-08 openstack需要几台虚拟机</a></li><li><a href=/docs/2024-12-08-openstack%E5%92%8Ckubernetes%E5%8C%BA%E5%88%AB/>2024-12-08 openstack和kubernetes区别</a></li><li><a href=/docs/2024-12-08-nano%E6%93%8D%E4%BD%9C/>2024-12-08 nano操作</a></li><li><a href=/docs/2024-12-08-mutilpass%E6%93%8D%E4%BD%9C/>2024-12-08 mutilpass操作</a></li><li><a href=/docs/2024-12-08-devstack/>2024-12-08 devstack</a></li><li><a href=/docs/2024-12-07-microk8s/>2024-12-07 microk8s</a></li><li><a href=/docs/2024-12-05-kubeasz%E9%83%A8%E7%BD%B2k8s/>2024-12-05 kubeasz部署k8s</a></li><li><a href=/docs/2024-10-20-%E5%88%9B%E5%BB%BA%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/>2024-10-20 使用 Keepalived 和 HAproxy 创建高可用 Kubernetes 集群</a></li><li><a href=/docs/%E9%A1%B6%E7%BA%A7devops%E5%B7%A5%E5%85%B7%E5%A4%A7%E7%9B%98%E7%82%B9-ding-ji-devops-gong-ju-da-pan-dian/>2024-08-02 顶级devops工具大盘点</a></li><li><a href=/docs/%E6%B8%85%E7%90%86docker%E9%95%9C%E5%83%8F-qing-li-docker-jing-xiang/>2024-08-02 清理docker镜像</a></li><li><a href=/docs/%E6%9E%84%E5%BB%BA%E5%AE%B9%E5%99%A8%E9%95%9C%E5%83%8F%E5%88%A9%E5%99%A8buildkit-gou-jian-rong-qi-jing-xiang-li-qi-buildkit/>2024-08-02 构建容器镜像利器buildkit</a></li><li><a href=/docs/%E6%98%AF%E6%8A%80%E6%9C%AF%E5%A4%A7%E7%A5%9E%E8%BF%98%E6%98%AF%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84%E9%83%A8%E7%9A%84%E7%A5%B8%E5%AE%B3-shi-ji-shu-da-shen-hai-shi-ji-chu-jia-gou-bu-de-huo-hai/>2024-08-02 是技术大神还是基础架构部的祸害</a></li><li><a href=/docs/%E6%90%AD%E4%B8%AA%E6%97%A5%E5%BF%97%E6%89%8B%E6%9C%BA%E7%B3%BB%E7%BB%9F%E4%B8%8D%E9%A6%99%E5%90%97-da-ge-ri-zhi-shou-ji-xi-tong-bu-xiang-ma/>2024-08-02 搭个日志手机系统不香吗</a></li><li><a href=/docs/%E6%88%91%E5%8F%AA%E6%83%B3%E5%81%9A%E6%8A%80%E6%9C%AF-%E8%B5%B0%E6%8A%80%E6%9C%AF%E8%B7%AF%E7%BA%BF-wo-zhi-xiang-zuo-ji-shu-zou-ji-shu-lu-xian/>2024-08-02 我只想做技术 走技术路线</a></li><li><a href=/docs/%E5%B8%B8%E8%A7%81linux%E8%BF%90%E7%BB%B4%E9%9D%A2%E8%AF%95%E9%A2%98-chang-jian-linux-yun-wei-mian-shi-ti/>2024-08-02 常见linux运维面试题</a></li><li><a href=/docs/%E5%A4%A7%E5%8E%82%E6%80%BB%E7%BB%93nginx%E9%AB%98%E5%B9%B6%E5%8F%91%E4%BC%98%E5%8C%96%E7%AC%94%E8%AE%B0-da-chang-zong-jie-nginx-gao-bing-fa-you-hua-bi-ji/>2024-08-02 大厂总结nginx高并发优化笔记</a></li><li><a href=/docs/%E5%8F%B2%E4%B8%8A%E6%9C%80%E7%89%9Bjenkins-pipeline%E6%B5%81%E6%B0%B4%E7%BA%BF%E8%AF%A6%E8%A7%A3-shi-shang-zui-niu-jenkinspipeline-liu-shui-xian-xiang-jie/>2024-08-02 史上最牛jenkins pipeline流水线详解</a></li><li><a href=/docs/teg%E4%B8%8Eistio%E9%9B%86%E6%88%90-teg-yu-istio-ji-cheng/>2024-08-02 TEG与istio集成</a></li><li><a href=/docs/prometheus-stack-prometheus-stack/>2024-08-02 prometheus-stack</a></li><li><a href=/docs/pixie-pixie/>2024-08-02 pixie</a></li><li><a href=/docs/nginx%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E6%83%8A%E7%BE%A4%E6%95%88%E5%BA%94-nginx-ru-he-jie-jue-jing-qun-xiao-ying/>2024-08-02 nginx如何解决惊群效应</a></li><li><a href=/docs/netctl%E6%A3%80%E6%B5%8B%E9%9B%86%E7%BE%A4pod%E9%97%B4%E8%BF%9E%E9%80%9A%E6%80%A7-netctl-jian-ce-ji-qun-pod-jian-lian-tong-xing/>2024-08-02 netctl检测集群pod间连通性</a></li><li><a href=/docs/linux%E8%BF%90%E7%BB%B4%E5%B7%A5%E7%A8%8B%E5%B8%8850%E4%B8%AA%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98-linux-yun-wei-gong-cheng-shi-50-ge-chang-jian-mian-shi-ti/>2024-08-02 linux运维工程师50个常见面试题</a></li><li><a href=/docs/linux%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-%E4%B8%83%E4%B8%AA%E5%AE%9E%E6%88%98%E7%BB%8F%E9%AA%8C-linux-xi-tong-xing-neng-you-hua-qi-ge-shi-zhan-jing-yan/>2024-08-02 linux系统性能优化 七个实战经验</a></li><li><a href=/docs/linux-awk%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E5%99%A8-8%E4%B8%AA%E6%A1%88%E4%BE%8B-linuxawk-wen-ben-chu-li-qi-8-ge-an-li/>2024-08-02 linux awk文本处理器 8个案例</a></li><li><a href=/docs/kubewharf-kubewharf/>2024-08-02 kubewharf</a></li><li><a href=/docs/kruise%E5%8E%9F%E5%9C%B0%E5%8D%87%E7%BA%A7%E8%A7%A3%E6%9E%90-kruise-yuan-de-sheng-ji-jie-xi/>2024-08-02 kruise原地升级解析</a></li><li><a href=/docs/k8s%E9%9D%A2%E8%AF%95%E9%A2%98-k8s-mian-shi-ti/>2024-08-02 K8S面试题</a></li><li><a href=/docs/k8s%E8%83%8C%E5%90%8Eservice%E6%98%AF%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C%E7%9A%84-k8s-bei-hou-service-shi-ru-he-gong-zuo-de/>2024-08-02 k8s背后service是如何工作的</a></li><li><a href=/docs/k8s%E7%9A%84%E6%9C%80%E5%90%8E%E4%B8%80%E5%9D%97%E6%8B%BC%E5%9B%BE-dbpaas-k8s-de-zui-hou-yi-kuai-pin-tu-dbpaas/>2024-08-02 K8S的最后一块拼图</a></li><li><a href=/docs/istio%E9%83%A8%E7%BD%B2-istio-bu-shu/>2024-08-02 istio部署</a></li><li><a href=/docs/istio-ingress-gateway-istio-ingress-gateway/>2024-08-02 istio-ingress-gateway</a></li><li><a href=/docs/godel-scheduler-godel-scheduler/>2024-08-02 godel-scheduler</a></li><li><a href=/docs/dockerfile%E5%AE%9A%E5%88%B6%E4%B8%93%E5%B1%9E%E9%95%9C%E5%83%8F-dockerfile-ding-zhi-zhuan-shu-jing-xiang/>2024-08-02 dockerfile定制专属镜像</a></li><li><a href=/docs/33%E6%AC%BEgitops%E4%B8%8Edevops%E4%B8%BB%E6%B5%81%E7%B3%BB%E7%BB%9F-33-kuan-gitops-yu-devops-zhu-liu-xi-tong/>2024-08-02 33款gitops与devops主流系统</a></li><li><a href=/docs/2024-8-1-linux%E8%BF%90%E7%BB%B4%E5%B7%A5%E7%A8%8B%E5%B8%8850%E4%B8%AA%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/>2024-08-01 linux面试题</a></li><li><a href=/docs/2024-8-1-%E5%B8%B8%E8%A7%81linux%E8%BF%90%E7%BB%B4%E9%9D%A2%E8%AF%95%E9%A2%98%E6%89%BE%E5%B7%A5%E4%BD%9C%E7%9A%84%E5%BF%85%E7%9C%8B/>2024-08-01 linux运维面试题</a></li><li><a href=/docs/2024-8-1-kubernetes%E9%9D%A2%E8%AF%95%E9%A2%98/>2024-08-01 k8s面试题</a></li><li><a href=/docs/openkruise%E8%AF%A6%E7%BB%86%E8%A7%A3%E9%87%8A%E4%BB%A5%E5%8F%8A%E5%8E%9F%E5%9C%B0%E5%8D%87%E7%BA%A7%E5%8F%8A%E5%85%A8%E9%93%BE%E8%B7%AF%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83%E6%96%B9%E6%A1%88-openkruise-xiang-xi-jie-shi-yi-ji-yuan-de-sheng-ji-ji-quan-lian-lu-hui-du-fa-bu-fang-an/>2024-07-22 OpenKruise详细解释以及原地升级及全链路灰度发布方案</a></li><li><a href=/docs/k8s%E4%B9%8Bingress-nginx%E5%8E%9F%E7%90%86%E5%8F%8A%E9%85%8D%E7%BD%AE-k8s-zhi-ingress-nginx-yuan-li-ji-pei-zhi/>2024-07-05 K8S之ingress-nginx原理及配置</a></li><li><a href=/docs/%E4%BD%BF%E7%94%A8cloudflarecf%E6%90%AD%E5%BB%BAdockerhub%E4%BB%A3%E7%90%86-shi-yong-cloudflarecf-da-jian-dockerhub-dai-li/>2024-06-28 使用cloudflare(CF)搭建dockerhub代理</a></li><li><a href=/docs/2024-5-14-%E5%8D%95master%E5%8D%95etcd%E6%94%B9%E9%80%A0/>2024-05-01 单master单etcd改造为3master3etcd</a></li><li><a href=/docs/2024-4-17-%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93/>2024-04-17 面试总结</a></li><li><a href=/docs/%E5%A6%82%E4%BD%95%E4%B8%BAk8s%E4%BF%9D%E9%A9%BE%E6%8A%A4%E8%88%AA-ru-he-wei-k8s-bao-jia-hu-hang/>2024-04-16 如何为K8S保驾护航</a></li><li><a href=/docs/k8s%E5%A6%82%E4%BD%95%E8%8E%B7%E5%BE%97-ip-k8s-ru-he-huo-de-ip/>2024-04-16 K8S如何获得 IP</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Bstateful_setgo%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-statefulsetgo-yuan-ma-jie-du/>2024-04-10 K8S控制器之stateful_set.go源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Bstateful_set_status_updatego%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-statefulsetstatusupdatego-yuan-ma-jie-du/>2024-04-10 K8S控制器之stateful_set_status_update.go源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Bstateful_set_controlgo%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-statefulsetcontrolgo-yuan-ma-jie-du/>2024-04-10 K8S控制器之stateful_set_control.go源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Bstateful_pod_controlgo%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-statefulpodcontrolgo-yuan-ma-jie-du/>2024-04-10 K8S控制器之stateful_pod_control.go源码解读</a></li><li><a href=/docs/k8s%E8%B0%83%E5%BA%A6%E5%99%A8-extendergo-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-diao-du-qi-extendergo-yuan-ma-jie-du/>2024-04-09 K8S调度器 extender.go 源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Bsyncgo-%E5%90%8C%E6%AD%A5-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-syncgo-tong-bu-yuan-ma-jie-du/ class=active>2024-04-09 K8S控制器之sync.go 同步 源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Brollbackgo-%E5%9B%9E%E6%BB%9A-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-rollbackgo-hui-gun-yuan-ma-jie-du/>2024-04-09 K8S控制器之rollback.go 回滚 源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Brecreatego-%E9%87%8D%E5%BB%BA-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-recreatego-zhong-jian-yuan-ma-jie-du/>2024-04-09 K8S控制器之recreate.go 重建 源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8B-schedulergo-%E8%B0%83%E5%BA%A6%E5%99%A8-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-schedulergo-diao-du-qi-yuan-ma-jie-du/>2024-04-09 K8S控制器之 scheduler.go 调度器 源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8B-rollinggo-%E6%BB%9A%E5%8A%A8%E6%9B%B4%E6%96%B0-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-rollinggo-gun-dong-geng-xin-yuan-ma-jie-du/>2024-04-09 K8S控制器之 rolling.go 滚动更新 源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8B-progressgo-%E8%BF%9B%E5%BA%A6-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-progressgo-jin-du-yuan-ma-jie-du/>2024-04-09 K8S控制器之 progress.go 进度 源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8B-deployment_controllergo%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-deploymentcontrollergo-yuan-ma-jie-du/>2024-04-09 K8S控制器之 deployment_controller.go源码解读</a></li><li><a href=/docs/k8s-%E8%B0%83%E5%BA%A6%E5%99%A8-scheduler_onego-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-diao-du-qi-scheduleronego-yuan-ma-jie-du/>2024-04-09 K8S 调度器 scheduler_one.go 源码解读</a></li><li><a href=/docs/%E5%BD%BB%E6%82%9F%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C-che-wu-rong-qi-wang-luo/>2024-04-07 彻悟容器网络</a></li><li><a href=/docs/%E9%9D%A2%E8%AF%95%E7%94%A8-golang-%E6%89%8B%E6%92%B8-lru-mian-shi-yong-golang-shou-lu-lru/>2024-04-03 面试用 Golang 手撸 LRU</a></li><li><a href=/docs/%E8%87%AA%E5%8A%A8%E5%B1%8F%E8%94%BDip%E6%94%BB%E5%87%BB-zi-dong-ping-bi-ip-gong-ji/>2024-04-03 自动屏蔽IP攻击</a></li><li><a href=/docs/%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85kubephere-li-xian-an-zhuang-kubephere/>2024-04-03 离线安装kubephere</a></li><li><a href=/docs/%E7%A3%81%E7%9B%98%E6%95%B0%E6%8D%AE%E6%81%A2%E5%A4%8D-ci-pan-shu-ju-hui-fu/>2024-04-03 磁盘数据恢复</a></li><li><a href=/docs/%E6%B8%85%E7%90%86%E6%AE%8B%E7%95%99%E7%9A%84calico%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6-qing-li-can-liu-de-calico-wang-luo/>2024-04-03 清理残留的calico网络插件</a></li><li><a href=/docs/%E6%B5%81%E9%87%8F%E4%BD%95%E5%A4%84%E6%9D%A5%E4%BD%95%E5%A4%84%E5%8E%BB-liu-liang-he-chu-lai-he-chu-qu/>2024-04-03 流量何处来何处去</a></li><li><a href=/docs/%E6%9E%81%E5%A4%A7%E6%8F%90%E9%AB%98%E5%B7%A5%E4%BD%9C%E6%95%88%E7%8E%87%E7%9A%84-linux-%E5%91%BD%E4%BB%A4-ji-da-ti-gao-gong-zuo-xiao-lv-de-linux-ming-ling/>2024-04-03 极大提高工作效率的 Linux 命令</a></li><li><a href=/docs/%E6%96%87%E5%AD%A6%E7%9A%84%E6%95%85%E4%B9%A1-wen-xue-de-gu-xiang/>2024-04-03 文学的故乡</a></li><li><a href=/docs/%E6%90%9E%E6%87%82k8s%E9%89%B4%E6%9D%83-gao-dong-k8s-jian-quan/>2024-04-03 搞懂K8S鉴权</a></li><li><a href=/docs/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E5%8E%9F%E7%90%86-rong-qi-wang-luo-yuan-li/>2024-04-03 容器网络原理</a></li><li><a href=/docs/%E5%AE%B9%E5%99%A8%E7%9A%84%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E4%B8%80-overlayfs-%E5%8E%9F%E7%90%86-rong-qi-de-wen-jian-xi-tong--yi-overlayfs-yuan-li/>2024-04-03 容器的文件系统 OverlayFS 原理</a></li><li><a href=/docs/%E5%AE%B9%E5%99%A8%E5%8E%9F%E7%90%86-rong-qi-yuan-li/>2024-04-03 容器原理</a></li><li><a href=/docs/%E5%AE%B9%E5%99%A8%E5%86%85%E7%9A%84-1-%E5%8F%B7%E8%BF%9B%E7%A8%8B-rong-qi-nei-de-1-hao-jin-cheng/>2024-04-03 容器内的 1 号进程</a></li><li><a href=/docs/%E5%AE%B9%E5%99%A8%E4%B8%AD%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90%E4%BB%A5%E5%8F%8A%E4%B8%8D%E5%90%8Cdnspolicy%E5%AF%B9%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90%E7%9A%84%E5%BD%B1%E5%93%8D-rong-qi-zhong-yu-ming-jie-xi-yi-ji-bu-tong-dnspolicy-dui-yu-ming-jie-xi-de-ying-xiang/>2024-04-03 容器中域名解析以及不同dnspolicy对域名解析的影响</a></li><li><a href=/docs/%E5%A6%82%E4%BD%95%E8%B0%83%E8%AF%95-crash-%E5%AE%B9%E5%99%A8%E7%9A%84%E7%BD%91%E7%BB%9C-ru-he-diao-shi-crash-rong-qi-de-wang-luo/>2024-04-03 如何调试 crash 容器的网络</a></li><li><a href=/docs/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8tekton%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BAcicd%E5%B9%B3%E5%8F%B0-ru-he-shi-yong-tekton-kuai-su-da-jian-cicd-ping-tai/>2024-04-03 如何使用tekton快速搭建CI/CD平台</a></li><li><a href=/docs/%E5%A4%A7%E8%A7%84%E6%A8%A1%E5%B9%B6%E5%8F%91%E4%B8%8B%E5%A6%82%E4%BD%95%E5%8A%A0%E5%BF%AB-pod-%E5%90%AF%E5%8A%A8%E9%80%9F%E5%BA%A6-da-gui-mo-bing-fa-xia-ru-he-jia-kuai-pod-qi-dong-su-du/>2024-04-03 大规模并发下如何加快 Pod 启动速度</a></li><li><a href=/docs/%E4%BD%BF%E7%94%A8kubernees-leases-%E8%BD%BB%E6%9D%BE%E5%AE%9E%E7%8E%B0leader-election-shi-yong-kuberneesleases-qing-song-shi-xian-leaderelection/>2024-04-03 使用kubernees leases 轻松实现leader election</a></li><li><a href=/docs/%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2k8s%E5%8A%A0%E8%8A%82%E7%82%B9%E6%93%8D%E4%BD%9C-er-jin-zhi-bu-shu-k8s-jia-jie-dian-cao-zuo/>2024-04-03 二进制部署K8S加节点操作</a></li><li><a href=/docs/%E4%B8%A4%E5%BC%A0%E5%9B%BE%E5%85%A8%E9%9D%A2%E7%90%86%E8%A7%A3k8s%E5%8E%9F%E7%90%86-liang-zhang-tu-quan-mian-li-jie-k8s-yuan-li/>2024-04-03 两张图全面理解K8S原理</a></li><li><a href=/docs/ssl%E8%AF%81%E4%B9%A6%E8%87%AA%E7%AD%BE%E5%8F%91-ssl-zheng-shu-zi-qian-fa/>2024-04-03 ssl证书自签发</a></li><li><a href=/docs/prometheus%E4%BC%81%E4%B8%9A%E7%BA%A7%E7%9B%91%E6%8E%A7%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93-prometheus-qi-ye-ji-jian-kong-shi-yong-zong-jie/>2024-04-03 prometheus企业级监控使用总结</a></li><li><a href=/docs/metallb-l2-%E5%8E%9F%E7%90%86-metallbl2-yuan-li/>2024-04-03 MetalLB L2 原理</a></li><li><a href=/docs/linux-%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E5%A4%A7%E5%85%A8-linux-xing-neng-you-hua-da-quan/>2024-04-03 Linux 性能优化大全</a></li><li><a href=/docs/kubernetes-%E8%AF%81%E4%B9%A6%E8%AF%A6%E8%A7%A3%E9%89%B4%E6%9D%83-kubernetes-zheng-shu-xiang-jie--jian-quan-/>2024-04-03 Kubernetes 证书详解(鉴权)</a></li><li><a href=/docs/kubernetes-%E8%AF%81%E4%B9%A6%E8%AF%A6%E8%A7%A3%E8%AE%A4%E8%AF%81-kubernetes-zheng-shu-xiang-jie--ren-zheng-/>2024-04-03 Kubernetes 证书详解(认证)</a></li><li><a href=/docs/kubernetes-%E6%BA%90%E7%A0%81%E7%BB%93%E6%9E%84-kubernetes-yuan-ma-jie-gou/>2024-04-03 Kubernetes 源码结构</a></li><li><a href=/docs/kubernetes-api-kubernetesapi/>2024-04-03 Kubernetes API</a></li><li><a href=/docs/kubekey%E6%B7%BB%E5%8A%A0%E6%96%B0%E8%8A%82%E7%82%B9-kubekey-tian-jia-xin-jie-dian/>2024-04-03 kubekey添加新节点</a></li><li><a href=/docs/k8s%E9%9D%A2%E8%AF%95%E5%AE%9D%E5%85%B8-k8s-mian-shi-bao-dian/>2024-04-03 K8S面试宝典</a></li><li><a href=/docs/k8s%E9%9D%A2%E8%AF%95%E5%A4%A7%E5%85%A8-k8s-mian-shi-da-quan/>2024-04-03 K8S面试大全</a></li><li><a href=/docs/k8s%E8%BF%90%E7%BB%B4%E4%B9%8B%E6%B8%85%E7%90%86%E7%A3%81%E7%9B%98-k8s-yun-wei-zhi-qing-li-ci-pan/>2024-04-03 k8s运维之清理磁盘</a></li><li><a href=/docs/k8s%E8%B0%83%E8%AF%95pod-k8s-diao-shi-pod/>2024-04-03 K8S调试POD</a></li><li><a href=/docs/k8s%E7%9A%84pod%E7%B1%BB%E5%9E%8B-k8s-de-pod-lei-xing/>2024-04-03 K8S的POD类型</a></li><li><a href=/docs/k8s%E5%BA%94%E7%94%A8%E7%9A%84%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5-k8s-ying-yong-de-zui-jia-shi-jian/>2024-04-03 k8s应用的最佳实践</a></li><li><a href=/docs/k8s%E5%91%BD%E4%BB%A4%E6%8C%87%E5%8D%97-k8s-ming-ling-zhi-nan/>2024-04-03 K8S命令指南</a></li><li><a href=/docs/k8s%E5%8E%9F%E5%9C%B0%E5%8D%87%E7%BA%A7-k8s-yuan-de-sheng-ji/>2024-04-03 K8S原地升级</a></li><li><a href=/docs/k8s-%E6%8E%A2%E9%92%88%E5%8E%9F%E7%90%86-k8s-tan-zhen-yuan-li/>2024-04-03 K8S 探针原理</a></li><li><a href=/docs/k8s-%E5%BC%80%E5%8F%91%E5%8F%AF%E4%B8%8D%E6%AD%A2-crud-k8s-kai-fa-ke-bu-zhi-crud/>2024-04-03 K8S 开发可不止 CRUD</a></li><li><a href=/docs/k8s-gpt-k8sgpt/>2024-04-03 K8S GPT</a></li><li><a href=/docs/k8s-csi-openebs%E5%8E%9F%E7%90%86-k8scsiopenebs-yuan-li/>2024-04-03 K8S csi openebs原理</a></li><li><a href=/docs/helm-chart%E5%92%8Crepo-helmchart-he-repo/>2024-04-03 helm chart和repo</a></li><li><a href=/docs/flanel%E7%BD%91%E7%BB%9C-flanel-wang-luo/>2024-04-03 flanel网络</a></li><li><a href=/docs/etcd%E7%A8%B3%E5%AE%9A%E6%80%A7%E5%8F%8A%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E5%AE%9E%E8%B7%B5-etcd-wen-ding-xing-ji-xing-neng-you-hua-shi-jian/>2024-04-03 ETCD稳定性及性能优化实践</a></li><li><a href=/docs/etcd%E5%A4%87%E4%BB%BD-etcd-bei-fen/>2024-04-03 ETCD备份</a></li><li><a href=/docs/docker%E9%87%8D%E8%A6%81%E7%9A%84%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86%E7%82%B9-docker-zhong-yao-de-wang-luo-zhi-shi-dian/>2024-04-03 Docker重要的网络知识点</a></li><li><a href=/docs/dockerfile%E7%9A%84copy%E5%92%8Cadd%E7%9A%84%E5%8C%BA%E5%88%AB-dockerfile-de-copy-he-add-de-qu-bie/>2024-04-03 dockerfile的copy和add的区别</a></li><li><a href=/docs/coredns%E4%B9%8B%E5%85%89-coredns-zhi-guang/>2024-04-03 COREDNS之光</a></li><li><a href=/docs/containerd-%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C-containerd-ji-ben-cao-zuo/>2024-04-03 Containerd 基本操作</a></li><li><a href=/docs/cni%E6%8F%92%E4%BB%B6%E9%80%89%E5%9E%8B-cni-cha-jian-xuan-xing/>2024-04-03 CNI插件选型</a></li><li><a href=/docs/client-go-%E6%9E%B6%E6%9E%84-client-go-jia-gou/>2024-04-03 Client-go 架构</a></li><li><a href=/docs/client-go-%E5%9B%9B%E7%A7%8D%E5%AE%A2%E6%88%B7%E7%AB%AF-client-go-si-zhong-ke-hu-duan/>2024-04-03 Client-go 四种客户端</a></li><li><a href=/docs/cicd%E6%80%9D%E8%80%83-cicd-si-kao/>2024-04-03 CICD思考</a></li><li><a href=/docs/calico%E7%BD%91%E7%BB%9C%E8%87%AA%E5%AE%9A%E4%B9%89-calico-wang-luo-zi-ding-yi/>2024-04-03 Calico网络自定义</a></li><li><a href=/docs/acme%E8%87%AA%E5%8A%A8%E6%9B%B4%E6%96%B0%E8%AF%81%E4%B9%A6-acme-zi-dong-geng-xin-zheng-shu/>2024-04-03 acme自动更新证书</a></li><li><a href=/docs/16%E4%B8%AA%E6%A6%82%E5%BF%B5%E5%B8%A6%E4%BD%A0%E5%85%A5%E9%97%A8-kubernetes-16-ge-gai-nian-dai-ni-ru-men-kubernetes/>2024-04-03 16个概念带你入门 Kubernetes</a></li><li><a href=/docs/%E9%9D%A2%E8%AF%950308-mian-shi-0308/>2024-04-03 面试0308</a></li><li><a href=/docs/600%E6%9D%A1%E6%9C%80%E5%BC%BAlinux%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93-600-tiao-zui-qiang-linux-ming-ling-zong-jie/>2024-04-03 600条最强linux命令总结</a></li><li><a href=/docs/16%E5%BC%A0%E7%A1%AC%E6%A0%B8%E5%9B%BE%E8%A7%A3k8s%E7%BD%91%E7%BB%9C-16-zhang-ying-he-tu-jie-k8s-wang-luo/>2024-04-03 16张硬核图解k8s网络</a></li><li><a href=/docs/k8s%E4%B9%8Bkubelet%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-zhi-kubelet-yuan-ma-jie-du/>2024-03-28 k8s之kubelet源码解读</a></li><li><a href=/docs/2024-3-19-%E4%B8%A4%E5%BC%A0%E5%9B%BE%E5%85%A8%E9%9D%A2%E7%90%86%E8%A7%A3k8s%E5%8E%9F%E7%90%86/>2024-03-19 两张图全面理解k8s原理</a></li><li><a href=/docs/2024-3-8-%E9%9D%A2%E8%AF%950308/>2024-03-08 面试</a></li><li><a href=/docs/2024-3-4-k8s%E6%B5%81%E9%87%8F%E9%93%BE%E8%B7%AF%E5%89%96%E6%9E%90/>2024-03-04 k8s流量链路剖析</a></li><li><a href=/docs/k8s-%E6%B5%81%E9%87%8F%E9%93%BE%E8%B7%AF%E5%89%96%E6%9E%90-k8s-liu-liang-lian-lu-pou-xi/>2024-03-04 K8S 流量链路剖析</a></li><li><a href=/docs/k8s-csi%E5%89%96%E6%9E%90%E6%BC%94%E8%BF%9B-k8scsi-pou-xi-yan-jin/>2024-03-04 K8S CSI剖析演进</a></li><li><a href=/docs/k8s-cni%E5%89%96%E6%9E%90%E6%BC%94%E8%BF%9B-k8scni-pou-xi-yan-jin/>2024-03-04 K8S CNI剖析演进</a></li><li><a href=/docs/2024-3-4-k8s-csi%E5%89%96%E6%9E%90/>2024-03-04 CSI剖析演进</a></li><li><a href=/docs/2024-3-4-cni%E5%89%96%E6%9E%90%E6%BC%94%E8%BF%9B/>2024-03-04 CNI剖析演进</a></li><li><a href=/docs/2024-2-26-%E9%9D%A2%E8%AF%95/>2024-02-26 面试</a></li><li><a href=/docs/2024-2-22-k8s%E9%9D%A2%E8%AF%95%E5%AE%9D%E5%85%B8/>2024-02-22 k8s面试宝典</a></li><li><a href=/docs/2024-2-22-k8s%E6%9E%B6%E6%9E%84%E5%B8%88%E9%9D%A2%E8%AF%95%E5%A4%A7%E5%85%A8/>2024-02-22 k8s架构师面试大全</a></li><li><a href=/docs/%E4%BD%BF%E7%94%A8-openfunction-%E5%9C%A8%E4%BB%BB%E4%BD%95%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD%E4%B8%8A%E8%BF%90%E8%A1%8C%E6%97%A0%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%B7%A5%E4%BD%9C%E8%B4%9F%E8%BD%BD-shi-yong-openfunction-zai-ren-he-ji-chu-she-shi-shang-yun-xing-wu-fu-wu-qi-gong-zuo-fu-zai/>2024-01-21 使用 OpenFunction 在任何基础设施上运行无服务器工作负载</a></li><li><a href=/docs/%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85%E9%9B%86%E7%BE%A4-li-xian-an-zhuang-ji-qun/>2023-09-28 离线安装集群</a></li><li><a href=/docs/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%AF%B4%E6%98%8E-cao-zuo-xi-tong-shuo-ming/>2023-09-28 操作系统说明</a></li><li><a href=/docs/%E5%BF%AB%E9%80%9F%E6%8C%87%E5%8D%97-kuai-su-zhi-nan/>2023-09-28 快速指南</a></li><li><a href=/docs/%E5%BC%80%E5%A7%8B%E4%BD%BF%E7%94%A8-cilium-kai-shi-shi-yong-cilium/>2023-09-28 开始使用 cilium</a></li><li><a href=/docs/%E5%A4%9A%E6%9E%B6%E6%9E%84%E6%94%AF%E6%8C%81-duo-jia-gou-zhi-chi/>2023-09-28 多架构支持</a></li><li><a href=/docs/%E5%85%AC%E6%9C%89%E4%BA%91%E4%B8%8A%E9%83%A8%E7%BD%B2-kubeasz-gong-you-yun-shang-bu-shu-kubeasz/>2023-09-28 公有云上部署</a></li><li><a href=/docs/%E4%B8%AA%E6%80%A7%E5%8C%96%E9%9B%86%E7%BE%A4%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE-ge-xing-hua-ji-qun-can-shu-pei-zhi/>2023-09-28 个性化集群参数配置</a></li><li><a href=/docs/network-check-network-check/>2023-09-28 network-check</a></li><li><a href=/docs/kube-router-%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6-kube-router-wang-luo-zu-jian/>2023-09-28 kube-router 网络组件</a></li><li><a href=/docs/ezctl-%E5%91%BD%E4%BB%A4%E8%A1%8C%E4%BB%8B%E7%BB%8D-ezctl-ming-ling-xing-jie-shao/>2023-09-28 ezctl 命令行介绍</a></li><li><a href=/docs/ex-lb-%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E9%83%A8%E7%BD%B2-ex-lb-fu-zai-jun-heng-bu-shu/>2023-09-28 EX-LB 负载均衡部署</a></li><li><a href=/docs/calico-%E9%85%8D%E7%BD%AE-bgp-route-reflectors-calico-pei-zhi-bgproutereflectors/>2023-09-28 calico 配置 BGP Route Reflectors</a></li><li><a href=/docs/07-%E5%AE%89%E8%A3%85%E9%9B%86%E7%BE%A4%E4%B8%BB%E8%A6%81%E6%8F%92%E4%BB%B6-07--an-zhuang-ji-qun-zhu-yao-cha-jian/>2023-09-28 15:26:42.651 07-安装集群主要插件</a></li><li><a href=/docs/08-k8s-%E9%9B%86%E7%BE%A4%E5%AD%98%E5%82%A8--k8s-ji-qun-cun-chu/>2023-09-28 08-K8S 集群存储</a></li><li><a href=/docs/06-%E5%AE%89%E8%A3%85%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6-06--an-zhuang-wang-luo-zu-jian/>2023-09-28 06-安装网络组件</a></li><li><a href=/docs/06-%E5%AE%89%E8%A3%85kube-ovn%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6-06--an-zhuang-kube-ovn-wang-luo-zu-jian/>2023-09-28 06-安装kube-ovn网络组件</a></li><li><a href=/docs/06-%E5%AE%89%E8%A3%85flannel%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6-06--an-zhuang-flannel-wang-luo-zu-jian/>2023-09-28 06-安装flannel网络组件</a></li><li><a href=/docs/06-%E5%AE%89%E8%A3%85cilium%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6-06--an-zhuang-cilium-wang-luo-zu-jian/>2023-09-28 06-安装cilium网络组件</a></li><li><a href=/docs/06-%E5%AE%89%E8%A3%85calico%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6-06--an-zhuang-calico-wang-luo-zu-jian/>2023-09-28 06-安装calico网络组件</a></li><li><a href=/docs/02-%E5%AE%89%E8%A3%85etcd%E9%9B%86%E7%BE%A4-02--an-zhuang-etcd-ji-qun/>2023-09-28 02-安装etcd集群</a></li><li><a href=/docs/00-%E9%9B%86%E7%BE%A4%E8%A7%84%E5%88%92%E5%92%8C%E5%9F%BA%E7%A1%80%E5%8F%82%E6%95%B0%E8%AE%BE%E5%AE%9A-00--ji-qun-gui-hua-he-ji-chu-can-shu-she-ding/>2023-09-28 00-集群规划和基础参数设定</a></li><li><a href=/docs/05-%E5%AE%89%E8%A3%85kube_node%E8%8A%82%E7%82%B9-05--an-zhuang-kubenode-jie-dian/>2023-09-28 05-安装kube_node节点</a></li><li><a href=/docs/04-%E5%AE%89%E8%A3%85kube_master%E8%8A%82%E7%82%B9-04--an-zhuang-kubemaster-jie-dian/>2023-09-28 04-安装kube_master节点</a></li><li><a href=/docs/03-%E5%AE%89%E8%A3%85%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6-03--an-zhuang-rong-qi-yun-xing-shi/>2023-09-28 03-安装容器运行时</a></li><li><a href=/docs/01-%E5%88%9B%E5%BB%BA%E8%AF%81%E4%B9%A6%E5%92%8C%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87-01--chuang-jian-zheng-shu-he-huan-jing-zhun-bei/>2023-09-28 01-创建证书和环境准备</a></li><li><a href=/docs/%E6%9C%89%E8%BF%993%E4%B8%AA%E8%BF%B9%E8%B1%A1%E4%BD%A0%E5%B0%B1%E8%AF%A5%E7%A6%BB%E8%81%8C%E4%BA%86-you-zhe-3-ge-ji-xiang--ni-jiu-gai-li-zhi-le/>2023-09-21 思考</a></li><li><a href=/docs/%E4%BD%BF%E7%94%A8-keepalived-%E5%92%8C-haproxy-%E5%88%9B%E5%BB%BA%E9%AB%98%E5%8F%AF%E7%94%A8-kubernetes-%E9%9B%86%E7%BE%A4-shi-yong-keepalived-he-haproxy-chuang-jian-gao-ke-yong-kubernetes-ji-qun/>2023-04-12 使用 Keepalived 和 HAproxy 创建高可用 Kubernetes 集群</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h3>2024-04-09 K8S控制器之sync.go 同步 源码解读</h3><label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents></nav></aside></header><article class="markdown book-article"><pre tabindex=0><code>/*
Copyright 2016 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the &#34;License&#34;);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &#34;AS IS&#34; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package deployment

import (
	&#34;context&#34;
	&#34;fmt&#34;
	&#34;reflect&#34;
	&#34;sort&#34;
	&#34;strconv&#34;

	apps &#34;k8s.io/api/apps/v1&#34;
	v1 &#34;k8s.io/api/core/v1&#34;
	&#34;k8s.io/apimachinery/pkg/api/errors&#34;
	metav1 &#34;k8s.io/apimachinery/pkg/apis/meta/v1&#34;
	&#34;k8s.io/kubernetes/pkg/controller&#34;
	deploymentutil &#34;k8s.io/kubernetes/pkg/controller/deployment/util&#34;
	labelsutil &#34;k8s.io/kubernetes/pkg/util/labels&#34;
)

// syncStatusOnly only updates Deployments Status and doesn&#39;t take any mutating actions.
func (dc *DeploymentController) syncStatusOnly(ctx context.Context, d *apps.Deployment, rsList []*apps.ReplicaSet) error {
	newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(ctx, d, rsList, false)
	if err != nil {
		return err
	}

	allRSs := append(oldRSs, newRS)
	return dc.syncDeploymentStatus(ctx, allRSs, newRS, d)
}

//该函数用于同步更新Deployment的状态，而不进行任何变更操作。
//它通过调用getAllReplicaSetsAndSyncRevision方法获取所有ReplicaSet的列表，并将新旧ReplicaSet区分开来。
//然后，它将所有ReplicaSet附加到旧ReplicaSet列表中，并调用syncDeploymentStatus方法来同步更新Deployment的状态。
//如果在获取ReplicaSet列表时出现错误，则返回该错误。

// sync is responsible for reconciling deployments on scaling events or when they
// are paused.
func (dc *DeploymentController) sync(ctx context.Context, d *apps.Deployment, rsList []*apps.ReplicaSet) error {
	newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(ctx, d, rsList, false)
	if err != nil {
		return err
	}
	if err := dc.scale(ctx, d, newRS, oldRSs); err != nil {
		// If we get an error while trying to scale, the deployment will be requeued
		// so we can abort this resync
		return err
	}

	// Clean up the deployment when it&#39;s paused and no rollback is in flight.
	if d.Spec.Paused &amp;&amp; getRollbackTo(d) == nil {
		if err := dc.cleanupDeployment(ctx, oldRSs, d); err != nil {
			return err
		}
	}

	allRSs := append(oldRSs, newRS)
	return dc.syncDeploymentStatus(ctx, allRSs, newRS, d)
}

//该函数是一个Go语言函数，定义在DeploymentController结构体中，名为sync。它负责在缩放事件或暂停时协调部署。
//函数使用context.Context作为上下文，接收一个*apps.Deployment类型的参数d和一个[]*apps.ReplicaSet类型的参数rsList，返回一个error类型的值。
//函数主要执行以下操作：
//1. 调用getAllReplicaSetsAndSyncRevision方法获取所有复制集并同步修订版本，返回新复制集、旧复制集和错误（如果有）。
//2. 如果scale方法调用失败，则返回错误，以便重新排队处理。
//3. 如果部署被暂停并且没有进行中的回滚，则尝试清理部署。
//4. 将旧复制集和新复制集合并为一个列表，并调用syncDeploymentStatus方法来同步部署状态。
//最后，函数返回可能的错误。

// checkPausedConditions checks if the given deployment is paused or not and adds an appropriate condition.
// These conditions are needed so that we won&#39;t accidentally report lack of progress for resumed deployments
// that were paused for longer than progressDeadlineSeconds.
func (dc *DeploymentController) checkPausedConditions(ctx context.Context, d *apps.Deployment) error {
	if !deploymentutil.HasProgressDeadline(d) {
		return nil
	}
	cond := deploymentutil.GetDeploymentCondition(d.Status, apps.DeploymentProgressing)
	if cond != nil &amp;&amp; cond.Reason == deploymentutil.TimedOutReason {
		// If we have reported lack of progress, do not overwrite it with a paused condition.
		return nil
	}
	pausedCondExists := cond != nil &amp;&amp; cond.Reason == deploymentutil.PausedDeployReason

	needsUpdate := false
	if d.Spec.Paused &amp;&amp; !pausedCondExists {
		condition := deploymentutil.NewDeploymentCondition(apps.DeploymentProgressing, v1.ConditionUnknown, deploymentutil.PausedDeployReason, &#34;Deployment is paused&#34;)
		deploymentutil.SetDeploymentCondition(&amp;d.Status, *condition)
		needsUpdate = true
	} else if !d.Spec.Paused &amp;&amp; pausedCondExists {
		condition := deploymentutil.NewDeploymentCondition(apps.DeploymentProgressing, v1.ConditionUnknown, deploymentutil.ResumedDeployReason, &#34;Deployment is resumed&#34;)
		deploymentutil.SetDeploymentCondition(&amp;d.Status, *condition)
		needsUpdate = true
	}

	if !needsUpdate {
		return nil
	}

	var err error
	_, err = dc.client.AppsV1().Deployments(d.Namespace).UpdateStatus(ctx, d, metav1.UpdateOptions{})
	return err
}

//该函数用于检查给定的部署是否暂停，并添加适当的条件。
//这些条件是必要的，以避免意外报告恢复的部署缺乏进展，这些部署被暂停的时间超过了progressDeadlineSeconds。
//函数首先检查部署是否具有进度截止时间，如果没有，则返回nil。
//然后获取部署的状态，如果状态中存在DeploymentProgressing条件且原因等于TimedOutReason，则不添加暂停条件。
//接下来，函数检查部署是否暂停以及是否存在暂停条件。
//如果部署已暂停但不存在暂停条件，则创建一个新的DeploymentProgressing条件并设置其原因为PausedDeployReason。
//如果部署未暂停但存在暂停条件，则创建一个新的DeploymentProgressing条件并设置其原因为ResumedDeployReason。
//最后，如果条件有更新，则使用client更新部署的状态。函数返回更新状态时可能发生的错误。

// getAllReplicaSetsAndSyncRevision returns all the replica sets for the provided deployment (new and all old), with new RS&#39;s and deployment&#39;s revision updated.
//
// rsList should come from getReplicaSetsForDeployment(d).
//
//  1. Get all old RSes this deployment targets, and calculate the max revision number among them (maxOldV).
//  2. Get new RS this deployment targets (whose pod template matches deployment&#39;s), and update new RS&#39;s revision number to (maxOldV + 1),
//     only if its revision number is smaller than (maxOldV + 1). If this step failed, we&#39;ll update it in the next deployment sync loop.
//  3. Copy new RS&#39;s revision number to deployment (update deployment&#39;s revision). If this step failed, we&#39;ll update it in the next deployment sync loop.
//
// Note that currently the deployment controller is using caches to avoid querying the server for reads.
// This may lead to stale reads of replica sets, thus incorrect deployment status.
func (dc *DeploymentController) getAllReplicaSetsAndSyncRevision(ctx context.Context, d *apps.Deployment, rsList []*apps.ReplicaSet, createIfNotExisted bool) (*apps.ReplicaSet, []*apps.ReplicaSet, error) {
	_, allOldRSs := deploymentutil.FindOldReplicaSets(d, rsList)

	// Get new replica set with the updated revision number
	newRS, err := dc.getNewReplicaSet(ctx, d, rsList, allOldRSs, createIfNotExisted)
	if err != nil {
		return nil, nil, err
	}

	return newRS, allOldRSs, nil
}

const (
	// limit revision history length to 100 element (~2000 chars)
	maxRevHistoryLengthInChars = 2000
)

// Returns a replica set that matches the intent of the given deployment. Returns nil if the new replica set doesn&#39;t exist yet.
// 1. Get existing new RS (the RS that the given deployment targets, whose pod template is the same as deployment&#39;s).
// 2. If there&#39;s existing new RS, update its revision number if it&#39;s smaller than (maxOldRevision + 1), where maxOldRevision is the max revision number among all old RSes.
// 3. If there&#39;s no existing new RS and createIfNotExisted is true, create one with appropriate revision number (maxOldRevision + 1) and replicas.
// Note that the pod-template-hash will be added to adopted RSes and pods.
func (dc *DeploymentController) getNewReplicaSet(ctx context.Context, d *apps.Deployment, rsList, oldRSs []*apps.ReplicaSet, createIfNotExisted bool) (*apps.ReplicaSet, error) {
	logger := klog.FromContext(ctx)
	existingNewRS := deploymentutil.FindNewReplicaSet(d, rsList)

	// Calculate the max revision number among all old RSes
	maxOldRevision := deploymentutil.MaxRevision(logger, oldRSs)
	// Calculate revision number for this new replica set
	newRevision := strconv.FormatInt(maxOldRevision+1, 10)

	// Latest replica set exists. We need to sync its annotations (includes copying all but
	// annotationsToSkip from the parent deployment, and update revision, desiredReplicas,
	// and maxReplicas) and also update the revision annotation in the deployment with the
	// latest revision.
	if existingNewRS != nil {
		rsCopy := existingNewRS.DeepCopy()

		// Set existing new replica set&#39;s annotation
		annotationsUpdated := deploymentutil.SetNewReplicaSetAnnotations(ctx, d, rsCopy, newRevision, true, maxRevHistoryLengthInChars)
		minReadySecondsNeedsUpdate := rsCopy.Spec.MinReadySeconds != d.Spec.MinReadySeconds
		if annotationsUpdated || minReadySecondsNeedsUpdate {
			rsCopy.Spec.MinReadySeconds = d.Spec.MinReadySeconds
			return dc.client.AppsV1().ReplicaSets(rsCopy.ObjectMeta.Namespace).Update(ctx, rsCopy, metav1.UpdateOptions{})
		}

		// Should use the revision in existingNewRS&#39;s annotation, since it set by before
		needsUpdate := deploymentutil.SetDeploymentRevision(d, rsCopy.Annotations[deploymentutil.RevisionAnnotation])
		// If no other Progressing condition has been recorded and we need to estimate the progress
		// of this deployment then it is likely that old users started caring about progress. In that
		// case we need to take into account the first time we noticed their new replica set.
		cond := deploymentutil.GetDeploymentCondition(d.Status, apps.DeploymentProgressing)
		if deploymentutil.HasProgressDeadline(d) &amp;&amp; cond == nil {
			msg := fmt.Sprintf(&#34;Found new replica set %q&#34;, rsCopy.Name)
			condition := deploymentutil.NewDeploymentCondition(apps.DeploymentProgressing, v1.ConditionTrue, deploymentutil.FoundNewRSReason, msg)
			deploymentutil.SetDeploymentCondition(&amp;d.Status, *condition)
			needsUpdate = true
		}

		if needsUpdate {
			var err error
			if _, err = dc.client.AppsV1().Deployments(d.Namespace).UpdateStatus(ctx, d, metav1.UpdateOptions{}); err != nil {
				return nil, err
			}
		}
		return rsCopy, nil
	}

	if !createIfNotExisted {
		return nil, nil
	}

	// new ReplicaSet does not exist, create one.
	newRSTemplate := *d.Spec.Template.DeepCopy()
	podTemplateSpecHash := controller.ComputeHash(&amp;newRSTemplate, d.Status.CollisionCount)
	newRSTemplate.Labels = labelsutil.CloneAndAddLabel(d.Spec.Template.Labels, apps.DefaultDeploymentUniqueLabelKey, podTemplateSpecHash)
	// Add podTemplateHash label to selector.
	newRSSelector := labelsutil.CloneSelectorAndAddLabel(d.Spec.Selector, apps.DefaultDeploymentUniqueLabelKey, podTemplateSpecHash)

	// Create new ReplicaSet
	newRS := apps.ReplicaSet{
		ObjectMeta: metav1.ObjectMeta{
			// Make the name deterministic, to ensure idempotence
			Name:            d.Name + &#34;-&#34; + podTemplateSpecHash,
			Namespace:       d.Namespace,
			OwnerReferences: []metav1.OwnerReference{*metav1.NewControllerRef(d, controllerKind)},
			Labels:          newRSTemplate.Labels,
		},
		Spec: apps.ReplicaSetSpec{
			Replicas:        new(int32),
			MinReadySeconds: d.Spec.MinReadySeconds,
			Selector:        newRSSelector,
			Template:        newRSTemplate,
		},
	}
	allRSs := append(oldRSs, &amp;newRS)
	newReplicasCount, err := deploymentutil.NewRSNewReplicas(d, allRSs, &amp;newRS)
	if err != nil {
		return nil, err
	}

	*(newRS.Spec.Replicas) = newReplicasCount
	// Set new replica set&#39;s annotation
	deploymentutil.SetNewReplicaSetAnnotations(ctx, d, &amp;newRS, newRevision, false, maxRevHistoryLengthInChars)
	// Create the new ReplicaSet. If it already exists, then we need to check for possible
	// hash collisions. If there is any other error, we need to report it in the status of
	// the Deployment.
	alreadyExists := false
	createdRS, err := dc.client.AppsV1().ReplicaSets(d.Namespace).Create(ctx, &amp;newRS, metav1.CreateOptions{})
	switch {
	// We may end up hitting this due to a slow cache or a fast resync of the Deployment.
	case errors.IsAlreadyExists(err):
		alreadyExists = true

		// Fetch a copy of the ReplicaSet.
		rs, rsErr := dc.rsLister.ReplicaSets(newRS.Namespace).Get(newRS.Name)
		if rsErr != nil {
			return nil, rsErr
		}

		// If the Deployment owns the ReplicaSet and the ReplicaSet&#39;s PodTemplateSpec is semantically
		// deep equal to the PodTemplateSpec of the Deployment, it&#39;s the Deployment&#39;s new ReplicaSet.
		// Otherwise, this is a hash collision and we need to increment the collisionCount field in
		// the status of the Deployment and requeue to try the creation in the next sync.
		controllerRef := metav1.GetControllerOf(rs)
		if controllerRef != nil &amp;&amp; controllerRef.UID == d.UID &amp;&amp; deploymentutil.EqualIgnoreHash(&amp;d.Spec.Template, &amp;rs.Spec.Template) {
			createdRS = rs
			err = nil
			break
		}

		// Matching ReplicaSet is not equal - increment the collisionCount in the DeploymentStatus
		// and requeue the Deployment.
		if d.Status.CollisionCount == nil {
			d.Status.CollisionCount = new(int32)
		}
		preCollisionCount := *d.Status.CollisionCount
		*d.Status.CollisionCount++
		// Update the collisionCount for the Deployment and let it requeue by returning the original
		// error.
		_, dErr := dc.client.AppsV1().Deployments(d.Namespace).UpdateStatus(ctx, d, metav1.UpdateOptions{})
		if dErr == nil {
			logger.V(2).Info(&#34;Found a hash collision for deployment - bumping collisionCount to resolve it&#34;, &#34;deployment&#34;, klog.KObj(d), &#34;oldCollisionCount&#34;, preCollisionCount, &#34;newCollisionCount&#34;, *d.Status.CollisionCount)
		}
		return nil, err
	case errors.HasStatusCause(err, v1.NamespaceTerminatingCause):
		// if the namespace is terminating, all subsequent creates will fail and we can safely do nothing
		return nil, err
	case err != nil:
		msg := fmt.Sprintf(&#34;Failed to create new replica set %q: %v&#34;, newRS.Name, err)
		if deploymentutil.HasProgressDeadline(d) {
			cond := deploymentutil.NewDeploymentCondition(apps.DeploymentProgressing, v1.ConditionFalse, deploymentutil.FailedRSCreateReason, msg)
			deploymentutil.SetDeploymentCondition(&amp;d.Status, *cond)
			// We don&#39;t really care about this error at this point, since we have a bigger issue to report.
			// TODO: Identify which errors are permanent and switch DeploymentIsFailed to take into account
			// these reasons as well. Related issue: https://github.com/kubernetes/kubernetes/issues/18568
			_, _ = dc.client.AppsV1().Deployments(d.Namespace).UpdateStatus(ctx, d, metav1.UpdateOptions{})
		}
		dc.eventRecorder.Eventf(d, v1.EventTypeWarning, deploymentutil.FailedRSCreateReason, msg)
		return nil, err
	}
	if !alreadyExists &amp;&amp; newReplicasCount &gt; 0 {
		dc.eventRecorder.Eventf(d, v1.EventTypeNormal, &#34;ScalingReplicaSet&#34;, &#34;Scaled up replica set %s to %d&#34;, createdRS.Name, newReplicasCount)
	}

	needsUpdate := deploymentutil.SetDeploymentRevision(d, newRevision)
	if !alreadyExists &amp;&amp; deploymentutil.HasProgressDeadline(d) {
		msg := fmt.Sprintf(&#34;Created new replica set %q&#34;, createdRS.Name)
		condition := deploymentutil.NewDeploymentCondition(apps.DeploymentProgressing, v1.ConditionTrue, deploymentutil.NewReplicaSetReason, msg)
		deploymentutil.SetDeploymentCondition(&amp;d.Status, *condition)
		needsUpdate = true
	}
	if needsUpdate {
		_, err = dc.client.AppsV1().Deployments(d.Namespace).UpdateStatus(ctx, d, metav1.UpdateOptions{})
	}
	return createdRS, err
}

//该函数是Go语言编写的，用于获取与给定部署相匹配的复制集（ReplicaSet）。
//如果该新的复制集不存在，且createIfNotExisted参数为true，则会创建一个新的复制集。
//函数首先查找与给定部署目标相匹配的现有新的复制集（即具有与部署相同的Pod模板）。
//然后，它会计算所有旧复制集的最大修订号（revision number），并为新的复制集计算一个适当的修订号。
//如果存在现有的新复制集，函数将更新其修订号和注释，并可能更新部署的状态。
//如果不存在新的复制集且createIfNotExisted为true，则函数将创建一个新的复制集，并更新部署的状态。
//该函数使用了context.Context来控制函数执行的上下文，使用了apps.Deployment、apps.ReplicaSet等结构体来表示部署和复制集的信息，
//使用了client来与Kubernetes API进行交互。
//函数的具体逻辑包括：
//1. 查找与给定部署相匹配的新的复制集。
//2. 计算所有旧复制集的最大修订号。
//3. 如果存在现有的新复制集，更新其修订号和注释，并可能更新部署的状态。
//4. 如果不存在新的复制集且createIfNotExisted为true，创建一个新的复制集，并更新部署的状态。
//该函数涉及的操作包括查找、更新和创建复制集，以及更新部署的状态。

// scale scales proportionally in order to mitigate risk. Otherwise, scaling up can increase the size
// of the new replica set and scaling down can decrease the sizes of the old ones, both of which would
// have the effect of hastening the rollout progress, which could produce a higher proportion of unavailable
// replicas in the event of a problem with the rolled out template. Should run only on scaling events or
// when a deployment is paused and not during the normal rollout process.
func (dc *DeploymentController) scale(ctx context.Context, deployment *apps.Deployment, newRS *apps.ReplicaSet, oldRSs []*apps.ReplicaSet) error {
	// If there is only one active replica set then we should scale that up to the full count of the
	// deployment. If there is no active replica set, then we should scale up the newest replica set.
	if activeOrLatest := deploymentutil.FindActiveOrLatest(newRS, oldRSs); activeOrLatest != nil {
		if *(activeOrLatest.Spec.Replicas) == *(deployment.Spec.Replicas) {
			return nil
		}
		_, _, err := dc.scaleReplicaSetAndRecordEvent(ctx, activeOrLatest, *(deployment.Spec.Replicas), deployment)
		return err
	}

	// If the new replica set is saturated, old replica sets should be fully scaled down.
	// This case handles replica set adoption during a saturated new replica set.
	if deploymentutil.IsSaturated(deployment, newRS) {
		for _, old := range controller.FilterActiveReplicaSets(oldRSs) {
			if _, _, err := dc.scaleReplicaSetAndRecordEvent(ctx, old, 0, deployment); err != nil {
				return err
			}
		}
		return nil
	}

	// There are old replica sets with pods and the new replica set is not saturated.
	// We need to proportionally scale all replica sets (new and old) in case of a
	// rolling deployment.
	if deploymentutil.IsRollingUpdate(deployment) {
		allRSs := controller.FilterActiveReplicaSets(append(oldRSs, newRS))
		allRSsReplicas := deploymentutil.GetReplicaCountForReplicaSets(allRSs)

		allowedSize := int32(0)
		if *(deployment.Spec.Replicas) &gt; 0 {
			allowedSize = *(deployment.Spec.Replicas) + deploymentutil.MaxSurge(*deployment)
		}

		// Number of additional replicas that can be either added or removed from the total
		// replicas count. These replicas should be distributed proportionally to the active
		// replica sets.
		deploymentReplicasToAdd := allowedSize - allRSsReplicas

		// The additional replicas should be distributed proportionally amongst the active
		// replica sets from the larger to the smaller in size replica set. Scaling direction
		// drives what happens in case we are trying to scale replica sets of the same size.
		// In such a case when scaling up, we should scale up newer replica sets first, and
		// when scaling down, we should scale down older replica sets first.
		var scalingOperation string
		switch {
		case deploymentReplicasToAdd &gt; 0:
			sort.Sort(controller.ReplicaSetsBySizeNewer(allRSs))
			scalingOperation = &#34;up&#34;

		case deploymentReplicasToAdd &lt; 0:
			sort.Sort(controller.ReplicaSetsBySizeOlder(allRSs))
			scalingOperation = &#34;down&#34;
		}

		// Iterate over all active replica sets and estimate proportions for each of them.
		// The absolute value of deploymentReplicasAdded should never exceed the absolute
		// value of deploymentReplicasToAdd.
		deploymentReplicasAdded := int32(0)
		nameToSize := make(map[string]int32)
		logger := klog.FromContext(ctx)
		for i := range allRSs {
			rs := allRSs[i]

			// Estimate proportions if we have replicas to add, otherwise simply populate
			// nameToSize with the current sizes for each replica set.
			if deploymentReplicasToAdd != 0 {
				proportion := deploymentutil.GetProportion(logger, rs, *deployment, deploymentReplicasToAdd, deploymentReplicasAdded)

				nameToSize[rs.Name] = *(rs.Spec.Replicas) + proportion
				deploymentReplicasAdded += proportion
			} else {
				nameToSize[rs.Name] = *(rs.Spec.Replicas)
			}
		}

		// Update all replica sets
		for i := range allRSs {
			rs := allRSs[i]

			// Add/remove any leftovers to the largest replica set.
			if i == 0 &amp;&amp; deploymentReplicasToAdd != 0 {
				leftover := deploymentReplicasToAdd - deploymentReplicasAdded
				nameToSize[rs.Name] = nameToSize[rs.Name] + leftover
				if nameToSize[rs.Name] &lt; 0 {
					nameToSize[rs.Name] = 0
				}
			}

			// TODO: Use transactions when we have them.
			if _, _, err := dc.scaleReplicaSet(ctx, rs, nameToSize[rs.Name], deployment, scalingOperation); err != nil {
				// Return as soon as we fail, the deployment is requeued
				return err
			}
		}
	}
	return nil
}

//该函数是一个Go语言函数，名为scale，属于DeploymentController类型。
//它接收四个参数：ctx是一个上下文对象，deployment是一个Deployment指针，newRS是一个ReplicaSet指针，oldRSs是一个ReplicaSet指针数组。
//函数返回一个错误。
//该函数用于根据给定的部署（deployment）和新的副本集（newRS）来比例地调整副本集的大小，以减轻风险。
//函数首先检查是否有活跃或最新的副本集需要进行大小调整，如果有，则将其调整到部署的指定大小。
//如果新的副本集已饱和，则将旧的副本集完全缩放 down。
//如果部署是滚动更新类型，则按比例缩放所有副本集（新旧副本集）。
//在滚动更新的情况下，函数会根据部署的规格（deployment.Spec.Replicas）和最大突增值（MaxSurge）计算出可以添加或移除的额外副本数量，
//并将这些副本按比例分配给所有活跃的副本集。
//在缩放过程中，函数会根据副本集的大小进行排序，并根据缩放操作的类型（上/下）选择合适的排序方式。
//函数会迭代所有活跃的副本集，并根据比例计算出每个副本集应该增加或减少的副本数量。
//最后，函数会更新所有副本集的大小，并在遇到错误时返回错误。
//总结： 该函数用于根据给定的部署和新的副本集来比例地调整副本集的大小，以减轻风险。
//它会根据不同的条件来判断如何缩放副本集，并将缩放操作应用于所有活跃的副本集。
//该函数在滚动更新的情况下，会按比例分配额外的副本数量给所有活跃的副本集。

func (dc *DeploymentController) scaleReplicaSetAndRecordEvent(ctx context.Context, rs *apps.ReplicaSet, newScale int32, deployment *apps.Deployment) (bool, *apps.ReplicaSet, error) {
	// No need to scale
	if *(rs.Spec.Replicas) == newScale {
		return false, rs, nil
	}
	var scalingOperation string
	if *(rs.Spec.Replicas) &lt; newScale {
		scalingOperation = &#34;up&#34;
	} else {
		scalingOperation = &#34;down&#34;
	}
	scaled, newRS, err := dc.scaleReplicaSet(ctx, rs, newScale, deployment, scalingOperation)
	return scaled, newRS, err
}

//该函数是一个Go语言函数，名为scaleReplicaSetAndRecordEvent，属于DeploymentController类型。
//它接收四个参数：ctx是一个上下文对象，rs是一个ReplicaSet指针，newScale是一个int32类型的变量，deployment是一个Deployment指针。
//函数返回三个值：一个布尔值，一个ReplicaSet指针和一个错误。
//函数首先检查当前ReplicaSet的副本数量是否已经等于目标副本数量newScale，如果是，则直接返回不进行任何操作。
//如果需要进行缩放操作，则根据目标副本数量与当前副本数量的关系，确定是向上扩展还是向下缩小。
//接着，函数调用dc.scaleReplicaSet方法来执行实际的缩放操作，并将缩放操作的结果返回。
//最后，函数返回缩放操作是否成功、新的ReplicaSet指针以及可能的错误信息。
//总结： 该函数用于根据给定的目标副本数量对ReplicaSet进行缩放操作，并记录相关事件。
//它会检查当前副本数量是否已经等于目标数量，如果是则不进行操作；否则，根据目标数量与当前数量的关系确定是向上扩展还是向下缩小，并执行相应的缩放操作。

func (dc *DeploymentController) scaleReplicaSet(ctx context.Context, rs *apps.ReplicaSet, newScale int32, deployment *apps.Deployment, scalingOperation string) (bool, *apps.ReplicaSet, error) {

	sizeNeedsUpdate := *(rs.Spec.Replicas) != newScale

	annotationsNeedUpdate := deploymentutil.ReplicasAnnotationsNeedUpdate(rs, *(deployment.Spec.Replicas), *(deployment.Spec.Replicas)+deploymentutil.MaxSurge(*deployment))

	scaled := false
	var err error
	if sizeNeedsUpdate || annotationsNeedUpdate {
		oldScale := *(rs.Spec.Replicas)
		rsCopy := rs.DeepCopy()
		*(rsCopy.Spec.Replicas) = newScale
		deploymentutil.SetReplicasAnnotations(rsCopy, *(deployment.Spec.Replicas), *(deployment.Spec.Replicas)+deploymentutil.MaxSurge(*deployment))
		rs, err = dc.client.AppsV1().ReplicaSets(rsCopy.Namespace).Update(ctx, rsCopy, metav1.UpdateOptions{})
		if err == nil &amp;&amp; sizeNeedsUpdate {
			scaled = true
			dc.eventRecorder.Eventf(deployment, v1.EventTypeNormal, &#34;ScalingReplicaSet&#34;, &#34;Scaled %s replica set %s to %d from %d&#34;, scalingOperation, rs.Name, newScale, oldScale)
		}
	}
	return scaled, rs, err
}

//该函数用于缩放副本集的副本数量。
//它首先检查副本集的当前副本数量是否与目标副本数量不同，以及副本集的注解是否需要更新。
//如果任一条件为真，则创建一个副本集的深拷贝，并更新其副本数量和注解。
//然后，使用更新后的副本集调用客户端的Update方法来更新副本集。
//如果更新成功并且副本数量发生了变化，则记录一个事件表示副本集已被缩放。
//函数返回一个布尔值表示副本集是否被缩放，更新后的副本集对象和可能出现的错误。

// cleanupDeployment is responsible for cleaning up a deployment ie. retains all but the latest N old replica sets
// where N=d.Spec.RevisionHistoryLimit. Old replica sets are older versions of the podtemplate of a deployment kept
// around by default 1) for historical reasons and 2) for the ability to rollback a deployment.
func (dc *DeploymentController) cleanupDeployment(ctx context.Context, oldRSs []*apps.ReplicaSet, deployment *apps.Deployment) error {
	logger := klog.FromContext(ctx)
	if !deploymentutil.HasRevisionHistoryLimit(deployment) {
		return nil
	}

	// Avoid deleting replica set with deletion timestamp set
	aliveFilter := func(rs *apps.ReplicaSet) bool {
		return rs != nil &amp;&amp; rs.ObjectMeta.DeletionTimestamp == nil
	}
	cleanableRSes := controller.FilterReplicaSets(oldRSs, aliveFilter)

	diff := int32(len(cleanableRSes)) - *deployment.Spec.RevisionHistoryLimit
	if diff &lt;= 0 {
		return nil
	}

	sort.Sort(deploymentutil.ReplicaSetsByRevision(cleanableRSes))
	logger.V(4).Info(&#34;Looking to cleanup old replica sets for deployment&#34;, &#34;deployment&#34;, klog.KObj(deployment))

	for i := int32(0); i &lt; diff; i++ {
		rs := cleanableRSes[i]
		// Avoid delete replica set with non-zero replica counts
		if rs.Status.Replicas != 0 || *(rs.Spec.Replicas) != 0 || rs.Generation &gt; rs.Status.ObservedGeneration || rs.DeletionTimestamp != nil {
			continue
		}
		logger.V(4).Info(&#34;Trying to cleanup replica set for deployment&#34;, &#34;replicaSet&#34;, klog.KObj(rs), &#34;deployment&#34;, klog.KObj(deployment))
		if err := dc.client.AppsV1().ReplicaSets(rs.Namespace).Delete(ctx, rs.Name, metav1.DeleteOptions{}); err != nil &amp;&amp; !errors.IsNotFound(err) {
			// Return error instead of aggregating and continuing DELETEs on the theory
			// that we may be overloading the api server.
			return err
		}
	}

	return nil
}

//该函数用于清理部署，即保留最新N个旧的副本集，其中N等于d.Spec.RevisionHistoryLimit。
//旧的副本集是部署的pod模板的旧版本，保留它们的原因有：
//1）历史原因；
//2）为了能够回滚部署。函数首先检查部署是否设置了修订历史限制，如果没有设置，则直接返回。
//然后，过滤掉具有删除时间戳的副本集，并计算需要清理的副本集数量。
//如果需要清理的副本集数量小于等于0，则直接返回。
//接下来，按修订版本对可清理的副本集进行排序，并尝试清理旧的副本集。
//对于每个需要清理的副本集，如果其副本数量不为零，或者期望的副本数量不为零，或者其生成代数大于观察到的生成代数，或者具有删除时间戳，则跳过清理。
//最后，如果清理过程中发生错误，则返回错误。

// syncDeploymentStatus checks if the status is up-to-date and sync it if necessary
func (dc *DeploymentController) syncDeploymentStatus(ctx context.Context, allRSs []*apps.ReplicaSet, newRS *apps.ReplicaSet, d *apps.Deployment) error {
	newStatus := calculateStatus(allRSs, newRS, d)

	if reflect.DeepEqual(d.Status, newStatus) {
		return nil
	}

	newDeployment := d
	newDeployment.Status = newStatus
	_, err := dc.client.AppsV1().Deployments(newDeployment.Namespace).UpdateStatus(ctx, newDeployment, metav1.UpdateOptions{})
	return err
}

//该函数是一个Go语言函数，用于检查部署（Deployment）的状态是否是最新的，如果不是最新的，则将其同步到最新的状态。
//函数定义在DeploymentController结构体中。
//函数接受四个参数：
//- ctx：上下文对象，用于控制函数执行期间的流程。
//- allRSs：一个包含所有副本集（ReplicaSet）的切片。
//- newRS：一个新的副本集。
//- d：一个部署对象。
//函数首先调用calculateStatus函数来计算最新的状态。
//然后，它将检查当前部署的状态是否与计算出的最新状态相等。
//如果相等，函数将直接返回，不做任何操作。
//如果不相等，函数将更新部署的状态为最新状态，并调用UpdateStatus方法将其更新到Kubernetes集群中。
//最后，函数返回可能发生的错误。

// calculateStatus calculates the latest status for the provided deployment by looking into the provided replica sets.
func calculateStatus(allRSs []*apps.ReplicaSet, newRS *apps.ReplicaSet, deployment *apps.Deployment) apps.DeploymentStatus {
	availableReplicas := deploymentutil.GetAvailableReplicaCountForReplicaSets(allRSs)
	totalReplicas := deploymentutil.GetReplicaCountForReplicaSets(allRSs)
	unavailableReplicas := totalReplicas - availableReplicas
	// If unavailableReplicas is negative, then that means the Deployment has more available replicas running than
	// desired, e.g. whenever it scales down. In such a case we should simply default unavailableReplicas to zero.
	if unavailableReplicas &lt; 0 {
		unavailableReplicas = 0
	}

	status := apps.DeploymentStatus{
		// TODO: Ensure that if we start retrying status updates, we won&#39;t pick up a new Generation value.
		ObservedGeneration:  deployment.Generation,
		Replicas:            deploymentutil.GetActualReplicaCountForReplicaSets(allRSs),
		UpdatedReplicas:     deploymentutil.GetActualReplicaCountForReplicaSets([]*apps.ReplicaSet{newRS}),
		ReadyReplicas:       deploymentutil.GetReadyReplicaCountForReplicaSets(allRSs),
		AvailableReplicas:   availableReplicas,
		UnavailableReplicas: unavailableReplicas,
		CollisionCount:      deployment.Status.CollisionCount,
	}

	// Copy conditions one by one so we won&#39;t mutate the original object.
	conditions := deployment.Status.Conditions
	for i := range conditions {
		status.Conditions = append(status.Conditions, conditions[i])
	}

	if availableReplicas &gt;= *(deployment.Spec.Replicas)-deploymentutil.MaxUnavailable(*deployment) {
		minAvailability := deploymentutil.NewDeploymentCondition(apps.DeploymentAvailable, v1.ConditionTrue, deploymentutil.MinimumReplicasAvailable, &#34;Deployment has minimum availability.&#34;)
		deploymentutil.SetDeploymentCondition(&amp;status, *minAvailability)
	} else {
		noMinAvailability := deploymentutil.NewDeploymentCondition(apps.DeploymentAvailable, v1.ConditionFalse, deploymentutil.MinimumReplicasUnavailable, &#34;Deployment does not have minimum availability.&#34;)
		deploymentutil.SetDeploymentCondition(&amp;status, *noMinAvailability)
	}

	return status
}

// isScalingEvent checks whether the provided deployment has been updated with a scaling event
// by looking at the desired-replicas annotation in the active replica sets of the deployment.
//
// rsList should come from getReplicaSetsForDeployment(d).
func (dc *DeploymentController) isScalingEvent(ctx context.Context, d *apps.Deployment, rsList []*apps.ReplicaSet) (bool, error) {
	newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(ctx, d, rsList, false)
	if err != nil {
		return false, err
	}
	allRSs := append(oldRSs, newRS)
	logger := klog.FromContext(ctx)
	for _, rs := range controller.FilterActiveReplicaSets(allRSs) {
		desired, ok := deploymentutil.GetDesiredReplicasAnnotation(logger, rs)
		if !ok {
			continue
		}
		if desired != *(d.Spec.Replicas) {
			return true, nil
		}
	}
	return false, nil
}

//该函数用于检查提供的deployment是否已通过调整活动replica set的数量来进行更新。
//它通过查看deployment的desired-replicas注解来判断
//。函数首先获取deployment的所有replica set并同步修订版本号，然后遍历所有活动的replica set，
//检查其desired-replicas注解与deployment的期望副本数是否一致，如果一致则返回false，否则返回true。
</code></pre></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents></nav></div></aside></main></body></html>