<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  Kubernetes
  #


  谈谈你对k8s的理解☆
  #

Kubernetes（简称 K8s）是一个开源的容器编排平台，旨在自动化容器化应用程序的部署、扩展和管理。它最初由 Google 设计，并在2014年开源。K8s 提供了一个集中的平台来管理和运行容器化的应用程序，尤其适合大规模的分布式系统和微服务架构。
从技术角度看，K8s 主要有以下几个核心组成部分：

集群（Cluster）： K8s 将多个节点组织成一个集群，集群中通常有两类节点：控制平面节点（Control Plane）和工作节点（Worker Node）。控制平面负责集群的管理与调度，而工作节点运行实际的应用程序。
Pod： Pod 是 Kubernetes 中最小的调度单位，通常包含一个或多个容器，这些容器共享同一个网络、存储和其他资源。Pod 是容器运行的基本单位。
Deployment： Deployment 用于管理和控制应用程序的副本和版本。它通过定义副本数来保证应用的高可用性，支持滚动更新、回滚等功能。
Service： Service 用于暴露应用程序的网络接口，并提供负载均衡。通过 Service，用户可以通过稳定的 IP 或 DNS 名称访问一组 Pod。
Ingress： Ingress 提供 HTTP 和 HTTPS 路由到集群内的服务，可以用于管理外部流量进入集群，支持负载均衡、SSL 终端等功能。
ConfigMap 和 Secret： ConfigMap 用于管理应用的配置数据，Secret 用于存储敏感数据（如密码、API 密钥等）。这些资源可以被容器在运行时动态加载。
Namespace： Namespace 提供了对资源的隔离，允许在同一个集群中多个团队或应用之间共享资源而不发生冲突。它相当于虚拟化的逻辑分区。
StatefulSet： StatefulSet 是一种控制器，用于管理有状态应用，如数据库等。与 Deployment 不同，StatefulSet 保证了 Pod 的稳定性和唯一性，适合需要持久存储和有序部署的应用。
Persistent Volume (PV) 和 Persistent Volume Claim (PVC)： PV 是集群内的存储资源，而 PVC 是用户对存储资源的请求。通过这种方式，K8s 支持动态存储的创建和销毁。


  关键优势：
  #


自动化：K8s 提供了自动化部署、滚动更新、扩展和恢复功能，极大地简化了应用程序生命周期管理。
高可用性：K8s 支持容错和高可用性，可以通过多副本部署、健康检查、自动恢复等机制保证应用的可靠性。
灵活性：通过声明式配置，K8s 让开发者可以专注于应用本身，而不必担心基础设施细节。
扩展性：K8s 提供了强大的插件和扩展机制，支持各种中间件、监控、日志等功能的集成。


  适用场景：
  #


微服务架构：K8s 非常适合容器化微服务的管理，能有效地处理服务之间的依赖、负载均衡、容错等。
大规模分布式应用：由于其自动化和扩展性，K8s 很适合处理大规模、复杂的分布式系统。
混合云/多云架构：K8s 提供跨平台的支持，使得应用能够在不同的云环境或本地数据中心之间无缝迁移。

总的来说，Kubernetes 是现代 DevOps 和持续集成/持续交付（CI/CD）的核心技术之一，能够有效地管理容器化应用，使得开发和运维更加高效、灵活和可靠。"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://qq547475331.github.io/docs/2025-2-26-k8s%E7%9B%B8%E5%85%B3/"><meta property="og:site_name" content="Guichen's Blog"><meta property="og:title" content="2025-2-16 k8s题目"><meta property="og:description" content="Kubernetes # 谈谈你对k8s的理解☆ # Kubernetes（简称 K8s）是一个开源的容器编排平台，旨在自动化容器化应用程序的部署、扩展和管理。它最初由 Google 设计，并在2014年开源。K8s 提供了一个集中的平台来管理和运行容器化的应用程序，尤其适合大规模的分布式系统和微服务架构。
从技术角度看，K8s 主要有以下几个核心组成部分：
集群（Cluster）： K8s 将多个节点组织成一个集群，集群中通常有两类节点：控制平面节点（Control Plane）和工作节点（Worker Node）。控制平面负责集群的管理与调度，而工作节点运行实际的应用程序。 Pod： Pod 是 Kubernetes 中最小的调度单位，通常包含一个或多个容器，这些容器共享同一个网络、存储和其他资源。Pod 是容器运行的基本单位。 Deployment： Deployment 用于管理和控制应用程序的副本和版本。它通过定义副本数来保证应用的高可用性，支持滚动更新、回滚等功能。 Service： Service 用于暴露应用程序的网络接口，并提供负载均衡。通过 Service，用户可以通过稳定的 IP 或 DNS 名称访问一组 Pod。 Ingress： Ingress 提供 HTTP 和 HTTPS 路由到集群内的服务，可以用于管理外部流量进入集群，支持负载均衡、SSL 终端等功能。 ConfigMap 和 Secret： ConfigMap 用于管理应用的配置数据，Secret 用于存储敏感数据（如密码、API 密钥等）。这些资源可以被容器在运行时动态加载。 Namespace： Namespace 提供了对资源的隔离，允许在同一个集群中多个团队或应用之间共享资源而不发生冲突。它相当于虚拟化的逻辑分区。 StatefulSet： StatefulSet 是一种控制器，用于管理有状态应用，如数据库等。与 Deployment 不同，StatefulSet 保证了 Pod 的稳定性和唯一性，适合需要持久存储和有序部署的应用。 Persistent Volume (PV) 和 Persistent Volume Claim (PVC)： PV 是集群内的存储资源，而 PVC 是用户对存储资源的请求。通过这种方式，K8s 支持动态存储的创建和销毁。 关键优势： # 自动化：K8s 提供了自动化部署、滚动更新、扩展和恢复功能，极大地简化了应用程序生命周期管理。 高可用性：K8s 支持容错和高可用性，可以通过多副本部署、健康检查、自动恢复等机制保证应用的可靠性。 灵活性：通过声明式配置，K8s 让开发者可以专注于应用本身，而不必担心基础设施细节。 扩展性：K8s 提供了强大的插件和扩展机制，支持各种中间件、监控、日志等功能的集成。 适用场景： # 微服务架构：K8s 非常适合容器化微服务的管理，能有效地处理服务之间的依赖、负载均衡、容错等。 大规模分布式应用：由于其自动化和扩展性，K8s 很适合处理大规模、复杂的分布式系统。 混合云/多云架构：K8s 提供跨平台的支持，使得应用能够在不同的云环境或本地数据中心之间无缝迁移。 总的来说，Kubernetes 是现代 DevOps 和持续集成/持续交付（CI/CD）的核心技术之一，能够有效地管理容器化应用，使得开发和运维更加高效、灵活和可靠。"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><title>2025-2-16 k8s题目 | Guichen's Blog</title>
<link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://qq547475331.github.io/docs/2025-2-26-k8s%E7%9B%B8%E5%85%B3/><link rel=stylesheet href=/book.min.6c8b9d2a1fc95075ed7da46ca81060b39add8fff6741ac51259f768929281e2c.css integrity="sha256-bIudKh/JUHXtfaRsqBBgs5rdj/9nQaxRJZ92iSkoHiw=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.f95c6b9a60666b7048cee04630cfc1e8d9370fb96c18c5c4d047db5e7a5d2e18.js integrity="sha256-+VxrmmBma3BIzuBGMM/B6Nk3D7lsGMXE0EfbXnpdLhg=" crossorigin=anonymous></script></head><script src=https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.2.3/mermaid.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){mermaid.initialize({startOnLoad:!0});let e=document.querySelectorAll("pre > code.language-mermaid");e.forEach(e=>{let t=document.createElement("div");t.classList.add("mermaid"),t.innerHTML=e.innerText,e.parentNode.replaceWith(t)}),mermaid.init(void 0,document.querySelectorAll(".mermaid"))})</script><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>Guichen's Blog</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><a href=/docs/2025-4-16-%E8%87%AA%E7%A0%94k8s%E5%B9%B3%E5%8F%B0/>2025-4-16 自研k8s平台</a></li><li><a href=/docs/2025-4-16-sleep%E7%9D%A1%E7%9C%A0%E5%BA%94%E7%94%A8/>2025-4-16 sleep睡眠应用</a></li><li><a href=/docs/2025-4-16-paas%E8%AE%BE%E8%AE%A1/>2025-4-16 paas开发记录</a></li><li><a href=/docs/2025-4-16-cursoe-free-vip/>2025-4-16 Cursor Free VIP</a></li><li><a href=/docs/2025-4-16-boss%E7%9B%B4%E8%81%98%E8%87%AA%E5%8A%A8%E6%8A%95%E9%80%92/>2025-4-16 BOSS直聘自动投递</a></li><li><a href=/docs/2025-3-30-metallb/>2025-3-30 metallb</a></li><li><a href=/docs/2025-3-24-%E8%87%AA%E6%88%91%E4%BB%8B%E7%BB%8D/>2025-3-24 自我介绍</a></li><li><a href=/docs/2025-3-20-victoriametrics-%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84/>2025-3-20 victoriametrics高可用架构</a></li><li><a href=/docs/2025-3-20-victoriametrics%E6%9E%B6%E6%9E%84/>2025-3-20 victoriametrics 架构</a></li><li><a href=/docs/2025-3-20-victoriametrics%E5%92%8Cthanos%E5%AF%B9%E6%AF%94/>2025-3-20 VictoriaMetrics 和 Thanos 对比</a></li><li><a href=/docs/2025-3-20-thanos%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84/>2025-3-20 thanos高可用架构</a></li><li><a href=/docs/2025-3-20-thanos%E6%9E%B6%E6%9E%84/>2025-3-20 thanos架构</a></li><li><a href=/docs/2025-3-18-5w-pod%E5%8E%8B%E6%B5%8B%E5%A4%8D%E7%9B%98/>2025-3-18 5w pod压测复盘</a></li><li><a href=/docs/2025-3-14-%E7%81%AB%E5%B1%B1%E4%BA%91%E8%BF%81%E7%A7%BB%E5%B7%A5%E7%A8%8B%E5%B8%88%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/>2025-3-14 火山云迁移工程师面试记录</a></li><li><a href=/docs/2025-3-14-vivo%E9%9D%A2%E8%AF%95/>2025-3-14 vivo面试</a></li><li><a href=/docs/2025-3-13-istio%E6%B5%81%E9%87%8F%E5%88%86%E6%9E%90/>2025-3-13 istio流量分析</a></li><li><a href=/docs/2025-3-13-calico%E4%B8%89%E7%A7%8D%E6%A8%A1%E5%BC%8F%E4%B8%8B%E6%B5%81%E9%87%8F%E4%BC%A0%E8%BE%93%E8%B7%AF%E5%BE%84%E5%88%86%E6%9E%90/>2025-3-13 calico三种模式下流量传输</a></li><li><a href=/docs/2025-3-12-%E5%A1%94%E8%B5%9E%E9%9D%A2%E8%AF%95/>2025-3-12 塔赞面试</a></li><li><a href=/docs/2025-3-12-%E8%BF%BD%E8%A7%85%E9%9D%A2%E8%AF%95/>2025-3-12 追觅面试</a></li><li><a href=/docs/2025-3-8-k8s%E5%88%A0%E9%99%A4pod-deployment%E7%9A%84%E6%B5%81%E7%A8%8B%E5%9B%BE%E8%AF%A6%E8%A7%A3/>2025-3-08 k8s删除pod或deployment的流程图详解</a></li><li><a href=/docs/2025-3-8-k8s%E5%88%9B%E5%BB%BApod-deployment%E6%B5%81%E7%A8%8B%E5%9B%BE%E8%AF%A6%E8%A7%A3/>2025-3-08 k8s创建pod流程图详解</a></li><li><a href=/docs/2025-2-28-prometheus%E9%A2%98%E7%9B%AE/>2025-2-28 prometheus面试题</a></li><li><a href=/docs/2025-2-26-%E9%9D%A2%E8%AF%950225/>2025-2-25 面试0225</a></li><li><a href=/docs/2025-2-24-%E9%AB%98%E7%BA%A7%E8%BF%90%E7%BB%B4%E9%9D%A2%E8%AF%95%E9%A2%98_ai_linux%E9%83%A8%E5%88%86/>2025-2-24 高级运维面试题-linux部分</a></li><li><a href=/docs/2025-2-24-%E4%B8%AD%E7%BA%A7%E8%BF%90%E7%BB%B4%E9%9D%A2%E8%AF%95%E9%A2%98_%E9%A2%98%E7%9B%AE/>2025-2-24 中级运维面试题</a></li><li><a href=/docs/2025-2-24-%E9%9D%A2%E8%AF%950224/>2025-2-24 0224面试</a></li><li><a href=/docs/2025-2-20-%E9%9D%A2%E8%AF%950220/>2025-2-20 面试0220</a></li><li><a href=/docs/2025-2-19-%E9%9D%A2%E8%AF%950219/>2025-2-19 面试0219</a></li><li><a href=/docs/2025-2-18-%E9%9D%A2%E8%AF%95/>2025-2-18 面试2025-0218</a></li><li><a href=/docs/2025-2-26-k8s%E7%9B%B8%E5%85%B3/ class=active>2025-2-16 k8s题目</a></li><li><a href=/docs/2025-2-12-%E9%9D%A2%E8%AF%950212/>2025-2-12 面试0212</a></li><li><a href=/docs/2025-2-11-%E9%9D%A2%E8%AF%950211/>2025-2-11 面试2025-02-11</a></li><li><a href=/docs/2025-2-7-%E8%AE%A1%E5%88%922/>2025-2-07 美国码农计划</a></li><li><a href=/docs/2025-2-7-%E8%AE%A1%E5%88%92/>2025-2-07 美国码农薪酬</a></li><li><a href=/docs/2025-2-7-k8s%E7%BB%84%E4%BB%B6/>2025-2-07 k8s组件</a></li><li><a href=/docs/2025-1-16-k8s%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E6%8C%87%E5%8D%97/>2025-1-16 k8s常见故障指南</a></li><li><a href=/docs/2025-1-1-%E8%A6%81%E4%B8%8D%E8%A6%81%E5%88%9B%E4%B8%9A/>2025-1-1 要不要创业</a></li><li><a href=/docs/2025-1-1-%E6%97%A9%E6%9C%9F%E6%A8%A1%E5%BC%8F/>2025-1-1 早期模式</a></li><li><a href=/docs/2025-1-1-%E5%A4%A7%E5%A0%B0%E6%B2%B3-%E6%88%91%E7%9A%84%E4%BF%9D%E5%A7%86/>2025-1-1 大堰河-我的保姆</a></li><li><a href=/docs/2025-1-1-%E5%88%9D%E5%88%9B%E5%85%AC%E5%8F%B8/>2025-1-1 初创公司</a></li><li><a href=/docs/2025-1-1-%E5%88%9B%E4%B8%9A%E8%80%85%E4%BA%A4%E6%B5%81/>2025-1-1 创业者交流</a></li><li><a href=/docs/2025-1-1-%E5%88%9B%E4%B8%9A%E7%82%B9%E5%AD%90/>2025-1-1 创业点子</a></li><li><a href=/docs/2025-1-1-sealos%E8%8E%B7%E6%8A%95/>2025-1-1 sealos获投</a></li><li><a href=/docs/2024-12-10-docker-registrry/>2024-12-10 docker registrry</a></li><li><a href=/docs/2024-12-09-openstack-ssh%E8%BF%9E%E6%8E%A5/>2024-12-09 openstack ssh连接</a></li><li><a href=/docs/2024-12-08-mutilpass%E9%83%A8%E7%BD%B2openstack/>2024-12-09 mutilpass部署openstack devstack形式</a></li><li><a href=/docs/2024-12-09-helmchart-%E9%83%A8%E7%BD%B2flask%E5%BA%94%E7%94%A8/>2024-12-09 helmchart 部署flask应用</a></li><li><a href=/docs/2024-12-09-docker-daemon.json/>2024-12-09 docker daemon.json</a></li><li><a href=/docs/2024-12-08-%E5%9D%97%E5%AD%98%E5%82%A8%E5%92%8C%E5%AF%B9%E8%B1%A1%E5%82%A8%E5%AD%98%E5%8C%BA%E5%88%AB/>2024-12-08 块存储和对象储存区别</a></li><li><a href=/docs/2024-12-08-openstack%E9%9C%80%E8%A6%81%E5%87%A0%E5%8F%B0%E8%99%9A%E6%8B%9F%E6%9C%BA/>2024-12-08 openstack需要几台虚拟机</a></li><li><a href=/docs/2024-12-08-openstack%E5%92%8Ckubernetes%E5%8C%BA%E5%88%AB/>2024-12-08 openstack和kubernetes区别</a></li><li><a href=/docs/2024-12-08-nano%E6%93%8D%E4%BD%9C/>2024-12-08 nano操作</a></li><li><a href=/docs/2024-12-08-mutilpass%E6%93%8D%E4%BD%9C/>2024-12-08 mutilpass操作</a></li><li><a href=/docs/2024-12-08-devstack/>2024-12-08 devstack</a></li><li><a href=/docs/2024-12-07-microk8s/>2024-12-07 microk8s</a></li><li><a href=/docs/2024-12-05-kubeasz%E9%83%A8%E7%BD%B2k8s/>2024-12-05 kubeasz部署k8s</a></li><li><a href=/docs/2024-10-20-%E5%88%9B%E5%BB%BA%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/>2024-10-20 使用 Keepalived 和 HAproxy 创建高可用 Kubernetes 集群</a></li><li><a href=/docs/%E9%A1%B6%E7%BA%A7devops%E5%B7%A5%E5%85%B7%E5%A4%A7%E7%9B%98%E7%82%B9-ding-ji-devops-gong-ju-da-pan-dian/>2024-08-02 顶级devops工具大盘点</a></li><li><a href=/docs/%E6%B8%85%E7%90%86docker%E9%95%9C%E5%83%8F-qing-li-docker-jing-xiang/>2024-08-02 清理docker镜像</a></li><li><a href=/docs/%E6%9E%84%E5%BB%BA%E5%AE%B9%E5%99%A8%E9%95%9C%E5%83%8F%E5%88%A9%E5%99%A8buildkit-gou-jian-rong-qi-jing-xiang-li-qi-buildkit/>2024-08-02 构建容器镜像利器buildkit</a></li><li><a href=/docs/%E6%98%AF%E6%8A%80%E6%9C%AF%E5%A4%A7%E7%A5%9E%E8%BF%98%E6%98%AF%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84%E9%83%A8%E7%9A%84%E7%A5%B8%E5%AE%B3-shi-ji-shu-da-shen-hai-shi-ji-chu-jia-gou-bu-de-huo-hai/>2024-08-02 是技术大神还是基础架构部的祸害</a></li><li><a href=/docs/%E6%90%AD%E4%B8%AA%E6%97%A5%E5%BF%97%E6%89%8B%E6%9C%BA%E7%B3%BB%E7%BB%9F%E4%B8%8D%E9%A6%99%E5%90%97-da-ge-ri-zhi-shou-ji-xi-tong-bu-xiang-ma/>2024-08-02 搭个日志手机系统不香吗</a></li><li><a href=/docs/%E6%88%91%E5%8F%AA%E6%83%B3%E5%81%9A%E6%8A%80%E6%9C%AF-%E8%B5%B0%E6%8A%80%E6%9C%AF%E8%B7%AF%E7%BA%BF-wo-zhi-xiang-zuo-ji-shu-zou-ji-shu-lu-xian/>2024-08-02 我只想做技术 走技术路线</a></li><li><a href=/docs/%E5%B8%B8%E8%A7%81linux%E8%BF%90%E7%BB%B4%E9%9D%A2%E8%AF%95%E9%A2%98-chang-jian-linux-yun-wei-mian-shi-ti/>2024-08-02 常见linux运维面试题</a></li><li><a href=/docs/%E5%A4%A7%E5%8E%82%E6%80%BB%E7%BB%93nginx%E9%AB%98%E5%B9%B6%E5%8F%91%E4%BC%98%E5%8C%96%E7%AC%94%E8%AE%B0-da-chang-zong-jie-nginx-gao-bing-fa-you-hua-bi-ji/>2024-08-02 大厂总结nginx高并发优化笔记</a></li><li><a href=/docs/%E5%8F%B2%E4%B8%8A%E6%9C%80%E7%89%9Bjenkins-pipeline%E6%B5%81%E6%B0%B4%E7%BA%BF%E8%AF%A6%E8%A7%A3-shi-shang-zui-niu-jenkinspipeline-liu-shui-xian-xiang-jie/>2024-08-02 史上最牛jenkins pipeline流水线详解</a></li><li><a href=/docs/teg%E4%B8%8Eistio%E9%9B%86%E6%88%90-teg-yu-istio-ji-cheng/>2024-08-02 TEG与istio集成</a></li><li><a href=/docs/prometheus-stack-prometheus-stack/>2024-08-02 prometheus-stack</a></li><li><a href=/docs/pixie-pixie/>2024-08-02 pixie</a></li><li><a href=/docs/nginx%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E6%83%8A%E7%BE%A4%E6%95%88%E5%BA%94-nginx-ru-he-jie-jue-jing-qun-xiao-ying/>2024-08-02 nginx如何解决惊群效应</a></li><li><a href=/docs/netctl%E6%A3%80%E6%B5%8B%E9%9B%86%E7%BE%A4pod%E9%97%B4%E8%BF%9E%E9%80%9A%E6%80%A7-netctl-jian-ce-ji-qun-pod-jian-lian-tong-xing/>2024-08-02 netctl检测集群pod间连通性</a></li><li><a href=/docs/linux%E8%BF%90%E7%BB%B4%E5%B7%A5%E7%A8%8B%E5%B8%8850%E4%B8%AA%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98-linux-yun-wei-gong-cheng-shi-50-ge-chang-jian-mian-shi-ti/>2024-08-02 linux运维工程师50个常见面试题</a></li><li><a href=/docs/linux%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-%E4%B8%83%E4%B8%AA%E5%AE%9E%E6%88%98%E7%BB%8F%E9%AA%8C-linux-xi-tong-xing-neng-you-hua-qi-ge-shi-zhan-jing-yan/>2024-08-02 linux系统性能优化 七个实战经验</a></li><li><a href=/docs/linux-awk%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E5%99%A8-8%E4%B8%AA%E6%A1%88%E4%BE%8B-linuxawk-wen-ben-chu-li-qi-8-ge-an-li/>2024-08-02 linux awk文本处理器 8个案例</a></li><li><a href=/docs/kubewharf-kubewharf/>2024-08-02 kubewharf</a></li><li><a href=/docs/kruise%E5%8E%9F%E5%9C%B0%E5%8D%87%E7%BA%A7%E8%A7%A3%E6%9E%90-kruise-yuan-de-sheng-ji-jie-xi/>2024-08-02 kruise原地升级解析</a></li><li><a href=/docs/k8s%E9%9D%A2%E8%AF%95%E9%A2%98-k8s-mian-shi-ti/>2024-08-02 K8S面试题</a></li><li><a href=/docs/k8s%E8%83%8C%E5%90%8Eservice%E6%98%AF%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C%E7%9A%84-k8s-bei-hou-service-shi-ru-he-gong-zuo-de/>2024-08-02 k8s背后service是如何工作的</a></li><li><a href=/docs/k8s%E7%9A%84%E6%9C%80%E5%90%8E%E4%B8%80%E5%9D%97%E6%8B%BC%E5%9B%BE-dbpaas-k8s-de-zui-hou-yi-kuai-pin-tu-dbpaas/>2024-08-02 K8S的最后一块拼图</a></li><li><a href=/docs/istio%E9%83%A8%E7%BD%B2-istio-bu-shu/>2024-08-02 istio部署</a></li><li><a href=/docs/istio-ingress-gateway-istio-ingress-gateway/>2024-08-02 istio-ingress-gateway</a></li><li><a href=/docs/godel-scheduler-godel-scheduler/>2024-08-02 godel-scheduler</a></li><li><a href=/docs/dockerfile%E5%AE%9A%E5%88%B6%E4%B8%93%E5%B1%9E%E9%95%9C%E5%83%8F-dockerfile-ding-zhi-zhuan-shu-jing-xiang/>2024-08-02 dockerfile定制专属镜像</a></li><li><a href=/docs/33%E6%AC%BEgitops%E4%B8%8Edevops%E4%B8%BB%E6%B5%81%E7%B3%BB%E7%BB%9F-33-kuan-gitops-yu-devops-zhu-liu-xi-tong/>2024-08-02 33款gitops与devops主流系统</a></li><li><a href=/docs/2024-8-1-linux%E8%BF%90%E7%BB%B4%E5%B7%A5%E7%A8%8B%E5%B8%8850%E4%B8%AA%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/>2024-08-01 linux面试题</a></li><li><a href=/docs/2024-8-1-%E5%B8%B8%E8%A7%81linux%E8%BF%90%E7%BB%B4%E9%9D%A2%E8%AF%95%E9%A2%98%E6%89%BE%E5%B7%A5%E4%BD%9C%E7%9A%84%E5%BF%85%E7%9C%8B/>2024-08-01 linux运维面试题</a></li><li><a href=/docs/2024-8-1-kubernetes%E9%9D%A2%E8%AF%95%E9%A2%98/>2024-08-01 k8s面试题</a></li><li><a href=/docs/openkruise%E8%AF%A6%E7%BB%86%E8%A7%A3%E9%87%8A%E4%BB%A5%E5%8F%8A%E5%8E%9F%E5%9C%B0%E5%8D%87%E7%BA%A7%E5%8F%8A%E5%85%A8%E9%93%BE%E8%B7%AF%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83%E6%96%B9%E6%A1%88-openkruise-xiang-xi-jie-shi-yi-ji-yuan-de-sheng-ji-ji-quan-lian-lu-hui-du-fa-bu-fang-an/>2024-07-22 OpenKruise详细解释以及原地升级及全链路灰度发布方案</a></li><li><a href=/docs/k8s%E4%B9%8Bingress-nginx%E5%8E%9F%E7%90%86%E5%8F%8A%E9%85%8D%E7%BD%AE-k8s-zhi-ingress-nginx-yuan-li-ji-pei-zhi/>2024-07-05 K8S之ingress-nginx原理及配置</a></li><li><a href=/docs/%E4%BD%BF%E7%94%A8cloudflarecf%E6%90%AD%E5%BB%BAdockerhub%E4%BB%A3%E7%90%86-shi-yong-cloudflarecf-da-jian-dockerhub-dai-li/>2024-06-28 使用cloudflare(CF)搭建dockerhub代理</a></li><li><a href=/docs/2024-5-14-%E5%8D%95master%E5%8D%95etcd%E6%94%B9%E9%80%A0/>2024-05-01 单master单etcd改造为3master3etcd</a></li><li><a href=/docs/2024-4-17-%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93/>2024-04-17 面试总结</a></li><li><a href=/docs/%E5%A6%82%E4%BD%95%E4%B8%BAk8s%E4%BF%9D%E9%A9%BE%E6%8A%A4%E8%88%AA-ru-he-wei-k8s-bao-jia-hu-hang/>2024-04-16 如何为K8S保驾护航</a></li><li><a href=/docs/k8s%E5%A6%82%E4%BD%95%E8%8E%B7%E5%BE%97-ip-k8s-ru-he-huo-de-ip/>2024-04-16 K8S如何获得 IP</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Bstateful_setgo%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-statefulsetgo-yuan-ma-jie-du/>2024-04-10 K8S控制器之stateful_set.go源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Bstateful_set_status_updatego%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-statefulsetstatusupdatego-yuan-ma-jie-du/>2024-04-10 K8S控制器之stateful_set_status_update.go源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Bstateful_set_controlgo%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-statefulsetcontrolgo-yuan-ma-jie-du/>2024-04-10 K8S控制器之stateful_set_control.go源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Bstateful_pod_controlgo%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-statefulpodcontrolgo-yuan-ma-jie-du/>2024-04-10 K8S控制器之stateful_pod_control.go源码解读</a></li><li><a href=/docs/k8s%E8%B0%83%E5%BA%A6%E5%99%A8-extendergo-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-diao-du-qi-extendergo-yuan-ma-jie-du/>2024-04-09 K8S调度器 extender.go 源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Bsyncgo-%E5%90%8C%E6%AD%A5-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-syncgo-tong-bu-yuan-ma-jie-du/>2024-04-09 K8S控制器之sync.go 同步 源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Brollbackgo-%E5%9B%9E%E6%BB%9A-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-rollbackgo-hui-gun-yuan-ma-jie-du/>2024-04-09 K8S控制器之rollback.go 回滚 源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Brecreatego-%E9%87%8D%E5%BB%BA-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-recreatego-zhong-jian-yuan-ma-jie-du/>2024-04-09 K8S控制器之recreate.go 重建 源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8B-schedulergo-%E8%B0%83%E5%BA%A6%E5%99%A8-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-schedulergo-diao-du-qi-yuan-ma-jie-du/>2024-04-09 K8S控制器之 scheduler.go 调度器 源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8B-rollinggo-%E6%BB%9A%E5%8A%A8%E6%9B%B4%E6%96%B0-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-rollinggo-gun-dong-geng-xin-yuan-ma-jie-du/>2024-04-09 K8S控制器之 rolling.go 滚动更新 源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8B-progressgo-%E8%BF%9B%E5%BA%A6-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-progressgo-jin-du-yuan-ma-jie-du/>2024-04-09 K8S控制器之 progress.go 进度 源码解读</a></li><li><a href=/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8B-deployment_controllergo%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-deploymentcontrollergo-yuan-ma-jie-du/>2024-04-09 K8S控制器之 deployment_controller.go源码解读</a></li><li><a href=/docs/k8s-%E8%B0%83%E5%BA%A6%E5%99%A8-scheduler_onego-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-diao-du-qi-scheduleronego-yuan-ma-jie-du/>2024-04-09 K8S 调度器 scheduler_one.go 源码解读</a></li><li><a href=/docs/%E5%BD%BB%E6%82%9F%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C-che-wu-rong-qi-wang-luo/>2024-04-07 彻悟容器网络</a></li><li><a href=/docs/%E9%9D%A2%E8%AF%95%E7%94%A8-golang-%E6%89%8B%E6%92%B8-lru-mian-shi-yong-golang-shou-lu-lru/>2024-04-03 面试用 Golang 手撸 LRU</a></li><li><a href=/docs/%E8%87%AA%E5%8A%A8%E5%B1%8F%E8%94%BDip%E6%94%BB%E5%87%BB-zi-dong-ping-bi-ip-gong-ji/>2024-04-03 自动屏蔽IP攻击</a></li><li><a href=/docs/%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85kubephere-li-xian-an-zhuang-kubephere/>2024-04-03 离线安装kubephere</a></li><li><a href=/docs/%E7%A3%81%E7%9B%98%E6%95%B0%E6%8D%AE%E6%81%A2%E5%A4%8D-ci-pan-shu-ju-hui-fu/>2024-04-03 磁盘数据恢复</a></li><li><a href=/docs/%E6%B8%85%E7%90%86%E6%AE%8B%E7%95%99%E7%9A%84calico%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6-qing-li-can-liu-de-calico-wang-luo/>2024-04-03 清理残留的calico网络插件</a></li><li><a href=/docs/%E6%B5%81%E9%87%8F%E4%BD%95%E5%A4%84%E6%9D%A5%E4%BD%95%E5%A4%84%E5%8E%BB-liu-liang-he-chu-lai-he-chu-qu/>2024-04-03 流量何处来何处去</a></li><li><a href=/docs/%E6%9E%81%E5%A4%A7%E6%8F%90%E9%AB%98%E5%B7%A5%E4%BD%9C%E6%95%88%E7%8E%87%E7%9A%84-linux-%E5%91%BD%E4%BB%A4-ji-da-ti-gao-gong-zuo-xiao-lv-de-linux-ming-ling/>2024-04-03 极大提高工作效率的 Linux 命令</a></li><li><a href=/docs/%E6%96%87%E5%AD%A6%E7%9A%84%E6%95%85%E4%B9%A1-wen-xue-de-gu-xiang/>2024-04-03 文学的故乡</a></li><li><a href=/docs/%E6%90%9E%E6%87%82k8s%E9%89%B4%E6%9D%83-gao-dong-k8s-jian-quan/>2024-04-03 搞懂K8S鉴权</a></li><li><a href=/docs/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E5%8E%9F%E7%90%86-rong-qi-wang-luo-yuan-li/>2024-04-03 容器网络原理</a></li><li><a href=/docs/%E5%AE%B9%E5%99%A8%E7%9A%84%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E4%B8%80-overlayfs-%E5%8E%9F%E7%90%86-rong-qi-de-wen-jian-xi-tong--yi-overlayfs-yuan-li/>2024-04-03 容器的文件系统 OverlayFS 原理</a></li><li><a href=/docs/%E5%AE%B9%E5%99%A8%E5%8E%9F%E7%90%86-rong-qi-yuan-li/>2024-04-03 容器原理</a></li><li><a href=/docs/%E5%AE%B9%E5%99%A8%E5%86%85%E7%9A%84-1-%E5%8F%B7%E8%BF%9B%E7%A8%8B-rong-qi-nei-de-1-hao-jin-cheng/>2024-04-03 容器内的 1 号进程</a></li><li><a href=/docs/%E5%AE%B9%E5%99%A8%E4%B8%AD%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90%E4%BB%A5%E5%8F%8A%E4%B8%8D%E5%90%8Cdnspolicy%E5%AF%B9%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90%E7%9A%84%E5%BD%B1%E5%93%8D-rong-qi-zhong-yu-ming-jie-xi-yi-ji-bu-tong-dnspolicy-dui-yu-ming-jie-xi-de-ying-xiang/>2024-04-03 容器中域名解析以及不同dnspolicy对域名解析的影响</a></li><li><a href=/docs/%E5%A6%82%E4%BD%95%E8%B0%83%E8%AF%95-crash-%E5%AE%B9%E5%99%A8%E7%9A%84%E7%BD%91%E7%BB%9C-ru-he-diao-shi-crash-rong-qi-de-wang-luo/>2024-04-03 如何调试 crash 容器的网络</a></li><li><a href=/docs/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8tekton%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BAcicd%E5%B9%B3%E5%8F%B0-ru-he-shi-yong-tekton-kuai-su-da-jian-cicd-ping-tai/>2024-04-03 如何使用tekton快速搭建CI/CD平台</a></li><li><a href=/docs/%E5%A4%A7%E8%A7%84%E6%A8%A1%E5%B9%B6%E5%8F%91%E4%B8%8B%E5%A6%82%E4%BD%95%E5%8A%A0%E5%BF%AB-pod-%E5%90%AF%E5%8A%A8%E9%80%9F%E5%BA%A6-da-gui-mo-bing-fa-xia-ru-he-jia-kuai-pod-qi-dong-su-du/>2024-04-03 大规模并发下如何加快 Pod 启动速度</a></li><li><a href=/docs/%E4%BD%BF%E7%94%A8kubernees-leases-%E8%BD%BB%E6%9D%BE%E5%AE%9E%E7%8E%B0leader-election-shi-yong-kuberneesleases-qing-song-shi-xian-leaderelection/>2024-04-03 使用kubernees leases 轻松实现leader election</a></li><li><a href=/docs/%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2k8s%E5%8A%A0%E8%8A%82%E7%82%B9%E6%93%8D%E4%BD%9C-er-jin-zhi-bu-shu-k8s-jia-jie-dian-cao-zuo/>2024-04-03 二进制部署K8S加节点操作</a></li><li><a href=/docs/%E4%B8%A4%E5%BC%A0%E5%9B%BE%E5%85%A8%E9%9D%A2%E7%90%86%E8%A7%A3k8s%E5%8E%9F%E7%90%86-liang-zhang-tu-quan-mian-li-jie-k8s-yuan-li/>2024-04-03 两张图全面理解K8S原理</a></li><li><a href=/docs/ssl%E8%AF%81%E4%B9%A6%E8%87%AA%E7%AD%BE%E5%8F%91-ssl-zheng-shu-zi-qian-fa/>2024-04-03 ssl证书自签发</a></li><li><a href=/docs/prometheus%E4%BC%81%E4%B8%9A%E7%BA%A7%E7%9B%91%E6%8E%A7%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93-prometheus-qi-ye-ji-jian-kong-shi-yong-zong-jie/>2024-04-03 prometheus企业级监控使用总结</a></li><li><a href=/docs/metallb-l2-%E5%8E%9F%E7%90%86-metallbl2-yuan-li/>2024-04-03 MetalLB L2 原理</a></li><li><a href=/docs/linux-%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E5%A4%A7%E5%85%A8-linux-xing-neng-you-hua-da-quan/>2024-04-03 Linux 性能优化大全</a></li><li><a href=/docs/kubernetes-%E8%AF%81%E4%B9%A6%E8%AF%A6%E8%A7%A3%E9%89%B4%E6%9D%83-kubernetes-zheng-shu-xiang-jie--jian-quan-/>2024-04-03 Kubernetes 证书详解(鉴权)</a></li><li><a href=/docs/kubernetes-%E8%AF%81%E4%B9%A6%E8%AF%A6%E8%A7%A3%E8%AE%A4%E8%AF%81-kubernetes-zheng-shu-xiang-jie--ren-zheng-/>2024-04-03 Kubernetes 证书详解(认证)</a></li><li><a href=/docs/kubernetes-%E6%BA%90%E7%A0%81%E7%BB%93%E6%9E%84-kubernetes-yuan-ma-jie-gou/>2024-04-03 Kubernetes 源码结构</a></li><li><a href=/docs/kubernetes-api-kubernetesapi/>2024-04-03 Kubernetes API</a></li><li><a href=/docs/kubekey%E6%B7%BB%E5%8A%A0%E6%96%B0%E8%8A%82%E7%82%B9-kubekey-tian-jia-xin-jie-dian/>2024-04-03 kubekey添加新节点</a></li><li><a href=/docs/k8s%E9%9D%A2%E8%AF%95%E5%AE%9D%E5%85%B8-k8s-mian-shi-bao-dian/>2024-04-03 K8S面试宝典</a></li><li><a href=/docs/k8s%E9%9D%A2%E8%AF%95%E5%A4%A7%E5%85%A8-k8s-mian-shi-da-quan/>2024-04-03 K8S面试大全</a></li><li><a href=/docs/k8s%E8%BF%90%E7%BB%B4%E4%B9%8B%E6%B8%85%E7%90%86%E7%A3%81%E7%9B%98-k8s-yun-wei-zhi-qing-li-ci-pan/>2024-04-03 k8s运维之清理磁盘</a></li><li><a href=/docs/k8s%E8%B0%83%E8%AF%95pod-k8s-diao-shi-pod/>2024-04-03 K8S调试POD</a></li><li><a href=/docs/k8s%E7%9A%84pod%E7%B1%BB%E5%9E%8B-k8s-de-pod-lei-xing/>2024-04-03 K8S的POD类型</a></li><li><a href=/docs/k8s%E5%BA%94%E7%94%A8%E7%9A%84%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5-k8s-ying-yong-de-zui-jia-shi-jian/>2024-04-03 k8s应用的最佳实践</a></li><li><a href=/docs/k8s%E5%91%BD%E4%BB%A4%E6%8C%87%E5%8D%97-k8s-ming-ling-zhi-nan/>2024-04-03 K8S命令指南</a></li><li><a href=/docs/k8s%E5%8E%9F%E5%9C%B0%E5%8D%87%E7%BA%A7-k8s-yuan-de-sheng-ji/>2024-04-03 K8S原地升级</a></li><li><a href=/docs/k8s-%E6%8E%A2%E9%92%88%E5%8E%9F%E7%90%86-k8s-tan-zhen-yuan-li/>2024-04-03 K8S 探针原理</a></li><li><a href=/docs/k8s-%E5%BC%80%E5%8F%91%E5%8F%AF%E4%B8%8D%E6%AD%A2-crud-k8s-kai-fa-ke-bu-zhi-crud/>2024-04-03 K8S 开发可不止 CRUD</a></li><li><a href=/docs/k8s-gpt-k8sgpt/>2024-04-03 K8S GPT</a></li><li><a href=/docs/k8s-csi-openebs%E5%8E%9F%E7%90%86-k8scsiopenebs-yuan-li/>2024-04-03 K8S csi openebs原理</a></li><li><a href=/docs/helm-chart%E5%92%8Crepo-helmchart-he-repo/>2024-04-03 helm chart和repo</a></li><li><a href=/docs/flanel%E7%BD%91%E7%BB%9C-flanel-wang-luo/>2024-04-03 flanel网络</a></li><li><a href=/docs/etcd%E7%A8%B3%E5%AE%9A%E6%80%A7%E5%8F%8A%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E5%AE%9E%E8%B7%B5-etcd-wen-ding-xing-ji-xing-neng-you-hua-shi-jian/>2024-04-03 ETCD稳定性及性能优化实践</a></li><li><a href=/docs/etcd%E5%A4%87%E4%BB%BD-etcd-bei-fen/>2024-04-03 ETCD备份</a></li><li><a href=/docs/docker%E9%87%8D%E8%A6%81%E7%9A%84%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86%E7%82%B9-docker-zhong-yao-de-wang-luo-zhi-shi-dian/>2024-04-03 Docker重要的网络知识点</a></li><li><a href=/docs/dockerfile%E7%9A%84copy%E5%92%8Cadd%E7%9A%84%E5%8C%BA%E5%88%AB-dockerfile-de-copy-he-add-de-qu-bie/>2024-04-03 dockerfile的copy和add的区别</a></li><li><a href=/docs/coredns%E4%B9%8B%E5%85%89-coredns-zhi-guang/>2024-04-03 COREDNS之光</a></li><li><a href=/docs/containerd-%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C-containerd-ji-ben-cao-zuo/>2024-04-03 Containerd 基本操作</a></li><li><a href=/docs/cni%E6%8F%92%E4%BB%B6%E9%80%89%E5%9E%8B-cni-cha-jian-xuan-xing/>2024-04-03 CNI插件选型</a></li><li><a href=/docs/client-go-%E6%9E%B6%E6%9E%84-client-go-jia-gou/>2024-04-03 Client-go 架构</a></li><li><a href=/docs/client-go-%E5%9B%9B%E7%A7%8D%E5%AE%A2%E6%88%B7%E7%AB%AF-client-go-si-zhong-ke-hu-duan/>2024-04-03 Client-go 四种客户端</a></li><li><a href=/docs/cicd%E6%80%9D%E8%80%83-cicd-si-kao/>2024-04-03 CICD思考</a></li><li><a href=/docs/calico%E7%BD%91%E7%BB%9C%E8%87%AA%E5%AE%9A%E4%B9%89-calico-wang-luo-zi-ding-yi/>2024-04-03 Calico网络自定义</a></li><li><a href=/docs/acme%E8%87%AA%E5%8A%A8%E6%9B%B4%E6%96%B0%E8%AF%81%E4%B9%A6-acme-zi-dong-geng-xin-zheng-shu/>2024-04-03 acme自动更新证书</a></li><li><a href=/docs/16%E4%B8%AA%E6%A6%82%E5%BF%B5%E5%B8%A6%E4%BD%A0%E5%85%A5%E9%97%A8-kubernetes-16-ge-gai-nian-dai-ni-ru-men-kubernetes/>2024-04-03 16个概念带你入门 Kubernetes</a></li><li><a href=/docs/%E9%9D%A2%E8%AF%950308-mian-shi-0308/>2024-04-03 面试0308</a></li><li><a href=/docs/600%E6%9D%A1%E6%9C%80%E5%BC%BAlinux%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93-600-tiao-zui-qiang-linux-ming-ling-zong-jie/>2024-04-03 600条最强linux命令总结</a></li><li><a href=/docs/16%E5%BC%A0%E7%A1%AC%E6%A0%B8%E5%9B%BE%E8%A7%A3k8s%E7%BD%91%E7%BB%9C-16-zhang-ying-he-tu-jie-k8s-wang-luo/>2024-04-03 16张硬核图解k8s网络</a></li><li><a href=/docs/k8s%E4%B9%8Bkubelet%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-zhi-kubelet-yuan-ma-jie-du/>2024-03-28 k8s之kubelet源码解读</a></li><li><a href=/docs/2024-3-19-%E4%B8%A4%E5%BC%A0%E5%9B%BE%E5%85%A8%E9%9D%A2%E7%90%86%E8%A7%A3k8s%E5%8E%9F%E7%90%86/>2024-03-19 两张图全面理解k8s原理</a></li><li><a href=/docs/2024-3-8-%E9%9D%A2%E8%AF%950308/>2024-03-08 面试</a></li><li><a href=/docs/2024-3-4-k8s%E6%B5%81%E9%87%8F%E9%93%BE%E8%B7%AF%E5%89%96%E6%9E%90/>2024-03-04 k8s流量链路剖析</a></li><li><a href=/docs/k8s-%E6%B5%81%E9%87%8F%E9%93%BE%E8%B7%AF%E5%89%96%E6%9E%90-k8s-liu-liang-lian-lu-pou-xi/>2024-03-04 K8S 流量链路剖析</a></li><li><a href=/docs/k8s-csi%E5%89%96%E6%9E%90%E6%BC%94%E8%BF%9B-k8scsi-pou-xi-yan-jin/>2024-03-04 K8S CSI剖析演进</a></li><li><a href=/docs/k8s-cni%E5%89%96%E6%9E%90%E6%BC%94%E8%BF%9B-k8scni-pou-xi-yan-jin/>2024-03-04 K8S CNI剖析演进</a></li><li><a href=/docs/2024-3-4-k8s-csi%E5%89%96%E6%9E%90/>2024-03-04 CSI剖析演进</a></li><li><a href=/docs/2024-3-4-cni%E5%89%96%E6%9E%90%E6%BC%94%E8%BF%9B/>2024-03-04 CNI剖析演进</a></li><li><a href=/docs/2024-2-26-%E9%9D%A2%E8%AF%95/>2024-02-26 面试</a></li><li><a href=/docs/2024-2-22-k8s%E9%9D%A2%E8%AF%95%E5%AE%9D%E5%85%B8/>2024-02-22 k8s面试宝典</a></li><li><a href=/docs/2024-2-22-k8s%E6%9E%B6%E6%9E%84%E5%B8%88%E9%9D%A2%E8%AF%95%E5%A4%A7%E5%85%A8/>2024-02-22 k8s架构师面试大全</a></li><li><a href=/docs/%E4%BD%BF%E7%94%A8-openfunction-%E5%9C%A8%E4%BB%BB%E4%BD%95%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD%E4%B8%8A%E8%BF%90%E8%A1%8C%E6%97%A0%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%B7%A5%E4%BD%9C%E8%B4%9F%E8%BD%BD-shi-yong-openfunction-zai-ren-he-ji-chu-she-shi-shang-yun-xing-wu-fu-wu-qi-gong-zuo-fu-zai/>2024-01-21 使用 OpenFunction 在任何基础设施上运行无服务器工作负载</a></li><li><a href=/docs/%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85%E9%9B%86%E7%BE%A4-li-xian-an-zhuang-ji-qun/>2023-09-28 离线安装集群</a></li><li><a href=/docs/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%AF%B4%E6%98%8E-cao-zuo-xi-tong-shuo-ming/>2023-09-28 操作系统说明</a></li><li><a href=/docs/%E5%BF%AB%E9%80%9F%E6%8C%87%E5%8D%97-kuai-su-zhi-nan/>2023-09-28 快速指南</a></li><li><a href=/docs/%E5%BC%80%E5%A7%8B%E4%BD%BF%E7%94%A8-cilium-kai-shi-shi-yong-cilium/>2023-09-28 开始使用 cilium</a></li><li><a href=/docs/%E5%A4%9A%E6%9E%B6%E6%9E%84%E6%94%AF%E6%8C%81-duo-jia-gou-zhi-chi/>2023-09-28 多架构支持</a></li><li><a href=/docs/%E5%85%AC%E6%9C%89%E4%BA%91%E4%B8%8A%E9%83%A8%E7%BD%B2-kubeasz-gong-you-yun-shang-bu-shu-kubeasz/>2023-09-28 公有云上部署</a></li><li><a href=/docs/%E4%B8%AA%E6%80%A7%E5%8C%96%E9%9B%86%E7%BE%A4%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE-ge-xing-hua-ji-qun-can-shu-pei-zhi/>2023-09-28 个性化集群参数配置</a></li><li><a href=/docs/network-check-network-check/>2023-09-28 network-check</a></li><li><a href=/docs/kube-router-%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6-kube-router-wang-luo-zu-jian/>2023-09-28 kube-router 网络组件</a></li><li><a href=/docs/ezctl-%E5%91%BD%E4%BB%A4%E8%A1%8C%E4%BB%8B%E7%BB%8D-ezctl-ming-ling-xing-jie-shao/>2023-09-28 ezctl 命令行介绍</a></li><li><a href=/docs/ex-lb-%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E9%83%A8%E7%BD%B2-ex-lb-fu-zai-jun-heng-bu-shu/>2023-09-28 EX-LB 负载均衡部署</a></li><li><a href=/docs/calico-%E9%85%8D%E7%BD%AE-bgp-route-reflectors-calico-pei-zhi-bgproutereflectors/>2023-09-28 calico 配置 BGP Route Reflectors</a></li><li><a href=/docs/07-%E5%AE%89%E8%A3%85%E9%9B%86%E7%BE%A4%E4%B8%BB%E8%A6%81%E6%8F%92%E4%BB%B6-07--an-zhuang-ji-qun-zhu-yao-cha-jian/>2023-09-28 15:26:42.651 07-安装集群主要插件</a></li><li><a href=/docs/08-k8s-%E9%9B%86%E7%BE%A4%E5%AD%98%E5%82%A8--k8s-ji-qun-cun-chu/>2023-09-28 08-K8S 集群存储</a></li><li><a href=/docs/06-%E5%AE%89%E8%A3%85%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6-06--an-zhuang-wang-luo-zu-jian/>2023-09-28 06-安装网络组件</a></li><li><a href=/docs/06-%E5%AE%89%E8%A3%85kube-ovn%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6-06--an-zhuang-kube-ovn-wang-luo-zu-jian/>2023-09-28 06-安装kube-ovn网络组件</a></li><li><a href=/docs/06-%E5%AE%89%E8%A3%85flannel%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6-06--an-zhuang-flannel-wang-luo-zu-jian/>2023-09-28 06-安装flannel网络组件</a></li><li><a href=/docs/06-%E5%AE%89%E8%A3%85cilium%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6-06--an-zhuang-cilium-wang-luo-zu-jian/>2023-09-28 06-安装cilium网络组件</a></li><li><a href=/docs/06-%E5%AE%89%E8%A3%85calico%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6-06--an-zhuang-calico-wang-luo-zu-jian/>2023-09-28 06-安装calico网络组件</a></li><li><a href=/docs/02-%E5%AE%89%E8%A3%85etcd%E9%9B%86%E7%BE%A4-02--an-zhuang-etcd-ji-qun/>2023-09-28 02-安装etcd集群</a></li><li><a href=/docs/00-%E9%9B%86%E7%BE%A4%E8%A7%84%E5%88%92%E5%92%8C%E5%9F%BA%E7%A1%80%E5%8F%82%E6%95%B0%E8%AE%BE%E5%AE%9A-00--ji-qun-gui-hua-he-ji-chu-can-shu-she-ding/>2023-09-28 00-集群规划和基础参数设定</a></li><li><a href=/docs/05-%E5%AE%89%E8%A3%85kube_node%E8%8A%82%E7%82%B9-05--an-zhuang-kubenode-jie-dian/>2023-09-28 05-安装kube_node节点</a></li><li><a href=/docs/04-%E5%AE%89%E8%A3%85kube_master%E8%8A%82%E7%82%B9-04--an-zhuang-kubemaster-jie-dian/>2023-09-28 04-安装kube_master节点</a></li><li><a href=/docs/03-%E5%AE%89%E8%A3%85%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6-03--an-zhuang-rong-qi-yun-xing-shi/>2023-09-28 03-安装容器运行时</a></li><li><a href=/docs/01-%E5%88%9B%E5%BB%BA%E8%AF%81%E4%B9%A6%E5%92%8C%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87-01--chuang-jian-zheng-shu-he-huan-jing-zhun-bei/>2023-09-28 01-创建证书和环境准备</a></li><li><a href=/docs/%E6%9C%89%E8%BF%993%E4%B8%AA%E8%BF%B9%E8%B1%A1%E4%BD%A0%E5%B0%B1%E8%AF%A5%E7%A6%BB%E8%81%8C%E4%BA%86-you-zhe-3-ge-ji-xiang--ni-jiu-gai-li-zhi-le/>2023-09-21 思考</a></li><li><a href=/docs/%E4%BD%BF%E7%94%A8-keepalived-%E5%92%8C-haproxy-%E5%88%9B%E5%BB%BA%E9%AB%98%E5%8F%AF%E7%94%A8-kubernetes-%E9%9B%86%E7%BE%A4-shi-yong-keepalived-he-haproxy-chuang-jian-gao-ke-yong-kubernetes-ji-qun/>2023-04-12 使用 Keepalived 和 HAproxy 创建高可用 Kubernetes 集群</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h3>2025-2-16 k8s题目</h3><label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#kubernetes>Kubernetes</a></li><li><a href=#谈谈你对k8s的理解>谈谈你对k8s的理解☆</a><ul><li><a href=#关键优势>关键优势：</a></li><li><a href=#适用场景>适用场景：</a></li></ul></li><li><a href=#k8s集群架构是什么>k8s集群架构是什么☆</a><ul><li><a href=#1-控制平面control-plane>1. <strong>控制平面（Control Plane）</strong></a></li><li><a href=#2-工作节点worker-node>2. <strong>工作节点（Worker Node）</strong></a></li><li><a href=#3-k8s-集群架构示意图>3. <strong>K8s 集群架构示意图</strong></a></li><li><a href=#4-其他重要组件>4. <strong>其他重要组件</strong></a></li><li><a href=#5-高可用性与冗余>5. <strong>高可用性与冗余</strong></a></li><li><a href=#总结>总结</a></li></ul></li><li><a href=#简述pod创建过程>简述Pod创建过程☆</a><ul><li><a href=#1-创建-pod-配置文件>1. <strong>创建 Pod 配置文件</strong></a></li><li><a href=#2-提交-pod-配置到-kubernetes-api-server>2. <strong>提交 Pod 配置到 Kubernetes API Server</strong></a></li><li><a href=#3-api-server-存储-pod-配置>3. <strong>API Server 存储 Pod 配置</strong></a></li><li><a href=#4-调度scheduler选择合适的工作节点>4. <strong>调度（Scheduler）选择合适的工作节点</strong></a></li><li><a href=#5-kubelet-在工作节点上创建-pod>5. <strong>Kubelet 在工作节点上创建 Pod</strong></a></li><li><a href=#6-pod-启动并运行>6. <strong>Pod 启动并运行</strong></a></li><li><a href=#7-集群状态更新>7. <strong>集群状态更新</strong></a></li><li><a href=#8-监控与自动恢复>8. <strong>监控与自动恢复</strong></a></li><li><a href=#总结-1>总结</a></li></ul></li><li><a href=#简述删除一个pod流程>简述删除一个Pod流程</a><ul><li><a href=#1-用户发起删除请求>1. <strong>用户发起删除请求</strong></a></li><li><a href=#2-api-server-接受并验证请求>2. <strong>API Server 接受并验证请求</strong></a></li><li><a href=#3-pod-状态更新>3. <strong>Pod 状态更新</strong></a></li><li><a href=#4-删除-pod-相关的控制器对象>4. <strong>删除 Pod 相关的控制器对象</strong></a></li><li><a href=#5-kubelet-监测到-pod-删除请求>5. <strong>Kubelet 监测到 Pod 删除请求</strong></a></li><li><a href=#6-pod-资源回收>6. <strong>Pod 资源回收</strong></a></li><li><a href=#7-api-server-完成删除操作>7. <strong>API Server 完成删除操作</strong></a></li><li><a href=#8-删除完成>8. <strong>删除完成</strong></a></li><li><a href=#总结-2>总结</a></li></ul></li><li><a href=#不同node上的pod之间的通信过程>不同node上的Pod之间的通信过程☆</a><ul><li><a href=#1-pod-网络模型>1. <strong>Pod 网络模型</strong></a></li><li><a href=#2-cni-插件容器网络接口>2. <strong>CNI 插件（容器网络接口）</strong></a></li><li><a href=#3-通信过程>3. <strong>通信过程</strong></a></li><li><a href=#4-网络策略network-policy>4. <strong>网络策略（Network Policy）</strong></a></li><li><a href=#5-service-和负载均衡>5. <strong>Service 和负载均衡</strong></a></li><li><a href=#总结-3>总结</a></li></ul></li><li><a href=#pod创建pending状态的原因>pod创建Pending状态的原因☆</a><ul><li><a href=#1-资源不足>1. <strong>资源不足</strong></a></li><li><a href=#2-调度器找不到合适的节点>2. <strong>调度器找不到合适的节点</strong></a></li><li><a href=#3-taints-和-tolerations>3. <strong>Taints 和 Tolerations</strong></a></li><li><a href=#4-调度器无法找到适合的节点>4. <strong>调度器无法找到适合的节点</strong></a></li><li><a href=#5-集群容量不足>5. <strong>集群容量不足</strong></a></li><li><a href=#6-persistentvolume-pv-或-persistentvolumeclaim-pvc-问题>6. <strong>PersistentVolume (PV) 或 PersistentVolumeClaim (PVC) 问题</strong></a></li><li><a href=#7-网络问题>7. <strong>网络问题</strong></a></li><li><a href=#8-镜像拉取失败>8. <strong>镜像拉取失败</strong></a></li><li><a href=#9-kubernetes-控制平面问题>9. <strong>Kubernetes 控制平面问题</strong></a></li><li><a href=#如何排查-pending-状态的-pod>如何排查 Pending 状态的 Pod</a></li><li><a href=#总结-4>总结</a></li></ul></li><li><a href=#deployment和statefulset区别>deployment和statefulset区别☆</a><ul><li><a href=#1-pod-的标识和命名>1. <strong>Pod 的标识和命名</strong></a></li><li><a href=#2-pod-生命周期>2. <strong>Pod 生命周期</strong></a></li><li><a href=#3-存储>3. <strong>存储</strong></a></li><li><a href=#4-扩缩容>4. <strong>扩缩容</strong></a></li><li><a href=#5-网络>5. <strong>网络</strong></a></li><li><a href=#6-适用场景>6. <strong>适用场景</strong></a></li><li><a href=#7-滚动更新>7. <strong>滚动更新</strong></a></li><li><a href=#总结对比>总结对比</a></li><li><a href=#适用场景示例>适用场景示例</a></li></ul></li><li><a href=#kube-proxy有什么作用>kube-proxy有什么作用☆</a><ul><li><a href=#kube-proxy-的主要作用><code>kube-proxy</code> 的主要作用</a></li><li><a href=#kube-proxy-的工作原理><code>kube-proxy</code> 的工作原理</a></li><li><a href=#kube-proxy-的工作流程><code>kube-proxy</code> 的工作流程</a></li><li><a href=#kube-proxy-的使用场景><code>kube-proxy</code> 的使用场景</a></li><li><a href=#总结-5>总结</a></li></ul></li><li><a href=#kube-proxy怎么修改ipvs规则>kube-proxy怎么修改ipvs规则</a><ul><li><a href=#修改-ipvs-规则>修改 IPVS 规则</a></li><li><a href=#总结-6>总结</a></li></ul></li><li><a href=#ipvs为什么比iptables效率高>ipvs为什么比iptables效率高</a><ul><li><a href=#1-专门为负载均衡设计>1. <strong>专门为负载均衡设计</strong></a></li><li><a href=#2-性能优化和内核支持>2. <strong>性能优化和内核支持</strong></a></li><li><a href=#3-规则查找方式>3. <strong>规则查找方式</strong></a></li><li><a href=#4-连接状态管理>4. <strong>连接状态管理</strong></a></li><li><a href=#5-负载均衡算法>5. <strong>负载均衡算法</strong></a></li><li><a href=#6-高并发处理能力>6. <strong>高并发处理能力</strong></a></li><li><a href=#7-内存使用和效率>7. <strong>内存使用和效率</strong></a></li><li><a href=#8-流量转发的直接性>8. <strong>流量转发的直接性</strong></a></li><li><a href=#总结-7>总结</a></li></ul></li><li><a href=#pod之间访问不通怎么排查>pod之间访问不通怎么排查☆</a><ul><li><a href=#1-检查-pod-之间的网络连接>1. <strong>检查 Pod 之间的网络连接</strong></a></li><li><a href=#2-检查网络插件是否正常工作>2. <strong>检查网络插件是否正常工作</strong></a></li><li><a href=#3-检查网络策略network-policies>3. <strong>检查网络策略（Network Policies）</strong></a></li><li><a href=#4-检查>4. <strong>检查 <code>kube-proxy</code> 状态</strong></a></li><li><a href=#5-检查节点网络和防火墙设置>5. <strong>检查节点网络和防火墙设置</strong></a></li><li><a href=#6-查看-dns-配置>6. <strong>查看 DNS 配置</strong></a></li><li><a href=#7-查看-service-配置>7. <strong>查看 Service 配置</strong></a></li><li><a href=#8-pod-重启和调试>8. <strong>Pod 重启和调试</strong></a></li><li><a href=#9-集群的网络诊断工具>9. <strong>集群的网络诊断工具</strong></a></li><li><a href=#总结-8>总结</a></li></ul></li><li><a href=#k8s中network-policy的实现原理>k8s中Network Policy的实现原理</a><ul><li><a href=#network-policy-的基本原理>Network Policy 的基本原理</a></li><li><a href=#network-policy-的工作流程>Network Policy 的工作流程</a></li><li><a href=#例子如何使用-network-policy>例子：如何使用 Network Policy</a></li><li><a href=#网络策略的关键点总结>网络策略的关键点总结</a></li><li><a href=#结论>结论</a></li></ul></li><li><a href=#探针有哪些探测方法有哪些>探针有哪些？探测方法有哪些？</a><ul><li><a href=#1-liveness-probe存活探针>1. <strong>Liveness Probe</strong>（存活探针）</a></li><li><a href=#2-readiness-probe就绪探针>2. <strong>Readiness Probe</strong>（就绪探针）</a></li><li><a href=#3-startup-probe启动探针>3. <strong>Startup Probe</strong>（启动探针）</a></li><li><a href=#探测方法probe-types>探测方法（Probe Types）</a></li><li><a href=#配置探针时的常用参数>配置探针时的常用参数</a></li><li><a href=#示例完整的探针配置>示例：完整的探针配置</a></li><li><a href=#总结-9>总结</a></li></ul></li><li><a href=#pod健康检查失败可能的原因和排查思路>pod健康检查失败可能的原因和排查思路</a><ul><li><a href=#常见的-pod-健康检查失败的原因>常见的 Pod 健康检查失败的原因</a></li><li><a href=#健康检查失败的排查思路>健康检查失败的排查思路</a></li><li><a href=#总结-10>总结</a></li></ul></li><li><a href=#k8s的service是什么>k8s的Service是什么☆</a><ul><li><a href=#service-的主要作用>Service 的主要作用</a></li><li><a href=#service-的类型>Service 的类型</a></li><li><a href=#service-的工作原理>Service 的工作原理</a></li><li><a href=#service-的健康检查>Service 的健康检查</a></li><li><a href=#访问-service-的方式>访问 Service 的方式</a></li><li><a href=#总结-11>总结</a></li></ul></li><li><a href=#metrics-server采集指标数据链路>metrics-server采集指标数据链路</a><ul><li><a href=#1-node-端的-kubelet-上报资源指标>1. <strong>Node 端的 Kubelet 上报资源指标</strong></a></li><li><a href=#2-metrics-server-收集数据>2. <strong>Metrics Server 收集数据</strong></a></li><li><a href=#3-数据聚合和处理>3. <strong>数据聚合和处理</strong></a></li><li><a href=#4-通过-kubernetes-api-提供访问>4. <strong>通过 Kubernetes API 提供访问</strong></a></li><li><a href=#5-pod-autoscaling-的触发>5. <strong>Pod Autoscaling 的触发</strong></a></li><li><a href=#6-数据过期与刷新>6. <strong>数据过期与刷新</strong></a></li><li><a href=#7-数据存储与可视化>7. <strong>数据存储与可视化</strong></a></li><li><a href=#采集数据链路总结>采集数据链路总结：</a></li></ul></li><li><a href=#k8s服务发现有哪些方式>k8s服务发现有哪些方式？</a><ul><li><a href=#1-dns-服务发现>1. <strong>DNS 服务发现</strong></a></li><li><a href=#2-clusterip内网访问>2. <strong>ClusterIP（内网访问）</strong></a></li><li><a href=#3-nodeport外部访问>3. <strong>NodePort（外部访问）</strong></a></li><li><a href=#4-loadbalancer云环境外部访问>4. <strong>LoadBalancer（云环境外部访问）</strong></a></li><li><a href=#5-externalname外部服务访问>5. <strong>ExternalName（外部服务访问）</strong></a></li><li><a href=#6-headless-service无头服务>6. <strong>Headless Service（无头服务）</strong></a></li><li><a href=#7-endpoints手动定义服务发现>7. <strong>Endpoints（手动定义服务发现）</strong></a></li><li><a href=#总结-12>总结</a></li></ul></li><li><a href=#pod几种常用状态>pod几种常用状态</a><ul><li><a href=#1-pending>1. <strong>Pending</strong></a></li><li><a href=#2-running>2. <strong>Running</strong></a></li><li><a href=#3-succeeded>3. <strong>Succeeded</strong></a></li><li><a href=#4-failed>4. <strong>Failed</strong></a></li><li><a href=#5-crashloopbackoff>5. <strong>CrashLoopBackOff</strong></a></li><li><a href=#6-unknown>6. <strong>Unknown</strong></a></li><li><a href=#7-terminating>7. <strong>Terminating</strong></a></li><li><a href=#8-waiting>8. <strong>Waiting</strong></a></li><li><a href=#总结-13>总结</a></li></ul></li><li><a href=#pod-生命周期的钩子函数>Pod 生命周期的钩子函数</a><ul><li><a href=#1-poststart>1. <strong>PostStart</strong></a></li><li><a href=#2-prestop>2. <strong>PreStop</strong></a></li><li><a href=#3-限制与注意事项>3. <strong>限制与注意事项</strong></a></li><li><a href=#4-钩子的执行方式>4. <strong>钩子的执行方式</strong></a></li><li><a href=#总结-14>总结</a></li></ul></li><li><a href=#calico和flannel区别>Calico和flannel区别☆</a><ul><li><a href=#1-架构设计>1. <strong>架构设计</strong></a></li><li><a href=#2-网络模型>2. <strong>网络模型</strong></a></li><li><a href=#3-网络策略network-policy>3. <strong>网络策略（Network Policy）</strong></a></li><li><a href=#4-性能>4. <strong>性能</strong></a></li><li><a href=#5-ip-地址管理>5. <strong>IP 地址管理</strong></a></li><li><a href=#6-扩展性>6. <strong>扩展性</strong></a></li><li><a href=#7-支持的环境>7. <strong>支持的环境</strong></a></li><li><a href=#总结-15>总结</a></li></ul></li><li><a href=#calico网络原理组网方式>calico网络原理、组网方式</a><ul><li><a href=#1-网络原理>1. <strong>网络原理</strong></a></li><li><a href=#2-calico-的组网方式>2. <strong>Calico 的组网方式</strong></a></li><li><a href=#3-calico-的主要组件>3. <strong>Calico 的主要组件</strong></a></li><li><a href=#4-calico-的优势>4. <strong>Calico 的优势</strong></a></li><li><a href=#总结-16>总结</a></li><li><a href=#ip-in-ip-模式的工作原理>IP-in-IP 模式的工作原理</a></li><li><a href=#ip-in-ip-模式的优缺点>IP-in-IP 模式的优缺点</a></li><li><a href=#使用场景>使用场景</a></li><li><a href=#配置-ip-in-ip-模式>配置 IP-in-IP 模式</a></li><li><a href=#总结-17>总结</a></li><li><a href=#vxlan-的工作原理>VXLAN 的工作原理</a></li><li><a href=#vxlan-模式的优缺点>VXLAN 模式的优缺点</a></li><li><a href=#vxlan-与-ip-in-ip-的对比>VXLAN 与 IP-in-IP 的对比</a></li><li><a href=#使用场景-1>使用场景</a></li><li><a href=#配置-vxlan-模式>配置 VXLAN 模式</a></li><li><a href=#总结-18>总结</a></li></ul></li><li><a href=#network-policy使用场景>Network Policy使用场景</a><ul><li><a href=#network-policy-的使用场景>Network Policy 的使用场景</a></li><li><a href=#示例定义-network-policy>示例：定义 Network Policy</a></li><li><a href=#网络策略的限制和注意事项>网络策略的限制和注意事项</a></li><li><a href=#总结-19>总结</a></li></ul></li><li><a href=#kubectl-exec-实现的原理>kubectl exec 实现的原理</a><ul><li><a href=#kubectl-exec-原理概述><code>kubectl exec</code> 原理概述</a></li><li><a href=#kubectl-exec-的组件与交互><code>kubectl exec</code> 的组件与交互</a></li><li><a href=#kubectl-exec-使用示例><code>kubectl exec</code> 使用示例</a></li><li><a href=#安全与权限控制>安全与权限控制</a></li><li><a href=#总结-20>总结</a></li></ul></li><li><a href=#cgroup中限制cpu的方式有哪些>cgroup中限制CPU的方式有哪些</a><ul><li><a href=#1-cpu-时间限制cpucfs_>1. <strong>CPU 时间限制（cpu.cfs_quota_us）</strong></a></li><li><a href=#2-cpu-核心限制cpucpus>2. <strong>CPU 核心限制（cpu.cpus）</strong></a></li><li><a href=#3-cpu-权重cpushares>3. <strong>CPU 权重（cpu.shares）</strong></a></li><li><a href=#4-cpu-限制优先级cpurt_>4. <strong>CPU 限制优先级（cpu.rt_runtime_us）</strong></a></li><li><a href=#5-cpu-子系统的-cfscompletely-fair-scheduler策略>5. <strong>CPU 子系统的 CFS（Completely Fair Scheduler）策略</strong></a></li><li><a href=#6-cpu-控制组调度器>6. <strong>CPU 控制组调度器（<code>cpuset</code>）</strong></a></li><li><a href=#总结-21>总结</a></li></ul></li><li><a href=#kubeconfig存放内容>kubeconfig存放内容</a><ul><li><a href=#kubeconfig-文件的结构><code>kubeconfig</code> 文件的结构</a></li><li><a href=#kubeconfig-文件示例><code>kubeconfig</code> 文件示例</a></li><li><a href=#kubeconfig-文件的关键字段><code>kubeconfig</code> 文件的关键字段</a></li><li><a href=#示例解析>示例解析</a></li><li><a href=#存放位置>存放位置</a></li><li><a href=#使用-kubectl-选择不同的-kubeconfig-文件>使用 <code>kubectl</code> 选择不同的 kubeconfig 文件</a></li><li><a href=#小结>小结</a></li></ul></li><li><a href=#pod-dns解析流程>pod DNS解析流程☆</a><ul><li><a href=#1-dns-配置>1. <strong>DNS 配置</strong></a></li><li><a href=#2-dns-查询流程>2. <strong>DNS 查询流程</strong></a></li><li><a href=#3-dns-解析示例>3. <strong>DNS 解析示例</strong></a></li><li><a href=#4-dns-解析规则>4. <strong>DNS 解析规则</strong></a></li><li><a href=#5-dns-服务的工作原理>5. <strong>DNS 服务的工作原理</strong></a></li><li><a href=#6-dns-解析优化与故障排查>6. <strong>DNS 解析优化与故障排查</strong></a></li><li><a href=#7-dns-解析的常见问题>7. <strong>DNS 解析的常见问题</strong></a></li><li><a href=#总结-22>总结</a></li></ul></li><li><a href=#traefik对比nginx-ingress优点>traefik对比nginx ingress优点</a><ul><li><a href=#1-traefik-的优点><strong>1. Traefik 的优点</strong></a></li><li><a href=#2-nginx-ingress-controller-的优点><strong>2. Nginx Ingress Controller 的优点</strong></a></li><li><a href=#总结-23><strong>总结：</strong></a></li></ul></li><li><a href=#harbor有哪些组件>Harbor有哪些组件</a><ul><li><a href=#1-core-核心服务>1. <strong>Core (核心服务)</strong></a></li><li><a href=#2-registry-镜像仓库>2. <strong>Registry (镜像仓库)</strong></a></li><li><a href=#3-portal-web-ui>3. <strong>Portal (Web UI)</strong></a></li><li><a href=#4-jobservice-任务服务>4. <strong>Jobservice (任务服务)</strong></a></li><li><a href=#5-notary-镜像签名>5. <strong>Notary (镜像签名)</strong></a></li><li><a href=#6-clair-镜像安全扫描>6. <strong>Clair (镜像安全扫描)</strong></a></li><li><a href=#7-chartmuseum-helm-仓库>7. <strong>Chartmuseum (Helm 仓库)</strong></a></li><li><a href=#8-log-日志系统>8. <strong>Log (日志系统)</strong></a></li><li><a href=#9-database-数据库>9. <strong>Database (数据库)</strong></a></li><li><a href=#10-redis-缓存服务>10. <strong>Redis (缓存服务)</strong></a></li><li><a href=#11-nginx-反向代理>11. <strong>NGINX (反向代理)</strong></a></li><li><a href=#总结-24><strong>总结：</strong></a></li></ul></li><li><a href=#harbor高可用怎么实现>Harbor高可用怎么实现</a><ul><li><a href=#1-harbor-组件的冗余设计>1. <strong>Harbor 组件的冗余设计</strong></a></li><li><a href=#2-高可用的部署架构>2. <strong>高可用的部署架构</strong></a></li><li><a href=#3-harbor-高可用的部署方案常见架构>3. <strong>Harbor 高可用的部署方案（常见架构）</strong></a></li><li><a href=#4-故障恢复与备份>4. <strong>故障恢复与备份</strong></a></li><li><a href=#总结-25>总结：</a></li></ul></li><li><a href=#etcd调优>ETCD调优</a><ul><li><a href=#1-硬件配置优化>1. <strong>硬件配置优化</strong></a></li><li><a href=#2-网络配置优化>2. <strong>网络配置优化</strong></a></li><li><a href=#3-etcd-配置参数调整>3. <strong>etcd 配置参数调整</strong></a></li><li><a href=#4-集群规模与拓扑设计>4. <strong>集群规模与拓扑设计</strong></a></li><li><a href=#5-监控与维护>5. <strong>监控与维护</strong></a></li><li><a href=#6-故障排查>6. <strong>故障排查</strong></a></li><li><a href=#总结-26>总结：</a></li></ul></li><li><a href=#假设k8s集群规模上千需要注意的问题有哪些>假设k8s集群规模上千，需要注意的问题有哪些？</a><ul><li><a href=#1-集群架构与资源规划>1. <strong>集群架构与资源规划</strong></a></li><li><a href=#2-资源管理与调度>2. <strong>资源管理与调度</strong></a></li><li><a href=#3-性能与可伸缩性>3. <strong>性能与可伸缩性</strong></a></li><li><a href=#4-高可用与容灾>4. <strong>高可用与容灾</strong></a></li><li><a href=#5-安全与合规>5. <strong>安全与合规</strong></a></li><li><a href=#6-故障排查与诊断>6. <strong>故障排查与诊断</strong></a></li><li><a href=#总结-27>总结</a></li><li><a href=#关键点总结>关键点总结：</a></li><li><a href=#总结-28>总结：</a></li></ul></li><li><a href=#节点notready可能的原因会导致哪些问题>节点NotReady可能的原因？会导致哪些问题？☆</a><ul><li><a href=#可能导致节点-notready-状态的原因>可能导致节点 <code>NotReady</code> 状态的原因</a></li><li><a href=#节点-notready-状态可能导致的问题>节点 <code>NotReady</code> 状态可能导致的问题</a></li><li><a href=#排查节点-notready-状态的思路>排查节点 <code>NotReady</code> 状态的思路</a></li><li><a href=#总结-29>总结</a></li></ul></li><li><a href=#service和endpoints是如何关联的>service和endpoints是如何关联的？</a><ul><li><a href=#1-service-的作用>1. <strong>Service 的作用</strong></a></li><li><a href=#2-endpoints-的作用>2. <strong>Endpoints 的作用</strong></a></li><li><a href=#3-service-和-endpoints-的关联方式>3. <strong>Service 和 Endpoints 的关联方式</strong></a></li><li><a href=#4-流程说明>4. <strong>流程说明</strong></a></li><li><a href=#5-endpoints-的更新>5. <strong>Endpoints 的更新</strong></a></li><li><a href=#6-手动管理-endpoints>6. <strong>手动管理 Endpoints</strong></a></li><li><a href=#7-集群外的访问>7. <strong>集群外的访问</strong></a></li><li><a href=#8-总结>8. <strong>总结：</strong></a></li></ul></li><li><a href=#replicasetdeployment功能是怎么实现的>ReplicaSet、Deployment功能是怎么实现的？</a><ul><li><a href=#1-replicaset-的功能和实现原理>1. <strong>ReplicaSet</strong> 的功能和实现原理</a></li><li><a href=#2-deployment-的功能和实现原理>2. <strong>Deployment</strong> 的功能和实现原理</a></li><li><a href=#3-replicaset-与-deployment-的区别>3. <strong>ReplicaSet 与 Deployment 的区别</strong></a></li><li><a href=#4-deployment-如何实现滚动更新>4. <strong>Deployment 如何实现滚动更新</strong></a></li><li><a href=#5-总结>5. <strong>总结</strong></a></li></ul></li><li><a href=#scheduler调度流程>scheduler调度流程</a><ul><li><a href=#1-调度器的工作流程概述>1. <strong>调度器的工作流程概述</strong></a></li><li><a href=#2-scheduler-调度流程的详细步骤>2. <strong>Scheduler 调度流程的详细步骤</strong></a></li><li><a href=#3-scheduler-插件体系>3. <strong>Scheduler 插件体系</strong></a></li><li><a href=#4-调度策略>4. <strong>调度策略</strong></a></li><li><a href=#5-总结-1>5. <strong>总结</strong></a></li></ul></li><li><a href=#hpa怎么实现的>HPA怎么实现的☆</a><ul><li><a href=#1-hpa-主要功能>1. <strong>HPA 主要功能</strong></a></li><li><a href=#2-hpa-工作原理>2. <strong>HPA 工作原理</strong></a></li><li><a href=#3-hpa-扩展和缩减的决策>3. <strong>HPA 扩展和缩减的决策</strong></a></li><li><a href=#4-hpa-相关的限制和注意事项>4. <strong>HPA 相关的限制和注意事项</strong></a></li><li><a href=#5-总结-2>5. <strong>总结</strong></a></li></ul></li><li><a href=#request-limit底层是怎么限制的>request limit底层是怎么限制的☆</a><ul><li><a href=#1-请求资源request与限制资源limit的定义>1. <strong>请求资源（request）与限制资源（limit）的定义</strong></a></li><li><a href=#2-底层原理cgroup-和容器运行时>2. <strong>底层原理：cgroup 和容器运行时</strong></a></li><li><a href=#3-调度器如何使用-request-和-limit>3. <strong>调度器如何使用 request 和 limit</strong></a></li><li><a href=#4-总结>4. <strong>总结</strong></a></li></ul></li><li><a href=#helm工作原理是什么>helm工作原理是什么？</a><ul><li><a href=#helm-的核心组件>Helm 的核心组件</a></li><li><a href=#helm-工作原理>Helm 工作原理</a></li><li><a href=#helm-的模板渲染>Helm 的模板渲染</a></li><li><a href=#helm-的优势>Helm 的优势</a></li><li><a href=#helm-与-kubernetes-资源的关系>Helm 与 Kubernetes 资源的关系</a></li><li><a href=#总结-30>总结</a></li></ul></li><li><a href=#helm-chart-rollback实现过程是什么>helm chart rollback实现过程是什么？</a><ul><li><a href=#helm-chart-rollback-实现过程>Helm Chart Rollback 实现过程</a></li><li><a href=#1-helm-release-的版本管理>1. <strong>Helm Release 的版本管理</strong></a></li><li><a href=#2-执行-rollback-操作>2. <strong>执行 Rollback 操作</strong></a></li><li><a href=#3-helm-rollback-中的关键操作>3. <strong>Helm Rollback 中的关键操作</strong></a></li><li><a href=#4-helm-rollback>4. <strong><code>helm rollback</code> 示例</strong></a></li><li><a href=#5-rollback-时的注意事项>5. <strong>RollBack 时的注意事项</strong></a></li><li><a href=#6-helm-回滚工作原理总结>6. <strong>Helm 回滚工作原理总结</strong></a></li></ul></li><li><a href=#velero备份与恢复流程是什么>velero备份与恢复流程是什么</a><ul><li><a href=#1-velero-架构概述>1. <strong>Velero 架构概述</strong></a></li><li><a href=#2-velero-备份流程>2. <strong>Velero 备份流程</strong></a></li><li><a href=#3-velero-恢复流程>3. <strong>Velero 恢复流程</strong></a></li><li><a href=#4-velero-的增量备份和恢复>4. <strong>Velero 的增量备份和恢复</strong></a></li><li><a href=#5-velero-的高级特性>5. <strong>Velero 的高级特性</strong></a></li><li><a href=#6-常见命令汇总>6. <strong>常见命令汇总</strong></a></li><li><a href=#7-总结>7. <strong>总结</strong></a></li></ul></li><li><a href=#docker网络模式>docker网络模式</a><ul><li><a href=#1-bridge-网络模式>1. <strong>Bridge 网络模式</strong></a></li><li><a href=#2-host-网络模式>2. <strong>Host 网络模式</strong></a></li><li><a href=#3-none-网络模式>3. <strong>None 网络模式</strong></a></li><li><a href=#4-container-网络模式>4. <strong>Container 网络模式</strong></a></li><li><a href=#5-overlay-网络模式>5. <strong>Overlay 网络模式</strong></a></li><li><a href=#6-macvlan-网络模式>6. <strong>Macvlan 网络模式</strong></a></li><li><a href=#7-ipvlan-网络模式>7. <strong>IPvlan 网络模式</strong></a></li><li><a href=#8-docker-网络模式总结>8. <strong>Docker 网络模式总结</strong></a></li></ul></li><li><a href=#docker和container区别>docker和container区别☆</a><ul><li><a href=#1-容器container>1. <strong>容器（Container）</strong></a></li><li><a href=#2-docker>2. <strong>Docker</strong></a></li><li><a href=#3-docker-与容器的区别>3. <strong>Docker 与容器的区别</strong></a></li><li><a href=#4-总结-1>4. <strong>总结</strong></a></li></ul></li><li><a href=#如何减dockerfile成镜像体积>如何减⼩dockerfile⽣成镜像体积？</a><ul><li><a href=#1-选择合适的基础镜像>1. <strong>选择合适的基础镜像</strong></a></li><li><a href=#2-减少不必要的文件>2. <strong>减少不必要的文件</strong></a></li><li><a href=#3-合并多个>3. <strong>合并多个 <code>RUN</code> 命令</strong></a></li><li><a href=#4-清理临时文件和缓存>4. <strong>清理临时文件和缓存</strong></a></li><li><a href=#5-使用多阶段构建multi-stage-builds>5. <strong>使用多阶段构建（Multi-stage builds）</strong></a></li><li><a href=#6-避免安装不必要的依赖>6. <strong>避免安装不必要的依赖</strong></a></li><li><a href=#7-压缩镜像文件>7. <strong>压缩镜像文件</strong></a></li><li><a href=#8-减少镜像中的层数>8. <strong>减少镜像中的层数</strong></a></li><li><a href=#9-优化文件格式和大小>9. <strong>优化文件格式和大小</strong></a></li><li><a href=#10-使用-docker-buildkit-和缓存机制>10. <strong>使用 Docker BuildKit 和缓存机制</strong></a></li></ul></li><li><a href=#k8s日志采集方案>k8s日志采集方案</a><ul><li><a href=#1-kubernetes-日志来源>1. <strong>Kubernetes 日志来源</strong></a></li><li><a href=#2-常见的-kubernetes-日志采集方案>2. <strong>常见的 Kubernetes 日志采集方案</strong></a></li><li><a href=#3-日志采集方案的选型>3. <strong>日志采集方案的选型</strong></a></li><li><a href=#4-日志采集最佳实践>4. <strong>日志采集最佳实践</strong></a></li></ul></li><li><a href=#pause容器的用途>Pause容器的用途☆</a><ul><li><a href=#pause-容器的主要用途>Pause 容器的主要用途</a></li><li><a href=#如何启动-pause-容器>如何启动 Pause 容器</a></li><li><a href=#典型的-pause-容器镜像>典型的 Pause 容器镜像</a></li><li><a href=#总结-31>总结</a></li></ul></li><li><a href=#k8s证书过期怎么更新>k8s证书过期怎么更新</a><ul><li><a href=#1-检查证书的有效期>1. <strong>检查证书的有效期</strong></a></li><li><a href=#2-使用>2. <strong>使用 <code>kubeadm</code> 更新证书</strong></a></li><li><a href=#3-手动更新证书>3. <strong>手动更新证书</strong></a></li><li><a href=#4-kubernetes-ca-证书的更新>4. <strong>Kubernetes CA 证书的更新</strong></a></li><li><a href=#5-automating-certificate-rotation>5. <strong>Automating Certificate Rotation</strong></a></li><li><a href=#6-检查证书更新后的状态>6. <strong>检查证书更新后的状态</strong></a></li><li><a href=#总结-32>总结</a></li></ul></li><li><a href=#k8s-qos等级>K8S QoS等级☆</a><ul><li><a href=#kubernetes-qos-等级>Kubernetes QoS 等级</a></li><li><a href=#1-guaranteed保证>1. <strong>Guaranteed（保证）</strong></a></li><li><a href=#2-burstable突发>2. <strong>Burstable（突发）</strong></a></li><li><a href=#3-besteffort尽力而为>3. <strong>BestEffort（尽力而为）</strong></a></li><li><a href=#qos-影响>QoS 影响</a></li><li><a href=#总结-33>总结</a></li></ul></li><li><a href=#k8s节点维护注意事项>k8s节点维护注意事项</a><ul><li><a href=#1-确保高可用性>1. <strong>确保高可用性</strong></a></li><li><a href=#2-检查节点状态>2. <strong>检查节点状态</strong></a></li><li><a href=#3-将节点从调度池中移除>3. <strong>将节点从调度池中移除</strong></a></li><li><a href=#4-迁移-pod-到其他节点>4. <strong>迁移 Pod 到其他节点</strong></a></li><li><a href=#5-执行节点维护操作>5. <strong>执行节点维护操作</strong></a></li><li><a href=#6-重新加入节点>6. <strong>重新加入节点</strong></a></li><li><a href=#7-验证节点恢复>7. <strong>验证节点恢复</strong></a></li><li><a href=#8-检查-pod-的健康状态>8. <strong>检查 Pod 的健康状态</strong></a></li><li><a href=#9-监控与日志>9. <strong>监控与日志</strong></a></li><li><a href=#10-通知相关人员>10. <strong>通知相关人员</strong></a></li><li><a href=#11-升级节点版本>11. <strong>升级节点版本</strong></a></li><li><a href=#12-更新节点的-kubelet-配置>12. <strong>更新节点的 kubelet 配置</strong></a></li><li><a href=#13-定期执行节点健康检查>13. <strong>定期执行节点健康检查</strong></a></li><li><a href=#总结-34>总结</a></li></ul></li><li><a href=#headless-service和clusterip区别>Headless Service和ClusterIP区别☆</a><ul><li><a href=#1-clusterip-service>1. <strong>ClusterIP Service</strong></a></li><li><a href=#2-headless-service>2. <strong>Headless Service</strong></a></li><li><a href=#clusterip-与-headless-service-的区别><strong>ClusterIP 与 Headless Service 的区别</strong></a></li><li><a href=#总结-35>总结</a></li></ul></li><li><a href=#不进行负载均衡直接访问每个-pod-比如数据库有三个实例pod用headless的模式那么应用连接这个数据库的方式是啥三个不同的svc>不进行负载均衡，直接访问每个 Pod？ 比如数据库有三个实例pod用headless的模式，那么应用连接这个数据库的方式是啥？三个不同的svc？</a><ul><li><a href=#1-通过-dns-直接访问每个-pod>1. <strong>通过 DNS 直接访问每个 Pod</strong></a></li><li><a href=#2-通过多个-service-访问每个实例>2. <strong>通过多个 Service 访问每个实例</strong></a></li><li><a href=#适用场景-1>适用场景</a></li><li><a href=#总结-36>总结</a></li></ul></li><li><a href=#db-0dbdefaultsvcclusterlocal-db-1dbdefaultsvcclusterlocal-db-2dbdefaultsvcclusterlocal-这不就是3个不同svc吗>db-0.db.default.svc.cluster.local db-1.db.default.svc.cluster.local db-2.db.default.svc.cluster.local 这不就是3个不同svc吗</a><ul><li><a href=#headless-service-和-pod-的-dns-解析><strong>Headless Service 和 Pod 的 DNS 解析</strong></a></li><li><a href=#为什么这些看起来像是多个服务><strong>为什么这些看起来像是多个服务</strong></a></li><li><a href=#总结-37><strong>总结</strong></a></li></ul></li><li><a href=#在-headless-service-中实际上-并没有三个不同的-service而是-一个-service但它会为每个-pod-分配一个独立的-dns-记录那这个一个svc是啥>在 Headless Service 中，实际上 并没有三个不同的 Service，而是 一个 Service，但它会为每个 Pod 分配一个独立的 DNS 记录，那这个一个svc是啥</a><ul><li><a href=#具体来说>具体来说：</a></li><li><a href=#举个例子>举个例子：</a></li><li><a href=#所以这个一个-service-是>所以，<strong>这个一个 Service</strong> 是：</a></li><li><a href=#结论-1>结论：</a></li></ul></li><li><a href=#linux容器技术的基础原理>Linux容器技术的基础原理</a><ul><li><a href=#1-命名空间namespaces>1. <strong>命名空间（Namespaces）</strong></a></li><li><a href=#2-控制组cgroups>2. <strong>控制组（cgroups）</strong></a></li><li><a href=#3-联合文件系统union-file-system>3. <strong>联合文件系统（Union File System）</strong></a></li><li><a href=#4-容器镜像container-images>4. <strong>容器镜像（Container Images）</strong></a></li><li><a href=#5-容器的隔离性与安全性>5. <strong>容器的隔离性与安全性</strong></a></li><li><a href=#总结-38>总结</a></li></ul></li><li><a href=#kubernetes-pod的常见调度方式>Kubernetes Pod的常见调度方式</a><ul><li><a href=#常见的-pod-调度方式>常见的 Pod 调度方式</a></li><li><a href=#8-daemonset>8. <strong>DaemonSet</strong></a></li><li><a href=#9-静态和动态调度static-and-dynamic-scheduling>9. <strong>静态和动态调度（Static and Dynamic Scheduling）</strong></a></li><li><a href=#10-资源调度策略>10. <strong>资源调度策略</strong></a></li><li><a href=#总结-39>总结：</a></li></ul></li><li><a href=#kubernetes-ingress原理>kubernetes Ingress原理☆</a><ul><li><a href=#kubernetes-ingress-原理>Kubernetes Ingress 原理</a></li><li><a href=#1-ingress-资源>1. <strong>Ingress 资源</strong></a></li><li><a href=#2-ingress-controller>2. <strong>Ingress Controller</strong></a></li><li><a href=#3-ingress-的工作原理>3. <strong>Ingress 的工作原理</strong></a></li><li><a href=#4-ingress-的特性>4. <strong>Ingress 的特性</strong></a></li><li><a href=#5-ingress-与-service-的关系>5. <strong>Ingress 与 Service 的关系</strong></a></li><li><a href=#6-ingress-的使用场景>6. <strong>Ingress 的使用场景</strong></a></li><li><a href=#7-ingress-controller-部署>7. <strong>Ingress Controller 部署</strong></a></li><li><a href=#8-常见的-ingress-controller>8. <strong>常见的 Ingress Controller</strong></a></li><li><a href=#总结-40>总结</a></li></ul></li><li><a href=#kubernetes各模块如何与api-server通信>Kubernetes各模块如何与API Server通信</a><ul><li><a href=#各模块与-api-server-的通信方式>各模块与 API Server 的通信方式</a></li><li><a href=#具体通信流程>具体通信流程</a></li><li><a href=#总结-41>总结</a></li></ul></li><li><a href=#kubelet监控worker节点如何实现>kubelet监控worker节点如何实现</a><ul><li><a href=#1-kubelet-的健康检查>1. <strong>kubelet 的健康检查</strong></a></li><li><a href=#2-kubelet-的指标暴露>2. <strong>kubelet 的指标暴露</strong></a></li><li><a href=#3-资源使用监控>3. <strong>资源使用监控</strong></a></li><li><a href=#4-事件记录>4. <strong>事件记录</strong></a></li><li><a href=#5-集群级别的监控系统>5. <strong>集群级别的监控系统</strong></a></li><li><a href=#6-node-conditions节点状态>6. <strong>Node Conditions（节点状态）</strong></a></li><li><a href=#7-资源调度与驱逐>7. <strong>资源调度与驱逐</strong></a></li><li><a href=#8-故障监控与通知>8. <strong>故障监控与通知</strong></a></li><li><a href=#9-日志监控>9. <strong>日志监控</strong></a></li><li><a href=#10-系统级监控>10. <strong>系统级监控</strong></a></li><li><a href=#总结-42>总结</a></li></ul></li><li><a href=#容器时区不一致如何解决>容器时区不一致如何解决？</a><ul><li><a href=#1-使用宿主机的时区>1. <strong>使用宿主机的时区</strong></a></li><li><a href=#2-设置容器内的时区>2. <strong>设置容器内的时区</strong></a></li><li><a href=#3-使用>3. <strong>使用 <code>UTC</code> 时区</strong></a></li><li><a href=#4-使用-docker-compose-或-kubernetes-的-configmap>4. <strong>使用 Docker Compose 或 Kubernetes 的 ConfigMap</strong></a></li><li><a href=#总结-43>总结</a></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><h2 id=kubernetes>Kubernetes
<a class=anchor href=#kubernetes>#</a></h2><h2 id=谈谈你对k8s的理解>谈谈你对k8s的理解☆
<a class=anchor href=#%e8%b0%88%e8%b0%88%e4%bd%a0%e5%af%b9k8s%e7%9a%84%e7%90%86%e8%a7%a3>#</a></h2><p>Kubernetes（简称 K8s）是一个开源的容器编排平台，旨在自动化容器化应用程序的部署、扩展和管理。它最初由 Google 设计，并在2014年开源。K8s 提供了一个集中的平台来管理和运行容器化的应用程序，尤其适合大规模的分布式系统和微服务架构。</p><p>从技术角度看，K8s 主要有以下几个核心组成部分：</p><ol><li><strong>集群（Cluster）</strong>： K8s 将多个节点组织成一个集群，集群中通常有两类节点：控制平面节点（Control Plane）和工作节点（Worker Node）。控制平面负责集群的管理与调度，而工作节点运行实际的应用程序。</li><li><strong>Pod</strong>： Pod 是 Kubernetes 中最小的调度单位，通常包含一个或多个容器，这些容器共享同一个网络、存储和其他资源。Pod 是容器运行的基本单位。</li><li><strong>Deployment</strong>： Deployment 用于管理和控制应用程序的副本和版本。它通过定义副本数来保证应用的高可用性，支持滚动更新、回滚等功能。</li><li><strong>Service</strong>： Service 用于暴露应用程序的网络接口，并提供负载均衡。通过 Service，用户可以通过稳定的 IP 或 DNS 名称访问一组 Pod。</li><li><strong>Ingress</strong>： Ingress 提供 HTTP 和 HTTPS 路由到集群内的服务，可以用于管理外部流量进入集群，支持负载均衡、SSL 终端等功能。</li><li><strong>ConfigMap 和 Secret</strong>： ConfigMap 用于管理应用的配置数据，Secret 用于存储敏感数据（如密码、API 密钥等）。这些资源可以被容器在运行时动态加载。</li><li><strong>Namespace</strong>： Namespace 提供了对资源的隔离，允许在同一个集群中多个团队或应用之间共享资源而不发生冲突。它相当于虚拟化的逻辑分区。</li><li><strong>StatefulSet</strong>： StatefulSet 是一种控制器，用于管理有状态应用，如数据库等。与 Deployment 不同，StatefulSet 保证了 Pod 的稳定性和唯一性，适合需要持久存储和有序部署的应用。</li><li><strong>Persistent Volume (PV) 和 Persistent Volume Claim (PVC)</strong>： PV 是集群内的存储资源，而 PVC 是用户对存储资源的请求。通过这种方式，K8s 支持动态存储的创建和销毁。</li></ol><h3 id=关键优势>关键优势：
<a class=anchor href=#%e5%85%b3%e9%94%ae%e4%bc%98%e5%8a%bf>#</a></h3><ul><li><strong>自动化</strong>：K8s 提供了自动化部署、滚动更新、扩展和恢复功能，极大地简化了应用程序生命周期管理。</li><li><strong>高可用性</strong>：K8s 支持容错和高可用性，可以通过多副本部署、健康检查、自动恢复等机制保证应用的可靠性。</li><li><strong>灵活性</strong>：通过声明式配置，K8s 让开发者可以专注于应用本身，而不必担心基础设施细节。</li><li><strong>扩展性</strong>：K8s 提供了强大的插件和扩展机制，支持各种中间件、监控、日志等功能的集成。</li></ul><h3 id=适用场景>适用场景：
<a class=anchor href=#%e9%80%82%e7%94%a8%e5%9c%ba%e6%99%af>#</a></h3><ul><li><strong>微服务架构</strong>：K8s 非常适合容器化微服务的管理，能有效地处理服务之间的依赖、负载均衡、容错等。</li><li><strong>大规模分布式应用</strong>：由于其自动化和扩展性，K8s 很适合处理大规模、复杂的分布式系统。</li><li><strong>混合云/多云架构</strong>：K8s 提供跨平台的支持，使得应用能够在不同的云环境或本地数据中心之间无缝迁移。</li></ul><p>总的来说，Kubernetes 是现代 DevOps 和持续集成/持续交付（CI/CD）的核心技术之一，能够有效地管理容器化应用，使得开发和运维更加高效、灵活和可靠。</p><h2 id=k8s集群架构是什么>k8s集群架构是什么☆
<a class=anchor href=#k8s%e9%9b%86%e7%be%a4%e6%9e%b6%e6%9e%84%e6%98%af%e4%bb%80%e4%b9%88>#</a></h2><p>Kubernetes (K8s) 集群架构由多个组件组成，主要分为控制平面（Control Plane）和工作节点（Worker Node）。控制平面负责集群的管理与调度，而工作节点则运行实际的应用程序和服务。下面是 K8s 集群架构的主要组件：</p><h3 id=1-控制平面control-plane>1. <strong>控制平面（Control Plane）</strong>
<a class=anchor href=#1-%e6%8e%a7%e5%88%b6%e5%b9%b3%e9%9d%a2control-plane>#</a></h3><p>控制平面是集群的大脑，负责管理整个集群的状态。它处理所有的任务调度、资源分配和集群状态维护。控制平面通常由多个组件组成，以下是控制平面的核心组件：</p><ul><li><strong>API Server (<code>kube-apiserver</code>)</strong>：<ul><li>作为 K8s 集群的入口点，所有的操作请求（如部署、修改、删除资源）都会通过 API Server 进行。它提供了集群资源的 RESTful API 接口，所有的组件和用户与 K8s 的交互都是通过它来实现的。</li><li>它是集群中的唯一管理接口，并且与其他控制平面组件（如 etcd、controller manager）进行通信。</li></ul></li><li><strong>etcd</strong>：<ul><li>一个高可用的键值存储数据库，K8s 使用它来存储集群的所有数据和状态，包括节点信息、Pod 状态、配置等。所有的集群状态都保存在 etcd 中，是集群的 &ldquo;源数据&rdquo;。</li><li>etcd 通常是一个分布式系统，可以保证数据的一致性和高可用性。</li></ul></li><li><strong>Controller Manager (<code>kube-controller-manager</code>)</strong>：<ul><li>控制器是 K8s 的守护进程，负责维护集群状态并根据需要进行调整。它监控 K8s 集群的状态，并执行相应的操作。例如，ReplicaSet 控制器会确保指定数量的副本 Pods 始终处于运行状态。</li><li>Controller Manager 会通过 API Server 与 etcd 进行交互，以便同步集群的状态。</li></ul></li><li><strong>Scheduler (<code>kube-scheduler</code>)</strong>：<ul><li>调度器负责将待运行的 Pod 调度到合适的工作节点上。它根据节点的资源利用情况、优先级、调度策略等因素来选择最适合的节点。</li><li>调度器还会考虑 Pod 的 Affinity、Taints 和 Tolerations 等设置，确保 Pod 能够在合适的环境中运行。</li></ul></li></ul><h3 id=2-工作节点worker-node>2. <strong>工作节点（Worker Node）</strong>
<a class=anchor href=#2-%e5%b7%a5%e4%bd%9c%e8%8a%82%e7%82%b9worker-node>#</a></h3><p>工作节点是实际承载应用程序和服务的地方。每个工作节点上都运行着一组 K8s 组件，主要包括：</p><ul><li><strong>Kubelet (<code>kubelet</code>)</strong>：<ul><li>Kubelet 是每个工作节点上的主要代理，负责确保容器在节点上按照 Pod 定义的规范运行。它监控容器的生命周期，并与 API Server 通信，确保节点上的资源与集群状态保持一致。</li><li>它会定期向 API Server 报告节点和 Pod 的状态，确保集群中的状态与期望的一致。</li></ul></li><li><strong>Kube Proxy (<code>kube-proxy</code>)</strong>：<ul><li>Kube Proxy 是工作节点上的网络代理，负责维护网络规则，确保 Pod 能够通过正确的网络路由进行通信。</li><li>它通常会在每个节点上运行，并提供服务发现、负载均衡等功能。它根据 Service 的定义配置网络规则，使得流量能够正确地路由到相应的 Pod。</li></ul></li><li><strong>Container Runtime</strong>：<ul><li>容器运行时是负责容器生命周期管理的组件，K8s 支持多种容器运行时，包括 Docker、containerd、CRI-O 等。容器运行时负责拉取镜像、创建和销毁容器等操作。</li><li>在大多数 K8s 集群中，Docker 是默认的容器运行时，但现在 Kubernetes 推荐使用 containerd 或 CRI-O 等作为容器运行时。</li></ul></li></ul><h3 id=3-k8s-集群架构示意图>3. <strong>K8s 集群架构示意图</strong>
<a class=anchor href=#3-k8s-%e9%9b%86%e7%be%a4%e6%9e%b6%e6%9e%84%e7%a4%ba%e6%84%8f%e5%9b%be>#</a></h3><pre tabindex=0><code>                +--------------------------------------+
                |            Control Plane            |
                |                                      |
                |  +------------+  +----------------+  |
                |  |  API Server|  |   Controller   |  |
                |  +------------+  |    Manager     |  |
                |  +------------+  +----------------+  |
                |  |    etcd    |                      |
                |  +------------+  +----------------+  |
                |  | Scheduler  |                      |
                |  +------------+                      |
                +--------------------------------------+
                           |              |
             +-------------+--------------+-------------+
             |                                             |
  +-----------------+                         +-----------------+
  |    Worker Node 1|                         |    Worker Node 2|
  |                 |                         |                 |
  |  +----------+   |                         |  +----------+   |
  |  |  Kubelet |   |                         |  |  Kubelet |   |
  |  +----------+   |                         |  +----------+   |
  |  +----------+   |                         |  +----------+   |
  |  | Kube Proxy|   |                         |  | Kube Proxy|   |
  |  +----------+   |                         |  +----------+   |
  |  +------------+  |                         |  +------------+  |
  |  | Container  |   |                         |  | Container  |   |
  |  | Runtime    |   |                         |  | Runtime    |   |
  |  +------------+  |                         |  +------------+  |
  +-----------------+                         +-----------------+
</code></pre><h3 id=4-其他重要组件>4. <strong>其他重要组件</strong>
<a class=anchor href=#4-%e5%85%b6%e4%bb%96%e9%87%8d%e8%a6%81%e7%bb%84%e4%bb%b6>#</a></h3><ul><li><p>Ingress Controller</p><p>：</p><ul><li>Ingress 是 Kubernetes 中的一个 API 对象，用于管理外部流量的访问控制。Ingress Controller 是实现 Ingress 规则的组件，它负责将 HTTP/HTTPS 请求路由到集群内的服务。</li></ul></li><li><p>Metrics Server</p><p>：</p><ul><li>Metrics Server 是 K8s 集群的监控组件，它收集每个节点和 Pod 的资源使用情况（如 CPU、内存等）。它为自动扩缩容（如 Horizontal Pod Autoscaler）和其他资源管理提供数据支持。</li></ul></li></ul><h3 id=5-高可用性与冗余>5. <strong>高可用性与冗余</strong>
<a class=anchor href=#5-%e9%ab%98%e5%8f%af%e7%94%a8%e6%80%a7%e4%b8%8e%e5%86%97%e4%bd%99>#</a></h3><ul><li>K8s 支持控制平面的高可用部署，可以通过在多个主机上部署多个控制平面组件（如 API Server、Controller Manager、Scheduler 和 etcd）来确保系统的容错能力。</li><li>工作节点也可以通过水平扩展进行增加，确保集群的弹性和负载分担。</li></ul><h3 id=总结>总结
<a class=anchor href=#%e6%80%bb%e7%bb%93>#</a></h3><p>Kubernetes 集群架构采用了控制平面和工作节点的设计，将集群管理与应用运行分开，使得集群的维护、扩展和管理更加灵活和高效。控制平面负责集群的整体管理，而工作节点则专注于运行容器化的应用程序。通过这些组件的协作，K8s 能够实现容器化应用的自动化部署、扩展、故障恢复等关键功能。</p><h2 id=简述pod创建过程>简述Pod创建过程☆
<a class=anchor href=#%e7%ae%80%e8%bf%b0pod%e5%88%9b%e5%bb%ba%e8%bf%87%e7%a8%8b>#</a></h2><p>Pod 是 Kubernetes 中最小的调度单位，它是由一个或多个容器组成的一个集合，这些容器共享同一个网络、存储和配置。Pod 的创建过程可以分为以下几个关键步骤：</p><h3 id=1-创建-pod-配置文件>1. <strong>创建 Pod 配置文件</strong>
<a class=anchor href=#1-%e5%88%9b%e5%bb%ba-pod-%e9%85%8d%e7%bd%ae%e6%96%87%e4%bb%b6>#</a></h3><p>Pod 通常通过 YAML 或 JSON 格式的配置文件定义。配置文件描述了 Pod 的所需资源、容器的镜像、环境变量、挂载的存储卷等信息。以下是一个简单的 Pod 配置示例：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Pod</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-pod</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>containers</span>:
</span></span><span style=display:flex><span>  - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-container</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>image</span>: <span style=color:#ae81ff>nginx:latest</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>ports</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>containerPort</span>: <span style=color:#ae81ff>80</span>
</span></span></code></pre></div><h3 id=2-提交-pod-配置到-kubernetes-api-server>2. <strong>提交 Pod 配置到 Kubernetes API Server</strong>
<a class=anchor href=#2-%e6%8f%90%e4%ba%a4-pod-%e9%85%8d%e7%bd%ae%e5%88%b0-kubernetes-api-server>#</a></h3><p>用户通过 <code>kubectl</code> 或其他客户端工具，将 Pod 的配置文件提交到 Kubernetes API Server。API Server 是集群的入口，它接收请求并将 Pod 配置存储在集群的数据库中（通常是 etcd）。</p><p>例如，使用 <code>kubectl</code> 提交 Pod：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f pod.yaml
</span></span></code></pre></div><h3 id=3-api-server-存储-pod-配置>3. <strong>API Server 存储 Pod 配置</strong>
<a class=anchor href=#3-api-server-%e5%ad%98%e5%82%a8-pod-%e9%85%8d%e7%bd%ae>#</a></h3><p>在收到请求后，API Server 会将 Pod 配置存储在 etcd 中，确保集群的状态得到持久化存储。API Server 会对请求进行验证和授权，确保只有合法的操作能够提交。</p><h3 id=4-调度scheduler选择合适的工作节点>4. <strong>调度（Scheduler）选择合适的工作节点</strong>
<a class=anchor href=#4-%e8%b0%83%e5%ba%a6scheduler%e9%80%89%e6%8b%a9%e5%90%88%e9%80%82%e7%9a%84%e5%b7%a5%e4%bd%9c%e8%8a%82%e7%82%b9>#</a></h3><p>Kubernetes 的调度器（Scheduler）会根据 Pod 的资源需求和集群的资源状况，决定将 Pod 调度到哪个工作节点。调度器会考虑以下因素：</p><ul><li>资源要求：CPU、内存等。</li><li>节点的资源利用率。</li><li>节点标签和亲和性规则。</li><li>Taints 和 Tolerations 等调度策略。</li></ul><p>调度器选择合适的节点后，会将 Pod 的调度信息更新到 API Server 中。</p><h3 id=5-kubelet-在工作节点上创建-pod>5. <strong>Kubelet 在工作节点上创建 Pod</strong>
<a class=anchor href=#5-kubelet-%e5%9c%a8%e5%b7%a5%e4%bd%9c%e8%8a%82%e7%82%b9%e4%b8%8a%e5%88%9b%e5%bb%ba-pod>#</a></h3><p>在 Pod 被调度到工作节点后，工作节点上的 Kubelet 负责管理该节点的容器和 Pod。Kubelet 会定期向 API Server 查询集群状态，发现 Pod 已经被调度到自己节点后，Kubelet 会执行以下操作：</p><ul><li>拉取容器镜像（如果镜像未缓存）。</li><li>创建并启动容器。</li><li>配置网络：Kubelet 为 Pod 中的所有容器配置网络，使它们能够通过集群的网络与其他容器通信。</li><li>挂载存储：如果 Pod 中定义了卷（Volume），Kubelet 会挂载对应的存储到容器中。</li></ul><p>Kubelet 会通过容器运行时（如 Docker、containerd）来启动容器，并确保容器处于运行状态。</p><h3 id=6-pod-启动并运行>6. <strong>Pod 启动并运行</strong>
<a class=anchor href=#6-pod-%e5%90%af%e5%8a%a8%e5%b9%b6%e8%bf%90%e8%a1%8c>#</a></h3><p>一旦容器启动成功，Kubelet 会定期检查 Pod 和容器的健康状况。Pod 中的容器如果启动成功且健康，就进入运行状态，Kubelet 会向 API Server 汇报节点和 Pod 的状态，API Server 会更新集群的状态信息。</p><h3 id=7-集群状态更新>7. <strong>集群状态更新</strong>
<a class=anchor href=#7-%e9%9b%86%e7%be%a4%e7%8a%b6%e6%80%81%e6%9b%b4%e6%96%b0>#</a></h3><p>最终，API Server 会将 Pod 的状态同步到集群的各个组件，确保所有节点都知道 Pod 的状态。集群中的其他服务（如 Service、Ingress 等）可以根据 Pod 的状态进行流量调度。</p><h3 id=8-监控与自动恢复>8. <strong>监控与自动恢复</strong>
<a class=anchor href=#8-%e7%9b%91%e6%8e%a7%e4%b8%8e%e8%87%aa%e5%8a%a8%e6%81%a2%e5%a4%8d>#</a></h3><p>Kubelet 和控制器持续监控 Pod 的健康状况和运行状态。如果 Pod 中的容器崩溃或停止运行，Kubelet 会根据 Pod 的定义重新启动容器，保证 Pod 处于期望的状态。</p><h3 id=总结-1>总结
<a class=anchor href=#%e6%80%bb%e7%bb%93-1>#</a></h3><p>Pod 创建过程从用户定义 Pod 配置文件开始，到 API Server 存储配置，调度器决定节点，Kubelet 在节点上拉取镜像并启动容器，最后 Pod 进入运行状态并由 Kubelet 定期检查健康。这一过程展示了 Kubernetes 自动化部署和容器管理的能力，确保应用始终处于期望的状态。</p><h2 id=简述删除一个pod流程>简述删除一个Pod流程
<a class=anchor href=#%e7%ae%80%e8%bf%b0%e5%88%a0%e9%99%a4%e4%b8%80%e4%b8%aapod%e6%b5%81%e7%a8%8b>#</a></h2><p>删除一个 Pod 的过程可以分为以下几个步骤。整个流程涉及用户发起删除请求、Kubernetes 控制平面处理请求、以及工作节点上的 Kubelet 进行容器的销毁和资源的回收。</p><h3 id=1-用户发起删除请求>1. <strong>用户发起删除请求</strong>
<a class=anchor href=#1-%e7%94%a8%e6%88%b7%e5%8f%91%e8%b5%b7%e5%88%a0%e9%99%a4%e8%af%b7%e6%b1%82>#</a></h3><p>用户可以通过 <code>kubectl</code> 或其他客户端工具发起删除 Pod 的请求。例如：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl delete pod &lt;pod-name&gt;
</span></span></code></pre></div><p>这个命令会向 Kubernetes API Server 发送删除 Pod 的请求。</p><h3 id=2-api-server-接受并验证请求>2. <strong>API Server 接受并验证请求</strong>
<a class=anchor href=#2-api-server-%e6%8e%a5%e5%8f%97%e5%b9%b6%e9%aa%8c%e8%af%81%e8%af%b7%e6%b1%82>#</a></h3><p>API Server 接收到删除 Pod 的请求后，会验证该请求是否合法。验证过程包括：</p><ul><li>确保用户有足够的权限删除 Pod。</li><li>确保 Pod 确实存在于集群中。</li></ul><p>如果验证通过，API Server 会将删除请求存储到集群的数据库（etcd），并更新集群的状态。</p><h3 id=3-pod-状态更新>3. <strong>Pod 状态更新</strong>
<a class=anchor href=#3-pod-%e7%8a%b6%e6%80%81%e6%9b%b4%e6%96%b0>#</a></h3><p>当 API Server 确认删除 Pod 的请求后，它会更新集群的状态并将 Pod 状态标记为 &ldquo;Terminating&rdquo;。此时，Pod 仍在集群中，但标记为即将被删除。</p><h3 id=4-删除-pod-相关的控制器对象>4. <strong>删除 Pod 相关的控制器对象</strong>
<a class=anchor href=#4-%e5%88%a0%e9%99%a4-pod-%e7%9b%b8%e5%85%b3%e7%9a%84%e6%8e%a7%e5%88%b6%e5%99%a8%e5%af%b9%e8%b1%a1>#</a></h3><p>如果 Pod 是由某些控制器（如 Deployment、StatefulSet 等）管理的，控制器会识别到 Pod 被删除，并根据控制器的设置（例如 ReplicaSet）决定是否重新创建 Pod。这意味着如果 Pod 是通过 Deployment 等控制器创建的，控制器会根据设置创建一个新的 Pod 来替代被删除的 Pod。</p><h3 id=5-kubelet-监测到-pod-删除请求>5. <strong>Kubelet 监测到 Pod 删除请求</strong>
<a class=anchor href=#5-kubelet-%e7%9b%91%e6%b5%8b%e5%88%b0-pod-%e5%88%a0%e9%99%a4%e8%af%b7%e6%b1%82>#</a></h3><p>当 API Server 更新 Pod 状态为 &ldquo;Terminating&rdquo; 后，Kubelet 会在所在工作节点上监测到 Pod 被删除的请求。Kubelet 会执行以下操作：</p><ul><li><strong>停止容器</strong>：Kubelet 会通知容器运行时（如 Docker 或 containerd）停止运行 Pod 中的所有容器。</li><li><strong>删除容器实例</strong>：容器运行时会销毁这些容器，包括清理网络接口、挂载的存储等资源。</li><li><strong>清理相关资源</strong>：Kubelet 会清理 Pod 相关的资源，如临时存储卷、日志文件等。</li></ul><h3 id=6-pod-资源回收>6. <strong>Pod 资源回收</strong>
<a class=anchor href=#6-pod-%e8%b5%84%e6%ba%90%e5%9b%9e%e6%94%b6>#</a></h3><p>当容器被终止并且相关的资源被清理后，Kubelet 会向 API Server 发送更新，标记 Pod 已经成功删除。</p><h3 id=7-api-server-完成删除操作>7. <strong>API Server 完成删除操作</strong>
<a class=anchor href=#7-api-server-%e5%ae%8c%e6%88%90%e5%88%a0%e9%99%a4%e6%93%8d%e4%bd%9c>#</a></h3><p>在 Kubelet 确认 Pod 已经被完全删除并清理后，API Server 会将 Pod 的状态从 &ldquo;Terminating&rdquo; 更新为已删除状态，并从 etcd 中彻底删除该 Pod 的记录。</p><h3 id=8-删除完成>8. <strong>删除完成</strong>
<a class=anchor href=#8-%e5%88%a0%e9%99%a4%e5%ae%8c%e6%88%90>#</a></h3><p>Pod 完全删除后，集群的状态更新，Pod 在 Kubernetes 集群中不再存在，相关的网络、存储和其他资源也被回收。</p><h3 id=总结-2>总结
<a class=anchor href=#%e6%80%bb%e7%bb%93-2>#</a></h3><p>删除 Pod 的过程涉及以下关键步骤：</p><ul><li>用户发起删除请求，API Server 验证并更新 Pod 状态为 &ldquo;Terminating&rdquo;。</li><li>Kubelet 停止容器，清理资源并向 API Server 上报删除状态。</li><li>API Server 在确认删除完成后，从 etcd 中删除 Pod 记录，删除过程完成。</li></ul><p>对于由控制器管理的 Pod，控制器会根据设置决定是否重新创建 Pod，以维持预期的副本数。</p><h2 id=不同node上的pod之间的通信过程>不同node上的Pod之间的通信过程☆
<a class=anchor href=#%e4%b8%8d%e5%90%8cnode%e4%b8%8a%e7%9a%84pod%e4%b9%8b%e9%97%b4%e7%9a%84%e9%80%9a%e4%bf%a1%e8%bf%87%e7%a8%8b>#</a></h2><p>Kubernetes 中不同节点（Node）上的 Pod 之间的通信过程涉及多个 Kubernetes 组件和底层网络技术。K8s 的网络模型要求每个 Pod 都有一个唯一的 IP 地址，并且允许 Pod 之间无障碍地进行通信，无论它们位于同一节点还是不同节点。</p><p>以下是不同节点上的 Pod 之间通信的过程：</p><h3 id=1-pod-网络模型>1. <strong>Pod 网络模型</strong>
<a class=anchor href=#1-pod-%e7%bd%91%e7%bb%9c%e6%a8%a1%e5%9e%8b>#</a></h3><p>K8s 的网络模型要求每个 Pod 都有一个独立的 IP 地址。这意味着：</p><ul><li><strong>Pod 间通信</strong>：不同节点上的 Pod 可以直接通过其 IP 地址进行通信。</li><li><strong>容器间通信</strong>：同一 Pod 内的容器共享同一 IP 地址和端口，因此容器之间可以直接通过 localhost 进行通信。</li></ul><p>这种设计理念避免了复杂的 NAT 转换和端口映射，使得容器和服务之间的通信更加直观。</p><h3 id=2-cni-插件容器网络接口>2. <strong>CNI 插件（容器网络接口）</strong>
<a class=anchor href=#2-cni-%e6%8f%92%e4%bb%b6%e5%ae%b9%e5%99%a8%e7%bd%91%e7%bb%9c%e6%8e%a5%e5%8f%a3>#</a></h3><p>Kubernetes 使用 CNI（Container Network Interface）插件来管理集群网络，CNI 插件负责 Pod 网络的创建、删除、路由和通信。常见的 CNI 插件包括 Calico、Flannel、Weave、Cilium 等。</p><ul><li>CNI 插件在节点上配置网络规则，确保每个 Pod 拥有唯一的 IP 地址，并能够通过底层网络进行通信。</li><li>CNI 插件可能会使用不同的底层网络技术，例如 VXLAN、VLAN、或直接使用物理网络。</li></ul><h3 id=3-通信过程>3. <strong>通信过程</strong>
<a class=anchor href=#3-%e9%80%9a%e4%bf%a1%e8%bf%87%e7%a8%8b>#</a></h3><p>假设有两个 Pod，Pod A 位于 Node 1，Pod B 位于 Node 2，Pod A 和 Pod B 需要进行通信。这个过程通常包括以下几个步骤：</p><h4 id=31-pod-a-发起请求>3.1 <strong>Pod A 发起请求</strong>
<a class=anchor href=#31-pod-a-%e5%8f%91%e8%b5%b7%e8%af%b7%e6%b1%82>#</a></h4><p>Pod A 想要访问 Pod B，它会向 Pod B 的 IP 地址发送请求。在 Kubernetes 网络模型下，这个请求不会经过主机的 NAT（网络地址转换）层，而是直接发送到目标 Pod 的 IP 地址。</p><h4 id=32-cni-插件处理请求>3.2 <strong>CNI 插件处理请求</strong>
<a class=anchor href=#32-cni-%e6%8f%92%e4%bb%b6%e5%a4%84%e7%90%86%e8%af%b7%e6%b1%82>#</a></h4><p>Pod A 的网络流量会由 CNI 插件进行处理：</p><ul><li>CNI 插件会将 Pod A 的请求通过容器网络转换为底层网络的格式，并决定如何路由到目标 Pod 的节点（Node 2）。这通常是通过在每个节点上配置路由表和使用网络隧道（如 VXLAN 或 GRE）来实现跨节点通信。</li></ul><h4 id=33-跨节点通信>3.3 <strong>跨节点通信</strong>
<a class=anchor href=#33-%e8%b7%a8%e8%8a%82%e7%82%b9%e9%80%9a%e4%bf%a1>#</a></h4><ul><li>如果 Pod A 和 Pod B 在不同节点上，CNI 插件会使用底层网络（如 VXLAN 隧道或 BGP 路由）将流量从 Node 1 路由到 Node 2。</li><li>通常，Pod A 发送的流量首先到达 Node 1 的物理网络接口，然后通过 CNI 插件封装和路由到 Node 2。</li></ul><h4 id=34-node-2-处理流量>3.4 <strong>Node 2 处理流量</strong>
<a class=anchor href=#34-node-2-%e5%a4%84%e7%90%86%e6%b5%81%e9%87%8f>#</a></h4><p>Node 2 上的 CNI 插件接收到从 Node 1 发来的请求，并根据目标 IP 地址找到相应的 Pod B。CNI 插件会通过相应的网络接口和路由规则将流量传递到 Pod B 上。</p><h4 id=35-pod-b-响应请求>3.5 <strong>Pod B 响应请求</strong>
<a class=anchor href=#35-pod-b-%e5%93%8d%e5%ba%94%e8%af%b7%e6%b1%82>#</a></h4><p>Pod B 收到请求后，进行处理并发送响应。Pod B 的响应流量会经过类似的过程：</p><ul><li>它会将响应发送到 Pod A 的 IP 地址。</li><li>如果 Pod A 和 Pod B 在不同的节点上，响应会通过 Node 2 到 Node 1，再由 CNI 插件将响应路由到 Pod A。</li></ul><h3 id=4-网络策略network-policy>4. <strong>网络策略（Network Policy）</strong>
<a class=anchor href=#4-%e7%bd%91%e7%bb%9c%e7%ad%96%e7%95%a5network-policy>#</a></h3><p>K8s 还允许管理员定义网络策略，以限制 Pod 之间的通信。网络策略允许你控制哪些 Pod 可以与其他 Pod 通信，基于标签选择器、命名空间等规则来定义流量的允许与拒绝。</p><h3 id=5-service-和负载均衡>5. <strong>Service 和负载均衡</strong>
<a class=anchor href=#5-service-%e5%92%8c%e8%b4%9f%e8%bd%bd%e5%9d%87%e8%a1%a1>#</a></h3><p>通常，Pod 是临时的和无状态的，所以 K8s 提供了 <strong>Service</strong> 作为 Pod 的抽象。Service 提供一个稳定的访问点，可以自动负载均衡请求到多个后端 Pod。</p><ul><li>当 Pod A 想要与 Pod B 通信时，它可能通过 Service 名称而不是直接通过 IP 地址来访问 Pod B。Kubernetes 会通过 <strong>ClusterIP</strong> 或 <strong>DNS</strong> 解析 Pod B 的 Service 地址，并将流量路由到实际的 Pod。</li><li>如果 Pod B 在不同节点上，Service 会使用底层网络组件（如 kube-proxy）进行流量转发。</li></ul><h3 id=总结-3>总结
<a class=anchor href=#%e6%80%bb%e7%bb%93-3>#</a></h3><p>不同节点上的 Pod 之间的通信过程包括：</p><ol><li>Pod A 发起请求，向目标 Pod 的 IP 地址发送数据。</li><li>CNI 插件处理流量，通过底层网络进行路由。流量可能经过网络隧道（如 VXLAN）跨越节点。</li><li>目标节点（Node 2）上的 CNI 插件接收流量，并将其转发到目标 Pod（Pod B）。</li><li>Pod B 处理请求并返回响应，响应过程与请求过程类似。</li></ol><p>整个过程依赖于 Kubernetes 的网络模型和 CNI 插件来实现跨节点的容器通信。此外，网络策略、Service 和负载均衡等功能可以进一步优化和管理 Pod 之间的通信。</p><h2 id=pod创建pending状态的原因>pod创建Pending状态的原因☆
<a class=anchor href=#pod%e5%88%9b%e5%bb%bapending%e7%8a%b6%e6%80%81%e7%9a%84%e5%8e%9f%e5%9b%a0>#</a></h2><p>Pod 进入 <strong>Pending</strong> 状态的原因通常是由于 Kubernetes 在调度和分配资源时遇到了问题。<strong>Pending</strong> 状态表示 Pod 已经被创建，但尚未分配到合适的节点上，或者在节点上尚未成功运行。以下是常见的导致 Pod 进入 Pending 状态的原因：</p><h3 id=1-资源不足>1. <strong>资源不足</strong>
<a class=anchor href=#1-%e8%b5%84%e6%ba%90%e4%b8%8d%e8%b6%b3>#</a></h3><ul><li><p>CPU/内存不足</p><p>：如果集群中没有节点能够提供满足 Pod 所需资源（如 CPU 或内存）的空间，Pod 会一直处于 Pending 状态，直到资源可用。</p><ul><li><strong>解决方法</strong>：检查集群中的节点资源使用情况（例如使用 <code>kubectl describe pod &lt;pod-name></code> 查看 Pod 的详细信息），并考虑扩展集群或优化资源配置。</li></ul></li></ul><h3 id=2-调度器找不到合适的节点>2. <strong>调度器找不到合适的节点</strong>
<a class=anchor href=#2-%e8%b0%83%e5%ba%a6%e5%99%a8%e6%89%be%e4%b8%8d%e5%88%b0%e5%90%88%e9%80%82%e7%9a%84%e8%8a%82%e7%82%b9>#</a></h3><ul><li><p><strong>节点资源限制</strong>：Pod 可能要求的资源（如 CPU、内存、存储）超过了集群中任何节点的可用资源，导致调度器无法找到合适的节点来运行 Pod。</p></li><li><p>节点选择约束</p><p>：Pod 可能带有某些调度约束，如节点亲和性（Affinity）或反亲和性（Anti-Affinity），这些约束可能限制了调度器选择节点，从而导致无法调度 Pod。</p><ul><li><strong>解决方法</strong>：查看 Pod 的调度策略和节点资源，确保集群资源充足，并调整节点选择规则。</li></ul></li></ul><h3 id=3-taints-和-tolerations>3. <strong>Taints 和 Tolerations</strong>
<a class=anchor href=#3-taints-%e5%92%8c-tolerations>#</a></h3><ul><li><p>节点上有 Taints，Pod 没有相应的 Tolerations</p><p>：如果某个节点有 Taint（污点），而 Pod 没有相应的 Toleration（容忍），则调度器会拒绝将 Pod 调度到该节点。</p><ul><li><strong>解决方法</strong>：检查节点是否有 Taints，并确保 Pod 配置了合适的 Tolerations。</li></ul></li></ul><h3 id=4-调度器无法找到适合的节点>4. <strong>调度器无法找到适合的节点</strong>
<a class=anchor href=#4-%e8%b0%83%e5%ba%a6%e5%99%a8%e6%97%a0%e6%b3%95%e6%89%be%e5%88%b0%e9%80%82%e5%90%88%e7%9a%84%e8%8a%82%e7%82%b9>#</a></h3><ul><li><p>Pod 亲和性或反亲和性规则</p><p>：Pod 可能具有亲和性（Affinity）或反亲和性（Anti-Affinity）规则，这些规则可能要求将 Pod 调度到特定的节点或者避免将 Pod 调度到某些节点上。如果没有节点满足这些规则，Pod 会处于 Pending 状态。</p><ul><li><strong>解决方法</strong>：检查 Pod 的亲和性和反亲和性规则，确保这些规则在当前集群中有节点可以满足。</li></ul></li></ul><h3 id=5-集群容量不足>5. <strong>集群容量不足</strong>
<a class=anchor href=#5-%e9%9b%86%e7%be%a4%e5%ae%b9%e9%87%8f%e4%b8%8d%e8%b6%b3>#</a></h3><ul><li><p>集群容量不足</p><p>：集群中的节点可能因为负载过高或资源不足，导致无法分配新 Pod。</p><ul><li><strong>解决方法</strong>：通过监控工具检查集群资源使用情况，扩展集群或释放部分资源。</li></ul></li></ul><h3 id=6-persistentvolume-pv-或-persistentvolumeclaim-pvc-问题>6. <strong>PersistentVolume (PV) 或 PersistentVolumeClaim (PVC) 问题</strong>
<a class=anchor href=#6-persistentvolume-pv-%e6%88%96-persistentvolumeclaim-pvc-%e9%97%ae%e9%a2%98>#</a></h3><ul><li><p>PVC 绑定失败</p><p>：如果 Pod 依赖于 PersistentVolume（持久化卷），但 PVC 没有成功绑定到合适的 PV，Pod 也会处于 Pending 状态，直到绑定成功。</p><ul><li><strong>解决方法</strong>：检查 PVC 的状态和 PV 资源，确保 PVC 可以成功绑定到合适的 PV。</li></ul></li></ul><h3 id=7-网络问题>7. <strong>网络问题</strong>
<a class=anchor href=#7-%e7%bd%91%e7%bb%9c%e9%97%ae%e9%a2%98>#</a></h3><ul><li><p>CNI 插件问题</p><p>：如果集群的网络插件（CNI 插件）出现故障或配置问题，Pod 可能无法启动。网络问题会导致 Pod 的网络接口无法配置，造成 Pod 无法成功运行。</p><ul><li><strong>解决方法</strong>：检查 CNI 插件的配置和状态，确保网络正常工作。</li></ul></li></ul><h3 id=8-镜像拉取失败>8. <strong>镜像拉取失败</strong>
<a class=anchor href=#8-%e9%95%9c%e5%83%8f%e6%8b%89%e5%8f%96%e5%a4%b1%e8%b4%a5>#</a></h3><ul><li><p>镜像拉取失败</p><p>：Pod 可能需要从远程仓库拉取容器镜像。如果镜像拉取失败（例如，镜像仓库不可访问、认证失败或镜像不存在），Pod 将无法启动并会保持 Pending 状态。</p><ul><li><strong>解决方法</strong>：检查镜像是否存在，仓库是否可访问，并确保 Pod 配置了正确的镜像拉取凭证（如 Docker Hub 或私有镜像仓库的认证信息）。</li></ul></li></ul><h3 id=9-kubernetes-控制平面问题>9. <strong>Kubernetes 控制平面问题</strong>
<a class=anchor href=#9-kubernetes-%e6%8e%a7%e5%88%b6%e5%b9%b3%e9%9d%a2%e9%97%ae%e9%a2%98>#</a></h3><ul><li><p>控制平面出现故障</p><p>：如果 Kubernetes 的控制平面出现问题，例如 API Server 或调度器不可用，Pod 可能无法正确调度到工作节点。</p><ul><li><strong>解决方法</strong>：检查控制平面的状态，确保 API Server 和调度器正常运行。</li></ul></li></ul><h3 id=如何排查-pending-状态的-pod>如何排查 Pending 状态的 Pod
<a class=anchor href=#%e5%a6%82%e4%bd%95%e6%8e%92%e6%9f%a5-pending-%e7%8a%b6%e6%80%81%e7%9a%84-pod>#</a></h3><ol><li><p><strong>查看 Pod 状态</strong>： 使用以下命令查看 Pod 的详细信息，了解它为什么没有被调度：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl describe pod &lt;pod-name&gt;
</span></span></code></pre></div><p>在输出中查看 <code>Events</code> 部分，通常会列出导致 Pod Pending 的原因。</p></li><li><p><strong>查看节点资源</strong>： 使用以下命令查看集群中的节点资源和使用情况：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl describe node &lt;node-name&gt;
</span></span></code></pre></div></li><li><p><strong>检查资源请求和限制</strong>： 查看 Pod 配置文件，检查资源请求（requests）和限制（limits）的设置，确保它们不会超出节点的可用资源。</p></li></ol><h3 id=总结-4>总结
<a class=anchor href=#%e6%80%bb%e7%bb%93-4>#</a></h3><p>Pod 进入 <strong>Pending</strong> 状态通常是由于以下几个原因：资源不足、调度约束问题、Taints 和 Tolerations 设置、PersistentVolume 的绑定问题、镜像拉取失败等。通过查看 Pod 的描述信息和集群资源，能够帮助诊断问题，并采取相应的措施来解决。</p><h2 id=deployment和statefulset区别>deployment和statefulset区别☆
<a class=anchor href=#deployment%e5%92%8cstatefulset%e5%8c%ba%e5%88%ab>#</a></h2><p><code>Deployment</code> 和 <code>StatefulSet</code> 都是 Kubernetes 中常用的控制器，它们的主要区别在于如何管理和部署 Pod，以及它们对 Pod 的生命周期和存储的处理方式。下面是它们的主要区别：</p><h3 id=1-pod-的标识和命名>1. <strong>Pod 的标识和命名</strong>
<a class=anchor href=#1-pod-%e7%9a%84%e6%a0%87%e8%af%86%e5%92%8c%e5%91%bd%e5%90%8d>#</a></h3><ul><li><p>Deployment</p><p>：</p><ul><li>在 Deployment 中，所有的 Pod 都是无状态的，它们被认为是相同的、可替代的 Pod。每个 Pod 的名字是由 Kubernetes 自动生成的，通常是随机的。</li><li>例如，如果你有 3 个副本，Pod 名称可能是 <code>pod-xxxxxx-abcde</code>，每个 Pod 没有明确的身份。</li></ul></li><li><p>StatefulSet</p><p>：</p><ul><li>StatefulSet 为每个 Pod 提供唯一的标识符。Pod 的名字是基于 StatefulSet 名称和一个索引号，例如：<code>pod-name-0</code>, <code>pod-name-1</code>, <code>pod-name-2</code>。这些 Pod 会按照特定的顺序进行创建、删除和扩缩容。</li><li>Pod 的顺序性是重要的，意味着它们的启动顺序、终止顺序都需要被严格遵循。</li></ul></li></ul><h3 id=2-pod-生命周期>2. <strong>Pod 生命周期</strong>
<a class=anchor href=#2-pod-%e7%94%9f%e5%91%bd%e5%91%a8%e6%9c%9f>#</a></h3><ul><li><p>Deployment</p><p>：</p><ul><li>Deployment 中的 Pod 是 <strong>无状态的</strong>，每个 Pod 是独立的、没有顺序依赖的。如果一个 Pod 被删除，Kubernetes 会创建一个新的 Pod 来替代它，而新的 Pod 可能在其他节点上运行，并且其网络地址和存储都可能不同。</li></ul></li><li><p>StatefulSet</p><p>：</p><ul><li>StatefulSet 中的 Pod 是 <strong>有状态的</strong>，每个 Pod 都有一个固定的标识符，且 Pod 在启动和终止时会按照一定顺序进行。例如，StatefulSet 会按顺序启动 <code>pod-name-0</code>、<code>pod-name-1</code> 等，并在删除时按逆序删除它们（<code>pod-name-2</code>、<code>pod-name-1</code>、<code>pod-name-0</code>）。</li><li>这种顺序性对需要有序启动或终止的应用非常重要。</li></ul></li></ul><h3 id=3-存储>3. <strong>存储</strong>
<a class=anchor href=#3-%e5%ad%98%e5%82%a8>#</a></h3><ul><li><p>Deployment</p><p>：</p><ul><li>Deployment 中的 Pod 通常使用 <strong>临时存储</strong>，即每次 Pod 启动时，都会创建新的存储卷，并且 Pod 的生命周期与存储卷的生命周期是分开的。这意味着每个 Pod 启动时都可能得到不同的存储资源，且这些存储资源不会持久化。</li></ul></li><li><p>StatefulSet</p><p>：</p><ul><li>StatefulSet 提供了对 <strong>持久存储的支持</strong>，每个 Pod 都可以绑定到一个持久的存储卷。即使 Pod 被删除和重建，它们也可以重新挂载到之前的持久存储。这对于数据库、文件系统等需要持久化数据的应用非常重要。</li><li>StatefulSet 使用 <code>PersistentVolumeClaim</code> 来为每个 Pod 动态分配持久存储。</li></ul></li></ul><h3 id=4-扩缩容>4. <strong>扩缩容</strong>
<a class=anchor href=#4-%e6%89%a9%e7%bc%a9%e5%ae%b9>#</a></h3><ul><li><p>Deployment</p><p>：</p><ul><li>Deployment 可以方便地进行横向扩展和缩容，所有的 Pod 都是相同的，没有顺序依赖。</li><li>Pod 的扩容和缩容是同时发生的，不考虑 Pod 的启动顺序。</li></ul></li><li><p>StatefulSet</p><p>：</p><ul><li>StatefulSet 也支持扩容，但每次增加一个 Pod 都会按顺序进行。新的 Pod 会根据原有 Pod 的索引顺序进行创建和分配资源。</li><li>在缩容时，Pod 会按相反顺序进行删除，确保有状态的应用正确地停止。</li></ul></li></ul><h3 id=5-网络>5. <strong>网络</strong>
<a class=anchor href=#5-%e7%bd%91%e7%bb%9c>#</a></h3><ul><li><p>Deployment</p><p>：</p><ul><li>在 Deployment 中，Pod 的网络是无状态的，每个 Pod 都可以通过 Kubernetes Service 与其他 Pod 进行通信，但这些 Pod 的 IP 地址是临时的、随机分配的。</li><li>这意味着 Pod 的 IP 地址可能会随着 Pod 的重启而变化，因此不能依赖 IP 地址来识别 Pod。</li></ul></li><li><p>StatefulSet</p><p>：</p><ul><li>在 StatefulSet 中，每个 Pod 都有一个唯一且固定的网络标识符。例如，<code>pod-name-0</code> 会有一个稳定的 DNS 名称 <code>pod-name-0.&lt;statefulset-name>.&lt;namespace>.svc.cluster.local</code>，即使 Pod 被重新调度或重启，它的 DNS 名称和 IP 地址依然是固定的。</li></ul></li></ul><h3 id=6-适用场景>6. <strong>适用场景</strong>
<a class=anchor href=#6-%e9%80%82%e7%94%a8%e5%9c%ba%e6%99%af>#</a></h3><ul><li><p>Deployment</p><p>：</p><ul><li>适用于无状态应用，比如 Web 服务器、API 服务等。这些应用可以轻松地被替换，Pod 之间没有数据共享的需求。</li></ul></li><li><p>StatefulSet</p><p>：</p><ul><li>适用于有状态应用，如数据库、分布式文件系统、缓存系统等。这些应用需要确保数据的持久性、Pod 的顺序性以及唯一性。</li></ul></li></ul><h3 id=7-滚动更新>7. <strong>滚动更新</strong>
<a class=anchor href=#7-%e6%bb%9a%e5%8a%a8%e6%9b%b4%e6%96%b0>#</a></h3><ul><li><p>Deployment</p><p>：</p><ul><li>Deployment 支持 <strong>滚动更新</strong>，即通过逐步更新 Pod 的方式，确保应用不中断。每次更新一个 Pod，直到所有 Pod 都被更新为新版本。</li></ul></li><li><p>StatefulSet</p><p>：</p><ul><li>StatefulSet 也支持滚动更新，但它会按顺序更新 Pod，确保每个 Pod 在更新过程中都能按顺序处理。这对于有状态应用的有序更新非常重要。</li></ul></li></ul><h3 id=总结对比>总结对比
<a class=anchor href=#%e6%80%bb%e7%bb%93%e5%af%b9%e6%af%94>#</a></h3><table><thead><tr><th>特性</th><th>Deployment</th><th>StatefulSet</th></tr></thead><tbody><tr><td><strong>Pod 名称</strong></td><td>随机生成的名称</td><td>固定的名称，带有索引号</td></tr><tr><td><strong>Pod 顺序</strong></td><td>无顺序依赖</td><td>有顺序依赖，创建和删除时有顺序性</td></tr><tr><td><strong>存储</strong></td><td>临时存储，Pod 之间的存储不共享</td><td>每个 Pod 都有自己的持久化存储</td></tr><tr><td><strong>网络</strong></td><td>Pod 的 IP 是临时的、动态分配的</td><td>每个 Pod 有稳定的 DNS 名称和 IP 地址</td></tr><tr><td><strong>扩缩容</strong></td><td>Pod 可以同时扩容和缩容，无顺序依赖</td><td>扩容时按顺序增加，缩容时按顺序删除</td></tr><tr><td><strong>适用场景</strong></td><td>无状态应用，如 Web 服务、API 服务等</td><td>有状态应用，如数据库、分布式存储系统等</td></tr><tr><td><strong>滚动更新</strong></td><td>支持滚动更新，逐个更新 Pod</td><td>支持滚动更新，按顺序更新 Pod</td></tr></tbody></table><h3 id=适用场景示例>适用场景示例
<a class=anchor href=#%e9%80%82%e7%94%a8%e5%9c%ba%e6%99%af%e7%a4%ba%e4%be%8b>#</a></h3><ul><li><strong>Deployment</strong>：Web 服务、API 网关、前端服务等，这些服务不依赖于特定的状态或数据，可以轻松替换。</li><li><strong>StatefulSet</strong>：数据库（如 MySQL、PostgreSQL、MongoDB）、分布式存储系统（如 Cassandra、Elasticsearch）、分布式缓存（如 Redis）等，这些应用需要持久化数据、稳定的网络标识符和有序的扩缩容。</li></ul><p>通过理解这些区别，你可以根据应用的需求选择合适的控制器进行部署和管理。</p><h2 id=kube-proxy有什么作用>kube-proxy有什么作用☆
<a class=anchor href=#kube-proxy%e6%9c%89%e4%bb%80%e4%b9%88%e4%bd%9c%e7%94%a8>#</a></h2><p><code>kube-proxy</code> 是 Kubernetes 集群中的一个重要组件，负责实现 <strong>Pod 网络通信</strong> 和 <strong>Service 负载均衡</strong>。它是 Kubernetes 中的 <strong>网络代理</strong>，运行在每个节点上，确保 Pod 和 Service 之间的流量能够正确路由和转发。</p><h3 id=kube-proxy-的主要作用><code>kube-proxy</code> 的主要作用
<a class=anchor href=#kube-proxy-%e7%9a%84%e4%b8%bb%e8%a6%81%e4%bd%9c%e7%94%a8>#</a></h3><ol><li><strong>Service 负载均衡</strong>
<code>kube-proxy</code> 负责根据 Kubernetes 中定义的 Service，提供负载均衡功能。它将来自集群外部或其他 Pod 的流量路由到 Service 的后端 Pod 上。<ul><li><strong>ClusterIP Service</strong>：<code>kube-proxy</code> 通过 iptables 或 IPVS（取决于配置）将流量从 Service 的虚拟 IP（VIP）转发到实际的后端 Pod。</li><li><strong>NodePort Service</strong>：<code>kube-proxy</code> 在每个节点上开放一个固定端口，当外部流量访问该端口时，<code>kube-proxy</code> 会将流量转发到对应的 Pod。</li><li><strong>LoadBalancer Service</strong>：在云环境中，<code>kube-proxy</code> 会配合云提供商的负载均衡器，将外部流量通过负载均衡器转发到集群中的 Pod。</li></ul></li><li><strong>路由与转发流量</strong>
<code>kube-proxy</code> 负责在每个节点上管理网络路由规则。它监听 Kubernetes API Server 上关于 Service 的变化，并动态更新节点上的路由规则，确保流量能够正确地转发到对应的 Pod。</li><li><strong>Service 的虚拟 IP (VIP) 转发</strong>
每个 Service 在 Kubernetes 中都有一个虚拟 IP（ClusterIP），用于在集群内部提供访问。<code>kube-proxy</code> 会将流量从这个虚拟 IP 路由到对应的 Pod 集合，实现集群内的负载均衡。</li><li><strong>负载均衡算法</strong>
<code>kube-proxy</code> 通过不同的负载均衡算法来决定如何分发流量到后端的 Pod。常见的负载均衡策略包括：<ul><li><strong>轮询</strong>：流量按顺序均匀分配到各个 Pod。</li><li><strong>最少连接数</strong>：将流量转发到当前连接数最少的 Pod（当使用 IPVS 时可用）。</li><li><strong>源地址哈希</strong>：根据客户端的 IP 地址将请求固定路由到同一个 Pod。</li></ul></li></ol><h3 id=kube-proxy-的工作原理><code>kube-proxy</code> 的工作原理
<a class=anchor href=#kube-proxy-%e7%9a%84%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86>#</a></h3><p><code>kube-proxy</code> 的工作原理主要取决于两种模式：<strong>iptables 模式</strong> 和 <strong>IPVS 模式</strong>。</p><h4 id=1-iptables-模式>1. <strong>iptables 模式</strong>
<a class=anchor href=#1-iptables-%e6%a8%a1%e5%bc%8f>#</a></h4><p>在 <code>iptables</code> 模式下，<code>kube-proxy</code> 使用 Linux 内核的 <code>iptables</code> 工具来配置流量转发规则。具体过程如下：</p><ul><li><code>kube-proxy</code> 会为每个 Service 创建一个对应的 <code>iptables</code> 规则，将流量从 Service 的 ClusterIP 转发到对应的 Pod。</li><li>这些规则在节点的防火墙中生效，确保流量能够被正确地路由到后端 Pod。</li><li><code>kube-proxy</code> 会定期更新这些规则，以适应 Service 和 Pod 的变化。</li></ul><p>优点：</p><ul><li><code>iptables</code> 模式简单且不依赖额外的工具，适合小规模集群。</li></ul><p>缺点：</p><ul><li>随着集群规模增大，<code>iptables</code> 规则可能会变得复杂，性能也可能成为瓶颈。</li></ul><h4 id=2-ipvs-模式>2. <strong>IPVS 模式</strong>
<a class=anchor href=#2-ipvs-%e6%a8%a1%e5%bc%8f>#</a></h4><p><code>IPVS</code>（IP Virtual Server）是 Linux 内核的一个高级负载均衡功能，可以提供比 <code>iptables</code> 更高效的流量路由能力。<code>kube-proxy</code> 在 IPVS 模式下将流量转发到后端 Pod。</p><ul><li><code>kube-proxy</code> 会通过 IPVS 配置负载均衡规则，提供高效的流量转发。</li><li>IPVS 模式支持多种负载均衡算法，如轮询、最少连接数、基于源 IP 的哈希等。</li></ul><p>优点：</p><ul><li>性能比 <code>iptables</code> 模式更好，适合大型集群和高流量环境。</li><li>支持更多的负载均衡策略，适用于更复杂的流量调度需求。</li></ul><p>缺点：</p><ul><li>需要在集群节点上启用 IPVS，且配置较为复杂。</li></ul><h3 id=kube-proxy-的工作流程><code>kube-proxy</code> 的工作流程
<a class=anchor href=#kube-proxy-%e7%9a%84%e5%b7%a5%e4%bd%9c%e6%b5%81%e7%a8%8b>#</a></h3><ol><li><strong>监听 API Server</strong>
<code>kube-proxy</code> 持续监听 Kubernetes API Server，关注 Service 和 Endpoints 的变化。每当 Service 或 Pod 状态发生变化时，<code>kube-proxy</code> 会更新对应的路由规则。</li><li><strong>更新路由规则</strong>
根据 Kubernetes 中的 Service 配置，<code>kube-proxy</code> 会为每个 Service 创建合适的流量转发规则（无论是 <code>iptables</code> 规则还是 IPVS 规则），并在每个节点上生效。</li><li><strong>转发流量</strong>
当集群中的某个 Pod 发送流量到 Service 时，<code>kube-proxy</code> 会根据路由规则将流量转发到对应的 Pod。如果使用 <code>NodePort</code> 或 <code>LoadBalancer</code>，外部请求也会被转发到正确的 Pod。</li><li><strong>健康检查和负载均衡</strong>
<code>kube-proxy</code> 会定期检查 Pod 的健康状态，确保将流量仅转发给健康的 Pod。它通过监听 Endpoints 更新信息，确保服务的 Pod 集合是最新的。</li></ol><h3 id=kube-proxy-的使用场景><code>kube-proxy</code> 的使用场景
<a class=anchor href=#kube-proxy-%e7%9a%84%e4%bd%bf%e7%94%a8%e5%9c%ba%e6%99%af>#</a></h3><ul><li><strong>集群内服务的负载均衡</strong>：无论是 ClusterIP、NodePort 还是 LoadBalancer 服务，<code>kube-proxy</code> 都负责流量转发和负载均衡。</li><li><strong>外部访问 Kubernetes 服务</strong>：通过 NodePort 或 LoadBalancer 类型的 Service，<code>kube-proxy</code> 将外部流量转发到集群中的 Pod。</li><li><strong>内部服务发现与负载均衡</strong>：集群内的 Pod 可以通过 Service 名称进行访问，<code>kube-proxy</code> 会确保流量被正确地转发到对应的 Pod。</li></ul><h3 id=总结-5>总结
<a class=anchor href=#%e6%80%bb%e7%bb%93-5>#</a></h3><p><code>kube-proxy</code> 是 Kubernetes 集群中处理 <strong>网络代理</strong> 和 <strong>负载均衡</strong> 的重要组件，它通过维护 Service 和 Pod 之间的流量转发规则，确保集群内部和外部的网络通信能够顺畅进行。<code>kube-proxy</code> 的核心功能是确保流量从 Service 到 Pod 的正确路由，同时提供负载均衡能力。根据集群的规模和需求，它可以使用 <code>iptables</code> 或 <code>IPVS</code> 模式来实现流量路由。</p><h2 id=kube-proxy怎么修改ipvs规则>kube-proxy怎么修改ipvs规则
<a class=anchor href=#kube-proxy%e6%80%8e%e4%b9%88%e4%bf%ae%e6%94%b9ipvs%e8%a7%84%e5%88%99>#</a></h2><p>在 Kubernetes 中，<code>kube-proxy</code> 使用 IPVS 模式时，流量转发是通过 Linux 内核的 <strong>IP Virtual Server (IPVS)</strong> 来实现的。<code>kube-proxy</code> 会动态地根据 Kubernetes 服务（Service）和 Endpoints 的变化来更新 IPVS 规则。</p><h3 id=修改-ipvs-规则>修改 IPVS 规则
<a class=anchor href=#%e4%bf%ae%e6%94%b9-ipvs-%e8%a7%84%e5%88%99>#</a></h3><p>通常情况下，<code>kube-proxy</code> 会自动管理 IPVS 规则，你可以通过以下方式修改或查看 IPVS 规则：</p><h4 id=1-启用-ipvs-模式>1. <strong>启用 IPVS 模式</strong>
<a class=anchor href=#1-%e5%90%af%e7%94%a8-ipvs-%e6%a8%a1%e5%bc%8f>#</a></h4><p>在 Kubernetes 中启用 IPVS 模式，需要在启动 <code>kube-proxy</code> 时进行配置。你可以通过修改 <code>kube-proxy</code> 的配置文件或命令行参数来启用 IPVS 模式。</p><ul><li><p><strong>修改 <code>kube-proxy</code> 配置文件（通常是 <code>/etc/kubernetes/manifests/kube-proxy.yaml</code>）</strong>，并设置 <code>proxy-mode</code> 为 <code>ipvs</code>。</p><p>示例配置：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>apps/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>DaemonSet</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>kube-proxy</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>kube-system</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>template</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>containers</span>:
</span></span><span style=display:flex><span>        - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>kube-proxy</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>image</span>: <span style=color:#ae81ff>k8s.gcr.io/kube-proxy:v1.23.0</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>command</span>:
</span></span><span style=display:flex><span>            - <span style=color:#ae81ff>/bin/sh</span>
</span></span><span style=display:flex><span>            - -<span style=color:#ae81ff>c</span>
</span></span><span style=display:flex><span>            - |<span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>              # 修改 kube-proxy 启动参数
</span></span></span><span style=display:flex><span><span style=color:#e6db74>              /usr/local/bin/kube-proxy --proxy-mode=ipvs</span>
</span></span></code></pre></div><p>这个配置确保了 <code>kube-proxy</code> 启动时会使用 IPVS 模式。</p></li><li><p>如果你使用的是 <strong>kubeadm</strong> 部署的集群，也可以通过配置文件 <code>/etc/kubernetes/kube-proxy-config.yaml</code> 来设置。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>kubeproxy.config.k8s.io/v1alpha1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>KubeProxyConfiguration</span>
</span></span><span style=display:flex><span><span style=color:#f92672>mode</span>: <span style=color:#e6db74>&#34;ipvs&#34;</span>
</span></span></code></pre></div></li></ul><p>修改完配置后，<code>kube-proxy</code> 会重启并切换到 IPVS 模式。</p><h4 id=2-查看-ipvs-规则>2. <strong>查看 IPVS 规则</strong>
<a class=anchor href=#2-%e6%9f%a5%e7%9c%8b-ipvs-%e8%a7%84%e5%88%99>#</a></h4><p>你可以使用 <code>ipvsadm</code> 工具来查看当前的 IPVS 规则。该工具显示的是当前节点上 IPVS 路由的配置。</p><ul><li><p>在每个节点上，执行以下命令来查看当前的 IPVS 规则：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ipvsadm -L -n
</span></span></code></pre></div><p>这个命令将列出当前的 IPVS 负载均衡规则。输出示例：</p><pre tabindex=0><code>TCP  10.96.0.1:80 rr 1 1 1
-&gt; 10.244.0.2:80  masq
-&gt; 10.244.0.3:80  masq
</code></pre></li><li><p>解释：</p><ul><li><code>10.96.0.1:80</code> 是 Service 的虚拟 IP 和端口。</li><li>后面跟的是实际 Pod 的 IP 地址和端口（如 <code>10.244.0.2:80</code> 和 <code>10.244.0.3:80</code>），它们是该 Service 后端的 Pod。</li><li><code>masq</code> 表示使用源地址伪装。</li></ul></li></ul><h4 id=3-手动修改-ipvs-规则>3. <strong>手动修改 IPVS 规则</strong>
<a class=anchor href=#3-%e6%89%8b%e5%8a%a8%e4%bf%ae%e6%94%b9-ipvs-%e8%a7%84%e5%88%99>#</a></h4><p>如果你需要手动修改 IPVS 规则（例如，调整负载均衡策略或进行故障排查），可以使用 <code>ipvsadm</code> 工具。</p><p><strong>添加一条新的 IPVS 规则</strong>：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ipvsadm -A -t 10.96.0.1:80 -s rr
</span></span><span style=display:flex><span>ipvsadm -a -t 10.96.0.1:80 -r 10.244.0.2:80 -m
</span></span><span style=display:flex><span>ipvsadm -a -t 10.96.0.1:80 -r 10.244.0.3:80 -m
</span></span></code></pre></div><p>这会向 IPVS 中添加一个新的服务，绑定虚拟 IP <code>10.96.0.1:80</code>，并将流量均匀地负载到两个 Pod（<code>10.244.0.2:80</code> 和 <code>10.244.0.3:80</code>）上，采用轮询负载均衡策略（<code>-s rr</code>）。</p><p><strong>删除 IPVS 规则</strong>：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ipvsadm -D -t 10.96.0.1:80
</span></span></code></pre></div><p>这会删除虚拟 IP <code>10.96.0.1:80</code> 的规则。</p><h4 id=4-修改负载均衡策略>4. <strong>修改负载均衡策略</strong>
<a class=anchor href=#4-%e4%bf%ae%e6%94%b9%e8%b4%9f%e8%bd%bd%e5%9d%87%e8%a1%a1%e7%ad%96%e7%95%a5>#</a></h4><p>如果你希望修改负载均衡算法，可以通过 <code>ipvsadm</code> 工具来更改负载均衡策略。</p><ul><li><p>修改为</p><p>最少连接数</p><p>：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ipvsadm -A -t 10.96.0.1:80 -s lc
</span></span></code></pre></div></li><li><p>修改为</p><p>基于源 IP 的哈希</p><p>：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ipvsadm -A -t 10.96.0.1:80 -s sh
</span></span></code></pre></div></li></ul><p><code>-s</code> 参数后跟的负载均衡策略选项：</p><ul><li><code>rr</code>: 轮询 (Round Robin)</li><li><code>lc</code>: 最少连接数 (Least Connections)</li><li><code>sh</code>: 源地址哈希 (Source Hashing)</li></ul><h4 id=5-重新应用-ipvs-规则>5. <strong>重新应用 IPVS 规则</strong>
<a class=anchor href=#5-%e9%87%8d%e6%96%b0%e5%ba%94%e7%94%a8-ipvs-%e8%a7%84%e5%88%99>#</a></h4><p>由于 <code>kube-proxy</code> 会自动更新 IPVS 规则，修改规则后需要确保 <code>kube-proxy</code> 重新加载配置并应用新的规则。</p><ul><li><p>你可以通过重新启动</p><pre tabindex=0><code>kube-proxy
</code></pre><p>来确保它重新加载和应用新的 IPVS 配置：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl -n kube-system rollout restart daemonset kube-proxy
</span></span></code></pre></div></li></ul><h3 id=总结-6>总结
<a class=anchor href=#%e6%80%bb%e7%bb%93-6>#</a></h3><ul><li><code>kube-proxy</code> 使用 IPVS 模式时，负责在集群节点上配置和管理 IPVS 规则来实现负载均衡。</li><li>通过 <code>ipvsadm</code> 工具，可以查看、修改和删除 IPVS 规则。</li><li>如果需要修改 <code>kube-proxy</code> 的行为或负载均衡策略，可以通过修改配置文件来实现。</li><li>一般情况下，不建议手动修改 <code>kube-proxy</code> 的 IPVS 规则，除非你在进行故障排查或高级定制配置。</li></ul><h2 id=ipvs为什么比iptables效率高>ipvs为什么比iptables效率高
<a class=anchor href=#ipvs%e4%b8%ba%e4%bb%80%e4%b9%88%e6%af%94iptables%e6%95%88%e7%8e%87%e9%ab%98>#</a></h2><p>IPVS（IP Virtual Server）相比 iptables 更高效的原因，主要体现在以下几个方面：</p><h3 id=1-专门为负载均衡设计>1. <strong>专门为负载均衡设计</strong>
<a class=anchor href=#1-%e4%b8%93%e9%97%a8%e4%b8%ba%e8%b4%9f%e8%bd%bd%e5%9d%87%e8%a1%a1%e8%ae%be%e8%ae%a1>#</a></h3><ul><li><strong>IPVS</strong> 是一个专门为负载均衡设计的内核模块，旨在提供高效的流量调度和转发。它通过内核级的负载均衡算法和高效的路由机制，可以直接对流量进行负载均衡，减少了用户空间与内核空间的交互。</li><li><strong>iptables</strong> 则是一个更通用的网络防火墙工具，虽然也可以用于流量转发和负载均衡，但它的初衷并非专门优化负载均衡。因此，<code>iptables</code> 在处理复杂的流量转发时，不如 IPVS 高效。</li></ul><h3 id=2-性能优化和内核支持>2. <strong>性能优化和内核支持</strong>
<a class=anchor href=#2-%e6%80%a7%e8%83%bd%e4%bc%98%e5%8c%96%e5%92%8c%e5%86%85%e6%a0%b8%e6%94%af%e6%8c%81>#</a></h3><ul><li><strong>IPVS</strong> 使用了 <strong>内核中的负载均衡模块</strong>，并且支持多种负载均衡算法（如轮询、最少连接、基于源 IP 的哈希等），它能更高效地处理网络流量。</li><li><strong>iptables</strong> 工作在更底层的网络过滤和控制层面，它通过查找匹配规则、设置目标（如 DNAT、SNAT）等操作来决定数据包的处理方式，处理过程相对复杂，需要更多的规则匹配和计算。</li></ul><h3 id=3-规则查找方式>3. <strong>规则查找方式</strong>
<a class=anchor href=#3-%e8%a7%84%e5%88%99%e6%9f%a5%e6%89%be%e6%96%b9%e5%bc%8f>#</a></h3><ul><li><strong>IPVS</strong> 使用 <strong>哈希表</strong> 来存储和查找目标 Pod 或服务的 IP 地址，采用高效的散列算法，这使得它能够快速地做出路由决策。</li><li><strong>iptables</strong> 在处理流量时，通过 <strong>链（chains）和规则（rules）</strong> 进行逐条匹配，尤其是在有大量规则时，性能可能会下降，因为需要依次遍历所有的规则来找到匹配项。这在大规模集群中可能会导致较高的延迟。</li></ul><h3 id=4-连接状态管理>4. <strong>连接状态管理</strong>
<a class=anchor href=#4-%e8%bf%9e%e6%8e%a5%e7%8a%b6%e6%80%81%e7%ae%a1%e7%90%86>#</a></h3><ul><li><strong>IPVS</strong> 会对每个流量连接进行高效的状态管理。它能够维护每个连接的状态，并基于连接的特性（如源 IP、目标端口等）做出高效的负载均衡决策。</li><li><strong>iptables</strong> 也可以进行连接跟踪，但在流量处理时，它的效率不如 IPVS，因为它需要逐个规则检查，而 IPVS 是根据每个连接的具体情况快速决定流向。</li></ul><h3 id=5-负载均衡算法>5. <strong>负载均衡算法</strong>
<a class=anchor href=#5-%e8%b4%9f%e8%bd%bd%e5%9d%87%e8%a1%a1%e7%ae%97%e6%b3%95>#</a></h3><ul><li><strong>IPVS</strong> 支持多种高效的负载均衡算法，并能够实时地根据不同的网络情况动态选择合适的策略。这使得 IPVS 能够根据流量模式进行灵活的负载均衡。</li><li><strong>iptables</strong> 需要用户在规则中显式配置负载均衡逻辑，且支持的负载均衡算法有限，性能也相对较差。</li></ul><h3 id=6-高并发处理能力>6. <strong>高并发处理能力</strong>
<a class=anchor href=#6-%e9%ab%98%e5%b9%b6%e5%8f%91%e5%a4%84%e7%90%86%e8%83%bd%e5%8a%9b>#</a></h3><ul><li><strong>IPVS</strong> 在设计时就考虑到高并发、高负载的情况，它通过 <strong>hashing 和负载均衡池</strong> 来管理流量转发，能够高效地处理成千上万的请求。</li><li><strong>iptables</strong> 的性能在大规模规则和高并发流量下容易下降，尤其是在复杂规则下。</li></ul><h3 id=7-内存使用和效率>7. <strong>内存使用和效率</strong>
<a class=anchor href=#7-%e5%86%85%e5%ad%98%e4%bd%bf%e7%94%a8%e5%92%8c%e6%95%88%e7%8e%87>#</a></h3><ul><li><strong>IPVS</strong> 对于每个 Service 和 Pod 的信息进行高效的存储和访问，内存的使用更加高效。</li><li><strong>iptables</strong> 在每次流量到达时，必须遍历所有的规则链，尤其是在有大量规则的情况下，会造成内存和 CPU 的消耗。</li></ul><h3 id=8-流量转发的直接性>8. <strong>流量转发的直接性</strong>
<a class=anchor href=#8-%e6%b5%81%e9%87%8f%e8%bd%ac%e5%8f%91%e7%9a%84%e7%9b%b4%e6%8e%a5%e6%80%a7>#</a></h3><ul><li><strong>IPVS</strong> 通过内核中直接的流量转发机制，将流量从客户端直接转发到后端 Pod，而无需额外的计算。</li><li><strong>iptables</strong> 在流量转发时，必须依次处理每条规则，通常需要更复杂的处理步骤，导致处理延迟和性能开销。</li></ul><h3 id=总结-7>总结
<a class=anchor href=#%e6%80%bb%e7%bb%93-7>#</a></h3><p>IPVS 比 iptables 高效的主要原因是它作为专门为负载均衡设计的内核模块，在设计上进行了优化，支持高效的流量调度和转发机制，且具有更高效的规则查找方式、连接管理以及负载均衡算法。而 <code>iptables</code> 更多是作为一个通用的网络过滤工具，其设计上并没有专门针对负载均衡进行优化，因此在流量处理和高并发环境下，性能较 IPVS 要低。</p><h2 id=pod之间访问不通怎么排查>pod之间访问不通怎么排查☆
<a class=anchor href=#pod%e4%b9%8b%e9%97%b4%e8%ae%bf%e9%97%ae%e4%b8%8d%e9%80%9a%e6%80%8e%e4%b9%88%e6%8e%92%e6%9f%a5>#</a></h2><p>当 <strong>Pod 之间无法通信</strong> 时，可以通过以下步骤来排查和解决问题。这个过程涉及从网络配置到 Kubernetes 资源的逐步检查，确保流量能够正确地流动。</p><h3 id=1-检查-pod-之间的网络连接>1. <strong>检查 Pod 之间的网络连接</strong>
<a class=anchor href=#1-%e6%a3%80%e6%9f%a5-pod-%e4%b9%8b%e9%97%b4%e7%9a%84%e7%bd%91%e7%bb%9c%e8%bf%9e%e6%8e%a5>#</a></h3><ul><li><p><strong>Ping 测试</strong>：首先，确认 Pod 是否可以相互 ping 通。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl exec -it &lt;pod-name&gt; -- ping &lt;target-pod-ip&gt;
</span></span></code></pre></div><p>如果无法 ping 通，说明可能是网络问题。</p></li><li><p><strong>检查 Pod 的 IP 地址</strong>：使用以下命令查看 Pod 的 IP 地址。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get pod &lt;pod-name&gt; -o wide
</span></span></code></pre></div><p>确保 Pod 的 IP 地址是有效的，并且与其他 Pod 的 IP 地址不冲突。</p></li></ul><h3 id=2-检查网络插件是否正常工作>2. <strong>检查网络插件是否正常工作</strong>
<a class=anchor href=#2-%e6%a3%80%e6%9f%a5%e7%bd%91%e7%bb%9c%e6%8f%92%e4%bb%b6%e6%98%af%e5%90%a6%e6%ad%a3%e5%b8%b8%e5%b7%a5%e4%bd%9c>#</a></h3><p>Kubernetes 中的网络插件（如 Calico、Flannel、Weave 等）负责 Pod 之间的通信。你需要确认网络插件是否在正常工作。</p><ul><li><p>查看网络插件的 Pod 状态：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get pods -n kube-system
</span></span></code></pre></div><p>检查相关网络插件 Pod 的状态，确保它们没有崩溃（CrashLoopBackOff）或者处于 <strong>NotReady</strong> 状态。</p></li><li><p>如果使用 <strong>Calico</strong>，可以查看其日志：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl logs &lt;calico-pod-name&gt; -n kube-system
</span></span></code></pre></div><p>查找是否有网络错误或者警告。</p></li></ul><h3 id=3-检查网络策略network-policies>3. <strong>检查网络策略（Network Policies）</strong>
<a class=anchor href=#3-%e6%a3%80%e6%9f%a5%e7%bd%91%e7%bb%9c%e7%ad%96%e7%95%a5network-policies>#</a></h3><p>如果你在集群中使用了 <strong>Network Policies</strong>，这些策略可能会限制 Pod 之间的流量。</p><ul><li><p>查看是否有正在应用的 Network Policy：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get networkpolicy
</span></span></code></pre></div></li><li><p>如果有 Network Policy，确保它们没有不小心阻止 Pod 之间的流量。可以暂时禁用网络策略，测试是否是它们导致了通信问题。</p></li></ul><h3 id=4-检查>4. <strong>检查 <code>kube-proxy</code> 状态</strong>
<a class=anchor href=#4-%e6%a3%80%e6%9f%a5>#</a></h3><p><code>kube-proxy</code> 负责在集群中配置负载均衡和服务代理，确保网络流量正确转发。如果 <code>kube-proxy</code> 出现问题，可能导致 Pod 之间无法通信。</p><ul><li><p>查看 <code>kube-proxy</code> Pod 状态：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get pods -n kube-system -l k8s-app<span style=color:#f92672>=</span>kube-proxy
</span></span></code></pre></div></li><li><p>检查 <code>kube-proxy</code> 是否正常运行，特别是是否报错。</p></li><li><p>如果 <code>kube-proxy</code> 的日志中有问题，可以通过重启 <code>kube-proxy</code> 进行排查：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl -n kube-system rollout restart daemonset kube-proxy
</span></span></code></pre></div></li></ul><h3 id=5-检查节点网络和防火墙设置>5. <strong>检查节点网络和防火墙设置</strong>
<a class=anchor href=#5-%e6%a3%80%e6%9f%a5%e8%8a%82%e7%82%b9%e7%bd%91%e7%bb%9c%e5%92%8c%e9%98%b2%e7%81%ab%e5%a2%99%e8%ae%be%e7%bd%ae>#</a></h3><p>如果 Pod 跨节点通信出现问题，可能是节点之间的网络连接或防火墙设置导致的。</p><ul><li><p>确认节点之间的 <strong>网络连接</strong> 是否正常（比如使用 <code>ping</code> 或 <code>telnet</code> 测试端口）。</p></li><li><p>检查节点的 <strong>防火墙设置</strong>（例如，iptables 规则）是否限制了 Pod 之间的通信。特别是确保没有防火墙阻止跨节点的流量。</p><p>你可以检查每个节点的 iptables 设置，查看是否有任何规则阻止流量：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>iptables -L -n
</span></span></code></pre></div></li></ul><h3 id=6-查看-dns-配置>6. <strong>查看 DNS 配置</strong>
<a class=anchor href=#6-%e6%9f%a5%e7%9c%8b-dns-%e9%85%8d%e7%bd%ae>#</a></h3><p>有时 Pod 之间的访问失败可能是由于 DNS 配置不正确，导致 Pod 无法解析其他 Pod 的主机名。</p><ul><li><p>查看 DNS 服务（如 <code>CoreDNS</code>）的 Pod 状态：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get pods -n kube-system -l k8s-app<span style=color:#f92672>=</span>coredns
</span></span></code></pre></div></li><li><p>查看 CoreDNS 的日志，检查是否存在解析问题：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl logs &lt;coredns-pod-name&gt; -n kube-system
</span></span></code></pre></div></li><li><p>如果是基于 DNS 名称访问出现问题，尝试使用 Pod 的 IP 地址直接访问目标 Pod，确认是否是 DNS 问题。</p></li></ul><h3 id=7-查看-service-配置>7. <strong>查看 Service 配置</strong>
<a class=anchor href=#7-%e6%9f%a5%e7%9c%8b-service-%e9%85%8d%e7%bd%ae>#</a></h3><p>如果是通过 Service 访问 Pod，但发现通信异常，可能是 Service 配置错误导致的。</p><ul><li><p>查看 Service 配置：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get svc &lt;service-name&gt; -o yaml
</span></span></code></pre></div></li><li><p>检查 <strong>ClusterIP</strong>、<strong>端口</strong>、<strong>selector</strong> 是否正确，确保 Service 的 selector 能匹配到目标 Pod。</p></li></ul><h3 id=8-pod-重启和调试>8. <strong>Pod 重启和调试</strong>
<a class=anchor href=#8-pod-%e9%87%8d%e5%90%af%e5%92%8c%e8%b0%83%e8%af%95>#</a></h3><p>如果通过上述方法仍然无法定位问题，尝试重启相关的 Pod 或服务，看看是否能解决问题。你可以尝试删除 Pod，让 Kubernetes 自动重建：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl delete pod &lt;pod-name&gt;
</span></span></code></pre></div><p>或者重启节点上的网络插件、kube-proxy，看看是否能够恢复通信。</p><h3 id=9-集群的网络诊断工具>9. <strong>集群的网络诊断工具</strong>
<a class=anchor href=#9-%e9%9b%86%e7%be%a4%e7%9a%84%e7%bd%91%e7%bb%9c%e8%af%8a%e6%96%ad%e5%b7%a5%e5%85%b7>#</a></h3><p>可以使用 Kubernetes 集群中的一些诊断工具来帮助你定位问题。例如：</p><ul><li><strong>Calico</strong> 提供的诊断工具（如果使用 Calico 作为网络插件）。</li><li><strong>Weave Net</strong> 提供的 <code>weave status</code> 命令。</li><li><strong>Flannel</strong> 提供的 <code>flannel</code> 命令，查看网络状态和日志。</li></ul><h3 id=总结-8>总结
<a class=anchor href=#%e6%80%bb%e7%bb%93-8>#</a></h3><p>排查 Pod 之间无法通信的原因需要从以下几个方面入手：</p><ol><li><strong>确认 Pod 网络配置正常</strong>。</li><li><strong>检查网络插件的运行状态</strong>。</li><li><strong>检查是否有 Network Policy 阻止通信</strong>。</li><li><strong>检查 kube-proxy 状态</strong>。</li><li><strong>检查节点之间的网络连接和防火墙</strong>。</li><li><strong>检查 DNS 配置</strong>。</li><li><strong>查看 Service 配置</strong>。</li><li><strong>使用诊断工具</strong>进行更深入的排查。</li></ol><p>通常，Pod 之间无法通信的常见原因包括网络插件故障、Network Policies 配置错误、节点防火墙设置不当，或者 kube-proxy 的配置问题。</p><h2 id=k8s中network-policy的实现原理>k8s中Network Policy的实现原理
<a class=anchor href=#k8s%e4%b8%adnetwork-policy%e7%9a%84%e5%ae%9e%e7%8e%b0%e5%8e%9f%e7%90%86>#</a></h2><p>在 Kubernetes 中，<strong>Network Policy</strong> 是一种用于控制 Pod 之间网络流量的机制。它允许你定义哪些流量可以进入或离开某个 Pod，从而在网络层面上进行细粒度的流量控制。Kubernetes 本身并不直接实现 Network Policy，而是依赖于网络插件（如 <strong>Calico</strong>、<strong>Cilium</strong>、<strong>Weave</strong> 等）来实施这些策略。</p><h3 id=network-policy-的基本原理>Network Policy 的基本原理
<a class=anchor href=#network-policy-%e7%9a%84%e5%9f%ba%e6%9c%ac%e5%8e%9f%e7%90%86>#</a></h3><ol><li><strong>网络插件支持</strong>：<ul><li>Kubernetes 的网络策略由网络插件实现。不同的插件有不同的实现方式，但所有插件都遵循 Kubernetes 提供的 <strong>NetworkPolicy API</strong> 规范。</li><li>例如，Calico、Cilium 和 Weave 都支持 Network Policy，并在其内部实现了策略的拦截和执行。</li></ul></li><li><strong>选择器（Selector）</strong>：<ul><li><strong>Pod Selector</strong>：Network Policy 使用标签选择器（Label Selector）来选择应用此策略的 Pod。通常使用 <code>matchLabels</code> 或 <code>matchExpressions</code> 来定义目标 Pod。</li><li><strong>Namespace Selector</strong>：除了 Pod 选择器，Network Policy 还可以选择基于命名空间的流量。例如，可以指定来自特定命名空间的流量。</li><li>通过这些选择器，Network Policy 确定哪些 Pod 允许或拒绝流量。</li></ul></li><li><strong>入站（Ingress）和出站（Egress）规则</strong>：<ul><li><strong>Ingress</strong>：指定哪些流量可以进入匹配的 Pod。Ingress 规则定义了允许从哪些源 IP 地址或 Pod 进入 Pod。</li><li><strong>Egress</strong>：指定哪些流量可以从匹配的 Pod 出去。Egress 规则定义了允许 Pod 向哪些目的地址或 Pod 发送流量。</li><li>这些规则允许对流量进行细粒度控制，可以基于源地址、目标地址、端口等进行限制。</li></ul></li><li><strong>允许和拒绝规则</strong>：<ul><li>默认情况下，所有 Pod 都能与集群中的其他 Pod 通信，但如果定义了 Network Policy，它会显式地控制哪些流量可以进出 Pod。</li><li><strong>允许</strong>：可以通过 Network Policy 允许某些流量。</li><li><strong>拒绝</strong>：Network Policy 允许通过选择器指定的规则拒绝流量。如果没有匹配到规则的流量，将被拒绝。</li></ul></li></ol><h3 id=network-policy-的工作流程>Network Policy 的工作流程
<a class=anchor href=#network-policy-%e7%9a%84%e5%b7%a5%e4%bd%9c%e6%b5%81%e7%a8%8b>#</a></h3><ol><li><p><strong>定义 Network Policy</strong>：</p><ul><li>Network Policy 通常通过 YAML 配置文件来定义。它包括 Ingress 和 Egress 规则，以及匹配标签选择器（selectors）来选择 Pod。</li></ul><p>示例 Network Policy YAML：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>networking.k8s.io/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>NetworkPolicy</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>allow-nginx</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>default</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>podSelector</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>matchLabels</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>app</span>: <span style=color:#ae81ff>nginx</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>ingress</span>:
</span></span><span style=display:flex><span>  - <span style=color:#f92672>from</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>podSelector</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>matchLabels</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>app</span>: <span style=color:#ae81ff>frontend</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>egress</span>:
</span></span><span style=display:flex><span>  - <span style=color:#f92672>to</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>podSelector</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>matchLabels</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>app</span>: <span style=color:#ae81ff>backend</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>policyTypes</span>:
</span></span><span style=display:flex><span>  - <span style=color:#ae81ff>Ingress</span>
</span></span><span style=display:flex><span>  - <span style=color:#ae81ff>Egress</span>
</span></span></code></pre></div><p>这个示例表示：</p><ul><li>只有带有标签 <code>app: nginx</code> 的 Pod 可以接受来自标签为 <code>app: frontend</code> 的 Pod 的流量。</li><li>这些 Pod 也可以向标签为 <code>app: backend</code> 的 Pod 发送流量。</li></ul></li><li><p><strong>插件拦截和实施</strong>：</p><ul><li><p>网络插件通过在节点上配置</p><p>iptables</p><p>或</p><p>eBPF</p><p>规则来实现 Network Policy 的功能。</p><ul><li><strong>iptables</strong>：某些插件（如 Calico）使用 <code>iptables</code> 来拦截和过滤网络流量，根据 Network Policy 定义的规则进行控制。</li><li><strong>eBPF</strong>：现代的网络插件（如 Cilium）可能会使用 <strong>eBPF</strong>（扩展的 BPF，Berkley Packet Filter）技术，以更高效的方式处理流量。</li></ul></li></ul></li><li><p><strong>策略传播和执行</strong>：</p><ul><li>当网络策略（Network Policy）被创建或更新时，网络插件会立即重新计算流量规则并将其应用到相应的网络接口或路由表上。</li><li>网络插件会根据规则在 <strong>网络层</strong> 对 Pod 的流量进行控制，允许符合条件的流量进入或离开 Pod，同时阻止不符合条件的流量。</li></ul></li><li><p><strong>默认行为</strong>：</p><ul><li>如果集群没有任何 Network Policy，则所有 Pod 默认是 <strong>无约束的</strong>，即 Pod 之间的通信是完全开放的。</li><li>一旦应用了任何 Network Policy，默认的行为变为 <strong>拒绝所有流量</strong>，除非显式地允许流量。这意味着如果没有规则指定某些流量，可以访问某个 Pod，则该流量将被拒绝。</li></ul></li></ol><h3 id=例子如何使用-network-policy>例子：如何使用 Network Policy
<a class=anchor href=#%e4%be%8b%e5%ad%90%e5%a6%82%e4%bd%95%e4%bd%bf%e7%94%a8-network-policy>#</a></h3><p>假设我们有以下场景：</p><ol><li><strong>Pod A</strong>：标签 <code>app=frontend</code></li><li><strong>Pod B</strong>：标签 <code>app=backend</code></li></ol><p>我们希望 <strong>frontend</strong> Pod 可以与 <strong>backend</strong> Pod 通信，但 <strong>backend</strong> Pod 不允许与 <strong>frontend</strong> 以外的 Pod 通信。</p><h4 id=定义-network-policy>定义 Network Policy：
<a class=anchor href=#%e5%ae%9a%e4%b9%89-network-policy>#</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>networking.k8s.io/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>NetworkPolicy</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>allow-frontend-to-backend</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>default</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>podSelector</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>matchLabels</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>app</span>: <span style=color:#ae81ff>backend</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>ingress</span>:
</span></span><span style=display:flex><span>  - <span style=color:#f92672>from</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>podSelector</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>matchLabels</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>app</span>: <span style=color:#ae81ff>frontend</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>policyTypes</span>:
</span></span><span style=display:flex><span>  - <span style=color:#ae81ff>Ingress</span>
</span></span></code></pre></div><p>这个策略做了以下事情：</p><ul><li>允许标签为 <code>app: frontend</code> 的 Pod 向标签为 <code>app: backend</code> 的 Pod 发送流量。</li><li>除了 <code>frontend</code> Pod，其他 Pod 都无法访问 <code>backend</code> Pod。</li></ul><h3 id=网络策略的关键点总结>网络策略的关键点总结
<a class=anchor href=#%e7%bd%91%e7%bb%9c%e7%ad%96%e7%95%a5%e7%9a%84%e5%85%b3%e9%94%ae%e7%82%b9%e6%80%bb%e7%bb%93>#</a></h3><ol><li><strong>默认拒绝所有流量</strong>：没有 Network Policy 时，Pod 之间默认是开放的；有 Network Policy 后，默认拒绝所有流量，除非显式允许。</li><li><strong>支持细粒度控制</strong>：可以基于 Pod 标签、命名空间标签、IP 地址等来定义流量控制规则。</li><li><strong>网络插件依赖</strong>：Network Policy 的实际执行是依赖于 Kubernetes 集群中使用的网络插件，插件负责将策略转化为底层的网络规则（如 <code>iptables</code> 或 <code>eBPF</code>）。</li><li><strong>策略类型</strong>：可以定义 <code>Ingress</code> 和 <code>Egress</code> 策略来分别控制入站和出站流量。</li></ol><h3 id=结论>结论
<a class=anchor href=#%e7%bb%93%e8%ae%ba>#</a></h3><p>Kubernetes 中的 Network Policy 提供了灵活、细粒度的网络流量控制，能够在集群内部有效地隔离不同 Pod 和服务的通信。它通过网络插件将高层定义的策略转化为底层的网络配置，从而实现流量控制。</p><h2 id=探针有哪些探测方法有哪些>探针有哪些？探测方法有哪些？
<a class=anchor href=#%e6%8e%a2%e9%92%88%e6%9c%89%e5%93%aa%e4%ba%9b%e6%8e%a2%e6%b5%8b%e6%96%b9%e6%b3%95%e6%9c%89%e5%93%aa%e4%ba%9b>#</a></h2><p>在 Kubernetes 中，<strong>探针（Probe）</strong> 是一种用于检测和管理容器健康状况的机制。探针能够定期检查容器的健康状态，如果容器的健康状态出现异常，Kubernetes 会根据设置的策略进行处理（如重启容器）。探针主要有三种类型：</p><h3 id=1-liveness-probe存活探针>1. <strong>Liveness Probe</strong>（存活探针）
<a class=anchor href=#1-liveness-probe%e5%ad%98%e6%b4%bb%e6%8e%a2%e9%92%88>#</a></h3><ul><li><strong>作用</strong>：检测容器是否处于存活状态。如果容器死掉或变得不可恢复，Liveness Probe 会触发容器重启（由 kubelet 管理）。如果容器无法恢复到健康状态，Kubernetes 会根据策略重新启动容器。</li><li><strong>使用场景</strong>：用于检测容器是否处于“挂掉”状态，例如应用死锁、无限循环等。</li></ul><h3 id=2-readiness-probe就绪探针>2. <strong>Readiness Probe</strong>（就绪探针）
<a class=anchor href=#2-readiness-probe%e5%b0%b1%e7%bb%aa%e6%8e%a2%e9%92%88>#</a></h3><ul><li><strong>作用</strong>：用于检测容器是否准备好接收流量。如果容器未准备好（如正在启动或正在初始化），则 Kubernetes 不会将流量发送到该容器，直到它通过了就绪探针检查。</li><li><strong>使用场景</strong>：用于检测应用是否已经完全启动并可以接受请求。例如，在数据库连接完成之前，可以设置就绪探针来保证流量不被发送到未完全准备好的容器。</li></ul><h3 id=3-startup-probe启动探针>3. <strong>Startup Probe</strong>（启动探针）
<a class=anchor href=#3-startup-probe%e5%90%af%e5%8a%a8%e6%8e%a2%e9%92%88>#</a></h3><ul><li><strong>作用</strong>：用于检查容器是否已经成功启动。它比 Liveness Probe 更加关注容器的启动过程，尤其是对于启动时间较长的容器。通常，在容器启动过程中，Startup Probe 会持续检查容器的状态，如果启动超时，Kubernetes 会重启容器。</li><li><strong>使用场景</strong>：用于处理启动时间长、初始化步骤复杂的应用，确保容器在启动时有足够的时间完成启动。</li></ul><hr><h3 id=探测方法probe-types>探测方法（Probe Types）
<a class=anchor href=#%e6%8e%a2%e6%b5%8b%e6%96%b9%e6%b3%95probe-types>#</a></h3><p>Kubernetes 中的探针可以通过以下几种方法进行探测：</p><h4 id=1-http-get-probe>1. <strong>HTTP GET Probe</strong>
<a class=anchor href=#1-http-get-probe>#</a></h4><ul><li><p><strong>描述</strong>：通过向容器内部发起 HTTP 请求来探测容器的健康状态。Kubernetes 会检查返回的 HTTP 状态码，以此判断容器是否健康。</p></li><li><p><strong>使用场景</strong>：适用于基于 HTTP 服务的容器，例如 Web 应用、API 服务等。</p></li><li><p><strong>配置示例</strong>：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>livenessProbe</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>httpGet</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>path</span>: <span style=color:#ae81ff>/healthz</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>port</span>: <span style=color:#ae81ff>8080</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>initialDelaySeconds</span>: <span style=color:#ae81ff>3</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>periodSeconds</span>: <span style=color:#ae81ff>5</span>
</span></span></code></pre></div></li></ul><p>这段配置说明 Kubernetes 会向容器的 <code>/healthz</code> 路径发起 HTTP 请求，检查容器是否健康。返回的 HTTP 状态码为 2xx 时，视为健康。</p><h4 id=2-tcp-socket-probe>2. <strong>TCP Socket Probe</strong>
<a class=anchor href=#2-tcp-socket-probe>#</a></h4><ul><li><p><strong>描述</strong>：通过尝试与容器内部的 TCP 端口建立连接来探测容器的健康状态。成功建立连接时认为容器健康。</p></li><li><p><strong>使用场景</strong>：适用于需要 TCP 连接的服务，如数据库、缓存服务等。</p></li><li><p><strong>配置示例</strong>：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>livenessProbe</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>tcpSocket</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>port</span>: <span style=color:#ae81ff>3306</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>initialDelaySeconds</span>: <span style=color:#ae81ff>3</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>periodSeconds</span>: <span style=color:#ae81ff>5</span>
</span></span></code></pre></div></li></ul><p>这段配置表示 Kubernetes 会尝试连接到容器的 TCP 端口 <code>3306</code>，成功建立连接时认为容器健康。</p><h4 id=3-exec-probe>3. <strong>Exec Probe</strong>
<a class=anchor href=#3-exec-probe>#</a></h4><ul><li><p><strong>描述</strong>：通过在容器内执行一个命令来探测容器的健康状态。如果命令执行成功（返回值为 0），则认为容器健康；如果返回值非零，则认为容器不健康。</p></li><li><p><strong>使用场景</strong>：适用于需要特定的命令来判断健康状态的容器。</p></li><li><p><strong>配置示例</strong>：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>livenessProbe</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>exec</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>command</span>:
</span></span><span style=display:flex><span>      - <span style=color:#e6db74>&#34;cat&#34;</span>
</span></span><span style=display:flex><span>      - <span style=color:#e6db74>&#34;/tmp/healthcheck&#34;</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>initialDelaySeconds</span>: <span style=color:#ae81ff>3</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>periodSeconds</span>: <span style=color:#ae81ff>5</span>
</span></span></code></pre></div></li></ul><p>这段配置表示 Kubernetes 会在容器内部执行 <code>cat /tmp/healthcheck</code> 命令，若命令成功执行，视为容器健康；如果命令失败，则认为容器不健康。</p><hr><h3 id=配置探针时的常用参数>配置探针时的常用参数
<a class=anchor href=#%e9%85%8d%e7%bd%ae%e6%8e%a2%e9%92%88%e6%97%b6%e7%9a%84%e5%b8%b8%e7%94%a8%e5%8f%82%e6%95%b0>#</a></h3><ol><li><strong>initialDelaySeconds</strong>：容器启动后首次探测前的延迟时间。它允许容器有时间启动并准备好接收探测。</li><li><strong>periodSeconds</strong>：每次探测之间的时间间隔。默认值是 10 秒。</li><li><strong>timeoutSeconds</strong>：探测请求的超时时间。如果探测超时，会认为容器不健康。默认是 1 秒。</li><li><strong>successThreshold</strong>：连续成功的探测次数。默认为 1。如果设置为大于 1，Kubernetes 会要求探针连续成功多次才会认为容器健康。</li><li><strong>failureThreshold</strong>：连续失败的探测次数。默认为 3。如果设置为大于 1，Kubernetes 会要求探针连续失败多次才会认为容器不可用。</li></ol><hr><h3 id=示例完整的探针配置>示例：完整的探针配置
<a class=anchor href=#%e7%a4%ba%e4%be%8b%e5%ae%8c%e6%95%b4%e7%9a%84%e6%8e%a2%e9%92%88%e9%85%8d%e7%bd%ae>#</a></h3><p>以下是一个包含三种探针的完整示例，展示如何配置 Liveness、Readiness 和 Startup Probe：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Pod</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>mypod</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>containers</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>mycontainer</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>image</span>: <span style=color:#ae81ff>myimage:latest</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>livenessProbe</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>httpGet</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>path</span>: <span style=color:#ae81ff>/healthz</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>port</span>: <span style=color:#ae81ff>8080</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>initialDelaySeconds</span>: <span style=color:#ae81ff>3</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>periodSeconds</span>: <span style=color:#ae81ff>5</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>readinessProbe</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>httpGet</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>path</span>: <span style=color:#ae81ff>/readiness</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>port</span>: <span style=color:#ae81ff>8080</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>initialDelaySeconds</span>: <span style=color:#ae81ff>5</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>periodSeconds</span>: <span style=color:#ae81ff>5</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>startupProbe</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>httpGet</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>path</span>: <span style=color:#ae81ff>/startup</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>port</span>: <span style=color:#ae81ff>8080</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>initialDelaySeconds</span>: <span style=color:#ae81ff>10</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>periodSeconds</span>: <span style=color:#ae81ff>5</span>
</span></span></code></pre></div><h3 id=总结-9>总结
<a class=anchor href=#%e6%80%bb%e7%bb%93-9>#</a></h3><p>Kubernetes 提供了 <strong>Liveness Probe</strong>、<strong>Readiness Probe</strong> 和 <strong>Startup Probe</strong> 三种探针来帮助管理容器的健康状态和流量调度。探针通过不同的方式（HTTP GET、TCP Socket、Exec 命令）来检查容器的健康状况，并采取相应的措施，如重启容器或阻止流量发送到不健康的容器。</p><h2 id=pod健康检查失败可能的原因和排查思路>pod健康检查失败可能的原因和排查思路
<a class=anchor href=#pod%e5%81%a5%e5%ba%b7%e6%a3%80%e6%9f%a5%e5%a4%b1%e8%b4%a5%e5%8f%af%e8%83%bd%e7%9a%84%e5%8e%9f%e5%9b%a0%e5%92%8c%e6%8e%92%e6%9f%a5%e6%80%9d%e8%b7%af>#</a></h2><p>当 Kubernetes 中的 <strong>Pod 健康检查（Health Check）</strong> 失败时，可能会导致容器被重启或者流量被拒绝。健康检查失败的原因可能有很多种，通常与应用、配置、网络、资源等问题相关。以下是一些常见的原因和排查思路。</p><h3 id=常见的-pod-健康检查失败的原因>常见的 Pod 健康检查失败的原因
<a class=anchor href=#%e5%b8%b8%e8%a7%81%e7%9a%84-pod-%e5%81%a5%e5%ba%b7%e6%a3%80%e6%9f%a5%e5%a4%b1%e8%b4%a5%e7%9a%84%e5%8e%9f%e5%9b%a0>#</a></h3><ol><li><strong>应用本身的问题</strong>：<ul><li><strong>应用崩溃或挂掉</strong>：应用进程崩溃或死锁，导致健康检查失败。</li><li><strong>应用启动慢</strong>：如果应用启动时间较长，可能未能在设置的 <code>initialDelaySeconds</code> 内完成启动，导致健康检查失败。</li><li><strong>应用依赖问题</strong>：应用可能依赖于其他服务或资源（如数据库、外部 API 等），如果依赖的服务不可用，应用可能无法启动或运行，导致健康检查失败。</li></ul></li><li><strong>健康检查配置问题</strong>：<ul><li><strong>路径或端口错误</strong>：配置的 HTTP 路径、端口号或 TCP 端口号不正确，导致探针无法访问容器。</li><li><strong>命令执行错误</strong>：如果使用的是 <code>exec</code> 探针，命令返回非零状态码，则会被认为健康检查失败。</li><li><strong>探针超时</strong>：如果探针的超时时间 (<code>timeoutSeconds</code>) 配置过短，探针可能会在容器响应前超时。</li><li><strong>探针的 <code>initialDelaySeconds</code> 配置不足</strong>：容器刚启动时，可能需要一些时间才能启动完成，探针需要给予容器足够的时间来初始化。</li></ul></li><li><strong>资源不足</strong>：<ul><li><strong>CPU 或内存不足</strong>：如果容器的 CPU 或内存资源不足，可能会导致容器变得不响应或死掉，从而导致健康检查失败。</li><li><strong>节点压力</strong>：如果节点负载过高或资源不够，可能会影响容器的性能，导致健康检查失败。</li></ul></li><li><strong>网络问题</strong>：<ul><li><strong>网络延迟或不可达</strong>：如果容器依赖网络资源（如数据库或外部 API），并且网络出现故障或延迟，健康检查可能无法成功。</li><li><strong>防火墙或网络策略问题</strong>：Network Policy 或防火墙规则可能限制了容器之间或容器与外部服务之间的通信。</li></ul></li><li><strong>依赖服务不可用</strong>：<ul><li>如果应用依赖于其他服务（如数据库、消息队列等），这些服务的不可用会导致应用不能正常工作，从而导致健康检查失败。</li></ul></li></ol><hr><h3 id=健康检查失败的排查思路>健康检查失败的排查思路
<a class=anchor href=#%e5%81%a5%e5%ba%b7%e6%a3%80%e6%9f%a5%e5%a4%b1%e8%b4%a5%e7%9a%84%e6%8e%92%e6%9f%a5%e6%80%9d%e8%b7%af>#</a></h3><h4 id=1-检查容器日志>1. <strong>检查容器日志</strong>
<a class=anchor href=#1-%e6%a3%80%e6%9f%a5%e5%ae%b9%e5%99%a8%e6%97%a5%e5%bf%97>#</a></h4><ul><li><p>查看容器的日志以确定是否有应用崩溃、错误或异常。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl logs &lt;pod-name&gt; -c &lt;container-name&gt;
</span></span></code></pre></div></li><li><p>查找启动过程中是否有错误消息或异常，尤其是与依赖服务相关的错误。</p></li></ul><h4 id=2-检查探针配置>2. <strong>检查探针配置</strong>
<a class=anchor href=#2-%e6%a3%80%e6%9f%a5%e6%8e%a2%e9%92%88%e9%85%8d%e7%bd%ae>#</a></h4><ul><li><p>确认</p><pre tabindex=0><code>livenessProbe
</code></pre><p>和</p><pre tabindex=0><code>readinessProbe
</code></pre><p>的配置是否正确，尤其是路径、端口和命令。</p><ul><li>对于 <strong>HTTP</strong> 探针，确保配置的路径和端口是正确的，并且容器内的服务在该路径上提供响应。</li><li>对于 <strong>TCP</strong> 探针，确保指定的端口是容器内实际开放的端口。</li><li>对于 <strong>exec</strong> 探针，确保命令是容器内有效的命令，并且可以正确执行。</li></ul></li><li><p>查看探针配置中的超时时间、初始延迟和检查周期，是否合理配置：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>livenessProbe</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>httpGet</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>path</span>: <span style=color:#ae81ff>/healthz</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>port</span>: <span style=color:#ae81ff>8080</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>initialDelaySeconds</span>: <span style=color:#ae81ff>5</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>timeoutSeconds</span>: <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>periodSeconds</span>: <span style=color:#ae81ff>5</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>failureThreshold</span>: <span style=color:#ae81ff>3</span>
</span></span></code></pre></div></li><li><p>如果 <code>initialDelaySeconds</code> 配置过小，容器还未启动完就进行健康检查，可能导致失败。</p></li></ul><h4 id=3-检查容器资源限制>3. <strong>检查容器资源限制</strong>
<a class=anchor href=#3-%e6%a3%80%e6%9f%a5%e5%ae%b9%e5%99%a8%e8%b5%84%e6%ba%90%e9%99%90%e5%88%b6>#</a></h4><ul><li><p>检查容器的</p><p>CPU</p><p>和</p><p>内存</p><p>配置，是否给容器分配了足够的资源。资源不足可能导致容器响应缓慢或挂掉。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>resources</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>requests</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>memory</span>: <span style=color:#e6db74>&#34;64Mi&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>cpu</span>: <span style=color:#e6db74>&#34;250m&#34;</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>limits</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>memory</span>: <span style=color:#e6db74>&#34;128Mi&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>cpu</span>: <span style=color:#e6db74>&#34;500m&#34;</span>
</span></span></code></pre></div></li><li><p>如果容器使用的资源接近或超过节点的限制，可能会导致容器崩溃或无法及时响应健康检查。</p></li></ul><h4 id=4-检查依赖服务的可用性>4. <strong>检查依赖服务的可用性</strong>
<a class=anchor href=#4-%e6%a3%80%e6%9f%a5%e4%be%9d%e8%b5%96%e6%9c%8d%e5%8a%a1%e7%9a%84%e5%8f%af%e7%94%a8%e6%80%a7>#</a></h4><ul><li><p>如果容器依赖外部服务（如数据库、消息队列等），检查这些服务是否可用。如果是数据库连接问题，可以查看应用日志或数据库日志。</p></li><li><p>通过外部工具（如</p><pre tabindex=0><code>telnet
</code></pre><p>或</p><pre tabindex=0><code>curl
</code></pre><p>）测试容器和外部服务的连接：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>curl &lt;service-ip&gt;:&lt;port&gt;
</span></span></code></pre></div><p>如果有网络连接问题，可能需要检查 Kubernetes 集群的网络配置，特别是</p><p>Network Policy</p><p>或防火墙设置。</p></li></ul><h4 id=5-查看-kubernetes-事件>5. <strong>查看 Kubernetes 事件</strong>
<a class=anchor href=#5-%e6%9f%a5%e7%9c%8b-kubernetes-%e4%ba%8b%e4%bb%b6>#</a></h4><ul><li><p>查看 Pod 的事件信息，了解容器重启和探针失败的详细信息：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl describe pod &lt;pod-name&gt;
</span></span></code></pre></div></li><li><p>查找与健康检查相关的错误信息，例如：</p><ul><li>&ldquo;Liveness probe failed&rdquo; 或 &ldquo;Readiness probe failed&rdquo;</li><li>&ldquo;Back-off restarting failed container&rdquo;（容器重启失败）</li></ul></li></ul><h4 id=6-检查节点的资源使用情况>6. <strong>检查节点的资源使用情况</strong>
<a class=anchor href=#6-%e6%a3%80%e6%9f%a5%e8%8a%82%e7%82%b9%e7%9a%84%e8%b5%84%e6%ba%90%e4%bd%bf%e7%94%a8%e6%83%85%e5%86%b5>#</a></h4><ul><li><p>确认节点的 CPU 和内存是否出现瓶颈，导致容器无法正常运行。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl top nodes
</span></span><span style=display:flex><span>kubectl top pods
</span></span></code></pre></div></li><li><p>如果节点资源不足，可能需要扩展节点或调整资源配额。</p></li></ul><h4 id=7-查看网络插件和策略>7. <strong>查看网络插件和策略</strong>
<a class=anchor href=#7-%e6%9f%a5%e7%9c%8b%e7%bd%91%e7%bb%9c%e6%8f%92%e4%bb%b6%e5%92%8c%e7%ad%96%e7%95%a5>#</a></h4><ul><li><p>检查是否有</p><p>Network Policy</p><p>阻止了 Pod 之间的通信。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get networkpolicy
</span></span></code></pre></div></li><li><p>如果使用了网络插件（如 Calico、Cilium 等），检查插件的状态和日志，确认网络是否正常。</p></li></ul><h4 id=8-验证探针的路径是否有效>8. <strong>验证探针的路径是否有效</strong>
<a class=anchor href=#8-%e9%aa%8c%e8%af%81%e6%8e%a2%e9%92%88%e7%9a%84%e8%b7%af%e5%be%84%e6%98%af%e5%90%a6%e6%9c%89%e6%95%88>#</a></h4><ul><li><p>如果健康检查是基于 HTTP 请求的，确保容器内部的服务可以响应正确的 HTTP 状态码。你可以通过以下命令手动检查服务：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>curl http://localhost:&lt;port&gt;/healthz
</span></span></code></pre></div></li><li><p>检查 HTTP 服务器是否已经启动，并且健康检查路径（如 <code>/healthz</code>）返回的是成功的状态码（如 200 OK）。</p></li></ul><hr><h3 id=总结-10>总结
<a class=anchor href=#%e6%80%bb%e7%bb%93-10>#</a></h3><p>Pod 健康检查失败的原因可能包括：</p><ol><li>应用本身的问题（崩溃、启动慢等）。</li><li>健康检查配置错误（路径、端口、命令不正确，超时设置不合理等）。</li><li>资源不足，导致容器无法正常响应。</li><li>网络问题，导致容器无法访问外部服务或其他 Pod。</li><li>依赖服务不可用，影响应用的启动或运行。</li></ol><p>排查时，应从容器日志、探针配置、资源使用、依赖服务、Kubernetes 事件等多方面入手，逐步定位问题。</p><h2 id=k8s的service是什么>k8s的Service是什么☆
<a class=anchor href=#k8s%e7%9a%84service%e6%98%af%e4%bb%80%e4%b9%88>#</a></h2><p>Kubernetes 中的 <strong>Service</strong> 是一种抽象层，用于定义一组 <strong>Pod</strong> 的访问方式。它是一个负载均衡器，将客户端请求转发到后端的 <strong>Pod</strong> 上，确保即使 Pod 的 IP 地址变化，外部访问也能够稳定地连接到正确的 Pod。Service 还可以提供内部或外部网络访问入口，用于跨 Pod 或外部访问服务。</p><h3 id=service-的主要作用>Service 的主要作用
<a class=anchor href=#service-%e7%9a%84%e4%b8%bb%e8%a6%81%e4%bd%9c%e7%94%a8>#</a></h3><ol><li><strong>负载均衡</strong>：<ul><li>Kubernetes <strong>Service</strong> 会自动将请求负载均衡到其背后的多个 Pod 上。通过将请求轮流分发给后端 Pod，Service 提供了一种高可用的访问方式。</li></ul></li><li><strong>服务发现</strong>：<ul><li>Service 提供一个稳定的 IP 地址和 DNS 名称（如 <code>&lt;service-name>.&lt;namespace>.svc.cluster.local</code>），使得即使 Pod 的 IP 地址变化，客户端仍然能够通过 Service 的 IP 或 DNS 访问该服务。</li></ul></li><li><strong>抽象网络通信</strong>：<ul><li>Service 提供了一个稳定的网络接口，而不用关心 Pod 的具体网络信息。Pod 可能会被动态创建、删除或重启，但客户端始终可以通过 Service 访问服务。</li></ul></li></ol><hr><h3 id=service-的类型>Service 的类型
<a class=anchor href=#service-%e7%9a%84%e7%b1%bb%e5%9e%8b>#</a></h3><p>Kubernetes 提供了多种类型的 Service，以满足不同的需求：</p><ol><li><p><strong>ClusterIP（默认类型）</strong>：</p><ul><li><strong>作用</strong>：为 Service 分配一个集群内的虚拟 IP（VIP），并且只能在集群内部访问该服务。</li><li><strong>使用场景</strong>：适用于集群内部的服务通信。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Service</span>
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-service</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>selector</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>app</span>: <span style=color:#ae81ff>myapp</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>ports</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>protocol</span>: <span style=color:#ae81ff>TCP</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>port</span>: <span style=color:#ae81ff>80</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>targetPort</span>: <span style=color:#ae81ff>8080</span>
</span></span></code></pre></div><ul><li><strong>访问方式</strong>：通过 <code>&lt;service-name>.&lt;namespace>.svc.cluster.local</code> 或 <code>&lt;service-ip></code> 进行访问。</li></ul></li><li><p><strong>NodePort</strong>：</p><ul><li><strong>作用</strong>：将 Service 映射到集群节点的一个特定端口，使得外部可以通过 <code>&lt;node-ip>:&lt;node-port></code> 访问服务。</li><li><strong>使用场景</strong>：适用于需要外部访问集群内服务的场景，通常用于开发、测试环境。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Service</span>
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-service</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>selector</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>app</span>: <span style=color:#ae81ff>myapp</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>ports</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>protocol</span>: <span style=color:#ae81ff>TCP</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>port</span>: <span style=color:#ae81ff>80</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>targetPort</span>: <span style=color:#ae81ff>8080</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>nodePort</span>: <span style=color:#ae81ff>30001</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>type</span>: <span style=color:#ae81ff>NodePort</span>
</span></span></code></pre></div><ul><li><strong>访问方式</strong>：通过 <code>&lt;node-ip>:&lt;node-port></code> 进行访问，外部用户可以通过集群节点的 IP 地址和指定端口访问服务。</li></ul></li><li><p><strong>LoadBalancer</strong>：</p><ul><li><strong>作用</strong>：将 Service 映射到云提供商的负载均衡器（如 AWS、GCP、Azure）上，外部流量可以通过云负载均衡器的 IP 或 DNS 名称访问集群中的服务。</li><li><strong>使用场景</strong>：适用于生产环境，需要通过云负载均衡器提供外部访问。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Service</span>
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-service</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>selector</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>app</span>: <span style=color:#ae81ff>myapp</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>ports</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>protocol</span>: <span style=color:#ae81ff>TCP</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>port</span>: <span style=color:#ae81ff>80</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>targetPort</span>: <span style=color:#ae81ff>8080</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>type</span>: <span style=color:#ae81ff>LoadBalancer</span>
</span></span></code></pre></div><ul><li><strong>访问方式</strong>：通过云负载均衡器的公共 IP 或 DNS 名称进行访问。</li></ul></li><li><p><strong>ExternalName</strong>：</p><ul><li><strong>作用</strong>：将 Service 映射到外部 DNS 名称，而不是集群内的 Pod。这意味着 Kubernetes 会将流量转发到指定的外部服务（通常是外部 DNS 名称）上。</li><li><strong>使用场景</strong>：适用于需要将内部服务与外部服务连接的场景，例如访问外部数据库或 API。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Service</span>
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-service</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>type</span>: <span style=color:#ae81ff>ExternalName</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>externalName</span>: <span style=color:#ae81ff>example.com</span>
</span></span></code></pre></div><ul><li><strong>访问方式</strong>：通过 <code>&lt;service-name>.&lt;namespace>.svc.cluster.local</code> 进行访问，Kubernetes 会将请求转发到外部服务 <code>example.com</code>。</li></ul></li></ol><hr><h3 id=service-的工作原理>Service 的工作原理
<a class=anchor href=#service-%e7%9a%84%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86>#</a></h3><p>Kubernetes 的 <strong>Service</strong> 是通过与 <strong>Endpoints</strong> 配合来工作的。当 Service 与 Pod 进行绑定时，Kubernetes 会自动为该 Service 创建一个 <strong>Endpoint</strong> 对象，记录与该 Service 关联的 Pod 的 IP 地址。</p><ul><li><strong>选择器（Selector）</strong>：Service 使用标签选择器来选择哪些 Pod 作为该 Service 的后端。标签选择器会选择符合指定标签的 Pod。</li><li><strong>端口映射（Port Mapping）</strong>：Service 会将客户端的请求转发到后端 Pod 的指定端口。如果有多个 Pod，Kubernetes 会对请求进行负载均衡。</li></ul><p>当一个请求发往 <strong>ClusterIP</strong> 或 <strong>NodePort</strong> 时，Kubernetes 会使用 <strong>iptables</strong> 或 <strong>IPVS</strong> 进行负载均衡，选择一个健康的 Pod 来响应请求。</p><hr><h3 id=service-的健康检查>Service 的健康检查
<a class=anchor href=#service-%e7%9a%84%e5%81%a5%e5%ba%b7%e6%a3%80%e6%9f%a5>#</a></h3><p>Kubernetes <strong>Service</strong> 本身并不执行健康检查，而是通过 <strong>Pod 的健康检查</strong>（如 Liveness Probe 和 Readiness Probe）来判断后端 Pod 是否健康。当一个 Pod 健康检查失败时，Kubernetes 会从 Service 的 Endpoints 列表中移除该 Pod，从而不再将流量发送到该 Pod。</p><hr><h3 id=访问-service-的方式>访问 Service 的方式
<a class=anchor href=#%e8%ae%bf%e9%97%ae-service-%e7%9a%84%e6%96%b9%e5%bc%8f>#</a></h3><ol><li><strong>内部访问</strong>：<ul><li>Service 的 <strong>ClusterIP</strong> 可以在集群内通过 <code>&lt;service-name>.&lt;namespace>.svc.cluster.local</code> 进行访问。集群中的所有 Pod 都可以通过该域名访问该 Service。</li></ul></li><li><strong>外部访问</strong>：<ul><li>对于 <strong>NodePort</strong> 和 <strong>LoadBalancer</strong> 类型的 Service，外部用户可以通过 <code>&lt;node-ip>:&lt;node-port></code> 或云负载均衡器的 IP 或 DNS 进行访问。</li></ul></li></ol><hr><h3 id=总结-11>总结
<a class=anchor href=#%e6%80%bb%e7%bb%93-11>#</a></h3><ul><li><strong>Service</strong> 是 Kubernetes 中的一种抽象，它提供了稳定的网络接口，帮助 Pod 提供负载均衡和服务发现。</li><li>Kubernetes 支持多种类型的 Service，适用于不同的访问场景（如集群内部访问、外部访问、云负载均衡器等）。</li><li>Service 使用标签选择器来确定要暴露的 Pod，通过 Endpoints 维护 Pod 的 IP 地址。</li><li>通过健康检查机制，Kubernetes 会确保仅向健康的 Pod 发送流量。</li></ul><h2 id=metrics-server采集指标数据链路>metrics-server采集指标数据链路
<a class=anchor href=#metrics-server%e9%87%87%e9%9b%86%e6%8c%87%e6%a0%87%e6%95%b0%e6%8d%ae%e9%93%be%e8%b7%af>#</a></h2><p><code>metrics-server</code> 是 Kubernetes 集群中的一个轻量级、聚合型的监控组件，主要用于收集集群内节点和 Pod 的资源使用情况（如 CPU、内存等）。它提供了这些指标数据，供 <strong>Horizontal Pod Autoscaler (HPA)</strong> 和其他 Kubernetes 组件使用。<code>metrics-server</code> 不持久化数据，它只将资源指标从节点和 Pod 中采集并聚合后提供给 API 服务供其他组件查询。</p><p>以下是 <strong>metrics-server</strong> 采集指标数据的完整链路：</p><hr><h3 id=1-node-端的-kubelet-上报资源指标>1. <strong>Node 端的 Kubelet 上报资源指标</strong>
<a class=anchor href=#1-node-%e7%ab%af%e7%9a%84-kubelet-%e4%b8%8a%e6%8a%a5%e8%b5%84%e6%ba%90%e6%8c%87%e6%a0%87>#</a></h3><ul><li><p>每个 Node 上的 <strong>Kubelet</strong> 会周期性地收集本节点上所有 Pod 的资源使用情况（如 CPU 和内存）。</p></li><li><p>Kubelet</p><p>会通过</p><pre tabindex=0><code>/metrics/cadvisor
</code></pre><p>和</p><pre tabindex=0><code>/stats/summary
</code></pre><p>等接口提供当前节点和 Pod 的资源使用数据。</p><ul><li><code>/metrics/cadvisor</code>：提供 cAdvisor 中收集的容器级别的资源指标。</li><li><code>/stats/summary</code>：提供节点级别和容器级别的资源使用数据。</li></ul></li></ul><p>这些资源数据以 JSON 格式提供，Kubelet 定期更新这些指标。</p><hr><h3 id=2-metrics-server-收集数据>2. <strong>Metrics Server 收集数据</strong>
<a class=anchor href=#2-metrics-server-%e6%94%b6%e9%9b%86%e6%95%b0%e6%8d%ae>#</a></h3><ul><li><code>metrics-server</code> 会定期通过 <strong>Kubelet 的 Metrics API</strong> 获取每个 Node 和 Pod 的资源使用数据。</li><li>Metrics Server 并不直接向 Node 发送请求，而是通过 Kubernetes API Server 访问集群中所有 Node 的 Kubelet 端点，收集指标数据。<ul><li><strong>查询方式</strong>：<code>metrics-server</code> 使用 Kubernetes API 访问 <code>/apis/metrics.k8s.io/v1beta1/nodes</code> 和 <code>/apis/metrics.k8s.io/v1beta1/pods</code> 等接口获取节点和 Pod 的资源使用情况。</li></ul></li><li>Metrics Server 采集的数据包括：<ul><li><strong>节点（Node）</strong>：CPU 使用率、内存使用量、磁盘使用量等。</li><li><strong>Pod</strong>：每个容器的 CPU 和内存使用量。</li></ul></li></ul><hr><h3 id=3-数据聚合和处理>3. <strong>数据聚合和处理</strong>
<a class=anchor href=#3-%e6%95%b0%e6%8d%ae%e8%81%9a%e5%90%88%e5%92%8c%e5%a4%84%e7%90%86>#</a></h3><ul><li><code>metrics-server</code> 会将从所有节点收集到的资源数据进行汇总和处理，并将它们作为 <strong>Metrics API</strong> 提供给 Kubernetes API Server。</li><li>Metrics 数据被以 <strong>实时</strong> 的方式提供给集群内其他的组件（如 <strong>Horizontal Pod Autoscaler (HPA)</strong>）来进行自动扩缩容等操作。</li></ul><hr><h3 id=4-通过-kubernetes-api-提供访问>4. <strong>通过 Kubernetes API 提供访问</strong>
<a class=anchor href=#4-%e9%80%9a%e8%bf%87-kubernetes-api-%e6%8f%90%e4%be%9b%e8%ae%bf%e9%97%ae>#</a></h3><ul><li><p>其他组件（如 HPA）可以通过 Kubernetes API 查询和使用这些资源数据。</p><ul><li><p>例如，Horizontal Pod Autoscaler 会通过以下方式查询资源指标数据：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get --raw <span style=color:#e6db74>&#34;/apis/metrics.k8s.io/v1beta1/namespaces/default/pods&#34;</span>
</span></span></code></pre></div></li><li><p>这将返回一个 JSON 响应，其中包含了指定命名空间中所有 Pod 的资源使用情况（CPU、内存等）。</p></li></ul></li></ul><hr><h3 id=5-pod-autoscaling-的触发>5. <strong>Pod Autoscaling 的触发</strong>
<a class=anchor href=#5-pod-autoscaling-%e7%9a%84%e8%a7%a6%e5%8f%91>#</a></h3><ul><li><strong>Horizontal Pod Autoscaler (HPA)</strong> 会定期查询 <strong>metrics-server</strong> 提供的指标数据（如 CPU、内存利用率等）来决定是否扩缩容。</li><li>如果 Pod 的资源使用超出了设置的阈值，HPA 会增加 Pod 的副本数量；反之，则会减少副本数。</li></ul><hr><h3 id=6-数据过期与刷新>6. <strong>数据过期与刷新</strong>
<a class=anchor href=#6-%e6%95%b0%e6%8d%ae%e8%bf%87%e6%9c%9f%e4%b8%8e%e5%88%b7%e6%96%b0>#</a></h3><ul><li><code>metrics-server</code> 会定期重新拉取数据，通常每 60 秒就会刷新一次数据。它不会持久化数据，只会在内存中保持最近的资源使用数据。</li><li>旧的指标数据会被定期清除，不会长期保存，因此 <code>metrics-server</code> 是一个“短期”聚合工具，用于实时监控和扩缩容决策。</li></ul><hr><h3 id=7-数据存储与可视化>7. <strong>数据存储与可视化</strong>
<a class=anchor href=#7-%e6%95%b0%e6%8d%ae%e5%ad%98%e5%82%a8%e4%b8%8e%e5%8f%af%e8%a7%86%e5%8c%96>#</a></h3><ul><li>虽然 <code>metrics-server</code> 负责实时采集并提供指标数据，但它并不负责长期存储这些数据。如果需要长期存储并进行分析，可以考虑使用 <strong>Prometheus</strong> 来收集、存储和可视化资源使用数据。</li><li>Prometheus 和 <code>metrics-server</code> 不同，Prometheus 会定期从 Kubelet 和其他组件拉取数据，并将数据存储在长期数据库中，便于后续的查询和分析。</li></ul><hr><h3 id=采集数据链路总结>采集数据链路总结：
<a class=anchor href=#%e9%87%87%e9%9b%86%e6%95%b0%e6%8d%ae%e9%93%be%e8%b7%af%e6%80%bb%e7%bb%93>#</a></h3><ol><li><strong>Kubelet</strong> 定期通过 <code>/metrics/cadvisor</code> 和 <code>/stats/summary</code> 等接口提供本节点和 Pod 的资源使用情况。</li><li><strong>Metrics Server</strong> 从 Kubernetes API Server 查询各个节点和 Pod 的资源使用数据。</li><li><strong>Metrics Server</strong> 聚合和处理数据后，作为 <strong>Metrics API</strong> 提供给集群内部组件。</li><li><strong>HPA</strong> 等组件查询 <strong>Metrics API</strong> 以决定是否扩缩容。</li><li><strong>Prometheus</strong> 可用于长期存储和可视化资源使用数据。</li></ol><hr><p>通过这个链路，<code>metrics-server</code> 为 Kubernetes 集群提供了实时的资源监控，帮助其他组件做出自动化决策，如自动扩容、缩容等。</p><h2 id=k8s服务发现有哪些方式>k8s服务发现有哪些方式？
<a class=anchor href=#k8s%e6%9c%8d%e5%8a%a1%e5%8f%91%e7%8e%b0%e6%9c%89%e5%93%aa%e4%ba%9b%e6%96%b9%e5%bc%8f>#</a></h2><p>Kubernetes 提供了多种 <strong>服务发现</strong> 机制，用于让应用和服务之间能够彼此找到并进行通信。服务发现的目标是使服务能够在动态变化的环境中，尤其是容器和 Pod 的 IP 地址不断变化时，依然能够稳定地进行访问。Kubernetes 的服务发现方式主要有以下几种：</p><h3 id=1-dns-服务发现>1. <strong>DNS 服务发现</strong>
<a class=anchor href=#1-dns-%e6%9c%8d%e5%8a%a1%e5%8f%91%e7%8e%b0>#</a></h3><p>Kubernetes 内建的 <strong>DNS 服务</strong> 是最常用的服务发现机制，它为集群中的所有服务提供了一个稳定的名称解析系统。</p><ul><li><p><strong>如何工作</strong>：</p><ul><li>每个 Kubernetes 集群通常会运行一个 <strong>CoreDNS</strong> 或 <strong>kube-dns</strong> 服务，负责解析集群内的 DNS 请求。</li><li>每当你创建一个 <strong>Service</strong>，Kubernetes 会为该 Service 分配一个 DNS 名称。默认情况下，服务的 DNS 名称由 <code>&lt;service-name>.&lt;namespace>.svc.cluster.local</code> 构成。</li><li>例如，如果你有一个名为 <code>my-service</code> 的 Service 位于 <code>default</code> 命名空间中，那么该服务的 DNS 名称将是 <code>my-service.default.svc.cluster.local</code>。</li></ul></li><li><p><strong>DNS 查询</strong>：</p><ul><li><p>集群中的 Pod 可以通过 DNS 名称访问其他服务：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>curl http://my-service.default.svc.cluster.local
</span></span></code></pre></div></li><li><p>Kubernetes 通过 <strong>CoreDNS</strong>（或 <strong>kube-dns</strong>）将 DNS 查询解析到对应的 Service 的 ClusterIP。</p></li></ul></li><li><p><strong>应用场景</strong>：</p><ul><li><strong>集群内部通信</strong>：服务与服务之间的通信通常使用 DNS 名称。</li><li><strong>Pod 和 Service 之间</strong>：Pod 内的应用可以通过 DNS 名称访问服务，无论该服务背后的 Pod 如何变化。</li></ul></li></ul><h3 id=2-clusterip内网访问>2. <strong>ClusterIP（内网访问）</strong>
<a class=anchor href=#2-clusterip%e5%86%85%e7%bd%91%e8%ae%bf%e9%97%ae>#</a></h3><p><code>ClusterIP</code> 是 Kubernetes 中 <strong>Service</strong> 的默认类型，用于将服务暴露为一个集群内部的虚拟 IP（VIP）。</p><ul><li><p>如何工作</p><p>：</p><ul><li>每个 Kubernetes Service 类型为 <code>ClusterIP</code> 时，Kubernetes 会为该 Service 分配一个虚拟 IP 地址，该 IP 地址在集群内部是固定的，其他 Pod 可以通过该 IP 地址访问服务。</li><li>服务发现可以通过 DNS 解析该 IP 地址。</li></ul></li><li><p>应用场景</p><p>：</p><ul><li>适用于集群内部的服务通信，尤其是当服务不需要暴露到外部网络时。</li><li>例如，在 Kubernetes 集群中的应用 Pod 可以通过访问 <code>my-service.default.svc.cluster.local</code> 来访问 <code>ClusterIP</code> 类型的 Service。</li></ul></li></ul><h3 id=3-nodeport外部访问>3. <strong>NodePort（外部访问）</strong>
<a class=anchor href=#3-nodeport%e5%a4%96%e9%83%a8%e8%ae%bf%e9%97%ae>#</a></h3><p><code>NodePort</code> 是 Kubernetes 中 <strong>Service</strong> 的一种类型，它将服务暴露到集群外部，并将请求通过集群节点的指定端口转发到服务的 <code>ClusterIP</code>。</p><ul><li><p>如何工作</p><p>：</p><ul><li>Kubernetes 为每个 <code>NodePort</code> 服务分配一个端口（范围通常是 30000-32767），该端口在集群中的每个节点上都可以访问。</li><li>通过访问集群任意节点的 <code>&lt;node-ip>:&lt;node-port></code>，请求将被转发到该 Service 的 <code>ClusterIP</code> 上，从而访问服务。</li></ul></li><li><p>应用场景</p><p>：</p><ul><li>适用于将 Kubernetes 服务暴露到集群外部的场景，常用于开发和测试环境。</li><li>通过该方式，可以在 Kubernetes 集群外部通过节点的 IP 和端口访问集群内的服务。</li></ul></li></ul><h3 id=4-loadbalancer云环境外部访问>4. <strong>LoadBalancer（云环境外部访问）</strong>
<a class=anchor href=#4-loadbalancer%e4%ba%91%e7%8e%af%e5%a2%83%e5%a4%96%e9%83%a8%e8%ae%bf%e9%97%ae>#</a></h3><p><code>LoadBalancer</code> 是 Kubernetes 中的一个 <strong>Service</strong> 类型，通常与云服务提供商（如 AWS、Azure、GCP）结合使用，自动创建一个外部负载均衡器。</p><ul><li><strong>如何工作</strong>：<ul><li>当创建 <code>LoadBalancer</code> 类型的 Service 时，云提供商会自动创建一个外部负载均衡器，并将负载均衡器的 IP 地址或 DNS 名称映射到该 Service 上。</li><li>通过负载均衡器的 IP 地址或 DNS 名称，外部用户可以访问服务，负载均衡器会将流量转发到 <code>ClusterIP</code> 或 <code>NodePort</code> 类型的服务上。</li></ul></li><li><strong>应用场景</strong>：<ul><li>适用于生产环境，尤其是需要将服务暴露到公网，且希望由云提供商管理负载均衡的场景。</li><li>比如，Kubernetes 在 AWS 上时，创建 <code>LoadBalancer</code> 服务会自动申请一个 ELB（Elastic Load Balancer）。</li></ul></li></ul><h3 id=5-externalname外部服务访问>5. <strong>ExternalName（外部服务访问）</strong>
<a class=anchor href=#5-externalname%e5%a4%96%e9%83%a8%e6%9c%8d%e5%8a%a1%e8%ae%bf%e9%97%ae>#</a></h3><p><code>ExternalName</code> 是 Kubernetes 中的一个 <strong>Service</strong> 类型，允许将一个 Service 映射到外部 DNS 名称，而不是集群内部的 Pod 或 <code>ClusterIP</code>。</p><ul><li><p>如何工作</p><p>：</p><ul><li>当创建 <code>ExternalName</code> 类型的 Service 时，Kubernetes 不会暴露一个虚拟 IP，而是将请求转发到指定的外部 DNS 名称。</li><li>通过这种方式，Kubernetes 中的应用可以访问外部的服务，而不需要了解外部服务的 IP 地址。</li></ul></li><li><p>应用场景</p><p>：</p><ul><li>适用于需要访问集群外部服务的场景。例如，访问外部数据库、API 服务或第三方服务时，可以通过 Kubernetes 创建一个 <code>ExternalName</code> 类型的 Service。</li><li>例如，创建一个名为 <code>external-db</code> 的 Service，将其映射到外部 DNS <code>db.example.com</code> 上，集群内的 Pod 通过 <code>external-db.default.svc.cluster.local</code> 访问外部数据库。</li></ul></li></ul><h3 id=6-headless-service无头服务>6. <strong>Headless Service（无头服务）</strong>
<a class=anchor href=#6-headless-service%e6%97%a0%e5%a4%b4%e6%9c%8d%e5%8a%a1>#</a></h3><p>Headless Service 是 Kubernetes 中 <strong>Service</strong> 的一种特殊类型，用于不使用 ClusterIP 或负载均衡机制，而是直接通过 DNS 解析访问每个 Pod。</p><ul><li><p>如何工作</p><p>：</p><ul><li>在创建 Service 时，设置 <code>ClusterIP: None</code>，即禁用 <code>ClusterIP</code>，同时使用 DNS 名称进行服务发现。</li><li>这种方式不会为 Service 创建虚拟 IP 地址，而是为每个 Pod 创建一个 DNS A 记录，Pod 的 DNS 名称格式为 <code>&lt;pod-name>.&lt;service-name>.&lt;namespace>.svc.cluster.local</code>。</li></ul></li><li><p>应用场景</p><p>：</p><ul><li>适用于需要直接访问 Pod 的情况，常见于有状态应用，如数据库集群（例如 MongoDB、Cassandra）等，Pod 的 IP 地址在集群中动态变化时，服务发现可以直接访问每个 Pod。</li></ul></li></ul><h3 id=7-endpoints手动定义服务发现>7. <strong>Endpoints（手动定义服务发现）</strong>
<a class=anchor href=#7-endpoints%e6%89%8b%e5%8a%a8%e5%ae%9a%e4%b9%89%e6%9c%8d%e5%8a%a1%e5%8f%91%e7%8e%b0>#</a></h3><p>Kubernetes 的 <strong>Endpoints</strong> 是一种低级别的服务发现方式，用于映射服务与后端 Pod 之间的关系。</p><ul><li><strong>如何工作</strong>：<ul><li>Endpoints 是一种资源对象，它描述了 Service 后端 Pod 的 IP 地址和端口。</li><li>手动创建 Endpoint 对象，将外部或其他 Pod 作为后端服务进行访问。</li></ul></li><li><strong>应用场景</strong>：<ul><li>当服务不使用 <code>Service</code> 对象或不使用 Kubernetes 自动化的服务发现时，可以手动定义 Endpoint 对象来实现服务发现。</li><li>例如，当需要将外部数据库或服务纳入到 Kubernetes 集群的服务发现中时。</li></ul></li></ul><hr><h3 id=总结-12>总结
<a class=anchor href=#%e6%80%bb%e7%bb%93-12>#</a></h3><p>Kubernetes 提供了多种服务发现方式，适用于不同的场景：</p><ol><li><strong>DNS 服务发现</strong>：使用 DNS 名称访问服务，适合集群内部通信。</li><li><strong>ClusterIP</strong>：为服务分配一个集群内部的虚拟 IP，适合内部服务通信。</li><li><strong>NodePort</strong>：将服务暴露到外部，通过节点的 IP 和端口访问。</li><li><strong>LoadBalancer</strong>：与云负载均衡器结合，适合将服务暴露到公网。</li><li><strong>ExternalName</strong>：将 Service 映射到外部 DNS 名称，适合访问外部服务。</li><li><strong>Headless Service</strong>：通过 DNS 直接解析每个 Pod，适用于有状态服务。</li><li><strong>Endpoints</strong>：手动定义服务与后端 Pod 的映射关系，用于特殊场景。</li></ol><p>不同的服务发现方式可以根据需要选择和配置，提供灵活的服务访问和负载均衡机制。</p><h2 id=pod几种常用状态>pod几种常用状态
<a class=anchor href=#pod%e5%87%a0%e7%a7%8d%e5%b8%b8%e7%94%a8%e7%8a%b6%e6%80%81>#</a></h2><p>在 Kubernetes 中，Pod 的生命周期有多个状态，每个状态代表了 Pod 当前的不同阶段或健康状况。以下是一些常见的 Pod 状态及其说明：</p><h3 id=1-pending>1. <strong>Pending</strong>
<a class=anchor href=#1-pending>#</a></h3><ul><li><p><strong>描述</strong>：Pod 已经被创建并且调度到一个节点，但还没有开始运行，通常是因为 Pod 的容器镜像还没有下载，或者资源（如 CPU、内存）尚未准备好。</p></li><li><p>原因</p><p>：</p><ul><li>资源不足（例如节点没有足够的内存或 CPU 来启动 Pod）。</li><li>容器镜像下载失败或拉取时间过长。</li><li>Pod 的调度过程还未完成。</li></ul></li></ul><h3 id=2-running>2. <strong>Running</strong>
<a class=anchor href=#2-running>#</a></h3><ul><li><p><strong>描述</strong>：Pod 已经调度到某个节点并且至少有一个容器正在运行。</p></li><li><p>原因</p><p>：</p><ul><li>Pod 的所有容器都已经成功启动，并且正在运行中。</li><li>Pod 可以接受请求，容器内的应用可以开始提供服务。</li></ul></li></ul><h3 id=3-succeeded>3. <strong>Succeeded</strong>
<a class=anchor href=#3-succeeded>#</a></h3><ul><li><p><strong>描述</strong>：Pod 内的所有容器已经成功执行完毕并退出，没有发生任何错误或异常。</p></li><li><p>原因</p><p>：</p><ul><li>适用于一次性任务（如批处理任务），容器正常完成任务后退出。</li></ul></li></ul><h3 id=4-failed>4. <strong>Failed</strong>
<a class=anchor href=#4-failed>#</a></h3><ul><li><p><strong>描述</strong>：Pod 内的容器因为错误退出（退出码非零），且重启策略不允许重启容器。</p></li><li><p>原因</p><p>：</p><ul><li>容器内的应用崩溃或执行过程中发生错误导致容器失败退出。</li><li>容器的重启策略为 <code>Never</code> 或 <code>OnFailure</code> 且失败条件不满足时。</li></ul></li></ul><h3 id=5-crashloopbackoff>5. <strong>CrashLoopBackOff</strong>
<a class=anchor href=#5-crashloopbackoff>#</a></h3><ul><li><p><strong>描述</strong>：Pod 的容器在启动后不久崩溃，并且 Kubernetes 尝试多次重启容器，仍然无法恢复正常状态。每次重启都会有一定的延迟（BackOff），以避免过度尝试重启。</p></li><li><p>原因</p><p>：</p><ul><li>容器的启动过程失败，例如应用配置错误、环境问题、依赖服务未启动等。</li><li>当容器因某种原因失败并尝试重启，但在多个重启后仍然无法恢复时，状态会变为 <code>CrashLoopBackOff</code>。</li></ul></li></ul><h3 id=6-unknown>6. <strong>Unknown</strong>
<a class=anchor href=#6-unknown>#</a></h3><ul><li><p><strong>描述</strong>：Kubernetes 无法获取 Pod 的状态，通常表示节点与集群管理组件之间的通信出现问题。</p></li><li><p>原因</p><p>：</p><ul><li>节点与 Kubernetes API Server 之间的网络或通信出现问题，导致无法获取 Pod 状态。</li></ul></li></ul><h3 id=7-terminating>7. <strong>Terminating</strong>
<a class=anchor href=#7-terminating>#</a></h3><ul><li><p><strong>描述</strong>：Pod 正在被删除，并且 Kubernetes 已经开始终止容器和清理资源。</p></li><li><p>原因</p><p>：</p><ul><li>用户或控制器发出了删除 Pod 的命令，Pod 正在关闭并释放资源。</li><li>容器和 Pod 的删除过程可能会有一定的延迟，特别是在删除过程中等待容器的退出或清理临时存储等。</li></ul></li></ul><h3 id=8-waiting>8. <strong>Waiting</strong>
<a class=anchor href=#8-waiting>#</a></h3><ul><li><p><strong>描述</strong>：Pod 内的容器正在等待某些条件才能启动。常见的等待原因包括容器镜像下载失败、挂载存储卷出现问题等。</p></li><li><p>原因</p><p>：</p><ul><li>容器在启动时遇到阻塞状态，可能是因为某些依赖没有准备好（如挂载的存储还没就绪）。</li><li>容器被限制启动（如因为节点资源不足）。</li></ul></li></ul><hr><h3 id=总结-13>总结
<a class=anchor href=#%e6%80%bb%e7%bb%93-13>#</a></h3><p>常见的 Pod 状态包括：</p><ol><li><strong>Pending</strong>：正在等待资源和调度。</li><li><strong>Running</strong>：正在运行。</li><li><strong>Succeeded</strong>：成功完成任务并退出。</li><li><strong>Failed</strong>：由于错误容器退出。</li><li><strong>CrashLoopBackOff</strong>：容器多次失败并在重启时有延迟。</li><li><strong>Unknown</strong>：无法获取 Pod 状态，通常是通信问题。</li><li><strong>Terminating</strong>：Pod 正在被删除。</li><li><strong>Waiting</strong>：容器处于等待状态，通常是等待某些条件满足。</li></ol><p>理解这些状态有助于排查和诊断 Kubernetes 集群中的应用问题，尤其是当 Pod 状态不正常时，可以根据不同的状态做出相应的排查与处理。</p><h2 id=pod-生命周期的钩子函数>Pod 生命周期的钩子函数
<a class=anchor href=#pod-%e7%94%9f%e5%91%bd%e5%91%a8%e6%9c%9f%e7%9a%84%e9%92%a9%e5%ad%90%e5%87%bd%e6%95%b0>#</a></h2><p>在 Kubernetes 中，Pod 生命周期钩子函数（Lifecycle Hooks）用于在 Pod 生命周期的特定阶段执行用户自定义操作。钩子函数允许在容器启动、终止时执行特定的任务，通常用于初始化、清理、日志记录、通知等。常见的钩子函数包括：</p><h3 id=1-poststart>1. <strong>PostStart</strong>
<a class=anchor href=#1-poststart>#</a></h3><ul><li><p><strong>描述</strong>：在容器启动之后立即执行的钩子函数。此钩子会在容器的应用程序启动之前执行。</p></li><li><p><strong>使用场景</strong>：</p><ul><li>用于容器启动后执行初始化任务，例如数据库初始化、数据迁移等。</li><li>可以用来启动容器内的外部服务或做其他准备工作。</li></ul></li><li><p><strong>注意事项</strong>：</p><ul><li><code>PostStart</code> 钩子在容器启动完成之前就会执行，因此如果该钩子执行失败，容器仍然会被认为启动失败。</li></ul></li><li><p><strong>配置示例</strong>：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>lifecycle</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>postStart</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>exec</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>command</span>: [<span style=color:#e6db74>&#34;sh&#34;</span>, <span style=color:#e6db74>&#34;-c&#34;</span>, <span style=color:#e6db74>&#34;echo Container started&#34;</span>]
</span></span></code></pre></div></li></ul><h3 id=2-prestop>2. <strong>PreStop</strong>
<a class=anchor href=#2-prestop>#</a></h3><ul><li><p><strong>描述</strong>：在容器终止之前执行的钩子函数。Kubernetes 在发送 <code>SIGTERM</code> 信号时会触发 <code>PreStop</code> 钩子，以便容器能够执行清理工作。</p></li><li><p><strong>使用场景</strong>：</p><ul><li>用于在容器关闭之前执行一些清理任务，例如关闭外部连接、保存状态或清理临时文件等。</li><li>适合在容器终止时进行优雅的关闭，以确保没有丢失数据或影响业务。</li></ul></li><li><p><strong>注意事项</strong>：</p><ul><li><code>PreStop</code> 钩子可能会受到容器的终止时间限制（通常为 30 秒，取决于 <code>terminationGracePeriodSeconds</code> 设置），如果钩子执行时间超过限制，容器将被强制杀死。</li></ul></li><li><p><strong>配置示例</strong>：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>lifecycle</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>preStop</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>exec</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>command</span>: [<span style=color:#e6db74>&#34;sh&#34;</span>, <span style=color:#e6db74>&#34;-c&#34;</span>, <span style=color:#e6db74>&#34;echo Container stopping&#34;</span>]
</span></span></code></pre></div></li></ul><h3 id=3-限制与注意事项>3. <strong>限制与注意事项</strong>
<a class=anchor href=#3-%e9%99%90%e5%88%b6%e4%b8%8e%e6%b3%a8%e6%84%8f%e4%ba%8b%e9%a1%b9>#</a></h3><ul><li>钩子函数的 <strong>执行时间有限制</strong>，尤其是在容器终止时的 <code>PreStop</code> 钩子，通常 Kubernetes 会根据 <code>terminationGracePeriodSeconds</code> 配置给容器一定的优雅终止时间（默认为 30 秒）。如果钩子执行超过该时间，容器会被强制终止。</li><li><strong>钩子执行的错误处理</strong>：如果钩子执行失败，Kubernetes 会根据具体情况采取不同的策略（如容器启动失败时 Pod 进入 Crash 状态）。特别是 <code>PostStart</code> 钩子失败时，容器会被视为启动失败，Kubernetes 会尝试重新启动容器。</li></ul><h3 id=4-钩子的执行方式>4. <strong>钩子的执行方式</strong>
<a class=anchor href=#4-%e9%92%a9%e5%ad%90%e7%9a%84%e6%89%a7%e8%a1%8c%e6%96%b9%e5%bc%8f>#</a></h3><p>Kubernetes 支持以下几种方式来执行钩子函数：</p><ul><li><strong>Exec</strong>：在容器内执行指定的命令。适用于需要执行自定义命令的场景。</li><li><strong>HTTP</strong>：向容器内的某个 HTTP 服务发送请求。适用于需要与容器内服务进行交互的场景。</li></ul><p>示例：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>lifecycle</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>postStart</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>httpGet</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>path</span>: <span style=color:#ae81ff>/healthz</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>port</span>: <span style=color:#ae81ff>8080</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>preStop</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>exec</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>command</span>: [<span style=color:#e6db74>&#34;sh&#34;</span>, <span style=color:#e6db74>&#34;-c&#34;</span>, <span style=color:#e6db74>&#34;curl -X POST http://localhost:8080/shutdown&#34;</span>]
</span></span></code></pre></div><h3 id=总结-14>总结
<a class=anchor href=#%e6%80%bb%e7%bb%93-14>#</a></h3><p>Pod 生命周期钩子函数 <code>PostStart</code> 和 <code>PreStop</code> 可以帮助用户在容器启动和停止时执行自定义操作。<code>PostStart</code> 钩子在容器启动后执行，而 <code>PreStop</code> 钩子在容器终止前执行。合理使用钩子函数可以帮助容器在生命周期内进行一些必要的初始化和清理操作，从而提高应用的稳定性和健壮性。</p><h2 id=calico和flannel区别>Calico和flannel区别☆
<a class=anchor href=#calico%e5%92%8cflannel%e5%8c%ba%e5%88%ab>#</a></h2><p>Calico 和 Flannel 都是 Kubernetes 中常用的网络插件，它们提供了容器间的网络通信支持，但在实现方式、功能和特性上存在一些显著的差异。以下是它们的主要区别：</p><h3 id=1-架构设计>1. <strong>架构设计</strong>
<a class=anchor href=#1-%e6%9e%b6%e6%9e%84%e8%ae%be%e8%ae%a1>#</a></h3><ul><li><strong>Flannel</strong>：<ul><li>Flannel 是一个简单的网络插件，设计上比较轻量级，主要关注为 Kubernetes 提供一个扁平化的、可扩展的网络模型。</li><li>Flannel 使用的是 <strong>Overlay 网络</strong>，通过在每个节点上创建虚拟网络（通常使用 VXLAN 或者其他 tunneling 协议）来实现容器之间的通信。</li><li>由于 Flannel 的架构简单，它的网络性能相对较低，适用于对性能要求不太高的场景。</li></ul></li><li><strong>Calico</strong>：<ul><li>Calico 提供了一个更加灵活和强大的网络解决方案，支持 <strong>Layer 3 网络</strong>，不仅支持 Overlay 网络（使用 IP-in-IP 或 VXLAN），还可以使用 <strong>路由模式</strong>（如 BGP）进行容器间的通信。</li><li>Calico 的架构设计允许更加细粒度的控制，包括网络策略、IP 地址管理等功能。</li></ul></li></ul><h3 id=2-网络模型>2. <strong>网络模型</strong>
<a class=anchor href=#2-%e7%bd%91%e7%bb%9c%e6%a8%a1%e5%9e%8b>#</a></h3><ul><li><strong>Flannel</strong>：<ul><li>使用 <strong>Overlay 网络</strong>：每个 Pod 都分配一个虚拟 IP 地址，并通过隧道技术（VXLAN 或其它方式）将这些 IP 地址连接到其他节点的 Pod 上。Pod 之间的通信通过隧道（tunnel）实现。</li><li>网络通信相对简单，但可能存在一些性能开销。</li></ul></li><li><strong>Calico</strong>：<ul><li>支持 <strong>Overlay</strong> 和 <strong>非 Overlay（路由）网络</strong>：可以根据需求选择是否使用隧道。如果使用路由模式，Pod 会直接使用物理网络进行通信，减少了网络层的开销。</li><li>在非 Overlay 模式下，Calico 会利用 <strong>BGP（边界网关协议）</strong> 来实现不同节点之间的直接路由，而不是依赖于隧道。</li></ul></li></ul><h3 id=3-网络策略network-policy>3. <strong>网络策略（Network Policy）</strong>
<a class=anchor href=#3-%e7%bd%91%e7%bb%9c%e7%ad%96%e7%95%a5network-policy>#</a></h3><ul><li><strong>Flannel</strong>：<ul><li>Flannel 本身不提供网络策略功能。它依赖于 Kubernetes 本身的网络策略（NetworkPolicy）来控制 Pod 之间的通信，或者需要与其他插件（如 Cilium）配合使用来实现网络策略。</li><li>Flannel 只是提供了基础的网络通信能力。</li></ul></li><li><strong>Calico</strong>：<ul><li>Calico 提供了强大的 <strong>网络策略（NetworkPolicy）</strong> 功能，允许用户在粒度上定义哪些 Pod 可以与哪些 Pod 通信，控制流量的入口和出口。Calico 的网络策略比 Kubernetes 原生的网络策略更为强大，支持细粒度的控制。</li></ul></li></ul><h3 id=4-性能>4. <strong>性能</strong>
<a class=anchor href=#4-%e6%80%a7%e8%83%bd>#</a></h3><ul><li><strong>Flannel</strong>：<ul><li>由于使用了 Overlay 网络，Flannel 会在网络通信时引入一定的性能开销，尤其是在高流量的场景下。</li><li>对于需要高性能的网络通信，Flannel 的性能可能不如 Calico。</li></ul></li><li><strong>Calico</strong>：<ul><li>Calico 的性能较高，尤其在使用 <strong>路由模式</strong>（非 Overlay）时，Pod 之间的通信直接通过物理网络路由，不需要额外的隧道，因此性能较优。</li><li>在支持大规模集群和高吞吐量时，Calico 表现得更好。</li></ul></li></ul><h3 id=5-ip-地址管理>5. <strong>IP 地址管理</strong>
<a class=anchor href=#5-ip-%e5%9c%b0%e5%9d%80%e7%ae%a1%e7%90%86>#</a></h3><ul><li><strong>Flannel</strong>：<ul><li>Flannel 提供简单的 IP 地址管理方案，通常会为每个节点分配一个子网，并为每个 Pod 分配一个虚拟 IP 地址。</li><li>Flannel 使用 etcd 作为后端存储来管理 IP 地址分配。</li></ul></li><li><strong>Calico</strong>：<ul><li>Calico 提供灵活的 IP 地址管理，支持直接在物理网络上分配 IP 地址，支持多种 IP 地址池配置，支持与现有的 IPAM（IP 地址管理）系统集成。</li><li>Calico 不仅可以为 Kubernetes 分配 IP 地址，还可以扩展到容器之外的网络范围。</li></ul></li></ul><h3 id=6-扩展性>6. <strong>扩展性</strong>
<a class=anchor href=#6-%e6%89%a9%e5%b1%95%e6%80%a7>#</a></h3><ul><li><strong>Flannel</strong>：<ul><li>Flannel 的功能相对简单，适用于需要快速部署且不需要复杂网络策略的场景。</li><li>虽然 Flannel 支持插件扩展，但它的扩展性相对较差，特别是在涉及到复杂的流量控制和安全策略时。</li></ul></li><li><strong>Calico</strong>：<ul><li>Calico 提供了更高的扩展性，支持与其他系统的集成（如 CNI、BGP、网络策略管理等），能够适应复杂的企业级需求。</li><li>它支持更复杂的网络环境，并且可以在不同的云平台和裸机环境中高效运行。</li></ul></li></ul><h3 id=7-支持的环境>7. <strong>支持的环境</strong>
<a class=anchor href=#7-%e6%94%af%e6%8c%81%e7%9a%84%e7%8e%af%e5%a2%83>#</a></h3><ul><li><strong>Flannel</strong>：<ul><li>适合轻量级的 Kubernetes 集群，通常用于中小规模或对网络策略和性能要求较低的环境。</li><li>支持大部分的 CNI 插件和 Kubernetes 部署，适用于简化的网络需求。</li></ul></li><li><strong>Calico</strong>：<ul><li>适合对网络性能、可扩展性和安全性有较高要求的企业级环境，尤其在需要高级网络策略和 IP 地址管理的场景中表现出色。</li><li>支持大型集群、大规模部署以及复杂的网络拓扑结构。</li></ul></li></ul><h3 id=总结-15>总结
<a class=anchor href=#%e6%80%bb%e7%bb%93-15>#</a></h3><table><thead><tr><th>特性</th><th><strong>Flannel</strong></th><th><strong>Calico</strong></th></tr></thead><tbody><tr><td>网络类型</td><td>Overlay 网络（VXLAN 或其它）</td><td>支持 Overlay 和路由模式（BGP）</td></tr><tr><td>网络策略</td><td>不支持（依赖 Kubernetes 原生 NetworkPolicy）</td><td>支持强大的网络策略（NetworkPolicy）</td></tr><tr><td>性能</td><td>性能较低，主要用于小规模环境</td><td>性能高，支持大规模和高吞吐量的集群</td></tr><tr><td>IP 地址管理</td><td>简单的 IP 地址分配和管理</td><td>灵活的 IP 地址管理，支持与外部 IPAM 集成</td></tr><tr><td>扩展性</td><td>扩展性较差，适用于轻量级需求</td><td>高度可扩展，适用于复杂和企业级环境</td></tr><tr><td>适用场景</td><td>小规模集群，低延迟应用，简化的网络需求</td><td>大规模集群，复杂的网络策略和安全需求</td></tr></tbody></table><p>总结来说，如果你需要一个简单的网络插件来快速部署 Kubernetes 集群，Flannel 是一个不错的选择；而如果你的集群需要支持更高的网络性能、更复杂的网络策略或需要更强的扩展性，Calico 是更合适的选择。</p><h2 id=calico网络原理组网方式>calico网络原理、组网方式
<a class=anchor href=#calico%e7%bd%91%e7%bb%9c%e5%8e%9f%e7%90%86%e7%bb%84%e7%bd%91%e6%96%b9%e5%bc%8f>#</a></h2><p>Calico 是一个为 Kubernetes 提供高效网络连接和安全策略的容器网络插件。它支持高性能的网络通信和复杂的网络策略，同时能与现有网络基础设施兼容，支持大规模的云原生应用。Calico 网络的原理和组网方式主要基于以下几个关键概念：</p><h3 id=1-网络原理>1. <strong>网络原理</strong>
<a class=anchor href=#1-%e7%bd%91%e7%bb%9c%e5%8e%9f%e7%90%86>#</a></h3><p>Calico 的核心原理基于 <strong>Layer 3 网络</strong>，并采用 <strong>路由模式</strong>（BGP）进行网络通信，而不是依赖于传统的 Overlay 网络（如 VXLAN）。这使得 Calico 在性能和可扩展性方面具有显著优势。它主要通过以下几个机制来提供高效的网络服务：</p><h4 id=11-ip-in-ip-或-vxlan-隧道>1.1 <strong>IP-in-IP 或 VXLAN 隧道</strong>
<a class=anchor href=#11-ip-in-ip-%e6%88%96-vxlan-%e9%9a%a7%e9%81%93>#</a></h4><ul><li>在支持路由模式的情况下，Calico 允许容器通过物理网络直接通信，而不需要通过隧道。容器的 IP 地址直接映射到物理网络上的 IP 地址。</li><li>如果容器的网络无法直接在物理网络上路由（例如在某些隔离的环境中），Calico 会通过 <strong>IP-in-IP 隧道</strong> 或 <strong>VXLAN</strong> 隧道来封装网络流量。这样，容器仍然能够通过隧道来进行通信，确保网络的隔离和连接。</li></ul><h4 id=12-路由模式bgp>1.2 <strong>路由模式（BGP）</strong>
<a class=anchor href=#12-%e8%b7%af%e7%94%b1%e6%a8%a1%e5%bc%8fbgp>#</a></h4><ul><li>Calico 支持 <strong>BGP（边界网关协议）</strong>，用于管理不同节点之间的路由信息。每个节点都会向其他节点通告其容器网络的路由信息，使用 BGP 协议动态交换路由表。</li><li>这种路由机制允许节点之间直接通信，避免了传统 Overlay 网络带来的性能开销，同时保持网络的灵活性和可扩展性。</li></ul><h4 id=13-ip-地址管理ipam>1.3 <strong>IP 地址管理（IPAM）</strong>
<a class=anchor href=#13-ip-%e5%9c%b0%e5%9d%80%e7%ae%a1%e7%90%86ipam>#</a></h4><ul><li>Calico 提供了 <strong>IP 地址管理</strong>（IPAM），为每个 Pod 分配一个唯一的 IP 地址。它可以使用自动化的方式为每个 Pod 分配 IP 地址，也可以与外部 IPAM 系统集成（例如与云提供商的 IP 地址池进行集成）。</li><li>通过这种 IP 地址管理，Calico 可以确保 Pod 和服务在 Kubernetes 集群中拥有独立且唯一的 IP 地址。</li></ul><h3 id=2-calico-的组网方式>2. <strong>Calico 的组网方式</strong>
<a class=anchor href=#2-calico-%e7%9a%84%e7%bb%84%e7%bd%91%e6%96%b9%e5%bc%8f>#</a></h3><p>Calico 支持多种组网方式，根据网络拓扑和需求的不同，用户可以选择不同的部署方式。主要的组网方式包括：</p><h4 id=21-layer-3-路由模式无-overlay>2.1 <strong>Layer 3 路由模式（无 Overlay）</strong>
<a class=anchor href=#21-layer-3-%e8%b7%af%e7%94%b1%e6%a8%a1%e5%bc%8f%e6%97%a0-overlay>#</a></h4><ul><li><p><strong>原理</strong>：在路由模式下，Calico 使用 BGP 动态地将每个节点的容器网络 IP（Pod 的 IP 地址）公布到整个集群中。容器直接在物理网络上进行通信，不需要任何隧道或封装。</p></li><li><p>优点</p><p>：</p><ul><li>性能更高：因为容器间的通信不需要通过隧道，而是直接通过物理网络路由。</li><li>低延迟：直接的 Layer 3 路由减少了网络开销。</li><li>可扩展性好：能够轻松支持大型集群，因为 BGP 可以自动适应网络的扩展。</li></ul></li><li><p><strong>使用场景</strong>：适合对性能有较高要求的大型集群，尤其是需要大规模、高吞吐量容器通信的环境。</p></li></ul><h4 id=22-overlay-网络模式ip-in-ip-或-vxlan>2.2 <strong>Overlay 网络模式（IP-in-IP 或 VXLAN）</strong>
<a class=anchor href=#22-overlay-%e7%bd%91%e7%bb%9c%e6%a8%a1%e5%bc%8fip-in-ip-%e6%88%96-vxlan>#</a></h4><ul><li><p><strong>原理</strong>：如果物理网络无法直接支持容器间的直接路由，Calico 会使用 <strong>IP-in-IP</strong> 或 <strong>VXLAN</strong> 隧道技术来封装 Pod 的流量。通过隧道，Pod 可以像在 Overlay 网络上一样进行通信。</p></li><li><p>优点</p><p>：</p><ul><li>网络隔离：Overlay 网络可以在不同的数据中心或网络段之间提供容器隔离。</li><li>容易配置：适用于在不同物理网络环境下的 Kubernetes 集群，确保容器网络的可达性。</li></ul></li><li><p><strong>使用场景</strong>：适合在需要跨多个网络环境或云环境运行的 Kubernetes 集群，或者希望对网络流量进行封装和隔离的场景。</p></li></ul><h4 id=23-calico-与其他网络插件的结合>2.3 <strong>Calico 与其他网络插件的结合</strong>
<a class=anchor href=#23-calico-%e4%b8%8e%e5%85%b6%e4%bb%96%e7%bd%91%e7%bb%9c%e6%8f%92%e4%bb%b6%e7%9a%84%e7%bb%93%e5%90%88>#</a></h4><ul><li>Calico 可以与其他 CNI 插件（如 Flannel、Weave 等）结合使用，在这些插件的基础上提供更多的功能。例如，Calico 可以在 Flannel 或 Weave 网络基础上提供增强的网络策略功能，允许用户实现更细粒度的流量控制和安全策略。</li></ul><h4 id=24-与-kubernetes-网络策略结合>2.4 <strong>与 Kubernetes 网络策略结合</strong>
<a class=anchor href=#24-%e4%b8%8e-kubernetes-%e7%bd%91%e7%bb%9c%e7%ad%96%e7%95%a5%e7%bb%93%e5%90%88>#</a></h4><ul><li>Calico 本身提供了强大的 <strong>网络策略（NetworkPolicy）</strong> 功能，用户可以基于标签、命名空间等条件控制容器之间的通信。网络策略允许控制 Pod 之间、Pod 和外部服务之间的流量，实现容器化应用的安全隔离。</li></ul><h3 id=3-calico-的主要组件>3. <strong>Calico 的主要组件</strong>
<a class=anchor href=#3-calico-%e7%9a%84%e4%b8%bb%e8%a6%81%e7%bb%84%e4%bb%b6>#</a></h3><p>Calico 主要由以下几个组件组成：</p><ul><li><strong>Felix</strong>：Calico 的核心代理组件，负责节点之间的路由更新、网络策略的执行、以及容器网络的管理。Felix 会在每个节点上运行，并与 Etcd 或其他存储后端进行交互，更新路由和网络策略。</li><li><strong>BGP</strong>：使用 BGP 协议动态地交换节点之间的路由信息。每个节点上都会有一个 BGP 路由器，它将每个 Pod 的 IP 地址与节点的物理网络地址关联起来。</li><li><strong>Calico CNI 插件</strong>：为 Kubernetes 提供容器网络功能，负责为每个 Pod 分配 IP 地址，并与 Etcd 进行交互来管理 IP 地址和路由。</li><li><strong>Etcd</strong>：作为分布式的存储后端，用来存储 Calico 的配置信息、路由表、网络策略等数据。</li><li><strong>API Server</strong>：通过 Calico 的 API，用户可以配置网络策略、查看网络状态等。它为管理和操作 Calico 提供了一个易于使用的接口。</li></ul><h3 id=4-calico-的优势>4. <strong>Calico 的优势</strong>
<a class=anchor href=#4-calico-%e7%9a%84%e4%bc%98%e5%8a%bf>#</a></h3><ul><li><strong>高性能</strong>：通过直接的 Layer 3 路由和 BGP 协议，避免了 Overlay 网络的性能开销，提供了低延迟的网络连接。</li><li><strong>灵活性</strong>：支持多种网络模型（如 IP-in-IP、VXLAN 和纯路由模式），可以根据不同的需求和网络环境选择适合的模式。</li><li><strong>扩展性</strong>：Calico 设计上适合大规模集群，能够通过 BGP 动态交换路由，适应大规模和动态变化的集群环境。</li><li><strong>网络安全</strong>：通过网络策略（NetworkPolicy），Calico 允许用户定义容器间的通信规则，确保集群的网络安全。</li><li><strong>支持多种部署环境</strong>：Calico 支持在云环境、物理服务器、虚拟化环境以及裸机环境中运行，能够满足不同场景的需求。</li></ul><h3 id=总结-16>总结
<a class=anchor href=#%e6%80%bb%e7%bb%93-16>#</a></h3><p>Calico 是一个强大的网络插件，它基于 Layer 3 路由和 BGP 协议提供高效的容器网络解决方案，支持多种组网模式（如 Overlay 和路由模式），并提供高级的网络策略管理。无论是在需要高性能的容器通信环境，还是在需要灵活、安全的网络隔离和流量控制的场景中，Calico 都能够提供优异的支持。</p><p><strong>IP-in-IP (IPIP)</strong> 是 Calico 支持的一种 <strong>Overlay 网络模式</strong>，它为不同节点上的 Pod 提供网络隔离和跨节点通信。与 BGP 模式相比，IPIP 模式使用隧道技术封装 Pod 的 IP 数据包，使得容器在不同节点间的通信能够通过隧道进行，即使在物理网络上无法直接路由。</p><h3 id=ip-in-ip-模式的工作原理>IP-in-IP 模式的工作原理
<a class=anchor href=#ip-in-ip-%e6%a8%a1%e5%bc%8f%e7%9a%84%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86>#</a></h3><p>在 <strong>IP-in-IP 模式</strong>下，Calico 为每个 Pod 分配一个独立的 IP 地址，这些地址属于一个大的网络范围。当 Pod 在不同节点之间进行通信时，Calico 会使用 IP-in-IP 隧道技术将流量从一个节点传输到另一个节点，具体的工作流程如下：</p><ol><li><strong>隧道封装</strong>：<ul><li>当一个 Pod 发送数据包到另一个节点的 Pod 时，源节点会将数据包封装在 IP-in-IP 隧道中。即，数据包的原始目的 IP 地址是目标 Pod 的 IP 地址，而源地址是源 Pod 的 IP 地址。</li><li>在封装的过程中，源节点会将原始数据包的内容保留在内层 IP 数据包中，外层则加上了源节点的 IP 地址和目标节点的 IP 地址。这种封装技术保证了数据包能够跨节点传输。</li></ul></li><li><strong>隧道解封装</strong>：<ul><li>当数据包到达目标节点时，目标节点会根据外层的 IP 信息解封装数据包，并将内层的数据包转发到目标 Pod。</li><li>目标节点知道如何将内层数据包发送到正确的 Pod，因为它已经包含了目标 Pod 的 IP 地址。</li></ul></li><li><strong>与路由模式的区别</strong>：<ul><li>在 <strong>IP-in-IP 模式</strong>下，容器的 IP 地址和网络流量被封装成 IP 数据包，通过隧道进行跨节点传输。每个节点上的 Calico 代理 (Felix) 会负责管理隧道的建立和流量的转发。</li><li>与之相对，<strong>BGP 路由模式</strong>则不使用隧道，而是直接通过物理网络路由 Pod 之间的流量，依赖于 BGP 路由协议来传递和更新路由信息。</li></ul></li></ol><h3 id=ip-in-ip-模式的优缺点>IP-in-IP 模式的优缺点
<a class=anchor href=#ip-in-ip-%e6%a8%a1%e5%bc%8f%e7%9a%84%e4%bc%98%e7%bc%ba%e7%82%b9>#</a></h3><h4 id=优点>优点：
<a class=anchor href=#%e4%bc%98%e7%82%b9>#</a></h4><ol><li><strong>跨节点通信支持</strong>：<ul><li>IP-in-IP 模式能够实现跨节点和跨数据中心的 Pod 之间的通信，特别适用于网络拓扑较复杂或者无法直接通过物理网络路由 Pod 的环境。</li></ul></li><li><strong>简单易配置</strong>：<ul><li>在使用 Calico 的 IP-in-IP 模式时，配置通常较为简单，不需要复杂的路由配置，适合在一些较简单的环境中快速部署。</li></ul></li><li><strong>隔离性</strong>：<ul><li>由于每个节点之间的流量被封装在隧道中，它提供了较好的网络隔离性，避免了不同网络间的干扰。</li></ul></li></ol><h4 id=缺点>缺点：
<a class=anchor href=#%e7%bc%ba%e7%82%b9>#</a></h4><ol><li><strong>性能开销</strong>：<ul><li>隧道封装和解封装过程会带来额外的性能开销，特别是对于高吞吐量或低延迟要求的应用。封装后的数据包体积增大，网络带宽和 CPU 资源会有所消耗。</li></ul></li><li><strong>调试复杂性</strong>：<ul><li>使用 Overlay 网络时，网络拓扑更加复杂，网络故障排查可能更困难。你需要对隧道的状态进行监控，以确保数据包能够正确地封装和解封装。</li></ul></li><li><strong>不如 BGP 路由模式高效</strong>：<ul><li>相比于直接的路由模式（BGP），IP-in-IP 模式通过隧道传输流量，性能可能会受到影响，尤其是在大规模集群和对性能要求高的场景下。</li></ul></li></ol><h3 id=使用场景>使用场景
<a class=anchor href=#%e4%bd%bf%e7%94%a8%e5%9c%ba%e6%99%af>#</a></h3><ul><li><strong>跨节点网络隔离</strong>：在不同节点或数据中心的 Pod 之间需要安全隔离的环境下，IP-in-IP 模式适合提供封装的网络连接。</li><li><strong>不支持直接路由的网络环境</strong>：如果物理网络不支持跨节点直接路由，或者网络中存在复杂的防火墙和路由规则，IP-in-IP 模式能够通过隧道穿越这些限制，实现 Pod 间的通信。</li></ul><h3 id=配置-ip-in-ip-模式>配置 IP-in-IP 模式
<a class=anchor href=#%e9%85%8d%e7%bd%ae-ip-in-ip-%e6%a8%a1%e5%bc%8f>#</a></h3><p>Calico 默认启用 IP-in-IP 模式，并通过 <code>calicoctl</code> 工具来进行相关配置。你可以在 <strong>Calico 配置文件</strong> 中设置是否启用 IP-in-IP 模式，或者在集群安装过程中选择使用此模式。</p><p>例如，要启用或禁用 IP-in-IP，可以修改 Calico 配置中的 <code>calico_backend</code> 设置：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>calicoctl config set ipip_enabled true
</span></span></code></pre></div><p>这会启动 IP-in-IP 模式，所有 Pod 之间的跨节点流量都会通过 IP-in-IP 隧道进行传输。</p><h3 id=总结-17>总结
<a class=anchor href=#%e6%80%bb%e7%bb%93-17>#</a></h3><ul><li><strong>IP-in-IP 模式</strong> 是 Calico 的一种 <strong>Overlay 网络模式</strong>，通过隧道技术封装 Pod 流量，使其能够跨节点进行通信，适用于需要网络隔离、跨网络拓扑的场景。</li><li>该模式适合较简单的部署场景，但可能会引入性能开销。</li><li>对于性能要求较高、网络拓扑支持直接路由的集群，<strong>BGP 路由模式</strong>可能是更合适的选择。</li></ul><p><strong>VXLAN (Virtual Extensible LAN)</strong> 是一种用于构建虚拟化数据中心的 Overlay 网络技术，可以解决传统 Layer 2 网络在大型、分布式环境中的扩展性问题。Calico 支持 VXLAN 模式作为其 Overlay 网络的一种形式，用于实现跨节点的容器网络连接。与 <strong>IP-in-IP</strong> 模式类似，<strong>VXLAN</strong> 也是一种封装技术，但其工作原理和优缺点略有不同。</p><h3 id=vxlan-的工作原理>VXLAN 的工作原理
<a class=anchor href=#vxlan-%e7%9a%84%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86>#</a></h3><p>在 Calico 中使用 VXLAN 时，每个节点上的容器（Pod）都会分配一个 IP 地址，跨节点通信时，流量会通过 VXLAN 隧道进行封装和传输。具体过程如下：</p><ol><li><strong>隧道封装</strong>：<ul><li>当 Pod 发送数据包到另一个节点上的 Pod 时，源节点会将数据包封装在 VXLAN 隧道中。封装后的数据包会包含原始数据包的内容（Pod 的 IP 地址、数据等），并加上外层的 VXLAN 头，外层头包含源节点的 VXLAN 端口和目标节点的 VXLAN 端口信息。</li><li>这种封装方式使得数据包能够在物理网络上进行传输，即使在物理网络上没有直接的路由路径。</li></ul></li><li><strong>隧道解封装</strong>：<ul><li>当数据包到达目标节点时，目标节点的 VXLAN 解封装器会去掉 VXLAN 头，并将原始数据包转发到目标 Pod。</li><li>这意味着，目标 Pod 收到的包看起来就像是来自直接网络，而不是来自远程节点。</li></ul></li><li><strong>VXLAN 网络标识符 (VNI)</strong>：<ul><li>VXLAN 使用 <strong>VNI（VXLAN Network Identifier）</strong> 来标识不同的虚拟网络。每个 VXLAN 隧道会有一个唯一的 VNI，确保不同虚拟网络之间的数据包不混淆。</li><li>对于 Calico 来说，通常每个 Kubernetes 集群都会使用一个 VXLAN 隧道标识符（VNI）来区分网络流量。</li></ul></li></ol><h3 id=vxlan-模式的优缺点>VXLAN 模式的优缺点
<a class=anchor href=#vxlan-%e6%a8%a1%e5%bc%8f%e7%9a%84%e4%bc%98%e7%bc%ba%e7%82%b9>#</a></h3><h4 id=优点-1>优点：
<a class=anchor href=#%e4%bc%98%e7%82%b9-1>#</a></h4><ol><li><strong>网络隔离性强</strong>：<ul><li>VXLAN 提供了强大的网络隔离功能。每个 VXLAN 隧道都能将流量隔离在虚拟网络中，避免了不同网络之间的干扰。</li></ul></li><li><strong>跨数据中心支持</strong>：<ul><li>VXLAN 能够非常方便地在跨数据中心或跨地域的环境中实现容器网络的连接。在多个数据中心之间，VXLAN 可以有效地承载虚拟网络流量。</li></ul></li><li><strong>大规模支持</strong>：<ul><li>VXLAN 支持最大 16M 个虚拟网络（VNI），远远超出了 VLAN 的 4096 限制。这使得 VXLAN 适用于大规模的容器网络和虚拟化环境。</li></ul></li><li><strong>透明性</strong>：<ul><li>VXLAN 技术与现有的物理网络无关，物理网络不需要支持 VXLAN，便可以通过 Overlay 网络将虚拟网络连接在一起。VXLAN 是在数据链路层进行封装，物理网络的设备不需要进行特殊处理。</li></ul></li></ol><h4 id=缺点-1>缺点：
<a class=anchor href=#%e7%bc%ba%e7%82%b9-1>#</a></h4><ol><li><strong>性能开销</strong>：<ul><li>像 IP-in-IP 一样，VXLAN 也会带来封装和解封装的性能开销。封装会使得每个数据包的大小增加，从而带来额外的 CPU 和网络带宽消耗。</li></ul></li><li><strong>复杂性</strong>：<ul><li>配置和管理 VXLAN 网络相对复杂，需要确保每个 VXLAN 隧道的配置正确，同时要管理虚拟网络标识符（VNI）。这可能对操作人员的技能要求更高。</li></ul></li><li><strong>调试难度</strong>：<ul><li>由于数据包在通过物理网络传输时被封装，网络故障排查和调试可能变得更加困难。你需要监控 VXLAN 隧道的健康状况，并对封装数据包进行解包和分析。</li></ul></li><li><strong>广播流量</strong>：<ul><li>VXLAN 在某些情况下可能会引入广播流量的开销，尤其是在需要广播的场景（如 ARP、DNS 等）。这些流量需要在 VXLAN 网络中传播，增加了网络负担。</li></ul></li></ol><h3 id=vxlan-与-ip-in-ip-的对比>VXLAN 与 IP-in-IP 的对比
<a class=anchor href=#vxlan-%e4%b8%8e-ip-in-ip-%e7%9a%84%e5%af%b9%e6%af%94>#</a></h3><ul><li><strong>封装方式</strong>：<ul><li><strong>IP-in-IP</strong>：仅使用 IP 封装数据包，封装的开销较小。</li><li><strong>VXLAN</strong>：使用 VXLAN 头和 UDP 封装，封装开销较大，因为每个数据包都需要添加额外的 VXLAN 和 UDP 头。</li></ul></li><li><strong>可扩展性</strong>：<ul><li><strong>VXLAN</strong>：支持最多 16M 个虚拟网络（VNI），适合大规模集群和大规模虚拟网络。</li><li><strong>IP-in-IP</strong>：相较之下，支持的虚拟网络数量较少，适用于较小的集群或环境。</li></ul></li><li><strong>跨数据中心支持</strong>：<ul><li><strong>VXLAN</strong>：具有较强的跨数据中心支持能力，适合于多数据中心部署。</li><li><strong>IP-in-IP</strong>：可以支持跨数据中心，但由于 VXLAN 的设计，它通常在大规模、跨数据中心环境中表现更好。</li></ul></li></ul><h3 id=使用场景-1>使用场景
<a class=anchor href=#%e4%bd%bf%e7%94%a8%e5%9c%ba%e6%99%af-1>#</a></h3><ul><li><strong>跨数据中心和多地域部署</strong>：<ul><li>VXLAN 是支持跨多个数据中心或不同网络环境的容器网络的理想选择。由于其较强的可扩展性和支持大量虚拟网络，适合大规模容器化应用和多云环境。</li></ul></li><li><strong>大规模容器集群</strong>：<ul><li>对于需要支持数千甚至数万个容器的大规模 Kubernetes 集群，VXLAN 提供了更好的隔离性和可扩展性。</li></ul></li><li><strong>有高度网络隔离需求的环境</strong>：<ul><li>VXLAN 能够提供强大的网络隔离能力，确保不同的应用、环境或租户之间的流量完全隔离，适合多租户环境和安全性要求较高的场景。</li></ul></li></ul><h3 id=配置-vxlan-模式>配置 VXLAN 模式
<a class=anchor href=#%e9%85%8d%e7%bd%ae-vxlan-%e6%a8%a1%e5%bc%8f>#</a></h3><p>Calico 的 VXLAN 模式通常通过修改配置来启用。通过修改 <strong>calicoctl</strong> 工具中的设置，或者在 Kubernetes 部署时选择启用 VXLAN。</p><p>例如，要启用 VXLAN 模式，可以在 Calico 配置文件中进行以下设置：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>calicoctl config set vxlan_enabled true
</span></span></code></pre></div><p>这会启用 VXLAN 模式，Pod 之间的跨节点流量将通过 VXLAN 隧道进行封装和传输。</p><h3 id=总结-18>总结
<a class=anchor href=#%e6%80%bb%e7%bb%93-18>#</a></h3><ul><li><strong>VXLAN</strong> 是一种用于在分布式数据中心中实现 Overlay 网络的技术，能够提供网络隔离、跨数据中心连接和大规模虚拟网络的支持。</li><li><strong>Calico 的 VXLAN 模式</strong>允许通过 VXLAN 隧道来实现跨节点、跨数据中心的 Pod 之间的通信，适用于需要高可扩展性和高隔离性的场景。</li><li>与 <strong>IP-in-IP</strong> 相比，VXLAN 提供更好的扩展性和支持更多虚拟网络，但也带来了更大的性能开销和更复杂的配置管理。</li></ul><h2 id=network-policy使用场景>Network Policy使用场景
<a class=anchor href=#network-policy%e4%bd%bf%e7%94%a8%e5%9c%ba%e6%99%af>#</a></h2><p><strong>Network Policy</strong> 是 Kubernetes 中的一种功能，允许你定义和控制 Pod 之间以及 Pod 和外部世界之间的网络通信规则。它通过定义访问控制规则来限制哪些流量可以进入或离开 Pod，从而提高了 Kubernetes 集群的安全性和可控性。</p><h3 id=network-policy-的使用场景>Network Policy 的使用场景
<a class=anchor href=#network-policy-%e7%9a%84%e4%bd%bf%e7%94%a8%e5%9c%ba%e6%99%af>#</a></h3><h4 id=1-多租户环境>1. <strong>多租户环境</strong>
<a class=anchor href=#1-%e5%a4%9a%e7%a7%9f%e6%88%b7%e7%8e%af%e5%a2%83>#</a></h4><ul><li>在 Kubernetes 集群中，多个团队或租户可能共享同一个集群资源。在这种环境下，网络策略可以帮助确保不同租户的资源相互隔离，防止一个租户的应用程序与另一个租户的应用程序进行不必要的通信。</li><li><strong>示例</strong>：你可以创建一个网络策略来禁止不同命名空间中的 Pod 之间的通信，只允许它们访问共享的服务，如数据库、API 网关等。</li></ul><h4 id=2-提高安全性>2. <strong>提高安全性</strong>
<a class=anchor href=#2-%e6%8f%90%e9%ab%98%e5%ae%89%e5%85%a8%e6%80%a7>#</a></h4><ul><li>网络策略允许你定义细粒度的安全规则，限制 Pod 之间的通信。通过阻止不必要的流量，减少了攻击面，降低了潜在的安全风险。</li><li><strong>示例</strong>：只允许来自特定命名空间或标签的流量访问某个敏感的服务或数据库，从而确保即使某些 Pod 被攻破，攻击者无法轻易访问其他关键服务。</li></ul><h4 id=3-限制-pod-外部访问>3. <strong>限制 Pod 外部访问</strong>
<a class=anchor href=#3-%e9%99%90%e5%88%b6-pod-%e5%a4%96%e9%83%a8%e8%ae%bf%e9%97%ae>#</a></h4><ul><li>在某些场景下，可能希望限制某些 Pod 只能在集群内部通信，而不允许它们访问外部网络或互联网。这有助于防止不必要的外部依赖或数据泄露。</li><li><strong>示例</strong>：限制一个数据库 Pod 只允许访问内部流量，而不允许从外部网络发起连接。</li></ul><h4 id=4-微服务架构中的通信控制>4. <strong>微服务架构中的通信控制</strong>
<a class=anchor href=#4-%e5%be%ae%e6%9c%8d%e5%8a%a1%e6%9e%b6%e6%9e%84%e4%b8%ad%e7%9a%84%e9%80%9a%e4%bf%a1%e6%8e%a7%e5%88%b6>#</a></h4><ul><li>在微服务架构中，各个服务（Pod）通常需要进行通信。通过 Network Policy，可以确保不同服务之间的通信是有规则的，避免不必要的服务之间的流量。</li><li><strong>示例</strong>：你可以限制微服务之间的流量，使得只有认证过的服务（如 API 网关）可以访问后台数据库服务，而其他服务无法直接访问数据库。</li></ul><h4 id=5-网络隔离与分段>5. <strong>网络隔离与分段</strong>
<a class=anchor href=#5-%e7%bd%91%e7%bb%9c%e9%9a%94%e7%a6%bb%e4%b8%8e%e5%88%86%e6%ae%b5>#</a></h4><ul><li>Kubernetes 中的 Network Policy 可用于将网络流量隔离到不同的安全区域或服务段。这对于不同网络段、不同权限级别的服务进行隔离是非常有用的。</li><li><strong>示例</strong>：在同一个集群中，前端应用（如 Web 服务器）可以通过网络策略与后端数据库进行通信，而与其他服务（如日志收集或监控服务）之间的流量则被限制。</li></ul><h4 id=6-控制服务间的流量>6. <strong>控制服务间的流量</strong>
<a class=anchor href=#6-%e6%8e%a7%e5%88%b6%e6%9c%8d%e5%8a%a1%e9%97%b4%e7%9a%84%e6%b5%81%e9%87%8f>#</a></h4><ul><li>在微服务架构中，服务之间可能有多个交互方式，例如 REST、gRPC 或消息队列等。通过 Network Policy，可以明确规定每种通信方式的流量来源和目标，从而提升系统的安全性和可维护性。</li><li><strong>示例</strong>：控制 Web 服务与后端应用程序之间的通信，只允许 Web 服务访问后端数据库的某些端点，而禁止访问其他敏感服务。</li></ul><h4 id=7-限制暴露到外部的服务>7. <strong>限制暴露到外部的服务</strong>
<a class=anchor href=#7-%e9%99%90%e5%88%b6%e6%9a%b4%e9%9c%b2%e5%88%b0%e5%a4%96%e9%83%a8%e7%9a%84%e6%9c%8d%e5%8a%a1>#</a></h4><ul><li>对于某些服务，可能需要限制其在内部通信的同时，避免暴露到集群外部。Network Policy 使得仅特定的 Pod 或 IP 地址能够访问这些服务，增强了集群的边界安全性。</li><li><strong>示例</strong>：你可以创建一个策略，禁止数据库 Pod 从外部直接访问，或者只允许从特定的客户端应用程序访问。</li></ul><h4 id=8-防止恶意流量>8. <strong>防止恶意流量</strong>
<a class=anchor href=#8-%e9%98%b2%e6%ad%a2%e6%81%b6%e6%84%8f%e6%b5%81%e9%87%8f>#</a></h4><ul><li>通过 Network Policy，可以阻止恶意流量进入集群。例如，可以防止不受信任的外部流量进入集群，避免不必要的暴露。</li><li><strong>示例</strong>：如果你希望限制只有特定的 IP 地址段能够访问服务，可以通过网络策略阻止其他不信任的流量。</li></ul><h3 id=示例定义-network-policy>示例：定义 Network Policy
<a class=anchor href=#%e7%a4%ba%e4%be%8b%e5%ae%9a%e4%b9%89-network-policy>#</a></h3><p>以下是一个简单的示例，定义一个只允许特定命名空间内的 Pod 之间通信的网络策略：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>networking.k8s.io/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>NetworkPolicy</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>allow-same-namespace</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>podSelector</span>: {}
</span></span><span style=display:flex><span>  <span style=color:#f92672>ingress</span>:
</span></span><span style=display:flex><span>  - <span style=color:#f92672>from</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>podSelector</span>: {}
</span></span><span style=display:flex><span>      <span style=color:#f92672>namespaceSelector</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>matchLabels</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>name</span>: <span style=color:#ae81ff>mynamespace</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>policyTypes</span>:
</span></span><span style=display:flex><span>  - <span style=color:#ae81ff>Ingress</span>
</span></span></code></pre></div><p>在这个示例中：</p><ul><li><strong>podSelector</strong>：空值表示对所有 Pod 都应用该策略。</li><li><strong>ingress</strong>：规则表明只有来自 <code>mynamespace</code> 命名空间中的 Pod 才能访问这些 Pod。</li><li><strong>policyTypes</strong>：表示这条策略是对 <code>Ingress</code> 流量进行控制。</li></ul><h3 id=网络策略的限制和注意事项>网络策略的限制和注意事项
<a class=anchor href=#%e7%bd%91%e7%bb%9c%e7%ad%96%e7%95%a5%e7%9a%84%e9%99%90%e5%88%b6%e5%92%8c%e6%b3%a8%e6%84%8f%e4%ba%8b%e9%a1%b9>#</a></h3><ol><li><strong>默认拒绝所有流量</strong>：<ul><li>如果没有定义任何网络策略，Kubernetes 集群中的 Pod 默认可以自由通信。然而，一旦定义了网络策略，Kubernetes 会应用 &ldquo;默认拒绝&rdquo; 规则，即默认会阻止所有流量，除非明确允许。</li></ul></li><li><strong>只支持支持 Network Policy 的 CNI 插件</strong>：<ul><li>不是所有 CNI 插件都支持 Kubernetes 的 Network Policy。需要使用支持 Network Policy 的插件，如 <strong>Calico</strong>、<strong>Cilium</strong>、<strong>Weave</strong> 等。</li></ul></li><li><strong>只能控制 Pod 之间的流量</strong>：<ul><li>网络策略只会影响 Pod 之间的流量。它不能直接控制集群外部访问（如 LoadBalancer 服务暴露的端口）。因此，外部流量的控制需要结合 Kubernetes 的 Service 和防火墙规则。</li></ul></li><li><strong>Ingress 和 Egress 流量</strong>：<ul><li>Network Policy 目前主要支持控制 <strong>Ingress</strong>（入站流量）和 <strong>Egress</strong>（出站流量）。你可以通过定义不同的规则来控制流量的进入和离开 Pod。</li></ul></li></ol><h3 id=总结-19>总结
<a class=anchor href=#%e6%80%bb%e7%bb%93-19>#</a></h3><p><strong>Network Policy</strong> 提供了一种强大的方式来控制 Kubernetes 集群内 Pod 之间的通信，特别是在多租户环境、微服务架构以及对安全性要求较高的场景中非常有用。它能够帮助你：</p><ul><li>实现网络隔离和分段；</li><li>提高集群的安全性；</li><li>细粒度地控制服务间的流量；</li><li>防止恶意流量进入集群。</li></ul><p>为了确保网络安全性，合理设计和实施 Network Policy 是 Kubernetes 安全架构中的重要一环。</p><h2 id=kubectl-exec-实现的原理>kubectl exec 实现的原理
<a class=anchor href=#kubectl-exec-%e5%ae%9e%e7%8e%b0%e7%9a%84%e5%8e%9f%e7%90%86>#</a></h2><p><code>kubectl exec</code> 是一个常用的命令，用于在 Kubernetes 集群中的 Pod 内执行命令。它允许用户直接在容器内执行交互式命令或获取容器的输出信息。理解 <code>kubectl exec</code> 的原理涉及 Kubernetes 中 Pod 和容器的管理机制，尤其是如何通过 API Server 与容器进行通信。</p><h3 id=kubectl-exec-原理概述><code>kubectl exec</code> 原理概述
<a class=anchor href=#kubectl-exec-%e5%8e%9f%e7%90%86%e6%a6%82%e8%bf%b0>#</a></h3><ol><li><strong>命令发起和处理</strong>：<ul><li>当用户在终端中运行 <code>kubectl exec</code> 命令时，<code>kubectl</code> 客户端会构造一个请求，发送到 Kubernetes 集群的 API Server。</li><li>API Server 会验证请求的合法性（例如，用户是否具有执行命令的权限，Pod 是否存在，容器是否运行等），并且检查请求中指定的 Pod 和容器是否匹配。</li></ul></li><li><strong>通过 API Server 访问容器</strong>：<ul><li>API Server 接收到 <code>kubectl exec</code> 请求后，它会调用容器运行时接口（如 Docker、containerd 等）来执行相应的命令。</li><li>Kubernetes 并不会直接执行容器中的命令，而是通过 API Server 中的 <strong>PodProxy</strong> 或 <strong>ContainerRuntimeInterface (CRI)</strong> 与容器运行时进行交互。</li><li><strong>PodProxy</strong> 是 Kubernetes 用来与容器交互的组件，它可以通过 HTTP 和 WebSocket 协议向容器发送指令。</li></ul></li><li><strong>WebSocket 或 HTTP 连接</strong>：<ul><li><code>kubectl exec</code> 使用 <strong>WebSocket</strong> 协议与容器建立一个双向通信通道。这使得客户端（<code>kubectl</code>）与容器之间可以进行实时交互。</li><li>如果执行的命令是交互式的（如启动一个 shell），则 <code>kubectl</code> 会保持与容器的 WebSocket 连接，用于接收和发送数据。</li><li>如果是非交互式命令（例如 <code>kubectl exec pod -- ls</code>），则命令会在容器内执行并返回输出，执行完毕后连接断开。</li></ul></li><li><strong>容器执行命令</strong>：<ul><li>在 WebSocket 连接建立之后，API Server 会通过容器运行时启动指定的命令。容器运行时负责在 Pod 的容器内执行该命令，并将命令的标准输出（stdout）和标准错误（stderr）发送回 WebSocket 通道。</li><li>容器的标准输入（stdin）也通过 WebSocket 连接传递，允许用户与容器进行交互。</li></ul></li><li><strong>客户端与容器交互</strong>：<ul><li>对于交互式的命令，<code>kubectl</code> 客户端将会将用户的输入通过 WebSocket 连接发送到容器，同时接收容器的输出并显示在终端中。</li><li>对于非交互式命令，<code>kubectl exec</code> 会等待命令执行完成，并输出执行结果。</li></ul></li><li><strong>WebSocket 通信的终止</strong>：<ul><li>当命令执行完毕，WebSocket 连接关闭，<code>kubectl</code> 会将容器的退出码返回给用户。</li><li>这时，<code>kubectl</code> 命令会终止并返回命令执行的结果或错误。</li></ul></li></ol><h3 id=kubectl-exec-的组件与交互><code>kubectl exec</code> 的组件与交互
<a class=anchor href=#kubectl-exec-%e7%9a%84%e7%bb%84%e4%bb%b6%e4%b8%8e%e4%ba%a4%e4%ba%92>#</a></h3><ol><li><strong>kubectl 客户端</strong>：<ul><li>发起 <code>exec</code> 请求并处理用户输入/输出，管理与 API Server 的连接。</li></ul></li><li><strong>API Server</strong>：<ul><li>处理请求并进行认证、授权检查，然后通过 PodProxy 或 CRI 调用容器运行时来执行命令。</li></ul></li><li><strong>PodProxy</strong>：<ul><li>Kubernetes 内部组件，负责将请求转发到正确的容器运行时，并与容器进行通信。</li></ul></li><li><strong>容器运行时（如 Docker、containerd 等）</strong>：<ul><li>执行命令，返回命令的输出并将其传递给 API Server。</li></ul></li><li><strong>WebSocket 连接</strong>：<ul><li>用于实现客户端和容器之间的双向实时通信，特别适用于交互式命令。</li></ul></li></ol><h3 id=kubectl-exec-使用示例><code>kubectl exec</code> 使用示例
<a class=anchor href=#kubectl-exec-%e4%bd%bf%e7%94%a8%e7%a4%ba%e4%be%8b>#</a></h3><h4 id=1-执行交互式命令如进入容器内的-shell>1. 执行交互式命令（如进入容器内的 Shell）：
<a class=anchor href=#1-%e6%89%a7%e8%a1%8c%e4%ba%a4%e4%ba%92%e5%bc%8f%e5%91%bd%e4%bb%a4%e5%a6%82%e8%bf%9b%e5%85%a5%e5%ae%b9%e5%99%a8%e5%86%85%e7%9a%84-shell>#</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl exec -it &lt;pod-name&gt; -- /bin/bash
</span></span></code></pre></div><ul><li><code>-it</code> 选项表示启用交互式终端，<code>/bin/bash</code> 是容器内的命令，用户可以与容器内的 shell 进行交互。</li></ul><h4 id=2-执行非交互式命令>2. 执行非交互式命令：
<a class=anchor href=#2-%e6%89%a7%e8%a1%8c%e9%9d%9e%e4%ba%a4%e4%ba%92%e5%bc%8f%e5%91%bd%e4%bb%a4>#</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl exec &lt;pod-name&gt; -- ls /app
</span></span></code></pre></div><ul><li>这个命令会在 Pod 内执行 <code>ls /app</code>，并将输出返回给用户。</li></ul><h4 id=3-执行命令并获取输出>3. 执行命令并获取输出：
<a class=anchor href=#3-%e6%89%a7%e8%a1%8c%e5%91%bd%e4%bb%a4%e5%b9%b6%e8%8e%b7%e5%8f%96%e8%be%93%e5%87%ba>#</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl exec &lt;pod-name&gt; -- cat /etc/hostname
</span></span></code></pre></div><ul><li>该命令会在容器内执行 <code>cat</code> 命令，并将结果（容器的主机名）返回给用户。</li></ul><h3 id=安全与权限控制>安全与权限控制
<a class=anchor href=#%e5%ae%89%e5%85%a8%e4%b8%8e%e6%9d%83%e9%99%90%e6%8e%a7%e5%88%b6>#</a></h3><p>Kubernetes 使用 RBAC（角色基础访问控制）来控制谁可以执行 <code>kubectl exec</code>。如果用户没有相应的权限，执行 <code>exec</code> 操作将被拒绝。</p><p>通常，RBAC 会为不同的角色定义权限，例如：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Role</span>
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>rbac.authorization.k8s.io/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>default</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>exec-role</span>
</span></span><span style=display:flex><span><span style=color:#f92672>rules</span>:
</span></span><span style=display:flex><span>- <span style=color:#f92672>apiGroups</span>: [<span style=color:#e6db74>&#34;&#34;</span>]
</span></span><span style=display:flex><span>  <span style=color:#f92672>resources</span>: [<span style=color:#e6db74>&#34;pods/exec&#34;</span>]
</span></span><span style=display:flex><span>  <span style=color:#f92672>verbs</span>: [<span style=color:#e6db74>&#34;create&#34;</span>]
</span></span></code></pre></div><p>在这个例子中，<code>create</code> 动作允许用户在 Pod 中执行命令。</p><h3 id=总结-20>总结
<a class=anchor href=#%e6%80%bb%e7%bb%93-20>#</a></h3><p><code>kubectl exec</code> 的实现原理涉及以下几个步骤：</p><ol><li><strong>客户端发起请求</strong>，并通过 API Server 与 Kubernetes 集群通信。</li><li><strong>API Server 进行认证和授权检查</strong>，并通过 PodProxy 或容器运行时接口（CRI）与容器进行交互。</li><li><strong>WebSocket 连接</strong> 用于容器与客户端之间的双向实时通信。</li><li><strong>容器执行命令</strong>，并将标准输出、标准错误返回给客户端。</li></ol><p>通过这些机制，<code>kubectl exec</code> 提供了在 Kubernetes 集群中灵活执行命令和与容器进行交互的功能。</p><h2 id=cgroup中限制cpu的方式有哪些>cgroup中限制CPU的方式有哪些
<a class=anchor href=#cgroup%e4%b8%ad%e9%99%90%e5%88%b6cpu%e7%9a%84%e6%96%b9%e5%bc%8f%e6%9c%89%e5%93%aa%e4%ba%9b>#</a></h2><p>在 <strong>cgroup</strong>（控制组）中，限制 <strong>CPU</strong> 资源的方式主要有以下几种。这些方式帮助管理员在系统中对进程或容器的 CPU 资源进行细粒度的管理和控制，确保在多任务和多容器环境中，系统资源能够公平地分配，避免某些进程或容器占用过多的 CPU。</p><h3 id=1-cpu-时间限制cpucfs_>1. <strong>CPU 时间限制（cpu.cfs_quota_us）</strong>
<a class=anchor href=#1-cpu-%e6%97%b6%e9%97%b4%e9%99%90%e5%88%b6cpucfs_>#</a></h3><ul><li><strong>描述</strong>：<code>cpu.cfs_quota_us</code> 用于限制某个 cgroup 在指定时间周期内（通过 <code>cpu.cfs_period_us</code> 设置）可以使用的最大 CPU 时间。</li><li><strong>原理</strong>：系统每隔一段时间（<code>cpu.cfs_period_us</code>，默认 100ms）会重置并统计每个 cgroup 使用的 CPU 时间。<code>cpu.cfs_quota_us</code> 设置了一个限制值，表示该 cgroup 在一个周期内可以使用的最大 CPU 时间。如果使用时间超过这个值，其他任务将会被暂停，直到下一个周期开始。</li></ul><p><strong>使用示例</strong>：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># 限制进程在 100ms 的时间周期内最多只能使用 50ms 的 CPU 时间</span>
</span></span><span style=display:flex><span>echo <span style=color:#ae81ff>50000</span> &gt; /sys/fs/cgroup/cpu/cpu.cfs_quota_us
</span></span><span style=display:flex><span>echo <span style=color:#ae81ff>100000</span> &gt; /sys/fs/cgroup/cpu/cpu.cfs_period_us
</span></span></code></pre></div><ul><li><strong>默认值</strong>：<code>cpu.cfs_quota_us</code> 默认为 <code>-1</code>，表示没有限制。<code>cpu.cfs_period_us</code> 默认为 <code>100000</code>（100ms）。</li><li><strong>作用</strong>：如果你设置了 <code>cpu.cfs_quota_us = 50000</code>，那么在每个周期（100ms）中，该 cgroup 内的进程最多只能使用 50ms 的 CPU 时间。</li></ul><h3 id=2-cpu-核心限制cpucpus>2. <strong>CPU 核心限制（cpu.cpus）</strong>
<a class=anchor href=#2-cpu-%e6%a0%b8%e5%bf%83%e9%99%90%e5%88%b6cpucpus>#</a></h3><ul><li><strong>描述</strong>：通过设置 <code>cpu.cpus</code>，可以限制 cgroup 内的进程只能在指定的 CPU 核心上运行。</li><li><strong>原理</strong>：此限制可以将进程固定在指定的 CPU 核心上执行，避免跨多个 CPU 核心进行调度，从而达到更精确的资源控制。</li></ul><p><strong>使用示例</strong>：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># 将进程限制在 CPU 核心 0 和 1 上运行</span>
</span></span><span style=display:flex><span>echo 0,1 &gt; /sys/fs/cgroup/cpu/cpu.cpus
</span></span></code></pre></div><ul><li><strong>作用</strong>：通过设置 <code>cpu.cpus = 0,1</code>，该 cgroup 中的进程只能在 CPU 核心 0 和 1 上执行，避免了进程在其他核心上调度，优化了性能。</li></ul><h3 id=3-cpu-权重cpushares>3. <strong>CPU 权重（cpu.shares）</strong>
<a class=anchor href=#3-cpu-%e6%9d%83%e9%87%8dcpushares>#</a></h3><ul><li><strong>描述</strong>：<code>cpu.shares</code> 用于定义进程/容器相对于其他进程/容器的 CPU 权重。它不会限制 CPU 使用的时间或数量，而是通过控制进程的 CPU 分配比例来实现资源调度。</li><li><strong>原理</strong>：<code>cpu.shares</code> 是一个相对的值，用来表示该 cgroup 在没有 CPU 限制时的调度优先级。它与其他 cgroup 中的进程竞争 CPU 时间时，系统会按照比例进行分配。例如，如果两个 cgroup 的 <code>cpu.shares</code> 分别为 512 和 1024，那么在 CPU 资源不足时，第二个 cgroup 会获得两倍于第一个 cgroup 的 CPU 时间。</li></ul><p><strong>使用示例</strong>：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># 设置进程的 CPU 权重为 512（默认值为 1024）</span>
</span></span><span style=display:flex><span>echo <span style=color:#ae81ff>512</span> &gt; /sys/fs/cgroup/cpu/cpu.shares
</span></span></code></pre></div><ul><li><strong>默认值</strong>：<code>cpu.shares</code> 默认为 1024。</li><li><strong>作用</strong>：如果两个进程的 <code>cpu.shares</code> 分别为 1024 和 512，则第一个进程的 CPU 时间是第二个进程的两倍。</li></ul><h3 id=4-cpu-限制优先级cpurt_>4. <strong>CPU 限制优先级（cpu.rt_runtime_us）</strong>
<a class=anchor href=#4-cpu-%e9%99%90%e5%88%b6%e4%bc%98%e5%85%88%e7%ba%a7cpurt_>#</a></h3><ul><li><strong>描述</strong>：<code>cpu.rt_runtime_us</code> 用于限制实时进程（例如低延迟要求的进程）的最大运行时间。通常用于实时调度。</li><li><strong>原理</strong>：实时进程通常需要比普通进程更精确的 CPU 调度。通过设置 <code>cpu.rt_runtime_us</code>，可以限制该 cgroup 中的实时进程最多可使用的 CPU 时间（单位是微秒）。如果该进程在一个周期内使用的时间超过这个值，调度器会限制进程的运行直到下一个周期。</li></ul><p><strong>使用示例</strong>：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># 设置实时进程最大可用的 CPU 时间为 95000 微秒</span>
</span></span><span style=display:flex><span>echo <span style=color:#ae81ff>95000</span> &gt; /sys/fs/cgroup/cpu/cpu.rt_runtime_us
</span></span></code></pre></div><ul><li><strong>默认值</strong>：<code>cpu.rt_runtime_us</code> 默认为 <code>950000</code>（即 950ms）。</li></ul><h3 id=5-cpu-子系统的-cfscompletely-fair-scheduler策略>5. <strong>CPU 子系统的 CFS（Completely Fair Scheduler）策略</strong>
<a class=anchor href=#5-cpu-%e5%ad%90%e7%b3%bb%e7%bb%9f%e7%9a%84-cfscompletely-fair-scheduler%e7%ad%96%e7%95%a5>#</a></h3><ul><li><strong>描述</strong>：CFS 是 Linux 内核的调度策略，它会根据任务的优先级、资源需求等进行公平调度。通过 cgroup 控制 CFS 的调度策略，可以控制进程/容器的 CPU 使用情况。</li><li><strong>控制项</strong>：<ul><li><strong>cpu.cfs_period_us</strong>：控制 CPU 时间片的周期（单位：微秒）。</li><li><strong>cpu.cfs_quota_us</strong>：控制进程可用的最大 CPU 时间。</li></ul></li></ul><p><strong>使用场景</strong>：</p><ul><li>适用于需要对 CPU 使用进行精细化控制的场景，如容器化环境中，防止单个容器占用过多的 CPU 资源。</li></ul><h3 id=6-cpu-控制组调度器>6. <strong>CPU 控制组调度器（<code>cpuset</code>）</strong>
<a class=anchor href=#6-cpu-%e6%8e%a7%e5%88%b6%e7%bb%84%e8%b0%83%e5%ba%a6%e5%99%a8>#</a></h3><ul><li><strong>描述</strong>：<code>cpuset</code> 是 cgroup 的一个子系统，它允许管理员将进程限制在特定的 CPU 核心上运行，同时也可以限制内存的使用。与 <code>cpu.cpus</code> 类似，<code>cpuset</code> 控制进程只能在特定的 CPU 核心上调度执行。</li></ul><p><strong>使用示例</strong>：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># 设置容器只能使用 CPU 核心 0 和 1</span>
</span></span><span style=display:flex><span>echo 0,1 &gt; /sys/fs/cgroup/cpuset/cpuset.cpus
</span></span></code></pre></div><ul><li><strong>作用</strong>：通过 <code>cpuset</code> 限制 CPU 核心，防止进程在多个 CPU 核心之间迁移，从而实现更高效的资源使用。</li></ul><hr><h3 id=总结-21>总结
<a class=anchor href=#%e6%80%bb%e7%bb%93-21>#</a></h3><p>通过 cgroup，用户可以从多个角度对进程或容器的 CPU 使用进行限制和优化，常见的方式包括：</p><ol><li><strong>CPU 时间限制</strong>（<code>cpu.cfs_quota_us</code> 和 <code>cpu.cfs_period_us</code>）。</li><li><strong>CPU 核心绑定</strong>（<code>cpu.cpus</code>）。</li><li><strong>CPU 权重</strong>（<code>cpu.shares</code>）。</li><li><strong>实时 CPU 限制</strong>（<code>cpu.rt_runtime_us</code>）。</li><li><strong>控制进程在指定 CPU 上运行</strong>（<code>cpuset</code>）。</li></ol><p>这些方式可以帮助在多任务、多容器环境下合理分配 CPU 资源，提高集群的整体性能和稳定性。</p><h2 id=kubeconfig存放内容>kubeconfig存放内容
<a class=anchor href=#kubeconfig%e5%ad%98%e6%94%be%e5%86%85%e5%ae%b9>#</a></h2><p><code>kubeconfig</code> 文件是 Kubernetes 配置文件，存储了与 Kubernetes 集群进行交互所需要的所有信息。<code>kubeconfig</code> 文件的内容包括了集群信息、认证信息和上下文信息，允许 <code>kubectl</code> 等工具与 Kubernetes API Server 进行通信。一个典型的 <code>kubeconfig</code> 文件可以包含多个集群的配置，以及如何连接到每个集群的详细信息。</p><h3 id=kubeconfig-文件的结构><code>kubeconfig</code> 文件的结构
<a class=anchor href=#kubeconfig-%e6%96%87%e4%bb%b6%e7%9a%84%e7%bb%93%e6%9e%84>#</a></h3><p><code>kubeconfig</code> 文件通常是一个 YAML 格式的文件，包含以下几部分：</p><ol><li><strong>apiVersion</strong>：文件的 API 版本，通常是 <code>v1</code>。</li><li><strong>clusters</strong>：描述 Kubernetes 集群的信息。每个集群信息包含集群的名称、API Server 地址以及（可选的）TLS 配置。</li><li><strong>contexts</strong>：定义不同的上下文，每个上下文指定了用户、集群和命名空间的组合。</li><li><strong>current-context</strong>：当前使用的上下文，指定默认连接的集群、用户等。</li><li><strong>users</strong>：存储与 Kubernetes 集群通信所需要的认证信息，通常包括证书、令牌或用户名密码等。</li><li><strong>preferences</strong>：一些用户的个人偏好配置（通常不常用）。</li></ol><h3 id=kubeconfig-文件示例><code>kubeconfig</code> 文件示例
<a class=anchor href=#kubeconfig-%e6%96%87%e4%bb%b6%e7%a4%ba%e4%be%8b>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Config</span>
</span></span><span style=display:flex><span><span style=color:#f92672>clusters</span>:
</span></span><span style=display:flex><span>- <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-cluster</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>cluster</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>server</span>: <span style=color:#ae81ff>https://k8s-api-server.example.com:6443</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>certificate-authority-data</span>: <span style=color:#ae81ff>&lt;certificate-data&gt; </span> <span style=color:#75715e># 或者使用 certificate-authority</span>
</span></span><span style=display:flex><span><span style=color:#f92672>users</span>:
</span></span><span style=display:flex><span>- <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-user</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>user</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>client-certificate-data</span>: <span style=color:#ae81ff>&lt;client-cert-data&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>client-key-data</span>: <span style=color:#ae81ff>&lt;client-key-data&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>token</span>: <span style=color:#ae81ff>&lt;token&gt; </span> <span style=color:#75715e># 如果使用令牌认证</span>
</span></span><span style=display:flex><span><span style=color:#f92672>contexts</span>:
</span></span><span style=display:flex><span>- <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-context</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>context</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>cluster</span>: <span style=color:#ae81ff>my-cluster</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>user</span>: <span style=color:#ae81ff>my-user</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>default </span> <span style=color:#75715e># 可选</span>
</span></span><span style=display:flex><span><span style=color:#f92672>current-context</span>: <span style=color:#ae81ff>my-context</span>
</span></span><span style=display:flex><span><span style=color:#f92672>preferences</span>: {}
</span></span></code></pre></div><h3 id=kubeconfig-文件的关键字段><code>kubeconfig</code> 文件的关键字段
<a class=anchor href=#kubeconfig-%e6%96%87%e4%bb%b6%e7%9a%84%e5%85%b3%e9%94%ae%e5%ad%97%e6%ae%b5>#</a></h3><h4 id=1-clusters>1. <strong>clusters</strong>
<a class=anchor href=#1-clusters>#</a></h4><ul><li><strong>name</strong>：集群的名称，可以是任意名称，通常与集群名称对应。</li><li><strong>cluster.server</strong>：API Server 的地址，通常是 HTTPS 协议。它是 Kubernetes 集群的入口点，<code>kubectl</code> 会连接到这个地址。</li><li><strong>cluster.certificate-authority-data</strong>：用于验证 API Server 的证书的证书数据（Base64 编码）。可以替代 <code>certificate-authority</code> 字段，后者是一个指向证书文件的路径。</li></ul><h4 id=2-users>2. <strong>users</strong>
<a class=anchor href=#2-users>#</a></h4><ul><li><strong>name</strong>：用户的名称，可以是任意名称，通常与认证方式（如用户名、证书或令牌）对应。</li><li><strong>user.client-certificate-data</strong>：Base64 编码的客户端证书数据，用于通过客户端证书进行认证。</li><li><strong>user.client-key-data</strong>：Base64 编码的客户端密钥数据，配合客户端证书使用。</li><li><strong>user.token</strong>：如果使用 Bearer Token 认证，存储在这里。通常用于基于令牌的认证。</li><li><strong>user.username</strong> 和 <strong>user.password</strong>：如果使用 HTTP 基本认证，用户名和密码可以在这里配置。</li></ul><h4 id=3-contexts>3. <strong>contexts</strong>
<a class=anchor href=#3-contexts>#</a></h4><ul><li><strong>name</strong>：上下文的名称，可以根据使用的集群和用户来命名。</li><li><strong>context.cluster</strong>：指定使用的集群名称，引用 <code>clusters</code> 部分中的集群。</li><li><strong>context.user</strong>：指定使用的用户名称，引用 <code>users</code> 部分中的用户。</li><li><strong>context.namespace</strong>：指定默认的命名空间。可选，通常是 <code>default</code>。</li></ul><h4 id=4-current-context>4. <strong>current-context</strong>
<a class=anchor href=#4-current-context>#</a></h4><ul><li>这个字段指定当前要使用的上下文名称。<code>kubectl</code> 将根据这个上下文连接到指定的集群并使用相应的认证信息。</li></ul><h4 id=5-preferences>5. <strong>preferences</strong>
<a class=anchor href=#5-preferences>#</a></h4><ul><li>一些用户偏好配置，通常为空。</li></ul><h3 id=示例解析>示例解析
<a class=anchor href=#%e7%a4%ba%e4%be%8b%e8%a7%a3%e6%9e%90>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Config</span>
</span></span><span style=display:flex><span><span style=color:#f92672>clusters</span>:
</span></span><span style=display:flex><span>- <span style=color:#f92672>name</span>: <span style=color:#ae81ff>prod-cluster</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>cluster</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>server</span>: <span style=color:#ae81ff>https://prod-api-server.example.com:6443</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>certificate-authority-data</span>: <span style=color:#ae81ff>&lt;prod-cert-data&gt;</span>
</span></span><span style=display:flex><span><span style=color:#f92672>users</span>:
</span></span><span style=display:flex><span>- <span style=color:#f92672>name</span>: <span style=color:#ae81ff>prod-user</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>user</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>client-certificate-data</span>: <span style=color:#ae81ff>&lt;prod-cert-data&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>client-key-data</span>: <span style=color:#ae81ff>&lt;prod-key-data&gt;</span>
</span></span><span style=display:flex><span><span style=color:#f92672>contexts</span>:
</span></span><span style=display:flex><span>- <span style=color:#f92672>name</span>: <span style=color:#ae81ff>prod-context</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>context</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>cluster</span>: <span style=color:#ae81ff>prod-cluster</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>user</span>: <span style=color:#ae81ff>prod-user</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>prod-namespace</span>
</span></span><span style=display:flex><span><span style=color:#f92672>current-context</span>: <span style=color:#ae81ff>prod-context</span>
</span></span></code></pre></div><ol><li><strong>clusters</strong> 部分指定了 <code>prod-cluster</code> 集群的信息，API Server 地址是 <code>https://prod-api-server.example.com:6443</code>。</li><li><strong>users</strong> 部分定义了 <code>prod-user</code> 用户的认证信息，使用客户端证书进行身份验证。</li><li><strong>contexts</strong> 部分定义了一个名为 <code>prod-context</code> 的上下文，指定了使用 <code>prod-cluster</code> 集群和 <code>prod-user</code> 用户，且默认命名空间为 <code>prod-namespace</code>。</li><li><strong>current-context</strong> 设置为 <code>prod-context</code>，表示 <code>kubectl</code> 将使用这个上下文连接到集群。</li></ol><h3 id=存放位置>存放位置
<a class=anchor href=#%e5%ad%98%e6%94%be%e4%bd%8d%e7%bd%ae>#</a></h3><ul><li><strong>Linux/MacOS</strong>：<code>~/.kube/config</code></li><li><strong>Windows</strong>：<code>%USERPROFILE%\.kube\config</code></li></ul><h3 id=使用-kubectl-选择不同的-kubeconfig-文件>使用 <code>kubectl</code> 选择不同的 kubeconfig 文件
<a class=anchor href=#%e4%bd%bf%e7%94%a8-kubectl-%e9%80%89%e6%8b%a9%e4%b8%8d%e5%90%8c%e7%9a%84-kubeconfig-%e6%96%87%e4%bb%b6>#</a></h3><p>可以通过 <code>--kubeconfig</code> 参数来指定使用不同的 <code>kubeconfig</code> 文件：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl --kubeconfig<span style=color:#f92672>=</span>/path/to/your/kubeconfig get pods
</span></span></code></pre></div><p>如果没有指定 <code>--kubeconfig</code> 参数，<code>kubectl</code> 默认会使用 <code>~/.kube/config</code>（Linux 和 macOS）或 <code>%USERPROFILE%\.kube\config</code>（Windows）路径下的文件。</p><h3 id=小结>小结
<a class=anchor href=#%e5%b0%8f%e7%bb%93>#</a></h3><p><code>kubeconfig</code> 文件是 Kubernetes 用来配置集群访问、认证和上下文切换的核心文件。它通过指定多个集群、用户和上下文的配置，帮助用户方便地与多个集群进行交互。</p><h2 id=pod-dns解析流程>pod DNS解析流程☆
<a class=anchor href=#pod-dns%e8%a7%a3%e6%9e%90%e6%b5%81%e7%a8%8b>#</a></h2><p>在 Kubernetes 中，<strong>Pod DNS 解析</strong>是一个重要的功能，它允许 Pod 在集群内部通过服务名进行通信，而不需要使用 IP 地址。Kubernetes 使用内建的 DNS 服务（通常是 <code>kube-dns</code> 或 <code>CoreDNS</code>）来处理域名解析请求。以下是 Pod DNS 解析的流程：</p><h3 id=1-dns-配置>1. <strong>DNS 配置</strong>
<a class=anchor href=#1-dns-%e9%85%8d%e7%bd%ae>#</a></h3><p>每个 Pod 默认会使用 Kubernetes 集群内部的 DNS 服务器进行域名解析。这个 DNS 服务器通常是 <code>kube-dns</code> 或 <code>CoreDNS</code>，其服务运行在集群内部。</p><ul><li><strong>DNS 服务</strong>的地址通常为 <code>kube-dns.kube-system.svc.cluster.local</code>（或者 CoreDNS 的地址 <code>coredns.kube-system.svc.cluster.local</code>）。</li><li>在每个 Pod 中，DNS 服务的 IP 地址通常会被配置到 <code>/etc/resolv.conf</code> 文件中，Pod 会通过该配置进行 DNS 查询。</li></ul><h3 id=2-dns-查询流程>2. <strong>DNS 查询流程</strong>
<a class=anchor href=#2-dns-%e6%9f%a5%e8%af%a2%e6%b5%81%e7%a8%8b>#</a></h3><p>当一个 Pod 想要解析一个域名（如 <code>myservice.default.svc.cluster.local</code>）时，它会通过 <code>/etc/resolv.conf</code> 配置中指定的 DNS 服务器进行查询。整个解析过程如下：</p><h4 id=a-查询初始化><strong>a. 查询初始化</strong>
<a class=anchor href=#a-%e6%9f%a5%e8%af%a2%e5%88%9d%e5%a7%8b%e5%8c%96>#</a></h4><ol><li>Pod 中的应用程序发起 DNS 请求，查询域名。</li><li>DNS 请求会被发送到 Pod 中配置的 DNS 服务器（通常是 Kubernetes 集群中的 <code>CoreDNS</code> 或 <code>kube-dns</code>）。</li></ol><h4 id=b-dns-请求到达-dns-服务><strong>b. DNS 请求到达 DNS 服务</strong>
<a class=anchor href=#b-dns-%e8%af%b7%e6%b1%82%e5%88%b0%e8%be%be-dns-%e6%9c%8d%e5%8a%a1>#</a></h4><ol><li>DNS 请求到达 Kubernetes 集群中的 DNS 服务（如 <code>CoreDNS</code> 或 <code>kube-dns</code>）。</li><li>DNS 服务根据请求的域名查找其对应的记录。</li></ol><h4 id=c-服务发现><strong>c. 服务发现</strong>
<a class=anchor href=#c-%e6%9c%8d%e5%8a%a1%e5%8f%91%e7%8e%b0>#</a></h4><ul><li><strong>服务解析</strong>：如果请求的域名是一个 Kubernetes 服务（如 <code>myservice.default.svc.cluster.local</code>），DNS 服务会根据服务的名称、命名空间和类型查找该服务的相关信息（如 Pod 的 IP 地址或服务的负载均衡器 IP）。</li><li><strong>Pod 名称解析</strong>：如果请求的域名是一个特定 Pod 的名称（如 <code>mypod.default.svc.cluster.local</code>），DNS 服务会返回该 Pod 的 IP 地址。</li></ul><h4 id=d-返回结果><strong>d. 返回结果</strong>
<a class=anchor href=#d-%e8%bf%94%e5%9b%9e%e7%bb%93%e6%9e%9c>#</a></h4><ol><li>DNS 服务将解析结果（如 IP 地址）返回给发起请求的 Pod。</li><li>Pod 中的应用程序接收到解析结果，并使用该 IP 地址与目标 Pod 或服务进行通信。</li></ol><h3 id=3-dns-解析示例>3. <strong>DNS 解析示例</strong>
<a class=anchor href=#3-dns-%e8%a7%a3%e6%9e%90%e7%a4%ba%e4%be%8b>#</a></h3><p>假设有以下 Kubernetes 服务和 Pod 配置：</p><ul><li>服务名：<code>myservice</code></li><li>命名空间：<code>default</code></li><li>服务类型：<code>ClusterIP</code></li></ul><ol><li><p>一个 Pod 想要访问 <code>myservice</code> 服务，DNS 查询将发起如下请求：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>myservice.default.svc.cluster.local
</span></span></code></pre></div></li><li><p>DNS 服务（<code>CoreDNS</code> 或 <code>kube-dns</code>）会查询该服务，并返回与 <code>myservice</code> 相关的 Pod 的 IP 地址。</p></li><li><p>Pod 收到返回的 IP 地址后，可以直接通过该 IP 与服务进行通信。</p></li></ol><h3 id=4-dns-解析规则>4. <strong>DNS 解析规则</strong>
<a class=anchor href=#4-dns-%e8%a7%a3%e6%9e%90%e8%a7%84%e5%88%99>#</a></h3><ul><li><strong>Service 名称解析</strong>： Kubernetes 中的服务名称解析是通过 <code>service.namespace.svc.cluster.local</code> 格式进行的。例如，<code>myservice.default.svc.cluster.local</code> 会解析到 <code>default</code> 命名空间中的 <code>myservice</code> 服务。</li><li><strong>Pod 名称解析</strong>： Pod 也可以通过 <code>&lt;pod-name>.&lt;namespace>.pod.cluster.local</code> 进行解析。例如，<code>mypod.default.pod.cluster.local</code> 会解析到 <code>default</code> 命名空间中的 <code>mypod</code> Pod。</li><li><strong>域名后缀</strong>： Kubernetes DNS 的域名后缀通常是：<ul><li><code>svc.cluster.local</code>：服务的完整域名。</li><li><code>pod.cluster.local</code>：Pod 的完整域名。</li><li><code>namespace.svc.cluster.local</code>：命名空间内的服务名。</li></ul></li><li><strong>DNS 缓存</strong>：Pod 中的 DNS 解析通常会有缓存，避免每次都进行查询，但缓存时间有限，通常是 5 分钟。</li></ul><h3 id=5-dns-服务的工作原理>5. <strong>DNS 服务的工作原理</strong>
<a class=anchor href=#5-dns-%e6%9c%8d%e5%8a%a1%e7%9a%84%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86>#</a></h3><ul><li><strong>CoreDNS / kube-dns</strong>：Kubernetes 使用 <code>CoreDNS</code> 或 <code>kube-dns</code> 来提供集群内部的 DNS 服务。它们会根据配置解析服务名、Pod 名称等。</li><li><strong>Service 和 Endpoints</strong>：每个 Kubernetes 服务都有一个对应的 <code>Endpoints</code> 资源，表示该服务背后的 Pod IP 地址。DNS 服务会查找 <code>Endpoints</code> 并返回 Pod 的 IP 地址。</li><li><strong>Pod DNS 配置</strong>：Kubernetes 会通过环境变量和 <code>resolv.conf</code> 配置将 DNS 服务信息传递给 Pod，以确保它能够正确地查询域名。</li></ul><h3 id=6-dns-解析优化与故障排查>6. <strong>DNS 解析优化与故障排查</strong>
<a class=anchor href=#6-dns-%e8%a7%a3%e6%9e%90%e4%bc%98%e5%8c%96%e4%b8%8e%e6%95%85%e9%9a%9c%e6%8e%92%e6%9f%a5>#</a></h3><ul><li><p><strong>CoreDNS 配置</strong>：如果需要调整 DNS 解析的行为，可以通过配置 <code>CoreDNS</code> 插件来修改查询规则或增强性能。</p></li><li><p>DNS 故障排查</p><p>：如果 Pod 无法解析域名，可能的原因包括 DNS 配置问题、</p><pre tabindex=0><code>CoreDNS
</code></pre><p>或</p><pre tabindex=0><code>kube-dns
</code></pre><p>服务故障、网络问题等。可以通过以下方式排查：</p><ul><li>查看 Pod 中的 <code>/etc/resolv.conf</code> 配置，确认 DNS 服务器是否正确。</li><li>检查 <code>CoreDNS</code> 或 <code>kube-dns</code> Pod 的状态，查看是否存在异常。</li><li>使用 <code>nslookup</code> 或 <code>dig</code> 命令从 Pod 内部测试 DNS 解析。</li></ul></li></ul><h3 id=7-dns-解析的常见问题>7. <strong>DNS 解析的常见问题</strong>
<a class=anchor href=#7-dns-%e8%a7%a3%e6%9e%90%e7%9a%84%e5%b8%b8%e8%a7%81%e9%97%ae%e9%a2%98>#</a></h3><ul><li><strong>DNS 超时</strong>：如果 DNS 查询超时，通常是因为 DNS 服务（<code>CoreDNS</code> 或 <code>kube-dns</code>）不可用或网络存在问题。</li><li><strong>DNS 名称解析失败</strong>：可能是由于没有正确的 <code>Endpoints</code>，或服务未正确创建，导致无法解析服务名。</li><li><strong>Pod 之间的 DNS 问题</strong>：可能是由于 Pod 的 DNS 配置不正确或网络策略（Network Policy）阻止了 DNS 请求。</li></ul><hr><h3 id=总结-22>总结
<a class=anchor href=#%e6%80%bb%e7%bb%93-22>#</a></h3><p>Kubernetes 中的 DNS 解析依赖于 <code>CoreDNS</code> 或 <code>kube-dns</code> 服务，它能够将集群内的服务名、Pod 名称等解析为相应的 IP 地址，使得 Pod 之间可以通过 DNS 进行通信。DNS 解析的主要流程是通过 <code>CoreDNS</code> 解析服务名、Pod 名称等，并返回相应的 IP 地址供 Pod 使用。通过合理的配置和排查，可以确保 Kubernetes 集群内 DNS 解析正常工作。</p><h2 id=traefik对比nginx-ingress优点>traefik对比nginx ingress优点
<a class=anchor href=#traefik%e5%af%b9%e6%af%94nginx-ingress%e4%bc%98%e7%82%b9>#</a></h2><p><strong>Traefik</strong> 和 <strong>Nginx Ingress Controller</strong> 都是 Kubernetes 中常用的反向代理和负载均衡解决方案。虽然两者都可以作为 Ingress Controller，但它们有一些不同的设计理念、功能特点和优缺点。下面是对比这两者的一些优点：</p><h3 id=1-traefik-的优点><strong>1. Traefik 的优点</strong>
<a class=anchor href=#1-traefik-%e7%9a%84%e4%bc%98%e7%82%b9>#</a></h3><h4 id=a-原生支持动态配置><strong>a. 原生支持动态配置</strong>
<a class=anchor href=#a-%e5%8e%9f%e7%94%9f%e6%94%af%e6%8c%81%e5%8a%a8%e6%80%81%e9%85%8d%e7%bd%ae>#</a></h4><ul><li><strong>Traefik</strong> 提供了强大的动态配置能力，它会实时监控 Kubernetes 集群中的资源（如 Ingress、Service、Pod 等）并自动更新路由配置，而不需要手动重载或重新启动服务。Traefik 可以通过监控 Kubernetes API 自动发现并动态更新路由规则，具有更好的灵活性。</li><li><strong>Nginx</strong> 也支持动态配置，但它的配置文件通常需要手动更新，并且需要重新加载 Nginx 才能应用新配置。</li></ul><h4 id=b-更易于配置><strong>b. 更易于配置</strong>
<a class=anchor href=#b-%e6%9b%b4%e6%98%93%e4%ba%8e%e9%85%8d%e7%bd%ae>#</a></h4><ul><li><strong>Traefik</strong> 提供了基于声明式配置的简化操作方式，且其配置文件和文档简单易懂，用户可以更快速地部署和配置。它可以通过 YAML 配置文件直接与 Kubernetes 集成。</li><li><strong>Nginx</strong> 的配置可能稍显复杂，尤其是在处理较复杂的负载均衡、反向代理等场景时，通常需要更多的细致配置。</li></ul><h4 id=c-内建支持多种负载均衡算法><strong>c. 内建支持多种负载均衡算法</strong>
<a class=anchor href=#c-%e5%86%85%e5%bb%ba%e6%94%af%e6%8c%81%e5%a4%9a%e7%a7%8d%e8%b4%9f%e8%bd%bd%e5%9d%87%e8%a1%a1%e7%ae%97%e6%b3%95>#</a></h4><ul><li><strong>Traefik</strong> 支持多种负载均衡策略，如轮询、加权轮询、最少连接等，可以根据需求轻松切换和调整负载均衡策略。</li><li><strong>Nginx</strong> 也支持负载均衡，但 Traefik 的策略更为灵活，且配置更为简便。</li></ul><h4 id=d-集成-lets-encrypt-支持><strong>d. 集成 Let’s Encrypt 支持</strong>
<a class=anchor href=#d-%e9%9b%86%e6%88%90-lets-encrypt-%e6%94%af%e6%8c%81>#</a></h4><ul><li><strong>Traefik</strong> 内建对 <strong>Let’s Encrypt</strong> 的自动支持，能够自动获取和续期 SSL/TLS 证书。只需要在 Traefik 配置中开启自动 SSL 支持，Traefik 就会为你的服务自动生成并管理 SSL 证书。</li><li><strong>Nginx</strong> 需要额外配置脚本或手动集成 Let’s Encrypt，虽然有相关工具，但相比 Traefik 更为复杂。</li></ul><h4 id=e-更好的微服务架构支持><strong>e. 更好的微服务架构支持</strong>
<a class=anchor href=#e-%e6%9b%b4%e5%a5%bd%e7%9a%84%e5%be%ae%e6%9c%8d%e5%8a%a1%e6%9e%b6%e6%9e%84%e6%94%af%e6%8c%81>#</a></h4><ul><li><strong>Traefik</strong> 是专为现代微服务架构设计的，它支持 Kubernetes、Docker 和其他容器化环境的集成。Traefik 可以根据容器的标签、标签集等信息动态地调整流量路由，适合高动态环境。</li><li><strong>Nginx</strong> 主要是传统的 HTTP 服务器，虽然也支持 Kubernetes，但在容器化和微服务环境中的灵活性和自动化程度上不如 Traefik。</li></ul><h4 id=f-内建支持-websocket-和-http2><strong>f. 内建支持 WebSocket 和 HTTP/2</strong>
<a class=anchor href=#f-%e5%86%85%e5%bb%ba%e6%94%af%e6%8c%81-websocket-%e5%92%8c-http2>#</a></h4><ul><li><strong>Traefik</strong> 原生支持 WebSocket 和 HTTP/2，且配置非常简单。</li><li><strong>Nginx</strong> 也支持这些协议，但相对来说配置复杂，且需要额外的模块支持。</li></ul><h4 id=g-ui-控制面板><strong>g. UI 控制面板</strong>
<a class=anchor href=#g-ui-%e6%8e%a7%e5%88%b6%e9%9d%a2%e6%9d%bf>#</a></h4><ul><li><strong>Traefik</strong> 提供了一个方便的 <strong>Web UI</strong>，通过该界面可以监控 Traefik 的健康状况、查看路由配置和访问日志等。这对于运维和开发人员调试非常方便。</li><li><strong>Nginx</strong> 没有内建 UI，虽然可以通过第三方工具（如 Nginx Amplify）进行监控，但并不如 Traefik 自带的 UI 那么方便。</li></ul><h4 id=h-支持多种后端><strong>h. 支持多种后端</strong>
<a class=anchor href=#h-%e6%94%af%e6%8c%81%e5%a4%9a%e7%a7%8d%e5%90%8e%e7%ab%af>#</a></h4><ul><li><strong>Traefik</strong> 可以同时作为反向代理支持 Kubernetes、Docker、Mesos、Consul 等多种后端架构，这使得它在多种环境下的可移植性和扩展性更强。</li><li><strong>Nginx</strong> 主要是面向传统的 Web 服务器/反向代理需求，尽管它也支持 Kubernetes，但支持的后端系统不如 Traefik 广泛。</li></ul><h3 id=2-nginx-ingress-controller-的优点><strong>2. Nginx Ingress Controller 的优点</strong>
<a class=anchor href=#2-nginx-ingress-controller-%e7%9a%84%e4%bc%98%e7%82%b9>#</a></h3><p>虽然 <strong>Traefik</strong> 有很多优点，但 <strong>Nginx Ingress Controller</strong> 也有一些优势，尤其在某些企业环境中，Nginx 的成熟和稳定性使它成为很多用户的选择。以下是 Nginx 的一些优点：</p><h4 id=a-性能稳定><strong>a. 性能稳定</strong>
<a class=anchor href=#a-%e6%80%a7%e8%83%bd%e7%a8%b3%e5%ae%9a>#</a></h4><ul><li><strong>Nginx</strong> 是经过多年稳定测试的高性能 Web 服务器，处理静态内容和反向代理流量的性能非常强大。在面对高并发流量时，Nginx 可以提供非常高的吞吐量。</li><li><strong>Traefik</strong> 也有很好的性能，但在某些极端高并发的情况下，Nginx 可能会有更优的表现。</li></ul><h4 id=b-丰富的功能和扩展性><strong>b. 丰富的功能和扩展性</strong>
<a class=anchor href=#b-%e4%b8%b0%e5%af%8c%e7%9a%84%e5%8a%9f%e8%83%bd%e5%92%8c%e6%89%a9%e5%b1%95%e6%80%a7>#</a></h4><ul><li><strong>Nginx</strong> 拥有非常丰富的功能支持，例如复杂的负载均衡、内容缓存、请求重写、SSL/TLS 优化等。对于一些复杂的 Web 应用场景，Nginx 提供了更加细致的控制和扩展。</li><li><strong>Traefik</strong> 主要针对微服务架构，虽然也提供了许多负载均衡功能，但在细粒度的流量控制和复杂的 HTTP 配置方面，Nginx 更加灵活和强大。</li></ul><h4 id=c-社区支持和文档><strong>c. 社区支持和文档</strong>
<a class=anchor href=#c-%e7%a4%be%e5%8c%ba%e6%94%af%e6%8c%81%e5%92%8c%e6%96%87%e6%a1%a3>#</a></h4><ul><li><strong>Nginx</strong> 具有庞大的社区和成熟的文档资源，广泛应用于全球许多企业的生产环境，因此拥有大量的解决方案和最佳实践。</li><li><strong>Traefik</strong> 的社区支持也在快速增长，但与 Nginx 相比，其历史稍短，且某些高级特性可能需要更多的社区贡献和测试。</li></ul><h4 id=d-安全性><strong>d. 安全性</strong>
<a class=anchor href=#d-%e5%ae%89%e5%85%a8%e6%80%a7>#</a></h4><ul><li><strong>Nginx</strong> 在安全性方面非常成熟，拥有强大的认证和防火墙功能，广泛用于企业级 Web 应用中。</li><li><strong>Traefik</strong> 也非常注重安全性，但在某些特性上可能没有 Nginx 那么多的配置选项和细节。</li></ul><h4 id=e-企业级支持><strong>e. 企业级支持</strong>
<a class=anchor href=#e-%e4%bc%81%e4%b8%9a%e7%ba%a7%e6%94%af%e6%8c%81>#</a></h4><ul><li><strong>Nginx</strong> 提供了企业级的支持和付费版本，能够满足一些大型企业对稳定性和高级功能的需求（如 WAF、防火墙、负载均衡、流量分析等）。</li><li><strong>Traefik</strong> 主要是开源项目，虽然也有商业支持（Traefik Labs 提供的 Traefik Enterprise），但相比之下 Nginx 在企业级市场上占有更大的份额。</li></ul><hr><h3 id=总结-23><strong>总结：</strong>
<a class=anchor href=#%e6%80%bb%e7%bb%93-23>#</a></h3><table><thead><tr><th>特性</th><th><strong>Traefik</strong></th><th><strong>Nginx Ingress Controller</strong></th></tr></thead><tbody><tr><td><strong>配置灵活性</strong></td><td>动态配置，无需重载，简化配置</td><td>配置较复杂，通常需要手动更新并重载</td></tr><tr><td><strong>自动 SSL/TLS</strong></td><td>内建自动 Let’s Encrypt 支持</td><td>需要额外配置支持</td></tr><tr><td><strong>微服务架构支持</strong></td><td>强大的微服务支持，自动化与容器化集成</td><td>支持较好，但配置和灵活性较低</td></tr><tr><td><strong>UI 支持</strong></td><td>原生支持 Web UI</td><td>无原生 Web UI，需第三方工具</td></tr><tr><td><strong>性能</strong></td><td>对高并发有良好支持，但 Nginx 更优</td><td>极高的性能和吞吐量，适合高并发</td></tr><tr><td><strong>后端支持</strong></td><td>支持多种后端（Kubernetes、Docker 等）</td><td>主要支持 Kubernetes</td></tr><tr><td><strong>社区与文档</strong></td><td>新兴社区，快速成长</td><td>成熟社区和文档资源丰富</td></tr></tbody></table><p><strong>选择哪种 Ingress Controller</strong> 取决于您的需求：</p><ul><li>如果您的 Kubernetes 环境是高度动态的微服务架构，并且需要快速的配置和自动化支持，<strong>Traefik</strong> 可能更适合。</li><li>如果您需要稳定、强大且性能优化的反向代理服务，特别是在高负载、高并发的环境中，且需要丰富的自定义配置和更长的历史支持，<strong>Nginx Ingress Controller</strong> 会是更好的选择。</li></ul><h2 id=harbor有哪些组件>Harbor有哪些组件
<a class=anchor href=#harbor%e6%9c%89%e5%93%aa%e4%ba%9b%e7%bb%84%e4%bb%b6>#</a></h2><p><strong>Harbor</strong> 是一个开源的企业级 Docker 镜像仓库，支持 Helm chart 存储和管理，具有图形化管理界面和多种安全功能。它的组件设计旨在提供完整的镜像管理服务，确保镜像存储的安全性、可管理性和可靠性。下面是 Harbor 的主要组件：</p><h3 id=1-core-核心服务>1. <strong>Core (核心服务)</strong>
<a class=anchor href=#1-core-%e6%a0%b8%e5%bf%83%e6%9c%8d%e5%8a%a1>#</a></h3><ul><li><strong>核心服务</strong> 是 Harbor 的核心功能部分，负责用户的认证与授权、镜像的管理、任务调度、系统配置和日志管理等。</li></ul><h3 id=2-registry-镜像仓库>2. <strong>Registry (镜像仓库)</strong>
<a class=anchor href=#2-registry-%e9%95%9c%e5%83%8f%e4%bb%93%e5%ba%93>#</a></h3><ul><li><strong>Registry</strong> 是 Harbor 的核心组成部分，提供镜像的存储、查询、推送和拉取等基本功能。它是基于 Docker Registry 实现的，并对其进行了扩展，支持更多的企业级特性。</li><li>Harbor 使用 <strong>Notary</strong> 来为存储的镜像提供签名功能，确保镜像的安全性。</li></ul><h3 id=3-portal-web-ui>3. <strong>Portal (Web UI)</strong>
<a class=anchor href=#3-portal-web-ui>#</a></h3><ul><li><strong>Portal</strong> 是 Harbor 的图形化管理界面，提供用户友好的界面来管理镜像仓库、用户、项目、权限等。通过 UI，用户可以进行镜像上传、删除、查看、拉取等操作，同时可以监控镜像的使用情况和安全性。</li></ul><h3 id=4-jobservice-任务服务>4. <strong>Jobservice (任务服务)</strong>
<a class=anchor href=#4-jobservice-%e4%bb%bb%e5%8a%a1%e6%9c%8d%e5%8a%a1>#</a></h3><ul><li><strong>Jobservice</strong> 是 Harbor 用于处理后台任务的组件。它负责执行异步任务，如镜像扫描、镜像清理等任务。</li><li>它通过任务队列异步处理任务，从而减轻核心服务的负担。</li></ul><h3 id=5-notary-镜像签名>5. <strong>Notary (镜像签名)</strong>
<a class=anchor href=#5-notary-%e9%95%9c%e5%83%8f%e7%ad%be%e5%90%8d>#</a></h3><ul><li><strong>Notary</strong> 是 Harbor 用于实现镜像签名的组件。它通过签名确保镜像的来源和内容在传输过程中不被篡改，从而提升安全性。Harbor 使用 Notary 来确保镜像在生产环境中的可信度。</li></ul><h3 id=6-clair-镜像安全扫描>6. <strong>Clair (镜像安全扫描)</strong>
<a class=anchor href=#6-clair-%e9%95%9c%e5%83%8f%e5%ae%89%e5%85%a8%e6%89%ab%e6%8f%8f>#</a></h3><ul><li><strong>Clair</strong> 是 Harbor 集成的一个静态分析工具，用于扫描镜像中的安全漏洞。它检查镜像中的软件包是否存在已知的安全漏洞，并生成漏洞报告。</li><li>Clair 可以集成到 Harbor 中，当镜像被推送或拉取时，自动扫描镜像内容并报告安全问题。</li></ul><h3 id=7-chartmuseum-helm-仓库>7. <strong>Chartmuseum (Helm 仓库)</strong>
<a class=anchor href=#7-chartmuseum-helm-%e4%bb%93%e5%ba%93>#</a></h3><ul><li><strong>Chartmuseum</strong> 是 Harbor 用于 Helm chart 管理的组件。它支持存储和管理 Helm chart，允许用户上传、下载和管理 Helm chart 存储库。</li><li>通过这个组件，用户可以将 Helm chart 作为 Kubernetes 部署包存储和版本管理。</li></ul><h3 id=8-log-日志系统>8. <strong>Log (日志系统)</strong>
<a class=anchor href=#8-log-%e6%97%a5%e5%bf%97%e7%b3%bb%e7%bb%9f>#</a></h3><ul><li><strong>Log</strong> 是 Harbor 的日志管理模块，用于记录系统运行过程中的重要事件、错误、警告等信息。它有助于运维人员监控和诊断系统问题。</li><li>Harbor 可以通过集成其他日志系统（如 ELK）来扩展日志功能。</li></ul><h3 id=9-database-数据库>9. <strong>Database (数据库)</strong>
<a class=anchor href=#9-database-%e6%95%b0%e6%8d%ae%e5%ba%93>#</a></h3><ul><li><strong>数据库</strong> 存储了 Harbor 的所有元数据，包括用户信息、项目、镜像标签、存储配置信息等。通常，Harbor 使用 <strong>PostgreSQL</strong> 作为其数据库后端。</li></ul><h3 id=10-redis-缓存服务>10. <strong>Redis (缓存服务)</strong>
<a class=anchor href=#10-redis-%e7%bc%93%e5%ad%98%e6%9c%8d%e5%8a%a1>#</a></h3><ul><li><strong>Redis</strong> 是 Harbor 的缓存层，用于提高系统性能，减轻数据库的负载。它缓存一些常用的请求数据和会话信息。</li></ul><h3 id=11-nginx-反向代理>11. <strong>NGINX (反向代理)</strong>
<a class=anchor href=#11-nginx-%e5%8f%8d%e5%90%91%e4%bb%a3%e7%90%86>#</a></h3><ul><li><strong>NGINX</strong> 是 Harbor 的反向代理和负载均衡器，负责将用户的请求路由到 Harbor 的各个组件上。它也支持 SSL/TLS 加密，确保数据传输的安全性。</li></ul><hr><h3 id=总结-24><strong>总结：</strong>
<a class=anchor href=#%e6%80%bb%e7%bb%93-24>#</a></h3><p>Harbor 的主要组件包括：</p><ul><li><strong>Core</strong>：核心服务，提供认证、授权、镜像管理等功能。</li><li><strong>Registry</strong>：镜像仓库，存储和管理镜像。</li><li><strong>Portal</strong>：Web UI，提供用户图形化界面。</li><li><strong>Jobservice</strong>：任务服务，处理异步任务。</li><li><strong>Notary</strong>：镜像签名，确保镜像可信。</li><li><strong>Clair</strong>：镜像安全扫描，检测漏洞。</li><li><strong>Chartmuseum</strong>：Helm 仓库，存储 Helm charts。</li><li><strong>Log</strong>：日志系统，记录系统事件。</li><li><strong>Database</strong>：数据库，存储元数据。</li><li><strong>Redis</strong>：缓存服务，提高性能。</li><li><strong>NGINX</strong>：反向代理，提供负载均衡和加密服务。</li></ul><p>这些组件共同工作，提供一个完整、安全、易于管理的容器镜像仓库。</p><h2 id=harbor高可用怎么实现>Harbor高可用怎么实现
<a class=anchor href=#harbor%e9%ab%98%e5%8f%af%e7%94%a8%e6%80%8e%e4%b9%88%e5%ae%9e%e7%8e%b0>#</a></h2><p>在生产环境中，<strong>Harbor</strong> 的高可用（HA）是确保容器镜像仓库服务持续可用、可靠的关键。为了保证 <strong>Harbor</strong> 的高可用性，需要对多个组件进行设计和部署，以减少单点故障的风险。下面是实现 <strong>Harbor 高可用</strong> 的一些方法和最佳实践：</p><h3 id=1-harbor-组件的冗余设计>1. <strong>Harbor 组件的冗余设计</strong>
<a class=anchor href=#1-harbor-%e7%bb%84%e4%bb%b6%e7%9a%84%e5%86%97%e4%bd%99%e8%ae%be%e8%ae%a1>#</a></h3><p>为了确保 Harbor 的高可用性，首先需要对 Harbor 的关键组件进行冗余设计，确保任何单点故障不会导致整个系统停机。</p><h4 id=a-harbor-核心服务-core-的冗余><strong>a. Harbor 核心服务 (Core) 的冗余</strong>
<a class=anchor href=#a-harbor-%e6%a0%b8%e5%bf%83%e6%9c%8d%e5%8a%a1-core-%e7%9a%84%e5%86%97%e4%bd%99>#</a></h4><ul><li><strong>Core</strong> 组件是 Harbor 的核心服务，负责处理用户请求、权限管理、任务调度等功能。在高可用部署中，通常需要部署多个 <strong>Core</strong> 服务实例，使用负载均衡器来分发流量。可以将多个 <strong>Core</strong> 实例部署到不同的节点上，确保如果某个实例不可用，其他实例可以继续处理请求。</li></ul><h4 id=b-负载均衡器-load-balancer><strong>b. 负载均衡器 (Load Balancer)</strong>
<a class=anchor href=#b-%e8%b4%9f%e8%bd%bd%e5%9d%87%e8%a1%a1%e5%99%a8-load-balancer>#</a></h4><ul><li><strong>NGINX</strong> 或其他负载均衡器（如 HAProxy）通常用于在多个 Harbor 核心服务实例之间分发流量。负载均衡器通常会部署在 <strong>Harbor</strong> 的前端，代理用户请求并将其转发到可用的服务实例。</li><li>使用 <strong>Keepalived</strong> 或 <strong>HAProxy</strong> 等工具来实现虚拟 IP 地址（VIP）的高可用，可以在一个实例宕机时自动将流量切换到健康的实例。</li></ul><h4 id=c-harbor-registry-的冗余><strong>c. Harbor Registry 的冗余</strong>
<a class=anchor href=#c-harbor-registry-%e7%9a%84%e5%86%97%e4%bd%99>#</a></h4><ul><li><strong>Registry</strong> 是 Harbor 的核心组件，负责存储 Docker 镜像和其他容器化应用。在高可用部署中，应该考虑将 <strong>Registry</strong> 的数据存储使用 <strong>分布式存储系统</strong>，如 <strong>Ceph</strong> 或 <strong>GlusterFS</strong>，并在多个节点之间进行冗余备份。</li><li>可以通过使用 <strong>Harbor 的多副本 Registry 存储</strong>（例如部署多个 Harbor 实例，配置多个 Registry 存储位置）来确保 Registry 数据的高可用性。</li></ul><h4 id=d-数据库的高可用设计><strong>d. 数据库的高可用设计</strong>
<a class=anchor href=#d-%e6%95%b0%e6%8d%ae%e5%ba%93%e7%9a%84%e9%ab%98%e5%8f%af%e7%94%a8%e8%ae%be%e8%ae%a1>#</a></h4><ul><li><strong>Harbor</strong> 默认使用 <strong>PostgreSQL</strong> 作为数据库后端。为了保证数据库的高可用性，可以使用 <strong>PostgreSQL 集群</strong>（如 <strong>Patroni</strong> 或 <strong>PgBouncer</strong>）来实现主从复制和自动故障切换。</li><li>数据库可以部署为主从架构，主数据库处理写请求，从数据库处理读请求。如果主数据库出现故障，可以自动切换到从数据库。</li></ul><h4 id=e-redis-缓存的高可用设计><strong>e. Redis 缓存的高可用设计</strong>
<a class=anchor href=#e-redis-%e7%bc%93%e5%ad%98%e7%9a%84%e9%ab%98%e5%8f%af%e7%94%a8%e8%ae%be%e8%ae%a1>#</a></h4><ul><li><strong>Redis</strong> 被用于 Harbor 的缓存层，在高可用部署中，建议使用 <strong>Redis 集群模式</strong> 或 <strong>Redis Sentinel</strong> 来实现 Redis 的高可用。Redis Sentinel 会自动监控 Redis 实例的状态，并在主节点发生故障时自动切换到从节点。</li></ul><h4 id=f-clair安全扫描服务的冗余><strong>f. Clair（安全扫描服务）的冗余</strong>
<a class=anchor href=#f-clair%e5%ae%89%e5%85%a8%e6%89%ab%e6%8f%8f%e6%9c%8d%e5%8a%a1%e7%9a%84%e5%86%97%e4%bd%99>#</a></h4><ul><li><strong>Clair</strong> 是 Harbor 中用于镜像安全扫描的组件。在高可用部署中，可以部署多个 Clair 实例，并通过负载均衡器进行流量分配。这样，即使某个 Clair 实例出现故障，其他实例也能继续提供镜像扫描服务。</li></ul><h4 id=g-jobservice-的冗余><strong>g. Jobservice 的冗余</strong>
<a class=anchor href=#g-jobservice-%e7%9a%84%e5%86%97%e4%bd%99>#</a></h4><ul><li><strong>Jobservice</strong> 负责处理后台任务（如镜像扫描等）。为确保任务的高可用性，可以部署多个 <strong>Jobservice</strong> 实例，并使用负载均衡器来分配任务请求。</li></ul><h4 id=h-notary-服务的冗余><strong>h. Notary 服务的冗余</strong>
<a class=anchor href=#h-notary-%e6%9c%8d%e5%8a%a1%e7%9a%84%e5%86%97%e4%bd%99>#</a></h4><ul><li><strong>Notary</strong> 用于实现镜像签名，确保镜像的完整性和可信性。为了确保 Notary 的高可用性，可以将其部署为多个实例，并通过负载均衡进行流量分发。</li></ul><hr><h3 id=2-高可用的部署架构>2. <strong>高可用的部署架构</strong>
<a class=anchor href=#2-%e9%ab%98%e5%8f%af%e7%94%a8%e7%9a%84%e9%83%a8%e7%bd%b2%e6%9e%b6%e6%9e%84>#</a></h3><p>为了实现 Harbor 的高可用性，通常需要按照以下方式部署 Harbor：</p><h4 id=a-多节点部署><strong>a. 多节点部署</strong>
<a class=anchor href=#a-%e5%a4%9a%e8%8a%82%e7%82%b9%e9%83%a8%e7%bd%b2>#</a></h4><ul><li>将 Harbor 的各个组件（如 Core、Registry、Clair、Notary 等）分别部署在多个节点上，确保即使某个节点故障，其他节点依然可以提供服务。</li><li>在多个节点上部署 <strong>Harbor</strong> 的各个组件，并通过 <strong>负载均衡器</strong> 来实现流量分发和故障切换。</li></ul><h4 id=b-数据存储的高可用><strong>b. 数据存储的高可用</strong>
<a class=anchor href=#b-%e6%95%b0%e6%8d%ae%e5%ad%98%e5%82%a8%e7%9a%84%e9%ab%98%e5%8f%af%e7%94%a8>#</a></h4><ul><li>使用 <strong>分布式存储系统</strong>（如 Ceph、GlusterFS 或 NFS）来存储 Harbor 的数据，这样即使某个存储节点出现故障，其他存储节点仍然可以提供数据访问。</li><li>对于 Harbor 的 <strong>数据库</strong>，可以使用 <strong>PostgreSQL 主从复制</strong> 或 <strong>Patroni</strong> 集群来保证数据库的高可用性。</li></ul><h4 id=c-使用容器编排工具如-kubernetes><strong>c. 使用容器编排工具（如 Kubernetes）</strong>
<a class=anchor href=#c-%e4%bd%bf%e7%94%a8%e5%ae%b9%e5%99%a8%e7%bc%96%e6%8e%92%e5%b7%a5%e5%85%b7%e5%a6%82-kubernetes>#</a></h4><ul><li>在 Kubernetes 集群中部署 Harbor 时，可以利用 Kubernetes 的 <strong>Pod 副本</strong>、<strong>Deployment</strong> 和 <strong>StatefulSet</strong> 等功能来实现服务的高可用。</li><li>Kubernetes 的 <strong>Horizontal Pod Autoscaling</strong>（水平自动扩展）可以在流量增加时自动扩展 Harbor 的服务实例数，从而提高容器的可用性和负载能力。</li></ul><h4 id=d-外部存储服务><strong>d. 外部存储服务</strong>
<a class=anchor href=#d-%e5%a4%96%e9%83%a8%e5%ad%98%e5%82%a8%e6%9c%8d%e5%8a%a1>#</a></h4><ul><li>使用 <strong>外部存储服务</strong>（如 <strong>S3</strong> 或 <strong>阿里云 OSS</strong>）来存储镜像数据，这样可以避免由于本地存储故障导致的数据丢失或不可用。</li></ul><hr><h3 id=3-harbor-高可用的部署方案常见架构>3. <strong>Harbor 高可用的部署方案（常见架构）</strong>
<a class=anchor href=#3-harbor-%e9%ab%98%e5%8f%af%e7%94%a8%e7%9a%84%e9%83%a8%e7%bd%b2%e6%96%b9%e6%a1%88%e5%b8%b8%e8%a7%81%e6%9e%b6%e6%9e%84>#</a></h3><p>以下是一个常见的 <strong>Harbor 高可用架构</strong> 示例：</p><pre tabindex=0><code>               +-------------------------+
               |       Load Balancer      |
               +-------------------------+
                    |           |
                    v           v
        +----------------+  +----------------+
        |   Harbor Core  |  |   Harbor Core  |
        |    Instance 1  |  |    Instance 2  |
        +----------------+  +----------------+
                    |           |
          +-------------------------------+
          |         Harbor Registry        |
          |         (Shared Storage)       |
          +-------------------------------+
                    |           |
                    v           v
            +-------------------------+
            |        Database (HA)     |
            +-------------------------+
                    |
                    v
              +-------------+
              |   Redis HA  |
              +-------------+
                    |
                    v
               +----------------+
               |    Clair (HA)   |
               +----------------+
                    |
                    v
              +----------------+
              |  Jobservice (HA)|
              +----------------+
</code></pre><ul><li><strong>负载均衡器</strong>：将用户请求分发到多个 Harbor Core 实例。</li><li><strong>Harbor Core 实例</strong>：多个 Harbor 核心服务实例，支持负载均衡。</li><li><strong>Registry 存储</strong>：使用共享的分布式存储系统或对象存储（如 S3）。</li><li><strong>数据库</strong>：使用高可用数据库架构（如 PostgreSQL 主从复制）。</li><li><strong>Redis</strong>：使用 Redis Sentinel 或集群模式，保证缓存高可用。</li><li><strong>Clair</strong>：使用多个 Clair 实例提供镜像扫描服务。</li><li><strong>Jobservice</strong>：多个 Jobservice 实例处理后台任务。</li></ul><hr><h3 id=4-故障恢复与备份>4. <strong>故障恢复与备份</strong>
<a class=anchor href=#4-%e6%95%85%e9%9a%9c%e6%81%a2%e5%a4%8d%e4%b8%8e%e5%a4%87%e4%bb%bd>#</a></h3><p>为了确保高可用架构的健壮性，还需要制定灾难恢复（DR）策略和定期备份：</p><ul><li><strong>定期备份</strong> Harbor 数据（包括镜像、数据库、配置文件等）以便在发生故障时进行恢复。</li><li>使用 <strong>自动化备份</strong> 和 <strong>异地备份</strong> 确保数据不会丢失，并能在灾难发生后快速恢复服务。</li></ul><hr><h3 id=总结-25>总结：
<a class=anchor href=#%e6%80%bb%e7%bb%93-25>#</a></h3><p>实现 <strong>Harbor 高可用</strong> 需要通过以下方式：</p><ol><li><strong>冗余设计</strong>：确保核心组件（Core、Registry、Clair、Notary 等）部署多个实例，负载均衡分发流量。</li><li><strong>数据库和 Redis 高可用</strong>：使用 PostgreSQL 主从复制或 Redis Sentinel 实现数据和缓存的高可用。</li><li><strong>分布式存储</strong>：使用分布式存储系统（如 Ceph、GlusterFS 或 S3）确保数据高可用。</li><li><strong>容器编排工具</strong>：如 Kubernetes 提供自动扩展和容错能力。</li><li><strong>灾难恢复与备份</strong>：定期备份 Harbor 的重要数据，确保灾难发生时可以恢复。</li></ol><p>通过这些方法，Harbor 能够实现高可用部署，保证容器镜像服务的稳定性和持续性。</p><h2 id=etcd调优>ETCD调优
<a class=anchor href=#etcd%e8%b0%83%e4%bc%98>#</a></h2><p><strong>etcd</strong> 是 Kubernetes 中的核心组件，负责存储集群的所有配置信息、元数据和状态数据。因为 etcd 是分布式的键值存储数据库，它的性能直接影响到 Kubernetes 集群的可靠性和性能。因此，etcd 的调优对确保集群的高可用性、低延迟和高吞吐量至关重要。</p><p>在实际应用中，etcd 的调优可以涉及多个方面，包括硬件配置、网络配置、etcd 配置参数调整以及监控和维护策略。以下是一些常见的 etcd 调优方法：</p><h3 id=1-硬件配置优化>1. <strong>硬件配置优化</strong>
<a class=anchor href=#1-%e7%a1%ac%e4%bb%b6%e9%85%8d%e7%bd%ae%e4%bc%98%e5%8c%96>#</a></h3><h4 id=a-存储硬盘><strong>a. 存储硬盘</strong>
<a class=anchor href=#a-%e5%ad%98%e5%82%a8%e7%a1%ac%e7%9b%98>#</a></h4><ul><li><strong>磁盘 I/O 性能</strong>：etcd 对磁盘 I/O 性能要求较高，特别是当存储的数据量较大时。推荐使用 <strong>SSD</strong> 存储而不是传统的 <strong>HDD</strong>，因为 SSD 提供更高的读写性能。</li><li><strong>RAID 配置</strong>：如果使用 RAID 阵列，请使用 <strong>RAID 10</strong> 来提供较好的读写性能和数据冗余。</li></ul><h4 id=b-内存><strong>b. 内存</strong>
<a class=anchor href=#b-%e5%86%85%e5%ad%98>#</a></h4><ul><li><strong>内存大小</strong>：etcd 的数据存储主要依赖内存，因此需要足够的内存来缓解频繁的磁盘 I/O。建议在每个 etcd 节点上分配至少 16 GB 或更多的内存。</li><li><strong>内存调整</strong>：etcd 在存储数据时，尽量让数据驻留在内存中以提高查询速度。如果 etcd 运行在较低内存的机器上，可以考虑增加内存，特别是在等候大量请求的场景下。</li></ul><h4 id=c-cpu><strong>c. CPU</strong>
<a class=anchor href=#c-cpu>#</a></h4><ul><li>etcd 需要较高的计算能力来处理事务、复制和集群协调等任务，尤其是在大量写操作的情况下，确保有足够的 CPU 核心来处理并发请求。</li></ul><hr><h3 id=2-网络配置优化>2. <strong>网络配置优化</strong>
<a class=anchor href=#2-%e7%bd%91%e7%bb%9c%e9%85%8d%e7%bd%ae%e4%bc%98%e5%8c%96>#</a></h3><h4 id=a-网络延迟><strong>a. 网络延迟</strong>
<a class=anchor href=#a-%e7%bd%91%e7%bb%9c%e5%bb%b6%e8%bf%9f>#</a></h4><ul><li>etcd 节点之间需要进行高频率的通信，因此，网络延迟对 etcd 的性能影响非常大。应保证 etcd 集群的节点位于低延迟的网络环境中，尽量避免跨地区和不同数据中心部署。</li><li><strong>建议</strong>：使用 <strong>10Gbps</strong> 或更高速率的网络连接，并避免使用共享带宽。</li></ul><h4 id=b-流量限制与拥塞><strong>b. 流量限制与拥塞</strong>
<a class=anchor href=#b-%e6%b5%81%e9%87%8f%e9%99%90%e5%88%b6%e4%b8%8e%e6%8b%a5%e5%a1%9e>#</a></h4><ul><li>配置网络的带宽限制和防止拥塞对于 etcd 性能至关重要，尤其是当数据写入量大时。如果使用 Kubernetes 时，保证网络中没有其他服务干扰 etcd 的网络流量。</li></ul><h4 id=c-配置通信加密><strong>c. 配置通信加密</strong>
<a class=anchor href=#c-%e9%85%8d%e7%bd%ae%e9%80%9a%e4%bf%a1%e5%8a%a0%e5%af%86>#</a></h4><ul><li>etcd 支持 <strong>TLS 加密</strong> 来保护节点间的通信。虽然加密会带来一些性能开销，但它对集群的安全性至关重要。务必确保使用强加密算法并正确配置证书。</li></ul><hr><h3 id=3-etcd-配置参数调整>3. <strong>etcd 配置参数调整</strong>
<a class=anchor href=#3-etcd-%e9%85%8d%e7%bd%ae%e5%8f%82%e6%95%b0%e8%b0%83%e6%95%b4>#</a></h3><h4 id=a><strong>a. <code>--max-request-bytes</code></strong>
<a class=anchor href=#a>#</a></h4><ul><li>这个参数用于限制每个请求的最大字节数。如果你在集群中有非常大的请求或非常频繁的请求，增大该值可以避免请求被拒绝。</li><li>推荐设置：根据需求调整，通常 1 MB 到 10 MB 之间合适。</li></ul><h4 id=b><strong>b. <code>--heartbeat-interval</code> 和 <code>--election-timeout</code></strong>
<a class=anchor href=#b>#</a></h4><ul><li><strong><code>--heartbeat-interval</code></strong>：控制 leader 节点和 follower 节点之间心跳的时间间隔，默认 100ms。如果心跳间隔设置得太长，集群内的节点可能会过早认为 leader 节点失效，从而触发选举。</li><li><strong><code>--election-timeout</code></strong>：控制选举超时时间，默认 1000ms。如果这个时间设置得太短，会导致频繁的选举过程；设置得过长，会导致集群恢复时间过慢。</li><li><strong>建议</strong>：一般情况下，心跳间隔 <code>--heartbeat-interval</code> 设置为 100ms 到 500ms，选举超时 <code>--election-timeout</code> 设置为 1000ms 到 2000ms。</li></ul><h4 id=c><strong>c. <code>--quota-backend-bytes</code></strong>
<a class=anchor href=#c>#</a></h4><ul><li>此参数控制 etcd 的数据存储最大限制，它表示 etcd 数据库文件最大占用的字节数。超过该值时，etcd 将阻止更多的数据写入。</li><li>推荐设置：根据实际情况调整，通常为 8 GB 或 16 GB。</li></ul><h4 id=d><strong>d. <code>--max-txn-ops</code></strong>
<a class=anchor href=#d>#</a></h4><ul><li>该参数用于设置单个事务中最大操作数。设置过大可能导致事务执行缓慢，甚至超时。</li><li>推荐设置：一般不需要设置过大，默认值（<code>--max-txn-ops=128</code>）适合大多数场景。</li></ul><h4 id=e><strong>e. <code>--snapshot-count</code></strong>
<a class=anchor href=#e>#</a></h4><ul><li><code>--snapshot-count</code> 是控制 etcd 每多少个写入操作后生成一个快照。快照有助于减少存储使用量，并提高数据库恢复速度。</li><li>推荐设置：根据数据量大小，可以设置为 10000-50000。</li></ul><h4 id=f><strong>f. <code>--compression</code></strong>
<a class=anchor href=#f>#</a></h4><ul><li>etcd 支持启用 <strong>数据压缩</strong> 来节省存储空间。启用压缩可能会带来额外的 CPU 开销，但对于存储密集型的应用，启用压缩可以有效减少磁盘空间占用。</li><li>推荐设置：<code>--compression=snappy</code> 或 <code>--compression=lz4</code>，它们提供了较好的压缩率和性能。</li></ul><hr><h3 id=4-集群规模与拓扑设计>4. <strong>集群规模与拓扑设计</strong>
<a class=anchor href=#4-%e9%9b%86%e7%be%a4%e8%a7%84%e6%a8%a1%e4%b8%8e%e6%8b%93%e6%89%91%e8%ae%be%e8%ae%a1>#</a></h3><h4 id=a-节点数目><strong>a. 节点数目</strong>
<a class=anchor href=#a-%e8%8a%82%e7%82%b9%e6%95%b0%e7%9b%ae>#</a></h4><ul><li><strong>etcd 集群的节点数</strong>：etcd 是一个 <strong>奇数个节点</strong> 的分布式集群，以保证选举过程中不会出现脑裂现象。推荐部署 3、5 或 7 个节点的集群。</li><li>需要注意的是，节点数目越多，选举和一致性保证的成本也越高。通常，3 个节点适用于大多数生产环境。</li></ul><h4 id=b-分布式部署><strong>b. 分布式部署</strong>
<a class=anchor href=#b-%e5%88%86%e5%b8%83%e5%bc%8f%e9%83%a8%e7%bd%b2>#</a></h4><ul><li>如果 etcd 的负载过高，或者需要支持跨数据中心或跨区域的高可用集群，可以考虑使用 <strong>跨数据中心的 etcd 集群</strong>，但需要注意网络延迟和带宽要求。</li></ul><h4 id=c-自动化扩展><strong>c. 自动化扩展</strong>
<a class=anchor href=#c-%e8%87%aa%e5%8a%a8%e5%8c%96%e6%89%a9%e5%b1%95>#</a></h4><ul><li>etcd 集群中的节点可以根据需求进行自动扩展。比如可以使用 <strong>Kubernetes</strong> 的 <strong>StatefulSet</strong> 来管理 etcd 的自动扩展和故障恢复。</li></ul><hr><h3 id=5-监控与维护>5. <strong>监控与维护</strong>
<a class=anchor href=#5-%e7%9b%91%e6%8e%a7%e4%b8%8e%e7%bb%b4%e6%8a%a4>#</a></h3><h4 id=a-监控><strong>a. 监控</strong>
<a class=anchor href=#a-%e7%9b%91%e6%8e%a7>#</a></h4><ul><li><p>使用</p><p>Prometheus</p><p>和</p><p>Grafana</p><p>来监控 etcd 的各项指标，包括但不限于：</p><ul><li><strong>etcd_disk_wal_fsync_duration_seconds</strong>：磁盘 WAL 写入的延迟。</li><li><strong>etcd_network_round_trip_time_seconds</strong>：网络往返时延。</li><li><strong>etcd_server_has_leader</strong>：查看是否有 leader 节点。</li><li><strong>etcd_backend_commit_duration_seconds</strong>：后端提交的延迟。</li></ul></li></ul><h4 id=b-定期备份><strong>b. 定期备份</strong>
<a class=anchor href=#b-%e5%ae%9a%e6%9c%9f%e5%a4%87%e4%bb%bd>#</a></h4><ul><li>定期备份 etcd 数据是非常重要的，尤其是在配置变更和集群扩展时。可以使用 <strong>etcdctl snapshot save</strong> 命令进行备份，建议每天进行一次完整备份，并将备份存储在不同的位置（如远程存储）。</li><li>在集群发生故障时，定期备份可以帮助恢复集群状态，减少数据丢失。</li></ul><h4 id=c-定期清理><strong>c. 定期清理</strong>
<a class=anchor href=#c-%e5%ae%9a%e6%9c%9f%e6%b8%85%e7%90%86>#</a></h4><ul><li>清理 <strong>etcd</strong> 中的无用数据，如过期的 Key 或历史版本，确保 etcd 的磁盘空间得到有效管理。</li></ul><hr><h3 id=6-故障排查>6. <strong>故障排查</strong>
<a class=anchor href=#6-%e6%95%85%e9%9a%9c%e6%8e%92%e6%9f%a5>#</a></h3><ul><li><strong>ETCD 超时问题</strong>：etcd 集群出现超时时，检查节点间的网络延迟、磁盘 I/O 性能以及 etcd 配置中的超时参数。</li><li><strong>数据恢复</strong>：在 etcd 出现故障时，使用备份恢复数据，并确保恢复的节点与其他节点之间的数据一致性。</li><li><strong>负载过高</strong>：检查 etcd 的 CPU 和内存使用情况，查看是否存在内存泄漏或 CPU 使用率过高的情况，适时调整资源配置。</li></ul><hr><h3 id=总结-26>总结：
<a class=anchor href=#%e6%80%bb%e7%bb%93-26>#</a></h3><p>通过合理的硬件配置、网络优化、etcd 参数调整、集群规模设计以及良好的监控和备份策略，可以有效提高 etcd 的性能和稳定性，确保 Kubernetes 集群的可靠运行。</p><h2 id=假设k8s集群规模上千需要注意的问题有哪些>假设k8s集群规模上千，需要注意的问题有哪些？
<a class=anchor href=#%e5%81%87%e8%ae%bek8s%e9%9b%86%e7%be%a4%e8%a7%84%e6%a8%a1%e4%b8%8a%e5%8d%83%e9%9c%80%e8%a6%81%e6%b3%a8%e6%84%8f%e7%9a%84%e9%97%ae%e9%a2%98%e6%9c%89%e5%93%aa%e4%ba%9b>#</a></h2><p>在 Kubernetes 集群规模达到上千节点时，集群的管理、性能、可用性和安全性都会面临更高的挑战。为了确保集群在大规模环境下的高效运行，需要注意以下几个方面：</p><h3 id=1-集群架构与资源规划>1. <strong>集群架构与资源规划</strong>
<a class=anchor href=#1-%e9%9b%86%e7%be%a4%e6%9e%b6%e6%9e%84%e4%b8%8e%e8%b5%84%e6%ba%90%e8%a7%84%e5%88%92>#</a></h3><h4 id=a-集群拓扑设计><strong>a. 集群拓扑设计</strong>
<a class=anchor href=#a-%e9%9b%86%e7%be%a4%e6%8b%93%e6%89%91%e8%ae%be%e8%ae%a1>#</a></h4><ul><li><strong>高可用架构</strong>：确保 <strong>API Server</strong>、<strong>etcd</strong>、<strong>控制面组件</strong> 和 <strong>工作节点</strong> 等关键组件具有高可用性。需要部署多个副本，以保证任意组件的单点故障不会导致集群的不可用。</li><li><strong>分布式部署</strong>：根据节点的数量，将集群分布在多个物理或虚拟数据中心，以减少单个机房或机架的故障影响。通过 Kubernetes <strong>多区域集群</strong> 或 <strong>多集群</strong> 架构来提升灾备能力。</li></ul><h4 id=b-api-server><strong>b. API Server</strong>
<a class=anchor href=#b-api-server>#</a></h4><ul><li>对于上千节点的集群，需要对 <strong>API Server</strong> 的负载均衡、水平扩展（<code>--max-requests-inflight</code>、<code>--max-mutating-requests-inflight</code> 等）进行调整。</li><li>确保有足够的 API Server 副本，合理使用 <strong>Ingress</strong> 或 <strong>负载均衡器</strong> 进行流量分配，避免单点故障。</li></ul><h4 id=c-etcd-集群><strong>c. etcd 集群</strong>
<a class=anchor href=#c-etcd-%e9%9b%86%e7%be%a4>#</a></h4><ul><li>集群规模较大时，<strong>etcd</strong> 的性能瓶颈可能会成为集群的瓶颈。建议使用至少 5 个 <strong>etcd 节点</strong>，并确保它们处于低延迟的网络环境中。配置合理的备份策略，并确保其容量足以存储集群状态。</li></ul><h4 id=d-网络拓扑与性能><strong>d. 网络拓扑与性能</strong>
<a class=anchor href=#d-%e7%bd%91%e7%bb%9c%e6%8b%93%e6%89%91%e4%b8%8e%e6%80%a7%e8%83%bd>#</a></h4><ul><li>需要考虑 <strong>Pod 网络的性能</strong>，如网络插件（Calico、Flannel、Cilium 等）选择和网络拓扑设计。对于大规模集群，使用 <strong>IPVS</strong> 模式的 kube-proxy 会有更好的性能。</li><li><strong>网络分段</strong>：合理规划 <strong>网络策略</strong> 和 <strong>命名空间</strong>，确保集群内流量高效、可控且安全。</li></ul><hr><h3 id=2-资源管理与调度>2. <strong>资源管理与调度</strong>
<a class=anchor href=#2-%e8%b5%84%e6%ba%90%e7%ae%a1%e7%90%86%e4%b8%8e%e8%b0%83%e5%ba%a6>#</a></h3><h4 id=a-节点管理><strong>a. 节点管理</strong>
<a class=anchor href=#a-%e8%8a%82%e7%82%b9%e7%ae%a1%e7%90%86>#</a></h4><ul><li>对于上千个节点，手动管理变得复杂。需要借助 <strong>自动化工具</strong> 来动态添加、删除和管理节点，确保集群始终保持健康状态。</li><li>使用 <strong>Node Affinity</strong> 和 <strong>Taints/Tolerations</strong> 来确保 Pod 调度到适当的节点上，避免资源冲突。</li></ul><h4 id=b-集群资源配额><strong>b. 集群资源配额</strong>
<a class=anchor href=#b-%e9%9b%86%e7%be%a4%e8%b5%84%e6%ba%90%e9%85%8d%e9%a2%9d>#</a></h4><ul><li>需要在 <strong>命名空间级别</strong> 配置合理的资源配额，避免某个命名空间或应用占用过多资源，影响整个集群的稳定性。</li><li>通过 <strong>Horizontal Pod Autoscaling (HPA)</strong> 和 <strong>Vertical Pod Autoscaling (VPA)</strong> 来自动调整 Pod 的资源请求，确保资源得到合理分配。</li></ul><h4 id=c-集群容量规划><strong>c. 集群容量规划</strong>
<a class=anchor href=#c-%e9%9b%86%e7%be%a4%e5%ae%b9%e9%87%8f%e8%a7%84%e5%88%92>#</a></h4><ul><li>需要定期进行 <strong>容量规划</strong>，根据应用需求预测 CPU、内存和存储的需求。通过监控数据和负载测试评估集群资源的使用情况，并做出相应调整。</li></ul><hr><h3 id=3-性能与可伸缩性>3. <strong>性能与可伸缩性</strong>
<a class=anchor href=#3-%e6%80%a7%e8%83%bd%e4%b8%8e%e5%8f%af%e4%bc%b8%e7%bc%a9%e6%80%a7>#</a></h3><h4 id=a-控制平面性能优化><strong>a. 控制平面性能优化</strong>
<a class=anchor href=#a-%e6%8e%a7%e5%88%b6%e5%b9%b3%e9%9d%a2%e6%80%a7%e8%83%bd%e4%bc%98%e5%8c%96>#</a></h4><ul><li>在规模较大的集群中，<strong>控制平面</strong>（如 API Server、Controller Manager 等）可能成为瓶颈。需要考虑通过 <strong>水平扩展</strong> 来增加 API Server 的副本数。</li><li>增加 <strong>etcd</strong> 节点的数量以减少锁争用，并确保其存储性能足够支撑大量节点和请求。</li></ul><h4 id=b-pod-和服务的性能优化><strong>b. Pod 和服务的性能优化</strong>
<a class=anchor href=#b-pod-%e5%92%8c%e6%9c%8d%e5%8a%a1%e7%9a%84%e6%80%a7%e8%83%bd%e4%bc%98%e5%8c%96>#</a></h4><ul><li><strong>Pod 调度优化</strong>：确保 Pod 调度的高效性，合理配置 <strong>资源请求和限制</strong>，避免资源不足或资源过载的情况。</li><li>对 <strong>Services</strong> 使用 <strong>ClusterIP</strong> 模式时，使用 <strong>IPVS</strong> 模式的 kube-proxy 来提高服务发现性能。</li><li>使用 <strong>网络插件</strong>（如 Calico 或 Cilium）来提高集群内的网络性能，确保大规模集群中的容器网络通信不会成为瓶颈。</li></ul><h4 id=c-日志和监控系统的扩展><strong>c. 日志和监控系统的扩展</strong>
<a class=anchor href=#c-%e6%97%a5%e5%bf%97%e5%92%8c%e7%9b%91%e6%8e%a7%e7%b3%bb%e7%bb%9f%e7%9a%84%e6%89%a9%e5%b1%95>#</a></h4><ul><li><strong>监控系统</strong>（如 Prometheus、Grafana）需要确保能够处理大规模集群的指标数据。如果指标量过大，可以考虑使用 <strong>Prometheus 联邦</strong> 或 <strong>分布式 Prometheus</strong> 系统来处理。</li><li><strong>日志收集系统</strong>（如 ELK 或 Loki）需要做适当的扩展，确保日志数据存储和处理不会影响集群性能。</li></ul><hr><h3 id=4-高可用与容灾>4. <strong>高可用与容灾</strong>
<a class=anchor href=#4-%e9%ab%98%e5%8f%af%e7%94%a8%e4%b8%8e%e5%ae%b9%e7%81%be>#</a></h3><h4 id=a-高可用性设计><strong>a. 高可用性设计</strong>
<a class=anchor href=#a-%e9%ab%98%e5%8f%af%e7%94%a8%e6%80%a7%e8%ae%be%e8%ae%a1>#</a></h4><ul><li><strong>API Server、etcd、Controller Manager、Scheduler</strong> 等控制面组件必须具有高可用性，至少部署 3 个副本，确保在节点或区域故障时，控制面组件能够继续运行。</li><li><strong>负载均衡器</strong>：对于大规模集群，建议部署高性能的 <strong>负载均衡器</strong>（如 HAProxy、Nginx），并确保它们支持 API Server 负载均衡和流量控制。</li></ul><h4 id=b-灾难恢复与备份><strong>b. 灾难恢复与备份</strong>
<a class=anchor href=#b-%e7%81%be%e9%9a%be%e6%81%a2%e5%a4%8d%e4%b8%8e%e5%a4%87%e4%bb%bd>#</a></h4><ul><li>定期备份 <strong>etcd 数据</strong>，并且确保备份数据的可靠性。考虑使用多区域存储或异地备份，确保在灾难发生时能够快速恢复。</li><li>配置 <strong>跨区域故障恢复</strong>，通过 Kubernetes 的 <strong>多集群架构</strong> 来实现跨区域的灾难恢复。</li></ul><hr><h3 id=5-安全与合规>5. <strong>安全与合规</strong>
<a class=anchor href=#5-%e5%ae%89%e5%85%a8%e4%b8%8e%e5%90%88%e8%a7%84>#</a></h3><h4 id=a-集群安全><strong>a. 集群安全</strong>
<a class=anchor href=#a-%e9%9b%86%e7%be%a4%e5%ae%89%e5%85%a8>#</a></h4><ul><li><strong>身份认证与授权</strong>：大规模集群中，应该使用强制认证的方式来确保集群的安全。确保 <strong>RBAC（Role-Based Access Control）</strong> 和 <strong>PodSecurityPolicy</strong> 被严格配置，避免不必要的权限暴露。</li><li><strong>Network Policy</strong>：使用 <strong>NetworkPolicy</strong> 来限制 Pod 之间和 Pod 与外部的通信，减少潜在的安全风险。</li><li><strong>Pod 安全</strong>：配置 <strong>PodSecurity</strong>（PSP）策略，确保容器在适当的安全上下文中运行，限制特权容器等不安全的操作。</li></ul><h4 id=b-集群合规性><strong>b. 集群合规性</strong>
<a class=anchor href=#b-%e9%9b%86%e7%be%a4%e5%90%88%e8%a7%84%e6%80%a7>#</a></h4><ul><li>由于集群规模较大，可能涉及合规要求，因此需要定期审查集群的安全策略和操作日志，确保其符合公司或行业的安全标准。</li><li><strong>容器镜像扫描</strong>：对容器镜像进行安全扫描，确保没有已知漏洞的镜像被用于生产环境。</li></ul><hr><h3 id=6-故障排查与诊断>6. <strong>故障排查与诊断</strong>
<a class=anchor href=#6-%e6%95%85%e9%9a%9c%e6%8e%92%e6%9f%a5%e4%b8%8e%e8%af%8a%e6%96%ad>#</a></h3><h4 id=a-异常检测与诊断><strong>a. 异常检测与诊断</strong>
<a class=anchor href=#a-%e5%bc%82%e5%b8%b8%e6%a3%80%e6%b5%8b%e4%b8%8e%e8%af%8a%e6%96%ad>#</a></h4><ul><li>使用 <strong>Prometheus、Grafana、Alertmanager</strong> 等工具建立完善的告警系统，能够及时检测到集群中出现的异常（如 CPU/内存资源超限、Pod 网络延迟过高等）。</li><li><strong>日志分析</strong>：使用 <strong>ELK Stack（Elasticsearch、Logstash、Kibana）</strong> 或 <strong>Loki</strong> 来收集和分析集群的日志。对于大规模集群，日志存储和查询需要足够的性能来支持高频率的写入和查询操作。</li></ul><h4 id=b-节点故障检测><strong>b. 节点故障检测</strong>
<a class=anchor href=#b-%e8%8a%82%e7%82%b9%e6%95%85%e9%9a%9c%e6%a3%80%e6%b5%8b>#</a></h4><ul><li>在大规模集群中，节点故障是不可避免的。通过 <strong>Kubernetes 的自动扩展功能</strong>（如节点自动扩展和 Pod 自动重启）来快速恢复集群健康状态。</li><li><strong>节点故障转移</strong>：通过 <strong>PodDisruptionBudget</strong> 和 <strong>Pod Affinity/Anti-Affinity</strong> 策略来确保节点故障时应用能继续运行。</li></ul><h4 id=c-监控与优化><strong>c. 监控与优化</strong>
<a class=anchor href=#c-%e7%9b%91%e6%8e%a7%e4%b8%8e%e4%bc%98%e5%8c%96>#</a></h4><ul><li>定期评估集群的性能，利用 <strong>Prometheus</strong> 收集 <strong>etcd</strong>、<strong>kubelet</strong>、<strong>Scheduler</strong>、<strong>Controller Manager</strong> 等关键组件的性能数据。</li><li>对于大规模集群，可以考虑使用 <strong>Kubernetes Federation</strong> 来管理多个集群，并通过联邦控制多个集群的状态和健康检查。</li></ul><hr><h3 id=总结-27>总结
<a class=anchor href=#%e6%80%bb%e7%bb%93-27>#</a></h3><p>当 Kubernetes 集群规模达到上千节点时，面临的挑战和复杂性大大增加。为了确保集群的可用性、性能和安全性，需要采取以下措施：</p><ol><li>进行合理的 <strong>集群架构设计</strong> 和 <strong>资源规划</strong>，确保集群的高可用性和可伸缩性。</li><li>优化 <strong>API Server</strong>、<strong>etcd</strong> 和 <strong>网络插件</strong> 的配置，以支持大规模的负载。</li><li>部署 <strong>监控、日志收集系统</strong>，并进行 <strong>故障排查</strong>，及时发现并处理集群中的问题。</li><li>实现强有力的 <strong>安全措施</strong> 和 <strong>合规策略</strong>，确保集群的安全性和合规性。</li></ol><p>通过这些措施，可以确保大规模集群的稳定性和高效运行。</p><p>如果在 Kubernetes 中的 <strong>Pod</strong> 没有显式设置 <code>resources.requests</code> 和 <code>resources.limits</code>，那么 <strong>VPA</strong> 会基于容器的实际资源使用情况自动生成推荐的资源请求（<code>request</code>）和限制（<code>limit</code>），但它不能直接强制限制容器的资源。在这种情况下，VPA 会根据 <strong>历史使用数据</strong> 为容器提供合理的资源请求和限制的建议。</p><h3 id=关键点总结>关键点总结：
<a class=anchor href=#%e5%85%b3%e9%94%ae%e7%82%b9%e6%80%bb%e7%bb%93>#</a></h3><ol><li><strong>VPA 的工作方式</strong>：<ul><li>VPA 会监控容器的资源使用情况（例如 CPU 和内存），然后基于过去的使用模式来调整推荐的 <code>resources.requests</code> 和 <code>resources.limits</code>。</li><li>如果你没有设置 <code>request</code> 和 <code>limit</code>，VPA 将会通过观察容器的实际负载来建议一个合适的资源请求配置，并自动应用这个推荐值。</li></ul></li><li><strong><code>requests</code> 和 <code>limits</code> 重要性</strong>：<ul><li><strong><code>request</code></strong>：是容器启动时请求的资源量，调度器依据该值来决定将容器调度到哪个节点。Pod 在启动时会至少保证这个资源量。</li><li><strong><code>limit</code></strong>：是容器使用的资源上限。如果容器使用超过 <code>limit</code> 的资源，Kubernetes 会限制容器的资源使用，并且可能会杀死该容器。</li></ul></li><li><strong>VPA 的作用</strong>：<ul><li><strong>VPA 推荐资源配置</strong>：如果 Pod 没有设置 <code>resources.requests</code> 和 <code>resources.limits</code>，VPA 会基于监控到的资源使用情况，自动为这些 Pod 提供资源请求和限制的推荐值。这些推荐值的计算依赖于容器的历史资源消耗数据。</li><li><strong>自动调整</strong>：VPA 会自动调整容器的资源请求，保证容器获得合适的资源配置，避免容器因为资源不足而崩溃（比如 OOM），也避免资源过多分配造成浪费。</li></ul></li><li><strong>VPA 不会直接强制执行限制</strong>：<ul><li><strong>VPA 只会推荐</strong>，它并不会直接限制容器的资源，必须结合 <code>Deployment</code> 或其他控制器来应用这些调整。如果没有设置 <code>resources.requests</code> 和 <code>resources.limits</code>，VPA 会依据使用情况推荐适当的值。</li></ul></li></ol><h3 id=总结-28>总结：
<a class=anchor href=#%e6%80%bb%e7%bb%93-28>#</a></h3><ul><li>如果 Pod 没有设置 <code>resources.requests</code> 和 <code>resources.limits</code>，VPA 会 <strong>基于实际资源使用情况</strong> 提供推荐，但它不会直接强制执行资源限制，且这些推荐值必须在 <strong>Deployment</strong> 或其他控制器中应用。</li><li>为了让 VPA 工作，并有效避免资源瓶颈或浪费，建议还是显式为每个 Pod 设置 <code>requests</code> 和 <code>limits</code>，让 VPA 在此基础上进行优化。</li></ul><h2 id=节点notready可能的原因会导致哪些问题>节点NotReady可能的原因？会导致哪些问题？☆
<a class=anchor href=#%e8%8a%82%e7%82%b9notready%e5%8f%af%e8%83%bd%e7%9a%84%e5%8e%9f%e5%9b%a0%e4%bc%9a%e5%af%bc%e8%87%b4%e5%93%aa%e4%ba%9b%e9%97%ae%e9%a2%98>#</a></h2><p>在 Kubernetes 中，<strong>节点（Node）处于 <code>NotReady</code> 状态</strong> 表示该节点无法正常与控制平面（如 API Server）进行通信或其健康状况不符合要求。<code>NotReady</code> 状态通常表示该节点上的 <strong>Kubelet</strong> 或 <strong>容器运行时</strong> 等关键组件出现了问题，导致 Kubernetes 无法调度或管理该节点上的 Pod。</p><h3 id=可能导致节点-notready-状态的原因>可能导致节点 <code>NotReady</code> 状态的原因
<a class=anchor href=#%e5%8f%af%e8%83%bd%e5%af%bc%e8%87%b4%e8%8a%82%e7%82%b9-notready-%e7%8a%b6%e6%80%81%e7%9a%84%e5%8e%9f%e5%9b%a0>#</a></h3><ol><li><strong>Kubelet 健康检查失败</strong>：<ul><li><strong>Kubelet</strong> 是 Kubernetes 节点的核心组件，负责与控制平面通信、监控节点上的 Pod 和容器。如果 Kubelet 无法正常启动或无法与 API Server 通信，节点会变为 <code>NotReady</code>。</li><li>可能的原因包括：Kubelet 配置错误、API Server 无法访问、Kubelet 进程崩溃等。</li></ul></li><li><strong>节点资源不足</strong>：<ul><li>节点上的 <strong>CPU、内存</strong> 或 <strong>磁盘</strong> 等资源耗尽可能导致 Kubelet 无法正常运行或节点负载过高，从而进入 <code>NotReady</code> 状态。</li><li>例如，磁盘满了导致日志写入失败，或者内存耗尽导致系统性能问题。</li></ul></li><li><strong>容器运行时问题</strong>：<ul><li>Kubernetes 使用容器运行时（如 Docker 或 containerd）来管理容器。如果容器运行时出现故障（如死锁、崩溃、配置错误等），则会影响节点的健康状况。</li><li>例如，容器运行时无法启动或没有运行，导致 Kubelet 无法管理节点上的 Pod。</li></ul></li><li><strong>网络故障</strong>：<ul><li>如果节点与集群其他节点或 API Server 之间的网络通信失败，节点无法与控制平面同步，导致状态被标记为 <code>NotReady</code>。</li><li>网络问题可能包括 DNS 解析失败、路由或防火墙配置错误等。</li></ul></li><li><strong>Pod 或容器状态异常</strong>：<ul><li>节点上运行的某些 Pod 或容器可能处于崩溃状态，影响整个节点的健康。例如，多个容器进入 CrashLoopBackOff 状态，可能导致节点的资源紧张，从而影响节点状态。</li></ul></li><li><strong>磁盘或文件系统错误</strong>：<ul><li>节点的磁盘或文件系统出现故障（如损坏、无法挂载等），也可能导致节点无法正常运行，进而导致 <code>NotReady</code> 状态。</li></ul></li><li><strong>Kube-proxy 问题</strong>：<ul><li><strong>Kube-proxy</strong> 是负责实现服务负载均衡和网络代理的组件。如果 kube-proxy 出现故障或配置错误，也可能导致节点处于 <code>NotReady</code> 状态，尤其是涉及到网络通信的部分。</li></ul></li><li><strong>节点健康检查配置错误</strong>：<ul><li>Kubernetes 节点的健康检查（如 <code>kubelet</code> 自身的健康检查）可以配置为自动检查节点的健康状况。如果健康检查失败，节点也会被标记为 <code>NotReady</code>。</li></ul></li><li><strong>节点证书问题</strong>：<ul><li>如果节点的证书过期或被吊销，Kubelet 与 API Server 之间的 TLS 通信无法正常进行，节点可能会进入 <code>NotReady</code> 状态。</li></ul></li><li><strong>资源调度冲突</strong>：<ul><li>节点上有未解决的资源冲突或依赖问题，可能导致 Kubelet 或其他关键组件不能正常工作。</li></ul></li></ol><hr><h3 id=节点-notready-状态可能导致的问题>节点 <code>NotReady</code> 状态可能导致的问题
<a class=anchor href=#%e8%8a%82%e7%82%b9-notready-%e7%8a%b6%e6%80%81%e5%8f%af%e8%83%bd%e5%af%bc%e8%87%b4%e7%9a%84%e9%97%ae%e9%a2%98>#</a></h3><ol><li><strong>Pod 无法调度</strong>：<ul><li>Kubernetes 调度器会避免将新的 Pod 调度到 <code>NotReady</code> 状态的节点，因为该节点无法提供必要的资源或服务。</li></ul></li><li><strong>现有 Pod 无法访问</strong>：<ul><li>如果节点被标记为 <code>NotReady</code>，节点上已经运行的 Pod 可能会面临访问失败或无法与其他服务通信的问题。这可能导致集群的服务中断。</li></ul></li><li><strong>影响服务可用性</strong>：<ul><li>如果一个或多个关键节点进入 <code>NotReady</code> 状态，且这些节点运行着重要的应用或服务，可能会导致应用的部分功能不可用，甚至出现服务完全宕机的情况。</li></ul></li><li><strong>影响负载均衡与网络通信</strong>：<ul><li>节点在 <code>NotReady</code> 状态时，Kubernetes 集群中的 <strong>kube-proxy</strong> 可能会更新其路由信息，从而导致集群中某些网络流量被错误路由或丢失。</li><li>如果节点使用的是 IPVS 或 iptables 模式，负载均衡会受到影响。</li></ul></li><li><strong>资源管理混乱</strong>：<ul><li>节点进入 <code>NotReady</code> 状态会导致节点上的资源无法有效管理，Pod 的重调度可能导致其他节点资源的过载，影响整个集群的性能。</li></ul></li><li><strong>可能影响自动扩展</strong>：<ul><li>对于启用了 <strong>自动扩展</strong>（如 HPA 和 VPA）的集群，如果节点 <code>NotReady</code>，自动扩展可能无法在预定时间内完成任务，从而导致资源分配不均衡。</li></ul></li></ol><hr><h3 id=排查节点-notready-状态的思路>排查节点 <code>NotReady</code> 状态的思路
<a class=anchor href=#%e6%8e%92%e6%9f%a5%e8%8a%82%e7%82%b9-notready-%e7%8a%b6%e6%80%81%e7%9a%84%e6%80%9d%e8%b7%af>#</a></h3><ol><li><strong>检查 Kubelet 状态</strong>：<ul><li>使用 <code>systemctl status kubelet</code> 或查看 <code>journalctl -u kubelet</code> 日志，检查 Kubelet 是否正常运行，是否有错误信息。</li></ul></li><li><strong>查看节点状态</strong>：<ul><li>使用 <code>kubectl describe node &lt;node-name></code> 查看节点的详细信息，特别是 Events 部分，看是否有相关的错误提示或警告信息。</li></ul></li><li><strong>检查资源使用情况</strong>：<ul><li>查看节点的资源使用情况，使用 <code>kubectl top node &lt;node-name></code> 或通过其他监控工具（如 Prometheus）查看节点的 CPU、内存和磁盘使用情况，检查是否出现资源耗尽的情况。</li></ul></li><li><strong>检查容器运行时状态</strong>：<ul><li>检查容器运行时的日志，使用 <code>docker info</code> 或 <code>containerd</code> 工具查看容器运行时是否正常运行。</li></ul></li><li><strong>查看网络状态</strong>：<ul><li>使用 <code>kubectl get pods --all-namespaces -o wide</code> 检查 Pod 分布和节点网络状态，确保没有网络故障，Pod 能正常与其他节点通信。</li></ul></li><li><strong>检查磁盘和文件系统</strong>：<ul><li>确保节点的磁盘和文件系统没有出现故障。可以查看磁盘空间使用情况 (<code>df -h</code>)，或检查是否有磁盘挂载问题。</li></ul></li><li><strong>检查节点证书和身份验证</strong>：<ul><li>确保节点证书没有过期，并且 Kubelet 能与 API Server 正常通信。</li></ul></li><li><strong>检查节点健康检查配置</strong>：<ul><li>查看节点的健康检查配置是否正常，确认 Kubelet 和 API Server 之间的连接没有问题。</li></ul></li><li><strong>使用监控工具</strong>：<ul><li>通过集群监控工具（如 Prometheus、Grafana）实时监控节点和 Pod 的状态，及时发现问题并处理。</li></ul></li></ol><hr><h3 id=总结-29>总结
<a class=anchor href=#%e6%80%bb%e7%bb%93-29>#</a></h3><p>节点 <code>NotReady</code> 状态的原因多种多样，常见的有 Kubelet 健康检查失败、资源不足、容器运行时故障、网络问题等。<code>NotReady</code> 状态会导致 Pod 无法调度、现有服务无法访问、负载均衡异常等问题，严重时会影响整个集群的可用性和稳定性。解决此问题需要排查节点日志、资源使用情况、网络状态等，逐步定位故障源并进行修复。</p><h2 id=service和endpoints是如何关联的>service和endpoints是如何关联的？
<a class=anchor href=#service%e5%92%8cendpoints%e6%98%af%e5%a6%82%e4%bd%95%e5%85%b3%e8%81%94%e7%9a%84>#</a></h2><p>在 Kubernetes 中，<strong>Service</strong> 和 <strong>Endpoints</strong> 是两个非常重要的资源对象，它们通过 <strong>Endpoints</strong> 来实现对 <strong>Service</strong> 的访问路由和负载均衡。</p><h3 id=1-service-的作用>1. <strong>Service 的作用</strong>
<a class=anchor href=#1-service-%e7%9a%84%e4%bd%9c%e7%94%a8>#</a></h3><p><code>Service</code> 是 Kubernetes 中一个抽象层，用于暴露一组运行在 Pod 中的应用或服务，它提供一个稳定的网络访问地址（DNS 或 IP），而这些 Pod 可能会随着时间的推移而被创建或销毁。通过 Service，可以确保即使 Pod 被替换或重启，客户端仍然能够通过 Service 访问到相应的应用。</p><h3 id=2-endpoints-的作用>2. <strong>Endpoints 的作用</strong>
<a class=anchor href=#2-endpoints-%e7%9a%84%e4%bd%9c%e7%94%a8>#</a></h3><p><code>Endpoints</code> 资源是 Kubernetes 中 Service 对象的实际目标。每当 Service 创建时，Kubernetes 会为它自动创建对应的 Endpoints 资源。<code>Endpoints</code> 列出了与某个 Service 相关的所有 Pod 的 IP 地址和端口。通过这些 Endpoints，Kubernetes 知道如何将流量路由到这些 Pod。</p><h3 id=3-service-和-endpoints-的关联方式>3. <strong>Service 和 Endpoints 的关联方式</strong>
<a class=anchor href=#3-service-%e5%92%8c-endpoints-%e7%9a%84%e5%85%b3%e8%81%94%e6%96%b9%e5%bc%8f>#</a></h3><p>Service 和 Endpoints 是通过 Pod 的标签（Labels）进行关联的。具体来说，Service 会根据其 <strong>selector</strong> 字段来选择一组 Pod，这些 Pod 组成了 Service 的目标。在创建 Service 时，Kubernetes 会根据 selector 自动发现符合条件的 Pod，并将其 IP 和端口添加到对应的 Endpoints 资源中。</p><ul><li><strong>Service</strong>：通过 <code>spec.selector</code> 字段来定义匹配哪些 Pod（例如根据标签选择）。</li><li><strong>Endpoints</strong>：Service 创建后，自动生成对应的 Endpoints 资源，列出符合选择条件的 Pod 的 IP 地址和端口。</li></ul><h3 id=4-流程说明>4. <strong>流程说明</strong>
<a class=anchor href=#4-%e6%b5%81%e7%a8%8b%e8%af%b4%e6%98%8e>#</a></h3><ol><li><p><strong>创建 Service</strong>：</p><ul><li>用户通过 <code>kubectl expose</code> 或 YAML 文件创建一个 Service，通常会设置 <code>spec.selector</code> 来指定一个标签选择器，用于匹配一组 Pod。例如：</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Service</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-service</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>selector</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>app</span>: <span style=color:#ae81ff>my-app</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>ports</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>port</span>: <span style=color:#ae81ff>80</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>targetPort</span>: <span style=color:#ae81ff>8080</span>
</span></span></code></pre></div><p>这时，Kubernetes 会创建一个名为 <code>my-service</code> 的 Service，并根据 selector 查找匹配的 Pod。</p></li><li><p><strong>匹配 Pod</strong>：</p><ul><li><p>Kubernetes 会根据</p><pre tabindex=0><code>Service
</code></pre><p>的 selector 查找所有匹配的 Pod，假设有以下 Pod：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Pod</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-pod-1</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>labels</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>app</span>: <span style=color:#ae81ff>my-app</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>containers</span>:
</span></span><span style=display:flex><span>  - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-container</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>image</span>: <span style=color:#ae81ff>nginx</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>ports</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>containerPort</span>: <span style=color:#ae81ff>8080</span>
</span></span></code></pre></div></li></ul></li><li><p><strong>创建 Endpoints</strong>：</p><ul><li>对于上述匹配到的 Pod，Kubernetes 会自动为 <code>my-service</code> 创建一个 Endpoints 资源，并将匹配的 Pod 的 IP 地址和端口号列出。例如：</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Endpoints</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-service</span>
</span></span><span style=display:flex><span><span style=color:#f92672>subsets</span>:
</span></span><span style=display:flex><span>  - <span style=color:#f92672>addresses</span>:
</span></span><span style=display:flex><span>      - <span style=color:#f92672>ip</span>: <span style=color:#ae81ff>10.1.2.3</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>ports</span>:
</span></span><span style=display:flex><span>      - <span style=color:#f92672>port</span>: <span style=color:#ae81ff>8080</span>
</span></span></code></pre></div><p>这个 Endpoints 资源记录了 <code>my-pod-1</code> 的 IP 地址和端口。</p></li><li><p><strong>负载均衡与流量转发</strong>：</p><ul><li>当请求发送到 <code>my-service</code> 时，Kubernetes 会查询对应的 Endpoints 资源，选择一个 Pod 的 IP 地址并转发请求。负载均衡可以通过 Round Robin（轮询）方式进行，即请求会轮流转发给不同的 Pod。</li></ul></li></ol><h3 id=5-endpoints-的更新>5. <strong>Endpoints 的更新</strong>
<a class=anchor href=#5-endpoints-%e7%9a%84%e6%9b%b4%e6%96%b0>#</a></h3><ul><li>如果 Pod 的 IP 地址发生变化（比如 Pod 被重新调度或重新创建），Kubernetes 会自动更新 Endpoints 资源，确保 Service 始终指向当前可用的 Pod。</li><li>例如，Pod 被删除或重新调度到其他节点时，Endpoints 中会移除掉已删除 Pod 的信息，加入新 Pod 的 IP 地址。</li></ul><h3 id=6-手动管理-endpoints>6. <strong>手动管理 Endpoints</strong>
<a class=anchor href=#6-%e6%89%8b%e5%8a%a8%e7%ae%a1%e7%90%86-endpoints>#</a></h3><p>在某些场景下，也可以手动创建或管理 Endpoints，尤其是当你需要将 Service 暴露给外部资源（如外部数据库）时。这时，你可以在 Service 中不使用 <code>selector</code>，而是手动指定 Endpoints 对象，给定固定的 IP 和端口。</p><p>例如，手动创建一个 Endpoints 来指向外部服务：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Endpoints</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-service</span>
</span></span><span style=display:flex><span><span style=color:#f92672>subsets</span>:
</span></span><span style=display:flex><span>  - <span style=color:#f92672>addresses</span>:
</span></span><span style=display:flex><span>      - <span style=color:#f92672>ip</span>: <span style=color:#ae81ff>192.168.1.100</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>ports</span>:
</span></span><span style=display:flex><span>      - <span style=color:#f92672>port</span>: <span style=color:#ae81ff>80</span>
</span></span></code></pre></div><h3 id=7-集群外的访问>7. <strong>集群外的访问</strong>
<a class=anchor href=#7-%e9%9b%86%e7%be%a4%e5%a4%96%e7%9a%84%e8%ae%bf%e9%97%ae>#</a></h3><p>对于 <strong>ClusterIP</strong> 类型的 Service，客户端只能在集群内部访问这个 Service。如果你需要从集群外部访问某个 Service，可以使用 <strong>NodePort</strong> 或 <strong>LoadBalancer</strong> 类型的 Service，这时 Kubernetes 会在集群外部创建端口映射。</p><h3 id=8-总结>8. <strong>总结：</strong>
<a class=anchor href=#8-%e6%80%bb%e7%bb%93>#</a></h3><ul><li><strong>Service</strong> 是一个逻辑上的抽象，用于暴露一组后端 Pod。</li><li><strong>Endpoints</strong> 是 Service 的实际目标，记录了所有匹配 Pod 的 IP 地址和端口信息。</li><li>Kubernetes 会根据 Service 的 selector 自动维护 Endpoints 列表，确保流量能够正确路由到正确的 Pod。</li></ul><p>这种机制确保了在 Pod 被动态调度、重新启动或者创建新的 Pod 时，Service 的访问接口始终保持稳定，而 Pod 的具体实现和 IP 地址可以发生变化。</p><h2 id=replicasetdeployment功能是怎么实现的>ReplicaSet、Deployment功能是怎么实现的？
<a class=anchor href=#replicasetdeployment%e5%8a%9f%e8%83%bd%e6%98%af%e6%80%8e%e4%b9%88%e5%ae%9e%e7%8e%b0%e7%9a%84>#</a></h2><p><strong>ReplicaSet</strong> 和 <strong>Deployment</strong> 都是 Kubernetes 中用于管理 Pod 副本和确保集群中应用高可用的资源对象，但它们在功能和使用场景上有一些区别。下面详细讲解这两个对象的功能和实现原理。</p><h3 id=1-replicaset-的功能和实现原理>1. <strong>ReplicaSet</strong> 的功能和实现原理
<a class=anchor href=#1-replicaset-%e7%9a%84%e5%8a%9f%e8%83%bd%e5%92%8c%e5%ae%9e%e7%8e%b0%e5%8e%9f%e7%90%86>#</a></h3><h4 id=功能>功能
<a class=anchor href=#%e5%8a%9f%e8%83%bd>#</a></h4><p><strong>ReplicaSet</strong> 是一种用于维护一组 <strong>Pod 副本</strong> 的控制器。它确保某个时刻集群中总是有指定数量的副本 Pod 在运行，并且这些 Pod 的定义始终符合预期的状态。</p><ul><li><strong>副本数量管理</strong>：ReplicaSet 会根据 <code>spec.replicas</code> 中定义的副本数来管理 Pod 的数量。如果当前的 Pod 数量少于指定的副本数，ReplicaSet 会创建新的 Pod；如果 Pod 数量超过了指定数量，它会删除多余的 Pod。</li><li><strong>Pod 副本的健康管理</strong>：ReplicaSet 会持续监控 Pod 的健康状况。如果某个 Pod 出现故障（如崩溃或被删除），ReplicaSet 会自动创建新的 Pod 来替代它。</li></ul><h4 id=实现原理>实现原理
<a class=anchor href=#%e5%ae%9e%e7%8e%b0%e5%8e%9f%e7%90%86>#</a></h4><p>ReplicaSet 通过一个 <strong>标签选择器（Label Selector）</strong> 来确定哪些 Pod 是由该 ReplicaSet 管理的。ReplicaSet 会与其管理的 Pod 的标签进行匹配，确保符合条件的 Pod 始终保持在集群中。</p><ol><li><p><strong>创建 ReplicaSet</strong>： 用户通过 <code>kubectl</code> 或 YAML 配置创建 ReplicaSet。ReplicaSet 会根据 <code>spec.replicas</code> 中指定的副本数来管理 Pod 副本，确保创建的 Pod 数量始终保持一致。 示例：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>apps/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ReplicaSet</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-replicaset</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>replicas</span>: <span style=color:#ae81ff>3</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>selector</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>matchLabels</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>app</span>: <span style=color:#ae81ff>my-app</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>template</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>labels</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>app</span>: <span style=color:#ae81ff>my-app</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>containers</span>:
</span></span><span style=display:flex><span>      - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-container</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>image</span>: <span style=color:#ae81ff>nginx</span>
</span></span></code></pre></div></li><li><p><strong>控制器的工作原理</strong>：</p><ul><li>ReplicaSet 会监控和维护指定数量的 Pod 副本。</li><li>如果某个 Pod 被删除或失败，ReplicaSet 会创建新的 Pod 进行替代。</li><li>ReplicaSet 会基于 Pod 模板（<code>template</code>）来创建新的 Pod，并确保其满足与 Selector 匹配的条件。</li></ul></li><li><p><strong>Pod 状态的监控</strong>： ReplicaSet 会通过 Kubernetes 的控制循环来检测 Pod 的状态，并通过 API 服务器与 Kubernetes 控制平面进行交互，确保 Pod 的数量和健康状况符合要求。</p></li></ol><h4 id=注意>注意：
<a class=anchor href=#%e6%b3%a8%e6%84%8f>#</a></h4><ul><li><strong>ReplicaSet 不会自己更新 Pod</strong>，它仅负责确保指定数量的 Pod 副本在集群中。如果 Pod 模板（如容器镜像、端口等）发生变化，需要手动删除旧的 ReplicaSet 并创建一个新的。</li></ul><hr><h3 id=2-deployment-的功能和实现原理>2. <strong>Deployment</strong> 的功能和实现原理
<a class=anchor href=#2-deployment-%e7%9a%84%e5%8a%9f%e8%83%bd%e5%92%8c%e5%ae%9e%e7%8e%b0%e5%8e%9f%e7%90%86>#</a></h3><h4 id=功能-1>功能
<a class=anchor href=#%e5%8a%9f%e8%83%bd-1>#</a></h4><p><strong>Deployment</strong> 是在 ReplicaSet 的基础上提供更高级别管理功能的控制器，它不仅能管理 ReplicaSet 的副本数，还提供了 <strong>滚动更新</strong>、<strong>回滚</strong>、<strong>暂停和恢复</strong>等功能。Deployment 通过控制 ReplicaSet 来确保指定数量的 Pod 副本保持运行，同时能够进行无缝的应用更新。</p><h4 id=实现原理-1>实现原理
<a class=anchor href=#%e5%ae%9e%e7%8e%b0%e5%8e%9f%e7%90%86-1>#</a></h4><p>Deployment 主要通过 <strong>ReplicaSet</strong> 来管理 Pod 副本和滚动更新过程。每次更新应用时，Deployment 会创建一个新的 ReplicaSet 来替代旧的 ReplicaSet，而旧的 ReplicaSet 会逐步减少 Pod 副本的数量，直到所有 Pod 都由新的 ReplicaSet 管理。</p><ol><li><p><strong>创建 Deployment</strong>： 用户可以通过 <code>kubectl</code> 或 YAML 文件创建 Deployment。在 Deployment 中，用户指定了 Pod 模板、所需副本数、更新策略等。Deployment 会自动管理 ReplicaSet，并且管理更新和回滚的过程。 示例：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>apps/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Deployment</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-deployment</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>replicas</span>: <span style=color:#ae81ff>3</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>selector</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>matchLabels</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>app</span>: <span style=color:#ae81ff>my-app</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>template</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>labels</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>app</span>: <span style=color:#ae81ff>my-app</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>containers</span>:
</span></span><span style=display:flex><span>      - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-container</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>image</span>: <span style=color:#ae81ff>nginx:1.19</span>
</span></span></code></pre></div></li><li><p><strong>滚动更新</strong>：</p><ul><li>当 Deployment 更新时，它会创建一个新的 ReplicaSet，并逐渐停止旧 ReplicaSet 中的 Pod，创建新的 Pod 来替代它们。这个过程称为 <strong>滚动更新（Rolling Update）</strong>。</li><li>默认情况下，Deployment 会确保更新时保持高可用性。通过控制器的智能调度，Kubernetes 会在 Pod 更新过程中确保某些副本始终处于运行状态。</li></ul></li><li><p><strong>回滚功能</strong>：</p><ul><li>如果新的更新失败，用户可以回滚到之前的版本。Deployment 会自动跟踪其历史版本，允许在必要时进行快速回滚。</li><li><code>kubectl rollout undo</code> 可以用来回滚到上一个 Deployment 版本。</li></ul></li><li><p><strong>暂停和恢复</strong>：</p><ul><li>如果在应用更新时需要暂停，可以使用 <code>kubectl rollout pause</code> 暂停 Deployment 更新。暂停后，可以手动调整 Deployment 的配置，然后使用 <code>kubectl rollout resume</code> 恢复更新。</li></ul></li><li><p><strong>滚动更新策略</strong>：</p><ul><li><code>maxSurge</code>：指定更新过程中最多可以超出副本数的 Pod 数量。</li><li><code>maxUnavailable</code>：指定更新过程中最多不可用的 Pod 数量。</li></ul></li><li><p><strong>Deployment 更新过程</strong>：</p><ul><li>当用户修改 Deployment（例如更新容器镜像）时，Deployment 会创建一个新的 ReplicaSet，并根据滚动更新策略逐步替换旧的 Pod。</li></ul></li></ol><hr><h3 id=3-replicaset-与-deployment-的区别>3. <strong>ReplicaSet 与 Deployment 的区别</strong>
<a class=anchor href=#3-replicaset-%e4%b8%8e-deployment-%e7%9a%84%e5%8c%ba%e5%88%ab>#</a></h3><table><thead><tr><th>特性</th><th><strong>ReplicaSet</strong></th><th><strong>Deployment</strong></th></tr></thead><tbody><tr><td>主要功能</td><td>只负责管理 Pod 副本数</td><td>负责管理 ReplicaSet，支持滚动更新、回滚、暂停、恢复</td></tr><tr><td>更新 Pod 的方式</td><td>不支持自动更新 Pod</td><td>支持自动滚动更新和回滚</td></tr><tr><td>控制器功能</td><td>仅管理 Pod 副本的数量</td><td>管理 ReplicaSet，提供更多高级功能</td></tr><tr><td>用途</td><td>多用于在 Deployment 中作为控制器</td><td>用于应用部署，自动更新、回滚等功能</td></tr><tr><td>适用场景</td><td>需要固定副本数的场景</td><td>需要持续部署、更新和回滚的场景</td></tr></tbody></table><h3 id=4-deployment-如何实现滚动更新>4. <strong>Deployment 如何实现滚动更新</strong>
<a class=anchor href=#4-deployment-%e5%a6%82%e4%bd%95%e5%ae%9e%e7%8e%b0%e6%bb%9a%e5%8a%a8%e6%9b%b4%e6%96%b0>#</a></h3><p>Deployment 使用 <strong>Rolling Update</strong> 策略来平滑地更新应用。更新过程的关键步骤如下：</p><ol><li><strong>创建新 ReplicaSet</strong>：当 Deployment 更新时，Kubernetes 会创建一个新的 ReplicaSet。</li><li><strong>逐步替换 Pod</strong>：在更新过程中，旧的 ReplicaSet 会逐步缩减 Pod 副本的数量，而新的 ReplicaSet 会逐步增加 Pod 副本的数量。</li><li><strong>健康检查</strong>：Kubernetes 会检查每个 Pod 的健康状态，只有在新 Pod 启动并通过健康检查后，才会删除旧 Pod。</li><li><strong>可配置策略</strong>：通过 <code>maxSurge</code> 和 <code>maxUnavailable</code> 参数，用户可以配置更新的速度和策略。</li></ol><hr><h3 id=5-总结>5. <strong>总结</strong>
<a class=anchor href=#5-%e6%80%bb%e7%bb%93>#</a></h3><ul><li><strong>ReplicaSet</strong> 主要用于确保一组 Pod 副本数量的管理，它本身不支持自动更新，因此需要手动管理 Pod 模板和副本数量。</li><li><strong>Deployment</strong> 则在 ReplicaSet 的基础上提供了更高级的功能，支持滚动更新、回滚、暂停和恢复等功能，适合用于生产环境中的应用管理。通过 Deployment，Kubernetes 可以自动管理 ReplicaSet 的更新过程，确保应用的高可用性和版本控制。</li></ul><p>一般来说，在实际使用中，<strong>Deployment</strong> 是更常见和推荐的选择，因为它提供了更多的管理和更新功能，而 <strong>ReplicaSet</strong> 通常是通过 Deployment 间接管理的。</p><h2 id=scheduler调度流程>scheduler调度流程
<a class=anchor href=#scheduler%e8%b0%83%e5%ba%a6%e6%b5%81%e7%a8%8b>#</a></h2><p>Kubernetes 的 <strong>Scheduler</strong> 负责将 Pod 调度到合适的 Node 上运行。调度的目标是确保 Pod 在集群中的高效运行，并满足资源需求、拓扑要求等各种约束条件。下面是 Kubernetes <strong>Scheduler</strong> 的详细调度流程：</p><h3 id=1-调度器的工作流程概述>1. <strong>调度器的工作流程概述</strong>
<a class=anchor href=#1-%e8%b0%83%e5%ba%a6%e5%99%a8%e7%9a%84%e5%b7%a5%e4%bd%9c%e6%b5%81%e7%a8%8b%e6%a6%82%e8%bf%b0>#</a></h3><p>Kubernetes Scheduler 主要包括以下几个步骤：</p><ol><li><strong>监听待调度的 Pod</strong>：Scheduler 监视 API 服务器中的 Pod 列表，查找没有被调度的 Pod（状态为 <code>Pending</code> 的 Pod）。</li><li><strong>选择适合的节点</strong>：Scheduler 根据 Pod 的资源需求、节点的可用资源、调度策略、亲和性和反亲和性等信息选择一个合适的节点。</li><li><strong>将 Pod 绑定到节点</strong>：一旦选择了节点，Scheduler 会将 Pod 绑定到该节点，并将信息更新到 API 服务器。</li><li><strong>执行调度</strong>：节点的 kubelet 会接收到更新，并开始启动容器来运行该 Pod。</li></ol><h3 id=2-scheduler-调度流程的详细步骤>2. <strong>Scheduler 调度流程的详细步骤</strong>
<a class=anchor href=#2-scheduler-%e8%b0%83%e5%ba%a6%e6%b5%81%e7%a8%8b%e7%9a%84%e8%af%a6%e7%bb%86%e6%ad%a5%e9%aa%a4>#</a></h3><h4 id=步骤-1监听待调度的-pod>步骤 1：监听待调度的 Pod
<a class=anchor href=#%e6%ad%a5%e9%aa%a4-1%e7%9b%91%e5%90%ac%e5%be%85%e8%b0%83%e5%ba%a6%e7%9a%84-pod>#</a></h4><ul><li><strong>Pod 状态为 Pending</strong>：Scheduler 只会调度状态为 <code>Pending</code> 的 Pod，也就是那些还没有被绑定到节点的 Pod。</li><li><strong>Pod 和调度器</strong>：Pod 会首先被创建并发送到 Kubernetes 的 API 服务器，Scheduler 会监控这些 Pod 的状态。</li><li><strong>Pod 资源需求</strong>：每个 Pod 在创建时可以定义其对资源（如 CPU、内存、存储等）的请求和限制（<code>resources.requests</code> 和 <code>resources.limits</code>）。</li></ul><h4 id=步骤-2调度器选择适合的节点>步骤 2：调度器选择适合的节点
<a class=anchor href=#%e6%ad%a5%e9%aa%a4-2%e8%b0%83%e5%ba%a6%e5%99%a8%e9%80%89%e6%8b%a9%e9%80%82%e5%90%88%e7%9a%84%e8%8a%82%e7%82%b9>#</a></h4><p>Scheduler 会根据以下几个因素来选择合适的节点：</p><ol><li><strong>Node 可用性</strong>：<ul><li>Scheduler 会查询所有可用节点，并根据 Node 的资源（CPU、内存、存储等）和 Pod 的资源请求来筛选合适的节点。</li><li>如果节点资源不足，Pod 将无法调度到该节点。</li></ul></li><li><strong>Pod 的亲和性和反亲和性（Affinity/Anti-Affinity）</strong>：<ul><li><strong>NodeAffinity</strong>：Pod 可以指定必须调度到具有特定标签的节点上，或者排除特定标签的节点。</li><li><strong>PodAffinity</strong>：Pod 可以指定它应该和其他特定的 Pod 一起调度到同一个节点上。</li><li><strong>PodAntiAffinity</strong>：Pod 可以指定它应该避免和其他特定的 Pod 调度到同一个节点上。 亲和性和反亲和性可以基于标签选择器来指定。</li></ul></li><li><strong>资源请求和限制</strong>：<ul><li>Scheduler 会根据 Pod 的资源请求和节点的资源使用情况进行匹配。</li><li>例如，如果 Pod 请求 2 CPU 和 4GB 内存，而节点只有 1 CPU 和 2GB 内存，那么该 Pod 无法被调度到这个节点。</li></ul></li><li><strong>Taints 和 Tolerations</strong>：<ul><li>节点可以通过 <strong>taints</strong> 来标记其不适合某些 Pod 的调度，只有具有相应 <strong>tolerations</strong> 的 Pod 才能被调度到这些节点上。</li><li>Taints 是一种保护机制，防止 Pod 被调度到不适合的节点。</li></ul></li><li><strong>资源均衡</strong>：<ul><li>Scheduler 会尽量保证集群资源的均衡，避免某些节点资源的过度消耗。</li><li>Kubernetes 会根据各个节点的资源使用情况（例如 CPU、内存的消耗量）来做负载均衡，确保 Pod 被均匀调度。</li></ul></li><li><strong>Priority 和 Preemption</strong>：<ul><li>Kubernetes 支持 Pod 优先级，调度器会优先调度优先级更高的 Pod。</li><li>如果集群中没有足够的资源来调度新 Pod，调度器会选择<strong>抢占</strong>低优先级的 Pod，释放资源来调度高优先级的 Pod。</li></ul></li><li><strong>其他约束</strong>：<ul><li><strong>PodDisruptionBudgets（PDB）</strong>：定义了最小的 Pod 副本数量，Scheduler 会确保不会调度导致 PDB 限制的 Pod。</li><li><strong>静态调度器插件（例如, VolumeScheduling）</strong>：调度时会考虑卷的可用性，避免选择那些无法挂载某个特定卷的节点。</li></ul></li></ol><h4 id=步骤-3绑定-pod-到节点>步骤 3：绑定 Pod 到节点
<a class=anchor href=#%e6%ad%a5%e9%aa%a4-3%e7%bb%91%e5%ae%9a-pod-%e5%88%b0%e8%8a%82%e7%82%b9>#</a></h4><p>一旦 Scheduler 选择了合适的节点，下一步是将该 Pod 绑定到节点。这个操作实际上是修改 API 服务器中的 Pod 状态，将 Pod 的 <code>nodeName</code> 字段更新为选中的节点名称。</p><ul><li><strong>Pod Binding</strong>：这是调度的最后一步，Scheduler 更新 Pod 资源对象的 <code>nodeName</code> 字段。</li><li><strong>更新 API 服务器</strong>：Pod 状态会更新为 <code>Scheduled</code>，并指明将该 Pod 调度到的节点。</li></ul><h4 id=步骤-4执行调度>步骤 4：执行调度
<a class=anchor href=#%e6%ad%a5%e9%aa%a4-4%e6%89%a7%e8%a1%8c%e8%b0%83%e5%ba%a6>#</a></h4><p>一旦 Pod 被绑定到节点，<strong>kubelet</strong> 会开始在该节点上执行 Pod 的启动操作，具体步骤如下：</p><ol><li><strong>kubelet 启动容器</strong>：kubelet 通过容器运行时（如 Docker、containerd 等）启动 Pod 中的容器。</li><li><strong>Pod 运行</strong>：Pod 在节点上运行，kubelet 会监控容器的状态，并将 Pod 的健康状态报告给 API 服务器。</li><li><strong>Pod 健康检查</strong>：如果 Pod 配置了探针（liveness、readiness），kubelet 会定期执行探针，确保 Pod 健康。</li></ol><h3 id=3-scheduler-插件体系>3. <strong>Scheduler 插件体系</strong>
<a class=anchor href=#3-scheduler-%e6%8f%92%e4%bb%b6%e4%bd%93%e7%b3%bb>#</a></h3><p>Kubernetes Scheduler 是一个高度可扩展的组件，可以通过插件化的方式进行扩展。调度过程中的很多步骤可以通过不同的插件来实现，常见的插件包括：</p><ul><li><strong>过滤插件（Filter Plugin）</strong>：用于过滤不符合条件的节点。比如，如果节点没有足够的 CPU 或内存资源，Pod 就不能被调度到该节点。</li><li><strong>优选插件（Score Plugin）</strong>：对节点进行评分，选择最合适的节点。</li><li><strong>Preemption 插件</strong>：当集群资源不足时，Preemption 插件会通过抢占低优先级 Pod 来释放资源供高优先级 Pod 使用。</li></ul><h3 id=4-调度策略>4. <strong>调度策略</strong>
<a class=anchor href=#4-%e8%b0%83%e5%ba%a6%e7%ad%96%e7%95%a5>#</a></h3><p>调度器使用不同的策略来优化调度：</p><ul><li><strong>Round-robin 调度策略</strong>：当集群资源分布均匀时，调度器可能使用 round-robin 算法来轮流将 Pod 分配到不同的节点。</li><li><strong>资源分配优化</strong>：调度器通过均衡集群资源的使用来避免过载，并确保 Pod 调度到合适的节点上，最大限度地提高资源利用率。</li></ul><h3 id=5-总结-1>5. <strong>总结</strong>
<a class=anchor href=#5-%e6%80%bb%e7%bb%93-1>#</a></h3><p>Kubernetes Scheduler 是 Kubernetes 中非常重要的组件，负责将 <strong>Pod</strong> 调度到集群中的合适 <strong>Node</strong> 上运行。调度过程包括从监听待调度的 Pod、根据节点资源和调度策略选择节点、将 Pod 绑定到节点以及最终通过 kubelet 启动 Pod 的过程。调度器还支持通过插件扩展调度功能，并提供灵活的调度策略来优化集群资源利用。</p><h2 id=hpa怎么实现的>HPA怎么实现的☆
<a class=anchor href=#hpa%e6%80%8e%e4%b9%88%e5%ae%9e%e7%8e%b0%e7%9a%84>#</a></h2><p><strong>Horizontal Pod Autoscaler (HPA)</strong> 是 Kubernetes 中的一个自动化组件，它根据负载（如 CPU、内存使用率等）自动扩展或缩减 Pod 的数量。HPA 可以确保应用在负载增加时能够自动扩展 Pod 副本数，在负载减少时自动缩减，从而保持系统的高效运行。</p><h3 id=1-hpa-主要功能>1. <strong>HPA 主要功能</strong>
<a class=anchor href=#1-hpa-%e4%b8%bb%e8%a6%81%e5%8a%9f%e8%83%bd>#</a></h3><ul><li><strong>自动扩展（Scaling Up）</strong>：当应用负载（如 CPU 使用率、内存使用率等）超过设定的阈值时，HPA 会增加 Pod 的副本数。</li><li><strong>自动缩减（Scaling Down）</strong>：当应用负载低于设定的阈值时，HPA 会减少 Pod 的副本数。</li><li><strong>资源监控</strong>：HPA 定期检查应用的资源使用情况（CPU/内存等）并做出扩缩容决策。</li></ul><h3 id=2-hpa-工作原理>2. <strong>HPA 工作原理</strong>
<a class=anchor href=#2-hpa-%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86>#</a></h3><p>HPA 的实现过程主要依赖于 <strong>Metrics Server</strong> 提供的指标数据（如 CPU 使用率、内存使用量等），并通过比较实际指标与目标指标来决定是否需要扩容或缩容。</p><h4 id=21-hpa-组件>2.1 <strong>HPA 组件</strong>
<a class=anchor href=#21-hpa-%e7%bb%84%e4%bb%b6>#</a></h4><ul><li><strong>Metrics Server</strong>：HPA 依赖于 Metrics Server，它收集 Kubernetes 集群中的资源使用数据（例如 CPU 和内存使用量），并将这些数据提供给 HPA。</li><li><strong>HPA 控制器</strong>：HPA 控制器在 Kubernetes 中运行，它会定期获取集群中 Pod 的资源指标，并根据这些指标自动调整 Pod 副本的数量。</li></ul><h4 id=22-hpa-的工作流程>2.2 <strong>HPA 的工作流程</strong>
<a class=anchor href=#22-hpa-%e7%9a%84%e5%b7%a5%e4%bd%9c%e6%b5%81%e7%a8%8b>#</a></h4><ol><li><p><strong>创建 HPA 资源</strong>： 用户创建一个 HPA 资源，并指定目标应用的资源类型（如 CPU 使用率）以及目标值。例如，目标是当 CPU 使用率达到 50% 时，自动扩容 Pod 副本。</p><p>示例：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>autoscaling/v2</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>HorizontalPodAutoscaler</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-app-hpa</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>scaleTargetRef</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>apps/v1</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Deployment</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-app</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>minReplicas</span>: <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>maxReplicas</span>: <span style=color:#ae81ff>10</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>metrics</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>type</span>: <span style=color:#ae81ff>Resource</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>resource</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>name</span>: <span style=color:#ae81ff>cpu</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>target</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>type</span>: <span style=color:#ae81ff>Utilization</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>averageUtilization</span>: <span style=color:#ae81ff>50</span>
</span></span></code></pre></div><ul><li><strong>scaleTargetRef</strong>：指定了要自动扩展的资源对象（通常是一个 Deployment）。</li><li><strong>minReplicas</strong> 和 <strong>maxReplicas</strong>：设置 Pod 副本数的最小值和最大值，HPA 会在这个范围内进行扩缩容。</li><li><strong>metrics</strong>：指定需要监控的资源类型，这里监控的是 CPU 使用率，并且目标是达到 50% 的平均 CPU 使用率。</li></ul></li><li><p><strong>获取资源使用指标</strong>： HPA 控制器会定期从 <strong>Metrics Server</strong> 获取集群中各个 Pod 的资源使用数据。Metrics Server 会通过 <code>kubelet</code> 获取每个节点的资源数据并上报给 API 服务器，然后 HPA 控制器可以查询这些数据。</p></li><li><p><strong>计算当前负载与目标负载的差异</strong>： HPA 控制器会计算当前 Pod 的资源使用情况（例如，CPU 使用率），并与配置文件中的目标值进行比较。例如，如果目标 CPU 使用率是 50%，而当前使用率是 70%，HPA 控制器就会启动扩容操作。</p></li><li><p><strong>计算需要的副本数</strong>： 基于当前的资源使用情况和目标值，HPA 会计算需要多少个副本才能使资源使用达到目标。例如，若 CPU 使用率为 70%，目标是 50%，则 HPA 会根据这个比例来决定是否增加 Pod 副本数。</p><p>公式：</p><pre tabindex=0><code>desiredReplicas = currentReplicas * (currentUsage / targetUsage)
</code></pre><p>如果计算出的副本数超出了 <code>maxReplicas</code>，那么副本数会被限制在 <code>maxReplicas</code>；如果小于 <code>minReplicas</code>，副本数会被限制在 <code>minReplicas</code>。</p></li><li><p><strong>调整副本数</strong>： HPA 控制器会根据计算的副本数修改 Deployment、ReplicaSet 或 StatefulSet 的 <code>replicas</code> 字段，Kubernetes 会根据这个变更开始扩容或缩容 Pod。</p></li></ol><h4 id=23-hpa-的触发周期>2.3 <strong>HPA 的触发周期</strong>
<a class=anchor href=#23-hpa-%e7%9a%84%e8%a7%a6%e5%8f%91%e5%91%a8%e6%9c%9f>#</a></h4><ul><li><strong>每 15 秒检查一次</strong>：默认情况下，HPA 控制器每 15 秒检查一次资源指标，并基于当前负载情况决定是否扩展或缩减 Pod 副本数。</li><li><strong>计算周期</strong>：每次检查时，HPA 控制器会分析过去 1 分钟内的平均负载。HPA 会根据这个时间窗口的负载来做出决策。</li></ul><h3 id=3-hpa-扩展和缩减的决策>3. <strong>HPA 扩展和缩减的决策</strong>
<a class=anchor href=#3-hpa-%e6%89%a9%e5%b1%95%e5%92%8c%e7%bc%a9%e5%87%8f%e7%9a%84%e5%86%b3%e7%ad%96>#</a></h3><p>HPA 会基于设定的 <strong>metrics</strong> 和 <strong>目标值</strong> 来做出扩展或缩减决策，常见的度量标准有：</p><ul><li><strong>CPU 使用率</strong>：HPA 会监控集群中 Pod 的 CPU 使用率，并根据当前使用情况来扩展或缩减 Pod 数量。</li><li><strong>内存使用率</strong>：除了 CPU 外，内存使用率也是一个常见的衡量标准，虽然默认情况下 HPA 只支持 CPU，但可以通过自定义指标扩展来监控内存或其他资源。</li><li><strong>自定义指标</strong>：用户还可以使用自定义指标（如来自 Prometheus 等监控工具的数据）来驱动 HPA 扩展 Pod 数量。</li></ul><h4 id=扩容scaling-up>扩容（Scaling Up）：
<a class=anchor href=#%e6%89%a9%e5%ae%b9scaling-up>#</a></h4><ul><li>当负载超过预设目标（例如 CPU 使用率 > 50%）时，HPA 会增加 Pod 数量，以缓解负载。</li></ul><h4 id=缩容scaling-down>缩容（Scaling Down）：
<a class=anchor href=#%e7%bc%a9%e5%ae%b9scaling-down>#</a></h4><ul><li>当负载低于预设目标（例如 CPU 使用率 &lt; 50%）时，HPA 会减少 Pod 数量，以节省资源。</li></ul><h3 id=4-hpa-相关的限制和注意事项>4. <strong>HPA 相关的限制和注意事项</strong>
<a class=anchor href=#4-hpa-%e7%9b%b8%e5%85%b3%e7%9a%84%e9%99%90%e5%88%b6%e5%92%8c%e6%b3%a8%e6%84%8f%e4%ba%8b%e9%a1%b9>#</a></h3><ol><li><strong>Metrics Server 的依赖</strong>：HPA 依赖于 Metrics Server 提供的指标数据，必须确保 Metrics Server 正常运行，否则 HPA 无法获取资源使用数据，可能导致无法触发自动扩缩容。</li><li><strong>扩缩容延迟</strong>：HPA 的调度决策是基于一定时间窗口内的平均负载，因此在负载突增的情况下，可能会有一定的延迟。</li><li><strong>最小副本数和最大副本数限制</strong>：可以通过 <code>minReplicas</code> 和 <code>maxReplicas</code> 配置来限制 Pod 副本数的最小值和最大值，避免过度扩容或缩容。</li><li><strong>不适用于所有类型的负载</strong>：HPA 主要针对计算密集型负载（如 CPU 和内存）。对于 I/O 密集型或网络密集型负载，HPA 可能并不总是最佳解决方案。</li><li><strong>资源请求和限制</strong>：为了使 HPA 正常工作，Pod 必须定义资源的 <code>requests</code> 和 <code>limits</code>，否则 HPA 将无法准确监控 Pod 的负载并进行扩缩容。</li></ol><h3 id=5-总结-2>5. <strong>总结</strong>
<a class=anchor href=#5-%e6%80%bb%e7%bb%93-2>#</a></h3><p><strong>Horizontal Pod Autoscaler (HPA)</strong> 是 Kubernetes 的一种自动扩展机制，根据应用的资源负载（如 CPU 使用率、内存使用量等）来自动调整 Pod 的副本数。它通过与 <strong>Metrics Server</strong> 配合，实时获取资源使用数据，并根据这些数据动态扩展或缩减 Pod 数量，以保证应用的高可用性和资源的高效利用。HPA 可以根据目标的 CPU 使用率、内存使用量等指标来控制副本数，从而自动适应应用的负载变化。</p><h2 id=request-limit底层是怎么限制的>request limit底层是怎么限制的☆
<a class=anchor href=#request-limit%e5%ba%95%e5%b1%82%e6%98%af%e6%80%8e%e4%b9%88%e9%99%90%e5%88%b6%e7%9a%84>#</a></h2><p>在 Kubernetes 中，Pod 的资源限制（<code>request</code> 和 <code>limit</code>）主要是通过 <strong>cgroup</strong> 来实现的，<code>request</code> 和 <code>limit</code> 的值会直接映射到容器的 cgroup 配置中，控制容器使用的资源（CPU 和内存）。这些限制是在容器运行时通过 <strong>容器运行时</strong>（如 Docker、containerd 等）和 <strong>Linux cgroup</strong> 系统实现的。</p><h3 id=1-请求资源request与限制资源limit的定义>1. <strong>请求资源（request）与限制资源（limit）的定义</strong>
<a class=anchor href=#1-%e8%af%b7%e6%b1%82%e8%b5%84%e6%ba%90request%e4%b8%8e%e9%99%90%e5%88%b6%e8%b5%84%e6%ba%90limit%e7%9a%84%e5%ae%9a%e4%b9%89>#</a></h3><ul><li><strong>request</strong>：容器在调度时请求的资源量（CPU 或内存）。它表示 Pod 在集群中调度时需要的最低资源量。如果一个 Pod 没有设置 <code>request</code>，则调度器可能会将 Pod 放置到资源充足的节点上，但不会强制资源保证。</li><li><strong>limit</strong>：容器能够使用的最大资源量。如果容器请求超过 <code>limit</code> 的资源，Linux 内核会限制容器的资源使用（例如，通过 OOM Killer 来终止过度占用内存的容器）。<code>limit</code> 限制的是容器的资源上限，防止单个容器占用过多资源，影响其他容器。</li></ul><h3 id=2-底层原理cgroup-和容器运行时>2. <strong>底层原理：cgroup 和容器运行时</strong>
<a class=anchor href=#2-%e5%ba%95%e5%b1%82%e5%8e%9f%e7%90%86cgroup-%e5%92%8c%e5%ae%b9%e5%99%a8%e8%bf%90%e8%a1%8c%e6%97%b6>#</a></h3><p>容器通过 <strong>cgroup</strong>（Control Groups）来实现资源的隔离和限制。cgroup 是 Linux 内核的一个特性，用来限制、控制和监视进程的资源使用。</p><h4 id=21-cgroup-和-kubernetes-中的资源限制>2.1 <strong>cgroup 和 Kubernetes 中的资源限制</strong>
<a class=anchor href=#21-cgroup-%e5%92%8c-kubernetes-%e4%b8%ad%e7%9a%84%e8%b5%84%e6%ba%90%e9%99%90%e5%88%b6>#</a></h4><ul><li><strong>CPU 限制</strong>：容器的 <code>request</code> 和 <code>limit</code> 会被转换为 cgroup 中的 CPU 配额。例如，如果容器的 <code>request</code> 设置为 1 CPU，<code>limit</code> 设置为 2 CPU，cgroup 会为该容器分配 1 个 CPU 的请求资源，并且限制它最多只能使用 2 个 CPU 的资源。</li><li><strong>内存限制</strong>：容器的内存 <code>request</code> 和 <code>limit</code> 会对应到 cgroup 的内存子系统（<code>memory</code>），指定容器可用的最小和最大内存。当容器使用的内存超过了 <code>limit</code>，会触发 OOM（Out of Memory）情况，内核会根据 <code>limit</code> 设置对容器进行杀掉或限制。</li></ul><h4 id=22-容器运行时如何应用>2.2 <strong>容器运行时如何应用 <code>request</code> 和 <code>limit</code></strong>
<a class=anchor href=#22-%e5%ae%b9%e5%99%a8%e8%bf%90%e8%a1%8c%e6%97%b6%e5%a6%82%e4%bd%95%e5%ba%94%e7%94%a8>#</a></h4><p>容器运行时（如 Docker 或 containerd）负责启动容器，并将 <code>request</code> 和 <code>limit</code> 转换为 cgroup 的配置。具体实现过程如下：</p><ol><li><p><strong>容器启动时</strong>：容器运行时读取 Pod 配置中的 <code>request</code> 和 <code>limit</code> 值，并将其映射到相应的 cgroup 设置中。比如，对于 CPU 限制，Docker 会使用 cgroup 的 <code>cpu.cfs_quota_us</code> 和 <code>cpu.cfs_period_us</code> 来限制 CPU 使用。</p><ul><li><strong>CPU 请求</strong>（request）：容器的 <code>request</code> 会被转换为 <code>cpu.cfs_quota_us</code>，表示容器在调度时需要的 CPU 资源。</li><li><strong>CPU 限制</strong>（limit）：容器的 <code>limit</code> 会被转换为 <code>cpu.cfs_quota_us</code>，表示容器的最大 CPU 配额。</li></ul></li><li><p><strong>资源管理</strong>：</p><ul><li><strong>CPU</strong>：容器的 CPU 使用会受到 cgroup 的管理，容器的 CPU 使用会被限制在指定的配额范围内。</li><li><strong>内存</strong>：内存限制是通过 cgroup 的 <code>memory.limit_in_bytes</code> 来实现的。如果容器请求的内存超过 <code>limit</code>，容器将被杀死（OOM）。</li></ul></li><li><p><strong>容器资源超限</strong>：</p><ul><li><p>当容器超出</p><pre tabindex=0><code>limit
</code></pre><p>资源时，Linux 内核会根据资源类型采取不同的策略：</p><ul><li><strong>CPU 限制</strong>：如果容器使用的 CPU 超过了设置的 <code>limit</code>，容器会被限制只能使用一定量的 CPU 时间，无法进一步占用更多 CPU。</li><li><strong>内存限制</strong>：如果容器的内存使用超出了 <code>limit</code>，Linux 内核会通过 OOM（Out Of Memory）机制来终止该容器，以保证系统的稳定性。</li></ul></li></ul></li></ol><h4 id=23-linux-cgroup-实现方式>2.3 <strong>Linux Cgroup 实现方式</strong>
<a class=anchor href=#23-linux-cgroup-%e5%ae%9e%e7%8e%b0%e6%96%b9%e5%bc%8f>#</a></h4><p>在 Linux 内核中，资源限制是通过 cgroup（控制组）实现的。cgroup 用于限制、控制和监控进程的资源使用，包括 CPU、内存、磁盘 I/O 等。</p><ul><li><strong>CPU 限制</strong>：cgroup 通过 <code>cpu.cfs_quota_us</code> 和 <code>cpu.cfs_period_us</code> 设置 CPU 配额。<ul><li><code>cpu.cfs_period_us</code>：设置 CPU 周期的总时长（默认是 100ms）。</li><li><code>cpu.cfs_quota_us</code>：设置容器在周期内允许使用的最大时间（默认是 -1，表示没有限制）。比如，如果设定为 50000 微秒，容器每个周期只能使用 50ms 的 CPU。</li></ul></li><li><strong>内存限制</strong>：cgroup 使用 <code>memory.limit_in_bytes</code> 来限制容器的最大内存使用量。如果容器的内存超过了 <code>limit</code>，内核会触发 OOM（Out of Memory）机制，杀死进程。</li></ul><h3 id=3-调度器如何使用-request-和-limit>3. <strong>调度器如何使用 request 和 limit</strong>
<a class=anchor href=#3-%e8%b0%83%e5%ba%a6%e5%99%a8%e5%a6%82%e4%bd%95%e4%bd%bf%e7%94%a8-request-%e5%92%8c-limit>#</a></h3><p>在 Kubernetes 中，<strong>调度器</strong>会根据 Pod 的 <code>request</code> 来选择合适的节点进行调度。调度器会确保节点有足够的资源（CPU、内存）来满足 Pod 的 <code>request</code>，同时避免超出节点的资源限制。</p><ul><li><strong>request</strong>：调度时参考 Pod 的 <code>request</code>，确保节点有足够的资源来调度 Pod。<code>request</code> 是保证 Pod 最低可以获得的资源。</li><li><strong>limit</strong>：<code>limit</code> 不会影响调度过程，但会影响容器的资源使用。容器的实际资源使用不能超过 <code>limit</code>，否则会被操作系统限制或杀死。</li></ul><h3 id=4-总结>4. <strong>总结</strong>
<a class=anchor href=#4-%e6%80%bb%e7%bb%93>#</a></h3><p>在 Kubernetes 中，<code>request</code> 和 <code>limit</code> 是通过 Linux cgroup 来实现资源控制的：</p><ul><li><strong>request</strong> 表示容器请求的最低资源量，会影响调度时选择节点的过程。</li><li><strong>limit</strong> 表示容器可以使用的最大资源量，超过这个限制会导致容器被限制或杀死。</li><li><strong>cgroup</strong> 是实现这些限制的底层机制，它通过限制 CPU 配额、内存限制等来控制容器的资源使用。</li></ul><p>通过 <code>request</code> 和 <code>limit</code>，Kubernetes 提供了一个灵活的资源管理机制，确保容器在集群中的资源使用是可控的，并且能够避免资源过度消耗或浪费。</p><h2 id=helm工作原理是什么>helm工作原理是什么？
<a class=anchor href=#helm%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86%e6%98%af%e4%bb%80%e4%b9%88>#</a></h2><p><strong>Helm</strong> 是 Kubernetes 上的一个包管理工具，类似于 Linux 上的 <code>apt</code> 或 <code>yum</code>，用来简化 Kubernetes 应用的安装、管理和发布过程。Helm 主要通过 <strong>Charts</strong> 和 <strong>Releases</strong> 来管理 Kubernetes 应用，能够有效地打包、配置、发布和维护 Kubernetes 资源。</p><h3 id=helm-的核心组件>Helm 的核心组件
<a class=anchor href=#helm-%e7%9a%84%e6%a0%b8%e5%bf%83%e7%bb%84%e4%bb%b6>#</a></h3><ol><li><p><strong>Chart</strong>：</p><ul><li><p><strong>Chart</strong> 是 Helm 的核心，代表一个可以部署的应用包。它是一组 Kubernetes YAML 文件的集合，用于描述 Kubernetes 中一个应用所需的所有资源。Chart 包含了部署应用所需的所有配置文件（如 Deployments、Services、ConfigMaps、Secrets 等）。</p></li><li><p>Chart</p><p>可能包含：</p><ul><li><code>templates/</code> 目录：包含 Kubernetes 资源的模板文件，支持使用 Helm 的模板语法进行变量替换和控制逻辑。</li><li><code>values.yaml</code> 文件：定义默认的配置值，用于替换模板中的变量。</li><li><code>charts/</code> 目录：存放依赖的其他 Charts。</li><li><code>Chart.yaml</code> 文件：包含 Chart 的元数据（如名称、版本、描述等）。</li><li><code>templates/</code> 目录下的模板会被渲染成具体的 Kubernetes 资源定义。</li></ul></li></ul></li><li><p><strong>Release</strong>：</p><ul><li><strong>Release</strong> 是 Helm 安装 Chart 的一个实例。当你使用 Helm 安装一个 Chart 时，Helm 会创建一个 Release，将 Chart 配置值与 Kubernetes 集群中的资源进行实际的绑定。</li><li>每个 Helm Release 都有一个名称和版本号，可以在同一集群中多次安装同一个 Chart，生成不同的 Release 实例。</li></ul></li><li><p><strong>Helm 仓库</strong>：</p><ul><li>Helm 仓库是 Chart 的存储库，用来发布和分享 Chart 包。常见的 Helm 仓库包括官方的 Helm 仓库和第三方仓库。Helm 可以从这些仓库中拉取 Charts 进行安装。</li></ul></li></ol><h3 id=helm-工作原理>Helm 工作原理
<a class=anchor href=#helm-%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86>#</a></h3><h4 id=1-安装-chart>1. <strong>安装 Chart</strong>
<a class=anchor href=#1-%e5%ae%89%e8%a3%85-chart>#</a></h4><p>Helm 安装 Chart 的过程可以分为以下几个步骤：</p><ol><li><strong>初始化 Helm 客户端</strong>： Helm 客户端会与 Kubernetes API 服务器交互。安装和配置 Kubernetes 上的 Helm Tiller（Helm v2 中使用的服务端组件）或者在 Helm v3 中，客户端本身直接与 Kubernetes API 交互。</li><li><strong>获取 Chart</strong>： Helm 可以从本地目录或远程 Helm 仓库获取 Chart。Helm 也支持 Chart 的版本管理。</li><li><strong>渲染模板</strong>： Helm 使用模板引擎（基于 Go 模板）渲染 <code>Chart</code> 中的 Kubernetes 配置文件（如 Deployments、Services 等）。在渲染过程中，Helm 会根据 <code>values.yaml</code> 文件中的默认值，或者通过 <code>--set</code> 参数传入的自定义值，来替换模板文件中的变量。</li><li><strong>安装资源</strong>： 渲染后的 Kubernetes 配置文件会被应用到 Kubernetes 集群中，Helm 会使用 <code>kubectl</code> 或 Kubernetes API 提交这些资源，创建应用的实际部署。</li><li><strong>创建 Release</strong>： 当 Helm 成功安装应用后，会创建一个 <strong>Release</strong>，每个 Release 都会有一个唯一的名称和版本号。这个 Release 记录了 Chart 配置、版本信息以及安装的资源对象。</li></ol><h4 id=2-升级和回滚>2. <strong>升级和回滚</strong>
<a class=anchor href=#2-%e5%8d%87%e7%ba%a7%e5%92%8c%e5%9b%9e%e6%bb%9a>#</a></h4><ul><li><strong>升级</strong>： Helm 允许你通过修改 <code>values.yaml</code> 或直接使用 <code>--set</code> 来修改 Chart 配置。当你更新 Chart 或配置时，可以使用 <code>helm upgrade</code> 命令进行应用升级。Helm 会通过对比新旧版本，生成更新的资源定义，并应用到 Kubernetes 集群中。</li><li><strong>回滚</strong>： 如果升级后的应用出现问题，Helm 还支持通过 <code>helm rollback</code> 命令将应用回滚到先前的版本。Helm 会重新应用旧版本的配置和资源，恢复到先前的状态。</li></ul><h4 id=3-卸载应用>3. <strong>卸载应用</strong>
<a class=anchor href=#3-%e5%8d%b8%e8%bd%bd%e5%ba%94%e7%94%a8>#</a></h4><p>使用 <code>helm uninstall</code> 命令可以删除已经安装的 Release，Helm 会删除与该 Release 相关的所有 Kubernetes 资源（如 Pods、Services、Deployments 等）。</p><h4 id=4-依赖管理>4. <strong>依赖管理</strong>
<a class=anchor href=#4-%e4%be%9d%e8%b5%96%e7%ae%a1%e7%90%86>#</a></h4><p>Helm 支持通过 <code>charts/</code> 目录管理 Chart 依赖。一个 Chart 可以依赖其他 Chart，这样可以实现 Chart 的模块化管理。Helm 会在安装时自动下载和安装 Chart 的依赖包。</p><h3 id=helm-的模板渲染>Helm 的模板渲染
<a class=anchor href=#helm-%e7%9a%84%e6%a8%a1%e6%9d%bf%e6%b8%b2%e6%9f%93>#</a></h3><p>Helm 使用 Go 模板语法渲染 Kubernetes 资源的 YAML 文件。模板语法允许你定义变量、条件语句、循环等，从而使得 Helm 配置更加灵活和动态。</p><p>常用的模板语法有：</p><ul><li><code>{{ .Release.Name }}</code>：获取当前 Release 的名称。</li><li><code>{{ .Values.someValue }}</code>：获取 <code>values.yaml</code> 文件中的某个值。</li><li><code>{{ if .Values.enabled }} ... {{ end }}</code>：条件判断，只有当某个值为 <code>true</code> 时才渲染某些资源。</li><li><code>{{ range .Values.items }} ... {{ end }}</code>：循环渲染多个资源。</li></ul><h3 id=helm-的优势>Helm 的优势
<a class=anchor href=#helm-%e7%9a%84%e4%bc%98%e5%8a%bf>#</a></h3><ol><li><strong>简化 Kubernetes 部署</strong>： Helm 通过 Charts 模块化部署应用，减少了大量手动创建 Kubernetes 资源配置文件的工作。</li><li><strong>易于版本管理</strong>： Helm 允许你管理应用的多个版本，可以轻松地升级、回滚和恢复应用。</li><li><strong>参数化配置</strong>： Helm 允许你使用模板和 <code>values.yaml</code> 文件来动态配置 Kubernetes 资源，使得应用的配置可以灵活适配不同环境。</li><li><strong>依赖管理</strong>： Helm 支持 Chart 之间的依赖关系，简化了应用的复用和集成。</li><li><strong>应用发布和共享</strong>： Helm 使得 Chart 的共享和复用变得更加容易，你可以使用公共的 Helm 仓库或自己搭建私有仓库，方便团队或社区共享应用。</li></ol><h3 id=helm-与-kubernetes-资源的关系>Helm 与 Kubernetes 资源的关系
<a class=anchor href=#helm-%e4%b8%8e-kubernetes-%e8%b5%84%e6%ba%90%e7%9a%84%e5%85%b3%e7%b3%bb>#</a></h3><p>Helm 主要通过将 Chart 渲染为 Kubernetes 配置文件（如 <code>Deployment</code>、<code>Service</code> 等）来管理应用。它并不会替代 Kubernetes 原生的资源管理，而是简化了 Kubernetes 应用的配置、安装、升级和卸载等操作。Helm 通过模板化和参数化的方式，提供了一个强大的工具，帮助开发和运维团队更高效地管理 Kubernetes 上的应用。</p><h3 id=总结-30>总结
<a class=anchor href=#%e6%80%bb%e7%bb%93-30>#</a></h3><ul><li><strong>Helm</strong> 是 Kubernetes 的包管理工具，通过 Charts、Releases 和 Helm 仓库帮助用户简化应用的安装、配置、发布和管理。</li><li><strong>Chart</strong> 是 Helm 包的核心，包含 Kubernetes 资源的模板和配置文件。</li><li><strong>Release</strong> 是 Chart 的一个具体实例，在 Kubernetes 集群中创建和管理。</li><li>Helm 使用模板引擎将变量渲染为实际的 Kubernetes 资源配置，并通过 <code>kubectl</code> 提交到 Kubernetes 集群。</li><li>Helm 还提供了版本管理、升级、回滚、依赖管理等功能。</li></ul><p>通过 Helm，用户可以更加方便地管理和发布 Kubernetes 应用，增强了应用的可移植性和可管理性。</p><h2 id=helm-chart-rollback实现过程是什么>helm chart rollback实现过程是什么？
<a class=anchor href=#helm-chart-rollback%e5%ae%9e%e7%8e%b0%e8%bf%87%e7%a8%8b%e6%98%af%e4%bb%80%e4%b9%88>#</a></h2><p>在 Helm 中，<strong><code>rollback</code></strong> 是一种回滚操作，它允许用户将一个已安装的 Helm <strong>Release</strong> 恢复到先前的某个版本。这在应用升级后出现问题时非常有用，可以快速将应用恢复到一个已知的稳定状态。</p><h3 id=helm-chart-rollback-实现过程>Helm Chart Rollback 实现过程
<a class=anchor href=#helm-chart-rollback-%e5%ae%9e%e7%8e%b0%e8%bf%87%e7%a8%8b>#</a></h3><p>Helm 的 <code>rollback</code> 操作依赖于 Helm <strong>Release</strong> 的版本管理机制。每次通过 Helm 安装、升级或修改一个 Release 时，Helm 都会为该 Release 创建一个新的版本，并将相关的 Kubernetes 资源状态记录到 Helm 的内部存储（称为 <strong>Helm 存储库</strong>）。通过 <code>rollback</code> 命令，用户可以选择恢复某个历史版本。</p><p>下面是 Helm Chart rollback 的实现过程：</p><h3 id=1-helm-release-的版本管理>1. <strong>Helm Release 的版本管理</strong>
<a class=anchor href=#1-helm-release-%e7%9a%84%e7%89%88%e6%9c%ac%e7%ae%a1%e7%90%86>#</a></h3><p>Helm 会为每个安装或升级的 <strong>Release</strong> 创建一个唯一的版本号，并将每个版本的 Kubernetes 资源的当前状态保存在 Helm 的存储中。每次 Helm 操作（安装、升级、修改等）都会生成一个新的 Release 版本。</p><ul><li><strong>Release Version</strong>：每次执行 <code>helm upgrade</code> 或 <code>helm install</code> 时，Helm 会自动增加 Release 的版本号，并在内部存储中保存该版本对应的 Kubernetes 资源清单（如 <code>Deployment</code>、<code>Service</code> 等）。</li><li>Helm 会将 Release 的版本存储在 Kubernetes 集群中的 <strong>ConfigMap</strong> 或 <strong>Secret</strong> 中，具体存储方式取决于 Helm 配置。</li></ul><h3 id=2-执行-rollback-操作>2. <strong>执行 Rollback 操作</strong>
<a class=anchor href=#2-%e6%89%a7%e8%a1%8c-rollback-%e6%93%8d%e4%bd%9c>#</a></h3><p>当我们执行 <code>helm rollback</code> 命令时，Helm 会：</p><ol><li><strong>读取 Release 的历史版本</strong>：<ul><li><code>helm rollback</code> 命令指定的版本号或默认的上一个版本，Helm 会从内部存储中找到该版本对应的 Kubernetes 资源清单。</li><li>Helm 默认会回滚到最近的一个版本（通过 <code>--retries</code> 参数或指定具体版本号），如果没有指定版本号，则会回滚到上一个版本。</li></ul></li><li><strong>生成新版本的 Kubernetes 资源</strong>：<ul><li>Helm 会从历史版本中提取之前的 Kubernetes 资源清单，这些清单包括之前版本的 <strong>Deployment</strong>、<strong>Service</strong>、<strong>ConfigMap</strong>、<strong>Secrets</strong> 等。</li><li>Helm 会将这些资源清单再次渲染并提交到 Kubernetes 集群。渲染时，Helm 会使用模板文件和 <code>values.yaml</code> 配置文件，确保这些资源的配置与历史版本一致。</li></ul></li><li><strong>应用历史版本的资源配置</strong>：<ul><li>生成的 Kubernetes 资源将通过 Kubernetes API 被应用到集群中，从而恢复到历史版本的状态。</li><li>Kubernetes 会根据资源定义执行相应的创建、更新或删除操作。如果版本回滚导致 Kubernetes 资源（例如 Deployment）发生变更，Kubernetes 会执行 <strong>滚动更新</strong>（Rolling Update）操作，逐步用历史版本替换当前运行中的 Pods。</li></ul></li><li><strong>更新 Release 版本</strong>：<ul><li>一旦 Kubernetes 集群成功应用了回滚的资源配置，Helm 会将 Release 的版本号更新为新版本。</li><li>这个新版本的 Release 包含了回滚后的配置，确保 Helm 能够正确跟踪回滚后的版本。</li></ul></li><li><strong>确认回滚结果</strong>：<ul><li>Helm 会返回一个成功回滚的信息，表示已经将应用恢复到之前的状态。</li></ul></li></ol><h3 id=3-helm-rollback-中的关键操作>3. <strong>Helm Rollback 中的关键操作</strong>
<a class=anchor href=#3-helm-rollback-%e4%b8%ad%e7%9a%84%e5%85%b3%e9%94%ae%e6%93%8d%e4%bd%9c>#</a></h3><ol><li><strong>版本回滚</strong>：Helm <code>rollback</code> 会从内部存储中获取某个版本的 Kubernetes 资源定义（包括渲染后的 YAML），并将其应用到集群中。</li><li><strong>资源替换</strong>：Helm 会根据历史版本的资源清单进行更新，更新的过程可能涉及到 <strong>Deployment</strong> 的滚动更新、<strong>ConfigMap</strong> 和 <strong>Secrets</strong> 的替换等操作。</li><li><strong>状态恢复</strong>：Helm 会将 Kubernetes 资源的实际状态恢复为历史版本的状态，包括镜像版本、环境变量、配置文件等。</li></ol><h3 id=4-helm-rollback>4. <strong><code>helm rollback</code> 示例</strong>
<a class=anchor href=#4-helm-rollback>#</a></h3><p>假设我们有一个名为 <code>my-release</code> 的 Helm Release，当前安装版本为 3，并且想将其回滚到版本 2。可以执行以下命令：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>helm rollback my-release <span style=color:#ae81ff>2</span>
</span></span></code></pre></div><p>这个命令会将 <code>my-release</code> 的 Release 配置恢复到版本 2，Helm 会从 Kubernetes 集群中获取并恢复版本 2 时的所有资源配置。</p><p>如果没有指定版本号，Helm 会默认回滚到上一个版本（即版本 2）：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>helm rollback my-release
</span></span></code></pre></div><h3 id=5-rollback-时的注意事项>5. <strong>RollBack 时的注意事项</strong>
<a class=anchor href=#5-rollback-%e6%97%b6%e7%9a%84%e6%b3%a8%e6%84%8f%e4%ba%8b%e9%a1%b9>#</a></h3><ul><li><strong>数据丢失</strong>：如果回滚涉及到应用的数据库或持久化存储，可能会导致数据丢失或状态不一致，因此在回滚前一定要评估数据的状态。</li><li><strong>时间窗口</strong>：回滚操作可能会影响应用的可用性，尤其是当容器中的数据未持久化时。可以通过 Kubernetes 的 <code>Deployment</code> 滚动更新机制来减少影响。</li><li><strong>回滚到合适的版本</strong>：如果在回滚操作后出现问题，可以继续回滚到更早的版本，Helm 支持无限回滚，但每次回滚都需要检查资源和应用状态。</li></ul><h3 id=6-helm-回滚工作原理总结>6. <strong>Helm 回滚工作原理总结</strong>
<a class=anchor href=#6-helm-%e5%9b%9e%e6%bb%9a%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86%e6%80%bb%e7%bb%93>#</a></h3><ul><li>Helm 通过在每次安装或升级时保存资源配置清单，并将其记录为一个版本。</li><li><code>helm rollback</code> 会通过回滚操作恢复历史版本的 Kubernetes 资源清单。</li><li>回滚操作通过替换当前资源来恢复应用的状态。</li><li>Kubernetes 使用滚动更新的方式平滑地替换 Pods，确保系统稳定运行。</li></ul><p>通过 <code>helm rollback</code>，你可以轻松恢复 Helm Release 到一个健康的历史状态，避免因应用升级失败或其他问题导致的服务中断。</p><h2 id=velero备份与恢复流程是什么>velero备份与恢复流程是什么
<a class=anchor href=#velero%e5%a4%87%e4%bb%bd%e4%b8%8e%e6%81%a2%e5%a4%8d%e6%b5%81%e7%a8%8b%e6%98%af%e4%bb%80%e4%b9%88>#</a></h2><p><strong>Velero</strong> 是一个开源工具，用于备份和恢复 Kubernetes 集群中的资源和持久化数据。它支持备份 Kubernetes 集群中的所有资源，包括应用、配置、Secrets、Persistent Volume（PV）等，支持跨集群恢复。</p><p>Velero 的备份和恢复过程通常分为以下几步：</p><h3 id=1-velero-架构概述>1. <strong>Velero 架构概述</strong>
<a class=anchor href=#1-velero-%e6%9e%b6%e6%9e%84%e6%a6%82%e8%bf%b0>#</a></h3><p>Velero 由以下几个主要组件组成：</p><ul><li><strong>Velero 客户端</strong>：用于启动备份、恢复和其他管理操作的 CLI 工具。</li><li><strong>Velero 服务器</strong>：一个在 Kubernetes 集群中运行的控制器，它负责管理和调度备份、恢复任务。它运行在 <code>velero</code> 命名空间中。</li><li><strong>存储后端</strong>：Velero 使用对象存储（如 AWS S3、Google Cloud Storage、Azure Blob Storage、MinIO 等）来存储备份文件。</li><li><strong>Backup和Restore</strong>：Backup 是一个操作对象，描述备份任务的配置，而 Restore 则描述从备份中恢复的操作。</li></ul><h3 id=2-velero-备份流程>2. <strong>Velero 备份流程</strong>
<a class=anchor href=#2-velero-%e5%a4%87%e4%bb%bd%e6%b5%81%e7%a8%8b>#</a></h3><p>Velero 的备份流程分为以下几个步骤：</p><h4 id=步骤-1安装-velero>步骤 1：安装 Velero
<a class=anchor href=#%e6%ad%a5%e9%aa%a4-1%e5%ae%89%e8%a3%85-velero>#</a></h4><p>首先，需要安装并配置 Velero。Velero 会与 Kubernetes API 服务器交互，通过控制器进行备份和恢复操作。</p><ol><li>配置存储后端（如 S3、MinIO、GCS 等），并创建凭证用于访问存储。</li><li>使用 Helm 或 kubectl 安装 Velero 到 Kubernetes 集群中。</li><li>配置 Velero 连接到存储后端和集群。</li></ol><p>安装命令（以 Helm 为例）：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>helm install velero vmware-tanzu/velero --namespace velero --set configuration.provider<span style=color:#f92672>=</span>aws --set configuration.backupStorageLocation.bucket<span style=color:#f92672>=</span>&lt;your-bucket&gt; --set configuration.backupStorageLocation.config.region<span style=color:#f92672>=</span>&lt;your-region&gt;
</span></span></code></pre></div><h4 id=步骤-2执行备份>步骤 2：执行备份
<a class=anchor href=#%e6%ad%a5%e9%aa%a4-2%e6%89%a7%e8%a1%8c%e5%a4%87%e4%bb%bd>#</a></h4><ol><li><strong>备份所有资源</strong>：使用 <code>velero backup create</code> 命令可以创建一个备份，备份将包含整个集群的所有资源，包括命名空间、Pod、服务、部署、配置映射等。还可以选择指定某些资源或命名空间。</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>velero backup create &lt;backup-name&gt; --include-namespaces &lt;namespace1&gt;,&lt;namespace2&gt;
</span></span></code></pre></div><ul><li><code>--include-namespaces</code>：指定要备份的命名空间。如果不指定，默认备份所有命名空间。</li><li><code>--include-resources</code>：指定要备份的特定资源。</li><li><code>--exclude-resources</code>：指定要排除的资源。</li><li><code>--snapshot-volumes</code>：指定是否备份持久化卷（PV）的快照。</li></ul><ol><li><p>备份策略</p><p>：</p><ul><li><strong>资源备份</strong>：包括 Kubernetes 的 API 资源（Pod、Deployment、Service、ConfigMap、Secret 等）。</li><li><strong>持久化卷备份</strong>：如果有持久化存储（例如 PV 和 PVC），Velero 会根据存储提供者的支持情况，创建快照并将其备份。</li></ul></li><li><p><strong>备份检查</strong>：备份任务会开始执行，Velero 会监控任务的状态。可以使用以下命令查看备份的状态：</p></li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>velero backup describe &lt;backup-name&gt; --details
</span></span></code></pre></div><ol><li><strong>备份完成</strong>：一旦备份成功完成，备份数据就会存储在指定的对象存储中。你可以随时根据需要恢复数据。</li></ol><h4 id=步骤-3定期备份>步骤 3：定期备份
<a class=anchor href=#%e6%ad%a5%e9%aa%a4-3%e5%ae%9a%e6%9c%9f%e5%a4%87%e4%bb%bd>#</a></h4><p>Velero 还支持使用 <strong>CronJob</strong> 定期备份集群。你可以使用 Velero 的 CronBackup 功能指定定期的备份任务。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>velero backup create &lt;backup-name&gt; --schedule <span style=color:#e6db74>&#34;0 2 * * *&#34;</span>
</span></span></code></pre></div><h3 id=3-velero-恢复流程>3. <strong>Velero 恢复流程</strong>
<a class=anchor href=#3-velero-%e6%81%a2%e5%a4%8d%e6%b5%81%e7%a8%8b>#</a></h3><p>恢复是将之前备份的数据恢复到 Kubernetes 集群中，恢复的过程包括资源恢复和持久化数据恢复。</p><h4 id=步骤-1创建恢复任务>步骤 1：创建恢复任务
<a class=anchor href=#%e6%ad%a5%e9%aa%a4-1%e5%88%9b%e5%bb%ba%e6%81%a2%e5%a4%8d%e4%bb%bb%e5%8a%a1>#</a></h4><ol><li>使用 <code>velero restore create</code> 命令来创建恢复任务。你可以选择恢复整个备份，也可以选择恢复部分命名空间、资源等。</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>velero restore create --from-backup &lt;backup-name&gt;
</span></span></code></pre></div><ul><li><code>--from-backup</code>：指定要恢复的备份名称。</li><li><code>--namespace-mappings</code>：如果恢复时需要将备份中的某些命名空间映射到目标集群的不同命名空间，可以使用此选项。</li><li><code>--include-resources</code> 和 <code>--exclude-resources</code>：指定要恢复的具体资源。</li></ul><h4 id=步骤-2查看恢复状态>步骤 2：查看恢复状态
<a class=anchor href=#%e6%ad%a5%e9%aa%a4-2%e6%9f%a5%e7%9c%8b%e6%81%a2%e5%a4%8d%e7%8a%b6%e6%80%81>#</a></h4><p>恢复过程会创建一个新的 <code>Restore</code> 资源，Velero 会根据备份中的资源和配置逐个恢复。这些恢复任务会在 Kubernetes 集群中逐步应用。</p><p>查看恢复状态：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>velero restore describe &lt;restore-name&gt; --details
</span></span></code></pre></div><h4 id=步骤-3恢复持久化数据>步骤 3：恢复持久化数据
<a class=anchor href=#%e6%ad%a5%e9%aa%a4-3%e6%81%a2%e5%a4%8d%e6%8c%81%e4%b9%85%e5%8c%96%e6%95%b0%e6%8d%ae>#</a></h4><p>如果备份中包括持久化卷，Velero 会在恢复时尝试从对象存储中恢复快照并创建相应的 Persistent Volume 和 Persistent Volume Claim。</p><ul><li>如果集群中的持久化存储与原集群不同，可能需要手动调整存储类（StorageClass）或更改 PVC 配置。</li></ul><p>恢复完成后，应用的资源和持久化数据会被恢复到 Kubernetes 集群中。</p><h3 id=4-velero-的增量备份和恢复>4. <strong>Velero 的增量备份和恢复</strong>
<a class=anchor href=#4-velero-%e7%9a%84%e5%a2%9e%e9%87%8f%e5%a4%87%e4%bb%bd%e5%92%8c%e6%81%a2%e5%a4%8d>#</a></h3><p>Velero 支持增量备份，这意味着它只会备份自上次备份以来发生更改的资源和数据。因此，Velero 不需要每次都对整个集群进行全量备份，可以减少备份的时间和存储空间。</p><p>增量备份的原理：</p><ul><li>Velero 会记录每个备份点的时间戳，并在后续备份中仅备份自上次备份之后有变化的资源。</li></ul><p>增量恢复：在恢复时，Velero 会从最近的备份恢复数据，并根据需要补充任何增量变化。</p><h3 id=5-velero-的高级特性>5. <strong>Velero 的高级特性</strong>
<a class=anchor href=#5-velero-%e7%9a%84%e9%ab%98%e7%ba%a7%e7%89%b9%e6%80%a7>#</a></h3><ul><li><strong>跨集群备份和恢复</strong>：Velero 支持将备份从一个集群恢复到另一个集群。例如，你可以在灾难恢复场景中使用 Velero 将应用从一个集群恢复到另一个集群。</li><li><strong>备份和恢复资源与持久化数据</strong>：Velero 支持不仅备份 Kubernetes 资源（如 Pods 和 Services），还可以备份持久化数据（如 Persistent Volumes）。</li><li><strong>存储加密</strong>：Velero 支持在对象存储中加密备份数据，确保备份的安全性。</li></ul><h3 id=6-常见命令汇总>6. <strong>常见命令汇总</strong>
<a class=anchor href=#6-%e5%b8%b8%e8%a7%81%e5%91%bd%e4%bb%a4%e6%b1%87%e6%80%bb>#</a></h3><ul><li><p>备份</p><p>：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>velero backup create &lt;backup-name&gt; --include-namespaces &lt;namespace1&gt;,&lt;namespace2&gt; --snapshot-volumes
</span></span></code></pre></div></li><li><p>查看备份状态</p><p>：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>velero backup describe &lt;backup-name&gt; --details
</span></span></code></pre></div></li><li><p>恢复</p><p>：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>velero restore create --from-backup &lt;backup-name&gt;
</span></span></code></pre></div></li><li><p>查看恢复状态</p><p>：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>velero restore describe &lt;restore-name&gt; --details
</span></span></code></pre></div></li><li><p>删除备份</p><p>：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>velero backup delete &lt;backup-name&gt;
</span></span></code></pre></div></li><li><p>删除恢复任务</p><p>：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>velero restore delete &lt;restore-name&gt;
</span></span></code></pre></div></li></ul><h3 id=7-总结>7. <strong>总结</strong>
<a class=anchor href=#7-%e6%80%bb%e7%bb%93>#</a></h3><p>Velero 是 Kubernetes 集群备份和恢复的重要工具，它提供了灵活的备份、恢复、增量备份、持久化数据备份等功能，适用于生产环境中的灾难恢复、跨集群迁移等场景。使用 Velero，管理员可以简化备份操作，确保 Kubernetes 集群的可靠性和数据的安全性。</p><h2 id=docker网络模式>docker网络模式
<a class=anchor href=#docker%e7%bd%91%e7%bb%9c%e6%a8%a1%e5%bc%8f>#</a></h2><p>Docker 提供了多种网络模式，以支持不同的容器化网络需求。每种网络模式都有其适用场景，选择合适的网络模式能优化容器间的通信、网络隔离及安全性。Docker 中主要的网络模式有以下几种：</p><h3 id=1-bridge-网络模式>1. <strong>Bridge 网络模式</strong>
<a class=anchor href=#1-bridge-%e7%bd%91%e7%bb%9c%e6%a8%a1%e5%bc%8f>#</a></h3><ul><li><p><strong>简介</strong>：默认的网络模式，适用于单机 Docker 环境中的容器。所有容器都连接到同一个虚拟网络桥（<code>docker0</code>），并通过桥接交换数据。</p></li><li><p><strong>特点</strong>：</p><ul><li>容器间可以通过 IP 地址互相访问。</li><li>通过端口映射，可以使容器暴露到主机上。</li><li>容器默认会分配一个内部 IP 地址，可以通过端口映射将其暴露给外部。</li></ul></li><li><p><strong>使用场景</strong>：</p><ul><li>默认的容器网络模式。</li><li>适用于需要容器之间隔离但需要主机间访问的场景。</li></ul></li><li><p><strong>命令示例</strong>：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker network create --driver bridge my-bridge-network
</span></span></code></pre></div></li></ul><h3 id=2-host-网络模式>2. <strong>Host 网络模式</strong>
<a class=anchor href=#2-host-%e7%bd%91%e7%bb%9c%e6%a8%a1%e5%bc%8f>#</a></h3><ul><li><p><strong>简介</strong>：容器直接使用宿主机的网络栈，而不创建一个独立的虚拟网络接口。容器不会被分配私有 IP 地址，而是直接与宿主机共享 IP。</p></li><li><p><strong>特点</strong>：</p><ul><li>容器与宿主机共享网络资源，容器的网络与宿主机一致。</li><li>不需要端口映射，容器的端口即是宿主机的端口。</li><li>网络性能较好，因为容器与宿主机共享网络。</li></ul></li><li><p><strong>使用场景</strong>：</p><ul><li>对于需要高性能网络通信的容器，尤其是需要直接访问宿主机网络的场景。</li><li>一些需要运行在宿主机上的服务，比如负载均衡器和代理服务。</li></ul></li><li><p><strong>命令示例</strong>：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker run --network host my-image
</span></span></code></pre></div></li></ul><h3 id=3-none-网络模式>3. <strong>None 网络模式</strong>
<a class=anchor href=#3-none-%e7%bd%91%e7%bb%9c%e6%a8%a1%e5%bc%8f>#</a></h3><ul><li><p><strong>简介</strong>：容器没有任何网络连接。容器没有任何网络接口，也无法与其他容器或外部网络通信。</p></li><li><p><strong>特点</strong>：</p><ul><li>适用于需要完全网络隔离的容器。</li><li>容器内的进程不能访问外部网络。</li><li>适用于需要自定义网络配置的场景，可以在容器内手动配置网络。</li></ul></li><li><p><strong>使用场景</strong>：</p><ul><li>需要完全隔离的容器。</li><li>容器内部需要配置静态 IP 地址或运行特殊的网络应用。</li></ul></li><li><p><strong>命令示例</strong>：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker run --network none my-image
</span></span></code></pre></div></li></ul><h3 id=4-container-网络模式>4. <strong>Container 网络模式</strong>
<a class=anchor href=#4-container-%e7%bd%91%e7%bb%9c%e6%a8%a1%e5%bc%8f>#</a></h3><ul><li><p><strong>简介</strong>：容器共享另一个容器的网络命名空间，即两个容器共享相同的网络接口、IP 地址和端口。网络模式是通过 <code>--network container:&lt;container-name></code> 指定的。</p></li><li><p><strong>特点</strong>：</p><ul><li>共享网络接口，容器之间使用相同的 IP 地址。</li><li>适用于需要紧密耦合的容器应用，它们通过相同的网络环境协同工作。</li><li>使用该模式时，容器之间的网络非常直接，不需要额外的端口映射。</li></ul></li><li><p><strong>使用场景</strong>：</p><ul><li>需要共享网络接口的容器，通常用于多个容器共同提供某一服务（如主从数据库架构）时。</li></ul></li><li><p><strong>命令示例</strong>：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker run --network container:&lt;other-container-name&gt; my-image
</span></span></code></pre></div></li></ul><h3 id=5-overlay-网络模式>5. <strong>Overlay 网络模式</strong>
<a class=anchor href=#5-overlay-%e7%bd%91%e7%bb%9c%e6%a8%a1%e5%bc%8f>#</a></h3><ul><li><p><strong>简介</strong>：用于跨多个 Docker 主机的容器间网络通信。Overlay 网络通过 Docker 的 Swarm 模式或其他集群管理工具（如 Kubernetes）来连接不同主机上的容器，使它们仿佛在同一个网络中。</p></li><li><p><strong>特点</strong>：</p><ul><li>在多主机环境下创建虚拟的网络，允许容器跨主机通信。</li><li>Overlay 网络使用 VXLAN 技术，在物理网络之上创建一个虚拟的网络。</li><li>需要 Docker Swarm 或其他容器编排工具支持。</li></ul></li><li><p><strong>使用场景</strong>：</p><ul><li>适用于需要跨主机通信的容器，尤其是在使用 Docker Swarm 或 Kubernetes 等容器编排工具时。</li></ul></li><li><p><strong>命令示例</strong>：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker network create --driver overlay my-overlay-network
</span></span></code></pre></div></li></ul><h3 id=6-macvlan-网络模式>6. <strong>Macvlan 网络模式</strong>
<a class=anchor href=#6-macvlan-%e7%bd%91%e7%bb%9c%e6%a8%a1%e5%bc%8f>#</a></h3><ul><li><p><strong>简介</strong>：Macvlan 网络模式使每个容器都拥有自己的 MAC 地址，容器通过物理网络直接与外部通信。每个容器都像一个物理主机，具有自己的 IP 地址。</p></li><li><p><strong>特点</strong>：</p><ul><li>容器直接与物理网络连接，具有独立的 MAC 地址和 IP 地址。</li><li>适用于需要容器与外部网络（如企业网络）无缝连接的场景。</li><li>需要配置宿主机的网络接口和桥接。</li></ul></li><li><p><strong>使用场景</strong>：</p><ul><li>容器需要直接暴露给外部网络。</li><li>适用于一些企业环境，尤其是对网络隔离和控制要求较高的场景。</li></ul></li><li><p><strong>命令示例</strong>：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker network create -d macvlan --subnet<span style=color:#f92672>=</span>192.168.1.0/24 --gateway<span style=color:#f92672>=</span>192.168.1.1 -o parent<span style=color:#f92672>=</span>eth0 my-macvlan-network
</span></span></code></pre></div></li></ul><h3 id=7-ipvlan-网络模式>7. <strong>IPvlan 网络模式</strong>
<a class=anchor href=#7-ipvlan-%e7%bd%91%e7%bb%9c%e6%a8%a1%e5%bc%8f>#</a></h3><ul><li><p><strong>简介</strong>：与 Macvlan 类似，IPvlan 允许容器具有独立的 IP 地址，但不同的是，IPvlan 工作在数据链路层以下，容器和宿主机共享一个 MAC 地址。IPvlan 支持更高效的网络管理。</p></li><li><p><strong>特点</strong>：</p><ul><li>提供与 Macvlan 类似的网络隔离功能，但在某些环境下效率更高。</li><li>支持多种模式，如 <code>L2</code> 和 <code>L3</code> 模式，可以在不同的应用场景下选择。</li></ul></li><li><p><strong>使用场景</strong>：</p><ul><li>在一些需要高性能且不希望每个容器都占用独立 MAC 地址的场景中，IPvlan 是一个不错的选择。</li></ul></li><li><p><strong>命令示例</strong>：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker network create -d ipvlan --subnet<span style=color:#f92672>=</span>192.168.1.0/24 --gateway<span style=color:#f92672>=</span>192.168.1.1 -o parent<span style=color:#f92672>=</span>eth0 my-ipvlan-network
</span></span></code></pre></div></li></ul><h3 id=8-docker-网络模式总结>8. <strong>Docker 网络模式总结</strong>
<a class=anchor href=#8-docker-%e7%bd%91%e7%bb%9c%e6%a8%a1%e5%bc%8f%e6%80%bb%e7%bb%93>#</a></h3><table><thead><tr><th>网络模式</th><th>特点</th><th>使用场景</th></tr></thead><tbody><tr><td><strong>Bridge</strong></td><td>默认模式，容器与宿主机隔离，通过端口映射进行通信。</td><td>单机部署，默认的容器网络模式。</td></tr><tr><td><strong>Host</strong></td><td>容器共享宿主机的网络栈，无需端口映射。</td><td>高性能网络需求或直接访问宿主机网络的场景。</td></tr><tr><td><strong>None</strong></td><td>容器没有网络连接，完全隔离。</td><td>完全隔离的容器，通常用于特殊配置。</td></tr><tr><td><strong>Container</strong></td><td>容器共享另一个容器的网络命名空间。</td><td>紧密耦合的容器，通常用于多个容器共同提供服务的场景。</td></tr><tr><td><strong>Overlay</strong></td><td>跨主机容器通信，通常在 Docker Swarm 或 Kubernetes 中使用。</td><td>多主机部署，跨主机容器通信。</td></tr><tr><td><strong>Macvlan</strong></td><td>每个容器都有自己的 MAC 地址，直接与物理网络通信。</td><td>容器直接暴露给外部网络，通常用于企业环境。</td></tr><tr><td><strong>IPvlan</strong></td><td>容器共享宿主机的 MAC 地址，工作在 L2 和 L3 层。</td><td>高效的网络隔离，适用于大规模环境。</td></tr></tbody></table><p>选择合适的网络模式可以帮助容器化应用在性能、安全性和网络隔离上达到最佳效果。</p><h2 id=docker和container区别>docker和container区别☆
<a class=anchor href=#docker%e5%92%8ccontainer%e5%8c%ba%e5%88%ab>#</a></h2><p><strong>Docker</strong> 和 <strong>容器</strong>（Container）是紧密相关的概念，但它们的含义和作用有所不同。下面是这两者的区别：</p><h3 id=1-容器container>1. <strong>容器（Container）</strong>
<a class=anchor href=#1-%e5%ae%b9%e5%99%a8container>#</a></h3><ul><li><strong>定义</strong>：容器是一种轻量级、可移植的虚拟化技术，它通过操作系统级别的虚拟化（而非硬件虚拟化）来隔离应用及其依赖项，确保它们可以在任何环境中运行而不受外部环境影响。</li><li><strong>特点</strong>：<ul><li><strong>隔离性</strong>：容器提供应用的隔离环境，但与虚拟机（VM）不同，它们不需要运行整个操作系统，而是共享宿主机的操作系统内核。</li><li><strong>轻量级</strong>：容器通常比虚拟机更小，启动更快，因为它们不包含完整的操作系统。</li><li><strong>便携性</strong>：容器包含应用及其依赖，能够在任何支持容器的环境中运行，提供了一种“构建一次，随处运行”的方式。</li><li><strong>资源共享</strong>：容器使用宿主机的内核，并通过隔离机制（如命名空间、cgroup）来保证容器之间的资源隔离。</li></ul></li><li><strong>容器的使用场景</strong>：<ul><li>微服务架构：每个微服务都可以运行在不同的容器中，且彼此之间隔离。</li><li>自动化部署：容器可以在任何地方启动应用，极大简化了应用的部署和迁移。</li><li>CI/CD 流水线：容器可用于持续集成和持续交付，确保在不同环境中一致运行。</li></ul></li></ul><h3 id=2-docker>2. <strong>Docker</strong>
<a class=anchor href=#2-docker>#</a></h3><ul><li><strong>定义</strong>：Docker 是一个开源的容器化平台，用于开发、打包、部署和运行应用程序。它简化了容器的创建、部署和管理，提供了一整套工具链。</li><li><strong>特点</strong>：<ul><li><strong>容器管理工具</strong>：Docker 提供了方便的命令行工具和图形化界面，可以轻松创建、运行和管理容器。</li><li><strong>镜像管理</strong>：Docker 通过镜像来打包应用及其依赖，Docker Hub 提供了官方和用户贡献的镜像库，用户可以方便地拉取镜像或将自己的镜像推送到镜像库。</li><li><strong>Docker 引擎</strong>：Docker 引擎是容器的运行时，负责容器的生命周期管理，包括容器的创建、启动、停止、销毁等。</li><li><strong>容器编排</strong>：Docker 支持容器的编排功能，尤其是通过 <strong>Docker Swarm</strong> 和 <strong>Docker Compose</strong>，使得多容器部署和管理变得更容易。</li></ul></li><li><strong>Docker 的使用场景</strong>：<ul><li>在本地开发和测试：开发人员使用 Docker 容器在本地构建、运行、测试和调试应用。</li><li>在生产环境中运行应用：Docker 容器能够快速启动和停止，非常适合高效、可扩展的生产环境。</li><li>容器编排：使用 Docker Compose 或 Docker Swarm 来管理多容器的应用部署。</li></ul></li></ul><h3 id=3-docker-与容器的区别>3. <strong>Docker 与容器的区别</strong>
<a class=anchor href=#3-docker-%e4%b8%8e%e5%ae%b9%e5%99%a8%e7%9a%84%e5%8c%ba%e5%88%ab>#</a></h3><table><thead><tr><th><strong>对比项</strong></th><th><strong>容器（Container）</strong></th><th><strong>Docker</strong></th></tr></thead><tbody><tr><td><strong>定义</strong></td><td>一种轻量级、可移植的应用封装和运行环境。</td><td>一个容器化平台，提供创建、部署和管理容器的工具。</td></tr><tr><td><strong>作用</strong></td><td>提供应用的隔离环境，确保应用的一致性和便携性。</td><td>提供容器管理和自动化工具，帮助开发和运维人员管理容器。</td></tr><tr><td><strong>技术层面</strong></td><td>容器是虚拟化技术的一种实现方式，依赖于宿主机的内核。</td><td>Docker 是容器化的实现工具，使用容器技术管理应用。</td></tr><tr><td><strong>使用范围</strong></td><td>容器可以由任何容器引擎（如 Docker、Podman）管理。</td><td>Docker 是目前最流行的容器引擎，提供完整的容器生命周期管理。</td></tr><tr><td><strong>容器管理</strong></td><td>容器本身只是一个封装应用的单元，不提供容器生命周期管理。</td><td>Docker 提供从创建、部署到运行和销毁容器的一站式管理功能。</td></tr><tr><td><strong>镜像管理</strong></td><td>容器本身是基于镜像启动的，但容器管理工具需要提供镜像的创建、推送和拉取功能。</td><td>Docker 提供镜像构建、发布和拉取工具（如 Dockerfile 和 Docker Hub）。</td></tr></tbody></table><h3 id=4-总结-1>4. <strong>总结</strong>
<a class=anchor href=#4-%e6%80%bb%e7%bb%93-1>#</a></h3><ul><li><strong>容器</strong> 是一种虚拟化技术，用于隔离和运行应用程序，而 <strong>Docker</strong> 是一种具体的容器化平台，提供了完整的工具链来创建、管理和部署容器。</li><li><strong>Docker</strong> 作为最流行的容器引擎，简化了容器的使用，极大地推动了容器技术的普及和发展。</li></ul><h2 id=如何减dockerfile成镜像体积>如何减⼩dockerfile⽣成镜像体积？
<a class=anchor href=#%e5%a6%82%e4%bd%95%e5%87%8fdockerfile%e6%88%90%e9%95%9c%e5%83%8f%e4%bd%93%e7%a7%af>#</a></h2><p>在 Dockerfile 中生成镜像时，镜像体积的大小是一个很重要的考虑因素。减小镜像体积不仅可以提高镜像的拉取速度和部署效率，还能减少存储空间的消耗。以下是一些常见的优化方法，可以有效减小 Docker 镜像的体积：</p><h3 id=1-选择合适的基础镜像>1. <strong>选择合适的基础镜像</strong>
<a class=anchor href=#1-%e9%80%89%e6%8b%a9%e5%90%88%e9%80%82%e7%9a%84%e5%9f%ba%e7%a1%80%e9%95%9c%e5%83%8f>#</a></h3><ul><li><p>使用 <strong>精简版的基础镜像</strong>，如 <code>alpine</code>，它比传统的 <code>ubuntu</code>、<code>debian</code> 等镜像更小，只有几 MB。</p></li><li><p>如果不需要完整的操作系统功能，可以选择 <strong>更小的镜像</strong>，例如：</p><ul><li><code>alpine</code>: 体积小，适用于大多数轻量级应用。</li><li><code>busybox</code>: 更小，适用于非常精简的环境。</li><li><code>debian:slim</code> 或 <code>ubuntu:20.04</code> 的精简版本。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-dockerfile data-lang=dockerfile><span style=display:flex><span><span style=color:#66d9ef>FROM</span><span style=color:#e6db74> alpine:3.14</span><span style=color:#960050;background-color:#1e0010>
</span></span></span></code></pre></div></li></ul><h3 id=2-减少不必要的文件>2. <strong>减少不必要的文件</strong>
<a class=anchor href=#2-%e5%87%8f%e5%b0%91%e4%b8%8d%e5%bf%85%e8%a6%81%e7%9a%84%e6%96%87%e4%bb%b6>#</a></h3><ul><li><p>通过 <code>.dockerignore</code> 文件排除不必要的文件和目录，如构建文件、文档、测试文件等，避免将其添加到镜像中。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plaintext data-lang=plaintext><span style=display:flex><span>.git
</span></span><span style=display:flex><span>node_modules
</span></span><span style=display:flex><span>test/
</span></span><span style=display:flex><span>*.md
</span></span></code></pre></div></li><li><p>在构建时确保 <strong>不将临时文件或无用文件</strong> 添加到镜像。</p></li></ul><h3 id=3-合并多个>3. <strong>合并多个 <code>RUN</code> 命令</strong>
<a class=anchor href=#3-%e5%90%88%e5%b9%b6%e5%a4%9a%e4%b8%aa>#</a></h3><ul><li><p>Dockerfile 中的每一条 <code>RUN</code> 命令都会生成一个新的镜像层，<strong>减少层数</strong> 可以有效减小镜像体积。将多个相关的 <code>RUN</code> 命令合并成一个，可以减少中间层的数量。</p></li><li><p>使用 <code>&&</code> 将多个命令链接在一起，避免每个命令产生新的层。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-dockerfile data-lang=dockerfile><span style=display:flex><span><span style=color:#66d9ef>RUN</span> apt-get update <span style=color:#f92672>&amp;&amp;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    apt-get install -y <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    curl <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    vim <span style=color:#f92672>&amp;&amp;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    rm -rf /var/lib/apt/lists/*<span style=color:#960050;background-color:#1e0010>
</span></span></span></code></pre></div></li></ul><h3 id=4-清理临时文件和缓存>4. <strong>清理临时文件和缓存</strong>
<a class=anchor href=#4-%e6%b8%85%e7%90%86%e4%b8%b4%e6%97%b6%e6%96%87%e4%bb%b6%e5%92%8c%e7%bc%93%e5%ad%98>#</a></h3><ul><li><p>安装软件包后，要 <strong>清理缓存</strong> 和 <strong>临时文件</strong>，如包管理器缓存、日志文件等，以减小镜像体积。</p><ul><li><p>对于 <strong>APT</strong> 包管理器（如 Ubuntu 和 Debian）：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-dockerfile data-lang=dockerfile><span style=display:flex><span><span style=color:#66d9ef>RUN</span> apt-get update <span style=color:#f92672>&amp;&amp;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    apt-get install -y curl <span style=color:#f92672>&amp;&amp;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    apt-get clean <span style=color:#f92672>&amp;&amp;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    rm -rf /var/lib/apt/lists/*<span style=color:#960050;background-color:#1e0010>
</span></span></span></code></pre></div></li><li><p>对于 <strong>YUM</strong>（如 CentOS 或 RHEL）：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-dockerfile data-lang=dockerfile><span style=display:flex><span><span style=color:#66d9ef>RUN</span> yum install -y curl <span style=color:#f92672>&amp;&amp;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    yum clean all <span style=color:#f92672>&amp;&amp;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    rm -rf /var/cache/yum<span style=color:#960050;background-color:#1e0010>
</span></span></span></code></pre></div></li><li><p>对于 <strong>npm</strong>：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-dockerfile data-lang=dockerfile><span style=display:flex><span><span style=color:#66d9ef>RUN</span> npm install <span style=color:#f92672>&amp;&amp;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    npm cache clean --force<span style=color:#960050;background-color:#1e0010>
</span></span></span></code></pre></div></li></ul></li></ul><h3 id=5-使用多阶段构建multi-stage-builds>5. <strong>使用多阶段构建（Multi-stage builds）</strong>
<a class=anchor href=#5-%e4%bd%bf%e7%94%a8%e5%a4%9a%e9%98%b6%e6%ae%b5%e6%9e%84%e5%bb%bamulti-stage-builds>#</a></h3><ul><li><p><strong>多阶段构建</strong> 可以帮助你在构建过程中只保留最终镜像所需的内容，而不包含构建过程中使用的中间文件和工具。例如，可以在第一阶段使用一个较大的构建镜像（如包含编译工具），在第二阶段只复制编译好的产物。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-dockerfile data-lang=dockerfile><span style=display:flex><span><span style=color:#75715e># 第一阶段：构建阶段</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>FROM</span><span style=color:#e6db74> node:14 AS builder</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>WORKDIR</span><span style=color:#e6db74> /app</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>COPY</span> . .<span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>RUN</span> npm install <span style=color:#f92672>&amp;&amp;</span> npm run build<span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#75715e># 第二阶段：运行阶段</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>FROM</span><span style=color:#e6db74> node:14-slim</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>WORKDIR</span><span style=color:#e6db74> /app</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>COPY</span> --from<span style=color:#f92672>=</span>builder /app/dist /app<span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>CMD</span> [<span style=color:#e6db74>&#34;node&#34;</span>, <span style=color:#e6db74>&#34;app.js&#34;</span>]<span style=color:#960050;background-color:#1e0010>
</span></span></span></code></pre></div><p>这样做可以避免将不必要的构建工具和依赖（如 <code>node_modules</code>）包含在最终镜像中。</p></li></ul><h3 id=6-避免安装不必要的依赖>6. <strong>避免安装不必要的依赖</strong>
<a class=anchor href=#6-%e9%81%bf%e5%85%8d%e5%ae%89%e8%a3%85%e4%b8%8d%e5%bf%85%e8%a6%81%e7%9a%84%e4%be%9d%e8%b5%96>#</a></h3><ul><li>只安装应用运行所需的最小依赖，避免安装多余的包或工具。使用精简的镜像和最小的依赖集，避免安装开发工具和测试库。</li></ul><h3 id=7-压缩镜像文件>7. <strong>压缩镜像文件</strong>
<a class=anchor href=#7-%e5%8e%8b%e7%bc%a9%e9%95%9c%e5%83%8f%e6%96%87%e4%bb%b6>#</a></h3><ul><li><p>对于某些应用，尤其是静态文件（如网页、图片等），可以在容器中使用工具进行压缩，减少文件大小。例如，使用 <code>gzip</code> 压缩文件。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-dockerfile data-lang=dockerfile><span style=display:flex><span><span style=color:#66d9ef>RUN</span> gzip -r /app/static<span style=color:#960050;background-color:#1e0010>
</span></span></span></code></pre></div></li></ul><h3 id=8-减少镜像中的层数>8. <strong>减少镜像中的层数</strong>
<a class=anchor href=#8-%e5%87%8f%e5%b0%91%e9%95%9c%e5%83%8f%e4%b8%ad%e7%9a%84%e5%b1%82%e6%95%b0>#</a></h3><ul><li><p>通过 <strong>合并命令</strong> 和 <strong>避免多次复制</strong>，可以减少镜像的层数，从而减小镜像体积。例如，尽量避免在 Dockerfile 中多次 <code>COPY</code> 同一个目录。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-dockerfile data-lang=dockerfile><span style=display:flex><span><span style=color:#66d9ef>COPY</span> . /app<span style=color:#960050;background-color:#1e0010>
</span></span></span></code></pre></div><p>而不是：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-dockerfile data-lang=dockerfile><span style=display:flex><span><span style=color:#66d9ef>COPY</span> src /app/src<span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>COPY</span> config /app/config<span style=color:#960050;background-color:#1e0010>
</span></span></span></code></pre></div></li></ul><h3 id=9-优化文件格式和大小>9. <strong>优化文件格式和大小</strong>
<a class=anchor href=#9-%e4%bc%98%e5%8c%96%e6%96%87%e4%bb%b6%e6%a0%bc%e5%bc%8f%e5%92%8c%e5%a4%a7%e5%b0%8f>#</a></h3><ul><li><p>对于应用中包含的二进制文件或压缩文件，使用 <strong>优化过的格式</strong> 和 <strong>最小化的文件版本</strong>。例如，使用 <code>strip</code> 删除二进制文件中的调试信息。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-dockerfile data-lang=dockerfile><span style=display:flex><span><span style=color:#66d9ef>RUN</span> strip /usr/local/bin/my-app<span style=color:#960050;background-color:#1e0010>
</span></span></span></code></pre></div></li></ul><h3 id=10-使用-docker-buildkit-和缓存机制>10. <strong>使用 Docker BuildKit 和缓存机制</strong>
<a class=anchor href=#10-%e4%bd%bf%e7%94%a8-docker-buildkit-%e5%92%8c%e7%bc%93%e5%ad%98%e6%9c%ba%e5%88%b6>#</a></h3><ul><li><p>启用 <strong>Docker BuildKit</strong>，它支持更高效的构建过程并提供更强的缓存机制，能够有效减少镜像的体积。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>DOCKER_BUILDKIT<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span> docker build -t myimage .
</span></span></code></pre></div></li></ul><p>通过这些优化方法，能够显著减少 Docker 镜像的体积，从而提升构建效率、减少存储和网络带宽的消耗。</p><h2 id=k8s日志采集方案>k8s日志采集方案
<a class=anchor href=#k8s%e6%97%a5%e5%bf%97%e9%87%87%e9%9b%86%e6%96%b9%e6%a1%88>#</a></h2><p>在 Kubernetes 环境中，日志采集是运维和监控的核心组成部分。一个良好的日志采集方案能帮助团队有效地监控、排查故障和审计操作。以下是一些常见的 Kubernetes 日志采集方案，以及如何实现这些方案。</p><h3 id=1-kubernetes-日志来源>1. <strong>Kubernetes 日志来源</strong>
<a class=anchor href=#1-kubernetes-%e6%97%a5%e5%bf%97%e6%9d%a5%e6%ba%90>#</a></h3><ul><li><strong>容器日志</strong>：每个 Pod 中运行的容器都会生成标准输出（stdout）和标准错误（stderr）日志，这些日志是容器运行时日志的主要来源。</li><li><strong>节点日志</strong>：Kubernetes 节点的系统日志，例如 kubelet、kube-proxy、docker（或 containerd）等相关组件的日志。</li><li><strong>应用日志</strong>：应用本身生成的日志文件，通常存储在容器内部或者通过外部存储系统（如日志服务器）进行管理。</li></ul><h3 id=2-常见的-kubernetes-日志采集方案>2. <strong>常见的 Kubernetes 日志采集方案</strong>
<a class=anchor href=#2-%e5%b8%b8%e8%a7%81%e7%9a%84-kubernetes-%e6%97%a5%e5%bf%97%e9%87%87%e9%9b%86%e6%96%b9%e6%a1%88>#</a></h3><p>以下是几种常见的日志采集方案，通常通过结合使用多个工具来实现更强大的日志处理能力：</p><h4 id=1-efkelasticsearch-fluentd-kibana>1. <strong>EFK（Elasticsearch, Fluentd, Kibana）</strong>
<a class=anchor href=#1-efkelasticsearch-fluentd-kibana>#</a></h4><p>EFK 是 Kubernetes 中最常见的日志采集方案之一。它提供了集中式的日志收集、存储、查询和展示。</p><ul><li><strong>Fluentd</strong>：Fluentd 是一个开源的日志收集器，负责从各个 Kubernetes 节点（或容器）收集日志数据。它能够将日志数据转发到 Elasticsearch 或其他后端存储系统。</li><li><strong>Elasticsearch</strong>：Elasticsearch 是一个强大的分布式搜索和分析引擎，存储和索引日志数据。它可以高效地处理海量日志数据，支持快速查询。</li><li><strong>Kibana</strong>：Kibana 提供了一个 Web 界面，用于可视化 Elasticsearch 中存储的日志数据。用户可以通过 Kibana 来查询、分析和展示日志数据。</li></ul><p><strong>工作流程</strong>：</p><ul><li>Fluentd 收集容器的标准输出和标准错误日志。</li><li>Fluentd 将日志转发到 Elasticsearch 存储和索引。</li><li>用户通过 Kibana 进行日志查询和可视化展示。</li></ul><p><strong>优点</strong>：</p><ul><li>强大的搜索和分析能力。</li><li>丰富的可视化功能。</li><li>高度可扩展。</li></ul><p><strong>缺点</strong>：</p><ul><li>配置相对复杂，尤其是对于大规模集群。</li><li>Elasticsearch 需要大量资源，尤其是存储和计算资源。</li></ul><h4 id=2-elk-stack-elasticsearch-logstash-kibana>2. <strong>ELK Stack (Elasticsearch, Logstash, Kibana)</strong>
<a class=anchor href=#2-elk-stack-elasticsearch-logstash-kibana>#</a></h4><p>ELK Stack 与 EFK 类似，但使用 <strong>Logstash</strong> 替代了 Fluentd。</p><ul><li><strong>Logstash</strong>：Logstash 负责日志的采集、处理和转发。与 Fluentd 类似，Logstash 也能够从多个数据源收集日志并进行过滤和转换。</li><li><strong>Elasticsearch</strong>：存储和索引日志数据。</li><li><strong>Kibana</strong>：用于展示和分析日志数据。</li></ul><p><strong>优点</strong>：</p><ul><li>Logstash 提供了强大的日志处理能力，能够进行更复杂的数据管道处理。</li><li>高度集成的工具链，适合需要复杂日志处理的场景。</li></ul><p><strong>缺点</strong>：</p><ul><li>Logstash 性能上相对 Fluentd 稍弱，尤其在处理大规模日志数据时，可能会面临瓶颈。</li><li>ELK stack 的资源消耗较大。</li></ul><h4 id=3-fluent-bit--elasticsearch--kibanaefk变种>3. <strong>Fluent Bit + Elasticsearch + Kibana（EFK变种）</strong>
<a class=anchor href=#3-fluent-bit--elasticsearch--kibanaefk%e5%8f%98%e7%a7%8d>#</a></h4><p>Fluent Bit 是 Fluentd 的轻量级替代品，适用于资源受限的环境。它的工作原理与 Fluentd 相似，但更专注于高效、低资源消耗。</p><ul><li><strong>Fluent Bit</strong>：作为日志收集器，负责从 Kubernetes 节点或容器中收集日志。</li><li><strong>Elasticsearch</strong>：存储和索引日志。</li><li><strong>Kibana</strong>：可视化展示日志。</li></ul><p><strong>优点</strong>：</p><ul><li>比 Fluentd 更轻量化，适合资源有限的集群。</li><li>配置简单、性能优异。</li></ul><p><strong>缺点</strong>：</p><ul><li>Fluent Bit 相对于 Fluentd 功能较少，定制化和插件支持较弱。</li><li>Elasticsearch 资源占用较高。</li></ul><h4 id=4-prometheus--loki--grafanaplg-stack>4. <strong>Prometheus + Loki + Grafana（PLG Stack）</strong>
<a class=anchor href=#4-prometheus--loki--grafanaplg-stack>#</a></h4><p>这是一个新的日志采集方案，它依赖于 <strong>Prometheus</strong> 和 <strong>Grafana</strong>，并结合 <strong>Loki</strong> 作为日志收集组件。Loki 是一个由 Grafana Labs 提供的日志聚合系统，专门用于与 Prometheus 和 Grafana 配合使用。</p><ul><li><strong>Loki</strong>：Loki 是一个轻量级的日志聚合系统，可以高效地存储和查询日志数据。它的设计灵感来自于 Prometheus，旨在提供简化的日志存储和查询机制。</li><li><strong>Prometheus</strong>：用于监控和收集集群的指标数据。</li><li><strong>Grafana</strong>：用于展示日志数据和指标数据，提供统一的可视化面板。</li></ul><p><strong>工作流程</strong>：</p><ul><li>Loki 从各个节点和容器中收集日志。</li><li>Grafana 用于展示日志数据和监控数据，提供统一的视图。</li></ul><p><strong>优点</strong>：</p><ul><li>Loki 和 Prometheus 都是由 Grafana Labs 提供，且设计理念相近，便于集成。</li><li>简化了日志和指标的聚合和展示。</li><li>轻量级，适用于资源较小的集群。</li></ul><p><strong>缺点</strong>：</p><ul><li>相较于 ELK 或 EFK，Loki 在查询和可视化上可能略显不足。</li><li>适合以日志为主的轻量级日志收集需求。</li></ul><h4 id=5-loggly-datadog-等商业化解决方案>5. <strong>Loggly, Datadog 等商业化解决方案</strong>
<a class=anchor href=#5-loggly-datadog-%e7%ad%89%e5%95%86%e4%b8%9a%e5%8c%96%e8%a7%a3%e5%86%b3%e6%96%b9%e6%a1%88>#</a></h4><ul><li>这些工具提供了即插即用的日志聚合解决方案，并集成了监控、分析、告警等功能。</li><li>适合那些没有时间或资源管理自己的日志基础设施的团队。</li></ul><p><strong>优点</strong>：</p><ul><li>易于部署和使用，通常只需要配置日志代理即可开始收集日志。</li><li>提供强大的搜索、分析和可视化功能。</li></ul><p><strong>缺点</strong>：</p><ul><li>需要付费，长期使用可能会产生较高的成本。</li><li>自定义和扩展性有限。</li></ul><hr><h3 id=3-日志采集方案的选型>3. <strong>日志采集方案的选型</strong>
<a class=anchor href=#3-%e6%97%a5%e5%bf%97%e9%87%87%e9%9b%86%e6%96%b9%e6%a1%88%e7%9a%84%e9%80%89%e5%9e%8b>#</a></h3><ul><li><strong>规模较小的集群</strong>：如果你的集群较小，且没有复杂的日志分析需求，推荐使用 <strong>Fluent Bit + Elasticsearch + Kibana</strong> 方案，简单高效。</li><li><strong>大规模集群</strong>：如果集群较大，且需要强大的日志处理能力，推荐使用 <strong>EFK Stack</strong> 或 <strong>ELK Stack</strong>，这些方案提供了灵活的日志处理、存储和可视化能力。</li><li><strong>Prometheus 用户</strong>：如果你的集群已经使用 <strong>Prometheus</strong> 进行监控，推荐使用 <strong>Loki + Grafana</strong> 进行日志收集和展示，能够与 Prometheus 数据源无缝集成。</li><li><strong>商业化方案</strong>：如果你希望减少运维工作量并快速部署，且愿意为此支付费用，可以选择像 <strong>Datadog</strong> 或 <strong>Loggly</strong> 等商业日志管理平台。</li></ul><h3 id=4-日志采集最佳实践>4. <strong>日志采集最佳实践</strong>
<a class=anchor href=#4-%e6%97%a5%e5%bf%97%e9%87%87%e9%9b%86%e6%9c%80%e4%bd%b3%e5%ae%9e%e8%b7%b5>#</a></h3><ul><li><strong>集中式日志收集</strong>：确保所有节点和容器的日志都通过一个集中的系统收集，以便于管理和分析。</li><li><strong>日志保留策略</strong>：根据需要设定合适的日志保留策略，不必保留所有日志数据，以节省存储空间。</li><li><strong>日志聚合和索引</strong>：对于大量日志数据，合理配置聚合和索引策略，避免日志数据过多导致系统性能下降。</li><li><strong>日志分析和告警</strong>：根据日志内容设置关键字告警，当出现异常时及时通知相关人员，快速响应。</li></ul><p>通过采用合适的日志采集方案，能够确保 Kubernetes 环境中的日志管理高效且易于扩展。</p><h2 id=pause容器的用途>Pause容器的用途☆
<a class=anchor href=#pause%e5%ae%b9%e5%99%a8%e7%9a%84%e7%94%a8%e9%80%94>#</a></h2><p>在 Kubernetes 中，<strong>Pause 容器</strong> 是一种特殊的容器，通常作为 <strong>Pod 的占位容器</strong>，用于保持 Pod 中的网络命名空间（network namespace）不被销毁。它并不执行任何实际的工作负载，而是作为一个虚拟的容器存在，用来维持 Pod 的状态，特别是在 Pod 需要共享网络命名空间时。Pause 容器本身并不运行任何服务或应用程序，通常用于管理和维护 Pod 的生命周期。</p><h3 id=pause-容器的主要用途>Pause 容器的主要用途
<a class=anchor href=#pause-%e5%ae%b9%e5%99%a8%e7%9a%84%e4%b8%bb%e8%a6%81%e7%94%a8%e9%80%94>#</a></h3><ol><li><strong>网络命名空间的管理</strong>
在 Kubernetes 中，Pod 是一个基本的部署单元，Pod 中的所有容器共享相同的网络命名空间（network namespace）。Pause 容器充当了这个网络命名空间的“守护进程”。它本身不执行任何业务逻辑，只是确保 Pod 的网络命名空间持续存在。<ul><li>它会创建并持有 Pod 的网络命名空间，允许其他容器在同一个命名空间中运行，并与 Pod 内的其他容器通过网络进行通信。</li></ul></li><li><strong>Pod 生命周期的管理</strong>
在 Pod 的生命周期中，Pause 容器是第一个启动的容器，也是最后一个退出的容器。即使 Pod 中的其他容器因故障或重启而被终止，Pause 容器会保持运行，直到整个 Pod 被删除。这确保了在 Pod 内的所有容器共享同一网络和存储卷。</li><li><strong>容器间共享存储卷</strong>
在 Pod 内，容器之间共享存储卷，而 Pause 容器通过维持网络命名空间的持续性，间接帮助 Pod 内其他容器访问共享的存储卷。它并不直接访问存储卷，但它的存在使得其他容器能够使用这些卷。</li><li><strong>Pod 的资源隔离</strong>
Pause 容器在 Pod 中扮演的角色类似于占位符，确保 Pod 内的资源（如网络和存储）被正确隔离，并且网络命名空间不会被其他容器的启动和停止所干扰。</li></ol><h3 id=如何启动-pause-容器>如何启动 Pause 容器
<a class=anchor href=#%e5%a6%82%e4%bd%95%e5%90%af%e5%8a%a8-pause-%e5%ae%b9%e5%99%a8>#</a></h3><p>在 Kubernetes 中，Pause 容器是由 kubelet 自动启动的，用户通常不需要手动管理它。它在每个 Pod 的生命周期中自动创建，并由 kubelet 作为基础容器启动。当 Pod 中其他容器启动时，它们会共享该 Pause 容器所提供的网络命名空间。</p><h3 id=典型的-pause-容器镜像>典型的 Pause 容器镜像
<a class=anchor href=#%e5%85%b8%e5%9e%8b%e7%9a%84-pause-%e5%ae%b9%e5%99%a8%e9%95%9c%e5%83%8f>#</a></h3><ul><li>在 Kubernetes 中，Pause 容器通常使用的是 <strong>k8s.gcr.io/pause</strong> 镜像。</li><li>它非常轻量，通常只包含一个简单的 <code>/pause</code> 进程，确保网络命名空间处于活动状态。</li><li>它的体积极小，通常在 100 KB 左右，最大限度地减少资源消耗。</li></ul><h3 id=总结-31>总结
<a class=anchor href=#%e6%80%bb%e7%bb%93-31>#</a></h3><p>Pause 容器是 Kubernetes 中用于网络命名空间管理的特殊容器。它不执行任何业务逻辑，只是确保 Pod 的网络命名空间能够持续存在，并在 Pod 生命周期中充当一个基础容器，支持容器间共享网络和存储资源。</p><h2 id=k8s证书过期怎么更新>k8s证书过期怎么更新
<a class=anchor href=#k8s%e8%af%81%e4%b9%a6%e8%bf%87%e6%9c%9f%e6%80%8e%e4%b9%88%e6%9b%b4%e6%96%b0>#</a></h2><p>Kubernetes 集群使用了多种证书，主要包括 API Server、Kubelet、Controller Manager 和 Scheduler 等组件的证书。如果这些证书过期，Kubernetes 集群将无法正常工作，因此及时更新证书是非常重要的。</p><h3 id=1-检查证书的有效期>1. <strong>检查证书的有效期</strong>
<a class=anchor href=#1-%e6%a3%80%e6%9f%a5%e8%af%81%e4%b9%a6%e7%9a%84%e6%9c%89%e6%95%88%e6%9c%9f>#</a></h3><p>首先，您需要确认哪些证书已经过期。可以使用以下命令查看证书的有效期：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get certificatesigningrequests -A
</span></span></code></pre></div><p>或者查看具体证书的有效期：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>openssl x509 -in /etc/kubernetes/pki/apiserver.crt -noout -dates
</span></span></code></pre></div><p>这个命令可以查看 API Server 证书的有效期。其他组件证书路径通常位于 <code>/etc/kubernetes/pki/</code> 下。</p><h3 id=2-使用>2. <strong>使用 <code>kubeadm</code> 更新证书</strong>
<a class=anchor href=#2-%e4%bd%bf%e7%94%a8>#</a></h3><p>如果您的集群是通过 <code>kubeadm</code> 部署的，<code>kubeadm</code> 提供了方便的证书更新工具。</p><h4 id=步骤>步骤：
<a class=anchor href=#%e6%ad%a5%e9%aa%a4>#</a></h4><ol><li><p><strong>查看证书过期情况</strong> 使用 <code>kubeadm certs check-expiration</code> 命令来检查证书的到期日期：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubeadm certs check-expiration
</span></span></code></pre></div></li><li><p><strong>更新证书</strong> 如果某些证书已经过期或即将过期，可以使用 <code>kubeadm certs renew</code> 命令来更新证书。例如，更新 API Server 证书：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubeadm certs renew apiserver
</span></span></code></pre></div><p>您可以根据需要更新其他证书：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubeadm certs renew apiserver-kubelet-client
</span></span><span style=display:flex><span>kubeadm certs renew controller-manager
</span></span><span style=display:flex><span>kubeadm certs renew scheduler
</span></span></code></pre></div></li><li><p><strong>更新完成后重启相关组件</strong> 证书更新后，Kubernetes 组件需要重新加载这些证书。通常可以通过重启组件来实现：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>systemctl restart kubelet
</span></span></code></pre></div></li><li><p><strong>验证证书更新</strong> 更新证书后，再次使用 <code>check-expiration</code> 命令确认证书是否已经更新。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubeadm certs check-expiration
</span></span></code></pre></div></li></ol><h3 id=3-手动更新证书>3. <strong>手动更新证书</strong>
<a class=anchor href=#3-%e6%89%8b%e5%8a%a8%e6%9b%b4%e6%96%b0%e8%af%81%e4%b9%a6>#</a></h3><p>如果集群不是通过 <code>kubeadm</code> 部署的，您可能需要手动更新证书。以下是手动更新 Kubernetes 组件证书的步骤：</p><ol><li><p><strong>生成新的证书</strong>： 使用 <code>openssl</code> 工具或其他证书管理工具生成新的证书和密钥。例如：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>openssl req -new -newkey rsa:2048 -days <span style=color:#ae81ff>365</span> -nodes -keyout /etc/kubernetes/pki/apiserver.key -out /etc/kubernetes/pki/apiserver.csr
</span></span></code></pre></div></li><li><p><strong>签发证书</strong>： 使用一个 Kubernetes CA 证书（通常是由 Kubernetes 集群管理员管理的证书）来签署新的证书：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>openssl x509 -req -in /etc/kubernetes/pki/apiserver.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out /etc/kubernetes/pki/apiserver.crt -days <span style=color:#ae81ff>365</span>
</span></span></code></pre></div></li><li><p><strong>更新证书后重启组件</strong>： 更新证书后，您需要重启 Kubernetes 组件（如 kube-apiserver、kubelet 等）：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>systemctl restart kubelet
</span></span></code></pre></div></li></ol><h3 id=4-kubernetes-ca-证书的更新>4. <strong>Kubernetes CA 证书的更新</strong>
<a class=anchor href=#4-kubernetes-ca-%e8%af%81%e4%b9%a6%e7%9a%84%e6%9b%b4%e6%96%b0>#</a></h3><p>在某些情况下，您可能还需要更新 Kubernetes 的根证书（CA 证书）。这通常发生在证书的根 CA 证书过期时。在这种情况下，您需要：</p><ol><li>生成新的 CA 证书。</li><li>使用新证书重新签署所有 Kubernetes 组件的证书。</li><li>分发新的 CA 证书到集群中所有节点，确保每个节点的证书库都包含新的 CA 证书。</li></ol><h3 id=5-automating-certificate-rotation>5. <strong>Automating Certificate Rotation</strong>
<a class=anchor href=#5-automating-certificate-rotation>#</a></h3><p>在 Kubernetes 1.14+ 中，Kubernetes 支持自动证书轮换。您可以在 API Server 中启用自动证书轮换功能，这样可以避免手动更新证书。具体配置需要在 API Server 启动时通过 <code>--rotate-certificates</code> 参数启用：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kube-apiserver --rotate-certificates<span style=color:#f92672>=</span>true
</span></span></code></pre></div><h3 id=6-检查证书更新后的状态>6. <strong>检查证书更新后的状态</strong>
<a class=anchor href=#6-%e6%a3%80%e6%9f%a5%e8%af%81%e4%b9%a6%e6%9b%b4%e6%96%b0%e5%90%8e%e7%9a%84%e7%8a%b6%e6%80%81>#</a></h3><p>证书更新完成后，可以使用以下命令检查集群状态，确保组件正常工作：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get nodes
</span></span><span style=display:flex><span>kubectl get pods --all-namespaces
</span></span></code></pre></div><h3 id=总结-32>总结
<a class=anchor href=#%e6%80%bb%e7%bb%93-32>#</a></h3><p>更新 Kubernetes 证书的步骤包括：</p><ol><li>使用 <code>kubeadm</code> 或手动生成和签发新的证书。</li><li>更新证书后，重启相关组件（如 API Server、Kubelet、Controller Manager 等）。</li><li>对于使用 <code>kubeadm</code> 部署的集群，<code>kubeadm certs renew</code> 是一个便捷的工具，简化了证书更新的流程。</li></ol><p>在管理大规模集群时，证书的自动轮换和管理非常重要，因此建议尽量使用自动证书更新机制。</p><h2 id=k8s-qos等级>K8S QoS等级☆
<a class=anchor href=#k8s-qos%e7%ad%89%e7%ba%a7>#</a></h2><p>Kubernetes 中的 <strong>QoS (Quality of Service)</strong> 是一种机制，用于定义和管理 Pod 中容器的资源优先级与保障级别，帮助 Kubernetes 在资源紧张时进行资源调度与分配。Kubernetes 将 Pod 分为三种 QoS 等级，基于其 <code>resource requests</code> 和 <code>resource limits</code> 设置来决定。不同的 QoS 等级影响到 Pod 在资源竞争时的优先级和行为。</p><h3 id=kubernetes-qos-等级>Kubernetes QoS 等级
<a class=anchor href=#kubernetes-qos-%e7%ad%89%e7%ba%a7>#</a></h3><ol><li><strong>Guaranteed（保证）</strong></li><li><strong>Burstable（突发）</strong></li><li><strong>BestEffort（尽力而为）</strong></li></ol><p>每个 Pod 的 QoS 等级由其 <strong>CPU</strong> 和 <strong>内存</strong> 资源的 <code>requests</code> 和 <code>limits</code> 设置决定。下面是每种 QoS 等级的详细描述和规则：</p><hr><h3 id=1-guaranteed保证>1. <strong>Guaranteed（保证）</strong>
<a class=anchor href=#1-guaranteed%e4%bf%9d%e8%af%81>#</a></h3><p><strong>Guaranteed</strong> QoS 等级的 Pod 会在资源争用时获得最高优先级和保障，适用于对资源有严格要求的应用。</p><ul><li><strong>条件</strong>：Pod 中的所有容器都必须同时设置 <code>requests</code> 和 <code>limits</code>，并且 <code>requests</code> 必须等于 <code>limits</code>。</li><li><strong>资源保障</strong>：Pod 会被保证拥有其请求的所有资源（包括 CPU 和内存）。当资源紧张时，Kubernetes 会优先保证 Guaranteed Pod 的资源。</li></ul><p><strong>示例</strong>：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Pod</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>guaranteed-pod</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>containers</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>app-container</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>image</span>: <span style=color:#ae81ff>my-app-image</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>resources</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>requests</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>memory</span>: <span style=color:#e6db74>&#34;500Mi&#34;</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>cpu</span>: <span style=color:#e6db74>&#34;500m&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>limits</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>memory</span>: <span style=color:#e6db74>&#34;500Mi&#34;</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>cpu</span>: <span style=color:#e6db74>&#34;500m&#34;</span>
</span></span></code></pre></div><p><strong>优点</strong>：</p><ul><li>对资源有严格的保障，Pod 中的每个容器都会根据设置的请求和限制获得所需资源。</li><li>在资源竞争时，Guaranteed Pod 优先级最高。</li></ul><p><strong>缺点</strong>：</p><ul><li>需要显式地设置 <code>requests</code> 和 <code>limits</code>，并且两者的值要相等，这对于灵活的资源分配可能有些限制。</li></ul><hr><h3 id=2-burstable突发>2. <strong>Burstable（突发）</strong>
<a class=anchor href=#2-burstable%e7%aa%81%e5%8f%91>#</a></h3><p><strong>Burstable</strong> QoS 等级的 Pod 表示它们对资源有一定要求，但在资源充足时可以突发使用更多的资源。适用于负载波动较大的应用。</p><ul><li><strong>条件</strong>：Pod 中的至少一个容器设置了 <code>requests</code> 和 <code>limits</code>，但是 <strong>requests != limits</strong>，即容器的请求资源小于其限制资源。</li><li><strong>资源保障</strong>：Pod 中的容器会保证最小的资源请求（<code>requests</code>），但可以在节点资源充足时使用更多的资源，最多不超过 <code>limits</code> 设置的资源。</li></ul><p><strong>示例</strong>：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Pod</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>burstable-pod</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>containers</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>app-container</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>image</span>: <span style=color:#ae81ff>my-app-image</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>resources</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>requests</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>memory</span>: <span style=color:#e6db74>&#34;200Mi&#34;</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>cpu</span>: <span style=color:#e6db74>&#34;200m&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>limits</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>memory</span>: <span style=color:#e6db74>&#34;500Mi&#34;</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>cpu</span>: <span style=color:#e6db74>&#34;500m&#34;</span>
</span></span></code></pre></div><p><strong>优点</strong>：</p><ul><li>对资源有一定保障，但可以灵活调整以应对负载波动。</li><li>可以利用节点空闲的计算资源，适合非关键任务的应用。</li></ul><p><strong>缺点</strong>：</p><ul><li>如果节点资源紧张，Pod 的容器可能会被限制在 <code>requests</code> 范围内，可能无法突发使用更多的资源。</li></ul><hr><h3 id=3-besteffort尽力而为>3. <strong>BestEffort（尽力而为）</strong>
<a class=anchor href=#3-besteffort%e5%b0%bd%e5%8a%9b%e8%80%8c%e4%b8%ba>#</a></h3><p><strong>BestEffort</strong> QoS 等级的 Pod 没有设置 <code>requests</code> 和 <code>limits</code>，也就是说它不请求任何资源，也不限制资源。适用于对资源要求不高且可以接受资源不稳定的应用。</p><ul><li><strong>条件</strong>：Pod 中的所有容器都没有设置 <code>requests</code> 和 <code>limits</code>，或者这两个值都设置为 0。</li><li><strong>资源保障</strong>：Pod 不会得到任何资源保障。在资源紧张时，BestEffort Pod 最先被驱逐或限制资源。</li></ul><p><strong>示例</strong>：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Pod</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>besteffort-pod</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>containers</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>app-container</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>image</span>: <span style=color:#ae81ff>my-app-image</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>resources</span>: {}
</span></span></code></pre></div><p><strong>优点</strong>：</p><ul><li>不需要明确设置资源请求和限制，灵活性高。</li><li>适用于负载较轻或不太重要的应用。</li></ul><p><strong>缺点</strong>：</p><ul><li>在资源紧张时，BestEffort Pod 会被最先驱逐或者限制，资源保障最差。</li></ul><hr><h3 id=qos-影响>QoS 影响
<a class=anchor href=#qos-%e5%bd%b1%e5%93%8d>#</a></h3><h4 id=1-pod-驱逐eviction>1. <strong>Pod 驱逐（Eviction）</strong>
<a class=anchor href=#1-pod-%e9%a9%b1%e9%80%90eviction>#</a></h4><p>当节点资源不足时，Kubernetes 会驱逐资源占用较高或不重要的 Pod。Pod 的 QoS 等级决定了它被驱逐的优先级：</p><ul><li><strong>Guaranteed</strong> Pod 最不可能被驱逐。</li><li><strong>Burstable</strong> Pod 会在资源紧张时被优先考虑驱逐。</li><li><strong>BestEffort</strong> Pod 最容易被驱逐。</li></ul><h4 id=2-pod-优先级>2. <strong>Pod 优先级</strong>
<a class=anchor href=#2-pod-%e4%bc%98%e5%85%88%e7%ba%a7>#</a></h4><p>Kubernetes 使用 QoS 等级来管理不同 Pod 之间的优先级。在资源竞争时，QoS 等级较高的 Pod 会获得更多的资源。</p><ul><li><strong>Guaranteed</strong> Pod 会优先获得 CPU 和内存资源。</li><li><strong>Burstable</strong> Pod 会在资源充足时获得更多的资源，但当资源不足时，它们可能会受到限制。</li><li><strong>BestEffort</strong> Pod 只会在节点有多余资源时运行，并且在资源紧张时会被优先驱逐。</li></ul><hr><h3 id=总结-33>总结
<a class=anchor href=#%e6%80%bb%e7%bb%93-33>#</a></h3><p>Kubernetes 中的 QoS 等级根据 Pod 中容器的资源请求和限制决定，主要有三种等级：</p><ol><li><strong>Guaranteed</strong>：容器的请求和限制相等，提供最高的资源保障。</li><li><strong>Burstable</strong>：容器的请求和限制不相等，提供灵活的资源使用能力。</li><li><strong>BestEffort</strong>：容器没有设置请求和限制，提供最低的资源保障，容易被驱逐。</li></ol><p>正确设置资源请求和限制（<code>requests</code> 和 <code>limits</code>）可以帮助 Kubernetes 管理和调度 Pod 的资源，并确保在资源不足时关键应用能够优先运行。</p><h2 id=k8s节点维护注意事项>k8s节点维护注意事项
<a class=anchor href=#k8s%e8%8a%82%e7%82%b9%e7%bb%b4%e6%8a%a4%e6%b3%a8%e6%84%8f%e4%ba%8b%e9%a1%b9>#</a></h2><p>在 Kubernetes 中，<strong>节点维护</strong>是指对集群中的一个或多个节点进行操作，例如升级、修复、重新配置等。进行节点维护时，需要注意确保集群的高可用性、避免影响正在运行的应用程序，并遵循最佳实践来确保维护过程顺利完成。</p><h3 id=1-确保高可用性>1. <strong>确保高可用性</strong>
<a class=anchor href=#1-%e7%a1%ae%e4%bf%9d%e9%ab%98%e5%8f%af%e7%94%a8%e6%80%a7>#</a></h3><p>在进行节点维护时，必须保证集群的高可用性，避免单点故障。一般来说，建议集群至少有 <strong>3 个 Master 节点</strong> 和 <strong>3 个 Worker 节点</strong>。这样，在节点维护期间，其他节点可以继续提供服务。</p><h3 id=2-检查节点状态>2. <strong>检查节点状态</strong>
<a class=anchor href=#2-%e6%a3%80%e6%9f%a5%e8%8a%82%e7%82%b9%e7%8a%b6%e6%80%81>#</a></h3><p>在进行任何操作之前，首先要检查节点的状态。你可以使用以下命令查看节点的健康状况：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get nodes
</span></span></code></pre></div><p>确保节点处于 <strong>Ready</strong> 状态。若节点有问题，要先解决问题再进行维护。</p><h3 id=3-将节点从调度池中移除>3. <strong>将节点从调度池中移除</strong>
<a class=anchor href=#3-%e5%b0%86%e8%8a%82%e7%82%b9%e4%bb%8e%e8%b0%83%e5%ba%a6%e6%b1%a0%e4%b8%ad%e7%a7%bb%e9%99%a4>#</a></h3><p>在维护节点之前，建议将其从调度池中移除，防止调度新的 Pod 到该节点。可以通过以下命令将节点标记为不可调度（<code>NoSchedule</code>）：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl cordon &lt;node-name&gt;
</span></span></code></pre></div><p>这会防止 Kubernetes 向该节点调度新 Pod，但已经在该节点上的 Pod 会继续运行。</p><h3 id=4-迁移-pod-到其他节点>4. <strong>迁移 Pod 到其他节点</strong>
<a class=anchor href=#4-%e8%bf%81%e7%a7%bb-pod-%e5%88%b0%e5%85%b6%e4%bb%96%e8%8a%82%e7%82%b9>#</a></h3><p>为了确保不会影响到正在运行的应用程序，您应该将该节点上的 Pod 移到其他健康的节点上。可以使用以下命令将该节点上的 Pod 驱逐（Evict）：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl drain &lt;node-name&gt; --ignore-daemonsets --delete-local-data
</span></span></code></pre></div><ul><li><code>--ignore-daemonsets</code>：忽略 DaemonSet 控制的 Pod，因为它们需要在所有节点上运行。</li><li><code>--delete-local-data</code>：如果节点上有本地存储的数据，选择删除这些数据。</li></ul><p>这会导致集群中的 Pod 重新调度到其他节点，确保业务不中断。</p><h3 id=5-执行节点维护操作>5. <strong>执行节点维护操作</strong>
<a class=anchor href=#5-%e6%89%a7%e8%a1%8c%e8%8a%82%e7%82%b9%e7%bb%b4%e6%8a%a4%e6%93%8d%e4%bd%9c>#</a></h3><p>在节点从调度池中移除并且所有 Pod 被迁移之后，可以开始执行维护操作。这可能包括：</p><ul><li>节点的操作系统更新或补丁。</li><li>Kubernetes 组件（如 kubelet、docker）更新。</li><li>硬件修复或更换。</li><li>网络配置更改等。</li></ul><h3 id=6-重新加入节点>6. <strong>重新加入节点</strong>
<a class=anchor href=#6-%e9%87%8d%e6%96%b0%e5%8a%a0%e5%85%a5%e8%8a%82%e7%82%b9>#</a></h3><p>维护完成后，节点可以重新加入集群。首先，您需要将节点标记为可调度（<code>Schedule</code>）：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl uncordon &lt;node-name&gt;
</span></span></code></pre></div><p>这会将节点重新加入调度池，允许新的 Pod 被调度到该节点。</p><h3 id=7-验证节点恢复>7. <strong>验证节点恢复</strong>
<a class=anchor href=#7-%e9%aa%8c%e8%af%81%e8%8a%82%e7%82%b9%e6%81%a2%e5%a4%8d>#</a></h3><p>确保节点恢复并正常运行。检查节点状态：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get nodes
</span></span></code></pre></div><p>确保节点处于 <strong>Ready</strong> 状态。检查所有 Pod 是否能够在节点上正常运行：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get pods --all-namespaces -o wide
</span></span></code></pre></div><p>确保该节点上的 Pod 已经恢复，且没有出现问题。</p><h3 id=8-检查-pod-的健康状态>8. <strong>检查 Pod 的健康状态</strong>
<a class=anchor href=#8-%e6%a3%80%e6%9f%a5-pod-%e7%9a%84%e5%81%a5%e5%ba%b7%e7%8a%b6%e6%80%81>#</a></h3><p>维护期间迁移的 Pod 可能需要一段时间来恢复。使用以下命令检查 Pod 的状态，确保它们在新的节点上正确运行：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get pods --all-namespaces
</span></span></code></pre></div><p>对于出现问题的 Pod，可以使用以下命令进一步排查：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl describe pod &lt;pod-name&gt;
</span></span><span style=display:flex><span>kubectl logs &lt;pod-name&gt;
</span></span></code></pre></div><h3 id=9-监控与日志>9. <strong>监控与日志</strong>
<a class=anchor href=#9-%e7%9b%91%e6%8e%a7%e4%b8%8e%e6%97%a5%e5%bf%97>#</a></h3><p>在节点维护过程中，建议密切关注监控系统（如 Prometheus、Grafana 等）以及日志（如 ELK、Fluentd）来查看是否有异常情况。特别是要关注以下几个方面：</p><ul><li>CPU 和内存的使用情况。</li><li>节点负载和容器健康状态。</li><li>是否有新的警告或错误信息。</li></ul><h3 id=10-通知相关人员>10. <strong>通知相关人员</strong>
<a class=anchor href=#10-%e9%80%9a%e7%9f%a5%e7%9b%b8%e5%85%b3%e4%ba%ba%e5%91%98>#</a></h3><p>进行节点维护时，应通知集群的相关人员（如开发人员、运维人员），告知正在进行维护操作，避免不必要的干扰。此外，也可以提前设置维护窗口时间，以减少对生产环境的影响。</p><h3 id=11-升级节点版本>11. <strong>升级节点版本</strong>
<a class=anchor href=#11-%e5%8d%87%e7%ba%a7%e8%8a%82%e7%82%b9%e7%89%88%e6%9c%ac>#</a></h3><p>如果在维护过程中涉及节点升级（例如升级操作系统或 Kubernetes 版本），确保遵循 Kubernetes 官方的升级文档。可以使用 <code>kubeadm</code> 工具进行节点升级：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sudo kubeadm upgrade plan
</span></span><span style=display:flex><span>sudo kubeadm upgrade apply &lt;version&gt;
</span></span></code></pre></div><h3 id=12-更新节点的-kubelet-配置>12. <strong>更新节点的 kubelet 配置</strong>
<a class=anchor href=#12-%e6%9b%b4%e6%96%b0%e8%8a%82%e7%82%b9%e7%9a%84-kubelet-%e9%85%8d%e7%bd%ae>#</a></h3><p>如果有必要更新节点的 <code>kubelet</code> 配置（例如修改证书、网络配置等），可以编辑 <code>/etc/kubernetes/kubelet.conf</code> 配置文件，并重新启动 <code>kubelet</code>：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sudo systemctl restart kubelet
</span></span></code></pre></div><h3 id=13-定期执行节点健康检查>13. <strong>定期执行节点健康检查</strong>
<a class=anchor href=#13-%e5%ae%9a%e6%9c%9f%e6%89%a7%e8%a1%8c%e8%8a%82%e7%82%b9%e5%81%a5%e5%ba%b7%e6%a3%80%e6%9f%a5>#</a></h3><p>为了避免节点长期处于不健康的状态，建议定期执行节点健康检查，并进行必要的维护。例如，可以定期检查磁盘空间、内存使用情况、网络延迟等指标。</p><hr><h3 id=总结-34>总结
<a class=anchor href=#%e6%80%bb%e7%bb%93-34>#</a></h3><p>节点维护时，需要谨慎处理以保证集群的稳定性和业务的持续运行。维护过程的关键步骤包括：</p><ol><li>将节点标记为不可调度（<code>cordon</code>），并迁移 Pod。</li><li>执行节点维护操作，如升级或硬件修复。</li><li>完成维护后，将节点标记为可调度（<code>uncordon</code>），并检查 Pod 的恢复情况。</li><li>确保节点健康并重新加入集群。</li><li>监控系统和日志，确保集群稳定运行。</li></ol><p>通过以上步骤，可以确保 Kubernetes 集群在节点维护期间不会受到过多影响，并且维护操作顺利完成。</p><h2 id=headless-service和clusterip区别>Headless Service和ClusterIP区别☆
<a class=anchor href=#headless-service%e5%92%8cclusterip%e5%8c%ba%e5%88%ab>#</a></h2><p>在 Kubernetes 中，<strong>Headless Service</strong> 和 <strong>ClusterIP Service</strong> 都是 Service 类型的两种不同形式，用于在集群内暴露和访问应用。但它们的实现方式和适用场景有所不同。</p><h3 id=1-clusterip-service>1. <strong>ClusterIP Service</strong>
<a class=anchor href=#1-clusterip-service>#</a></h3><p><strong>ClusterIP</strong> 是 Kubernetes 默认的 Service 类型，用于在集群内部暴露服务。它为集群内的 Pod 提供一个虚拟 IP 地址，通过这个虚拟 IP，客户端可以访问服务。</p><ul><li><strong>特性</strong>：<ul><li>为 Service 分配一个虚拟 IP 地址（ClusterIP）。</li><li>客户端通过该 IP 来访问服务。</li><li>集群内的任何 Pod 都可以通过这个虚拟 IP 来访问服务，Kubernetes 会自动进行负载均衡，选择后端 Pod。</li><li>不会暴露服务到集群外部。</li></ul></li><li><strong>典型用法</strong>：<ul><li>用于集群内部的服务通信。</li><li>例如，应用的数据库服务或微服务之间的通信。</li></ul></li><li><strong>示例</strong>：</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Service</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-service</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>selector</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>app</span>: <span style=color:#ae81ff>my-app</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>ports</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>protocol</span>: <span style=color:#ae81ff>TCP</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>port</span>: <span style=color:#ae81ff>80</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>targetPort</span>: <span style=color:#ae81ff>8080</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>type</span>: <span style=color:#ae81ff>ClusterIP</span>
</span></span></code></pre></div><hr><h3 id=2-headless-service>2. <strong>Headless Service</strong>
<a class=anchor href=#2-headless-service>#</a></h3><p><strong>Headless Service</strong> 是一种特殊的 Service 类型，它不为服务分配虚拟 IP（ClusterIP），而是直接暴露每个 Pod 的 IP 地址。它通常用于需要直接访问每个 Pod 的场景，比如 StatefulSet 和需要 Pod 级别 DNS 解析的应用。</p><ul><li><strong>特性</strong>：<ul><li>不分配虚拟 IP（<code>ClusterIP: None</code>），客户端直接通过 DNS 查询获得每个 Pod 的 IP 地址。</li><li>每个后端 Pod 都有自己的 DNS 记录，通常是 <code>pod-name.service-name.namespace.svc.cluster.local</code> 形式。</li><li>适合 StatefulSet 和需要 Pod 直接访问的应用。</li><li>不进行负载均衡，客户端需要自己处理负载均衡（比如使用 DNS 轮询）。</li></ul></li><li><strong>典型用法</strong>：<ul><li>用于 StatefulSet、分布式数据库、需要直接访问各个 Pod 的应用。</li><li>例如，分布式数据库（如 Cassandra、Elasticsearch）中需要直接访问每个节点。</li></ul></li><li><strong>示例</strong>：</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Service</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-headless-service</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>clusterIP</span>: <span style=color:#ae81ff>None</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>selector</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>app</span>: <span style=color:#ae81ff>my-app</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>ports</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>protocol</span>: <span style=color:#ae81ff>TCP</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>port</span>: <span style=color:#ae81ff>80</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>targetPort</span>: <span style=color:#ae81ff>8080</span>
</span></span></code></pre></div><hr><h3 id=clusterip-与-headless-service-的区别><strong>ClusterIP 与 Headless Service 的区别</strong>
<a class=anchor href=#clusterip-%e4%b8%8e-headless-service-%e7%9a%84%e5%8c%ba%e5%88%ab>#</a></h3><table><thead><tr><th>特性</th><th><strong>ClusterIP Service</strong></th><th><strong>Headless Service</strong></th></tr></thead><tbody><tr><td><strong>虚拟 IP 地址</strong></td><td>会为 Service 分配一个虚拟 IP 地址</td><td>不分配虚拟 IP 地址 (<code>clusterIP: None</code>)</td></tr><tr><td><strong>负载均衡</strong></td><td>通过虚拟 IP 地址进行负载均衡</td><td>不进行负载均衡，直接访问每个 Pod</td></tr><tr><td><strong>访问方式</strong></td><td>通过虚拟 IP 访问，Kubernetes 负责负载均衡</td><td>通过 DNS 查询访问，每个 Pod 都有自己的 DNS 记录</td></tr><tr><td><strong>适用场景</strong></td><td>适用于集群内部服务，需要负载均衡的场景</td><td>适用于需要直接访问每个 Pod 的场景，如 StatefulSet</td></tr><tr><td><strong>DNS 记录</strong></td><td>DNS 解析返回 Service 的虚拟 IP</td><td>每个 Pod 都有自己的 DNS 记录</td></tr></tbody></table><h3 id=总结-35>总结
<a class=anchor href=#%e6%80%bb%e7%bb%93-35>#</a></h3><ul><li><strong>ClusterIP Service</strong> 是最常见的 Service 类型，适用于需要负载均衡和虚拟 IP 的场景。</li><li><strong>Headless Service</strong> 则不提供虚拟 IP，而是暴露每个 Pod 的 IP 地址，适合需要直接访问每个 Pod 的应用，通常用于 StatefulSet 和需要基于 DNS 解析 Pod 的场景。</li></ul><h2 id=不进行负载均衡直接访问每个-pod-比如数据库有三个实例pod用headless的模式那么应用连接这个数据库的方式是啥三个不同的svc>不进行负载均衡，直接访问每个 Pod？ 比如数据库有三个实例pod用headless的模式，那么应用连接这个数据库的方式是啥？三个不同的svc？
<a class=anchor href=#%e4%b8%8d%e8%bf%9b%e8%a1%8c%e8%b4%9f%e8%bd%bd%e5%9d%87%e8%a1%a1%e7%9b%b4%e6%8e%a5%e8%ae%bf%e9%97%ae%e6%af%8f%e4%b8%aa-pod-%e6%af%94%e5%a6%82%e6%95%b0%e6%8d%ae%e5%ba%93%e6%9c%89%e4%b8%89%e4%b8%aa%e5%ae%9e%e4%be%8bpod%e7%94%a8headless%e7%9a%84%e6%a8%a1%e5%bc%8f%e9%82%a3%e4%b9%88%e5%ba%94%e7%94%a8%e8%bf%9e%e6%8e%a5%e8%bf%99%e4%b8%aa%e6%95%b0%e6%8d%ae%e5%ba%93%e7%9a%84%e6%96%b9%e5%bc%8f%e6%98%af%e5%95%a5%e4%b8%89%e4%b8%aa%e4%b8%8d%e5%90%8c%e7%9a%84svc>#</a></h2><p>是的，<strong>Headless Service</strong> 在 Kubernetes 中的一个关键特性就是它不会进行负载均衡，而是将请求直接路由到每个 Pod。对于数据库实例的场景，比如一个数据库有三个实例（Pod），使用 Headless Service 后，应用程序的连接方式会有所不同。通常可以通过以下两种方式来进行连接：</p><h3 id=1-通过-dns-直接访问每个-pod>1. <strong>通过 DNS 直接访问每个 Pod</strong>
<a class=anchor href=#1-%e9%80%9a%e8%bf%87-dns-%e7%9b%b4%e6%8e%a5%e8%ae%bf%e9%97%ae%e6%af%8f%e4%b8%aa-pod>#</a></h3><p>在 <strong>Headless Service</strong> 中，每个 Pod 会有自己的 DNS 记录，而不是通过一个共享的虚拟 IP 访问整个服务。Kubernetes 会为每个 Pod 生成一个 DNS 记录，通常是 <code>&lt;pod-name>.&lt;service-name>.&lt;namespace>.svc.cluster.local</code>。</p><p>假设你有一个名为 <code>db</code> 的 <strong>Headless Service</strong>，并且该 Service 后面有三个 Pod（数据库实例）：</p><ul><li><code>db-0</code></li><li><code>db-1</code></li><li><code>db-2</code></li></ul><p>Kubernetes 会为每个 Pod 创建以下 DNS 记录：</p><ul><li><code>db-0.db.default.svc.cluster.local</code></li><li><code>db-1.db.default.svc.cluster.local</code></li><li><code>db-2.db.default.svc.cluster.local</code></li></ul><p>应用程序可以通过 DNS 查询来直接连接到这三个 Pod。比如，如果应用程序需要连接数据库实例 <code>db-1</code>，它可以使用 <code>db-1.db.default.svc.cluster.local</code>。</p><p><strong>示例</strong>：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Service</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>db</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>clusterIP</span>: <span style=color:#ae81ff>None </span> <span style=color:#75715e># 设置为 None 表示 Headless Service</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>selector</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>app</span>: <span style=color:#ae81ff>database</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>ports</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>port</span>: <span style=color:#ae81ff>5432</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>targetPort</span>: <span style=color:#ae81ff>5432</span>
</span></span></code></pre></div><p><strong>DNS 解析</strong>：</p><ul><li><code>db-0.db.default.svc.cluster.local</code></li><li><code>db-1.db.default.svc.cluster.local</code></li><li><code>db-2.db.default.svc.cluster.local</code></li></ul><p>应用程序可以通过这些 DNS 名称直接连接数据库实例。</p><h3 id=2-通过多个-service-访问每个实例>2. <strong>通过多个 Service 访问每个实例</strong>
<a class=anchor href=#2-%e9%80%9a%e8%bf%87%e5%a4%9a%e4%b8%aa-service-%e8%ae%bf%e9%97%ae%e6%af%8f%e4%b8%aa%e5%ae%9e%e4%be%8b>#</a></h3><p>另一种方式是为每个 Pod 创建一个单独的 Service。虽然这不常见，但在某些情况下，可能需要为每个实例单独暴露一个 Service。每个 Pod 都有自己的 Headless Service 实例，并且每个 Service 都会绑定一个特定的 Pod。</p><p>如果你有三个数据库实例 <code>db-0</code>、<code>db-1</code> 和 <code>db-2</code>，可以创建三个单独的 Service 来暴露每个数据库实例：</p><ul><li><code>db-0-service</code></li><li><code>db-1-service</code></li><li><code>db-2-service</code></li></ul><p>每个 Service 仅暴露一个 Pod，应用程序可以通过相应的 Service 名称来连接到特定的 Pod。</p><p><strong>示例</strong>：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Service</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>db-0-service</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>clusterIP</span>: <span style=color:#ae81ff>None</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>selector</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>pod-name</span>: <span style=color:#ae81ff>db-0</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>ports</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>port</span>: <span style=color:#ae81ff>5432</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>targetPort</span>: <span style=color:#ae81ff>5432</span>
</span></span></code></pre></div><p>然后可以创建类似的 <code>db-1-service</code> 和 <code>db-2-service</code>，这样应用程序就可以通过 <code>db-0-service</code>、<code>db-1-service</code> 和 <code>db-2-service</code> 来访问不同的数据库实例。</p><h3 id=适用场景-1>适用场景
<a class=anchor href=#%e9%80%82%e7%94%a8%e5%9c%ba%e6%99%af-1>#</a></h3><ul><li><strong>数据库实例或有状态服务</strong>：例如，使用 StatefulSet 部署数据库时，使用 Headless Service 更合适，因为每个 Pod（数据库实例）通常都有一个固定的名称，而每个实例之间的状态可能有所不同，需要应用程序进行特定的连接。</li><li><strong>有状态的分布式应用</strong>：一些分布式系统（如 Cassandra、Elasticsearch 等）可能需要应用程序通过 Pod 的 DNS 名称进行访问，来确保每个节点能被精确识别和访问。</li></ul><h3 id=总结-36>总结
<a class=anchor href=#%e6%80%bb%e7%bb%93-36>#</a></h3><ul><li>在 <strong>Headless Service</strong> 中，应用程序通常通过 Pod 的 <strong>DNS 名称</strong> 来直接访问数据库实例，而不是通过一个统一的 IP 地址。</li><li>每个 Pod 都会有一个 DNS 记录，应用程序可以根据需要选择连接某个特定的 Pod。</li><li>另一种做法是为每个 Pod 创建独立的 Service，但这通常不常见，因为 DNS 的方式更灵活。</li></ul><p>因此，<strong>Headless Service</strong> 模式并不是通过多个不同的 Service 来访问不同的数据库实例，而是通过单一的 Service 暴露多个 Pod 的 DNS 名称，让应用程序可以直接访问特定的 Pod。</p><h2 id=db-0dbdefaultsvcclusterlocal-db-1dbdefaultsvcclusterlocal-db-2dbdefaultsvcclusterlocal-这不就是3个不同svc吗>db-0.db.default.svc.cluster.local db-1.db.default.svc.cluster.local db-2.db.default.svc.cluster.local 这不就是3个不同svc吗
<a class=anchor href=#db-0dbdefaultsvcclusterlocal-db-1dbdefaultsvcclusterlocal-db-2dbdefaultsvcclusterlocal-%e8%bf%99%e4%b8%8d%e5%b0%b1%e6%98%af3%e4%b8%aa%e4%b8%8d%e5%90%8csvc%e5%90%97>#</a></h2><p>你提出的疑问是正确的，但在 <strong>Headless Service</strong> 中，实际上 <strong>并没有三个不同的 Service</strong>，而是 <strong>一个 Service</strong>，但它会为每个 Pod 分配一个独立的 DNS 记录。</p><p>让我们再详细解释一下：</p><h3 id=headless-service-和-pod-的-dns-解析><strong>Headless Service 和 Pod 的 DNS 解析</strong>
<a class=anchor href=#headless-service-%e5%92%8c-pod-%e7%9a%84-dns-%e8%a7%a3%e6%9e%90>#</a></h3><p>在 Kubernetes 中，当你创建一个 <strong>Headless Service</strong> 时（<code>clusterIP: None</code>），Kubernetes 不会为该 Service 分配虚拟 IP，而是直接将每个与该 Service 相关联的 Pod 暴露出来，并为每个 Pod 创建一个 DNS 记录。具体来说，Kubernetes 会为每个 Pod 创建一个 DNS 记录，它们遵循以下格式：</p><pre tabindex=0><code>&lt;pod-name&gt;.&lt;service-name&gt;.&lt;namespace&gt;.svc.cluster.local
</code></pre><p>假设你有一个名为 <code>db</code> 的 <strong>Headless Service</strong>，它选择了名为 <code>db-0</code>、<code>db-1</code> 和 <code>db-2</code> 的三个 Pod。Kubernetes 会为这三个 Pod 创建以下 DNS 记录：</p><ul><li><code>db-0.db.default.svc.cluster.local</code></li><li><code>db-1.db.default.svc.cluster.local</code></li><li><code>db-2.db.default.svc.cluster.local</code></li></ul><p>虽然这些看起来像是多个服务的 DNS 名称，但它们实际上都指向同一个 <strong>Headless Service</strong>，而不是三个独立的 Service。<strong>Headless Service</strong> 只是为每个 Pod 提供了一个独立的 DNS 名称，让客户端可以直接通过 DNS 查询访问到每个 Pod。</p><h3 id=为什么这些看起来像是多个服务><strong>为什么这些看起来像是多个服务</strong>
<a class=anchor href=#%e4%b8%ba%e4%bb%80%e4%b9%88%e8%bf%99%e4%ba%9b%e7%9c%8b%e8%b5%b7%e6%9d%a5%e5%83%8f%e6%98%af%e5%a4%9a%e4%b8%aa%e6%9c%8d%e5%8a%a1>#</a></h3><p>这些 DNS 名称看起来像是为每个 Pod 创建了一个独立的 Service，但本质上它们都是由一个 <strong>Headless Service</strong> 创建的。这是因为 Kubernetes 会使用 DNS 动态解析并将流量发送到正确的 Pod。例如：</p><ul><li><code>db-0.db.default.svc.cluster.local</code> 解析到 <code>db-0</code> Pod 的 IP 地址。</li><li><code>db-1.db.default.svc.cluster.local</code> 解析到 <code>db-1</code> Pod 的 IP 地址。</li><li><code>db-2.db.default.svc.cluster.local</code> 解析到 <code>db-2</code> Pod 的 IP 地址。</li></ul><h3 id=总结-37><strong>总结</strong>
<a class=anchor href=#%e6%80%bb%e7%bb%93-37>#</a></h3><ul><li><strong>Headless Service</strong> 并不会为每个 Pod 创建独立的 Service，它只创建一个 <strong>Service</strong>，但为每个 Pod 创建一个独立的 DNS 记录。</li><li>每个 Pod 都会有一个唯一的 DNS 名称，客户端可以直接通过这些 DNS 名称来访问特定的 Pod。</li><li>这些 DNS 记录的存在使得应用程序可以有选择性地访问某个 Pod，而不是通过负载均衡访问 Service 的虚拟 IP 地址。</li></ul><p>希望这样能帮助你更好地理解 <strong>Headless Service</strong> 的工作原理。</p><h2 id=在-headless-service-中实际上-并没有三个不同的-service而是-一个-service但它会为每个-pod-分配一个独立的-dns-记录那这个一个svc是啥>在 Headless Service 中，实际上 并没有三个不同的 Service，而是 一个 Service，但它会为每个 Pod 分配一个独立的 DNS 记录，那这个一个svc是啥
<a class=anchor href=#%e5%9c%a8-headless-service-%e4%b8%ad%e5%ae%9e%e9%99%85%e4%b8%8a-%e5%b9%b6%e6%b2%a1%e6%9c%89%e4%b8%89%e4%b8%aa%e4%b8%8d%e5%90%8c%e7%9a%84-service%e8%80%8c%e6%98%af-%e4%b8%80%e4%b8%aa-service%e4%bd%86%e5%ae%83%e4%bc%9a%e4%b8%ba%e6%af%8f%e4%b8%aa-pod-%e5%88%86%e9%85%8d%e4%b8%80%e4%b8%aa%e7%8b%ac%e7%ab%8b%e7%9a%84-dns-%e8%ae%b0%e5%bd%95%e9%82%a3%e8%bf%99%e4%b8%aa%e4%b8%80%e4%b8%aasvc%e6%98%af%e5%95%a5>#</a></h2><p>在 Kubernetes 中，<strong>Headless Service</strong> 是一个特殊类型的 Service，它的核心特点是 <strong>不会分配 ClusterIP</strong>，而是通过 DNS 来暴露集群中的多个 Pod。虽然每个 Pod 会有一个独立的 DNS 名称，但是它们都是由同一个 <strong>Headless Service</strong> 来管理和暴露的。</p><h3 id=具体来说>具体来说：
<a class=anchor href=#%e5%85%b7%e4%bd%93%e6%9d%a5%e8%af%b4>#</a></h3><ol><li><p>Service 的定义（Headless Service）</p><p>：</p><ul><li>你创建的 Service 在 <strong>Headless Service</strong> 模式下是一个单独的 Service，它的 <code>clusterIP: None</code>。</li><li>该 Service 会选择一组 Pod（通常是通过标签选择器选择），但它并不创建一个统一的虚拟 IP（ClusterIP）。相反，它为每个 Pod 提供一个 DNS 名称。</li></ul></li><li><p>DNS 解析</p><p>：</p><ul><li><p>在 Headless Service 中，Kubernetes 会为每个 Pod 创建一个 DNS 记录，格式如下：</p><pre tabindex=0><code>&lt;pod-name&gt;.&lt;service-name&gt;.&lt;namespace&gt;.svc.cluster.local
</code></pre></li><li><p>这些 DNS 记录并不是独立的 Service，而是通过 <strong>同一个 Service</strong> 来进行管理的。</p></li></ul></li></ol><h3 id=举个例子>举个例子：
<a class=anchor href=#%e4%b8%be%e4%b8%aa%e4%be%8b%e5%ad%90>#</a></h3><p>假设你有一个数据库应用，并使用了 StatefulSet 来管理数据库实例，每个数据库实例是一个 Pod。你为这些 Pod 创建了一个 <strong>Headless Service</strong>，比如叫 <code>db</code>。</p><p><strong>Service 定义</strong>：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Service</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>db</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>clusterIP</span>: <span style=color:#ae81ff>None </span> <span style=color:#75715e># Headless Service</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>selector</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>app</span>: <span style=color:#ae81ff>database</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>ports</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>port</span>: <span style=color:#ae81ff>5432</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>targetPort</span>: <span style=color:#ae81ff>5432</span>
</span></span></code></pre></div><ul><li><code>db</code> 是 Service 名称。</li><li>选择器 <code>app: database</code> 用于匹配一组 Pod（例如 <code>db-0</code>、<code>db-1</code> 和 <code>db-2</code>）。</li><li>由于这是一个 <strong>Headless Service</strong>，它没有 <code>clusterIP</code>，而是将每个 Pod 的 IP 地址通过 DNS 暴露出来。</li></ul><p><strong>Pod 的 DNS 记录</strong>：</p><ul><li><code>db-0.db.default.svc.cluster.local</code></li><li><code>db-1.db.default.svc.cluster.local</code></li><li><code>db-2.db.default.svc.cluster.local</code></li></ul><h3 id=所以这个一个-service-是>所以，<strong>这个一个 Service</strong> 是：
<a class=anchor href=#%e6%89%80%e4%bb%a5%e8%bf%99%e4%b8%aa%e4%b8%80%e4%b8%aa-service-%e6%98%af>#</a></h3><ul><li>它是一个 <strong>Headless Service</strong>，可以让你通过 DNS 直接访问集群中的每个 Pod。</li><li>每个 Pod 有一个 <strong>独立的 DNS 名称</strong>，但它们都是由同一个 Service 进行管理和暴露的。</li></ul><h3 id=结论-1>结论：
<a class=anchor href=#%e7%bb%93%e8%ae%ba-1>#</a></h3><ul><li>在 <strong>Headless Service</strong> 中，虽然每个 Pod 会有独立的 DNS 名称，但它们共享 <strong>同一个 Service</strong>，并通过该 Service 的 DNS 解析机制来直接访问每个 Pod。</li><li>这并不是创建多个独立的 Service，而是同一个 Service 为每个 Pod 提供了 DNS 名称，因此客户端可以选择直接访问某个特定的 Pod。</li></ul><h2 id=linux容器技术的基础原理>Linux容器技术的基础原理
<a class=anchor href=#linux%e5%ae%b9%e5%99%a8%e6%8a%80%e6%9c%af%e7%9a%84%e5%9f%ba%e7%a1%80%e5%8e%9f%e7%90%86>#</a></h2><p>Linux 容器技术的基础原理主要依赖于 Linux 内核提供的一些核心功能，这些功能允许在同一台物理机或虚拟机上运行多个相互隔离的应用或进程。容器能够共享宿主机的操作系统内核，但在用户空间中实现进程隔离，从而提高资源利用率和灵活性。</p><p>Linux 容器技术的关键原理包括：</p><h3 id=1-命名空间namespaces>1. <strong>命名空间（Namespaces）</strong>
<a class=anchor href=#1-%e5%91%bd%e5%90%8d%e7%a9%ba%e9%97%b4namespaces>#</a></h3><p>命名空间是 Linux 容器技术的核心之一。它为每个容器提供了一个独立的、隔离的环境，使得容器中的进程只能看到自己的资源，而无法看到宿主机或其他容器的资源。Linux 支持多种命名空间：</p><ul><li><strong>PID 命名空间（Process ID Namespace）</strong>：每个容器可以拥有自己的一套进程 ID，进程 ID 在容器内是唯一的，但在宿主机上可能会重复。它使得容器可以在隔离的环境中运行进程。</li><li><strong>NET 命名空间（Network Namespace）</strong>：为容器提供独立的网络栈，包括 IP 地址、路由表、端口等。每个容器有自己的网络接口、IP 地址、端口等资源。</li><li><strong>MNT 命名空间（Mount Namespace）</strong>：容器有自己的文件系统视图，可以挂载独立的文件系统或挂载点，而不影响宿主机或其他容器。</li><li><strong>UTS 命名空间（UTS Namespace）</strong>：容器可以有自己的主机名和域名，不受宿主机的影响。</li><li><strong>IPC 命名空间（IPC Namespace）</strong>：容器之间的进程间通信（IPC）是隔离的，例如共享内存、信号量等。</li><li><strong>USER 命名空间（User Namespace）</strong>：容器内部的用户和组 ID 是独立的，可以允许容器内的进程拥有 root 权限，但它们在宿主机上并不是 root 用户。</li><li><strong>CGROUPS 命名空间（Control Group Namespace）</strong>：负责将资源（如 CPU、内存、磁盘 IO 等）分配给容器。可以限制容器使用的系统资源，并进行监控。</li></ul><h3 id=2-控制组cgroups>2. <strong>控制组（cgroups）</strong>
<a class=anchor href=#2-%e6%8e%a7%e5%88%b6%e7%bb%84cgroups>#</a></h3><p>控制组（Cgroups，Control Groups）是 Linux 内核的另一个重要功能，用于控制和限制进程的资源使用情况。容器内的进程会被分配到一个或多个控制组中，以便限制它们使用的资源。</p><ul><li><strong>资源限制</strong>：Cgroups 可以限制容器使用的 CPU 时间、内存、磁盘 I/O 等。</li><li><strong>资源监控</strong>：可以监控容器中进程的资源使用情况，如 CPU 使用率、内存消耗等。</li><li><strong>资源分配</strong>：允许对多个容器进行资源隔离和公平分配，例如，限制容器使用的最大 CPU 核心数、最大内存大小等。</li></ul><p>通过 Cgroups，Linux 容器可以精确控制每个容器的资源配额，从而避免某个容器占用过多资源，影响宿主机或其他容器的运行。</p><h3 id=3-联合文件系统union-file-system>3. <strong>联合文件系统（Union File System）</strong>
<a class=anchor href=#3-%e8%81%94%e5%90%88%e6%96%87%e4%bb%b6%e7%b3%bb%e7%bb%9funion-file-system>#</a></h3><p>联合文件系统（UnionFS）是 Linux 容器技术的另一个关键组成部分。它使得多个文件系统可以挂载在同一个目录下，并且以“只读”和“可写”层次结构组合。</p><ul><li><p><strong>联合挂载</strong>：容器中的文件系统是由多个层叠加的文件系统组成，每个层次可以是只读的，也可以是可写的。容器的根文件系统通常是只读的，而写操作会被记录到容器的可写层中。</p></li><li><p><strong>镜像和容器</strong>：容器镜像实际上是一个包含多个层的文件系统，镜像中的每一层都是一个不可变的只读层，而容器是一个运行中的实例，包含一个可写的层。</p></li><li><p>常见的联合文件系统</p><p>：</p><ul><li><strong>OverlayFS</strong>：常用于现代容器的联合文件系统。它通过两个目录（一个是只读层，另一个是可写层）进行联合挂载。</li><li><strong>AUFS</strong>：另一个常用的联合文件系统，用于 Docker 旧版本。</li></ul></li></ul><p>联合文件系统能够提高存储效率，因为多个容器可以共享相同的只读镜像层，从而节省磁盘空间。</p><h3 id=4-容器镜像container-images>4. <strong>容器镜像（Container Images）</strong>
<a class=anchor href=#4-%e5%ae%b9%e5%99%a8%e9%95%9c%e5%83%8fcontainer-images>#</a></h3><p>容器镜像是容器化应用的标准化打包格式，包含了运行容器所需的操作系统文件、应用程序、依赖包以及运行时环境。容器镜像是不可变的，一旦创建，它就不能被修改。</p><ul><li><strong>镜像层次</strong>：容器镜像通常由多个层（Layer）组成，每一层都代表了对文件系统的一个修改，例如安装某个软件包或配置文件的变动。</li><li><strong>镜像存储和分发</strong>：容器镜像可以存储在本地或远程仓库（如 Docker Hub 或私有仓库）中，容器可以从这些仓库拉取镜像来创建实例。</li></ul><h3 id=5-容器的隔离性与安全性>5. <strong>容器的隔离性与安全性</strong>
<a class=anchor href=#5-%e5%ae%b9%e5%99%a8%e7%9a%84%e9%9a%94%e7%a6%bb%e6%80%a7%e4%b8%8e%e5%ae%89%e5%85%a8%e6%80%a7>#</a></h3><p>容器技术提供了资源隔离、进程隔离和文件系统隔离等多重隔离机制：</p><ul><li><strong>进程隔离</strong>：容器内的进程通过 PID 命名空间与宿主机和其他容器的进程隔离。</li><li><strong>网络隔离</strong>：容器通过 Network 命名空间拥有独立的网络栈，可以有独立的 IP 地址、端口等，不会与宿主机或其他容器共享网络资源。</li><li><strong>文件系统隔离</strong>：通过 MNT 命名空间和联合文件系统，容器有自己的文件系统视图，可以对文件进行读写操作，隔离了容器间的文件系统。</li><li><strong>用户隔离</strong>：通过 User 命名空间，容器内的用户和宿主机的用户是隔离的，容器内的 root 用户并不对应宿主机的 root 用户，从而提高安全性。</li></ul><h3 id=总结-38>总结
<a class=anchor href=#%e6%80%bb%e7%bb%93-38>#</a></h3><p>Linux 容器技术的基础原理是基于以下几个关键概念：</p><ul><li><strong>命名空间（Namespaces）</strong>：提供进程、网络、文件系统等方面的隔离。</li><li><strong>控制组（Cgroups）</strong>：控制容器的资源分配与限制。</li><li><strong>联合文件系统（UnionFS）</strong>：允许多个只读层和一个可写层组成容器的文件系统。</li><li><strong>容器镜像（Container Images）</strong>：应用及其依赖的打包格式，提供可移植性。</li><li><strong>安全性与隔离</strong>：通过隔离机制（如用户隔离、网络隔离等）确保容器的独立性与安全性。</li></ul><p>这些技术共同构成了现代 Linux 容器化的基础，能够让应用程序在隔离、可移植和高效的环境中运行。</p><h2 id=kubernetes-pod的常见调度方式>Kubernetes Pod的常见调度方式
<a class=anchor href=#kubernetes-pod%e7%9a%84%e5%b8%b8%e8%a7%81%e8%b0%83%e5%ba%a6%e6%96%b9%e5%bc%8f>#</a></h2><p>Kubernetes 中，Pod 的调度是由 <strong>kube-scheduler</strong> 控制的，调度的核心目的是根据一定的规则将 Pod 安排到合适的节点上执行。调度是一个非常重要的过程，因为它直接关系到容器的运行效率、资源利用率和稳定性。</p><h3 id=常见的-pod-调度方式>常见的 Pod 调度方式
<a class=anchor href=#%e5%b8%b8%e8%a7%81%e7%9a%84-pod-%e8%b0%83%e5%ba%a6%e6%96%b9%e5%bc%8f>#</a></h3><ol><li><p><strong>默认调度（Default Scheduling）</strong></p><ul><li><p>默认情况下，Kubernetes 会根据每个 Pod 的资源请求（如 CPU 和内存）、节点的可用资源以及调度策略来选择一个合适的节点进行调度。<code>kube-scheduler</code> 会选择一个符合要求的节点。</p></li><li><p>工作流程</p><p>：</p><ul><li><code>kube-scheduler</code> 获取待调度的 Pod。</li><li>它会检查各个节点的资源，确定节点是否满足 Pod 的资源需求。</li><li>调度策略还会考虑节点的标签、污点、容忍度等信息来做进一步筛选。</li><li>如果符合条件，Pod 会被调度到合适的节点上。</li></ul></li></ul></li><li><p><strong>节点亲和性（Node Affinity）</strong></p><ul><li><p><strong>Node Affinity</strong> 是对节点选择的进一步限制，它允许用户根据节点的标签来指定调度规则。通过设置 <strong>Pod 的 nodeAffinity</strong>，可以将 Pod 调度到具有特定标签的节点上。</p></li><li><p>支持的规则</p><p>：</p><ul><li><strong>requiredDuringSchedulingIgnoredDuringExecution</strong>：硬性要求 Pod 必须调度到符合条件的节点上。</li><li><strong>preferredDuringSchedulingIgnoredDuringExecution</strong>：优选规则，如果某个节点符合条件，则 Pod 更倾向于调度到该节点上，但如果没有匹配的节点，也不会拒绝调度。</li></ul></li><li><p><strong>使用场景</strong>：如果你有一些特定的硬件要求（比如 GPU 或特殊的存储设备），你可以使用节点亲和性来将 Pod 调度到特定类型的节点上。</p></li></ul></li><li><p><strong>Pod 亲和性（Pod Affinity）</strong></p><ul><li><p><strong>Pod Affinity</strong> 和 <strong>Pod Anti-Affinity</strong> 是通过 Pod 的标签来控制 Pod 与其他 Pod 的关系。</p></li><li><p><strong>Pod Affinity</strong>：指定 Pod 应该尽可能地调度到与其他特定 Pod 一起运行的节点上。</p></li><li><p><strong>Pod Anti-Affinity</strong>：指定 Pod 不应该与其他特定 Pod 一起运行。</p></li><li><p>使用场景</p><p>：</p><ul><li>你可能希望某些服务（如数据库集群）在同一节点上运行，以减少网络延迟。</li><li>相反，可能希望将某些服务（如不同的微服务）分散到不同的节点上，以提高可用性。</li></ul></li></ul></li><li><p><strong>污点和容忍度（Taints and Tolerations）</strong></p><ul><li><strong>污点（Taints）</strong> 是节点的属性，用于标记一个节点不可用于特定的 Pod。污点会使得 Pod 无法调度到该节点，除非 Pod 有相应的容忍度（Toleration）。</li><li><strong>容忍度（Tolerations）</strong> 是 Pod 的属性，用于标记 Pod 可以接受哪些污点。</li><li><strong>使用场景</strong>：通常用于节点的隔离，例如，如果某些节点不适合某些特定的负载（如特定的硬件要求），可以在这些节点上设置污点，确保不适合的 Pod 不会调度到这些节点。</li></ul></li><li><p><strong>资源请求和限制（Resource Requests and Limits）</strong></p><ul><li>在 Kubernetes 中，Pod 可以设置资源请求（request）和限制（limit），这些信息会被调度器用来决定 Pod 应该调度到哪个节点上。<ul><li><strong>Resource Requests</strong>：指示容器正常运行所需要的资源，调度器根据这些请求来决定容器应该运行在哪个节点上。</li><li><strong>Resource Limits</strong>：设置容器的最大资源使用量。</li></ul></li><li>调度器通过检查节点的资源（CPU、内存等）来选择适合的节点，并确保节点的资源满足 Pod 的请求。</li></ul></li><li><p><strong>调度策略（Scheduling Policies）</strong></p><ul><li><p>Priority and Preemption（优先级与抢占）</p><p>：</p><ul><li>Kubernetes 支持对 Pod 设置优先级，通过优先级，较高优先级的 Pod 可以抢占较低优先级 Pod 的资源，尤其是在集群资源紧张时。</li><li><strong>Preemption</strong>：如果集群资源不足，且一个新的 Pod 需要资源，则优先级较低的 Pod 可能会被抢占，以释放资源给优先级较高的 Pod。</li></ul></li><li><p>Scheduling via Custom Scheduler（自定义调度器）</p><p>：</p><ul><li>Kubernetes 支持自定义调度器，用户可以根据自己的需求编写自定义的调度逻辑，替换默认调度器或与其配合使用。</li><li><strong>使用场景</strong>：如果用户需要实现一些非常复杂的调度规则，或者要考虑业务逻辑（如应用的拓扑结构、流量路由等），可以编写自定义调度器。</li></ul></li></ul></li><li><p><strong>Affinity & Anti-Affinity（亲和性与反亲和性）</strong></p><ul><li><strong>Pod Affinity</strong>：Pod 之间的亲和性关系，允许将 Pod 调度到具有相同标签的节点上，以便在物理上靠近运行。</li><li><strong>Pod Anti-Affinity</strong>：反亲和性，允许避免将 Pod 调度到具有特定标签的节点上。</li></ul></li></ol><h3 id=8-daemonset>8. <strong>DaemonSet</strong>
<a class=anchor href=#8-daemonset>#</a></h3><ul><li><strong>DaemonSet</strong> 是一种特殊的控制器，用于确保每个节点上运行一个 Pod。当新的节点加入集群时，DaemonSet 会确保该节点上运行一个 Pod。</li><li><strong>使用场景</strong>：通常用于需要在每个节点上运行的应用程序（如日志收集、监控代理等）。</li></ul><h3 id=9-静态和动态调度static-and-dynamic-scheduling>9. <strong>静态和动态调度（Static and Dynamic Scheduling）</strong>
<a class=anchor href=#9-%e9%9d%99%e6%80%81%e5%92%8c%e5%8a%a8%e6%80%81%e8%b0%83%e5%ba%a6static-and-dynamic-scheduling>#</a></h3><ul><li><strong>静态调度</strong>：Pod 在创建时已经指定了调度的目标节点。</li><li><strong>动态调度</strong>：Pod 会在调度器中等待，直到调度器决定最合适的节点进行调度。</li></ul><h3 id=10-资源调度策略>10. <strong>资源调度策略</strong>
<a class=anchor href=#10-%e8%b5%84%e6%ba%90%e8%b0%83%e5%ba%a6%e7%ad%96%e7%95%a5>#</a></h3><ul><li><strong>软/硬性约束</strong>：Pod 调度可以通过软约束（如 <code>affinity</code> 和 <code>anti-affinity</code>）和硬性约束（如资源请求和限制）来实现。软约束不会阻止 Pod 调度，而硬性约束则是必须满足的条件。</li></ul><h3 id=总结-39>总结：
<a class=anchor href=#%e6%80%bb%e7%bb%93-39>#</a></h3><p>Kubernetes 提供了多种灵活的调度策略来满足不同的需求，包括但不限于：</p><ul><li>节点亲和性与 Pod 亲和性；</li><li>污点与容忍度；</li><li>自定义调度器；</li><li>资源请求与限制；</li><li>优先级和抢占。</li></ul><p>根据不同的应用场景，Kubernetes 提供了强大的调度功能，可以帮助优化资源利用、提高可用性、保障应用的稳定性。</p><h2 id=kubernetes-ingress原理>kubernetes Ingress原理☆
<a class=anchor href=#kubernetes-ingress%e5%8e%9f%e7%90%86>#</a></h2><h3 id=kubernetes-ingress-原理>Kubernetes Ingress 原理
<a class=anchor href=#kubernetes-ingress-%e5%8e%9f%e7%90%86>#</a></h3><p><strong>Ingress</strong> 是 Kubernetes 中用于管理外部访问到集群服务的 API 资源。它提供了一种通过 HTTP/HTTPS 协议访问集群内部服务的方式，同时允许对外部访问进行更细粒度的控制和路由。</p><p>Ingress 的原理依赖于以下几个核心概念：</p><h3 id=1-ingress-资源>1. <strong>Ingress 资源</strong>
<a class=anchor href=#1-ingress-%e8%b5%84%e6%ba%90>#</a></h3><p>Ingress 是一个集群级别的资源对象，用于定义如何将外部请求路由到 Kubernetes 集群内的服务。它通常由 URL 路径、主机名、协议等信息组成，用于将流量路由到不同的服务和端口。</p><p>Ingress 资源的基本结构如下：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>networking.k8s.io/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Ingress</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-ingress</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>rules</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>host</span>: <span style=color:#ae81ff>www.example.com</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>http</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>paths</span>:
</span></span><span style=display:flex><span>          - <span style=color:#f92672>path</span>: <span style=color:#ae81ff>/api</span>
</span></span><span style=display:flex><span>            <span style=color:#f92672>pathType</span>: <span style=color:#ae81ff>Prefix</span>
</span></span><span style=display:flex><span>            <span style=color:#f92672>backend</span>:
</span></span><span style=display:flex><span>              <span style=color:#f92672>service</span>:
</span></span><span style=display:flex><span>                <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-api-service</span>
</span></span><span style=display:flex><span>                <span style=color:#f92672>port</span>:
</span></span><span style=display:flex><span>                  <span style=color:#f92672>number</span>: <span style=color:#ae81ff>80</span>
</span></span><span style=display:flex><span>          - <span style=color:#f92672>path</span>: <span style=color:#ae81ff>/web</span>
</span></span><span style=display:flex><span>            <span style=color:#f92672>pathType</span>: <span style=color:#ae81ff>Prefix</span>
</span></span><span style=display:flex><span>            <span style=color:#f92672>backend</span>:
</span></span><span style=display:flex><span>              <span style=color:#f92672>service</span>:
</span></span><span style=display:flex><span>                <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-web-service</span>
</span></span><span style=display:flex><span>                <span style=color:#f92672>port</span>:
</span></span><span style=display:flex><span>                  <span style=color:#f92672>number</span>: <span style=color:#ae81ff>80</span>
</span></span></code></pre></div><ul><li><strong>host</strong>：指定请求的主机名。</li><li><strong>http/paths</strong>：定义了基于 URL 路径的路由规则。</li><li><strong>backend</strong>：指定了请求将被转发到哪个服务及其端口。</li></ul><h3 id=2-ingress-controller>2. <strong>Ingress Controller</strong>
<a class=anchor href=#2-ingress-controller>#</a></h3><p>Ingress 资源本身并不处理外部流量。它只是定义了如何路由流量，而真正负责执行流量路由和访问控制的组件是 <strong>Ingress Controller</strong>。</p><p>Ingress Controller 是一种负责执行 Ingress 资源定义的控制器，它通常运行在集群内，监视集群中的 Ingress 资源，并根据其配置将流量转发到相应的服务。Ingress Controller 通常是一个基于反向代理的应用，如 Nginx、Traefik 或 HAProxy。</p><ul><li><strong>Ingress Controller</strong> 会读取 Ingress 资源，分析规则并配置负载均衡器或反向代理。</li><li>它会监听集群中的 Ingress 资源变化，并动态更新其路由规则。</li></ul><p>常见的 Ingress Controller：</p><ul><li><strong>Nginx Ingress Controller</strong>：Nginx 基于反向代理的实现。</li><li><strong>Traefik</strong>：一种现代的反向代理，支持多种功能，如自动发现、WebSocket 等。</li><li><strong>HAProxy</strong>：一个成熟的负载均衡器和反向代理，也可作为 Ingress Controller 使用。</li></ul><h3 id=3-ingress-的工作原理>3. <strong>Ingress 的工作原理</strong>
<a class=anchor href=#3-ingress-%e7%9a%84%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86>#</a></h3><h4 id=1-外部请求到达-ingress-controller>1. <strong>外部请求到达 Ingress Controller</strong>
<a class=anchor href=#1-%e5%a4%96%e9%83%a8%e8%af%b7%e6%b1%82%e5%88%b0%e8%be%be-ingress-controller>#</a></h4><p>外部客户端通过 HTTP 或 HTTPS 请求访问指定的 URL（如 <code>www.example.com</code>）。Ingress Controller 通常通过暴露一个外部 IP 或 LoadBalancer 的方式接收这些请求。</p><h4 id=2-ingress-controller-解析-ingress-资源>2. <strong>Ingress Controller 解析 Ingress 资源</strong>
<a class=anchor href=#2-ingress-controller-%e8%a7%a3%e6%9e%90-ingress-%e8%b5%84%e6%ba%90>#</a></h4><ul><li>Ingress Controller 会根据请求的域名（<code>host</code>）和路径（<code>path</code>）匹配相应的规则。</li><li>如果请求的 URL 匹配某个 Ingress 资源的规则，Ingress Controller 会将请求路由到相应的后端服务。</li></ul><h4 id=3-请求转发到后端服务>3. <strong>请求转发到后端服务</strong>
<a class=anchor href=#3-%e8%af%b7%e6%b1%82%e8%bd%ac%e5%8f%91%e5%88%b0%e5%90%8e%e7%ab%af%e6%9c%8d%e5%8a%a1>#</a></h4><ul><li>Ingress Controller 根据匹配的规则，将请求转发到相应的 Kubernetes Service。</li><li>Service 会将流量传递到对应的 Pod 上，Pod 再将请求处理并返回响应。</li></ul><h4 id=4-返回响应>4. <strong>返回响应</strong>
<a class=anchor href=#4-%e8%bf%94%e5%9b%9e%e5%93%8d%e5%ba%94>#</a></h4><ul><li>后端服务的 Pod 处理请求后，响应会通过 Kubernetes 的网络模型返回给 Ingress Controller。</li><li>最终，Ingress Controller 将响应返回给外部客户端。</li></ul><h3 id=4-ingress-的特性>4. <strong>Ingress 的特性</strong>
<a class=anchor href=#4-ingress-%e7%9a%84%e7%89%b9%e6%80%a7>#</a></h3><ul><li><p><strong>路径路由</strong>：可以基于 URL 路径将请求路由到不同的服务。例如，<code>/api</code> 路径的请求可以被路由到 <code>my-api-service</code>，而 <code>/web</code> 路径的请求可以被路由到 <code>my-web-service</code>。</p></li><li><p><strong>主机名路由</strong>：可以基于域名路由流量。Ingress 可以处理不同的域名并将流量转发到不同的服务。</p></li><li><p><strong>TLS 支持</strong>：Ingress 支持通过 TLS/SSL 加密来保护 HTTP 流量。可以在 Ingress 资源中指定 TLS 配置，指定密钥和证书以支持 HTTPS。</p><p>示例：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>tls</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>hosts</span>:
</span></span><span style=display:flex><span>        - <span style=color:#e6db74>&#34;www.example.com&#34;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>secretName</span>: <span style=color:#ae81ff>example-tls</span>
</span></span></code></pre></div></li><li><p><strong>负载均衡</strong>：Ingress Controller 通过负载均衡策略将请求分发到不同的 Pod，支持 Round-robin、Least Connection 等策略。</p></li><li><p><strong>反向代理</strong>：Ingress Controller 作为反向代理处理外部请求，并转发到集群内部服务。它通过反向代理帮助管理跨多个服务的流量。</p></li><li><p><strong>基于规则的路由</strong>：Ingress 支持基于 URL 路径、主机名等条件的路由规则，也可以结合自定义的 header 进行路由。</p></li></ul><h3 id=5-ingress-与-service-的关系>5. <strong>Ingress 与 Service 的关系</strong>
<a class=anchor href=#5-ingress-%e4%b8%8e-service-%e7%9a%84%e5%85%b3%e7%b3%bb>#</a></h3><p>Ingress 并不直接暴露 Pod，而是通过 <strong>Service</strong> 暴露 Pod。具体来说，Ingress Controller 会将流量转发到一个 Kubernetes Service，然后由 Service 将流量进一步分发到相应的 Pod。</p><h3 id=6-ingress-的使用场景>6. <strong>Ingress 的使用场景</strong>
<a class=anchor href=#6-ingress-%e7%9a%84%e4%bd%bf%e7%94%a8%e5%9c%ba%e6%99%af>#</a></h3><ul><li><strong>暴露 HTTP/HTTPS 服务</strong>：使用 Ingress 可以将 HTTP/HTTPS 服务暴露给外部，通常用于 Web 应用、API 等服务。</li><li><strong>基于域名的路由</strong>：可以使用 Ingress 根据域名将流量路由到不同的服务，例如 <code>api.example.com</code> 和 <code>www.example.com</code> 路由到不同的服务。</li><li><strong>TLS/SSL 加密</strong>：Ingress 支持配置 TLS，帮助实现 HTTPS 加密通信。</li><li><strong>集群内部负载均衡</strong>：Ingress Controller 可以作为集群内部的负载均衡器，自动分配流量到不同的后端服务。</li></ul><h3 id=7-ingress-controller-部署>7. <strong>Ingress Controller 部署</strong>
<a class=anchor href=#7-ingress-controller-%e9%83%a8%e7%bd%b2>#</a></h3><p>Ingress Controller 通常作为一个 Deployment 在 Kubernetes 中运行，它监听 Kubernetes 中的 Ingress 资源并管理路由。部署 Ingress Controller 的基本步骤如下：</p><ol><li>选择一个适合的 Ingress Controller（如 Nginx、Traefik 等）。</li><li>使用 Kubernetes 的 YAML 配置文件部署 Ingress Controller。</li><li>配置 Ingress 资源，指定如何路由流量到服务。</li></ol><h3 id=8-常见的-ingress-controller>8. <strong>常见的 Ingress Controller</strong>
<a class=anchor href=#8-%e5%b8%b8%e8%a7%81%e7%9a%84-ingress-controller>#</a></h3><ul><li><strong>Nginx Ingress Controller</strong>：<ul><li>最常用的 Ingress Controller，实现了基于 Nginx 的反向代理和负载均衡。</li><li>适用于大多数标准的 HTTP/HTTPS 流量管理场景。</li></ul></li><li><strong>Traefik</strong>：<ul><li>一个现代的反向代理，支持动态发现、自动配置和高级功能如 WebSocket。</li><li>支持多种后端和协议，适用于微服务架构。</li></ul></li><li><strong>HAProxy</strong>：<ul><li>适用于需要高性能、稳定的负载均衡场景。</li><li>支持多种高级路由、负载均衡策略和健康检查。</li></ul></li></ul><h3 id=总结-40>总结
<a class=anchor href=#%e6%80%bb%e7%bb%93-40>#</a></h3><p>Kubernetes 的 Ingress 通过定义如何将外部 HTTP/HTTPS 请求路由到集群内部的服务来提供外部访问功能。其工作原理依赖于 Ingress 资源和 Ingress Controller，后者执行路由、负载均衡和安全性等功能。通过 Ingress，用户可以轻松实现外部访问控制、路径和域名路由、TLS 加密等功能。</p><h2 id=kubernetes各模块如何与api-server通信>Kubernetes各模块如何与API Server通信
<a class=anchor href=#kubernetes%e5%90%84%e6%a8%a1%e5%9d%97%e5%a6%82%e4%bd%95%e4%b8%8eapi-server%e9%80%9a%e4%bf%a1>#</a></h2><p>在 Kubernetes 中，所有的组件（包括调度器、控制器、节点等）都通过 <strong>API Server</strong> 进行通信。API Server 是 Kubernetes 集群的核心组件，负责处理集群内所有的请求，并提供 Kubernetes API（REST API）供客户端和集群各个模块进行交互。API Server 是集群的统一入口，所有的请求都会经过 API Server 进行认证、授权、路由和处理。</p><h3 id=各模块与-api-server-的通信方式>各模块与 API Server 的通信方式
<a class=anchor href=#%e5%90%84%e6%a8%a1%e5%9d%97%e4%b8%8e-api-server-%e7%9a%84%e9%80%9a%e4%bf%a1%e6%96%b9%e5%bc%8f>#</a></h3><ol><li><p><strong>kube-apiserver</strong></p><ul><li><p><strong>功能</strong>：Kubernetes 的 API Server 是集群的核心组件，所有的请求（包括内部和外部的请求）都通过它进行交互。它处理 Kubernetes API（例如资源对象创建、更新、删除、查询等），并提供 RESTful API 供客户端和其他组件进行访问。</p></li><li><p>通信方式</p><p>：</p><ul><li><strong>客户端（如 kubectl 或应用程序）</strong>：用户通过 <code>kubectl</code> 等客户端工具与 API Server 交互，执行各种操作（如创建 Pod、查看节点、更新配置等）。这些操作最终都会通过 REST API 发送到 API Server。</li><li><strong>控制器</strong>：集群中的控制器（如 Deployment Controller、ReplicaSet Controller、StatefulSet Controller 等）会定期访问 API Server 查询集群的当前状态，执行控制循环（如确保 Pod 的副本数符合预期）。它们通过 Kubernetes Client 来与 API Server 通信。</li><li><strong>etcd</strong>：API Server 将集群的状态数据（如 Pod、Service、Node 等资源对象）保存在 etcd 中，它是集群的持久化存储。API Server 会通过 etcd API 与 etcd 进行交互来读取和写入数据。</li></ul></li></ul></li><li><p><strong>kube-scheduler</strong></p><ul><li><p><strong>功能</strong>：调度器负责将 Pod 调度到合适的节点上。它监视 API Server 中的 Pod 对象，并根据资源、节点状态等信息决定 Pod 的调度位置。</p></li><li><p>通信方式</p><p>：</p><ul><li>kube-scheduler 会定期访问 API Server 来查看待调度的 Pod（通过 <code>ListPods</code> 或 <code>Watch</code> API）并获取节点资源（通过 <code>ListNodes</code> 或 <code>Watch</code> API）。</li><li>调度完成后，kube-scheduler 会通过 API Server 更新 Pod 的调度信息（例如将 Pod 的 <code>nodeName</code> 更新为调度的目标节点）。</li></ul></li></ul></li><li><p><strong>kube-controller-manager</strong></p><ul><li><p><strong>功能</strong>：控制器管理器负责维护集群的期望状态，确保集群资源达到期望的状态。例如，ReplicaSet 控制器会确保指定数量的 Pod 副本在集群中始终保持运行。</p></li><li><p>通信方式</p><p>：</p><ul><li>控制器通过 API Server 查询集群的当前状态，并根据期望状态执行操作。</li><li>例如，ReplicaSet 控制器会定期查询 API Server 中的 ReplicaSet 对象，并检查实际的 Pod 副本数量是否符合期望。如果不符合，它会创建或删除 Pod 来实现副本数的调整。</li></ul></li></ul></li><li><p><strong>kubelet</strong></p><ul><li><p><strong>功能</strong>：每个节点上运行的 kubelet 负责管理该节点上的 Pod 和容器的生命周期，并向 API Server 报告节点状态、Pod 状态等信息。</p></li><li><p>通信方式</p><p>：</p><ul><li>kubelet 定期向 API Server 提交节点和 Pod 的状态信息（通过 <code>POST</code> 或 <code>PUT</code> 请求）。</li><li>kubelet 还通过 API Server 向调度器请求分配待调度的 Pod，并管理这些 Pod 的容器生命周期。</li><li>kubelet 会拉取 Pod 的配置文件、容器镜像等信息，确保节点上的容器按预期运行。</li></ul></li></ul></li><li><p><strong>etcd</strong></p><ul><li><p><strong>功能</strong>：etcd 是 Kubernetes 的分布式键值存储，负责持久化存储 Kubernetes 集群的所有数据（如 Pod、Service、ConfigMap 等）。</p></li><li><p>通信方式</p><p>：</p><ul><li>API Server 会通过 gRPC 调用与 etcd 进行交互，确保数据的读取和写入。API Server 是唯一与 etcd 交互的组件。</li><li>API Server 将集群的状态数据保存在 etcd 中，并通过 etcd 提供的接口进行持久化存储。</li></ul></li></ul></li><li><p><strong>kubectl</strong></p><ul><li><p><strong>功能</strong>：kubectl 是 Kubernetes 的命令行工具，用于与 API Server 交互，执行对集群资源的管理和操作。</p></li><li><p>通信方式</p><p>：</p><ul><li>用户通过 <code>kubectl</code> 向 API Server 发送 RESTful 请求（如 <code>kubectl apply -f</code>, <code>kubectl get pods</code> 等命令），然后 API Server 处理请求并返回结果。</li></ul></li></ul></li><li><p><strong>集群外部客户端</strong></p><ul><li><p><strong>功能</strong>：外部应用程序或用户可以通过 API Server 提供的 REST API 来访问 Kubernetes 集群资源。</p></li><li><p>通信方式</p><p>：</p><ul><li>外部客户端通常会使用 HTTPS 与 API Server 通信，进行身份认证和授权后，通过 Kubernetes API 进行资源操作。</li></ul></li></ul></li></ol><h3 id=具体通信流程>具体通信流程
<a class=anchor href=#%e5%85%b7%e4%bd%93%e9%80%9a%e4%bf%a1%e6%b5%81%e7%a8%8b>#</a></h3><p>假设有一个操作需要通过 <code>kubectl</code> 创建一个 Pod，整个过程的通信流程如下：</p><ol><li><strong>用户发起请求</strong>：用户通过 <code>kubectl</code> 提交一个创建 Pod 的请求，<code>kubectl</code> 会将请求转化为 HTTP 请求（通常是一个 <code>POST</code> 请求）发送到 API Server。</li><li><strong>API Server 认证和授权</strong>：API Server 会首先进行用户认证（如使用 Token 或证书）和授权（如 RBAC 控制），确认请求是否具有执行权限。</li><li><strong>API Server 校验请求</strong>：API Server 校验请求的有效性（如资源的格式、字段是否正确等），并将请求转发到 etcd 进行持久化。</li><li><strong>API Server 更新资源</strong>：API Server 将请求处理后，向 etcd 存储 Pod 的信息。</li><li><strong>kubectl 接收响应</strong>：API Server 处理完成后，返回一个响应给 <code>kubectl</code>，如返回 Pod 创建成功的信息。</li><li><strong>Pod 调度与启动</strong>：如果是新的 Pod，调度器（scheduler）会读取 Pod 请求，决定将其调度到哪个节点，节点的 kubelet 会启动容器，开始运行该 Pod。</li></ol><h3 id=总结-41>总结
<a class=anchor href=#%e6%80%bb%e7%bb%93-41>#</a></h3><p>Kubernetes 中的各模块（如 kubelet、kube-scheduler、kube-controller-manager）都通过 API Server 与集群资源进行交互。API Server 提供了统一的 REST API，负责处理来自集群内部和外部的请求，所有组件都依赖于它来获取和修改集群的状态信息。此外，API Server 与 etcd 直接交互，持久化所有集群的状态。通过这些模块的相互通信，Kubernetes 实现了高度自动化和灵活的集群管理。</p><h2 id=kubelet监控worker节点如何实现>kubelet监控worker节点如何实现
<a class=anchor href=#kubelet%e7%9b%91%e6%8e%a7worker%e8%8a%82%e7%82%b9%e5%a6%82%e4%bd%95%e5%ae%9e%e7%8e%b0>#</a></h2><p>在 Kubernetes 中，<strong>kubelet</strong> 是运行在每个 <strong>worker 节点</strong> 上的关键组件，负责管理该节点上的 Pod 和容器的生命周期。它不仅确保节点上的 Pod 按照预期运行，还定期向 Kubernetes 集群的 <strong>API Server</strong> 报告节点和 Pod 的状态。</p><p>为了对 <strong>worker 节点</strong> 进行有效的监控，<strong>kubelet</strong> 通过以下几种方式提供监控功能：</p><h3 id=1-kubelet-的健康检查>1. <strong>kubelet 的健康检查</strong>
<a class=anchor href=#1-kubelet-%e7%9a%84%e5%81%a5%e5%ba%b7%e6%a3%80%e6%9f%a5>#</a></h3><p>kubelet 提供了几种不同类型的健康检查接口，这些接口可以帮助监控 <strong>worker 节点</strong> 和 <strong>Pod</strong> 的健康状态。</p><h4 id=11-liveness-和-readiness-probes>1.1 <strong>Liveness 和 Readiness Probes</strong>
<a class=anchor href=#11-liveness-%e5%92%8c-readiness-probes>#</a></h4><ul><li><strong>Liveness Probes</strong> 用于检查容器是否处于健康状态，是否需要重启。</li><li><strong>Readiness Probes</strong> 用于检查容器是否准备好接受流量。</li></ul><p>这些探针通过设置在容器中的检查点，kubelet 定期访问它们。如果探针失败，kubelet 会采取相应的行动（如重启容器或将 Pod 从服务负载均衡中移除）。</p><h4 id=12-kubelet-自身健康检查>1.2 <strong>kubelet 自身健康检查</strong>
<a class=anchor href=#12-kubelet-%e8%87%aa%e8%ba%ab%e5%81%a5%e5%ba%b7%e6%a3%80%e6%9f%a5>#</a></h4><ul><li><strong>/healthz</strong>：这个端点用于检查 kubelet 是否正常运行。</li><li><strong>/metrics</strong>：提供 Prometheus 格式的指标数据，供外部系统采集。</li></ul><h3 id=2-kubelet-的指标暴露>2. <strong>kubelet 的指标暴露</strong>
<a class=anchor href=#2-kubelet-%e7%9a%84%e6%8c%87%e6%a0%87%e6%9a%b4%e9%9c%b2>#</a></h3><p>kubelet 本身通过暴露 <strong>/metrics</strong> 端点提供了丰富的指标信息，这些信息包括节点的状态、容器的资源使用情况等。常见的指标有：</p><ul><li><strong>容器 CPU 使用情况</strong></li><li><strong>容器内存使用情况</strong></li><li><strong>Pod 状态</strong></li><li><strong>网络流量和磁盘使用情况</strong></li></ul><p>这些指标可以通过 <strong>Prometheus</strong> 等监控系统进行抓取，用于对 worker 节点和容器进行实时监控。</p><h4 id=21-prometheus-监控>2.1 <strong>Prometheus 监控</strong>
<a class=anchor href=#21-prometheus-%e7%9b%91%e6%8e%a7>#</a></h4><ul><li><strong>Prometheus</strong> 是 Kubernetes 中最常用的监控工具之一，支持抓取 <strong>/metrics</strong> 端点的数据并进行存储和分析。</li><li>Prometheus 可以通过配置 <strong>kubelet</strong> 的 <code>/metrics</code> 端点来收集每个节点的资源使用数据（如 CPU 和内存消耗等）。</li></ul><h4 id=22-node-exporter>2.2 <strong>Node Exporter</strong>
<a class=anchor href=#22-node-exporter>#</a></h4><ul><li><strong>Node Exporter</strong> 是 Prometheus 生态中的一个组件，用于收集主机的硬件和操作系统指标（如 CPU、内存、磁盘等）。</li><li>通过 <strong>Node Exporter</strong> 和 Prometheus 配合，能够提供更详细的节点层级的监控数据。</li></ul><h3 id=3-资源使用监控>3. <strong>资源使用监控</strong>
<a class=anchor href=#3-%e8%b5%84%e6%ba%90%e4%bd%bf%e7%94%a8%e7%9b%91%e6%8e%a7>#</a></h3><p>Kubernetes 集群中的每个节点都会暴露一些关于资源使用的关键指标，如 CPU、内存、网络带宽等。kubelet 会持续监控这些资源的使用情况，并将其暴露给外部监控系统。</p><ul><li><strong>资源使用数据</strong>：kubelet 会定期向 API Server 提交该节点上的资源使用情况，包括 CPU 和内存的使用情况。</li><li><strong>节点资源压力</strong>：kubelet 会根据节点的资源使用情况计算资源压力，并根据压力情况采取相应的调度或驱逐策略。例如，当节点的内存使用率过高时，kubelet 会通知调度器驱逐 Pod。</li></ul><h3 id=4-事件记录>4. <strong>事件记录</strong>
<a class=anchor href=#4-%e4%ba%8b%e4%bb%b6%e8%ae%b0%e5%bd%95>#</a></h3><p>kubelet 会将节点上的事件（如容器启动、停止、Pod 状态变化等）记录下来。这些事件可以通过 <strong>kubectl get events</strong> 或 <strong>API Server</strong> 查询。通过事件监控，可以追踪节点和 Pod 的状态变化，快速发现潜在问题。</p><h3 id=5-集群级别的监控系统>5. <strong>集群级别的监控系统</strong>
<a class=anchor href=#5-%e9%9b%86%e7%be%a4%e7%ba%a7%e5%88%ab%e7%9a%84%e7%9b%91%e6%8e%a7%e7%b3%bb%e7%bb%9f>#</a></h3><ul><li><strong>Kubernetes Dashboard</strong>：Kubernetes 提供了一个可视化的 Web 控制台，能够展示每个节点和 Pod 的状态，并显示健康检查信息。</li><li><strong>Prometheus + Grafana</strong>：Prometheus 和 Grafana 是最常用的监控组合。Prometheus 用于数据收集，Grafana 用于数据可视化。通过对 kubelet 提供的指标进行可视化，用户可以方便地查看节点和容器的健康状况。</li></ul><h3 id=6-node-conditions节点状态>6. <strong>Node Conditions（节点状态）</strong>
<a class=anchor href=#6-node-conditions%e8%8a%82%e7%82%b9%e7%8a%b6%e6%80%81>#</a></h3><p>kubelet 通过 <strong>Node Conditions</strong> 来反映节点的健康状态。节点的状态包括以下几种：</p><ul><li><strong>Ready</strong>：节点是否正常运行并可接受调度。</li><li><strong>MemoryPressure</strong>：节点内存是否足够。</li><li><strong>DiskPressure</strong>：节点磁盘是否足够。</li><li><strong>PIDPressure</strong>：节点是否面临进程数压力。</li><li><strong>NetworkUnavailable</strong>：节点是否存在网络问题。</li></ul><p>这些节点状态通过 <strong>kubectl describe node</strong> 或 API 查询来查看。</p><h3 id=7-资源调度与驱逐>7. <strong>资源调度与驱逐</strong>
<a class=anchor href=#7-%e8%b5%84%e6%ba%90%e8%b0%83%e5%ba%a6%e4%b8%8e%e9%a9%b1%e9%80%90>#</a></h3><p>kubelet 会根据节点的资源使用情况判断是否需要驱逐某些 Pod，保证节点的资源使用不会过载。例如，当节点的内存消耗过高时，kubelet 会触发驱逐策略，选择合适的 Pod 进行驱逐，从而避免节点崩溃。</p><h3 id=8-故障监控与通知>8. <strong>故障监控与通知</strong>
<a class=anchor href=#8-%e6%95%85%e9%9a%9c%e7%9b%91%e6%8e%a7%e4%b8%8e%e9%80%9a%e7%9f%a5>#</a></h3><p>kubelet 会定期检测节点上的 Pod 是否健康，并在出现问题时发出警告。监控系统可以设置告警，及时通知运维人员。</p><ul><li><strong>Node NotReady</strong>：当节点处于 <code>NotReady</code> 状态时，kubelet 会发出相关事件，表示该节点出现问题。</li><li><strong>Pod Restart</strong>：如果 Pod 在节点上频繁重启，kubelet 会记录事件，表明 Pod 可能存在问题。</li></ul><h3 id=9-日志监控>9. <strong>日志监控</strong>
<a class=anchor href=#9-%e6%97%a5%e5%bf%97%e7%9b%91%e6%8e%a7>#</a></h3><ul><li>kubelet 会记录关于节点、Pod 和容器的日志。通过集成日志收集系统（如 <strong>Fluentd</strong> 或 <strong>ELK Stack</strong>），可以集中管理和监控节点和容器的日志。</li><li>日志中包含了节点健康、容器启动、Pod 调度等信息。</li></ul><h3 id=10-系统级监控>10. <strong>系统级监控</strong>
<a class=anchor href=#10-%e7%b3%bb%e7%bb%9f%e7%ba%a7%e7%9b%91%e6%8e%a7>#</a></h3><p>除了容器和 Kubernetes 资源的监控，kubelet 还可以通过集成 <strong>Linux 系统监控工具</strong>（如 <strong>sysstat</strong>、<strong>collectd</strong>）来监控操作系统层面的性能指标。这些指标可以提供操作系统级别的性能数据，帮助管理员分析节点的性能瓶颈。</p><h3 id=总结-42>总结
<a class=anchor href=#%e6%80%bb%e7%bb%93-42>#</a></h3><p>在 Kubernetes 中，<strong>kubelet</strong> 通过多种方式监控 <strong>worker 节点</strong> 的健康状态，包括：</p><ol><li>健康检查（Liveness 和 Readiness Probe）</li><li>提供 <code>/metrics</code> 端点暴露节点和容器的资源使用数据</li><li>利用 <strong>Prometheus</strong>、<strong>Grafana</strong> 等工具进行指标收集和可视化</li><li>定期向 API Server 提交节点状态和事件信息</li><li>基于节点条件（如内存、磁盘、网络等）评估节点的健康状况</li><li>通过日志和事件跟踪节点和 Pod 的运行状态</li></ol><p>这些监控方法确保了 Kubernetes 集群的高可用性和健康运行，同时能够及时发现和解决问题。</p><h2 id=容器时区不一致如何解决>容器时区不一致如何解决？
<a class=anchor href=#%e5%ae%b9%e5%99%a8%e6%97%b6%e5%8c%ba%e4%b8%8d%e4%b8%80%e8%87%b4%e5%a6%82%e4%bd%95%e8%a7%a3%e5%86%b3>#</a></h2><p>在容器化环境中，时区不一致的问题是比较常见的，特别是当容器和宿主机的时区配置不同或容器内没有正确的时区信息时。为了解决这个问题，通常可以通过以下几种方法来确保容器的时区一致性：</p><h3 id=1-使用宿主机的时区>1. <strong>使用宿主机的时区</strong>
<a class=anchor href=#1-%e4%bd%bf%e7%94%a8%e5%ae%bf%e4%b8%bb%e6%9c%ba%e7%9a%84%e6%97%b6%e5%8c%ba>#</a></h3><p>最简单的方式是让容器使用宿主机的时区。可以通过将宿主机的时区文件挂载到容器中来实现。</p><h4 id=操作方法>操作方法：
<a class=anchor href=#%e6%93%8d%e4%bd%9c%e6%96%b9%e6%b3%95>#</a></h4><ul><li>在启动容器时，使用 <code>-v</code> 参数将宿主机的时区文件挂载到容器中。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker run -v /etc/localtime:/etc/localtime:ro -v /etc/timezone:/etc/timezone:ro my-container
</span></span></code></pre></div><ul><li><code>-v /etc/localtime:/etc/localtime:ro</code> 让容器共享宿主机的时区配置。</li><li><code>-v /etc/timezone:/etc/timezone:ro</code> 使容器中的 <code>/etc/timezone</code> 配置与宿主机一致。</li></ul><p>在 Kubernetes 中，也可以通过类似的方式使用 <code>hostPath</code> 将宿主机的时区文件挂载到容器中。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Pod</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>time-zone-pod</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>containers</span>:
</span></span><span style=display:flex><span>  - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-container</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>image</span>: <span style=color:#ae81ff>my-container-image</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>volumeMounts</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>mountPath</span>: <span style=color:#ae81ff>/etc/localtime</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>name</span>: <span style=color:#ae81ff>localtime</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>readOnly</span>: <span style=color:#66d9ef>true</span>
</span></span><span style=display:flex><span>    - <span style=color:#f92672>mountPath</span>: <span style=color:#ae81ff>/etc/timezone</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>name</span>: <span style=color:#ae81ff>timezone</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>readOnly</span>: <span style=color:#66d9ef>true</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>volumes</span>:
</span></span><span style=display:flex><span>  - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>localtime</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>hostPath</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>path</span>: <span style=color:#ae81ff>/etc/localtime</span>
</span></span><span style=display:flex><span>  - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>timezone</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>hostPath</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>path</span>: <span style=color:#ae81ff>/etc/timezone</span>
</span></span></code></pre></div><p>这种方法能确保容器和宿主机使用相同的时区。</p><h3 id=2-设置容器内的时区>2. <strong>设置容器内的时区</strong>
<a class=anchor href=#2-%e8%ae%be%e7%bd%ae%e5%ae%b9%e5%99%a8%e5%86%85%e7%9a%84%e6%97%b6%e5%8c%ba>#</a></h3><p>如果你希望容器使用特定的时区（例如与宿主机时区不同），可以在容器内设置时区。一般来说，可以通过以下两种方法来设置容器时区：</p><h4 id=方法-1-修改-etctimezone-和-etclocaltime>方法 1: 修改 <code>/etc/timezone</code> 和 <code>/etc/localtime</code>
<a class=anchor href=#%e6%96%b9%e6%b3%95-1-%e4%bf%ae%e6%94%b9-etctimezone-%e5%92%8c-etclocaltime>#</a></h4><ul><li>你可以在 Dockerfile 中添加相应的指令来设置时区，例如：</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Dockerfile data-lang=Dockerfile><span style=display:flex><span><span style=color:#66d9ef>RUN</span> apt-get update <span style=color:#f92672>&amp;&amp;</span> apt-get install -y tzdata<span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>RUN</span> ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime <span style=color:#f92672>&amp;&amp;</span> echo <span style=color:#e6db74>&#34;Asia/Shanghai&#34;</span> &gt; /etc/timezone<span style=color:#960050;background-color:#1e0010>
</span></span></span></code></pre></div><ul><li><code>tzdata</code> 是时区数据库包，安装后可以选择设置时区。</li><li><code>ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime</code> 命令会创建一个符号链接，让容器使用上海时区。</li><li>通过 <code>echo "Asia/Shanghai" > /etc/timezone</code> 设置时区名称。</li></ul><h4 id=方法-2-通过环境变量设置时区>方法 2: 通过环境变量设置时区
<a class=anchor href=#%e6%96%b9%e6%b3%95-2-%e9%80%9a%e8%bf%87%e7%8e%af%e5%a2%83%e5%8f%98%e9%87%8f%e8%ae%be%e7%bd%ae%e6%97%b6%e5%8c%ba>#</a></h4><ul><li>在运行容器时，你可以通过设置环境变量来指定时区，通常容器内的应用程序会使用该环境变量来调整时区。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker run -e TZ<span style=color:#f92672>=</span>Asia/Shanghai my-container
</span></span></code></pre></div><ul><li><code>TZ</code> 环境变量是许多 Linux 系统和容器中常用的时区环境变量，应用程序（如 Python、Java）可以读取此变量来设置时区。</li></ul><p>在 Kubernetes 中，你也可以通过设置环境变量 <code>TZ</code> 来设置时区：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Pod</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>time-zone-pod</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>containers</span>:
</span></span><span style=display:flex><span>  - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-container</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>image</span>: <span style=color:#ae81ff>my-container-image</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>env</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>TZ</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>value</span>: <span style=color:#e6db74>&#34;Asia/Shanghai&#34;</span>
</span></span></code></pre></div><h3 id=3-使用>3. <strong>使用 <code>UTC</code> 时区</strong>
<a class=anchor href=#3-%e4%bd%bf%e7%94%a8>#</a></h3><p>如果应用不依赖特定时区，建议将容器的时区设置为 <code>UTC</code>，因为 UTC 是不受夏令时影响的标准时间，适用于大多数场景。</p><p>可以在 Dockerfile 中直接设置容器时区为 UTC：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Dockerfile data-lang=Dockerfile><span style=display:flex><span><span style=color:#66d9ef>RUN</span> ln -sf /usr/share/zoneinfo/UTC /etc/localtime <span style=color:#f92672>&amp;&amp;</span> echo <span style=color:#e6db74>&#34;UTC&#34;</span> &gt; /etc/timezone<span style=color:#960050;background-color:#1e0010>
</span></span></span></code></pre></div><p>或者在运行容器时通过环境变量设置：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker run -e TZ<span style=color:#f92672>=</span>UTC my-container
</span></span></code></pre></div><p>在 Kubernetes 中也是类似的做法：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Pod</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>time-zone-pod</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>containers</span>:
</span></span><span style=display:flex><span>  - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-container</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>image</span>: <span style=color:#ae81ff>my-container-image</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>env</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>TZ</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>value</span>: <span style=color:#e6db74>&#34;UTC&#34;</span>
</span></span></code></pre></div><h3 id=4-使用-docker-compose-或-kubernetes-的-configmap>4. <strong>使用 Docker Compose 或 Kubernetes 的 ConfigMap</strong>
<a class=anchor href=#4-%e4%bd%bf%e7%94%a8-docker-compose-%e6%88%96-kubernetes-%e7%9a%84-configmap>#</a></h3><p>如果你需要在多个容器中统一设置时区，可以通过 <strong>Docker Compose</strong> 或 <strong>Kubernetes ConfigMap</strong> 来集中管理时区配置。</p><h4 id=docker-compose-示例>Docker Compose 示例：
<a class=anchor href=#docker-compose-%e7%a4%ba%e4%be%8b>#</a></h4><p>在 <code>docker-compose.yml</code> 文件中设置时区环境变量：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>version</span>: <span style=color:#e6db74>&#39;3&#39;</span>
</span></span><span style=display:flex><span><span style=color:#f92672>services</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>app</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>image</span>: <span style=color:#ae81ff>my-container</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>environment</span>:
</span></span><span style=display:flex><span>      - <span style=color:#ae81ff>TZ=Asia/Shanghai</span>
</span></span></code></pre></div><h4 id=kubernetes-configmap-示例>Kubernetes ConfigMap 示例：
<a class=anchor href=#kubernetes-configmap-%e7%a4%ba%e4%be%8b>#</a></h4><p>你可以创建一个 ConfigMap 来设置时区，并将其挂载到多个容器中：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ConfigMap</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>time-zone-config</span>
</span></span><span style=display:flex><span><span style=color:#f92672>data</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>timezone</span>: <span style=color:#e6db74>&#34;Asia/Shanghai&#34;</span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Pod</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>time-zone-pod</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>containers</span>:
</span></span><span style=display:flex><span>  - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-container</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>image</span>: <span style=color:#ae81ff>my-container-image</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>envFrom</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>configMapRef</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>name</span>: <span style=color:#ae81ff>time-zone-config</span>
</span></span></code></pre></div><h3 id=总结-43>总结
<a class=anchor href=#%e6%80%bb%e7%bb%93-43>#</a></h3><p>容器时区不一致的问题可以通过以下几种方式解决：</p><ol><li><strong>使用宿主机时区</strong>：通过挂载宿主机的时区文件（如 <code>/etc/localtime</code> 和 <code>/etc/timezone</code>）。</li><li><strong>容器内设置时区</strong>：通过设置 <code>/etc/timezone</code> 和 <code>/etc/localtime</code> 或设置环境变量 <code>TZ</code>。</li><li><strong>使用 UTC 时区</strong>：容器设置为 UTC，避免时区变化带来的问题。</li><li><strong>集中管理时区</strong>：通过 Docker Compose 或 Kubernetes ConfigMap 管理和统一容器时区。</li></ol><p>这些方法都能帮助确保容器中的时区与预期一致，避免时区差异导致的各种问题。</p></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#kubernetes>Kubernetes</a></li><li><a href=#谈谈你对k8s的理解>谈谈你对k8s的理解☆</a><ul><li><a href=#关键优势>关键优势：</a></li><li><a href=#适用场景>适用场景：</a></li></ul></li><li><a href=#k8s集群架构是什么>k8s集群架构是什么☆</a><ul><li><a href=#1-控制平面control-plane>1. <strong>控制平面（Control Plane）</strong></a></li><li><a href=#2-工作节点worker-node>2. <strong>工作节点（Worker Node）</strong></a></li><li><a href=#3-k8s-集群架构示意图>3. <strong>K8s 集群架构示意图</strong></a></li><li><a href=#4-其他重要组件>4. <strong>其他重要组件</strong></a></li><li><a href=#5-高可用性与冗余>5. <strong>高可用性与冗余</strong></a></li><li><a href=#总结>总结</a></li></ul></li><li><a href=#简述pod创建过程>简述Pod创建过程☆</a><ul><li><a href=#1-创建-pod-配置文件>1. <strong>创建 Pod 配置文件</strong></a></li><li><a href=#2-提交-pod-配置到-kubernetes-api-server>2. <strong>提交 Pod 配置到 Kubernetes API Server</strong></a></li><li><a href=#3-api-server-存储-pod-配置>3. <strong>API Server 存储 Pod 配置</strong></a></li><li><a href=#4-调度scheduler选择合适的工作节点>4. <strong>调度（Scheduler）选择合适的工作节点</strong></a></li><li><a href=#5-kubelet-在工作节点上创建-pod>5. <strong>Kubelet 在工作节点上创建 Pod</strong></a></li><li><a href=#6-pod-启动并运行>6. <strong>Pod 启动并运行</strong></a></li><li><a href=#7-集群状态更新>7. <strong>集群状态更新</strong></a></li><li><a href=#8-监控与自动恢复>8. <strong>监控与自动恢复</strong></a></li><li><a href=#总结-1>总结</a></li></ul></li><li><a href=#简述删除一个pod流程>简述删除一个Pod流程</a><ul><li><a href=#1-用户发起删除请求>1. <strong>用户发起删除请求</strong></a></li><li><a href=#2-api-server-接受并验证请求>2. <strong>API Server 接受并验证请求</strong></a></li><li><a href=#3-pod-状态更新>3. <strong>Pod 状态更新</strong></a></li><li><a href=#4-删除-pod-相关的控制器对象>4. <strong>删除 Pod 相关的控制器对象</strong></a></li><li><a href=#5-kubelet-监测到-pod-删除请求>5. <strong>Kubelet 监测到 Pod 删除请求</strong></a></li><li><a href=#6-pod-资源回收>6. <strong>Pod 资源回收</strong></a></li><li><a href=#7-api-server-完成删除操作>7. <strong>API Server 完成删除操作</strong></a></li><li><a href=#8-删除完成>8. <strong>删除完成</strong></a></li><li><a href=#总结-2>总结</a></li></ul></li><li><a href=#不同node上的pod之间的通信过程>不同node上的Pod之间的通信过程☆</a><ul><li><a href=#1-pod-网络模型>1. <strong>Pod 网络模型</strong></a></li><li><a href=#2-cni-插件容器网络接口>2. <strong>CNI 插件（容器网络接口）</strong></a></li><li><a href=#3-通信过程>3. <strong>通信过程</strong></a></li><li><a href=#4-网络策略network-policy>4. <strong>网络策略（Network Policy）</strong></a></li><li><a href=#5-service-和负载均衡>5. <strong>Service 和负载均衡</strong></a></li><li><a href=#总结-3>总结</a></li></ul></li><li><a href=#pod创建pending状态的原因>pod创建Pending状态的原因☆</a><ul><li><a href=#1-资源不足>1. <strong>资源不足</strong></a></li><li><a href=#2-调度器找不到合适的节点>2. <strong>调度器找不到合适的节点</strong></a></li><li><a href=#3-taints-和-tolerations>3. <strong>Taints 和 Tolerations</strong></a></li><li><a href=#4-调度器无法找到适合的节点>4. <strong>调度器无法找到适合的节点</strong></a></li><li><a href=#5-集群容量不足>5. <strong>集群容量不足</strong></a></li><li><a href=#6-persistentvolume-pv-或-persistentvolumeclaim-pvc-问题>6. <strong>PersistentVolume (PV) 或 PersistentVolumeClaim (PVC) 问题</strong></a></li><li><a href=#7-网络问题>7. <strong>网络问题</strong></a></li><li><a href=#8-镜像拉取失败>8. <strong>镜像拉取失败</strong></a></li><li><a href=#9-kubernetes-控制平面问题>9. <strong>Kubernetes 控制平面问题</strong></a></li><li><a href=#如何排查-pending-状态的-pod>如何排查 Pending 状态的 Pod</a></li><li><a href=#总结-4>总结</a></li></ul></li><li><a href=#deployment和statefulset区别>deployment和statefulset区别☆</a><ul><li><a href=#1-pod-的标识和命名>1. <strong>Pod 的标识和命名</strong></a></li><li><a href=#2-pod-生命周期>2. <strong>Pod 生命周期</strong></a></li><li><a href=#3-存储>3. <strong>存储</strong></a></li><li><a href=#4-扩缩容>4. <strong>扩缩容</strong></a></li><li><a href=#5-网络>5. <strong>网络</strong></a></li><li><a href=#6-适用场景>6. <strong>适用场景</strong></a></li><li><a href=#7-滚动更新>7. <strong>滚动更新</strong></a></li><li><a href=#总结对比>总结对比</a></li><li><a href=#适用场景示例>适用场景示例</a></li></ul></li><li><a href=#kube-proxy有什么作用>kube-proxy有什么作用☆</a><ul><li><a href=#kube-proxy-的主要作用><code>kube-proxy</code> 的主要作用</a></li><li><a href=#kube-proxy-的工作原理><code>kube-proxy</code> 的工作原理</a></li><li><a href=#kube-proxy-的工作流程><code>kube-proxy</code> 的工作流程</a></li><li><a href=#kube-proxy-的使用场景><code>kube-proxy</code> 的使用场景</a></li><li><a href=#总结-5>总结</a></li></ul></li><li><a href=#kube-proxy怎么修改ipvs规则>kube-proxy怎么修改ipvs规则</a><ul><li><a href=#修改-ipvs-规则>修改 IPVS 规则</a></li><li><a href=#总结-6>总结</a></li></ul></li><li><a href=#ipvs为什么比iptables效率高>ipvs为什么比iptables效率高</a><ul><li><a href=#1-专门为负载均衡设计>1. <strong>专门为负载均衡设计</strong></a></li><li><a href=#2-性能优化和内核支持>2. <strong>性能优化和内核支持</strong></a></li><li><a href=#3-规则查找方式>3. <strong>规则查找方式</strong></a></li><li><a href=#4-连接状态管理>4. <strong>连接状态管理</strong></a></li><li><a href=#5-负载均衡算法>5. <strong>负载均衡算法</strong></a></li><li><a href=#6-高并发处理能力>6. <strong>高并发处理能力</strong></a></li><li><a href=#7-内存使用和效率>7. <strong>内存使用和效率</strong></a></li><li><a href=#8-流量转发的直接性>8. <strong>流量转发的直接性</strong></a></li><li><a href=#总结-7>总结</a></li></ul></li><li><a href=#pod之间访问不通怎么排查>pod之间访问不通怎么排查☆</a><ul><li><a href=#1-检查-pod-之间的网络连接>1. <strong>检查 Pod 之间的网络连接</strong></a></li><li><a href=#2-检查网络插件是否正常工作>2. <strong>检查网络插件是否正常工作</strong></a></li><li><a href=#3-检查网络策略network-policies>3. <strong>检查网络策略（Network Policies）</strong></a></li><li><a href=#4-检查>4. <strong>检查 <code>kube-proxy</code> 状态</strong></a></li><li><a href=#5-检查节点网络和防火墙设置>5. <strong>检查节点网络和防火墙设置</strong></a></li><li><a href=#6-查看-dns-配置>6. <strong>查看 DNS 配置</strong></a></li><li><a href=#7-查看-service-配置>7. <strong>查看 Service 配置</strong></a></li><li><a href=#8-pod-重启和调试>8. <strong>Pod 重启和调试</strong></a></li><li><a href=#9-集群的网络诊断工具>9. <strong>集群的网络诊断工具</strong></a></li><li><a href=#总结-8>总结</a></li></ul></li><li><a href=#k8s中network-policy的实现原理>k8s中Network Policy的实现原理</a><ul><li><a href=#network-policy-的基本原理>Network Policy 的基本原理</a></li><li><a href=#network-policy-的工作流程>Network Policy 的工作流程</a></li><li><a href=#例子如何使用-network-policy>例子：如何使用 Network Policy</a></li><li><a href=#网络策略的关键点总结>网络策略的关键点总结</a></li><li><a href=#结论>结论</a></li></ul></li><li><a href=#探针有哪些探测方法有哪些>探针有哪些？探测方法有哪些？</a><ul><li><a href=#1-liveness-probe存活探针>1. <strong>Liveness Probe</strong>（存活探针）</a></li><li><a href=#2-readiness-probe就绪探针>2. <strong>Readiness Probe</strong>（就绪探针）</a></li><li><a href=#3-startup-probe启动探针>3. <strong>Startup Probe</strong>（启动探针）</a></li><li><a href=#探测方法probe-types>探测方法（Probe Types）</a></li><li><a href=#配置探针时的常用参数>配置探针时的常用参数</a></li><li><a href=#示例完整的探针配置>示例：完整的探针配置</a></li><li><a href=#总结-9>总结</a></li></ul></li><li><a href=#pod健康检查失败可能的原因和排查思路>pod健康检查失败可能的原因和排查思路</a><ul><li><a href=#常见的-pod-健康检查失败的原因>常见的 Pod 健康检查失败的原因</a></li><li><a href=#健康检查失败的排查思路>健康检查失败的排查思路</a></li><li><a href=#总结-10>总结</a></li></ul></li><li><a href=#k8s的service是什么>k8s的Service是什么☆</a><ul><li><a href=#service-的主要作用>Service 的主要作用</a></li><li><a href=#service-的类型>Service 的类型</a></li><li><a href=#service-的工作原理>Service 的工作原理</a></li><li><a href=#service-的健康检查>Service 的健康检查</a></li><li><a href=#访问-service-的方式>访问 Service 的方式</a></li><li><a href=#总结-11>总结</a></li></ul></li><li><a href=#metrics-server采集指标数据链路>metrics-server采集指标数据链路</a><ul><li><a href=#1-node-端的-kubelet-上报资源指标>1. <strong>Node 端的 Kubelet 上报资源指标</strong></a></li><li><a href=#2-metrics-server-收集数据>2. <strong>Metrics Server 收集数据</strong></a></li><li><a href=#3-数据聚合和处理>3. <strong>数据聚合和处理</strong></a></li><li><a href=#4-通过-kubernetes-api-提供访问>4. <strong>通过 Kubernetes API 提供访问</strong></a></li><li><a href=#5-pod-autoscaling-的触发>5. <strong>Pod Autoscaling 的触发</strong></a></li><li><a href=#6-数据过期与刷新>6. <strong>数据过期与刷新</strong></a></li><li><a href=#7-数据存储与可视化>7. <strong>数据存储与可视化</strong></a></li><li><a href=#采集数据链路总结>采集数据链路总结：</a></li></ul></li><li><a href=#k8s服务发现有哪些方式>k8s服务发现有哪些方式？</a><ul><li><a href=#1-dns-服务发现>1. <strong>DNS 服务发现</strong></a></li><li><a href=#2-clusterip内网访问>2. <strong>ClusterIP（内网访问）</strong></a></li><li><a href=#3-nodeport外部访问>3. <strong>NodePort（外部访问）</strong></a></li><li><a href=#4-loadbalancer云环境外部访问>4. <strong>LoadBalancer（云环境外部访问）</strong></a></li><li><a href=#5-externalname外部服务访问>5. <strong>ExternalName（外部服务访问）</strong></a></li><li><a href=#6-headless-service无头服务>6. <strong>Headless Service（无头服务）</strong></a></li><li><a href=#7-endpoints手动定义服务发现>7. <strong>Endpoints（手动定义服务发现）</strong></a></li><li><a href=#总结-12>总结</a></li></ul></li><li><a href=#pod几种常用状态>pod几种常用状态</a><ul><li><a href=#1-pending>1. <strong>Pending</strong></a></li><li><a href=#2-running>2. <strong>Running</strong></a></li><li><a href=#3-succeeded>3. <strong>Succeeded</strong></a></li><li><a href=#4-failed>4. <strong>Failed</strong></a></li><li><a href=#5-crashloopbackoff>5. <strong>CrashLoopBackOff</strong></a></li><li><a href=#6-unknown>6. <strong>Unknown</strong></a></li><li><a href=#7-terminating>7. <strong>Terminating</strong></a></li><li><a href=#8-waiting>8. <strong>Waiting</strong></a></li><li><a href=#总结-13>总结</a></li></ul></li><li><a href=#pod-生命周期的钩子函数>Pod 生命周期的钩子函数</a><ul><li><a href=#1-poststart>1. <strong>PostStart</strong></a></li><li><a href=#2-prestop>2. <strong>PreStop</strong></a></li><li><a href=#3-限制与注意事项>3. <strong>限制与注意事项</strong></a></li><li><a href=#4-钩子的执行方式>4. <strong>钩子的执行方式</strong></a></li><li><a href=#总结-14>总结</a></li></ul></li><li><a href=#calico和flannel区别>Calico和flannel区别☆</a><ul><li><a href=#1-架构设计>1. <strong>架构设计</strong></a></li><li><a href=#2-网络模型>2. <strong>网络模型</strong></a></li><li><a href=#3-网络策略network-policy>3. <strong>网络策略（Network Policy）</strong></a></li><li><a href=#4-性能>4. <strong>性能</strong></a></li><li><a href=#5-ip-地址管理>5. <strong>IP 地址管理</strong></a></li><li><a href=#6-扩展性>6. <strong>扩展性</strong></a></li><li><a href=#7-支持的环境>7. <strong>支持的环境</strong></a></li><li><a href=#总结-15>总结</a></li></ul></li><li><a href=#calico网络原理组网方式>calico网络原理、组网方式</a><ul><li><a href=#1-网络原理>1. <strong>网络原理</strong></a></li><li><a href=#2-calico-的组网方式>2. <strong>Calico 的组网方式</strong></a></li><li><a href=#3-calico-的主要组件>3. <strong>Calico 的主要组件</strong></a></li><li><a href=#4-calico-的优势>4. <strong>Calico 的优势</strong></a></li><li><a href=#总结-16>总结</a></li><li><a href=#ip-in-ip-模式的工作原理>IP-in-IP 模式的工作原理</a></li><li><a href=#ip-in-ip-模式的优缺点>IP-in-IP 模式的优缺点</a></li><li><a href=#使用场景>使用场景</a></li><li><a href=#配置-ip-in-ip-模式>配置 IP-in-IP 模式</a></li><li><a href=#总结-17>总结</a></li><li><a href=#vxlan-的工作原理>VXLAN 的工作原理</a></li><li><a href=#vxlan-模式的优缺点>VXLAN 模式的优缺点</a></li><li><a href=#vxlan-与-ip-in-ip-的对比>VXLAN 与 IP-in-IP 的对比</a></li><li><a href=#使用场景-1>使用场景</a></li><li><a href=#配置-vxlan-模式>配置 VXLAN 模式</a></li><li><a href=#总结-18>总结</a></li></ul></li><li><a href=#network-policy使用场景>Network Policy使用场景</a><ul><li><a href=#network-policy-的使用场景>Network Policy 的使用场景</a></li><li><a href=#示例定义-network-policy>示例：定义 Network Policy</a></li><li><a href=#网络策略的限制和注意事项>网络策略的限制和注意事项</a></li><li><a href=#总结-19>总结</a></li></ul></li><li><a href=#kubectl-exec-实现的原理>kubectl exec 实现的原理</a><ul><li><a href=#kubectl-exec-原理概述><code>kubectl exec</code> 原理概述</a></li><li><a href=#kubectl-exec-的组件与交互><code>kubectl exec</code> 的组件与交互</a></li><li><a href=#kubectl-exec-使用示例><code>kubectl exec</code> 使用示例</a></li><li><a href=#安全与权限控制>安全与权限控制</a></li><li><a href=#总结-20>总结</a></li></ul></li><li><a href=#cgroup中限制cpu的方式有哪些>cgroup中限制CPU的方式有哪些</a><ul><li><a href=#1-cpu-时间限制cpucfs_>1. <strong>CPU 时间限制（cpu.cfs_quota_us）</strong></a></li><li><a href=#2-cpu-核心限制cpucpus>2. <strong>CPU 核心限制（cpu.cpus）</strong></a></li><li><a href=#3-cpu-权重cpushares>3. <strong>CPU 权重（cpu.shares）</strong></a></li><li><a href=#4-cpu-限制优先级cpurt_>4. <strong>CPU 限制优先级（cpu.rt_runtime_us）</strong></a></li><li><a href=#5-cpu-子系统的-cfscompletely-fair-scheduler策略>5. <strong>CPU 子系统的 CFS（Completely Fair Scheduler）策略</strong></a></li><li><a href=#6-cpu-控制组调度器>6. <strong>CPU 控制组调度器（<code>cpuset</code>）</strong></a></li><li><a href=#总结-21>总结</a></li></ul></li><li><a href=#kubeconfig存放内容>kubeconfig存放内容</a><ul><li><a href=#kubeconfig-文件的结构><code>kubeconfig</code> 文件的结构</a></li><li><a href=#kubeconfig-文件示例><code>kubeconfig</code> 文件示例</a></li><li><a href=#kubeconfig-文件的关键字段><code>kubeconfig</code> 文件的关键字段</a></li><li><a href=#示例解析>示例解析</a></li><li><a href=#存放位置>存放位置</a></li><li><a href=#使用-kubectl-选择不同的-kubeconfig-文件>使用 <code>kubectl</code> 选择不同的 kubeconfig 文件</a></li><li><a href=#小结>小结</a></li></ul></li><li><a href=#pod-dns解析流程>pod DNS解析流程☆</a><ul><li><a href=#1-dns-配置>1. <strong>DNS 配置</strong></a></li><li><a href=#2-dns-查询流程>2. <strong>DNS 查询流程</strong></a></li><li><a href=#3-dns-解析示例>3. <strong>DNS 解析示例</strong></a></li><li><a href=#4-dns-解析规则>4. <strong>DNS 解析规则</strong></a></li><li><a href=#5-dns-服务的工作原理>5. <strong>DNS 服务的工作原理</strong></a></li><li><a href=#6-dns-解析优化与故障排查>6. <strong>DNS 解析优化与故障排查</strong></a></li><li><a href=#7-dns-解析的常见问题>7. <strong>DNS 解析的常见问题</strong></a></li><li><a href=#总结-22>总结</a></li></ul></li><li><a href=#traefik对比nginx-ingress优点>traefik对比nginx ingress优点</a><ul><li><a href=#1-traefik-的优点><strong>1. Traefik 的优点</strong></a></li><li><a href=#2-nginx-ingress-controller-的优点><strong>2. Nginx Ingress Controller 的优点</strong></a></li><li><a href=#总结-23><strong>总结：</strong></a></li></ul></li><li><a href=#harbor有哪些组件>Harbor有哪些组件</a><ul><li><a href=#1-core-核心服务>1. <strong>Core (核心服务)</strong></a></li><li><a href=#2-registry-镜像仓库>2. <strong>Registry (镜像仓库)</strong></a></li><li><a href=#3-portal-web-ui>3. <strong>Portal (Web UI)</strong></a></li><li><a href=#4-jobservice-任务服务>4. <strong>Jobservice (任务服务)</strong></a></li><li><a href=#5-notary-镜像签名>5. <strong>Notary (镜像签名)</strong></a></li><li><a href=#6-clair-镜像安全扫描>6. <strong>Clair (镜像安全扫描)</strong></a></li><li><a href=#7-chartmuseum-helm-仓库>7. <strong>Chartmuseum (Helm 仓库)</strong></a></li><li><a href=#8-log-日志系统>8. <strong>Log (日志系统)</strong></a></li><li><a href=#9-database-数据库>9. <strong>Database (数据库)</strong></a></li><li><a href=#10-redis-缓存服务>10. <strong>Redis (缓存服务)</strong></a></li><li><a href=#11-nginx-反向代理>11. <strong>NGINX (反向代理)</strong></a></li><li><a href=#总结-24><strong>总结：</strong></a></li></ul></li><li><a href=#harbor高可用怎么实现>Harbor高可用怎么实现</a><ul><li><a href=#1-harbor-组件的冗余设计>1. <strong>Harbor 组件的冗余设计</strong></a></li><li><a href=#2-高可用的部署架构>2. <strong>高可用的部署架构</strong></a></li><li><a href=#3-harbor-高可用的部署方案常见架构>3. <strong>Harbor 高可用的部署方案（常见架构）</strong></a></li><li><a href=#4-故障恢复与备份>4. <strong>故障恢复与备份</strong></a></li><li><a href=#总结-25>总结：</a></li></ul></li><li><a href=#etcd调优>ETCD调优</a><ul><li><a href=#1-硬件配置优化>1. <strong>硬件配置优化</strong></a></li><li><a href=#2-网络配置优化>2. <strong>网络配置优化</strong></a></li><li><a href=#3-etcd-配置参数调整>3. <strong>etcd 配置参数调整</strong></a></li><li><a href=#4-集群规模与拓扑设计>4. <strong>集群规模与拓扑设计</strong></a></li><li><a href=#5-监控与维护>5. <strong>监控与维护</strong></a></li><li><a href=#6-故障排查>6. <strong>故障排查</strong></a></li><li><a href=#总结-26>总结：</a></li></ul></li><li><a href=#假设k8s集群规模上千需要注意的问题有哪些>假设k8s集群规模上千，需要注意的问题有哪些？</a><ul><li><a href=#1-集群架构与资源规划>1. <strong>集群架构与资源规划</strong></a></li><li><a href=#2-资源管理与调度>2. <strong>资源管理与调度</strong></a></li><li><a href=#3-性能与可伸缩性>3. <strong>性能与可伸缩性</strong></a></li><li><a href=#4-高可用与容灾>4. <strong>高可用与容灾</strong></a></li><li><a href=#5-安全与合规>5. <strong>安全与合规</strong></a></li><li><a href=#6-故障排查与诊断>6. <strong>故障排查与诊断</strong></a></li><li><a href=#总结-27>总结</a></li><li><a href=#关键点总结>关键点总结：</a></li><li><a href=#总结-28>总结：</a></li></ul></li><li><a href=#节点notready可能的原因会导致哪些问题>节点NotReady可能的原因？会导致哪些问题？☆</a><ul><li><a href=#可能导致节点-notready-状态的原因>可能导致节点 <code>NotReady</code> 状态的原因</a></li><li><a href=#节点-notready-状态可能导致的问题>节点 <code>NotReady</code> 状态可能导致的问题</a></li><li><a href=#排查节点-notready-状态的思路>排查节点 <code>NotReady</code> 状态的思路</a></li><li><a href=#总结-29>总结</a></li></ul></li><li><a href=#service和endpoints是如何关联的>service和endpoints是如何关联的？</a><ul><li><a href=#1-service-的作用>1. <strong>Service 的作用</strong></a></li><li><a href=#2-endpoints-的作用>2. <strong>Endpoints 的作用</strong></a></li><li><a href=#3-service-和-endpoints-的关联方式>3. <strong>Service 和 Endpoints 的关联方式</strong></a></li><li><a href=#4-流程说明>4. <strong>流程说明</strong></a></li><li><a href=#5-endpoints-的更新>5. <strong>Endpoints 的更新</strong></a></li><li><a href=#6-手动管理-endpoints>6. <strong>手动管理 Endpoints</strong></a></li><li><a href=#7-集群外的访问>7. <strong>集群外的访问</strong></a></li><li><a href=#8-总结>8. <strong>总结：</strong></a></li></ul></li><li><a href=#replicasetdeployment功能是怎么实现的>ReplicaSet、Deployment功能是怎么实现的？</a><ul><li><a href=#1-replicaset-的功能和实现原理>1. <strong>ReplicaSet</strong> 的功能和实现原理</a></li><li><a href=#2-deployment-的功能和实现原理>2. <strong>Deployment</strong> 的功能和实现原理</a></li><li><a href=#3-replicaset-与-deployment-的区别>3. <strong>ReplicaSet 与 Deployment 的区别</strong></a></li><li><a href=#4-deployment-如何实现滚动更新>4. <strong>Deployment 如何实现滚动更新</strong></a></li><li><a href=#5-总结>5. <strong>总结</strong></a></li></ul></li><li><a href=#scheduler调度流程>scheduler调度流程</a><ul><li><a href=#1-调度器的工作流程概述>1. <strong>调度器的工作流程概述</strong></a></li><li><a href=#2-scheduler-调度流程的详细步骤>2. <strong>Scheduler 调度流程的详细步骤</strong></a></li><li><a href=#3-scheduler-插件体系>3. <strong>Scheduler 插件体系</strong></a></li><li><a href=#4-调度策略>4. <strong>调度策略</strong></a></li><li><a href=#5-总结-1>5. <strong>总结</strong></a></li></ul></li><li><a href=#hpa怎么实现的>HPA怎么实现的☆</a><ul><li><a href=#1-hpa-主要功能>1. <strong>HPA 主要功能</strong></a></li><li><a href=#2-hpa-工作原理>2. <strong>HPA 工作原理</strong></a></li><li><a href=#3-hpa-扩展和缩减的决策>3. <strong>HPA 扩展和缩减的决策</strong></a></li><li><a href=#4-hpa-相关的限制和注意事项>4. <strong>HPA 相关的限制和注意事项</strong></a></li><li><a href=#5-总结-2>5. <strong>总结</strong></a></li></ul></li><li><a href=#request-limit底层是怎么限制的>request limit底层是怎么限制的☆</a><ul><li><a href=#1-请求资源request与限制资源limit的定义>1. <strong>请求资源（request）与限制资源（limit）的定义</strong></a></li><li><a href=#2-底层原理cgroup-和容器运行时>2. <strong>底层原理：cgroup 和容器运行时</strong></a></li><li><a href=#3-调度器如何使用-request-和-limit>3. <strong>调度器如何使用 request 和 limit</strong></a></li><li><a href=#4-总结>4. <strong>总结</strong></a></li></ul></li><li><a href=#helm工作原理是什么>helm工作原理是什么？</a><ul><li><a href=#helm-的核心组件>Helm 的核心组件</a></li><li><a href=#helm-工作原理>Helm 工作原理</a></li><li><a href=#helm-的模板渲染>Helm 的模板渲染</a></li><li><a href=#helm-的优势>Helm 的优势</a></li><li><a href=#helm-与-kubernetes-资源的关系>Helm 与 Kubernetes 资源的关系</a></li><li><a href=#总结-30>总结</a></li></ul></li><li><a href=#helm-chart-rollback实现过程是什么>helm chart rollback实现过程是什么？</a><ul><li><a href=#helm-chart-rollback-实现过程>Helm Chart Rollback 实现过程</a></li><li><a href=#1-helm-release-的版本管理>1. <strong>Helm Release 的版本管理</strong></a></li><li><a href=#2-执行-rollback-操作>2. <strong>执行 Rollback 操作</strong></a></li><li><a href=#3-helm-rollback-中的关键操作>3. <strong>Helm Rollback 中的关键操作</strong></a></li><li><a href=#4-helm-rollback>4. <strong><code>helm rollback</code> 示例</strong></a></li><li><a href=#5-rollback-时的注意事项>5. <strong>RollBack 时的注意事项</strong></a></li><li><a href=#6-helm-回滚工作原理总结>6. <strong>Helm 回滚工作原理总结</strong></a></li></ul></li><li><a href=#velero备份与恢复流程是什么>velero备份与恢复流程是什么</a><ul><li><a href=#1-velero-架构概述>1. <strong>Velero 架构概述</strong></a></li><li><a href=#2-velero-备份流程>2. <strong>Velero 备份流程</strong></a></li><li><a href=#3-velero-恢复流程>3. <strong>Velero 恢复流程</strong></a></li><li><a href=#4-velero-的增量备份和恢复>4. <strong>Velero 的增量备份和恢复</strong></a></li><li><a href=#5-velero-的高级特性>5. <strong>Velero 的高级特性</strong></a></li><li><a href=#6-常见命令汇总>6. <strong>常见命令汇总</strong></a></li><li><a href=#7-总结>7. <strong>总结</strong></a></li></ul></li><li><a href=#docker网络模式>docker网络模式</a><ul><li><a href=#1-bridge-网络模式>1. <strong>Bridge 网络模式</strong></a></li><li><a href=#2-host-网络模式>2. <strong>Host 网络模式</strong></a></li><li><a href=#3-none-网络模式>3. <strong>None 网络模式</strong></a></li><li><a href=#4-container-网络模式>4. <strong>Container 网络模式</strong></a></li><li><a href=#5-overlay-网络模式>5. <strong>Overlay 网络模式</strong></a></li><li><a href=#6-macvlan-网络模式>6. <strong>Macvlan 网络模式</strong></a></li><li><a href=#7-ipvlan-网络模式>7. <strong>IPvlan 网络模式</strong></a></li><li><a href=#8-docker-网络模式总结>8. <strong>Docker 网络模式总结</strong></a></li></ul></li><li><a href=#docker和container区别>docker和container区别☆</a><ul><li><a href=#1-容器container>1. <strong>容器（Container）</strong></a></li><li><a href=#2-docker>2. <strong>Docker</strong></a></li><li><a href=#3-docker-与容器的区别>3. <strong>Docker 与容器的区别</strong></a></li><li><a href=#4-总结-1>4. <strong>总结</strong></a></li></ul></li><li><a href=#如何减dockerfile成镜像体积>如何减⼩dockerfile⽣成镜像体积？</a><ul><li><a href=#1-选择合适的基础镜像>1. <strong>选择合适的基础镜像</strong></a></li><li><a href=#2-减少不必要的文件>2. <strong>减少不必要的文件</strong></a></li><li><a href=#3-合并多个>3. <strong>合并多个 <code>RUN</code> 命令</strong></a></li><li><a href=#4-清理临时文件和缓存>4. <strong>清理临时文件和缓存</strong></a></li><li><a href=#5-使用多阶段构建multi-stage-builds>5. <strong>使用多阶段构建（Multi-stage builds）</strong></a></li><li><a href=#6-避免安装不必要的依赖>6. <strong>避免安装不必要的依赖</strong></a></li><li><a href=#7-压缩镜像文件>7. <strong>压缩镜像文件</strong></a></li><li><a href=#8-减少镜像中的层数>8. <strong>减少镜像中的层数</strong></a></li><li><a href=#9-优化文件格式和大小>9. <strong>优化文件格式和大小</strong></a></li><li><a href=#10-使用-docker-buildkit-和缓存机制>10. <strong>使用 Docker BuildKit 和缓存机制</strong></a></li></ul></li><li><a href=#k8s日志采集方案>k8s日志采集方案</a><ul><li><a href=#1-kubernetes-日志来源>1. <strong>Kubernetes 日志来源</strong></a></li><li><a href=#2-常见的-kubernetes-日志采集方案>2. <strong>常见的 Kubernetes 日志采集方案</strong></a></li><li><a href=#3-日志采集方案的选型>3. <strong>日志采集方案的选型</strong></a></li><li><a href=#4-日志采集最佳实践>4. <strong>日志采集最佳实践</strong></a></li></ul></li><li><a href=#pause容器的用途>Pause容器的用途☆</a><ul><li><a href=#pause-容器的主要用途>Pause 容器的主要用途</a></li><li><a href=#如何启动-pause-容器>如何启动 Pause 容器</a></li><li><a href=#典型的-pause-容器镜像>典型的 Pause 容器镜像</a></li><li><a href=#总结-31>总结</a></li></ul></li><li><a href=#k8s证书过期怎么更新>k8s证书过期怎么更新</a><ul><li><a href=#1-检查证书的有效期>1. <strong>检查证书的有效期</strong></a></li><li><a href=#2-使用>2. <strong>使用 <code>kubeadm</code> 更新证书</strong></a></li><li><a href=#3-手动更新证书>3. <strong>手动更新证书</strong></a></li><li><a href=#4-kubernetes-ca-证书的更新>4. <strong>Kubernetes CA 证书的更新</strong></a></li><li><a href=#5-automating-certificate-rotation>5. <strong>Automating Certificate Rotation</strong></a></li><li><a href=#6-检查证书更新后的状态>6. <strong>检查证书更新后的状态</strong></a></li><li><a href=#总结-32>总结</a></li></ul></li><li><a href=#k8s-qos等级>K8S QoS等级☆</a><ul><li><a href=#kubernetes-qos-等级>Kubernetes QoS 等级</a></li><li><a href=#1-guaranteed保证>1. <strong>Guaranteed（保证）</strong></a></li><li><a href=#2-burstable突发>2. <strong>Burstable（突发）</strong></a></li><li><a href=#3-besteffort尽力而为>3. <strong>BestEffort（尽力而为）</strong></a></li><li><a href=#qos-影响>QoS 影响</a></li><li><a href=#总结-33>总结</a></li></ul></li><li><a href=#k8s节点维护注意事项>k8s节点维护注意事项</a><ul><li><a href=#1-确保高可用性>1. <strong>确保高可用性</strong></a></li><li><a href=#2-检查节点状态>2. <strong>检查节点状态</strong></a></li><li><a href=#3-将节点从调度池中移除>3. <strong>将节点从调度池中移除</strong></a></li><li><a href=#4-迁移-pod-到其他节点>4. <strong>迁移 Pod 到其他节点</strong></a></li><li><a href=#5-执行节点维护操作>5. <strong>执行节点维护操作</strong></a></li><li><a href=#6-重新加入节点>6. <strong>重新加入节点</strong></a></li><li><a href=#7-验证节点恢复>7. <strong>验证节点恢复</strong></a></li><li><a href=#8-检查-pod-的健康状态>8. <strong>检查 Pod 的健康状态</strong></a></li><li><a href=#9-监控与日志>9. <strong>监控与日志</strong></a></li><li><a href=#10-通知相关人员>10. <strong>通知相关人员</strong></a></li><li><a href=#11-升级节点版本>11. <strong>升级节点版本</strong></a></li><li><a href=#12-更新节点的-kubelet-配置>12. <strong>更新节点的 kubelet 配置</strong></a></li><li><a href=#13-定期执行节点健康检查>13. <strong>定期执行节点健康检查</strong></a></li><li><a href=#总结-34>总结</a></li></ul></li><li><a href=#headless-service和clusterip区别>Headless Service和ClusterIP区别☆</a><ul><li><a href=#1-clusterip-service>1. <strong>ClusterIP Service</strong></a></li><li><a href=#2-headless-service>2. <strong>Headless Service</strong></a></li><li><a href=#clusterip-与-headless-service-的区别><strong>ClusterIP 与 Headless Service 的区别</strong></a></li><li><a href=#总结-35>总结</a></li></ul></li><li><a href=#不进行负载均衡直接访问每个-pod-比如数据库有三个实例pod用headless的模式那么应用连接这个数据库的方式是啥三个不同的svc>不进行负载均衡，直接访问每个 Pod？ 比如数据库有三个实例pod用headless的模式，那么应用连接这个数据库的方式是啥？三个不同的svc？</a><ul><li><a href=#1-通过-dns-直接访问每个-pod>1. <strong>通过 DNS 直接访问每个 Pod</strong></a></li><li><a href=#2-通过多个-service-访问每个实例>2. <strong>通过多个 Service 访问每个实例</strong></a></li><li><a href=#适用场景-1>适用场景</a></li><li><a href=#总结-36>总结</a></li></ul></li><li><a href=#db-0dbdefaultsvcclusterlocal-db-1dbdefaultsvcclusterlocal-db-2dbdefaultsvcclusterlocal-这不就是3个不同svc吗>db-0.db.default.svc.cluster.local db-1.db.default.svc.cluster.local db-2.db.default.svc.cluster.local 这不就是3个不同svc吗</a><ul><li><a href=#headless-service-和-pod-的-dns-解析><strong>Headless Service 和 Pod 的 DNS 解析</strong></a></li><li><a href=#为什么这些看起来像是多个服务><strong>为什么这些看起来像是多个服务</strong></a></li><li><a href=#总结-37><strong>总结</strong></a></li></ul></li><li><a href=#在-headless-service-中实际上-并没有三个不同的-service而是-一个-service但它会为每个-pod-分配一个独立的-dns-记录那这个一个svc是啥>在 Headless Service 中，实际上 并没有三个不同的 Service，而是 一个 Service，但它会为每个 Pod 分配一个独立的 DNS 记录，那这个一个svc是啥</a><ul><li><a href=#具体来说>具体来说：</a></li><li><a href=#举个例子>举个例子：</a></li><li><a href=#所以这个一个-service-是>所以，<strong>这个一个 Service</strong> 是：</a></li><li><a href=#结论-1>结论：</a></li></ul></li><li><a href=#linux容器技术的基础原理>Linux容器技术的基础原理</a><ul><li><a href=#1-命名空间namespaces>1. <strong>命名空间（Namespaces）</strong></a></li><li><a href=#2-控制组cgroups>2. <strong>控制组（cgroups）</strong></a></li><li><a href=#3-联合文件系统union-file-system>3. <strong>联合文件系统（Union File System）</strong></a></li><li><a href=#4-容器镜像container-images>4. <strong>容器镜像（Container Images）</strong></a></li><li><a href=#5-容器的隔离性与安全性>5. <strong>容器的隔离性与安全性</strong></a></li><li><a href=#总结-38>总结</a></li></ul></li><li><a href=#kubernetes-pod的常见调度方式>Kubernetes Pod的常见调度方式</a><ul><li><a href=#常见的-pod-调度方式>常见的 Pod 调度方式</a></li><li><a href=#8-daemonset>8. <strong>DaemonSet</strong></a></li><li><a href=#9-静态和动态调度static-and-dynamic-scheduling>9. <strong>静态和动态调度（Static and Dynamic Scheduling）</strong></a></li><li><a href=#10-资源调度策略>10. <strong>资源调度策略</strong></a></li><li><a href=#总结-39>总结：</a></li></ul></li><li><a href=#kubernetes-ingress原理>kubernetes Ingress原理☆</a><ul><li><a href=#kubernetes-ingress-原理>Kubernetes Ingress 原理</a></li><li><a href=#1-ingress-资源>1. <strong>Ingress 资源</strong></a></li><li><a href=#2-ingress-controller>2. <strong>Ingress Controller</strong></a></li><li><a href=#3-ingress-的工作原理>3. <strong>Ingress 的工作原理</strong></a></li><li><a href=#4-ingress-的特性>4. <strong>Ingress 的特性</strong></a></li><li><a href=#5-ingress-与-service-的关系>5. <strong>Ingress 与 Service 的关系</strong></a></li><li><a href=#6-ingress-的使用场景>6. <strong>Ingress 的使用场景</strong></a></li><li><a href=#7-ingress-controller-部署>7. <strong>Ingress Controller 部署</strong></a></li><li><a href=#8-常见的-ingress-controller>8. <strong>常见的 Ingress Controller</strong></a></li><li><a href=#总结-40>总结</a></li></ul></li><li><a href=#kubernetes各模块如何与api-server通信>Kubernetes各模块如何与API Server通信</a><ul><li><a href=#各模块与-api-server-的通信方式>各模块与 API Server 的通信方式</a></li><li><a href=#具体通信流程>具体通信流程</a></li><li><a href=#总结-41>总结</a></li></ul></li><li><a href=#kubelet监控worker节点如何实现>kubelet监控worker节点如何实现</a><ul><li><a href=#1-kubelet-的健康检查>1. <strong>kubelet 的健康检查</strong></a></li><li><a href=#2-kubelet-的指标暴露>2. <strong>kubelet 的指标暴露</strong></a></li><li><a href=#3-资源使用监控>3. <strong>资源使用监控</strong></a></li><li><a href=#4-事件记录>4. <strong>事件记录</strong></a></li><li><a href=#5-集群级别的监控系统>5. <strong>集群级别的监控系统</strong></a></li><li><a href=#6-node-conditions节点状态>6. <strong>Node Conditions（节点状态）</strong></a></li><li><a href=#7-资源调度与驱逐>7. <strong>资源调度与驱逐</strong></a></li><li><a href=#8-故障监控与通知>8. <strong>故障监控与通知</strong></a></li><li><a href=#9-日志监控>9. <strong>日志监控</strong></a></li><li><a href=#10-系统级监控>10. <strong>系统级监控</strong></a></li><li><a href=#总结-42>总结</a></li></ul></li><li><a href=#容器时区不一致如何解决>容器时区不一致如何解决？</a><ul><li><a href=#1-使用宿主机的时区>1. <strong>使用宿主机的时区</strong></a></li><li><a href=#2-设置容器内的时区>2. <strong>设置容器内的时区</strong></a></li><li><a href=#3-使用>3. <strong>使用 <code>UTC</code> 时区</strong></a></li><li><a href=#4-使用-docker-compose-或-kubernetes-的-configmap>4. <strong>使用 Docker Compose 或 Kubernetes 的 ConfigMap</strong></a></li><li><a href=#总结-43>总结</a></li></ul></li></ul></nav></div></aside></main></body></html>