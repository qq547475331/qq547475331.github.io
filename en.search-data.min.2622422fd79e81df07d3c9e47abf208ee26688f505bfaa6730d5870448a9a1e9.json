[{"id":0,"href":"/docs/example/","title":"Example Site","section":"Docs","content":" Introduction # Ferre hinnitibus erat accipitrem dixi Troiae tollens # Lorem markdownum, a quoque nutu est quodcumque mandasset veluti. Passim inportuna totidemque nympha fert; repetens pendent, poenarum guttura sed vacet non, mortali undas. Omnis pharetramque gramen portentificisque membris servatum novabis fallit de nubibus atque silvas mihi. Dixit repetitaque Quid; verrit longa; sententia mandat quascumque nescio solebat litore; noctes. Hostem haerentem circuit plenaque tamen.\nPedum ne indigenae finire invergens carpebat Velit posses summoque De fumos illa foret Est simul fameque tauri qua ad # Locum nullus nisi vomentes. Ab Persea sermone vela, miratur aratro; eandem Argolicas gener.\nMe sol # Nec dis certa fuit socer, Nonacria dies manet tacitaque sibi? Sucis est iactata Castrumque iudex, et iactato quoque terraeque es tandem et maternos vittis. Lumina litus bene poenamque animos callem ne tuas in leones illam dea cadunt genus, et pleno nunc in quod. Anumque crescentesque sanguinis progenies nuribus rustica tinguet. Pater omnes liquido creditis noctem.\nif (mirrored(icmp_dvd_pim, 3, smbMirroredHard) != lion(clickImportQueue, viralItunesBalancing, bankruptcy_file_pptp)) { file += ip_cybercrime_suffix; } if (runtimeSmartRom == netMarketingWord) { virusBalancingWin *= scriptPromptBespoke + raster(post_drive, windowsSli); cd = address_hertz_trojan; soap_ccd.pcbServerGigahertz(asp_hardware_isa, offlinePeopleware, nui); } else { megabyte.api = modem_flowchart - web + syntaxHalftoneAddress; } if (3 \u0026lt; mebibyteNetworkAnimated) { pharming_regular_error *= jsp_ribbon + algorithm * recycleMediaKindle( dvrSyntax, cdma); adf_sla *= hoverCropDrive; templateNtfs = -1 - vertical; } else { expressionCompressionVariable.bootMulti = white_eup_javascript( table_suffix); guidPpiPram.tracerouteLinux += rtfTerabyteQuicktime(1, managementRosetta(webcamActivex), 740874); } var virusTweetSsl = nullGigo; Trepident sitimque # Sentiet et ferali errorem fessam, coercet superbus, Ascaniumque in pennis mediis; dolor? Vidit imi Aeacon perfida propositos adde, tua Somni Fluctibus errante lustrat non.\nTamen inde, vos videt e flammis Scythica parantem rupisque pectora umbras. Haec ficta canistris repercusso simul ego aris Dixit! Esse Fama trepidare hunc crescendo vigor ululasse vertice exspatiantur celer tepidique petita aversata oculis iussa est me ferro.\n"},{"id":1,"href":"/docs/example/table-of-contents/with-toc/","title":"With ToC","section":"Table of Contents","content":" Caput vino delphine in tamen vias # Cognita laeva illo fracta # Lorem markdownum pavent auras, surgit nunc cingentibus libet Laomedonque que est. Pastor An arbor filia foedat, ne fugit aliter, per. Helicona illas et callida neptem est Oresitrophos caput, dentibus est venit. Tenet reddite famuli praesentem fortibus, quaeque vis foret si frondes gelidos gravidae circumtulit inpulit armenta nativum.\nTe at cruciabere vides rubentis manebo Maturuit in praetemptat ruborem ignara postquam habitasse Subitarum supplevit quoque fontesque venabula spretis modo Montis tot est mali quasque gravis Quinquennem domus arsit ipse Pellem turis pugnabant locavit Natus quaerere # Pectora et sine mulcere, coniuge dum tincta incurvae. Quis iam; est dextra Peneosque, metuis a verba, primo. Illa sed colloque suis: magno: gramen, aera excutiunt concipit.\nPhrygiae petendo suisque extimuit, super, pars quod audet! Turba negarem. Fuerat attonitus; et dextra retinet sidera ulnas undas instimulat vacuae generis? Agnus dabat et ignotis dextera, sic tibi pacis feriente at mora euhoeque comites hostem vestras Phineus. Vultuque sanguine dominoque metuit risi fama vergit summaque meus clarissimus artesque tinguebat successor nominis cervice caelicolae.\nLimitibus misere sit # Aurea non fata repertis praerupit feruntur simul, meae hosti lentaque citius levibus, cum sede dixit, Phaethon texta. Albentibus summos multifidasque iungitur loquendi an pectore, mihi ursaque omnia adfata, aeno parvumque in animi perlucentes. Epytus agis ait vixque clamat ornum adversam spondet, quid sceptra ipsum est. Reseret nec; saeva suo passu debentia linguam terga et aures et cervix de ubera. Coercet gelidumque manus, doluit volvitur induta?\nEnim sua # Iuvenilior filia inlustre templa quidem herbis permittat trahens huic. In cruribus proceres sole crescitque fata, quos quos; merui maris se non tamen in, mea.\nGermana aves pignus tecta # Mortalia rudibusque caelum cognosceret tantum aquis redito felicior texit, nec, aris parvo acre. Me parum contulerant multi tenentem, gratissime suis; vultum tu occupat deficeret corpora, sonum. E Actaea inplevit Phinea concepit nomenque potest sanguine captam nulla et, in duxisses campis non; mercede. Dicere cur Leucothoen obitum?\nPostibus mittam est nubibus principium pluma, exsecratur facta et. Iunge Mnemonidas pallamque pars; vere restitit alis flumina quae quoque, est ignara infestus Pyrrha. Di ducis terris maculatum At sede praemia manes nullaque!\n"},{"id":2,"href":"/docs/example/table-of-contents/without-toc/","title":"Without ToC","section":"Table of Contents","content":" At me ipso nepotibus nunc celebratior genus # Tanto oblite # Lorem markdownum pectora novis patenti igne sua opus aurae feras materiaque illic demersit imago et aristas questaque posset. Vomit quoque suo inhaesuro clara. Esse cumque, per referri triste. Ut exponit solisque communis in tendens vincetis agisque iamque huic bene ante vetat omina Thebae rates. Aeacus servat admonitu concidit, ad resimas vultus et rugas vultu dignamque Siphnon.\nQuam iugulum regia simulacra, plus meruit humo pecorumque haesit, ab discedunt dixit: ritu pharetramque. Exul Laurenti orantem modo, per densum missisque labor manibus non colla unum, obiectat. Tu pervia collo, fessus quae Cretenque Myconon crate! Tegumenque quae invisi sudore per vocari quaque plus ventis fluidos. Nodo perque, fugisse pectora sorores.\nSumme promissa supple vadit lenius # Quibus largis latebris aethera versato est, ait sentiat faciemque. Aequata alis nec Caeneus exululat inclite corpus est, ire tibi ostendens et tibi. Rigent et vires dique possent lumina; eadem dixit poma funeribus paret et felix reddebant ventis utile lignum.\nRemansit notam Stygia feroxque Et dabit materna Vipereas Phrygiaeque umbram sollicito cruore conlucere suus Quarum Elis corniger Nec ieiunia dixit Vertitur mos ortu ramosam contudit dumque; placabat ac lumen. Coniunx Amoris spatium poenamque cavernis Thebae Pleiadasque ponunt, rapiare cum quae parum nimium rima.\nQuidem resupinus inducto solebat una facinus quae # Credulitas iniqua praepetibus paruit prospexit, voce poena, sub rupit sinuatur, quin suum ventorumque arcadiae priori. Soporiferam erat formamque, fecit, invergens, nymphae mutat fessas ait finge.\nBaculum mandataque ne addere capiti violentior Altera duas quam hoc ille tenues inquit Sicula sidereus latrantis domoque ratae polluit comites Possit oro clausura namque se nunc iuvenisque Faciem posuit Quodque cum ponunt novercae nata vestrae aratra Ite extrema Phrygiis, patre dentibus, tonso perculit, enim blanda, manibus fide quos caput armis, posse! Nocendo fas Alcyonae lacertis structa ferarum manus fulmen dubius, saxa caelum effuge extremis fixum tumor adfecit bella, potentes? Dum nec insidiosa tempora tegit spirarunt. Per lupi pars foliis, porreximus humum negant sunt subposuere Sidone steterant auro. Memoraverit sine: ferrum idem Orion caelum heres gerebat fixis?\n"},{"id":3,"href":"/posts/creating-a-new-theme/","title":"Creating a New Theme","section":"Blog","content":" Introduction # This tutorial will show you how to create a simple theme in Hugo. I assume that you are familiar with HTML, the bash command line, and that you are comfortable using Markdown to format content. I\u0026rsquo;ll explain how Hugo uses templates and how you can organize your templates to create a theme. I won\u0026rsquo;t cover using CSS to style your theme.\nWe\u0026rsquo;ll start with creating a new site with a very basic template. Then we\u0026rsquo;ll add in a few pages and posts. With small variations on that, you will be able to create many different types of web sites.\nIn this tutorial, commands that you enter will start with the \u0026ldquo;$\u0026rdquo; prompt. The output will follow. Lines that start with \u0026ldquo;#\u0026rdquo; are comments that I\u0026rsquo;ve added to explain a point. When I show updates to a file, the \u0026ldquo;:wq\u0026rdquo; on the last line means to save the file.\nHere\u0026rsquo;s an example:\n## this is a comment $ echo this is a command this is a command ## edit the file $ vi foo.md +++ date = \u0026#34;2014-09-28\u0026#34; title = \u0026#34;creating a new theme\u0026#34; +++ bah and humbug :wq ## show it $ cat foo.md +++ date = \u0026#34;2014-09-28\u0026#34; title = \u0026#34;creating a new theme\u0026#34; +++ bah and humbug $ Some Definitions # There are a few concepts that you need to understand before creating a theme.\nSkins # Skins are the files responsible for the look and feel of your site. It’s the CSS that controls colors and fonts, it’s the Javascript that determines actions and reactions. It’s also the rules that Hugo uses to transform your content into the HTML that the site will serve to visitors.\nYou have two ways to create a skin. The simplest way is to create it in the layouts/ directory. If you do, then you don’t have to worry about configuring Hugo to recognize it. The first place that Hugo will look for rules and files is in the layouts/ directory so it will always find the skin.\nYour second choice is to create it in a sub-directory of the themes/ directory. If you do, then you must always tell Hugo where to search for the skin. It’s extra work, though, so why bother with it?\nThe difference between creating a skin in layouts/ and creating it in themes/ is very subtle. A skin in layouts/ can’t be customized without updating the templates and static files that it is built from. A skin created in themes/, on the other hand, can be and that makes it easier for other people to use it.\nThe rest of this tutorial will call a skin created in the themes/ directory a theme.\nNote that you can use this tutorial to create a skin in the layouts/ directory if you wish to. The main difference will be that you won’t need to update the site’s configuration file to use a theme.\nThe Home Page # The home page, or landing page, is the first page that many visitors to a site see. It is the index.html file in the root directory of the web site. Since Hugo writes files to the public/ directory, our home page is public/index.html.\nSite Configuration File # When Hugo runs, it looks for a configuration file that contains settings that override default values for the entire site. The file can use TOML, YAML, or JSON. I prefer to use TOML for my configuration files. If you prefer to use JSON or YAML, you’ll need to translate my examples. You’ll also need to change the name of the file since Hugo uses the extension to determine how to process it.\nHugo translates Markdown files into HTML. By default, Hugo expects to find Markdown files in your content/ directory and template files in your themes/ directory. It will create HTML files in your public/ directory. You can change this by specifying alternate locations in the configuration file.\nContent # Content is stored in text files that contain two sections. The first section is the “front matter,” which is the meta-information on the content. The second section contains Markdown that will be converted to HTML.\nFront Matter # The front matter is information about the content. Like the configuration file, it can be written in TOML, YAML, or JSON. Unlike the configuration file, Hugo doesn’t use the file’s extension to know the format. It looks for markers to signal the type. TOML is surrounded by “+++”, YAML by “---”, and JSON is enclosed in curly braces. I prefer to use TOML, so you’ll need to translate my examples if you prefer YAML or JSON.\nThe information in the front matter is passed into the template before the content is rendered into HTML.\nMarkdown # Content is written in Markdown which makes it easier to create the content. Hugo runs the content through a Markdown engine to create the HTML which will be written to the output file.\nTemplate Files # Hugo uses template files to render content into HTML. Template files are a bridge between the content and presentation. Rules in the template define what content is published, where it\u0026rsquo;s published to, and how it will rendered to the HTML file. The template guides the presentation by specifying the style to use.\nThere are three types of templates: single, list, and partial. Each type takes a bit of content as input and transforms it based on the commands in the template.\nHugo uses its knowledge of the content to find the template file used to render the content. If it can’t find a template that is an exact match for the content, it will shift up a level and search from there. It will continue to do so until it finds a matching template or runs out of templates to try. If it can’t find a template, it will use the default template for the site.\nPlease note that you can use the front matter to influence Hugo’s choice of templates.\nSingle Template # A single template is used to render a single piece of content. For example, an article or post would be a single piece of content and use a single template.\nList Template # A list template renders a group of related content. That could be a summary of recent postings or all articles in a category. List templates can contain multiple groups.\nThe homepage template is a special type of list template. Hugo assumes that the home page of your site will act as the portal for the rest of the content in the site.\nPartial Template # A partial template is a template that can be included in other templates. Partial templates must be called using the “partial” template command. They are very handy for rolling up common behavior. For example, your site may have a banner that all pages use. Instead of copying the text of the banner into every single and list template, you could create a partial with the banner in it. That way if you decide to change the banner, you only have to change the partial template.\nCreate a New Site # Let\u0026rsquo;s use Hugo to create a new web site. I\u0026rsquo;m a Mac user, so I\u0026rsquo;ll create mine in my home directory, in the Sites folder. If you\u0026rsquo;re using Linux, you might have to create the folder first.\nThe \u0026ldquo;new site\u0026rdquo; command will create a skeleton of a site. It will give you the basic directory structure and a useable configuration file.\n$ hugo new site ~/Sites/zafta $ cd ~/Sites/zafta $ ls -l total 8 drwxr-xr-x 7 quoha staff 238 Sep 29 16:49 . drwxr-xr-x 3 quoha staff 102 Sep 29 16:49 .. drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 archetypes -rw-r--r-- 1 quoha staff 82 Sep 29 16:49 config.toml drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 content drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 layouts drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 static $ Take a look in the content/ directory to confirm that it is empty.\nThe other directories (archetypes/, layouts/, and static/) are used when customizing a theme. That\u0026rsquo;s a topic for a different tutorial, so please ignore them for now.\nGenerate the HTML For the New Site # Running the hugo command with no options will read all the available content and generate the HTML files. It will also copy all static files (that\u0026rsquo;s everything that\u0026rsquo;s not content). Since we have an empty site, it won\u0026rsquo;t do much, but it will do it very quickly.\n$ hugo --verbose INFO: 2014/09/29 Using config file: config.toml INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/ WARN: 2014/09/29 Unable to locate layout: [index.html _default/list.html _default/single.html] WARN: 2014/09/29 Unable to locate layout: [404.html] 0 draft content 0 future content 0 pages created 0 tags created 0 categories created in 2 ms $ The \u0026ldquo;--verbose\u0026rdquo; flag gives extra information that will be helpful when we build the template. Every line of the output that starts with \u0026ldquo;INFO:\u0026rdquo; or \u0026ldquo;WARN:\u0026rdquo; is present because we used that flag. The lines that start with \u0026ldquo;WARN:\u0026rdquo; are warning messages. We\u0026rsquo;ll go over them later.\nWe can verify that the command worked by looking at the directory again.\n$ ls -l total 8 drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 archetypes -rw-r--r-- 1 quoha staff 82 Sep 29 16:49 config.toml drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 content drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 layouts drwxr-xr-x 4 quoha staff 136 Sep 29 17:02 public drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 static $ See that new public/ directory? Hugo placed all generated content there. When you\u0026rsquo;re ready to publish your web site, that\u0026rsquo;s the place to start. For now, though, let\u0026rsquo;s just confirm that we have what we\u0026rsquo;d expect from a site with no content.\n$ ls -l public total 16 -rw-r--r-- 1 quoha staff 416 Sep 29 17:02 index.xml -rw-r--r-- 1 quoha staff 262 Sep 29 17:02 sitemap.xml $ Hugo created two XML files, which is standard, but there are no HTML files.\nTest the New Site # Verify that you can run the built-in web server. It will dramatically shorten your development cycle if you do. Start it by running the \u0026ldquo;server\u0026rdquo; command. If it is successful, you will see output similar to the following:\n$ hugo server --verbose INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/ WARN: 2014/09/29 Unable to locate layout: [index.html _default/list.html _default/single.html] WARN: 2014/09/29 Unable to locate layout: [404.html] 0 draft content 0 future content 0 pages created 0 tags created 0 categories created in 2 ms Serving pages from /Users/quoha/Sites/zafta/public Web Server is available at http://localhost:1313 Press Ctrl+C to stop Connect to the listed URL (it\u0026rsquo;s on the line that starts with \u0026ldquo;Web Server\u0026rdquo;). If everything is working correctly, you should get a page that shows the following:\nindex.xml sitemap.xml That\u0026rsquo;s a listing of your public/ directory. Hugo didn\u0026rsquo;t create a home page because our site has no content. When there\u0026rsquo;s no index.html file in a directory, the server lists the files in the directory, which is what you should see in your browser.\nLet’s go back and look at those warnings again.\nWARN: 2014/09/29 Unable to locate layout: [index.html _default/list.html _default/single.html] WARN: 2014/09/29 Unable to locate layout: [404.html] That second warning is easier to explain. We haven’t created a template to be used to generate “page not found errors.” The 404 message is a topic for a separate tutorial.\nNow for the first warning. It is for the home page. You can tell because the first layout that it looked for was “index.html.” That’s only used by the home page.\nI like that the verbose flag causes Hugo to list the files that it\u0026rsquo;s searching for. For the home page, they are index.html, _default/list.html, and _default/single.html. There are some rules that we\u0026rsquo;ll cover later that explain the names and paths. For now, just remember that Hugo couldn\u0026rsquo;t find a template for the home page and it told you so.\nAt this point, you\u0026rsquo;ve got a working installation and site that we can build upon. All that’s left is to add some content and a theme to display it.\nCreate a New Theme # Hugo doesn\u0026rsquo;t ship with a default theme. There are a few available (I counted a dozen when I first installed Hugo) and Hugo comes with a command to create new themes.\nWe\u0026rsquo;re going to create a new theme called \u0026ldquo;zafta.\u0026rdquo; Since the goal of this tutorial is to show you how to fill out the files to pull in your content, the theme will not contain any CSS. In other words, ugly but functional.\nAll themes have opinions on content and layout. For example, Zafta uses \u0026ldquo;post\u0026rdquo; over \u0026ldquo;blog\u0026rdquo;. Strong opinions make for simpler templates but differing opinions make it tougher to use themes. When you build a theme, consider using the terms that other themes do.\nCreate a Skeleton # Use the hugo \u0026ldquo;new\u0026rdquo; command to create the skeleton of a theme. This creates the directory structure and places empty files for you to fill out.\n$ hugo new theme zafta $ ls -l total 8 drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 archetypes -rw-r--r-- 1 quoha staff 82 Sep 29 16:49 config.toml drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 content drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 layouts drwxr-xr-x 4 quoha staff 136 Sep 29 17:02 public drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 static drwxr-xr-x 3 quoha staff 102 Sep 29 17:31 themes $ find themes -type f | xargs ls -l -rw-r--r-- 1 quoha staff 1081 Sep 29 17:31 themes/zafta/LICENSE.md -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/archetypes/default.md -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/_default/list.html -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/_default/single.html -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/partials/footer.html -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/partials/header.html -rw-r--r-- 1 quoha staff 93 Sep 29 17:31 themes/zafta/theme.toml $ The skeleton includes templates (the files ending in .html), license file, a description of your theme (the theme.toml file), and an empty archetype.\nPlease take a minute to fill out the theme.toml and LICENSE.md files. They\u0026rsquo;re optional, but if you\u0026rsquo;re going to be distributing your theme, it tells the world who to praise (or blame). It\u0026rsquo;s also nice to declare the license so that people will know how they can use the theme.\n$ vi themes/zafta/theme.toml author = \u0026#34;michael d henderson\u0026#34; description = \u0026#34;a minimal working template\u0026#34; license = \u0026#34;MIT\u0026#34; name = \u0026#34;zafta\u0026#34; source_repo = \u0026#34;\u0026#34; tags = [\u0026#34;tags\u0026#34;, \u0026#34;categories\u0026#34;] :wq ## also edit themes/zafta/LICENSE.md and change ## the bit that says \u0026#34;YOUR_NAME_HERE\u0026#34; Note that the the skeleton\u0026rsquo;s template files are empty. Don\u0026rsquo;t worry, we\u0026rsquo;ll be changing that shortly.\n$ find themes/zafta -name \u0026#39;*.html\u0026#39; | xargs ls -l -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/_default/list.html -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/_default/single.html -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/partials/footer.html -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/partials/header.html $ Update the Configuration File to Use the Theme # Now that we\u0026rsquo;ve got a theme to work with, it\u0026rsquo;s a good idea to add the theme name to the configuration file. This is optional, because you can always add \u0026ldquo;-t zafta\u0026rdquo; on all your commands. I like to put it the configuration file because I like shorter command lines. If you don\u0026rsquo;t put it in the configuration file or specify it on the command line, you won\u0026rsquo;t use the template that you\u0026rsquo;re expecting to.\nEdit the file to add the theme, add a title for the site, and specify that all of our content will use the TOML format.\n$ vi config.toml theme = \u0026#34;zafta\u0026#34; baseurl = \u0026#34;\u0026#34; languageCode = \u0026#34;en-us\u0026#34; title = \u0026#34;zafta - totally refreshing\u0026#34; MetaDataFormat = \u0026#34;toml\u0026#34; :wq $ Generate the Site # Now that we have an empty theme, let\u0026rsquo;s generate the site again.\n$ hugo --verbose INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/ WARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html] 0 draft content 0 future content 0 pages created 0 tags created 0 categories created in 2 ms $ Did you notice that the output is different? The warning message for the home page has disappeared and we have an additional information line saying that Hugo is syncing from the theme\u0026rsquo;s directory.\nLet\u0026rsquo;s check the public/ directory to see what Hugo\u0026rsquo;s created.\n$ ls -l public total 16 drwxr-xr-x 2 quoha staff 68 Sep 29 17:56 css -rw-r--r-- 1 quoha staff 0 Sep 29 17:56 index.html -rw-r--r-- 1 quoha staff 407 Sep 29 17:56 index.xml drwxr-xr-x 2 quoha staff 68 Sep 29 17:56 js -rw-r--r-- 1 quoha staff 243 Sep 29 17:56 sitemap.xml $ Notice four things:\nHugo created a home page. This is the file public/index.html. Hugo created a css/ directory. Hugo created a js/ directory. Hugo claimed that it created 0 pages. It created a file and copied over static files, but didn\u0026rsquo;t create any pages. That\u0026rsquo;s because it considers a \u0026ldquo;page\u0026rdquo; to be a file created directly from a content file. It doesn\u0026rsquo;t count things like the index.html files that it creates automatically. The Home Page # Hugo supports many different types of templates. The home page is special because it gets its own type of template and its own template file. The file, layouts/index.html, is used to generate the HTML for the home page. The Hugo documentation says that this is the only required template, but that depends. Hugo\u0026rsquo;s warning message shows that it looks for three different templates:\nWARN: 2014/09/29 Unable to locate layout: [index.html _default/list.html _default/single.html] If it can\u0026rsquo;t find any of these, it completely skips creating the home page. We noticed that when we built the site without having a theme installed.\nWhen Hugo created our theme, it created an empty home page template. Now, when we build the site, Hugo finds the template and uses it to generate the HTML for the home page. Since the template file is empty, the HTML file is empty, too. If the template had any rules in it, then Hugo would have used them to generate the home page.\n$ find . -name index.html | xargs ls -l -rw-r--r-- 1 quoha staff 0 Sep 29 20:21 ./public/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 ./themes/zafta/layouts/index.html $ The Magic of Static # Hugo does two things when generating the site. It uses templates to transform content into HTML and it copies static files into the site. Unlike content, static files are not transformed. They are copied exactly as they are.\nHugo assumes that your site will use both CSS and JavaScript, so it creates directories in your theme to hold them. Remember opinions? Well, Hugo\u0026rsquo;s opinion is that you\u0026rsquo;ll store your CSS in a directory named css/ and your JavaScript in a directory named js/. If you don\u0026rsquo;t like that, you can change the directory names in your theme directory or even delete them completely. Hugo\u0026rsquo;s nice enough to offer its opinion, then behave nicely if you disagree.\n$ find themes/zafta -type d | xargs ls -ld drwxr-xr-x 7 quoha staff 238 Sep 29 17:38 themes/zafta drwxr-xr-x 3 quoha staff 102 Sep 29 17:31 themes/zafta/archetypes drwxr-xr-x 5 quoha staff 170 Sep 29 17:31 themes/zafta/layouts drwxr-xr-x 4 quoha staff 136 Sep 29 17:31 themes/zafta/layouts/_default drwxr-xr-x 4 quoha staff 136 Sep 29 17:31 themes/zafta/layouts/partials drwxr-xr-x 4 quoha staff 136 Sep 29 17:31 themes/zafta/static drwxr-xr-x 2 quoha staff 68 Sep 29 17:31 themes/zafta/static/css drwxr-xr-x 2 quoha staff 68 Sep 29 17:31 themes/zafta/static/js $ The Theme Development Cycle # When you\u0026rsquo;re working on a theme, you will make changes in the theme\u0026rsquo;s directory, rebuild the site, and check your changes in the browser. Hugo makes this very easy:\nPurge the public/ directory. Run the built in web server in watch mode. Open your site in a browser. Update the theme. Glance at your browser window to see changes. Return to step 4. I’ll throw in one more opinion: never work on a theme on a live site. Always work on a copy of your site. Make changes to your theme, test them, then copy them up to your site. For added safety, use a tool like Git to keep a revision history of your content and your theme. Believe me when I say that it is too easy to lose both your mind and your changes.\nCheck the main Hugo site for information on using Git with Hugo.\nPurge the public/ Directory # When generating the site, Hugo will create new files and update existing ones in the public/ directory. It will not delete files that are no longer used. For example, files that were created in the wrong directory or with the wrong title will remain. If you leave them, you might get confused by them later. I recommend cleaning out your site prior to generating it.\nNote: If you\u0026rsquo;re building on an SSD, you should ignore this. Churning on a SSD can be costly.\nHugo\u0026rsquo;s Watch Option # Hugo\u0026rsquo;s \u0026ldquo;--watch\u0026rdquo; option will monitor the content/ and your theme directories for changes and rebuild the site automatically.\nLive Reload # Hugo\u0026rsquo;s built in web server supports live reload. As pages are saved on the server, the browser is told to refresh the page. Usually, this happens faster than you can say, \u0026ldquo;Wow, that\u0026rsquo;s totally amazing.\u0026rdquo;\nDevelopment Commands # Use the following commands as the basis for your workflow.\n## purge old files. hugo will recreate the public directory. ## $ rm -rf public ## ## run hugo in watch mode ## $ hugo server --watch --verbose Here\u0026rsquo;s sample output showing Hugo detecting a change to the template for the home page. Once generated, the web browser automatically reloaded the page. I\u0026rsquo;ve said this before, it\u0026rsquo;s amazing.\n$ rm -rf public $ hugo server --watch --verbose INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/ WARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html] 0 draft content 0 future content 0 pages created 0 tags created 0 categories created in 2 ms Watching for changes in /Users/quoha/Sites/zafta/content Serving pages from /Users/quoha/Sites/zafta/public Web Server is available at http://localhost:1313 Press Ctrl+C to stop INFO: 2014/09/29 File System Event: [\u0026#34;/Users/quoha/Sites/zafta/themes/zafta/layouts/index.html\u0026#34;: MODIFY|ATTRIB] Change detected, rebuilding site WARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html] 0 draft content 0 future content 0 pages created 0 tags created 0 categories created in 1 ms Update the Home Page Template # The home page is one of a few special pages that Hugo creates automatically. As mentioned earlier, it looks for one of three files in the theme\u0026rsquo;s layout/ directory:\nindex.html _default/list.html _default/single.html We could update one of the default templates, but a good design decision is to update the most specific template available. That\u0026rsquo;s not a hard and fast rule (in fact, we\u0026rsquo;ll break it a few times in this tutorial), but it is a good generalization.\nMake a Static Home Page # Right now, that page is empty because we don\u0026rsquo;t have any content and we don\u0026rsquo;t have any logic in the template. Let\u0026rsquo;s change that by adding some text to the template.\n$ vi themes/zafta/layouts/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;hugo says hello!\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; :wq $ Build the web site and then verify the results.\n$ hugo --verbose INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/ WARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html] 0 draft content 0 future content 0 pages created 0 tags created 0 categories created in 2 ms $ find public -type f -name \u0026#39;*.html\u0026#39; | xargs ls -l -rw-r--r-- 1 quoha staff 78 Sep 29 21:26 public/index.html $ cat public/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;hugo says hello!\u0026lt;/p\u0026gt; \u0026lt;/html\u0026gt; Live Reload # Note: If you\u0026rsquo;re running the server with the --watch option, you\u0026rsquo;ll see different content in the file:\n$ cat public/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;hugo says hello!\u0026lt;/p\u0026gt; \u0026lt;script\u0026gt;document.write(\u0026#39;\u0026lt;script src=\u0026#34;http://\u0026#39; + (location.host || \u0026#39;localhost\u0026#39;).split(\u0026#39;:\u0026#39;)[0] + \u0026#39;:1313/livereload.js?mindelay=10\u0026#34;\u0026gt;\u0026lt;/\u0026#39; + \u0026#39;script\u0026gt;\u0026#39;)\u0026lt;/script\u0026gt;\u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; When you use --watch, the Live Reload script is added by Hugo. Look for live reload in the documentation to see what it does and how to disable it.\nBuild a \u0026ldquo;Dynamic\u0026rdquo; Home Page # \u0026ldquo;Dynamic home page?\u0026rdquo; Hugo\u0026rsquo;s a static web site generator, so this seems an odd thing to say. I mean let\u0026rsquo;s have the home page automatically reflect the content in the site every time Hugo builds it. We\u0026rsquo;ll use iteration in the template to do that.\nCreate New Posts # Now that we have the home page generating static content, let\u0026rsquo;s add some content to the site. We\u0026rsquo;ll display these posts as a list on the home page and on their own page, too.\nHugo has a command to generate a skeleton post, just like it does for sites and themes.\n$ hugo --verbose new post/first.md INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 attempting to create post/first.md of post INFO: 2014/09/29 curpath: /Users/quoha/Sites/zafta/themes/zafta/archetypes/default.md ERROR: 2014/09/29 Unable to Cast \u0026lt;nil\u0026gt; to map[string]interface{} $ That wasn\u0026rsquo;t very nice, was it?\nThe \u0026ldquo;new\u0026rdquo; command uses an archetype to create the post file. Hugo created an empty default archetype file, but that causes an error when there\u0026rsquo;s a theme. For me, the workaround was to create an archetypes file specifically for the post type.\n$ vi themes/zafta/archetypes/post.md +++ Description = \u0026#34;\u0026#34; Tags = [] Categories = [] +++ :wq $ find themes/zafta/archetypes -type f | xargs ls -l -rw-r--r-- 1 quoha staff 0 Sep 29 21:53 themes/zafta/archetypes/default.md -rw-r--r-- 1 quoha staff 51 Sep 29 21:54 themes/zafta/archetypes/post.md $ hugo --verbose new post/first.md INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 attempting to create post/first.md of post INFO: 2014/09/29 curpath: /Users/quoha/Sites/zafta/themes/zafta/archetypes/post.md INFO: 2014/09/29 creating /Users/quoha/Sites/zafta/content/post/first.md /Users/quoha/Sites/zafta/content/post/first.md created $ hugo --verbose new post/second.md INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 attempting to create post/second.md of post INFO: 2014/09/29 curpath: /Users/quoha/Sites/zafta/themes/zafta/archetypes/post.md INFO: 2014/09/29 creating /Users/quoha/Sites/zafta/content/post/second.md /Users/quoha/Sites/zafta/content/post/second.md created $ ls -l content/post total 16 -rw-r--r-- 1 quoha staff 104 Sep 29 21:54 first.md -rw-r--r-- 1 quoha staff 105 Sep 29 21:57 second.md $ cat content/post/first.md +++ Categories = [] Description = \u0026#34;\u0026#34; Tags = [] date = \u0026#34;2014-09-29T21:54:53-05:00\u0026#34; title = \u0026#34;first\u0026#34; +++ my first post $ cat content/post/second.md +++ Categories = [] Description = \u0026#34;\u0026#34; Tags = [] date = \u0026#34;2014-09-29T21:57:09-05:00\u0026#34; title = \u0026#34;second\u0026#34; +++ my second post $ Build the web site and then verify the results.\n$ rm -rf public $ hugo --verbose INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 found taxonomies: map[string]string{\u0026#34;category\u0026#34;:\u0026#34;categories\u0026#34;, \u0026#34;tag\u0026#34;:\u0026#34;tags\u0026#34;} WARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html] 0 draft content 0 future content 2 pages created 0 tags created 0 categories created in 4 ms $ The output says that it created 2 pages. Those are our new posts:\n$ find public -type f -name \u0026#39;*.html\u0026#39; | xargs ls -l -rw-r--r-- 1 quoha staff 78 Sep 29 22:13 public/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 22:13 public/post/first/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 22:13 public/post/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 22:13 public/post/second/index.html $ The new files are empty because because the templates used to generate the content are empty. The homepage doesn\u0026rsquo;t show the new content, either. We have to update the templates to add the posts.\nList and Single Templates # In Hugo, we have three major kinds of templates. There\u0026rsquo;s the home page template that we updated previously. It is used only by the home page. We also have \u0026ldquo;single\u0026rdquo; templates which are used to generate output for a single content file. We also have \u0026ldquo;list\u0026rdquo; templates that are used to group multiple pieces of content before generating output.\nGenerally speaking, list templates are named \u0026ldquo;list.html\u0026rdquo; and single templates are named \u0026ldquo;single.html.\u0026rdquo;\nThere are three other types of templates: partials, content views, and terms. We will not go into much detail on these.\nAdd Content to the Homepage # The home page will contain a list of posts. Let\u0026rsquo;s update its template to add the posts that we just created. The logic in the template will run every time we build the site.\n$ vi themes/zafta/layouts/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; {{ range first 10 .Data.Pages }} \u0026lt;h1\u0026gt;{{ .Title }}\u0026lt;/h1\u0026gt; {{ end }} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; :wq $ Hugo uses the Go template engine. That engine scans the template files for commands which are enclosed between \u0026ldquo;{{\u0026rdquo; and \u0026ldquo;}}\u0026rdquo;. In our template, the commands are:\nrange .Title end The \u0026ldquo;range\u0026rdquo; command is an iterator. We\u0026rsquo;re going to use it to go through the first ten pages. Every HTML file that Hugo creates is treated as a page, so looping through the list of pages will look at every file that will be created.\nThe \u0026ldquo;.Title\u0026rdquo; command prints the value of the \u0026ldquo;title\u0026rdquo; variable. Hugo pulls it from the front matter in the Markdown file.\nThe \u0026ldquo;end\u0026rdquo; command signals the end of the range iterator. The engine loops back to the top of the iteration when it finds \u0026ldquo;end.\u0026rdquo; Everything between the \u0026ldquo;range\u0026rdquo; and \u0026ldquo;end\u0026rdquo; is evaluated every time the engine goes through the iteration. In this file, that would cause the title from the first ten pages to be output as heading level one.\nIt\u0026rsquo;s helpful to remember that some variables, like .Data, are created before any output files. Hugo loads every content file into the variable and then gives the template a chance to process before creating the HTML files.\nBuild the web site and then verify the results.\n$ rm -rf public $ hugo --verbose INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 found taxonomies: map[string]string{\u0026#34;tag\u0026#34;:\u0026#34;tags\u0026#34;, \u0026#34;category\u0026#34;:\u0026#34;categories\u0026#34;} WARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html] 0 draft content 0 future content 2 pages created 0 tags created 0 categories created in 4 ms $ find public -type f -name \u0026#39;*.html\u0026#39; | xargs ls -l -rw-r--r-- 1 quoha staff 94 Sep 29 22:23 public/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 22:23 public/post/first/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 22:23 public/post/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 22:23 public/post/second/index.html $ cat public/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;second\u0026lt;/h1\u0026gt; \u0026lt;h1\u0026gt;first\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; $ Congratulations, the home page shows the title of the two posts. The posts themselves are still empty, but let\u0026rsquo;s take a moment to appreciate what we\u0026rsquo;ve done. Your template now generates output dynamically. Believe it or not, by inserting the range command inside of those curly braces, you\u0026rsquo;ve learned everything you need to know to build a theme. All that\u0026rsquo;s really left is understanding which template will be used to generate each content file and becoming familiar with the commands for the template engine.\nAnd, if that were entirely true, this tutorial would be much shorter. There are a few things to know that will make creating a new template much easier. Don\u0026rsquo;t worry, though, that\u0026rsquo;s all to come.\nAdd Content to the Posts # We\u0026rsquo;re working with posts, which are in the content/post/ directory. That means that their section is \u0026ldquo;post\u0026rdquo; (and if we don\u0026rsquo;t do something weird, their type is also \u0026ldquo;post\u0026rdquo;).\nHugo uses the section and type to find the template file for every piece of content. Hugo will first look for a template file that matches the section or type name. If it can\u0026rsquo;t find one, then it will look in the _default/ directory. There are some twists that we\u0026rsquo;ll cover when we get to categories and tags, but for now we can assume that Hugo will try post/single.html, then _default/single.html.\nNow that we know the search rule, let\u0026rsquo;s see what we actually have available:\n$ find themes/zafta -name single.html | xargs ls -l -rw-r--r-- 1 quoha staff 132 Sep 29 17:31 themes/zafta/layouts/_default/single.html We could create a new template, post/single.html, or change the default. Since we don\u0026rsquo;t know of any other content types, let\u0026rsquo;s start with updating the default.\nRemember, any content that we haven\u0026rsquo;t created a template for will end up using this template. That can be good or bad. Bad because I know that we\u0026rsquo;re going to be adding different types of content and we\u0026rsquo;re going to end up undoing some of the changes we\u0026rsquo;ve made. It\u0026rsquo;s good because we\u0026rsquo;ll be able to see immediate results. It\u0026rsquo;s also good to start here because we can start to build the basic layout for the site. As we add more content types, we\u0026rsquo;ll refactor this file and move logic around. Hugo makes that fairly painless, so we\u0026rsquo;ll accept the cost and proceed.\nPlease see the Hugo documentation on template rendering for all the details on determining which template to use. And, as the docs mention, if you\u0026rsquo;re building a single page application (SPA) web site, you can delete all of the other templates and work with just the default single page. That\u0026rsquo;s a refreshing amount of joy right there.\nUpdate the Template File # $ vi themes/zafta/layouts/_default/single.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;{{ .Title }}\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;{{ .Title }}\u0026lt;/h1\u0026gt; {{ .Content }} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; :wq $ Build the web site and verify the results.\n$ rm -rf public $ hugo --verbose INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 found taxonomies: map[string]string{\u0026#34;tag\u0026#34;:\u0026#34;tags\u0026#34;, \u0026#34;category\u0026#34;:\u0026#34;categories\u0026#34;} WARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html] 0 draft content 0 future content 2 pages created 0 tags created 0 categories created in 4 ms $ find public -type f -name \u0026#39;*.html\u0026#39; | xargs ls -l -rw-r--r-- 1 quoha staff 94 Sep 29 22:40 public/index.html -rw-r--r-- 1 quoha staff 125 Sep 29 22:40 public/post/first/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 22:40 public/post/index.html -rw-r--r-- 1 quoha staff 128 Sep 29 22:40 public/post/second/index.html $ cat public/post/first/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;first\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;first\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;my first post\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; $ cat public/post/second/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;second\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;second\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;my second post\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; $ Notice that the posts now have content. You can go to localhost:1313/post/first to verify.\nLinking to Content # The posts are on the home page. Let\u0026rsquo;s add a link from there to the post. Since this is the home page, we\u0026rsquo;ll update its template.\n$ vi themes/zafta/layouts/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; {{ range first 10 .Data.Pages }} \u0026lt;h1\u0026gt;\u0026lt;a href=\u0026#34;{{ .Permalink }}\u0026#34;\u0026gt;{{ .Title }}\u0026lt;/a\u0026gt;\u0026lt;/h1\u0026gt; {{ end }} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Build the web site and verify the results.\n$ rm -rf public $ hugo --verbose INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 found taxonomies: map[string]string{\u0026#34;tag\u0026#34;:\u0026#34;tags\u0026#34;, \u0026#34;category\u0026#34;:\u0026#34;categories\u0026#34;} WARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html] 0 draft content 0 future content 2 pages created 0 tags created 0 categories created in 4 ms $ find public -type f -name \u0026#39;*.html\u0026#39; | xargs ls -l -rw-r--r-- 1 quoha staff 149 Sep 29 22:44 public/index.html -rw-r--r-- 1 quoha staff 125 Sep 29 22:44 public/post/first/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 22:44 public/post/index.html -rw-r--r-- 1 quoha staff 128 Sep 29 22:44 public/post/second/index.html $ cat public/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;\u0026lt;a href=\u0026#34;/post/second/\u0026#34;\u0026gt;second\u0026lt;/a\u0026gt;\u0026lt;/h1\u0026gt; \u0026lt;h1\u0026gt;\u0026lt;a href=\u0026#34;/post/first/\u0026#34;\u0026gt;first\u0026lt;/a\u0026gt;\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; $ Create a Post Listing # We have the posts displaying on the home page and on their own page. We also have a file public/post/index.html that is empty. Let\u0026rsquo;s make it show a list of all posts (not just the first ten).\nWe need to decide which template to update. This will be a listing, so it should be a list template. Let\u0026rsquo;s take a quick look and see which list templates are available.\n$ find themes/zafta -name list.html | xargs ls -l -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/_default/list.html As with the single post, we have to decide to update _default/list.html or create post/list.html. We still don\u0026rsquo;t have multiple content types, so let\u0026rsquo;s stay consistent and update the default list template.\nCreating Top Level Pages # Let\u0026rsquo;s add an \u0026ldquo;about\u0026rdquo; page and display it at the top level (as opposed to a sub-level like we did with posts).\nThe default in Hugo is to use the directory structure of the content/ directory to guide the location of the generated html in the public/ directory. Let\u0026rsquo;s verify that by creating an \u0026ldquo;about\u0026rdquo; page at the top level:\n$ vi content/about.md +++ title = \u0026#34;about\u0026#34; description = \u0026#34;about this site\u0026#34; date = \u0026#34;2014-09-27\u0026#34; slug = \u0026#34;about time\u0026#34; +++ ## about us i\u0026#39;m speechless :wq Generate the web site and verify the results.\n$ find public -name \u0026#39;*.html\u0026#39; | xargs ls -l -rw-rw-r-- 1 mdhender staff 334 Sep 27 15:08 public/about-time/index.html -rw-rw-r-- 1 mdhender staff 527 Sep 27 15:08 public/index.html -rw-rw-r-- 1 mdhender staff 358 Sep 27 15:08 public/post/first-post/index.html -rw-rw-r-- 1 mdhender staff 0 Sep 27 15:08 public/post/index.html -rw-rw-r-- 1 mdhender staff 342 Sep 27 15:08 public/post/second-post/index.html Notice that the page wasn\u0026rsquo;t created at the top level. It was created in a sub-directory named \u0026lsquo;about-time/\u0026rsquo;. That name came from our slug. Hugo will use the slug to name the generated content. It\u0026rsquo;s a reasonable default, by the way, but we can learn a few things by fighting it for this file.\nOne other thing. Take a look at the home page.\n$ cat public/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;\u0026lt;a href=\u0026#34;http://localhost:1313/post/theme/\u0026#34;\u0026gt;creating a new theme\u0026lt;/a\u0026gt;\u0026lt;/h1\u0026gt; \u0026lt;h1\u0026gt;\u0026lt;a href=\u0026#34;http://localhost:1313/about-time/\u0026#34;\u0026gt;about\u0026lt;/a\u0026gt;\u0026lt;/h1\u0026gt; \u0026lt;h1\u0026gt;\u0026lt;a href=\u0026#34;http://localhost:1313/post/second-post/\u0026#34;\u0026gt;second\u0026lt;/a\u0026gt;\u0026lt;/h1\u0026gt; \u0026lt;h1\u0026gt;\u0026lt;a href=\u0026#34;http://localhost:1313/post/first-post/\u0026#34;\u0026gt;first\u0026lt;/a\u0026gt;\u0026lt;/h1\u0026gt; \u0026lt;script\u0026gt;document.write(\u0026#39;\u0026lt;script src=\u0026#34;http://\u0026#39; + (location.host || \u0026#39;localhost\u0026#39;).split(\u0026#39;:\u0026#39;)[0] + \u0026#39;:1313/livereload.js?mindelay=10\u0026#34;\u0026gt;\u0026lt;/\u0026#39; + \u0026#39;script\u0026gt;\u0026#39;)\u0026lt;/script\u0026gt;\u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Notice that the \u0026ldquo;about\u0026rdquo; link is listed with the posts? That\u0026rsquo;s not desirable, so let\u0026rsquo;s change that first.\n$ vi themes/zafta/layouts/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;posts\u0026lt;/h1\u0026gt; {{ range first 10 .Data.Pages }} {{ if eq .Type \u0026#34;post\u0026#34;}} \u0026lt;h2\u0026gt;\u0026lt;a href=\u0026#34;{{ .Permalink }}\u0026#34;\u0026gt;{{ .Title }}\u0026lt;/a\u0026gt;\u0026lt;/h2\u0026gt; {{ end }} {{ end }} \u0026lt;h1\u0026gt;pages\u0026lt;/h1\u0026gt; {{ range .Data.Pages }} {{ if eq .Type \u0026#34;page\u0026#34; }} \u0026lt;h2\u0026gt;\u0026lt;a href=\u0026#34;{{ .Permalink }}\u0026#34;\u0026gt;{{ .Title }}\u0026lt;/a\u0026gt;\u0026lt;/h2\u0026gt; {{ end }} {{ end }} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; :wq Generate the web site and verify the results. The home page has two sections, posts and pages, and each section has the right set of headings and links in it.\nBut, that about page still renders to about-time/index.html.\n$ find public -name \u0026#39;*.html\u0026#39; | xargs ls -l -rw-rw-r-- 1 mdhender staff 334 Sep 27 15:33 public/about-time/index.html -rw-rw-r-- 1 mdhender staff 645 Sep 27 15:33 public/index.html -rw-rw-r-- 1 mdhender staff 358 Sep 27 15:33 public/post/first-post/index.html -rw-rw-r-- 1 mdhender staff 0 Sep 27 15:33 public/post/index.html -rw-rw-r-- 1 mdhender staff 342 Sep 27 15:33 public/post/second-post/index.html Knowing that hugo is using the slug to generate the file name, the simplest solution is to change the slug. Let\u0026rsquo;s do it the hard way and change the permalink in the configuration file.\n$ vi config.toml [permalinks] page = \u0026#34;/:title/\u0026#34; about = \u0026#34;/:filename/\u0026#34; Generate the web site and verify that this didn\u0026rsquo;t work. Hugo lets \u0026ldquo;slug\u0026rdquo; or \u0026ldquo;URL\u0026rdquo; override the permalinks setting in the configuration file. Go ahead and comment out the slug in content/about.md, then generate the web site to get it to be created in the right place.\nSharing Templates # If you\u0026rsquo;ve been following along, you probably noticed that posts have titles in the browser and the home page doesn\u0026rsquo;t. That\u0026rsquo;s because we didn\u0026rsquo;t put the title in the home page\u0026rsquo;s template (layouts/index.html). That\u0026rsquo;s an easy thing to do, but let\u0026rsquo;s look at a different option.\nWe can put the common bits into a shared template that\u0026rsquo;s stored in the themes/zafta/layouts/partials/ directory.\nCreate the Header and Footer Partials # In Hugo, a partial is a sugar-coated template. Normally a template reference has a path specified. Partials are different. Hugo searches for them along a TODO defined search path. This makes it easier for end-users to override the theme\u0026rsquo;s presentation.\n$ vi themes/zafta/layouts/partials/header.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;{{ .Title }}\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; :wq $ vi themes/zafta/layouts/partials/footer.html \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; :wq Update the Home Page Template to Use the Partials # The most noticeable difference between a template call and a partials call is the lack of path:\n{{ template \u0026#34;theme/partials/header.html\u0026#34; . }} versus\n{{ partial \u0026#34;header.html\u0026#34; . }} Both pass in the context.\nLet\u0026rsquo;s change the home page template to use these new partials.\n$ vi themes/zafta/layouts/index.html {{ partial \u0026#34;header.html\u0026#34; . }} \u0026lt;h1\u0026gt;posts\u0026lt;/h1\u0026gt; {{ range first 10 .Data.Pages }} {{ if eq .Type \u0026#34;post\u0026#34;}} \u0026lt;h2\u0026gt;\u0026lt;a href=\u0026#34;{{ .Permalink }}\u0026#34;\u0026gt;{{ .Title }}\u0026lt;/a\u0026gt;\u0026lt;/h2\u0026gt; {{ end }} {{ end }} \u0026lt;h1\u0026gt;pages\u0026lt;/h1\u0026gt; {{ range .Data.Pages }} {{ if or (eq .Type \u0026#34;page\u0026#34;) (eq .Type \u0026#34;about\u0026#34;) }} \u0026lt;h2\u0026gt;\u0026lt;a href=\u0026#34;{{ .Permalink }}\u0026#34;\u0026gt;{{ .Type }} - {{ .Title }} - {{ .RelPermalink }}\u0026lt;/a\u0026gt;\u0026lt;/h2\u0026gt; {{ end }} {{ end }} {{ partial \u0026#34;footer.html\u0026#34; . }} :wq Generate the web site and verify the results. The title on the home page is now \u0026ldquo;your title here\u0026rdquo;, which comes from the \u0026ldquo;title\u0026rdquo; variable in the config.toml file.\nUpdate the Default Single Template to Use the Partials # $ vi themes/zafta/layouts/_default/single.html {{ partial \u0026#34;header.html\u0026#34; . }} \u0026lt;h1\u0026gt;{{ .Title }}\u0026lt;/h1\u0026gt; {{ .Content }} {{ partial \u0026#34;footer.html\u0026#34; . }} :wq Generate the web site and verify the results. The title on the posts and the about page should both reflect the value in the markdown file.\nAdd “Date Published” to Posts # It\u0026rsquo;s common to have posts display the date that they were written or published, so let\u0026rsquo;s add that. The front matter of our posts has a variable named \u0026ldquo;date.\u0026rdquo; It\u0026rsquo;s usually the date the content was created, but let\u0026rsquo;s pretend that\u0026rsquo;s the value we want to display.\nAdd “Date Published” to the Template # We\u0026rsquo;ll start by updating the template used to render the posts. The template code will look like:\n{{ .Date.Format \u0026#34;Mon, Jan 2, 2006\u0026#34; }} Posts use the default single template, so we\u0026rsquo;ll change that file.\n$ vi themes/zafta/layouts/_default/single.html {{ partial \u0026#34;header.html\u0026#34; . }} \u0026lt;h1\u0026gt;{{ .Title }}\u0026lt;/h1\u0026gt; \u0026lt;h2\u0026gt;{{ .Date.Format \u0026#34;Mon, Jan 2, 2006\u0026#34; }}\u0026lt;/h2\u0026gt; {{ .Content }} {{ partial \u0026#34;footer.html\u0026#34; . }} :wq Generate the web site and verify the results. The posts now have the date displayed in them. There\u0026rsquo;s a problem, though. The \u0026ldquo;about\u0026rdquo; page also has the date displayed.\nAs usual, there are a couple of ways to make the date display only on posts. We could do an \u0026ldquo;if\u0026rdquo; statement like we did on the home page. Another way would be to create a separate template for posts.\nThe \u0026ldquo;if\u0026rdquo; solution works for sites that have just a couple of content types. It aligns with the principle of \u0026ldquo;code for today,\u0026rdquo; too.\nLet\u0026rsquo;s assume, though, that we\u0026rsquo;ve made our site so complex that we feel we have to create a new template type. In Hugo-speak, we\u0026rsquo;re going to create a section template.\nLet\u0026rsquo;s restore the default single template before we forget.\n$ mkdir themes/zafta/layouts/post $ vi themes/zafta/layouts/_default/single.html {{ partial \u0026#34;header.html\u0026#34; . }} \u0026lt;h1\u0026gt;{{ .Title }}\u0026lt;/h1\u0026gt; {{ .Content }} {{ partial \u0026#34;footer.html\u0026#34; . }} :wq Now we\u0026rsquo;ll update the post\u0026rsquo;s version of the single template. If you remember Hugo\u0026rsquo;s rules, the template engine will use this version over the default.\n$ vi themes/zafta/layouts/post/single.html {{ partial \u0026#34;header.html\u0026#34; . }} \u0026lt;h1\u0026gt;{{ .Title }}\u0026lt;/h1\u0026gt; \u0026lt;h2\u0026gt;{{ .Date.Format \u0026#34;Mon, Jan 2, 2006\u0026#34; }}\u0026lt;/h2\u0026gt; {{ .Content }} {{ partial \u0026#34;footer.html\u0026#34; . }} :wq Note that we removed the date logic from the default template and put it in the post template. Generate the web site and verify the results. Posts have dates and the about page doesn\u0026rsquo;t.\nDon\u0026rsquo;t Repeat Yourself # DRY is a good design goal and Hugo does a great job supporting it. Part of the art of a good template is knowing when to add a new template and when to update an existing one. While you\u0026rsquo;re figuring that out, accept that you\u0026rsquo;ll be doing some refactoring. Hugo makes that easy and fast, so it\u0026rsquo;s okay to delay splitting up a template.\n"},{"id":4,"href":"/posts/migrate-from-jekyll/","title":"Migrating from Jekyll","section":"Blog","content":" Move static content to static # Jekyll has a rule that any directory not starting with _ will be copied as-is to the _site output. Hugo keeps all static content under static. You should therefore move it all there. With Jekyll, something that looked like\n▾ \u0026lt;root\u0026gt;/ ▾ images/ logo.png should become\n▾ \u0026lt;root\u0026gt;/ ▾ static/ ▾ images/ logo.png Additionally, you\u0026rsquo;ll want any files that should reside at the root (such as CNAME) to be moved to static.\nCreate your Hugo configuration file # Hugo can read your configuration as JSON, YAML or TOML. Hugo supports parameters custom configuration too. Refer to the Hugo configuration documentation for details.\nSet your configuration publish folder to _site # The default is for Jekyll to publish to _site and for Hugo to publish to public. If, like me, you have _site mapped to a git submodule on the gh-pages branch, you\u0026rsquo;ll want to do one of two alternatives:\nChange your submodule to point to map gh-pages to public instead of _site (recommended).\ngit submodule deinit _site git rm _site git submodule add -b gh-pages git@github.com:your-username/your-repo.git public Or, change the Hugo configuration to use _site instead of public.\n{ .. \u0026quot;publishdir\u0026quot;: \u0026quot;_site\u0026quot;, .. } Convert Jekyll templates to Hugo templates # That\u0026rsquo;s the bulk of the work right here. The documentation is your friend. You should refer to Jekyll\u0026rsquo;s template documentation if you need to refresh your memory on how you built your blog and Hugo\u0026rsquo;s template to learn Hugo\u0026rsquo;s way.\nAs a single reference data point, converting my templates for heyitsalex.net took me no more than a few hours.\nConvert Jekyll plugins to Hugo shortcodes # Jekyll has plugins; Hugo has shortcodes. It\u0026rsquo;s fairly trivial to do a port.\nImplementation # As an example, I was using a custom image_tag plugin to generate figures with caption when running Jekyll. As I read about shortcodes, I found Hugo had a nice built-in shortcode that does exactly the same thing.\nJekyll\u0026rsquo;s plugin:\nmodule Jekyll class ImageTag \u0026lt; Liquid::Tag @url = nil @caption = nil @class = nil @link = nil // Patterns IMAGE_URL_WITH_CLASS_AND_CAPTION = IMAGE_URL_WITH_CLASS_AND_CAPTION_AND_LINK = /(\\w+)(\\s+)((https?:\\/\\/|\\/)(\\S+))(\\s+)\u0026quot;(.*?)\u0026quot;(\\s+)-\u0026gt;((https?:\\/\\/|\\/)(\\S+))(\\s*)/i IMAGE_URL_WITH_CAPTION = /((https?:\\/\\/|\\/)(\\S+))(\\s+)\u0026quot;(.*?)\u0026quot;/i IMAGE_URL_WITH_CLASS = /(\\w+)(\\s+)((https?:\\/\\/|\\/)(\\S+))/i IMAGE_URL = /((https?:\\/\\/|\\/)(\\S+))/i def initialize(tag_name, markup, tokens) super if markup =~ IMAGE_URL_WITH_CLASS_AND_CAPTION_AND_LINK @class = $1 @url = $3 @caption = $7 @link = $9 elsif markup =~ IMAGE_URL_WITH_CLASS_AND_CAPTION @class = $1 @url = $3 @caption = $7 elsif markup =~ IMAGE_URL_WITH_CAPTION @url = $1 @caption = $5 elsif markup =~ IMAGE_URL_WITH_CLASS @class = $1 @url = $3 elsif markup =~ IMAGE_URL @url = $1 end end def render(context) if @class source = \u0026quot;\u0026lt;figure class='#{@class}'\u0026gt;\u0026quot; else source = \u0026quot;\u0026lt;figure\u0026gt;\u0026quot; end if @link source += \u0026quot;\u0026lt;a href=\\\u0026quot;#{@link}\\\u0026quot;\u0026gt;\u0026quot; end source += \u0026quot;\u0026lt;img src=\\\u0026quot;#{@url}\\\u0026quot;\u0026gt;\u0026quot; if @link source += \u0026quot;\u0026lt;/a\u0026gt;\u0026quot; end source += \u0026quot;\u0026lt;figcaption\u0026gt;#{@caption}\u0026lt;/figcaption\u0026gt;\u0026quot; if @caption source += \u0026quot;\u0026lt;/figure\u0026gt;\u0026quot; source end end end Liquid::Template.register_tag('image', Jekyll::ImageTag) is written as this Hugo shortcode:\n\u0026lt;!-- image --\u0026gt; \u0026lt;figure {{ with .Get \u0026quot;class\u0026quot; }}class=\u0026quot;{{.}}\u0026quot;{{ end }}\u0026gt; {{ with .Get \u0026quot;link\u0026quot;}}\u0026lt;a href=\u0026quot;{{.}}\u0026quot;\u0026gt;{{ end }} \u0026lt;img src=\u0026quot;{{ .Get \u0026quot;src\u0026quot; }}\u0026quot; {{ if or (.Get \u0026quot;alt\u0026quot;) (.Get \u0026quot;caption\u0026quot;) }}alt=\u0026quot;{{ with .Get \u0026quot;alt\u0026quot;}}{{.}}{{else}}{{ .Get \u0026quot;caption\u0026quot; }}{{ end }}\u0026quot;{{ end }} /\u0026gt; {{ if .Get \u0026quot;link\u0026quot;}}\u0026lt;/a\u0026gt;{{ end }} {{ if or (or (.Get \u0026quot;title\u0026quot;) (.Get \u0026quot;caption\u0026quot;)) (.Get \u0026quot;attr\u0026quot;)}} \u0026lt;figcaption\u0026gt;{{ if isset .Params \u0026quot;title\u0026quot; }} {{ .Get \u0026quot;title\u0026quot; }}{{ end }} {{ if or (.Get \u0026quot;caption\u0026quot;) (.Get \u0026quot;attr\u0026quot;)}}\u0026lt;p\u0026gt; {{ .Get \u0026quot;caption\u0026quot; }} {{ with .Get \u0026quot;attrlink\u0026quot;}}\u0026lt;a href=\u0026quot;{{.}}\u0026quot;\u0026gt; {{ end }} {{ .Get \u0026quot;attr\u0026quot; }} {{ if .Get \u0026quot;attrlink\u0026quot;}}\u0026lt;/a\u0026gt; {{ end }} \u0026lt;/p\u0026gt; {{ end }} \u0026lt;/figcaption\u0026gt; {{ end }} \u0026lt;/figure\u0026gt; \u0026lt;!-- image --\u0026gt; Usage # I simply changed:\n{% image full http://farm5.staticflickr.com/4136/4829260124_57712e570a_o_d.jpg \u0026quot;One of my favorite touristy-type photos. I secretly waited for the good light while we were \u0026quot;having fun\u0026quot; and took this. Only regret: a stupid pole in the top-left corner of the frame I had to clumsily get rid of at post-processing.\u0026quot; -\u0026gt;http://www.flickr.com/photos/alexnormand/4829260124/in/set-72157624547713078/ %} to this (this example uses a slightly extended version named fig, different than the built-in figure):\n{{% fig class=\u0026quot;full\u0026quot; src=\u0026quot;http://farm5.staticflickr.com/4136/4829260124_57712e570a_o_d.jpg\u0026quot; title=\u0026quot;One of my favorite touristy-type photos. I secretly waited for the good light while we were having fun and took this. Only regret: a stupid pole in the top-left corner of the frame I had to clumsily get rid of at post-processing.\u0026quot; link=\u0026quot;http://www.flickr.com/photos/alexnormand/4829260124/in/set-72157624547713078/\u0026quot; %}} As a bonus, the shortcode named parameters are, arguably, more readable.\nFinishing touches # Fix content # Depending on the amount of customization that was done with each post with Jekyll, this step will require more or less effort. There are no hard and fast rules here except that hugo server --watch is your friend. Test your changes and fix errors as needed.\nClean up # You\u0026rsquo;ll want to remove the Jekyll configuration at this point. If you have anything else that isn\u0026rsquo;t used, delete it.\nA practical example in a diff # Hey, it\u0026rsquo;s Alex was migrated in less than a father-with-kids day from Jekyll to Hugo. You can see all the changes (and screw-ups) by looking at this diff.\n"},{"id":5,"href":"/docs/example/table-of-contents/","title":"Table of Contents","section":"Example Site","content":" Ubi loqui # Mentem genus facietque salire tempus bracchia # Lorem markdownum partu paterno Achillem. Habent amne generosi aderant ad pellem nec erat sustinet merces columque haec et, dixit minus nutrit accipiam subibis subdidit. Temeraria servatum agros qui sed fulva facta. Primum ultima, dedit, suo quisque linguae medentes fixo: tum petis.\nRapit vocant si hunc siste adspice # Ora precari Patraeque Neptunia, dixit Danae Cithaeron armaque maxima in nati Coniugis templis fluidove. Effugit usus nec ingreditur agmen ac manus conlato. Nullis vagis nequiquam vultibus aliquos altera suum venis teneas fretum. Armos remotis hoc sine ferrea iuncta quam!\nLocus fuit caecis # Nefas discordemque domino montes numen tum humili nexilibusque exit, Iove. Quae miror esse, scelerisque Melaneus viribus. Miseri laurus. Hoc est proposita me ante aliquid, aura inponere candidioribus quidque accendit bella, sumpta. Intravit quam erat figentem hunc, motus de fontes parvo tempestate.\niscsi_virus = pitch(json_in_on(eupViral), northbridge_services_troubleshooting, personal( firmware_rw.trash_rw_crm.device(interactive_gopher_personal, software, -1), megabit, ergonomicsSoftware(cmyk_usb_panel, mips_whitelist_duplex, cpa))); if (5) { managementNetwork += dma - boolean; kilohertz_token = 2; honeypot_affiliate_ergonomics = fiber; } mouseNorthbridge = byte(nybble_xmp_modem.horse_subnet( analogThroughputService * graphicPoint, drop(daw_bit, dnsIntranet), gateway_ospf), repository.domain_key.mouse(serverData(fileNetwork, trim_duplex_file), cellTapeDirect, token_tooltip_mashup( ripcordingMashup))); module_it = honeypot_driver(client_cold_dvr(593902, ripping_frequency) + coreLog.joystick(componentUdpLink), windows_expansion_touchscreen); bashGigabit.external.reality(2, server_hardware_codec.flops.ebookSampling( ciscNavigationBacklink, table + cleanDriver), indexProtocolIsp); Placabilis coactis nega ingemuit ignoscat nimia non # Frontis turba. Oculi gravis est Delphice; inque praedaque sanguine manu non.\nif (ad_api) { zif += usb.tiffAvatarRate(subnet, digital_rt) + exploitDrive; gigaflops(2 - bluetooth, edi_asp_memory.gopher(queryCursor, laptop), panel_point_firmware); spyware_bash.statePopApplet = express_netbios_digital( insertion_troubleshooting.brouter(recordFolderUs), 65); } recursionCoreRay = -5; if (hub == non) { portBoxVirus = soundWeb(recursive_card(rwTechnologyLeopard), font_radcab, guidCmsScalable + reciprocalMatrixPim); left.bug = screenshot; } else { tooltipOpacity = raw_process_permalink(webcamFontUser, -1); executable_router += tape; } if (tft) { bandwidthWeb *= social_page; } else { regular += 611883; thumbnail /= system_lag_keyboard; } Caesorum illa tu sentit micat vestes papyriferi # Inde aderam facti; Theseus vis de tauri illa peream. Oculos uberaque non regisque vobis cursuque, opus venit quam vulnera. Et maiora necemque, lege modo; gestanda nitidi, vero? Dum ne pectoraque testantur.\nVenasque repulsa Samos qui, exspectatum eram animosque hinc, aut manes, Assyrii. Cupiens auctoribus pariter rubet, profana magni super nocens. Vos ius sibilat inpar turba visae iusto! Sedes ante dum superest extrema.\n"},{"id":6,"href":"/posts/goisforlovers/","title":"(Hu)go Template Primer","section":"Blog","content":"Hugo uses the excellent Go html/template library for its template engine. It is an extremely lightweight engine that provides a very small amount of logic. In our experience that it is just the right amount of logic to be able to create a good static website. If you have used other template systems from different languages or frameworks you will find a lot of similarities in Go templates.\nThis document is a brief primer on using Go templates. The Go docs provide more details.\nIntroduction to Go Templates # Go templates provide an extremely simple template language. It adheres to the belief that only the most basic of logic belongs in the template or view layer. One consequence of this simplicity is that Go templates parse very quickly.\nA unique characteristic of Go templates is they are content aware. Variables and content will be sanitized depending on the context of where they are used. More details can be found in the Go docs.\nBasic Syntax # Golang templates are HTML files with the addition of variables and functions.\nGo variables and functions are accessible within {{ }}\nAccessing a predefined variable \u0026ldquo;foo\u0026rdquo;:\n{{ foo }} Parameters are separated using spaces\nCalling the add function with input of 1, 2:\n{{ add 1 2 }} Methods and fields are accessed via dot notation\nAccessing the Page Parameter \u0026ldquo;bar\u0026rdquo;\n{{ .Params.bar }} Parentheses can be used to group items together\n{{ if or (isset .Params \u0026quot;alt\u0026quot;) (isset .Params \u0026quot;caption\u0026quot;) }} Caption {{ end }} Variables # Each Go template has a struct (object) made available to it. In hugo each template is passed either a page or a node struct depending on which type of page you are rendering. More details are available on the variables page.\nA variable is accessed by referencing the variable name.\n\u0026lt;title\u0026gt;{{ .Title }}\u0026lt;/title\u0026gt; Variables can also be defined and referenced.\n{{ $address := \u0026quot;123 Main St.\u0026quot;}} {{ $address }} Functions # Go template ship with a few functions which provide basic functionality. The Go template system also provides a mechanism for applications to extend the available functions with their own. Hugo template functions provide some additional functionality we believe are useful for building websites. Functions are called by using their name followed by the required parameters separated by spaces. Template functions cannot be added without recompiling hugo.\nExample:\n{{ add 1 2 }} Includes # When including another template you will pass to it the data it will be able to access. To pass along the current context please remember to include a trailing dot. The templates location will always be starting at the /layout/ directory within Hugo.\nExample:\n{{ template \u0026quot;chrome/header.html\u0026quot; . }} Logic # Go templates provide the most basic iteration and conditional logic.\nIteration # Just like in Go, the Go templates make heavy use of range to iterate over a map, array or slice. The following are different examples of how to use range.\nExample 1: Using Context\n{{ range array }} {{ . }} {{ end }} Example 2: Declaring value variable name\n{{range $element := array}} {{ $element }} {{ end }} Example 2: Declaring key and value variable name\n{{range $index, $element := array}} {{ $index }} {{ $element }} {{ end }} Conditionals # If, else, with, or, \u0026amp; and provide the framework for handling conditional logic in Go Templates. Like range, each statement is closed with end.\nGo Templates treat the following values as false:\nfalse 0 any array, slice, map, or string of length zero Example 1: If\n{{ if isset .Params \u0026quot;title\u0026quot; }}\u0026lt;h4\u0026gt;{{ index .Params \u0026quot;title\u0026quot; }}\u0026lt;/h4\u0026gt;{{ end }} Example 2: If -\u0026gt; Else\n{{ if isset .Params \u0026quot;alt\u0026quot; }} {{ index .Params \u0026quot;alt\u0026quot; }} {{else}} {{ index .Params \u0026quot;caption\u0026quot; }} {{ end }} Example 3: And \u0026amp; Or\n{{ if and (or (isset .Params \u0026quot;title\u0026quot;) (isset .Params \u0026quot;caption\u0026quot;)) (isset .Params \u0026quot;attr\u0026quot;)}} Example 4: With\nAn alternative way of writing \u0026ldquo;if\u0026rdquo; and then referencing the same value is to use \u0026ldquo;with\u0026rdquo; instead. With rebinds the context . within its scope, and skips the block if the variable is absent.\nThe first example above could be simplified as:\n{{ with .Params.title }}\u0026lt;h4\u0026gt;{{ . }}\u0026lt;/h4\u0026gt;{{ end }} Example 5: If -\u0026gt; Else If\n{{ if isset .Params \u0026quot;alt\u0026quot; }} {{ index .Params \u0026quot;alt\u0026quot; }} {{ else if isset .Params \u0026quot;caption\u0026quot; }} {{ index .Params \u0026quot;caption\u0026quot; }} {{ end }} Pipes # One of the most powerful components of Go templates is the ability to stack actions one after another. This is done by using pipes. Borrowed from unix pipes, the concept is simple, each pipeline\u0026rsquo;s output becomes the input of the following pipe.\nBecause of the very simple syntax of Go templates, the pipe is essential to being able to chain together function calls. One limitation of the pipes is that they only can work with a single value and that value becomes the last parameter of the next pipeline.\nA few simple examples should help convey how to use the pipe.\nExample 1 :\n{{ if eq 1 1 }} Same {{ end }} is the same as\n{{ eq 1 1 | if }} Same {{ end }} It does look odd to place the if at the end, but it does provide a good illustration of how to use the pipes.\nExample 2 :\n{{ index .Params \u0026quot;disqus_url\u0026quot; | html }} Access the page parameter called \u0026ldquo;disqus_url\u0026rdquo; and escape the HTML.\nExample 3 :\n{{ if or (or (isset .Params \u0026quot;title\u0026quot;) (isset .Params \u0026quot;caption\u0026quot;)) (isset .Params \u0026quot;attr\u0026quot;)}} Stuff Here {{ end }} Could be rewritten as\n{{ isset .Params \u0026quot;caption\u0026quot; | or isset .Params \u0026quot;title\u0026quot; | or isset .Params \u0026quot;attr\u0026quot; | if }} Stuff Here {{ end }} Context (aka. the dot) # The most easily overlooked concept to understand about Go templates is that {{ . }} always refers to the current context. In the top level of your template this will be the data set made available to it. Inside of a iteration it will have the value of the current item. When inside of a loop the context has changed. . will no longer refer to the data available to the entire page. If you need to access this from within the loop you will likely want to set it to a variable instead of depending on the context.\nExample:\n{{ $title := .Site.Title }} {{ range .Params.tags }} \u0026lt;li\u0026gt; \u0026lt;a href=\u0026quot;{{ $baseurl }}/tags/{{ . | urlize }}\u0026quot;\u0026gt;{{ . }}\u0026lt;/a\u0026gt; - {{ $title }} \u0026lt;/li\u0026gt; {{ end }} Notice how once we have entered the loop the value of {{ . }} has changed. We have defined a variable outside of the loop so we have access to it from within the loop.\nHugo Parameters # Hugo provides the option of passing values to the template language through the site configuration (for sitewide values), or through the meta data of each specific piece of content. You can define any values of any type (supported by your front matter/config format) and use them however you want to inside of your templates.\nUsing Content (page) Parameters # In each piece of content you can provide variables to be used by the templates. This happens in the front matter.\nAn example of this is used in this documentation site. Most of the pages benefit from having the table of contents provided. Sometimes the TOC just doesn\u0026rsquo;t make a lot of sense. We\u0026rsquo;ve defined a variable in our front matter of some pages to turn off the TOC from being displayed.\nHere is the example front matter:\n--- title: \u0026#34;Permalinks\u0026#34; date: \u0026#34;2013-11-18\u0026#34; aliases: - \u0026#34;/doc/permalinks/\u0026#34; groups: [\u0026#34;extras\u0026#34;] groups_weight: 30 notoc: true --- Here is the corresponding code inside of the template:\n{{ if not .Params.notoc }} \u0026lt;div id=\u0026quot;toc\u0026quot; class=\u0026quot;well col-md-4 col-sm-6\u0026quot;\u0026gt; {{ .TableOfContents }} \u0026lt;/div\u0026gt; {{ end }} Using Site (config) Parameters # In your top-level configuration file (eg, config.yaml) you can define site parameters, which are values which will be available to you in chrome.\nFor instance, you might declare:\nparams: CopyrightHTML: \u0026#34;Copyright \u0026amp;#xA9; 2013 John Doe. All Rights Reserved.\u0026#34; TwitterUser: \u0026#34;spf13\u0026#34; SidebarRecentLimit: 5 Within a footer layout, you might then declare a \u0026lt;footer\u0026gt; which is only provided if the CopyrightHTML parameter is provided, and if it is given, you would declare it to be HTML-safe, so that the HTML entity is not escaped again. This would let you easily update just your top-level config file each January 1st, instead of hunting through your templates.\n{{if .Site.Params.CopyrightHTML}}\u0026lt;footer\u0026gt; \u0026lt;div class=\u0026#34;text-center\u0026#34;\u0026gt;{{.Site.Params.CopyrightHTML | safeHtml}}\u0026lt;/div\u0026gt; \u0026lt;/footer\u0026gt;{{end}} An alternative way of writing the \u0026ldquo;if\u0026rdquo; and then referencing the same value is to use \u0026ldquo;with\u0026rdquo; instead. With rebinds the context . within its scope, and skips the block if the variable is absent:\n{{with .Site.Params.TwitterUser}}\u0026lt;span class=\u0026#34;twitter\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;https://twitter.com/{{.}}\u0026#34; rel=\u0026#34;author\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;/images/twitter.png\u0026#34; width=\u0026#34;48\u0026#34; height=\u0026#34;48\u0026#34; title=\u0026#34;Twitter: {{.}}\u0026#34; alt=\u0026#34;Twitter\u0026#34;\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;/span\u0026gt;{{end}} Finally, if you want to pull \u0026ldquo;magic constants\u0026rdquo; out of your layouts, you can do so, such as in this example:\n\u0026lt;nav class=\u0026#34;recent\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;Recent Posts\u0026lt;/h1\u0026gt; \u0026lt;ul\u0026gt;{{range first .Site.Params.SidebarRecentLimit .Site.Recent}} \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;{{.RelPermalink}}\u0026#34;\u0026gt;{{.Title}}\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; {{end}}\u0026lt;/ul\u0026gt; \u0026lt;/nav\u0026gt; "},{"id":7,"href":"/posts/hugoisforlovers/","title":"Getting Started with Hugo","section":"Blog","content":" Step 1. Install Hugo # Go to Hugo releases and download the appropriate version for your OS and architecture.\nSave it somewhere specific as we will be using it in the next step.\nMore complete instructions are available at Install Hugo\nStep 2. Build the Docs # Hugo has its own example site which happens to also be the documentation site you are reading right now.\nFollow the following steps:\nClone the Hugo repository Go into the repo Run hugo in server mode and build the docs Open your browser to http://localhost:1313 Corresponding pseudo commands:\ngit clone https://github.com/spf13/hugo cd hugo /path/to/where/you/installed/hugo server --source=./docs \u0026gt; 29 pages created \u0026gt; 0 tags index created \u0026gt; in 27 ms \u0026gt; Web Server is available at http://localhost:1313 \u0026gt; Press ctrl+c to stop Once you\u0026rsquo;ve gotten here, follow along the rest of this page on your local build.\nStep 3. Change the docs site # Stop the Hugo process by hitting Ctrl+C.\nNow we are going to run hugo again, but this time with hugo in watch mode.\n/path/to/hugo/from/step/1/hugo server --source=./docs --watch \u0026gt; 29 pages created \u0026gt; 0 tags index created \u0026gt; in 27 ms \u0026gt; Web Server is available at http://localhost:1313 \u0026gt; Watching for changes in /Users/spf13/Code/hugo/docs/content \u0026gt; Press ctrl+c to stop Open your favorite editor and change one of the source content pages. How about changing this very file to fix the typo. How about changing this very file to fix the typo.\nContent files are found in docs/content/. Unless otherwise specified, files are located at the same relative location as the url, in our case docs/content/overview/quickstart.md.\nChange and save this file.. Notice what happened in your terminal.\n\u0026gt; Change detected, rebuilding site \u0026gt; 29 pages created \u0026gt; 0 tags index created \u0026gt; in 26 ms Refresh the browser and observe that the typo is now fixed.\nNotice how quick that was. Try to refresh the site before it\u0026rsquo;s finished building. I double dare you. Having nearly instant feedback enables you to have your creativity flow without waiting for long builds.\nStep 4. Have fun # The best way to learn something is to play with it.\n"},{"id":8,"href":"/docs/example/collapsed/3rd-level/4th-level/","title":"4th Level","section":"3rd Level","content":" 4th Level of Menu # Caesorum illa tu sentit micat vestes papyriferi # Inde aderam facti; Theseus vis de tauri illa peream. Oculos uberaque non regisque vobis cursuque, opus venit quam vulnera. Et maiora necemque, lege modo; gestanda nitidi, vero? Dum ne pectoraque testantur.\nVenasque repulsa Samos qui, exspectatum eram animosque hinc, aut manes, Assyrii. Cupiens auctoribus pariter rubet, profana magni super nocens. Vos ius sibilat inpar turba visae iusto! Sedes ante dum superest extrema.\n"},{"id":9,"href":"/docs/example/collapsed/3rd-level/","title":"3rd Level","section":"Collapsed","content":" 3rd Level of Menu # Nefas discordemque domino montes numen tum humili nexilibusque exit, Iove. Quae miror esse, scelerisque Melaneus viribus. Miseri laurus. Hoc est proposita me ante aliquid, aura inponere candidioribus quidque accendit bella, sumpta. Intravit quam erat figentem hunc, motus de fontes parvo tempestate.\niscsi_virus = pitch(json_in_on(eupViral), northbridge_services_troubleshooting, personal( firmware_rw.trash_rw_crm.device(interactive_gopher_personal, software, -1), megabit, ergonomicsSoftware(cmyk_usb_panel, mips_whitelist_duplex, cpa))); if (5) { managementNetwork += dma - boolean; kilohertz_token = 2; honeypot_affiliate_ergonomics = fiber; } mouseNorthbridge = byte(nybble_xmp_modem.horse_subnet( analogThroughputService * graphicPoint, drop(daw_bit, dnsIntranet), gateway_ospf), repository.domain_key.mouse(serverData(fileNetwork, trim_duplex_file), cellTapeDirect, token_tooltip_mashup( ripcordingMashup))); module_it = honeypot_driver(client_cold_dvr(593902, ripping_frequency) + coreLog.joystick(componentUdpLink), windows_expansion_touchscreen); bashGigabit.external.reality(2, server_hardware_codec.flops.ebookSampling( ciscNavigationBacklink, table + cleanDriver), indexProtocolIsp); "},{"id":10,"href":"/docs/example/hidden/","title":"Hidden","section":"Example Site","content":" This page is hidden in menu # Quondam non pater est dignior ille Eurotas # Latent te facies # Lorem markdownum arma ignoscas vocavit quoque ille texit mandata mentis ultimus, frementes, qui in vel. Hippotades Peleus pennas conscia cuiquam Caeneus quas.\nPater demittere evincitque reddunt Maxime adhuc pressit huc Danaas quid freta Soror ego Luctus linguam saxa ultroque prior Tatiumque inquit Saepe liquitur subita superata dederat Anius sudor Cum honorum Latona # O fallor in sustinui iussorum equidem. Nymphae operi oris alii fronde parens dumque, in auro ait mox ingenti proxima iamdudum maius?\nreality(burnDocking(apache_nanometer), pad.property_data_programming.sectorBrowserPpga(dataMask, 37, recycleRup)); intellectualVaporwareUser += -5 * 4; traceroute_key_upnp /= lag_optical(android.smb(thyristorTftp)); surge_host_golden = mca_compact_device(dual_dpi_opengl, 33, commerce_add_ppc); if (lun_ipv) { verticalExtranet(1, thumbnail_ttl, 3); bar_graphics_jpeg(chipset - sector_xmp_beta); } Fronde cetera dextrae sequens pennis voce muneris # Acta cretus diem restet utque; move integer, oscula non inspirat, noctisque scelus! Nantemque in suas vobis quamvis, et labori!\nvar runtimeDiskCompiler = home - array_ad_software; if (internic \u0026gt; disk) { emoticonLockCron += 37 + bps - 4; wan_ansi_honeypot.cardGigaflops = artificialStorageCgi; simplex -= downloadAccess; } var volumeHardeningAndroid = pixel + tftp + onProcessorUnmount; sector(memory(firewire + interlaced, wired)); "},{"id":11,"href":"/docs/shortcodes/buttons/","title":"Buttons","section":"Shortcodes","content":" Buttons # Buttons are styled links that can lead to local page or external link.\nExample # {{\u0026lt; button relref=\u0026#34;/\u0026#34; [class=\u0026#34;...\u0026#34;] \u0026gt;}}Get Home{{\u0026lt; /button \u0026gt;}} {{\u0026lt; button href=\u0026#34;https://github.com/alex-shpak/hugo-book\u0026#34; \u0026gt;}}Contribute{{\u0026lt; /button \u0026gt;}} Get Home Contribute "},{"id":12,"href":"/docs/shortcodes/columns/","title":"Columns","section":"Shortcodes","content":" Columns # Columns help organize shorter pieces of content horizontally for readability.\nExample # {{% columns [ratio=\u0026#34;1:1\u0026#34;] [class=\u0026#34;...\u0026#34;] %}} \u0026lt;!-- begin columns block --\u0026gt; # Left Content Lorem markdownum insigne... \u0026lt;---\u0026gt; \u0026lt;!-- magic separator, between columns --\u0026gt; # Mid Content Lorem markdownum insigne... \u0026lt;---\u0026gt; \u0026lt;!-- magic separator, between columns --\u0026gt; # Right Content Lorem markdownum insigne... {{% /columns %}} Left Content # Lorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.\nMid Content # Lorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter!\nRight Content # Lorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.\nSettings size ratio for columns # {{% columns ratio=\u0026#34;1:2\u0026#34; %}} \u0026lt;!-- begin columns block --\u0026gt; ## x1 Column Lorem markdownum insigne... \u0026lt;---\u0026gt; \u0026lt;!-- magic separator, between columns --\u0026gt; ## x2 Column Lorem markdownum insigne... {{% /columns %}} x1 Column # Lorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.\nx2 Column # Lorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter!\nLorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.\n"},{"id":13,"href":"/docs/shortcodes/details/","title":"Details","section":"Shortcodes","content":" Details # Details shortcode is a helper for details html5 element. It is going to replace expand shortcode.\nExample # {{% details \u0026#34;Title\u0026#34; [open] %}} ## Markdown content Lorem markdownum insigne... {{% /details %}} {{% details title=\u0026#34;Title\u0026#34; open=true %}} ## Markdown content Lorem markdownum insigne... {{% /details %}} Markdown content # Lorem markdownum insigne\u0026hellip;\n"},{"id":14,"href":"/docs/shortcodes/hints/","title":"Hints","section":"Shortcodes","content":" Hints # Hint shortcode can be used as hint/alerts/notification block.\nThere are 3 colors to choose: info, warning and danger.\n{{% hint [info|warning|danger] %}} **Markdown content** Lorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa {{% /hint %}} Example # Markdown content\nLorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa\nMarkdown content\nLorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa\nMarkdown content\nLorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa\n"},{"id":15,"href":"/docs/shortcodes/mermaid/","title":"Mermaid","section":"Shortcodes","content":" Mermaid Chart # MermaidJS is library for generating svg charts and diagrams from text.\nOverride Mermaid initialization config\nTo override the initialization config for Mermaid, create a mermaid.json file in your assets folder!\nExample # {{\u0026lt; mermaid [class=\u0026#34;...\u0026#34;] \u0026gt;}} stateDiagram-v2 State1: The state with a note note right of State1 Important information! You can write notes. end note State1 --\u0026gt; State2 note left of State2 : This is the note to the left. {{\u0026lt; /mermaid \u0026gt;}} "},{"id":16,"href":"/docs/shortcodes/section/","title":"Section","section":"Shortcodes","content":" Section # Section renders pages in section as definition list, using title and description. Optional param summary can be used to show or hide page summary\nExample # {{\u0026lt; section [summary] \u0026gt;}} First Page First page # Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\nSecond Page Second Page # Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\n"},{"id":17,"href":"/docs/shortcodes/section/first-page/","title":"First Page","section":"Section","content":" First page # Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\nDuis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n"},{"id":18,"href":"/docs/shortcodes/section/second-page/","title":"Second Page","section":"Section","content":" Second Page # Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\nDuis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n"},{"id":19,"href":"/docs/shortcodes/tabs/","title":"Tabs","section":"Shortcodes","content":" Tabs # Tabs let you organize content by context, for example installation instructions for each supported platform.\n{{% tabs \u0026#34;id\u0026#34; %}} {{% tab \u0026#34;MacOS\u0026#34; %}} # MacOS Content {{% /tab %}} {{% tab \u0026#34;Linux\u0026#34; %}} # Linux Content {{% /tab %}} {{% tab \u0026#34;Windows\u0026#34; %}} # Windows Content {{% /tab %}} {{% /tabs %}} Example # "},{"id":20,"href":"/docs/00-%E9%9B%86%E7%BE%A4%E8%A7%84%E5%88%92%E5%92%8C%E5%9F%BA%E7%A1%80%E5%8F%82%E6%95%B0%E8%AE%BE%E5%AE%9A-00--ji-qun-gui-hua-he-ji-chu-can-shu-she-ding/","title":"00-集群规划和基础参数设定 2023-09-28 15:19:02.214","section":"Docs","content":" 0-集群规划和基础参数设定 # HA architecture # 注意1：确保各节点时区设置一致、时间同步。 如果你的环境没有提供NTP 时间同步，推荐集成安装chrony 注意2：确保在干净的系统上开始安装，不要使用曾经装过kubeadm或其他k8s发行版的环境 注意3：建议操作系统升级到新的稳定内核，请结合阅读内核升级文档 注意4：在公有云上创建多主集群，请结合阅读在公有云上部署 kubeasz 高可用集群所需节点配置如下 # 角色 数量 描述 部署节点 1 运行ansible/ezctl命令，一般复用第一个master节点 etcd节点 3 注意etcd集群需要1,3,5,\u0026hellip;奇数个节点，一般复用master节点 master节点 2 高可用集群至少2个master节点 node节点 n 运行应用负载的节点，可根据需要提升机器配置/增加节点数 机器配置：\nmaster节点：4c/8g内存/50g硬盘 worker节点：建议8c/32g内存/200g硬盘以上 注意：默认配置下容器运行时和kubelet会占用/var的磁盘空间，如果磁盘分区特殊，可以设置config.yml中的容器运行时和kubelet数据目录：CONTAINERD_STORAGE_DIR DOCKER_STORAGE_DIR KUBELET_ROOT_DIR\n在 kubeasz 2x 版本，多节点高可用集群安装可以使用2种方式\n1.按照本文步骤先规划准备，预先配置节点信息后，直接安装多节点高可用集群 2.先部署单节点集群 AllinOne部署，然后通过 节点添加 扩容成高可用集群 部署步骤 # 以下示例创建一个4节点的多主高可用集群，文档中命令默认都需要root权限运行。\n1.基础系统配置 # 2c/4g内存/40g硬盘（该配置仅测试用） 最小化安装Ubuntu 16.04 server或者CentOS 7 Minimal 配置基础网络、更新源、SSH登录等 2.在每个节点安装依赖工具 # 推荐使用ansible in docker 容器化方式运行，无需安装额外依赖。\n3.准备ssh免密登陆 # 配置从部署节点能够ssh免密登陆所有节点，并且设置python软连接\n本地客户端生成公私钥：（一路回车默认即可） ssh-keygen ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.0.51 ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.0.52 ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.0.53 #$IP为所有节点地址包括自身，按照提示输入yes 和root密码 ssh-copy-id $IP ssh-copy-id 192.168.0.51 ssh-copy-id 192.168.0.52 ssh-copy-id 192.168.0.53 4.在部署节点编排k8s安装 # 4.1 下载项目源码、二进制及离线镜像 下载工具脚本ezdown，举例使用kubeasz版本3.5.0\nexport release=3.5.0 wget https://github.com/easzlab/kubeasz/releases/download/${release}/ezdown chmod +x ./ezdown export release=3.6.1 wget https://github.com/easzlab/kubeasz/releases/download/${release}/ezdown chmod +x ./ezdown 下载kubeasz代码、二进制、默认容器镜像（更多关于ezdown的参数，运行./ezdown 查看）\n# 国内环境 ./ezdown -D # 海外环境 #./ezdown -D -m standard 【可选】下载额外容器镜像（cilium,flannel,prometheus等）\n# 按需下载 ./ezdown -X flannel ./ezdown -X prometheus ... 【可选】下载离线系统包 (适用于无法使用yum/apt仓库情形)\n./ezdown -P 上述脚本运行成功后，所有文件（kubeasz代码、二进制、离线镜像）均已整理好放入目录/etc/kubeasz\n4.2 创建集群配置实例\ncentos7-2009-1 centos7-2009-2 centos7-2009-3 # 容器化运行kubeasz ./ezdown -S # 创建新集群 k8s-01 docker exec -it kubeasz ezctl new k8s-01 2021-01-19 10:48:23 DEBUG generate custom cluster files in /etc/kubeasz/clusters/k8s-01 2021-01-19 10:48:23 DEBUG set version of common plugins 2021-01-19 10:48:23 DEBUG cluster k8s-01: files successfully created. 2021-01-19 10:48:23 INFO next steps 1: to config \u0026#39;/etc/kubeasz/clusters/k8s-01/hosts\u0026#39; 2021-01-19 10:48:23 INFO next steps 2: to config \u0026#39;/etc/kubeasz/clusters/k8s-01/config.yml\u0026#39; 然后根据提示配置\u0026rsquo;/etc/kubeasz/clusters/k8s-01/hosts\u0026rsquo; 和 \u0026lsquo;/etc/kubeasz/clusters/k8s-01/config.yml\u0026rsquo;：根据前面节点规划修改hosts 文件和其他集群层面的主要配置选项；其他集群组件等配置项可以在config.yml 文件中修改。\n4.3 开始安装 如果你对集群安装流程不熟悉，请阅读项目首页 安装步骤 讲解后分步安装，并对 每步都进行验证 #建议使用alias命令，查看~/.bashrc 文件应该包含：alias dk=\u0026#39;docker exec -it kubeasz\u0026#39; source ~/.bashrc # 一键安装，等价于执行docker exec -it kubeasz ezctl setup k8s-01 all dk ezctl setup k8s-01 all # 或者分步安装，具体使用 dk ezctl help setup 查看分步安装帮助信息 # dk ezctl setup k8s-01 01 # dk ezctl setup k8s-01 02 # dk ezctl setup k8s-01 03 # dk ezctl setup k8s-01 04 ... dk ezctl setup k8s-01 07 [root@centos7-2009-1 ~]# systemctl stop etcd [root@centos7-2009-1 ~]# systemctl stop kube-apiserver [root@centos7-2009-1 ~]# systemctl stop kube-scheduler [root@centos7-2009-1 ~]# systemctl stop kube-controller-manager [root@centos7-2009-1 ~]# systemctl stop kubelet 更多ezctl使用帮助，请参考[这里\n"},{"id":21,"href":"/docs/01-%E5%88%9B%E5%BB%BA%E8%AF%81%E4%B9%A6%E5%92%8C%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87-01--chuang-jian-zheng-shu-he-huan-jing-zhun-bei/","title":"01-创建证书和环境准备 2023-09-28 15:22:19.812","section":"Docs","content":" 01-创建证书和环境准备 # 本步骤主要完成:\n(optional) role:os-harden，可选系统加固，符合linux安全基线，详见upstream (optional) role:chrony，可选集群节点时间同步 role:deploy，创建CA证书、集群组件访问apiserver所需的各种kubeconfig role:prepare，系统基础环境配置、分发CA证书、kubectl客户端安装 deploy 角色 # 主要任务讲解：roles/deploy/tasks/main.yml\n创建 CA 证书 # kubernetes 系统各组件需要使用 TLS 证书对通信进行加密，使用 CloudFlare 的 PKI 工具集生成自签名的 CA 证书，用来签名后续创建的其它 TLS 证书。参考阅读\n根据认证对象可以将证书分成三类：服务器证书server cert，客户端证书client cert，对等证书peer cert(既是server cert又是client cert)，在kubernetes 集群中需要的证书种类如下：\netcd 节点需要标识自己服务的server cert，也需要client cert与etcd集群其他节点交互，当然可以分别指定2个证书，为方便这里使用一个对等证书 master 节点需要标识 apiserver服务的server cert，也需要client cert连接etcd集群，这里也使用一个对等证书 kubectl calico kube-proxy 只需要client cert，因此证书请求中 hosts 字段可以为空 kubelet 需要标识自己服务的server cert，也需要client cert请求apiserver，也使用一个对等证书 整个集群要使用统一的CA 证书，只需要在ansible控制端创建，然后分发给其他节点；为了保证安装的幂等性，如果已经存在CA 证书，就跳过创建CA 步骤\n创建 CA 配置文件 ca-config.json.j2 # { \u0026#34;signing\u0026#34;: { \u0026#34;default\u0026#34;: { \u0026#34;expiry\u0026#34;: \u0026#34;{{ CERT_EXPIRY }}\u0026#34; }, \u0026#34;profiles\u0026#34;: { \u0026#34;kubernetes\u0026#34;: { \u0026#34;usages\u0026#34;: [ \u0026#34;signing\u0026#34;, \u0026#34;key encipherment\u0026#34;, \u0026#34;server auth\u0026#34;, \u0026#34;client auth\u0026#34; ], \u0026#34;expiry\u0026#34;: \u0026#34;{{ CERT_EXPIRY }}\u0026#34; }, \u0026#34;kcfg\u0026#34;: { \u0026#34;usages\u0026#34;: [ \u0026#34;signing\u0026#34;, \u0026#34;key encipherment\u0026#34;, \u0026#34;client auth\u0026#34; ], \u0026#34;expiry\u0026#34;: \u0026#34;{{ CUSTOM_EXPIRY }}\u0026#34; } } } } signing：表示该证书可用于签名其它证书；生成的 ca.pem 证书中 CA=TRUE； server auth：表示可以用该 CA 对 server 提供的证书进行验证； client auth：表示可以用该 CA 对 client 提供的证书进行验证； profile kubernetes 包含了server auth和client auth，所以可以签发三种不同类型证书；expiry 证书有效期，默认50年 profile kcfg 在后面客户端kubeconfig证书管理中用到 创建 CA 证书签名请求 ca-csr.json.j2 # { \u0026#34;CN\u0026#34;: \u0026#34;kubernetes-ca\u0026#34;, \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;CN\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;HangZhou\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;XS\u0026#34;, \u0026#34;O\u0026#34;: \u0026#34;k8s\u0026#34;, \u0026#34;OU\u0026#34;: \u0026#34;System\u0026#34; } ], \u0026#34;ca\u0026#34;: { \u0026#34;expiry\u0026#34;: \u0026#34;876000h\u0026#34; } } ca expiry 指定ca证书的有效期，默认100年 生成CA 证书和私钥 # cfssl gencert -initca ca-csr.json | cfssljson -bare ca 生成 kubeconfig 配置文件 # kubectl使用~/.kube/config 配置文件与kube-apiserver进行交互，且拥有管理 K8S集群的完全权限，\n准备kubectl使用的admin 证书签名请求 admin-csr.json.j2\n{ \u0026#34;CN\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;hosts\u0026#34;: [], \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;CN\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;HangZhou\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;XS\u0026#34;, \u0026#34;O\u0026#34;: \u0026#34;system:masters\u0026#34;, \u0026#34;OU\u0026#34;: \u0026#34;System\u0026#34; } ] } kubectl 使用客户端证书可以不指定hosts 字段 证书请求中 O 指定该证书的 Group 为 system:masters，而 RBAC 预定义的 ClusterRoleBinding 将 Group system:masters 与 ClusterRole cluster-admin 绑定，这就赋予了kubectl所有集群权限 $ kubectl describe clusterrolebinding cluster-admin Name: cluster-admin Labels: kubernetes.io/bootstrapping=rbac-defaults Annotations: rbac.authorization.kubernetes.io/autoupdate=true Role: Kind: ClusterRole Name: cluster-admin Subjects: Kind Name Namespace ---- ---- --------- Group system:masters 生成 admin 用户证书 # cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin 生成 ~/.kube/config 配置文件 # 使用kubectl config 生成kubeconfig 自动保存到 ~/.kube/config，生成后 cat ~/.kube/config可以验证配置文件包含 kube-apiserver 地址、证书、用户名等信息。\nkubectl config set-cluster kubernetes --certificate-authority=ca.pem --embed-certs=true --server=127.0.0.1:8443 kubectl config set-credentials admin --client-certificate=admin.pem --embed-certs=true --client-key=admin-key.pem kubectl config set-context kubernetes --cluster=kubernetes --user=admin kubectl config use-context kubernetes 生成 kube-proxy.kubeconfig 配置文件 # 创建 kube-proxy 证书请求\n{ \u0026#34;CN\u0026#34;: \u0026#34;system:kube-proxy\u0026#34;, \u0026#34;hosts\u0026#34;: [], \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;CN\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;HangZhou\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;XS\u0026#34;, \u0026#34;O\u0026#34;: \u0026#34;k8s\u0026#34;, \u0026#34;OU\u0026#34;: \u0026#34;System\u0026#34; } ] } kube-proxy 使用客户端证书可以不指定hosts 字段 CN 指定该证书的 User 为 system:kube-proxy，预定义的 ClusterRoleBinding system:node-proxier 将User system:kube-proxy 与 Role system:node-proxier 绑定，授予了调用 kube-apiserver Proxy 相关 API 的权限； $ kubectl describe clusterrolebinding system:node-proxier Name: system:node-proxier Labels: kubernetes.io/bootstrapping=rbac-defaults Annotations: rbac.authorization.kubernetes.io/autoupdate=true Role: Kind: ClusterRole Name: system:node-proxier Subjects: Kind Name Namespace ---- ---- --------- User system:kube-proxy 生成 system:kube-proxy 用户证书 # cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy 生成 kube-proxy.kubeconfig # 使用kubectl config 生成kubeconfig 自动保存到 kube-proxy.kubeconfig\nkubectl config set-cluster kubernetes --certificate-authority=ca.pem --embed-certs=true --server=127.0.0.1:8443 --kubeconfig=kube-proxy.kubeconfig kubectl config set-credentials kube-proxy --client-certificate=kube-proxy.pem --embed-certs=true --client-key=kube-proxy-key.pem --kubeconfig=kube-proxy.kubeconfig kubectl config set-context default --cluster=kubernetes --user=kube-proxy --kubeconfig=kube-proxy.kubeconfig kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig 创建kube-controller-manager 和 kube-scheduler 组件的kubeconfig 文件 # 过程与创建kube-proxy.kubeconfig 类似，略。\nprepare 角色 # 请在另外窗口打开roles/prepare/tasks/main.yml 文件，比较简单直观\n设置基础操作系统软件和系统参数，请阅读脚本中的注释内容 创建一些基础文件目录、环境变量以及添加本地镜像仓库easzlab.io.local的域名解析 分发kubeconfig等配置文件 后一篇\n"},{"id":22,"href":"/docs/02-%E5%AE%89%E8%A3%85etcd%E9%9B%86%E7%BE%A4-02--an-zhuang-etcd-ji-qun/","title":"02-安装etcd集群 2023-09-28 15:23:07.363","section":"Docs","content":" 02-安装etcd集群 # kuberntes 集群使用 etcd 存储所有数据，是最重要的组件之一，注意 etcd集群需要奇数个节点(1,3,5\u0026hellip;)，本文档使用3个节点做集群。\n请在另外窗口打开roles/etcd/tasks/main.yml 文件，对照看以下讲解内容。\n创建etcd证书 # 注意：证书是在部署节点创建好之后推送到目标etcd节点上去的，以增加ca证书的安全性\n创建ectd证书请求 etcd-csr.json.j2\n{ \u0026#34;CN\u0026#34;: \u0026#34;etcd\u0026#34;, \u0026#34;hosts\u0026#34;: [ {% for host in groups[\u0026#39;etcd\u0026#39;] %} \u0026#34;{{ host }}\u0026#34;, {% endfor %} \u0026#34;127.0.0.1\u0026#34; ], \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;CN\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;HangZhou\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;XS\u0026#34;, \u0026#34;O\u0026#34;: \u0026#34;k8s\u0026#34;, \u0026#34;OU\u0026#34;: \u0026#34;System\u0026#34; } ] } etcd使用对等证书，hosts 字段必须指定授权使用该证书的 etcd 节点 IP，这里枚举了所有ectd节点的地址 创建etcd 服务文件 etcd.service.j2 # [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target Documentation=https://github.com/coreos [Service] Type=notify WorkingDirectory={{ ETCD_DATA_DIR }} ExecStart={{ bin_dir }}/etcd \\ --name=etcd-{{ inventory_hostname }} \\ --cert-file={{ ca_dir }}/etcd.pem \\ --key-file={{ ca_dir }}/etcd-key.pem \\ --peer-cert-file={{ ca_dir }}/etcd.pem \\ --peer-key-file={{ ca_dir }}/etcd-key.pem \\ --trusted-ca-file={{ ca_dir }}/ca.pem \\ --peer-trusted-ca-file={{ ca_dir }}/ca.pem \\ --initial-advertise-peer-urls=https://{{ inventory_hostname }}:2380 \\ --listen-peer-urls=https://{{ inventory_hostname }}:2380 \\ --listen-client-urls=https://{{ inventory_hostname }}:2379,http://127.0.0.1:2379 \\ --advertise-client-urls=https://{{ inventory_hostname }}:2379 \\ --initial-cluster-token=etcd-cluster-0 \\ --initial-cluster={{ ETCD_NODES }} \\ --initial-cluster-state={{ CLUSTER_STATE }} \\ --data-dir={{ ETCD_DATA_DIR }} \\ --wal-dir={{ ETCD_WAL_DIR }} \\ --snapshot-count=50000 \\ --auto-compaction-retention=1 \\ --auto-compaction-mode=periodic \\ --max-request-bytes=10485760 \\ --quota-backend-bytes=8589934592 Restart=always RestartSec=15 LimitNOFILE=65536 OOMScoreAdjust=-999 [Install] WantedBy=multi-user.target 完整参数列表请使用 etcd --help 查询 注意etcd 即需要服务器证书也需要客户端证书，为方便使用一个peer 证书代替两个证书 --initial-cluster-state 值为 new 时，--name 的参数值必须位于 --initial-cluster 列表中 --snapshot-count --auto-compaction-retention 一些性能优化参数，请查阅etcd项目文档 设置--data-dir 和--wal-dir 使用不同磁盘目录，可以避免磁盘io竞争，提高性能，具体请参考etcd项目文档 验证etcd集群状态 # systemctl status etcd 查看服务状态 journalctl -u etcd 查看运行日志 在任一 etcd 集群节点上执行如下命令 # 根据hosts中配置设置shell变量 $NODE_IPS export NODE_IPS=\u0026#34;192.168.1.1 192.168.1.2 192.168.1.3\u0026#34; for ip in ${NODE_IPS}; do ETCDCTL_API=3 etcdctl \\ --endpoints=https://${ip}:2379 \\ --cacert=/etc/kubernetes/ssl/ca.pem \\ --cert=/etc/kubernetes/ssl/etcd.pem \\ --key=/etc/kubernetes/ssl/etcd-key.pem \\ endpoint health; done for ip in ${NODE_IPS}; do ETCDCTL_API=3 etcdctl \\ --endpoints=https://${ip}:2379 \\ --cacert=/etc/kubernetes/ssl/ca.pem \\ --cert=/etc/kubernetes/ssl/etcd.pem \\ --key=/etc/kubernetes/ssl/etcd-key.pem \\ --write-out=table endpoint status; done # 根据hosts中配置设置shell变量 $NODE_IPS export NODE_IPS=\u0026#34;192.168.0.51 192.168.0.52 192.168.0.53\u0026#34; for ip in ${NODE_IPS}; do ETCDCTL_API=3 etcdctl \\ --endpoints=https://${ip}:2379 \\ --cacert=/etc/kubernetes/ssl/ca.pem \\ --cert=/etc/kubernetes/ssl/etcd.pem \\ --key=/etc/kubernetes/ssl/etcd-key.pem \\ endpoint health; done for ip in ${NODE_IPS}; do ETCDCTL_API=3 etcdctl \\ --endpoints=https://${ip}:2379 \\ --cacert=/etc/kubernetes/ssl/ca.pem \\ --cert=/etc/kubernetes/ssl/etcd.pem \\ --key=/etc/kubernetes/ssl/etcd-key.pem \\ --write-out=table endpoint status; done 预期结果：\nhttps://192.168.1.1:2379 is healthy: successfully committed proposal: took = 2.210885ms https://192.168.1.2:2379 is healthy: successfully committed proposal: took = 2.784043ms https://192.168.1.3:2379 is healthy: successfully committed proposal: took = 3.275709ms 三台 etcd 的输出均为 healthy 时表示集群服务正常。\n后一篇\n"},{"id":23,"href":"/docs/2025-2-24-%E9%9D%A2%E8%AF%950224/","title":"0224面试","section":"Docs","content":" 比较好的监控是怎样的 prometheus grafana alertmanager 考虑存储 大规模 2000台机器 考虑告警延迟 考虑长期储存 考虑监控业务的全面性 准确性 告警实时性 # 在你的场景中，涉及到大规模的监控部署，2000台机器、长时间的存储、实时告警等多个方面。要构建一个高效、可靠的监控系统，你可以基于 Prometheus、Grafana 和 Alertmanager 这些工具进行设计，但需要优化配置，以满足大规模、低延迟、高可用、长时间存储等需求。\n1. Prometheus 架构设计 # 水平扩展：单个 Prometheus 实例无法支撑大规模的数据采集和存储，因此需要考虑水平扩展。可以使用多个 Prometheus 实例分担不同的监控目标（例如，按区域或按机器类型分配 Prometheus 实例）。也可以使用 Prometheus Federation（联邦）来将多个 Prometheus 实例的指标数据汇聚到一个主 Prometheus 实例中。 存储优化：Prometheus 本身的存储是针对短期数据存储优化的，支持时序数据的压缩。但对于长期存储，Prometheus 并不适合将数据保留很长时间。可以使用 Thanos 或 Cortex 来为 Prometheus 提供分布式存储，支持长期数据保留。这样可以跨多个 Prometheus 实例集中存储数据，进行长时间查询。 数据分片：如果你的监控数据量非常大（如 2000 台机器），你可以通过数据分片（sharding）来分散负载，减少单个 Prometheus 实例的压力。Thanos 和 Cortex 都支持这种分片模型。 2. Grafana 可视化 # 仪表盘设计：Grafana 适用于展示大规模监控数据。你可以设计多个针对不同层级的仪表盘，例如：硬件监控、应用性能监控、网络流量等。Grafana 允许将数据按需聚合或细分，以帮助从不同维度监控业务指标。 分布式查询：通过 Thanos 或 Cortex 等分布式存储系统，Grafana 可以访问跨多个 Prometheus 实例的数据，呈现完整的监控视图。 查询性能优化：随着监控系统规模的扩大，Grafana 查询的数据量增大。你需要优化 Prometheus 的查询性能，避免低效的查询（如不加时间范围的查询），或者使用预计算的聚合数据来减轻查询负担。 3. Alertmanager 告警管理 # 告警延迟：为了确保告警的实时性，Prometheus 的抓取周期（默认 15 秒）需要优化。如果告警的延迟非常关键，可能需要缩短抓取周期或者增加更多 Prometheus 实例，分担告警流量。 告警策略设计：针对大规模的监控系统，告警的去重和抑制非常重要，避免过多无效告警。可以根据告警的严重性、关联性进行聚合，避免“告警风暴”。Alertmanager 可以进行告警的抑制和分组，减少重复告警。 通知渠道：Alertmanager 可以与 Slack、邮件、Webhook 等通知渠道集成，根据告警级别不同，选择不同的通知方式（例如，严重告警通过短信，轻微告警通过邮件）。 告警灵敏度：在大规模系统中，容易产生很多低优先级的告警。可以通过设置 Prometheus 的告警规则，精细化告警条件，确保告警的准确性和有效性。 4. 存储与长期数据保留 # 长期存储解决方案\n：Prometheus 默认的数据存储只适合短期使用。对于长期存储，可以引入\nThanos\n或\nCortex\n，这些工具可以将数据长期存储，并且提供跨 Prometheus 实例的查询功能。\nThanos：Thanos 提供了一种基于对象存储的分布式存储解决方案，可以将数据从本地存储推送到云端或分布式对象存储（如 AWS S3），并且支持数据的压缩与索引。 Cortex：Cortex 是另一个用于分布式时序数据存储的开源项目，它提供水平扩展、长期存储等特性，适合大规模监控部署。 Cold Storage：对于不经常访问的数据，可以将较老的时序数据存储到 Cold Storage 中，减少对存储性能的影响。\n5. 业务全面性和准确性 # 指标覆盖全面：确保监控覆盖所有业务的关键指标，除了常规的硬件、网络、应用性能指标外，还要考虑业务指标（如用户行为、支付订单量等）。可以使用 Prometheus Exporter 来收集更多应用特定的指标。 PromQL 查询优化：为了确保监控数据的准确性和高效性，使用 PromQL 时需要优化查询，避免过度聚合和计算。例如，可以使用汇总（rate、avg）等函数在 Prometheus 采集数据时进行计算，减少数据量，提高查询效率。 6. 监控系统的高可用性 # Prometheus 高可用性：可以通过 Prometheus HA 部署多个 Prometheus 实例来保证高可用性。例如，主 Prometheus 实例失效时，副本可以接管工作。 Alertmanager 高可用性：Alertmanager 也需要做高可用部署，可以通过 Alertmanager 集群 来实现。 冗余存储：通过 Thanos 或 Cortex 等解决方案提供冗余存储，以确保数据不丢失。 总结 # 对于大规模监控系统（如 2000 台机器），你需要：\n分布式 Prometheus 架构：使用多个 Prometheus 实例，并结合 Thanos 或 Cortex 等分布式存储解决方案进行长期数据存储。 高效的告警策略：优化告警规则，使用告警去重和抑制，确保告警的实时性和准确性。 灵活的可视化设计：使用 Grafana 来设计分层、灵活的仪表盘，确保监控的全面性。 优化存储和查询：通过压缩、分片、冷存储等手段降低存储成本并提高查询性能。 通过这些优化，你可以确保监控系统不仅能够处理大规模的环境，还能保证告警和数据查询的实时性和准确性。你有具体的部署需求或技术栈细节吗？\n数据库的高可用的设计 # 数据库的高可用性设计是确保数据库服务在出现故障时，能够自动恢复并保持最小的停机时间，以确保系统的连续性和可靠性。为了实现高可用性，通常会涉及到数据复制、故障转移、负载均衡、备份策略等多个方面。\n下面是几种常见的数据库高可用设计方案：\n1. 主从复制 (Master-Slave Replication) # 主从复制是数据库高可用性的经典设计，主数据库处理写操作，从数据库负责读取操作。这种设计可以通过从数据库来分担读取负载，同时在主数据库故障时，能够将从数据库提升为新的主数据库。\n特点： # 主数据库：负责处理所有写操作。 从数据库：通过同步复制，接收主数据库的变更，处理读取操作。 故障转移：如果主数据库不可用，从数据库可以通过自动或手动的方式提升为主数据库。 缺点： # 主从复制通常是异步的，这意味着如果主数据库发生故障，从数据库可能会丢失一些未同步的数据。 实现： # MySQL 和 PostgreSQL 都支持主从复制。 MySQL Group Replication 或 PostgreSQL Streaming Replication 可提供更强的复制和故障转移能力。 2. 双主复制 (Master-Master Replication) # 双主复制通过让两个数据库实例互相作为主数据库来实现高可用性。这种方案允许数据库之间相互复制，且两个数据库都可以进行写操作。\n特点： # 双主复制使得两个数据库实例可以同时进行读写操作。 当一个数据库实例发生故障时，另一个实例仍然可以提供服务。 比主从复制的负载分担能力更强。 缺点： # 数据冲突：如果两个主数据库同时进行写操作，可能会发生数据冲突。需要通过解决冲突的机制（如版本控制或冲突解决策略）来避免这种情况。 相较于主从复制，双主复制的实现更复杂，特别是在数据库的写冲突管理上。 实现： # MySQL 的 Group Replication 或 Galera Cluster。 PostgreSQL 使用 Bucardo 或 PgPool-II。 3. 数据库集群 (Database Clustering) # 数据库集群将多个数据库节点组成一个集群，通常集群中的每个节点都有相同的数据副本，并通过一致性协议来保证数据的一致性。集群模式通常能提供更高的可用性和更强的数据冗余能力。\n特点： # 集群中的每个节点都存储数据副本，并具有读写能力。 提供自动故障转移和负载均衡。 数据一致性通常使用分布式协议（如 Paxos 或 Raft）来确保。 实现： # MySQL Cluster：基于 NDB (Network Database) 存储引擎，支持高可用性、自动分区和自动故障转移。 PostgreSQL：通过 Citus 扩展实现分布式数据库集群。 Cassandra 和 MongoDB：本身是分布式数据库，原生支持高可用性和分布式存储。 4. 故障转移与负载均衡 (Failover and Load Balancing) # 故障转移是指在主数据库节点发生故障时，自动切换到一个备份数据库（从数据库或复制节点），保持服务可用。而负载均衡则用于在多个数据库节点之间分配读写请求。\n实现： # 故障转移管理工具：例如 MHA (Master High Availability) 或 Orchestrator，可以用于自动监控数据库实例，发生故障时自动将流量切换到健康的节点。 负载均衡器：可以使用负载均衡器（如 HAProxy、ProxySQL 或 PgBouncer）来将请求分配到多个数据库实例，从而实现负载均衡。 5. 同步与异步复制 # 同步复制：所有数据库节点都保持一致性，所有写操作必须在所有节点上成功才能完成。这种方法提供更强的一致性，但会影响性能，特别是在远程节点时延较大的情况下。 异步复制：主数据库先提交写操作，数据随后异步同步到从节点。这种方法更注重性能，能够提高系统吞吐量，但在主数据库故障时可能会丢失一些未同步的数据。 实现： # MySQL 支持同步复制（通过 GTID 或 Semi-Synchronous Replication）。 PostgreSQL 支持流式复制，可以通过配置不同的同步复制策略来实现。 6. 备份与恢复 (Backup and Restore) # 定期备份数据库是确保数据安全和可恢复性的基础。数据库的高可用性不仅仅依赖于实时的故障转移，还需要有健全的备份策略。\n特点： # 定期备份数据。 增量备份：只备份发生变化的数据，减少备份时间和存储空间。 异地备份：将备份存储到不同地理位置，以防止由于自然灾害等因素导致的数据丢失。 实现： # 使用数据库的内建工具，如 mysqldump、pg_dump、Percona XtraBackup 等，结合脚本定期备份。 数据备份需要确保备份文件的存储与恢复高效性，尽量避免因备份过程导致的性能瓶颈。 7. 云原生数据库高可用设计 # 如果部署在云环境中（如 AWS、Azure 或 Google Cloud），云平台通常提供数据库的高可用性功能，如自动备份、自动故障转移、多区域部署等。\n特点： # 自动故障转移：云平台可提供跨可用区的自动故障转移。 分布式存储：数据可以分布在多个区域，提高容灾能力。 弹性扩展：根据负载需求自动扩展计算和存储资源。 实现： # Amazon RDS 和 Aurora：提供数据库自动备份、故障转移和多区域部署功能。 Google Cloud SQL 和 Azure Database：提供高可用性和自动扩展功能。 总结：高可用数据库设计的考虑要点 # 冗余和数据复制：通过主从、双主或集群复制来保证数据的冗余。 故障转移与恢复：自动化故障转移机制，最小化系统的停机时间。 负载均衡：通过负载均衡器分担读写请求，提高吞吐量。 备份策略：定期备份数据库，并确保可以在灾难发生时快速恢复。 监控和告警：实时监控数据库健康状态，设置合理的告警策略，确保在出现故障时能够及时响应。 你是否有特定的数据库类型或架构部署需求，或者是否涉及到云平台或自建环境的高可用设计？这样可以更有针对性地给出建议。\n业务监控怎么做的？ # 业务监控是确保业务系统（无论是网站、应用还是其他类型的服务）健康、稳定、可靠的关键环节。它不仅关注系统层面的性能指标（如 CPU、内存、网络带宽等），还关注业务本身的指标，如用户行为、订单量、收入、转化率等。有效的业务监控帮助团队及时发现问题、分析瓶颈、优化流程，并做出更好的决策。\n业务监控设计的关键步骤： # 定义关键业务指标 (KPI) 选择合适的监控工具 设置告警与响应机制 实时监控与数据分析 监控系统的维护与优化 1. 定义关键业务指标 (KPI) # 首先，确定需要监控的业务指标是关键步骤。不同的业务场景，监控的指标也会有所不同。以下是一些常见的业务监控指标：\n常见业务监控指标： # 用户相关指标\n：\n活跃用户数 (DAU/MAU)：日活跃用户数和月活跃用户数，反映用户参与度。 用户留存率：例如第1天、第7天、第30天的留存率，衡量用户粘性。 用户转化率：如从注册到购买的转化率，帮助分析用户行为路径。 业务行为指标\n：\n订单量、销售额：直接衡量业务收入。 用户行为路径：用户从浏览、加入购物车到最终购买的转化路径。 支付成功率：反映支付环节的顺畅度。 系统健康指标\n：\n响应时间：业务接口或页面的响应时间，过长可能影响用户体验。 错误率：如HTTP 5xx 错误，API 错误，监控是否存在服务故障。 系统负载：监控服务器、数据库的负载情况，防止过载。 业务指标示例： # 电商平台：订单量、购物车放弃率、支付成功率、转化率、退款率。 SaaS 平台：用户注册数、付费用户数、功能使用频率、续费率。 社交平台：用户活跃度、评论数、分享数。 2. 选择合适的监控工具 # 为了有效地监控业务，你可以选择合适的监控工具。根据需求，监控工具可以分为 系统层级监控 和 业务层级监控。\n系统层级监控工具： # Prometheus + Grafana：用于监控系统性能指标（如 CPU 使用率、内存、硬盘、网络流量等）并将其可视化，提供详细的图表和报警功能。 Datadog：综合性能监控工具，支持云、容器、应用程序等的全栈监控，适合大规模微服务环境。 Zabbix、Nagios：传统的监控工具，用于监控主机、网络设备、应用程序等。 业务层级监控工具： # Google Analytics、Mixpanel、Amplitude：这些工具可以帮助你收集用户行为数据，分析用户的使用模式、转化率、留存等业务数据。 Prometheus + 自定义 Metrics Exporter：通过将业务指标暴露为 Prometheus 监控的形式，可以对业务指标进行自定义监控和告警设置。 Elasticsearch + Kibana (ELK Stack)：用于日志分析，可以挖掘出用户行为数据、业务异常等信息。 Grafana + InfluxDB：适合高频率的时序数据监控，常用于监控业务系统的实时数据。 3. 设置告警与响应机制 # 业务监控的核心之一是告警系统，确保你能够及时发现问题并采取行动。告警设计不仅仅要监控系统崩溃或性能瓶颈，还要能够对业务异常做出反应。\n告警策略： # 阈值告警：当某个指标超过或低于预定的阈值时触发告警。例如，订单量异常波动、支付成功率过低、错误率过高等。 趋势告警：监控指标的增长趋势或衰退趋势，例如用户活跃度下降、订单数量连续数小时下降等。 异常检测：一些先进的监控系统（如 Datadog、Prometheus）可以根据历史数据和模式分析，自动发现异常变化，自动触发告警。 告警渠道： # Slack、微信、钉钉：将告警信息推送到团队沟通工具，确保团队可以及时响应。 邮件、短信、电话：对于严重故障，可能需要通过电话、短信等方式告知相关人员。 自动化响应：通过集成自动化工具（如 Runbook），在发生常见问题时可以自动触发修复流程。 4. 实时监控与数据分析 # 实时监控是确保业务持续健康运营的核心。除了实时监控的核心业务指标外，还需要综合分析，识别潜在的问题。\n关键实时数据： # 数据仪表盘：在 Grafana、Datadog 或 Kibana 上创建实时仪表盘，展示核心业务指标（如活跃用户、转化率、订单量等）。 追踪用户行为：集成用户行为分析工具，如 Google Analytics、Mixpanel，监控用户在系统中的路径和关键行为，识别潜在的用户流失或问题区域。 数据分析： # 回归分析：基于历史数据分析出业务的季节性变化、周期性波动，预测未来的趋势和需求。 A/B 测试：在业务上做出更改时（例如修改用户界面、调整流程等），可以使用 A/B 测试来验证新变化是否提高了业务指标。 5. 监控系统的维护与优化 # 随着业务的发展和系统的变化，业务监控也需要不断地维护和优化。定期评估和调整监控策略是非常重要的。\n维护工作： # 定期回顾监控指标：确保业务监控覆盖了所有重要的业务指标，及时更新和优化监控内容。 优化告警设置：告警的频率和内容需要根据实际情况进行调整，避免“告警疲劳”，同时确保高优先级告警不会被遗漏。 性能优化：优化监控系统的性能，避免监控本身对生产环境造成额外的压力。 高效的报警管理： # 告警去重和合并：避免相同问题引发多个告警。使用合并、去重等策略，将相似的告警合并为一个事件进行处理。 总结 # 业务监控的核心目标是确保业务运行的健康和可持续性。有效的业务监控不仅帮助你确保系统正常运作，还能帮助你识别潜在的业务瓶颈、改进用户体验、优化业务流程。设计业务监控时，要：\n确定并定义关键业务指标（KPI）。 使用合适的工具收集和分析业务数据。 配置实时监控和告警系统，确保能够快速响应。 定期优化监控和告警策略，确保监控系统高效稳定。 如果你有特定的业务场景或者需要更详细的实现建议，我可以提供更具体的指导！\n业务pod告警规则设计和例子 # 在 Kubernetes 环境中，使用 Prometheus 和 Alertmanager 结合 Pod 监控和告警是非常常见的做法。以下是一些设计业务 Pod 的告警规则的常见场景，以及具体的告警规则示例。\n1. 监控指标 # 首先，确保你已经在 Kubernetes 中通过 Prometheus Operator 或者 Kube-Prometheus 等工具集成了 Prometheus 监控，并能够收集到以下指标：\nPod CPU 使用率：container_cpu_usage_seconds_total Pod 内存使用率：container_memory_usage_bytes Pod 网络流量：container_network_receive_bytes_total、container_network_transmit_bytes_total Pod 磁盘 I/O：container_fs_usage_bytes Pod 重启次数：kube_pod_container_status_restarts_total Pod 状态：kube_pod_status_phase（如 Pending、Running、Succeeded、Failed 等） Pod 健康检查：kube_pod_container_status_ready 2. 常见告警规则设计 # 业务 Pod 的告警规则需要涵盖以下几个方面：\na. 资源使用告警（CPU、内存等） # 这些告警规则主要是帮助你及时发现资源瓶颈或者不正常的资源消耗。过高的 CPU 或内存使用率可能会导致 Pod 变慢或者重启。\nCPU 使用率告警：\n- alert: PodHighCPUUsage expr: rate(container_cpu_usage_seconds_total{container!=\u0026#34;\u0026#34;,pod=~\u0026#34;.*\u0026#34;}[5m]) \u0026gt; 0.9 for: 1m labels: severity: critical annotations: summary: \u0026#34;Pod {{ $labels.pod }} is using too much CPU ({{ $value }} cores)\u0026#34; description: \u0026#34;Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is consuming too much CPU.\u0026#34; 说明： 这个规则表示如果 Pod 的 CPU 使用率超过 90%（1 核 CPU），并且持续时间达到 1 分钟，则触发告警。\n内存使用率告警：\n- alert: PodHighMemoryUsage expr: container_memory_usage_bytes{container!=\u0026#34;\u0026#34;,pod=~\u0026#34;.*\u0026#34;} / container_spec_memory_limit_bytes{container!=\u0026#34;\u0026#34;,pod=~\u0026#34;.*\u0026#34;} \u0026gt; 0.9 for: 1m labels: severity: critical annotations: summary: \u0026#34;Pod {{ $labels.pod }} is using too much memory ({{ $value }} bytes)\u0026#34; description: \u0026#34;Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is consuming too much memory.\u0026#34; 说明： 如果 Pod 的内存使用超过 90%（container_spec_memory_limit_bytes），并且持续时间超过 1 分钟，则触发告警。\nb. Pod 重启次数告警 # Pod 重启次数过多通常表示应用程序存在不稳定性。你可以使用此告警规则检测 Pod 重启情况。\n- alert: PodRestartingTooManyTimes expr: kube_pod_container_status_restarts_total{container!=\u0026#34;\u0026#34;,pod=~\u0026#34;.*\u0026#34;} \u0026gt; 5 for: 5m labels: severity: critical annotations: summary: \u0026#34;Pod {{ $labels.pod }} has restarted more than 5 times in the last 5 minutes\u0026#34; description: \u0026#34;Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} has restarted {{ $value }} times in the last 5 minutes.\u0026#34; 说明： 如果某个 Pod 在 5 分钟内重启次数超过 5 次，则触发告警。\nc. Pod 健康检查失败告警 # 如果 Pod 无法通过健康检查，通常是应用程序出现问题。这类告警可以帮助你及时发现应用不健康的状态。\n- alert: PodUnhealthy expr: kube_pod_container_status_ready{container!=\u0026#34;\u0026#34;,pod=~\u0026#34;.*\u0026#34;} == 0 for: 2m labels: severity: critical annotations: summary: \u0026#34;Pod {{ $labels.pod }} is not ready\u0026#34; description: \u0026#34;Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} has failed the readiness check for more than 2 minutes.\u0026#34; 说明： 如果 Pod 在 2 分钟内无法通过准备就绪检查（container_status_ready == 0），则触发告警。\nd. Pod 启动延迟告警 # 如果 Pod 启动时间过长，可能是应用或环境存在问题。通过监控 Pod 的启动时间，可以提前发现潜在问题。\n- alert: PodStartupDelay expr: time() - kube_pod_start_time{pod=~\u0026#34;.*\u0026#34;} \u0026gt; 600 labels: severity: high annotations: summary: \u0026#34;Pod {{ $labels.pod }} is taking too long to start\u0026#34; description: \u0026#34;Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} has not started within 10 minutes.\u0026#34; 说明： 如果 Pod 启动时间超过 10 分钟，则触发告警。\ne. Pod 状态异常告警 # Pod 的状态异常（如进入 CrashLoopBackOff、Pending 等）也需要进行监控。这些告警有助于检测 Pods 是否发生了故障或无法调度。\n- alert: PodStatusFailed expr: kube_pod_status_phase{phase=\u0026#34;Failed\u0026#34;,pod=~\u0026#34;.*\u0026#34;} \u0026gt; 0 for: 5m labels: severity: critical annotations: summary: \u0026#34;Pod {{ $labels.pod }} has failed\u0026#34; description: \u0026#34;Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} has been in the Failed state for more than 5 minutes.\u0026#34; 说明： 如果 Pod 处于 Failed 状态超过 5 分钟，则触发告警。\nf. 网络流量告警 # 监控 Pod 的网络流量（入站和出站流量）可以帮助你识别潜在的网络瓶颈或滥用。\n- alert: PodHighNetworkUsage expr: (rate(container_network_receive_bytes_total{pod=~\u0026#34;.*\u0026#34;}[5m]) + rate(container_network_transmit_bytes_total{pod=~\u0026#34;.*\u0026#34;}[5m])) \u0026gt; 100000000 for: 5m labels: severity: medium annotations: summary: \u0026#34;Pod {{ $labels.pod }} is using too much network bandwidth\u0026#34; description: \u0026#34;Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is using more than 100 MB of network bandwidth in the last 5 minutes.\u0026#34; 说明： 如果某个 Pod 在过去 5 分钟内的网络流量超过 100 MB，则触发告警。\n3. 告警规则设置中的注意事项 # for 参数：防止瞬时的波动导致频繁触发告警。通常会设置一定的时间阈值，只有当条件持续一段时间时才会触发告警。 severity 标签：根据不同的告警严重性（如 critical、warning、info 等）来分配优先级，有助于团队更好地响应告警。 告警延迟和去重：避免同一问题频繁发送告警，通过告警去重、合并等机制减少告警噪音。 4. 告警通知与响应 # Alertmanager：可以将这些告警集成到 Alertmanager 中，并根据告警的严重性将告警通过 Slack、邮件、钉钉等渠道通知给相关团队。 自动化响应：例如，可以设置自动化的脚本，触发某些操作来解决常见问题（如重新启动 Pod，或者重新调度 Pod 到健康节点）。 总结 # 业务 Pod 的告警规则设计需要根据业务需求来定制，关注应用的稳定性、性能瓶颈以及潜在的故障。通过合理的告警规则，可以确保及时发现问题并采取必要的修复措施，提高业务系统的可用性和稳定性。\n还有吗？比如黑盒探针检测 借口探活 # 当然，除了直接监控资源使用、Pod 重启等指标，**黑盒探针（Black-box Probing）和接口健康检查（Endpoint Health Checks）**也是业务监控和告警中非常重要的一部分，尤其是在微服务架构中，确保每个服务或组件是否健康、正常运行。\n1. 黑盒探针（Black-box Probing） # 黑盒探针是从外部检测应用服务的健康状况，而不需要访问服务内部的状态。常见的黑盒探针通常是通过 HTTP 请求、TCP 连接或者其他协议来模拟用户请求，确认服务是否能够正常响应。这种方式特别适用于无法直接访问内部状态的场景。\na. HTTP 探针（HTTP Probe） # 通常应用程序会暴露一个 HTTP 健康检查接口（例如 /health 或 /status），黑盒探针可以通过定期访问该接口来检测服务的健康状况。\n健康检查规则示例：\n- alert: HTTPHealthCheckFailed expr: http_requests_total{job=\u0026#34;your-service\u0026#34;, status=\u0026#34;5xx\u0026#34;} \u0026gt; 10 for: 5m labels: severity: critical annotations: summary: \u0026#34;Your service has returned 5xx errors more than 10 times in the last 5 minutes\u0026#34; description: \u0026#34;The health check endpoint for your service has failed and returned 5xx errors.\u0026#34; 说明： 如果健康检查的 HTTP 请求在 5 分钟内返回了超过 10 次的 5xx 错误，说明服务出现了问题，触发告警。\nb. TCP 探针 # 如果你的应用服务不提供 HTTP 接口，你可以通过 TCP 探针进行监控，验证服务端口是否可用。这通常适用于数据库、消息队列等服务。\nTCP 探针示例：\n- alert: TCPConnectionFailed expr: increase(tcp_connections_failed_total{job=\u0026#34;your-service\u0026#34;, port=\u0026#34;your-port\u0026#34;}[5m]) \u0026gt; 0 for: 5m labels: severity: critical annotations: summary: \u0026#34;TCP connection to your service failed\u0026#34; description: \u0026#34;There have been failed TCP connections to your service on port {{ $labels.port }} in the last 5 minutes.\u0026#34; 说明： 如果在 5 分钟内某个服务的 TCP 连接失败次数大于 0 次，触发告警。\nc. DNS 查询探针 # 你还可以通过 DNS 查询来检测域名解析的健康状况，尤其在使用云服务或容器化服务时，可能需要确保 DNS 服务能够正确解析。\nDNS 探针示例：\n- alert: DNSResolutionFailed expr: increase(dns_lookup_failure_total{job=\u0026#34;your-dns-probe\u0026#34;}[5m]) \u0026gt; 0 for: 5m labels: severity: high annotations: summary: \u0026#34;DNS resolution failure detected\u0026#34; description: \u0026#34;DNS resolution failure detected for domain {{ $labels.domain }}.\u0026#34; 说明： 如果某个 DNS 查询在过去 5 分钟内失败，触发告警。\n2. 接口健康检查（Endpoint Health Check） # 接口健康检查是在应用服务中定义的专门的健康检查端点，这些端点暴露了服务自身的一些基本健康状态和资源依赖状态。例如，数据库连接、缓存系统、外部 API 的连接状态等。通过定期访问这些健康检查端点，可以确保业务的各项依赖是否正常。\na. Web 服务健康检查端点 # 应用程序通常会暴露一个 HTTP 服务健康检查端点（如 /healthz），用于返回服务是否健康。这些端点通常会检查数据库连接、队列服务、外部依赖等。\n接口健康检查告警规则：\n- alert: WebServiceHealthCheckFailed expr: kube_pod_container_status_restarts_total{container=\u0026#34;your-container\u0026#34;, pod=~\u0026#34;your-service-pod.*\u0026#34;, container=\u0026#34;your-container\u0026#34;} \u0026gt; 0 for: 5m labels: severity: critical annotations: summary: \u0026#34;Health check failed for web service\u0026#34; description: \u0026#34;The web service has failed the health check for more than 5 minutes.\u0026#34; 说明： 如果 Web 服务的健康检查端点无法访问，且服务重启次数增加，触发告警。\nb. 外部 API 接口健康检查 # 如果你的业务依赖外部的 API 服务，监控这些外部 API 的可用性也是非常重要的。你可以为外部 API 设置接口探活，并监控它们的响应时间和返回码。\n外部 API 健康检查规则：\n- alert: ExternalAPIHealthCheckFailed expr: http_requests_total{job=\u0026#34;external-api\u0026#34;, status=\u0026#34;5xx\u0026#34;} \u0026gt; 5 for: 5m labels: severity: high annotations: summary: \u0026#34;External API returned 5xx error\u0026#34; description: \u0026#34;The external API is returning 5xx errors, which may affect your service.\u0026#34; 说明： 如果外部 API 接口在 5 分钟内返回超过 5 次的 5xx 错误，说明外部 API 服务异常，触发告警。\nc. 数据库连接健康检查 # 如果应用依赖数据库，数据库的健康检查也非常重要。通过检查数据库的连接池状态、查询响应时间等，可以确保数据库服务正常。\n数据库连接健康检查规则：\n- alert: DatabaseConnectionFailed expr: rate(database_connection_failures_total{database=\u0026#34;your-database\u0026#34;}[5m]) \u0026gt; 0 for: 5m labels: severity: critical annotations: summary: \u0026#34;Database connection failed\u0026#34; description: \u0026#34;There have been failed connections to your database in the last 5 minutes.\u0026#34; 说明： 如果数据库连接失败的次数超过了设定的阈值（这里是 5 次），并且持续超过 5 分钟，则触发告警。\n3. 健康检查和探针常见的设计考虑 # 探针的频率和延迟：不要过于频繁地发送健康检查请求，这样可能会导致额外的负担，尤其在负载较大的生产环境中。通常每 30 秒到 1 分钟进行一次探测。 探针的可靠性：确保健康检查端点返回的状态能够真实反映服务健康。如果你的健康检查端点依赖其他资源（如数据库），则健康检查的定义需要特别小心，避免因外部依赖故障而错误地判定服务不健康。 状态码的配置：健康检查通常会返回 HTTP 状态码（如 200 表示正常，500 表示异常）。可以根据不同的错误码设置不同级别的告警。 4. 综合应用健康检查与告警 # 通常情况下，可以将多种健康检查结合起来，形成多层次的监控：\n黑盒探针：检测服务本身是否可访问。 接口健康检查：检查服务内部依赖的各个组件（如数据库、缓存等）是否健康。 应用日志监控：通过分析日志进一步验证服务的业务健康性。 总结： # 黑盒探针和接口健康检查是微服务架构中非常重要的监控手段，能够帮助你检测服务是否能正常对外提供功能，及时发现服务中断、依赖故障等问题。通过合理设计健康检查端点和探针规则，并结合告警系统，可以及时响应系统故障、提高业务系统的可靠性和可用性。\n外部的流量进入到k8s集群内部的过程 # 外部流量进入 Kubernetes 集群的过程涉及多个组件和配置，主要依赖于 Kubernetes 的Ingress、Service、Ingress Controller 等功能。这些组件共同工作，将外部流量引导到集群内部的合适服务。以下是一个完整的外部流量进入 Kubernetes 集群的流程：\n1. 外部流量的入口： # 外部流量通常是从外部世界（例如用户的浏览器、客户端应用等）通过互联网发送的 HTTP/HTTPS 请求。这些流量需要通过特定的入口进入 Kubernetes 集群。\na. 外部访问的入口点 # 在 Kubernetes 中，外部流量通常会通过以下几种方式进入集群：\nLoadBalancer：在云环境中使用负载均衡器（例如 AWS ELB、Azure Load Balancer、GCP Load Balancer）暴露服务。 Ingress Controller：通过 Ingress 控制器将流量路由到内部服务。 NodePort：通过 Kubernetes 集群节点的 IP 和端口暴露服务，通常用于调试或小规模应用。 ClusterIP：通过内网服务，通常与其他集群内部组件通信，较少用于外部访问。 2. 通过 LoadBalancer（云环境） # 在云环境中，通常使用 LoadBalancer 类型的 Service 将外部流量引导到集群内部的服务。\n流程： # 用户请求：外部流量（如 HTTP 请求）通过 DNS 或 IP 地址访问集群。 云负载均衡器：请求首先到达云负载均衡器（如 AWS ELB）。此负载均衡器将流量转发到 Kubernetes 集群中的节点。 Kubernetes NodePort：负载均衡器会将流量转发到 Kubernetes 集群的每个节点上的某个端口（NodePort）。NodePort 是 Kubernetes 服务的一部分，它为每个节点开放一个固定端口，将流量转发到相应的 Pod。 Pod 选择与转发：NodePort 服务将流量路由到相应的 Pod 上，最终到达应用。 3. 通过 Ingress（HTTP/HTTPS 路由） # Ingress 是 Kubernetes 中一种高效的流量路由机制，能够将外部 HTTP/HTTPS 请求根据规则路由到集群内部的服务。Ingress 控制器处理所有的流量路由和规则。\n流程： # 用户请求：外部用户通过浏览器或客户端发送 HTTP 或 HTTPS 请求到集群外部的 Ingress Controller（通常通过域名，如 www.example.com）。\nIngress Controller\n：Ingress Controller 是一个负责接受外部 HTTP/HTTPS 请求并根据 Ingress 资源定义的规则进行路由的组件。Ingress Controller 通常由 NGINX、Traefik 等开源工具实现。\n例如，NGINX Ingress Controller 作为反向代理，接收外部请求。 路由规则匹配\n：Ingress Controller 根据配置的 Ingress 规则来决定将流量转发到哪个服务。这些规则可以基于路径、主机名等进行路由，例如：\n请求 /api/* 可能被路由到 api-service 服务。 请求 /web/* 可能被路由到 web-service 服务。 服务转发：Ingress Controller 将流量转发到 Kubernetes 内部的相应服务（Service）。这些服务会将请求进一步转发到后端的 Pod。\n4. Ingress 资源配置 # Ingress 是 Kubernetes 中的一个 API 资源，它定义了如何路由外部 HTTP(S) 流量到集群内的服务。Ingress 资源通常需要结合 Ingress Controller 使用。\nIngress 资源配置示例：\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: my-ingress namespace: default spec: rules: - host: myapp.example.com http: paths: - path: /api pathType: Prefix backend: service: name: api-service port: number: 80 - path: /web pathType: Prefix backend: service: name: web-service port: number: 80 说明：\nhost: myapp.example.com 表示该规则适用于域名 myapp.example.com。 请求路径为 /api 会被转发到 api-service 服务的 80 端口。 请求路径为 /web 会被转发到 web-service 服务的 80 端口。 5. Ingress Controller 配置与实现 # Ingress Controller 是实现 Ingress 资源规则的实际组件，它负责接收流量并根据 Ingress 资源的定义进行路由。常见的 Ingress Controller 有：\nNGINX Ingress Controller：一个常用的、功能强大的 Ingress Controller。 Traefik：一个动态反向代理，支持自动发现服务并创建路由规则。 HAProxy：另一种流行的反向代理和负载均衡器。 Ingress Controller 一般通过一个 LoadBalancer 或 NodePort 类型的 Service 将外部流量引入 Kubernetes 集群内部。\n例如，使用 NGINX Ingress Controller： # 安装 NGINX Ingress Controller：通常使用 Helm 来安装。 配置 Ingress 资源：为每个需要暴露的服务配置 Ingress 资源，并为它们配置路由规则。 外部流量进入：外部请求通过配置好的域名进入 LoadBalancer 或 NodePort，最终通过 Ingress Controller 路由到内部服务。 6. NodePort # 在一些非生产环境或小规模集群中，NodePort 也是一个常见的将外部流量引导到集群内部的方式。\n流程： # 用户请求：外部请求会通过集群节点的公共 IP 和 NodePort（例如 30000 端口）访问。 集群节点接收流量：每个 Kubernetes 节点都会在指定的 NodePort 上开放端口，接收到流量后，它会将流量转发到集群内部的 Service。 服务转发到 Pod：服务通过 Kubernetes 的 Service 机制，将流量路由到相应的 Pod。 NodePort 示例： # apiVersion: v1 kind: Service metadata: name: my-service spec: type: NodePort selector: app: myapp ports: - port: 80 targetPort: 8080 nodePort: 30000 说明：\nNodePort 类型的 Service 会在所有节点的 30000 端口上监听流量，并将流量转发到目标端口（这里是 8080）。 7. 总结： # 外部流量进入 Kubernetes 集群的过程通常涉及以下步骤：\n流量入口：外部流量通过负载均衡器、Ingress 或 NodePort 进入 Kubernetes 集群。 Ingress Controller：Ingress Controller 接收 HTTP/HTTPS 请求，并根据定义的路由规则（在 Ingress 资源中配置）将流量转发到合适的 Service。 Service 路由到 Pod：Kubernetes Service 负责根据选择器将流量分发到后端的 Pod。 这种多层的架构提供了强大的路由能力，灵活的负载均衡和自动化的流量管理，使得 Kubernetes 在处理大规模、分布式系统的流量时非常高效。\n反问面试官 问啥 # 在面试中，反问面试官不仅能帮助你更好地了解公司和职位，也能展示出你对该职位的兴趣和职业规划的思考。以下是一些适合在面试时反问面试官的问题，可以根据面试的具体内容进行选择：\n1. 关于公司文化和团队氛围 # \u0026ldquo;贵公司最看重团队合作的哪一方面？你能描述一下团队的工作氛围吗？\u0026rdquo; 这个问题能帮助你了解公司文化、团队合作的方式以及公司是否注重个人成长和支持团队协作。 \u0026ldquo;公司有哪些方式支持员工的成长和职业发展？\u0026rdquo; 这问题能帮助你了解公司是否提供培训、晋升机会、以及支持员工技能提升的资源。 2. 关于职位本身和日常工作 # \u0026ldquo;这个职位的日常工作内容和挑战主要有哪些？\u0026rdquo; 通过这个问题，你可以了解具体工作职责是否符合你的预期。 \u0026ldquo;这个岗位对一个新人来说，最大的挑战通常是什么？\u0026rdquo; 这样的问题可以让你了解入职初期的挑战，帮助你做出更好的准备。 \u0026ldquo;对于这个岗位，团队的优先目标是什么？在接下来的几个月内，团队最关注的事情会是什么？\u0026rdquo; 你可以了解团队当前的目标和未来计划，从而判断你是否能在这个团队中发挥重要作用。 3. 关于项目和技术 # \u0026ldquo;目前团队正在进行的最大项目是什么？我在这个岗位上的工作会如何参与其中？\u0026rdquo; 这个问题能够帮助你理解你未来会参与哪些项目，团队的工作重点是什么。 \u0026ldquo;团队目前使用的技术栈是怎样的？未来是否有技术更新或升级的计划？\u0026rdquo; 了解技术栈能帮助你评估是否符合你的技术背景，以及团队的技术发展方向。 \u0026ldquo;这个岗位在跨部门协作中扮演什么角色？是否会和其他团队密切合作？\u0026rdquo; 如果这个岗位涉及到跨团队合作，了解这些细节能帮助你更好地适应工作环境。 4. 关于公司前景和行业 # \u0026ldquo;公司在未来几年的战略目标是什么？如何看待行业的变化与发展？\u0026rdquo; 这个问题能帮助你了解公司对未来的规划，是否具备良好的发展前景，及公司如何应对行业变化。 \u0026ldquo;你如何看待公司在未来几年内的增长潜力和挑战？\u0026rdquo; 这个问题能帮助你判断公司是否在行业中有竞争力，以及未来可能面临的挑战。 5. 关于领导和管理风格 # \u0026ldquo;您能描述一下您的管理风格吗？\u0026rdquo; 通过了解面试官的管理风格，你可以判断你是否适应他们的工作方式和沟通模式。 \u0026ldquo;在您的团队中，成功的员工通常具备哪些特质？\u0026rdquo; 这个问题可以帮助你了解团队对员工的期望，以及哪些行为和表现会被视为成功。 6. 关于工作与生活的平衡 # \u0026ldquo;公司如何支持员工保持良好的工作与生活平衡？\u0026rdquo; 如果你重视工作与生活的平衡，这个问题能够帮助你了解公司的工作强度和弹性。 \u0026ldquo;公司对远程工作或灵活办公的政策是什么？\u0026rdquo; 这个问题特别适合现在工作环境发生变化的公司，尤其在疫情后，很多公司开始提供更灵活的工作安排。 7. 关于绩效评估与反馈 # \u0026ldquo;公司如何进行员工绩效评估？有没有定期的反馈机制？\u0026rdquo; 这个问题可以让你了解公司是否有明确的绩效评估标准，以及你如何获得反馈和成长。 \u0026ldquo;在过去的员工中，成功晋升的典型案例是怎样的？\u0026rdquo; 这个问题可以帮助你了解公司晋升机制和员工的职业发展轨迹。 8. 关于公司的一些具体问题 # \u0026ldquo;公司最近有没有在技术或业务上做出过重要的变动或调整？\u0026rdquo; 通过这个问题，你可以了解公司最近的变化或新的战略，帮助你评估公司是否在进行转型或升级。 \u0026ldquo;公司如何处理技术债务或解决技术挑战？是否有相关的改进计划？\u0026rdquo; 如果你对技术深度比较感兴趣，这个问题能帮助你了解公司如何应对技术上的难题，是否有计划去优化现有的系统。 9. 关于后续流程 # \u0026ldquo;接下来的面试流程是什么样的？\u0026rdquo; 通过了解后续的面试流程，你可以做好准备，了解是否还需要技术测试、团队面试等。 \u0026ldquo;您认为我在这次面试中有哪些方面表现得特别好或需要改进的地方？\u0026rdquo; 这个问题可以帮助你得到即时反馈，同时也能显示出你对自我改进的积极态度。 小贴士： # 根据面试进展提问：有时面试官会在面试过程中主动提供公司情况或者职位的相关信息。如果在某些方面已经了解，可以避免重复提问，而是根据对方提供的信息做深入反问。 提问要有重点：选择你真正感兴趣的领域提问，而不是仅仅为了提问而提问，这样能够让面试官感受到你的诚意。 避免问薪资福利问题：虽然薪资福利是非常重要的，但在第一次面试中，过早提及这些话题可能让面试官觉得你对职位的兴趣不够强烈。可以等到面试后期或面试官主动提出时再谈。 通过精心准备和有针对性的反问，你可以更好地展现自己的兴趣和能力，同时也能帮助你做出是否接受该职位的决策。\n高可用做么做？多AZ?跨机房容灾，跨地域容灾，双平面容灾 # 在设计高可用（High Availability, HA）架构时，目的是确保在面对硬件故障、网络中断、数据中心宕机等意外事件时，系统仍然能够保持连续服务，不会影响用户的正常使用。不同的高可用策略有不同的实现方式，下面我们会介绍一些常见的高可用设计方法，包括多可用区（Multi-AZ）、跨机房容灾、跨地域容灾以及双平面容灾等。\n1. 多可用区（Multi-AZ）高可用架构 # 多可用区是指将应用和服务部署在同一地域的多个独立的可用区（Availability Zone，AZ）中。可用区是云提供商（如 AWS、GCP、Azure 等）提供的多个物理隔离的区域，它们在同一个地理区域内，但物理上完全独立，因此可以承受局部的硬件或网络故障。\n关键点： # 负载均衡：使用负载均衡器（如 AWS Elastic Load Balancer，Azure Load Balancer）将请求分发到不同可用区的实例。 跨 AZ 数据同步：例如，使用数据库的主从复制、分布式存储等，确保数据在不同 AZ 之间同步。 冗余和备份：部署冗余实例，确保即使某个 AZ 故障，其他 AZ 中的实例可以继续提供服务。 示例： # Web 应用：在多可用区中部署 Web 应用实例，通过负载均衡器将流量分发到各个实例上。 数据库：数据库采用主从复制或集群方式，数据实时同步到多个 AZ。 优点： # 高可用性：保证单个 AZ 故障时，其他 AZ 能继续提供服务。 成本较低：相比于跨地域和跨机房，成本较为经济。 缺点： # 单地域故障：如果整个地域发生灾难（如地域级别的故障），可能无法提供服务。 2. 跨机房容灾 # 跨机房容灾是指将应用和数据部署在不同的物理机房内，通常是同一个城市或区域内的多个机房。每个机房可以看作是一个独立的灾难域，机房之间通过专用的网络连接，保障数据和服务的高可用性。\n关键点： # 异地备份和同步：各个机房之间需要通过专用网络或者公网进行数据同步和备份，常见的技术有主从复制、分布式数据库、对象存储同步等。 故障切换：在一个机房发生故障时，应用可以自动切换到另一个机房。通常，使用DNS 切换或负载均衡器来实现。 示例： # Web 应用：在不同的机房部署 Web 服务，当一个机房出现故障时，流量可以切换到另一个机房的服务实例。 数据库：使用数据库的跨机房复制，确保数据实时同步，机房间的数据一致性。 优点： # 容灾能力强：即使某个机房发生灾难，其他机房可以接管服务。 灵活性：可以根据需求扩展至更多机房。 缺点： # 成本高：跨机房的网络和存储同步成本较高。 网络延迟：不同机房间的延迟可能较大，尤其是跨区域或跨城市的情况下。 3. 跨地域容灾（Cross-Region Disaster Recovery） # 跨地域容灾是指将系统的多个副本分布在不同的地理区域，通常是不同的城市或国家。这样可以有效应对某个地区发生灾难（如自然灾害、大规模网络中断等）导致的故障。\n关键点： # 数据同步和备份：使用跨地域的数据库复制、文件存储同步、数据备份等手段确保数据在不同区域间的高可用。 自动故障切换：通过 DNS 或负载均衡器等技术，在灾难发生时，自动将流量引导到健康的区域。 低延迟访问：通过智能路由和流量引导，确保用户的请求能够尽可能地被就近的地域处理，减少延迟。 示例： # Web 应用：通过跨地域的负载均衡器和流量路由，将流量分发到不同区域的服务实例。 数据库：使用跨地域的数据库同步和灾难恢复策略，保证主数据中心失效时，数据不会丢失，且能在备份地区恢复。 优点： # 地域灾难容忍：能够应对大范围的地理灾难，保证全局服务的可用性。 用户体验优化：将用户请求路由到最近的地域，减少延迟，提高性能。 缺点： # 成本高：跨地域的数据同步、备份以及负载均衡的成本较高。 延迟问题：跨地域的数据同步可能会受到网络延迟和带宽限制的影响。 4. 双平面容灾（Dual-plane Disaster Recovery） # 双平面容灾通常指的是在同一应用系统中实现两个独立的运维平面，一个负责生产环境，另一个负责灾难恢复环境。生产环境和灾难恢复环境之间存在完全的隔离和自动化切换机制，以确保高可用性。\n关键点： # 双活架构：两个环境通常是双活的，即两个平面都能承载流量。通过负载均衡或者 DNS 切换，在发生故障时实现平滑切换。 自动化切换：当生产环境出现问题时，灾难恢复环境可以无缝接管业务，确保业务不中断。 数据同步：生产环境和灾难恢复环境之间需要实时同步数据，确保切换时数据一致性。 示例： # Web 服务：两个平面分别部署在不同的可用区、不同的机房或不同的地域中，保证任意一方出现故障时，另一方可以继续服务。 数据库同步：生产数据库和灾难恢复数据库保持同步，灾难恢复数据库在平时处于冷备状态，发生故障时启用。 优点： # 无缝切换：当发生灾难时，切换平面几乎没有中断。 高可靠性：确保服务的零停机时间，能够承受硬件故障、区域灾难等。 缺点： # 实现复杂：双平面架构需要较高的自动化水平和技术能力来维持两边环境的一致性和同步。 成本高：保持两个独立且活跃的平面需要较高的基础设施和运维成本。 总结 # 高可用架构设计需要根据具体的需求、预算、容灾目标以及服务重要性来决定。以下是几种常见的高可用策略的总结：\n多可用区（Multi-AZ）：适用于单地域内的高可用架构，通过多个 AZ 部署服务，保证单个 AZ 故障时，其他 AZ 仍然可以提供服务。 跨机房容灾：适用于同一地区内的多个机房，通过机房间的数据同步和冗余部署，确保一个机房宕机时业务不受影响。 跨地域容灾：适用于应对大规模地域灾难，能够在不同地理位置的灾难恢复环境中保证服务的持续性。 双平面容灾：适用于需要零停机时间的业务，通过双活架构保证在任一平面发生故障时，另一平面能够无缝接管服务。 每种策略都有不同的优缺点，可以根据公司的具体需求进行选择和结合，打造灵活可靠的高可用架构。\n你遇到的棘手的问题？怎么解决的？ # 单master改造为3master，3etcd\n单master改造\n"},{"id":24,"href":"/docs/2025-2-26-k8s%E7%9B%B8%E5%85%B3/","title":"0226 k8s题目","section":"Docs","content":" Kubernetes # 谈谈你对k8s的理解☆ # Kubernetes（简称 K8s）是一个开源的容器编排平台，旨在自动化容器化应用程序的部署、扩展和管理。它最初由 Google 设计，并在2014年开源。K8s 提供了一个集中的平台来管理和运行容器化的应用程序，尤其适合大规模的分布式系统和微服务架构。\n从技术角度看，K8s 主要有以下几个核心组成部分：\n集群（Cluster）： K8s 将多个节点组织成一个集群，集群中通常有两类节点：控制平面节点（Control Plane）和工作节点（Worker Node）。控制平面负责集群的管理与调度，而工作节点运行实际的应用程序。 Pod： Pod 是 Kubernetes 中最小的调度单位，通常包含一个或多个容器，这些容器共享同一个网络、存储和其他资源。Pod 是容器运行的基本单位。 Deployment： Deployment 用于管理和控制应用程序的副本和版本。它通过定义副本数来保证应用的高可用性，支持滚动更新、回滚等功能。 Service： Service 用于暴露应用程序的网络接口，并提供负载均衡。通过 Service，用户可以通过稳定的 IP 或 DNS 名称访问一组 Pod。 Ingress： Ingress 提供 HTTP 和 HTTPS 路由到集群内的服务，可以用于管理外部流量进入集群，支持负载均衡、SSL 终端等功能。 ConfigMap 和 Secret： ConfigMap 用于管理应用的配置数据，Secret 用于存储敏感数据（如密码、API 密钥等）。这些资源可以被容器在运行时动态加载。 Namespace： Namespace 提供了对资源的隔离，允许在同一个集群中多个团队或应用之间共享资源而不发生冲突。它相当于虚拟化的逻辑分区。 StatefulSet： StatefulSet 是一种控制器，用于管理有状态应用，如数据库等。与 Deployment 不同，StatefulSet 保证了 Pod 的稳定性和唯一性，适合需要持久存储和有序部署的应用。 Persistent Volume (PV) 和 Persistent Volume Claim (PVC)： PV 是集群内的存储资源，而 PVC 是用户对存储资源的请求。通过这种方式，K8s 支持动态存储的创建和销毁。 关键优势： # 自动化：K8s 提供了自动化部署、滚动更新、扩展和恢复功能，极大地简化了应用程序生命周期管理。 高可用性：K8s 支持容错和高可用性，可以通过多副本部署、健康检查、自动恢复等机制保证应用的可靠性。 灵活性：通过声明式配置，K8s 让开发者可以专注于应用本身，而不必担心基础设施细节。 扩展性：K8s 提供了强大的插件和扩展机制，支持各种中间件、监控、日志等功能的集成。 适用场景： # 微服务架构：K8s 非常适合容器化微服务的管理，能有效地处理服务之间的依赖、负载均衡、容错等。 大规模分布式应用：由于其自动化和扩展性，K8s 很适合处理大规模、复杂的分布式系统。 混合云/多云架构：K8s 提供跨平台的支持，使得应用能够在不同的云环境或本地数据中心之间无缝迁移。 总的来说，Kubernetes 是现代 DevOps 和持续集成/持续交付（CI/CD）的核心技术之一，能够有效地管理容器化应用，使得开发和运维更加高效、灵活和可靠。\nk8s集群架构是什么☆ # Kubernetes (K8s) 集群架构由多个组件组成，主要分为控制平面（Control Plane）和工作节点（Worker Node）。控制平面负责集群的管理与调度，而工作节点则运行实际的应用程序和服务。下面是 K8s 集群架构的主要组件：\n1. 控制平面（Control Plane） # 控制平面是集群的大脑，负责管理整个集群的状态。它处理所有的任务调度、资源分配和集群状态维护。控制平面通常由多个组件组成，以下是控制平面的核心组件：\nAPI Server (kube-apiserver)： 作为 K8s 集群的入口点，所有的操作请求（如部署、修改、删除资源）都会通过 API Server 进行。它提供了集群资源的 RESTful API 接口，所有的组件和用户与 K8s 的交互都是通过它来实现的。 它是集群中的唯一管理接口，并且与其他控制平面组件（如 etcd、controller manager）进行通信。 etcd： 一个高可用的键值存储数据库，K8s 使用它来存储集群的所有数据和状态，包括节点信息、Pod 状态、配置等。所有的集群状态都保存在 etcd 中，是集群的 \u0026ldquo;源数据\u0026rdquo;。 etcd 通常是一个分布式系统，可以保证数据的一致性和高可用性。 Controller Manager (kube-controller-manager)： 控制器是 K8s 的守护进程，负责维护集群状态并根据需要进行调整。它监控 K8s 集群的状态，并执行相应的操作。例如，ReplicaSet 控制器会确保指定数量的副本 Pods 始终处于运行状态。 Controller Manager 会通过 API Server 与 etcd 进行交互，以便同步集群的状态。 Scheduler (kube-scheduler)： 调度器负责将待运行的 Pod 调度到合适的工作节点上。它根据节点的资源利用情况、优先级、调度策略等因素来选择最适合的节点。 调度器还会考虑 Pod 的 Affinity、Taints 和 Tolerations 等设置，确保 Pod 能够在合适的环境中运行。 2. 工作节点（Worker Node） # 工作节点是实际承载应用程序和服务的地方。每个工作节点上都运行着一组 K8s 组件，主要包括：\nKubelet (kubelet)： Kubelet 是每个工作节点上的主要代理，负责确保容器在节点上按照 Pod 定义的规范运行。它监控容器的生命周期，并与 API Server 通信，确保节点上的资源与集群状态保持一致。 它会定期向 API Server 报告节点和 Pod 的状态，确保集群中的状态与期望的一致。 Kube Proxy (kube-proxy)： Kube Proxy 是工作节点上的网络代理，负责维护网络规则，确保 Pod 能够通过正确的网络路由进行通信。 它通常会在每个节点上运行，并提供服务发现、负载均衡等功能。它根据 Service 的定义配置网络规则，使得流量能够正确地路由到相应的 Pod。 Container Runtime： 容器运行时是负责容器生命周期管理的组件，K8s 支持多种容器运行时，包括 Docker、containerd、CRI-O 等。容器运行时负责拉取镜像、创建和销毁容器等操作。 在大多数 K8s 集群中，Docker 是默认的容器运行时，但现在 Kubernetes 推荐使用 containerd 或 CRI-O 等作为容器运行时。 3. K8s 集群架构示意图 # +--------------------------------------+ | Control Plane | | | | +------------+ +----------------+ | | | API Server| | Controller | | | +------------+ | Manager | | | +------------+ +----------------+ | | | etcd | | | +------------+ +----------------+ | | | Scheduler | | | +------------+ | +--------------------------------------+ | | +-------------+--------------+-------------+ | | +-----------------+ +-----------------+ | Worker Node 1| | Worker Node 2| | | | | | +----------+ | | +----------+ | | | Kubelet | | | | Kubelet | | | +----------+ | | +----------+ | | +----------+ | | +----------+ | | | Kube Proxy| | | | Kube Proxy| | | +----------+ | | +----------+ | | +------------+ | | +------------+ | | | Container | | | | Container | | | | Runtime | | | | Runtime | | | +------------+ | | +------------+ | +-----------------+ +-----------------+ 4. 其他重要组件 # Ingress Controller\n：\nIngress 是 Kubernetes 中的一个 API 对象，用于管理外部流量的访问控制。Ingress Controller 是实现 Ingress 规则的组件，它负责将 HTTP/HTTPS 请求路由到集群内的服务。 Metrics Server\n：\nMetrics Server 是 K8s 集群的监控组件，它收集每个节点和 Pod 的资源使用情况（如 CPU、内存等）。它为自动扩缩容（如 Horizontal Pod Autoscaler）和其他资源管理提供数据支持。 5. 高可用性与冗余 # K8s 支持控制平面的高可用部署，可以通过在多个主机上部署多个控制平面组件（如 API Server、Controller Manager、Scheduler 和 etcd）来确保系统的容错能力。 工作节点也可以通过水平扩展进行增加，确保集群的弹性和负载分担。 总结 # Kubernetes 集群架构采用了控制平面和工作节点的设计，将集群管理与应用运行分开，使得集群的维护、扩展和管理更加灵活和高效。控制平面负责集群的整体管理，而工作节点则专注于运行容器化的应用程序。通过这些组件的协作，K8s 能够实现容器化应用的自动化部署、扩展、故障恢复等关键功能。\n简述Pod创建过程☆ # Pod 是 Kubernetes 中最小的调度单位，它是由一个或多个容器组成的一个集合，这些容器共享同一个网络、存储和配置。Pod 的创建过程可以分为以下几个关键步骤：\n1. 创建 Pod 配置文件 # Pod 通常通过 YAML 或 JSON 格式的配置文件定义。配置文件描述了 Pod 的所需资源、容器的镜像、环境变量、挂载的存储卷等信息。以下是一个简单的 Pod 配置示例：\napiVersion: v1 kind: Pod metadata: name: my-pod spec: containers: - name: my-container image: nginx:latest ports: - containerPort: 80 2. 提交 Pod 配置到 Kubernetes API Server # 用户通过 kubectl 或其他客户端工具，将 Pod 的配置文件提交到 Kubernetes API Server。API Server 是集群的入口，它接收请求并将 Pod 配置存储在集群的数据库中（通常是 etcd）。\n例如，使用 kubectl 提交 Pod：\nkubectl apply -f pod.yaml 3. API Server 存储 Pod 配置 # 在收到请求后，API Server 会将 Pod 配置存储在 etcd 中，确保集群的状态得到持久化存储。API Server 会对请求进行验证和授权，确保只有合法的操作能够提交。\n4. 调度（Scheduler）选择合适的工作节点 # Kubernetes 的调度器（Scheduler）会根据 Pod 的资源需求和集群的资源状况，决定将 Pod 调度到哪个工作节点。调度器会考虑以下因素：\n资源要求：CPU、内存等。 节点的资源利用率。 节点标签和亲和性规则。 Taints 和 Tolerations 等调度策略。 调度器选择合适的节点后，会将 Pod 的调度信息更新到 API Server 中。\n5. Kubelet 在工作节点上创建 Pod # 在 Pod 被调度到工作节点后，工作节点上的 Kubelet 负责管理该节点的容器和 Pod。Kubelet 会定期向 API Server 查询集群状态，发现 Pod 已经被调度到自己节点后，Kubelet 会执行以下操作：\n拉取容器镜像（如果镜像未缓存）。 创建并启动容器。 配置网络：Kubelet 为 Pod 中的所有容器配置网络，使它们能够通过集群的网络与其他容器通信。 挂载存储：如果 Pod 中定义了卷（Volume），Kubelet 会挂载对应的存储到容器中。 Kubelet 会通过容器运行时（如 Docker、containerd）来启动容器，并确保容器处于运行状态。\n6. Pod 启动并运行 # 一旦容器启动成功，Kubelet 会定期检查 Pod 和容器的健康状况。Pod 中的容器如果启动成功且健康，就进入运行状态，Kubelet 会向 API Server 汇报节点和 Pod 的状态，API Server 会更新集群的状态信息。\n7. 集群状态更新 # 最终，API Server 会将 Pod 的状态同步到集群的各个组件，确保所有节点都知道 Pod 的状态。集群中的其他服务（如 Service、Ingress 等）可以根据 Pod 的状态进行流量调度。\n8. 监控与自动恢复 # Kubelet 和控制器持续监控 Pod 的健康状况和运行状态。如果 Pod 中的容器崩溃或停止运行，Kubelet 会根据 Pod 的定义重新启动容器，保证 Pod 处于期望的状态。\n总结 # Pod 创建过程从用户定义 Pod 配置文件开始，到 API Server 存储配置，调度器决定节点，Kubelet 在节点上拉取镜像并启动容器，最后 Pod 进入运行状态并由 Kubelet 定期检查健康。这一过程展示了 Kubernetes 自动化部署和容器管理的能力，确保应用始终处于期望的状态。\n简述删除一个Pod流程 # 删除一个 Pod 的过程可以分为以下几个步骤。整个流程涉及用户发起删除请求、Kubernetes 控制平面处理请求、以及工作节点上的 Kubelet 进行容器的销毁和资源的回收。\n1. 用户发起删除请求 # 用户可以通过 kubectl 或其他客户端工具发起删除 Pod 的请求。例如：\nkubectl delete pod \u0026lt;pod-name\u0026gt; 这个命令会向 Kubernetes API Server 发送删除 Pod 的请求。\n2. API Server 接受并验证请求 # API Server 接收到删除 Pod 的请求后，会验证该请求是否合法。验证过程包括：\n确保用户有足够的权限删除 Pod。 确保 Pod 确实存在于集群中。 如果验证通过，API Server 会将删除请求存储到集群的数据库（etcd），并更新集群的状态。\n3. Pod 状态更新 # 当 API Server 确认删除 Pod 的请求后，它会更新集群的状态并将 Pod 状态标记为 \u0026ldquo;Terminating\u0026rdquo;。此时，Pod 仍在集群中，但标记为即将被删除。\n4. 删除 Pod 相关的控制器对象 # 如果 Pod 是由某些控制器（如 Deployment、StatefulSet 等）管理的，控制器会识别到 Pod 被删除，并根据控制器的设置（例如 ReplicaSet）决定是否重新创建 Pod。这意味着如果 Pod 是通过 Deployment 等控制器创建的，控制器会根据设置创建一个新的 Pod 来替代被删除的 Pod。\n5. Kubelet 监测到 Pod 删除请求 # 当 API Server 更新 Pod 状态为 \u0026ldquo;Terminating\u0026rdquo; 后，Kubelet 会在所在工作节点上监测到 Pod 被删除的请求。Kubelet 会执行以下操作：\n停止容器：Kubelet 会通知容器运行时（如 Docker 或 containerd）停止运行 Pod 中的所有容器。 删除容器实例：容器运行时会销毁这些容器，包括清理网络接口、挂载的存储等资源。 清理相关资源：Kubelet 会清理 Pod 相关的资源，如临时存储卷、日志文件等。 6. Pod 资源回收 # 当容器被终止并且相关的资源被清理后，Kubelet 会向 API Server 发送更新，标记 Pod 已经成功删除。\n7. API Server 完成删除操作 # 在 Kubelet 确认 Pod 已经被完全删除并清理后，API Server 会将 Pod 的状态从 \u0026ldquo;Terminating\u0026rdquo; 更新为已删除状态，并从 etcd 中彻底删除该 Pod 的记录。\n8. 删除完成 # Pod 完全删除后，集群的状态更新，Pod 在 Kubernetes 集群中不再存在，相关的网络、存储和其他资源也被回收。\n总结 # 删除 Pod 的过程涉及以下关键步骤：\n用户发起删除请求，API Server 验证并更新 Pod 状态为 \u0026ldquo;Terminating\u0026rdquo;。 Kubelet 停止容器，清理资源并向 API Server 上报删除状态。 API Server 在确认删除完成后，从 etcd 中删除 Pod 记录，删除过程完成。 对于由控制器管理的 Pod，控制器会根据设置决定是否重新创建 Pod，以维持预期的副本数。\n不同node上的Pod之间的通信过程☆ # Kubernetes 中不同节点（Node）上的 Pod 之间的通信过程涉及多个 Kubernetes 组件和底层网络技术。K8s 的网络模型要求每个 Pod 都有一个唯一的 IP 地址，并且允许 Pod 之间无障碍地进行通信，无论它们位于同一节点还是不同节点。\n以下是不同节点上的 Pod 之间通信的过程：\n1. Pod 网络模型 # K8s 的网络模型要求每个 Pod 都有一个独立的 IP 地址。这意味着：\nPod 间通信：不同节点上的 Pod 可以直接通过其 IP 地址进行通信。 容器间通信：同一 Pod 内的容器共享同一 IP 地址和端口，因此容器之间可以直接通过 localhost 进行通信。 这种设计理念避免了复杂的 NAT 转换和端口映射，使得容器和服务之间的通信更加直观。\n2. CNI 插件（容器网络接口） # Kubernetes 使用 CNI（Container Network Interface）插件来管理集群网络，CNI 插件负责 Pod 网络的创建、删除、路由和通信。常见的 CNI 插件包括 Calico、Flannel、Weave、Cilium 等。\nCNI 插件在节点上配置网络规则，确保每个 Pod 拥有唯一的 IP 地址，并能够通过底层网络进行通信。 CNI 插件可能会使用不同的底层网络技术，例如 VXLAN、VLAN、或直接使用物理网络。 3. 通信过程 # 假设有两个 Pod，Pod A 位于 Node 1，Pod B 位于 Node 2，Pod A 和 Pod B 需要进行通信。这个过程通常包括以下几个步骤：\n3.1 Pod A 发起请求 # Pod A 想要访问 Pod B，它会向 Pod B 的 IP 地址发送请求。在 Kubernetes 网络模型下，这个请求不会经过主机的 NAT（网络地址转换）层，而是直接发送到目标 Pod 的 IP 地址。\n3.2 CNI 插件处理请求 # Pod A 的网络流量会由 CNI 插件进行处理：\nCNI 插件会将 Pod A 的请求通过容器网络转换为底层网络的格式，并决定如何路由到目标 Pod 的节点（Node 2）。这通常是通过在每个节点上配置路由表和使用网络隧道（如 VXLAN 或 GRE）来实现跨节点通信。 3.3 跨节点通信 # 如果 Pod A 和 Pod B 在不同节点上，CNI 插件会使用底层网络（如 VXLAN 隧道或 BGP 路由）将流量从 Node 1 路由到 Node 2。 通常，Pod A 发送的流量首先到达 Node 1 的物理网络接口，然后通过 CNI 插件封装和路由到 Node 2。 3.4 Node 2 处理流量 # Node 2 上的 CNI 插件接收到从 Node 1 发来的请求，并根据目标 IP 地址找到相应的 Pod B。CNI 插件会通过相应的网络接口和路由规则将流量传递到 Pod B 上。\n3.5 Pod B 响应请求 # Pod B 收到请求后，进行处理并发送响应。Pod B 的响应流量会经过类似的过程：\n它会将响应发送到 Pod A 的 IP 地址。 如果 Pod A 和 Pod B 在不同的节点上，响应会通过 Node 2 到 Node 1，再由 CNI 插件将响应路由到 Pod A。 4. 网络策略（Network Policy） # K8s 还允许管理员定义网络策略，以限制 Pod 之间的通信。网络策略允许你控制哪些 Pod 可以与其他 Pod 通信，基于标签选择器、命名空间等规则来定义流量的允许与拒绝。\n5. Service 和负载均衡 # 通常，Pod 是临时的和无状态的，所以 K8s 提供了 Service 作为 Pod 的抽象。Service 提供一个稳定的访问点，可以自动负载均衡请求到多个后端 Pod。\n当 Pod A 想要与 Pod B 通信时，它可能通过 Service 名称而不是直接通过 IP 地址来访问 Pod B。Kubernetes 会通过 ClusterIP 或 DNS 解析 Pod B 的 Service 地址，并将流量路由到实际的 Pod。 如果 Pod B 在不同节点上，Service 会使用底层网络组件（如 kube-proxy）进行流量转发。 总结 # 不同节点上的 Pod 之间的通信过程包括：\nPod A 发起请求，向目标 Pod 的 IP 地址发送数据。 CNI 插件处理流量，通过底层网络进行路由。流量可能经过网络隧道（如 VXLAN）跨越节点。 目标节点（Node 2）上的 CNI 插件接收流量，并将其转发到目标 Pod（Pod B）。 Pod B 处理请求并返回响应，响应过程与请求过程类似。 整个过程依赖于 Kubernetes 的网络模型和 CNI 插件来实现跨节点的容器通信。此外，网络策略、Service 和负载均衡等功能可以进一步优化和管理 Pod 之间的通信。\npod创建Pending状态的原因☆ # Pod 进入 Pending 状态的原因通常是由于 Kubernetes 在调度和分配资源时遇到了问题。Pending 状态表示 Pod 已经被创建，但尚未分配到合适的节点上，或者在节点上尚未成功运行。以下是常见的导致 Pod 进入 Pending 状态的原因：\n1. 资源不足 # CPU/内存不足\n：如果集群中没有节点能够提供满足 Pod 所需资源（如 CPU 或内存）的空间，Pod 会一直处于 Pending 状态，直到资源可用。\n解决方法：检查集群中的节点资源使用情况（例如使用 kubectl describe pod \u0026lt;pod-name\u0026gt; 查看 Pod 的详细信息），并考虑扩展集群或优化资源配置。 2. 调度器找不到合适的节点 # 节点资源限制：Pod 可能要求的资源（如 CPU、内存、存储）超过了集群中任何节点的可用资源，导致调度器无法找到合适的节点来运行 Pod。\n节点选择约束\n：Pod 可能带有某些调度约束，如节点亲和性（Affinity）或反亲和性（Anti-Affinity），这些约束可能限制了调度器选择节点，从而导致无法调度 Pod。\n解决方法：查看 Pod 的调度策略和节点资源，确保集群资源充足，并调整节点选择规则。 3. Taints 和 Tolerations # 节点上有 Taints，Pod 没有相应的 Tolerations\n：如果某个节点有 Taint（污点），而 Pod 没有相应的 Toleration（容忍），则调度器会拒绝将 Pod 调度到该节点。\n解决方法：检查节点是否有 Taints，并确保 Pod 配置了合适的 Tolerations。 4. 调度器无法找到适合的节点 # Pod 亲和性或反亲和性规则\n：Pod 可能具有亲和性（Affinity）或反亲和性（Anti-Affinity）规则，这些规则可能要求将 Pod 调度到特定的节点或者避免将 Pod 调度到某些节点上。如果没有节点满足这些规则，Pod 会处于 Pending 状态。\n解决方法：检查 Pod 的亲和性和反亲和性规则，确保这些规则在当前集群中有节点可以满足。 5. 集群容量不足 # 集群容量不足\n：集群中的节点可能因为负载过高或资源不足，导致无法分配新 Pod。\n解决方法：通过监控工具检查集群资源使用情况，扩展集群或释放部分资源。 6. PersistentVolume (PV) 或 PersistentVolumeClaim (PVC) 问题 # PVC 绑定失败\n：如果 Pod 依赖于 PersistentVolume（持久化卷），但 PVC 没有成功绑定到合适的 PV，Pod 也会处于 Pending 状态，直到绑定成功。\n解决方法：检查 PVC 的状态和 PV 资源，确保 PVC 可以成功绑定到合适的 PV。 7. 网络问题 # CNI 插件问题\n：如果集群的网络插件（CNI 插件）出现故障或配置问题，Pod 可能无法启动。网络问题会导致 Pod 的网络接口无法配置，造成 Pod 无法成功运行。\n解决方法：检查 CNI 插件的配置和状态，确保网络正常工作。 8. 镜像拉取失败 # 镜像拉取失败\n：Pod 可能需要从远程仓库拉取容器镜像。如果镜像拉取失败（例如，镜像仓库不可访问、认证失败或镜像不存在），Pod 将无法启动并会保持 Pending 状态。\n解决方法：检查镜像是否存在，仓库是否可访问，并确保 Pod 配置了正确的镜像拉取凭证（如 Docker Hub 或私有镜像仓库的认证信息）。 9. Kubernetes 控制平面问题 # 控制平面出现故障\n：如果 Kubernetes 的控制平面出现问题，例如 API Server 或调度器不可用，Pod 可能无法正确调度到工作节点。\n解决方法：检查控制平面的状态，确保 API Server 和调度器正常运行。 如何排查 Pending 状态的 Pod # 查看 Pod 状态： 使用以下命令查看 Pod 的详细信息，了解它为什么没有被调度：\nkubectl describe pod \u0026lt;pod-name\u0026gt; 在输出中查看 Events 部分，通常会列出导致 Pod Pending 的原因。\n查看节点资源： 使用以下命令查看集群中的节点资源和使用情况：\nkubectl describe node \u0026lt;node-name\u0026gt; 检查资源请求和限制： 查看 Pod 配置文件，检查资源请求（requests）和限制（limits）的设置，确保它们不会超出节点的可用资源。\n总结 # Pod 进入 Pending 状态通常是由于以下几个原因：资源不足、调度约束问题、Taints 和 Tolerations 设置、PersistentVolume 的绑定问题、镜像拉取失败等。通过查看 Pod 的描述信息和集群资源，能够帮助诊断问题，并采取相应的措施来解决。\ndeployment和statefulset区别☆ # Deployment 和 StatefulSet 都是 Kubernetes 中常用的控制器，它们的主要区别在于如何管理和部署 Pod，以及它们对 Pod 的生命周期和存储的处理方式。下面是它们的主要区别：\n1. Pod 的标识和命名 # Deployment\n：\n在 Deployment 中，所有的 Pod 都是无状态的，它们被认为是相同的、可替代的 Pod。每个 Pod 的名字是由 Kubernetes 自动生成的，通常是随机的。 例如，如果你有 3 个副本，Pod 名称可能是 pod-xxxxxx-abcde，每个 Pod 没有明确的身份。 StatefulSet\n：\nStatefulSet 为每个 Pod 提供唯一的标识符。Pod 的名字是基于 StatefulSet 名称和一个索引号，例如：pod-name-0, pod-name-1, pod-name-2。这些 Pod 会按照特定的顺序进行创建、删除和扩缩容。 Pod 的顺序性是重要的，意味着它们的启动顺序、终止顺序都需要被严格遵循。 2. Pod 生命周期 # Deployment\n：\nDeployment 中的 Pod 是 无状态的，每个 Pod 是独立的、没有顺序依赖的。如果一个 Pod 被删除，Kubernetes 会创建一个新的 Pod 来替代它，而新的 Pod 可能在其他节点上运行，并且其网络地址和存储都可能不同。 StatefulSet\n：\nStatefulSet 中的 Pod 是 有状态的，每个 Pod 都有一个固定的标识符，且 Pod 在启动和终止时会按照一定顺序进行。例如，StatefulSet 会按顺序启动 pod-name-0、pod-name-1 等，并在删除时按逆序删除它们（pod-name-2、pod-name-1、pod-name-0）。 这种顺序性对需要有序启动或终止的应用非常重要。 3. 存储 # Deployment\n：\nDeployment 中的 Pod 通常使用 临时存储，即每次 Pod 启动时，都会创建新的存储卷，并且 Pod 的生命周期与存储卷的生命周期是分开的。这意味着每个 Pod 启动时都可能得到不同的存储资源，且这些存储资源不会持久化。 StatefulSet\n：\nStatefulSet 提供了对 持久存储的支持，每个 Pod 都可以绑定到一个持久的存储卷。即使 Pod 被删除和重建，它们也可以重新挂载到之前的持久存储。这对于数据库、文件系统等需要持久化数据的应用非常重要。 StatefulSet 使用 PersistentVolumeClaim 来为每个 Pod 动态分配持久存储。 4. 扩缩容 # Deployment\n：\nDeployment 可以方便地进行横向扩展和缩容，所有的 Pod 都是相同的，没有顺序依赖。 Pod 的扩容和缩容是同时发生的，不考虑 Pod 的启动顺序。 StatefulSet\n：\nStatefulSet 也支持扩容，但每次增加一个 Pod 都会按顺序进行。新的 Pod 会根据原有 Pod 的索引顺序进行创建和分配资源。 在缩容时，Pod 会按相反顺序进行删除，确保有状态的应用正确地停止。 5. 网络 # Deployment\n：\n在 Deployment 中，Pod 的网络是无状态的，每个 Pod 都可以通过 Kubernetes Service 与其他 Pod 进行通信，但这些 Pod 的 IP 地址是临时的、随机分配的。 这意味着 Pod 的 IP 地址可能会随着 Pod 的重启而变化，因此不能依赖 IP 地址来识别 Pod。 StatefulSet\n：\n在 StatefulSet 中，每个 Pod 都有一个唯一且固定的网络标识符。例如，pod-name-0 会有一个稳定的 DNS 名称 pod-name-0.\u0026lt;statefulset-name\u0026gt;.\u0026lt;namespace\u0026gt;.svc.cluster.local，即使 Pod 被重新调度或重启，它的 DNS 名称和 IP 地址依然是固定的。 6. 适用场景 # Deployment\n：\n适用于无状态应用，比如 Web 服务器、API 服务等。这些应用可以轻松地被替换，Pod 之间没有数据共享的需求。 StatefulSet\n：\n适用于有状态应用，如数据库、分布式文件系统、缓存系统等。这些应用需要确保数据的持久性、Pod 的顺序性以及唯一性。 7. 滚动更新 # Deployment\n：\nDeployment 支持 滚动更新，即通过逐步更新 Pod 的方式，确保应用不中断。每次更新一个 Pod，直到所有 Pod 都被更新为新版本。 StatefulSet\n：\nStatefulSet 也支持滚动更新，但它会按顺序更新 Pod，确保每个 Pod 在更新过程中都能按顺序处理。这对于有状态应用的有序更新非常重要。 总结对比 # 特性 Deployment StatefulSet Pod 名称 随机生成的名称 固定的名称，带有索引号 Pod 顺序 无顺序依赖 有顺序依赖，创建和删除时有顺序性 存储 临时存储，Pod 之间的存储不共享 每个 Pod 都有自己的持久化存储 网络 Pod 的 IP 是临时的、动态分配的 每个 Pod 有稳定的 DNS 名称和 IP 地址 扩缩容 Pod 可以同时扩容和缩容，无顺序依赖 扩容时按顺序增加，缩容时按顺序删除 适用场景 无状态应用，如 Web 服务、API 服务等 有状态应用，如数据库、分布式存储系统等 滚动更新 支持滚动更新，逐个更新 Pod 支持滚动更新，按顺序更新 Pod 适用场景示例 # Deployment：Web 服务、API 网关、前端服务等，这些服务不依赖于特定的状态或数据，可以轻松替换。 StatefulSet：数据库（如 MySQL、PostgreSQL、MongoDB）、分布式存储系统（如 Cassandra、Elasticsearch）、分布式缓存（如 Redis）等，这些应用需要持久化数据、稳定的网络标识符和有序的扩缩容。 通过理解这些区别，你可以根据应用的需求选择合适的控制器进行部署和管理。\nkube-proxy有什么作用☆ # kube-proxy 是 Kubernetes 集群中的一个重要组件，负责实现 Pod 网络通信 和 Service 负载均衡。它是 Kubernetes 中的 网络代理，运行在每个节点上，确保 Pod 和 Service 之间的流量能够正确路由和转发。\nkube-proxy 的主要作用 # Service 负载均衡 kube-proxy 负责根据 Kubernetes 中定义的 Service，提供负载均衡功能。它将来自集群外部或其他 Pod 的流量路由到 Service 的后端 Pod 上。 ClusterIP Service：kube-proxy 通过 iptables 或 IPVS（取决于配置）将流量从 Service 的虚拟 IP（VIP）转发到实际的后端 Pod。 NodePort Service：kube-proxy 在每个节点上开放一个固定端口，当外部流量访问该端口时，kube-proxy 会将流量转发到对应的 Pod。 LoadBalancer Service：在云环境中，kube-proxy 会配合云提供商的负载均衡器，将外部流量通过负载均衡器转发到集群中的 Pod。 路由与转发流量 kube-proxy 负责在每个节点上管理网络路由规则。它监听 Kubernetes API Server 上关于 Service 的变化，并动态更新节点上的路由规则，确保流量能够正确地转发到对应的 Pod。 Service 的虚拟 IP (VIP) 转发 每个 Service 在 Kubernetes 中都有一个虚拟 IP（ClusterIP），用于在集群内部提供访问。kube-proxy 会将流量从这个虚拟 IP 路由到对应的 Pod 集合，实现集群内的负载均衡。 负载均衡算法 kube-proxy 通过不同的负载均衡算法来决定如何分发流量到后端的 Pod。常见的负载均衡策略包括： 轮询：流量按顺序均匀分配到各个 Pod。 最少连接数：将流量转发到当前连接数最少的 Pod（当使用 IPVS 时可用）。 源地址哈希：根据客户端的 IP 地址将请求固定路由到同一个 Pod。 kube-proxy 的工作原理 # kube-proxy 的工作原理主要取决于两种模式：iptables 模式 和 IPVS 模式。\n1. iptables 模式 # 在 iptables 模式下，kube-proxy 使用 Linux 内核的 iptables 工具来配置流量转发规则。具体过程如下：\nkube-proxy 会为每个 Service 创建一个对应的 iptables 规则，将流量从 Service 的 ClusterIP 转发到对应的 Pod。 这些规则在节点的防火墙中生效，确保流量能够被正确地路由到后端 Pod。 kube-proxy 会定期更新这些规则，以适应 Service 和 Pod 的变化。 优点：\niptables 模式简单且不依赖额外的工具，适合小规模集群。 缺点：\n随着集群规模增大，iptables 规则可能会变得复杂，性能也可能成为瓶颈。 2. IPVS 模式 # IPVS（IP Virtual Server）是 Linux 内核的一个高级负载均衡功能，可以提供比 iptables 更高效的流量路由能力。kube-proxy 在 IPVS 模式下将流量转发到后端 Pod。\nkube-proxy 会通过 IPVS 配置负载均衡规则，提供高效的流量转发。 IPVS 模式支持多种负载均衡算法，如轮询、最少连接数、基于源 IP 的哈希等。 优点：\n性能比 iptables 模式更好，适合大型集群和高流量环境。 支持更多的负载均衡策略，适用于更复杂的流量调度需求。 缺点：\n需要在集群节点上启用 IPVS，且配置较为复杂。 kube-proxy 的工作流程 # 监听 API Server kube-proxy 持续监听 Kubernetes API Server，关注 Service 和 Endpoints 的变化。每当 Service 或 Pod 状态发生变化时，kube-proxy 会更新对应的路由规则。 更新路由规则 根据 Kubernetes 中的 Service 配置，kube-proxy 会为每个 Service 创建合适的流量转发规则（无论是 iptables 规则还是 IPVS 规则），并在每个节点上生效。 转发流量 当集群中的某个 Pod 发送流量到 Service 时，kube-proxy 会根据路由规则将流量转发到对应的 Pod。如果使用 NodePort 或 LoadBalancer，外部请求也会被转发到正确的 Pod。 健康检查和负载均衡 kube-proxy 会定期检查 Pod 的健康状态，确保将流量仅转发给健康的 Pod。它通过监听 Endpoints 更新信息，确保服务的 Pod 集合是最新的。 kube-proxy 的使用场景 # 集群内服务的负载均衡：无论是 ClusterIP、NodePort 还是 LoadBalancer 服务，kube-proxy 都负责流量转发和负载均衡。 外部访问 Kubernetes 服务：通过 NodePort 或 LoadBalancer 类型的 Service，kube-proxy 将外部流量转发到集群中的 Pod。 内部服务发现与负载均衡：集群内的 Pod 可以通过 Service 名称进行访问，kube-proxy 会确保流量被正确地转发到对应的 Pod。 总结 # kube-proxy 是 Kubernetes 集群中处理 网络代理 和 负载均衡 的重要组件，它通过维护 Service 和 Pod 之间的流量转发规则，确保集群内部和外部的网络通信能够顺畅进行。kube-proxy 的核心功能是确保流量从 Service 到 Pod 的正确路由，同时提供负载均衡能力。根据集群的规模和需求，它可以使用 iptables 或 IPVS 模式来实现流量路由。\nkube-proxy怎么修改ipvs规则 # 在 Kubernetes 中，kube-proxy 使用 IPVS 模式时，流量转发是通过 Linux 内核的 IP Virtual Server (IPVS) 来实现的。kube-proxy 会动态地根据 Kubernetes 服务（Service）和 Endpoints 的变化来更新 IPVS 规则。\n修改 IPVS 规则 # 通常情况下，kube-proxy 会自动管理 IPVS 规则，你可以通过以下方式修改或查看 IPVS 规则：\n1. 启用 IPVS 模式 # 在 Kubernetes 中启用 IPVS 模式，需要在启动 kube-proxy 时进行配置。你可以通过修改 kube-proxy 的配置文件或命令行参数来启用 IPVS 模式。\n修改 kube-proxy 配置文件（通常是 /etc/kubernetes/manifests/kube-proxy.yaml），并设置 proxy-mode 为 ipvs。\n示例配置：\napiVersion: apps/v1 kind: DaemonSet metadata: name: kube-proxy namespace: kube-system spec: template: spec: containers: - name: kube-proxy image: k8s.gcr.io/kube-proxy:v1.23.0 command: - /bin/sh - -c - | # 修改 kube-proxy 启动参数 /usr/local/bin/kube-proxy --proxy-mode=ipvs 这个配置确保了 kube-proxy 启动时会使用 IPVS 模式。\n如果你使用的是 kubeadm 部署的集群，也可以通过配置文件 /etc/kubernetes/kube-proxy-config.yaml 来设置。\napiVersion: kubeproxy.config.k8s.io/v1alpha1 kind: KubeProxyConfiguration mode: \u0026#34;ipvs\u0026#34; 修改完配置后，kube-proxy 会重启并切换到 IPVS 模式。\n2. 查看 IPVS 规则 # 你可以使用 ipvsadm 工具来查看当前的 IPVS 规则。该工具显示的是当前节点上 IPVS 路由的配置。\n在每个节点上，执行以下命令来查看当前的 IPVS 规则：\nipvsadm -L -n 这个命令将列出当前的 IPVS 负载均衡规则。输出示例：\nTCP 10.96.0.1:80 rr 1 1 1 -\u0026gt; 10.244.0.2:80 masq -\u0026gt; 10.244.0.3:80 masq 解释：\n10.96.0.1:80 是 Service 的虚拟 IP 和端口。 后面跟的是实际 Pod 的 IP 地址和端口（如 10.244.0.2:80 和 10.244.0.3:80），它们是该 Service 后端的 Pod。 masq 表示使用源地址伪装。 3. 手动修改 IPVS 规则 # 如果你需要手动修改 IPVS 规则（例如，调整负载均衡策略或进行故障排查），可以使用 ipvsadm 工具。\n添加一条新的 IPVS 规则：\nipvsadm -A -t 10.96.0.1:80 -s rr ipvsadm -a -t 10.96.0.1:80 -r 10.244.0.2:80 -m ipvsadm -a -t 10.96.0.1:80 -r 10.244.0.3:80 -m 这会向 IPVS 中添加一个新的服务，绑定虚拟 IP 10.96.0.1:80，并将流量均匀地负载到两个 Pod（10.244.0.2:80 和 10.244.0.3:80）上，采用轮询负载均衡策略（-s rr）。\n删除 IPVS 规则：\nipvsadm -D -t 10.96.0.1:80 这会删除虚拟 IP 10.96.0.1:80 的规则。\n4. 修改负载均衡策略 # 如果你希望修改负载均衡算法，可以通过 ipvsadm 工具来更改负载均衡策略。\n修改为\n最少连接数\n：\nipvsadm -A -t 10.96.0.1:80 -s lc 修改为\n基于源 IP 的哈希\n：\nipvsadm -A -t 10.96.0.1:80 -s sh -s 参数后跟的负载均衡策略选项：\nrr: 轮询 (Round Robin) lc: 最少连接数 (Least Connections) sh: 源地址哈希 (Source Hashing) 5. 重新应用 IPVS 规则 # 由于 kube-proxy 会自动更新 IPVS 规则，修改规则后需要确保 kube-proxy 重新加载配置并应用新的规则。\n你可以通过重新启动\nkube-proxy 来确保它重新加载和应用新的 IPVS 配置：\nkubectl -n kube-system rollout restart daemonset kube-proxy 总结 # kube-proxy 使用 IPVS 模式时，负责在集群节点上配置和管理 IPVS 规则来实现负载均衡。 通过 ipvsadm 工具，可以查看、修改和删除 IPVS 规则。 如果需要修改 kube-proxy 的行为或负载均衡策略，可以通过修改配置文件来实现。 一般情况下，不建议手动修改 kube-proxy 的 IPVS 规则，除非你在进行故障排查或高级定制配置。 ipvs为什么比iptables效率高 # IPVS（IP Virtual Server）相比 iptables 更高效的原因，主要体现在以下几个方面：\n1. 专门为负载均衡设计 # IPVS 是一个专门为负载均衡设计的内核模块，旨在提供高效的流量调度和转发。它通过内核级的负载均衡算法和高效的路由机制，可以直接对流量进行负载均衡，减少了用户空间与内核空间的交互。 iptables 则是一个更通用的网络防火墙工具，虽然也可以用于流量转发和负载均衡，但它的初衷并非专门优化负载均衡。因此，iptables 在处理复杂的流量转发时，不如 IPVS 高效。 2. 性能优化和内核支持 # IPVS 使用了 内核中的负载均衡模块，并且支持多种负载均衡算法（如轮询、最少连接、基于源 IP 的哈希等），它能更高效地处理网络流量。 iptables 工作在更底层的网络过滤和控制层面，它通过查找匹配规则、设置目标（如 DNAT、SNAT）等操作来决定数据包的处理方式，处理过程相对复杂，需要更多的规则匹配和计算。 3. 规则查找方式 # IPVS 使用 哈希表 来存储和查找目标 Pod 或服务的 IP 地址，采用高效的散列算法，这使得它能够快速地做出路由决策。 iptables 在处理流量时，通过 链（chains）和规则（rules） 进行逐条匹配，尤其是在有大量规则时，性能可能会下降，因为需要依次遍历所有的规则来找到匹配项。这在大规模集群中可能会导致较高的延迟。 4. 连接状态管理 # IPVS 会对每个流量连接进行高效的状态管理。它能够维护每个连接的状态，并基于连接的特性（如源 IP、目标端口等）做出高效的负载均衡决策。 iptables 也可以进行连接跟踪，但在流量处理时，它的效率不如 IPVS，因为它需要逐个规则检查，而 IPVS 是根据每个连接的具体情况快速决定流向。 5. 负载均衡算法 # IPVS 支持多种高效的负载均衡算法，并能够实时地根据不同的网络情况动态选择合适的策略。这使得 IPVS 能够根据流量模式进行灵活的负载均衡。 iptables 需要用户在规则中显式配置负载均衡逻辑，且支持的负载均衡算法有限，性能也相对较差。 6. 高并发处理能力 # IPVS 在设计时就考虑到高并发、高负载的情况，它通过 hashing 和负载均衡池 来管理流量转发，能够高效地处理成千上万的请求。 iptables 的性能在大规模规则和高并发流量下容易下降，尤其是在复杂规则下。 7. 内存使用和效率 # IPVS 对于每个 Service 和 Pod 的信息进行高效的存储和访问，内存的使用更加高效。 iptables 在每次流量到达时，必须遍历所有的规则链，尤其是在有大量规则的情况下，会造成内存和 CPU 的消耗。 8. 流量转发的直接性 # IPVS 通过内核中直接的流量转发机制，将流量从客户端直接转发到后端 Pod，而无需额外的计算。 iptables 在流量转发时，必须依次处理每条规则，通常需要更复杂的处理步骤，导致处理延迟和性能开销。 总结 # IPVS 比 iptables 高效的主要原因是它作为专门为负载均衡设计的内核模块，在设计上进行了优化，支持高效的流量调度和转发机制，且具有更高效的规则查找方式、连接管理以及负载均衡算法。而 iptables 更多是作为一个通用的网络过滤工具，其设计上并没有专门针对负载均衡进行优化，因此在流量处理和高并发环境下，性能较 IPVS 要低。\npod之间访问不通怎么排查☆ # 当 Pod 之间无法通信 时，可以通过以下步骤来排查和解决问题。这个过程涉及从网络配置到 Kubernetes 资源的逐步检查，确保流量能够正确地流动。\n1. 检查 Pod 之间的网络连接 # Ping 测试：首先，确认 Pod 是否可以相互 ping 通。\nkubectl exec -it \u0026lt;pod-name\u0026gt; -- ping \u0026lt;target-pod-ip\u0026gt; 如果无法 ping 通，说明可能是网络问题。\n检查 Pod 的 IP 地址：使用以下命令查看 Pod 的 IP 地址。\nkubectl get pod \u0026lt;pod-name\u0026gt; -o wide 确保 Pod 的 IP 地址是有效的，并且与其他 Pod 的 IP 地址不冲突。\n2. 检查网络插件是否正常工作 # Kubernetes 中的网络插件（如 Calico、Flannel、Weave 等）负责 Pod 之间的通信。你需要确认网络插件是否在正常工作。\n查看网络插件的 Pod 状态：\nkubectl get pods -n kube-system 检查相关网络插件 Pod 的状态，确保它们没有崩溃（CrashLoopBackOff）或者处于 NotReady 状态。\n如果使用 Calico，可以查看其日志：\nkubectl logs \u0026lt;calico-pod-name\u0026gt; -n kube-system 查找是否有网络错误或者警告。\n3. 检查网络策略（Network Policies） # 如果你在集群中使用了 Network Policies，这些策略可能会限制 Pod 之间的流量。\n查看是否有正在应用的 Network Policy：\nkubectl get networkpolicy 如果有 Network Policy，确保它们没有不小心阻止 Pod 之间的流量。可以暂时禁用网络策略，测试是否是它们导致了通信问题。\n4. 检查 kube-proxy 状态 # kube-proxy 负责在集群中配置负载均衡和服务代理，确保网络流量正确转发。如果 kube-proxy 出现问题，可能导致 Pod 之间无法通信。\n查看 kube-proxy Pod 状态：\nkubectl get pods -n kube-system -l k8s-app=kube-proxy 检查 kube-proxy 是否正常运行，特别是是否报错。\n如果 kube-proxy 的日志中有问题，可以通过重启 kube-proxy 进行排查：\nkubectl -n kube-system rollout restart daemonset kube-proxy 5. 检查节点网络和防火墙设置 # 如果 Pod 跨节点通信出现问题，可能是节点之间的网络连接或防火墙设置导致的。\n确认节点之间的 网络连接 是否正常（比如使用 ping 或 telnet 测试端口）。\n检查节点的 防火墙设置（例如，iptables 规则）是否限制了 Pod 之间的通信。特别是确保没有防火墙阻止跨节点的流量。\n你可以检查每个节点的 iptables 设置，查看是否有任何规则阻止流量：\niptables -L -n 6. 查看 DNS 配置 # 有时 Pod 之间的访问失败可能是由于 DNS 配置不正确，导致 Pod 无法解析其他 Pod 的主机名。\n查看 DNS 服务（如 CoreDNS）的 Pod 状态：\nkubectl get pods -n kube-system -l k8s-app=coredns 查看 CoreDNS 的日志，检查是否存在解析问题：\nkubectl logs \u0026lt;coredns-pod-name\u0026gt; -n kube-system 如果是基于 DNS 名称访问出现问题，尝试使用 Pod 的 IP 地址直接访问目标 Pod，确认是否是 DNS 问题。\n7. 查看 Service 配置 # 如果是通过 Service 访问 Pod，但发现通信异常，可能是 Service 配置错误导致的。\n查看 Service 配置：\nkubectl get svc \u0026lt;service-name\u0026gt; -o yaml 检查 ClusterIP、端口、selector 是否正确，确保 Service 的 selector 能匹配到目标 Pod。\n8. Pod 重启和调试 # 如果通过上述方法仍然无法定位问题，尝试重启相关的 Pod 或服务，看看是否能解决问题。你可以尝试删除 Pod，让 Kubernetes 自动重建：\nkubectl delete pod \u0026lt;pod-name\u0026gt; 或者重启节点上的网络插件、kube-proxy，看看是否能够恢复通信。\n9. 集群的网络诊断工具 # 可以使用 Kubernetes 集群中的一些诊断工具来帮助你定位问题。例如：\nCalico 提供的诊断工具（如果使用 Calico 作为网络插件）。 Weave Net 提供的 weave status 命令。 Flannel 提供的 flannel 命令，查看网络状态和日志。 总结 # 排查 Pod 之间无法通信的原因需要从以下几个方面入手：\n确认 Pod 网络配置正常。 检查网络插件的运行状态。 检查是否有 Network Policy 阻止通信。 检查 kube-proxy 状态。 检查节点之间的网络连接和防火墙。 检查 DNS 配置。 查看 Service 配置。 使用诊断工具进行更深入的排查。 通常，Pod 之间无法通信的常见原因包括网络插件故障、Network Policies 配置错误、节点防火墙设置不当，或者 kube-proxy 的配置问题。\nk8s中Network Policy的实现原理 # 在 Kubernetes 中，Network Policy 是一种用于控制 Pod 之间网络流量的机制。它允许你定义哪些流量可以进入或离开某个 Pod，从而在网络层面上进行细粒度的流量控制。Kubernetes 本身并不直接实现 Network Policy，而是依赖于网络插件（如 Calico、Cilium、Weave 等）来实施这些策略。\nNetwork Policy 的基本原理 # 网络插件支持： Kubernetes 的网络策略由网络插件实现。不同的插件有不同的实现方式，但所有插件都遵循 Kubernetes 提供的 NetworkPolicy API 规范。 例如，Calico、Cilium 和 Weave 都支持 Network Policy，并在其内部实现了策略的拦截和执行。 选择器（Selector）： Pod Selector：Network Policy 使用标签选择器（Label Selector）来选择应用此策略的 Pod。通常使用 matchLabels 或 matchExpressions 来定义目标 Pod。 Namespace Selector：除了 Pod 选择器，Network Policy 还可以选择基于命名空间的流量。例如，可以指定来自特定命名空间的流量。 通过这些选择器，Network Policy 确定哪些 Pod 允许或拒绝流量。 入站（Ingress）和出站（Egress）规则： Ingress：指定哪些流量可以进入匹配的 Pod。Ingress 规则定义了允许从哪些源 IP 地址或 Pod 进入 Pod。 Egress：指定哪些流量可以从匹配的 Pod 出去。Egress 规则定义了允许 Pod 向哪些目的地址或 Pod 发送流量。 这些规则允许对流量进行细粒度控制，可以基于源地址、目标地址、端口等进行限制。 允许和拒绝规则： 默认情况下，所有 Pod 都能与集群中的其他 Pod 通信，但如果定义了 Network Policy，它会显式地控制哪些流量可以进出 Pod。 允许：可以通过 Network Policy 允许某些流量。 拒绝：Network Policy 允许通过选择器指定的规则拒绝流量。如果没有匹配到规则的流量，将被拒绝。 Network Policy 的工作流程 # 定义 Network Policy：\nNetwork Policy 通常通过 YAML 配置文件来定义。它包括 Ingress 和 Egress 规则，以及匹配标签选择器（selectors）来选择 Pod。 示例 Network Policy YAML：\napiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-nginx namespace: default spec: podSelector: matchLabels: app: nginx ingress: - from: - podSelector: matchLabels: app: frontend egress: - to: - podSelector: matchLabels: app: backend policyTypes: - Ingress - Egress 这个示例表示：\n只有带有标签 app: nginx 的 Pod 可以接受来自标签为 app: frontend 的 Pod 的流量。 这些 Pod 也可以向标签为 app: backend 的 Pod 发送流量。 插件拦截和实施：\n网络插件通过在节点上配置\niptables\n或\neBPF\n规则来实现 Network Policy 的功能。\niptables：某些插件（如 Calico）使用 iptables 来拦截和过滤网络流量，根据 Network Policy 定义的规则进行控制。 eBPF：现代的网络插件（如 Cilium）可能会使用 eBPF（扩展的 BPF，Berkley Packet Filter）技术，以更高效的方式处理流量。 策略传播和执行：\n当网络策略（Network Policy）被创建或更新时，网络插件会立即重新计算流量规则并将其应用到相应的网络接口或路由表上。 网络插件会根据规则在 网络层 对 Pod 的流量进行控制，允许符合条件的流量进入或离开 Pod，同时阻止不符合条件的流量。 默认行为：\n如果集群没有任何 Network Policy，则所有 Pod 默认是 无约束的，即 Pod 之间的通信是完全开放的。 一旦应用了任何 Network Policy，默认的行为变为 拒绝所有流量，除非显式地允许流量。这意味着如果没有规则指定某些流量，可以访问某个 Pod，则该流量将被拒绝。 例子：如何使用 Network Policy # 假设我们有以下场景：\nPod A：标签 app=frontend Pod B：标签 app=backend 我们希望 frontend Pod 可以与 backend Pod 通信，但 backend Pod 不允许与 frontend 以外的 Pod 通信。\n定义 Network Policy： # apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-frontend-to-backend namespace: default spec: podSelector: matchLabels: app: backend ingress: - from: - podSelector: matchLabels: app: frontend policyTypes: - Ingress 这个策略做了以下事情：\n允许标签为 app: frontend 的 Pod 向标签为 app: backend 的 Pod 发送流量。 除了 frontend Pod，其他 Pod 都无法访问 backend Pod。 网络策略的关键点总结 # 默认拒绝所有流量：没有 Network Policy 时，Pod 之间默认是开放的；有 Network Policy 后，默认拒绝所有流量，除非显式允许。 支持细粒度控制：可以基于 Pod 标签、命名空间标签、IP 地址等来定义流量控制规则。 网络插件依赖：Network Policy 的实际执行是依赖于 Kubernetes 集群中使用的网络插件，插件负责将策略转化为底层的网络规则（如 iptables 或 eBPF）。 策略类型：可以定义 Ingress 和 Egress 策略来分别控制入站和出站流量。 结论 # Kubernetes 中的 Network Policy 提供了灵活、细粒度的网络流量控制，能够在集群内部有效地隔离不同 Pod 和服务的通信。它通过网络插件将高层定义的策略转化为底层的网络配置，从而实现流量控制。\n探针有哪些？探测方法有哪些？ # 在 Kubernetes 中，探针（Probe） 是一种用于检测和管理容器健康状况的机制。探针能够定期检查容器的健康状态，如果容器的健康状态出现异常，Kubernetes 会根据设置的策略进行处理（如重启容器）。探针主要有三种类型：\n1. Liveness Probe（存活探针） # 作用：检测容器是否处于存活状态。如果容器死掉或变得不可恢复，Liveness Probe 会触发容器重启（由 kubelet 管理）。如果容器无法恢复到健康状态，Kubernetes 会根据策略重新启动容器。 使用场景：用于检测容器是否处于“挂掉”状态，例如应用死锁、无限循环等。 2. Readiness Probe（就绪探针） # 作用：用于检测容器是否准备好接收流量。如果容器未准备好（如正在启动或正在初始化），则 Kubernetes 不会将流量发送到该容器，直到它通过了就绪探针检查。 使用场景：用于检测应用是否已经完全启动并可以接受请求。例如，在数据库连接完成之前，可以设置就绪探针来保证流量不被发送到未完全准备好的容器。 3. Startup Probe（启动探针） # 作用：用于检查容器是否已经成功启动。它比 Liveness Probe 更加关注容器的启动过程，尤其是对于启动时间较长的容器。通常，在容器启动过程中，Startup Probe 会持续检查容器的状态，如果启动超时，Kubernetes 会重启容器。 使用场景：用于处理启动时间长、初始化步骤复杂的应用，确保容器在启动时有足够的时间完成启动。 探测方法（Probe Types） # Kubernetes 中的探针可以通过以下几种方法进行探测：\n1. HTTP GET Probe # 描述：通过向容器内部发起 HTTP 请求来探测容器的健康状态。Kubernetes 会检查返回的 HTTP 状态码，以此判断容器是否健康。\n使用场景：适用于基于 HTTP 服务的容器，例如 Web 应用、API 服务等。\n配置示例：\nlivenessProbe: httpGet: path: /healthz port: 8080 initialDelaySeconds: 3 periodSeconds: 5 这段配置说明 Kubernetes 会向容器的 /healthz 路径发起 HTTP 请求，检查容器是否健康。返回的 HTTP 状态码为 2xx 时，视为健康。\n2. TCP Socket Probe # 描述：通过尝试与容器内部的 TCP 端口建立连接来探测容器的健康状态。成功建立连接时认为容器健康。\n使用场景：适用于需要 TCP 连接的服务，如数据库、缓存服务等。\n配置示例：\nlivenessProbe: tcpSocket: port: 3306 initialDelaySeconds: 3 periodSeconds: 5 这段配置表示 Kubernetes 会尝试连接到容器的 TCP 端口 3306，成功建立连接时认为容器健康。\n3. Exec Probe # 描述：通过在容器内执行一个命令来探测容器的健康状态。如果命令执行成功（返回值为 0），则认为容器健康；如果返回值非零，则认为容器不健康。\n使用场景：适用于需要特定的命令来判断健康状态的容器。\n配置示例：\nlivenessProbe: exec: command: - \u0026#34;cat\u0026#34; - \u0026#34;/tmp/healthcheck\u0026#34; initialDelaySeconds: 3 periodSeconds: 5 这段配置表示 Kubernetes 会在容器内部执行 cat /tmp/healthcheck 命令，若命令成功执行，视为容器健康；如果命令失败，则认为容器不健康。\n配置探针时的常用参数 # initialDelaySeconds：容器启动后首次探测前的延迟时间。它允许容器有时间启动并准备好接收探测。 periodSeconds：每次探测之间的时间间隔。默认值是 10 秒。 timeoutSeconds：探测请求的超时时间。如果探测超时，会认为容器不健康。默认是 1 秒。 successThreshold：连续成功的探测次数。默认为 1。如果设置为大于 1，Kubernetes 会要求探针连续成功多次才会认为容器健康。 failureThreshold：连续失败的探测次数。默认为 3。如果设置为大于 1，Kubernetes 会要求探针连续失败多次才会认为容器不可用。 示例：完整的探针配置 # 以下是一个包含三种探针的完整示例，展示如何配置 Liveness、Readiness 和 Startup Probe：\napiVersion: v1 kind: Pod metadata: name: mypod spec: containers: - name: mycontainer image: myimage:latest livenessProbe: httpGet: path: /healthz port: 8080 initialDelaySeconds: 3 periodSeconds: 5 readinessProbe: httpGet: path: /readiness port: 8080 initialDelaySeconds: 5 periodSeconds: 5 startupProbe: httpGet: path: /startup port: 8080 initialDelaySeconds: 10 periodSeconds: 5 总结 # Kubernetes 提供了 Liveness Probe、Readiness Probe 和 Startup Probe 三种探针来帮助管理容器的健康状态和流量调度。探针通过不同的方式（HTTP GET、TCP Socket、Exec 命令）来检查容器的健康状况，并采取相应的措施，如重启容器或阻止流量发送到不健康的容器。\npod健康检查失败可能的原因和排查思路 # 当 Kubernetes 中的 Pod 健康检查（Health Check） 失败时，可能会导致容器被重启或者流量被拒绝。健康检查失败的原因可能有很多种，通常与应用、配置、网络、资源等问题相关。以下是一些常见的原因和排查思路。\n常见的 Pod 健康检查失败的原因 # 应用本身的问题： 应用崩溃或挂掉：应用进程崩溃或死锁，导致健康检查失败。 应用启动慢：如果应用启动时间较长，可能未能在设置的 initialDelaySeconds 内完成启动，导致健康检查失败。 应用依赖问题：应用可能依赖于其他服务或资源（如数据库、外部 API 等），如果依赖的服务不可用，应用可能无法启动或运行，导致健康检查失败。 健康检查配置问题： 路径或端口错误：配置的 HTTP 路径、端口号或 TCP 端口号不正确，导致探针无法访问容器。 命令执行错误：如果使用的是 exec 探针，命令返回非零状态码，则会被认为健康检查失败。 探针超时：如果探针的超时时间 (timeoutSeconds) 配置过短，探针可能会在容器响应前超时。 探针的 initialDelaySeconds 配置不足：容器刚启动时，可能需要一些时间才能启动完成，探针需要给予容器足够的时间来初始化。 资源不足： CPU 或内存不足：如果容器的 CPU 或内存资源不足，可能会导致容器变得不响应或死掉，从而导致健康检查失败。 节点压力：如果节点负载过高或资源不够，可能会影响容器的性能，导致健康检查失败。 网络问题： 网络延迟或不可达：如果容器依赖网络资源（如数据库或外部 API），并且网络出现故障或延迟，健康检查可能无法成功。 防火墙或网络策略问题：Network Policy 或防火墙规则可能限制了容器之间或容器与外部服务之间的通信。 依赖服务不可用： 如果应用依赖于其他服务（如数据库、消息队列等），这些服务的不可用会导致应用不能正常工作，从而导致健康检查失败。 健康检查失败的排查思路 # 1. 检查容器日志 # 查看容器的日志以确定是否有应用崩溃、错误或异常。\nkubectl logs \u0026lt;pod-name\u0026gt; -c \u0026lt;container-name\u0026gt; 查找启动过程中是否有错误消息或异常，尤其是与依赖服务相关的错误。\n2. 检查探针配置 # 确认\nlivenessProbe 和\nreadinessProbe 的配置是否正确，尤其是路径、端口和命令。\n对于 HTTP 探针，确保配置的路径和端口是正确的，并且容器内的服务在该路径上提供响应。 对于 TCP 探针，确保指定的端口是容器内实际开放的端口。 对于 exec 探针，确保命令是容器内有效的命令，并且可以正确执行。 查看探针配置中的超时时间、初始延迟和检查周期，是否合理配置：\nlivenessProbe: httpGet: path: /healthz port: 8080 initialDelaySeconds: 5 timeoutSeconds: 2 periodSeconds: 5 failureThreshold: 3 如果 initialDelaySeconds 配置过小，容器还未启动完就进行健康检查，可能导致失败。\n3. 检查容器资源限制 # 检查容器的\nCPU\n和\n内存\n配置，是否给容器分配了足够的资源。资源不足可能导致容器响应缓慢或挂掉。\nresources: requests: memory: \u0026#34;64Mi\u0026#34; cpu: \u0026#34;250m\u0026#34; limits: memory: \u0026#34;128Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; 如果容器使用的资源接近或超过节点的限制，可能会导致容器崩溃或无法及时响应健康检查。\n4. 检查依赖服务的可用性 # 如果容器依赖外部服务（如数据库、消息队列等），检查这些服务是否可用。如果是数据库连接问题，可以查看应用日志或数据库日志。\n通过外部工具（如\ntelnet 或\ncurl ）测试容器和外部服务的连接：\ncurl \u0026lt;service-ip\u0026gt;:\u0026lt;port\u0026gt; 如果有网络连接问题，可能需要检查 Kubernetes 集群的网络配置，特别是\nNetwork Policy\n或防火墙设置。\n5. 查看 Kubernetes 事件 # 查看 Pod 的事件信息，了解容器重启和探针失败的详细信息：\nkubectl describe pod \u0026lt;pod-name\u0026gt; 查找与健康检查相关的错误信息，例如：\n\u0026ldquo;Liveness probe failed\u0026rdquo; 或 \u0026ldquo;Readiness probe failed\u0026rdquo; \u0026ldquo;Back-off restarting failed container\u0026rdquo;（容器重启失败） 6. 检查节点的资源使用情况 # 确认节点的 CPU 和内存是否出现瓶颈，导致容器无法正常运行。\nkubectl top nodes kubectl top pods 如果节点资源不足，可能需要扩展节点或调整资源配额。\n7. 查看网络插件和策略 # 检查是否有\nNetwork Policy\n阻止了 Pod 之间的通信。\nkubectl get networkpolicy 如果使用了网络插件（如 Calico、Cilium 等），检查插件的状态和日志，确认网络是否正常。\n8. 验证探针的路径是否有效 # 如果健康检查是基于 HTTP 请求的，确保容器内部的服务可以响应正确的 HTTP 状态码。你可以通过以下命令手动检查服务：\ncurl http://localhost:\u0026lt;port\u0026gt;/healthz 检查 HTTP 服务器是否已经启动，并且健康检查路径（如 /healthz）返回的是成功的状态码（如 200 OK）。\n总结 # Pod 健康检查失败的原因可能包括：\n应用本身的问题（崩溃、启动慢等）。 健康检查配置错误（路径、端口、命令不正确，超时设置不合理等）。 资源不足，导致容器无法正常响应。 网络问题，导致容器无法访问外部服务或其他 Pod。 依赖服务不可用，影响应用的启动或运行。 排查时，应从容器日志、探针配置、资源使用、依赖服务、Kubernetes 事件等多方面入手，逐步定位问题。\nk8s的Service是什么☆ # Kubernetes 中的 Service 是一种抽象层，用于定义一组 Pod 的访问方式。它是一个负载均衡器，将客户端请求转发到后端的 Pod 上，确保即使 Pod 的 IP 地址变化，外部访问也能够稳定地连接到正确的 Pod。Service 还可以提供内部或外部网络访问入口，用于跨 Pod 或外部访问服务。\nService 的主要作用 # 负载均衡： Kubernetes Service 会自动将请求负载均衡到其背后的多个 Pod 上。通过将请求轮流分发给后端 Pod，Service 提供了一种高可用的访问方式。 服务发现： Service 提供一个稳定的 IP 地址和 DNS 名称（如 \u0026lt;service-name\u0026gt;.\u0026lt;namespace\u0026gt;.svc.cluster.local），使得即使 Pod 的 IP 地址变化，客户端仍然能够通过 Service 的 IP 或 DNS 访问该服务。 抽象网络通信： Service 提供了一个稳定的网络接口，而不用关心 Pod 的具体网络信息。Pod 可能会被动态创建、删除或重启，但客户端始终可以通过 Service 访问服务。 Service 的类型 # Kubernetes 提供了多种类型的 Service，以满足不同的需求：\nClusterIP（默认类型）：\n作用：为 Service 分配一个集群内的虚拟 IP（VIP），并且只能在集群内部访问该服务。 使用场景：适用于集群内部的服务通信。 kind: Service apiVersion: v1 metadata: name: my-service spec: selector: app: myapp ports: - protocol: TCP port: 80 targetPort: 8080 访问方式：通过 \u0026lt;service-name\u0026gt;.\u0026lt;namespace\u0026gt;.svc.cluster.local 或 \u0026lt;service-ip\u0026gt; 进行访问。 NodePort：\n作用：将 Service 映射到集群节点的一个特定端口，使得外部可以通过 \u0026lt;node-ip\u0026gt;:\u0026lt;node-port\u0026gt; 访问服务。 使用场景：适用于需要外部访问集群内服务的场景，通常用于开发、测试环境。 kind: Service apiVersion: v1 metadata: name: my-service spec: selector: app: myapp ports: - protocol: TCP port: 80 targetPort: 8080 nodePort: 30001 type: NodePort 访问方式：通过 \u0026lt;node-ip\u0026gt;:\u0026lt;node-port\u0026gt; 进行访问，外部用户可以通过集群节点的 IP 地址和指定端口访问服务。 LoadBalancer：\n作用：将 Service 映射到云提供商的负载均衡器（如 AWS、GCP、Azure）上，外部流量可以通过云负载均衡器的 IP 或 DNS 名称访问集群中的服务。 使用场景：适用于生产环境，需要通过云负载均衡器提供外部访问。 kind: Service apiVersion: v1 metadata: name: my-service spec: selector: app: myapp ports: - protocol: TCP port: 80 targetPort: 8080 type: LoadBalancer 访问方式：通过云负载均衡器的公共 IP 或 DNS 名称进行访问。 ExternalName：\n作用：将 Service 映射到外部 DNS 名称，而不是集群内的 Pod。这意味着 Kubernetes 会将流量转发到指定的外部服务（通常是外部 DNS 名称）上。 使用场景：适用于需要将内部服务与外部服务连接的场景，例如访问外部数据库或 API。 kind: Service apiVersion: v1 metadata: name: my-service spec: type: ExternalName externalName: example.com 访问方式：通过 \u0026lt;service-name\u0026gt;.\u0026lt;namespace\u0026gt;.svc.cluster.local 进行访问，Kubernetes 会将请求转发到外部服务 example.com。 Service 的工作原理 # Kubernetes 的 Service 是通过与 Endpoints 配合来工作的。当 Service 与 Pod 进行绑定时，Kubernetes 会自动为该 Service 创建一个 Endpoint 对象，记录与该 Service 关联的 Pod 的 IP 地址。\n选择器（Selector）：Service 使用标签选择器来选择哪些 Pod 作为该 Service 的后端。标签选择器会选择符合指定标签的 Pod。 端口映射（Port Mapping）：Service 会将客户端的请求转发到后端 Pod 的指定端口。如果有多个 Pod，Kubernetes 会对请求进行负载均衡。 当一个请求发往 ClusterIP 或 NodePort 时，Kubernetes 会使用 iptables 或 IPVS 进行负载均衡，选择一个健康的 Pod 来响应请求。\nService 的健康检查 # Kubernetes Service 本身并不执行健康检查，而是通过 Pod 的健康检查（如 Liveness Probe 和 Readiness Probe）来判断后端 Pod 是否健康。当一个 Pod 健康检查失败时，Kubernetes 会从 Service 的 Endpoints 列表中移除该 Pod，从而不再将流量发送到该 Pod。\n访问 Service 的方式 # 内部访问： Service 的 ClusterIP 可以在集群内通过 \u0026lt;service-name\u0026gt;.\u0026lt;namespace\u0026gt;.svc.cluster.local 进行访问。集群中的所有 Pod 都可以通过该域名访问该 Service。 外部访问： 对于 NodePort 和 LoadBalancer 类型的 Service，外部用户可以通过 \u0026lt;node-ip\u0026gt;:\u0026lt;node-port\u0026gt; 或云负载均衡器的 IP 或 DNS 进行访问。 总结 # Service 是 Kubernetes 中的一种抽象，它提供了稳定的网络接口，帮助 Pod 提供负载均衡和服务发现。 Kubernetes 支持多种类型的 Service，适用于不同的访问场景（如集群内部访问、外部访问、云负载均衡器等）。 Service 使用标签选择器来确定要暴露的 Pod，通过 Endpoints 维护 Pod 的 IP 地址。 通过健康检查机制，Kubernetes 会确保仅向健康的 Pod 发送流量。 metrics-server采集指标数据链路 # metrics-server 是 Kubernetes 集群中的一个轻量级、聚合型的监控组件，主要用于收集集群内节点和 Pod 的资源使用情况（如 CPU、内存等）。它提供了这些指标数据，供 Horizontal Pod Autoscaler (HPA) 和其他 Kubernetes 组件使用。metrics-server 不持久化数据，它只将资源指标从节点和 Pod 中采集并聚合后提供给 API 服务供其他组件查询。\n以下是 metrics-server 采集指标数据的完整链路：\n1. Node 端的 Kubelet 上报资源指标 # 每个 Node 上的 Kubelet 会周期性地收集本节点上所有 Pod 的资源使用情况（如 CPU 和内存）。\nKubelet\n会通过\n/metrics/cadvisor 和\n/stats/summary 等接口提供当前节点和 Pod 的资源使用数据。\n/metrics/cadvisor：提供 cAdvisor 中收集的容器级别的资源指标。 /stats/summary：提供节点级别和容器级别的资源使用数据。 这些资源数据以 JSON 格式提供，Kubelet 定期更新这些指标。\n2. Metrics Server 收集数据 # metrics-server 会定期通过 Kubelet 的 Metrics API 获取每个 Node 和 Pod 的资源使用数据。 Metrics Server 并不直接向 Node 发送请求，而是通过 Kubernetes API Server 访问集群中所有 Node 的 Kubelet 端点，收集指标数据。 查询方式：metrics-server 使用 Kubernetes API 访问 /apis/metrics.k8s.io/v1beta1/nodes 和 /apis/metrics.k8s.io/v1beta1/pods 等接口获取节点和 Pod 的资源使用情况。 Metrics Server 采集的数据包括： 节点（Node）：CPU 使用率、内存使用量、磁盘使用量等。 Pod：每个容器的 CPU 和内存使用量。 3. 数据聚合和处理 # metrics-server 会将从所有节点收集到的资源数据进行汇总和处理，并将它们作为 Metrics API 提供给 Kubernetes API Server。 Metrics 数据被以 实时 的方式提供给集群内其他的组件（如 Horizontal Pod Autoscaler (HPA)）来进行自动扩缩容等操作。 4. 通过 Kubernetes API 提供访问 # 其他组件（如 HPA）可以通过 Kubernetes API 查询和使用这些资源数据。\n例如，Horizontal Pod Autoscaler 会通过以下方式查询资源指标数据：\nkubectl get --raw \u0026#34;/apis/metrics.k8s.io/v1beta1/namespaces/default/pods\u0026#34; 这将返回一个 JSON 响应，其中包含了指定命名空间中所有 Pod 的资源使用情况（CPU、内存等）。\n5. Pod Autoscaling 的触发 # Horizontal Pod Autoscaler (HPA) 会定期查询 metrics-server 提供的指标数据（如 CPU、内存利用率等）来决定是否扩缩容。 如果 Pod 的资源使用超出了设置的阈值，HPA 会增加 Pod 的副本数量；反之，则会减少副本数。 6. 数据过期与刷新 # metrics-server 会定期重新拉取数据，通常每 60 秒就会刷新一次数据。它不会持久化数据，只会在内存中保持最近的资源使用数据。 旧的指标数据会被定期清除，不会长期保存，因此 metrics-server 是一个“短期”聚合工具，用于实时监控和扩缩容决策。 7. 数据存储与可视化 # 虽然 metrics-server 负责实时采集并提供指标数据，但它并不负责长期存储这些数据。如果需要长期存储并进行分析，可以考虑使用 Prometheus 来收集、存储和可视化资源使用数据。 Prometheus 和 metrics-server 不同，Prometheus 会定期从 Kubelet 和其他组件拉取数据，并将数据存储在长期数据库中，便于后续的查询和分析。 采集数据链路总结： # Kubelet 定期通过 /metrics/cadvisor 和 /stats/summary 等接口提供本节点和 Pod 的资源使用情况。 Metrics Server 从 Kubernetes API Server 查询各个节点和 Pod 的资源使用数据。 Metrics Server 聚合和处理数据后，作为 Metrics API 提供给集群内部组件。 HPA 等组件查询 Metrics API 以决定是否扩缩容。 Prometheus 可用于长期存储和可视化资源使用数据。 通过这个链路，metrics-server 为 Kubernetes 集群提供了实时的资源监控，帮助其他组件做出自动化决策，如自动扩容、缩容等。\nk8s服务发现有哪些方式？ # Kubernetes 提供了多种 服务发现 机制，用于让应用和服务之间能够彼此找到并进行通信。服务发现的目标是使服务能够在动态变化的环境中，尤其是容器和 Pod 的 IP 地址不断变化时，依然能够稳定地进行访问。Kubernetes 的服务发现方式主要有以下几种：\n1. DNS 服务发现 # Kubernetes 内建的 DNS 服务 是最常用的服务发现机制，它为集群中的所有服务提供了一个稳定的名称解析系统。\n如何工作：\n每个 Kubernetes 集群通常会运行一个 CoreDNS 或 kube-dns 服务，负责解析集群内的 DNS 请求。 每当你创建一个 Service，Kubernetes 会为该 Service 分配一个 DNS 名称。默认情况下，服务的 DNS 名称由 \u0026lt;service-name\u0026gt;.\u0026lt;namespace\u0026gt;.svc.cluster.local 构成。 例如，如果你有一个名为 my-service 的 Service 位于 default 命名空间中，那么该服务的 DNS 名称将是 my-service.default.svc.cluster.local。 DNS 查询：\n集群中的 Pod 可以通过 DNS 名称访问其他服务：\ncurl http://my-service.default.svc.cluster.local Kubernetes 通过 CoreDNS（或 kube-dns）将 DNS 查询解析到对应的 Service 的 ClusterIP。\n应用场景：\n集群内部通信：服务与服务之间的通信通常使用 DNS 名称。 Pod 和 Service 之间：Pod 内的应用可以通过 DNS 名称访问服务，无论该服务背后的 Pod 如何变化。 2. ClusterIP（内网访问） # ClusterIP 是 Kubernetes 中 Service 的默认类型，用于将服务暴露为一个集群内部的虚拟 IP（VIP）。\n如何工作\n：\n每个 Kubernetes Service 类型为 ClusterIP 时，Kubernetes 会为该 Service 分配一个虚拟 IP 地址，该 IP 地址在集群内部是固定的，其他 Pod 可以通过该 IP 地址访问服务。 服务发现可以通过 DNS 解析该 IP 地址。 应用场景\n：\n适用于集群内部的服务通信，尤其是当服务不需要暴露到外部网络时。 例如，在 Kubernetes 集群中的应用 Pod 可以通过访问 my-service.default.svc.cluster.local 来访问 ClusterIP 类型的 Service。 3. NodePort（外部访问） # NodePort 是 Kubernetes 中 Service 的一种类型，它将服务暴露到集群外部，并将请求通过集群节点的指定端口转发到服务的 ClusterIP。\n如何工作\n：\nKubernetes 为每个 NodePort 服务分配一个端口（范围通常是 30000-32767），该端口在集群中的每个节点上都可以访问。 通过访问集群任意节点的 \u0026lt;node-ip\u0026gt;:\u0026lt;node-port\u0026gt;，请求将被转发到该 Service 的 ClusterIP 上，从而访问服务。 应用场景\n：\n适用于将 Kubernetes 服务暴露到集群外部的场景，常用于开发和测试环境。 通过该方式，可以在 Kubernetes 集群外部通过节点的 IP 和端口访问集群内的服务。 4. LoadBalancer（云环境外部访问） # LoadBalancer 是 Kubernetes 中的一个 Service 类型，通常与云服务提供商（如 AWS、Azure、GCP）结合使用，自动创建一个外部负载均衡器。\n如何工作： 当创建 LoadBalancer 类型的 Service 时，云提供商会自动创建一个外部负载均衡器，并将负载均衡器的 IP 地址或 DNS 名称映射到该 Service 上。 通过负载均衡器的 IP 地址或 DNS 名称，外部用户可以访问服务，负载均衡器会将流量转发到 ClusterIP 或 NodePort 类型的服务上。 应用场景： 适用于生产环境，尤其是需要将服务暴露到公网，且希望由云提供商管理负载均衡的场景。 比如，Kubernetes 在 AWS 上时，创建 LoadBalancer 服务会自动申请一个 ELB（Elastic Load Balancer）。 5. ExternalName（外部服务访问） # ExternalName 是 Kubernetes 中的一个 Service 类型，允许将一个 Service 映射到外部 DNS 名称，而不是集群内部的 Pod 或 ClusterIP。\n如何工作\n：\n当创建 ExternalName 类型的 Service 时，Kubernetes 不会暴露一个虚拟 IP，而是将请求转发到指定的外部 DNS 名称。 通过这种方式，Kubernetes 中的应用可以访问外部的服务，而不需要了解外部服务的 IP 地址。 应用场景\n：\n适用于需要访问集群外部服务的场景。例如，访问外部数据库、API 服务或第三方服务时，可以通过 Kubernetes 创建一个 ExternalName 类型的 Service。 例如，创建一个名为 external-db 的 Service，将其映射到外部 DNS db.example.com 上，集群内的 Pod 通过 external-db.default.svc.cluster.local 访问外部数据库。 6. Headless Service（无头服务） # Headless Service 是 Kubernetes 中 Service 的一种特殊类型，用于不使用 ClusterIP 或负载均衡机制，而是直接通过 DNS 解析访问每个 Pod。\n如何工作\n：\n在创建 Service 时，设置 ClusterIP: None，即禁用 ClusterIP，同时使用 DNS 名称进行服务发现。 这种方式不会为 Service 创建虚拟 IP 地址，而是为每个 Pod 创建一个 DNS A 记录，Pod 的 DNS 名称格式为 \u0026lt;pod-name\u0026gt;.\u0026lt;service-name\u0026gt;.\u0026lt;namespace\u0026gt;.svc.cluster.local。 应用场景\n：\n适用于需要直接访问 Pod 的情况，常见于有状态应用，如数据库集群（例如 MongoDB、Cassandra）等，Pod 的 IP 地址在集群中动态变化时，服务发现可以直接访问每个 Pod。 7. Endpoints（手动定义服务发现） # Kubernetes 的 Endpoints 是一种低级别的服务发现方式，用于映射服务与后端 Pod 之间的关系。\n如何工作： Endpoints 是一种资源对象，它描述了 Service 后端 Pod 的 IP 地址和端口。 手动创建 Endpoint 对象，将外部或其他 Pod 作为后端服务进行访问。 应用场景： 当服务不使用 Service 对象或不使用 Kubernetes 自动化的服务发现时，可以手动定义 Endpoint 对象来实现服务发现。 例如，当需要将外部数据库或服务纳入到 Kubernetes 集群的服务发现中时。 总结 # Kubernetes 提供了多种服务发现方式，适用于不同的场景：\nDNS 服务发现：使用 DNS 名称访问服务，适合集群内部通信。 ClusterIP：为服务分配一个集群内部的虚拟 IP，适合内部服务通信。 NodePort：将服务暴露到外部，通过节点的 IP 和端口访问。 LoadBalancer：与云负载均衡器结合，适合将服务暴露到公网。 ExternalName：将 Service 映射到外部 DNS 名称，适合访问外部服务。 Headless Service：通过 DNS 直接解析每个 Pod，适用于有状态服务。 Endpoints：手动定义服务与后端 Pod 的映射关系，用于特殊场景。 不同的服务发现方式可以根据需要选择和配置，提供灵活的服务访问和负载均衡机制。\npod几种常用状态 # 在 Kubernetes 中，Pod 的生命周期有多个状态，每个状态代表了 Pod 当前的不同阶段或健康状况。以下是一些常见的 Pod 状态及其说明：\n1. Pending # 描述：Pod 已经被创建并且调度到一个节点，但还没有开始运行，通常是因为 Pod 的容器镜像还没有下载，或者资源（如 CPU、内存）尚未准备好。\n原因\n：\n资源不足（例如节点没有足够的内存或 CPU 来启动 Pod）。 容器镜像下载失败或拉取时间过长。 Pod 的调度过程还未完成。 2. Running # 描述：Pod 已经调度到某个节点并且至少有一个容器正在运行。\n原因\n：\nPod 的所有容器都已经成功启动，并且正在运行中。 Pod 可以接受请求，容器内的应用可以开始提供服务。 3. Succeeded # 描述：Pod 内的所有容器已经成功执行完毕并退出，没有发生任何错误或异常。\n原因\n：\n适用于一次性任务（如批处理任务），容器正常完成任务后退出。 4. Failed # 描述：Pod 内的容器因为错误退出（退出码非零），且重启策略不允许重启容器。\n原因\n：\n容器内的应用崩溃或执行过程中发生错误导致容器失败退出。 容器的重启策略为 Never 或 OnFailure 且失败条件不满足时。 5. CrashLoopBackOff # 描述：Pod 的容器在启动后不久崩溃，并且 Kubernetes 尝试多次重启容器，仍然无法恢复正常状态。每次重启都会有一定的延迟（BackOff），以避免过度尝试重启。\n原因\n：\n容器的启动过程失败，例如应用配置错误、环境问题、依赖服务未启动等。 当容器因某种原因失败并尝试重启，但在多个重启后仍然无法恢复时，状态会变为 CrashLoopBackOff。 6. Unknown # 描述：Kubernetes 无法获取 Pod 的状态，通常表示节点与集群管理组件之间的通信出现问题。\n原因\n：\n节点与 Kubernetes API Server 之间的网络或通信出现问题，导致无法获取 Pod 状态。 7. Terminating # 描述：Pod 正在被删除，并且 Kubernetes 已经开始终止容器和清理资源。\n原因\n：\n用户或控制器发出了删除 Pod 的命令，Pod 正在关闭并释放资源。 容器和 Pod 的删除过程可能会有一定的延迟，特别是在删除过程中等待容器的退出或清理临时存储等。 8. Waiting # 描述：Pod 内的容器正在等待某些条件才能启动。常见的等待原因包括容器镜像下载失败、挂载存储卷出现问题等。\n原因\n：\n容器在启动时遇到阻塞状态，可能是因为某些依赖没有准备好（如挂载的存储还没就绪）。 容器被限制启动（如因为节点资源不足）。 总结 # 常见的 Pod 状态包括：\nPending：正在等待资源和调度。 Running：正在运行。 Succeeded：成功完成任务并退出。 Failed：由于错误容器退出。 CrashLoopBackOff：容器多次失败并在重启时有延迟。 Unknown：无法获取 Pod 状态，通常是通信问题。 Terminating：Pod 正在被删除。 Waiting：容器处于等待状态，通常是等待某些条件满足。 理解这些状态有助于排查和诊断 Kubernetes 集群中的应用问题，尤其是当 Pod 状态不正常时，可以根据不同的状态做出相应的排查与处理。\nPod 生命周期的钩子函数 # 在 Kubernetes 中，Pod 生命周期钩子函数（Lifecycle Hooks）用于在 Pod 生命周期的特定阶段执行用户自定义操作。钩子函数允许在容器启动、终止时执行特定的任务，通常用于初始化、清理、日志记录、通知等。常见的钩子函数包括：\n1. PostStart # 描述：在容器启动之后立即执行的钩子函数。此钩子会在容器的应用程序启动之前执行。\n使用场景：\n用于容器启动后执行初始化任务，例如数据库初始化、数据迁移等。 可以用来启动容器内的外部服务或做其他准备工作。 注意事项：\nPostStart 钩子在容器启动完成之前就会执行，因此如果该钩子执行失败，容器仍然会被认为启动失败。 配置示例：\nlifecycle: postStart: exec: command: [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;echo Container started\u0026#34;] 2. PreStop # 描述：在容器终止之前执行的钩子函数。Kubernetes 在发送 SIGTERM 信号时会触发 PreStop 钩子，以便容器能够执行清理工作。\n使用场景：\n用于在容器关闭之前执行一些清理任务，例如关闭外部连接、保存状态或清理临时文件等。 适合在容器终止时进行优雅的关闭，以确保没有丢失数据或影响业务。 注意事项：\nPreStop 钩子可能会受到容器的终止时间限制（通常为 30 秒，取决于 terminationGracePeriodSeconds 设置），如果钩子执行时间超过限制，容器将被强制杀死。 配置示例：\nlifecycle: preStop: exec: command: [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;echo Container stopping\u0026#34;] 3. 限制与注意事项 # 钩子函数的 执行时间有限制，尤其是在容器终止时的 PreStop 钩子，通常 Kubernetes 会根据 terminationGracePeriodSeconds 配置给容器一定的优雅终止时间（默认为 30 秒）。如果钩子执行超过该时间，容器会被强制终止。 钩子执行的错误处理：如果钩子执行失败，Kubernetes 会根据具体情况采取不同的策略（如容器启动失败时 Pod 进入 Crash 状态）。特别是 PostStart 钩子失败时，容器会被视为启动失败，Kubernetes 会尝试重新启动容器。 4. 钩子的执行方式 # Kubernetes 支持以下几种方式来执行钩子函数：\nExec：在容器内执行指定的命令。适用于需要执行自定义命令的场景。 HTTP：向容器内的某个 HTTP 服务发送请求。适用于需要与容器内服务进行交互的场景。 示例：\nlifecycle: postStart: httpGet: path: /healthz port: 8080 preStop: exec: command: [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;curl -X POST http://localhost:8080/shutdown\u0026#34;] 总结 # Pod 生命周期钩子函数 PostStart 和 PreStop 可以帮助用户在容器启动和停止时执行自定义操作。PostStart 钩子在容器启动后执行，而 PreStop 钩子在容器终止前执行。合理使用钩子函数可以帮助容器在生命周期内进行一些必要的初始化和清理操作，从而提高应用的稳定性和健壮性。\nCalico和flannel区别☆ # Calico 和 Flannel 都是 Kubernetes 中常用的网络插件，它们提供了容器间的网络通信支持，但在实现方式、功能和特性上存在一些显著的差异。以下是它们的主要区别：\n1. 架构设计 # Flannel： Flannel 是一个简单的网络插件，设计上比较轻量级，主要关注为 Kubernetes 提供一个扁平化的、可扩展的网络模型。 Flannel 使用的是 Overlay 网络，通过在每个节点上创建虚拟网络（通常使用 VXLAN 或者其他 tunneling 协议）来实现容器之间的通信。 由于 Flannel 的架构简单，它的网络性能相对较低，适用于对性能要求不太高的场景。 Calico： Calico 提供了一个更加灵活和强大的网络解决方案，支持 Layer 3 网络，不仅支持 Overlay 网络（使用 IP-in-IP 或 VXLAN），还可以使用 路由模式（如 BGP）进行容器间的通信。 Calico 的架构设计允许更加细粒度的控制，包括网络策略、IP 地址管理等功能。 2. 网络模型 # Flannel： 使用 Overlay 网络：每个 Pod 都分配一个虚拟 IP 地址，并通过隧道技术（VXLAN 或其它方式）将这些 IP 地址连接到其他节点的 Pod 上。Pod 之间的通信通过隧道（tunnel）实现。 网络通信相对简单，但可能存在一些性能开销。 Calico： 支持 Overlay 和 非 Overlay（路由）网络：可以根据需求选择是否使用隧道。如果使用路由模式，Pod 会直接使用物理网络进行通信，减少了网络层的开销。 在非 Overlay 模式下，Calico 会利用 BGP（边界网关协议） 来实现不同节点之间的直接路由，而不是依赖于隧道。 3. 网络策略（Network Policy） # Flannel： Flannel 本身不提供网络策略功能。它依赖于 Kubernetes 本身的网络策略（NetworkPolicy）来控制 Pod 之间的通信，或者需要与其他插件（如 Cilium）配合使用来实现网络策略。 Flannel 只是提供了基础的网络通信能力。 Calico： Calico 提供了强大的 网络策略（NetworkPolicy） 功能，允许用户在粒度上定义哪些 Pod 可以与哪些 Pod 通信，控制流量的入口和出口。Calico 的网络策略比 Kubernetes 原生的网络策略更为强大，支持细粒度的控制。 4. 性能 # Flannel： 由于使用了 Overlay 网络，Flannel 会在网络通信时引入一定的性能开销，尤其是在高流量的场景下。 对于需要高性能的网络通信，Flannel 的性能可能不如 Calico。 Calico： Calico 的性能较高，尤其在使用 路由模式（非 Overlay）时，Pod 之间的通信直接通过物理网络路由，不需要额外的隧道，因此性能较优。 在支持大规模集群和高吞吐量时，Calico 表现得更好。 5. IP 地址管理 # Flannel： Flannel 提供简单的 IP 地址管理方案，通常会为每个节点分配一个子网，并为每个 Pod 分配一个虚拟 IP 地址。 Flannel 使用 etcd 作为后端存储来管理 IP 地址分配。 Calico： Calico 提供灵活的 IP 地址管理，支持直接在物理网络上分配 IP 地址，支持多种 IP 地址池配置，支持与现有的 IPAM（IP 地址管理）系统集成。 Calico 不仅可以为 Kubernetes 分配 IP 地址，还可以扩展到容器之外的网络范围。 6. 扩展性 # Flannel： Flannel 的功能相对简单，适用于需要快速部署且不需要复杂网络策略的场景。 虽然 Flannel 支持插件扩展，但它的扩展性相对较差，特别是在涉及到复杂的流量控制和安全策略时。 Calico： Calico 提供了更高的扩展性，支持与其他系统的集成（如 CNI、BGP、网络策略管理等），能够适应复杂的企业级需求。 它支持更复杂的网络环境，并且可以在不同的云平台和裸机环境中高效运行。 7. 支持的环境 # Flannel： 适合轻量级的 Kubernetes 集群，通常用于中小规模或对网络策略和性能要求较低的环境。 支持大部分的 CNI 插件和 Kubernetes 部署，适用于简化的网络需求。 Calico： 适合对网络性能、可扩展性和安全性有较高要求的企业级环境，尤其在需要高级网络策略和 IP 地址管理的场景中表现出色。 支持大型集群、大规模部署以及复杂的网络拓扑结构。 总结 # 特性 Flannel Calico 网络类型 Overlay 网络（VXLAN 或其它） 支持 Overlay 和路由模式（BGP） 网络策略 不支持（依赖 Kubernetes 原生 NetworkPolicy） 支持强大的网络策略（NetworkPolicy） 性能 性能较低，主要用于小规模环境 性能高，支持大规模和高吞吐量的集群 IP 地址管理 简单的 IP 地址分配和管理 灵活的 IP 地址管理，支持与外部 IPAM 集成 扩展性 扩展性较差，适用于轻量级需求 高度可扩展，适用于复杂和企业级环境 适用场景 小规模集群，低延迟应用，简化的网络需求 大规模集群，复杂的网络策略和安全需求 总结来说，如果你需要一个简单的网络插件来快速部署 Kubernetes 集群，Flannel 是一个不错的选择；而如果你的集群需要支持更高的网络性能、更复杂的网络策略或需要更强的扩展性，Calico 是更合适的选择。\ncalico网络原理、组网方式 # Calico 是一个为 Kubernetes 提供高效网络连接和安全策略的容器网络插件。它支持高性能的网络通信和复杂的网络策略，同时能与现有网络基础设施兼容，支持大规模的云原生应用。Calico 网络的原理和组网方式主要基于以下几个关键概念：\n1. 网络原理 # Calico 的核心原理基于 Layer 3 网络，并采用 路由模式（BGP）进行网络通信，而不是依赖于传统的 Overlay 网络（如 VXLAN）。这使得 Calico 在性能和可扩展性方面具有显著优势。它主要通过以下几个机制来提供高效的网络服务：\n1.1 IP-in-IP 或 VXLAN 隧道 # 在支持路由模式的情况下，Calico 允许容器通过物理网络直接通信，而不需要通过隧道。容器的 IP 地址直接映射到物理网络上的 IP 地址。 如果容器的网络无法直接在物理网络上路由（例如在某些隔离的环境中），Calico 会通过 IP-in-IP 隧道 或 VXLAN 隧道来封装网络流量。这样，容器仍然能够通过隧道来进行通信，确保网络的隔离和连接。 1.2 路由模式（BGP） # Calico 支持 BGP（边界网关协议），用于管理不同节点之间的路由信息。每个节点都会向其他节点通告其容器网络的路由信息，使用 BGP 协议动态交换路由表。 这种路由机制允许节点之间直接通信，避免了传统 Overlay 网络带来的性能开销，同时保持网络的灵活性和可扩展性。 1.3 IP 地址管理（IPAM） # Calico 提供了 IP 地址管理（IPAM），为每个 Pod 分配一个唯一的 IP 地址。它可以使用自动化的方式为每个 Pod 分配 IP 地址，也可以与外部 IPAM 系统集成（例如与云提供商的 IP 地址池进行集成）。 通过这种 IP 地址管理，Calico 可以确保 Pod 和服务在 Kubernetes 集群中拥有独立且唯一的 IP 地址。 2. Calico 的组网方式 # Calico 支持多种组网方式，根据网络拓扑和需求的不同，用户可以选择不同的部署方式。主要的组网方式包括：\n2.1 Layer 3 路由模式（无 Overlay） # 原理：在路由模式下，Calico 使用 BGP 动态地将每个节点的容器网络 IP（Pod 的 IP 地址）公布到整个集群中。容器直接在物理网络上进行通信，不需要任何隧道或封装。\n优点\n：\n性能更高：因为容器间的通信不需要通过隧道，而是直接通过物理网络路由。 低延迟：直接的 Layer 3 路由减少了网络开销。 可扩展性好：能够轻松支持大型集群，因为 BGP 可以自动适应网络的扩展。 使用场景：适合对性能有较高要求的大型集群，尤其是需要大规模、高吞吐量容器通信的环境。\n2.2 Overlay 网络模式（IP-in-IP 或 VXLAN） # 原理：如果物理网络无法直接支持容器间的直接路由，Calico 会使用 IP-in-IP 或 VXLAN 隧道技术来封装 Pod 的流量。通过隧道，Pod 可以像在 Overlay 网络上一样进行通信。\n优点\n：\n网络隔离：Overlay 网络可以在不同的数据中心或网络段之间提供容器隔离。 容易配置：适用于在不同物理网络环境下的 Kubernetes 集群，确保容器网络的可达性。 使用场景：适合在需要跨多个网络环境或云环境运行的 Kubernetes 集群，或者希望对网络流量进行封装和隔离的场景。\n2.3 Calico 与其他网络插件的结合 # Calico 可以与其他 CNI 插件（如 Flannel、Weave 等）结合使用，在这些插件的基础上提供更多的功能。例如，Calico 可以在 Flannel 或 Weave 网络基础上提供增强的网络策略功能，允许用户实现更细粒度的流量控制和安全策略。 2.4 与 Kubernetes 网络策略结合 # Calico 本身提供了强大的 网络策略（NetworkPolicy） 功能，用户可以基于标签、命名空间等条件控制容器之间的通信。网络策略允许控制 Pod 之间、Pod 和外部服务之间的流量，实现容器化应用的安全隔离。 3. Calico 的主要组件 # Calico 主要由以下几个组件组成：\nFelix：Calico 的核心代理组件，负责节点之间的路由更新、网络策略的执行、以及容器网络的管理。Felix 会在每个节点上运行，并与 Etcd 或其他存储后端进行交互，更新路由和网络策略。 BGP：使用 BGP 协议动态地交换节点之间的路由信息。每个节点上都会有一个 BGP 路由器，它将每个 Pod 的 IP 地址与节点的物理网络地址关联起来。 Calico CNI 插件：为 Kubernetes 提供容器网络功能，负责为每个 Pod 分配 IP 地址，并与 Etcd 进行交互来管理 IP 地址和路由。 Etcd：作为分布式的存储后端，用来存储 Calico 的配置信息、路由表、网络策略等数据。 API Server：通过 Calico 的 API，用户可以配置网络策略、查看网络状态等。它为管理和操作 Calico 提供了一个易于使用的接口。 4. Calico 的优势 # 高性能：通过直接的 Layer 3 路由和 BGP 协议，避免了 Overlay 网络的性能开销，提供了低延迟的网络连接。 灵活性：支持多种网络模型（如 IP-in-IP、VXLAN 和纯路由模式），可以根据不同的需求和网络环境选择适合的模式。 扩展性：Calico 设计上适合大规模集群，能够通过 BGP 动态交换路由，适应大规模和动态变化的集群环境。 网络安全：通过网络策略（NetworkPolicy），Calico 允许用户定义容器间的通信规则，确保集群的网络安全。 支持多种部署环境：Calico 支持在云环境、物理服务器、虚拟化环境以及裸机环境中运行，能够满足不同场景的需求。 总结 # Calico 是一个强大的网络插件，它基于 Layer 3 路由和 BGP 协议提供高效的容器网络解决方案，支持多种组网模式（如 Overlay 和路由模式），并提供高级的网络策略管理。无论是在需要高性能的容器通信环境，还是在需要灵活、安全的网络隔离和流量控制的场景中，Calico 都能够提供优异的支持。\nIP-in-IP (IPIP) 是 Calico 支持的一种 Overlay 网络模式，它为不同节点上的 Pod 提供网络隔离和跨节点通信。与 BGP 模式相比，IPIP 模式使用隧道技术封装 Pod 的 IP 数据包，使得容器在不同节点间的通信能够通过隧道进行，即使在物理网络上无法直接路由。\nIP-in-IP 模式的工作原理 # 在 IP-in-IP 模式下，Calico 为每个 Pod 分配一个独立的 IP 地址，这些地址属于一个大的网络范围。当 Pod 在不同节点之间进行通信时，Calico 会使用 IP-in-IP 隧道技术将流量从一个节点传输到另一个节点，具体的工作流程如下：\n隧道封装： 当一个 Pod 发送数据包到另一个节点的 Pod 时，源节点会将数据包封装在 IP-in-IP 隧道中。即，数据包的原始目的 IP 地址是目标 Pod 的 IP 地址，而源地址是源 Pod 的 IP 地址。 在封装的过程中，源节点会将原始数据包的内容保留在内层 IP 数据包中，外层则加上了源节点的 IP 地址和目标节点的 IP 地址。这种封装技术保证了数据包能够跨节点传输。 隧道解封装： 当数据包到达目标节点时，目标节点会根据外层的 IP 信息解封装数据包，并将内层的数据包转发到目标 Pod。 目标节点知道如何将内层数据包发送到正确的 Pod，因为它已经包含了目标 Pod 的 IP 地址。 与路由模式的区别： 在 IP-in-IP 模式下，容器的 IP 地址和网络流量被封装成 IP 数据包，通过隧道进行跨节点传输。每个节点上的 Calico 代理 (Felix) 会负责管理隧道的建立和流量的转发。 与之相对，BGP 路由模式则不使用隧道，而是直接通过物理网络路由 Pod 之间的流量，依赖于 BGP 路由协议来传递和更新路由信息。 IP-in-IP 模式的优缺点 # 优点： # 跨节点通信支持： IP-in-IP 模式能够实现跨节点和跨数据中心的 Pod 之间的通信，特别适用于网络拓扑较复杂或者无法直接通过物理网络路由 Pod 的环境。 简单易配置： 在使用 Calico 的 IP-in-IP 模式时，配置通常较为简单，不需要复杂的路由配置，适合在一些较简单的环境中快速部署。 隔离性： 由于每个节点之间的流量被封装在隧道中，它提供了较好的网络隔离性，避免了不同网络间的干扰。 缺点： # 性能开销： 隧道封装和解封装过程会带来额外的性能开销，特别是对于高吞吐量或低延迟要求的应用。封装后的数据包体积增大，网络带宽和 CPU 资源会有所消耗。 调试复杂性： 使用 Overlay 网络时，网络拓扑更加复杂，网络故障排查可能更困难。你需要对隧道的状态进行监控，以确保数据包能够正确地封装和解封装。 不如 BGP 路由模式高效： 相比于直接的路由模式（BGP），IP-in-IP 模式通过隧道传输流量，性能可能会受到影响，尤其是在大规模集群和对性能要求高的场景下。 使用场景 # 跨节点网络隔离：在不同节点或数据中心的 Pod 之间需要安全隔离的环境下，IP-in-IP 模式适合提供封装的网络连接。 不支持直接路由的网络环境：如果物理网络不支持跨节点直接路由，或者网络中存在复杂的防火墙和路由规则，IP-in-IP 模式能够通过隧道穿越这些限制，实现 Pod 间的通信。 配置 IP-in-IP 模式 # Calico 默认启用 IP-in-IP 模式，并通过 calicoctl 工具来进行相关配置。你可以在 Calico 配置文件 中设置是否启用 IP-in-IP 模式，或者在集群安装过程中选择使用此模式。\n例如，要启用或禁用 IP-in-IP，可以修改 Calico 配置中的 calico_backend 设置：\ncalicoctl config set ipip_enabled true 这会启动 IP-in-IP 模式，所有 Pod 之间的跨节点流量都会通过 IP-in-IP 隧道进行传输。\n总结 # IP-in-IP 模式 是 Calico 的一种 Overlay 网络模式，通过隧道技术封装 Pod 流量，使其能够跨节点进行通信，适用于需要网络隔离、跨网络拓扑的场景。 该模式适合较简单的部署场景，但可能会引入性能开销。 对于性能要求较高、网络拓扑支持直接路由的集群，BGP 路由模式可能是更合适的选择。 VXLAN (Virtual Extensible LAN) 是一种用于构建虚拟化数据中心的 Overlay 网络技术，可以解决传统 Layer 2 网络在大型、分布式环境中的扩展性问题。Calico 支持 VXLAN 模式作为其 Overlay 网络的一种形式，用于实现跨节点的容器网络连接。与 IP-in-IP 模式类似，VXLAN 也是一种封装技术，但其工作原理和优缺点略有不同。\nVXLAN 的工作原理 # 在 Calico 中使用 VXLAN 时，每个节点上的容器（Pod）都会分配一个 IP 地址，跨节点通信时，流量会通过 VXLAN 隧道进行封装和传输。具体过程如下：\n隧道封装： 当 Pod 发送数据包到另一个节点上的 Pod 时，源节点会将数据包封装在 VXLAN 隧道中。封装后的数据包会包含原始数据包的内容（Pod 的 IP 地址、数据等），并加上外层的 VXLAN 头，外层头包含源节点的 VXLAN 端口和目标节点的 VXLAN 端口信息。 这种封装方式使得数据包能够在物理网络上进行传输，即使在物理网络上没有直接的路由路径。 隧道解封装： 当数据包到达目标节点时，目标节点的 VXLAN 解封装器会去掉 VXLAN 头，并将原始数据包转发到目标 Pod。 这意味着，目标 Pod 收到的包看起来就像是来自直接网络，而不是来自远程节点。 VXLAN 网络标识符 (VNI)： VXLAN 使用 VNI（VXLAN Network Identifier） 来标识不同的虚拟网络。每个 VXLAN 隧道会有一个唯一的 VNI，确保不同虚拟网络之间的数据包不混淆。 对于 Calico 来说，通常每个 Kubernetes 集群都会使用一个 VXLAN 隧道标识符（VNI）来区分网络流量。 VXLAN 模式的优缺点 # 优点： # 网络隔离性强： VXLAN 提供了强大的网络隔离功能。每个 VXLAN 隧道都能将流量隔离在虚拟网络中，避免了不同网络之间的干扰。 跨数据中心支持： VXLAN 能够非常方便地在跨数据中心或跨地域的环境中实现容器网络的连接。在多个数据中心之间，VXLAN 可以有效地承载虚拟网络流量。 大规模支持： VXLAN 支持最大 16M 个虚拟网络（VNI），远远超出了 VLAN 的 4096 限制。这使得 VXLAN 适用于大规模的容器网络和虚拟化环境。 透明性： VXLAN 技术与现有的物理网络无关，物理网络不需要支持 VXLAN，便可以通过 Overlay 网络将虚拟网络连接在一起。VXLAN 是在数据链路层进行封装，物理网络的设备不需要进行特殊处理。 缺点： # 性能开销： 像 IP-in-IP 一样，VXLAN 也会带来封装和解封装的性能开销。封装会使得每个数据包的大小增加，从而带来额外的 CPU 和网络带宽消耗。 复杂性： 配置和管理 VXLAN 网络相对复杂，需要确保每个 VXLAN 隧道的配置正确，同时要管理虚拟网络标识符（VNI）。这可能对操作人员的技能要求更高。 调试难度： 由于数据包在通过物理网络传输时被封装，网络故障排查和调试可能变得更加困难。你需要监控 VXLAN 隧道的健康状况，并对封装数据包进行解包和分析。 广播流量： VXLAN 在某些情况下可能会引入广播流量的开销，尤其是在需要广播的场景（如 ARP、DNS 等）。这些流量需要在 VXLAN 网络中传播，增加了网络负担。 VXLAN 与 IP-in-IP 的对比 # 封装方式： IP-in-IP：仅使用 IP 封装数据包，封装的开销较小。 VXLAN：使用 VXLAN 头和 UDP 封装，封装开销较大，因为每个数据包都需要添加额外的 VXLAN 和 UDP 头。 可扩展性： VXLAN：支持最多 16M 个虚拟网络（VNI），适合大规模集群和大规模虚拟网络。 IP-in-IP：相较之下，支持的虚拟网络数量较少，适用于较小的集群或环境。 跨数据中心支持： VXLAN：具有较强的跨数据中心支持能力，适合于多数据中心部署。 IP-in-IP：可以支持跨数据中心，但由于 VXLAN 的设计，它通常在大规模、跨数据中心环境中表现更好。 使用场景 # 跨数据中心和多地域部署： VXLAN 是支持跨多个数据中心或不同网络环境的容器网络的理想选择。由于其较强的可扩展性和支持大量虚拟网络，适合大规模容器化应用和多云环境。 大规模容器集群： 对于需要支持数千甚至数万个容器的大规模 Kubernetes 集群，VXLAN 提供了更好的隔离性和可扩展性。 有高度网络隔离需求的环境： VXLAN 能够提供强大的网络隔离能力，确保不同的应用、环境或租户之间的流量完全隔离，适合多租户环境和安全性要求较高的场景。 配置 VXLAN 模式 # Calico 的 VXLAN 模式通常通过修改配置来启用。通过修改 calicoctl 工具中的设置，或者在 Kubernetes 部署时选择启用 VXLAN。\n例如，要启用 VXLAN 模式，可以在 Calico 配置文件中进行以下设置：\ncalicoctl config set vxlan_enabled true 这会启用 VXLAN 模式，Pod 之间的跨节点流量将通过 VXLAN 隧道进行封装和传输。\n总结 # VXLAN 是一种用于在分布式数据中心中实现 Overlay 网络的技术，能够提供网络隔离、跨数据中心连接和大规模虚拟网络的支持。 Calico 的 VXLAN 模式允许通过 VXLAN 隧道来实现跨节点、跨数据中心的 Pod 之间的通信，适用于需要高可扩展性和高隔离性的场景。 与 IP-in-IP 相比，VXLAN 提供更好的扩展性和支持更多虚拟网络，但也带来了更大的性能开销和更复杂的配置管理。 Network Policy使用场景 # Network Policy 是 Kubernetes 中的一种功能，允许你定义和控制 Pod 之间以及 Pod 和外部世界之间的网络通信规则。它通过定义访问控制规则来限制哪些流量可以进入或离开 Pod，从而提高了 Kubernetes 集群的安全性和可控性。\nNetwork Policy 的使用场景 # 1. 多租户环境 # 在 Kubernetes 集群中，多个团队或租户可能共享同一个集群资源。在这种环境下，网络策略可以帮助确保不同租户的资源相互隔离，防止一个租户的应用程序与另一个租户的应用程序进行不必要的通信。 示例：你可以创建一个网络策略来禁止不同命名空间中的 Pod 之间的通信，只允许它们访问共享的服务，如数据库、API 网关等。 2. 提高安全性 # 网络策略允许你定义细粒度的安全规则，限制 Pod 之间的通信。通过阻止不必要的流量，减少了攻击面，降低了潜在的安全风险。 示例：只允许来自特定命名空间或标签的流量访问某个敏感的服务或数据库，从而确保即使某些 Pod 被攻破，攻击者无法轻易访问其他关键服务。 3. 限制 Pod 外部访问 # 在某些场景下，可能希望限制某些 Pod 只能在集群内部通信，而不允许它们访问外部网络或互联网。这有助于防止不必要的外部依赖或数据泄露。 示例：限制一个数据库 Pod 只允许访问内部流量，而不允许从外部网络发起连接。 4. 微服务架构中的通信控制 # 在微服务架构中，各个服务（Pod）通常需要进行通信。通过 Network Policy，可以确保不同服务之间的通信是有规则的，避免不必要的服务之间的流量。 示例：你可以限制微服务之间的流量，使得只有认证过的服务（如 API 网关）可以访问后台数据库服务，而其他服务无法直接访问数据库。 5. 网络隔离与分段 # Kubernetes 中的 Network Policy 可用于将网络流量隔离到不同的安全区域或服务段。这对于不同网络段、不同权限级别的服务进行隔离是非常有用的。 示例：在同一个集群中，前端应用（如 Web 服务器）可以通过网络策略与后端数据库进行通信，而与其他服务（如日志收集或监控服务）之间的流量则被限制。 6. 控制服务间的流量 # 在微服务架构中，服务之间可能有多个交互方式，例如 REST、gRPC 或消息队列等。通过 Network Policy，可以明确规定每种通信方式的流量来源和目标，从而提升系统的安全性和可维护性。 示例：控制 Web 服务与后端应用程序之间的通信，只允许 Web 服务访问后端数据库的某些端点，而禁止访问其他敏感服务。 7. 限制暴露到外部的服务 # 对于某些服务，可能需要限制其在内部通信的同时，避免暴露到集群外部。Network Policy 使得仅特定的 Pod 或 IP 地址能够访问这些服务，增强了集群的边界安全性。 示例：你可以创建一个策略，禁止数据库 Pod 从外部直接访问，或者只允许从特定的客户端应用程序访问。 8. 防止恶意流量 # 通过 Network Policy，可以阻止恶意流量进入集群。例如，可以防止不受信任的外部流量进入集群，避免不必要的暴露。 示例：如果你希望限制只有特定的 IP 地址段能够访问服务，可以通过网络策略阻止其他不信任的流量。 示例：定义 Network Policy # 以下是一个简单的示例，定义一个只允许特定命名空间内的 Pod 之间通信的网络策略：\napiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-same-namespace spec: podSelector: {} ingress: - from: - podSelector: {} namespaceSelector: matchLabels: name: mynamespace policyTypes: - Ingress 在这个示例中：\npodSelector：空值表示对所有 Pod 都应用该策略。 ingress：规则表明只有来自 mynamespace 命名空间中的 Pod 才能访问这些 Pod。 policyTypes：表示这条策略是对 Ingress 流量进行控制。 网络策略的限制和注意事项 # 默认拒绝所有流量： 如果没有定义任何网络策略，Kubernetes 集群中的 Pod 默认可以自由通信。然而，一旦定义了网络策略，Kubernetes 会应用 \u0026ldquo;默认拒绝\u0026rdquo; 规则，即默认会阻止所有流量，除非明确允许。 只支持支持 Network Policy 的 CNI 插件： 不是所有 CNI 插件都支持 Kubernetes 的 Network Policy。需要使用支持 Network Policy 的插件，如 Calico、Cilium、Weave 等。 只能控制 Pod 之间的流量： 网络策略只会影响 Pod 之间的流量。它不能直接控制集群外部访问（如 LoadBalancer 服务暴露的端口）。因此，外部流量的控制需要结合 Kubernetes 的 Service 和防火墙规则。 Ingress 和 Egress 流量： Network Policy 目前主要支持控制 Ingress（入站流量）和 Egress（出站流量）。你可以通过定义不同的规则来控制流量的进入和离开 Pod。 总结 # Network Policy 提供了一种强大的方式来控制 Kubernetes 集群内 Pod 之间的通信，特别是在多租户环境、微服务架构以及对安全性要求较高的场景中非常有用。它能够帮助你：\n实现网络隔离和分段； 提高集群的安全性； 细粒度地控制服务间的流量； 防止恶意流量进入集群。 为了确保网络安全性，合理设计和实施 Network Policy 是 Kubernetes 安全架构中的重要一环。\nkubectl exec 实现的原理 # kubectl exec 是一个常用的命令，用于在 Kubernetes 集群中的 Pod 内执行命令。它允许用户直接在容器内执行交互式命令或获取容器的输出信息。理解 kubectl exec 的原理涉及 Kubernetes 中 Pod 和容器的管理机制，尤其是如何通过 API Server 与容器进行通信。\nkubectl exec 原理概述 # 命令发起和处理： 当用户在终端中运行 kubectl exec 命令时，kubectl 客户端会构造一个请求，发送到 Kubernetes 集群的 API Server。 API Server 会验证请求的合法性（例如，用户是否具有执行命令的权限，Pod 是否存在，容器是否运行等），并且检查请求中指定的 Pod 和容器是否匹配。 通过 API Server 访问容器： API Server 接收到 kubectl exec 请求后，它会调用容器运行时接口（如 Docker、containerd 等）来执行相应的命令。 Kubernetes 并不会直接执行容器中的命令，而是通过 API Server 中的 PodProxy 或 ContainerRuntimeInterface (CRI) 与容器运行时进行交互。 PodProxy 是 Kubernetes 用来与容器交互的组件，它可以通过 HTTP 和 WebSocket 协议向容器发送指令。 WebSocket 或 HTTP 连接： kubectl exec 使用 WebSocket 协议与容器建立一个双向通信通道。这使得客户端（kubectl）与容器之间可以进行实时交互。 如果执行的命令是交互式的（如启动一个 shell），则 kubectl 会保持与容器的 WebSocket 连接，用于接收和发送数据。 如果是非交互式命令（例如 kubectl exec pod -- ls），则命令会在容器内执行并返回输出，执行完毕后连接断开。 容器执行命令： 在 WebSocket 连接建立之后，API Server 会通过容器运行时启动指定的命令。容器运行时负责在 Pod 的容器内执行该命令，并将命令的标准输出（stdout）和标准错误（stderr）发送回 WebSocket 通道。 容器的标准输入（stdin）也通过 WebSocket 连接传递，允许用户与容器进行交互。 客户端与容器交互： 对于交互式的命令，kubectl 客户端将会将用户的输入通过 WebSocket 连接发送到容器，同时接收容器的输出并显示在终端中。 对于非交互式命令，kubectl exec 会等待命令执行完成，并输出执行结果。 WebSocket 通信的终止： 当命令执行完毕，WebSocket 连接关闭，kubectl 会将容器的退出码返回给用户。 这时，kubectl 命令会终止并返回命令执行的结果或错误。 kubectl exec 的组件与交互 # kubectl 客户端： 发起 exec 请求并处理用户输入/输出，管理与 API Server 的连接。 API Server： 处理请求并进行认证、授权检查，然后通过 PodProxy 或 CRI 调用容器运行时来执行命令。 PodProxy： Kubernetes 内部组件，负责将请求转发到正确的容器运行时，并与容器进行通信。 容器运行时（如 Docker、containerd 等）： 执行命令，返回命令的输出并将其传递给 API Server。 WebSocket 连接： 用于实现客户端和容器之间的双向实时通信，特别适用于交互式命令。 kubectl exec 使用示例 # 1. 执行交互式命令（如进入容器内的 Shell）： # kubectl exec -it \u0026lt;pod-name\u0026gt; -- /bin/bash -it 选项表示启用交互式终端，/bin/bash 是容器内的命令，用户可以与容器内的 shell 进行交互。 2. 执行非交互式命令： # kubectl exec \u0026lt;pod-name\u0026gt; -- ls /app 这个命令会在 Pod 内执行 ls /app，并将输出返回给用户。 3. 执行命令并获取输出： # kubectl exec \u0026lt;pod-name\u0026gt; -- cat /etc/hostname 该命令会在容器内执行 cat 命令，并将结果（容器的主机名）返回给用户。 安全与权限控制 # Kubernetes 使用 RBAC（角色基础访问控制）来控制谁可以执行 kubectl exec。如果用户没有相应的权限，执行 exec 操作将被拒绝。\n通常，RBAC 会为不同的角色定义权限，例如：\nkind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: namespace: default name: exec-role rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;pods/exec\u0026#34;] verbs: [\u0026#34;create\u0026#34;] 在这个例子中，create 动作允许用户在 Pod 中执行命令。\n总结 # kubectl exec 的实现原理涉及以下几个步骤：\n客户端发起请求，并通过 API Server 与 Kubernetes 集群通信。 API Server 进行认证和授权检查，并通过 PodProxy 或容器运行时接口（CRI）与容器进行交互。 WebSocket 连接 用于容器与客户端之间的双向实时通信。 容器执行命令，并将标准输出、标准错误返回给客户端。 通过这些机制，kubectl exec 提供了在 Kubernetes 集群中灵活执行命令和与容器进行交互的功能。\ncgroup中限制CPU的方式有哪些 # 在 cgroup（控制组）中，限制 CPU 资源的方式主要有以下几种。这些方式帮助管理员在系统中对进程或容器的 CPU 资源进行细粒度的管理和控制，确保在多任务和多容器环境中，系统资源能够公平地分配，避免某些进程或容器占用过多的 CPU。\n1. CPU 时间限制（cpu.cfs_quota_us） # 描述：cpu.cfs_quota_us 用于限制某个 cgroup 在指定时间周期内（通过 cpu.cfs_period_us 设置）可以使用的最大 CPU 时间。 原理：系统每隔一段时间（cpu.cfs_period_us，默认 100ms）会重置并统计每个 cgroup 使用的 CPU 时间。cpu.cfs_quota_us 设置了一个限制值，表示该 cgroup 在一个周期内可以使用的最大 CPU 时间。如果使用时间超过这个值，其他任务将会被暂停，直到下一个周期开始。 使用示例：\n# 限制进程在 100ms 的时间周期内最多只能使用 50ms 的 CPU 时间 echo 50000 \u0026gt; /sys/fs/cgroup/cpu/cpu.cfs_quota_us echo 100000 \u0026gt; /sys/fs/cgroup/cpu/cpu.cfs_period_us 默认值：cpu.cfs_quota_us 默认为 -1，表示没有限制。cpu.cfs_period_us 默认为 100000（100ms）。 作用：如果你设置了 cpu.cfs_quota_us = 50000，那么在每个周期（100ms）中，该 cgroup 内的进程最多只能使用 50ms 的 CPU 时间。 2. CPU 核心限制（cpu.cpus） # 描述：通过设置 cpu.cpus，可以限制 cgroup 内的进程只能在指定的 CPU 核心上运行。 原理：此限制可以将进程固定在指定的 CPU 核心上执行，避免跨多个 CPU 核心进行调度，从而达到更精确的资源控制。 使用示例：\n# 将进程限制在 CPU 核心 0 和 1 上运行 echo 0,1 \u0026gt; /sys/fs/cgroup/cpu/cpu.cpus 作用：通过设置 cpu.cpus = 0,1，该 cgroup 中的进程只能在 CPU 核心 0 和 1 上执行，避免了进程在其他核心上调度，优化了性能。 3. CPU 权重（cpu.shares） # 描述：cpu.shares 用于定义进程/容器相对于其他进程/容器的 CPU 权重。它不会限制 CPU 使用的时间或数量，而是通过控制进程的 CPU 分配比例来实现资源调度。 原理：cpu.shares 是一个相对的值，用来表示该 cgroup 在没有 CPU 限制时的调度优先级。它与其他 cgroup 中的进程竞争 CPU 时间时，系统会按照比例进行分配。例如，如果两个 cgroup 的 cpu.shares 分别为 512 和 1024，那么在 CPU 资源不足时，第二个 cgroup 会获得两倍于第一个 cgroup 的 CPU 时间。 使用示例：\n# 设置进程的 CPU 权重为 512（默认值为 1024） echo 512 \u0026gt; /sys/fs/cgroup/cpu/cpu.shares 默认值：cpu.shares 默认为 1024。 作用：如果两个进程的 cpu.shares 分别为 1024 和 512，则第一个进程的 CPU 时间是第二个进程的两倍。 4. CPU 限制优先级（cpu.rt_runtime_us） # 描述：cpu.rt_runtime_us 用于限制实时进程（例如低延迟要求的进程）的最大运行时间。通常用于实时调度。 原理：实时进程通常需要比普通进程更精确的 CPU 调度。通过设置 cpu.rt_runtime_us，可以限制该 cgroup 中的实时进程最多可使用的 CPU 时间（单位是微秒）。如果该进程在一个周期内使用的时间超过这个值，调度器会限制进程的运行直到下一个周期。 使用示例：\n# 设置实时进程最大可用的 CPU 时间为 95000 微秒 echo 95000 \u0026gt; /sys/fs/cgroup/cpu/cpu.rt_runtime_us 默认值：cpu.rt_runtime_us 默认为 950000（即 950ms）。 5. CPU 子系统的 CFS（Completely Fair Scheduler）策略 # 描述：CFS 是 Linux 内核的调度策略，它会根据任务的优先级、资源需求等进行公平调度。通过 cgroup 控制 CFS 的调度策略，可以控制进程/容器的 CPU 使用情况。 控制项： cpu.cfs_period_us：控制 CPU 时间片的周期（单位：微秒）。 cpu.cfs_quota_us：控制进程可用的最大 CPU 时间。 使用场景：\n适用于需要对 CPU 使用进行精细化控制的场景，如容器化环境中，防止单个容器占用过多的 CPU 资源。 6. CPU 控制组调度器（cpuset） # 描述：cpuset 是 cgroup 的一个子系统，它允许管理员将进程限制在特定的 CPU 核心上运行，同时也可以限制内存的使用。与 cpu.cpus 类似，cpuset 控制进程只能在特定的 CPU 核心上调度执行。 使用示例：\n# 设置容器只能使用 CPU 核心 0 和 1 echo 0,1 \u0026gt; /sys/fs/cgroup/cpuset/cpuset.cpus 作用：通过 cpuset 限制 CPU 核心，防止进程在多个 CPU 核心之间迁移，从而实现更高效的资源使用。 总结 # 通过 cgroup，用户可以从多个角度对进程或容器的 CPU 使用进行限制和优化，常见的方式包括：\nCPU 时间限制（cpu.cfs_quota_us 和 cpu.cfs_period_us）。 CPU 核心绑定（cpu.cpus）。 CPU 权重（cpu.shares）。 实时 CPU 限制（cpu.rt_runtime_us）。 控制进程在指定 CPU 上运行（cpuset）。 这些方式可以帮助在多任务、多容器环境下合理分配 CPU 资源，提高集群的整体性能和稳定性。\nkubeconfig存放内容 # kubeconfig 文件是 Kubernetes 配置文件，存储了与 Kubernetes 集群进行交互所需要的所有信息。kubeconfig 文件的内容包括了集群信息、认证信息和上下文信息，允许 kubectl 等工具与 Kubernetes API Server 进行通信。一个典型的 kubeconfig 文件可以包含多个集群的配置，以及如何连接到每个集群的详细信息。\nkubeconfig 文件的结构 # kubeconfig 文件通常是一个 YAML 格式的文件，包含以下几部分：\napiVersion：文件的 API 版本，通常是 v1。 clusters：描述 Kubernetes 集群的信息。每个集群信息包含集群的名称、API Server 地址以及（可选的）TLS 配置。 contexts：定义不同的上下文，每个上下文指定了用户、集群和命名空间的组合。 current-context：当前使用的上下文，指定默认连接的集群、用户等。 users：存储与 Kubernetes 集群通信所需要的认证信息，通常包括证书、令牌或用户名密码等。 preferences：一些用户的个人偏好配置（通常不常用）。 kubeconfig 文件示例 # apiVersion: v1 kind: Config clusters: - name: my-cluster cluster: server: https://k8s-api-server.example.com:6443 certificate-authority-data: \u0026lt;certificate-data\u0026gt; # 或者使用 certificate-authority users: - name: my-user user: client-certificate-data: \u0026lt;client-cert-data\u0026gt; client-key-data: \u0026lt;client-key-data\u0026gt; token: \u0026lt;token\u0026gt; # 如果使用令牌认证 contexts: - name: my-context context: cluster: my-cluster user: my-user namespace: default # 可选 current-context: my-context preferences: {} kubeconfig 文件的关键字段 # 1. clusters # name：集群的名称，可以是任意名称，通常与集群名称对应。 cluster.server：API Server 的地址，通常是 HTTPS 协议。它是 Kubernetes 集群的入口点，kubectl 会连接到这个地址。 cluster.certificate-authority-data：用于验证 API Server 的证书的证书数据（Base64 编码）。可以替代 certificate-authority 字段，后者是一个指向证书文件的路径。 2. users # name：用户的名称，可以是任意名称，通常与认证方式（如用户名、证书或令牌）对应。 user.client-certificate-data：Base64 编码的客户端证书数据，用于通过客户端证书进行认证。 user.client-key-data：Base64 编码的客户端密钥数据，配合客户端证书使用。 user.token：如果使用 Bearer Token 认证，存储在这里。通常用于基于令牌的认证。 user.username 和 user.password：如果使用 HTTP 基本认证，用户名和密码可以在这里配置。 3. contexts # name：上下文的名称，可以根据使用的集群和用户来命名。 context.cluster：指定使用的集群名称，引用 clusters 部分中的集群。 context.user：指定使用的用户名称，引用 users 部分中的用户。 context.namespace：指定默认的命名空间。可选，通常是 default。 4. current-context # 这个字段指定当前要使用的上下文名称。kubectl 将根据这个上下文连接到指定的集群并使用相应的认证信息。 5. preferences # 一些用户偏好配置，通常为空。 示例解析 # apiVersion: v1 kind: Config clusters: - name: prod-cluster cluster: server: https://prod-api-server.example.com:6443 certificate-authority-data: \u0026lt;prod-cert-data\u0026gt; users: - name: prod-user user: client-certificate-data: \u0026lt;prod-cert-data\u0026gt; client-key-data: \u0026lt;prod-key-data\u0026gt; contexts: - name: prod-context context: cluster: prod-cluster user: prod-user namespace: prod-namespace current-context: prod-context clusters 部分指定了 prod-cluster 集群的信息，API Server 地址是 https://prod-api-server.example.com:6443。 users 部分定义了 prod-user 用户的认证信息，使用客户端证书进行身份验证。 contexts 部分定义了一个名为 prod-context 的上下文，指定了使用 prod-cluster 集群和 prod-user 用户，且默认命名空间为 prod-namespace。 current-context 设置为 prod-context，表示 kubectl 将使用这个上下文连接到集群。 存放位置 # Linux/MacOS：~/.kube/config Windows：%USERPROFILE%\\.kube\\config 使用 kubectl 选择不同的 kubeconfig 文件 # 可以通过 --kubeconfig 参数来指定使用不同的 kubeconfig 文件：\nkubectl --kubeconfig=/path/to/your/kubeconfig get pods 如果没有指定 --kubeconfig 参数，kubectl 默认会使用 ~/.kube/config（Linux 和 macOS）或 %USERPROFILE%\\.kube\\config（Windows）路径下的文件。\n小结 # kubeconfig 文件是 Kubernetes 用来配置集群访问、认证和上下文切换的核心文件。它通过指定多个集群、用户和上下文的配置，帮助用户方便地与多个集群进行交互。\npod DNS解析流程☆ # 在 Kubernetes 中，Pod DNS 解析是一个重要的功能，它允许 Pod 在集群内部通过服务名进行通信，而不需要使用 IP 地址。Kubernetes 使用内建的 DNS 服务（通常是 kube-dns 或 CoreDNS）来处理域名解析请求。以下是 Pod DNS 解析的流程：\n1. DNS 配置 # 每个 Pod 默认会使用 Kubernetes 集群内部的 DNS 服务器进行域名解析。这个 DNS 服务器通常是 kube-dns 或 CoreDNS，其服务运行在集群内部。\nDNS 服务的地址通常为 kube-dns.kube-system.svc.cluster.local（或者 CoreDNS 的地址 coredns.kube-system.svc.cluster.local）。 在每个 Pod 中，DNS 服务的 IP 地址通常会被配置到 /etc/resolv.conf 文件中，Pod 会通过该配置进行 DNS 查询。 2. DNS 查询流程 # 当一个 Pod 想要解析一个域名（如 myservice.default.svc.cluster.local）时，它会通过 /etc/resolv.conf 配置中指定的 DNS 服务器进行查询。整个解析过程如下：\na. 查询初始化 # Pod 中的应用程序发起 DNS 请求，查询域名。 DNS 请求会被发送到 Pod 中配置的 DNS 服务器（通常是 Kubernetes 集群中的 CoreDNS 或 kube-dns）。 b. DNS 请求到达 DNS 服务 # DNS 请求到达 Kubernetes 集群中的 DNS 服务（如 CoreDNS 或 kube-dns）。 DNS 服务根据请求的域名查找其对应的记录。 c. 服务发现 # 服务解析：如果请求的域名是一个 Kubernetes 服务（如 myservice.default.svc.cluster.local），DNS 服务会根据服务的名称、命名空间和类型查找该服务的相关信息（如 Pod 的 IP 地址或服务的负载均衡器 IP）。 Pod 名称解析：如果请求的域名是一个特定 Pod 的名称（如 mypod.default.svc.cluster.local），DNS 服务会返回该 Pod 的 IP 地址。 d. 返回结果 # DNS 服务将解析结果（如 IP 地址）返回给发起请求的 Pod。 Pod 中的应用程序接收到解析结果，并使用该 IP 地址与目标 Pod 或服务进行通信。 3. DNS 解析示例 # 假设有以下 Kubernetes 服务和 Pod 配置：\n服务名：myservice 命名空间：default 服务类型：ClusterIP 一个 Pod 想要访问 myservice 服务，DNS 查询将发起如下请求：\nmyservice.default.svc.cluster.local DNS 服务（CoreDNS 或 kube-dns）会查询该服务，并返回与 myservice 相关的 Pod 的 IP 地址。\nPod 收到返回的 IP 地址后，可以直接通过该 IP 与服务进行通信。\n4. DNS 解析规则 # Service 名称解析： Kubernetes 中的服务名称解析是通过 service.namespace.svc.cluster.local 格式进行的。例如，myservice.default.svc.cluster.local 会解析到 default 命名空间中的 myservice 服务。 Pod 名称解析： Pod 也可以通过 \u0026lt;pod-name\u0026gt;.\u0026lt;namespace\u0026gt;.pod.cluster.local 进行解析。例如，mypod.default.pod.cluster.local 会解析到 default 命名空间中的 mypod Pod。 域名后缀： Kubernetes DNS 的域名后缀通常是： svc.cluster.local：服务的完整域名。 pod.cluster.local：Pod 的完整域名。 namespace.svc.cluster.local：命名空间内的服务名。 DNS 缓存：Pod 中的 DNS 解析通常会有缓存，避免每次都进行查询，但缓存时间有限，通常是 5 分钟。 5. DNS 服务的工作原理 # CoreDNS / kube-dns：Kubernetes 使用 CoreDNS 或 kube-dns 来提供集群内部的 DNS 服务。它们会根据配置解析服务名、Pod 名称等。 Service 和 Endpoints：每个 Kubernetes 服务都有一个对应的 Endpoints 资源，表示该服务背后的 Pod IP 地址。DNS 服务会查找 Endpoints 并返回 Pod 的 IP 地址。 Pod DNS 配置：Kubernetes 会通过环境变量和 resolv.conf 配置将 DNS 服务信息传递给 Pod，以确保它能够正确地查询域名。 6. DNS 解析优化与故障排查 # CoreDNS 配置：如果需要调整 DNS 解析的行为，可以通过配置 CoreDNS 插件来修改查询规则或增强性能。\nDNS 故障排查\n：如果 Pod 无法解析域名，可能的原因包括 DNS 配置问题、\nCoreDNS 或\nkube-dns 服务故障、网络问题等。可以通过以下方式排查：\n查看 Pod 中的 /etc/resolv.conf 配置，确认 DNS 服务器是否正确。 检查 CoreDNS 或 kube-dns Pod 的状态，查看是否存在异常。 使用 nslookup 或 dig 命令从 Pod 内部测试 DNS 解析。 7. DNS 解析的常见问题 # DNS 超时：如果 DNS 查询超时，通常是因为 DNS 服务（CoreDNS 或 kube-dns）不可用或网络存在问题。 DNS 名称解析失败：可能是由于没有正确的 Endpoints，或服务未正确创建，导致无法解析服务名。 Pod 之间的 DNS 问题：可能是由于 Pod 的 DNS 配置不正确或网络策略（Network Policy）阻止了 DNS 请求。 总结 # Kubernetes 中的 DNS 解析依赖于 CoreDNS 或 kube-dns 服务，它能够将集群内的服务名、Pod 名称等解析为相应的 IP 地址，使得 Pod 之间可以通过 DNS 进行通信。DNS 解析的主要流程是通过 CoreDNS 解析服务名、Pod 名称等，并返回相应的 IP 地址供 Pod 使用。通过合理的配置和排查，可以确保 Kubernetes 集群内 DNS 解析正常工作。\ntraefik对比nginx ingress优点 # Traefik 和 Nginx Ingress Controller 都是 Kubernetes 中常用的反向代理和负载均衡解决方案。虽然两者都可以作为 Ingress Controller，但它们有一些不同的设计理念、功能特点和优缺点。下面是对比这两者的一些优点：\n1. Traefik 的优点 # a. 原生支持动态配置 # Traefik 提供了强大的动态配置能力，它会实时监控 Kubernetes 集群中的资源（如 Ingress、Service、Pod 等）并自动更新路由配置，而不需要手动重载或重新启动服务。Traefik 可以通过监控 Kubernetes API 自动发现并动态更新路由规则，具有更好的灵活性。 Nginx 也支持动态配置，但它的配置文件通常需要手动更新，并且需要重新加载 Nginx 才能应用新配置。 b. 更易于配置 # Traefik 提供了基于声明式配置的简化操作方式，且其配置文件和文档简单易懂，用户可以更快速地部署和配置。它可以通过 YAML 配置文件直接与 Kubernetes 集成。 Nginx 的配置可能稍显复杂，尤其是在处理较复杂的负载均衡、反向代理等场景时，通常需要更多的细致配置。 c. 内建支持多种负载均衡算法 # Traefik 支持多种负载均衡策略，如轮询、加权轮询、最少连接等，可以根据需求轻松切换和调整负载均衡策略。 Nginx 也支持负载均衡，但 Traefik 的策略更为灵活，且配置更为简便。 d. 集成 Let’s Encrypt 支持 # Traefik 内建对 Let’s Encrypt 的自动支持，能够自动获取和续期 SSL/TLS 证书。只需要在 Traefik 配置中开启自动 SSL 支持，Traefik 就会为你的服务自动生成并管理 SSL 证书。 Nginx 需要额外配置脚本或手动集成 Let’s Encrypt，虽然有相关工具，但相比 Traefik 更为复杂。 e. 更好的微服务架构支持 # Traefik 是专为现代微服务架构设计的，它支持 Kubernetes、Docker 和其他容器化环境的集成。Traefik 可以根据容器的标签、标签集等信息动态地调整流量路由，适合高动态环境。 Nginx 主要是传统的 HTTP 服务器，虽然也支持 Kubernetes，但在容器化和微服务环境中的灵活性和自动化程度上不如 Traefik。 f. 内建支持 WebSocket 和 HTTP/2 # Traefik 原生支持 WebSocket 和 HTTP/2，且配置非常简单。 Nginx 也支持这些协议，但相对来说配置复杂，且需要额外的模块支持。 g. UI 控制面板 # Traefik 提供了一个方便的 Web UI，通过该界面可以监控 Traefik 的健康状况、查看路由配置和访问日志等。这对于运维和开发人员调试非常方便。 Nginx 没有内建 UI，虽然可以通过第三方工具（如 Nginx Amplify）进行监控，但并不如 Traefik 自带的 UI 那么方便。 h. 支持多种后端 # Traefik 可以同时作为反向代理支持 Kubernetes、Docker、Mesos、Consul 等多种后端架构，这使得它在多种环境下的可移植性和扩展性更强。 Nginx 主要是面向传统的 Web 服务器/反向代理需求，尽管它也支持 Kubernetes，但支持的后端系统不如 Traefik 广泛。 2. Nginx Ingress Controller 的优点 # 虽然 Traefik 有很多优点，但 Nginx Ingress Controller 也有一些优势，尤其在某些企业环境中，Nginx 的成熟和稳定性使它成为很多用户的选择。以下是 Nginx 的一些优点：\na. 性能稳定 # Nginx 是经过多年稳定测试的高性能 Web 服务器，处理静态内容和反向代理流量的性能非常强大。在面对高并发流量时，Nginx 可以提供非常高的吞吐量。 Traefik 也有很好的性能，但在某些极端高并发的情况下，Nginx 可能会有更优的表现。 b. 丰富的功能和扩展性 # Nginx 拥有非常丰富的功能支持，例如复杂的负载均衡、内容缓存、请求重写、SSL/TLS 优化等。对于一些复杂的 Web 应用场景，Nginx 提供了更加细致的控制和扩展。 Traefik 主要针对微服务架构，虽然也提供了许多负载均衡功能，但在细粒度的流量控制和复杂的 HTTP 配置方面，Nginx 更加灵活和强大。 c. 社区支持和文档 # Nginx 具有庞大的社区和成熟的文档资源，广泛应用于全球许多企业的生产环境，因此拥有大量的解决方案和最佳实践。 Traefik 的社区支持也在快速增长，但与 Nginx 相比，其历史稍短，且某些高级特性可能需要更多的社区贡献和测试。 d. 安全性 # Nginx 在安全性方面非常成熟，拥有强大的认证和防火墙功能，广泛用于企业级 Web 应用中。 Traefik 也非常注重安全性，但在某些特性上可能没有 Nginx 那么多的配置选项和细节。 e. 企业级支持 # Nginx 提供了企业级的支持和付费版本，能够满足一些大型企业对稳定性和高级功能的需求（如 WAF、防火墙、负载均衡、流量分析等）。 Traefik 主要是开源项目，虽然也有商业支持（Traefik Labs 提供的 Traefik Enterprise），但相比之下 Nginx 在企业级市场上占有更大的份额。 总结： # 特性 Traefik Nginx Ingress Controller 配置灵活性 动态配置，无需重载，简化配置 配置较复杂，通常需要手动更新并重载 自动 SSL/TLS 内建自动 Let’s Encrypt 支持 需要额外配置支持 微服务架构支持 强大的微服务支持，自动化与容器化集成 支持较好，但配置和灵活性较低 UI 支持 原生支持 Web UI 无原生 Web UI，需第三方工具 性能 对高并发有良好支持，但 Nginx 更优 极高的性能和吞吐量，适合高并发 后端支持 支持多种后端（Kubernetes、Docker 等） 主要支持 Kubernetes 社区与文档 新兴社区，快速成长 成熟社区和文档资源丰富 选择哪种 Ingress Controller 取决于您的需求：\n如果您的 Kubernetes 环境是高度动态的微服务架构，并且需要快速的配置和自动化支持，Traefik 可能更适合。 如果您需要稳定、强大且性能优化的反向代理服务，特别是在高负载、高并发的环境中，且需要丰富的自定义配置和更长的历史支持，Nginx Ingress Controller 会是更好的选择。 Harbor有哪些组件 # Harbor 是一个开源的企业级 Docker 镜像仓库，支持 Helm chart 存储和管理，具有图形化管理界面和多种安全功能。它的组件设计旨在提供完整的镜像管理服务，确保镜像存储的安全性、可管理性和可靠性。下面是 Harbor 的主要组件：\n1. Core (核心服务) # 核心服务 是 Harbor 的核心功能部分，负责用户的认证与授权、镜像的管理、任务调度、系统配置和日志管理等。 2. Registry (镜像仓库) # Registry 是 Harbor 的核心组成部分，提供镜像的存储、查询、推送和拉取等基本功能。它是基于 Docker Registry 实现的，并对其进行了扩展，支持更多的企业级特性。 Harbor 使用 Notary 来为存储的镜像提供签名功能，确保镜像的安全性。 3. Portal (Web UI) # Portal 是 Harbor 的图形化管理界面，提供用户友好的界面来管理镜像仓库、用户、项目、权限等。通过 UI，用户可以进行镜像上传、删除、查看、拉取等操作，同时可以监控镜像的使用情况和安全性。 4. Jobservice (任务服务) # Jobservice 是 Harbor 用于处理后台任务的组件。它负责执行异步任务，如镜像扫描、镜像清理等任务。 它通过任务队列异步处理任务，从而减轻核心服务的负担。 5. Notary (镜像签名) # Notary 是 Harbor 用于实现镜像签名的组件。它通过签名确保镜像的来源和内容在传输过程中不被篡改，从而提升安全性。Harbor 使用 Notary 来确保镜像在生产环境中的可信度。 6. Clair (镜像安全扫描) # Clair 是 Harbor 集成的一个静态分析工具，用于扫描镜像中的安全漏洞。它检查镜像中的软件包是否存在已知的安全漏洞，并生成漏洞报告。 Clair 可以集成到 Harbor 中，当镜像被推送或拉取时，自动扫描镜像内容并报告安全问题。 7. Chartmuseum (Helm 仓库) # Chartmuseum 是 Harbor 用于 Helm chart 管理的组件。它支持存储和管理 Helm chart，允许用户上传、下载和管理 Helm chart 存储库。 通过这个组件，用户可以将 Helm chart 作为 Kubernetes 部署包存储和版本管理。 8. Log (日志系统) # Log 是 Harbor 的日志管理模块，用于记录系统运行过程中的重要事件、错误、警告等信息。它有助于运维人员监控和诊断系统问题。 Harbor 可以通过集成其他日志系统（如 ELK）来扩展日志功能。 9. Database (数据库) # 数据库 存储了 Harbor 的所有元数据，包括用户信息、项目、镜像标签、存储配置信息等。通常，Harbor 使用 PostgreSQL 作为其数据库后端。 10. Redis (缓存服务) # Redis 是 Harbor 的缓存层，用于提高系统性能，减轻数据库的负载。它缓存一些常用的请求数据和会话信息。 11. NGINX (反向代理) # NGINX 是 Harbor 的反向代理和负载均衡器，负责将用户的请求路由到 Harbor 的各个组件上。它也支持 SSL/TLS 加密，确保数据传输的安全性。 总结： # Harbor 的主要组件包括：\nCore：核心服务，提供认证、授权、镜像管理等功能。 Registry：镜像仓库，存储和管理镜像。 Portal：Web UI，提供用户图形化界面。 Jobservice：任务服务，处理异步任务。 Notary：镜像签名，确保镜像可信。 Clair：镜像安全扫描，检测漏洞。 Chartmuseum：Helm 仓库，存储 Helm charts。 Log：日志系统，记录系统事件。 Database：数据库，存储元数据。 Redis：缓存服务，提高性能。 NGINX：反向代理，提供负载均衡和加密服务。 这些组件共同工作，提供一个完整、安全、易于管理的容器镜像仓库。\nHarbor高可用怎么实现 # 在生产环境中，Harbor 的高可用（HA）是确保容器镜像仓库服务持续可用、可靠的关键。为了保证 Harbor 的高可用性，需要对多个组件进行设计和部署，以减少单点故障的风险。下面是实现 Harbor 高可用 的一些方法和最佳实践：\n1. Harbor 组件的冗余设计 # 为了确保 Harbor 的高可用性，首先需要对 Harbor 的关键组件进行冗余设计，确保任何单点故障不会导致整个系统停机。\na. Harbor 核心服务 (Core) 的冗余 # Core 组件是 Harbor 的核心服务，负责处理用户请求、权限管理、任务调度等功能。在高可用部署中，通常需要部署多个 Core 服务实例，使用负载均衡器来分发流量。可以将多个 Core 实例部署到不同的节点上，确保如果某个实例不可用，其他实例可以继续处理请求。 b. 负载均衡器 (Load Balancer) # NGINX 或其他负载均衡器（如 HAProxy）通常用于在多个 Harbor 核心服务实例之间分发流量。负载均衡器通常会部署在 Harbor 的前端，代理用户请求并将其转发到可用的服务实例。 使用 Keepalived 或 HAProxy 等工具来实现虚拟 IP 地址（VIP）的高可用，可以在一个实例宕机时自动将流量切换到健康的实例。 c. Harbor Registry 的冗余 # Registry 是 Harbor 的核心组件，负责存储 Docker 镜像和其他容器化应用。在高可用部署中，应该考虑将 Registry 的数据存储使用 分布式存储系统，如 Ceph 或 GlusterFS，并在多个节点之间进行冗余备份。 可以通过使用 Harbor 的多副本 Registry 存储（例如部署多个 Harbor 实例，配置多个 Registry 存储位置）来确保 Registry 数据的高可用性。 d. 数据库的高可用设计 # Harbor 默认使用 PostgreSQL 作为数据库后端。为了保证数据库的高可用性，可以使用 PostgreSQL 集群（如 Patroni 或 PgBouncer）来实现主从复制和自动故障切换。 数据库可以部署为主从架构，主数据库处理写请求，从数据库处理读请求。如果主数据库出现故障，可以自动切换到从数据库。 e. Redis 缓存的高可用设计 # Redis 被用于 Harbor 的缓存层，在高可用部署中，建议使用 Redis 集群模式 或 Redis Sentinel 来实现 Redis 的高可用。Redis Sentinel 会自动监控 Redis 实例的状态，并在主节点发生故障时自动切换到从节点。 f. Clair（安全扫描服务）的冗余 # Clair 是 Harbor 中用于镜像安全扫描的组件。在高可用部署中，可以部署多个 Clair 实例，并通过负载均衡器进行流量分配。这样，即使某个 Clair 实例出现故障，其他实例也能继续提供镜像扫描服务。 g. Jobservice 的冗余 # Jobservice 负责处理后台任务（如镜像扫描等）。为确保任务的高可用性，可以部署多个 Jobservice 实例，并使用负载均衡器来分配任务请求。 h. Notary 服务的冗余 # Notary 用于实现镜像签名，确保镜像的完整性和可信性。为了确保 Notary 的高可用性，可以将其部署为多个实例，并通过负载均衡进行流量分发。 2. 高可用的部署架构 # 为了实现 Harbor 的高可用性，通常需要按照以下方式部署 Harbor：\na. 多节点部署 # 将 Harbor 的各个组件（如 Core、Registry、Clair、Notary 等）分别部署在多个节点上，确保即使某个节点故障，其他节点依然可以提供服务。 在多个节点上部署 Harbor 的各个组件，并通过 负载均衡器 来实现流量分发和故障切换。 b. 数据存储的高可用 # 使用 分布式存储系统（如 Ceph、GlusterFS 或 NFS）来存储 Harbor 的数据，这样即使某个存储节点出现故障，其他存储节点仍然可以提供数据访问。 对于 Harbor 的 数据库，可以使用 PostgreSQL 主从复制 或 Patroni 集群来保证数据库的高可用性。 c. 使用容器编排工具（如 Kubernetes） # 在 Kubernetes 集群中部署 Harbor 时，可以利用 Kubernetes 的 Pod 副本、Deployment 和 StatefulSet 等功能来实现服务的高可用。 Kubernetes 的 Horizontal Pod Autoscaling（水平自动扩展）可以在流量增加时自动扩展 Harbor 的服务实例数，从而提高容器的可用性和负载能力。 d. 外部存储服务 # 使用 外部存储服务（如 S3 或 阿里云 OSS）来存储镜像数据，这样可以避免由于本地存储故障导致的数据丢失或不可用。 3. Harbor 高可用的部署方案（常见架构） # 以下是一个常见的 Harbor 高可用架构 示例：\n+-------------------------+ | Load Balancer | +-------------------------+ | | v v +----------------+ +----------------+ | Harbor Core | | Harbor Core | | Instance 1 | | Instance 2 | +----------------+ +----------------+ | | +-------------------------------+ | Harbor Registry | | (Shared Storage) | +-------------------------------+ | | v v +-------------------------+ | Database (HA) | +-------------------------+ | v +-------------+ | Redis HA | +-------------+ | v +----------------+ | Clair (HA) | +----------------+ | v +----------------+ | Jobservice (HA)| +----------------+ 负载均衡器：将用户请求分发到多个 Harbor Core 实例。 Harbor Core 实例：多个 Harbor 核心服务实例，支持负载均衡。 Registry 存储：使用共享的分布式存储系统或对象存储（如 S3）。 数据库：使用高可用数据库架构（如 PostgreSQL 主从复制）。 Redis：使用 Redis Sentinel 或集群模式，保证缓存高可用。 Clair：使用多个 Clair 实例提供镜像扫描服务。 Jobservice：多个 Jobservice 实例处理后台任务。 4. 故障恢复与备份 # 为了确保高可用架构的健壮性，还需要制定灾难恢复（DR）策略和定期备份：\n定期备份 Harbor 数据（包括镜像、数据库、配置文件等）以便在发生故障时进行恢复。 使用 自动化备份 和 异地备份 确保数据不会丢失，并能在灾难发生后快速恢复服务。 总结： # 实现 Harbor 高可用 需要通过以下方式：\n冗余设计：确保核心组件（Core、Registry、Clair、Notary 等）部署多个实例，负载均衡分发流量。 数据库和 Redis 高可用：使用 PostgreSQL 主从复制或 Redis Sentinel 实现数据和缓存的高可用。 分布式存储：使用分布式存储系统（如 Ceph、GlusterFS 或 S3）确保数据高可用。 容器编排工具：如 Kubernetes 提供自动扩展和容错能力。 灾难恢复与备份：定期备份 Harbor 的重要数据，确保灾难发生时可以恢复。 通过这些方法，Harbor 能够实现高可用部署，保证容器镜像服务的稳定性和持续性。\nETCD调优 # etcd 是 Kubernetes 中的核心组件，负责存储集群的所有配置信息、元数据和状态数据。因为 etcd 是分布式的键值存储数据库，它的性能直接影响到 Kubernetes 集群的可靠性和性能。因此，etcd 的调优对确保集群的高可用性、低延迟和高吞吐量至关重要。\n在实际应用中，etcd 的调优可以涉及多个方面，包括硬件配置、网络配置、etcd 配置参数调整以及监控和维护策略。以下是一些常见的 etcd 调优方法：\n1. 硬件配置优化 # a. 存储硬盘 # 磁盘 I/O 性能：etcd 对磁盘 I/O 性能要求较高，特别是当存储的数据量较大时。推荐使用 SSD 存储而不是传统的 HDD，因为 SSD 提供更高的读写性能。 RAID 配置：如果使用 RAID 阵列，请使用 RAID 10 来提供较好的读写性能和数据冗余。 b. 内存 # 内存大小：etcd 的数据存储主要依赖内存，因此需要足够的内存来缓解频繁的磁盘 I/O。建议在每个 etcd 节点上分配至少 16 GB 或更多的内存。 内存调整：etcd 在存储数据时，尽量让数据驻留在内存中以提高查询速度。如果 etcd 运行在较低内存的机器上，可以考虑增加内存，特别是在等候大量请求的场景下。 c. CPU # etcd 需要较高的计算能力来处理事务、复制和集群协调等任务，尤其是在大量写操作的情况下，确保有足够的 CPU 核心来处理并发请求。 2. 网络配置优化 # a. 网络延迟 # etcd 节点之间需要进行高频率的通信，因此，网络延迟对 etcd 的性能影响非常大。应保证 etcd 集群的节点位于低延迟的网络环境中，尽量避免跨地区和不同数据中心部署。 建议：使用 10Gbps 或更高速率的网络连接，并避免使用共享带宽。 b. 流量限制与拥塞 # 配置网络的带宽限制和防止拥塞对于 etcd 性能至关重要，尤其是当数据写入量大时。如果使用 Kubernetes 时，保证网络中没有其他服务干扰 etcd 的网络流量。 c. 配置通信加密 # etcd 支持 TLS 加密 来保护节点间的通信。虽然加密会带来一些性能开销，但它对集群的安全性至关重要。务必确保使用强加密算法并正确配置证书。 3. etcd 配置参数调整 # a. --max-request-bytes # 这个参数用于限制每个请求的最大字节数。如果你在集群中有非常大的请求或非常频繁的请求，增大该值可以避免请求被拒绝。 推荐设置：根据需求调整，通常 1 MB 到 10 MB 之间合适。 b. --heartbeat-interval 和 --election-timeout # --heartbeat-interval：控制 leader 节点和 follower 节点之间心跳的时间间隔，默认 100ms。如果心跳间隔设置得太长，集群内的节点可能会过早认为 leader 节点失效，从而触发选举。 --election-timeout：控制选举超时时间，默认 1000ms。如果这个时间设置得太短，会导致频繁的选举过程；设置得过长，会导致集群恢复时间过慢。 建议：一般情况下，心跳间隔 --heartbeat-interval 设置为 100ms 到 500ms，选举超时 --election-timeout 设置为 1000ms 到 2000ms。 c. --quota-backend-bytes # 此参数控制 etcd 的数据存储最大限制，它表示 etcd 数据库文件最大占用的字节数。超过该值时，etcd 将阻止更多的数据写入。 推荐设置：根据实际情况调整，通常为 8 GB 或 16 GB。 d. --max-txn-ops # 该参数用于设置单个事务中最大操作数。设置过大可能导致事务执行缓慢，甚至超时。 推荐设置：一般不需要设置过大，默认值（--max-txn-ops=128）适合大多数场景。 e. --snapshot-count # --snapshot-count 是控制 etcd 每多少个写入操作后生成一个快照。快照有助于减少存储使用量，并提高数据库恢复速度。 推荐设置：根据数据量大小，可以设置为 10000-50000。 f. --compression # etcd 支持启用 数据压缩 来节省存储空间。启用压缩可能会带来额外的 CPU 开销，但对于存储密集型的应用，启用压缩可以有效减少磁盘空间占用。 推荐设置：--compression=snappy 或 --compression=lz4，它们提供了较好的压缩率和性能。 4. 集群规模与拓扑设计 # a. 节点数目 # etcd 集群的节点数：etcd 是一个 奇数个节点 的分布式集群，以保证选举过程中不会出现脑裂现象。推荐部署 3、5 或 7 个节点的集群。 需要注意的是，节点数目越多，选举和一致性保证的成本也越高。通常，3 个节点适用于大多数生产环境。 b. 分布式部署 # 如果 etcd 的负载过高，或者需要支持跨数据中心或跨区域的高可用集群，可以考虑使用 跨数据中心的 etcd 集群，但需要注意网络延迟和带宽要求。 c. 自动化扩展 # etcd 集群中的节点可以根据需求进行自动扩展。比如可以使用 Kubernetes 的 StatefulSet 来管理 etcd 的自动扩展和故障恢复。 5. 监控与维护 # a. 监控 # 使用\nPrometheus\n和\nGrafana\n来监控 etcd 的各项指标，包括但不限于：\netcd_disk_wal_fsync_duration_seconds：磁盘 WAL 写入的延迟。 etcd_network_round_trip_time_seconds：网络往返时延。 etcd_server_has_leader：查看是否有 leader 节点。 etcd_backend_commit_duration_seconds：后端提交的延迟。 b. 定期备份 # 定期备份 etcd 数据是非常重要的，尤其是在配置变更和集群扩展时。可以使用 etcdctl snapshot save 命令进行备份，建议每天进行一次完整备份，并将备份存储在不同的位置（如远程存储）。 在集群发生故障时，定期备份可以帮助恢复集群状态，减少数据丢失。 c. 定期清理 # 清理 etcd 中的无用数据，如过期的 Key 或历史版本，确保 etcd 的磁盘空间得到有效管理。 6. 故障排查 # ETCD 超时问题：etcd 集群出现超时时，检查节点间的网络延迟、磁盘 I/O 性能以及 etcd 配置中的超时参数。 数据恢复：在 etcd 出现故障时，使用备份恢复数据，并确保恢复的节点与其他节点之间的数据一致性。 负载过高：检查 etcd 的 CPU 和内存使用情况，查看是否存在内存泄漏或 CPU 使用率过高的情况，适时调整资源配置。 总结： # 通过合理的硬件配置、网络优化、etcd 参数调整、集群规模设计以及良好的监控和备份策略，可以有效提高 etcd 的性能和稳定性，确保 Kubernetes 集群的可靠运行。\n假设k8s集群规模上千，需要注意的问题有哪些？ # 在 Kubernetes 集群规模达到上千节点时，集群的管理、性能、可用性和安全性都会面临更高的挑战。为了确保集群在大规模环境下的高效运行，需要注意以下几个方面：\n1. 集群架构与资源规划 # a. 集群拓扑设计 # 高可用架构：确保 API Server、etcd、控制面组件 和 工作节点 等关键组件具有高可用性。需要部署多个副本，以保证任意组件的单点故障不会导致集群的不可用。 分布式部署：根据节点的数量，将集群分布在多个物理或虚拟数据中心，以减少单个机房或机架的故障影响。通过 Kubernetes 多区域集群 或 多集群 架构来提升灾备能力。 b. API Server # 对于上千节点的集群，需要对 API Server 的负载均衡、水平扩展（--max-requests-inflight、--max-mutating-requests-inflight 等）进行调整。 确保有足够的 API Server 副本，合理使用 Ingress 或 负载均衡器 进行流量分配，避免单点故障。 c. etcd 集群 # 集群规模较大时，etcd 的性能瓶颈可能会成为集群的瓶颈。建议使用至少 5 个 etcd 节点，并确保它们处于低延迟的网络环境中。配置合理的备份策略，并确保其容量足以存储集群状态。 d. 网络拓扑与性能 # 需要考虑 Pod 网络的性能，如网络插件（Calico、Flannel、Cilium 等）选择和网络拓扑设计。对于大规模集群，使用 IPVS 模式的 kube-proxy 会有更好的性能。 网络分段：合理规划 网络策略 和 命名空间，确保集群内流量高效、可控且安全。 2. 资源管理与调度 # a. 节点管理 # 对于上千个节点，手动管理变得复杂。需要借助 自动化工具 来动态添加、删除和管理节点，确保集群始终保持健康状态。 使用 Node Affinity 和 Taints/Tolerations 来确保 Pod 调度到适当的节点上，避免资源冲突。 b. 集群资源配额 # 需要在 命名空间级别 配置合理的资源配额，避免某个命名空间或应用占用过多资源，影响整个集群的稳定性。 通过 Horizontal Pod Autoscaling (HPA) 和 Vertical Pod Autoscaling (VPA) 来自动调整 Pod 的资源请求，确保资源得到合理分配。 c. 集群容量规划 # 需要定期进行 容量规划，根据应用需求预测 CPU、内存和存储的需求。通过监控数据和负载测试评估集群资源的使用情况，并做出相应调整。 3. 性能与可伸缩性 # a. 控制平面性能优化 # 在规模较大的集群中，控制平面（如 API Server、Controller Manager 等）可能成为瓶颈。需要考虑通过 水平扩展 来增加 API Server 的副本数。 增加 etcd 节点的数量以减少锁争用，并确保其存储性能足够支撑大量节点和请求。 b. Pod 和服务的性能优化 # Pod 调度优化：确保 Pod 调度的高效性，合理配置 资源请求和限制，避免资源不足或资源过载的情况。 对 Services 使用 ClusterIP 模式时，使用 IPVS 模式的 kube-proxy 来提高服务发现性能。 使用 网络插件（如 Calico 或 Cilium）来提高集群内的网络性能，确保大规模集群中的容器网络通信不会成为瓶颈。 c. 日志和监控系统的扩展 # 监控系统（如 Prometheus、Grafana）需要确保能够处理大规模集群的指标数据。如果指标量过大，可以考虑使用 Prometheus 联邦 或 分布式 Prometheus 系统来处理。 日志收集系统（如 ELK 或 Loki）需要做适当的扩展，确保日志数据存储和处理不会影响集群性能。 4. 高可用与容灾 # a. 高可用性设计 # API Server、etcd、Controller Manager、Scheduler 等控制面组件必须具有高可用性，至少部署 3 个副本，确保在节点或区域故障时，控制面组件能够继续运行。 负载均衡器：对于大规模集群，建议部署高性能的 负载均衡器（如 HAProxy、Nginx），并确保它们支持 API Server 负载均衡和流量控制。 b. 灾难恢复与备份 # 定期备份 etcd 数据，并且确保备份数据的可靠性。考虑使用多区域存储或异地备份，确保在灾难发生时能够快速恢复。 配置 跨区域故障恢复，通过 Kubernetes 的 多集群架构 来实现跨区域的灾难恢复。 5. 安全与合规 # a. 集群安全 # 身份认证与授权：大规模集群中，应该使用强制认证的方式来确保集群的安全。确保 RBAC（Role-Based Access Control） 和 PodSecurityPolicy 被严格配置，避免不必要的权限暴露。 Network Policy：使用 NetworkPolicy 来限制 Pod 之间和 Pod 与外部的通信，减少潜在的安全风险。 Pod 安全：配置 PodSecurity（PSP）策略，确保容器在适当的安全上下文中运行，限制特权容器等不安全的操作。 b. 集群合规性 # 由于集群规模较大，可能涉及合规要求，因此需要定期审查集群的安全策略和操作日志，确保其符合公司或行业的安全标准。 容器镜像扫描：对容器镜像进行安全扫描，确保没有已知漏洞的镜像被用于生产环境。 6. 故障排查与诊断 # a. 异常检测与诊断 # 使用 Prometheus、Grafana、Alertmanager 等工具建立完善的告警系统，能够及时检测到集群中出现的异常（如 CPU/内存资源超限、Pod 网络延迟过高等）。 日志分析：使用 ELK Stack（Elasticsearch、Logstash、Kibana） 或 Loki 来收集和分析集群的日志。对于大规模集群，日志存储和查询需要足够的性能来支持高频率的写入和查询操作。 b. 节点故障检测 # 在大规模集群中，节点故障是不可避免的。通过 Kubernetes 的自动扩展功能（如节点自动扩展和 Pod 自动重启）来快速恢复集群健康状态。 节点故障转移：通过 PodDisruptionBudget 和 Pod Affinity/Anti-Affinity 策略来确保节点故障时应用能继续运行。 c. 监控与优化 # 定期评估集群的性能，利用 Prometheus 收集 etcd、kubelet、Scheduler、Controller Manager 等关键组件的性能数据。 对于大规模集群，可以考虑使用 Kubernetes Federation 来管理多个集群，并通过联邦控制多个集群的状态和健康检查。 总结 # 当 Kubernetes 集群规模达到上千节点时，面临的挑战和复杂性大大增加。为了确保集群的可用性、性能和安全性，需要采取以下措施：\n进行合理的 集群架构设计 和 资源规划，确保集群的高可用性和可伸缩性。 优化 API Server、etcd 和 网络插件 的配置，以支持大规模的负载。 部署 监控、日志收集系统，并进行 故障排查，及时发现并处理集群中的问题。 实现强有力的 安全措施 和 合规策略，确保集群的安全性和合规性。 通过这些措施，可以确保大规模集群的稳定性和高效运行。\n如果在 Kubernetes 中的 Pod 没有显式设置 resources.requests 和 resources.limits，那么 VPA 会基于容器的实际资源使用情况自动生成推荐的资源请求（request）和限制（limit），但它不能直接强制限制容器的资源。在这种情况下，VPA 会根据 历史使用数据 为容器提供合理的资源请求和限制的建议。\n关键点总结： # VPA 的工作方式： VPA 会监控容器的资源使用情况（例如 CPU 和内存），然后基于过去的使用模式来调整推荐的 resources.requests 和 resources.limits。 如果你没有设置 request 和 limit，VPA 将会通过观察容器的实际负载来建议一个合适的资源请求配置，并自动应用这个推荐值。 requests 和 limits 重要性： request：是容器启动时请求的资源量，调度器依据该值来决定将容器调度到哪个节点。Pod 在启动时会至少保证这个资源量。 limit：是容器使用的资源上限。如果容器使用超过 limit 的资源，Kubernetes 会限制容器的资源使用，并且可能会杀死该容器。 VPA 的作用： VPA 推荐资源配置：如果 Pod 没有设置 resources.requests 和 resources.limits，VPA 会基于监控到的资源使用情况，自动为这些 Pod 提供资源请求和限制的推荐值。这些推荐值的计算依赖于容器的历史资源消耗数据。 自动调整：VPA 会自动调整容器的资源请求，保证容器获得合适的资源配置，避免容器因为资源不足而崩溃（比如 OOM），也避免资源过多分配造成浪费。 VPA 不会直接强制执行限制： VPA 只会推荐，它并不会直接限制容器的资源，必须结合 Deployment 或其他控制器来应用这些调整。如果没有设置 resources.requests 和 resources.limits，VPA 会依据使用情况推荐适当的值。 总结： # 如果 Pod 没有设置 resources.requests 和 resources.limits，VPA 会 基于实际资源使用情况 提供推荐，但它不会直接强制执行资源限制，且这些推荐值必须在 Deployment 或其他控制器中应用。 为了让 VPA 工作，并有效避免资源瓶颈或浪费，建议还是显式为每个 Pod 设置 requests 和 limits，让 VPA 在此基础上进行优化。 节点NotReady可能的原因？会导致哪些问题？☆ # 在 Kubernetes 中，节点（Node）处于 NotReady 状态 表示该节点无法正常与控制平面（如 API Server）进行通信或其健康状况不符合要求。NotReady 状态通常表示该节点上的 Kubelet 或 容器运行时 等关键组件出现了问题，导致 Kubernetes 无法调度或管理该节点上的 Pod。\n可能导致节点 NotReady 状态的原因 # Kubelet 健康检查失败： Kubelet 是 Kubernetes 节点的核心组件，负责与控制平面通信、监控节点上的 Pod 和容器。如果 Kubelet 无法正常启动或无法与 API Server 通信，节点会变为 NotReady。 可能的原因包括：Kubelet 配置错误、API Server 无法访问、Kubelet 进程崩溃等。 节点资源不足： 节点上的 CPU、内存 或 磁盘 等资源耗尽可能导致 Kubelet 无法正常运行或节点负载过高，从而进入 NotReady 状态。 例如，磁盘满了导致日志写入失败，或者内存耗尽导致系统性能问题。 容器运行时问题： Kubernetes 使用容器运行时（如 Docker 或 containerd）来管理容器。如果容器运行时出现故障（如死锁、崩溃、配置错误等），则会影响节点的健康状况。 例如，容器运行时无法启动或没有运行，导致 Kubelet 无法管理节点上的 Pod。 网络故障： 如果节点与集群其他节点或 API Server 之间的网络通信失败，节点无法与控制平面同步，导致状态被标记为 NotReady。 网络问题可能包括 DNS 解析失败、路由或防火墙配置错误等。 Pod 或容器状态异常： 节点上运行的某些 Pod 或容器可能处于崩溃状态，影响整个节点的健康。例如，多个容器进入 CrashLoopBackOff 状态，可能导致节点的资源紧张，从而影响节点状态。 磁盘或文件系统错误： 节点的磁盘或文件系统出现故障（如损坏、无法挂载等），也可能导致节点无法正常运行，进而导致 NotReady 状态。 Kube-proxy 问题： Kube-proxy 是负责实现服务负载均衡和网络代理的组件。如果 kube-proxy 出现故障或配置错误，也可能导致节点处于 NotReady 状态，尤其是涉及到网络通信的部分。 节点健康检查配置错误： Kubernetes 节点的健康检查（如 kubelet 自身的健康检查）可以配置为自动检查节点的健康状况。如果健康检查失败，节点也会被标记为 NotReady。 节点证书问题： 如果节点的证书过期或被吊销，Kubelet 与 API Server 之间的 TLS 通信无法正常进行，节点可能会进入 NotReady 状态。 资源调度冲突： 节点上有未解决的资源冲突或依赖问题，可能导致 Kubelet 或其他关键组件不能正常工作。 节点 NotReady 状态可能导致的问题 # Pod 无法调度： Kubernetes 调度器会避免将新的 Pod 调度到 NotReady 状态的节点，因为该节点无法提供必要的资源或服务。 现有 Pod 无法访问： 如果节点被标记为 NotReady，节点上已经运行的 Pod 可能会面临访问失败或无法与其他服务通信的问题。这可能导致集群的服务中断。 影响服务可用性： 如果一个或多个关键节点进入 NotReady 状态，且这些节点运行着重要的应用或服务，可能会导致应用的部分功能不可用，甚至出现服务完全宕机的情况。 影响负载均衡与网络通信： 节点在 NotReady 状态时，Kubernetes 集群中的 kube-proxy 可能会更新其路由信息，从而导致集群中某些网络流量被错误路由或丢失。 如果节点使用的是 IPVS 或 iptables 模式，负载均衡会受到影响。 资源管理混乱： 节点进入 NotReady 状态会导致节点上的资源无法有效管理，Pod 的重调度可能导致其他节点资源的过载，影响整个集群的性能。 可能影响自动扩展： 对于启用了 自动扩展（如 HPA 和 VPA）的集群，如果节点 NotReady，自动扩展可能无法在预定时间内完成任务，从而导致资源分配不均衡。 排查节点 NotReady 状态的思路 # 检查 Kubelet 状态： 使用 systemctl status kubelet 或查看 journalctl -u kubelet 日志，检查 Kubelet 是否正常运行，是否有错误信息。 查看节点状态： 使用 kubectl describe node \u0026lt;node-name\u0026gt; 查看节点的详细信息，特别是 Events 部分，看是否有相关的错误提示或警告信息。 检查资源使用情况： 查看节点的资源使用情况，使用 kubectl top node \u0026lt;node-name\u0026gt; 或通过其他监控工具（如 Prometheus）查看节点的 CPU、内存和磁盘使用情况，检查是否出现资源耗尽的情况。 检查容器运行时状态： 检查容器运行时的日志，使用 docker info 或 containerd 工具查看容器运行时是否正常运行。 查看网络状态： 使用 kubectl get pods --all-namespaces -o wide 检查 Pod 分布和节点网络状态，确保没有网络故障，Pod 能正常与其他节点通信。 检查磁盘和文件系统： 确保节点的磁盘和文件系统没有出现故障。可以查看磁盘空间使用情况 (df -h)，或检查是否有磁盘挂载问题。 检查节点证书和身份验证： 确保节点证书没有过期，并且 Kubelet 能与 API Server 正常通信。 检查节点健康检查配置： 查看节点的健康检查配置是否正常，确认 Kubelet 和 API Server 之间的连接没有问题。 使用监控工具： 通过集群监控工具（如 Prometheus、Grafana）实时监控节点和 Pod 的状态，及时发现问题并处理。 总结 # 节点 NotReady 状态的原因多种多样，常见的有 Kubelet 健康检查失败、资源不足、容器运行时故障、网络问题等。NotReady 状态会导致 Pod 无法调度、现有服务无法访问、负载均衡异常等问题，严重时会影响整个集群的可用性和稳定性。解决此问题需要排查节点日志、资源使用情况、网络状态等，逐步定位故障源并进行修复。\nservice和endpoints是如何关联的？ # 在 Kubernetes 中，Service 和 Endpoints 是两个非常重要的资源对象，它们通过 Endpoints 来实现对 Service 的访问路由和负载均衡。\n1. Service 的作用 # Service 是 Kubernetes 中一个抽象层，用于暴露一组运行在 Pod 中的应用或服务，它提供一个稳定的网络访问地址（DNS 或 IP），而这些 Pod 可能会随着时间的推移而被创建或销毁。通过 Service，可以确保即使 Pod 被替换或重启，客户端仍然能够通过 Service 访问到相应的应用。\n2. Endpoints 的作用 # Endpoints 资源是 Kubernetes 中 Service 对象的实际目标。每当 Service 创建时，Kubernetes 会为它自动创建对应的 Endpoints 资源。Endpoints 列出了与某个 Service 相关的所有 Pod 的 IP 地址和端口。通过这些 Endpoints，Kubernetes 知道如何将流量路由到这些 Pod。\n3. Service 和 Endpoints 的关联方式 # Service 和 Endpoints 是通过 Pod 的标签（Labels）进行关联的。具体来说，Service 会根据其 selector 字段来选择一组 Pod，这些 Pod 组成了 Service 的目标。在创建 Service 时，Kubernetes 会根据 selector 自动发现符合条件的 Pod，并将其 IP 和端口添加到对应的 Endpoints 资源中。\nService：通过 spec.selector 字段来定义匹配哪些 Pod（例如根据标签选择）。 Endpoints：Service 创建后，自动生成对应的 Endpoints 资源，列出符合选择条件的 Pod 的 IP 地址和端口。 4. 流程说明 # 创建 Service：\n用户通过 kubectl expose 或 YAML 文件创建一个 Service，通常会设置 spec.selector 来指定一个标签选择器，用于匹配一组 Pod。例如： apiVersion: v1 kind: Service metadata: name: my-service spec: selector: app: my-app ports: - port: 80 targetPort: 8080 这时，Kubernetes 会创建一个名为 my-service 的 Service，并根据 selector 查找匹配的 Pod。\n匹配 Pod：\nKubernetes 会根据\nService 的 selector 查找所有匹配的 Pod，假设有以下 Pod：\napiVersion: v1 kind: Pod metadata: name: my-pod-1 labels: app: my-app spec: containers: - name: my-container image: nginx ports: - containerPort: 8080 创建 Endpoints：\n对于上述匹配到的 Pod，Kubernetes 会自动为 my-service 创建一个 Endpoints 资源，并将匹配的 Pod 的 IP 地址和端口号列出。例如： apiVersion: v1 kind: Endpoints metadata: name: my-service subsets: - addresses: - ip: 10.1.2.3 ports: - port: 8080 这个 Endpoints 资源记录了 my-pod-1 的 IP 地址和端口。\n负载均衡与流量转发：\n当请求发送到 my-service 时，Kubernetes 会查询对应的 Endpoints 资源，选择一个 Pod 的 IP 地址并转发请求。负载均衡可以通过 Round Robin（轮询）方式进行，即请求会轮流转发给不同的 Pod。 5. Endpoints 的更新 # 如果 Pod 的 IP 地址发生变化（比如 Pod 被重新调度或重新创建），Kubernetes 会自动更新 Endpoints 资源，确保 Service 始终指向当前可用的 Pod。 例如，Pod 被删除或重新调度到其他节点时，Endpoints 中会移除掉已删除 Pod 的信息，加入新 Pod 的 IP 地址。 6. 手动管理 Endpoints # 在某些场景下，也可以手动创建或管理 Endpoints，尤其是当你需要将 Service 暴露给外部资源（如外部数据库）时。这时，你可以在 Service 中不使用 selector，而是手动指定 Endpoints 对象，给定固定的 IP 和端口。\n例如，手动创建一个 Endpoints 来指向外部服务：\napiVersion: v1 kind: Endpoints metadata: name: my-service subsets: - addresses: - ip: 192.168.1.100 ports: - port: 80 7. 集群外的访问 # 对于 ClusterIP 类型的 Service，客户端只能在集群内部访问这个 Service。如果你需要从集群外部访问某个 Service，可以使用 NodePort 或 LoadBalancer 类型的 Service，这时 Kubernetes 会在集群外部创建端口映射。\n8. 总结： # Service 是一个逻辑上的抽象，用于暴露一组后端 Pod。 Endpoints 是 Service 的实际目标，记录了所有匹配 Pod 的 IP 地址和端口信息。 Kubernetes 会根据 Service 的 selector 自动维护 Endpoints 列表，确保流量能够正确路由到正确的 Pod。 这种机制确保了在 Pod 被动态调度、重新启动或者创建新的 Pod 时，Service 的访问接口始终保持稳定，而 Pod 的具体实现和 IP 地址可以发生变化。\nReplicaSet、Deployment功能是怎么实现的？ # ReplicaSet 和 Deployment 都是 Kubernetes 中用于管理 Pod 副本和确保集群中应用高可用的资源对象，但它们在功能和使用场景上有一些区别。下面详细讲解这两个对象的功能和实现原理。\n1. ReplicaSet 的功能和实现原理 # 功能 # ReplicaSet 是一种用于维护一组 Pod 副本 的控制器。它确保某个时刻集群中总是有指定数量的副本 Pod 在运行，并且这些 Pod 的定义始终符合预期的状态。\n副本数量管理：ReplicaSet 会根据 spec.replicas 中定义的副本数来管理 Pod 的数量。如果当前的 Pod 数量少于指定的副本数，ReplicaSet 会创建新的 Pod；如果 Pod 数量超过了指定数量，它会删除多余的 Pod。 Pod 副本的健康管理：ReplicaSet 会持续监控 Pod 的健康状况。如果某个 Pod 出现故障（如崩溃或被删除），ReplicaSet 会自动创建新的 Pod 来替代它。 实现原理 # ReplicaSet 通过一个 标签选择器（Label Selector） 来确定哪些 Pod 是由该 ReplicaSet 管理的。ReplicaSet 会与其管理的 Pod 的标签进行匹配，确保符合条件的 Pod 始终保持在集群中。\n创建 ReplicaSet： 用户通过 kubectl 或 YAML 配置创建 ReplicaSet。ReplicaSet 会根据 spec.replicas 中指定的副本数来管理 Pod 副本，确保创建的 Pod 数量始终保持一致。 示例：\napiVersion: apps/v1 kind: ReplicaSet metadata: name: my-replicaset spec: replicas: 3 selector: matchLabels: app: my-app template: metadata: labels: app: my-app spec: containers: - name: my-container image: nginx 控制器的工作原理：\nReplicaSet 会监控和维护指定数量的 Pod 副本。 如果某个 Pod 被删除或失败，ReplicaSet 会创建新的 Pod 进行替代。 ReplicaSet 会基于 Pod 模板（template）来创建新的 Pod，并确保其满足与 Selector 匹配的条件。 Pod 状态的监控： ReplicaSet 会通过 Kubernetes 的控制循环来检测 Pod 的状态，并通过 API 服务器与 Kubernetes 控制平面进行交互，确保 Pod 的数量和健康状况符合要求。\n注意： # ReplicaSet 不会自己更新 Pod，它仅负责确保指定数量的 Pod 副本在集群中。如果 Pod 模板（如容器镜像、端口等）发生变化，需要手动删除旧的 ReplicaSet 并创建一个新的。 2. Deployment 的功能和实现原理 # 功能 # Deployment 是在 ReplicaSet 的基础上提供更高级别管理功能的控制器，它不仅能管理 ReplicaSet 的副本数，还提供了 滚动更新、回滚、暂停和恢复等功能。Deployment 通过控制 ReplicaSet 来确保指定数量的 Pod 副本保持运行，同时能够进行无缝的应用更新。\n实现原理 # Deployment 主要通过 ReplicaSet 来管理 Pod 副本和滚动更新过程。每次更新应用时，Deployment 会创建一个新的 ReplicaSet 来替代旧的 ReplicaSet，而旧的 ReplicaSet 会逐步减少 Pod 副本的数量，直到所有 Pod 都由新的 ReplicaSet 管理。\n创建 Deployment： 用户可以通过 kubectl 或 YAML 文件创建 Deployment。在 Deployment 中，用户指定了 Pod 模板、所需副本数、更新策略等。Deployment 会自动管理 ReplicaSet，并且管理更新和回滚的过程。 示例：\napiVersion: apps/v1 kind: Deployment metadata: name: my-deployment spec: replicas: 3 selector: matchLabels: app: my-app template: metadata: labels: app: my-app spec: containers: - name: my-container image: nginx:1.19 滚动更新：\n当 Deployment 更新时，它会创建一个新的 ReplicaSet，并逐渐停止旧 ReplicaSet 中的 Pod，创建新的 Pod 来替代它们。这个过程称为 滚动更新（Rolling Update）。 默认情况下，Deployment 会确保更新时保持高可用性。通过控制器的智能调度，Kubernetes 会在 Pod 更新过程中确保某些副本始终处于运行状态。 回滚功能：\n如果新的更新失败，用户可以回滚到之前的版本。Deployment 会自动跟踪其历史版本，允许在必要时进行快速回滚。 kubectl rollout undo 可以用来回滚到上一个 Deployment 版本。 暂停和恢复：\n如果在应用更新时需要暂停，可以使用 kubectl rollout pause 暂停 Deployment 更新。暂停后，可以手动调整 Deployment 的配置，然后使用 kubectl rollout resume 恢复更新。 滚动更新策略：\nmaxSurge：指定更新过程中最多可以超出副本数的 Pod 数量。 maxUnavailable：指定更新过程中最多不可用的 Pod 数量。 Deployment 更新过程：\n当用户修改 Deployment（例如更新容器镜像）时，Deployment 会创建一个新的 ReplicaSet，并根据滚动更新策略逐步替换旧的 Pod。 3. ReplicaSet 与 Deployment 的区别 # 特性 ReplicaSet Deployment 主要功能 只负责管理 Pod 副本数 负责管理 ReplicaSet，支持滚动更新、回滚、暂停、恢复 更新 Pod 的方式 不支持自动更新 Pod 支持自动滚动更新和回滚 控制器功能 仅管理 Pod 副本的数量 管理 ReplicaSet，提供更多高级功能 用途 多用于在 Deployment 中作为控制器 用于应用部署，自动更新、回滚等功能 适用场景 需要固定副本数的场景 需要持续部署、更新和回滚的场景 4. Deployment 如何实现滚动更新 # Deployment 使用 Rolling Update 策略来平滑地更新应用。更新过程的关键步骤如下：\n创建新 ReplicaSet：当 Deployment 更新时，Kubernetes 会创建一个新的 ReplicaSet。 逐步替换 Pod：在更新过程中，旧的 ReplicaSet 会逐步缩减 Pod 副本的数量，而新的 ReplicaSet 会逐步增加 Pod 副本的数量。 健康检查：Kubernetes 会检查每个 Pod 的健康状态，只有在新 Pod 启动并通过健康检查后，才会删除旧 Pod。 可配置策略：通过 maxSurge 和 maxUnavailable 参数，用户可以配置更新的速度和策略。 5. 总结 # ReplicaSet 主要用于确保一组 Pod 副本数量的管理，它本身不支持自动更新，因此需要手动管理 Pod 模板和副本数量。 Deployment 则在 ReplicaSet 的基础上提供了更高级的功能，支持滚动更新、回滚、暂停和恢复等功能，适合用于生产环境中的应用管理。通过 Deployment，Kubernetes 可以自动管理 ReplicaSet 的更新过程，确保应用的高可用性和版本控制。 一般来说，在实际使用中，Deployment 是更常见和推荐的选择，因为它提供了更多的管理和更新功能，而 ReplicaSet 通常是通过 Deployment 间接管理的。\nscheduler调度流程 # Kubernetes 的 Scheduler 负责将 Pod 调度到合适的 Node 上运行。调度的目标是确保 Pod 在集群中的高效运行，并满足资源需求、拓扑要求等各种约束条件。下面是 Kubernetes Scheduler 的详细调度流程：\n1. 调度器的工作流程概述 # Kubernetes Scheduler 主要包括以下几个步骤：\n监听待调度的 Pod：Scheduler 监视 API 服务器中的 Pod 列表，查找没有被调度的 Pod（状态为 Pending 的 Pod）。 选择适合的节点：Scheduler 根据 Pod 的资源需求、节点的可用资源、调度策略、亲和性和反亲和性等信息选择一个合适的节点。 将 Pod 绑定到节点：一旦选择了节点，Scheduler 会将 Pod 绑定到该节点，并将信息更新到 API 服务器。 执行调度：节点的 kubelet 会接收到更新，并开始启动容器来运行该 Pod。 2. Scheduler 调度流程的详细步骤 # 步骤 1：监听待调度的 Pod # Pod 状态为 Pending：Scheduler 只会调度状态为 Pending 的 Pod，也就是那些还没有被绑定到节点的 Pod。 Pod 和调度器：Pod 会首先被创建并发送到 Kubernetes 的 API 服务器，Scheduler 会监控这些 Pod 的状态。 Pod 资源需求：每个 Pod 在创建时可以定义其对资源（如 CPU、内存、存储等）的请求和限制（resources.requests 和 resources.limits）。 步骤 2：调度器选择适合的节点 # Scheduler 会根据以下几个因素来选择合适的节点：\nNode 可用性： Scheduler 会查询所有可用节点，并根据 Node 的资源（CPU、内存、存储等）和 Pod 的资源请求来筛选合适的节点。 如果节点资源不足，Pod 将无法调度到该节点。 Pod 的亲和性和反亲和性（Affinity/Anti-Affinity）： NodeAffinity：Pod 可以指定必须调度到具有特定标签的节点上，或者排除特定标签的节点。 PodAffinity：Pod 可以指定它应该和其他特定的 Pod 一起调度到同一个节点上。 PodAntiAffinity：Pod 可以指定它应该避免和其他特定的 Pod 调度到同一个节点上。 亲和性和反亲和性可以基于标签选择器来指定。 资源请求和限制： Scheduler 会根据 Pod 的资源请求和节点的资源使用情况进行匹配。 例如，如果 Pod 请求 2 CPU 和 4GB 内存，而节点只有 1 CPU 和 2GB 内存，那么该 Pod 无法被调度到这个节点。 Taints 和 Tolerations： 节点可以通过 taints 来标记其不适合某些 Pod 的调度，只有具有相应 tolerations 的 Pod 才能被调度到这些节点上。 Taints 是一种保护机制，防止 Pod 被调度到不适合的节点。 资源均衡： Scheduler 会尽量保证集群资源的均衡，避免某些节点资源的过度消耗。 Kubernetes 会根据各个节点的资源使用情况（例如 CPU、内存的消耗量）来做负载均衡，确保 Pod 被均匀调度。 Priority 和 Preemption： Kubernetes 支持 Pod 优先级，调度器会优先调度优先级更高的 Pod。 如果集群中没有足够的资源来调度新 Pod，调度器会选择抢占低优先级的 Pod，释放资源来调度高优先级的 Pod。 其他约束： PodDisruptionBudgets（PDB）：定义了最小的 Pod 副本数量，Scheduler 会确保不会调度导致 PDB 限制的 Pod。 静态调度器插件（例如, VolumeScheduling）：调度时会考虑卷的可用性，避免选择那些无法挂载某个特定卷的节点。 步骤 3：绑定 Pod 到节点 # 一旦 Scheduler 选择了合适的节点，下一步是将该 Pod 绑定到节点。这个操作实际上是修改 API 服务器中的 Pod 状态，将 Pod 的 nodeName 字段更新为选中的节点名称。\nPod Binding：这是调度的最后一步，Scheduler 更新 Pod 资源对象的 nodeName 字段。 更新 API 服务器：Pod 状态会更新为 Scheduled，并指明将该 Pod 调度到的节点。 步骤 4：执行调度 # 一旦 Pod 被绑定到节点，kubelet 会开始在该节点上执行 Pod 的启动操作，具体步骤如下：\nkubelet 启动容器：kubelet 通过容器运行时（如 Docker、containerd 等）启动 Pod 中的容器。 Pod 运行：Pod 在节点上运行，kubelet 会监控容器的状态，并将 Pod 的健康状态报告给 API 服务器。 Pod 健康检查：如果 Pod 配置了探针（liveness、readiness），kubelet 会定期执行探针，确保 Pod 健康。 3. Scheduler 插件体系 # Kubernetes Scheduler 是一个高度可扩展的组件，可以通过插件化的方式进行扩展。调度过程中的很多步骤可以通过不同的插件来实现，常见的插件包括：\n过滤插件（Filter Plugin）：用于过滤不符合条件的节点。比如，如果节点没有足够的 CPU 或内存资源，Pod 就不能被调度到该节点。 优选插件（Score Plugin）：对节点进行评分，选择最合适的节点。 Preemption 插件：当集群资源不足时，Preemption 插件会通过抢占低优先级 Pod 来释放资源供高优先级 Pod 使用。 4. 调度策略 # 调度器使用不同的策略来优化调度：\nRound-robin 调度策略：当集群资源分布均匀时，调度器可能使用 round-robin 算法来轮流将 Pod 分配到不同的节点。 资源分配优化：调度器通过均衡集群资源的使用来避免过载，并确保 Pod 调度到合适的节点上，最大限度地提高资源利用率。 5. 总结 # Kubernetes Scheduler 是 Kubernetes 中非常重要的组件，负责将 Pod 调度到集群中的合适 Node 上运行。调度过程包括从监听待调度的 Pod、根据节点资源和调度策略选择节点、将 Pod 绑定到节点以及最终通过 kubelet 启动 Pod 的过程。调度器还支持通过插件扩展调度功能，并提供灵活的调度策略来优化集群资源利用。\nHPA怎么实现的☆ # Horizontal Pod Autoscaler (HPA) 是 Kubernetes 中的一个自动化组件，它根据负载（如 CPU、内存使用率等）自动扩展或缩减 Pod 的数量。HPA 可以确保应用在负载增加时能够自动扩展 Pod 副本数，在负载减少时自动缩减，从而保持系统的高效运行。\n1. HPA 主要功能 # 自动扩展（Scaling Up）：当应用负载（如 CPU 使用率、内存使用率等）超过设定的阈值时，HPA 会增加 Pod 的副本数。 自动缩减（Scaling Down）：当应用负载低于设定的阈值时，HPA 会减少 Pod 的副本数。 资源监控：HPA 定期检查应用的资源使用情况（CPU/内存等）并做出扩缩容决策。 2. HPA 工作原理 # HPA 的实现过程主要依赖于 Metrics Server 提供的指标数据（如 CPU 使用率、内存使用量等），并通过比较实际指标与目标指标来决定是否需要扩容或缩容。\n2.1 HPA 组件 # Metrics Server：HPA 依赖于 Metrics Server，它收集 Kubernetes 集群中的资源使用数据（例如 CPU 和内存使用量），并将这些数据提供给 HPA。 HPA 控制器：HPA 控制器在 Kubernetes 中运行，它会定期获取集群中 Pod 的资源指标，并根据这些指标自动调整 Pod 副本的数量。 2.2 HPA 的工作流程 # 创建 HPA 资源： 用户创建一个 HPA 资源，并指定目标应用的资源类型（如 CPU 使用率）以及目标值。例如，目标是当 CPU 使用率达到 50% 时，自动扩容 Pod 副本。\n示例：\napiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: my-app-hpa spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: my-app minReplicas: 2 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 50 scaleTargetRef：指定了要自动扩展的资源对象（通常是一个 Deployment）。 minReplicas 和 maxReplicas：设置 Pod 副本数的最小值和最大值，HPA 会在这个范围内进行扩缩容。 metrics：指定需要监控的资源类型，这里监控的是 CPU 使用率，并且目标是达到 50% 的平均 CPU 使用率。 获取资源使用指标： HPA 控制器会定期从 Metrics Server 获取集群中各个 Pod 的资源使用数据。Metrics Server 会通过 kubelet 获取每个节点的资源数据并上报给 API 服务器，然后 HPA 控制器可以查询这些数据。\n计算当前负载与目标负载的差异： HPA 控制器会计算当前 Pod 的资源使用情况（例如，CPU 使用率），并与配置文件中的目标值进行比较。例如，如果目标 CPU 使用率是 50%，而当前使用率是 70%，HPA 控制器就会启动扩容操作。\n计算需要的副本数： 基于当前的资源使用情况和目标值，HPA 会计算需要多少个副本才能使资源使用达到目标。例如，若 CPU 使用率为 70%，目标是 50%，则 HPA 会根据这个比例来决定是否增加 Pod 副本数。\n公式：\ndesiredReplicas = currentReplicas * (currentUsage / targetUsage) 如果计算出的副本数超出了 maxReplicas，那么副本数会被限制在 maxReplicas；如果小于 minReplicas，副本数会被限制在 minReplicas。\n调整副本数： HPA 控制器会根据计算的副本数修改 Deployment、ReplicaSet 或 StatefulSet 的 replicas 字段，Kubernetes 会根据这个变更开始扩容或缩容 Pod。\n2.3 HPA 的触发周期 # 每 15 秒检查一次：默认情况下，HPA 控制器每 15 秒检查一次资源指标，并基于当前负载情况决定是否扩展或缩减 Pod 副本数。 计算周期：每次检查时，HPA 控制器会分析过去 1 分钟内的平均负载。HPA 会根据这个时间窗口的负载来做出决策。 3. HPA 扩展和缩减的决策 # HPA 会基于设定的 metrics 和 目标值 来做出扩展或缩减决策，常见的度量标准有：\nCPU 使用率：HPA 会监控集群中 Pod 的 CPU 使用率，并根据当前使用情况来扩展或缩减 Pod 数量。 内存使用率：除了 CPU 外，内存使用率也是一个常见的衡量标准，虽然默认情况下 HPA 只支持 CPU，但可以通过自定义指标扩展来监控内存或其他资源。 自定义指标：用户还可以使用自定义指标（如来自 Prometheus 等监控工具的数据）来驱动 HPA 扩展 Pod 数量。 扩容（Scaling Up）： # 当负载超过预设目标（例如 CPU 使用率 \u0026gt; 50%）时，HPA 会增加 Pod 数量，以缓解负载。 缩容（Scaling Down）： # 当负载低于预设目标（例如 CPU 使用率 \u0026lt; 50%）时，HPA 会减少 Pod 数量，以节省资源。 4. HPA 相关的限制和注意事项 # Metrics Server 的依赖：HPA 依赖于 Metrics Server 提供的指标数据，必须确保 Metrics Server 正常运行，否则 HPA 无法获取资源使用数据，可能导致无法触发自动扩缩容。 扩缩容延迟：HPA 的调度决策是基于一定时间窗口内的平均负载，因此在负载突增的情况下，可能会有一定的延迟。 最小副本数和最大副本数限制：可以通过 minReplicas 和 maxReplicas 配置来限制 Pod 副本数的最小值和最大值，避免过度扩容或缩容。 不适用于所有类型的负载：HPA 主要针对计算密集型负载（如 CPU 和内存）。对于 I/O 密集型或网络密集型负载，HPA 可能并不总是最佳解决方案。 资源请求和限制：为了使 HPA 正常工作，Pod 必须定义资源的 requests 和 limits，否则 HPA 将无法准确监控 Pod 的负载并进行扩缩容。 5. 总结 # Horizontal Pod Autoscaler (HPA) 是 Kubernetes 的一种自动扩展机制，根据应用的资源负载（如 CPU 使用率、内存使用量等）来自动调整 Pod 的副本数。它通过与 Metrics Server 配合，实时获取资源使用数据，并根据这些数据动态扩展或缩减 Pod 数量，以保证应用的高可用性和资源的高效利用。HPA 可以根据目标的 CPU 使用率、内存使用量等指标来控制副本数，从而自动适应应用的负载变化。\nrequest limit底层是怎么限制的☆ # 在 Kubernetes 中，Pod 的资源限制（request 和 limit）主要是通过 cgroup 来实现的，request 和 limit 的值会直接映射到容器的 cgroup 配置中，控制容器使用的资源（CPU 和内存）。这些限制是在容器运行时通过 容器运行时（如 Docker、containerd 等）和 Linux cgroup 系统实现的。\n1. 请求资源（request）与限制资源（limit）的定义 # request：容器在调度时请求的资源量（CPU 或内存）。它表示 Pod 在集群中调度时需要的最低资源量。如果一个 Pod 没有设置 request，则调度器可能会将 Pod 放置到资源充足的节点上，但不会强制资源保证。 limit：容器能够使用的最大资源量。如果容器请求超过 limit 的资源，Linux 内核会限制容器的资源使用（例如，通过 OOM Killer 来终止过度占用内存的容器）。limit 限制的是容器的资源上限，防止单个容器占用过多资源，影响其他容器。 2. 底层原理：cgroup 和容器运行时 # 容器通过 cgroup（Control Groups）来实现资源的隔离和限制。cgroup 是 Linux 内核的一个特性，用来限制、控制和监视进程的资源使用。\n2.1 cgroup 和 Kubernetes 中的资源限制 # CPU 限制：容器的 request 和 limit 会被转换为 cgroup 中的 CPU 配额。例如，如果容器的 request 设置为 1 CPU，limit 设置为 2 CPU，cgroup 会为该容器分配 1 个 CPU 的请求资源，并且限制它最多只能使用 2 个 CPU 的资源。 内存限制：容器的内存 request 和 limit 会对应到 cgroup 的内存子系统（memory），指定容器可用的最小和最大内存。当容器使用的内存超过了 limit，会触发 OOM（Out of Memory）情况，内核会根据 limit 设置对容器进行杀掉或限制。 2.2 容器运行时如何应用 request 和 limit # 容器运行时（如 Docker 或 containerd）负责启动容器，并将 request 和 limit 转换为 cgroup 的配置。具体实现过程如下：\n容器启动时：容器运行时读取 Pod 配置中的 request 和 limit 值，并将其映射到相应的 cgroup 设置中。比如，对于 CPU 限制，Docker 会使用 cgroup 的 cpu.cfs_quota_us 和 cpu.cfs_period_us 来限制 CPU 使用。\nCPU 请求（request）：容器的 request 会被转换为 cpu.cfs_quota_us，表示容器在调度时需要的 CPU 资源。 CPU 限制（limit）：容器的 limit 会被转换为 cpu.cfs_quota_us，表示容器的最大 CPU 配额。 资源管理：\nCPU：容器的 CPU 使用会受到 cgroup 的管理，容器的 CPU 使用会被限制在指定的配额范围内。 内存：内存限制是通过 cgroup 的 memory.limit_in_bytes 来实现的。如果容器请求的内存超过 limit，容器将被杀死（OOM）。 容器资源超限：\n当容器超出\nlimit 资源时，Linux 内核会根据资源类型采取不同的策略：\nCPU 限制：如果容器使用的 CPU 超过了设置的 limit，容器会被限制只能使用一定量的 CPU 时间，无法进一步占用更多 CPU。 内存限制：如果容器的内存使用超出了 limit，Linux 内核会通过 OOM（Out Of Memory）机制来终止该容器，以保证系统的稳定性。 2.3 Linux Cgroup 实现方式 # 在 Linux 内核中，资源限制是通过 cgroup（控制组）实现的。cgroup 用于限制、控制和监控进程的资源使用，包括 CPU、内存、磁盘 I/O 等。\nCPU 限制：cgroup 通过 cpu.cfs_quota_us 和 cpu.cfs_period_us 设置 CPU 配额。 cpu.cfs_period_us：设置 CPU 周期的总时长（默认是 100ms）。 cpu.cfs_quota_us：设置容器在周期内允许使用的最大时间（默认是 -1，表示没有限制）。比如，如果设定为 50000 微秒，容器每个周期只能使用 50ms 的 CPU。 内存限制：cgroup 使用 memory.limit_in_bytes 来限制容器的最大内存使用量。如果容器的内存超过了 limit，内核会触发 OOM（Out of Memory）机制，杀死进程。 3. 调度器如何使用 request 和 limit # 在 Kubernetes 中，调度器会根据 Pod 的 request 来选择合适的节点进行调度。调度器会确保节点有足够的资源（CPU、内存）来满足 Pod 的 request，同时避免超出节点的资源限制。\nrequest：调度时参考 Pod 的 request，确保节点有足够的资源来调度 Pod。request 是保证 Pod 最低可以获得的资源。 limit：limit 不会影响调度过程，但会影响容器的资源使用。容器的实际资源使用不能超过 limit，否则会被操作系统限制或杀死。 4. 总结 # 在 Kubernetes 中，request 和 limit 是通过 Linux cgroup 来实现资源控制的：\nrequest 表示容器请求的最低资源量，会影响调度时选择节点的过程。 limit 表示容器可以使用的最大资源量，超过这个限制会导致容器被限制或杀死。 cgroup 是实现这些限制的底层机制，它通过限制 CPU 配额、内存限制等来控制容器的资源使用。 通过 request 和 limit，Kubernetes 提供了一个灵活的资源管理机制，确保容器在集群中的资源使用是可控的，并且能够避免资源过度消耗或浪费。\nhelm工作原理是什么？ # Helm 是 Kubernetes 上的一个包管理工具，类似于 Linux 上的 apt 或 yum，用来简化 Kubernetes 应用的安装、管理和发布过程。Helm 主要通过 Charts 和 Releases 来管理 Kubernetes 应用，能够有效地打包、配置、发布和维护 Kubernetes 资源。\nHelm 的核心组件 # Chart：\nChart 是 Helm 的核心，代表一个可以部署的应用包。它是一组 Kubernetes YAML 文件的集合，用于描述 Kubernetes 中一个应用所需的所有资源。Chart 包含了部署应用所需的所有配置文件（如 Deployments、Services、ConfigMaps、Secrets 等）。\nChart\n可能包含：\ntemplates/ 目录：包含 Kubernetes 资源的模板文件，支持使用 Helm 的模板语法进行变量替换和控制逻辑。 values.yaml 文件：定义默认的配置值，用于替换模板中的变量。 charts/ 目录：存放依赖的其他 Charts。 Chart.yaml 文件：包含 Chart 的元数据（如名称、版本、描述等）。 templates/ 目录下的模板会被渲染成具体的 Kubernetes 资源定义。 Release：\nRelease 是 Helm 安装 Chart 的一个实例。当你使用 Helm 安装一个 Chart 时，Helm 会创建一个 Release，将 Chart 配置值与 Kubernetes 集群中的资源进行实际的绑定。 每个 Helm Release 都有一个名称和版本号，可以在同一集群中多次安装同一个 Chart，生成不同的 Release 实例。 Helm 仓库：\nHelm 仓库是 Chart 的存储库，用来发布和分享 Chart 包。常见的 Helm 仓库包括官方的 Helm 仓库和第三方仓库。Helm 可以从这些仓库中拉取 Charts 进行安装。 Helm 工作原理 # 1. 安装 Chart # Helm 安装 Chart 的过程可以分为以下几个步骤：\n初始化 Helm 客户端： Helm 客户端会与 Kubernetes API 服务器交互。安装和配置 Kubernetes 上的 Helm Tiller（Helm v2 中使用的服务端组件）或者在 Helm v3 中，客户端本身直接与 Kubernetes API 交互。 获取 Chart： Helm 可以从本地目录或远程 Helm 仓库获取 Chart。Helm 也支持 Chart 的版本管理。 渲染模板： Helm 使用模板引擎（基于 Go 模板）渲染 Chart 中的 Kubernetes 配置文件（如 Deployments、Services 等）。在渲染过程中，Helm 会根据 values.yaml 文件中的默认值，或者通过 --set 参数传入的自定义值，来替换模板文件中的变量。 安装资源： 渲染后的 Kubernetes 配置文件会被应用到 Kubernetes 集群中，Helm 会使用 kubectl 或 Kubernetes API 提交这些资源，创建应用的实际部署。 创建 Release： 当 Helm 成功安装应用后，会创建一个 Release，每个 Release 都会有一个唯一的名称和版本号。这个 Release 记录了 Chart 配置、版本信息以及安装的资源对象。 2. 升级和回滚 # 升级： Helm 允许你通过修改 values.yaml 或直接使用 --set 来修改 Chart 配置。当你更新 Chart 或配置时，可以使用 helm upgrade 命令进行应用升级。Helm 会通过对比新旧版本，生成更新的资源定义，并应用到 Kubernetes 集群中。 回滚： 如果升级后的应用出现问题，Helm 还支持通过 helm rollback 命令将应用回滚到先前的版本。Helm 会重新应用旧版本的配置和资源，恢复到先前的状态。 3. 卸载应用 # 使用 helm uninstall 命令可以删除已经安装的 Release，Helm 会删除与该 Release 相关的所有 Kubernetes 资源（如 Pods、Services、Deployments 等）。\n4. 依赖管理 # Helm 支持通过 charts/ 目录管理 Chart 依赖。一个 Chart 可以依赖其他 Chart，这样可以实现 Chart 的模块化管理。Helm 会在安装时自动下载和安装 Chart 的依赖包。\nHelm 的模板渲染 # Helm 使用 Go 模板语法渲染 Kubernetes 资源的 YAML 文件。模板语法允许你定义变量、条件语句、循环等，从而使得 Helm 配置更加灵活和动态。\n常用的模板语法有：\n{{ .Release.Name }}：获取当前 Release 的名称。 {{ .Values.someValue }}：获取 values.yaml 文件中的某个值。 {{ if .Values.enabled }} ... {{ end }}：条件判断，只有当某个值为 true 时才渲染某些资源。 {{ range .Values.items }} ... {{ end }}：循环渲染多个资源。 Helm 的优势 # 简化 Kubernetes 部署： Helm 通过 Charts 模块化部署应用，减少了大量手动创建 Kubernetes 资源配置文件的工作。 易于版本管理： Helm 允许你管理应用的多个版本，可以轻松地升级、回滚和恢复应用。 参数化配置： Helm 允许你使用模板和 values.yaml 文件来动态配置 Kubernetes 资源，使得应用的配置可以灵活适配不同环境。 依赖管理： Helm 支持 Chart 之间的依赖关系，简化了应用的复用和集成。 应用发布和共享： Helm 使得 Chart 的共享和复用变得更加容易，你可以使用公共的 Helm 仓库或自己搭建私有仓库，方便团队或社区共享应用。 Helm 与 Kubernetes 资源的关系 # Helm 主要通过将 Chart 渲染为 Kubernetes 配置文件（如 Deployment、Service 等）来管理应用。它并不会替代 Kubernetes 原生的资源管理，而是简化了 Kubernetes 应用的配置、安装、升级和卸载等操作。Helm 通过模板化和参数化的方式，提供了一个强大的工具，帮助开发和运维团队更高效地管理 Kubernetes 上的应用。\n总结 # Helm 是 Kubernetes 的包管理工具，通过 Charts、Releases 和 Helm 仓库帮助用户简化应用的安装、配置、发布和管理。 Chart 是 Helm 包的核心，包含 Kubernetes 资源的模板和配置文件。 Release 是 Chart 的一个具体实例，在 Kubernetes 集群中创建和管理。 Helm 使用模板引擎将变量渲染为实际的 Kubernetes 资源配置，并通过 kubectl 提交到 Kubernetes 集群。 Helm 还提供了版本管理、升级、回滚、依赖管理等功能。 通过 Helm，用户可以更加方便地管理和发布 Kubernetes 应用，增强了应用的可移植性和可管理性。\nhelm chart rollback实现过程是什么？ # 在 Helm 中，rollback 是一种回滚操作，它允许用户将一个已安装的 Helm Release 恢复到先前的某个版本。这在应用升级后出现问题时非常有用，可以快速将应用恢复到一个已知的稳定状态。\nHelm Chart Rollback 实现过程 # Helm 的 rollback 操作依赖于 Helm Release 的版本管理机制。每次通过 Helm 安装、升级或修改一个 Release 时，Helm 都会为该 Release 创建一个新的版本，并将相关的 Kubernetes 资源状态记录到 Helm 的内部存储（称为 Helm 存储库）。通过 rollback 命令，用户可以选择恢复某个历史版本。\n下面是 Helm Chart rollback 的实现过程：\n1. Helm Release 的版本管理 # Helm 会为每个安装或升级的 Release 创建一个唯一的版本号，并将每个版本的 Kubernetes 资源的当前状态保存在 Helm 的存储中。每次 Helm 操作（安装、升级、修改等）都会生成一个新的 Release 版本。\nRelease Version：每次执行 helm upgrade 或 helm install 时，Helm 会自动增加 Release 的版本号，并在内部存储中保存该版本对应的 Kubernetes 资源清单（如 Deployment、Service 等）。 Helm 会将 Release 的版本存储在 Kubernetes 集群中的 ConfigMap 或 Secret 中，具体存储方式取决于 Helm 配置。 2. 执行 Rollback 操作 # 当我们执行 helm rollback 命令时，Helm 会：\n读取 Release 的历史版本： helm rollback 命令指定的版本号或默认的上一个版本，Helm 会从内部存储中找到该版本对应的 Kubernetes 资源清单。 Helm 默认会回滚到最近的一个版本（通过 --retries 参数或指定具体版本号），如果没有指定版本号，则会回滚到上一个版本。 生成新版本的 Kubernetes 资源： Helm 会从历史版本中提取之前的 Kubernetes 资源清单，这些清单包括之前版本的 Deployment、Service、ConfigMap、Secrets 等。 Helm 会将这些资源清单再次渲染并提交到 Kubernetes 集群。渲染时，Helm 会使用模板文件和 values.yaml 配置文件，确保这些资源的配置与历史版本一致。 应用历史版本的资源配置： 生成的 Kubernetes 资源将通过 Kubernetes API 被应用到集群中，从而恢复到历史版本的状态。 Kubernetes 会根据资源定义执行相应的创建、更新或删除操作。如果版本回滚导致 Kubernetes 资源（例如 Deployment）发生变更，Kubernetes 会执行 滚动更新（Rolling Update）操作，逐步用历史版本替换当前运行中的 Pods。 更新 Release 版本： 一旦 Kubernetes 集群成功应用了回滚的资源配置，Helm 会将 Release 的版本号更新为新版本。 这个新版本的 Release 包含了回滚后的配置，确保 Helm 能够正确跟踪回滚后的版本。 确认回滚结果： Helm 会返回一个成功回滚的信息，表示已经将应用恢复到之前的状态。 3. Helm Rollback 中的关键操作 # 版本回滚：Helm rollback 会从内部存储中获取某个版本的 Kubernetes 资源定义（包括渲染后的 YAML），并将其应用到集群中。 资源替换：Helm 会根据历史版本的资源清单进行更新，更新的过程可能涉及到 Deployment 的滚动更新、ConfigMap 和 Secrets 的替换等操作。 状态恢复：Helm 会将 Kubernetes 资源的实际状态恢复为历史版本的状态，包括镜像版本、环境变量、配置文件等。 4. helm rollback 示例 # 假设我们有一个名为 my-release 的 Helm Release，当前安装版本为 3，并且想将其回滚到版本 2。可以执行以下命令：\nhelm rollback my-release 2 这个命令会将 my-release 的 Release 配置恢复到版本 2，Helm 会从 Kubernetes 集群中获取并恢复版本 2 时的所有资源配置。\n如果没有指定版本号，Helm 会默认回滚到上一个版本（即版本 2）：\nhelm rollback my-release 5. RollBack 时的注意事项 # 数据丢失：如果回滚涉及到应用的数据库或持久化存储，可能会导致数据丢失或状态不一致，因此在回滚前一定要评估数据的状态。 时间窗口：回滚操作可能会影响应用的可用性，尤其是当容器中的数据未持久化时。可以通过 Kubernetes 的 Deployment 滚动更新机制来减少影响。 回滚到合适的版本：如果在回滚操作后出现问题，可以继续回滚到更早的版本，Helm 支持无限回滚，但每次回滚都需要检查资源和应用状态。 6. Helm 回滚工作原理总结 # Helm 通过在每次安装或升级时保存资源配置清单，并将其记录为一个版本。 helm rollback 会通过回滚操作恢复历史版本的 Kubernetes 资源清单。 回滚操作通过替换当前资源来恢复应用的状态。 Kubernetes 使用滚动更新的方式平滑地替换 Pods，确保系统稳定运行。 通过 helm rollback，你可以轻松恢复 Helm Release 到一个健康的历史状态，避免因应用升级失败或其他问题导致的服务中断。\nvelero备份与恢复流程是什么 # Velero 是一个开源工具，用于备份和恢复 Kubernetes 集群中的资源和持久化数据。它支持备份 Kubernetes 集群中的所有资源，包括应用、配置、Secrets、Persistent Volume（PV）等，支持跨集群恢复。\nVelero 的备份和恢复过程通常分为以下几步：\n1. Velero 架构概述 # Velero 由以下几个主要组件组成：\nVelero 客户端：用于启动备份、恢复和其他管理操作的 CLI 工具。 Velero 服务器：一个在 Kubernetes 集群中运行的控制器，它负责管理和调度备份、恢复任务。它运行在 velero 命名空间中。 存储后端：Velero 使用对象存储（如 AWS S3、Google Cloud Storage、Azure Blob Storage、MinIO 等）来存储备份文件。 Backup和Restore：Backup 是一个操作对象，描述备份任务的配置，而 Restore 则描述从备份中恢复的操作。 2. Velero 备份流程 # Velero 的备份流程分为以下几个步骤：\n步骤 1：安装 Velero # 首先，需要安装并配置 Velero。Velero 会与 Kubernetes API 服务器交互，通过控制器进行备份和恢复操作。\n配置存储后端（如 S3、MinIO、GCS 等），并创建凭证用于访问存储。 使用 Helm 或 kubectl 安装 Velero 到 Kubernetes 集群中。 配置 Velero 连接到存储后端和集群。 安装命令（以 Helm 为例）：\nhelm install velero vmware-tanzu/velero --namespace velero --set configuration.provider=aws --set configuration.backupStorageLocation.bucket=\u0026lt;your-bucket\u0026gt; --set configuration.backupStorageLocation.config.region=\u0026lt;your-region\u0026gt; 步骤 2：执行备份 # 备份所有资源：使用 velero backup create 命令可以创建一个备份，备份将包含整个集群的所有资源，包括命名空间、Pod、服务、部署、配置映射等。还可以选择指定某些资源或命名空间。 velero backup create \u0026lt;backup-name\u0026gt; --include-namespaces \u0026lt;namespace1\u0026gt;,\u0026lt;namespace2\u0026gt; --include-namespaces：指定要备份的命名空间。如果不指定，默认备份所有命名空间。 --include-resources：指定要备份的特定资源。 --exclude-resources：指定要排除的资源。 --snapshot-volumes：指定是否备份持久化卷（PV）的快照。 备份策略\n：\n资源备份：包括 Kubernetes 的 API 资源（Pod、Deployment、Service、ConfigMap、Secret 等）。 持久化卷备份：如果有持久化存储（例如 PV 和 PVC），Velero 会根据存储提供者的支持情况，创建快照并将其备份。 备份检查：备份任务会开始执行，Velero 会监控任务的状态。可以使用以下命令查看备份的状态：\nvelero backup describe \u0026lt;backup-name\u0026gt; --details 备份完成：一旦备份成功完成，备份数据就会存储在指定的对象存储中。你可以随时根据需要恢复数据。 步骤 3：定期备份 # Velero 还支持使用 CronJob 定期备份集群。你可以使用 Velero 的 CronBackup 功能指定定期的备份任务。\nvelero backup create \u0026lt;backup-name\u0026gt; --schedule \u0026#34;0 2 * * *\u0026#34; 3. Velero 恢复流程 # 恢复是将之前备份的数据恢复到 Kubernetes 集群中，恢复的过程包括资源恢复和持久化数据恢复。\n步骤 1：创建恢复任务 # 使用 velero restore create 命令来创建恢复任务。你可以选择恢复整个备份，也可以选择恢复部分命名空间、资源等。 velero restore create --from-backup \u0026lt;backup-name\u0026gt; --from-backup：指定要恢复的备份名称。 --namespace-mappings：如果恢复时需要将备份中的某些命名空间映射到目标集群的不同命名空间，可以使用此选项。 --include-resources 和 --exclude-resources：指定要恢复的具体资源。 步骤 2：查看恢复状态 # 恢复过程会创建一个新的 Restore 资源，Velero 会根据备份中的资源和配置逐个恢复。这些恢复任务会在 Kubernetes 集群中逐步应用。\n查看恢复状态：\nvelero restore describe \u0026lt;restore-name\u0026gt; --details 步骤 3：恢复持久化数据 # 如果备份中包括持久化卷，Velero 会在恢复时尝试从对象存储中恢复快照并创建相应的 Persistent Volume 和 Persistent Volume Claim。\n如果集群中的持久化存储与原集群不同，可能需要手动调整存储类（StorageClass）或更改 PVC 配置。 恢复完成后，应用的资源和持久化数据会被恢复到 Kubernetes 集群中。\n4. Velero 的增量备份和恢复 # Velero 支持增量备份，这意味着它只会备份自上次备份以来发生更改的资源和数据。因此，Velero 不需要每次都对整个集群进行全量备份，可以减少备份的时间和存储空间。\n增量备份的原理：\nVelero 会记录每个备份点的时间戳，并在后续备份中仅备份自上次备份之后有变化的资源。 增量恢复：在恢复时，Velero 会从最近的备份恢复数据，并根据需要补充任何增量变化。\n5. Velero 的高级特性 # 跨集群备份和恢复：Velero 支持将备份从一个集群恢复到另一个集群。例如，你可以在灾难恢复场景中使用 Velero 将应用从一个集群恢复到另一个集群。 备份和恢复资源与持久化数据：Velero 支持不仅备份 Kubernetes 资源（如 Pods 和 Services），还可以备份持久化数据（如 Persistent Volumes）。 存储加密：Velero 支持在对象存储中加密备份数据，确保备份的安全性。 6. 常见命令汇总 # 备份\n：\nvelero backup create \u0026lt;backup-name\u0026gt; --include-namespaces \u0026lt;namespace1\u0026gt;,\u0026lt;namespace2\u0026gt; --snapshot-volumes 查看备份状态\n：\nvelero backup describe \u0026lt;backup-name\u0026gt; --details 恢复\n：\nvelero restore create --from-backup \u0026lt;backup-name\u0026gt; 查看恢复状态\n：\nvelero restore describe \u0026lt;restore-name\u0026gt; --details 删除备份\n：\nvelero backup delete \u0026lt;backup-name\u0026gt; 删除恢复任务\n：\nvelero restore delete \u0026lt;restore-name\u0026gt; 7. 总结 # Velero 是 Kubernetes 集群备份和恢复的重要工具，它提供了灵活的备份、恢复、增量备份、持久化数据备份等功能，适用于生产环境中的灾难恢复、跨集群迁移等场景。使用 Velero，管理员可以简化备份操作，确保 Kubernetes 集群的可靠性和数据的安全性。\ndocker网络模式 # Docker 提供了多种网络模式，以支持不同的容器化网络需求。每种网络模式都有其适用场景，选择合适的网络模式能优化容器间的通信、网络隔离及安全性。Docker 中主要的网络模式有以下几种：\n1. Bridge 网络模式 # 简介：默认的网络模式，适用于单机 Docker 环境中的容器。所有容器都连接到同一个虚拟网络桥（docker0），并通过桥接交换数据。\n特点：\n容器间可以通过 IP 地址互相访问。 通过端口映射，可以使容器暴露到主机上。 容器默认会分配一个内部 IP 地址，可以通过端口映射将其暴露给外部。 使用场景：\n默认的容器网络模式。 适用于需要容器之间隔离但需要主机间访问的场景。 命令示例：\ndocker network create --driver bridge my-bridge-network 2. Host 网络模式 # 简介：容器直接使用宿主机的网络栈，而不创建一个独立的虚拟网络接口。容器不会被分配私有 IP 地址，而是直接与宿主机共享 IP。\n特点：\n容器与宿主机共享网络资源，容器的网络与宿主机一致。 不需要端口映射，容器的端口即是宿主机的端口。 网络性能较好，因为容器与宿主机共享网络。 使用场景：\n对于需要高性能网络通信的容器，尤其是需要直接访问宿主机网络的场景。 一些需要运行在宿主机上的服务，比如负载均衡器和代理服务。 命令示例：\ndocker run --network host my-image 3. None 网络模式 # 简介：容器没有任何网络连接。容器没有任何网络接口，也无法与其他容器或外部网络通信。\n特点：\n适用于需要完全网络隔离的容器。 容器内的进程不能访问外部网络。 适用于需要自定义网络配置的场景，可以在容器内手动配置网络。 使用场景：\n需要完全隔离的容器。 容器内部需要配置静态 IP 地址或运行特殊的网络应用。 命令示例：\ndocker run --network none my-image 4. Container 网络模式 # 简介：容器共享另一个容器的网络命名空间，即两个容器共享相同的网络接口、IP 地址和端口。网络模式是通过 --network container:\u0026lt;container-name\u0026gt; 指定的。\n特点：\n共享网络接口，容器之间使用相同的 IP 地址。 适用于需要紧密耦合的容器应用，它们通过相同的网络环境协同工作。 使用该模式时，容器之间的网络非常直接，不需要额外的端口映射。 使用场景：\n需要共享网络接口的容器，通常用于多个容器共同提供某一服务（如主从数据库架构）时。 命令示例：\ndocker run --network container:\u0026lt;other-container-name\u0026gt; my-image 5. Overlay 网络模式 # 简介：用于跨多个 Docker 主机的容器间网络通信。Overlay 网络通过 Docker 的 Swarm 模式或其他集群管理工具（如 Kubernetes）来连接不同主机上的容器，使它们仿佛在同一个网络中。\n特点：\n在多主机环境下创建虚拟的网络，允许容器跨主机通信。 Overlay 网络使用 VXLAN 技术，在物理网络之上创建一个虚拟的网络。 需要 Docker Swarm 或其他容器编排工具支持。 使用场景：\n适用于需要跨主机通信的容器，尤其是在使用 Docker Swarm 或 Kubernetes 等容器编排工具时。 命令示例：\ndocker network create --driver overlay my-overlay-network 6. Macvlan 网络模式 # 简介：Macvlan 网络模式使每个容器都拥有自己的 MAC 地址，容器通过物理网络直接与外部通信。每个容器都像一个物理主机，具有自己的 IP 地址。\n特点：\n容器直接与物理网络连接，具有独立的 MAC 地址和 IP 地址。 适用于需要容器与外部网络（如企业网络）无缝连接的场景。 需要配置宿主机的网络接口和桥接。 使用场景：\n容器需要直接暴露给外部网络。 适用于一些企业环境，尤其是对网络隔离和控制要求较高的场景。 命令示例：\ndocker network create -d macvlan --subnet=192.168.1.0/24 --gateway=192.168.1.1 -o parent=eth0 my-macvlan-network 7. IPvlan 网络模式 # 简介：与 Macvlan 类似，IPvlan 允许容器具有独立的 IP 地址，但不同的是，IPvlan 工作在数据链路层以下，容器和宿主机共享一个 MAC 地址。IPvlan 支持更高效的网络管理。\n特点：\n提供与 Macvlan 类似的网络隔离功能，但在某些环境下效率更高。 支持多种模式，如 L2 和 L3 模式，可以在不同的应用场景下选择。 使用场景：\n在一些需要高性能且不希望每个容器都占用独立 MAC 地址的场景中，IPvlan 是一个不错的选择。 命令示例：\ndocker network create -d ipvlan --subnet=192.168.1.0/24 --gateway=192.168.1.1 -o parent=eth0 my-ipvlan-network 8. Docker 网络模式总结 # 网络模式 特点 使用场景 Bridge 默认模式，容器与宿主机隔离，通过端口映射进行通信。 单机部署，默认的容器网络模式。 Host 容器共享宿主机的网络栈，无需端口映射。 高性能网络需求或直接访问宿主机网络的场景。 None 容器没有网络连接，完全隔离。 完全隔离的容器，通常用于特殊配置。 Container 容器共享另一个容器的网络命名空间。 紧密耦合的容器，通常用于多个容器共同提供服务的场景。 Overlay 跨主机容器通信，通常在 Docker Swarm 或 Kubernetes 中使用。 多主机部署，跨主机容器通信。 Macvlan 每个容器都有自己的 MAC 地址，直接与物理网络通信。 容器直接暴露给外部网络，通常用于企业环境。 IPvlan 容器共享宿主机的 MAC 地址，工作在 L2 和 L3 层。 高效的网络隔离，适用于大规模环境。 选择合适的网络模式可以帮助容器化应用在性能、安全性和网络隔离上达到最佳效果。\ndocker和container区别☆ # Docker 和 容器（Container）是紧密相关的概念，但它们的含义和作用有所不同。下面是这两者的区别：\n1. 容器（Container） # 定义：容器是一种轻量级、可移植的虚拟化技术，它通过操作系统级别的虚拟化（而非硬件虚拟化）来隔离应用及其依赖项，确保它们可以在任何环境中运行而不受外部环境影响。 特点： 隔离性：容器提供应用的隔离环境，但与虚拟机（VM）不同，它们不需要运行整个操作系统，而是共享宿主机的操作系统内核。 轻量级：容器通常比虚拟机更小，启动更快，因为它们不包含完整的操作系统。 便携性：容器包含应用及其依赖，能够在任何支持容器的环境中运行，提供了一种“构建一次，随处运行”的方式。 资源共享：容器使用宿主机的内核，并通过隔离机制（如命名空间、cgroup）来保证容器之间的资源隔离。 容器的使用场景： 微服务架构：每个微服务都可以运行在不同的容器中，且彼此之间隔离。 自动化部署：容器可以在任何地方启动应用，极大简化了应用的部署和迁移。 CI/CD 流水线：容器可用于持续集成和持续交付，确保在不同环境中一致运行。 2. Docker # 定义：Docker 是一个开源的容器化平台，用于开发、打包、部署和运行应用程序。它简化了容器的创建、部署和管理，提供了一整套工具链。 特点： 容器管理工具：Docker 提供了方便的命令行工具和图形化界面，可以轻松创建、运行和管理容器。 镜像管理：Docker 通过镜像来打包应用及其依赖，Docker Hub 提供了官方和用户贡献的镜像库，用户可以方便地拉取镜像或将自己的镜像推送到镜像库。 Docker 引擎：Docker 引擎是容器的运行时，负责容器的生命周期管理，包括容器的创建、启动、停止、销毁等。 容器编排：Docker 支持容器的编排功能，尤其是通过 Docker Swarm 和 Docker Compose，使得多容器部署和管理变得更容易。 Docker 的使用场景： 在本地开发和测试：开发人员使用 Docker 容器在本地构建、运行、测试和调试应用。 在生产环境中运行应用：Docker 容器能够快速启动和停止，非常适合高效、可扩展的生产环境。 容器编排：使用 Docker Compose 或 Docker Swarm 来管理多容器的应用部署。 3. Docker 与容器的区别 # 对比项 容器（Container） Docker 定义 一种轻量级、可移植的应用封装和运行环境。 一个容器化平台，提供创建、部署和管理容器的工具。 作用 提供应用的隔离环境，确保应用的一致性和便携性。 提供容器管理和自动化工具，帮助开发和运维人员管理容器。 技术层面 容器是虚拟化技术的一种实现方式，依赖于宿主机的内核。 Docker 是容器化的实现工具，使用容器技术管理应用。 使用范围 容器可以由任何容器引擎（如 Docker、Podman）管理。 Docker 是目前最流行的容器引擎，提供完整的容器生命周期管理。 容器管理 容器本身只是一个封装应用的单元，不提供容器生命周期管理。 Docker 提供从创建、部署到运行和销毁容器的一站式管理功能。 镜像管理 容器本身是基于镜像启动的，但容器管理工具需要提供镜像的创建、推送和拉取功能。 Docker 提供镜像构建、发布和拉取工具（如 Dockerfile 和 Docker Hub）。 4. 总结 # 容器 是一种虚拟化技术，用于隔离和运行应用程序，而 Docker 是一种具体的容器化平台，提供了完整的工具链来创建、管理和部署容器。 Docker 作为最流行的容器引擎，简化了容器的使用，极大地推动了容器技术的普及和发展。 如何减⼩dockerfile⽣成镜像体积？ # 在 Dockerfile 中生成镜像时，镜像体积的大小是一个很重要的考虑因素。减小镜像体积不仅可以提高镜像的拉取速度和部署效率，还能减少存储空间的消耗。以下是一些常见的优化方法，可以有效减小 Docker 镜像的体积：\n1. 选择合适的基础镜像 # 使用 精简版的基础镜像，如 alpine，它比传统的 ubuntu、debian 等镜像更小，只有几 MB。\n如果不需要完整的操作系统功能，可以选择 更小的镜像，例如：\nalpine: 体积小，适用于大多数轻量级应用。 busybox: 更小，适用于非常精简的环境。 debian:slim 或 ubuntu:20.04 的精简版本。 FROM alpine:3.14 2. 减少不必要的文件 # 通过 .dockerignore 文件排除不必要的文件和目录，如构建文件、文档、测试文件等，避免将其添加到镜像中。\n.git node_modules test/ *.md 在构建时确保 不将临时文件或无用文件 添加到镜像。\n3. 合并多个 RUN 命令 # Dockerfile 中的每一条 RUN 命令都会生成一个新的镜像层，减少层数 可以有效减小镜像体积。将多个相关的 RUN 命令合并成一个，可以减少中间层的数量。\n使用 \u0026amp;\u0026amp; 将多个命令链接在一起，避免每个命令产生新的层。\nRUN apt-get update \u0026amp;\u0026amp; \\ apt-get install -y \\ curl \\ vim \u0026amp;\u0026amp; \\ rm -rf /var/lib/apt/lists/* 4. 清理临时文件和缓存 # 安装软件包后，要 清理缓存 和 临时文件，如包管理器缓存、日志文件等，以减小镜像体积。\n对于 APT 包管理器（如 Ubuntu 和 Debian）：\nRUN apt-get update \u0026amp;\u0026amp; \\ apt-get install -y curl \u0026amp;\u0026amp; \\ apt-get clean \u0026amp;\u0026amp; \\ rm -rf /var/lib/apt/lists/* 对于 YUM（如 CentOS 或 RHEL）：\nRUN yum install -y curl \u0026amp;\u0026amp; \\ yum clean all \u0026amp;\u0026amp; \\ rm -rf /var/cache/yum 对于 npm：\nRUN npm install \u0026amp;\u0026amp; \\ npm cache clean --force 5. 使用多阶段构建（Multi-stage builds） # 多阶段构建 可以帮助你在构建过程中只保留最终镜像所需的内容，而不包含构建过程中使用的中间文件和工具。例如，可以在第一阶段使用一个较大的构建镜像（如包含编译工具），在第二阶段只复制编译好的产物。\n# 第一阶段：构建阶段 FROM node:14 AS builder WORKDIR /app COPY . . RUN npm install \u0026amp;\u0026amp; npm run build # 第二阶段：运行阶段 FROM node:14-slim WORKDIR /app COPY --from=builder /app/dist /app CMD [\u0026#34;node\u0026#34;, \u0026#34;app.js\u0026#34;] 这样做可以避免将不必要的构建工具和依赖（如 node_modules）包含在最终镜像中。\n6. 避免安装不必要的依赖 # 只安装应用运行所需的最小依赖，避免安装多余的包或工具。使用精简的镜像和最小的依赖集，避免安装开发工具和测试库。 7. 压缩镜像文件 # 对于某些应用，尤其是静态文件（如网页、图片等），可以在容器中使用工具进行压缩，减少文件大小。例如，使用 gzip 压缩文件。\nRUN gzip -r /app/static 8. 减少镜像中的层数 # 通过 合并命令 和 避免多次复制，可以减少镜像的层数，从而减小镜像体积。例如，尽量避免在 Dockerfile 中多次 COPY 同一个目录。\nCOPY . /app 而不是：\nCOPY src /app/src COPY config /app/config 9. 优化文件格式和大小 # 对于应用中包含的二进制文件或压缩文件，使用 优化过的格式 和 最小化的文件版本。例如，使用 strip 删除二进制文件中的调试信息。\nRUN strip /usr/local/bin/my-app 10. 使用 Docker BuildKit 和缓存机制 # 启用 Docker BuildKit，它支持更高效的构建过程并提供更强的缓存机制，能够有效减少镜像的体积。\nDOCKER_BUILDKIT=1 docker build -t myimage . 通过这些优化方法，能够显著减少 Docker 镜像的体积，从而提升构建效率、减少存储和网络带宽的消耗。\nk8s日志采集方案 # 在 Kubernetes 环境中，日志采集是运维和监控的核心组成部分。一个良好的日志采集方案能帮助团队有效地监控、排查故障和审计操作。以下是一些常见的 Kubernetes 日志采集方案，以及如何实现这些方案。\n1. Kubernetes 日志来源 # 容器日志：每个 Pod 中运行的容器都会生成标准输出（stdout）和标准错误（stderr）日志，这些日志是容器运行时日志的主要来源。 节点日志：Kubernetes 节点的系统日志，例如 kubelet、kube-proxy、docker（或 containerd）等相关组件的日志。 应用日志：应用本身生成的日志文件，通常存储在容器内部或者通过外部存储系统（如日志服务器）进行管理。 2. 常见的 Kubernetes 日志采集方案 # 以下是几种常见的日志采集方案，通常通过结合使用多个工具来实现更强大的日志处理能力：\n1. EFK（Elasticsearch, Fluentd, Kibana） # EFK 是 Kubernetes 中最常见的日志采集方案之一。它提供了集中式的日志收集、存储、查询和展示。\nFluentd：Fluentd 是一个开源的日志收集器，负责从各个 Kubernetes 节点（或容器）收集日志数据。它能够将日志数据转发到 Elasticsearch 或其他后端存储系统。 Elasticsearch：Elasticsearch 是一个强大的分布式搜索和分析引擎，存储和索引日志数据。它可以高效地处理海量日志数据，支持快速查询。 Kibana：Kibana 提供了一个 Web 界面，用于可视化 Elasticsearch 中存储的日志数据。用户可以通过 Kibana 来查询、分析和展示日志数据。 工作流程：\nFluentd 收集容器的标准输出和标准错误日志。 Fluentd 将日志转发到 Elasticsearch 存储和索引。 用户通过 Kibana 进行日志查询和可视化展示。 优点：\n强大的搜索和分析能力。 丰富的可视化功能。 高度可扩展。 缺点：\n配置相对复杂，尤其是对于大规模集群。 Elasticsearch 需要大量资源，尤其是存储和计算资源。 2. ELK Stack (Elasticsearch, Logstash, Kibana) # ELK Stack 与 EFK 类似，但使用 Logstash 替代了 Fluentd。\nLogstash：Logstash 负责日志的采集、处理和转发。与 Fluentd 类似，Logstash 也能够从多个数据源收集日志并进行过滤和转换。 Elasticsearch：存储和索引日志数据。 Kibana：用于展示和分析日志数据。 优点：\nLogstash 提供了强大的日志处理能力，能够进行更复杂的数据管道处理。 高度集成的工具链，适合需要复杂日志处理的场景。 缺点：\nLogstash 性能上相对 Fluentd 稍弱，尤其在处理大规模日志数据时，可能会面临瓶颈。 ELK stack 的资源消耗较大。 3. Fluent Bit + Elasticsearch + Kibana（EFK变种） # Fluent Bit 是 Fluentd 的轻量级替代品，适用于资源受限的环境。它的工作原理与 Fluentd 相似，但更专注于高效、低资源消耗。\nFluent Bit：作为日志收集器，负责从 Kubernetes 节点或容器中收集日志。 Elasticsearch：存储和索引日志。 Kibana：可视化展示日志。 优点：\n比 Fluentd 更轻量化，适合资源有限的集群。 配置简单、性能优异。 缺点：\nFluent Bit 相对于 Fluentd 功能较少，定制化和插件支持较弱。 Elasticsearch 资源占用较高。 4. Prometheus + Loki + Grafana（PLG Stack） # 这是一个新的日志采集方案，它依赖于 Prometheus 和 Grafana，并结合 Loki 作为日志收集组件。Loki 是一个由 Grafana Labs 提供的日志聚合系统，专门用于与 Prometheus 和 Grafana 配合使用。\nLoki：Loki 是一个轻量级的日志聚合系统，可以高效地存储和查询日志数据。它的设计灵感来自于 Prometheus，旨在提供简化的日志存储和查询机制。 Prometheus：用于监控和收集集群的指标数据。 Grafana：用于展示日志数据和指标数据，提供统一的可视化面板。 工作流程：\nLoki 从各个节点和容器中收集日志。 Grafana 用于展示日志数据和监控数据，提供统一的视图。 优点：\nLoki 和 Prometheus 都是由 Grafana Labs 提供，且设计理念相近，便于集成。 简化了日志和指标的聚合和展示。 轻量级，适用于资源较小的集群。 缺点：\n相较于 ELK 或 EFK，Loki 在查询和可视化上可能略显不足。 适合以日志为主的轻量级日志收集需求。 5. Loggly, Datadog 等商业化解决方案 # 这些工具提供了即插即用的日志聚合解决方案，并集成了监控、分析、告警等功能。 适合那些没有时间或资源管理自己的日志基础设施的团队。 优点：\n易于部署和使用，通常只需要配置日志代理即可开始收集日志。 提供强大的搜索、分析和可视化功能。 缺点：\n需要付费，长期使用可能会产生较高的成本。 自定义和扩展性有限。 3. 日志采集方案的选型 # 规模较小的集群：如果你的集群较小，且没有复杂的日志分析需求，推荐使用 Fluent Bit + Elasticsearch + Kibana 方案，简单高效。 大规模集群：如果集群较大，且需要强大的日志处理能力，推荐使用 EFK Stack 或 ELK Stack，这些方案提供了灵活的日志处理、存储和可视化能力。 Prometheus 用户：如果你的集群已经使用 Prometheus 进行监控，推荐使用 Loki + Grafana 进行日志收集和展示，能够与 Prometheus 数据源无缝集成。 商业化方案：如果你希望减少运维工作量并快速部署，且愿意为此支付费用，可以选择像 Datadog 或 Loggly 等商业日志管理平台。 4. 日志采集最佳实践 # 集中式日志收集：确保所有节点和容器的日志都通过一个集中的系统收集，以便于管理和分析。 日志保留策略：根据需要设定合适的日志保留策略，不必保留所有日志数据，以节省存储空间。 日志聚合和索引：对于大量日志数据，合理配置聚合和索引策略，避免日志数据过多导致系统性能下降。 日志分析和告警：根据日志内容设置关键字告警，当出现异常时及时通知相关人员，快速响应。 通过采用合适的日志采集方案，能够确保 Kubernetes 环境中的日志管理高效且易于扩展。\nPause容器的用途☆ # 在 Kubernetes 中，Pause 容器 是一种特殊的容器，通常作为 Pod 的占位容器，用于保持 Pod 中的网络命名空间（network namespace）不被销毁。它并不执行任何实际的工作负载，而是作为一个虚拟的容器存在，用来维持 Pod 的状态，特别是在 Pod 需要共享网络命名空间时。Pause 容器本身并不运行任何服务或应用程序，通常用于管理和维护 Pod 的生命周期。\nPause 容器的主要用途 # 网络命名空间的管理 在 Kubernetes 中，Pod 是一个基本的部署单元，Pod 中的所有容器共享相同的网络命名空间（network namespace）。Pause 容器充当了这个网络命名空间的“守护进程”。它本身不执行任何业务逻辑，只是确保 Pod 的网络命名空间持续存在。 它会创建并持有 Pod 的网络命名空间，允许其他容器在同一个命名空间中运行，并与 Pod 内的其他容器通过网络进行通信。 Pod 生命周期的管理 在 Pod 的生命周期中，Pause 容器是第一个启动的容器，也是最后一个退出的容器。即使 Pod 中的其他容器因故障或重启而被终止，Pause 容器会保持运行，直到整个 Pod 被删除。这确保了在 Pod 内的所有容器共享同一网络和存储卷。 容器间共享存储卷 在 Pod 内，容器之间共享存储卷，而 Pause 容器通过维持网络命名空间的持续性，间接帮助 Pod 内其他容器访问共享的存储卷。它并不直接访问存储卷，但它的存在使得其他容器能够使用这些卷。 Pod 的资源隔离 Pause 容器在 Pod 中扮演的角色类似于占位符，确保 Pod 内的资源（如网络和存储）被正确隔离，并且网络命名空间不会被其他容器的启动和停止所干扰。 如何启动 Pause 容器 # 在 Kubernetes 中，Pause 容器是由 kubelet 自动启动的，用户通常不需要手动管理它。它在每个 Pod 的生命周期中自动创建，并由 kubelet 作为基础容器启动。当 Pod 中其他容器启动时，它们会共享该 Pause 容器所提供的网络命名空间。\n典型的 Pause 容器镜像 # 在 Kubernetes 中，Pause 容器通常使用的是 k8s.gcr.io/pause 镜像。 它非常轻量，通常只包含一个简单的 /pause 进程，确保网络命名空间处于活动状态。 它的体积极小，通常在 100 KB 左右，最大限度地减少资源消耗。 总结 # Pause 容器是 Kubernetes 中用于网络命名空间管理的特殊容器。它不执行任何业务逻辑，只是确保 Pod 的网络命名空间能够持续存在，并在 Pod 生命周期中充当一个基础容器，支持容器间共享网络和存储资源。\nk8s证书过期怎么更新 # Kubernetes 集群使用了多种证书，主要包括 API Server、Kubelet、Controller Manager 和 Scheduler 等组件的证书。如果这些证书过期，Kubernetes 集群将无法正常工作，因此及时更新证书是非常重要的。\n1. 检查证书的有效期 # 首先，您需要确认哪些证书已经过期。可以使用以下命令查看证书的有效期：\nkubectl get certificatesigningrequests -A 或者查看具体证书的有效期：\nopenssl x509 -in /etc/kubernetes/pki/apiserver.crt -noout -dates 这个命令可以查看 API Server 证书的有效期。其他组件证书路径通常位于 /etc/kubernetes/pki/ 下。\n2. 使用 kubeadm 更新证书 # 如果您的集群是通过 kubeadm 部署的，kubeadm 提供了方便的证书更新工具。\n步骤： # 查看证书过期情况 使用 kubeadm certs check-expiration 命令来检查证书的到期日期：\nkubeadm certs check-expiration 更新证书 如果某些证书已经过期或即将过期，可以使用 kubeadm certs renew 命令来更新证书。例如，更新 API Server 证书：\nkubeadm certs renew apiserver 您可以根据需要更新其他证书：\nkubeadm certs renew apiserver-kubelet-client kubeadm certs renew controller-manager kubeadm certs renew scheduler 更新完成后重启相关组件 证书更新后，Kubernetes 组件需要重新加载这些证书。通常可以通过重启组件来实现：\nsystemctl restart kubelet 验证证书更新 更新证书后，再次使用 check-expiration 命令确认证书是否已经更新。\nkubeadm certs check-expiration 3. 手动更新证书 # 如果集群不是通过 kubeadm 部署的，您可能需要手动更新证书。以下是手动更新 Kubernetes 组件证书的步骤：\n生成新的证书： 使用 openssl 工具或其他证书管理工具生成新的证书和密钥。例如：\nopenssl req -new -newkey rsa:2048 -days 365 -nodes -keyout /etc/kubernetes/pki/apiserver.key -out /etc/kubernetes/pki/apiserver.csr 签发证书： 使用一个 Kubernetes CA 证书（通常是由 Kubernetes 集群管理员管理的证书）来签署新的证书：\nopenssl x509 -req -in /etc/kubernetes/pki/apiserver.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out /etc/kubernetes/pki/apiserver.crt -days 365 更新证书后重启组件： 更新证书后，您需要重启 Kubernetes 组件（如 kube-apiserver、kubelet 等）：\nsystemctl restart kubelet 4. Kubernetes CA 证书的更新 # 在某些情况下，您可能还需要更新 Kubernetes 的根证书（CA 证书）。这通常发生在证书的根 CA 证书过期时。在这种情况下，您需要：\n生成新的 CA 证书。 使用新证书重新签署所有 Kubernetes 组件的证书。 分发新的 CA 证书到集群中所有节点，确保每个节点的证书库都包含新的 CA 证书。 5. Automating Certificate Rotation # 在 Kubernetes 1.14+ 中，Kubernetes 支持自动证书轮换。您可以在 API Server 中启用自动证书轮换功能，这样可以避免手动更新证书。具体配置需要在 API Server 启动时通过 --rotate-certificates 参数启用：\nkube-apiserver --rotate-certificates=true 6. 检查证书更新后的状态 # 证书更新完成后，可以使用以下命令检查集群状态，确保组件正常工作：\nkubectl get nodes kubectl get pods --all-namespaces 总结 # 更新 Kubernetes 证书的步骤包括：\n使用 kubeadm 或手动生成和签发新的证书。 更新证书后，重启相关组件（如 API Server、Kubelet、Controller Manager 等）。 对于使用 kubeadm 部署的集群，kubeadm certs renew 是一个便捷的工具，简化了证书更新的流程。 在管理大规模集群时，证书的自动轮换和管理非常重要，因此建议尽量使用自动证书更新机制。\nK8S QoS等级☆ # Kubernetes 中的 QoS (Quality of Service) 是一种机制，用于定义和管理 Pod 中容器的资源优先级与保障级别，帮助 Kubernetes 在资源紧张时进行资源调度与分配。Kubernetes 将 Pod 分为三种 QoS 等级，基于其 resource requests 和 resource limits 设置来决定。不同的 QoS 等级影响到 Pod 在资源竞争时的优先级和行为。\nKubernetes QoS 等级 # Guaranteed（保证） Burstable（突发） BestEffort（尽力而为） 每个 Pod 的 QoS 等级由其 CPU 和 内存 资源的 requests 和 limits 设置决定。下面是每种 QoS 等级的详细描述和规则：\n1. Guaranteed（保证） # Guaranteed QoS 等级的 Pod 会在资源争用时获得最高优先级和保障，适用于对资源有严格要求的应用。\n条件：Pod 中的所有容器都必须同时设置 requests 和 limits，并且 requests 必须等于 limits。 资源保障：Pod 会被保证拥有其请求的所有资源（包括 CPU 和内存）。当资源紧张时，Kubernetes 会优先保证 Guaranteed Pod 的资源。 示例：\napiVersion: v1 kind: Pod metadata: name: guaranteed-pod spec: containers: - name: app-container image: my-app-image resources: requests: memory: \u0026#34;500Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; limits: memory: \u0026#34;500Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; 优点：\n对资源有严格的保障，Pod 中的每个容器都会根据设置的请求和限制获得所需资源。 在资源竞争时，Guaranteed Pod 优先级最高。 缺点：\n需要显式地设置 requests 和 limits，并且两者的值要相等，这对于灵活的资源分配可能有些限制。 2. Burstable（突发） # Burstable QoS 等级的 Pod 表示它们对资源有一定要求，但在资源充足时可以突发使用更多的资源。适用于负载波动较大的应用。\n条件：Pod 中的至少一个容器设置了 requests 和 limits，但是 requests != limits，即容器的请求资源小于其限制资源。 资源保障：Pod 中的容器会保证最小的资源请求（requests），但可以在节点资源充足时使用更多的资源，最多不超过 limits 设置的资源。 示例：\napiVersion: v1 kind: Pod metadata: name: burstable-pod spec: containers: - name: app-container image: my-app-image resources: requests: memory: \u0026#34;200Mi\u0026#34; cpu: \u0026#34;200m\u0026#34; limits: memory: \u0026#34;500Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; 优点：\n对资源有一定保障，但可以灵活调整以应对负载波动。 可以利用节点空闲的计算资源，适合非关键任务的应用。 缺点：\n如果节点资源紧张，Pod 的容器可能会被限制在 requests 范围内，可能无法突发使用更多的资源。 3. BestEffort（尽力而为） # BestEffort QoS 等级的 Pod 没有设置 requests 和 limits，也就是说它不请求任何资源，也不限制资源。适用于对资源要求不高且可以接受资源不稳定的应用。\n条件：Pod 中的所有容器都没有设置 requests 和 limits，或者这两个值都设置为 0。 资源保障：Pod 不会得到任何资源保障。在资源紧张时，BestEffort Pod 最先被驱逐或限制资源。 示例：\napiVersion: v1 kind: Pod metadata: name: besteffort-pod spec: containers: - name: app-container image: my-app-image resources: {} 优点：\n不需要明确设置资源请求和限制，灵活性高。 适用于负载较轻或不太重要的应用。 缺点：\n在资源紧张时，BestEffort Pod 会被最先驱逐或者限制，资源保障最差。 QoS 影响 # 1. Pod 驱逐（Eviction） # 当节点资源不足时，Kubernetes 会驱逐资源占用较高或不重要的 Pod。Pod 的 QoS 等级决定了它被驱逐的优先级：\nGuaranteed Pod 最不可能被驱逐。 Burstable Pod 会在资源紧张时被优先考虑驱逐。 BestEffort Pod 最容易被驱逐。 2. Pod 优先级 # Kubernetes 使用 QoS 等级来管理不同 Pod 之间的优先级。在资源竞争时，QoS 等级较高的 Pod 会获得更多的资源。\nGuaranteed Pod 会优先获得 CPU 和内存资源。 Burstable Pod 会在资源充足时获得更多的资源，但当资源不足时，它们可能会受到限制。 BestEffort Pod 只会在节点有多余资源时运行，并且在资源紧张时会被优先驱逐。 总结 # Kubernetes 中的 QoS 等级根据 Pod 中容器的资源请求和限制决定，主要有三种等级：\nGuaranteed：容器的请求和限制相等，提供最高的资源保障。 Burstable：容器的请求和限制不相等，提供灵活的资源使用能力。 BestEffort：容器没有设置请求和限制，提供最低的资源保障，容易被驱逐。 正确设置资源请求和限制（requests 和 limits）可以帮助 Kubernetes 管理和调度 Pod 的资源，并确保在资源不足时关键应用能够优先运行。\nk8s节点维护注意事项 # 在 Kubernetes 中，节点维护是指对集群中的一个或多个节点进行操作，例如升级、修复、重新配置等。进行节点维护时，需要注意确保集群的高可用性、避免影响正在运行的应用程序，并遵循最佳实践来确保维护过程顺利完成。\n1. 确保高可用性 # 在进行节点维护时，必须保证集群的高可用性，避免单点故障。一般来说，建议集群至少有 3 个 Master 节点 和 3 个 Worker 节点。这样，在节点维护期间，其他节点可以继续提供服务。\n2. 检查节点状态 # 在进行任何操作之前，首先要检查节点的状态。你可以使用以下命令查看节点的健康状况：\nkubectl get nodes 确保节点处于 Ready 状态。若节点有问题，要先解决问题再进行维护。\n3. 将节点从调度池中移除 # 在维护节点之前，建议将其从调度池中移除，防止调度新的 Pod 到该节点。可以通过以下命令将节点标记为不可调度（NoSchedule）：\nkubectl cordon \u0026lt;node-name\u0026gt; 这会防止 Kubernetes 向该节点调度新 Pod，但已经在该节点上的 Pod 会继续运行。\n4. 迁移 Pod 到其他节点 # 为了确保不会影响到正在运行的应用程序，您应该将该节点上的 Pod 移到其他健康的节点上。可以使用以下命令将该节点上的 Pod 驱逐（Evict）：\nkubectl drain \u0026lt;node-name\u0026gt; --ignore-daemonsets --delete-local-data --ignore-daemonsets：忽略 DaemonSet 控制的 Pod，因为它们需要在所有节点上运行。 --delete-local-data：如果节点上有本地存储的数据，选择删除这些数据。 这会导致集群中的 Pod 重新调度到其他节点，确保业务不中断。\n5. 执行节点维护操作 # 在节点从调度池中移除并且所有 Pod 被迁移之后，可以开始执行维护操作。这可能包括：\n节点的操作系统更新或补丁。 Kubernetes 组件（如 kubelet、docker）更新。 硬件修复或更换。 网络配置更改等。 6. 重新加入节点 # 维护完成后，节点可以重新加入集群。首先，您需要将节点标记为可调度（Schedule）：\nkubectl uncordon \u0026lt;node-name\u0026gt; 这会将节点重新加入调度池，允许新的 Pod 被调度到该节点。\n7. 验证节点恢复 # 确保节点恢复并正常运行。检查节点状态：\nkubectl get nodes 确保节点处于 Ready 状态。检查所有 Pod 是否能够在节点上正常运行：\nkubectl get pods --all-namespaces -o wide 确保该节点上的 Pod 已经恢复，且没有出现问题。\n8. 检查 Pod 的健康状态 # 维护期间迁移的 Pod 可能需要一段时间来恢复。使用以下命令检查 Pod 的状态，确保它们在新的节点上正确运行：\nkubectl get pods --all-namespaces 对于出现问题的 Pod，可以使用以下命令进一步排查：\nkubectl describe pod \u0026lt;pod-name\u0026gt; kubectl logs \u0026lt;pod-name\u0026gt; 9. 监控与日志 # 在节点维护过程中，建议密切关注监控系统（如 Prometheus、Grafana 等）以及日志（如 ELK、Fluentd）来查看是否有异常情况。特别是要关注以下几个方面：\nCPU 和内存的使用情况。 节点负载和容器健康状态。 是否有新的警告或错误信息。 10. 通知相关人员 # 进行节点维护时，应通知集群的相关人员（如开发人员、运维人员），告知正在进行维护操作，避免不必要的干扰。此外，也可以提前设置维护窗口时间，以减少对生产环境的影响。\n11. 升级节点版本 # 如果在维护过程中涉及节点升级（例如升级操作系统或 Kubernetes 版本），确保遵循 Kubernetes 官方的升级文档。可以使用 kubeadm 工具进行节点升级：\nsudo kubeadm upgrade plan sudo kubeadm upgrade apply \u0026lt;version\u0026gt; 12. 更新节点的 kubelet 配置 # 如果有必要更新节点的 kubelet 配置（例如修改证书、网络配置等），可以编辑 /etc/kubernetes/kubelet.conf 配置文件，并重新启动 kubelet：\nsudo systemctl restart kubelet 13. 定期执行节点健康检查 # 为了避免节点长期处于不健康的状态，建议定期执行节点健康检查，并进行必要的维护。例如，可以定期检查磁盘空间、内存使用情况、网络延迟等指标。\n总结 # 节点维护时，需要谨慎处理以保证集群的稳定性和业务的持续运行。维护过程的关键步骤包括：\n将节点标记为不可调度（cordon），并迁移 Pod。 执行节点维护操作，如升级或硬件修复。 完成维护后，将节点标记为可调度（uncordon），并检查 Pod 的恢复情况。 确保节点健康并重新加入集群。 监控系统和日志，确保集群稳定运行。 通过以上步骤，可以确保 Kubernetes 集群在节点维护期间不会受到过多影响，并且维护操作顺利完成。\nHeadless Service和ClusterIP区别☆ # 在 Kubernetes 中，Headless Service 和 ClusterIP Service 都是 Service 类型的两种不同形式，用于在集群内暴露和访问应用。但它们的实现方式和适用场景有所不同。\n1. ClusterIP Service # ClusterIP 是 Kubernetes 默认的 Service 类型，用于在集群内部暴露服务。它为集群内的 Pod 提供一个虚拟 IP 地址，通过这个虚拟 IP，客户端可以访问服务。\n特性： 为 Service 分配一个虚拟 IP 地址（ClusterIP）。 客户端通过该 IP 来访问服务。 集群内的任何 Pod 都可以通过这个虚拟 IP 来访问服务，Kubernetes 会自动进行负载均衡，选择后端 Pod。 不会暴露服务到集群外部。 典型用法： 用于集群内部的服务通信。 例如，应用的数据库服务或微服务之间的通信。 示例： apiVersion: v1 kind: Service metadata: name: my-service spec: selector: app: my-app ports: - protocol: TCP port: 80 targetPort: 8080 type: ClusterIP 2. Headless Service # Headless Service 是一种特殊的 Service 类型，它不为服务分配虚拟 IP（ClusterIP），而是直接暴露每个 Pod 的 IP 地址。它通常用于需要直接访问每个 Pod 的场景，比如 StatefulSet 和需要 Pod 级别 DNS 解析的应用。\n特性： 不分配虚拟 IP（ClusterIP: None），客户端直接通过 DNS 查询获得每个 Pod 的 IP 地址。 每个后端 Pod 都有自己的 DNS 记录，通常是 pod-name.service-name.namespace.svc.cluster.local 形式。 适合 StatefulSet 和需要 Pod 直接访问的应用。 不进行负载均衡，客户端需要自己处理负载均衡（比如使用 DNS 轮询）。 典型用法： 用于 StatefulSet、分布式数据库、需要直接访问各个 Pod 的应用。 例如，分布式数据库（如 Cassandra、Elasticsearch）中需要直接访问每个节点。 示例： apiVersion: v1 kind: Service metadata: name: my-headless-service spec: clusterIP: None selector: app: my-app ports: - protocol: TCP port: 80 targetPort: 8080 ClusterIP 与 Headless Service 的区别 # 特性 ClusterIP Service Headless Service 虚拟 IP 地址 会为 Service 分配一个虚拟 IP 地址 不分配虚拟 IP 地址 (clusterIP: None) 负载均衡 通过虚拟 IP 地址进行负载均衡 不进行负载均衡，直接访问每个 Pod 访问方式 通过虚拟 IP 访问，Kubernetes 负责负载均衡 通过 DNS 查询访问，每个 Pod 都有自己的 DNS 记录 适用场景 适用于集群内部服务，需要负载均衡的场景 适用于需要直接访问每个 Pod 的场景，如 StatefulSet DNS 记录 DNS 解析返回 Service 的虚拟 IP 每个 Pod 都有自己的 DNS 记录 总结 # ClusterIP Service 是最常见的 Service 类型，适用于需要负载均衡和虚拟 IP 的场景。 Headless Service 则不提供虚拟 IP，而是暴露每个 Pod 的 IP 地址，适合需要直接访问每个 Pod 的应用，通常用于 StatefulSet 和需要基于 DNS 解析 Pod 的场景。 不进行负载均衡，直接访问每个 Pod？ 比如数据库有三个实例pod用headless的模式，那么应用连接这个数据库的方式是啥？三个不同的svc？ # 是的，Headless Service 在 Kubernetes 中的一个关键特性就是它不会进行负载均衡，而是将请求直接路由到每个 Pod。对于数据库实例的场景，比如一个数据库有三个实例（Pod），使用 Headless Service 后，应用程序的连接方式会有所不同。通常可以通过以下两种方式来进行连接：\n1. 通过 DNS 直接访问每个 Pod # 在 Headless Service 中，每个 Pod 会有自己的 DNS 记录，而不是通过一个共享的虚拟 IP 访问整个服务。Kubernetes 会为每个 Pod 生成一个 DNS 记录，通常是 \u0026lt;pod-name\u0026gt;.\u0026lt;service-name\u0026gt;.\u0026lt;namespace\u0026gt;.svc.cluster.local。\n假设你有一个名为 db 的 Headless Service，并且该 Service 后面有三个 Pod（数据库实例）：\ndb-0 db-1 db-2 Kubernetes 会为每个 Pod 创建以下 DNS 记录：\ndb-0.db.default.svc.cluster.local db-1.db.default.svc.cluster.local db-2.db.default.svc.cluster.local 应用程序可以通过 DNS 查询来直接连接到这三个 Pod。比如，如果应用程序需要连接数据库实例 db-1，它可以使用 db-1.db.default.svc.cluster.local。\n示例：\napiVersion: v1 kind: Service metadata: name: db spec: clusterIP: None # 设置为 None 表示 Headless Service selector: app: database ports: - port: 5432 targetPort: 5432 DNS 解析：\ndb-0.db.default.svc.cluster.local db-1.db.default.svc.cluster.local db-2.db.default.svc.cluster.local 应用程序可以通过这些 DNS 名称直接连接数据库实例。\n2. 通过多个 Service 访问每个实例 # 另一种方式是为每个 Pod 创建一个单独的 Service。虽然这不常见，但在某些情况下，可能需要为每个实例单独暴露一个 Service。每个 Pod 都有自己的 Headless Service 实例，并且每个 Service 都会绑定一个特定的 Pod。\n如果你有三个数据库实例 db-0、db-1 和 db-2，可以创建三个单独的 Service 来暴露每个数据库实例：\ndb-0-service db-1-service db-2-service 每个 Service 仅暴露一个 Pod，应用程序可以通过相应的 Service 名称来连接到特定的 Pod。\n示例：\napiVersion: v1 kind: Service metadata: name: db-0-service spec: clusterIP: None selector: pod-name: db-0 ports: - port: 5432 targetPort: 5432 然后可以创建类似的 db-1-service 和 db-2-service，这样应用程序就可以通过 db-0-service、db-1-service 和 db-2-service 来访问不同的数据库实例。\n适用场景 # 数据库实例或有状态服务：例如，使用 StatefulSet 部署数据库时，使用 Headless Service 更合适，因为每个 Pod（数据库实例）通常都有一个固定的名称，而每个实例之间的状态可能有所不同，需要应用程序进行特定的连接。 有状态的分布式应用：一些分布式系统（如 Cassandra、Elasticsearch 等）可能需要应用程序通过 Pod 的 DNS 名称进行访问，来确保每个节点能被精确识别和访问。 总结 # 在 Headless Service 中，应用程序通常通过 Pod 的 DNS 名称 来直接访问数据库实例，而不是通过一个统一的 IP 地址。 每个 Pod 都会有一个 DNS 记录，应用程序可以根据需要选择连接某个特定的 Pod。 另一种做法是为每个 Pod 创建独立的 Service，但这通常不常见，因为 DNS 的方式更灵活。 因此，Headless Service 模式并不是通过多个不同的 Service 来访问不同的数据库实例，而是通过单一的 Service 暴露多个 Pod 的 DNS 名称，让应用程序可以直接访问特定的 Pod。\ndb-0.db.default.svc.cluster.local db-1.db.default.svc.cluster.local db-2.db.default.svc.cluster.local 这不就是3个不同svc吗 # 你提出的疑问是正确的，但在 Headless Service 中，实际上 并没有三个不同的 Service，而是 一个 Service，但它会为每个 Pod 分配一个独立的 DNS 记录。\n让我们再详细解释一下：\nHeadless Service 和 Pod 的 DNS 解析 # 在 Kubernetes 中，当你创建一个 Headless Service 时（clusterIP: None），Kubernetes 不会为该 Service 分配虚拟 IP，而是直接将每个与该 Service 相关联的 Pod 暴露出来，并为每个 Pod 创建一个 DNS 记录。具体来说，Kubernetes 会为每个 Pod 创建一个 DNS 记录，它们遵循以下格式：\n\u0026lt;pod-name\u0026gt;.\u0026lt;service-name\u0026gt;.\u0026lt;namespace\u0026gt;.svc.cluster.local 假设你有一个名为 db 的 Headless Service，它选择了名为 db-0、db-1 和 db-2 的三个 Pod。Kubernetes 会为这三个 Pod 创建以下 DNS 记录：\ndb-0.db.default.svc.cluster.local db-1.db.default.svc.cluster.local db-2.db.default.svc.cluster.local 虽然这些看起来像是多个服务的 DNS 名称，但它们实际上都指向同一个 Headless Service，而不是三个独立的 Service。Headless Service 只是为每个 Pod 提供了一个独立的 DNS 名称，让客户端可以直接通过 DNS 查询访问到每个 Pod。\n为什么这些看起来像是多个服务 # 这些 DNS 名称看起来像是为每个 Pod 创建了一个独立的 Service，但本质上它们都是由一个 Headless Service 创建的。这是因为 Kubernetes 会使用 DNS 动态解析并将流量发送到正确的 Pod。例如：\ndb-0.db.default.svc.cluster.local 解析到 db-0 Pod 的 IP 地址。 db-1.db.default.svc.cluster.local 解析到 db-1 Pod 的 IP 地址。 db-2.db.default.svc.cluster.local 解析到 db-2 Pod 的 IP 地址。 总结 # Headless Service 并不会为每个 Pod 创建独立的 Service，它只创建一个 Service，但为每个 Pod 创建一个独立的 DNS 记录。 每个 Pod 都会有一个唯一的 DNS 名称，客户端可以直接通过这些 DNS 名称来访问特定的 Pod。 这些 DNS 记录的存在使得应用程序可以有选择性地访问某个 Pod，而不是通过负载均衡访问 Service 的虚拟 IP 地址。 希望这样能帮助你更好地理解 Headless Service 的工作原理。\n在 Headless Service 中，实际上 并没有三个不同的 Service，而是 一个 Service，但它会为每个 Pod 分配一个独立的 DNS 记录，那这个一个svc是啥 # 在 Kubernetes 中，Headless Service 是一个特殊类型的 Service，它的核心特点是 不会分配 ClusterIP，而是通过 DNS 来暴露集群中的多个 Pod。虽然每个 Pod 会有一个独立的 DNS 名称，但是它们都是由同一个 Headless Service 来管理和暴露的。\n具体来说： # Service 的定义（Headless Service）\n：\n你创建的 Service 在 Headless Service 模式下是一个单独的 Service，它的 clusterIP: None。 该 Service 会选择一组 Pod（通常是通过标签选择器选择），但它并不创建一个统一的虚拟 IP（ClusterIP）。相反，它为每个 Pod 提供一个 DNS 名称。 DNS 解析\n：\n在 Headless Service 中，Kubernetes 会为每个 Pod 创建一个 DNS 记录，格式如下：\n\u0026lt;pod-name\u0026gt;.\u0026lt;service-name\u0026gt;.\u0026lt;namespace\u0026gt;.svc.cluster.local 这些 DNS 记录并不是独立的 Service，而是通过 同一个 Service 来进行管理的。\n举个例子： # 假设你有一个数据库应用，并使用了 StatefulSet 来管理数据库实例，每个数据库实例是一个 Pod。你为这些 Pod 创建了一个 Headless Service，比如叫 db。\nService 定义：\napiVersion: v1 kind: Service metadata: name: db spec: clusterIP: None # Headless Service selector: app: database ports: - port: 5432 targetPort: 5432 db 是 Service 名称。 选择器 app: database 用于匹配一组 Pod（例如 db-0、db-1 和 db-2）。 由于这是一个 Headless Service，它没有 clusterIP，而是将每个 Pod 的 IP 地址通过 DNS 暴露出来。 Pod 的 DNS 记录：\ndb-0.db.default.svc.cluster.local db-1.db.default.svc.cluster.local db-2.db.default.svc.cluster.local 所以，这个一个 Service 是： # 它是一个 Headless Service，可以让你通过 DNS 直接访问集群中的每个 Pod。 每个 Pod 有一个 独立的 DNS 名称，但它们都是由同一个 Service 进行管理和暴露的。 结论： # 在 Headless Service 中，虽然每个 Pod 会有独立的 DNS 名称，但它们共享 同一个 Service，并通过该 Service 的 DNS 解析机制来直接访问每个 Pod。 这并不是创建多个独立的 Service，而是同一个 Service 为每个 Pod 提供了 DNS 名称，因此客户端可以选择直接访问某个特定的 Pod。 Linux容器技术的基础原理 # Linux 容器技术的基础原理主要依赖于 Linux 内核提供的一些核心功能，这些功能允许在同一台物理机或虚拟机上运行多个相互隔离的应用或进程。容器能够共享宿主机的操作系统内核，但在用户空间中实现进程隔离，从而提高资源利用率和灵活性。\nLinux 容器技术的关键原理包括：\n1. 命名空间（Namespaces） # 命名空间是 Linux 容器技术的核心之一。它为每个容器提供了一个独立的、隔离的环境，使得容器中的进程只能看到自己的资源，而无法看到宿主机或其他容器的资源。Linux 支持多种命名空间：\nPID 命名空间（Process ID Namespace）：每个容器可以拥有自己的一套进程 ID，进程 ID 在容器内是唯一的，但在宿主机上可能会重复。它使得容器可以在隔离的环境中运行进程。 NET 命名空间（Network Namespace）：为容器提供独立的网络栈，包括 IP 地址、路由表、端口等。每个容器有自己的网络接口、IP 地址、端口等资源。 MNT 命名空间（Mount Namespace）：容器有自己的文件系统视图，可以挂载独立的文件系统或挂载点，而不影响宿主机或其他容器。 UTS 命名空间（UTS Namespace）：容器可以有自己的主机名和域名，不受宿主机的影响。 IPC 命名空间（IPC Namespace）：容器之间的进程间通信（IPC）是隔离的，例如共享内存、信号量等。 USER 命名空间（User Namespace）：容器内部的用户和组 ID 是独立的，可以允许容器内的进程拥有 root 权限，但它们在宿主机上并不是 root 用户。 CGROUPS 命名空间（Control Group Namespace）：负责将资源（如 CPU、内存、磁盘 IO 等）分配给容器。可以限制容器使用的系统资源，并进行监控。 2. 控制组（cgroups） # 控制组（Cgroups，Control Groups）是 Linux 内核的另一个重要功能，用于控制和限制进程的资源使用情况。容器内的进程会被分配到一个或多个控制组中，以便限制它们使用的资源。\n资源限制：Cgroups 可以限制容器使用的 CPU 时间、内存、磁盘 I/O 等。 资源监控：可以监控容器中进程的资源使用情况，如 CPU 使用率、内存消耗等。 资源分配：允许对多个容器进行资源隔离和公平分配，例如，限制容器使用的最大 CPU 核心数、最大内存大小等。 通过 Cgroups，Linux 容器可以精确控制每个容器的资源配额，从而避免某个容器占用过多资源，影响宿主机或其他容器的运行。\n3. 联合文件系统（Union File System） # 联合文件系统（UnionFS）是 Linux 容器技术的另一个关键组成部分。它使得多个文件系统可以挂载在同一个目录下，并且以“只读”和“可写”层次结构组合。\n联合挂载：容器中的文件系统是由多个层叠加的文件系统组成，每个层次可以是只读的，也可以是可写的。容器的根文件系统通常是只读的，而写操作会被记录到容器的可写层中。\n镜像和容器：容器镜像实际上是一个包含多个层的文件系统，镜像中的每一层都是一个不可变的只读层，而容器是一个运行中的实例，包含一个可写的层。\n常见的联合文件系统\n：\nOverlayFS：常用于现代容器的联合文件系统。它通过两个目录（一个是只读层，另一个是可写层）进行联合挂载。 AUFS：另一个常用的联合文件系统，用于 Docker 旧版本。 联合文件系统能够提高存储效率，因为多个容器可以共享相同的只读镜像层，从而节省磁盘空间。\n4. 容器镜像（Container Images） # 容器镜像是容器化应用的标准化打包格式，包含了运行容器所需的操作系统文件、应用程序、依赖包以及运行时环境。容器镜像是不可变的，一旦创建，它就不能被修改。\n镜像层次：容器镜像通常由多个层（Layer）组成，每一层都代表了对文件系统的一个修改，例如安装某个软件包或配置文件的变动。 镜像存储和分发：容器镜像可以存储在本地或远程仓库（如 Docker Hub 或私有仓库）中，容器可以从这些仓库拉取镜像来创建实例。 5. 容器的隔离性与安全性 # 容器技术提供了资源隔离、进程隔离和文件系统隔离等多重隔离机制：\n进程隔离：容器内的进程通过 PID 命名空间与宿主机和其他容器的进程隔离。 网络隔离：容器通过 Network 命名空间拥有独立的网络栈，可以有独立的 IP 地址、端口等，不会与宿主机或其他容器共享网络资源。 文件系统隔离：通过 MNT 命名空间和联合文件系统，容器有自己的文件系统视图，可以对文件进行读写操作，隔离了容器间的文件系统。 用户隔离：通过 User 命名空间，容器内的用户和宿主机的用户是隔离的，容器内的 root 用户并不对应宿主机的 root 用户，从而提高安全性。 总结 # Linux 容器技术的基础原理是基于以下几个关键概念：\n命名空间（Namespaces）：提供进程、网络、文件系统等方面的隔离。 控制组（Cgroups）：控制容器的资源分配与限制。 联合文件系统（UnionFS）：允许多个只读层和一个可写层组成容器的文件系统。 容器镜像（Container Images）：应用及其依赖的打包格式，提供可移植性。 安全性与隔离：通过隔离机制（如用户隔离、网络隔离等）确保容器的独立性与安全性。 这些技术共同构成了现代 Linux 容器化的基础，能够让应用程序在隔离、可移植和高效的环境中运行。\nKubernetes Pod的常见调度方式 # Kubernetes 中，Pod 的调度是由 kube-scheduler 控制的，调度的核心目的是根据一定的规则将 Pod 安排到合适的节点上执行。调度是一个非常重要的过程，因为它直接关系到容器的运行效率、资源利用率和稳定性。\n常见的 Pod 调度方式 # 默认调度（Default Scheduling）\n默认情况下，Kubernetes 会根据每个 Pod 的资源请求（如 CPU 和内存）、节点的可用资源以及调度策略来选择一个合适的节点进行调度。kube-scheduler 会选择一个符合要求的节点。\n工作流程\n：\nkube-scheduler 获取待调度的 Pod。 它会检查各个节点的资源，确定节点是否满足 Pod 的资源需求。 调度策略还会考虑节点的标签、污点、容忍度等信息来做进一步筛选。 如果符合条件，Pod 会被调度到合适的节点上。 节点亲和性（Node Affinity）\nNode Affinity 是对节点选择的进一步限制，它允许用户根据节点的标签来指定调度规则。通过设置 Pod 的 nodeAffinity，可以将 Pod 调度到具有特定标签的节点上。\n支持的规则\n：\nrequiredDuringSchedulingIgnoredDuringExecution：硬性要求 Pod 必须调度到符合条件的节点上。 preferredDuringSchedulingIgnoredDuringExecution：优选规则，如果某个节点符合条件，则 Pod 更倾向于调度到该节点上，但如果没有匹配的节点，也不会拒绝调度。 使用场景：如果你有一些特定的硬件要求（比如 GPU 或特殊的存储设备），你可以使用节点亲和性来将 Pod 调度到特定类型的节点上。\nPod 亲和性（Pod Affinity）\nPod Affinity 和 Pod Anti-Affinity 是通过 Pod 的标签来控制 Pod 与其他 Pod 的关系。\nPod Affinity：指定 Pod 应该尽可能地调度到与其他特定 Pod 一起运行的节点上。\nPod Anti-Affinity：指定 Pod 不应该与其他特定 Pod 一起运行。\n使用场景\n：\n你可能希望某些服务（如数据库集群）在同一节点上运行，以减少网络延迟。 相反，可能希望将某些服务（如不同的微服务）分散到不同的节点上，以提高可用性。 污点和容忍度（Taints and Tolerations）\n污点（Taints） 是节点的属性，用于标记一个节点不可用于特定的 Pod。污点会使得 Pod 无法调度到该节点，除非 Pod 有相应的容忍度（Toleration）。 容忍度（Tolerations） 是 Pod 的属性，用于标记 Pod 可以接受哪些污点。 使用场景：通常用于节点的隔离，例如，如果某些节点不适合某些特定的负载（如特定的硬件要求），可以在这些节点上设置污点，确保不适合的 Pod 不会调度到这些节点。 资源请求和限制（Resource Requests and Limits）\n在 Kubernetes 中，Pod 可以设置资源请求（request）和限制（limit），这些信息会被调度器用来决定 Pod 应该调度到哪个节点上。 Resource Requests：指示容器正常运行所需要的资源，调度器根据这些请求来决定容器应该运行在哪个节点上。 Resource Limits：设置容器的最大资源使用量。 调度器通过检查节点的资源（CPU、内存等）来选择适合的节点，并确保节点的资源满足 Pod 的请求。 调度策略（Scheduling Policies）\nPriority and Preemption（优先级与抢占）\n：\nKubernetes 支持对 Pod 设置优先级，通过优先级，较高优先级的 Pod 可以抢占较低优先级 Pod 的资源，尤其是在集群资源紧张时。 Preemption：如果集群资源不足，且一个新的 Pod 需要资源，则优先级较低的 Pod 可能会被抢占，以释放资源给优先级较高的 Pod。 Scheduling via Custom Scheduler（自定义调度器）\n：\nKubernetes 支持自定义调度器，用户可以根据自己的需求编写自定义的调度逻辑，替换默认调度器或与其配合使用。 使用场景：如果用户需要实现一些非常复杂的调度规则，或者要考虑业务逻辑（如应用的拓扑结构、流量路由等），可以编写自定义调度器。 Affinity \u0026amp; Anti-Affinity（亲和性与反亲和性）\nPod Affinity：Pod 之间的亲和性关系，允许将 Pod 调度到具有相同标签的节点上，以便在物理上靠近运行。 Pod Anti-Affinity：反亲和性，允许避免将 Pod 调度到具有特定标签的节点上。 8. DaemonSet # DaemonSet 是一种特殊的控制器，用于确保每个节点上运行一个 Pod。当新的节点加入集群时，DaemonSet 会确保该节点上运行一个 Pod。 使用场景：通常用于需要在每个节点上运行的应用程序（如日志收集、监控代理等）。 9. 静态和动态调度（Static and Dynamic Scheduling） # 静态调度：Pod 在创建时已经指定了调度的目标节点。 动态调度：Pod 会在调度器中等待，直到调度器决定最合适的节点进行调度。 10. 资源调度策略 # 软/硬性约束：Pod 调度可以通过软约束（如 affinity 和 anti-affinity）和硬性约束（如资源请求和限制）来实现。软约束不会阻止 Pod 调度，而硬性约束则是必须满足的条件。 总结： # Kubernetes 提供了多种灵活的调度策略来满足不同的需求，包括但不限于：\n节点亲和性与 Pod 亲和性； 污点与容忍度； 自定义调度器； 资源请求与限制； 优先级和抢占。 根据不同的应用场景，Kubernetes 提供了强大的调度功能，可以帮助优化资源利用、提高可用性、保障应用的稳定性。\nkubernetes Ingress原理☆ # Kubernetes Ingress 原理 # Ingress 是 Kubernetes 中用于管理外部访问到集群服务的 API 资源。它提供了一种通过 HTTP/HTTPS 协议访问集群内部服务的方式，同时允许对外部访问进行更细粒度的控制和路由。\nIngress 的原理依赖于以下几个核心概念：\n1. Ingress 资源 # Ingress 是一个集群级别的资源对象，用于定义如何将外部请求路由到 Kubernetes 集群内的服务。它通常由 URL 路径、主机名、协议等信息组成，用于将流量路由到不同的服务和端口。\nIngress 资源的基本结构如下：\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: my-ingress spec: rules: - host: www.example.com http: paths: - path: /api pathType: Prefix backend: service: name: my-api-service port: number: 80 - path: /web pathType: Prefix backend: service: name: my-web-service port: number: 80 host：指定请求的主机名。 http/paths：定义了基于 URL 路径的路由规则。 backend：指定了请求将被转发到哪个服务及其端口。 2. Ingress Controller # Ingress 资源本身并不处理外部流量。它只是定义了如何路由流量，而真正负责执行流量路由和访问控制的组件是 Ingress Controller。\nIngress Controller 是一种负责执行 Ingress 资源定义的控制器，它通常运行在集群内，监视集群中的 Ingress 资源，并根据其配置将流量转发到相应的服务。Ingress Controller 通常是一个基于反向代理的应用，如 Nginx、Traefik 或 HAProxy。\nIngress Controller 会读取 Ingress 资源，分析规则并配置负载均衡器或反向代理。 它会监听集群中的 Ingress 资源变化，并动态更新其路由规则。 常见的 Ingress Controller：\nNginx Ingress Controller：Nginx 基于反向代理的实现。 Traefik：一种现代的反向代理，支持多种功能，如自动发现、WebSocket 等。 HAProxy：一个成熟的负载均衡器和反向代理，也可作为 Ingress Controller 使用。 3. Ingress 的工作原理 # 1. 外部请求到达 Ingress Controller # 外部客户端通过 HTTP 或 HTTPS 请求访问指定的 URL（如 www.example.com）。Ingress Controller 通常通过暴露一个外部 IP 或 LoadBalancer 的方式接收这些请求。\n2. Ingress Controller 解析 Ingress 资源 # Ingress Controller 会根据请求的域名（host）和路径（path）匹配相应的规则。 如果请求的 URL 匹配某个 Ingress 资源的规则，Ingress Controller 会将请求路由到相应的后端服务。 3. 请求转发到后端服务 # Ingress Controller 根据匹配的规则，将请求转发到相应的 Kubernetes Service。 Service 会将流量传递到对应的 Pod 上，Pod 再将请求处理并返回响应。 4. 返回响应 # 后端服务的 Pod 处理请求后，响应会通过 Kubernetes 的网络模型返回给 Ingress Controller。 最终，Ingress Controller 将响应返回给外部客户端。 4. Ingress 的特性 # 路径路由：可以基于 URL 路径将请求路由到不同的服务。例如，/api 路径的请求可以被路由到 my-api-service，而 /web 路径的请求可以被路由到 my-web-service。\n主机名路由：可以基于域名路由流量。Ingress 可以处理不同的域名并将流量转发到不同的服务。\nTLS 支持：Ingress 支持通过 TLS/SSL 加密来保护 HTTP 流量。可以在 Ingress 资源中指定 TLS 配置，指定密钥和证书以支持 HTTPS。\n示例：\nspec: tls: - hosts: - \u0026#34;www.example.com\u0026#34; secretName: example-tls 负载均衡：Ingress Controller 通过负载均衡策略将请求分发到不同的 Pod，支持 Round-robin、Least Connection 等策略。\n反向代理：Ingress Controller 作为反向代理处理外部请求，并转发到集群内部服务。它通过反向代理帮助管理跨多个服务的流量。\n基于规则的路由：Ingress 支持基于 URL 路径、主机名等条件的路由规则，也可以结合自定义的 header 进行路由。\n5. Ingress 与 Service 的关系 # Ingress 并不直接暴露 Pod，而是通过 Service 暴露 Pod。具体来说，Ingress Controller 会将流量转发到一个 Kubernetes Service，然后由 Service 将流量进一步分发到相应的 Pod。\n6. Ingress 的使用场景 # 暴露 HTTP/HTTPS 服务：使用 Ingress 可以将 HTTP/HTTPS 服务暴露给外部，通常用于 Web 应用、API 等服务。 基于域名的路由：可以使用 Ingress 根据域名将流量路由到不同的服务，例如 api.example.com 和 www.example.com 路由到不同的服务。 TLS/SSL 加密：Ingress 支持配置 TLS，帮助实现 HTTPS 加密通信。 集群内部负载均衡：Ingress Controller 可以作为集群内部的负载均衡器，自动分配流量到不同的后端服务。 7. Ingress Controller 部署 # Ingress Controller 通常作为一个 Deployment 在 Kubernetes 中运行，它监听 Kubernetes 中的 Ingress 资源并管理路由。部署 Ingress Controller 的基本步骤如下：\n选择一个适合的 Ingress Controller（如 Nginx、Traefik 等）。 使用 Kubernetes 的 YAML 配置文件部署 Ingress Controller。 配置 Ingress 资源，指定如何路由流量到服务。 8. 常见的 Ingress Controller # Nginx Ingress Controller： 最常用的 Ingress Controller，实现了基于 Nginx 的反向代理和负载均衡。 适用于大多数标准的 HTTP/HTTPS 流量管理场景。 Traefik： 一个现代的反向代理，支持动态发现、自动配置和高级功能如 WebSocket。 支持多种后端和协议，适用于微服务架构。 HAProxy： 适用于需要高性能、稳定的负载均衡场景。 支持多种高级路由、负载均衡策略和健康检查。 总结 # Kubernetes 的 Ingress 通过定义如何将外部 HTTP/HTTPS 请求路由到集群内部的服务来提供外部访问功能。其工作原理依赖于 Ingress 资源和 Ingress Controller，后者执行路由、负载均衡和安全性等功能。通过 Ingress，用户可以轻松实现外部访问控制、路径和域名路由、TLS 加密等功能。\nKubernetes各模块如何与API Server通信 # 在 Kubernetes 中，所有的组件（包括调度器、控制器、节点等）都通过 API Server 进行通信。API Server 是 Kubernetes 集群的核心组件，负责处理集群内所有的请求，并提供 Kubernetes API（REST API）供客户端和集群各个模块进行交互。API Server 是集群的统一入口，所有的请求都会经过 API Server 进行认证、授权、路由和处理。\n各模块与 API Server 的通信方式 # kube-apiserver\n功能：Kubernetes 的 API Server 是集群的核心组件，所有的请求（包括内部和外部的请求）都通过它进行交互。它处理 Kubernetes API（例如资源对象创建、更新、删除、查询等），并提供 RESTful API 供客户端和其他组件进行访问。\n通信方式\n：\n客户端（如 kubectl 或应用程序）：用户通过 kubectl 等客户端工具与 API Server 交互，执行各种操作（如创建 Pod、查看节点、更新配置等）。这些操作最终都会通过 REST API 发送到 API Server。 控制器：集群中的控制器（如 Deployment Controller、ReplicaSet Controller、StatefulSet Controller 等）会定期访问 API Server 查询集群的当前状态，执行控制循环（如确保 Pod 的副本数符合预期）。它们通过 Kubernetes Client 来与 API Server 通信。 etcd：API Server 将集群的状态数据（如 Pod、Service、Node 等资源对象）保存在 etcd 中，它是集群的持久化存储。API Server 会通过 etcd API 与 etcd 进行交互来读取和写入数据。 kube-scheduler\n功能：调度器负责将 Pod 调度到合适的节点上。它监视 API Server 中的 Pod 对象，并根据资源、节点状态等信息决定 Pod 的调度位置。\n通信方式\n：\nkube-scheduler 会定期访问 API Server 来查看待调度的 Pod（通过 ListPods 或 Watch API）并获取节点资源（通过 ListNodes 或 Watch API）。 调度完成后，kube-scheduler 会通过 API Server 更新 Pod 的调度信息（例如将 Pod 的 nodeName 更新为调度的目标节点）。 kube-controller-manager\n功能：控制器管理器负责维护集群的期望状态，确保集群资源达到期望的状态。例如，ReplicaSet 控制器会确保指定数量的 Pod 副本在集群中始终保持运行。\n通信方式\n：\n控制器通过 API Server 查询集群的当前状态，并根据期望状态执行操作。 例如，ReplicaSet 控制器会定期查询 API Server 中的 ReplicaSet 对象，并检查实际的 Pod 副本数量是否符合期望。如果不符合，它会创建或删除 Pod 来实现副本数的调整。 kubelet\n功能：每个节点上运行的 kubelet 负责管理该节点上的 Pod 和容器的生命周期，并向 API Server 报告节点状态、Pod 状态等信息。\n通信方式\n：\nkubelet 定期向 API Server 提交节点和 Pod 的状态信息（通过 POST 或 PUT 请求）。 kubelet 还通过 API Server 向调度器请求分配待调度的 Pod，并管理这些 Pod 的容器生命周期。 kubelet 会拉取 Pod 的配置文件、容器镜像等信息，确保节点上的容器按预期运行。 etcd\n功能：etcd 是 Kubernetes 的分布式键值存储，负责持久化存储 Kubernetes 集群的所有数据（如 Pod、Service、ConfigMap 等）。\n通信方式\n：\nAPI Server 会通过 gRPC 调用与 etcd 进行交互，确保数据的读取和写入。API Server 是唯一与 etcd 交互的组件。 API Server 将集群的状态数据保存在 etcd 中，并通过 etcd 提供的接口进行持久化存储。 kubectl\n功能：kubectl 是 Kubernetes 的命令行工具，用于与 API Server 交互，执行对集群资源的管理和操作。\n通信方式\n：\n用户通过 kubectl 向 API Server 发送 RESTful 请求（如 kubectl apply -f, kubectl get pods 等命令），然后 API Server 处理请求并返回结果。 集群外部客户端\n功能：外部应用程序或用户可以通过 API Server 提供的 REST API 来访问 Kubernetes 集群资源。\n通信方式\n：\n外部客户端通常会使用 HTTPS 与 API Server 通信，进行身份认证和授权后，通过 Kubernetes API 进行资源操作。 具体通信流程 # 假设有一个操作需要通过 kubectl 创建一个 Pod，整个过程的通信流程如下：\n用户发起请求：用户通过 kubectl 提交一个创建 Pod 的请求，kubectl 会将请求转化为 HTTP 请求（通常是一个 POST 请求）发送到 API Server。 API Server 认证和授权：API Server 会首先进行用户认证（如使用 Token 或证书）和授权（如 RBAC 控制），确认请求是否具有执行权限。 API Server 校验请求：API Server 校验请求的有效性（如资源的格式、字段是否正确等），并将请求转发到 etcd 进行持久化。 API Server 更新资源：API Server 将请求处理后，向 etcd 存储 Pod 的信息。 kubectl 接收响应：API Server 处理完成后，返回一个响应给 kubectl，如返回 Pod 创建成功的信息。 Pod 调度与启动：如果是新的 Pod，调度器（scheduler）会读取 Pod 请求，决定将其调度到哪个节点，节点的 kubelet 会启动容器，开始运行该 Pod。 总结 # Kubernetes 中的各模块（如 kubelet、kube-scheduler、kube-controller-manager）都通过 API Server 与集群资源进行交互。API Server 提供了统一的 REST API，负责处理来自集群内部和外部的请求，所有组件都依赖于它来获取和修改集群的状态信息。此外，API Server 与 etcd 直接交互，持久化所有集群的状态。通过这些模块的相互通信，Kubernetes 实现了高度自动化和灵活的集群管理。\nkubelet监控worker节点如何实现 # 在 Kubernetes 中，kubelet 是运行在每个 worker 节点 上的关键组件，负责管理该节点上的 Pod 和容器的生命周期。它不仅确保节点上的 Pod 按照预期运行，还定期向 Kubernetes 集群的 API Server 报告节点和 Pod 的状态。\n为了对 worker 节点 进行有效的监控，kubelet 通过以下几种方式提供监控功能：\n1. kubelet 的健康检查 # kubelet 提供了几种不同类型的健康检查接口，这些接口可以帮助监控 worker 节点 和 Pod 的健康状态。\n1.1 Liveness 和 Readiness Probes # Liveness Probes 用于检查容器是否处于健康状态，是否需要重启。 Readiness Probes 用于检查容器是否准备好接受流量。 这些探针通过设置在容器中的检查点，kubelet 定期访问它们。如果探针失败，kubelet 会采取相应的行动（如重启容器或将 Pod 从服务负载均衡中移除）。\n1.2 kubelet 自身健康检查 # /healthz：这个端点用于检查 kubelet 是否正常运行。 /metrics：提供 Prometheus 格式的指标数据，供外部系统采集。 2. kubelet 的指标暴露 # kubelet 本身通过暴露 /metrics 端点提供了丰富的指标信息，这些信息包括节点的状态、容器的资源使用情况等。常见的指标有：\n容器 CPU 使用情况 容器内存使用情况 Pod 状态 网络流量和磁盘使用情况 这些指标可以通过 Prometheus 等监控系统进行抓取，用于对 worker 节点和容器进行实时监控。\n2.1 Prometheus 监控 # Prometheus 是 Kubernetes 中最常用的监控工具之一，支持抓取 /metrics 端点的数据并进行存储和分析。 Prometheus 可以通过配置 kubelet 的 /metrics 端点来收集每个节点的资源使用数据（如 CPU 和内存消耗等）。 2.2 Node Exporter # Node Exporter 是 Prometheus 生态中的一个组件，用于收集主机的硬件和操作系统指标（如 CPU、内存、磁盘等）。 通过 Node Exporter 和 Prometheus 配合，能够提供更详细的节点层级的监控数据。 3. 资源使用监控 # Kubernetes 集群中的每个节点都会暴露一些关于资源使用的关键指标，如 CPU、内存、网络带宽等。kubelet 会持续监控这些资源的使用情况，并将其暴露给外部监控系统。\n资源使用数据：kubelet 会定期向 API Server 提交该节点上的资源使用情况，包括 CPU 和内存的使用情况。 节点资源压力：kubelet 会根据节点的资源使用情况计算资源压力，并根据压力情况采取相应的调度或驱逐策略。例如，当节点的内存使用率过高时，kubelet 会通知调度器驱逐 Pod。 4. 事件记录 # kubelet 会将节点上的事件（如容器启动、停止、Pod 状态变化等）记录下来。这些事件可以通过 kubectl get events 或 API Server 查询。通过事件监控，可以追踪节点和 Pod 的状态变化，快速发现潜在问题。\n5. 集群级别的监控系统 # Kubernetes Dashboard：Kubernetes 提供了一个可视化的 Web 控制台，能够展示每个节点和 Pod 的状态，并显示健康检查信息。 Prometheus + Grafana：Prometheus 和 Grafana 是最常用的监控组合。Prometheus 用于数据收集，Grafana 用于数据可视化。通过对 kubelet 提供的指标进行可视化，用户可以方便地查看节点和容器的健康状况。 6. Node Conditions（节点状态） # kubelet 通过 Node Conditions 来反映节点的健康状态。节点的状态包括以下几种：\nReady：节点是否正常运行并可接受调度。 MemoryPressure：节点内存是否足够。 DiskPressure：节点磁盘是否足够。 PIDPressure：节点是否面临进程数压力。 NetworkUnavailable：节点是否存在网络问题。 这些节点状态通过 kubectl describe node 或 API 查询来查看。\n7. 资源调度与驱逐 # kubelet 会根据节点的资源使用情况判断是否需要驱逐某些 Pod，保证节点的资源使用不会过载。例如，当节点的内存消耗过高时，kubelet 会触发驱逐策略，选择合适的 Pod 进行驱逐，从而避免节点崩溃。\n8. 故障监控与通知 # kubelet 会定期检测节点上的 Pod 是否健康，并在出现问题时发出警告。监控系统可以设置告警，及时通知运维人员。\nNode NotReady：当节点处于 NotReady 状态时，kubelet 会发出相关事件，表示该节点出现问题。 Pod Restart：如果 Pod 在节点上频繁重启，kubelet 会记录事件，表明 Pod 可能存在问题。 9. 日志监控 # kubelet 会记录关于节点、Pod 和容器的日志。通过集成日志收集系统（如 Fluentd 或 ELK Stack），可以集中管理和监控节点和容器的日志。 日志中包含了节点健康、容器启动、Pod 调度等信息。 10. 系统级监控 # 除了容器和 Kubernetes 资源的监控，kubelet 还可以通过集成 Linux 系统监控工具（如 sysstat、collectd）来监控操作系统层面的性能指标。这些指标可以提供操作系统级别的性能数据，帮助管理员分析节点的性能瓶颈。\n总结 # 在 Kubernetes 中，kubelet 通过多种方式监控 worker 节点 的健康状态，包括：\n健康检查（Liveness 和 Readiness Probe） 提供 /metrics 端点暴露节点和容器的资源使用数据 利用 Prometheus、Grafana 等工具进行指标收集和可视化 定期向 API Server 提交节点状态和事件信息 基于节点条件（如内存、磁盘、网络等）评估节点的健康状况 通过日志和事件跟踪节点和 Pod 的运行状态 这些监控方法确保了 Kubernetes 集群的高可用性和健康运行，同时能够及时发现和解决问题。\n容器时区不一致如何解决？ # 在容器化环境中，时区不一致的问题是比较常见的，特别是当容器和宿主机的时区配置不同或容器内没有正确的时区信息时。为了解决这个问题，通常可以通过以下几种方法来确保容器的时区一致性：\n1. 使用宿主机的时区 # 最简单的方式是让容器使用宿主机的时区。可以通过将宿主机的时区文件挂载到容器中来实现。\n操作方法： # 在启动容器时，使用 -v 参数将宿主机的时区文件挂载到容器中。 docker run -v /etc/localtime:/etc/localtime:ro -v /etc/timezone:/etc/timezone:ro my-container -v /etc/localtime:/etc/localtime:ro 让容器共享宿主机的时区配置。 -v /etc/timezone:/etc/timezone:ro 使容器中的 /etc/timezone 配置与宿主机一致。 在 Kubernetes 中，也可以通过类似的方式使用 hostPath 将宿主机的时区文件挂载到容器中。\napiVersion: v1 kind: Pod metadata: name: time-zone-pod spec: containers: - name: my-container image: my-container-image volumeMounts: - mountPath: /etc/localtime name: localtime readOnly: true - mountPath: /etc/timezone name: timezone readOnly: true volumes: - name: localtime hostPath: path: /etc/localtime - name: timezone hostPath: path: /etc/timezone 这种方法能确保容器和宿主机使用相同的时区。\n2. 设置容器内的时区 # 如果你希望容器使用特定的时区（例如与宿主机时区不同），可以在容器内设置时区。一般来说，可以通过以下两种方法来设置容器时区：\n方法 1: 修改 /etc/timezone 和 /etc/localtime # 你可以在 Dockerfile 中添加相应的指令来设置时区，例如： RUN apt-get update \u0026amp;\u0026amp; apt-get install -y tzdata RUN ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \u0026amp;\u0026amp; echo \u0026#34;Asia/Shanghai\u0026#34; \u0026gt; /etc/timezone tzdata 是时区数据库包，安装后可以选择设置时区。 ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 命令会创建一个符号链接，让容器使用上海时区。 通过 echo \u0026quot;Asia/Shanghai\u0026quot; \u0026gt; /etc/timezone 设置时区名称。 方法 2: 通过环境变量设置时区 # 在运行容器时，你可以通过设置环境变量来指定时区，通常容器内的应用程序会使用该环境变量来调整时区。 docker run -e TZ=Asia/Shanghai my-container TZ 环境变量是许多 Linux 系统和容器中常用的时区环境变量，应用程序（如 Python、Java）可以读取此变量来设置时区。 在 Kubernetes 中，你也可以通过设置环境变量 TZ 来设置时区：\napiVersion: v1 kind: Pod metadata: name: time-zone-pod spec: containers: - name: my-container image: my-container-image env: - name: TZ value: \u0026#34;Asia/Shanghai\u0026#34; 3. 使用 UTC 时区 # 如果应用不依赖特定时区，建议将容器的时区设置为 UTC，因为 UTC 是不受夏令时影响的标准时间，适用于大多数场景。\n可以在 Dockerfile 中直接设置容器时区为 UTC：\nRUN ln -sf /usr/share/zoneinfo/UTC /etc/localtime \u0026amp;\u0026amp; echo \u0026#34;UTC\u0026#34; \u0026gt; /etc/timezone 或者在运行容器时通过环境变量设置：\ndocker run -e TZ=UTC my-container 在 Kubernetes 中也是类似的做法：\napiVersion: v1 kind: Pod metadata: name: time-zone-pod spec: containers: - name: my-container image: my-container-image env: - name: TZ value: \u0026#34;UTC\u0026#34; 4. 使用 Docker Compose 或 Kubernetes 的 ConfigMap # 如果你需要在多个容器中统一设置时区，可以通过 Docker Compose 或 Kubernetes ConfigMap 来集中管理时区配置。\nDocker Compose 示例： # 在 docker-compose.yml 文件中设置时区环境变量：\nversion: \u0026#39;3\u0026#39; services: app: image: my-container environment: - TZ=Asia/Shanghai Kubernetes ConfigMap 示例： # 你可以创建一个 ConfigMap 来设置时区，并将其挂载到多个容器中：\napiVersion: v1 kind: ConfigMap metadata: name: time-zone-config data: timezone: \u0026#34;Asia/Shanghai\u0026#34; --- apiVersion: v1 kind: Pod metadata: name: time-zone-pod spec: containers: - name: my-container image: my-container-image envFrom: - configMapRef: name: time-zone-config 总结 # 容器时区不一致的问题可以通过以下几种方式解决：\n使用宿主机时区：通过挂载宿主机的时区文件（如 /etc/localtime 和 /etc/timezone）。 容器内设置时区：通过设置 /etc/timezone 和 /etc/localtime 或设置环境变量 TZ。 使用 UTC 时区：容器设置为 UTC，避免时区变化带来的问题。 集中管理时区：通过 Docker Compose 或 Kubernetes ConfigMap 管理和统一容器时区。 这些方法都能帮助确保容器中的时区与预期一致，避免时区差异导致的各种问题。\n"},{"id":25,"href":"/docs/03-%E5%AE%89%E8%A3%85%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6-03--an-zhuang-rong-qi-yun-xing-shi/","title":"03-安装容器运行时 2023-09-28 15:23:56.241","section":"Docs","content":" 03-安装容器运行时 # 项目根据k8s版本提供不同的默认容器运行时：\nk8s 版本 \u0026lt; 1.24 时，支持docker containerd 可选 k8s 版本 \u0026gt;= 1.24 时，仅支持 containerd 安装containerd # 作为 CNCF 毕业项目，containerd 致力于提供简洁、可靠、可扩展的容器运行时；它被设计用来集成到 kubernetes 等系统使用，而不是像 docker 那样独立使用。\n安装指南 https://github.com/containerd/cri/blob/master/docs/installation.md 客户端 circtl 使用指南 https://github.com/containerd/cri/blob/master/docs/crictl.md man 文档 https://github.com/containerd/containerd/tree/master/docs/man kubeasz 集成安装 containerd # 注意：k8s 1.24以后，项目已经设置默认容器运行时为 containerd，无需手动修改 执行安装：分步安装ezctl setup xxxx 03，一键安装ezctl setup xxxx all 命令对比 # 命令 docker crictl（推荐） ctr 查看容器列表 docker ps crictl ps ctr -n k8s.io c ls 查看容器详情 docker inspect crictl inspect ctr -n k8s.io c info 查看容器日志 docker logs crictl logs 无 容器内执行命令 docker exec crictl exec 无 挂载容器 docker attach crictl attach 无 容器资源使用 docker stats crictl stats 无 创建容器 docker create crictl create ctr -n k8s.io c create 启动容器 docker start crictl start ctr -n k8s.io run 停止容器 docker stop crictl stop 无 删除容器 docker rm crictl rm ctr -n k8s.io c del 查看镜像列表 docker images crictl images ctr -n k8s.io i ls 查看镜像详情 docker inspect crictl inspecti 无 拉取镜像 docker pull crictl pull ctr -n k8s.io i pull 推送镜像 docker push 无 ctr -n k8s.io i push 删除镜像 docker rmi crictl rmi ctr -n k8s.io i rm 查看Pod列表 无 crictl pods 无 查看Pod详情 无 crictl inspectp 无 启动Pod 无 crictl runp 无 停止Pod 无 crictl stopp 无 [后一篇](\n"},{"id":26,"href":"/docs/04-%E5%AE%89%E8%A3%85kube_master%E8%8A%82%E7%82%B9-04--an-zhuang-kubemaster-jie-dian/","title":"04-安装kube_master节点 2023-09-28 15:24:21.203","section":"Docs","content":" 04-安装kube_master节点 # 部署master节点主要包含三个组件apiserver scheduler controller-manager，其中： # apiserver提供集群管理的REST API接口，包括认证授权、数据校验以及集群状态变更等 # 只有API Server才直接操作etcd # 其他模块通过API Server查询或修改数据 # 提供其他模块之间的数据交互和通信的枢纽 # scheduler负责分配调度Pod到集群内的node节点 # 监听kube-apiserver，查询还未分配Node的Pod # 根据调度策略为这些Pod分配节点 # controller-manager由一系列的控制器组成，它通过apiserver监控整个集群的状态，并确保集群处于预期的工作状态 # 高可用机制 # apiserver 无状态服务，可以通过外部负载均衡实现高可用，如项目采用的两种高可用架构：HA-1x (#584)和 HA-2x (#585) # controller-manager 组件启动时会进行类似选举（leader）；当多副本存在时，如果原先leader挂掉，那么会选举出新的leader，从而保证高可用； # scheduler 类似选举机制 # 安装流程 # cat playbooks/04.kube-master.yml - hosts: kube_master roles: - kube-lb # 四层负载均衡，监听在127.0.0.1:6443，转发到真实master节点apiserver服务 - kube-master # - kube-node # 因为网络、监控等daemonset组件，master节点也推荐安装kubelet和kube-proxy服务 ... 创建 kubernetes 证书签名请求 # { \u0026#34;CN\u0026#34;: \u0026#34;kubernetes\u0026#34;, \u0026#34;hosts\u0026#34;: [ \u0026#34;127.0.0.1\u0026#34;, {% if groups[\u0026#39;ex_lb\u0026#39;]|length \u0026gt; 0 %} \u0026#34;{{ hostvars[groups[\u0026#39;ex_lb\u0026#39;][0]][\u0026#39;EX_APISERVER_VIP\u0026#39;] }}\u0026#34;, {% endif %} {% for host in groups[\u0026#39;kube_master\u0026#39;] %} \u0026#34;{{ host }}\u0026#34;, {% endfor %} \u0026#34;{{ CLUSTER_KUBERNETES_SVC_IP }}\u0026#34;, {% for host in MASTER_CERT_HOSTS %} \u0026#34;{{ host }}\u0026#34;, {% endfor %} \u0026#34;kubernetes\u0026#34;, \u0026#34;kubernetes.default\u0026#34;, \u0026#34;kubernetes.default.svc\u0026#34;, \u0026#34;kubernetes.default.svc.cluster\u0026#34;, \u0026#34;kubernetes.default.svc.cluster.local\u0026#34; ], \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;CN\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;HangZhou\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;XS\u0026#34;, \u0026#34;O\u0026#34;: \u0026#34;k8s\u0026#34;, \u0026#34;OU\u0026#34;: \u0026#34;System\u0026#34; } ] } kubernetes apiserver 使用对等证书，创建时hosts字段需要配置： # 如果配置 ex_lb，需要把 EX_APISERVER_VIP 也配置进去 # 如果需要外部访问 apiserver，可选在config.yml配置 MASTER_CERT_HOSTS # kubectl get svc 将看到集群中由api-server 创建的默认服务 kubernetes，因此也要把 kubernetes 服务名和各个服务域名也添加进去 # 创建apiserver的服务配置文件 # [Unit] Description=Kubernetes API Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] ExecStart={{ bin_dir }}/kube-apiserver \\ --allow-privileged=true \\ --anonymous-auth=false \\ --api-audiences=api,istio-ca \\ --authorization-mode=Node,RBAC \\ --bind-address={{ inventory_hostname }} \\ --client-ca-file={{ ca_dir }}/ca.pem \\ --endpoint-reconciler-type=lease \\ --etcd-cafile={{ ca_dir }}/ca.pem \\ --etcd-certfile={{ ca_dir }}/kubernetes.pem \\ --etcd-keyfile={{ ca_dir }}/kubernetes-key.pem \\ --etcd-servers={{ ETCD_ENDPOINTS }} \\ --kubelet-certificate-authority={{ ca_dir }}/ca.pem \\ --kubelet-client-certificate={{ ca_dir }}/kubernetes.pem \\ --kubelet-client-key={{ ca_dir }}/kubernetes-key.pem \\ --secure-port={{ SECURE_PORT }} \\ --service-account-issuer=https://kubernetes.default.svc \\ --service-account-signing-key-file={{ ca_dir }}/ca-key.pem \\ --service-account-key-file={{ ca_dir }}/ca.pem \\ --service-cluster-ip-range={{ SERVICE_CIDR }} \\ --service-node-port-range={{ NODE_PORT_RANGE }} \\ --tls-cert-file={{ ca_dir }}/kubernetes.pem \\ --tls-private-key-file={{ ca_dir }}/kubernetes-key.pem \\ --requestheader-client-ca-file={{ ca_dir }}/ca.pem \\ --requestheader-allowed-names= \\ --requestheader-extra-headers-prefix=X-Remote-Extra- \\ --requestheader-group-headers=X-Remote-Group \\ --requestheader-username-headers=X-Remote-User \\ --proxy-client-cert-file={{ ca_dir }}/aggregator-proxy.pem \\ --proxy-client-key-file={{ ca_dir }}/aggregator-proxy-key.pem \\ --enable-aggregator-routing=true \\ --v=2 Restart=always RestartSec=5 Type=notify LimitNOFILE=65536 [Install] WantedBy=multi-user.target Kubernetes 对 API 访问需要依次经过认证、授权和准入控制(admission controll)，认证解决用户是谁的问题，授权解决用户能做什么的问题，Admission Control则是资源管理方面的作用。 # 关于authorization-mode=Node,RBAC v1.7+支持Node授权，配合NodeRestriction准入控制来限制kubelet仅可访问node、endpoint、pod、service以及secret、configmap、PV和PVC等相关的资源；需要注意的是v1.7中Node 授权是默认开启的，v1.8中需要显式配置开启，否则 Node无法正常工作 # 详细参数配置请参考kube-apiserver --help，关于认证、授权和准入控制请阅读 # 增加了访问kubelet使用的证书配置，防止匿名访问kubelet的安全漏洞，详见漏洞说明 # 创建controller-manager 的服务文件 # [Unit] Description=Kubernetes Controller Manager Documentation=https://github.com/GoogleCloudPlatform/kubernetes [Service] ExecStart={{ bin_dir }}/kube-controller-manager \\ --allocate-node-cidrs=true \\ --authentication-kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \\ --authorization-kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \\ --bind-address=0.0.0.0 \\ --cluster-cidr={{ CLUSTER_CIDR }} \\ --cluster-name=kubernetes \\ --cluster-signing-cert-file={{ ca_dir }}/ca.pem \\ --cluster-signing-key-file={{ ca_dir }}/ca-key.pem \\ --kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \\ --leader-elect=true \\ --node-cidr-mask-size={{ NODE_CIDR_LEN }} \\ --root-ca-file={{ ca_dir }}/ca.pem \\ --service-account-private-key-file={{ ca_dir }}/ca-key.pem \\ --service-cluster-ip-range={{ SERVICE_CIDR }} \\ --use-service-account-credentials=true \\ --v=2 Restart=always RestartSec=5 [Install] WantedBy=multi-user.target \u0026ndash;cluster-cidr 指定 Cluster 中 Pod 的 CIDR 范围，该网段在各 Node 间必须路由可达(flannel/calico 等网络插件实现) # \u0026ndash;service-cluster-ip-range 参数指定 Cluster 中 Service 的CIDR范围，必须和 kube-apiserver 中的参数一致 # \u0026ndash;cluster-signing-* 指定的证书和私钥文件用来签名为 TLS BootStrap 创建的证书和私钥 # \u0026ndash;root-ca-file 用来对 kube-apiserver 证书进行校验，指定该参数后，才会在Pod 容器的 ServiceAccount 中放置该 CA 证书文件 # \u0026ndash;leader-elect=true 使用多节点选主的方式选择主节点。只有主节点才会启动所有控制器，而其他从节点则仅执行选主算法 # 创建scheduler 的服务文件 # [Unit] Description=Kubernetes Scheduler Documentation=https://github.com/GoogleCloudPlatform/kubernetes [Service] ExecStart={{ bin_dir }}/kube-scheduler \\ --authentication-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \\ --authorization-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \\ --bind-address=0.0.0.0 \\ --kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \\ --leader-elect=true \\ --v=2 Restart=always RestartSec=5 [Install] WantedBy=multi-user.target \u0026ndash;leader-elect=true 部署多台机器组成的 master 集群时选举产生一个处于工作状态的 kube-controller-manager 进程 # 在master 节点安装 node 服务: kubelet kube-proxy # 项目master 分支使用 DaemonSet 方式安装网络插件，如果master 节点不安装 kubelet 服务是无法安装网络插件的，如果 master 节点不安装网络插件，那么通过apiserver 方式无法访问 dashboard kibana等管理界面，ISSUES #130 # 在master 节点也同时成为 node 节点后，默认业务 POD也会调度到 master节点；可以使用 kubectl cordon命令禁止业务 POD调度到 master节点。 # master 集群的验证 # 运行 ansible-playbook 04.kube-master.yml 成功后，验证 master节点的主要组件： # # 查看进程状态 systemctl status kube-apiserver systemctl status kube-controller-manager systemctl status kube-scheduler # 查看进程运行日志 journalctl -u kube-apiserver journalctl -u kube-controller-manager journalctl -u kube-scheduler 执行 kubectl get componentstatus 可以看到\nNAME STATUS MESSAGE ERROR scheduler Healthy ok controller-manager Healthy ok etcd-0 Healthy {\u0026#34;health\u0026#34;: \u0026#34;true\u0026#34;} etcd-2 Healthy {\u0026#34;health\u0026#34;: \u0026#34;true\u0026#34;} etcd-1 Healthy {\u0026#34;health\u0026#34;: \u0026#34;true\u0026#34;} 后一篇\n"},{"id":27,"href":"/docs/05-%E5%AE%89%E8%A3%85kube_node%E8%8A%82%E7%82%B9-05--an-zhuang-kubenode-jie-dian/","title":"05-安装kube_node节点 2023-09-28 15:24:43.201","section":"Docs","content":" 05-安装kube_node节点 # kube_node 是集群中运行工作负载的节点，前置条件需要先部署好kube_master节点，它需要部署如下组件： # cat playbooks/05.kube-node.yml - hosts: kube_node roles: - { role: kube-lb, when: \u0026#34;inventory_hostname not in groups[\u0026#39;kube_master\u0026#39;]\u0026#34; } - { role: kube-node, when: \u0026#34;inventory_hostname not in groups[\u0026#39;kube_master\u0026#39;]\u0026#34; } kube-lb：由nginx裁剪编译的四层负载均衡，用于将请求转发到主节点的 apiserver服务 # kubelet：kube_node上最主要的组件 # kube-proxy： 发布应用服务与负载均衡 # 创建cni 基础网络插件配置文件 # 因为后续需要用 DaemonSet Pod方式运行k8s网络插件，所以kubelet.server服务必须开启cni相关参数，并且提供cni网络配置文件 # 创建 kubelet 的服务文件 # 根据官方建议独立使用 kubelet 配置文件，详见roles/kube-node/templates/kubelet-config.yaml.j2 # 必须先创建工作目录 /var/lib/kubelet # [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/GoogleCloudPlatform/kubernetes [Service] WorkingDirectory=/var/lib/kubelet ExecStartPre=/bin/mount -o remount,rw \u0026#39;/sys/fs/cgroup\u0026#39; {% if KUBE_RESERVED_ENABLED == \u0026#34;yes\u0026#34; or SYS_RESERVED_ENABLED == \u0026#34;yes\u0026#34; %} ExecStartPre=/bin/mkdir -p /sys/fs/cgroup/cpu/podruntime.slice ExecStartPre=/bin/mkdir -p /sys/fs/cgroup/cpuacct/podruntime.slice ExecStartPre=/bin/mkdir -p /sys/fs/cgroup/cpuset/podruntime.slice ExecStartPre=/bin/mkdir -p /sys/fs/cgroup/memory/podruntime.slice ExecStartPre=/bin/mkdir -p /sys/fs/cgroup/pids/podruntime.slice ExecStartPre=/bin/mkdir -p /sys/fs/cgroup/systemd/podruntime.slice ExecStartPre=/bin/mkdir -p /sys/fs/cgroup/cpu/system.slice ExecStartPre=/bin/mkdir -p /sys/fs/cgroup/cpuacct/system.slice ExecStartPre=/bin/mkdir -p /sys/fs/cgroup/cpuset/system.slice ExecStartPre=/bin/mkdir -p /sys/fs/cgroup/memory/system.slice ExecStartPre=/bin/mkdir -p /sys/fs/cgroup/pids/system.slice ExecStartPre=/bin/mkdir -p /sys/fs/cgroup/systemd/system.slice ExecStartPre=/bin/mkdir -p /sys/fs/cgroup/hugetlb/podruntime.slice ExecStartPre=/bin/mkdir -p /sys/fs/cgroup/hugetlb/system.slice {% endif %} ExecStart={{ bin_dir }}/kubelet \\ --config=/var/lib/kubelet/config.yaml \\ --container-runtime-endpoint=unix:///run/containerd/containerd.sock \\ --hostname-override={{ K8S_NODENAME }} \\ --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\ --root-dir={{ KUBELET_ROOT_DIR }} \\ --v=2 Restart=always RestartSec=5 [Install] WantedBy=multi-user.target \u0026ndash;ExecStartPre=/bin/mkdir -p xxx 对于某些系统（centos7）cpuset和hugetlb 是默认没有初始化system.slice 的，需要手动创建，否则在启用\u0026ndash;kube-reserved-cgroup 时会报错Failed to start ContainerManager Failed to enforce System Reserved Cgroup Limits # 关于kubelet资源预留相关配置请参考 https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/ # 创建 kube-proxy kubeconfig 文件 # 该步骤已经在 deploy节点完成，roles/deploy/tasks/main.yml # 生成的kube-proxy.kubeconfig 配置文件需要移动到/etc/kubernetes/目录，后续kube-proxy服务启动参数里面需要指定 # 创建 kube-proxy服务文件 # [Unit] Description=Kubernetes Kube-Proxy Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] WorkingDirectory=/var/lib/kube-proxy ExecStart={{ bin_dir }}/kube-proxy \\ --config=/var/lib/kube-proxy/kube-proxy-config.yaml Restart=always RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target 请注意 kube-proxy-config 文件的注释说明\n验证 node 状态 # systemctl status kubelet\t# 查看状态 systemctl status kube-proxy journalctl -u kubelet\t# 查看日志 journalctl -u kube-proxy systemctl restart kubelet systemctl restart kube-proxy 运行 kubectl get node 可以看到类似\nNAME STATUS ROLES AGE VERSION 192.168.1.42 Ready \u0026lt;none\u0026gt; 2d v1.9.0 192.168.1.43 Ready \u0026lt;none\u0026gt; 2d v1.9.0 192.168.1.44 Ready \u0026lt;none\u0026gt; 2d v1.9.0 [后一篇](\n"},{"id":28,"href":"/docs/06-%E5%AE%89%E8%A3%85calico%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6-06--an-zhuang-calico-wang-luo-zu-jian/","title":"06-安装calico网络组件 2023-09-28 15:34:28.483","section":"Docs","content":" 06-安装calico网络组件.md # calico 是k8s社区最流行的网络插件之一，也是k8s-conformance test 默认使用的网络插件，功能丰富，支持network policy；是当前kubeasz项目的默认网络插件。 # 如果需要安装calico，请在clusters/xxxx/hosts文件中设置变量 CLUSTER_NETWORK=\u0026quot;calico\u0026quot;，参考这里 # roles/calico/ ├── tasks │ └── main.yml ├── templates │ ├── calico-csr.json.j2 │ ├── calicoctl.cfg.j2 │ ├── calico-v3.15.yaml.j2 │ ├── calico-v3.19.yaml.j2 │ └── calico-v3.8.yaml.j2 └── vars └── main.yml 请在另外窗口打开roles/calico/tasks/main.yml文件，对照看以下讲解内容。\n创建calico 证书申请 # { \u0026#34;CN\u0026#34;: \u0026#34;calico\u0026#34;, \u0026#34;hosts\u0026#34;: [], \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;CN\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;HangZhou\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;XS\u0026#34;, \u0026#34;O\u0026#34;: \u0026#34;k8s\u0026#34;, \u0026#34;OU\u0026#34;: \u0026#34;System\u0026#34; } ] } calico 使用客户端证书，所以hosts字段可以为空；后续可以看到calico证书用在四个地方： # calico/node 这个docker 容器运行时访问 etcd 使用证书 # cni 配置文件中，cni 插件需要访问 etcd 使用证书 # calicoctl 操作集群网络时访问 etcd 使用证书 # calico/kube-controllers 同步集群网络策略时访问 etcd 使用证书 # 创建 calico DaemonSet yaml文件和rbac 文件 # 请对照 roles/calico/templates/calico.yaml.j2文件注释和以下注意内容 详细配置参数请参考[calico官方文档](https://projectcalico.docs.tigera.io/reference/node/configuration) 配置ETCD_ENDPOINTS 、CA、证书等，所有{{ }}变量与ansible hosts文件中设置对应 配置集群POD网络 CALICO_IPV4POOL_CIDR={{ CLUSTER_CIDR }} 配置FELIX_DEFAULTENDPOINTTOHOSTACTION=ACCEPT 默认允许Pod到Node的网络流量，更多[felix配置选项 https://projectcalico.docs.tigera.io/reference/felix/configuration) # 安装calico 网络 # 安装前检查主机名不能有大写字母，只能由小写字母 - . 组成 (name must consist of lower case alphanumeric characters, \u0026lsquo;-\u0026rsquo; or \u0026lsquo;.\u0026rsquo; (regex: a-z0-9?(.a-z0-9?)*))(calico-node v3.0.6以上已经解决主机大写字母问题) # 安装前必须确保各节点主机名不重复 ，calico node name 由节点主机名决定，如果重复，那么重复节点在etcd中只存储一份配置，BGP 邻居也不会建立。 # 安装之前必须确保kube_master和kube_node节点已经成功部署 # 轮询等待calico 网络插件安装完成，删除之前kube_node安装时默认cni网络配置 # [可选]配置calicoctl工具 calicoctl.cfg.j2 # apiVersion: projectcalico.org/v3 kind: CalicoAPIConfig metadata: spec: datastoreType: \u0026#34;etcdv3\u0026#34; etcdEndpoints: {{ ETCD_ENDPOINTS }} etcdKeyFile: /etc/calico/ssl/calico-key.pem etcdCertFile: /etc/calico/ssl/calico.pem etcdCACertFile: {{ ca_dir }}/ca.pem 验证calico网络 # 执行calico安装成功后可以验证如下：(需要等待镜像下载完成，有时候即便上一步已经配置了docker国内加速，还是可能比较慢，请确认以下容器运行起来以后，再执行后续验证步骤) # kubectl get pod --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-5c6b98d9df-xj2n4 1/1 Running 0 1m kube-system calico-node-4hr52 2/2 Running 0 1m kube-system calico-node-8ctc2 2/2 Running 0 1m kube-system calico-node-9t8md 2/2 Running 0 1m 查看网卡和路由信息\n先在集群创建几个测试pod: kubectl run test --image=busybox --replicas=3 sleep 30000 # # 查看网卡信息 ip a 可以看到包含类似cali1cxxx的网卡，是calico为测试pod生成的 # tunl0网卡现在不用管，是默认生成的，当开启IPIP 特性时使用的隧道 # # 查看路由 route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 192.168.1.1 0.0.0.0 UG 0 0 0 ens3 192.168.1.0 0.0.0.0 255.255.255.0 U 0 0 0 ens3 172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0 172.20.3.64 192.168.1.34 255.255.255.192 UG 0 0 0 ens3 172.20.33.128 0.0.0.0 255.255.255.192 U 0 0 0 * 172.20.33.129 0.0.0.0 255.255.255.255 UH 0 0 0 caliccc295a6d4f 172.20.104.0 192.168.1.35 255.255.255.192 UG 0 0 0 ens3 172.20.166.128 192.168.1.63 255.255.255.192 UG 0 0 0 ens3 查看所有calico节点状态\ncalicoctl node status Calico process is running. IPv4 BGP status +--------------+-------------------+-------+----------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +--------------+-------------------+-------+----------+-------------+ | 192.168.1.34 | node-to-node mesh | up | 12:34:00 | Established | | 192.168.1.35 | node-to-node mesh | up | 12:34:00 | Established | | 192.168.1.63 | node-to-node mesh | up | 12:34:01 | Established | +--------------+-------------------+-------+----------+-------------+ BGP 协议是通过TCP 连接来建立邻居的，因此可以用netstat 命令验证 BGP Peer\nnetstat -antlp|grep ESTABLISHED|grep 179 tcp 0 0 192.168.1.66:179 192.168.1.35:41316 ESTABLISHED 28479/bird tcp 0 0 192.168.1.66:179 192.168.1.34:40243 ESTABLISHED 28479/bird tcp 0 0 192.168.1.66:179 192.168.1.63:48979 ESTABLISHED 28479/bird 查看etcd中calico相关信息\n因为这里calico网络使用etcd存储数据，所以可以在etcd集群中查看数据 # calico 3.x 版本默认使用 etcd v3存储，登录集群的一个etcd 节点，查看命令： # # 查看所有calico相关数据 ETCDCTL_API=3 etcdctl --endpoints=\u0026#34;http://127.0.0.1:2379\u0026#34; get --prefix /calico # 查看 calico网络为各节点分配的网段 ETCDCTL_API=3 etcdctl --endpoints=\u0026#34;http://127.0.0.1:2379\u0026#34; get --prefix /calico/ipam/v2/host 下一步：设置 BGP Route Reflector # "},{"id":29,"href":"/docs/06-%E5%AE%89%E8%A3%85cilium%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6-06--an-zhuang-cilium-wang-luo-zu-jian/","title":"06-安装cilium网络组件 2023-09-28 15:35:23.884","section":"Docs","content":" 06-安装cilium网络组件 # cilium 是一个革新的网络与安全组件；基于 linux 内核新技术\u0026ndash;BPF，它可以透明、零侵入地实现服务间安全策略与可视化，主要优势如下：\n支持L3/L4, L7(如：HTTP/gRPC/Kafka)的安全策略 支持基于安全ID而不是地址+端口的传统防火墙策略 支持基于Overlay或Native Routing的扁平多节点pod网络 Overlay VXLAN 方式类似于 flannel 的VXLAN后端 高性能负载均衡，支持DSR 支持事件、策略跟踪和监控集成 cilium 项目当前文档比较完整，建议仔细阅读下官网文档\nkubeasz 集成安装 cilium # kubeasz 3.3.1 更新重写了cilium 安装流程，使用helm charts 方式，配置文件在 roles/cilium/templates/values.yaml.j2，请阅读原charts中values.yaml 文件后自定义修改。\n相关镜像已经离线打包并推送到本地镜像仓库，通过 ezdown -X 命令下载cilium等额外镜像 0.升级内核并重启 # Linux kernel \u0026gt;= 4.9.17，请阅读文档升级内核 etcd \u0026gt;= 3.1.0 or consul \u0026gt;= 0.6.4 1.选择cilium网络后安装 # 参考快速指南，设置/etc/kubeasz/clusters/xxx/hosts文件中变量 CLUSTER_NETWORK=\u0026quot;cilium\u0026quot; 下载额外镜像 ./ezdown -X cilium 和 ./ezdown -X network-check 执行集群安装 dk ezctl setup xxx all 注意默认安装后集成了cilium_connectivity_check 和 cilium_hubble，可以在/etc/kubeasz/clusters/xxx/config.yml配置关闭\ncilium_connectivity_check：检查集群cilium网络是否工作正常，非常实用 cilium_hubble：很酷很实用的监控、策略追踪排查工具 Cilium CLI 和 Hubble CLI 二进制已经默认包含在kubeasz-ext-bin 1.2.0版本中 https://github.com/kubeasz/dockerfiles/blob/master/kubeasz-ext-bin/Dockerfile\n2.验证 # 一键安装完成后如下，注意cilium_connectivity_check 中带multi-node的检查任务需要多节点集群才能完成\nkubectl get pod -A NAMESPACE NAME READY STATUS RESTARTS AGE cilium-test echo-a-5dd478f5d8-74xg5 1/1 Running 0 3m10s cilium-test echo-b-78c79f6cdd-t9vk6 1/1 Running 0 3m10s cilium-test echo-b-host-75c44b897-c8f5m 1/1 Running 0 3m10s cilium-test host-to-b-multi-node-clusterip-7895fd494c-92cb2 1/1 Running 0 2m59s cilium-test host-to-b-multi-node-headless-74bbc877b5-ffxxx 1/1 Running 0 2m59s cilium-test pod-to-a-allowed-cnp-598fc5c547-b885q 1/1 Running 0 2m59s cilium-test pod-to-a-b8b456c99-r6272 1/1 Running 0 2m59s cilium-test pod-to-a-denied-cnp-c78c44f5c-7xhkw 1/1 Running 0 2m59s cilium-test pod-to-b-intra-node-nodeport-6ccdb55779-j8gnd 1/1 Running 0 2m59s cilium-test pod-to-b-multi-node-clusterip-55d8448b5c-5b4nj 1/1 Running 0 2m59s cilium-test pod-to-b-multi-node-headless-5fbf655bb9-pszpr 1/1 Running 0 2m59s cilium-test pod-to-b-multi-node-nodeport-65f5b95569-qglb7 1/1 Running 0 2m59s cilium-test pod-to-external-1111-64496c754c-bvqlt 1/1 Running 0 2m59s cilium-test pod-to-external-fqdn-allow-baidu-cnp-6f96597855-c84zs 1/1 Running 0 2m59s kube-system cilium-7trcs 1/1 Running 0 3m42s kube-system cilium-hvclp 1/1 Running 0 3m42s kube-system cilium-operator-8566689975-vcxpp 1/1 Running 0 3m42s kube-system cilium-pw2sv 1/1 Running 0 3m42s kube-system cilium-qppnc 1/1 Running 0 3m42s kube-system coredns-84b58f6b4-m8x7s 1/1 Running 0 3m20s kube-system dashboard-metrics-scraper-864d79d497-92l2w 1/1 Running 0 3m14s kube-system hubble-relay-655dc744d7-8d9n7 1/1 Running 0 3m42s kube-system hubble-ui-54599d7967-lfkvk 2/2 Running 0 3m42s kube-system kubernetes-dashboard-5fc74cf5c6-pqdvc 1/1 Running 0 3m14s kube-system metrics-server-69797698d4-2jbg8 1/1 Running 0 3m17s kube-system node-local-dns-5n8gc 1/1 Running 0 3m19s kube-system node-local-dns-5pm2p 1/1 Running 0 3m19s kube-system node-local-dns-9x229 1/1 Running 0 3m19s kube-system node-local-dns-jz8lj 1/1 Running 0 3m19s 检查 cilium 节点状态\ncilium status /¯¯\\ /¯¯\\__/¯¯\\ Cilium: OK \\__/¯¯\\__/ Operator: OK /¯¯\\__/¯¯\\ Hubble: OK \\__/¯¯\\__/ ClusterMesh: disabled \\__/ DaemonSet cilium Desired: 4, Ready: 4/4, Available: 4/4 Deployment cilium-operator Desired: 1, Ready: 1/1, Available: 1/1 Deployment hubble-relay Desired: 1, Ready: 1/1, Available: 1/1 Deployment hubble-ui Desired: 1, Ready: 1/1, Available: 1/1 Containers: cilium Running: 4 cilium-operator Running: 1 hubble-relay Running: 1 hubble-ui Running: 1 Cluster Pods: 17/17 managed by Cilium Image versions hubble-relay easzlab.io.local:5000/cilium/hubble-relay:v1.11.6: 1 hubble-ui easzlab.io.local:5000/cilium/hubble-ui:v0.9.0: 1 hubble-ui easzlab.io.local:5000/cilium/hubble-ui-backend:v0.9.0: 1 cilium easzlab.io.local:5000/cilium/cilium:v1.11.6: 4 cilium-operator easzlab.io.local:5000/cilium/operator-generic:v1.11.6: 1 cilium network policy # cilium network policy 提供了比k8s network policy更丰富的网络安全策略功能，有兴趣的请阅读官网文档，以下是一个有趣的小例子：\n星战死星登陆系统 "},{"id":30,"href":"/docs/06-%E5%AE%89%E8%A3%85flannel%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6-06--an-zhuang-flannel-wang-luo-zu-jian/","title":"06-安装flannel网络组件 2023-09-28 15:35:50.039","section":"Docs","content":" 06-安装flannel网络组件.md # Flannel是最早应用到k8s集群的网络插件之一，简单高效，且提供多个后端backend模式供选择；本文介绍以DaemonSet Pod方式集成到k8s集群，需要在所有master节点和node节点安装。\nkubeasz 集成安装flannel # 参考快速指南，设置/etc/kubeasz/clusters/xxx/hosts文件中变量 CLUSTER_NETWORK=\u0026quot;flannel\u0026quot; 下载额外镜像 ./ezdown -X flannel 执行集群安装 dk ezctl setup xxx all 配置介绍 # Flannel CNI 插件的配置文件可以包含多个plugin 或由其调用其他plugin；Flannel DaemonSet Pod运行以后会生成/run/flannel/subnet.env 文件，例如：\nFLANNEL_NETWORK=10.1.0.0/16 FLANNEL_SUBNET=10.1.17.1/24 FLANNEL_MTU=1472 FLANNEL_IPMASQ=true 然后它利用这个文件信息去配置和调用bridge插件来生成容器网络，调用host-local来管理IP地址，例如：\n{ \u0026#34;name\u0026#34;: \u0026#34;mynet\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;bridge\u0026#34;, \u0026#34;mtu\u0026#34;: 1472, \u0026#34;ipMasq\u0026#34;: false, \u0026#34;isGateway\u0026#34;: true, \u0026#34;ipam\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;host-local\u0026#34;, \u0026#34;subnet\u0026#34;: \u0026#34;10.1.17.0/24\u0026#34; } } 更多相关介绍请阅读：\nflannel kubernetes 集成 flannel cni 插件 更多 cni 插件 Flannel DaemonSet yaml配置文件\n请阅读 roles/flannel/templates/kube-flannel.yaml.j2 内容，注意：\n注意：本安装方式，flannel 通过 apiserver 接口读取 podCidr 信息，详见 https://github.com/coreos/flannel/issues/847；因此想要修改节点pod网段掩码，请在clusters/xxxx/config.yml 中修改NODE_CIDR_LEN配置项 配置相关RBAC 权限和 service account 配置ConfigMap包含 CNI配置和 flannel配置(指定backend等)，在文件中相关设置对应 验证flannel网络 # 执行flannel安装成功后可以验证如下：(需要等待镜像下载完成，有时候即便上一步已经配置了docker国内加速，还是可能比较慢，请确认以下容器运行起来以后，再执行后续验证步骤)\n# kubectl get pod --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system kube-flannel-ds-m8mzm 1/1 Running 0 3m kube-system kube-flannel-ds-mnj6j 1/1 Running 0 3m kube-system kube-flannel-ds-mxn6k 1/1 Running 0 3m 在集群创建几个测试pod: kubectl run test --image=busybox --replicas=3 sleep 30000\n# kubectl get pod --all-namespaces -o wide|head -n 4 NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE default busy-5956b54c8b-ld4gb 1/1 Running 0 9m 172.20.2.7 192.168.1.1 default busy-5956b54c8b-lj9l9 1/1 Running 0 9m 172.20.1.5 192.168.1.2 default busy-5956b54c8b-wwpkz 1/1 Running 0 9m 172.20.0.6 192.168.1.3 # 查看路由 # ip route default via 192.168.1.254 dev ens3 onlink 192.168.1.0/24 dev ens3 proto kernel scope link src 192.168.1.1 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown 172.20.0.0/24 via 192.168.1.3 dev ens3 172.20.1.0/24 via 192.168.1.2 dev ens3 172.20.2.0/24 dev cni0 proto kernel scope link src 172.20.2.1 在各节点上分别 ping 这三个POD IP地址，确保能通：\nping 172.20.2.7 ping 172.20.1.5 ping 172.20.0.6 "},{"id":31,"href":"/docs/06-%E5%AE%89%E8%A3%85kube-ovn%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6-06--an-zhuang-kube-ovn-wang-luo-zu-jian/","title":"06-安装kube-ovn网络组件 2023-09-28 15:36:11.265","section":"Docs","content":" 06-安装kube-ovn网络组件.md # (以下文档暂未更新，以插件官网文档为准)\n由灵雀云开源的网络组件 kube-ovn，将已被 openstack 社区采用的成熟网络虚拟化技术 ovs/ovn 引入 kubernetes 平台；为 kubernetes 网络打开了新的大门，令人耳目一新；强烈推荐大家试用该网络组件，反馈建议以帮助项目早日走向成熟。\n介绍 https://blog.csdn.net/alauda_andy/article/details/88886128 项目地址 https://github.com/alauda/kube-ovn 特性介绍 # kube-ovn 提供了针对企业应用场景下容器网络实用功能，并为实现更高级的网络管理控制提供了可能性；现有主要功能:\n1.Namespace 和子网的绑定，以及子网间的访问控制; 2.静态IP分配; 3.动态QoS; 4.分布式和集中式网关; 5.内嵌 LoadBalancer; 6.Pod IP对外直接暴露 7.流量镜像 8.IPv6 kubeasz 集成安装 kube-ovn # kube-ovn 的安装十分简单，详见项目的安装文档；基于 kubeasz，以下两步将安装一个集成了 kube-ovn 网络的 k8s 集群；\n在 ansible hosts 中设置变量 CLUSTER_NETWORK=\u0026quot;kube-ovn\u0026quot; 执行安装 ansible-playbook 90.setup.yml 或者 ezctl setup kubeasz 项目为kube-ovn网络生成的 ansible role 如下：\nroles/kube-ovn ├── defaults │ └── main.yml\t# kube-ovn 相关配置文件 ├── tasks │ └── main.yml\t# 安装执行文件 └── templates ├── crd.yaml.j2\t# crd 模板 ├── kube-ovn.yaml.j2\t# kube-ovn yaml 模板 └── ovn.yaml.j2\t# ovn yaml 模板 安装成功后，可以验证所有 k8s 集群功能正常，查看集群的 pod 网络如下：\n$ kubectl get pod --all-namespaces -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES kube-ovn kube-ovn-cni-5php2 1/1 Running 2 35h 192.168.1.43 192.168.1.43 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-ovn kube-ovn-cni-7dwmx 1/1 Running 2 35h 192.168.1.42 192.168.1.42 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-ovn kube-ovn-cni-lhlvl 1/1 Running 2 35h 192.168.1.41 192.168.1.41 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-ovn kube-ovn-controller-57955db7b4-6x6hd 1/1 Running 0 35h 192.168.1.43 192.168.1.43 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-ovn kube-ovn-controller-57955db7b4-chvz4 1/1 Running 0 35h 192.168.1.42 192.168.1.42 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-ovn ovn-central-bb8747d77-tr5nz 1/1 Running 0 35h 192.168.1.41 192.168.1.41 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-ovn ovs-ovn-2qhhr 1/1 Running 0 35h 192.168.1.41 192.168.1.41 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-ovn ovs-ovn-np8rn 1/1 Running 0 35h 192.168.1.43 192.168.1.43 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-ovn ovs-ovn-pkjw4 1/1 Running 0 35h 192.168.1.42 192.168.1.42 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system coredns-55f46dd959-76qb5 1/1 Running 0 35h 10.16.0.12 192.168.1.42 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system coredns-55f46dd959-wn8kw 1/1 Running 0 35h 10.16.0.11 192.168.1.43 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system heapster-fdb7596d6-xmmrx 1/1 Running 0 35h 10.16.0.15 192.168.1.42 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kubernetes-dashboard-68ddcc97fc-dwzbf 1/1 Running 0 35h 10.16.0.14 192.168.1.42 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system metrics-server-6c898b5b8b-zvct2 1/1 Running 0 35h 10.16.0.13 192.168.1.43 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 直观上 kube-ovn 与传统 k8s 网络（flannel/calico等）比较最大的不同是 pod 子网的分配：\n传统网络插件下，集群中 pod 一般是不同 node 节点分配不同的子网；然后通过 overlay 等技术打通不同 node 节点的 pod 子网； kube-ovn 中 pod 网络根据其所在的 namespace 而定； namespace 在创建时可以根据 annotation 来配置它的子网/网关等参数；默认使用 10.16.0.0/16 的子网； 测试 namespace 子网分配 # 新建一个 subnet 并绑定 namespace 测试分配一个新的 pod 子网\n# 创建一个 namespace: test-ns $ cat \u0026gt; test-ns.yaml \u0026lt;\u0026lt; EOF apiVersion: v1 kind: Namespace metadata: annotations: name: test-ns EOF $ kubectl apply -f test-ns.yaml # 创建一个 subnet: test-subnet 并绑定 namespace test-ns $ cat \u0026gt; test-subnet.yaml \u0026lt;\u0026lt; EOF apiVersion: kubeovn.io/v1 kind: Subnet metadata: name: test-subnet spec: protocol: IPv4 default: false namespaces: - test-ns cidrBlock: 10.17.0.0/24 gateway: 10.17.0.1 excludeIps: - 10.17.0.1..10.17.0.10 EOF $ kubectl apply -f test-subnet.yaml # 在 test-ns 中创建 nginx 部署 $ kubectl run -n test-ns nginx --image=nginx --replicas=2 --port=80 --expose # 在 default 中创建 busy 客户端 $ kubectl run busy --image=busybox sleep 360000 创建成功后，查看 pod 地址的分配，可以看到确实 test-ns 中 pod 使用新的子网，而 default 中 pod 使用了默认子网，并验证 pod 之间的联通性（默认可通）\n$ kubectl get pod --all-namespaces -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES default busy-6c55ccddc5-qrm5j 1/1 Running 0 31h 10.16.0.16 192.168.1.43 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-ovn kube-ovn-cni-5php2 1/1 Running 2 35h 192.168.1.43 192.168.1.43 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-ovn kube-ovn-cni-7dwmx 1/1 Running 2 35h 192.168.1.42 192.168.1.42 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-ovn kube-ovn-cni-lhlvl 1/1 Running 2 35h 192.168.1.41 192.168.1.41 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-ovn kube-ovn-controller-57955db7b4-6x6hd 1/1 Running 0 35h 192.168.1.43 192.168.1.43 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-ovn kube-ovn-controller-57955db7b4-chvz4 1/1 Running 0 35h 192.168.1.42 192.168.1.42 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-ovn ovn-central-bb8747d77-tr5nz 1/1 Running 0 35h 192.168.1.41 192.168.1.41 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-ovn ovs-ovn-2qhhr 1/1 Running 0 35h 192.168.1.41 192.168.1.41 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-ovn ovs-ovn-np8rn 1/1 Running 0 35h 192.168.1.43 192.168.1.43 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-ovn ovs-ovn-pkjw4 1/1 Running 0 35h 192.168.1.42 192.168.1.42 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system coredns-55f46dd959-76qb5 1/1 Running 0 35h 10.16.0.12 192.168.1.42 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system coredns-55f46dd959-wn8kw 1/1 Running 0 35h 10.16.0.11 192.168.1.43 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system heapster-fdb7596d6-xmmrx 1/1 Running 0 35h 10.16.0.15 192.168.1.42 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kubernetes-dashboard-68ddcc97fc-dwzbf 1/1 Running 0 35h 10.16.0.14 192.168.1.42 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system metrics-server-6c898b5b8b-zvct2 1/1 Running 0 35h 10.16.0.13 192.168.1.43 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; test-ns nginx-755464dd6c-s6flj 1/1 Running 0 31h 10.17.0.12 192.168.1.42 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; test-ns nginx-755464dd6c-zct56 1/1 Running 0 31h 10.17.0.11 192.168.1.43 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 更多的测试（pod网络QOS限速，namespace网络隔离等）请参考 kube-ovn 项目说明文档 延伸阅读 # kube-ovn 官方文档 从 Bridge 到 OVS，探索虚拟交换机 "},{"id":32,"href":"/docs/06-%E5%AE%89%E8%A3%85%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6-06--an-zhuang-wang-luo-zu-jian/","title":"06-安装网络组件 2023-09-28 15:25:14.997","section":"Docs","content":" 06-安装网络组件 # 首先回顾下K8S网络设计原则，在配置集群网络插件或者实践K8S 应用/服务部署请牢记这些原则： # 1.每个Pod都拥有一个独立IP地址，Pod内所有容器共享一个网络命名空间 # 2.集群内所有Pod都在一个直接连通的扁平网络中，可通过IP直接访问 # 所有容器之间无需NAT就可以直接互相访问 # 所有Node和所有容器之间无需NAT就可以直接互相访问 # 容器自己看到的IP跟其他容器看到的一样 # 3.Service cluster IP只可在集群内部访问，外部请求需要通过NodePort、LoadBalance或者Ingress来访问 # Container Network Interface (CNI)是目前CNCF主推的网络模型，它由两部分组成： # CNI Plugin负责给容器配置网络，它包括两个基本的接口 # 配置网络: AddNetwork(net *NetworkConfig, rt *RuntimeConf) (types.Result, error) # 清理网络: DelNetwork(net *NetworkConfig, rt *RuntimeConf) error # IPAM Plugin负责给容器分配IP地址 # Kubernetes Pod的网络是这样创建的： # 每个Pod除了创建时指定的容器外，都有一个kubelet启动时指定的基础容器，即pause容器 # kubelet创建基础容器生成network namespace # kubelet调用网络CNI driver，由它根据配置调用具体的CNI 插件 # CNI 插件给基础容器配置网络 # Pod 中其他的容器共享使用基础容器的网络 # 本项目基于CNI driver 调用各种网络插件来配置kubernetes的网络，常用CNI插件有 flannel calico cilium等等，这些插件各有优势，也在互相借鉴学习优点，比如：在所有node节点都在一个二层网络时候，flannel提供hostgw实现，避免vxlan实现的udp封装开销，估计是目前最高效的；calico也针对L3 Fabric，推出了IPinIP的选项，利用了GRE隧道封装；因此这些插件都能适合很多实际应用场景。 # 项目当前内置支持的网络插件有：calico cilium flannel kube-ovn kube-router # 安装讲解 # 安装calico 安装cilium 安装flannel 安装kube-ovn 暂未更新 安装kube-router 暂未更新 参考 # kubernetes.io networking docs feiskyer-kubernetes指南网络章节 后一篇\n"},{"id":33,"href":"/docs/07-%E5%AE%89%E8%A3%85%E9%9B%86%E7%BE%A4%E4%B8%BB%E8%A6%81%E6%8F%92%E4%BB%B6-07--an-zhuang-ji-qun-zhu-yao-cha-jian/","title":"07-安装集群主要插件 2023-09-28 15:26:42.651","section":"Docs","content":" 07-安装集群主要插件 # 目前挑选一些常用、必要的插件自动集成到安装脚本之中: # 集群默认安装 # coredns nodelocaldns metrics-server dashboard kubeasz 默认安装上述基础插件，并支持离线方式安装(./ezdown -D 命令会自动下载组件镜像，并推送到本地镜像仓库easzlab.io.local:5000) # 集群可选安装 # prometheus network_check nfs_provisioner kubeasz 默认不安装上述插件，可以在配置文件(clusters/xxx/config.yml)中开启，支持离线方式安装(./ezdown -X 会额外下载这些组件镜像，并推送到本地镜像仓库easzlab.io.local:5000) # 安装脚本 # 详见roles/cluster-addon/ 目录 # 1.根据hosts文件中配置的CLUSTER_DNS_SVC_IP CLUSTER_DNS_DOMAIN等参数生成kubedns.yaml和coredns.yaml文件 # 2.注册变量pod_info，pod_info用来判断现有集群是否已经运行各种插件 # 3.根据pod_info和配置开关逐个进行/跳过插件安装 # 下一步 # 创建ex_lb节点组, 向集群外提供高可用apiserver # 创建集群持久化存储\n"},{"id":34,"href":"/docs/08-k8s-%E9%9B%86%E7%BE%A4%E5%AD%98%E5%82%A8--k8s-ji-qun-cun-chu/","title":"08-K8S 集群存储  2023-09-28 15:27:29.454","section":"Docs","content":" K8S 集群存储 # 前言 # 在kubernetes(k8s)中对于存储的资源抽象了两个概念，分别是PersistentVolume(PV)、PersistentVolumeClaim(PVC)。\nPV是集群中的资源 # PVC是对这些资源的请求。 # 如上面所说PV和PVC都只是抽象的概念，在k8s中是通过插件的方式提供具体的存储实现。目前包含有NFS、iSCSI和云提供商指定的存储系统，更多的存储实现参考官方文档。 # 以下介绍两种provisioner, 可以提供静态或者动态的PV # nfs-provisioner: NFS存储目录供应者 # local-path-provisioner: 本地存储目录供应者 # NFS存储目录供应者 # 首先我们需要一个NFS服务器，用于提供底层存储。通过文档nfs-server，我们可以创建一个NFS服务器。 # 静态 PV # 创建静态 pv，指定容量，访问模式，回收策略，存储类等 # apiVersion: v1 kind: PersistentVolume metadata: name: pv-es-0 spec: capacity: storage: 4Gi accessModes: - ReadWriteMany volumeMode: Filesystem persistentVolumeReclaimPolicy: Recycle storageClassName: \u0026#34;es-storage-class\u0026#34; nfs: # 根据实际共享目录修改 path: /share/es0 # 根据实际 nfs服务器地址修改 server: 192.168.1.208 创建 pvc即可绑定使用上述 pv了，具体请看后文 test pod例子 # 创建动态PV # 在一个工作k8s 集群中，PVC请求会很多，如果每次都需要管理员手动去创建对应的 PV资源，那就很不方便；因此 K8S还提供了多种 provisioner来动态创建 PV，不仅节省了管理员的时间，还可以根据StorageClasses封装不同类型的存储供 PVC 选用。 # 项目中以nfs-client-provisioner为例 https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner # 1.编辑集群配置文件：clusters/${集群名}/config.yml # ... 省略 # 在role:cluster-addon 中启用nfs-provisioner 安装 nfs_provisioner_install: \u0026#34;yes\u0026#34;\t# 修改为yes nfs_provisioner_namespace: \u0026#34;kube-system\u0026#34; nfs_provisioner_ver: \u0026#34;v4.0.1\u0026#34; nfs_storage_class: \u0026#34;managed-nfs-storage\u0026#34;\tnfs_server: \u0026#34;192.168.31.244\u0026#34;\t# 修改为实际nfs server地址 nfs_path: \u0026#34;/data/nfs\u0026#34;\t# 修改为实际的nfs共享目录 2.创建 nfs provisioner $ dk ezctl setup ${集群名} 07 # 执行成功后验证 $ kubectl get pod --all-namespaces |grep nfs-client kube-system nfs-client-provisioner-84ff87c669-ksw95 1/1 Running 0 21m 3.验证使用动态 PV # 在目录clusters/${集群名}/yml/nfs-provisioner/ 有个测试例子 # $ kubectl apply -f /etc/kubeasz/clusters/hello/yml/nfs-provisioner/test-pod.yaml # 验证测试pod kubectl get pod NAME READY STATUS RESTARTS AGE test-pod 0/1 Completed 0 6h36m # 验证自动创建的pv 资源， kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-44d34a50-e00b-4f6c-8005-40f5cc54af18 2Mi RWX Delete Bound default/test-claim managed-nfs-storage 6h36m # 验证PVC已经绑定成功：STATUS字段为 Bound kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE test-claim Bound pvc-44d34a50-e00b-4f6c-8005-40f5cc54af18 2Mi RWX managed-nfs-storage 6h37m 另外，Pod启动完成后，在挂载的目录中创建一个SUCCESS文件。我们可以到NFS服务器去看下： # . └── default-test-claim-pvc-44d34a50-e00b-4f6c-8005-40f5cc54af18 └── SUCCESS 如上，可以发现挂载的时候，nfs-client根据PVC自动创建了一个目录，我们Pod中挂载的/mnt，实际引用的就是该目录，而我们在/mnt下创建的SUCCESS文件，也自动写入到了这里。 # 后面当我们需要为上层应用提供持久化存储时，只需要提供StorageClass即可。很多应用都会根据StorageClass来创建他们的所需的PVC, 最后再把PVC挂载到他们的Deployment或StatefulSet中使用，比如：efk、jenkins等 # 本地存储目录供应者 # 当应用对于磁盘I/O性能要求高，比较适合本地文件目录存储，特别地可以本地挂载SSD磁盘（注意本地磁盘需要配置raid冗余策略）。Local Path Provisioner 可以方便地在k8s集群中使用本地文件目录存储。 # 在kubeasz项目中集成安装 # 1.编辑集群配置文件：clusters/${集群名}/config.yml # ... 省略 local_path_provisioner_install: \u0026#34;yes\u0026#34; # 修改为yes # 设置默认本地存储路径 local_path_provisioner_dir: \u0026#34;/opt/local-path-provisioner\u0026#34; 2.创建 local path provisioner # $ dk ezctl setup ${集群名} 07 # 执行成功后验证 $ kubectl get pod --all-namespaces |grep nfs-client-provisioner 3.验证使用（略） # "},{"id":35,"href":"/docs/16%E4%B8%AA%E6%A6%82%E5%BF%B5%E5%B8%A6%E4%BD%A0%E5%85%A5%E9%97%A8-kubernetes-16-ge-gai-nian-dai-ni-ru-men-kubernetes/","title":"16个概念带你入门 Kubernetes 2024-04-03 15:09:21.671","section":"Docs","content":" Kubernetes是Google开源的容器集群管理系统，是Google多年⼤规模容器管理技术Borg的开源版本，主要功能包括: # 基于容器的应用部署、维护和滚动升级 # 负载均衡和服务发现 # 跨机器和跨地区的集群调度 # 自动伸缩 # 无状态服务和有状态服务 # 广泛的Volume支持 # 插件机制保证扩展性 # Kubernetes发展非常迅速，已经成为容器编排领域的领导者，接下来我们将讲解Kubernetes中涉及到的一些主要概念。 # # 1、Pod # Pod是一组紧密关联的容器集合，支持多个容器在一个Pod中共享网络和文件系统，可以通过进程间通信和文件共享这种简单高效的方式完成服务，是Kubernetes调度的基本单位。Pod的设计理念是每个Pod都有一个唯一的IP。 # Pod具有如下特征： # 包含多个共享IPC、Network和UTC namespace的容器，可直接通过localhost通信 # 所有Pod内容器都可以访问共享的Volume，可以访问共享数据 # 优雅终止:Pod删除的时候先给其内的进程发送SIGTERM，等待一段时间(grace period)后才强制停止依然还在运行的进程 # 特权容器(通过SecurityContext配置)具有改变系统配置的权限(在网络插件中大量应用) # 支持三种重启策略（restartPolicy），分别是：Always、OnFailure、Never # 支持三种镜像拉取策略（imagePullPolicy），分别是：Always、Never、IfNotPresent # 资源限制，Kubernetes通过CGroup限制容器的CPU以及内存等资源，可以设置request以及limit值 # 健康检查，提供两种健康检查探针，分别是livenessProbe和redinessProbe，前者用于探测容器是否存活，如果探测失败，则根据重启策略进行重启操作，后者用于检查容器状态是否正常，如果检查容器状态不正常，则请求不会到达该Pod # Init container在所有容器运行之前执行，常用来初始化配置 # 容器生命周期钩子函数，用于监听容器生命周期的特定事件，并在事件发生时执行已注册的回调函数，支持两种钩子函数：postStart和preStop，前者是在容器启动后执行，后者是在容器停止前执行 # 2、Namespace # Namespace（命名空间）是对一组资源和对象的抽象集合，比如可以用来将系统内部的对象划分为不同的项目组或者用户组。常见的pod、service、replicaSet和deployment等都是属于某一个namespace的(默认是default)，而node, persistentVolumes等则不属于任何namespace。 # 常用namespace操作： # kubectl get namespace , 查询所有namespace # kubectl create namespacens-name ，创建namespace # kubectl delete namespacens-name , 删除namespace # 删除命名空间时，需注意以下几点： # 删除一个namespace会自动删除所有属于该namespace的资源。 # default 和 kube-system 命名空间不可删除。 # PersistentVolumes是不属于任何namespace的，但PersistentVolumeClaim是属于某个特定namespace的。 # Events是否属于namespace取决于产生events的对象。 # # ** ** # 3、Node # Node是Pod真正运行的主机，可以是物理机也可以是虚拟机。Node本质上不是Kubernetes来创建的， Kubernetes只是管理Node上的资源。为了管理Pod，每个Node节点上至少需要运行container runtime（Docker）、kubelet和kube-proxy服务。 # 常用node操作： # kubectl get nodes ，查询所有node # kubectl cordon $nodename , 将node标志为不可调度 # kubectl uncordon $nodename , 将node标志为可调度 # taint(污点) # 使用kubectl taint命令可以给某个Node节点设置污点，Node被设置上污点之后就和Pod之间存在了一种相斥的关系，可以让Node拒绝Pod的调度执行，甚至将Node已经存在的Pod驱逐出去。每个污点的组成：key=value:effect，当前taint effect支持如下三个选项： # NoSchedule：表示k8s将不会将Pod调度到具有该污点的Node上 # PreferNoSchedule：表示k8s将尽量避免将Pod调度到具有该污点的Node上 # NoExecute：表示k8s将不会将Pod调度到具有该污点的Node上，同时会将Node上已经存在的Pod驱逐出去 # 常用命令如下：\nkubectl taint node node0 key1=value1:NoShedule ，为node0设置不可调度污点 # kubectl taint node node0 key- ，将node0上key值为key1的污点移除 # kubectl taint node node1 node-role.kubernetes.io/master=:NoSchedule ，为kube-master节点设置不可调度污点 # kubectl taint node node1 node-role.kubernetes.io/master=PreferNoSchedule ，为kube-master节点设置尽量不可调度污点 # 容忍(Tolerations) # 设置了污点的Node将根据taint的effect：NoSchedule、PreferNoSchedule、NoExecute和Pod之间产生互斥的关系，Pod将在一定程度上不会被调度到Node上。 但我们可以在Pod上设置容忍(Toleration)，意思是设置了容忍的Pod将可以容忍污点的存在，可以被调度到存在污点的Node上。 # # ** ** # 4、Service # Service是对一组提供相同功能的Pods的抽象，并为他们提供一个统一的入口，借助 Service 应用可以方便的实现服务发现与负载均衡，并实现应用的零宕机升级。Service通过标签(label)来选取后端Pod，一般配合ReplicaSet或者Deployment来保证后端容器的正常运行。 # service 有如下四种类型，默认是ClusterIP： # ClusterIP: 默认类型，自动分配一个仅集群内部可以访问的虚拟IP # NodePort: 在ClusterIP基础上为Service在每台机器上绑定一个端口，这样就可以通过 NodeIP:NodePort 来访问该服务 # LoadBalancer: 在NodePort的基础上，借助cloud provider创建一个外部的负载均衡器，并将请求转发到 NodeIP:NodePort # ExternalName: 将服务通过DNS CNAME记录方式转发到指定的域名 # 另外，也可以将已有的服务以Service的形式加入到Kubernetes集群中来，只需要在创建 Service 的时候不指定Label selector，而是在Service创建好后手动为其添加endpoint。 # # # 5、Volume 存储卷 # 默认情况下容器的数据是非持久化的，容器消亡以后数据也会跟着丢失，所以Docker提供了Volume机制以便将数据持久化存储。Kubernetes提供了更强大的Volume机制和插件，解决了容器数据持久化以及容器间共享数据的问题。 # Kubernetes存储卷的生命周期与Pod绑定 # 容器挂掉后Kubelet再次重启容器时，Volume的数据依然还在 # Pod删除时，Volume才会清理。数据是否丢失取决于具体的Volume类型，比如emptyDir的数据会丢失，而PV的数据则不会丢 # 目前Kubernetes主要支持以下Volume类型： # emptyDir：Pod存在，emptyDir就会存在，容器挂掉不会引起emptyDir目录下的数据丢失，但是pod被删除或者迁移，emptyDir也会被删除 # hostPath：hostPath允许挂载Node上的文件系统到Pod里面去 # NFS（Network File System）：网络文件系统，Kubernetes中通过简单地配置就可以挂载NFS到Pod中，而NFS中的数据是可以永久保存的，同时NFS支持同时写操作。 # glusterfs：同NFS一样是一种网络文件系统，Kubernetes可以将glusterfs挂载到Pod中，并进行永久保存 # cephfs：一种分布式网络文件系统，可以挂载到Pod中，并进行永久保存 # subpath：Pod的多个容器使用同一个Volume时，会经常用到 # secret：密钥管理，可以将敏感信息进行加密之后保存并挂载到Pod中 # persistentVolumeClaim：用于将持久化存储（PersistentVolume）挂载到Pod中 # \u0026hellip;\n# # # 6、PersistentVolume(PV) 持久化存储卷 # PersistentVolume(PV)是集群之中的一块网络存储。跟 Node 一样，也是集群的资源。PersistentVolume (PV)和PersistentVolumeClaim (PVC)提供了方便的持久化卷: PV提供网络存储资源，而PVC请求存储资源并将其挂载到Pod中。 # PV的访问模式(accessModes)有三种: # ReadWriteOnce(RWO):是最基本的方式，可读可写，但只支持被单个Pod挂载。 # ReadOnlyMany(ROX):可以以只读的方式被多个Pod挂载。 # ReadWriteMany(RWX):这种存储可以以读写的方式被多个Pod共享。 # 不是每一种存储都支持这三种方式，像共享方式，目前支持的还比较少，比较常用的是 NFS。在PVC绑定PV时通常根据两个条件来绑定，一个是存储的大小，另一个就是 访问模式。 # PV的回收策略(persistentVolumeReclaimPolicy)也有三种 # Retain，不清理保留Volume(需要手动清理) # Recycle，删除数据，即 rm -rf /thevolume/* (只有NFS和HostPath支持) # Delete，删除存储资源 # # # 7、Deployment 无状态应用 # 一般情况下我们不需要手动创建Pod实例，而是采用更高一层的抽象或定义来管理Pod，针对无状态类型的应用，Kubernetes使用Deloyment的Controller对象与之对应。其典型的应用场景包括： # 定义Deployment来创建Pod和ReplicaSet # 滚动升级和回滚应用 # 扩容和缩容 # 暂停和继续Deployment # 常用的操作命令如下： # kubectl run www--image=10.0.0.183:5000/hanker/www:0.0.1--port=8080 生成一个Deployment对象 # kubectl get deployment--all-namespaces 查找Deployment # kubectl describe deployment www 查看某个Deployment # kubectl edit deployment www 编辑Deployment定义 # kubectldeletedeployment www 删除某Deployment # kubectl scale deployment/www--replicas=2 扩缩容操作，即修改Deployment下的Pod实例个数 # kubectlsetimage deployment/nginx-deployment nginx=nginx:1.9.1 更新镜像 # kubectl rollout undo deployment/nginx-deployment 回滚操作 # kubectl rollout status deployment/nginx-deployment 查看回滚进度 # kubectl autoscale deployment nginx-deployment--min=10--max=15--cpu-percent=80 启用水平伸缩（HPA - horizontal pod autoscaling），设置最小、最大实例数量以及目标cpu使用率 # kubectl rollout pause deployment/nginx-deployment 暂停更新Deployment # kubectl rollout resume deploy nginx 恢复更新Deployment # 更新策略 # .spec.strategy 指新的Pod替换旧的Pod的策略，有以下两种类型 # RollingUpdate 滚动升级，可以保证应用在升级期间，对外正常提供服务。 # Recreate 重建策略，在创建出新的Pod之前会先杀掉所有已存在的Pod。 # Deployment和ReplicaSet两者之间的关系 # 使用Deployment来创建ReplicaSet。ReplicaSet在后台创建pod，检查启动状态，看它是成功还是失败。 # 当执行更新操作时，会创建一个新的ReplicaSet，Deployment会按照控制的速率将pod从旧的ReplicaSet移 动到新的ReplicaSet中 # # 8、StatefulSet 有状态应用 # Deployments和ReplicaSets是为无状态服务设计的，那么StatefulSet则是为了有状态服务而设计，其应用场景包括： # 稳定的持久化存储，即Pod重新调度后还是能访问到相同的持久化数据，基于PVC来实现 # 稳定的网络标志，即Pod重新调度后其PodName和HostName不变，基于Headless Service(即没有Cluster IP的Service)来实现 # 有序部署，有序扩展，即Pod是有顺序的，在部署或者扩展的时候要依据定义的顺序依次进行操作(即从0到N-1，在下一个Pod运行之前所有之前的Pod必须都是Running和Ready状态)，基于init containers来实现 # 有序收缩，有序删除(即从N-1到0) # 支持两种更新策略： # OnDelete:当 .spec.template更新时，并不立即删除旧的Pod，而是等待用户手动删除这些旧Pod后自动创建新Pod。这是默认的更新策略，兼容v1.6版本的行为 # RollingUpdate:当 .spec.template 更新时，自动删除旧的Pod并创建新Pod替换。在更新时这些Pod是按逆序的方式进行，依次删除、创建并等待Pod变成Ready状态才进行下一个Pod的更新。 # # # 9、DaemonSet 守护进程集 # DaemonSet保证在特定或所有Node节点上都运行一个Pod实例，常用来部署一些集群的日志采集、监控或者其他系统管理应用。典型的应用包括: # 日志收集，比如fluentd，logstash等 # 系统监控，比如Prometheus Node Exporter，collectd等 # 系统程序，比如kube-proxy, kube-dns, glusterd, ceph，ingress-controller等 # 指定Node节点 # DaemonSet会忽略Node的unschedulable状态，有两种方式来指定Pod只运行在指定的Node节点上: # nodeSelector:只调度到匹配指定label的Node上 # nodeAffinity:功能更丰富的Node选择器，比如支持集合操作 # podAffinity:调度到满足条件的Pod所在的Node上 # 目前支持两种策略 # OnDelete: 默认策略，更新模板后，只有手动删除了旧的Pod后才会创建新的Pod # RollingUpdate: 更新DaemonSet模版后，自动删除旧的Pod并创建新的Pod # # # 10、Ingress # Kubernetes中的负载均衡我们主要用到了以下两种机制： # Service：使用Service提供集群内部的负载均衡，Kube-proxy负责将service请求负载均衡到后端的Pod中 # Ingress Controller：使用Ingress提供集群外部的负载均衡 # Service和Pod的IP仅可在集群内部访问。集群外部的请求需要通过负载均衡转发到service所在节点暴露的端口上，然后再由kube-proxy通过边缘路由器将其转发到相关的Pod，Ingress可以给service提供集群外部访问的URL、负载均衡、HTTP路由等，为了配置这些Ingress规则，集群管理员需要部署一个Ingress Controller，它监听Ingress和service的变化，并根据规则配置负载均衡并提供访问入口。 # 常用的ingress controller： # nginx # traefik # Kong # Openresty # # # # 11、Job \u0026amp; CronJob 任务和定时任务 # Job负责批量处理短暂的一次性任务 (short lived\u0026gt;CronJob即定时任务，就类似于Linux系统的crontab，在指定的时间周期运行指定的任务。 # # # 12、HPA（Horizontal Pod Autoscaling） 水平伸缩 # Horizontal Pod Autoscaling可以根据CPU、内存使用率或应用自定义metrics自动扩展Pod数量 (支持replication controller、deployment和replica set)。 # 控制管理器默认每隔30s查询metrics的资源使用情况(可以通过 \u0026ndash;horizontal-pod-autoscaler-sync-period 修改) # 支持三种metrics类型 # 预定义metrics(比如Pod的CPU)以利用率的方式计算 # 自定义的Pod metrics，以原始值(raw value)的方式计算 # 自定义的object metrics # 支持两种metrics查询方式:Heapster和自定义的REST API # 支持多metrics # 可以通过如下命令创建HPA： # kubectl autoscale deployment php-apache--cpu-percent=50--min=1--max=10 # # 13、Service Account # Service account是为了方便Pod里面的进程调用Kubernetes API或其他外部服务而设计的 # 授权\nService Account为服务提供了一种方便的认证机制，但它不关心授权的问题。可以配合RBAC(Role Based Access Control)来为Service Account鉴权，通过定义Role、RoleBinding、ClusterRole、ClusterRoleBinding来对sa进行授权。 # # # 14、Secret 密钥 # Sercert-密钥解决了密码、token、密钥等敏感数据的配置问题，而不需要把这些敏感数据暴露到镜像或者Pod Spec中。Secret可以以Volume或者环境变量的方式使用。有如下三种类型： # Service Account:用来访问Kubernetes API，由Kubernetes自动创建，并且会自动挂载到Pod的 /run/secrets/kubernetes.io/serviceaccount 目录中; # Opaque:base64编码格式的Secret，用来存储密码、密钥等; # kubernetes.io/dockerconfigjson: 用来存储私有docker registry的认证信息。 # # # 15、ConfigMap 配置中心 # ConfigMap用于保存配置数据的键值对，可以用来保存单个属性，也可以用来保存配置文件。ConfigMap跟secret很类似，但它可以更方便地处理不包含敏感信息的字符串。ConfigMap可以通过三种方式在Pod中使用，三种分别方式为:设置环境变量、设置容器命令行参数以及在Volume中直接挂载文件或目录。 # 可以使用 kubectl create configmap从文件、目录或者key-value字符串创建等创建 ConfigMap。也可以通过 kubectl create-f value.yaml 创建。 # 16、Resource Quotas 资源配额\n# 资源配额(Resource Quotas)是用来限制用户资源用量的一种机制。 # 资源配额有如下类型： # 计算资源，包括cpu和memory # cpu, limits.cpu, requests.cpu # memory, limits.memory, requests.memory # 存储资源，包括存储资源的总量以及指定storage class的总量 # requests.storage:存储资源总量，如500Gi # persistentvolumeclaims:pvc的个数 # storageclass.storage.k8s.io/requests.storage # storageclass.storage.k8s.io/persistentvolumeclaims # 对象数，即可创建的对象的个数 # pods, replicationcontrollers, configmaps, secrets # resourcequotas, persistentvolumeclaims # services, services.loadbalancers, services.nodeports # 它的工作原理为: # 资源配额应用在Namespace上，并且每个Namespace最多只能有一个 ResourceQuota 对象 # 开启计算资源配额后，创建容器时必须配置计算资源请求或限制(也可以 用LimitRange设置默认值) # 用户超额后禁止创建新的资源 # "},{"id":36,"href":"/docs/16%E5%BC%A0%E7%A1%AC%E6%A0%B8%E5%9B%BE%E8%A7%A3k8s%E7%BD%91%E7%BB%9C-16-zhang-ying-he-tu-jie-k8s-wang-luo/","title":"16张硬核图解k8s网络 2024-04-03 15:12:49.601","section":"Docs","content":" Overview # 本文将探讨 Kubernetes 中的网络模型，以及对各种网络模型进行分析。\nUnderlay Network Model # 什么是 Underlay Network # 底层网络 Underlay Network 顾名思义是指网络设备基础设施，如交换机，路由器, DWDM 使用网络介质将其链接成的物理网络拓扑，负责网络之间的数据包传输。\nUnderlay network topology\nunderlay network 可以是二层，也可以是三层；二层的典型例子是以太网 Ethernet，三层是的典型例子是互联网 Internet。\n而工作于二层的技术是 vlan，工作在三层的技术是由 OSPF, BGP 等协议组成。\nk8s 中的 underlay network # 在 kubernetes 中，underlay network 中比较典型的例子是通过将宿主机作为路由器设备，Pod 的网络则通过学习路由条目从而实现跨节点通讯。\nunderlay network topology in kubernetes\n这种模型下典型的有 flannel 的 host-gw 模式与 calico BGP 模式。\nflannel host-gw # flannel host-gw 模式中每个 Node 需要在同一个二层网络中，并将 Node 作为一个路由器，跨节点通讯将通过路由表方式进行，这样方式下将网络模拟成一个underlay network。\nlayer2 ethernet topology\nNotes：因为是通过路由方式，集群的 cidr 至少要配置 16，因为这样可以保证，跨节点的 Node 作为一层网络，同节点的 Pod 作为一个网络。如果不是这种用情况，路由表处于相同的网络中，会存在网络不可达\nCalico BGP # BGP（Border Gateway Protocol）是去中心化自治路由协议。它是通过维护 IP 路由表或前缀表来实现 AS （Autonomous System）之间的可访问性，属于向量路由协议。\nBGP network topology\n与 flannel 不同的是，Calico 提供了的 BGP 网络解决方案，在网络模型上，Calico 与 Flannel host-gw 是近似的，但在软件架构的实现上，flannel 使用 flanneld 进程来维护路由信息；而 Calico 是包含多个守护进程的，其中 Brid 进程是一个 BGP 客户端与路由反射器(Router Reflector)，BGP 客户端负责从 Felix 中获取路由并分发到其他 BGP Peer，而反射器在 BGP 中起了优化的作用。在同一个 IBGP 中，BGP 客户端仅需要和一个 RR 相连，这样减少了AS内部维护的大量的 BGP 连接。通常情况下，RR 是真实的路由设备，而 Bird 作为 BGP 客户端工作。\nCalico Network Architecture\nIPVLAN \u0026amp; MACVLAN # IPVLAN 和 MACVLAN 是一种网卡虚拟化技术，两者之间的区别为， IPVLAN 允许一个物理网卡拥有多个 IP 地址，并且所有的虚拟接口用同一个 MAC 地址；而 MACVLAN 则是相反的，其允许同一个网卡拥有多个 MAC 地址，而虚拟出的网卡可以没有 IP 地址。\n因为是网卡虚拟化技术，而不是网络虚拟化技术，本质上来说属于 Overlay network，这种方式在虚拟化环境中与 Overlay network 相比最大的特点就是可以将 Pod 的网络拉平到 Node 网络同级，从而提供更高的性能、低延迟的网络接口。本质上来说其网络模型属于下图中第二个。\nVirtual networking modes: bridging, multiplexing and SR-IOV\n虚拟网桥：创建一个虚拟网卡对(veth pair)，一头在容器内，一头在宿主机的 root namespaces 内。这样一来容器内发出的数据包可以通过网桥直接进入宿主机网络栈，而发往容器的数据包也可以经过网桥进入容器。 多路复用：使用一个中间网络设备，暴露多个虚拟网卡接口，容器网卡都可以介入这个中间设备，并通过 MAC/IP 地址来区分 packet 应该发往哪个容器设备。 硬件交换，为每个 Pod 分配一个虚拟网卡，这样一来，Pod 与 Pod 之间的连接关系就会变得非常清晰，因为近乎物理机之间的通信基础。如今大多数网卡都支持 SR-IOV 功能，该功能将单一的物理网卡虚拟成多个 VF 接口，每个 VF 接口都有单独的虚拟 PCIe 通道，这些虚拟的 PCIe 通道共用物理网卡的 PCIe 通道。 在 kubernetes 中 IPVLAN 这种网络模型下典型的 CNI 有，multus 与 danm。\nmultus # multus 是 intel 开源的 CNI 方案，是由传统的 cni 与 multus，并且提供了 SR-IOV CNI 插件使 K8s pod 能够连接到 SR-IOV VF 。这是使用了 IPVLAN/MACVLAN 的功能。\n当创建新的 Pod 后，SR-IOV 插件开始工作。配置 VF 将被移动到新的 CNI 名称空间。该插件根据 CNI 配置文件中的 “name” 选项设置接口名称。最后将 VF 状态设置为 UP。\n下图是一个 Multus 和 SR-IOV CNI 插件的网络环境，具有三个接口的 pod。\neth0 是 flannel 网络插件，也是作为 Pod 的默认网络 VF 是主机的物理端口 ens2f0 的实例化。这是英特尔 X710-DA4 上的一个端口。在 Pod 端的 VF 接口名称为 south0 。 这个 VF 使用了 DPDK 驱动程序，此 VF 是从主机的物理端口 ens2f1 实例化出的。这个是英特尔 ® X710-DA4 上另外一个端口。Pod 内的 VF 接口名称为 north0。该接口绑定到 DPDK 驱动程序 vfio-pci 。 Mutus networking Architecture overlay and SR-IOV\nNotes：术语\nNIC：network interface card，网卡 SR-IOV：single root I/O virtualization，硬件实现的功能，允许各虚拟机间共享 PCIe 设备。 VF：Virtual Function，基于 PF，与 PF 或者其他 VF 共享一个物理资源。 PF：PCIe Physical Function，拥有完全控制 PCIe 资源的能力 DPDK：Data Plane Development Kit 于此同时，也可以将主机接口直接移动到 Pod 的网络名称空间，当然这个接口是必须存在，并且不能是与默认网络使用同一个接口。这种情况下，在普通网卡的环境中，就直接将 Pod 网络与 Node 网络处于同一个平面内了。\nMutus networking Architecture overlay and ipvlan\ndanm # DANM 是诺基亚开源的 CNI 项目，目的是将电信级网络引入 kubernetes 中，与 multus 相同的是，也提供了 SR-IOV/DPDK 的硬件技术，并且支持 IPVLAN.\nOverlay Network Model # 什么是 Overlay # 叠加网络是使用网络虚拟化技术，在 underlay 网络上构建出的虚拟逻辑网络，而无需对物理网络架构进行更改。本质上来说，overlay network 使用的是一种或多种隧道协议 (tunneling)，通过将数据包封装，实现一个网络到另一个网络中的传输，具体来说隧道协议关注的是数据包（帧）。\noverlay network topology\n常见的网络隧道技术 # 通用路由封装 ( Generic Routing Encapsulation ) 用于将来自 IPv4/IPv6 的数据包封装为另一个协议的数据包中，通常工作与 L3 网络层中。 VxLAN (Virtual Extensible LAN)，是一个简单的隧道协议，本质上是将 L2 的以太网帧封装为 L4 中 UDP 数据包的方法，使用 4789 作为默认端口。VxLAN 也是 VLAN 的扩展，对于 4096（ 位 VLAN ID） 扩展为 1600 万（ 位 VN·ID ）个逻辑网络。 这种工作在 overlay 模型下典型的有 flannel 与 calico 中的的 VxLAN, IPIP 模式。\nIPIP # IP in IP 也是一种隧道协议，与 VxLAN 类似的是，IPIP 的实现也是通过 Linux 内核功能进行的封装。IPIP 需要内核模块 ipip.ko 使用命令查看内核是否加载 IPIP 模块lsmod | grep ipip ；使用命令modprobe ipip 加载。\nA simple IPIP network workflow\nKubernetes 中 IPIP 与 VxLAN 类似，也是通过网络隧道技术实现的。与 VxLAN 差别就是，VxLAN 本质上是一个 UDP 包，而 IPIP 则是将包封装在本身的报文包上。\nIPIP in kubernetes\nIPIP packet with wireshark unpack\nNotes：公有云可能不允许 IPIP 流量，例如 Azure\nVxLAN # kubernetes 中不管是 flannel 还是 calico VxLAN 的实现都是使用 Linux 内核功能进行的封装，Linux 对 vxlan 协议的支持时间并不久，2012 年 Stephen Hemminger 才把相关的工作合并到 kernel 中，并最终出现在 kernel 3.7.0 版本。为了稳定性和很多的功能，你可以会看到某些软件推荐在 3.9.0 或者 3.10.0 以后版本的 kernel 上使用 VxLAN。\nA simple VxLAN network topology\n在 kubernetes 中 vxlan 网络，例如 flannel，守护进程会根据 kubernetes 的 Node 而维护 VxLAN，名称为 flannel.1 这是 VNID，并维护这个网络的路由，当发生跨节点的流量时，本地会维护对端 VxLAN 设备的 MAC 地址，通过这个地址可以知道发送的目的端，这样就可以封包发送到对端，收到包的对端 VxLAN 设备 flannel.1 解包后得到真实的目的地址。\n查看 Forwarding database 列表\n$ bridge fdb 26:5e:87:90:91:fc dev flannel.1 dst 10.0.0.3 self permanent VxLAN in kubernetes\nVxLAN packet with wireshark unpack\nNotes：VxLAN 使用的 4789 端口，wireshark 应该是根据端口进行分析协议的，而 flannel 在 linux 中默认端口是 8472，此时抓包仅能看到是一个 UDP 包。\n通过上述的架构可以看出，隧道实际上是一个抽象的概念，并不是建立的真实的两端的隧道，而是通过将数据包封装成另一个数据包，通过物理设备传输后，经由相同的设备（网络隧道）进行解包实现网络的叠加。\nweave vxlan # weave 也是使用了 VxLAN 技术完成的包的封装，这个技术在 weave 中称之为 fastdp (fast data path)，与 calico 和 flannel 中用到的技术不同的，这里使用的是 Linux 内核中的 openvswitch datapath module，并且 weave 对网络流量进行了加密。\nweave fastdp network topology\nNotes：fastdp 工作在 Linux 内核版本 3.12 及更高版本，如果低于此版本的例如 CentOS7，weave 将工作在用户空间，weave 中称之为 sleeve mode\nReference # https://github.com/flannel-io/flannel/blob/master/Documentation/backends.md#host-gw https://projectcalico.docs.tigera.io/networking/bgp https://www.weave.works/docs/net/latest/concepts/router-encapsulation/ https://github.com/k8snetworkplumbingwg/sriov-network-device-plugin https://github.com/nokia/danm\n"},{"id":37,"href":"/docs/2024-12-07-microk8s/","title":"2024-12-07 microk8s","section":"Docs","content":"在mutipass中启动ubuntu虚拟机\nmultipass shell microk8s-vm MicroK8s 是一个由 Canonical（Ubuntu 的开发商）维护的轻量级 Kubernetes 发行版。它旨在为开发者、运维人员和边缘计算场景提供一个快速、简便的 Kubernetes 部署方式，适用于本地开发、测试环境、边缘设备以及生产场景。\nMicroK8s 的特点 # 轻量级 微内核设计，占用资源少，适合运行在笔记本电脑、工作站、树莓派或其他资源受限的设备上。 默认禁用许多附加组件，只有基本的 Kubernetes 核心功能。 易安装 提供单命令安装，尤其在 Ubuntu 系统上，使用 snap 包管理器即可快速安装。 支持多种操作系统，包括 Linux（Ubuntu、CentOS 等）、Windows 和 macOS（通过虚拟化工具）。 模块化 提供了多种可选的附加组件（addons），如 dns、ingress、storage、helm 等，用户可以根据需要启用或禁用这些组件。 开箱即用 直接内置了 Kubernetes 集群的核心组件，如 API Server、kubelet 和调度器，用户无需额外配置即可开始使用。 自动升级 使用 snap 管理，支持自动更新到最新版本。 跨平台 支持多种硬件架构（如 x86、ARM），适合开发、测试和物联网边缘计算场景。 兼容性 完全兼容 Kubernetes API，可以运行大多数 Kubernetes 应用程序。 支持与 kubectl、Helm 等工具集成。 MicroK8s 的用途 # 本地开发与测试 开发人员可以在本地快速启动一个 Kubernetes 集群，用于应用开发、测试和调试。 学习 Kubernetes 对于新手来说，MicroK8s 是学习 Kubernetes 的理想工具，因为它简单易用且资源需求低。 边缘计算 MicroK8s 轻量级的特性使其适合运行在物联网设备或资源有限的边缘设备上。 CI/CD 测试环境 用于快速创建临时 Kubernetes 环境以运行持续集成或自动化测试。 单节点或小型生产环境 在资源有限的情况下，可以作为单节点 Kubernetes 集群使用。 MicroK8s 的安装与使用 # 安装 MicroK8s # 在 Ubuntu 上安装 使用 snap 进行安装：\nsudo snap install microk8s --classic 安装完成后，检查版本：\nmicrok8s version 在其他 Linux 系统上安装 在支持 snap 的 Linux 系统上，同样可以使用 snap 安装。\n在 macOS 或 Windows 上安装 使用 Multipass 或其他虚拟化工具（如 VirtualBox）运行 MicroK8s。\n常用命令 # 启用或禁用附加组件：\nmicrok8s enable dns ingress storage helm3 microk8s disable dns 查看 Kubernetes 资源： 使用内置的 kubectl：\nmicrok8s kubectl get nodes microk8s kubectl get pods -A 检查 MicroK8s 的状态：\nmicrok8s status 重启或停止服务：\nmicrok8s stop microk8s start 配置用户权限（非 root 用户）：\nsudo usermod -a -G microk8s $USER sudo chown -R $USER ~/.kube newgrp microk8s MicroK8s 与其他 Kubernetes 发行版的对比 # 特性 MicroK8s Minikube K3s 轻量化 是 部分轻量化 极轻量化 安装方式 单命令（snap 包） VirtualBox 或其他虚拟化工具 二进制或安装脚本 资源需求 低 较高 极低 生产环境支持 支持（单节点或小型环境） 不推荐 支持 附加组件管理 支持，模块化组件 手动安装 部分模块化 云原生兼容性 完全兼容 完全兼容 部分特性简化 总结 # MicroK8s 是一个轻量级且功能强大的 Kubernetes 发行版，专为开发者、测试环境和边缘计算场景设计。它具有易安装、易管理、资源占用低等优点，同时保留了 Kubernetes 的核心功能，非常适合想快速入门 Kubernetes 或在本地进行开发测试的用户。\n如果你需要一个即开即用的 Kubernetes 集群，无论是本地开发还是边缘计算，MicroK8s 都是一个理想的选择。\n在 macOS 上安装 MicroK8s 相较于 Linux 环境略有不同，因为 macOS 本身不支持直接使用 snap 安装。要在 macOS 上安装 MicroK8s，你需要借助 Multipass（一个轻量级虚拟机管理工具）来运行一个虚拟机，从而部署 MicroK8s。\n以下是详细的安装步骤：\n1. 安装 Multipass # Multipass 是一个由 Canonical（Ubuntu 的开发公司）开发的工具，能够轻松管理 Ubuntu 虚拟机。在 macOS 上安装 MicroK8s 的步骤首先是通过 Multipass 启动一个虚拟机。\n（1）安装 Multipass # 你可以通过 Homebrew 安装 Multipass：\nbrew install --cask multipass （2）检查安装是否成功 # 安装完成后，可以检查 Multipass 是否安装成功：\nmultipass version 如果安装成功，你将看到 Multipass 的版本信息。\n2. 使用 Multipass 启动 Ubuntu 虚拟机 # MicroK8s 需要 Ubuntu 系统来运行，因此接下来你需要在 macOS 上通过 Multipass 创建一个 Ubuntu 虚拟机。\n（1）启动 Ubuntu 虚拟机 # 执行以下命令启动一个 Ubuntu 虚拟机（例如，启动一个 4GB 内存的 Ubuntu 20.04 虚拟机）：\nmultipass launch --name microk8s-vm --mem 4G --disk 20G 20.04 这将启动一个名为 microk8s-vm 的虚拟机，分配 4GB 内存和 20GB 硬盘。\n（2）进入虚拟机 # 进入虚拟机：\nmultipass shell microk8s-vm 3. 在虚拟机中安装 MicroK8s # 进入 Ubuntu 虚拟机后，你可以按照以下步骤安装 MicroK8s。\n（1）安装 MicroK8s # 在虚拟机内，运行以下命令安装 MicroK8s：\nsudo snap install microk8s --classic （2）验证安装 # 安装完成后，检查 MicroK8s 是否正常运行：\nmicrok8s status --wait-ready 这条命令会等待直到 MicroK8s 准备好并且可以正常使用。\nubuntu@microk8s-vm:~$ sudo snap install microk8s --classic microk8s (1.31/stable) v1.31.3 from Canonical✓ installed ubuntu@microk8s-vm:~$ microk8s status --wait-ready Insufficient permissions to access MicroK8s. You can either try again with sudo or add the user ubuntu to the \u0026#39;microk8s\u0026#39; group: sudo usermod -a -G microk8s ubuntu sudo chown -R ubuntu ~/.kube After this, reload the user groups either via a reboot or by running \u0026#39;newgrp microk8s\u0026#39;. ubuntu@microk8s-vm:~$ 这个错误信息表示，当前用户 ubuntu 没有足够的权限来访问 MicroK8s。为了修复这个问题，你需要将 ubuntu 用户添加到 microk8s 组，并更改相应的权限。错误信息中已经给出了修复步骤，下面是详细的解释和步骤：\n问题的原因 # MicroK8s 在安装时会创建一个名为 microk8s 的组，只有该组的成员才能访问 MicroK8s 的配置和执行相关命令。默认情况下，普通用户并不在 microk8s 组中，因此无法访问和管理 MicroK8s。\n解决方法 # 将用户添加到 microk8s 组 你需要将 ubuntu 用户添加到 microk8s 组中。这可以通过以下命令完成：\nsudo usermod -a -G microk8s ubuntu 修改权限 接下来，需要修改 Kubernetes 配置文件的权限，使得 ubuntu 用户可以访问它：\nsudo chown -R ubuntu ~/.kube 重新加载用户组 上述步骤会将用户添加到 microk8s 组，并将 .kube 目录的权限更新。为了让权限生效，重新加载用户组，可以使用以下命令：\nnewgrp microk8s 这会立即使 ubuntu 用户的权限更新，而不需要重启计算机。\n4. 再次检查 MicroK8s 状态 # 完成上述步骤后，你应该可以再次运行 microk8s status 命令来查看 MicroK8s 的状态：\nmicrok8s status --wait-ready 总结 # 你需要将 ubuntu 用户添加到 microk8s 组中，才能让该用户访问 MicroK8s。 更改 Kubernetes 配置文件的权限，并通过 newgrp 命令重新加载用户组，以确保权限生效。 执行这些步骤后，问题应该得到解决，你可以正常使用 MicroK8s。\n错误信息 chown: cannot access '/home/ubuntu/.kube': No such file or directory 表示在 /home/ubuntu/ 目录下找不到 .kube 文件夹。原因可能是当前用户还没有在 ~/.kube 目录下创建 Kubernetes 配置文件。通常，这是因为还没有执行过 kubectl 命令，或者在配置文件复制步骤中没有正确生成该目录。\n解决方法 # 1. 确保 .kube 目录存在 # 首先，你需要确保 Kubernetes 配置文件已经生成。执行以下命令生成 Kubernetes 配置文件：\n在 MicroK8s 内获取 kubeconfig 文件：\nsudo microk8s kubectl config view --raw \u0026gt; kubeconfig 将 kubeconfig 文件移动到 ~/.kube 目录： 创建 .kube 目录，并将 kubeconfig 文件移动到该目录下：\nmkdir -p ~/.kube mv kubeconfig ~/.kube/config 更新目录权限： 现在，你已经成功将 kubeconfig 文件放置在 .kube 目录中，可以重新执行权限修改命令：\nsudo chown -R ubuntu ~/.kube 2. 配置 kubectl 使用 MicroK8s 配置 # 如果你已经将配置文件正确放入 ~/.kube/config，可以尝试使用以下命令来测试是否成功配置了 kubectl：\nkubectl get nodes 如果配置正确，你应该能够看到集群中的节点信息。\n3. 重新加载用户组（如果必要） # 如果你已经将 ubuntu 用户添加到 microk8s 组，但还没有刷新用户组，可以执行以下命令：\nnewgrp microk8s 总结 # 错误的根本原因是 .kube 目录不存在。你需要确保配置文件 kubeconfig 被正确生成，并放置到 .kube/config 中。 执行 mkdir -p ~/.kube \u0026amp;\u0026amp; mv kubeconfig ~/.kube/config 可以确保配置文件正确放置。 修改文件权限时，使用 chown -R ubuntu ~/.kube 来确保当前用户有正确的访问权限。 完成这些步骤后，你应该能够正常访问 Kubernetes 集群并使用 kubectl 命令了。\n4. 配置 kubectl 访问 MicroK8s # 为了能够从宿主机（macOS）直接使用 kubectl 来管理 MicroK8s 集群，你需要配置访问权限。\n（1）获取 kubectl 配置文件 # 通过 Multipass 虚拟机执行以下命令，获取 Kubernetes 配置文件：\nsudo microk8s kubectl config view --raw \u0026gt; kubeconfig （2）将配置文件复制到宿主机 # 将该配置文件复制到宿主机（macOS）上的合适目录，比如 ~/.kube/config：\nmkdir -p ~/.kube cp kubeconfig ~/.kube/config （3）验证 kubectl 配置 # 验证 kubectl 是否能够连接到 MicroK8s 集群：\nkubectl get nodes 你应该会看到 microk8s-vm 作为节点，表示 kubectl 成功连接到 MicroK8s 集群。\n5. 启用必要的 MicroK8s 插件 # MicroK8s 提供了多个附加插件，可以根据需要启用它们。\n例如，启用 DNS 和 Ingress：\nmicrok8s enable dns ingress 你可以根据需求启用其他插件，如存储、Metrics Server 等：\nmicrok8s enable storage microk8s enable dashboard microk8s disable ingress microk8s disable storage microk8s disable dashboard 6. 访问 Kubernetes Dashboard（可选） # 如果启用了 Kubernetes Dashboard 插件，可以通过以下命令获取访问 Dashboard 的 token：\nmicrok8s dashboard-proxy 此命令会显示一个 URL，你可以在浏览器中访问该 URL，以便通过 Web 界面管理集群。\n7. 关闭和删除虚拟机 # 如果不再需要 MicroK8s，可以通过以下命令停止虚拟机：\nmultipass stop microk8s-vm 如果希望删除虚拟机及其所有数据，可以运行：\nmultipass delete microk8s-vm multipass purge 总结 # 在 macOS 上安装 MicroK8s 的关键步骤是使用 Multipass 启动一个 Ubuntu 虚拟机，并在虚拟机内安装和配置 MicroK8s。这样，你就可以在本地开发、测试和学习 Kubernetes 集群，或者部署轻量级的应用程序。如果需要更强大的功能，还可以启用多种插件。\n进入虚拟机\nmultipass shell microk8s-v 如果你想重新启动已停止的 microk8s-vm 实例，你可以使用以下命令：\nmultipass start microk8s-vm 这将重新启动并使虚拟机恢复运行状态。如果你想查看当前虚拟机的状态，可以使用：\nmultipass list 这个命令会列出所有虚拟机及其当前状态（如运行中或已停止）。\nwget https://github.com/derailed/k9s/releases/download/v0.32.7/k9s_linux_amd64.deb \u0026amp;\u0026amp; apt install ./k9s_linux_amd64.deb \u0026amp;\u0026amp; rm k9s_linux_amd64.deb wget https://github.com/derailed/k9s/releases/download/v0.32.7/k9s_linux_amd64.deb apt install ./k9s_linux_amd64.deb rm k9s_linux_amd64.deb Warning FailedCreatePodSandBox 92s kubelet Failed to create pod sandbox: rpc error: code = DeadlineExceeded desc = failed to g │ │ et sandbox image \u0026#34;registry.k8s.io/pause:3.7\u0026#34;: failed to pull image \u0026#34;registry.k8s.io/pause:3.7\u0026#34;: failed to pull and unpack image \u0026#34;registry.k8s.io/pa │ │ use:3.7\u0026#34;: failed to resolve reference \u0026#34;registry.k8s.io/pause:3.7\u0026#34;: failed to do request: Head \u0026#34;https://us-west2-docker.pkg.dev/v2/k8s-artifacts-pro │ │ d/images/pause/manifests/3.7\u0026#34;: dial tcp 74.125.199.82:443: i/o timeout │ │ Warning FailedCreatePodSandBox 6s (x7 over 5m15s) kubelet Failed to create pod sandbox: rpc error: code = DeadlineExceeded desc = failed to g │ │ et sandbox image \u0026#34;registry.k8s.io/pause:3.7\u0026#34;: failed to pull image \u0026#34;registry.k8s.io/pause:3.7\u0026#34;: failed to pull and unpack image \u0026#34;registry.k8s.io/pa │ │ use:3.7\u0026#34;: failed to resolve reference \u0026#34;registry.k8s.io/pause:3.7\u0026#34;: failed to do request: Head \u0026#34;https://us-west2-docker.pkg.dev/v2/k8s-artifacts-pro │ │ d/images/pause/manifests/3.7\u0026#34;: dial tcp 172.253.117.82:443: i/o timeout "},{"id":38,"href":"/docs/2024-12-08-devstack/","title":"2024-12-08 devstack","section":"Docs","content":" DevStack # DevStack 是一系列可扩展的脚本，用于根据来自 git master 的所有内容的最新版本快速构建完整的 OpenStack 环境。它可交互地用作开发环境，并作为 OpenStack 项目大部分功能测试的基础。\n源代码位于https://opendev.org/openstack/devstack。\n警告\nDevStack 将在安装过程中对您的系统进行重大更改。仅在专用于此目的的服务器或虚拟机上运行 DevStack。\n快速入门 # 安装 Linux # 从干净且最小限度的 Linux 系统安装开始。DevStack 尝试支持 Ubuntu 的两个最新 LTS 版本：Rocky Linux 9 和 openEuler。\n如果您没有偏好，Ubuntu 22.04（Jammy）是经过最多测试的，并且可能会运行得最顺利。\n添加 Stack 用户（可选） # DevStack 应该以非 root 用户身份运行，并启用 sudo（通常使用“ubuntu”或“cloud-user”等云镜像的标准登录就可以了）。\n如果你不使用云镜像，你可以创建一个单独的堆栈用户来运行 DevStack\n$ sudo useradd -s /bin/bash -d /opt/stack -m stack 确保用户的主目录stack对所有人都具有可执行权限，因为基于 RHEL 的发行版会创建它700，而 Ubuntu 21.04+750 可能会在部署期间导致问题。\n$ sudo chmod +x /opt/stack 由于该用户将对您的系统进行许多更改，因此它应该具有 sudo 权限：\n$ echo \u0026#34;stack ALL=(ALL) NOPASSWD: ALL\u0026#34; | sudo tee /etc/sudoers.d/stack $ sudo -u stack -i 下载 DevStack # $ git clone https://opendev.org/openstack/devstack $ cd devstack 该devstackrepo 包含安装 OpenStack 的脚本和配置文件的模板。\n创建 local.conf # local.conf在 devstack git repo 的根目录创建一个预设有四个密码的文件。\n[[local|localrc]] ADMIN_PASSWORD=secret DATABASE_PASSWORD=$ADMIN_PASSWORD RABBIT_PASSWORD=$ADMIN_PASSWORD SERVICE_PASSWORD=$ADMIN_PASSWORD 这是开始使用 DevStack 所需的最低配置。\n笔记\ndevstack 存储库中的samples目录下有一个示例local.conf文件。\n警告\n密码中只能使用字母数字字符，因为某些服务在使用特殊字符时无法运行。\n开始安装 # $ ./stack.sh 这将需要 15 - 30 分钟，主要取决于您的互联网连接速度。在此过程中将安装许多 git 树和包。\n利润！ # 您现在有一个可以运行的 DevStack！恭喜！\n您的 devstack 将安装keystone、glance、nova、 placement、cinder、neutron和horizon。浮动 IP 将可用，访客可以访问外部世界。\n您可以访问 Horizon 来体验 OpenStack 的 Web 界面，并从那里管理虚拟机、网络、卷和图像。\n您可以在您的 shell 中，然后使用 命令行工具来管理您的 devstack。source openrc``openstack\n您可以创建一个虚拟机并通过 SSH 连接到它。\n您可以运行已配置为与您的 devstack 配合使用的 tempest 测试。cd /opt/stack/tempest\n您可以对 OpenStack 代码进行更改并验证它们。\n进一步了解 # 详细了解我们的配置系统，以便根据您的需要定制 devstack。包括对默认网络进行调整。\n阅读人们所具有的特定设置的指南（注意：指南是即时贡献，可能并不总是与最新的 devstack 保持同步）。\n启用devstack 插件来支持基本 devstack 中不存在的附加服务、功能和配置。\n在您的 CI 中将 devstack 与Ansible 角色和 Zuul V3作业结合使用。使用此完整迁移方法将您的 devstack Zuul V2 作业迁移到 Zuul V3 。\n了解我们尝试使用 devstack 做什么，并通过对项目做出贡献来帮助我们。\n如果您是 devstack 的新贡献者，请参阅：所以您想贡献……\n内容 # 配置 本地配置文件 openrc 最低配置 历史注释 配置注意事项 系统范围的调试 统计信息 内存跟踪器 文件跟踪器 tcpdump 使用 Devstack 进行开发 检查服务 修补服务 测试补丁系列 测试库的更改 常问问题 常见问题 操作和配置 各种各样的 指南 一体化单虚拟机 一体机 一体化 LXC 容器 多节点实验室 带有 Neutron 网络的 DevStack 基于 KVM 的嵌套虚拟化的 DevStack Nova 和 devstack 配置 Octavia 使用 LDAP 部署 DevStack 为 DevStack 做出贡献 一般的 仓库布局 脚本 stackrc 文档 空格规则 控制结构规则 变量和函数 评审准则 进行更改、测试和 CI DevStack 网络 默认值 当地无障碍宾客 专用网络寻址 SSH 访问实例 远程 SSH 访问实例 概述 支持的组件 DevStack 插件注册表 插件 先决条件 插件接口 plugin.sh 合约 示例插件 插件执行顺序 系统包 在 OpenStack Gate 中使用插件 参见 在 DevStack 中使用 Systemd 为什么用这个来代替屏幕？ 单元结构 操控单位 查询日志 调试 未来工作 参考 暴风雨 将 Zuul V2 CI 作业迁移到 V3 在哪里托管作业定义。 从哪个基础工作开始 作业变量 DevStack Gate 标志 Zuul CI 职位 [Zuul CI 角色 ./stack.sh status\n"},{"id":39,"href":"/docs/2024-12-08-mutilpass%E6%93%8D%E4%BD%9C/","title":"2024-12-08 mutilpass操作","section":"Docs","content":"如果你想销毁一个已经创建的 multipass 虚拟机（例如 openstack-vm），可以按照以下步骤操作：\n1. 停止虚拟机 # 首先，停止虚拟机：\nmultipass stop openstack-vm 这将停止虚拟机，但不会删除它。如果你只想关闭虚拟机，可以跳过删除步骤。\n2. 删除虚拟机 # 要彻底销毁虚拟机并删除它的所有相关文件，你需要使用 multipass delete 命令：\nmultipass delete openstack-vm 这将删除虚拟机，但是文件系统和相关数据可能会被保留。\n3. 清理虚拟机数据 # 如果你希望完全清除虚拟机及其所有相关数据（包括磁盘映像、配置文件等），可以使用 multipass purge 命令：\nmultipass purge 这将清除所有已删除虚拟机的所有磁盘和缓存数据。\n完整的操作步骤 # 停止虚拟机：\nmultipass stop openstack-vm 删除虚拟机：\nmultipass delete openstack-vm 清理磁盘和缓存数据：\nmultipass purge 总结 # multipass stop \u0026lt;vm-name\u0026gt;：停止虚拟机。 multipass delete \u0026lt;vm-name\u0026gt;：删除虚拟机。 multipass purge：清除删除的虚拟机的所有相关数据。 这些步骤将帮助你完全销毁虚拟机。如果你需要重新创建虚拟机，可以再次使用 multipass launch 命令进行创建。\n"},{"id":40,"href":"/docs/2024-12-08-nano%E6%93%8D%E4%BD%9C/","title":"2024-12-08 nano操作","section":"Docs","content":"nano 是一个在 Unix-like 系统中非常流行的文本编辑器，适合新手和快速编辑配置文件。它是一个基于命令行的编辑器，功能简洁且易于使用。下面是一些基本的 nano 使用方法：\n1. 打开文件 # 要编辑一个文件，可以在命令行中输入：\nnano \u0026lt;文件路径\u0026gt; 如果文件不存在，nano 会创建一个新的文件。例如，要编辑 local.conf 文件，可以使用：\nnano local.conf 2. 编辑文件 # 进入 nano 编辑器后，你可以直接开始输入文本。光标可以用方向键移动，编辑内容就像在普通的文本编辑器中一样。\n3. 保存文件 # 按下 Ctrl + O（即按住 Ctrl 键，然后按 O 键）保存当前文件。屏幕底部会显示 File Name to Write: \u0026lt;文件路径\u0026gt;，表示文件已准备好保存。 按 Enter 键确认保存文件。 4. 退出 nano # 按\nCtrl + X\n退出\nnano 。如果你在退出前修改了文件，\nnano 会提示你是否保存更改：\n如果想保存，按 Y（Yes），然后按 Enter。 如果不想保存更改，按 N（No）。 5. 基本命令 # 在 nano 中，很多操作是通过 Ctrl + 字母键 来完成的。下面是一些常见的快捷键：\nCtrl + O: 保存文件（Write Out）。 Ctrl + X: 退出编辑器（Exit）。 Ctrl + W: 搜索文本（Where Is）。输入要搜索的内容，按 Enter 搜索。 Ctrl + K: 剪切当前行。 Ctrl + U: 粘贴剪切的内容。 Ctrl + J: 格式化文本（自动换行）。 Ctrl + C: 显示光标当前位置（行号和列号）。 Ctrl + T: 调用拼写检查。 6. 移动光标 # 箭头键：上下左右移动光标。 Ctrl + A：移动到行首。 Ctrl + E：移动到行尾。 Ctrl + Y：向上滚动一页。 Ctrl + V：向下滚动一页。 7. 删除文本 # Backspace：删除光标前面的字符。 Ctrl + D：删除光标下的字符。 8. 剪切、复制和粘贴 # Ctrl + K：剪切光标所在行。 Ctrl + U：粘贴剪切的内容。 Ctrl + ^（按住 Ctrl 后按 ^）开始选择文本，移动光标选择一块区域，选择完后可以通过 Ctrl + K 剪切选中的文本，或通过 Ctrl + U 粘贴。 9. 帮助信息 # 如果你需要帮助，按 Ctrl + G 可以查看 nano 的帮助文档，了解所有快捷键和功能。\n总结 # nano 是一个轻量级、易于使用的命令行文本编辑器。它通过简洁的快捷键使得文件编辑更加直观，不需要记住太多复杂的命令，适合用来快速编辑配置文件或进行简单的文本编辑。\n"},{"id":41,"href":"/docs/2024-12-08-openstack%E5%92%8Ckubernetes%E5%8C%BA%E5%88%AB/","title":"2024-12-08 openstack和kubernetes区别","section":"Docs","content":"OpenStack 和 Kubernetes 都是现代云计算和容器化技术中的重要组件，但它们关注的领域和解决的问题有所不同。下面是对两者关系的详细解释。\n1. OpenStack 是什么？ # OpenStack 是一个开源的云计算平台，旨在提供 IaaS（基础设施即服务），主要用于构建公共或私有云环境。它由一系列的模块组成，用于管理计算、存储、网络、安全等资源。\nOpenStack 的核心组件： # Nova：计算服务，负责虚拟机（VM）的管理和调度。 Neutron：网络服务，提供虚拟网络、子网、路由等功能。 Cinder：块存储服务，用于管理磁盘存储。 Glance：镜像服务，提供虚拟机镜像的存储和管理。 Keystone：身份认证服务，管理用户和权限。 Horizon：Web 控制面板，用于管理 OpenStack 环境。 OpenStack 主要面向虚拟化资源管理（主要是虚拟机），用于提供基础设施服务，支持创建、管理和销毁虚拟机、虚拟存储和虚拟网络。\n2. Kubernetes 是什么？ # Kubernetes 是一个开源的容器编排平台，用于自动化容器的部署、扩展和管理。它解决了容器化应用程序在大规模运行中的编排问题，特别是多容器环境的管理。\nKubernetes 的核心功能： # Pod：Kubernetes 的最小部署单元，包含一个或多个容器。 Service：定义了容器之间如何通信，并为容器提供一个稳定的网络入口。 Deployment：管理应用程序的副本和更新。 Node：运行容器的工作节点。 ReplicaSet：确保某个特定数量的 Pod 副本在任何时间点都在运行。 ConfigMap/Secret：管理应用的配置和敏感信息。 Kubernetes 主要面向 容器化 的应用部署和管理，尤其适用于微服务架构和分布式应用程序。\n3. OpenStack 和 Kubernetes 的关系 # OpenStack 和 Kubernetes 解决的是不同层次的问题，但它们也可以结合使用，为云环境提供更强大的功能。\nOpenStack 提供基础设施，Kubernetes 提供容器编排： # OpenStack 提供了虚拟机（VM）、存储、网络等基础设施层面的资源，可以被 Kubernetes 用来运行和管理容器化应用。 Kubernetes 运行在 OpenStack 提供的计算资源（虚拟机）上，并管理在这些资源上运行的容器化应用。 可以把 OpenStack 看作是为容器提供运行环境的基础设施层，而 Kubernetes 则是在此基础上提供容器编排和应用管理的功能。\n结合使用的场景： # OpenStack + Kubernetes 作为公有云和私有云的基础： OpenStack 提供私有云或混合云的基础设施，Kubernetes 可以在 OpenStack 虚拟机上运行，为应用提供容器编排。 在这种模式下，OpenStack 负责虚拟机和资源的管理，而 Kubernetes 负责容器的部署、扩展和管理。 Kubernetes 在 OpenStack 中作为计算资源管理： 在 OpenStack 环境中，Kubernetes 可以部署为应用管理平台，利用 OpenStack 提供的虚拟机、存储和网络资源，运行容器化应用。 OpenStack 可能使用 Kubernetes 来管理容器化应用的运行，而 Kubernetes 使用 OpenStack 的 Nova 计算服务提供容器实例的计算资源。 OpenStack Magnum： OpenStack 提供了一个名为 Magnum 的项目，它允许在 OpenStack 环境中直接运行 Kubernetes 集群。Magnum 是一个容器管理服务，可以通过 OpenStack 来管理和调度 Kubernetes 集群。 Magnum 可以通过 Docker 或 Kubernetes 来创建和管理容器集群，这使得 Kubernetes 可以无缝地运行在 OpenStack 的基础设施上。 共享资源和集成： OpenStack 的虚拟化和网络功能可以为 Kubernetes 提供底层资源支持，比如 OpenStack 的虚拟机（Nova）、网络（Neutron）和存储（Cinder）服务。 Kubernetes 可以使用 OpenStack 提供的 Block Storage（Cinder） 和 Object Storage（Swift），以及 Neutron 网络服务，实现更高效的存储和网络管理。 4. OpenStack 和 Kubernetes 的区别： # 尽管 OpenStack 和 Kubernetes 可以结合使用，但它们之间有以下一些本质的区别：\n特性 OpenStack Kubernetes 目标 提供 IaaS，管理虚拟机、存储、网络等基础设施 提供容器编排，管理容器化应用的生命周期 资源管理 管理计算、存储、网络等物理资源（如虚拟机） 管理容器资源，自动化部署、扩展、监控容器应用 部署方式 运行在虚拟机或物理主机上，提供虚拟机、存储等 部署容器应用并自动管理容器的扩展、负载均衡等 架构 主要基于虚拟化技术，支持虚拟机和存储等资源 基于容器技术，管理微服务架构和容器化应用 工作负载类型 虚拟机和物理机器上的应用 容器化的应用和微服务 使用场景 适用于私有云、公有云、混合云的基础设施搭建 适用于大规模、分布式的微服务架构和容器化应用 5. 总结： # OpenStack 是一个基础设施即服务（IaaS）平台，主要解决虚拟化资源的管理和提供，包括虚拟机、存储、网络等。 Kubernetes 是一个容器编排平台，专注于管理容器化应用的部署、扩展和运行。 它们是可以协同工作的：OpenStack 提供计算资源（虚拟机、存储、网络），而 Kubernetes 在这些计算资源上运行容器化应用，并自动进行应用的调度和管理。 通过 OpenStack 和 Kubernetes 的结合，用户可以在私有云或公有云环境中，灵活地使用虚拟机和容器来部署和管理应用，满足不同规模和需求的应用架构。 如果你的目标是构建一个高效的云环境，OpenStack 和 Kubernetes 是可以互补的，能够为基础设施层和应用层提供强大的支持。\n"},{"id":42,"href":"/docs/2024-12-08-openstack%E9%9C%80%E8%A6%81%E5%87%A0%E5%8F%B0%E8%99%9A%E6%8B%9F%E6%9C%BA/","title":"2024-12-08 openstack需要几台虚拟机","section":"Docs","content":"使用 OpenStack Ansible 部署 OpenStack 时，通常会按照不同的角色分配到多台虚拟机上，具体的虚拟机数量和分配方式取决于你选择的部署规模和设计架构。对于小规模的测试和实验环境，可以在一台虚拟机上部署所有组件，而对于生产环境，通常会将 OpenStack 的各个组件分布到不同的虚拟机上，以确保更好的性能和高可用性。\n以下是部署 OpenStack 时可能需要的虚拟机和每台虚拟机的角色说明：\n1. 控制节点（Controller Node） # 数量：1-2 台\n作用\n：\n控制节点负责管理 OpenStack 环境的所有核心服务，包括身份认证（Keystone）、图像服务（Glance）、计算服务（Nova）、网络服务（Neutron）、块存储服务（Cinder）、仪表板（Horizon）等。 控制节点通常部署 OpenStack 的 管理服务，这些服务需要处理来自计算节点和存储节点的请求。 需要更多的 CPU 和内存资源以处理控制和管理任务，尤其是数据库（如 MySQL）、消息队列（如 RabbitMQ）等需要在控制节点上运行。 对于生产环境，建议部署两个控制节点以提供高可用性，确保如果一个控制节点失败，另一个节点可以接管管理任务。 2. 计算节点（Compute Node） # 数量：至少 1 台（根据需要扩展）\n作用\n：\n计算节点负责运行虚拟机实例，提供计算资源。计算节点上会运行 Nova 计算服务，它是 OpenStack 中管理虚拟机生命周期的核心组件。 计算节点需要安装 KVM（或其他虚拟化技术）来运行虚拟机，并且需要充足的 CPU、内存和存储资源。 对于生产环境，通常会有多个计算节点，以分散虚拟机负载并提高容错能力。 3. 网络节点（Network Node） # 数量：1 台（可选，取决于网络需求）\n作用\n：\n网络节点负责处理 OpenStack 环境中的网络服务，通常会运行 Neutron 网络服务的关键组件（如 L3 路由器、DHCP、VPN 等）。 如果你需要使用 Neutron 的高级网络功能（如 VLAN、VXLAN、SDN 等），则建议单独设置网络节点。 在较小的环境中，网络服务通常可以与控制节点合并，但对于大规模部署，最好将网络服务独立出来。 4. 存储节点（Storage Node） # 数量：1-2 台（根据存储需求）\n作用\n：\n存储节点用于提供 OpenStack 的块存储服务，运行 Cinder 服务并提供底层存储。 存储节点可以提供硬盘存储来支持虚拟机的磁盘挂载，特别是在需要大规模存储的环境中非常重要。 如果你还计划使用 Ceph 存储或其他分布式存储解决方案，那么存储节点的数量可能会更多，且会配置相应的存储服务。 5. 数据库节点（Database Node） # 数量：1 台（可以与控制节点合并）\n作用\n：\n数据库节点用于承载 OpenStack 的数据库服务（如 MySQL 或 PostgreSQL）。 OpenStack 的控制节点通常依赖数据库来存储和管理各种元数据（如虚拟机信息、网络信息、用户权限等）。 在小规模环境中，数据库可以与控制节点合并，但对于生产环境，通常会将数据库服务分离，以提高性能和可用性。 6. 消息队列节点（Message Queue Node） # 数量：1 台（可以与控制节点合并）\n作用\n：\nOpenStack 使用消息队列（通常是 RabbitMQ 或 Kafka）来实现各个服务之间的异步通信。消息队列节点负责管理消息传递和队列。 与数据库节点一样，消息队列服务可以与控制节点合并，但在大规模部署中，通常会将其单独部署以保证高可用性。 7. 负载均衡节点（Load Balancer Node） # 数量：可选（通常用于生产环境）\n作用\n：\n在 OpenStack 中，负载均衡通常用于在多个控制节点之间分配流量，确保高可用性。可以使用 HAProxy 或 Keepalived 来实现负载均衡。 负载均衡节点可以与其他服务（如控制节点或网络节点）共享，但通常为了高可用性，建议单独配置。 8. 其他（如监控节点） # 数量：可选\n作用\n：\n你可能还需要部署监控节点来监控 OpenStack 环境的健康状况、性能指标等。常用的监控工具包括 Prometheus、Grafana、Zabbix 等。 如果你使用的是 Ceph 存储系统，还需要配置监控和管理 Ceph 集群的节点。 示例架构（小规模部署）： # 对于小规模的部署，你可以将所有组件部署在 3-4 台虚拟机上：\n虚拟机 1：控制节点 + 数据库 + 消息队列 虚拟机 2：计算节点 + 存储节点 虚拟机 3：网络节点（可选，如果需要复杂的网络服务） 虚拟机 4：负载均衡节点（可选） 示例架构（中大型部署）： # 对于中到大型部署，通常会使用更多的虚拟机来分散负载：\n虚拟机 1-2：控制节点（两个控制节点以保证高可用性） 虚拟机 3-6：计算节点（多台计算节点来支持更多虚拟机） 虚拟机 7：网络节点（用于提供高级网络功能，如 VLAN、VXLAN 等） 虚拟机 8-10：存储节点（提供 Cinder 存储服务，可能还包括 Ceph 存储） 虚拟机 11：负载均衡节点（提供高可用性） 虚拟机 12：监控节点（可选，用于集成监控和日志收集服务） 总结： # 最小部署：至少需要 1-2 台虚拟机，控制节点和计算节点可以合并。 生产环境：建议分配 3 台以上虚拟机，分别用于控制节点、计算节点和网络节点（如有需要）。存储和数据库等服务可以根据需求进一步拆分到不同节点。 实际的虚拟机数量和资源需求还要根据你的部署规模、网络设计、存储需求等因素进行调整。你可以根据需要的服务数量来决定每个角色的分配，并逐步扩展节点。\n"},{"id":43,"href":"/docs/2024-12-08-%E5%9D%97%E5%AD%98%E5%82%A8%E5%92%8C%E5%AF%B9%E8%B1%A1%E5%82%A8%E5%AD%98%E5%8C%BA%E5%88%AB/","title":"2024-12-08 块存储和对象储存区别","section":"Docs","content":"块存储和对象存储是两种常见的存储方式，它们在存储模型、使用场景、性能和管理方式等方面存在显著差异。理解这两者的区别，有助于选择适合的存储解决方案来满足特定应用和业务需求。\n1. 存储模型的区别 # 块存储（Block Storage）： 数据存储方式：块存储将数据分割为固定大小的块（通常为 512 字节或 4 KB）。每个块都有一个唯一的地址，计算机可以单独访问这些块。 如何工作：块存储像硬盘一样提供一个裸存储卷，应用程序和操作系统可以直接访问这些块。通常用作操作系统的磁盘、数据库存储等。 使用方式：通常用于需要高速访问和低延迟的应用场景，如数据库、高性能应用和虚拟机存储等。 对象存储（Object Storage）： 数据存储方式：对象存储是将数据以“对象”的形式存储，每个对象由数据、元数据和唯一的标识符（对象ID）组成。对象存储是一个无层次、扁平的存储系统，不像文件系统那样有目录结构。 如何工作：对象存储通过 RESTful API（如 Amazon S3 API）来访问数据，数据以对象形式存储并通过唯一的 URL 进行检索。每个对象通常是一个文件（如图片、视频、文档等），可以是任意大小。 使用方式：适用于大量的非结构化数据存储，如备份、日志存储、大数据存储、网站文件存储等。 2. 性能和可扩展性 # 块存储\n：\n性能：块存储通常提供较低的延迟和较高的吞吐量，因为它直接与操作系统交互，适合需要频繁、快速读取和写入操作的应用（如数据库、高性能计算、虚拟机等）。 可扩展性：扩展块存储通常需要通过增加磁盘或者在存储阵列中增加更多的存储单元来实现。扩展时可能需要手动干预或配置。 对象存储\n：\n性能：对象存储的性能通常不如块存储，因为它是基于 HTTP 协议的 RESTful API 来进行访问，因此延迟会相对较高。不过，适合存储大规模的数据并进行低频访问（如备份、归档等）。 可扩展性：对象存储天然具备高可扩展性，能够轻松处理 PB 级别的存储需求。它通过分布式架构和数据冗余技术，自动在多个节点之间扩展存储空间，具有更好的横向扩展性。 3. 数据访问方式 # 块存储： 访问方式：可以通过操作系统的文件系统直接访问和管理块存储，支持传统的文件系统结构（如 NTFS、EXT4、XFS 等），操作系统对块存储提供低级的读写操作。 适用场景：适合对存储进行频繁随机读写的应用，比如数据库、虚拟化环境中的虚拟机磁盘等。 对象存储： 访问方式：对象存储通过 API（如 RESTful API）进行访问，支持通过 Web 请求获取对象数据。数据不需要通过操作系统文件系统进行访问，而是通过唯一的 URL 地址或对象键来检索。 适用场景：适合存储大量非结构化数据，如视频、图片、音频、备份数据、日志文件、静态网页内容等。 4. 管理和操作 # 块存储： 管理方式：块存储的管理通常更复杂，特别是在大规模部署时。你需要分配、格式化和挂载每个磁盘，进行数据备份、恢复等操作。 优点：高性能，支持对文件系统、数据库等的直接管理。 缺点：管理开销较大，扩展时可能较为复杂，需要手动干预或配置。 对象存储： 管理方式：对象存储的管理相对简单，存储的对象通过唯一标识符（如对象ID）进行访问，自动管理冗余和扩展。通过简单的 API 和 Web 控制面板即可完成操作。 优点：易于扩展、管理和访问，适合存储大规模、非结构化的数据。 缺点：不适合低延迟或高频率随机读写操作，因为访问是基于 HTTP 协议，且没有传统文件系统的目录结构。 5. 常见使用场景 # 块存储： 数据库存储：块存储适用于需要高 I/O 性能的数据库应用，如关系型数据库（MySQL、PostgreSQL）或 NoSQL 数据库（MongoDB、Cassandra）。 虚拟化：虚拟机存储虚拟硬盘（如 VMware 或 KVM）使用块存储。 高性能应用：需要快速数据访问的高性能计算（HPC）应用、交易系统等。 对象存储： 大数据存储：存储海量的非结构化数据，如视频、音频、日志文件、图像等。 备份与归档：适合大规模备份和长期存储，很多公司将对象存储用于云备份解决方案。 内容分发网络（CDN）：将静态网站内容、视频流等分发到全球各地的节点，利用对象存储优化分发速度。 静态网页存储：存储 HTML 文件、图片和其他静态资源。 6. 冗余与容错机制 # 块存储： 冗余：块存储的冗余通常由存储设备（如 SAN、NAS）提供，可以使用 RAID 等技术确保数据的高可用性和冗余。 容错机制：块存储支持快速备份和恢复，但如果单台硬盘或存储设备出现故障，可能会影响存储的可用性。 对象存储： 冗余：对象存储通常内建数据冗余机制，像 AWS S3 等平台自动将数据复制到多个数据中心或节点，以确保数据的持久性和可用性。 容错机制：对象存储的高可用性和数据冗余通常通过分布式架构自动处理，具备更强的容错能力。 7. 总结 # 特性 块存储 (Block Storage) 对象存储 (Object Storage) 存储结构 数据被分割成固定大小的块 数据以对象（包括数据、元数据和ID）存储 访问方式 通过操作系统文件系统直接访问 通过 API（如 RESTful API）访问 性能 适合高性能、低延迟的应用 适合大规模、非结构化数据的存储和归档 管理复杂性 需要手动管理和配置，管理较为复杂 管理简便，自动化较强 扩展性 扩展较为复杂，可能需要人工干预 极高的扩展性，适合大规模存储 典型应用场景 数据库、虚拟机、性能要求高的应用 大数据存储、备份、归档、静态内容存储 冗余与容错 由存储设备（如 RAID）提供冗余 自动数据冗余和容错机制 总的来说，块存储适用于需要频繁随机读写和高性能的应用，如数据库和虚拟机磁盘；而对象存储适用于大规模的非结构化数据存储，如备份、归档、媒体内容分发等。\n"},{"id":44,"href":"/docs/2024-12-09-docker-daemon.json/","title":"2024-12-09 docker daemon.json","section":"Docs","content":"在国内使用 Docker 的朋友们，可能都遇到过配置镜像源来加速镜像拉取的操作。然而，最近几个月发现许多曾经常用的国内镜像站（包括各种云服务商和高校镜像站）已经无法使用。因此，本人开始搜索并汇总了目前可用的镜像站和镜像加速地址，并计划定期测试它们的可用性，并更新这个列表。如果您知道新的可用站点，也欢迎随时补充哦！\nDocker 镜像加速列表（20241202已更新） # cat \u0026gt; /etc/docker/daemon.json \u0026lt;\u0026lt; EOF { \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://docker.m.daocloud.io\u0026#34;, \u0026#34;https://hub.roker.org\u0026#34;, \u0026#34;https://si7y70hh.mirror.aliyuncs.com\u0026#34;, \u0026#34;https://registry.docker-cn.com\u0026#34;, \u0026#34;https://hub.geekery.cn\u0026#34;, \u0026#34;https://hub.littlediary.cn\u0026#34;, \u0026#34;https://docker.rainbond.cc\u0026#34;, \u0026#34;https://docker.unsee.tech\u0026#34;, \u0026#34;https://hub.crdz.gq\u0026#34;, \u0026#34;https://docker.nastool.de\u0026#34;, \u0026#34;https://hub.firefly.store\u0026#34;, \u0026#34;https://registry.dockermirror.com\u0026#34;, \u0026#34;https://docker.1panelproxy.com\u0026#34;, \u0026#34;https://rhub.rat.dev\u0026#34;, \u0026#34;https://docker.kejilion.pro\u0026#34;, \u0026#34;https://dhub.kubesre.xyz\u0026#34;, \u0026#34;https://docker.1panel.live\u0026#34;, \u0026#34;https://dockerpull.org\u0026#34;, \u0026#34;https://docker.udayun.com\u0026#34;, \u0026#34;https://docker.hlmirror.com\u0026#34;, \u0026#34;https://docker.mirrors.ustc.edu.cn\u0026#34;, \u0026#34;http://hub-mirror.c.163.com\u0026#34;, \u0026#34;https://dockerhub.azk8s.cn\u0026#34;, \u0026#34;https://hub.uuuadc.top\u0026#34;, \u0026#34;https://docker.anyhub.us.kg\u0026#34;, \u0026#34;https://dockerhub.jobcher.com\u0026#34;, \u0026#34;https://dockerhub.icu\u0026#34;, \u0026#34;https://cloudsx.top\u0026#34;, \u0026#34;https://docker.ckyl.me\u0026#34;, \u0026#34;https://docker.awsl9527.cn\u0026#34;, \u0026#34;https://docker.chenby.cn\u0026#34;, \u0026#34;https://docker.hpcloud.cloud\u0026#34;, \u0026#34;https://atomhub.openatom.cn\u0026#34;], \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;], \u0026#34;insecure-registries\u0026#34;: [\u0026#34;http://192.168.200.250\u0026#34;], \u0026#34;log-driver\u0026#34;: \u0026#34;json-file\u0026#34;, \u0026#34;log-opts\u0026#34;: { \u0026#34;max-size\u0026#34;: \u0026#34;1000m\u0026#34;,\u0026#34;max-file\u0026#34;: \u0026#34;6\u0026#34; }, \u0026#34;storage-driver\u0026#34;: \u0026#34;overlay2\u0026#34; } EOF 请注意！有些镜像站仅提供基础镜像或白名单镜像，如果某个加速地址无法拉取到所需的镜像，可以尝试切换到其他地址。有些代理站点是热心网友自费搭建的，请务必合理使用。如果侵犯了您的权益，请随时联系我，我会及时删除相关信息。感谢您的理解与支持！\nDockerHub 镜像仓库 是否正常 hub.geekery.cn 正常 hub.littlediary.cn 正常 docker.rainbond.cc 正常 docker.unsee.tech 正常 docker.m.daocloud.io 正常 hub.crdz.gq 正常 docker.nastool.de 正常 hub.firefly.store 正常 registry.dockermirror.com 正常 docker.1panelproxy.com 正常 hub.rat.dev 正常 docker.udayun.com 正常 docker.kejilion.pro 正常 dhub.kubesre.xyz 正常 docker.1panel.live 正常 dockerpull.org 正常 docker.hlmirror.com 新增 hub.xdark.top 失效 noohub.ru 失效 freeno.xyz 失效 docker-cf.registry.cyou 失效 dockerpull.com 失效 lynn520.xyz 失效 hub.yuzuha.cc 失效 ccr.ccs.tencentyun.com 失效 docker.chenby.cn 失效 docker.linkedbus.com 失效 docker.hlyun.org 失效 dockerproxy.cn 失效 docker.registry.cyou 失效 dockerproxy.com 失效 ginger20240704.asia 失效 hub.docker-ttc.xyz 失效 docker.mrxn.net 失效 docker.wget.at 失效 dislabaiot.xyz 失效 docker.hpcloud.cloud 失效 atomhub.openatom.cn 失效 docker.nat.tf 失效 doublezonline.cloud 失效 dockerproxy.github.io 失效 配置方式1：临时使用 # 直接使用，直接拿镜像域名拼接上官方镜像名，例如要拉去镜像 istio/distroless，可以用下面写法（不要带 https://）\ndocker pull docker.unsee.tech/istio/distroless 配置方式2：长久有效 # Ubuntu 16.04+、Debian 8+、CentOS 7+\n修改文件 /etc/docker/daemon.json（如果不存在则需要创建创建，注意不要写入中文，要带 https://），并重启服务。\n# 创建目录 sudo mkdir -p /etc/docker # 写入配置文件 sudo tee /etc/docker/daemon.json \u0026lt;\u0026lt;-\u0026#39;EOF\u0026#39; { \u0026#34;registry-mirrors\u0026#34;: [ \u0026#34;https://docker.unsee.tech\u0026#34;, \u0026#34;https://dockerpull.org\u0026#34;, \u0026#34;docker.1panel.live\u0026#34;, \u0026#34;https://dockerhub.icu\u0026#34; ] } EOF # 重启docker服务 sudo systemctl daemon-reload \u0026amp;\u0026amp; sudo systemctl restart docker 可直接使用 docker pull拉去镜像进行测试：\n或用以下命令检查是否生效：\nping -c 3 docker.unsee.tech "},{"id":45,"href":"/docs/2024-12-09-helmchart-%E9%83%A8%E7%BD%B2flask%E5%BA%94%E7%94%A8/","title":"2024-12-09 helmchart 部署flask应用","section":"Docs","content":"要使用 Helm 部署一个 Flask 应用，你需要进行几个步骤，主要包括：\n创建一个 Flask 应用。 编写 Dockerfile 来将 Flask 应用容器化。 推送容器镜像 到容器镜像仓库。 创建 Helm Chart 来描述如何部署应用。 步骤 1: 创建一个简单的 Flask 应用 # 首先，确保你已经创建了一个简单的 Flask 应用。例如：\napp.py：\nfrom flask import Flask app = Flask(__name__) @app.route(\u0026#39;/\u0026#39;) def hello_world(): return \u0026#34;Hello, World!\u0026#34; if __name__ == \u0026#39;__main__\u0026#39;: app.run(host=\u0026#39;0.0.0.0\u0026#39;, port=5000) 步骤 2: 编写 Dockerfile # 然后，为你的 Flask 应用创建一个 Dockerfile，将 Flask 应用容器化。假设你将 Flask 应用保存为 app.py。\nDockerfile：\n# 使用官方 Python 镜像 FROM python:3.9-slim # 设置工作目录 WORKDIR /app # 安装 Flask COPY requirements.txt /app/ RUN pip install --no-cache-dir -r requirements.txt RUN pip install flask werkzeug==1.0.1 # 复制 Flask 应用 COPY . /app # 运行 Flask 应用 CMD [\u0026#34;python\u0026#34;, \u0026#34;app.py\u0026#34;] 你需要创建 requirements.txt 文件来列出依赖项：\nrequirements.txt：\nFlask==2.0.2 步骤 3: 构建并推送 Docker 镜像 # 使用以下命令构建 Docker 镜像：\ndocker build -t 547475331/flask-app:latest . docker tag docker push 547475331/flasktest:latest 然后将镜像推送到 Docker 仓库（如 Docker Hub 或其他容器镜像仓库）：\ndocker push your-dockerhub-username/flask-app:latest 在 Ubuntu 上安装 Helm 的步骤如下：\n步骤 1: 更新系统的包管理器索引 # 首先，确保你的包管理器索引是最新的：\nsudo apt-get update 步骤 2: 安装 Helm 依赖项 # 你需要确保系统上安装了 curl 和 apt-transport-https，这些工具用于下载 Helm 安装脚本和管理 APT 仓库。\nsudo apt-get install curl apt-transport-https 步骤 3: 添加 Helm 的官方 APT 仓库 # 添加 Helm 的官方 APT 仓库的 GPG 密钥：\ncurl https://baltocdn.com/helm/signing.asc | sudo apt-key add - 然后，将 Helm 仓库添加到系统的 APT 源列表中：\nsudo apt-get install -y software-properties-common sudo add-apt-repository \u0026#34;deb https://baltocdn.com/helm/stable/debian/ all main\u0026#34; 步骤 4: 安装 Helm # 更新包管理器的索引：\nsudo apt-get update 然后，安装 Helm：\nsudo apt-get install helm 步骤 5: 验证 Helm 安装 # 安装完成后，你可以通过以下命令验证 Helm 是否安装成功：\nhelm version 如果 Helm 安装成功，你应该看到类似如下的输出：\nversion.BuildInfo{Version:\u0026#34;v3.9.0\u0026#34;, GitCommit:\u0026#34;abcdef123456\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, GoVersion:\u0026#34;go1.16.7\u0026#34;} 步骤 6: 可选 - 配置 Helm 仓库 # 你可以使用以下命令添加 Helm 仓库：\nhelm repo add stable https://charts.helm.sh/stable 然后更新你的 Helm 仓库列表：\nhelm repo update 完成 # 现在，Helm 应该已成功安装并配置完毕。你可以开始使用 Helm 部署 Kubernetes 应用、管理 Helm Charts 等。如果你有其他问题或遇到困难，欢迎随时提问！\n步骤 4: 创建 Helm Chart # 接下来，使用 Helm 创建一个 Chart 来部署 Flask 应用。假设你已经安装了 Helm。\n创建一个新的 Helm Chart： helm create flask-app 这将创建一个名为 flask-app 的文件夹，并且包含了一些 Helm 默认的文件和模板。\n编辑 Helm Chart 配置： 打开 flask-app/values.yaml，并修改以下内容：\n# 修改镜像的名称和标签 image: repository: your-dockerhub-username/flask-app tag: latest pullPolicy: IfNotPresent # 修改服务端口 service: port: 80 修改 flask-app/templates/deployment.yaml： 确保 deployment.yaml 中的容器端口与 Flask 应用的端口匹配（Flask 默认是 5000），但因为我们通常希望将它暴露在 80 端口上，记得在服务的配置中进行修改。\n编辑 flask-app/templates/deployment.yaml 中的 containerPort：\nports: - containerPort: 5000 步骤 5: 部署 Flask 应用 # 使用以下命令安装 Helm Chart：\nhelm install flask-app ./flask-app 这将使用你刚刚创建的 Helm Chart 部署 Flask 应用。\n步骤 6: 检查部署状态 # 你可以使用以下命令检查部署的状态：\nkubectl get pods 查看服务的暴露方式，确保你可以通过外部访问到 Flask 应用。\nkubectl get svc 你应该能够看到一个服务正在监听 HTTP 流量，并且你的 Flask 应用已经成功部署在 Kubernetes 集群中。\n这样，你就通过 Helm 部署了一个简单的 Flask 应用。如果你有其他特定的需求或者遇到问题，欢迎继续询问！\n是的，你的 Dockerfile 基本上已经正确地设置了 Python 和 Flask 应用的运行环境，并且通过显式安装指定版本的 werkzeug（1.0.1）来解决兼容性问题。不过，还有一些优化和改进可以让 Dockerfile 更加高效和标准化。\n以下是优化后的 Dockerfile：\n优化后的 Dockerfile # # 使用官方 Python 3.9-slim 镜像作为基础镜像 FROM python:3.9-slim # 设置工作目录 WORKDIR /app # 复制依赖文件（优先复制 requirements.txt，利用 Docker 缓存机制） COPY requirements.txt /app/ # 安装依赖（显式安装 werkzeug 的特定版本） RUN pip install --no-cache-dir -r requirements.txt \u0026amp;\u0026amp; \\ pip install werkzeug==1.0.1 RUN apt-get update \u0026amp;\u0026amp; apt-get install -y curl # 复制应用代码到工作目录 COPY . /app # 暴露应用端口（可选，通常 Flask 默认运行在 5000 端口） EXPOSE 5000 # 运行 Flask 应用 CMD [\u0026#34;python\u0026#34;, \u0026#34;app.py\u0026#34;] 改进点说明 # 缓存优化 将 COPY requirements.txt /app/ 和 RUN pip install 单独放在一起，这样如果应用代码（比如 .py 文件）变更而依赖未改变时，Docker 构建可以重用缓存，避免重复安装依赖。 明确暴露端口 使用 EXPOSE 5000 使镜像默认说明 Flask 应用监听的端口。虽然不强制，但有助于文档化容器的用途。 合并安装命令 在安装 requirements.txt 和单独安装 werkzeug 时，合并为一条命令，减少镜像层数并提高构建效率。 构建和运行步骤 # 1. 构建镜像 # docker build -t flask-app . 2. 运行容器 # docker run -d -p 5000:5000 flask-app 注意事项 # 检查 requirements.txt 文件 确保 requirements.txt 中没有指定 werkzeug 的其他版本。否则可能会覆盖 werkzeug==1.0.1 的安装。\n示例 requirements.txt：\nflask==2.0.3 some-other-library==1.2.3 代码文件结构 确保你的 Flask 应用文件夹结构类似以下内容：\napp/ ├── app.py ├── requirements.txt └── templates/ 运行测试 在容器内验证应用是否正常工作：\ndocker exec -it \u0026lt;container-id\u0026gt; bash curl http://localhost:5000 按照优化后的 Dockerfile 和步骤重新构建并运行容器，应该能够顺利启动你的 Flask 应用！如果仍然有问题，可以随时提供更多信息，我会继续协助你排查。\n根据报错信息，werkzeug 的版本可能与 Flask 不兼容。ContextVar 是较新的 werkzeug 中引入的模块，而你的镜像中可能安装了一个旧版本的 werkzeug。\n解决方法： # 检查并更新 werkzeug 版本 确保你的 Flask 应用依赖的 werkzeug 版本是兼容的。\n编辑 requirements.txt 文件，添加或更新如下内容：\nflask==2.1.3 werkzeug\u0026gt;=2.1.0 根据 Flask 的版本更新 werkzeug，这里假设使用 Flask 2.x 版本。\n重建 Docker 镜像 在更新 requirements.txt 后，重新构建镜像：\ndocker build -t flask-app . 验证更新后的镜像 运行容器：\ndocker run -d -p 5000:5000 flask-app docker logs -f \u0026lt;container-id\u0026gt; 其他排查 # 如果问题依然存在，可以检查 werkzeug 版本：\ndocker run -it flask-app python -c \u0026#34;import werkzeug; print(werkzeug.__version__)\u0026#34; 确认是否是预期的版本。\n如果无法解决，可以直接尝试安装一个具体的兼容版本：\npip install werkzeug==2.1.2 试试这些步骤，看是否能解决问题！\n你的 Dockerfile 结构是正确的，不过可以稍微优化一些细节：\n你可以将 apt-get 安装命令与 Python 包安装合并为一个层，这样可以减少镜像层数，提高构建效率。 在安装 curl 之前，如果是基于 python:3.9-slim 镜像，最好先安装 apt-transport-https，确保 apt 能正常使用。 可以在安装 Python 依赖时，使用 --no-cache-dir，避免缓存安装包，使镜像更小。 以下是改进后的 Dockerfile：\n# 使用官方 Python 3.9-slim 镜像作为基础镜像 FROM python:3.9-slim # 设置工作目录 WORKDIR /app # 安装系统依赖，包含 curl 和 apt-transport-https RUN apt-get update \u0026amp;\u0026amp; apt-get install -y \\ apt-transport-https \\ curl # 复制依赖文件（优先复制 requirements.txt，利用 Docker 缓存机制） COPY requirements.txt /app/ # 安装 Python 依赖，并安装特定版本的 werkzeug RUN pip install --no-cache-dir -r requirements.txt \u0026amp;\u0026amp; \\ pip install werkzeug==1.0.1 # 复制应用代码到工作目录 COPY . /app # 暴露应用端口（可选，通常 Flask 默认运行在 5000 端口） EXPOSE 5000 # 运行 Flask 应用 CMD [\u0026#34;python\u0026#34;, \u0026#34;app.py\u0026#34;] 这样 Dockerfile 会先更新系统包并安装 curl，然后安装 Python 依赖，最后复制代码并启动应用。\n构建镜像：\ndocker build -t flask-app . 运行容器：\ndocker run -d -p 5000:5000 flask-app 然后你就可以进入容器并通过 curl 测试应用了。\n"},{"id":46,"href":"/docs/2024-12-08-mutilpass%E9%83%A8%E7%BD%B2openstack/","title":"2024-12-09 mutilpass部署openstack devstack形式","section":"Docs","content":"在 macOS 上通过 Multipass 部署 OpenStack 测试环境是一个非常方便的方式，尤其是使用 All-in-One 部署方式来简化环境配置。All-in-One 部署将 OpenStack 的所有核心组件（如 Nova、Keystone、Glance、Cinder、Neutron 等）部署在一台虚拟机上，适合用于测试和开发环境。\n使用 Multipass 部署 OpenStack # 1. 准备工作： # 安装 Multipass：确保你已经安装了 Multipass。你可以通过官网（Multipass 官方网站）下载并安装。 安装 Ubuntu 虚拟机：创建一个 Ubuntu 虚拟机来部署 OpenStack。 2. 创建虚拟机： # 使用 Multipass 创建 Ubuntu 虚拟机时，你可以指定虚拟机的资源配置（如 CPU 核数、内存和硬盘空间）。以下是创建虚拟机的命令：\nmultipass launch --name openstack-vm --cpus 4 --mem 8G --disk 40G 22.04 --name openstack-vm：虚拟机的名称。 --cpus 4：分配 4 个 CPU 核心。 --mem 8G：分配 8GB 内存。 --disk 40G：分配 40GB 硬盘空间。 20.04：指定使用 Ubuntu 20.04 LTS 版本。 这将创建一台名为 openstack-vm 的虚拟机，具有 4 核 CPU、8GB 内存和 40GB 硬盘空间。\n3. 连接到虚拟机： # 创建虚拟机后，你可以通过以下命令连接到虚拟机的终端：\nmultipass shell openstack-vm 进入虚拟机后，你可以开始在里面安装 OpenStack。\n4. 在虚拟机中部署 OpenStack： # 有几种方式可以在 Ubuntu 虚拟机中安装 OpenStack：\n使用 DevStack：这是最常见的开发环境部署工具，适合在单一节点（All-in-One）部署 OpenStack。DevStack 会自动配置所有 OpenStack 组件，并且可以通过简单的配置来启动整个环境。 使用 Packstack：这是 Red Hat 开发的脚本工具，适用于 OpenStack 的快速部署。 下面是通过 DevStack 在虚拟机中部署 OpenStack 的简单步骤：\n5. 使用 DevStack 安装 OpenStack # 更新系统：\nsudo apt update sudo apt upgrade -y sudo apt install git -y 克隆 DevStack 仓库：\ngit clone https://opendev.org/openstack/devstack cd devstack 配置 DevStack： 你可以使用默认的配置文件，也可以创建一个自定义配置文件。在 devstack 目录下创建一个配置文件 local.conf，例如：\nvim local.conf 配置内容如下（修改为你自己的用户名和密码）：\n[[local|localrc]] ADMIN_PASSWORD=yourpassword DATABASE_PASSWORD=$ADMIN_PASSWORD RABBIT_PASSWORD=$ADMIN_PASSWORD SERVICE_PASSWORD=$ADMIN_PASSWORD [[local|localrc]] ADMIN_PASSWORD=1234 DATABASE_PASSWORD=1234 RABBIT_PASSWORD=1234 SERVICE_PASSWORD=1234 运行 DevStack 安装 OpenStack： 运行以下命令来安装 OpenStack：\n./stack.sh FORCE=yes ./stack.sh 这个过程会下载和安装所有需要的依赖，并配置 OpenStack 所有组件。安装过程可能需要一些时间。\n你遇到的错误信息表明 stack.sh 脚本在执行过程中遇到了一些问题。根据报错信息，可能是以下几个原因导致的：\n权限问题或目录不存在： 错误消息显示 functions-common 脚本试图访问 /opt/stack/logs/error.log 文件时找不到该目录。这通常是由于目标目录没有创建或当前用户没有足够权限导致的。 缺少必要的目录： OpenStack 的安装脚本会创建一些日志和临时目录。如果这些目录没有正确创建，可能会导致后续操作失败。 解决方案 # 1. 创建缺少的目录 # 尝试手动创建日志目录 /opt/stack/logs，并设置正确的权限：\nsudo mkdir -p /opt/stack/logs sudo chown -R ubuntu:ubuntu /opt/stack 这里假设你是以 ubuntu 用户身份运行的（根据错误信息中的 ubuntu@openstack-vm）。\n2. 确保脚本具有执行权限 # 如果是权限问题，确保脚本具有正确的执行权限：\nchmod +x stack.sh 3. 使用 FORCE=yes 重新运行脚本 # 如果你想绕过当前的错误并强制运行安装脚本，可以使用 FORCE=yes 环境变量来继续执行脚本：\nFORCE=yes ./stack.sh 这会强制脚本继续执行，但需要注意，这种方式可能会跳过一些错误检查。\n4. 查看更详细的日志 # 如果问题依旧没有解决，查看脚本日志的详细信息可以帮助找出根本原因。你可以查看 stack.sh 脚本所在目录中的其他日志文件，通常这些文件会有更多的错误信息。\ncat /opt/stack/logs/error.log 5. 清理之前的安装并重新开始 # 如果前面的步骤都没有解决问题，可以尝试清理之前的安装并重新运行：\n./unstack.sh ./clean.sh 然后再尝试重新运行 stack.sh：\n./stack.sh 6. 检查依赖和配置 # 确保系统已安装所有必要的依赖项，特别是网络相关的工具（如 git、python 等），并且你的配置文件（如 local.conf）没有遗漏或配置错误。\n总结 # 检查并创建缺少的目录 /opt/stack/logs。 确保脚本具有正确的执行权限。 尝试使用 FORCE=yes 重新运行脚本。 查看错误日志获取更多信息。 清理并重新开始安装过程。 这些步骤应该能帮助你解决安装过程中的问题。\n6. 完成安装后访问 OpenStack： # 安装完成后，DevStack 会启动一个默认的 Horizon Web 界面，你可以在浏览器中访问并登录。\n获取虚拟机的 IP 地址：\nmultipass list 在浏览器中访问 Horizon Web 界面： 访问 http://\u0026lt;VM-IP\u0026gt;/dashboard，并使用 DevStack 安装过程中设置的 ADMIN_PASSWORD 登录。\n推荐配置 # CPU：4 核 CPU（可以根据需要调整，2 核也能满足基础体验） 内存：8GB 内存（OpenStack 虽然可以在 4GB 内存上运行，但 8GB 会带来更好的体验，尤其是在启动虚拟机和运行服务时） 硬盘空间：40GB 硬盘（40GB 适合测试环境，如果需要运行多个虚拟机，可能需要更多存储空间） 总结 # 通过 Multipass 和 DevStack，你可以轻松在 macOS 上的 Ubuntu 虚拟机中部署一个测试用的 OpenStack 环境。资源配置方面，4 核 CPU、8GB 内存和 40GB 硬盘是一个合适的基础配置，可以支撑 OpenStack 的基础功能和一些虚拟机的运行。如果需要更多的虚拟机或服务，可以根据需求适当调整资源配置。\n/opt/stack/requirements/upper-constraints.txt zipp admin-openrc.sh 是一个环境配置文件，它包含了 OpenStack 管理员账户的认证信息，通常在安装和部署 OpenStack 时生成。这个文件用于设置环境变量，使得你可以通过 openstack 命令行工具与 OpenStack 进行交互。\n如何使用 admin-openrc.sh 文件： # 下载 admin-openrc.sh 文件\n如果你从 OpenStack 的 Web 控制面板（Horizon）下载了 admin-openrc.sh 文件，你需要在本地环境中使用它来设置命令行环境。\n通常，你可以在 Horizon 控制面板中找到 \u0026ldquo;Download OpenRC file\u0026rdquo; 的选项，选择下载 admin-openrc.sh，这个文件会包含你 OpenStack 管理员账号的认证信息（如用户名、密码等）。\n使用 source 命令加载环境变量\n打开终端并定位到 admin-openrc.sh 文件所在的目录，使用 source 命令加载该文件，这样文件中定义的环境变量就会在当前的 shell 会话中生效。\nsource admin-openrc.sh 执行后，它会要求你输入 OpenStack 管理员账号的密码。输入密码后，环境变量会设置好，你就可以使用 OpenStack 的命令行工具进行操作了。\n验证环境变量设置\n加载环境变量后，可以通过 openstack 命令验证是否设置成功。例如，查看当前项目的信息：\nopenstack project list 如果环境变量设置正确，你应该能够看到当前 OpenStack 环境中的所有项目。\n使用 openstack 命令行工具\n加载了环境变量之后，你就可以开始使用 OpenStack 的命令行工具（如 openstack server create，openstack image list，openstack flavor list 等）来管理你的 OpenStack 实例和资源。\n示例：\n查看所有镜像：\nopenstack image list 查看所有实例：\nopenstack server list 创建一个新的实例：\nopenstack server create --image \u0026lt;image_name\u0026gt; --flavor \u0026lt;flavor_name\u0026gt; --network \u0026lt;network_name\u0026gt; --key-name \u0026lt;key_name\u0026gt; \u0026lt;server_name\u0026gt; 示例步骤：如何使用 admin-openrc.sh 创建实例 # 加载 OpenRC 文件：\nsource admin-openrc.sh 查看可用的镜像：\nopenstack image list 你会看到类似下面的输出：\n+--------------------------------------+------------------+--------+ | ID | Name | Status | +--------------------------------------+------------------+--------+ | 1234abcd-5678-efgh-ijkl-mnopqrstuv | ubuntu-22.04 | active | | abcd1234-5678-efgh-ijkl-mnopqrstuv | centos-7 | active | +--------------------------------------+------------------+--------+ 查看可用的 flavor：\nopenstack flavor list 你会看到如下输出：\n+---------+--------+------+------+ | ID | Name | RAM | Disk | +---------+--------+------+------+ | 1 | m1.small | 2048 | 20 | | 2 | m1.medium | 4096 | 40 | +---------+--------+------+------+ 创建实例：\n假设你想使用 ubuntu-22.04 镜像，m1.small 规格，并连接到名为 private 的网络，命令如下：\nopenstack server create --image ubuntu-22.04 --flavor m1.small --network private --key-name my-key test-instance 这会创建一个名为 test-instance 的实例。\n查看实例状态：\nopenstack server list 你会看到实例的状态变为 ACTIVE，并且可以访问它。\n总结 # admin-openrc.sh 是一个包含 OpenStack 管理员认证信息的文件，加载它之后，你可以使用 OpenStack 的命令行工具来管理资源。 使用 source admin-openrc.sh 加载文件后，你可以创建实例、查看镜像、管理网络等。 加载后，你就可以使用 openstack 命令行工具执行各种操作，例如创建实例、查看网络、配置安全组等。 通过这些步骤，你可以成功使用 OpenStack 的 CLI 创建实例并管理资源。\nopenstack compute service list ubuntu@openstack-vm:~$ openstack compute service list +--------------------------------------+----------------+--------------+----------+---------+-------+----------------------------+ | ID | Binary | Host | Zone | Status | State | Updated At | +--------------------------------------+----------------+--------------+----------+---------+-------+----------------------------+ | 0aab74ff-7591-4f21-9d1e-eafc61e1a76f | nova-scheduler | openstack-vm | internal | enabled | up | 2024-12-08T22:40:48.000000 | | c2a831c6-6a29-4222-aa46-a6241de24d4f | nova-conductor | openstack-vm | internal | enabled | up | 2024-12-08T22:40:58.000000 | | ff2187dc-c994-44a2-92e3-db4be37f1005 | nova-conductor | openstack-vm | internal | enabled | up | 2024-12-08T22:41:21.000000 | | 6d0341c1-bacb-409e-8628-61e870bb83ac | nova-compute | openstack-vm | nova | enabled | up | 2024-12-08T22:41:16.000000 | +--------------------------------------+----------------+--------------+----------+---------+-------+----------------------------+ ubuntu@openstack-vm:~$ 在 DevStack 环境中重启 Nova 服务有几种方式：\n使用 systemctl 命令（如果是 systemd 管理的服务）： # 重启所有 nova 相关服务 sudo systemctl restart devstack@n-* # 重启具体服务 sudo systemctl restart devstack@n-api.service sudo systemctl restart devstack@n-cpu.service sudo systemctl restart devstack@n-cond.service sudo systemctl restart devstack@n-sch.service 使用 DevStack 自带的脚本： # 进入 devstack 目录 cd /opt/stack/devstack # 重启所有服务 ./unstack.sh ./stack.sh # 或者使用 rejoin-stack.sh ./rejoin-stack.sh 使用 screen 会话方式： # 查看所有 screen 会话 screen -ls # 重新连接到 nova 相关的 screen screen -r stack # 按 Ctrl-A + \u0026#34; 切换到需要的窗口（n-api, n-cpu 等） # 按 Ctrl-C 停止服务 # 重新运行服务命令 检查服务状态： # 查看 nova 服务状态 openstack compute service list # 查看具体服务日志 sudo tail -f /var/log/nova/nova-api.log sudo tail -f /var/log/nova/nova-compute.log 如果遇到问题：\n检查日志文件： # 查看系统日志 sudo journalctl -u devstack@n-* # 查看 nova 日志 cd /var/log/nova/ tail -f *.log 检查服务配置： # 检查 nova 配置文件 cat /etc/nova/nova.conf # 检查服务状态 openstack compute service list nova service-list 常见问题处理： # 清理数据库（谨慎使用） mysql -u root -p \u0026gt; use nova; \u0026gt; show tables; \u0026gt; truncate table instances; # 重置 nova 数据库 cd /opt/stack/devstack ./unstack.sh mysql -u root -p -e \u0026#34;drop database nova;\u0026#34; ./stack.sh 注意事项：\n重启前最好先检查服务状态 建议查看日志确认是否有错误 如果单个服务重启失败，可能需要重启相关依赖服务 重要数据操作前做好备份 使用 unstack.sh 关闭后再用 stack.sh 重启，实例是否还在取决于几个因素：\n默认情况下： 实例的数据会保留在数据库中 实例的状态可能会变为 SHUTOFF 或 ERROR 重启后需要手动启动实例 检查和恢复实例： # 查看实例状态 openstack server list --all # 启动处于关闭状态的实例 openstack server start 实例ID # 如果实例状态异常，可以尝试重置状态 openstack server reset-state --active 实例ID 如果想完全清除环境： # 清理所有数据（包括实例） cd /opt/stack/devstack ./clean.sh # 或者清理数据库 mysql -u root -p \u0026gt; drop database nova; \u0026gt; drop database neutron; 注意事项：\n建议在关闭前备份重要数据 如果实例很重要，最好在关闭前做快照 重启后检查网络和存储是否正常挂载 可能需要重新配置安全组和浮动IP 最佳实践：\n# 关闭前 openstack server list --all # 记录实例信息 openstack server stop 实例ID # 优雅关闭实例 # 关闭 devstack ./unstack.sh # 重启后 ./stack.sh openstack server start 实例ID # 启动实例 "},{"id":47,"href":"/docs/2024-12-09-openstack-ssh%E8%BF%9E%E6%8E%A5/","title":"2024-12-09 openstack ssh连接","section":"Docs","content":" DevStack 网络 # DevStack 体验的一个重要部分是默认为创建的来宾提供网络连接。这可能不是您特定测试环境的最佳选择，因此本文档尽力解释发生了什么。\n默认值 # 如果您不指定任何配置，您将获得以下内容：\nneutron（包括带有 openvswitch 的 l3） 每个 openstack 项目的私有项目网络 浮动 IP 范围为 172.24.4.0/24，网关为 172.24.4.1 演示项目在从 10.0.0.0/22 范围分配的子网上配置了固定 IP 由 neutron 控制的所有网络的接口br-ex（不连接到任何物理接口）。 根据主机的 resolv.conf 为访客提供 DNS 解析 允许创建的访客路由出去的 IP masq 规则 这将创建一个与单个主机隔离的环境。访客可以访问外部网络以获取软件包更新。Tempest 测试将在此环境中进行。\n笔记\n默认情况下，所有 OpenStack 环境都有安全组规则，阻止所有发往客户的入站数据包。如果您希望能够 ssh/ping 您创建的客户，则应运行以下命令。\nubuntu@openstack-vm:~$ openstack security group list +--------------------------------------+---------+------------------------+----------------------------------+------+ | ID | Name | Description | Project | Tags | +--------------------------------------+---------+------------------------+----------------------------------+------+ | 109ac0cd-daf6-481c-9594-867dbbdb129b | default | Default security group | 14278ce7bc604abd91e85b82e78ea45e | [] | | 73b1a108-a0db-49a3-973f-dc91c030d4dd | default | Default security group | 6175cac8870541d1973a0f1010981180 | [] | | 75e4b266-40bf-4bd8-a7d5-b2f0b0d6e84d | default | Default security group | a942dd11388440a6b8b9922fb405ad55 | [] | | 9f07480a-fded-4172-852b-c997653324dc | t1 | | 6175cac8870541d1973a0f1010981180 | [] | +--------------------------------------+---------+------------------------+----------------------------------+------+ ubuntu@openstack-vm:~$ 你可以使用以下命令删除指定的安全组：\nopenstack security group delete 109ac0cd-daf6-481c-9594-867dbbdb129b openstack security group delete 75e4b266-40bf-4bd8-a7d5-b2f0b0d6e84d 确保该安全组没有被任何实例使用，否则删除可能会失败。如果安全组被实例使用，你需要先将其从实例中移除，或者选择一个新的安全组。\n如果你遇到问题，随时告诉我！\nopenstack security group rule create --proto icmp --dst-port 0 default openstack security group rule create --proto tcp --dst-port 22 default openstack security group rule create --proto icmp --dst-port 0 73b1a108-a0db-49a3-973f-dc91c030d4dd openstack security group rule create --proto tcp --dst-port 22 73b1a108-a0db-49a3-973f-dc91c030d4dd openstack security group rule create --proto icmp --dst-port 0 9f07480a-fded-4172-852b-c997653324dc openstack security group rule create --proto icmp --dst-port 22 9f07480a-fded-4172-852b-c997653324dc 本地可访问的访客 # 如果您想让您的客人可以从网络上的其他机器访问，我们必须连接br-ex到物理接口。\n专用客户机界面 # 如果您的 devstack 服务器上有 2 个或更多接口，您可以分配一个接口给 neutron 进行全面管理。这不应该 与您用于 ssh 进入 devstack 服务器本身的接口相同。\n这是通过设置PUBLIC_INTERFACE属性来完成的。\n[[local|localrc]] PUBLIC_INTERFACE=eth1 这会将来自您的客户机的所有第 2 层流量放到主网络上。在此模式下运行时，ip masq 规则不会添加到您的 devstack 中，您负责确保本地网络上的路由工作。\n共享客户机接口 # 警告\n这不是推荐的配置。由于 OVS 和桥接之间的相互作用，如果您在启用网络的情况下重新启动设备，则可能会丢失系统的网络连接。\n如果您需要访客在网络上可访问，但只有 1 个接口（使用 NUC 之类的设备），您可以共享一个网络。但为了实现这一点，您需要手动设置大量地址，并确保所有地址都完全正确。\n[[local|localrc]] PUBLIC_INTERFACE=eth0 HOST_IP=10.42.0.52 FLOATING_RANGE=10.42.0.0/24 PUBLIC_NETWORK_GATEWAY=10.42.0.1 Q_FLOATING_ALLOCATION_POOL=start=10.42.0.250,end=10.42.0.254 为了使此方案有效，浮动 IP 网络必须与您服务器上的默认网络相匹配。这会破坏 HOST_IP 检测，因为我们默认排除浮动范围，因此您必须手动指定。\n这PUBLIC_NETWORK_GATEWAY是服务器通常用来脱离网络的网关。Q_FLOATING_ALLOCATION_POOL控制将要分配的浮动 IP 范围。由于我们正在共享您现有的网络，因此您需要为其分配本地 dhcp 服务器未分配的片段。否则，您很容易出现 IP 地址冲突，并对您的本地网络造成严重破坏。\n私有网络寻址 # IPV4_ADDRS_SAFE_TO_USE 私有网络地址由和变量控制IPV6_ADDRS_SAFE_TO_USE。这允许用户指定一个要使用的安全内部 IP 变量，无论是否使用子网池，都会引用该变量。\n对于IPv4，FIXED_RANGE和SUBNETPOOL_PREFIX_V4将直接默认为的值IPV4_ADDRS_SAFE_TO_USE。\n对于 IPv6，FIXED_RANGE_V6将默认为 的值的前 /64 IPV6_ADDRS_SAFE_TO_USE。如果IPV6_ADDRS_SAFE_TO_USE为 /64 或更小， FIXED_RANGE_V6将直接使用该值。 SUBNETPOOL_PREFIX_V6将直接默认为 的值 IPV6_ADDRS_SAFE_TO_USE。\nSSH 访问实例 # 为了验证连接性，您可以使用网络创建一个实例 $PRIVATE_NETWORK_NAME（默认值：）private，使用$PUBLIC_NETWORK_NAME网络创建一个浮动 IP（默认值：）public，然后将此浮动 IP 附加到该实例：\nopenstack keypair create --public-key ~/.ssh/id_rsa.pub test-keypair openstack server create --network private --key-name test-keypair ... test-server fip_id=$(openstack floating ip create public -f value -c id) openstack server add floating ip test-server ${fip_id} 完成后，请确保已为用于实例的安全组启用 SSH 和 ICMP（ping）访问。您可以创建自定义安全组并在创建实例时指定它，也可以在创建后添加它，或者您可以修改default每个项目默认创建的安全组。让我们做后者：\nopenstack security group rule create --proto icmp --dst-port 0 default openstack security group rule create --proto tcp --dst-port 22 default 最后，通过 SSH 进入实例。如果你使用了默认上传的 Cirros 实例，那么你可以运行以下命令：\nopenstack server ssh test-server -- -l cirros 这将使用cirros您在创建实例时配置的用户和密钥对进行连接。\n远程 SSH 访问实例 # 您还可以从其他主机通过 SSH 连接到 DevStack 主机上创建的实例。如果您正在现有云上的 VM 中部署 DevStack 并希望在本地计算机上进行开发，这将非常有用。有几种方法可以做到这一点。\n配置实例可供本地访问\n最明显的方法是将客户机配置为本地可访问，如上所述。这样做的好处是不需要在客户端上做进一步的工作。但是，这更复杂，需要云的支持或一些不明智的解决方法。\n使用 DevStack 主机作为跳转主机\n您可以选择使用 DevStack 主机作为跳转主机。要通过这种方式通过 SSH 连接到实例，请将标准-J选项传递给/ 命令。例如：openstack ssh``ssh\nopenstack server ssh test-server -- -l cirros -J username@devstack-host （其中test-server是现有实例的名称，如前所述 ，和username是devstack-host您的 DevStack 主机的用户名和主机名）。\n这也可以通过您的文件进行配置~/.ssh/config，因此非常轻松。但是，它仅允许 SSH 访问。如果您想要访问实例上的 Web 应用程序，则需要配置 SSH 隧道并使用选项转发选定端口-L。例如，要转发 HTTP 流量：\nopenstack server ssh test-server -- -l cirros -L 8080:username@devstack-host:80 （其中test-server是现有实例的名称，如前所述 ，和username是devstack-host您的 DevStack 主机的用户名和主机名）。\n你可以想象，这很快就会失控，特别是对于具有多个端口的更复杂的客户应用程序而言。\n使用代理或 VPN 工具\n$PUBLIC_NETWORK_NAME您可以使用代理或 VPN 工具为由 (默认值：public) 定义的网络浮动 IP 地址范围($FLOATING_RANGE默认值：172.24.4.0/24) 启用隧道。有许多此类工具可用于执行此操作。例如，我们可以使用一个名为shuttle的实用实用程序。要使用 启用隧道shuttle，首先确保您已允许 SSH 和 HTTP(S) 流量到您的 DevStack 主机。允许 HTTP(S) 流量是必要的，这样您就可以远程使用 OpenStack API。如何执行此操作取决于您的 DevStack 主机的运行位置。完成后，sshuttle在您的本地主机上安装：\nsudo apt-get install sshuttle || yum install sshuttle 最后，sshuttle使用浮动 IP 地址范围在本地主机上启动。例如，假设您使用 的默认值 $FLOATING_RANGE，您可以执行以下操作：\nsshuttle -r username@devstack-host 172.24.4.0/24 （其中username和devstack-host是您的 DevStack 主机的用户名和主机名）。\n您现在应该能够创建一个实例并通过 SSH 进入它：\nopenstack server ssh test-server -- -l cirros （其中test-server是现有实例的名称， 如前所述）\n你可以使用 openstack server start 命令来启动一个已经存在的虚拟机实例。\n根据你提供的实例列表，虚拟机名称为 t2，它的状态是 SHUTOFF。你可以通过以下命令来启动它：\nopenstack server start t2 如果你想启动多个实例，可以用空格分隔多个实例名称或 ID。例如：\nopenstack server start t2 another_instance 另外，如果你想查看实例启动状态，可以使用：\nopenstack server show t2 如果有任何问题或需要进一步的帮助，随时告诉我！\n要在外部连接 OpenStack 创建的实例，需要完成几个网络配置步骤：\n确保安全组配置正确： # 创建安全组规则，允许 SSH 访问（如果是 Linux 实例） openstack security group rule create --protocol tcp --dst-port 22:22 --remote-ip 0.0.0.0/0 default # 如果是 Windows 实例，需要开放 RDP 端口 openstack security group rule create --protocol tcp --dst-port 3389:3389 --remote-ip 0.0.0.0/0 default 分配浮动 IP： # 创建浮动 IP openstack floating ip create external-network # 将浮动 IP 绑定到实例 openstack server add floating ip INSTANCE_NAME FLOATING_IP 检查网络配置： # 查看实例的网络详情 openstack server show INSTANCE_NAME # 查看网络列表 openstack network list # 查看子网列表 openstack subnet list 主要配置步骤：\n网络配置： 确保有外部网络（external network） 创建路由器连接内部网络和外部网络 配置正确的 CIDR 和网关 路由器配置： # 创建路由器 openstack router create ROUTER_NAME # 设置路由器的外部网关 openstack router set ROUTER_NAME --external-gateway external-network # 将路由器连接到内部网络 openstack router add subnet ROUTER_NAME INTERNAL_SUBNET 验证连接： # Linux 实例 ssh -i private_key.pem user@FLOATING_IP # Windows 实例 # 使用远程桌面连接 FLOATING_IP 常见问题检查：\n确保安全组规则正确配置 验证网络拓扑是否正确 检查路由器配置 确认浮动 IP 是否正确绑定 检查实例的网络接口配置 如果仍然无法连接：\n检查实例内部的网络配置 验证防火墙规则 查看 OpenStack 网络服务日志 确认 IP 转发是否启用 检查 DNS 配置 浮动IP（Floating IP）可以理解为公网IP，它允许OpenStack内部的实例能够被外网访问。让我用一个简单的类比来解释：\n想象一个小区（OpenStack环境）：\n内部网络（Private IP）就像小区内部的房间号，只能在小区内部使用 浮动IP（Floating IP）就像小区的对外门牌号，外部人员可以通过这个找到你 具体工作原理：\n默认情况下，OpenStack实例只有内部IP： 实例A: 192.168.1.10 (内部IP，只能在OpenStack内部访问) 分配浮动IP后： 实例A: - 内部IP: 192.168.1.10 - 浮动IP: 203.0.113.101 (可以从外网访问) 配置步骤：\n创建浮动IP： # 从外部网络池中创建一个浮动IP openstack floating ip create external-network 查看可用的浮动IP： openstack floating ip list 分配给实例： # 方式1：通过命令行 openstack server add floating ip 实例名称 浮动IP地址 # 方式2：通过Web界面 # Instances -\u0026gt; Actions -\u0026gt; Associate Floating IP 使用场景：\n需要从外网SSH连接到实例 实例需要提供对外服务（如网站） 实例需要访问外网资源 注意事项：\n浮动IP是有限资源，用完后需要及时释放 一个实例可以有多个浮动IP 浮动IP可以在实例之间迁移 需要配置正确的安全组规则才能访问 "},{"id":48,"href":"/docs/2024-12-10-docker-registrry/","title":"2024-12-10 docker registrry","section":"Docs","content":"在 Ubuntu 上搭建一个轻量级的 Docker 镜像仓库，可以使用 Harbor 或者 Docker Registry。如果你想要一个简单、轻量的仓库，Docker Registry 是一个非常合适的选择，因为它是 Docker 官方提供的。\n步骤 1：安装 Docker # 确保你的系统上已安装 Docker，如果还没有，可以通过以下命令安装：\nsudo apt update sudo apt install -y docker.io 步骤 2：使用 Docker Registry 搭建镜像仓库 # Docker Registry 是 Docker 官方提供的开源镜像仓库，使用它可以轻松搭建私有镜像仓库。\n拉取 Docker Registry 镜像： Docker 官方提供了一个 Docker Registry 镜像，运行这个镜像就可以搭建一个私有仓库。\nsudo docker pull registry:2 运行 Docker Registry： 启动一个 Docker Registry 实例，默认情况下会使用端口 5000。\nsudo docker run -d -p 5000:5000 --name registry registry:2 这条命令将会拉取并运行一个 registry 容器，并将本地的 5000 端口映射到容器的 5000 端口。\n验证镜像仓库是否正常运行： 在浏览器中访问 http://\u0026lt;your-server-ip\u0026gt;:5000/v2/，如果一切顺利，你会看到一个响应，表示仓库正在运行。\n步骤 3：配置 Docker 客户端连接私有仓库 # 标记镜像： 假设你有一个本地的镜像 my-app，你可以将其标记为指向私有仓库：\nsudo docker tag my-app:latest localhost:5000/my-app:latest sudo docker tag flask-app:latest localhost:5000/547475331/flasktest:latest 推送镜像到私有仓库： 你可以将标记过的镜像推送到你刚刚搭建的私有仓库：\nsudo docker push localhost:5000/my-app:latest sudo docker push localhost:5000/547475331/flasktest:latest 从私有仓库拉取镜像： 如果你想从私有仓库拉取镜像，只需运行：\nsudo docker pull localhost:5000/my-app:latest sudo docker pull localhost:5000/547475331/flasktest:latest 可选：启用 HTTPS（生产环境） # 如果你准备将这个仓库用于生产环境，建议启用 HTTPS 来加密数据传输。你可以使用自签名证书或者从认证机构（如 Let\u0026rsquo;s Encrypt）获得证书。\n生成自签名证书：\nmkdir -p /certs openssl req \\ -newkey rsa:4096 -nodes -keyout /certs/domain.key \\ -x509 -out /certs/domain.crt 更新 Docker Registry 启动命令： 修改启动命令，添加证书和密钥的路径。\nsudo docker run -d -p 5000:5000 --name registry \\ -v /certs:/certs \\ -e REGISTRY_HTTP_SECRET=\u0026lt;your-secret\u0026gt; \\ -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt \\ -e REGISTRY_HTTP_TLS_KEY=/certs/domain.key \\ registry:2 通过这种方式，你就搭建了一个简单、轻量的 Docker 镜像仓库。如果你希望更高效、易用的管理界面，可以考虑使用 Harbor，它有更丰富的功能，但也需要更多的资源来支持。\n是的，Docker Registry 可以配置持久化存储。默认情况下，Docker Registry 存储镜像数据在容器的本地文件系统中，但为了持久化存储和确保镜像数据不会在容器重启或删除时丢失，你需要将存储目录挂载到宿主机的持久存储上。\nDocker Registry 持久化存储配置 # 要让 Docker Registry 使用持久化存储，通常做法是将宿主机的一个目录挂载到 Registry 容器的 /var/lib/registry 路径上，这样存储的数据将保存在宿主机的文件系统中，不会随着容器的删除而丢失。\n1. 使用宿主机挂载目录： # docker run -d -p 5000:5000 --name registry \\ -v /path/on/host:/var/lib/registry \\ registry:2 docker run -d -p 5000:5000 --name registry \\ -v /registry-data:/var/lib/registry \\ registry:2 docker run -d -p 5000:5000 --name registry \\ -v /registry-data:/var/lib/registry \\ --restart always \\ registry:2 /path/on/host: 这个路径是宿主机上的目录，会被挂载到 Docker Registry 容器的 /var/lib/registry 目录。你需要确保这个目录有足够的磁盘空间，以存储镜像。 2. 使用云存储或网络存储（如 NFS、AWS S3 等）： # 如果你需要在多个主机之间共享 Registry 存储，或者希望将镜像存储到云存储中，可以使用支持的后端存储。\nDocker Registry 支持多种持久化存储后端，包括：\nAmazon S3 Google Cloud Storage Microsoft Azure Storage OpenStack Swift NFS 例如，使用 S3 存储：\ndocker run -d -p 5000:5000 --name registry \\ -e REGISTRY_STORAGE=s3 \\ -e REGISTRY_STORAGE_S3_ACCESSKEY=\u0026lt;access-key\u0026gt; \\ -e REGISTRY_STORAGE_S3_SECRETKEY=\u0026lt;secret-key\u0026gt; \\ -e REGISTRY_STORAGE_S3_BUCKET=\u0026lt;bucket-name\u0026gt; \\ registry:2 在此配置中，镜像数据将存储在指定的 S3 桶中。\n3. 配置持久化数据库： # 如果你需要配置 Docker Registry 以持久化配置信息和身份验证等数据，可以考虑使用外部数据库（如 MySQL、PostgreSQL）来存储这些信息。\n总结： # 默认情况下，Docker Registry 的存储是临时的，仅在容器生命周期内有效。 要启用持久化存储，可以通过挂载宿主机目录或使用外部存储服务（如 AWS S3）来持久保存镜像数据。 "},{"id":49,"href":"/docs/2024-2-22-k8s%E6%9E%B6%E6%9E%84%E5%B8%88%E9%9D%A2%E8%AF%95%E5%A4%A7%E5%85%A8/","title":"2024-2-22 k8s架构师面试大全","section":"Docs","content":"一、Kubernetes 基础知识面试题10 道面试题\n1、什么是 Kubernetes？\nKubernetes 是一个开源容器管理工具，负责容器部署，容器扩缩容以及负载平衡。它提供了出色的社区，并与所有云提供商合作。因此，我们可以说 Kubernetes 不是一个容器化平台，而是一个多容器管理解决方案。 # 2、 Kubernetes 与 docker 什么关系？\nDocker 提供容器的生命周期管理，Docker 镜像构建运行时容器。但是，由于这些单独的容器必须通信，因此使用 Kubernetes。因此，我们说 Docker 构建容器，这些容器通过 Kubernetes 相互通信。因此，可以使用 Kubernetes 手动关联和编排在多个主机上运行的容器。 # 3、Kubernetes 与 Docker Swarm 的区别？\nDocker Swarm 和 Kubernetes 都可以用于类似目的。它们都是容器编排工具。 # 4、在主机和容器上部署应用程序有什么区别？\n上图左侧架构表示在主机上部署应用程序。因此，这种架构将具有操作系统，然后操作系统将具有内核，该内核将在应用程序所需的操作系统上安装各种库。因此，在这种框架中，你可以拥有 n 个应用程序，并且所有应用程序将共享该操作系统中存在的库。 # 上图右侧架构是容器中部署应用程序。这种架构将有一个内核，这是唯一一个在所有应用程序之间唯一共同的东西。各个块基本上是容器化的，并且这些块与其他应用程序隔离。因此，应用程序具有与系统其余部分隔离的必要库和二进制文件，并且不能被任何其他应用程序侵占。 # 5、Kubernetes 如何简化容器化部署？\n跨主机的容器都需要相互通信。因此，要做到这一点，你需要一些能够负载平衡，扩展和监控容器的东西。由于 Kubernetes 与云无关并且可以在任何公共/私有提供商上运行，因此可以简化容器化部署程序。 # 6、什么是 kubectl？\nKubectl 是一个平台，可以使用该平台将命令传递给集群。因此，它基本上为CLI 提供了针对 Kubernetes 集群运行命令的方法，以及创建和管理 Kubernetes组件的各种方法。 # 7、什么是 kubelet？\n这是一个代理服务，它在每个节点上运行，并使从服务器与主服务器通信。因此，Kubelet 处理 PodSpec 中提供给它的容器的描述，并确保 PodSpec 中描述的容器运行正常。可以创建 pod、删除 pod。 # 8、k8s 有哪些组件？\n1、etcd 保存了整个集群的状态； # 2、apiserver 提供了资源操作的唯一入口，并提供认证、授权、访问控制、API 注册和发现等机制； # 3、controller manager 负责维护集群的状态，比如故障检测、自动扩展、滚动更新等； # 4、scheduler 负责资源的调度，按照预定的调度策略将 Pod 调度到相应的机器上； # 5、kubelet 负责维护容器的生命周期，同时也负责 Volume（CVI）和网络（CNI）的管理； # 6、Container runtime 负责镜像管理以及 Pod 和容器的真正运行（CRI）； # 7、kube-proxy 负责为 Service 提供 cluster 内部的服务发现和负载均衡。 # 9、Pod SVC Node Container 之间如何相互访问：\n1、同 Pod 内的容器：同一个 Pod 的容器共享同一个网络命名空间可以直接进行通讯 # 2、同 Node 内不同 Pod 的容器：多个 Pod 都关联在同一个 Docker0 网桥上，通过 docker0 网桥完成相互通讯。 # 3、不同 Node 内 Pod 的容器：不同 Node 上的 docker0 可能会相同，PodIP 和 docker0 是同网段的，所以需要将 PodIP 和 NodeIP 进行关联且保障唯 # 4，不同 Pod 之间的数据通过物理机的端口进行转发即可完成通讯。 # 二、Kubernetes 架构面试题10 道面试题\n1、kubernetes 控制节点组件有哪些？\n**Kubernetes 控制节点组件：**kube-controller-manager，kube-apiserver，kube-scheduler、Kubernetes # **工作节点组件：**kubelet 和 kube-proxy # 2、谈谈你对 kube-proxy 的理解？\nkube-proxy 是 Kubernetes 的核心组件，部署在每个 Node 节点上，它是实现Kubernetes Service 的通信与负载均衡机制的重要组件; kube-proxy 负责为 Pod 创建代理服务，从 apiserver 获取所有 server 信息，并根据 server 信息创建代理服务，实现server 到 Pod 的请求路由和转发，从而实现 K8s 层级的虚拟转发网络。 # 3、kube-apiserver 和 kube-scheduler 的作用是什么？\nkube -apiserver 遵循横向扩展架构，是主节点控制平面的前端。将公开Kubernetes 主节点组件的所有 API，并负责在 Kubernetes 节点和 Kubernetes主组件之间建立通信。 # kube-scheduler 是调度器，负责根据资源需求选择最合适的节点来运行未调度的 pod，并跟踪资源利用率。它确保不在资源已满的节点上调度 pod。 # 4、你能简要介绍一下 Kubernetes 控制管理器吗？\nController Manager 作为集群内部的管理控制中心，负责集群内的 Node、Pod 副本、服务端点（Endpoint）、命名空间（Namespace）、服务账号（ServiceAccount）、资源定额（ResourceQuota）的管理，当某个 Node 意外宕机时，Controller Manager 会及时发现并执行自动化修复流程，确保集群始终处于预期的工作状态。 # 5、Kubernetes 如何简化容器化部署？\n跨主机的容器都需要相互通信。因此，要做到这一点，你需要一些能够负载平衡，扩展和监控容器的东西。由于 Kubernetes 与云无关并且可以在任何公共/私有提供商上运行，因此可以简化容器化部署程序。 # 6、什么是 ETCD?\nEtcd 是用 Go 编程语言编写的，是一个分布式键值存储，用于协调分布式工作。因此，Etcd 存储 Kubernetes 集群的配置数据，表示在任何给定时间点的集群状态。 # 7、什么是 Ingress，它是如何工作的？\nIngress 网络是一组规则，充当 Kubernetes 集群的入口点。这允许入站连接，可以将其配置为通过可访问的 URL，负载平衡流量或通过提供基于名称的虚拟主机从外部提供服务。因此，Ingress 是一个 API 对象，通常通过 HTTP 管理集群中服务的外部访问，是暴露服务的最有效方式。 # 8、什么是 Headless Service？\nHeadless Service 类似于“普通”服务，但没有群集 IP。此服务使您可以直接访问pod，而无需通过代理访问它。 # 2.1.9 什么是集群联邦\n在联邦集群的帮助下，可以将多个 Kubernetes 集群作为单个集群进行管理。因此，您可以在数据中心/云中创建多个 Kubernetes 集群，并使用联邦来在一个位置控制/管理它们。 # 9、 k8s 提供几种服务发现\n两种服务发现：\n1、环境变量：当创建一个 Pod 的时候，kubelet 会在该 Pod 中注入集群内所有Service 的相关环境变量。 # 2、DNS：可以通过 cluster add-on 的方式轻松的创建 CoreDNS 来对集群内的Service 进行服务发现。 # 以上两种方式，一个是基于 TCP，DNS 基于 UDP，它们都是建立在四层协议之上。 # 10、Pod 中的应用共享几种资源\n1、PID 命名空间：Pod 中的不同应用程序可以看到其他应用程序的进程 ID。 # 2、网络命名空间：Pod 中的多个容器能够访问同一个 IP 和端口范围。 # 3、IPC 命名空间：Pod 中的多个容器能够使用 SystemV IPC 或 POSIX 消息队列进行通信。 # 4、UTS 命名空间：Pod 中的多个容器共享一个主机名。 # 5、Volumes（共享存储卷）：Pod 中的各个容器可以访问在 Pod 级别定义的Volumes。 # 三、k8s 使用场景面试题10 道面试题\n场景 1：\n假设一家基于单一架构的公司处理众多产品。现在，随着公司在当今的扩展行业的扩展，他们的单一架构开始引发问题。 # 您如何看待公司从单一服务转向微服务并部署其服务容器？ # 解：\n由于公司的目标是从单一应用程序转向微服务，它们最终可以逐个构建，并行构建，只需在后台切换配置。然后他们可以将这些内置微服务放在 Kubernetes 平台上。因此他们可以从一次或两次迁移服务开始，并监控它们以确保一切运行稳定。一旦他们觉得一切顺利，他们就可以将其余的应用程序迁移到他们的 Kubernetes 集群中。 # 场景 2：\n考虑一家拥有分布式系统的跨国公司，拥有大量数据中心，虚拟机和许多从事各种任务的员工。 # 您认为这样 的公司如何以与 Kubernetes 一致的方式管理所有任务？ # 解：\n正如我们所有人都知道 IT 部门推出了数千个容器，其任务在分布式系统中遍布全球众多节点。在这种情况下，公司可以使用能够为基于云的应用程序提供敏捷性，横向扩展功能和DevOps 实践的东西。因此，该公司可以使用 Kubernetes 来定制他们的调度架构并支持多种容器格式。这使得容器任务之间的亲和性成为可能，从而提供更高的效率，并为各种容器网络解决方案和容器存储提供广泛支持。 # 场景 3：\n考虑一种情况，即公司希望通过维持最低成本来提高其效率和技术运营速度。您认为公司将如何实现这一目标？ # 解：\n公司可以通过构建 CI/CD 管道来实现 DevOps 方法，但是这里可能出现的一个问题是配置可能需要一段时间才能启动并运行。因此，在实施 CI/CD 管道之后，公司的下一步应该是在云环境中工作。一旦他们开始处理云环境，他们就可以在集群上安排容器，并可以在 Kubernetes 的帮助下进行协调。这种方法将有助于公司缩短部署时间，并在各种环境中加快速度。 # 场景 4：\n假设一家公司想要修改它的部署方法，并希望建立一个更具可扩展性和响应性的平台。 # 您如何看待这家公司能够实现这一目标以满足客户需求？ # 解：\n为了给数百万客户提供他们期望的数字体验，公司需要一个可扩展且响应迅速的平台，以便他们能够快速地将数据发送到客户网站。现在，要做到这一点，公司应该从他们的私有数据中心（如果他们使用任何）转移到任何云环境，如 AWS。不仅如此，他们还应该实现微服务架构，以便他们可以开始使用 Docker 容器。一旦他们准备好基础框架，他们就可以开始使用最好的编排平台，即 Kubernetes。这将使团队能够自主地构建应用程序并快速交付它们。 # 场景 5：\n考虑一家拥有非常分散的系统的跨国公司，期待解决整体代码库问题。 # 您认为公司如何解决他们的问题？ # 解\n那么，为了解决这个问题，我们可以将他们的单片代码库转移到微服务设计，然后每个微服务都可以被视为一个容器。因此，所有这些容器都可以在 Kubernetes 的帮助下进行部署和协调。 # 场景 6：\n我们所有人都知道，从单片到微服务的转变解决了开发方面的问题，但却增加了部署方面的问题。 # 公司如何解决部署方面的问题？ # 解\n团队可以试验容器编排平台，例如 Kubernetes，并在数据中心运行。因此，通过这种方式，公司可以生成模板化应用程序，在五分钟内部署它，并在此时将实际实例集中在暂存环境中。这种 Kubernetes 项目将有数十个并行运行的微服务，以提高生产率，即使节点出现故障，也可以立即重新安排，而不会影响性能。 # 场景 7：\n假设一家公司希望通过采用新技术来优化其工作负载的分配。 # 公司如何有效地实现这种资源分配？ # 解\n这个问题的解决方案就是 Kubernetes。Kubernetes 确保资源得到有效优化，并且只使用特定应用程序所需的那些资源。因此，通过使用最佳容器编排工具，公司可以有效地实现资源分配 # 场景 8：\n考虑一家拼车公司希望通过同时扩展其平台来增加服务器数量。 # 您认为公司如何处理服务器及其安装？ # 解\n公司可以采用集装箱化的概念。一旦他们将所有应用程序部署到容器中，他们就可以使用 Kubernetes 进行编排，并使用像 Prometheus 这样的容器监视工具来监视容器中的操作。因此，利用容器的这种使用，在数据中心中为它们提供更好的容量规划，因为它们现在将受到更少的限制，因为服务和它们运行的硬件之间存在抽象。 # 场景 9：\n考虑一种情况，公司希望向具有各种环境的客户提供所有必需的分发。您认为他们如何以动态的方式实现这一关键目标？ # 解\n该公司可以使用 Docker 环境，组建一个横截面团队，使用 Kubernetes 构建 Web应用程序。这种框架将帮助公司实现在最短的时间内将所需产品投入生产的目标。因此，在这样的机器运行的情况下，公司可以向所有具有各种环境的客户发放电子邮件。 # 场景 10：\n假设公司希望在不同的云基础架构上运行各种工作负载，从裸机到公共云。 # 公司将如何在不同界面的存在下实现这一目标？ # 解\n该公司可以将其基础设施分解为微服务，然后采用 Kubernetes。这将使公司在不同的云基础架构上运行各种工作负载。 # 四、k8s 真实面试场景-面试题总结12 道面试题\n1、deployment 创建 pod 流程？\n1）、kubectl 提交创建 pod 命令,api 响应命令,通过一系列认证授权,把 pod 数据存储到etcd,创建 deployment 资源并初始化. # 2）、controller 通过 list-watch 机制,监测发现新的 deployment,将该资源加入到内部工作队列,发现该资源没有关联的 pod 和 replicaset,启用 deployment controller 创建replicaset 资源,再启用 replicaset controller 创建 pod. # 3）、所有 controller 正常后.将 deployment,replicaset,pod 资源更新存储到etcd. # 4）、scheduler 通过 list-watch 机制,监测发现新的 pod,经过主机过滤主机打分规则,将pod 绑定(binding)到合适的主机. # 5）、将绑定结果存储到 etcd # 6）、kubelet 每隔 20s(可以自定义)向 kube-apiserver 通过 NodeName 获取自身Node 上所要运行的 pod 清单.通过与自己的内部缓存进行比较,新增加 pod. # 7）、启动 pod 启动容器 # 2、你知道 HPA 吗？HPA 有什么缺点？\nhpa 是 k8s 的自动扩缩容策略。v1 版可支持基于 cpu 的扩缩容，v2 版可支持基于 cpu、内存、自定义指标的扩容。默认情况下，pod 是一台台扩容的，所以类似于 微博热搜之类的 突然间 访问量剧增的情况下就显有些无力。所以需要修改 k8s 的调度规则。如果波动量比较大的服务容易造成业务抖动。主要是很被动，无法进行提前被容。 # HPA 可针对 cpu 和内存自定义参数弹性扩容，有 5 分钟的安全时间，不适合访问量波动大的业务。 # 3、蓝绿发布和灰度发布 k8s 怎么实现的？\n蓝绿发布通过 deployment 部署 pod，改变 service 或者 ingress 切换流量可以实现灰度发布通过 Ingress Controller 或者 istio 可以实现 # 4、如果一个 pod 创建过程中一直处于 pending 状态，你的处理思路是什么？\n1）通过 describe 查看 pod 详细信息 # 2）通过 logs 查看 pod 日志 # 通过详细信息和日志基本就可以把问题定位出来了 # 5、k8s 如何实现持久化存储？有几种方式?\nemptyDir、hostPath、pv、storageclass、ceph、nfs、gluster 等都可以实现 k8s数据持久化，也可以通过 storageclass 动态的从 nfs provisioner 或者 cephprovisioner 等供应商动态的划分存储做成 pv # 6、service 的 type 类型有几种？\nClusterIp：集群内部相关访问 # NodePort：可以在物理机映射端口 # ExternalName：可以对 service 做软连接 # LoadBalancer：使用的是云的 slb # 7、ceph 架构是什么？\nCeph 是统一存储系统，支持三种接口： # 1）、Object：有原生的 API，而且也兼容 Swift 和 S3 的 API\n2）、Block：支持精简配置、快照、克隆\n3）、File：Posix 接口，支持快照\nCeph 也是分布式存储系统，它的特点是：\n**高扩展性：**使用普通 x86 服务器，支持 10~1000 台服务器，支持 TB 到 PB 级的扩展。 # **高可靠性：**没有单点故障，多数据副本，自动管理，自动修复。 # **高性能：**数据分布均衡，并行化度高。对于 objects storage 和 block storage,不需要元数据服务器。 # 8、k8s 怎么对接 ceph？\n把 ceph rbd 或者 cephfs 做成 pvStorageclass 可以动态从 ceph provisioner 找到 ceph，然后生成 pv # 9、k8s 挂载 cephfs 和 ceph rbd 适用场景分析\nK8s 挂载 cephfs 可以支持跨 node 节点 pod 挂载 # K8s 挂载 ceph rbd 不支持跨 node 节点 pod 挂载 # 10、k8s 有几种探测方式，分别描述下具体作用？\nlivenessProbe：存活性探测\n许多应用程序经过长时间运行，最终过渡到无法运行的状态，除了重启，无法恢复。通常情况下，K8S 会发现应用程序已经终止，然后重启应用程序 pod。有时应用程序可能因为某些原因（后端服务故障等）导致暂时无法对外提供服务，但应用软件没有终止，导致K8S 无法隔离有故障的 pod，调用者可能会访问到有故障的 pod，导致业务不稳定。K8S提供 livenessProbe 来检测容器是否正常运行，并且对相应状况进行相应的补救措施。 # readinessProbe：就绪性探测\n在没有配置 readinessProbe 的资源对象中，pod 中的容器启动完成后，就认为 pod 中的应用程序可以对外提供服务，该 pod 就会加入相对应的 service，对外提供服务。但有时一些应用程序启动后，需要较长时间的加载才能对外服务，如果这时对外提供服务，执行结果必然无法达到预期效果，影响用户体验。比如使用 tomcat 的应用程序来说，并不是简单地说 tomcat 启动成功就可以对外提供服务的，还需要等待 spring 容器初始化，数据库连接上等等。 # startupProbe: 探测容器中的应用是否已经启动。\n如果提供了启动探测(startupprobe)，则禁用所有其他探测，直到它成功为止。如果启动探测失败，kubelet 将杀死容器，容器服从其重启策略进行重启。如果容器没有提供启动探测，则默认状态为成功Success。 # 11、 k8s 的 service 与 Ingress 区别\nService 是四层代理，只能基于 ip 和端口代理后端服务Ingress controller 是七层代理，可以通过 http 或者 https 的域名基于 cookie、地域、请求头、百分比进行代理转发。Ingress controller 代理需要找到对应的 Service，由 Service 向后代理 Pod # 12、说几个 k8s 的网络插件，说一下他们的差异\nflannel：支持地址分配，不支持网络策略 。 # calico：支持地址分配，支持网络策略。 # flannel： # vxlan：#扩展的虚拟局域网 # V 虚拟的 # X 扩展的 # lan 局域网 # flannel 支持多种后端：\n1、VxLAN：\n(1) vxlan 叠加网络模式 # (2) Directrouting # 2、host-gw: Host Gateway\n#直接路由模式，不推荐，只能在二层网络中，不支持跨网络，如果有成千上万的 Pod，容易产生广播风暴 # 3、UDP：一般不用这个模式，性能差\nflannel 方案：需要在每个节点上把发向容器的数据包进行封装后，再用隧道将封装后的数据包发送到运行着目标 Pod 的 node 节点上。目标 node 节点再负责去掉封装，将去除封装的数据包发送到目标 Pod 上。数据通信性能则大受影响 # **4、calico 方案：**在 k8s 多个网路解决方案中选择了延迟表现最好的-calico 方案，可以设置网络策略适用于大型集群。 # "},{"id":50,"href":"/docs/2024-2-22-k8s%E9%9D%A2%E8%AF%95%E5%AE%9D%E5%85%B8/","title":"2024-2-22 k8s面试宝典","section":"Docs","content":" 创建 Pod的主要流程? # 客户端提交 Pod 的配置信息(可以是 yaml 文件定义的信息)到 kube-apiserver. # Apiserver 收到指令后,通知 controllr-manager 创建一个资源对象 # controller-manager 通过 apiserver 将 pod 的配置信息存储到 ETCD 数据中心中 # kube-scheduler 检查到 pod 信息会开始调度预选,会先过滤不符合 Pod 资源配置要求的节点,然后开始调度调优,主要是挑选出更适合运行的 pod 节点,然后将 pod 的资源配置单发送到 node 节点上的 kubelet 组件上 # kubelet 根据 scheduler 发来的资源配置单运行 pod,运行成功后,将 pod 的运行的信息返回 scheduler, scheduler 将返回的 pod 运行状况的信息存储到 etcd 数据中心 # Pod 的重启策略 # • Pod 重启策略(RestartPolicy)应用于 Pod 内的所有容器,并且仅再 Pod 所处的 Node 上由 Kubelet 进行判断和重启操作.当某个容器异常退出或健康检查失败时,kubele 将根据 RestartPolicy 的设置来进行相应操作 # • pod 的重启策略包括 Always,OnFaliure 和 Never,默认值为 Always # • Always: 当容器失效时由 kubelet 自动重启该容器 # • OnFailure:当容器终止运行且退出不为 0 时, 由 kubelet 自动重启该容器 # • Nerve: 不论容器运行状态如何,kubelet 都不会重启该容器 # • 同时 pod 的容器策略与控制方式关联,当前可用于管理 Pod 的控制器包括 RelicatonController # Pod 的健康检查方式 # • LivenessProbe探针：用于判断容器是否存活（running状态），如果LivenessProbe探针探测到容器不健康，则kubelet将杀掉该容器，并根据容器的重启策略做相应处理。若一个容器不包含LivenessProbe探针，kubelet认为该容器的LivenessProbe探针返回值用于是“Success”。 # • ReadineeProbe探针：用于判断容器是否启动完成（ready状态）。如果ReadinessProbe探针探测到失败，则Pod的状态将被修改。Endpoint Controller将从Service的Endpoint中删除包含该容器所在Pod的Eenpoint。 # • startupProbe探针：启动检查机制，应用一些启动缓慢的业务，避免业务长时间启动而被上面两类探针kill掉。 # Pod 探针常见方式 # • ExecAction：在容器内执行一个命令，若返回码为0，则表明容器健康。 # • TCPSocketAction：通过容器的IP地址和端口号执行TCP检查，若能建立TCP连接，则表明容器健康。 # • HTTPGetAction：通过容器的IP地址、端口号及路径调用HTTP Get方法，若响应的状态码大于等于200且小于400，则表明容器健康。 # Pod 常见的调度方式 # • Deployment或RC：该调度策略主要功能就是自动部署一个容器应用的多份副本，以及持续监控副本的数量，在集群内始终维持用户指定的副本数量。 # • NodeSelector：定向调度，当需要手动指定将Pod调度到特定Node上，可以通过Node的标签（Label）和Pod的nodeSelector属性相匹配。 # • NodeAffinity亲和性调度：亲和性调度机制极大的扩展了Pod的调度能力，目前有两种节点亲和力表达： # • requiredDuringSchedulingIgnoredDuringExecution：硬规则，必须满足指定的规则，调度器才可以调度Pod至Node上（类似nodeSelector，语法不同）。 # • preferredDuringSchedulingIgnoredDuringExecution：软规则，优先调度至满足的Node的节点，但不强求，多个优先级规则还可以设置权重值。 # • Taints和Tolerations（污点和容忍）： # • Taint：使Node拒绝特定Pod运行； # • Toleration：为Pod的属性，表示Pod能容忍（运行）标注了Taint的Node。 # deployment升级策略? # • 在Deployment的定义中，可以通过spec.strategy指定Pod更新的策略，目前支持两种策略：Recreate（重建）和RollingUpdate（滚动更新），默认值为RollingUpdate。 # • Recreate：设置spec.strategy.type=Recreate，表示Deployment在更新Pod时，会先杀掉所有正在运行的Pod，然后创建新的Pod。 # • RollingUpdate：设置spec.strategy.type=RollingUpdate，表示Deployment会以滚动更新的方式来逐个更新Pod。同时，可以通过设置spec.strategy.rollingUpdate下的两个参数（maxUnavailable和maxSurge）来控制滚动更新的过程 # Kubernetes Service类型? # 通过创建Service，可以为一组具有相同功能的容器应用提供一个统一的入口地址，并且将请求负载分发到后端的各个容器应用上。其主要类型有： # • ClusterIP：虚拟的服务IP地址，该地址用于Kubernetes集群内部的Pod访问，在Node上kube-proxy通过设置的iptables规则进行转发； # • NodePort：使用宿主机的端口，使能够访问各Node的外部客户端通过Node的IP地址和端口号就能访问服务； # • LoadBalancer：使用外接负载均衡器完成到服务的负载分发，需要在spec.status.loadBalancer字段指定外部负载均衡器的IP地址，通常用于公有云。 # Service分发后端的策略? # Service负载分发的策略有：RoundRobin和SessionAffinity # • RoundRobin：默认为轮询模式，即轮询将请求转发到后端的各个Pod上。 # • SessionAffinity：基于客户端IP地址进行会话保持的模式，即第1次将某个客户端发起的请求转发到后端的某个Pod上，之后从相同的客户端发起的请求都将被转发到后端相同的Pod上。 # Kubernetes外部如何访问集群内的服务? # • 映射Pod到物理机：将Pod端口号映射到宿主机，即在Pod中采用hostPort方式，以使客户端应用能够通过物理机访问容器应用。 # • 映射Service到物理机：将Service端口号映射到宿主机，即在Service中采用nodePort方式，以使客户端应用能够通过物理机访问容器应用。 # • 映射Sercie到LoadBalancer：通过设置LoadBalancer映射到云服务商提供的LoadBalancer地址。这种用法仅用于在公有云服务提供商的云平台上设置Service的场景。 # Kubernetes ingress? # • Kubernetes的Ingress资源对象，用于将不同URL的访问请求转发到后端不同的Service，以实现HTTP层的业务路由机制。 # • Kubernetes使用了Ingress策略和Ingress Controller，两者结合并实现了一个完整的Ingress负载均衡器。使用Ingress进行负载分发时，Ingress Controller基于Ingress规则将客户端请求直接转发到Service对应的后端Endpoint（Pod）上，从而跳过kube-proxy的转发功能，kube-proxy不再起作用，全过程为：ingress controller + ingress 规则 \u0026mdash;-\u0026gt; services。 # • 同时当Ingress Controller提供的是对外服务，则实际上实现的是边缘路由器的功能。 # Kubernetes镜像的下载策略? # K8s的镜像下载策略有三种：Always、Never、IFNotPresent。 # • Always：镜像标签为latest时，总是从指定的仓库中获取镜像。 # • Never：禁止从仓库中下载镜像，也就是说只能使用本地镜像。 # • IfNotPresent：仅当本地没有对应镜像时，才从目标仓库中下载。 # 默认的镜像下载策略是：当镜像标签是latest时，默认策略是Always；当镜像标签是自定义时（也就是标签不是latest），那么默认策略是IfNotPresent。 # Kubernetes kubelet的作用? # • 在Kubernetes集群中，在每个Node（又称Worker）上都会启动一个kubelet服务进程。该进程用于处理Master下发到本节点的任务，管理Pod及Pod中的容器。每个kubelet进程都会在API Server上注册节点自身的信息，定期向Master汇报节点资源的使用情况，并通过cAdvisor监控容器和节点资源。 # Secret有哪些使用方式? # • 创建完secret之后，可通过如下三种方式使用： # • 在创建Pod时，通过为Pod指定Service Account来自动使用该Secret。 # • 通过挂载该Secret到Pod来使用它。 # • 在Docker镜像下载时使用，通过指定Pod的spc.ImagePullSecrets来引用它。 # Kubernetes CNI模型? # • CNI提供了一种应用容器的插件化网络解决方案，定义对容器网络进行操作和配置的规范，通过插件的形式对CNI接口进行实现。CNI仅关注在创建容器时分配网络资源，和在销毁容器时删除网络资源。在CNI模型中只涉及两个概念：容器和网络。 # • 容器（Container）：是拥有独立Linux网络命名空间的环境，例如使用Docker或rkt创建的容器。容器需要拥有自己的Linux网络命名空间，这是加入网络的必要条件。 # • 网络（Network）：表示可以互连的一组实体，这些实体拥有各自独立、唯一的IP地址，可以是容器、物理机或者其他网络设备（比如路由器）等。 # • 对容器网络的设置和操作都通过插件（Plugin）进行具体实现，CNI插件包括两种类型：CNI Plugin和IPAM（IP Address Management）Plugin。CNI Plugin负责为容器配置网络资源，IPAM Plugin负责对容器的IP地址进行分配和管理。IPAM Plugin作为CNI Plugin的一部分，与CNI Plugin协同工作。 # Kubernetes PV和PVC? # • PV是对底层网络共享存储的抽象，将共享存储定义为一种“资源”。 # • PVC则是用户对存储资源的一个“申请”。 # PV生命周期内的阶段? # 某个PV在生命周期中可能处于以下4个阶段（Phaes）之一。 # • Available：可用状态，还未与某个PVC绑定。 # • Bound：已与某个PVC绑定。 # • Released：绑定的PVC已经删除，资源已释放，但没有被集群回收。 # • Failed：自动资源回收失败。 # calico 网络模式 # 模式 数据包封包 优点 缺点 vxlan 封包， 在vxlan设备上将pod发来的数据包源、目的mac替换为本机vxlan网卡和对端节点vxlan网卡的mac。外层udp目的ip地址根据路由和对端vxlan的mac查fdb表获取 只要k8s节点间三层互通， 可以跨网段， 对主机网关路由没有特殊要求。各个node节点通过vxlan设备实现基于三层的”二层”互通, 三层即vxlan包封装在udp数据包中， 要求udp在k8s节点间三层可达；二层即vxlan封包的源mac地址和目的mac地址是自己的vxlan设备mac和对端vxlan设备mac。 需要进行vxlan的数据包封包和解包会存在一定的性能损耗 ipip 封包，在tunl0设备上将pod发来的数据包的mac层去掉，留下ip层封包。 外层数据包目的ip地址根据路由得到。 只要k8s节点间三层互通， 可以跨网段， 对主机网关路由没有特殊要求。 需要进行ipip的数据包封包和解包会存在一定的性能损耗 bgp 不需要进行数据包封包 不用封包解包，通过bgp协议可实现pod网络在主机间的三层可达， k8s节点不跨网段时和flannel的host-gw相似； 支持跨网段， 满足复杂的网络架构 跨网段时，需要主机网关路由也充当BGP Speaker能够学习到pod子网路由并实现pod子网路由的转发 fannel三种模式 # fannel三种模式 效率 calico 模式 UDP 性能较差，封包解包涉及到多次用户态和内核态交互 类似 IPIP VXLAN 性能较好，封包解包在内核态实现，内核转发数据，flanneld负责动态配置ARP和FDB（转发数据库）表项更新 类似VXLAN host-gw 性能最好，不需要再次封包，正常发包，目的容器所在的主机充当网关 flanneld 负责主机上路由表的刷新 类似 BGP 你知道的几种CNI网络插件，并详述其工作原理。K8s常用的CNI网络插件 （calico \u0026amp;\u0026amp; flannel），简述一下它们的工作原理和区别。 # \\1. calico根据iptables规则进行路由转发，并没有进行封包，解包的过程，这和flannel比起来效率就会快多 calico包括如下重要组件：Felix，etcd，BGP Client，BGP Route Reflector。下面分别说明一下这些组件。 # Felix：主要负责路由配置以及ACLS规则的配置以及下发，它存在在每个node节点上。 etcd：分布式键值存储，主要负责网络元数据一致性，确保Calico网络状态的准确性，可以与kubernetes共用； BGPClient(BIRD), 主要负责把 Felix写入 kernel的路由信息分发到当前 Calico网络，确保 workload间的通信的有效性； BGPRoute Reflector(BIRD), 大规模部署时使用，摒弃所有节点互联的mesh模式，通过一个或者多个 BGPRoute Reflector 来完成集中式的路由分发 通过将整个互联网的可扩展 IP网络原则压缩到数据中心级别，Calico在每一个计算节点利用 Linuxkernel 实现了一个高效的 vRouter来负责数据转发，而每个vRouter通过 BGP协议负责把自己上运行的 workload的路由信息向整个Calico网络内传播，小规模部署可以直接互联，大规模下可通过指定的BGProute reflector 来完成。这样保证最终所有的workload之间的数据流量都是通过 IP包的方式完成互联的。 # \\1. Flannel的工作原理： Flannel实质上是一种“覆盖网络(overlay network)”，也就是将TCP数据包装在另一种网络包里面进行路由转发和通信，目前已经支持UDP、VxLAN、AWS VPC和GCE路由等数据转发方式。 # 默认的节点间数据通信方式是UDP转发。 工作原理： 数据从源容器中发出后，经由所在主机的docker0虚拟网卡转发到flannel0虚拟网卡（先可以不经过docker0网卡，使用cni模式），这是个P2P的虚拟网卡，flanneld服务监听在网卡的另外一端。 Flannel通过Etcd服务维护了一张节点间的路由表，详细记录了各节点子网网段 。 源主机的flanneld服务将原本的数据内容UDP封装后根据自己的路由表投递给目的节点的flanneld服务，数据到达以后被解包，然后直接进入目的节点的flannel0虚拟网卡，然后被转发到目的主机的docker0虚拟网卡，最后就像本机容器通信一下的有docker0路由到达目标容器。 flannel在进行路由转发的基础上进行了封包解包的操作，这样浪费了CPU的计算资源。 # Worker节点宕机，简述Pods驱逐流程。 # \\1. 在 Kubernetes 集群中，当节点由于某些原因（网络、宕机等）不能正常工作时会被认定为不可用状态（Unknown 或者 False 状态），当时间超过了 pod-eviction-timeout 值时，那么节点上的所有 Pod 都会被节点控制器计划删除。 # \\2. Kubernetes 集群中有一个节点生命周期控制器：node_lifecycle_controller.go。它会与每一个节点上的 kubelet 进行通信，以收集各个节点已经节点上容器的相关状态信息。当超出一定时间后不能与 kubelet 通信，那么就会标记该节点为 Unknown 状态。并且节点生命周期控制器会自动创建代表状况的污点，用于防止调度器调度 pod 到该节点。 # \\3. 那么 Unknown 状态的节点上已经运行的 pod 会怎么处理呢？节点上的所有 Pod 都会被污点管理器（taint_manager.go）计划删除。而在节点被认定为不可用状态到删除节点上的 Pod 之间是有一段时间的，这段时间被称为容忍度。如果在不配置的情况下，Kubernetes 会自动给 Pod 添加一个 key 为 node.kubernetes.io/not-ready 的容忍度 并配置 tolerationSeconds=300，同样，Kubernetes 会给 Pod 添加一个 key 为 node.kubernetes.io/unreachable 的容忍度 并配置 tolerationSeconds=300。 # \\4. 当到了删除 Pod 时，污点管理器会创建污点标记事件，然后驱逐 pod 。这里需要注意的是由于已经不能与 kubelet 通信，所以该节点上的 Pod 在管理后台看到的是处于灰色标记，但是此时如果去获取 pod 的状态其实还是处于 Running 状态。每种类型的资源都有相应的资源控制器（Controller），例如：deployment_controller.go、stateful_set_control.go。每种控制器都在监听资源变化，从而做出相应的动作执行。deployment 控制器在监听到 Pod 被驱逐后会创建一个新的 Pod 出来，但是 Statefulset 控制器并不会创建出新的 Pod，原因是因为它可能会违反 StatefulSet 固有的至多一个的语义，可能出现具有相同身份的多个成员，这将可能是灾难性的，并且可能导致数据丢失。 # 你知道的K8s中几种Controller控制器，并详述其工作原理 # \\1. deployment：适合无状态的服务部署 适合部署无状态的应用服务，用来管理pod和replicaset，具有上线部署、副本设定、滚动更新、回滚等功能，还可提供声明式更新，例如只更新一个新的Image # • 编写yaml文件，并创建nginx服务pod资源。 # \\1. StatefullSet：适合有状态的服务部署 适合部署有状态应用，解决Pod的独立生命周期，保持Pod启动顺序和唯一性。 # • 稳定，唯一的网络标识符，持久存储（例如：etcd配置文件，节点地址发生变化，将无法使用） # • 有序，优雅的部署和扩展、删除和终止（例如：mysql主从关系，先启动主，再启动从）有序，滚动更新 # • 应用场景：例如数据库 # 无状态服务的特点： # • deployment 认为所有的pod都是一样的 # • 不用考虑顺序的要求 # • 不用考虑在哪个node节点上运行 # • 可以随意扩容和缩容 # 有状态服务的特点： # • 实例之间有差别，每个实例都有自己的独特性，元数据不同，例如etcd，zookeeper # • 实例之间不对等的关系，以及依靠外部存储的应用 # • 常规的service服务和无头服务的区别 # • service：一组Pod访问策略，提供cluster-IP群集之间通讯，还提供负载均衡和服务发现 # • Headless service 无头服务，不需要cluster-IP，直接绑定具体的Pod的IP，无头服务经常用于statefulset的有状态部署 # • 创建无头服务的service资源和dns资源，由于有状态服务的IP地址是动态的，所以使用无头服务的时候要绑定dns服务 # \\1. DaemonSet：一次部署，所有的node节点都会部署，例如一些典型的应用场景： 运行集群存储 daemon，例如在每个Node上运行 glusterd、ceph # • 在每个Node上运行日志收集 daemon，例如 fluentd、 logstash # • 在每个Node上运行监控 daemon，例如 Prometheus Node Exporter # • 在每一个Node上运行一个Pod # • 新加入的Node也同样会自动运行一个Pod # • 应用场景：监控，分布式存储，日志收集等 # \\1. Job：一次性的执行任务 # • 一次性执行任务，类似Linux中的job # • 应用场景：如离线数据处理，视频解码等业务 # \\1. Cronjob：周期性的执行任务 # • 周期性任务，像Linux的Crontab一样 # • 应用场景：如通知，备份等 # • 使用cronjob要慎重，用完之后要删掉，不然会占用很多资源 # ingress-controller的工作机制 # 通常情况下，service和pod的IP仅可在集群内部访问 # • k8s提供了service方式：NodePort 来提供对外的服务，外部的服务可以通过访问Node节点ip+NodePort端口来访问集群内部的资源，外部的请求先到达service所选中的节点上，然后负载均衡到每一个节点上。 # NodePort虽然提供了对外的方式但也有很大弊端： # • 由于service的实现方式：user_space 、iptebles、 3 ipvs、方式这三种方式只支持在4层协议通信，不支持7层协议，因此NodePort不能代理https服务。 # • NodePort 需要暴露service所属每个node节点上端口，当需求越来越多，端口数量过多，导致维护成本过高，并且集群不好管理。 # 原理 # • Ingress也是Kubernetes API的标准资源类型之一，它其实就是一组基于DNS名称（host）或URL路径把请求转发到指定的Service资源的规则。用于将集群外部的请求流量转发到集群内部完成的服务发布。我们需要明白的是，Ingress资源自身不能进行“流量穿透”，仅仅是一组规则的集合，这些集合规则还需要其他功能的辅助，比如监听某套接字，然后根据这些规则的匹配进行路由转发，这些能够为Ingress资源监听套接字并将流量转发的组件就是Ingress Controller。 # • Ingress 控制器不同于Deployment 等pod控制器的是，Ingress控制器不直接运行为kube-controller-manager的一部分，它仅仅是Kubernetes集群的一个附件，类似于CoreDNS，需要在集群上单独部署。 # • ingress controller通过监视api server获取相关ingress、service、endpoint、secret、node、configmap对象，并在程序内部不断循环监视相关service是否有新的endpoints变化，一旦发生变化则自动更新nginx.conf模板配置并产生新的配置文件进行reload # k8s的调度机制 # \\1. Scheduler工作原理： 请求及Scheduler调度步骤： # • 节点预选(Predicate)：排除完全不满足条件的节点，如内存大小，端口等条件不满足。 # • 节点优先级排序(Priority)：根据优先级选出最佳节点 # • 节点择优(Select)：根据优先级选定节点 # \\1. 具体步骤： # • 首先用户通过 Kubernetes 客户端 Kubectl 提交创建 Pod 的 Yaml 的文件，向Kubernetes 系统发起资源请求，该资源请求被提交到 # • Kubernetes 系统中，用户通过命令行工具 Kubectl 向 Kubernetes 集群即 APIServer 用 的方式发送“POST”请求，即创建 Pod 的请求。 # • APIServer 接收到请求后把创建 Pod 的信息存储到 Etcd 中，从集群运行那一刻起，资源调度系统 Scheduler 就会定时去监控 APIServer # • 通过 APIServer 得到创建 Pod 的信息，Scheduler 采用 watch 机制，一旦 Etcd 存储 Pod 信息成功便会立即通知APIServer， # • APIServer会立即把Pod创建的消息通知Scheduler，Scheduler发现 Pod 的属性中 Dest Node 为空时（Dest Node=””）便会立即触发调度流程进行调度。 # • 而这一个创建Pod对象，在调度的过程当中有3个阶段：节点预选、节点优选、节点选定，从而筛选出最佳的节点 # • 节点预选：基于一系列的预选规则对每个节点进行检查，将那些不符合条件的节点过滤，从而完成节点的预选 # • 节点优选：对预选出的节点进行优先级排序，以便选出最合适运行Pod对象的节点 # • 节点选定：从优先级排序结果中挑选出优先级最高的节点运行Pod，当这类节点多于1个时，则进行随机选择 # \\1. k8s的调用工作方式 # • Kubernetes调度器作为集群的大脑，在如何提高集群的资源利用率、保证集群中服务的稳定运行中也会变得越来越重要Kubernetes的资源分为两种属性。 # • 可压缩资源（例如CPU循环，Disk I/O带宽）都是可以被限制和被回收的，对于一个Pod来说可以降低这些资源的使用量而不去杀掉Pod。 # • 不可压缩资源（例如内存、硬盘空间）一般来说不杀掉Pod就没法回收。未来Kubernetes会加入更多资源，如网络带宽，存储IOPS的支持。 # kube-proxy的三种工作模式和原理 # \\1. userspace 模式 # • 该模式下kube-proxy会为每一个Service创建一个监听端口。发向Cluster IP的请求被Iptables规则重定向到Kube-proxy监听的端口上，Kube-proxy根据LB算法选择一个提供服务的Pod并和其建立链接，以将请求转发到Pod上。 # • 该模式下，Kube-proxy充当了一个四层Load balancer的角色。由于kube-proxy运行在userspace中，在进行转发处理时会增加两次内核和用户空间之间的数据拷贝，效率较另外两种模式低一些；好处是当后端的Pod不可用时，kube-proxy可以重试其他Pod。 # \\1. iptables 模式 # • 为了避免增加内核和用户空间的数据拷贝操作，提高转发效率，Kube-proxy提供了iptables模式。在该模式下，Kube-proxy为service后端的每个Pod创建对应的iptables规则，直接将发向Cluster IP的请求重定向到一个Pod IP。 # • 该模式下Kube-proxy不承担四层代理的角色，只负责创建iptables规则。该模式的优点是较userspace模式效率更高，但不能提供灵活的LB策略，当后端Pod不可用时也无法进行重试。 # \\1. 该模式和iptables类似，kube-proxy监控Pod的变化并创建相应的ipvs rules。ipvs也是在kernel模式下通过netfilter实现的，但采用了hash table来存储规则，因此在规则较多的情况下，Ipvs相对iptables转发效率更高。除此以外，ipvs支持更多的LB算法。如果要设置kube-proxy为ipvs模式，必须在操作系统中安装IPVS内核模块。 # 1.提交创建pod的yaml文件到kube-apiserver\r2.kube-controller-manager创建资源对象\r3.kubecontroller-manager将pod的配置信息存到etcd\r4.kube-scheduler将pod调度到合适的节点\r5.在该节点的kubelet上创建pod，pod建好后将pod的信息存到etcd "},{"id":51,"href":"/docs/2024-2-26-%E9%9D%A2%E8%AF%95/","title":"2024-2-26 面试","section":"Docs","content":" k8s 有哪些组件？ # 1、etcd保存了集群的状态 # 2、apiserver提供了资源操作的唯一入口，还提供认证，授权，访问控制，api注册和服务发现等 # 3、controller manager负责维护集群的状态，比如自动扩展，滚动更新，故障检测等 # 4、scheduler 负责集群资源的调度，安照预期的调度策略将pod调度到相应的node上 # 5、kubelet负责管理容器的生命周期，还负责共享存储卷和网络的管理 # 6、Container runtime 容器运行时，负责管理镜像的管理，pod和容器的真正运行 # 7、kube-proxy负责管理集群的服务发现和负载均衡 # Kubernetes（K8s）是一个功能强大的容器编排平台，它由多个组件构成，这些组件共同协作来实现集群的管理、调度、监控等功能。下面是 Kubernetes 主要组件的简要介绍：\n1. Kubernetes 核心组件 # 1.1 API Server (kube-apiserver) # 职责：API Server 是 Kubernetes 控制平面的核心组件，它暴露了 Kubernetes 集群的 REST API，所有的命令和操作（如创建、删除、更新 Pod 等）都通过 API Server 来进行。\n功能\n：\n接收来自用户、控制器、调度器等的请求。 负责验证和处理请求。 提供 REST API 接口，供客户端和其他组件使用。 存储集群状态数据（通过与 etcd 交互）。 1.2 Controller Manager (kube-controller-manager) # 职责：Controller Manager 负责运行集群的控制循环（controller loops），确保集群的实际状态与期望状态相一致。\n功能\n：\n启动各种控制器，如 Deployment Controller、ReplicaSet Controller、Node Controller 等。 处理集群资源的自我修复任务，如创建新的 Pod 来替代已死 Pod，扩缩容等。 1.3 Scheduler (kube-scheduler) # 职责：调度器负责将 Pod 分配到集群中的合适节点上。\n功能\n：\n根据集群资源情况和 Pod 的要求，选择最佳的节点来运行 Pod。 支持不同的调度策略，如资源请求、节点亲和性（Node Affinity）等。 1.4 etcd # 职责：etcd 是 Kubernetes 的分布式键值存储系统，用于存储集群的所有状态数据，包括 Pod、节点、Service 等所有资源对象的配置和元数据。\n功能\n：\n持久化存储集群的所有状态信息。 提供强一致性保障，确保所有集群组件共享最新的状态数据。 1.5 Cloud Controller Manager (kube-cloud-controller-manager) # 职责：Cloud Controller Manager 是为了支持云环境的 Kubernetes 扩展，它使得 Kubernetes 集群能够与云提供商的 API 集成。\n功能\n：\n负责与云提供商的控制平面交互，管理负载均衡器、存储卷等云资源。 让 Kubernetes 集群能够在多云环境中管理资源。 2. Kubernetes 节点组件 # 2.1 Kubelet # 职责：Kubelet 是集群中每个节点上的代理，负责管理节点上的容器（Pod）。\n功能\n：\n监控和管理节点上的容器，确保 Pod 中的容器处于正常状态。 从 API Server 获取 Pod 的规格，并执行容器的启动、停止等操作。 收集和报告节点的资源使用情况（如 CPU、内存）到 API Server。 2.2 Kube Proxy # 职责：Kube Proxy 是一个网络代理，它负责维护集群中的网络规则，以确保网络流量能够正确地转发到 Pod。\n功能\n：\n管理每个节点的网络流量，基于 iptables 或 IPVS 实现负载均衡。 确保 Service 的 DNS 名称能够被解析，并且流量能够正确地路由到后端的 Pod。 2.3 Container Runtime # 职责：容器运行时（如 Docker、containerd）负责运行容器并管理容器生命周期。\n功能\n：\n启动、停止容器。 拉取容器镜像并管理容器资源。 3. 高层组件和工具 # 3.1 Kubernetes Dashboard # 职责：Kubernetes Dashboard 是一个基于 Web 的 UI 工具，用于管理和监控 Kubernetes 集群。\n功能\n：\n提供图形化的界面，方便用户查看集群状态、Pod、Deployment、Service 等资源。 支持用户创建、更新、删除集群资源。 3.2 Helm # 职责：Helm 是 Kubernetes 的包管理工具，类似于 Linux 的 apt 或 yum。\n功能\n：\n通过 Helm Charts 来管理应用程序的部署。 Helm Charts 是预配置的 Kubernetes 资源文件集合，允许用户方便地部署和管理应用。 3.3 Ingress Controller # 职责：Ingress Controller 是一种专门处理 HTTP/HTTPS 请求的 Kubernetes 组件，它根据 Ingress 资源来路由流量。\n功能\n：\n控制外部流量进入集群。 配置负载均衡、SSL/TLS 终端和 URL 路由等。 常见的 Ingress Controller 有 Nginx Ingress Controller、Traefik 等。 4. Kubernetes 核心资源 # 除了上述组件，Kubernetes 还包括一些重要的资源对象，用于管理集群和应用程序：\n4.1 Pod # 职责：Pod 是 Kubernetes 中最小的可调度单元，包含一个或多个容器，所有容器共享同一网络和存储。 4.2 Service # 职责：Service 提供了一种稳定的访问方式，用于暴露一组 Pod。它抽象了 Pod 的 IP 地址，并可以提供负载均衡。 4.3 ReplicaSet # 职责：ReplicaSet 用于保证在任何时候都维持指定数量的 Pod 副本。它常常与 Deployment 一起使用。 4.4 Deployment # 职责：Deployment 是一种高级控制器，负责管理 Pod 的副本、更新和滚动更新等操作。 4.5 StatefulSet # 职责：StatefulSet 是 Kubernetes 中用于管理有状态应用的控制器，支持持久化存储和 Pod 的稳定标识符。 4.6 ConfigMap 和 Secret # 职责\n：\nConfigMap：用于存储非敏感的配置数据。 Secret：用于存储敏感数据，如密码、OAuth token 等。 4.7 PersistentVolume (PV) 和 PersistentVolumeClaim (PVC) # 职责：PV 和 PVC 用于管理 Kubernetes 中的持久存储。PV 是集群中的存储资源，PVC 是 Pod 请求存储资源的方式。 总结 # Kubernetes 是由多个组件组成的复杂系统，每个组件都在集群的不同部分起着关键作用。控制平面 主要负责管理集群的整体状态，如 API Server、调度器、控制器等；而 节点组件 则在每个工作节点上执行容器的管理和调度任务，确保容器能够按照预期正常运行。通过这些组件的协作，Kubernetes 提供了强大的容器管理能力，支持大规模的容器化应用部署和运维。\n创建 Pod的主要流程? # 1.客户端提交pod的配置信息(可以是yaml)到kube-apiserver # 2.apiserver收到信息后kube-controller-manager创建资源对象 # 3.kubecontroller将yaml信息存到etcd # 4.kubescheduler调度pod到某个节点 # 5.kubelet根据kube-scheduler发来的配置信息来运行pod，运行成功将pod信息返回给schedule，scheduler将信息返回并存储到到etcd # 在 Kubernetes 中，Pod 是最小的部署单元，通常用于部署一个或多个容器。创建 Pod 的主要流程包括从用户发起请求到容器在节点上运行的各个步骤。以下是创建 Pod 的主要流程：\n1. 用户请求创建 Pod # 创建 Pod 的第一步是用户通过 kubectl 或其他 Kubernetes API 客户端发起创建请求。\n通过 kubectl 命令： 用户可以通过 kubectl run 或 kubectl apply -f pod.yaml 命令创建 Pod。\nkubectl apply -f pod.yaml 或者通过 kubectl run 创建 Pod（通常用于临时 Pod）：\nkubectl run mypod --image=nginx 通过 Kubernetes API： 用户也可以通过 Kubernetes API 发起创建请求（通常由控制器自动发起，如 Deployment 或 StatefulSet）。这时，Pod 描述会通过 API 发送到 API Server。\n2. API Server 接收并验证请求 # Kubernetes 中的所有操作都会通过 API Server 来进行。无论是通过 kubectl 还是 API 发起的请求，都会到达 API Server。\n验证请求：API Server 对 Pod 请求进行验证，确保所请求的 Pod 描述符合 Kubernetes 的规范。例如，确保资源请求正确，标签、容忍等参数合法。 存储请求：验证通过后，API Server 会将 Pod 的描述（通常是 YAML 或 JSON 格式）存储到 Kubernetes 的 etcd 数据存储中，作为集群的当前状态。 3. 调度器选择节点（调度） # 当 API Server 成功接收到 Pod 创建请求并将其存储在 etcd 中后，接下来的任务是将 Pod 调度到一个合适的节点上。这是由 调度器（kube-scheduler） 完成的。\n调度过程： # 过滤阶段： 调度器会根据 Pod 的要求（如资源请求、亲和性/反亲和性等），对所有节点进行过滤，排除不符合条件的节点。 例如，某些节点的资源可能不足，或者节点与 Pod 的亲和性要求不匹配。 优选阶段： 对剩下的符合条件的节点，调度器会根据节点的负载、拓扑结构等因素打分，选择一个最合适的节点。 例如，调度器会考虑节点的 CPU 和内存使用情况，选择资源最空闲的节点。 绑定： 一旦选定了节点，调度器会将 Pod 与节点绑定，向 API Server 通知这个 Pod 已经成功调度到指定节点。 4. 节点上的 Kubelet 处理 Pod # 调度器将 Pod 绑定到特定节点后，Kubelet 会在该节点上进行具体的操作。\n拉取容器镜像：Kubelet 会根据 Pod 配置中的容器镜像信息，拉取相应的容器镜像到本地。如果镜像已经存在，则会跳过这一步。 创建容器：Kubelet 使用容器运行时（如 Docker、containerd）在节点上启动容器。Pod 可以包含一个或多个容器，Kubelet 会逐个启动它们。 设置网络和存储： Kubelet 为 Pod 配置网络，使得容器之间可以相互通信，并且可以访问外部网络。 如果 Pod 定义了持久存储（如卷），Kubelet 会挂载相关的存储资源。 5. Pod 运行与健康检查 # 当容器启动后，Kubelet 会持续监控 Pod 和容器的状态。Kubelet 会定期执行以下任务：\n健康检查（Liveness / Readiness）： Liveness Probe：检查容器是否仍然存活。如果 Pod 容器挂掉，Kubelet 会重启容器。 Readiness Probe：检查容器是否已准备好接受流量。如果容器不准备好，Kubelet 会将其从服务的负载均衡池中移除，直到它准备好。 资源监控： Kubelet 会报告节点和容器的资源使用情况（如 CPU、内存等）到 API Server。 日志与事件： Kubelet 会将容器的日志和事件收集起来，供用户查看。 6. 控制器的反馈（如 Deployment、StatefulSet） # 如果 Pod 是由某个控制器（如 Deployment、StatefulSet、DaemonSet）管理的，那么这些控制器会继续监视 Pod 的状态，确保集群的实际状态符合所期望的状态。如果有 Pod 创建失败，控制器会重新调度 Pod。\n例如，Deployment 会确保它管理的 Pod 数量始终保持一致。如果某个 Pod 不健康或不可用，Deployment 会创建一个新的 Pod 来替代它。 7. 更新集群状态 # 最终，Kubernetes 集群的状态（包括 Pod、节点、容器等信息）会更新到 etcd 中，以确保集群的最终状态一致。\n总结：创建 Pod 的主要流程 # 用户请求：用户通过 kubectl 或 API 创建 Pod。 API Server 验证和存储：API Server 验证请求并将 Pod 存储到 etcd 中。 调度器调度：调度器选择合适的节点，并将 Pod 绑定到该节点。 Kubelet 启动容器：Kubelet 在节点上拉取容器镜像并启动容器。 健康检查与资源监控：Kubelet 执行容器的健康检查，并持续报告资源使用情况。 控制器管理：控制器监控 Pod 的状态，确保集群的期望状态与实际状态一致。 集群状态更新：Pod 创建成功后，集群状态存储在 etcd 中。 通过这些步骤，Kubernetes 确保 Pod 在集群中得到高效、可靠的管理和调度。\nk8s的调度机制 # 1.节点预选 # 2.节点打分排序 # 3.节点选优 # Kubernetes (K8s) 的调度机制是其核心功能之一，用于决定容器应该在集群中的哪个节点上运行。K8s 调度器（Scheduler）负责根据节点的资源可用性、负载、拓扑、约束条件等因素，智能地将 Pod 分配到集群中的不同节点。调度是一个多阶段的过程，并且涉及到多个重要的调度策略和插件。\nKubernetes 调度流程概述 # Pod 创建 用户通过 kubectl 或其他 API 创建一个 Pod 请求，或者在某个 Deployment、StatefulSet 等控制器中声明了新的 Pod。 Kubernetes 控制平面通过 API Server 接收这个请求，并将 Pod 交给调度器来决定放置在哪个节点上。 调度器决定节点 调度器会检查集群中所有节点的资源使用情况，并根据一系列规则来选择最合适的节点。 Pod 绑定到节点 调度器选择合适的节点后，将 Pod 绑定到节点上，并将信息更新到 API Server。然后，节点上的 kubelet 负责拉取容器镜像、启动容器。 Kubernetes 调度的关键机制 # 调度阶段 Kubernetes 调度器通常按以下顺序工作： 过滤阶段 (Filter) 调度器通过节点的 节点选择器（NodeSelector）、资源限制、硬件/网络要求等，过滤出不符合条件的节点。 过滤阶段的目标是排除掉那些不适合运行该 Pod 的节点，通常根据一些基本的约束条件，如 CPU、内存等资源。 优选阶段 (Score) 对于剩下的候选节点，调度器通过 调度策略 进行排序（比如节点的负载、拓扑结构等），选择一个最优的节点。 在优选阶段，调度器会基于 Affinity（亲和性）和 Anti-Affinity（反亲和性）规则、Pod 的资源请求与限制、Pod 的拓扑要求等因素对节点进行评分。 绑定阶段 (Bind) 调度器最终选择一个节点并将 Pod 绑定到该节点上。这时，节点上的 kubelet 会负责启动容器。 Kubernetes 调度策略 # 资源请求和限制\n每个 Pod 可以定义 资源请求（request）和 资源限制（limit）。资源请求是调度器决定在哪里调度 Pod 的基础，而资源限制则是容器运行时能使用的最大资源量。 调度器会根据节点的可用资源（CPU、内存等）和 Pod 的资源请求来选择节点。 Node Affinity（节点亲和性）\n通过 nodeAffinity 规则，可以控制 Pod 只调度到符合特定标签的节点上。 硬亲和性（必需条件）和 软亲和性（优先条件）都可以通过 affinity 配置来指定。 示例：让 Pod 只调度到具有特定标签的节点上：\naffinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: \u0026#34;disktype\u0026#34; operator: In values: - ssd Pod Affinity 和 Anti-Affinity（Pod亲和性和反亲和性）\nPod Affinity：控制 Pod 应该尽可能地调度到与其他 Pod 相同或相邻的节点上。 Pod Anti-Affinity：控制 Pod 不应调度到与特定 Pod 相同或相邻的节点上。 示例：让 Pod 调度到与另一个特定 Pod 在同一个节点上：\naffinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: labelSelector: matchLabels: app: frontend topologyKey: \u0026#34;kubernetes.io/hostname\u0026#34; Taints 和 Tolerations（污点和容忍）\nTaints：允许节点标记为“污点”，即节点会拒绝调度任何没有适当容忍的 Pod。 Tolerations：Pod 可以设置容忍，允许其调度到有污点的节点上。 示例：一个节点加上污点，只有具有特定容忍的 Pod 才能调度到该节点。\nkubectl taint nodes node1 key=value:NoSchedule 在 Pod 配置中添加容忍：\ntolerations: - key: \u0026#34;key\u0026#34; operator: \u0026#34;Equal\u0026#34; value: \u0026#34;value\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; 资源负载均衡\n在选择节点时，调度器也会考虑节点的负载，确保集群资源的合理利用。调度器会避免某些节点资源过载，而其他节点资源闲置。 调度器会根据节点的 CPU、内存等资源的剩余情况来选择最合适的节点。 Kubernetes 默认调度器和自定义调度器 # 默认调度器：Kubernetes 使用 kube-scheduler 作为默认调度器，处理所有 Pod 的调度请求。 自定义调度器：如果有特殊的调度需求，可以使用自定义调度器。可以在集群中运行多个调度器，指定特定的调度策略和工作负载。 Kubernetes 调度器插件 # Kubernetes 调度器支持插件架构，允许在调度过程中使用不同的策略和插件来定制行为。常见的插件包括：\n预调度插件（如 NodeAffinity、TaintToleration 等） 调度策略插件（如 PodAffinity、ResourceLimits 等） 后调度插件（如调度后监听 Pod 状态的变化） 这些插件使得 Kubernetes 调度器能够灵活地满足各种业务场景的需求。\n总结 # Kubernetes 调度器负责根据节点的资源、拓扑、亲和性和反亲和性、污点和容忍等规则，智能地将 Pod 分配到最合适的节点上。调度过程包括 过滤、优选 和 绑定 阶段。通过使用 Node Affinity、Pod Affinity、Taints 和 Tolerations 等策略，可以精细控制 Pod 的调度行为，确保集群资源的高效利用和满足业务需求。\n"},{"id":52,"href":"/docs/2024-3-19-%E4%B8%A4%E5%BC%A0%E5%9B%BE%E5%85%A8%E9%9D%A2%E7%90%86%E8%A7%A3k8s%E5%8E%9F%E7%90%86/","title":"2024-3-19 两张图全面理解k8s原理","section":"Docs","content":" Kubernetes 简介 # Kubernetes 源于希腊语，意为“舵手”。k8s 缩写是因为 k 和 s 之间有八个字符的原因。它是 Google 在 2015 开源的容器调度编排的平台。\nKubernetes 作为一款优秀的容器编排工具，拥有非常精妙的架构设计。\nKubernetes 架构 # Kubernetes 是一个 Master + Worker 的架构，Master 可理解为 Kubernetes 的控制面，Worker 理解为 Kubernetes 的数据面。 # Master 节点一般只运行 Kubernetes 控制组件，是整个集群的大脑，一般不运行业务容器。 # Worker 节点是运行业务容器的节点。 # Master # Kubernetes Master 节点需要运行以下组件： # • **Kube-apiserver：**Kube-apiserver 是 Kubernetes 的 API 入口，是集群流量入口； # • **Kube-controller-manager：**Kube-controller-manager 包含很多个 Controller，用于调谐集群中的资源； # • **Kube-scheduler：**Kube-scheduler 是集群中默认调度器，给 Pod 选择最优节点调度； # • **Etcd：**Kuebernetes 的后端存储，集群中所有可持久化数据都存储在 Etcd 中。 # Kubernetes 中采用声明式 API，即 API 不需要关心具体功能实现，只需关心最终状态。一个 API 对应一个 Controller，具体功能由 Controller 实现同时调谐至预期状态。 # Master 节点的高可用一般取决于 Etcd，Etcd 高可用推荐三节点或者五节点，所以 Master 节点通常为三个或者五个，如果接入外部 Etcd 集群，那么 Master 节点可以是偶数个。 # 上图 Kubernetes 使用内部 Etcd，即 Etcd 与 Master 节点个数一致，部署在 Kubernetes 集群中。 # Etcd 集群一般要求三个、五个类似奇数个实例，Etcd 集群选举机制要求集群中半数以上的实例投票选举，如果集群是两个实例，那么一个实例宕机，剩下一个实例没有办法选举。同样四个实例和三个实例实际上效果是一样的。 # Worker # Kubernetes Worker 节点作为容器运行节点，需要部署以下组件： # • **CRI：**容器运行时，管理容器生命周期； # • **Kubelet：**管理 Pod 生命周期，Pod 是 Kubernetes 中最小调度单元； # • **CNI：**容器网络接口，实现 Kubernetes 中 Pod 间网络联通； # • **CSI：**容器存储接口，屏蔽底层存储实现，方便用户使用第三方存储； # • **Kube-proxy：**该组件主要实现多组 Pod 的负载均衡； # 为什么 Kubernetes 需要在容器上之上抽象一个 Pod 资源呢？大部分情况是一个 Pod 对应一个容器，有的场景就需要一个 Pod 对应多个容器，例如日志收集场景，每个 Pod 都会包含一个业务容器和一个日志收集容器，将这两个容器放在一个 Pod 里可用共享日志 Volume。 # Worker 节点的 Kubelet 需要注册到集群中，就需要每个 Worker 节点的 Kubelet 能够连接 Master 节点的 Kube-apiserver。如果集群中 Master 采用高可用部署，就会存在多个 Master，那么 Worker 节点的 Kubelet 就需要同时连接所有的 Kube-apiserver 保证高可用。实现这种高可用的方式有很多种，例如 Haproxy + Keepalived 、Nginx、Envoy 等。上图就是 LB 组件就代表这些实现负载 Kube-apsierver 的组件。 # 创建一个 Pod 需要经历哪些流程 # 当用户创建一个 Deployment 的时候，Kubernetes 中各组件的工作流程是如何的？ # • 用户通过 kubectl 创建一个 Deployment，请求会发给 Kube-apiserver； # • Kube-apiserver 会将 Deployment 的描述信息写入 Etcd 中，Kube-apiserver 将请求结果返回给用户； # • Kube-controller-manager 的 Deployment Controller 从 Kube-apiserver Watch 到 Deployment 的创建事件，并创建一个 ReplicaSet；\n• Kube-apiserver 会将 ReplicaSet 的描述信息写入 Etcd 中； # • Kube-controller-manager 的 ReplicaSet Controller 从 Kube-apiserver Watch 到 ReplicaSet 的创建事件，并创建一个 Pod；\n• Kube-apiserver 会将 Pod 的描述信息写入 Etcd 中； # • Kube-scheduler 从 Kube-apiserver Watch 到 Pod 的创建事件，并根据调度算法从集群中选择一个最优的节点，并更新 Pod 的 nodeName 字段； # • Kube-apiserver 会将 Pod 的更新信息写入 Etcd 中； # • 上述绑定的节点 Kubelet 从 Kube-apiserver Watch 到 Pod 绑定节点是自身，直接调用 CRI 创建容器； # • 结果返回，并写入 Etcd。 # 可以发现，Kubernetes 中各组件基本都是与 Kube-apiserver 进行数据流发送，整体流程非常清晰。\n总结 # Kubernetes 中组件较多，且初学较难理解，需要结合实践才能更深刻地掌握。 # Kubernetes 不管是架构设计还是软件开发设计思想都值得我们去深度思考和学习，都可以应用到平常项目开发中，例如声明式 API 思想、Master + Worker 架构设计等。 # "},{"id":53,"href":"/docs/2024-3-4-cni%E5%89%96%E6%9E%90%E6%BC%94%E8%BF%9B/","title":"2024-3-4 CNI剖析演进","section":"Docs","content":"\n"},{"id":54,"href":"/docs/2024-3-8-%E9%9D%A2%E8%AF%950308/","title":"2024-3-8 面试","section":"Docs","content":" 根据nginx日志 过滤nginx前十访问量的ip # 要根据Nginx日志过滤出前十访问量的IP，你可以使用awk、sort和head命令组合来完成这个任务。假设你的Nginx访问日志格式如下（这是Nginx的默认格式）： # bash复制代码 127.0.0.1 - - [10/Oct/2023:14:05:01 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 612 \u0026#34;-\u0026#34; \u0026#34;curl/7.68.0\u0026#34; 在这个日志中，第一个字段是客户端的IP地址。 # 你可以使用以下命令来过滤出前十访问量的IP： # bash复制代码 awk \u0026#39;{print $1}\u0026#39; access.log | sort | uniq -c | sort -nr | head -n 10 cat /var/log/nginx/access.log |awk \u0026#39;{print $1}\u0026#39;|sort|uniq -c |sort -nr|head -n 10 这个命令的解释如下： # awk '{print $1}' access.log：使用awk命令从access.log文件中提取每行的第一个字段（即IP地址）。 # sort：对提取出的IP地址进行排序。 # uniq -c：统计每个唯一IP地址的出现次数。 # sort -nr：按数字进行逆序排序，这样访问量最多的IP会排在最前面。 # head -n 10：只显示前10行，即访问量最多的前10个IP。 # 请注意，这个命令假设你的Nginx日志文件名是access.log。如果你的日志文件名不同，请相应地替换文件名。此外，这个命令也假设你的日志格式与上面所示的默认格式相匹配。如果你的日志格式不同，你可能需要调整awk命令中的字段选择器。 # cat access.log|awk \u0026#39;{print$1}\u0026#39;|sort|sort -nr|uniq -c|head -n 10 awk的 $nf啥意思 # 在awk命令中，$n是一个字段引用，其中n是一个数字，表示当前行的第n个字段。awk默认使用空白字符（通常是空格或制表符）作为字段分隔符，因此$1、$2、$3等分别代表每行的第一个、第二个、第三个字段，依此类推。 # 在处理Nginx的访问日志时，通常每个字段都是由特定的分隔符（如空格或特殊字符）分隔的。例如，在Nginx的默认日志格式中： # 复制代码\r127.0.0.1 - - [10/Oct/2023:14:05:01 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 612 \u0026#34;-\u0026#34; \u0026#34;curl/7.68.0\u0026#34; $1 通常是客户端的IP地址。 # $2 通常是身份验证的用户名（在这个例子中是 -，表示没有用户名）。 # $3 通常是用户的身份（通常也是 -）。 # $4 是请求时间。 # $5 是请求行（例如 \u0026quot;GET / HTTP/1.1\u0026quot;）。 # $6 是响应状态码（例如 200）。 # $7 是响应体的大小（例如 612）。 # 后续的字段可能包括引用页、用户代理等。 # 当你使用awk '{print $1}' access.log时，你正在告诉awk打印出每一行的第一个字段，即客户端的IP地址。 # awk也允许你通过-F选项来指定一个不同的字段分隔符。例如，如果Nginx日志使用不同的分隔符，你可以这样指定： # bash复制代码 awk -F\u0026#39; \u0026#39; \u0026#39;{print $1}\u0026#39; access.log 这里的-F' '告诉awk使用空格作为字段分隔符。但是，请注意，Nginx日志中的时间戳通常包含在方括号内，这可能会影响字段的计数。如果时间戳是一个字段，并且你想跳过它，你可能需要使用更复杂的awk脚本或者不同的方法来处理日志。\n如果你需要处理更复杂的日志格式，你可能需要调整awk脚本，以正确解析和提取所需的字段。例如，你可以使用正则表达式来匹配和提取特定的字段，或者调整字段分隔符以匹配日志中的实际格式。\nbash 正常返回啥 # 在 Bash 脚本或命令行中，一个命令“正常返回”通常意味着该命令执行成功并且没有错误。当命令成功执行时，它会返回一个退出状态码（exit status code），这个状态码通常是一个介于 0 到 255 之间的整数。在 Unix 和 Linux 系统中，约定俗成的是： # 退出状态码 0 表示命令执行成功。 # 非零的退出状态码表示命令执行出错或遇到了某种问题。不同的非零状态码通常用于指示不同类型的错误。 # 你可以使用 $? 这个特殊变量来获取上一个命令的退出状态码。例如： # #!/bin/bash echo \u0026#34;Running a command...\u0026#34; ls /nonexistentdirectory echo \u0026#34;Exit status: $?\u0026#34; 在这个脚本中，ls /nonexistentdirectory 命令会尝试列出一个不存在的目录的内容，这将导致一个错误，并返回一个非零的退出状态码。然后，echo \u0026quot;Exit status: $?\u0026quot; 会打印出这个状态码。 # 通常，你不需要手动检查每个命令的退出状态码，因为 Bash 脚本会在遇到非零退出状态码时停止执行（除非你在脚本中使用了错误处理机制，如 set -e）。但是，在编写需要处理错误或根据不同退出状态码采取不同动作的复杂脚本时，了解和使用退出状态码是非常重要的。 # 此外，有些命令可能会返回特定的非零状态码来表示不同类型的错误。例如，grep 命令在找不到匹配项时会返回 1，而在遇到其他错误时会返回不同的值。因此，理解你正在使用的命令的文档，以了解它们返回的不同状态码的含义，也是非常重要的。 # k8s创建pod的流程 # 1. 客户端提交 Pod 的配置信息(可以是 yaml 文件定义的信息)到 kube-apiserver.\r2. Apiserver 收到指令后,通知 controllr-manager 创建一个资源对象\r3. controller-manager 通过 apiserver 将 pod 的配置信息存储到 ETCD 数据中心中\r4. kube-scheduler 检查到 pod 信息会开始调度预选,会先过滤不符合 Pod 资源配置要求的节点,然后开始调度调优,主要是挑选出更适合运行的 pod 节点,然后将 pod 的资源配置单发送到 node 节点上的 kubelet 组件上\r5. kubelet 根据 scheduler 发来的资源配置单运行 pod,运行成功后,将 pod 的运行的信息返回 scheduler, scheduler 将返回的 pod 运行状况的信息存储到 etcd 数据中心 dockerfile的copy和add的区别 # Dockerfile中的COPY和ADD指令都用于将文件或目录从构建上下文复制到容器中，但它们之间存在一些关键区别：\r解压缩功能：\rCOPY指令仅用于复制本地文件到镜像，不涉及解压缩操作。\rADD指令除了复制文件外，还具有自动解压缩的功能。如果源路径为URL地址或压缩文件，ADD会尝试自动解压缩文件到目标路径。这意味着使用ADD时，如果源文件是压缩格式（如tar.gz），它会在复制过程中自动解压。\r源文件来源：\rCOPY指令只能从执行docker build所在的主机上读取资源并复制到镜像中。\rADD指令除了能够复制本地文件和目录外，还支持通过URL从远程服务器读取资源并复制到镜像中。这使得ADD在需要从外部源获取文件时非常有用。\r目录创建：\rADD指令在复制文件时，如果目标路径不存在，则会自动创建目标路径。而COPY指令没有这个自动创建路径的功能。\r最佳实践：\r由于COPY指令更简单且直接，通常推荐在只需要复制本地文件到容器中的情况下使用COPY，这样可以避免意外的解压缩行为，也更符合直觉。\rmultistage构建：\r在multistage构建的场景中，COPY指令具有特定的用法。它可以将前一阶段构建的产物拷贝到另一个镜像中。这是ADD指令所不具备的功能。\r综上所述，COPY和ADD指令在Dockerfile中各有其独特之处，选择使用哪个指令取决于具体的需求和场景。在大多数情况下，如果只需要复制本地文件且不需要解压缩或远程获取文件，COPY指令是更好的选择。然而，如果需要从远程URL获取文件或自动解压缩压缩文件，那么ADD指令将更为合适。 helm chart的包怎么打？ # Helm Chart包的打包过程可以通过以下步骤完成：\r安装Helm工具。\r创建一个新的目录，作为Helm包的根目录。\r在根目录中创建一个Chart.yaml文件，用于存储Helm包的元数据。\r在根目录中创建一个templates目录，用于存储Kubernetes资源的模板文件。这些模板文件可以包括各种Kubernetes资源的定义，例如Deployment、Service等。\r在templates目录中编写需要的Kubernetes资源模板文件。这些文件使用Go模板语言编写，可以根据需要引用values.yaml文件中的变量。\r使用Helm工具将模板文件打包成Chart包。这可以通过执行helm package \u0026lt;chart_path\u0026gt;命令完成，其中\u0026lt;chart_path\u0026gt;是包含Chart.yaml和templates目录的Helm包路径。\r打包完成后，你就得到了一个Helm Chart包，它包含了所有用于部署Kubernetes应用的必要文件和模板。你可以将这个包发布到Helm Chart仓库中，以便其他人使用，或者使用Helm工具将其安装到Kubernetes集群中。\r请注意，这只是一个基本的打包过程，实际的步骤可能会根据你的具体需求和Helm的版本有所不同。因此，在打包Helm Chart包之前，建议查阅Helm的官方文档或相关教程以获取更详细和准确的信息。 http的504错误 # 504错误代表网关超时 （Gateway timeout），是指服务器作为网关或代理，但是没有及时从上游服务器收到请求。这通常意味着上游服务器已关闭（不响应网关 / 代理），而不是上游服务器和网关/代理在交换数据的协议上不一致。\r首先，了解什么是网关。\r网络的基本概念：\r客户端:应用 C/S（客户端/服务器） B/S（浏览器/服务器）\r服务器：为客户端提供服务、数据、资源的机器\r请求：客户端向服务器索取数据\r响应：服务器对客户端请求作出反应，一般是返回给客户端数据\r在这之中，把nginx或Apache作为网关。一般服务的架构是：用PHP则是nginx+php的一系列进程，Apache+tomcat+JVM。\r网关超时就与nginx或Apache配置的超时时间，和与php线程、java线程的响应时间有关。以nginx与PHP为例：它的超时配置fastcgi_connect_timeout、fastcgi_send_timeout、fastcgi_read_timeout。nginx将请求丢给PHP来处理，某个PHP的线程响应时间假如是10s，在10s内没有响应给nginx就报超时。这时可以打开PHP慢日志记录，然后排查之。\r另外，数据库的慢查询也会导致504 。nginx只要进程没有死，一般不是nginx的问题。假如场景是：确定程序执行是正确的，比如向数据库插入大量数据，需要5分钟，nginx设置的超时时间是3分钟。这时候可以将超时时间临时设置为大于5分钟。 "},{"id":55,"href":"/docs/2024-4-17-%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93/","title":"2024-4-17 面试总结","section":"Docs","content":"1.查看主机的硬件信息的命令 包括cpu 网卡 显卡这些\r2.查看cpu负载情况 load的数值的含义\r3.优化做过哪些？内核参数优化，具体讲下\r4.nginx上的服务访问不通 怎么排查\r5.502和504的区别\r6.k8s的组件有哪些\r7.kubelet和kube-proxy在主节点上会跑吗\r8.k8s的网络插件用过哪些，calico的BGP模式讲下 1.查看主机的硬件信息的命令 包括cpu 网卡 显卡这些\nCPU信息：\r在终端输入：lscpu\r显卡信息：\rcat /etc/cpuinfo 或 cat /proc/cpuinfo\r使用命令：lspci | grep VGA （通常用于PCIe接口的显卡）\r或者 lshw -c display 获取更详细的显卡信息\r网卡信息：\r使用命令：lspci | grep Ethernet （对于PCIe接口的网卡）\r或者 ifconfig -a 或 ip addr show 查看网络接口及其IP配置\r若要获取更详细的硬件信息，可以使用 ethtool -i [interface_name]，其中 [interface_name] 是你想要查询的网卡名称。 2.查看cpu负载情况 load的数值的含义\n12:34:56 up 1 day, 3:23, 3 users, load average: 0.23, 0.25, 0.28\r其中，“load average”后面的三个数字分别表示过去1分钟、5分钟和15分钟的平均负载。这些数值表示的是在这段时间内，系统中处于运行（包括正在运行和等待运行）状态的平均进程数（或线程数）。\r负载数值的含义：\r单核CPU情况下，当负载平均值为1.0时，意味着CPU在这段时间里一直忙于处理进程，没有闲置时间，达到了理论上的满载状态。\r多核CPU环境中，负载平均值应当参照CPU核心数量。例如，对于一个8核CPU来说，理想的负载应该低于8.0，即所有核心都被充分利用且无进程等待。不过，实际上，由于进程并非总是能够完全并行处理，所以理想的负载水平往往低于实际核心数，一般认为是核心数的0.7倍左右较为健康。\r解读负载值：\r负载值小于1表明系统整体上是轻载的，CPU有足够的处理能力和时间处理新的请求。\r负载值接近或超过CPU核心数，意味着系统可能正面临资源瓶颈，有较多的进程在等待CPU时间片分配，可能会出现性能下降或响应延迟。\r长时间持续较高的负载可能意味着系统配置不足或者有进程出现了性能问题，需要进一步排查优化。 3.优化做过哪些？内核参数优化，具体讲下\n内核参数优化是Linux系统管理员进行系统性能调优的重要手段之一，主要目的是为了让内核更好地适应特定的应用场景和硬件环境，从而提升系统性能、稳定性和安全性。下面是一些具体的内核参数优化方向和实例：\r1. 处理器调度相关参数\rkernel.sched_migration_cost_ns: 进程迁移成本，影响CPU亲和性策略下的任务调度决策。\rkernel.sched_child_runs_first: 控制子进程是否优先执行，对于某些实时性要求高的应用程序可能需要调整。\rkernel.sched_min_granularity_ns 和 kernel.sched_latency_ns: 定义调度器最小粒度和目标延迟，影响任务切换频率。\r2. 内存管理参数\rvm.swappiness: 控制系统在物理内存不足时多快地开始交换内存页到磁盘。较小的值倾向于保持更多的内存数据不被换出。\rvm.dirty_ratio 和 vm.dirty_background_ratio: 控制内核脏页缓存的比例，影响I/O性能和数据同步策略。\r3. 文件系统和I/O相关参数\rfs.file-max: 系统允许的最大同时打开文件描述符数量，对高并发服务如数据库和Web服务器尤其重要。\rvm.max_map_count: 最大可映射区域数量，对于Elasticsearch等依赖大量内存映射的应用很重要。\rfs.inotify.max_user_watches, fs.inotify.max_queued_events 和 fs.inotify.max_user_instances: 优化inotify事件监听机制，对需要监控大量文件变化的应用有用。\r4. 网络栈相关参数\rnet.ipv4.tcp_syncookies: 控制是否启用SYN Cookies，有助于缓解SYN洪泛攻击。\rnet.ipv4.tcp_max_tw_buckets: 控制TIME_WAIT状态连接的最大数量，对高并发短连接场景进行优化。\rnet.core.somaxconn: 套接字监听队列的最大长度，影响服务器接受新连接的能力。\r5. TCP/IP参数\rnet.ipv4.tcp_keepalive_time: TCP连接保持活动探测的时间间隔。\rnet.ipv4.tcp_fin_timeout 和 net.ipv4.tcp_syn_retries: 控制TCP连接关闭和建立过程中的超时次数。\rnet.ipv4.tcp_window_scaling 和 net.ipv4.tcp_timestamps: 开启TCP窗口缩放和时间戳选项，有利于提高带宽利用率。\r6. 其他性能和安全相关参数\rkernel.pid_max: 系统允许的最大PID数量。\rkernel.shmmax 和 kernel.shmall: 控制共享内存段的最大大小和页数，对依赖共享内存的应用有意义。\r进行内核参数优化时，需结合实际应用场景分析系统瓶颈，不应盲目调整，否则可能导致系统不稳定或无法达到预期效果。调整后，通常会通过sysctl命令动态修改，并持久化至/etc/sysctl.conf文件中，确保重启后依然生效。对于某些关键系统服务，还应配合工具如systemd-tuned或sysctl.d目录下的配置文件进行更加精细化的调优。\r#modify the value of kernel.shmmax\rsed -i \u0026#39;s/kernel\\.shmmax/#kernel\\.shmmax/\u0026#39; /etc/sysctl.conf\rsed -i \u0026#34;/#kernel\\.shmmax/a \\kernel.shmmax = $half_mem\u0026#34; /etc/sysctl.conf\recho \u0026#34;kernel.shmmax = $half_mem\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.conf\recho \u0026#34;kernel.shmall = 2097152\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.conf\recho \u0026#34;kernel.pid_max = 65535\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.conf\recho \u0026#34;net.ipv4.neigh.default.gc_thresh1 = 4096\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.conf\recho \u0026#34;net.ipv4.neigh.default.gc_thresh2 = 8192\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.conf\recho \u0026#34;net.ipv4.neigh.default.gc_thresh3 = 8192\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.conf\recho \u0026#34;kernel.shmmni = 4096\u0026#34; \u0026gt;\u0026gt;/etc/sysctl.conf\recho \u0026#34;kernel.sem = 250 32000 100 128\u0026#34; \u0026gt;\u0026gt;/etc/sysctl.conf\recho \u0026#34;kernel.shmmni = 4096\u0026#34; \u0026gt;\u0026gt;/etc/sysctl.conf\recho \u0026#34;kernel.sem = 250 32000 100 128\u0026#34; \u0026gt;\u0026gt;/etc/sysctl.conf\r#set the other kernel parameters as follows:\recho \u0026#34;fs.aio-max-nr = 1048576\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.conf\recho \u0026#34;fs.file-max = 6815744\u0026#34; \u0026gt;\u0026gt;/etc/sysctl.conf\recho \u0026#34;net.ipv4.ip_local_port_range = 9000 65500\u0026#34;\u0026gt;\u0026gt;/etc/sysctl.conf\recho \u0026#34;net.core.rmem_default = 262144\u0026#34; \u0026gt;\u0026gt;/etc/sysctl.conf\recho \u0026#34;net.core.rmem_max = 4194304\u0026#34; \u0026gt;\u0026gt;/etc/sysctl.conf\recho \u0026#34;net.core.wmem_default = 262144\u0026#34; \u0026gt;\u0026gt;/etc/sysctl.conf\recho \u0026#34;net.core.wmem_max = 1048586\u0026#34; \u0026gt;\u0026gt;/etc/sysctl.conf\recho \u0026#34;net.ipv4.ip_forward=1\u0026#34; \u0026gt;\u0026gt;/etc/sysctl.conf\recho \u0026#34;net.bridge.bridge-nf-call-iptables=1\u0026#34; \u0026gt;\u0026gt;/etc/sysctl.conf\recho \u0026#34;net.bridge.bridge-nf-call-ip6tables=1\u0026#34; \u0026gt;\u0026gt;/etc/sysctl.conf\r这段脚本是对Linux系统内核参数进行设置，通过修改/etc/sysctl.conf文件，使得重启后这些参数也能生效。以下是各项参数的含义：\rsed -i \u0026#39;s/kernel.shmmax/#kernel.shmmax/\u0026#39; /etc/sysctl.conf：注释掉原有的kernel.shmmax配置项。shmmax是系统允许单个共享内存段的最大大小（单位通常是KB）。如果之前已存在该配置，则将其注释掉以便重新定义。\rsed -i \u0026#34;/#kernel.shmmax/a \\kernel.shmmax = $half_mem\u0026#34; /etc/sysctl.conf：在注释后的同一行添加新的kernel.shmmax配置，其值等于变量$half_mem，假设这个变量存储了系统一半的内存大小作为共享内存最大值。\r后续的echo语句都是向/etc/sysctl.conf追加新的配置项：\rkernel.shmall：决定系统范围内共享内存页面的数量，这里的值设为2097152。\rkernel.pid_max：系统支持的最大进程ID数，设为65535。\rnet.ipv4.neigh.default.gc_thresh1，gc_thresh2和gc_thresh3：IPv4邻居表条目的垃圾回收阈值，分别设定为4096，8192和8192，用于控制ARP高速缓存表的清理策略。\rkernel.shmmni：系统范围内共享内存段的最大数目，设为4096。\rkernel.sem：信号量的四个参数，分别为SEMMSL(每个信号量集合中信号量的最大数目)、SEMMNS(系统范围内的信号量最大数目)、SEMOPM(单个semop系统调用中操作数的最大数目)、SEMMNI(信号量集合的最大数目)，这里设为250, 32000, 100, 128。\rfs.aio-max-nr：异步I/O挂起操作的最大数量，设为1048576。\rfs.file-max：系统允许的最大文件句柄数，设为6815744。\rnet.ipv4.ip_local_port_range：本地端口范围，用于向外发起连接时可用的源端口范围，设为9000到65500。\rnet.core.rmem_default和net.core.rmem_max：接收缓冲区默认大小和最大大小，单位字节，分别设为262144和4194304。\rnet.core.wmem_default和net.core.wmem_max：发送缓冲区默认大小和最大大小，单位字节，分别设为262144和1048586。\rnet.ipv4.ip_forward：开启IP转发功能，设为1，表示允许本机做为路由器进行IP包转发。\rnet.bridge.bridge-nf-call-iptables和net.bridge.bridge-nf-call-ip6tables：允许在桥接模式下应用iptables规则，这对于虚拟化环境和容器环境中的网络过滤十分重要。\r以上参数的设置需要根据实际系统需求和应用场景进行合理配置，过大的值可能会浪费系统资源，而过小的值则可能导致性能瓶颈。 4.nginx上的服务访问不通 怎么排查\n如果 Nginx 上的服务无法访问，可能有多个原因，导致请求无法成功到达或响应。下面是一些常见的排查步骤，帮助你找出并解决问题。\n1. 检查 Nginx 配置文件 # 首先，确认 Nginx 配置是否正确，常见的问题包括端口未正确绑定、server_name 设置不正确、反向代理配置错误等。\n（1）检查 Nginx 配置文件 # Nginx 的默认配置文件通常位于 /etc/nginx/nginx.conf 或 /etc/nginx/sites-available/default（具体路径取决于你的操作系统和安装方式）。确保配置文件中没有语法错误。\n你可以运行以下命令来检查 Nginx 配置文件的语法：\nsudo nginx -t 如果有错误，Nginx 会给出详细的错误信息。解决配置问题后，重新加载配置：\nsudo systemctl reload nginx （2）检查 server_name 和监听端口 # 确保你的配置文件中有正确的 server_name（即域名或 IP 地址）和端口号。例如：\nserver { listen 80; server_name example.com; # 确保这个地址是正确的 location / { proxy_pass http://127.0.0.1:3000; # 正确设置代理的目标服务 } } （3）检查代理服务配置 # 如果 Nginx 作为反向代理，确保代理目标服务的地址和端口正确，且服务正在运行。\n2. 检查防火墙设置 # 防火墙设置可能阻止外部访问 Nginx 或应用服务的端口。\n（1）检查 Nginx 端口是否开放 # 确保防火墙允许通过 Nginx 使用的端口（通常是 80 和 443）。\n对于\nufw\n防火墙：\nsudo ufw allow \u0026#39;Nginx Full\u0026#39; sudo ufw enable 对于\nfirewalld\n：\nsudo firewall-cmd --permanent --add-service=http sudo firewall-cmd --permanent --add-service=https sudo firewall-cmd --reload （2）检查端口是否被占用 # 运行以下命令检查 Nginx 是否在监听指定的端口：\nsudo netstat -tulnp | grep nginx 或者使用：\nsudo ss -tulnp | grep nginx 3. 检查应用服务是否运行 # 如果 Nginx 配置了反向代理，确保目标应用服务正在运行。\n（1）检查应用服务 # 检查应用程序（如 Node.js、Python 等）是否在指定端口上运行。例如，对于 Node.js，可以运行：\nps aux | grep node 或者检查应用服务的状态：\nsudo systemctl status \u0026lt;your-app-service\u0026gt; （2）检查应用日志 # 查看应用服务的日志，确保没有报错。应用的日志文件路径通常在配置文件中指定，常见的日志文件包括：\n/var/log/nginx/access.log /var/log/nginx/error.log 应用的日志文件（如 /var/log/myapp.log） 4. 检查 Nginx 和应用服务的日志 # 查看 Nginx 错误日志可以帮助定位访问问题。\n（1）查看 Nginx 错误日志 # Nginx 错误日志通常位于 /var/log/nginx/error.log。查看日志中是否有任何明显的错误信息，例如连接超时、权限问题等：\nsudo tail -f /var/log/nginx/error.log （2）查看 Nginx 访问日志 # 访问日志可以帮助你确认请求是否成功到达 Nginx，以及是否成功被处理：\nsudo tail -f /var/log/nginx/access.log 5. 测试网络连接 # 使用 curl 或 wget 等工具测试从本地或外部访问 Nginx。\n（1）从本地访问 # 可以使用 curl 测试本地 Nginx 服务是否能正常访问：\ncurl http://localhost 如果本地访问没有问题，但外部访问无法连接，可能是防火墙、网络设置或端口配置问题。\n（2）从外部访问 # 如果你的服务器有公网 IP，可以尝试从外部机器访问：\ncurl http://your-server-ip 6. 检查 Nginx 服务状态 # 检查 Nginx 服务是否正常运行：\nsudo systemctl status nginx 如果 Nginx 没有运行，可以尝试重新启动服务：\nsudo systemctl restart nginx 7. 检查 DNS 配置 # 如果你使用域名来访问 Nginx 服务，确保 DNS 解析正确。你可以使用 nslookup 或 dig 命令检查 DNS 是否正确解析到服务器 IP。\n（1）使用 nslookup 检查域名解析 # nslookup example.com （2）使用 dig 命令 # dig example.com 总结 # 通过这些排查步骤，你应该能找出为什么 Nginx 服务无法访问的原因。常见的原因包括：\n配置错误（例如端口绑定、反向代理错误） 防火墙或端口未开放 应用服务未运行或配置错误 DNS 配置问题 逐步检查并解决这些问题，通常可以恢复服务的正常访问。如果需要更具体的帮助，可以提供错误日志或配置详情，我可以帮助你进一步分析。\n"},{"id":56,"href":"/docs/2024-8-1-kubernetes%E9%9D%A2%E8%AF%95%E9%A2%98/","title":"2024-8-1 k8s面试题","section":"Docs","content":" 简述ETCD及其特点？ # etcd 是 CoreOS 团队发起的开源项目，是一个管理配置信息和服务发现（service discovery）的项目，它的目标是构建一个高可用的分布式键值（key-value）数据库，基于 Go 语言实现。\n特点：\n简单：支持 REST 风格的 HTTP+JSON API 安全：支持 HTTPS 方式的访问 快速：支持并发 1k/s 的写操作 可靠：支持分布式结构，基于 Raft 的一致性算法，Raft 是一套通过选举主节点来实现分布式系统一致性的算法。 简述ETCD适应的场景？ # etcd基于其优秀的特点，可广泛的应用于以下场景：\n服务发现(Service Discovery)：服务发现主要解决在同一个分布式集群中的进程或服务，要如何才能找到对方并建立连接。本质上来说，服务发现就是想要了解集群中是否有进程在监听udp或tcp端口，并且通过名字就可以查找和连接。\n消息发布与订阅：在分布式系统中，最适用的一种组件间通信方式就是消息发布与订阅。即构建一个配置共享中心，数据提供者在这个配置中心发布消息，而消息使用者则订阅他们关心的主题，一旦主题有消息发布，就会实时通知订阅者。通过这种方式可以做到分布式系统配置的集中式管理与动态更新。应用中用到的一些配置信息放到etcd上进行集中管理。\n负载均衡：在分布式系统中，为了保证服务的高可用以及数据的一致性，通常都会把数据和服务部署多份，以此达到对等服务，即使其中的某一个服务失效了，也不影响使用。etcd本身分布式架构存储的信息访问支持负载均衡。etcd集群化以后，每个etcd的核心节点都可以处理用户的请求。所以，把数据量小但是访问频繁的消息数据直接存储到etcd中也可以实现负载均衡的效果。\n分布式通知与协调：与消息发布和订阅类似，都用到了etcd中的Watcher机制，通过注册与异步通知机制，实现分布式环境下不同系统之间的通知与协调，从而对数据变更做到实时处理。\n分布式锁：因为etcd使用Raft算法保持了数据的强一致性，某次操作存储到集群中的值必然是全局一致的，所以很容易实现分布式锁。锁服务有两种使用方式，一是保持独占，二是控制时序。\n集群监控与Leader竞选：通过etcd来进行监控实现起来非常简单并且实时性强。\n简述什么是Kubernetes？ # Kubernetes是一个全新的基于容器技术的分布式系统支撑平台。是Google开源的容器集群管理系统（谷歌内部:Borg）。在Docker技术的基础上，为容器化的应用提供部署运行、资源调度、服务发现和动态伸缩等一系列完整功能，提高了大规模容器集群管理的便捷性。并且具有完备的集群管理能力，多层次的安全防护和准入机制、多租户应用支撑能力、透明的服务注册和发现机制、內建智能负载均衡器、强大的故障发现和自我修复能力、服务滚动升级和在线扩容能力、可扩展的资源自动调度机制以及多粒度的资源配额管理能力。\n简述Kubernetes和Docker的关系？ # Docker 提供容器的生命周期管理和，Docker 镜像构建运行时容器。它的主要优点是将将软件/应用程序运行所需的设置和依赖项打包到一个容器中，从而实现了可移植性等优点。\nKubernetes 用于关联和编排在多个主机上运行的容器。\n简述Kubernetes中什么是Minikube、Kubectl、Kubelet？ # Minikube 是一种可以在本地轻松运行一个单节点 Kubernetes 群集的工具。\nKubectl 是一个命令行工具，可以使用该工具控制Kubernetes集群管理器，如检查群集资源，创建、删除和更新组件，查看应用程序。\nKubelet 是一个代理服务，它在每个节点上运行，并使从服务器与主服务器通信。\n简述Kubernetes常见的部署方式？ # 常见的Kubernetes部署方式有：\nkubeadm：也是推荐的一种部署方式； 二进制：网页上很多教程，未来我也会写一个 minikube：在本地轻松运行一个单节点 Kubernetes 群集的工具。 简述Kubernetes如何实现集群管理？ # 在集群管理方面，Kubernetes将集群中的机器划分为一个Master节点和一群工作节点Node。其中，在Master节点运行着集群管理相关的一组进程kube-apiserver、kube-controller-manager和kube-scheduler，这些进程实现了整个集群的资源管理、Pod调度、弹性伸缩、安全控制、系统监控和纠错等管理能力，并且都是全自动完成的。\n简述Kubernetes的优势、适应场景及其特点？\nKubernetes作为一个完备的分布式系统支撑平台，其主要优势：\n容器编排 轻量级 开源 弹性伸缩 负载均衡 Kubernetes常见场景：\n快速部署应用 快速扩展应用 无缝对接新的应用功能 节省资源，优化硬件资源的使用 Kubernetes相关特点：\n可移植: 支持公有云、私有云、混合云、多重云（multi-cloud）。 可扩展: 模块化,、插件化、可挂载、可组合。 自动化: 自动部署、自动重启、自动复制、自动伸缩/扩展。 简述Kubernetes的缺点或当前的不足之处？ # Kubernetes当前存在的缺点（不足）如下：\n安装过程和配置相对困难复杂。 管理服务相对繁琐。 运行和编译需要很多时间。 它比其他替代品更昂贵。 对于简单的应用程序来说，可能不需要涉及Kubernetes即可满足。 简述Kubernetes相关基础概念？ # master：k8s集群的管理节点，负责管理集群，提供集群的资源数据访问入口。拥有Etcd存储服务（可选），运行Api Server进程，Controller Manager服务进程及Scheduler服务进程。\nnode（worker）：Node（worker）是Kubernetes集群架构中运行Pod的服务节点，是Kubernetes集群操作的单元，用来承载被分配Pod的运行，是Pod运行的宿主机。运行docker eninge服务，守护进程kunelet及负载均衡器kube-proxy。\npod：运行于Node节点上，若干相关容器的组合。Pod内包含的容器运行在同一宿主机上，使用相同的网络命名空间、IP地址和端口，能够通过localhost进行通信。Pod是Kurbernetes进行创建、调度和管理的最小单位，它提供了比容器更高层次的抽象，使得部署和管理更加灵活。一个Pod可以包含一个容器或者多个相关容器。\nlabel：Kubernetes中的Label实质是一系列的Key/Value键值对，其中key与value可自定义。Label可以附加到各种资源对象上，如Node、Pod、Service、RC等。一个资源对象可以定义任意数量的Label，同一个Label也可以被添加到任意数量的资源对象上去。Kubernetes通过Label Selector（标签选择器）查询和筛选资源对象。\nReplication Controller：Replication Controller用来管理Pod的副本，保证集群中存在指定数量的Pod副本。集群中副本的数量大于指定数量，则会停止指定数量之外的多余容器数量。反之，则会启动少于指定数量个数的容器，保证数量不变。Replication Controller是实现弹性伸缩、动态扩容和滚动升级的核心。\nDeployment：Deployment在内部使用了RS来实现目的，Deployment相当于RC的一次升级，其最大的特色为可以随时获知当前Pod的部署进度。\nHPA（Horizontal Pod Autoscaler）：Pod的横向自动扩容，也是Kubernetes的一种资源，通过追踪分析RC控制的所有Pod目标的负载变化情况，来确定是否需要针对性的调整Pod副本数量。\nService：Service定义了Pod的逻辑集合和访问该集合的策略，是真实服务的抽象。Service提供了一个统一的服务访问入口以及服务代理和发现机制，关联多个相同Label的Pod，用户不需要了解后台Pod是如何运行。\nVolume：Volume是Pod中能够被多个容器访问的共享目录，Kubernetes中的Volume是定义在Pod上，可以被一个或多个Pod中的容器挂载到某个目录下。\nNamespace：Namespace用于实现多租户的资源隔离，可将集群内部的资源对象分配到不同的Namespace中，形成逻辑上的不同项目、小组或用户组，便于不同的Namespace在共享使用整个集群的资源的同时还能被分别管理。\n简述Kubernetes集群相关组件？ # Kubernetes Master控制组件，调度管理整个系统（集群），包含如下组件:\nKubernetes API Server：作为Kubernetes系统的入口，其封装了核心对象的增删改查操作，以RESTful API接口方式提供给外部客户和内部组件调用，集群内各个功能模块之间数据交互和通信的中心枢纽。\nKubernetes Scheduler：为新建立的Pod进行节点(node)选择(即分配机器)，负责集群的资源调度。\nKubernetes Controller：负责执行各种控制器，目前已经提供了很多控制器来保证Kubernetes的正常运行。\nReplication Controller：管理维护Replication Controller，关联Replication Controller和Pod，保证Replication Controller定义的副本数量与实际运行Pod数量一致。\nNode Controller：管理维护Node，定期检查Node的健康状态，标识出(失效|未失效)的Node节点。\nNamespace Controller：管理维护Namespace，定期清理无效的Namespace，包括Namesapce下的API对象，比如Pod、Service等。\nService Controller：管理维护Service，提供负载以及服务代理。\nEndPoints Controller：管理维护Endpoints，关联Service和Pod，创建Endpoints为Service的后端，当Pod发生变化时，实时更新Endpoints。\nService Account Controller：管理维护Service Account，为每个Namespace创建默认的Service Account，同时为Service Account创建Service Account Secret。\nPersistent Volume Controller：管理维护Persistent Volume和Persistent Volume Claim，为新的Persistent Volume Claim分配Persistent Volume进行绑定，为释放的Persistent Volume执行清理回收。\nDaemon Set Controller：管理维护Daemon Set，负责创建Daemon Pod，保证指定的Node上正常的运行Daemon Pod。\nDeployment Controller：管理维护Deployment，关联Deployment和Replication Controller，保证运行指定数量的Pod。当Deployment更新时，控制实现Replication Controller和Pod的更新。\nJob Controller：管理维护Job，为Jod创建一次性任务Pod，保证完成Job指定完成的任务数目\nPod Autoscaler Controller：实现Pod的自动伸缩，定时获取监控数据，进行策略匹配，当满足条件时执行Pod的伸缩动作。\n简述Kubernetes RC的机制？ # Replication Controller用来管理Pod的副本，保证集群中存在指定数量的Pod副本。当定义了RC并提交至Kubernetes集群中之后，Master节点上的Controller Manager组件获悉，并同时巡检系统中当前存活的目标Pod，并确保目标Pod实例的数量刚好等于此RC的期望值，若存在过多的Pod副本在运行，系统会停止一些Pod，反之则自动创建一些Pod。\n简述Kubernetes Replica Set 和 Replication Controller 之间有什么区别？\nReplica Set 和 Replication Controller 类似，都是确保在任何给定时间运行指定数量的 Pod 副本。不同之处在于RS 使用基于集合的选择器，而 Replication Controller 使用基于权限的选择器。\n简述kube-proxy作用？ # kube-proxy 运行在所有节点上，它监听 apiserver 中 service 和 endpoint 的变化情况，创建路由规则以提供服务 IP 和负载均衡功能。简单理解此进程是Service的透明代理兼负载均衡器，其核心功能是将到某个Service的访问请求转发到后端的多个Pod实例上。\n简述kube-proxy iptables原理？ # Kubernetes从1.2版本开始，将iptables作为kube-proxy的默认模式。iptables模式下的kube-proxy不再起到Proxy的作用，其核心功能：通过API Server的Watch接口实时跟踪Service与Endpoint的变更信息，并更新对应的iptables规则，Client的请求流量则通过iptables的NAT机制“直接路由”到目标Pod。\n简述kube-proxy ipvs原理？ # IPVS在Kubernetes1.11中升级为GA稳定版。IPVS则专门用于高性能负载均衡，并使用更高效的数据结构（Hash表），允许几乎无限的规模扩张，因此被kube-proxy采纳为最新模式。\n在IPVS模式下，使用iptables的扩展ipset，而不是直接调用iptables来生成规则链。iptables规则链是一个线性的数据结构，ipset则引入了带索引的数据结构，因此当规则很多时，也可以很高效地查找和匹配。\n可以将ipset简单理解为一个IP（段）的集合，这个集合的内容可以是IP地址、IP网段、端口等，iptables可以直接添加规则对这个“可变的集合”进行操作，这样做的好处在于可以大大减少iptables规则的数量，从而减少性能损耗。\n简述kube-proxy ipvs和iptables的异同？ # iptables与IPVS都是基于Netfilter实现的，但因为定位不同，二者有着本质的差别：iptables是为防火墙而设计的；IPVS则专门用于高性能负载均衡，并使用更高效的数据结构（Hash表），允许几乎无限的规模扩张。\n与iptables相比，IPVS拥有以下明显优势：\n1、为大型集群提供了更好的可扩展性和性能； 2、支持比iptables更复杂的复制均衡算法（最小负载、最少连接、加权等）； 3、支持服务器健康检查和连接重试等功能； 4、可以动态修改ipset的集合，即使iptables的规则正在使用这个集合。 简述Kubernetes中什么是静态Pod？ # 静态pod是由kubelet进行管理的仅存在于特定Node的Pod上，他们不能通过API Server进行管理，无法与ReplicationController、Deployment或者DaemonSet进行关联，并且kubelet无法对他们进行健康检查。静态Pod总是由kubelet进行创建，并且总是在kubelet所在的Node上运行。\n简述Kubernetes中Pod可能位于的状态？ # Pending：API Server已经创建该Pod，且Pod内还有一个或多个容器的镜像没有创建，包括正在下载镜像的过程。\nRunning：Pod内所有容器均已创建，且至少有一个容器处于运行状态、正在启动状态或正在重启状态。\nSucceeded：Pod内所有容器均成功执行退出，且不会重启。\nFailed：Pod内所有容器均已退出，但至少有一个容器退出为失败状态。\nUnknown：由于某种原因无法获取该Pod状态，可能由于网络通信不畅导致。\n简述Kubernetes创建一个Pod的主要流程？ # Kubernetes中创建一个Pod涉及多个组件之间联动，主要流程如下：\n1、客户端提交Pod的配置信息（可以是yaml文件定义的信息）到kube-apiserver。 2、Apiserver收到指令后，通知给controller-manager创建一个资源对象。 3、Controller-manager通过api-server将pod的配置信息存储到ETCD数据中心中。 4、Kube-scheduler检测到pod信息会开始调度预选，会先过滤掉不符合Pod资源配置要求的节点，然后开始调度调优，主要是挑选出更适合运行pod的节点，然后将pod的资源配置单发送到node节点上的kubelet组件上。 5、Kubelet根据scheduler发来的资源配置单运行pod，运行成功后，将pod的运行信息返回给scheduler，scheduler将返回的pod运行状况的信息存储到etcd数据中心。 简述Kubernetes中Pod的重启策略？ # Pod重启策略（RestartPolicy）应用于Pod内的所有容器，并且仅在Pod所处的Node上由kubelet进行判断和重启操作。当某个容器异常退出或者健康检查失败时，kubelet将根据RestartPolicy的设置来进行相应操作。\nPod的重启策略包括Always、OnFailure和Never，默认值为Always。\nAlways：当容器失效时，由kubelet自动重启该容器； OnFailure：当容器终止运行且退出码不为0时，由kubelet自动重启该容器； Never：不论容器运行状态如何，kubelet都不会重启该容器。 同时Pod的重启策略与控制方式关联，当前可用于管理Pod的控制器包括ReplicationController、Job、DaemonSet及直接管理kubelet管理（静态Pod）。\n不同控制器的重启策略限制如下：\nRC和DaemonSet：必须设置为Always，需要保证该容器持续运行； Job：OnFailure或Never，确保容器执行完成后不再重启； kubelet：在Pod失效时重启，不论将RestartPolicy设置为何值，也不会对Pod进行健康检查。 简述Kubernetes中Pod的健康检查方式？ # 对Pod的健康检查可以通过两类探针来检查：LivenessProbe和ReadinessProbe。\nLivenessProbe探针：用于判断容器是否存活（running状态），如果LivenessProbe探针探测到容器不健康，则kubelet将杀掉该容器，并根据容器的重启策略做相应处理。若一个容器不包含LivenessProbe探针，kubelet认为该容器的LivenessProbe探针返回值用于是“Success”。\nReadineeProbe探针：用于判断容器是否启动完成（ready状态）。如果ReadinessProbe探针探测到失败，则Pod的状态将被修改。Endpoint Controller将从Service的Endpoint中删除包含该容器所在Pod的Eenpoint。\nstartupProbe探针：启动检查机制，应用一些启动缓慢的业务，避免业务长时间启动而被上面两类探针kill掉。\n简述Kubernetes Pod的LivenessProbe探针的常见方式？ # kubelet定期执行LivenessProbe探针来诊断容器的健康状态，通常有以下三种方式：\nExecAction：在容器内执行一个命令，若返回码为0，则表明容器健康。\nTCPSocketAction：通过容器的IP地址和端口号执行TCP检查，若能建立TCP连接，则表明容器健康。\nHTTPGetAction：通过容器的IP地址、端口号及路径调用HTTP Get方法，若响应的状态码大于等于200且小于400，则表明容器健康。\n简述Kubernetes Pod的常见调度方式？ # Kubernetes中，Pod通常是容器的载体，主要有如下常见调度方式：\nDeployment或RC：该调度策略主要功能就是自动部署一个容器应用的多份副本，以及持续监控副本的数量，在集群内始终维持用户指定的副本数量。 NodeSelector：定向调度，当需要手动指定将Pod调度到特定Node上，可以通过Node的标签（Label）和Pod的nodeSelector属性相匹配。 NodeAffinity亲和性调度：亲和性调度机制极大的扩展了Pod的调度能力，目前有两种节点亲和力表达： requiredDuringSchedulingIgnoredDuringExecution：硬规则，必须满足指定的规则，调度器才可以调度Pod至Node上（类似nodeSelector，语法不同）。 preferredDuringSchedulingIgnoredDuringExecution：软规则，优先调度至满足的Node的节点，但不强求，多个优先级规则还可以设置权重值。 Taints和Tolerations（污点和容忍）： Taint：使Node拒绝特定Pod运行； Toleration：为Pod的属性，表示Pod能容忍（运行）标注了Taint的Node。 简述Kubernetes初始化容器（init container）？ # init container的运行方式与应用容器不同，它们必须先于应用容器执行完成，当设置了多个init container时，将按顺序逐个运行，并且只有前一个init container运行成功后才能运行后一个init container。当所有init container都成功运行后，Kubernetes才会初始化Pod的各种信息，并开始创建和运行应用容器。\n简述Kubernetes deployment升级过程？ # 初始创建Deployment时，系统创建了一个ReplicaSet，并按用户的需求创建了对应数量的Pod副本。 当更新Deployment时，系统创建了一个新的ReplicaSet，并将其副本数量扩展到1，然后将旧ReplicaSet缩减为2。 之后，系统继续按照相同的更新策略对新旧两个ReplicaSet进行逐个调整。 最后，新的ReplicaSet运行了对应个新版本Pod副本，旧的ReplicaSet副本数量则缩减为0。 简述Kubernetes deployment升级策略？ # 在Deployment的定义中，可以通过spec.strategy指定Pod更新的策略，目前支持两种策略：Recreate（重建）和RollingUpdate（滚动更新），默认值为RollingUpdate。\nRecreate：设置spec.strategy.type=Recreate，表示Deployment在更新Pod时，会先杀掉所有正在运行的Pod，然后创建新的Pod。\nRollingUpdate：设置spec.strategy.type=RollingUpdate，表示Deployment会以滚动更新的方式来逐个更新Pod。同时，可以通过设置spec.strategy.rollingUpdate下的两个参数（maxUnavailable和maxSurge）来控制滚动更新的过程。\n简述Kubernetes DaemonSet类型的资源特性？ # DaemonSet资源对象会在每个Kubernetes集群中的节点上运行，并且每个节点只能运行一个pod，这是它和deployment资源对象的最大也是唯一的区别。因此，在定义yaml文件中，不支持定义replicas。\n它的一般使用场景如下：\n在去做每个节点的日志收集工作。 监控每个节点的的运行状态。 简述Kubernetes自动扩容机制？ # Kubernetes使用Horizontal Pod Autoscaler（HPA）的控制器实现基于CPU使用率进行自动Pod扩缩容的功能。HPA控制器周期性地监测目标Pod的资源性能指标，并与HPA资源对象中的扩缩容条件进行对比，在满足条件时对Pod副本数量进行调整。\nHPA原理\nKubernetes中的某个Metrics Server（Heapster或自定义Metrics Server）持续采集所有Pod副本的指标数据。HPA控制器通过Metrics Server的API（Heapster的API或聚合API）获取这些数据，基于用户定义的扩缩容规则进行计算，得到目标Pod副本数量。\n当目标Pod副本数量与当前副本数量不同时，HPA控制器就向Pod的副本控制器（Deployment、RC或ReplicaSet）发起scale操作，调整Pod的副本数量，完成扩缩容操作。\n简述Kubernetes Service类型？ # 通过创建Service，可以为一组具有相同功能的容器应用提供一个统一的入口地址，并且将请求负载分发到后端的各个容器应用上。其主要类型有：\nClusterIP：虚拟的服务IP地址，该地址用于Kubernetes集群内部的Pod访问，在Node上kube-proxy通过设置的iptables规则进行转发； NodePort：使用宿主机的端口，使能够访问各Node的外部客户端通过Node的IP地址和端口号就能访问服务； LoadBalancer：使用外接负载均衡器完成到服务的负载分发，需要在spec.status.loadBalancer字段指定外部负载均衡器的IP地址，通常用于公有云。 简述Kubernetes Service分发后端的策略？ # Service负载分发的策略有：RoundRobin和SessionAffinity\nRoundRobin：默认为轮询模式，即轮询将请求转发到后端的各个Pod上。 SessionAffinity：基于客户端IP地址进行会话保持的模式，即第1次将某个客户端发起的请求转发到后端的某个Pod上，之后从相同的客户端发起的请求都将被转发到后端相同的Pod上。 简述Kubernetes Headless Service？ # 在某些应用场景中，若需要人为指定负载均衡器，不使用Service提供的默认负载均衡的功能，或者应用程序希望知道属于同组服务的其他实例。Kubernetes提供了Headless Service来实现这种功能，即不为Service设置ClusterIP（入口IP地址），仅通过Label Selector将后端的Pod列表返回给调用的客户端。\n简述Kubernetes外部如何访问集群内的服务？ # 对于Kubernetes，集群外的客户端默认情况，无法通过Pod的IP地址或者Service的虚拟IP地址:虚拟端口号进行访问。通常可以通过以下方式进行访问Kubernetes集群内的服务：\n映射Pod到物理机：将Pod端口号映射到宿主机，即在Pod中采用hostPort方式，以使客户端应用能够通过物理机访问容器应用。\n映射Service到物理机：将Service端口号映射到宿主机，即在Service中采用nodePort方式，以使客户端应用能够通过物理机访问容器应用。\n映射Sercie到LoadBalancer：通过设置LoadBalancer映射到云服务商提供的LoadBalancer地址。这种用法仅用于在公有云服务提供商的云平台上设置Service的场景。\n简述Kubernetes ingress？ # Kubernetes的Ingress资源对象，用于将不同URL的访问请求转发到后端不同的Service，以实现HTTP层的业务路由机制。\nKubernetes使用了Ingress策略和Ingress Controller，两者结合并实现了一个完整的Ingress负载均衡器。使用Ingress进行负载分发时，Ingress Controller基于Ingress规则将客户端请求直接转发到Service对应的后端Endpoint（Pod）上，从而跳过kube-proxy的转发功能，kube-proxy不再起作用，全过程为：ingress controller + ingress 规则 \u0026mdash;-\u0026gt; services。\n同时当Ingress Controller提供的是对外服务，则实际上实现的是边缘路由器的功能。\n简述Kubernetes镜像的下载策略？ # K8s的镜像下载策略有三种：Always、Never、IFNotPresent。\nAlways：镜像标签为latest时，总是从指定的仓库中获取镜像。 Never：禁止从仓库中下载镜像，也就是说只能使用本地镜像。 IfNotPresent：仅当本地没有对应镜像时，才从目标仓库中下载。默认的镜像下载策略是：当镜像标签是latest时，默认策略是Always；当镜像标签是自定义时（也就是标签不是latest），那么默认策略是IfNotPresent。 简述Kubernetes的负载均衡器？ # 负载均衡器是暴露服务的最常见和标准方式之一。\n根据工作环境使用两种类型的负载均衡器，即内部负载均衡器或外部负载均衡器。内部负载均衡器自动平衡负载并使用所需配置分配容器，而外部负载均衡器将流量从外部负载引导至后端容器。\n简述Kubernetes各模块如何与API Server通信？ # Kubernetes API Server作为集群的核心，负责集群各功能模块之间的通信。集群内的各个功能模块通过API Server将信息存入etcd，当需要获取和操作这些数据时，则通过API Server提供的REST接口（用GET、LIST或WATCH方法）来实现，从而实现各模块之间的信息交互。\n如kubelet进程与API Server的交互：每个Node上的kubelet每隔一个时间周期，就会调用一次API Server的REST接口报告自身状态，API Server在接收到这些信息后，会将节点状态信息更新到etcd中。\n如kube-controller-manager进程与API Server的交互：kube-controller-manager中的Node Controller模块通过API Server提供的Watch接口实时监控Node的信息，并做相应处理。\n如kube-scheduler进程与API Server的交互：Scheduler通过API Server的Watch接口监听到新建Pod副本的信息后，会检索所有符合该Pod要求的Node列表，开始执行Pod调度逻辑，在调度成功后将Pod绑定到目标节点上。\n简述Kubernetes Scheduler作用及实现原理？ # Kubernetes Scheduler是负责Pod调度的重要功能模块，Kubernetes Scheduler在整个系统中承担了“承上启下”的重要功能，“承上”是指它负责接收Controller Manager创建的新Pod，为其调度至目标Node；“启下”是指调度完成后，目标Node上的kubelet服务进程接管后继工作，负责Pod接下来生命周期。\nKubernetes Scheduler的作用是将待调度的Pod（API新创建的Pod、Controller Manager为补足副本而创建的Pod等）按照特定的调度算法和调度策略绑定（Binding）到集群中某个合适的Node上，并将绑定信息写入etcd中。\n在整个调度过程中涉及三个对象，分别是待调度Pod列表、可用Node列表，以及调度算法和策略。\nKubernetes Scheduler通过调度算法调度为待调度Pod列表中的每个Pod从Node列表中选择一个最适合的Node来实现Pod的调度。随后，目标节点上的kubelet通过API Server监听到Kubernetes Scheduler产生的Pod绑定事件，然后获取对应的Pod清单，下载Image镜像并启动容器。\n简述Kubernetes Scheduler使用哪两种算法将Pod绑定到worker节点？ # Kubernetes Scheduler根据如下两种调度算法将 Pod 绑定到最合适的工作节点：\n预选（Predicates）：输入是所有节点，输出是满足预选条件的节点。kube-scheduler根据预选策略过滤掉不满足策略的Nodes。如果某节点的资源不足或者不满足预选策略的条件则无法通过预选。如“Node的label必须与Pod的Selector一致”。\n优选（Priorities）：输入是预选阶段筛选出的节点，优选会根据优先策略为通过预选的Nodes进行打分排名，选择得分最高的Node。例如，资源越富裕、负载越小的Node可能具有越高的排名。\n简述Kubernetes kubelet的作用？ # 在Kubernetes集群中，在每个Node（又称Worker）上都会启动一个kubelet服务进程。该进程用于处理Master下发到本节点的任务，管理Pod及Pod中的容器。每个kubelet进程都会在API Server上注册节点自身的信息，定期向Master汇报节点资源的使用情况，并通过cAdvisor监控容器和节点资源。\n简述Kubernetes kubelet监控Worker节点资源是使用什么组件来实现的？ # kubelet使用cAdvisor对worker节点资源进行监控。在 Kubernetes 系统中，cAdvisor 已被默认集成到 kubelet 组件内，当 kubelet 服务启动时，它会自动启动 cAdvisor 服务，然后 cAdvisor 会实时采集所在节点的性能指标及在节点上运行的容器的性能指标。\n简述Kubernetes如何保证集群的安全性？ # Kubernetes通过一系列机制来实现集群的安全控制，主要有如下不同的维度：\n基础设施方面：保证容器与其所在宿主机的隔离；\n权限方面：\n最小权限原则：合理限制所有组件的权限，确保组件只执行它被授权的行为，通过限制单个组件的能力来限制它的权限范围。 用户权限：划分普通用户和管理员的角色。 集群方面：\nAPI Server的认证授权：Kubernetes集群中所有资源的访问和变更都是通过Kubernetes API Server来实现的，因此需要建议采用更安全的HTTPS或Token来识别和认证客户端身份（Authentication），以及随后访问权限的授权（Authorization）环节。 API Server的授权管理：通过授权策略来决定一个API调用是否合法。对合法用户进行授权并且随后在用户访问时进行鉴权，建议采用更安全的RBAC方式来提升集群安全授权。 敏感数据引入Secret机制：对于集群敏感数据建议使用Secret方式进行保护。 AdmissionControl（准入机制）：对kubernetes api的请求过程中，顺序为：先经过认证 \u0026amp; 授权，然后执行准入操作，最后对目标对象进行操作。 简述Kubernetes准入机制？ # 在对集群进行请求时，每个准入控制代码都按照一定顺序执行。如果有一个准入控制拒绝了此次请求，那么整个请求的结果将会立即返回，并提示用户相应的error信息。\n准入控制（AdmissionControl）准入控制本质上为一段准入代码，在对kubernetes api的请求过程中，顺序为：先经过认证 \u0026amp; 授权，然后执行准入操作，最后对目标对象进行操作。常用组件（控制代码）如下：\nAlwaysAdmit：允许所有请求 AlwaysDeny：禁止所有请求，多用于测试环境。 ServiceAccount：它将serviceAccounts实现了自动化，它会辅助serviceAccount做一些事情，比如如果pod没有serviceAccount属性，它会自动添加一个default，并确保pod的serviceAccount始终存在。 LimitRanger：观察所有的请求，确保没有违反已经定义好的约束条件，这些条件定义在namespace中LimitRange对象中。 NamespaceExists：观察所有的请求，如果请求尝试创建一个不存在的namespace，则这个请求被拒绝。 简述Kubernetes RBAC及其特点（优势）？ # RBAC是基于角色的访问控制，是一种基于个人用户的角色来管理对计算机或网络资源的访问的方法。\n相对于其他授权模式，RBAC具有如下优势：\n对集群中的资源和非资源权限均有完整的覆盖。 整个RBAC完全由几个API对象完成， 同其他API对象一样， 可以用kubectl或API进行操作。 可以在运行时进行调整，无须重新启动API Server。 简述Kubernetes Secret作用？ # Secret对象，主要作用是保管私密数据，比如密码、OAuth Tokens、SSH Keys等信息。将这些私密信息放在Secret对象中比直接放在Pod或Docker Image中更安全，也更便于使用和分发。\n简述Kubernetes Secret有哪些使用方式？ # 创建完secret之后，可通过如下三种方式使用：\n在创建Pod时，通过为Pod指定Service Account来自动使用该Secret。 通过挂载该Secret到Pod来使用它。 在Docker镜像下载时使用，通过指定Pod的spc.ImagePullSecrets来引用它。 简述Kubernetes PodSecurityPolicy机制？ # Kubernetes PodSecurityPolicy是为了更精细地控制Pod对资源的使用方式以及提升安全策略。在开启PodSecurityPolicy准入控制器后，Kubernetes默认不允许创建任何Pod，需要创建PodSecurityPolicy策略和相应的RBAC授权策略（Authorizing Policies），Pod才能创建成功。\n简述Kubernetes PodSecurityPolicy机制能实现哪些安全策略？ # 在PodSecurityPolicy对象中可以设置不同字段来控制Pod运行时的各种安全策略，常见的有：\n特权模式：privileged是否允许Pod以特权模式运行。 宿主机资源：控制Pod对宿主机资源的控制，如hostPID：是否允许Pod共享宿主机的进程空间。 用户和组：设置运行容器的用户ID（范围）或组（范围）。 提升权限：AllowPrivilegeEscalation：设置容器内的子进程是否可以提升权限，通常在设置非root用户（MustRunAsNonRoot）时进行设置。 SELinux：进行SELinux的相关配置。 简述Kubernetes网络模型？ # Kubernetes网络模型中每个Pod都拥有一个独立的IP地址，并假定所有Pod都在一个可以直接连通的、扁平的网络空间中。所以不管它们是否运行在同一个Node（宿主机）中，都要求它们可以直接通过对方的IP进行访问。设计这个原则的原因是，用户不需要额外考虑如何建立Pod之间的连接，也不需要考虑如何将容器端口映射到主机端口等问题。\n同时为每个Pod都设置一个IP地址的模型使得同一个Pod内的不同容器会共享同一个网络命名空间，也就是同一个Linux网络协议栈。这就意味着同一个Pod内的容器可以通过localhost来连接对方的端口。\n在Kubernetes的集群里，IP是以Pod为单位进行分配的。一个Pod内部的所有容器共享一个网络堆栈（相当于一个网络命名空间，它们的IP地址、网络设备、配置等都是共享的）。\n简述Kubernetes CNI模型？ # CNI提供了一种应用容器的插件化网络解决方案，定义对容器网络进行操作和配置的规范，通过插件的形式对CNI接口进行实现。CNI仅关注在创建容器时分配网络资源，和在销毁容器时删除网络资源。在CNI模型中只涉及两个概念：容器和网络。\n容器（Container）：是拥有独立Linux网络命名空间的环境，例如使用Docker或rkt创建的容器。容器需要拥有自己的Linux网络命名空间，这是加入网络的必要条件。\n网络（Network）：表示可以互连的一组实体，这些实体拥有各自独立、唯一的IP地址，可以是容器、物理机或者其他网络设备（比如路由器）等。\n对容器网络的设置和操作都通过插件（Plugin）进行具体实现，CNI插件包括两种类型：CNI Plugin和IPAM（IP Address Management）Plugin。CNI Plugin负责为容器配置网络资源，IPAM Plugin负责对容器的IP地址进行分配和管理。IPAM Plugin作为CNI Plugin的一部分，与CNI Plugin协同工作。\n简述Kubernetes网络策略？ # 为实现细粒度的容器间网络访问隔离策略，Kubernetes引入Network Policy。\nNetwork Policy的主要功能是对Pod间的网络通信进行限制和准入控制，设置允许访问或禁止访问的客户端Pod列表。Network Policy定义网络策略，配合策略控制器（Policy Controller）进行策略的实现。\n简述Kubernetes网络策略原理？ # Network Policy的工作原理主要为：policy controller需要实现一个API Listener，监听用户设置的Network Policy定义，并将网络访问规则通过各Node的Agent进行实际设置（Agent则需要通过CNI网络插件实现）。\n简述Kubernetes中flannel的作用？ # Flannel可以用于Kubernetes底层网络的实现，主要作用有：\n它能协助Kubernetes，给每一个Node上的Docker容器都分配互相不冲突的IP地址。 它能在这些IP地址之间建立一个覆盖网络（Overlay Network），通过这个覆盖网络，将数据包原封不动地传递到目标容器内。 简述Kubernetes Calico网络组件实现原理？ # Calico是一个基于BGP的纯三层的网络方案，与OpenStack、Kubernetes、AWS、GCE等云平台都能够良好地集成。\nCalico在每个计算节点都利用Linux Kernel实现了一个高效的vRouter来负责数据转发。每个vRouter都通过BGP协议把在本节点上运行的容器的路由信息向整个Calico网络广播，并自动设置到达其他节点的路由转发规则。\nCalico保证所有容器之间的数据流量都是通过IP路由的方式完成互联互通的。Calico节点组网时可以直接利用数据中心的网络结构（L2或者L3），不需要额外的NAT、隧道或者Overlay Network，没有额外的封包解包，能够节约CPU运算，提高网络效率。\n简述Kubernetes共享存储的作用？ # Kubernetes对于有状态的容器应用或者对数据需要持久化的应用，因此需要更加可靠的存储来保存应用产生的重要数据，以便容器应用在重建之后仍然可以使用之前的数据。因此需要使用共享存储。\n简述Kubernetes数据持久化的方式有哪些？ # Kubernetes 通过数据持久化来持久化保存重要数据，常见的方式有：\nEmptyDir（空目录）：没有指定要挂载宿主机上的某个目录，直接由Pod内保部映射到宿主机上。类似于docker中的manager volume。\n场景：\n只需要临时将数据保存在磁盘上，比如在合并/排序算法中； 作为两个容器的共享存储。 特性：\n同个pod里面的不同容器，共享同一个持久化目录，当pod节点删除时，volume的数据也会被删除。 emptyDir的数据持久化的生命周期和使用的pod一致，一般是作为临时存储使用。 Hostpath：将宿主机上已存在的目录或文件挂载到容器内部。类似于docker中的bind mount挂载方式。\n特性：增加了pod与节点之间的耦合。 PersistentVolume（简称PV）：如基于NFS服务的PV，也可以基于GFS的PV。它的作用是统一数据持久化目录，方便管理。\n简述Kubernetes PV和PVC？ # PV是对底层网络共享存储的抽象，将共享存储定义为一种“资源”。\nPVC则是用户对存储资源的一个“申请”。\n简述Kubernetes PV生命周期内的阶段？ # 某个PV在生命周期中可能处于以下4个阶段（Phaes）之一。\nAvailable：可用状态，还未与某个PVC绑定。 Bound：已与某个PVC绑定。 Released：绑定的PVC已经删除，资源已释放，但没有被集群回收。 Failed：自动资源回收失败。 简述Kubernetes所支持的存储供应模式？ # Kubernetes支持两种资源的存储供应模式：静态模式（Static）和动态模式（Dynamic）。\n静态模式：集群管理员手工创建许多PV，在定义PV时需要将后端存储的特性进行设置。\n动态模式：集群管理员无须手工创建PV，而是通过StorageClass的设置对后端存储进行描述，标记为某种类型。此时要求PVC对存储的类型进行声明，系统将自动完成PV的创建及与PVC的绑定。\n简述Kubernetes CSI模型？ # Kubernetes CSI是Kubernetes推出与容器对接的存储接口标准，存储提供方只需要基于标准接口进行存储插件的实现，就能使用Kubernetes的原生存储机制为容器提供存储服务。CSI使得存储提供方的代码能和Kubernetes代码彻底解耦，部署也与Kubernetes核心组件分离，显然，存储插件的开发由提供方自行维护，就能为Kubernetes用户提供更多的存储功能，也更加安全可靠。\nCSI包括CSI Controller和CSI Node：\nCSI Controller的主要功能是提供存储服务视角对存储资源和存储卷进行管理和操作。 CSI Node的主要功能是对主机（Node）上的Volume进行管理和操作。 简述Kubernetes Worker节点加入集群的过程？ # 通常需要对Worker节点进行扩容，从而将应用系统进行水平扩展。主要过程如下：\n1、在该Node上安装Docker、kubelet和kube-proxy服务； 2、然后配置kubelet和kubeproxy的启动参数，将Master URL指定为当前Kubernetes集群Master的地址，最后启动这些服务； 3、通过kubelet默认的自动注册机制，新的Worker将会自动加入现有的Kubernetes集群中； 4、Kubernetes Master在接受了新Worker的注册之后，会自动将其纳入当前集群的调度范围。 简述Kubernetes Pod如何实现对节点的资源控制？ # Kubernetes集群里的节点提供的资源主要是计算资源，计算资源是可计量的能被申请、分配和使用的基础资源。当前Kubernetes集群中的计算资源主要包括CPU、GPU及Memory。CPU与Memory是被Pod使用的，因此在配置Pod时可以通过参数CPU Request及Memory Request为其中的每个容器指定所需使用的CPU与Memory量，Kubernetes会根据Request的值去查找有足够资源的Node来调度此Pod。\n通常，一个程序所使用的CPU与Memory是一个动态的量，确切地说，是一个范围，跟它的负载密切相关：负载增加时，CPU和Memory的使用量也会增加。\n简述Kubernetes Requests和Limits如何影响Pod的调度？ # 当一个Pod创建成功时，Kubernetes调度器（Scheduler）会为该Pod选择一个节点来执行。对于每种计算资源（CPU和Memory）而言，每个节点都有一个能用于运行Pod的最大容量值。调度器在调度时，首先要确保调度后该节点上所有Pod的CPU和内存的Requests总和，不超过该节点能提供给Pod使用的CPU和Memory的最大容量值。\n简述Kubernetes Metric Service？ # 在Kubernetes从1.10版本后采用Metrics Server作为默认的性能数据采集和监控，主要用于提供核心指标（Core Metrics），包括Node、Pod的CPU和内存使用指标。\n对其他自定义指标（Custom Metrics）的监控则由Prometheus等组件来完成。\n简述Kubernetes中，如何使用EFK实现日志的统一管理？ # 在Kubernetes集群环境中，通常一个完整的应用或服务涉及组件过多，建议对日志系统进行集中化管理，通常采用EFK实现。\nEFK是 Elasticsearch、Fluentd 和 Kibana 的组合，其各组件功能如下：\nElasticsearch：是一个搜索引擎，负责存储日志并提供查询接口； Fluentd：负责从 Kubernetes 搜集日志，每个node节点上面的fluentd监控并收集该节点上面的系统日志，并将处理过后的日志信息发送给Elasticsearch； Kibana：提供了一个 Web GUI，用户可以浏览和搜索存储在 Elasticsearch 中的日志。 通过在每台node上部署一个以DaemonSet方式运行的fluentd来收集每台node上的日志。Fluentd将docker日志目录/var/lib/docker/containers和/var/log目录挂载到Pod中，然后Pod会在node节点的/var/log/pods目录中创建新的目录，可以区别不同的容器日志输出，该目录下有一个日志文件链接到/var/lib/docker/contianers目录下的容器日志输出。\n简述Kubernetes如何进行优雅的节点关机维护？ # 由于Kubernetes节点运行大量Pod，因此在进行关机维护之前，建议先使用kubectl drain将该节点的Pod进行驱逐，然后进行关机维护。\n简述Kubernetes集群联邦？ # Kubernetes集群联邦可以将多个Kubernetes集群作为一个集群进行管理。因此，可以在一个数据中心/云中创建多个Kubernetes集群，并使用集群联邦在一个地方控制/管理所有集群。\n简述Helm及其优势？ # Helm 是 Kubernetes 的软件包管理工具。类似 Ubuntu 中使用的apt、Centos中使用的yum 或者Python中的 pip 一样。\nHelm能够将一组K8S资源打包统一管理, 是查找、共享和使用为Kubernetes构建的软件的最佳方式。\nHelm中通常每个包称为一个Chart，一个Chart是一个目录（一般情况下会将目录进行打包压缩，形成name-version.tgz格式的单一文件，方便传输和存储）。\nHelm优势 # 在 Kubernetes中部署一个可以使用的应用，需要涉及到很多的 Kubernetes 资源的共同协作。使用helm则具有如下优势：\n统一管理、配置和更新这些分散的 k8s 的应用资源文件； 分发和复用一套应用模板； 将应用的一系列资源当做一个软件包管理。 对于应用发布者而言，可以通过 Helm 打包应用、管理应用依赖关系、管理应用版本并发布应用到软件仓库。 对于使用者而言，使用 Helm 后不用需要编写复杂的应用部署文件，可以以简单的方式在 Kubernetes 上查找、安装、升级、回滚、卸载应用程序。 "},{"id":57,"href":"/docs/2024-8-1-%E5%B8%B8%E8%A7%81linux%E8%BF%90%E7%BB%B4%E9%9D%A2%E8%AF%95%E9%A2%98%E6%89%BE%E5%B7%A5%E4%BD%9C%E7%9A%84%E5%BF%85%E7%9C%8B/","title":"2024-8-1 linux运维面试题","section":"Docs","content":"下面是一名运维人员求职数十家公司总结的 Linux运维面试题，给大家参考下~\n1、现在给你三百台服务器，你怎么对他们进行管理？\n管理3百台服务器的方式： 1）设定跳板机，使用统一账号登录，便于安全与登录的考量。 2）使用 salt、ansiable、puppet 进行系统的统一调度与配置的统一管理。 3）建立简单的服务器的系统、配置、应用的 cmdb 信息管理。便于查阅每台服务器上的各种信息记录。\n2、简述 raid0 raid1 raid5 三种工作模式的工作原理及特点\nRAID，可以把硬盘整合成一个大磁盘，还可以在大磁盘上再分区，放数据 还有一个大功能，多块盘放在一起可以有冗余（备份） RAID整合方式有很多，常用的：0 1 5 10\nRAID 0，可以是一块盘和 N 个盘组合\n优点读写快，是 RAID 中最好的 缺点：没有冗余，一块坏了数据就全没有了 RAID 1，只能2块盘，盘的大小可以不一样，以小的为准。\n10G+10G只有10G，另一个做备份。它有100%的冗余， 缺点：浪费资源，成本高 RAID 5 ，3块盘，容量计算10 *（n-1），损失一块盘\n特点，读写性能一般，读还好一点，写不好\n冗余从好到坏：RAID1 RAID10 RAID 5 RAID0 性能从好到坏：RAID0 RAID10 RAID5 RAID1 成本从低到高：RAID0 RAID5 RAID1 RAID10 单台服务器：很重要盘不多，系统盘，RAID1 数据库服务器：主库：RAID10 从库 RAID5\\RAID0（为了维护成本，RAID10） WEB服务器，如果没有太多的数据的话，RAID5,RAID0（单盘） 有多台，监控、应用服务器，RAID0 RAID5\n我们会根据数据的存储和访问的需求，去匹配对应的RAID级别\n3、LVS、Nginx、HAproxy 有什么区别？工作中你怎么选择？\nLVS：是基于四层的转发 HAproxy：是基于四层和七层的转发，是专业的代理服务器 Nginx：是WEB服务器，缓存服务器，又是反向代理服务器，可以做七层的转发 区别：LVS由于是基于四层的转发所以只能做端口的转发，而基于URL的、基于目录的这种转发LVS就做不了。\n工作选择：\nHAproxy 和 Nginx 由于可以做七层的转发，所以 URL 和目录的转发都可以做 在很大并发量的时候我们就要选择LVS，像中小型公司的话并发量没那么大 选择 HAproxy 或者 Nginx 足已，由于 HAproxy 由是专业的代理服务器 配置简单，所以中小型企业推荐使用 HAproxy 4、Squid、Varinsh 和 Nginx 有什么区别，工作中你怎么选择？\nSquid、Varinsh 和 Nginx 都是代理服务器\n什么是代理服务器：\n能当替用户去访问公网，并且能把访问到的数据缓存到服务器本地，等用户下次再访问相同的资源的时候，代理服务器直接从本地回应给用户，当本地没有的时候，我代替你去访问公网，我接收你的请求，我先在我自已的本地缓存找，如果我本地缓存有，我直接从我本地的缓存里回复你；如果我在我本地没有找到你要访问的缓存的数据，那么代理服务器就会代替你去访问公网。\n区别：\n1）Nginx 本来是反向代理/web服务器，用了插件可以做做这个副业，但是本身不支持特性挺多，只能缓存静态文件 2）从这些功能上。varnish 和 squid 是专业的 cache 服务，而 nginx 这些是第三方模块完成 3）varnish 本身的技术上优势要高于 squid，它采用了可视化页面缓存技术\n在内存的利用上，Varnish 比 Squid 具有优势，性能要比 Squid 高。还有强大的通过 Varnish 管理端口，可以使用正则表达式快速、批量地清除部分缓存。它是内存缓存，速度一流，但是内存缓存也限制了其容量，缓存页面和图片一般是挺好的\n4）squid 的优势在于完整的庞大的 cache 技术资料，和很多的应用生产环境\n工作中选择：\n要做 cache 服务的话，我们肯定是要选择专业的cache服务，优先选择squid或者varnish。\n5、Tomcat和Resin有什么区别，工作中你怎么选择？\n区别：Tomcat 用户数多，可参考文档多，Resin用户数少，可考虑文档少 最主要区别则是 Tomcat 是标准的 java 容器，不过性能方面比 resin 的要差一些。但稳定性和 java 程序的兼容性，应该是比 resin 的要好\n工作中选择：现在大公司都是用 resin，追求性能；而中小型公司都是用 Tomcat，追求稳定和程序的兼容\n6、什么是中间件？什么是jdk？\n中间件介绍：\n中间件是一种独立的系统软件或服务程序，分布式应用软件借助这种软件在不同的技术之间共享资源。中间件位于客户机/服务器的操作系统之上，管理计算机资源和网络通讯，是连接两个独立应用程序或独立系统的软件。\n相连接的系统，即使它们具有不同的接口，但通过中间件相互之间仍能交换信息。执行中间件的一个关键途径是信息传递，通过中间件，应用程序可以工作于多平台或OS环境。\njdk：jdk是Java的开发工具包 它是一种用于构建在 Java 平台上发布的应用程序、applet 和组件的开发环境\n7、讲述一下Tomcat8005、8009、8080三个端口的含义？\n8005 - 关闭时使用 8009 - 为AJP端口，即容器使用，如Apache能通过AJP协议访问Tomcat的8009端口 8080 - 一般应用使用\n8、什么叫CDN？\n即内容分发网络 其目的是通过在现有的 Internet 中增加一层新的网络架构，将网站的内容发布到最接近用户的网络边缘，使用户可就近取得所需的内容，提高用户访问网站的速度 9、什么叫网站灰度发布？\n灰度发布是指在黑与白之间，能够平滑过渡的一种发布方式。\nAB test 就是一种灰度发布方式，让一部用户继续用A，一部分用户开始用B，如果用户对B没有什么反对意见，那么逐步扩大范围，把所有用户都迁移到B上面来，灰度发布可以保证整体系统的稳定，在初始灰度的时候就可以发现、调整问题，以保证其影响度。\n10、简述 DNS 进行域名解析的过程？\n用户要访问 www.baidu.com，会先找本机的 host 文件，再找本地设置的 DNS 服务器，如果也没有的话，就去网络中找根服务器，根服务器反馈结果，说只能提供一级域名服务器.cn，就去找一级域名服务器，一级域名服务器说只能提供二级域名服务器.com.cn，就去找二级域名服务器。\n二级域服务器只能提供三级域名服务器.baidu.com.cn，就去找三级域名服务器，三级域名服务器正好有这个网站www.baidu.com，然后发给请求的服务器，保存一份之后，再发给客户端。\n11、RabbitMQ 是什么东西？\nRabbitMQ 也就是消息队列中间件，消息中间件是在消息的传息过程中保存消息的容器，消息中间件再将消息从它的源中到它的目标中标时充当中间人的作用。队列的主要目的是提供路由并保证消息的传递；如果发送消息时接收者不可用，消息队列不会保留消息，直到可以成功地传递为止，当然，消息队列保存消息也是有期限地。\n12、讲述一下LVS三种模式的工作过程？\nLVS 有三种负载均衡的模式，分别是 VS/NAT（nat 模式）、VS/DR(路由模式)、VS/TUN（隧道模式）\n一、NAT模式（VS-NAT）\n原理：就是把客户端发来的数据包的IP头的目的地址，在负载均衡器上换成其中一台RS的IP地址\n并发至此 RS 来处理，RS处理完后把数据交给负载均衡器，负载均衡器再把数据包原IP地址改为自己的IP\n将目的地址改为客户端IP地址即可期间，无论是进来的流量,还是出去的流量,都必须经过负载均衡器\n优点：集群中的物理服务器可以使用任何支持TCP/IP操作系统，只有负载均衡器需要一个合法的IP地址 缺点：扩展性有限。当服务器节点（普通PC服务器）增长过多时,负载均衡器将成为整个系统的瓶颈 因为所有的请求包和应答包的流向都经过负载均衡器。当服务器节点过多时\n大量的数据包都交汇在负载均衡器那，速度就会变慢！\n二、IP隧道模式（VS-TUN）\n原理：首先要知道，互联网上的大多Internet服务的请求包很短小，而应答包通常很大\n那么隧道模式就是，把客户端发来的数据包，封装一个新的IP头标记(仅目的IP)发给RS\nRS收到后，先把数据包的头解开,还原数据包,处理后,直接返回给客户端,不需要再经过\n负载均衡器。注意,由于RS需要对负载均衡器发过来的数据包进行还原,所以说必须支持\nIPTUNNEL协议，所以，在RS的内核中，必须编译支持IPTUNNEL这个选项\n优点：负载均衡器只负责将请求包分发给后端节点服务器，而RS将应答包直接发给用户。所以，减少了负载均衡器的大量数据流动，负载均衡器不再是系统的瓶颈，就能处理很巨大的请求量。这种方式，一台负载均衡器能够为很多RS进行分发。而且跑在公网上就能进行不同地域的分发。 缺点：隧道模式的RS节点需要合法IP，这种方式需要所有的服务器支持”IP Tunneling”(IP Encapsulation)协议，服务器可能只局限在部分 Linux 系统上 三、直接路由模式（VS-DR）\n原理：负载均衡器和RS都使用同一个IP对外服务但只有DR对ARP请求进行响应\n所有RS对本身这个IP的ARP请求保持静默也就是说,网关会把对这个服务IP的请求全部定向给DR\n而DR收到数据包后根据调度算法,找出对应的RS，把目的MAC地址改为RS的MAC（因为IP一致）\n并将请求分发给这台RS这时RS收到这个数据包，处理完成之后，由于IP一致，可以直接将数据返给客户\n则等于直接从客户端收到这个数据包无异，处理后直接返回给客户端\n由于负载均衡器要对二层包头进行改换，所以负载均衡器和RS之间必须在一个广播域\n也可以简单的理解为在同一台交换机上\n优点：和TUN（隧道模式）一样，负载均衡器也只是分发请求，应答包通过单独的路由方法返回给客户端\n与VS-TUN相比，VS-DR这种实现方式不需要隧道结构，因此可以使用大多数操作系统做为物理服务器。\n缺点：（不能说缺点，只能说是不足）要求负载均衡器的网卡必须与物理网卡在一个物理段上。\n13、MySQL 的 innodb 如何定位锁问题，MySQL 如何减少主从复制延迟？\nMySQL 的 innodb 如何定位锁问题:\n在使用 show engine innodb status检查引擎状态时，发现了死锁问题\n在5.5中，information_schema 库中增加了三个关于锁的表（MEMORY引擎）\ninnodb_trx ## 当前运行的所有事务\rinnodb_locks ## 当前出现的锁\rinnodb_lock_waits ## 锁等待的对应关系 MySQL 如何减少主从复制延迟：\n如果延迟比较大，就先确认以下几个因素：\n从库硬件比主库差，导致复制延迟 主从复制单线程，如果主库写并发太大，来不及传送到从库，就会导致延迟。更高版本的mysql可以支持多线程复制 慢SQL语句过多 网络延迟 master负载；主库读写压力大，导致复制延迟，架构的前端要加buffer及缓存层 slave负载；一般的做法是，使用多台 slave 来分摊读请求，再从这些 slave 中取一台专用的服务器。只作为备份用，不进行其他任何操作.另外， 2个可以减少延迟的参数：–slave-net-timeout=seconds 单位为秒 默认设置为 3600秒 #参数含义：当slave从主数据库读取log数据失败后，等待多久重新建立连接并获取数据–master-connect-retry=seconds 单位为秒 默认设置为 60秒\r#参数含义：当重新建立主从连接时，如果连接建立失败，间隔多久后重试通常配置以上2个参数可以减少网络问题导致的主从数据同步延迟 MySQL 数据库主从同步延迟解决方案\n最简单的减少slave同步延时的方案就是在架构上做优化，尽量让主库的DDL快速执行\n还有就是主库是写，对数据安全性较高，比如sync_binlog=1，innodb_flush_log_at_trx_commit= 1 之类的设置，而slave则不需要这么高的数据安全，完全可以讲sync_binlog设置为0或者关闭binlog\ninnodb_flushlog也可以设置为0来提高sql的执行效率。另外就是使用比主库更好的硬件设备作为slave\n14、如何重置 MySQL Root 密码？\n一、 在已知MYSQL数据库的ROOT用户密码的情况下，修改密码的方法：\n1、 在SHELL环境下，使用 mysqladmin 命令设置：\nmysqladmin –u root –p password “新密码” 回车后要求输入旧密码 2、 在mysql\u0026gt;环境中,使用update命令，直接更新 MySQL 库 user 表的数据：\nUpdate mysql.user set password=password(‘新密码’) where user=’root’;\rflush privileges;\r注意：mysql语句要以分号”；”结束 3、在mysql\u0026gt;环境中，使用 grant 命令，修改 root 用户的授权权限。\ngrant all on *.* to root@’localhost’ identified by ‘新密码’； 二、 如查忘记了mysql数据库的ROOT用户的密码，又如何做呢？方法如下：\n1、 关闭当前运行的mysqld服务程序：service mysqld stop（要先将mysqld添加为系统服务）\n2、 使用mysqld_safe脚本以安全模式（不加载授权表）启动mysqld 服务\n/usr/local/mysql/bin/mysqld_safe --skip-grant-table \u0026amp; 3、 使用空密码的root用户登录数据库，重新设置ROOT用户的密码\n＃mysql -u root\rMysql\u0026gt; Update mysql.user set password=password(‘新密码’) where user=’root’;\rMysql\u0026gt; flush privileges; 15、lvs/nginx/haproxy 优缺点\nNginx 的优点是： 1、工作在网络的7层之上，可以针对http应用做一些分流的策略，比如针对域名、目录结构，它的正则规则比HAProxy更为强大和灵活，这也是它目前广泛流行的主要原因之一，Nginx单凭这点可利用的场合就远多于LVS了。\n2、Nginx 对网络稳定性的依赖非常小，理论上能ping通就就能进行负载功能，这个也是它的优势之一，相反 LVS 对网络稳定性依赖比较大，这点本人深有体会；\n3、Nginx 安装和配置比较简单，测试起来比较方便，它基本能把错误用日志打印出来，LVS 的配置、测试就要花比较长的时间了，LVS对网络依赖比较大。\n4、可以承担高负载压力且稳定，在硬件不差的情况下一般能支撑几万次的并发量，负载度比LVS相对小些。\n5、Nginx可以通过端口检测到服务器内部的故障，比如根据服务器处理网页返回的状态码、超时等等，并且会把返回错误的请求重新提交到另一个节点，不过其中缺点就是不支持url来检测。比如用户正在上传一个文件，而处理该上传的节点刚好在上传过程中出现故障，Nginx会把上传切到另一台服务器重新处理，而LVS就直接断掉了。如果是上传一个很大的文件或者很重要的文件的话，用户可能会因此而不满。\n6、Nginx 不仅仅是一款优秀的负载均衡器/反向代理软件，它同时也是功能强大的Web应用服务器，LNMP也是近几年非常流行的web架构，在高流量的环境中稳定性也很好。\n7、Nginx现在作为Web反向加速缓存越来越成熟了，速度比传统的Squid服务器更快，可考虑用其作为反向代理加速器\n8、Nginx可作为中层反向代理使用，这一层面Nginx基本上无对手，唯一可以对比Nginx的就只有lighttpd了。不过lighttpd目前还没有做到Nginx完全的功能，配置也不那么清晰易读，社区资料也远远没Nginx活跃\n9、Nginx也可作为静态网页和图片服务器，这方面的性能也无对手。还有Nginx社区非常活跃，第三方模块也很多\nNginx 的缺点是： 1、Nginx仅能支持http、https和Email协议，这样就在适用范围上面小些，这个是它的缺点\n2、对后端服务器的健康检查，只支持通过端口来检测，不支持通过url来检测，不支持Session的直接保持，但能通过ip_hash来解决。\nLVS：使用Linux内核集群实现一个高性能、高可用的负载均衡服务器\n它具有很好的可伸缩性（Scalability)、可靠性（Reliability)和可管理性（Manageability)\nLVS的优点是： 1、抗负载能力强、是工作在网络4层之上仅作分发之用，没有流量的产生，这个特点也决定了它在负载均衡软件里的性能最强的，对内存和cpu资源消耗比较低。\n2、配置性比较低，这是一个缺点也是一个优点，因为没有可太多配置的东西，所以并不需要太多接触，大大减少了人为出错的几率\n3、工作稳定，因为其本身抗负载能力很强，自身有完整的双机热备方案，如LVS+Keepalived，不过我们在项目实施中用得最多的还是LVS/DR+Keepalived\n4、无流量，LVS只分发请求，而流量并不从它本身出去，这点保证了均衡器IO的性能不会收到大流量的影响。\n5、应用范围较广，因为LVS工作在4层，所以它几乎可对所有应用做负载均衡，包括http、数据库、在线聊天室等\nLVS的缺点是： 1、软件本身不支持正则表达式处理，不能做动静分离\n而现在许多网站在这方面都有较强的需求，这个是Nginx/HAProxy+Keepalived的优势所在\n2、如果是网站应用比较庞大的话，LVS/DR+Keepalived 实施起来就比较复杂了\n特别后面有Windows Server的机器的话，如果实施及配置还有维护过程就比较复杂了\n相对而言，Nginx/HAProxy+Keepalived 就简单多了。\nHAProxy的特点是： 1、HAProxy也是支持虚拟主机的。\n2、HAProxy 的优点能够补充 Nginx的 一些缺点，比如支持 Session 的保持，Cookie 的引导\n同时支持通过获取指定的url来检测后端服务器的状态 3、HAProxy跟LVS类似，本身就只是一款负载均衡软件\n单纯从效率上来讲HAProxy会比Nginx有更出色的负载均衡速度，在并发处理上也是优于Nginx的 4、HAProxy支持TCP协议的负载均衡转发，可以对MySQL读进行负载均衡\n对后端的 MySQL 节点进行检测和负载均衡，大家可以用 LVS+Keepalived 对 MySQL 主从做负载均衡 5、HAProxy负载均衡策略非常多，HAProxy的负载均衡算法现在具体有如下8种：\n①roundrobin，表示简单的轮询，这个不多说，这个是负载均衡基本都具备的；\n② static-rr，表示根据权重，建议关注；\n③leastconn，表示最少连接者先处理，建议关注；\n④ source，表示根据请求源IP，这个跟Nginx的IP_hash机制类似\n我们用其作为解决session问题的一种方法，建议关注； ⑤ri，表示根据请求的URI；\n⑥rl_param，表示根据请求的URl参数’balance url_param’ requires an URL parameter name；\n⑦hdr(name)，表示根据HTTP请求头来锁定每一次HTTP请求；\n⑧rdp-cookie(name)，表示根据据cookie(name)来锁定并哈希每一次TCP请求。\n16、mysql数据备份工具\nmysqldump工具 mysqldump是mysql自带的备份工具，目录在bin目录下面：/usr/local/mysql/bin/mysqldump\n支持基于innodb的热备份，但是由于是逻辑备份，所以速度不是很快，适合备份数据比较小的场景\nMysqldump完全备份+二进制日志可以实现基于时间点的恢复。\n基于LVM快照备份 在物理备份中，有基于文件系统的物理备份（LVM的快照），也可以直接用tar之类的命令对整个数据库目录\n进行打包备份，但是这些只能进行泠备份，不同的存储引擎备份的也不一样，myisam自动备份到表级别\n而innodb不开启独立表空间的话只能备份整个数据库。\ntar包备份 percona提供的xtrabackup工具\n支持innodb的物理热备份，支持完全备份，增量备份，而且速度非常快，支持innodb存储引起的数据在不同\n数据库之间迁移，支持复制模式下的从机备份恢复备份恢复，为了让xtrabackup支持更多的功能扩展\n可以设立独立表空间，打开 innodb_file_per_table功能，启用之后可以支持单独的表备份\n17、keepalive的工作原理和如何做到健康检查\nkeepalived是以VRRP协议为实现基础的，VRRP全称Virtual Router Redundancy Protocol，即虚拟路由冗余协议。\n虚拟路由冗余协议，可以认为是实现路由器高可用的协议，即将N台提供相同功能的路由器组成一个路由器组\n这个组里面有一个master和多个backup，master上面有一个对外提供服务的vip（该路由器所在局域网内\n其他机器的默认路由为该vip），master会发组播，当backup收不到vrrp包时就认为master宕掉了\n这时就需要根据VRRP的优先级来选举一个backup当master。这样就可以保证路由器的高可用了\nkeepalived主要有三个模块，分别是core、check和vrrp。core模块为keepalived的核心，负责主进程的启动、维护\n及全局配置文件的加载和解析。check负责健康检查，包括常见的各种检查方式，vrrp模块是来实现VRRP协议的\nKeepalived健康检查方式配置：\nHTTP_GET|SSL_GET\rHTTP_GET | SSL_GET\r{\rurl {\rpath /# HTTP/SSL 检查的url可以是多个\rdigest \u0026lt;STRING\u0026gt; # HTTP/SSL 检查后的摘要信息用工具genhash生成\rstatus_code 200# HTTP/SSL 检查返回的状态码\r}\rconnect_port 80 # 连接端口\rbindto\u0026lt;IPADD\u0026gt;\rconnect_timeout 3 # 连接超时时间\rnb_get_retry 3 # 重连次数\rdelay_before_retry 2 #连接间隔时间\r} 18、统计ip访问情况，要求分析nginx访问日志，找出访问页面数量在前十位的ip\ncat access.log | awk \u0026#39;{print $1}\u0026#39; | uniq -c | sort -rn | head -10 19、使用 tcpdump 监听主机为192.168.1.1，tcp端口为80的数据，同时将输出结果保存输出到 tcpdump.log\ntcpdump \u0026#39;host 192.168.1.1 and port 80\u0026#39; \u0026gt; tcpdump.log 20、如何将本地80 端口的请求转发到8080 端口，当前主机IP 为192.168.2.1\niptables -A PREROUTING -d 192.168.2.1 -p tcp -m tcp -dport 80 -j DNAT-to-destination 192.168.2.1:8080 21、你对现在运维工程师的理解和以及对其工作的认识\n运维工程师在公司当中责任重大，需要保证时刻为公司及客户提供最高、最快、最稳定、最安全的服务\n运维工程师的一个小小的失误，很有可能会对公司及客户造成重大损失\n因此运维工程师的工作需要严谨及富有创新精神\n22、实时抓取并显示当前系统中tcp 80端口的网络数据信息，请写出完整操作命令\ntcpdump -nn tcp port 80 23、服务器开不了机怎么解决一步步的排查\n24、Linux 系统中病毒怎么解决\n1）最简单有效的方法就是重装系统 2）要查的话就是找到病毒文件然后删除\n中毒之后一般机器 CPU、内存使用率会比较高\n机器向外发包等异常情况，排查方法简单介绍下\ntop 命令找到 CPU 使用率最高的进程\n一般病毒文件命名都比较乱，可以用 ps aux 找到病毒文件位置\nrm -f 命令删除病毒文件\n检查计划任务、开机启动项和病毒文件目录有无其他可以文件等\n3）由于即使删除病毒文件不排除有潜伏病毒，所以最好是把机器备份数据之后重装一下\n25、发现一个病毒文件你删了他又自动创建怎么解决\n公司的内网某台linux服务器流量莫名其妙的剧增，用 iftop 查看有连接外网的情况\n针对这种情况一般重点查看 netstat 连接的外网 ip 和端口。\n用 lsof -p pid 可以查看到具体是那些进程，哪些文件\n经查勘发现/root下有相关的配置conf.n hhe两个可疑文件，rm -rf后不到一分钟就自动生成了\n由此推断是某个母进程产生的这些文件。所以找到母进程就是找到罪魁祸首\n查杀病毒最好断掉外网访问，还好是内网服务器，可以通过内网访问\n断了内网，病毒就失去外联的能力，杀掉它就容易的多\n怎么找到呢，找了半天也没有看到蛛丝马迹，没办法只有ps axu一个个排查\n方法是查看可以的用户和和系统相似而又不是的冒牌货，果然，看到了如下进程可疑\n看不到图片就是/usr/bin/.sshd\n于是我杀掉所有.sshd相关的进程，然后直接删掉.sshd这个可执行文件\n然后才删掉了文章开头提到的自动复活的文件\n总结一下，遇到这种问题，如果不是太严重，尽量不要重装系统\n一般就是先断外网，然后利用iftop,ps,netstat,chattr,lsof,pstree这些工具顺藤摸瓜\n一般都能找到元凶。但是如果遇到诸如此类的问题\n/boot/efi/EFI/redhat/grub.efi: Heuristics.Broken.Executable FOUND，个人觉得就要重装系统了\n26、说说TCP/IP的七层模型\n应用层 (Application)： 网络服务与最终用户的一个接口。\n协议有：HTTP FTP TFTP SMTP SNMP DNS TELNET HTTPS POP3 DHCP\n表示层（Presentation Layer）： 数据的表示、安全、压缩。（在五层模型里面已经合并到了应用层）\n格式有，JPEG、ASCll、DECOIC、加密格式等\n会话层（Session Layer）： 建立、管理、终止会话。（在五层模型里面已经合并到了应用层）\n对应主机进程，指本地主机与远程主机正在进行的会话\n传输层 (Transport)： 定义传输数据的协议端口号，以及流控和差错校验。\n协议有：TCP UDP，数据包一旦离开网卡即进入网络传输层\n网络层 (Network)： 进行逻辑地址寻址，实现不同网络之间的路径选择。\n协议有：ICMP IGMP IP（IPV4 IPV6） ARP RARP\n数据链路层 (Link)： 建立逻辑连接、进行硬件地址寻址、差错校验等功能。（由底层网络定义协议）\n将比特组合成字节进而组合成帧，用MAC地址访问介质，错误发现但不能纠正\n物理层（Physical Layer）： 是计算机网络OSI模型中最低的一层\n物理层规定：为传输数据所需要的物理链路创建、维持、拆除\n而提供具有机械的，电子的，功能的和规范的特性\n简单的说，物理层确保原始的数据可在各种物理媒体上传输。局域网与广域网皆属第1、2层\n物理层是OSI的第一层，它虽然处于最底层，却是整个开放系统的基础\n物理层为设备之间的数据通信提供传输媒体及互连设备，为数据传输提供可靠的环境\n如果您想要用尽量少的词来记住这个第一层，那就是“信号和介质”。\n27、你常用的 Nginx 模块，用来做什么\nrewrite模块，实现重写功能\raccess模块：来源控制\rssl模块：安全加密\rngx_http_gzip_module：网络传输压缩模块\rngx_http_proxy_module 模块实现代理\rngx_http_upstream_module模块实现定义后端服务器列表\rngx_cache_purge实现缓存清除功能 28、请列出你了解的web服务器负载架构\nNginx Haproxy Keepalived LVS\n29、查看http的并发请求数与其TCP连接状态\nnetstat -n | awk \u0026#39;/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}\u0026#39; 还有ulimit -n 查看linux系统打开最大的文件描述符，这里默认1024\n不修改这里web服务器修改再大也没用，若要用就修改很几个办法，这里说其中一个：\n修改/etc/security/limits.conf\n* soft nofile 10240\r* hard nofile 10240 重启后生效\n30、用tcpdump嗅探80端口的访问看看谁最高\ntcpdump -i eth0 -tnn dst port 80 -c 1000 | awk -F\u0026#34;.\u0026#34; \u0026#39;{print $1\u0026#34;.\u0026#34;$2\u0026#34;.\u0026#34;$3\u0026#34;.\u0026#34;$4}\u0026#39;| sort | uniq -c | sort -nr |head -20 31、写一个脚本，实现判断192.168.1.0/24网络里，当前在线的IP有哪些，能ping通则认为在线\n#!/bin/bash\rfor ip in `seq 1 255`\rdo\r{\rping -c 1 192.168.1.$ip \u0026gt; /dev/null 2\u0026gt;\u0026amp;1\rif [ $? -eq 0 ]; then\recho 192.168.1.$ip UP\relse\recho 192.168.1.$ip DOWN\rfi\r}\u0026amp;\rdone\rwait 32、已知 apache 服务的访问日志按天记录在服务器本地目录/app/logs 下，由于磁盘空间紧张现在要求只能保留最近 7 天的访问日志！请问如何解决？请给出解决办法或配置或处理命令\n创建文件脚本：\n#!/bin/bash\rfor n in `seq 14`\rdo date -s \u0026#34;11/0$n/14\u0026#34;\rtouch access_www_`(date +%F)`.log\rdone 解决方法：\n# pwd/application/logs\r# ll\r-rw-r--r--. 1 root root 0 Jan 1 00:00 access_www_2015-01-01.log\r-rw-r--r--. 1 root root 0 Jan 2 00:00 access_www_2015-01-02.log\r-rw-r--r--. 1 root root 0 Jan 3 00:00 access_www_2015-01-03.log\r-rw-r--r--. 1 root root 0 Jan 4 00:00 access_www_2015-01-04.log\r-rw-r--r--. 1 root root 0 Jan 5 00:00 access_www_2015-01-05.log\r-rw-r--r--. 1 root root 0 Jan 6 00:00 access_www_2015-01-06.log\r-rw-r--r--. 1 root root 0 Jan 7 00:00 access_www_2015-01-07.log\r-rw-r--r--. 1 root root 0 Jan 8 00:00 access_www_2015-01-08.log\r-rw-r--r--. 1 root root 0 Jan 9 00:00 access_www_2015-01-09.log\r-rw-r--r--. 1 root root 0 Jan 10 00:00 access_www_2015-01-10.log\r-rw-r--r--. 1 root root 0 Jan 11 00:00 access_www_2015-01-11.log\r-rw-r--r--. 1 root root 0 Jan 12 00:00 access_www_2015-01-12.log\r-rw-r--r--. 1 root root 0 Jan 13 00:00 access_www_2015-01-13.log\r-rw-r--r--. 1 root root 0 Jan 14 00:00 access_www_2015-01-14.log\r# find /application/logs/ -type f -mtime +7 -name \u0026#34;*.log\u0026#34;|xargs rm –f ##也可以使用-exec rm -f {} \\;进行删除\r# ll\r-rw-r--r--. 1 root root 0 Jan 7 00:00 access_www_2015-01-07.log\r-rw-r--r--. 1 root root 0 Jan 8 00:00 access_www_2015-01-08.log\r-rw-r--r--. 1 root root 0 Jan 9 00:00 access_www_2015-01-09.log\r-rw-r--r--. 1 root root 0 Jan 10 00:00 access_www_2015-01-10.log\r-rw-r--r--. 1 root root 0 Jan 11 00:00 access_www_2015-01-11.log\r-rw-r--r--. 1 root root 0 Jan 12 00:00 access_www_2015-01-12.log\r-rw-r--r--. 1 root root 0 Jan 13 00:00 access_www_2015-01-13.log\r-rw-r--r--. 1 root root 0 Jan 14 00:00 access_www_2015-01-14.log 33、如何优化 Linux系统（可以不说太具体）？\n不用root，添加普通用户，通过sudo授权管理 更改默认的远程连接SSH服务端口及禁止root用户远程连接 定时自动更新服务器时间 配置国内yum源 关闭selinux及iptables（iptables工作场景如果有外网IP一定要打开，高并发除外） 调整文件描述符的数量 精简开机启动服务（crond rsyslog network sshd） 内核参数优化（/etc/sysctl.conf） 更改字符集，支持中文，但建议还是用英文字符集，防止乱码 锁定关键系统文件 清空/etc/issue，去除系统及内核版本登录前的屏幕显示\n34、请执行命令取出 linux 中 eth0 的 IP 地址(请用 cut，有能力者也可分别用 awk,sed 命令答）\ncut方法1：\n# ifconfig eth0|sed -n \u0026#39;2p\u0026#39;|cut -d \u0026#34;:\u0026#34; -f2|cut -d \u0026#34; \u0026#34; -f1\r192.168.20.130 awk方法2：\n# ifconfig eth0|awk \u0026#39;NR==2\u0026#39;|awk -F \u0026#34;:\u0026#34; \u0026#39;{print $2}\u0026#39;|awk \u0026#39;{print $1}\u0026#39;\r192.168.20.130 awk多分隔符方法3：\n# ifconfig eth0|awk \u0026#39;NR==2\u0026#39;|awk -F \u0026#34;[: ]+\u0026#34; \u0026#39;{print $4}\u0026#39;\r192.168.20.130 sed方法4:\n# ifconfig eth0|sed -n \u0026#39;/inet addr/p\u0026#39;|sed -r \u0026#39;s#^.*ddr:(.*)Bc.*$#\\1#g\u0026#39;\r192.168.20.130 35、每天晚上 12 点，打包站点目录/var/www/html 备份到/data 目录下（最好每次备份按时间生成不同的备份包）\n# cat a.sh #/bin/bash\rcd /var/www/ \u0026amp;\u0026amp; /bin/tar zcf /data/html-`date +%m-%d%H`.tar.gz html/\r# crontab –e\r00 00 * * * /bin/sh /root/a.sh "},{"id":58,"href":"/docs/2024-8-1-linux%E8%BF%90%E7%BB%B4%E5%B7%A5%E7%A8%8B%E5%B8%8850%E4%B8%AA%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/","title":"2024-8-1 linux面试题","section":"Docs","content":"1、请简述OSI七层网络模型有哪些层及各自的含义?\n物理层：底层数据传输，比如网线、网卡标准 数据链路层：定义数据的基本格式，如何传输，如何标识。比如网卡MAC地址 网络层：定义IP编码，定义路由功能，比如不同设备的数据转发 传输层：端到端传输数据的基本功能，比如TCP、UDP 会话层：控制应用程序之间会话能力，比如不同软件数据分发给不停软件 表示层：数据格式标识，基本压缩加密功能。 应用层：各种应用软件，包括 Web 应用。 2、在Linux的LVM分区格式下，请简述给根分区磁盘扩容的步骤? # 这个分3种\n第一种方法:\ngrowpart /dev/vda 1\rresize2fs /dev/vda1 第二种方法:\npartpeobe /dev/sda\rresize2fs /dev/vda1 第三种方法:\nfdisk /dev/sdb # n p 1 1 回车 回车 t 8e w\rpvcreate /dev/sdb1\rvgextend datavg /dev/sdb1\rlvextend -r -L +100%free /dev/mapper/datavg-lv01 3、讲述一下Tomcat8005、8009、8080三个端口的含义？ # 8005 关闭时使用 8009为AJP端口，即容器使用，如Apache能通过AJP协议访问Tomcat的8009端口来实现功能 8080 一般应用使用 4、简述DNS进行域名解析的过程？ # 迭代查询（返回最优结果）、递归查询（本地找DNS）用户要访问 www.baidu.com，会先找本机的host文件，再找本地设置的DNS服务器，如果也没有找到，就去网络中找根服务器，根服务器反馈结果，说只能提供一级域名服务器.cn，就去找一级域名服务器，一级域名服务器说只能提供二级域名服务器.com.cn,就去找二级域名服务器，二级域服务器只能提供三级域名服务器.baidu.com.cn，就去找三级域名服务器，三级域名服务器正好有这个网站www.baidu.com，然后发给请求的服务器，保存一份之后，再发给客户端。\n5、讲一下Keepalived的工作原理？ # 在一个虚拟路由器中，只有作为MASTER的VRRP(虚拟路由冗余协议)路由器会一直发送VRRP通告信息，BACKUP不会抢占MASTER，除非它的优先级更高。当MASTER不可用时(BACKUP收不到通告信息)多台BACKUP中优先级最高的这台会被抢占为MASTER。这种抢占是非常快速的(\u0026lt;1s)，以保证服务的连续性由于安全性考虑，VRRP包使用了加密协议进行加密。BACKUP不会发送通告信息，只会接收通告信息。\n6、LVS、Nginx、HAproxy有什么区别？工作中你怎么选择？ # LVS：\n抗负载能力强、工作在第4层仅作分发之用，没有流量的产生，这个特点也决定了它在负载均衡软件里的性能最强的；无流量，同时保证了均衡器IO的性能不会受到大流量的影响； 工作稳定，自身有完整的双机热备方案，如LVS+Keepalived和LVS+Heartbeat； 应用范围比较广，可以对所有应用做负载均衡； 配置简单，因为没有可太多配置的东西，所以并不需要太多接触，大大减少了人为出错的几率； LVS的缺点：\n软件本身不支持正则处理，不能做动静分离，这就凸显了Nginx/HAProxy+Keepalived的优势。 如果网站应用比较庞大，LVS/DR+Keepalived就比较复杂了，特别是后面有Windows Server应用的机器，实施及配置还有维护过程就比较麻烦，相对而言，Nginx/HAProxy+Keepalived就简单多了。 Nginx：\n工作在第7层，应用层，可以针对http应用做一些分流的策略。比如针对域名、目录结构。它的正则比HAProxy更为强大和灵活； Nginx对网络的依赖非常小，理论上能ping通就就能进行负载功能 Nginx安装和配置简单 可以承担高的负载压力且稳定，一般能支撑超过几万次的并发量； Nginx不仅仅是一款优秀的负载均衡器/反向代理软件，它同时也是功能强大的Web应用服务器。Nginx在处理静态页面、特别是抗高并发方面相对apache有优势； Nginx作为Web反向代理加速缓存越来越成熟，速度比传统的Squid服务器更快 Nginx的缺点：\nNginx不支持url来检测。 Nginx仅能支持http、https和Email协议 Nginx的Session的保持，Cookie的引导能力相对欠缺。 HAProxy：\nHAProxy是支持虚拟主机的，可以工作在4、7层(支持多网段)； 能够补充Nginx的一些缺点比如Session的保持，Cookie的引导等工作； 支持url检测后端的服务器； 它跟LVS一样，本身仅仅就只是一款负载均衡软件；单纯从效率上来讲HAProxy更会比Nginx有更出色的负载均衡速度，在并发处理上也是优于Nginx的； HAProxy可以对Mysql读进行负载均衡，对后端的MySQL节点进行检测和负载均衡，不过在后端的MySQL slaves数量超过10台时性能不如LVS； HAProxy的算法较多，达到8种； 工作选择：\nHAproxy和Nginx由于可以做七层的转发，所以URL和目录的转发都可以做在很大并发量的时候我们就要选择LVS，像中小型公司的话并发量没那么大选择HAproxy或者Nginx足已，由于HAproxy由是专业的代理服务器配置简单，所以中小型企业推荐使用HAproxy。\n7、docker的工作原理是什么，讲一下 # docker是一个Client-Server结构的系统，docker守护进程运行在宿主机上，守护进程从客户端接受命令并管理运行在主机上的容器，容器是一个运行时环境，这就是我们说的集装箱。\n8、docker的组成包含哪几大部分 # 一个完整的docker有以下几个部分组成：\ndocker client，客户端，为用户提供一系列可执行命令，用户用这些命令实现跟 docker daemon 交互； docker daemon，守护进程，一般在宿主主机后台运行，等待接收来自客户端的请求消息； docker image，镜像，镜像run之后就生成为docker容器； docker container，容器，一个系统级别的服务，拥有自己的ip和系统目录结构；运行容器前需要本地存在对应的镜像，如果本地不存在该镜像则就去镜像仓库下载。 docker 使用客户端-服务器 (C/S) 架构模式，使用远程api来管理和创建docker容器。docker 容器通过 docker 镜像来创建。容器与镜像的关系类似于面向对象编程中的对象与类。\n9、docker与传统虚拟机的区别什么？ # 传统虚拟机是需要安装整个操作系统的，然后再在上面安装业务应用，启动应用，通常需要几分钟去启动应用，而docker是直接使用镜像来运行业务容器的，其容器启动属于秒级别； Docker需要的资源更少，Docker在操作系统级别进行虚拟化，Docker容器和内核交互，几乎没有性能损耗，而虚拟机运行着整个操作系统，占用物理机的资源就比较多; Docker更轻量，Docker的架构可以共用一个内核与共享应用程序库，所占内存极小;同样的硬件环境，Docker运行的镜像数远多于虚拟机数量，对系统的利用率非常高; 与虚拟机相比，Docker隔离性更弱，Docker属于进程之间的隔离，虚拟机可实现系统级别隔离; Docker的安全性也更弱，Docker的租户root和宿主机root相同，一旦容器内的用户从普通用户权限提升为root权限，它就直接具备了宿主机的root权限，进而可进行无限制的操作。虚拟机租户root权限和宿主机的root虚拟机权限是分离的，并且虚拟机利用如Intel的VT-d和VT-x的ring-1硬件隔离技术，这种技术可以防止虚拟机突破和彼此交互，而容器至今还没有任何形式的硬件隔离; Docker的集中化管理工具还不算成熟，各种虚拟化技术都有成熟的管理工具，比如：VMware vCenter提供完备的虚拟机管理能力; Docker对业务的高可用支持是通过快速重新部署实现的，虚拟化具备负载均衡，高可用、容错、迁移和数据保护等经过生产实践检验的成熟保障机制，Vmware可承诺虚拟机99.999%高可用，保证业务连续性; 虚拟化创建是分钟级别的，Docker容器创建是秒级别的，Docker的快速迭代性，决定了无论是开发、测试、部署都可以节省大量时间; 虚拟机可以通过镜像实现环境交付的一致性，但镜像分发无法体系化，Docker在Dockerfile中记录了容器构建过程，可在集群中实现快速分发和快速部署。from wljslmz 10、docker技术的三大核心概念是什么？ # 镜像：镜像是一种轻量级、可执行的独立软件包，它包含运行某个软件所需的所有内容，我们把应用程序和配置依赖打包好形成一个可交付的运行环境(包括代码、运行时需要的库、环境变量和配置文件等)，这个打包好的运行环境就是image镜像文件。 容器：容器是基于镜像创建的，是镜像运行起来之后的一个实例，容器才是真正运行业务程序的地方。如果把镜像比作程序里面的类，那么容器就是对象。 镜像仓库：存放镜像的地方，研发工程师打包好镜像之后需要把镜像上传到镜像仓库中去，然后就可以运行有仓库权限的人拉取镜像来运行容器了。 11、centos镜像几个G，但是docker centos镜像才几百兆，这是为什么？ # 一个完整的Linux操作系统包含Linux内核和rootfs根文件系统，即我们熟悉的/dev、/proc/、/bin等目录。我们平时看到的centOS除了rootfs，还会选装很多软件，服务，图形桌面等，所以centOS镜像有好几个G也不足为奇。\n而对于容器镜像而言，所有容器都是共享宿主机的Linux 内核的，而对于docker镜像而言，docker镜像只需要提供一个很小的rootfs即可，只需要包含最基本的命令，工具，程序库即可，所有docker镜像才会这么小。\n12、讲一下镜像的分层结构以及为什么要使用镜像的分层结构？ # 一个新的镜像其实是从 base 镜像一层一层叠加生成的。每安装一个软件，dockerfile中使用RUM命令，就会在现有镜像的基础上增加一层，这样一层一层的叠加最后构成整个镜像。所以我们docker pull拉取一个镜像的时候会看到docker是一层层拉去的。\n分层机构最大的一个好处就是 ：共享资源。比如：有多个镜像都从相同的 base 镜像构建而来，那么 Docker Host 只需在磁盘上保存一份 base 镜像；同时内存中也只需加载一份 base 镜像，就可以为所有容器服务了。而且镜像的每一层都可以被共享。\n13、讲一下容器的copy-on-write特性，修改容器里面的内容会修改镜像吗？ # 我们知道，镜像是分层的，镜像的每一层都可以被共享，同时，镜像是只读的。当一个容器启动时，一个新的可写层被加载到镜像的顶部，这一层通常被称作“容器层”，“容器层”之下的都叫“镜像层”。\n所有对容器的改动 - 无论添加、删除、还是修改文件，都只会发生在容器层中，因为只有容器层是可写的，容器层下面的所有镜像层都是只读的。镜像层数量可能会很多，所有镜像层会联合在一起组成一个统一的文件系统。如果不同层中有一个相同路径的文件，比如 /a，上层的 /a 会覆盖下层的 /a，也就是说用户只能访问到上层中的文件 /a。在容器层中，用户看到的是一个叠加之后的文件系统。\n添加文件：在容器中创建文件时，新文件被添加到容器层中。 读取文件：在容器中读取某个文件时，Docker 会从上往下依次在各镜像层中查找此文件。一旦找到，立即将其复制到容器层，然后打开并读入内存。 修改文件：在容器中修改已存在的文件时，Docker 会从上往下依次在各镜像层中查找此文件。一旦找到，立即将其复制到容器层，然后修改之。 删除文件：在容器中删除文件时，Docker 也是从上往下依次在镜像层中查找此文件。找到后，会在容器层中记录下此删除操作。 只有当需要修改时才复制一份数据，这种特性被称作 Copy-on-Write。可见，容器层保存的是镜像变化的部分，不会对镜像本身进行任何修改。\n14、简单描述一下Dockerfile的整个构建镜像过程 # 首先，创建一个目录用于存放应用程序以及构建过程中使用到的各个文件等； 然后，在这个目录下创建一个Dockerfile文件，一般建议Dockerfile的文件名就是Dockerfile； 编写Dockerfile文件，编写指令，如，使用FORM指令指定基础镜像，COPY指令复制文件，RUN指令指定要运行的命令，ENV设置环境变量，EXPOSE指定容器要暴露的端口，WORKDIR设置当前工作目录，CMD容器启动时运行命令，等等指令构建镜像； Dockerfile编写完成就可以构建镜像了，使用docker build -t 镜像名:tag . 命令来构建镜像，最后一个点是表示当前目录，docker会默认寻找当前目录下的Dockerfile文件来构建镜像，如果不使用默认，可以使用-f参数来指定dockerfile文件，如：docker build -t 镜像名:tag -f /xx/xxx/Dockerfile ； 使用docker build命令构建之后，docker就会将当前目录下所有的文件发送给docker daemon，顺序执行Dockerfile文件里的指令，在这过程中会生成临时容器，在临时容器里面安装RUN指定的命令，安装成功后，docker底层会使用类似于docker commit命令来将容器保存为镜像，然后删除临时容器，以此类推，一层层的构建镜像，运行临时容器安装软件，直到最后的镜像构建成功。 15、Dockerfile构建镜像出现异常，如何排查？ # 首先，Dockerfile是一层一层的构建镜像，期间会产生一个或多个临时容器，构建过程中其实就是在临时容器里面安装应用，如果因为临时容器安装应用出现异常导致镜像构建失败，这时容器虽然被清理掉了，但是期间构建的中间镜像还在，那么我们可以根据异常时上一层已经构建好的临时镜像，将临时镜像运行为容器，然后在容器里面运行安装命令来定位具体的异常。\n16、Dockerfile的基本指令有哪些？ # FROM 指定基础镜像（必须为第一个指令，因为需要指定使用哪个基础镜像来构建镜像）； MAINTAINER 设置镜像作者相关信息，如作者名字，日期，邮件，联系方式等； COPY 复制文件到镜像； ADD 复制文件到镜像（ADD与COPY的区别在于，ADD会自动解压tar、zip、tgz、xz等归档文件，而COPY不会，同时ADD指令还可以接一个url下载文件地址，一般建议使用COPY复制文件即可，文件在宿主机上是什么样子复制到镜像里面就是什么样子这样比较好）； ENV 设置环境变量； EXPOSE 暴露容器进程的端口，仅仅是提示别人容器使用的哪个端口，没有过多作用； VOLUME 数据卷持久化，挂载一个目录； WORKDIR 设置工作目录，如果目录不在，则会自动创建目录； RUN 在容器中运行命令，RUN指令会创建新的镜像层，RUN指令经常被用于安装软件包； CMD 指定容器启动时默认运行哪些命令，如果有多个CMD，则只有最后一个生效，另外，CMD指令可以被docker run之后的参数替换； ENTRYOINT 指定容器启动时运行哪些命令，如果有多个ENTRYOINT，则只有最后一个生效，另外，如果Dockerfile中同时存在CMD和ENTRYOINT，那么CMD或docker run之后的参数将被当做参数传递给ENTRYOINT； 17、如何进入容器？使用哪个命令 # 进入容器有两种方法：docker attach、docker exec。\n18、什么是k8s？说出你的理解 # K8s是kubernetes的简称，其本质是一个开源的容器编排系统，主要用于管理容器化的应用，其目标是让部署容器化的应用简单并且高效（powerful）,Kubernetes提供了应用部署，规划，更新，维护的一种机制。\n说简单点：k8s就是一个编排容器的系统，一个可以管理容器应用全生命周期的工具，从创建应用，应用的部署，应用提供服务，扩容缩容应用，应用更新，都非常的方便，而且还可以做到故障自愈，所以，k8s是一个非常强大的容器编排系统。\n19、k8s的组件有哪些，作用分别是什么？ # k8s主要由master节点和node节点构成。master节点负责管理集群，node节点是容器应用真正运行的地方。\nmaster节点包含的组件有：kube-api-server、kube-controller-manager、kube-scheduler、etcd。 node节点包含的组件有：kubelet、kube-proxy、container-runtime。 kube-api-server：以下简称api-server，api-server是k8s最重要的核心组件之一，它是k8s集群管理的统一访问入口，提供了RESTful API接口, 实现了认证、授权和准入控制等安全功能；api-server还是其他组件之间的数据交互和通信的枢纽，其他组件彼此之间并不会直接通信，其他组件对资源对象的增、删、改、查和监听操作都是交由api-server处理后，api-server再提交给etcd数据库做持久化存储，只有api-server才能直接操作etcd数据库，其他组件都不能直接操作etcd数据库，其他组件都是通过api-server间接的读取，写入数据到etcd。\nkube-controller-manager：以下简称controller-manager，controller-manager是k8s中各种控制器的的管理者，是k8s集群内部的管理控制中心，也是k8s自动化功能的核心；controller-manager内部包含replication controller、node controller、deployment controller、endpoint controller等各种资源对象的控制器，每种控制器都负责一种特定资源的控制流程，而controller-manager正是这些controller的核心管理者。\nkube-scheduler：以下简称scheduler，scheduler负责集群资源调度，其作用是将待调度的pod通过一系列复杂的调度算法计算出最合适的node节点，然后将pod绑定到目标节点上。shceduler会根据pod的信息（关注微信公众号：网络技术联盟站），全部节点信息列表，过滤掉不符合要求的节点，过滤出一批候选节点，然后给候选节点打分，选分最高的就是最佳节点，scheduler就会把目标pod安置到该节点。\nEtcd：etcd是一个分布式的键值对存储数据库，主要是用于保存k8s集群状态数据，比如，pod，service等资源对象的信息；etcd可以是单个也可以有多个，多个就是etcd数据库集群，etcd通常部署奇数个实例，在大规模集群中，etcd有5个或7个节点就足够了；另外说明一点，etcd本质上可以不与master节点部署在一起，只要master节点能通过网络连接etcd数据库即可。\nkubelet：每个node节点上都有一个kubelet服务进程，kubelet作为连接master和各node之间的桥梁，负责维护pod和容器的生命周期，当监听到master下发到本节点的任务时，比如创建、更新、终止pod等任务，kubelet 即通过控制docker来创建、更新、销毁容器；每个kubelet进程都会在api-server上注册本节点自身的信息，用于定期向master汇报本节点资源的使用情况。\nkube-proxy：kube-proxy运行在node节点上，在Node节点上实现Pod网络代理，维护网络规则和四层负载均衡工作，kube-proxy会监听api-server中从而获取service和endpoint的变化情况，创建并维护路由规则以提供服务IP和负载均衡功能。简单理解此进程是Service的透明代理兼负载均衡器，其核心功能是将到某个Service的访问请求转发到后端的多个Pod实例上。\ncontainer-runtime：容器运行时环境，即运行容器所需要的一系列程序，目前k8s支持的容器运行时有很多，如docker、rkt或其他，比较受欢迎的是docker，但是新版的k8s已经宣布弃用docker。\n20、kubelet的功能、作用是什么？（重点，经常会问） # kubelet部署在每个node节点上的，它主要有4个功能：\n节点管理。kubelet启动时会向api-server进行注册，然后会定时的向api-server汇报本节点信息状态，资源使用状态等，这样master就能够知道node节点的资源剩余，节点是否失联等等相关的信息了。master知道了整个集群所有节点的资源情况，这对于 pod 的调度和正常运行至关重要。 pod管理。kubelet负责维护node节点上pod的生命周期，当kubelet监听到master的下发到自己节点的任务时，比如要创建、更新、删除一个pod，kubelet 就会通过CRI（容器运行时接口）插件来调用不同的容器运行时来创建、更新、删除容器；常见的容器运行时有docker、containerd、rkt等等这些容器运行时，我们最熟悉的就是docker了，但在新版本的k8s已经弃用docker了，k8s1.24版本中已经使用containerd作为容器运行时了。 容器健康检查。pod中可以定义启动探针、存活探针、就绪探针等3种，我们最常用的就是存活探针、就绪探针，kubelet 会定期调用容器中的探针来检测容器是否存活，是否就绪，如果是存活探针，则会根据探测结果对检查失败的容器进行相应的重启策略； Metrics Server资源监控。在node节点上部署Metrics Server用于监控node节点、pod的CPU、内存、文件系统、网络使用等资源使用情况，而kubelet则通过Metrics Server获取所在节点及容器的上的数据。 21、kube-api-server的端口是多少？各个pod是如何访问kube-api-server的？ # kube-api-server的端口是8080和6443，前者是http的端口，后者是https的端口，以我本机使用kubeadm安装的k8s为例：\n在命名空间的kube-system命名空间里，有一个名称为kube-api-master的pod，这个pod就是运行着kube-api-server进程，它绑定了master主机的ip地址和6443端口，但是在default命名空间下，存在一个叫kubernetes的服务，该服务对外暴露端口为443，目标端口6443，这个服务的ip地址是clusterip地址池里面的第一个地址，同时这个服务的yaml定义里面并没有指定标签选择器，也就是说这个kubernetes服务所对应的endpoint是手动创建的，该endpoint也是名称叫做kubernetes，该endpoint的yaml定义里面代理到master节点的6443端口，也就是kube-api-server的IP和端口。这样一来，其他pod访问kube-api-server的整个流程就是：pod创建后嵌入了环境变量，pod获取到了kubernetes这个服务的ip和443端口，请求到kubernetes这个服务其实就是转发到了master节点上的6443端口的kube-api-server这个pod里面。\n22、k8s中命名空间的作用是什么？ # amespace是kubernetes系统中的一种非常重要的资源，namespace的主要作用是用来实现多套环境的资源隔离，或者说是多租户的资源隔离。\nk8s通过将集群内部的资源分配到不同的namespace中，可以形成逻辑上的隔离，以方便不同的资源进行隔离使用和管理。不同的命名空间可以存在同名的资源，命名空间为资源提供了一个作用域。\n可以通过k8s的授权机制，将不同的namespace交给不同的租户进行管理，这样就实现了多租户的资源隔离，还可以结合k8s的资源配额机制，限定不同的租户能占用的资源，例如CPU使用量、内存使用量等等来实现租户可用资源的管理。\n23、pod资源控制器类型有哪些? # Deployments：Deployment为Pod和ReplicaSet提供声明式的更新能力。 ReplicaSet：ReplicaSet的目的是维护一组在任何时候都处于运行状态的Pod副本的稳定集合。因此，它通常用来保证给定数量的、完全相同的Pod的可用性。 StatefulSets：和Deployment类似，StatefulSet管理基于相同容器规约的一组Pod。但和Deployment不同的是，StatefulSet为它们的每个Pod维护了一个有粘性的ID。这些Pod是基于相同的规约来创建的，但是不能相互替换：无论怎么调度，每个Pod都有一个永久不变的ID。 DaemonSet：DaemonSet确保全部（或者某些）节点上运行一个Pod的副本。当有节点加入集群时，也会为他们新增一个Pod。当有节点从集群移除时，这些Pod也会被回收。删除DaemonSet将会删除它创建的所有Pod。 Jobs：Job会创建一个或者多个Pod，并将继续重试Pod的执行，直到指定数量的Pod成功终止。随着Pod成功结束，Job跟踪记录成功完成的Pod个数。当数量达到指定的成功个数阈值时，任务（即Job）结束。删除Job的操作会清除所创建的全部Pod。挂起Job的操作会删除Job的所有活跃Pod，直到Job被再次恢复执行。 Automatic Clean-up for Finished Jobs：TTL-after-finished控制器提供了一种TTL机制来限制已完成执行的资源对象的生命周期。TTL控制器目前只处理Job。 CronJob：一个CronJob对象就像crontab(crontable)文件中的一行。它用Cron格式进行编写，并周期性地在给定的调度时间执行Job。 ReplicationController：ReplicationController确保在任何时候都有特定数量的Pod副本处于运行状态。换句话说，ReplicationController确保一个Pod或一组同类的Pod总是可用的。 24、nginx算法策略 # 轮询（默认）\n加权轮询（轮询+weight）\nip_hash\n每一个请求的访问IP，都会映射成一个hash，再通过hash算法（hash值%node_count），分配到不同的后端服务器，访问ip相同的请求会固定访问同一个后端服务器，这样可以做到会话保持，解决session同步问题。\nleast_conn（最少连接）\n使用最少连接的负载平衡，nginx将尝试不会使繁忙的应用程序服务器超载请求过多，而是将新请求分发给不太繁忙的服务器。\n25、nignx常用模块 # upstream rewrite location proxy_pass 26、如何查看并且杀死僵尸进程？ # top —\u0026gt; task (line)—\u0026gt; zombie.\n把父进程杀掉，父进程死后，过继给1号进程init，init 始终负责清理僵尸进程，它产生的所有僵尸进程跟着消失；如果你使用kill ，一般都不能杀掉 defunct进程.。用了kill -15,kill -9以后 之后反而会多出更多的僵尸进程。\n27、搜索某个用户运行的进程 # pgrep -au neteagle 28、查看某个端口正在被哪个进程使用 # lsof -i :[port] 29、端口转发 # iptables -t nat -A PREROUTING -d 10.0.0.8 -p tcp --dport 80 -j REDIRECT --to-ports 8080 30、查看http的并发请求数与其TCP连接状态 # etstat-n|awk\u0026#39;/^tcp/{++b[$NF]}END{for(ainb)printa,b[a]}\u0026#39; 31、查看/var/log目录下文件数 # ls/var/log/-lR|grep\u0026#34;^-\u0026#34;|wc-l 32、linux系统启动流程 # 第一步：开机自检，加载BIOS 第二步：读取ＭＢＲ 第三步：Boot Loader　grub引导菜单 第四步：加载kernel内核 第五步：init进程依据inittab文件夹来设定运行级别 第六步：init进程执行rc.sysinit 第七步：启动内核模块 第八步：执行不同运行级别的脚本程序 第九步：执行/etc/rc.d/rc.lo 33、Linux文件类型 # -：常规文件，即file d：目录文件 b：block device 即块设备文件，如硬盘;支持以block为单位进行随机访问 c：character device 即字符设备文件，如键盘支持以character为单位进行线性访问 l：symbolic link 即符号链接文件（关注微信公众号：网络技术联盟站），又称软链接文件 p：pipe 即命名管道文件 s：socket 即套接字文件，用于实现两个进程进行通信 34、简述lvm，如何给使用lvm的/分区扩容？ # 功能：可以对磁盘进行动态管理。动态按需调整大小\n概念：\nPV 物理卷：物理卷在逻辑卷管理中处于最底层，它可以是实际物理硬盘上的分区，也可以是整个物理硬盘，也可以是raid设备。 VG 卷组：卷组建立在物理卷之上，一个卷组中至少要包括一个物理卷，在卷组建立之后可动态添加物理卷到卷组中。一个逻辑卷管理系统工程中可以只有一个卷组，也可以拥有多个卷组。 LV 逻辑卷：逻辑卷建立在卷组之上，卷组中的未分配空间可以用于建立新的逻辑卷，逻辑卷建立后可以动态地扩展和缩小空间。系统中的多个逻辑卷可以属于同一个卷组，也可以属于不同的多个卷组。 给/分区扩容步骤：\n添加磁盘 使用fdisk命令对新增加的磁盘进行分区 分区完成后修改分区类型为lvm 使用pvcreate创建物理卷 使用vgextend命令将新增加的分区加入到根目录分区中 使用lvextend命令进行扩容 使用xfs_growfs调整卷分区大小 35、如何在文本里面进行复制、粘贴，删除行，删除全部，按行查找和按字母查找。 # 以下操作全部在vi/vim命令行状态操作，不要在编辑状态操作：\n在文本里 移动到想要复制的行按yy想复制到哪就移动到哪，然后按P就黏贴了 删除行 移动到改行 按dd 删除全部dG这里注意G一定要大写 按行查找 :90 这样就是找到第90行 按字母查找 /path 这样就是找到path这个单词所在的位置，文本里可能存在多个，多次查找会显示在不同的位置。 36、符号链接与硬链接的区别 # 我们可以把符号链接，也就是软连接 当做是 windows系统里的 快捷方式。 硬链接 就好像是 又复制了一份. ln 3.txt 4.txt 这是硬链接，相当于复制，不可以跨分区，但修改3,4会跟着变，若删除3,4不受任何影响。 ln -s 3.txt 4.txt 这是软连接，相当于快捷方式。修改4,3也会跟着变，若删除3,4就坏掉了。不可以用了。 37、什么是正向代理？ # 一个位于客户端和原始服务器(origin server)之间的服务器，为了从原始服务器取得内容，客户端向代理发送一个请求并指定目标(原始服务器)，然后代理向原始服务器转交请求并将获得的内容返回给客户端。\n客户端才能使用正向代理。正向代理总结就一句话：代理端代理的是客户端。例如说：我们使用的OpenVPN 等等。\n38、什么是反向代理？ # 反向代理（Reverse Proxy）方式，是指以代理服务器来接受 Internet上的连接请求，然后将请求，发给内部网络上的服务器并将从服务器上得到的结果返回给 Internet 上请求连接的客户端，此时代理服务器对外就表现为一个反向代理服务器。\n反向代理总结就一句话：代理端代理的是服务端。\n39、什么是动态资源、静态资源分离？ # 动态资源、静态资源分离，是让动态网站里的动态网页根据一定规则把不变的资源和经常变的资源区分开来，动静资源做好了拆分以后我们就可以根据静态资源的特点将其做缓存操作，这就是网站静态化处理的核心思路。\n动态资源、静态资源分离简单的概括是：动态文件与静态文件的分离。\n在我们的软件开发中，有些请求是需要后台处理的（如：.jsp,.do 等等），有些请求是不需要经过后台处理的（如：css、html、jpg、js 等等文件），这些不需要经过后台处理的文件称为静态文件，否则动态文件。\n因此我们后台处理忽略静态文件。这会有人又说那我后台忽略静态文件不就完了吗？当然这是可以的，但是这样后台的请求次数就明显增多了。在我们对资源的响应速度有要求的时候，我们应该使用这种动静分离的策略去解决动、静分离将网站静态资源（HTML，JavaScript，CSS，img等文件）与后台应用分开部署，提高用户访问静态代码的速度，降低对后台应用访问\n这里我们将静态资源放到 Nginx 中，动态资源转发到 Tomcat 服务器中去。\n当然，因为现在七牛、阿里云等 CDN 服务已经很成熟，主流的做法，是把静态资源缓存到 CDN 服务中，从而提升访问速度。\n相比本地的 Nginx 来说，CDN 服务器由于在国内有更多的节点，可以实现用户的就近访问。并且，CDN 服务可以提供更大的带宽，不像我们自己的应用服务，提供的带宽是有限的。\n40、网站登陆缓慢是什么原因? # 网络带宽，这是一个很常见的瓶颈。 cpu、硬盘、内存配置过低，服务器负载不起来。 网站的开发代码不够完善，例如mysql语句没有进行优化，导致数据库的读写相当耗费时间。 数据库的瓶颈。当我们的数据库的数据变得越来越多的时候，那么对于数据库的读写压力肯定会变大。 41、a与b服务器不在同一网段怎么设置?设置完还ping不通怎么排查? # AB服务器不在同一个网段 首先把不同IP段的服务器划分给不同的vlan 在通过通过三层交换机添加虚拟IP路由实在不同网段的vlan的连接 42、在AB两台服务器之间通过一个服务器c做软路由使用给路由器c配置两块网卡并开启自身的路由功能 # vi /etc/sysconfig/network-scripts/ifcfg-eth0 查看网卡状况IP -a -s 网卡的名字\nA服务器设置相关网卡信息\n子网掩码：255.255.255.0\rIP=10.0.0.1\r网关=10.0.0.254\r重启网卡生效\r查看路由信息\rroute -n\r添加对应路由\rroute add -net 10.0.1.0/24 gw 10.0.0.11 B服务器的设置相关信息\nIP=10.0.1.10\r网关10.0.1.254\r重启网卡生效\rroute -n\r添加对应的路由\rroute add -net 10.0.0.0/24 gw 10.0.1.11 C服务器的两块网卡\n网卡1\rIP=10.0.0.11\r网关=10.0.0.254\r网卡2\rIP=10.0.1.11\r网关=10.0.1.254\r重启网卡生效\rroute -n\rvi /etc/sysctl.conf\rnet.ipv4.ip_forword = 1 43、如果PING不通怎么排查 # 首先先看看是不是网路接口故障水晶头或是网卡接口接触不良造成，其次检查交换机和路由等网络设备是有故障 是否关闭了防火墙和selinux机制 然后查看网卡和路由和网关是否配置正确 44、docker容器ping不通是什么原因? # ifconfig 查看一下docker0网桥，ping一下网桥看看是否通，有可能是网桥配置问题\nweave路由器端口6783\n安装docker容器的服务器没有关闭防火墙(访问一下安装docker物理机的，是否能访问，如果不能访问就变不能访问docker) docker在创建镜像的时候没有做端口映射(出现这种情况能访问物理机不能访问docker)使用dockers ps 查看镜像的端口映射情况 端口映射不正确 查看网络配置ping网桥看是否能ping通，有可能是网桥的原因 45、如果一台办公室内主机无法上网(打不开网站)，请给出你的排查步骤? # 首先确定物理链路是否联通正常。 查看本机IP，路由，DNS的设置情况是否达标。 telnet检查服务器的WEB有没有开启以及防火墙是否阻拦。 ping一下网关，进行最基础的检查，通了，表示能够到达服务器。 测试到网关或路由器的通常情况，先测网关，然后再测路由器一级一级的测试。 测试ping公网ip的通常情况(记住几个外部IP)， 测试DNS的通畅。ping出对应IP。 通过以上检查后，还在网管的路由器上进行检查。 46、如果我们的网站打开速度慢请说下您的排查思路? # 判断原因\n首先我要以用户的身份登录我们的网站，判断问题出现在我们自身原因，还是用户那边的原因。\n如果是用户问题有以下几个原因：\n用户那边的带宽 用户的浏览器器版本低，安装插件太多 中毒和电脑里的垃圾文件过多 用户主机的主机的性能和操作系统 如果是我们的网站自身问题有一下几个原因\n网络带宽 服务器的cpu、硬盘、内存过低服务器负载不起来也就是说服务器自身的性能方面 网站代码不够完善。如mysql语句没有进行优化导致数据库读写耗时 服务器未开启图片压缩 网页台下 死连接过多插件使用及js文件调用频繁网站服务器的速度或是租用空间所在的服务器速度 解决思路\n1、检测服务器速度的快慢\nping命令查看连接到服务器的时间和丢包情况(ping 测试网址的) 查看丢包率(1000个包没有丢一个是最理想的、一般一个速度好的机房丢包率不超过1%) ping值要小同城电信adsl ping平均值绝对不能超过20，一般都在10，跨省的平均值20-40属于正常 ping值要均匀最小值和最大值相差太大说明路由不稳定的表现 2、查看服务器自身性能\n查看cpu的使用率uptime\n查看内存情况 free -m\n查看I/O读写iostat 磁盘I/O读写等看看是那个进程大量占用系统资源导致我的服务器变慢\n3、看看访问最多的URL和IP有什么特征，如果是恶意URL和IP就把他屏蔽掉如果是善意的就限流有可能是CDN回源量大造成网站无法访问\n4、查看同台服务器上其他网站的打开速度，可以通过查询工具查看和自己在同一台服务器上的网站个数和网址可以看他们打开快慢\n5、电信和联通互访的问题\n如果是空间打开时快时慢，有时打不开那就是空间不稳定找空间商解决或是换空间伤，如果是有的地方快有的地方慢应该是网络线路问题，比如电信用户访问放在联通服务器上的网站，联通用户访问放在电信服务器上的网站，解决办吧是：使用双线空间或是多线空间\n6、从网站自身的原因\n网站的程序设计结构是否合理是否由于幻灯片代码影响网站打开速度(找程序设计相关人士解决) 网页的设计结构和代码错误(请专业人士进行修改) 网页的内容如：大尺寸图片、大尺寸flash、过多的引用其他网站内容，如果被引用内容的网站速度慢，也影响自身网站把。譬如友情连接可以把对方 的图片放到自己网站上 解决办法\n优化图片，限制图片大小尺寸，降低图片质量，减少图片数量 限定图片的格式：jpg，png，gif 减少http的请求数(当打开网页时浏览器会发出很多对象请求，每个对象的加载都会有所延时，如果网页上的对象很多就会花费大量的时间，去除不必要的对象，将临近的图片合成一张，合并css文件) f r o m ：w l j s l m z 46、如何查看二进制文件的内容 # 我们一般通过 hexdump 命令 来查看二进制文件的内容。\nhexdump -C XXX(文件名) -C 是参数 不同的参数有不同的意义\n-C 是比较规范的 十六进制和 ASCII 码显示\n-c 是单字节字符显示\n-b 单字节八进制显示\n-o 是双字节八进制显示\n-d 是双字节十进制显示\n-x 是双字节十六进制显示\n等等等等\n47、你是怎么备份数据的，包括数据库备份？ # 在生产环境下，不管是应用数据、还是数据库数据首先在部署的时候就会有主从架构、或者集群，这本身就是属于数据的热备份；其实考虑冷备份，用专门一台服务器做为备份服务器，比如可以用rsync+inotify配合计划任务来实现数据的冷备份，如果是发版的包备份，正常情况下有台发布服务器，每次发版都会保存好发版的包。\n48、zabbix常用术语你知道几个？ # 主机（host）：要监控的网络设备，可由IP或DNS名称指定； 主机组（hostgroup）：主机的逻辑容器，可以包含主机和模板，但同一个组织内的主机和模板不能互相链接；主机组通常在给用户或用户组指派监控权限时使用； 监控项（item）：一个特定监控指标的相关的数据；这些数据来自于被监控对象；item是zabbix进行数据收集的核心，相对某个监控对象，每个item都由\u0026quot;key\u0026quot;标识； 触发器（trigger）：一个表达式，用于评估某监控对象的特定item内接收到的数据是否在合理范围内，也就是阈值；接收的数据量大于阈值时，触发器状态将从\u0026quot;OK\u0026quot;转变为\u0026quot;Problem\u0026quot;，当数据再次恢复到合理范围，又转变为\u0026quot;OK\u0026quot;； 事件（event）：触发一个值得关注的事情，比如触发器状态转变，新的agent或重新上线的agent的自动注册等； 动作（action）：指对于特定事件事先定义的处理方法，如发送通知，何时执行操作； 报警升级（escalation）：发送警报或者执行远程命令的自定义方案，如每隔5分钟发送一次警报，共发送5次等； 媒介（media）：发送通知的手段或者通道，如Email、Jabber或者SMS等； 通知（notification）：通过选定的媒介向用户发送的有关某事件的信息；远程命令（remote command）：预定义的命令，可在被监控主机处于某特定条件下时自动执行； 模板（template）：用于快速定义被监控主机的预设条目集合，通常包含了item、trigger、graph、screen、application以及low-level discovery rule；模板可以直接链接至某个主机；应用（application）：一组item的集合； web场景（webscennario）：用于检测web站点可用性的一个活多个HTTP请求；前端（frontend）：Zabbix的web接口； 49、虚拟化技术有哪些表现形式 # 完全拟化技术：通过软件实现对操作系统的资源再分配，比较成熟，完全虚拟化代表技术：KVM、ESXI、Hyper-V。 半虚拟化技术：通过代码修改已有的系统，形成一种新的可虚拟化的系统，调用硬件资源去安装多个系统，整体速度上相对高一点，半虚拟化代表技术：Xen。 轻量级虚拟化：介于完全虚拟化、半虚拟化之间，轻量级虚拟化代表技术：Docker。 50、修改线上业务配置文件流程 # 先告知运维经理和业务相关开发人员 在测试环境测试，并备份之前的配置文件 测试无误后修改生产环境配置 观察生产环境是否正常，是否有报警 完成配置文件更改 "},{"id":59,"href":"/docs/2025-1-1-%E5%A4%A7%E5%A0%B0%E6%B2%B3-%E6%88%91%E7%9A%84%E4%BF%9D%E5%A7%86/","title":"2025-1-1 大堰河-我的保姆","section":"Docs","content":" 大堰河——我的保姆 # 【作者】艾青 【朝代】现代\n大堰河，是我的保姆。 她的名字就是生她的村庄的名字， 她是童养媳， 大堰河，是我的保姆。 我是地主的儿子； 也是吃了大堰河的奶而长大了的 大堰河的儿子。 大堰河以养育我而养育她的家， 而我，是吃了你的奶而被养育了的， 大堰河啊，我的保姆。 大堰河，今天我看到雪使我想起了你： 你的被雪压着的草盖的坟墓， 你的关闭了的故居檐头的枯死的瓦菲， 你的被典押了的一丈平方的园地， 你的门前的长了青苔的石椅， 大堰河，今天我看到雪使我想起了你。 你用你厚大的手掌把我抱在怀里，抚摸我； 在你搭好了灶火之后， 在你拍去了围裙上的炭灰之后， 在你尝到饭已煮熟了之后， 在你把乌黑的酱碗放到乌黑的桌子上之后， 在你补好了儿子们的为山腰的荆棘扯破的衣服之后， 在你把小儿被柴刀砍伤了的手包好之后， 在你把夫儿们的衬衣上的虱子一颗颗地掐死之后， 在你拿起了今天的第一颗鸡蛋之后， 你用你厚大的手掌把我抱在怀里，抚摸我。 我是地主的儿子， 在我吃光了你大堰河的奶之后， 我被生我的父母领回到自己的家里。 啊，大堰河，你为什么要哭？ 我做了生我的父母家里的新客了！ 我摸着红漆雕花的家具， 我摸着父母的睡床上金色的花纹， 我呆呆地看着檐头的我不认得的“天伦叙乐”的匾， 我摸着新换上的衣服的丝的和贝壳的纽扣， 我看着母亲怀里的不熟识的妹妹， 我坐着油漆过的安了火钵的炕凳， 我吃着碾了三番的白米的饭， 但，我是这般忸怩（niǔní）不安！因为我 我做了生我的父母家里的新客了。 大堰河，为了生活， 在她流尽了她的乳汁之后， 她就开始用抱过我的两臂劳动了； 她含着笑，洗着我们的衣服， 她含着笑，提着菜篮到村边的结冰的池塘去， 她含着笑，切着冰屑悉索的萝卜， 她含着笑，用手掏着猪吃的麦糟， 她含着笑，扇着炖肉的炉子的火， 她含着笑，背了团箕到广场上去， 晒好那些大豆和小麦， 大堰河，为了生活， 在她流尽了她的乳液之后， 她就用抱过我的两臂，劳动了。 大堰河，深爱着她的乳儿； 在年节里，为了他，忙着切那冬米的糖， 为了他，常悄悄地走到村边的她的家里去， 为了他，走到她的身边叫一声“妈”， 大堰河，把他画的大红大绿的关云长 贴在灶边的墙上， 大堰河，会对她的邻居夸口赞美她的乳儿； 大堰河曾做了一个不能对人说的梦： 在梦里，她吃着她的乳儿的婚酒， 坐在辉煌的结彩的堂上， 而她的娇美的媳妇亲切的叫她“婆婆” 。\u0026hellip;.. 大堰河，深爱着她的乳儿！ 大堰河，在她的梦没有做醒的时候已死了。 她死时，乳儿不在她的旁侧， 她死时，平时打骂她的丈夫也为她流泪， 五个儿子，个个哭得很悲， 她死时，轻轻地呼着她的乳儿的名字， 大堰河，已死了， 她死时，乳儿不在她的旁侧。 大堰河，含泪的去了！ 同着四十几年的人世生活的凌侮， 同着数不尽的奴隶的凄苦， 同着四块钱的棺材和几束稻草， 同着几尺长方的埋棺材的土地， 同着一手把的纸钱的灰， 大堰河，她含泪的去了。 这是大堰河所不知道的： 她的醉酒的丈夫已死去， 大儿做了土匪， 第二个死在炮火的烟里， 第三，第四，第五 在师傅和地主的叱骂声里过着日子。 而我，我是在写着给予这不公道的世界的咒语。 当我经了长长的漂泊回到故土时， 在山腰里，田野上， 兄弟们碰见时，是比六七年前更要亲密！ 这，这是为你，静静地睡着的大堰河 所不知道的啊！ 大堰河，今天，你的乳儿是在狱里， 写着一首呈给你的赞美诗， 呈给你黄土下紫色的灵魂， 呈给你拥抱过我的直伸着的手， 呈给你吻过我的唇， 呈给你泥黑的温柔的脸颜， 呈给你养育了我的乳房， 呈给你的儿子们，我的兄弟们， 呈给大地上一切的， 我的大堰河般的保姆和她们的儿子， 呈给爱我如爱她自己的儿子般的大堰河。 大堰河， 我是吃了你的奶而长大了的 你的儿子， 我敬你 爱你！ 一九三三年一月十四日，雪\n《大堰河——我的保姆》是现代诗人艾青的诗作。此诗以诗人幼年生活为背景，通过对诗人乳母大堰河的回忆与追思，集中描述了大堰河一生的悲苦经历，抒发了诗人对贫苦农妇大堰河及劳动人民的真挚怀念和感激之情，表达了诗人对中国广大劳动妇女的赞美以及对这“不公道的世界”的诅咒，从而激发世人对旧中国劳苦大众悲惨命运的同情以及对旧世界的强烈仇恨。全诗采用散文化的自由句式，不拘泥于句子的长短，不生硬地押韵，显得自然贴切，大量运用洋溢着农村风俗气息的生活细节，使得人物形象生动而真实。\n创作背景 # 《大堰河——我的保姆》1933年1月作于狱中。1931年4、5月间，诗人从法国回到上海。不久，参加“左翼美术家联盟”，与江丰、力扬等艺术青年，组织了“春地美术研究所”。1932年，诗人被当局逮捕，押在第二特区看守所，后以“宣传与三民主义不相容主义”罪被判入狱6年。他在狱中写下了这首诗。\n作品鉴赏 # 《大堰河，我的保姆》是艾青的成名之作。这是一个地主阶级叛逆的儿子献给他的真正母亲——中国大地善良而不幸的普通农妇的颂歌。\n这首诗感情真挚深切。诗中反复陈述：“大堰河，是我的保姆”，诗人是地主的儿子，长在“大堰河”的怀中，吮吸着她的乳汁，这不仅养育了诗人的身体，也养育了诗人的感情。诗人深深领受了她的爱，及至到了上学的年龄离开养母回到亲生父母身边的时候，他感到父母的陌生，更感到养母的对他的重要。养母正直、善良、朴素的品格影响了诗人的一生。这首诗从头到尾，始终围绕“我”与“她”的关系来写，他对大堰河深厚的感情，都表现在娓娓动情的陈述之中，他在监狱里，看见了雪就想到大堰河“被雪压着的草盖的坟墓”，想起她的故居园地，想起她对他的关怀和爱……于是他用他的深情的诗，表现了大堰河的具体劳作情景，也写了她心灵深处的感情波纹，就连她美丽的梦境，也同对乳儿的“幸福命运”的祝愿融合在一起。有了这样的真情，这样的心灵，才使这位劳动妇女形象更加崇高、完美，所以诗人要把热烈的颂扬，“呈给大地上一切的/我的大堰河般的保姆和他们的儿子/呈给爱我如爱她自己的儿子般的大堰河”。这样就使“大堰河”以某种象征意义，升华为永远与山河、村庄同在的人民的化身，或者说是中国农民的化身。\n艾青在《大堰河，我的保姆》开始表现他诗作的艺术特色，他首先是从“感觉”出发,像印象派画家那么重视感觉和感受，而且注意主观情感对感觉的渗入与融合。并在二者的融合中产生出多层次的联想，创造出既是清晰的,又具有广阔象征意义的视觉形象。诗总是具体的、有着鲜明形象的，如这首诗写大堰河的劳作,写大堰河的笑，写大堰河的爱和死。都呈现可视可感的立体的意象符号附加形容。最后叠句排比句的运用，如“呈给你黄土下紫色的灵魂/呈给你拥抱过我的直伸着的手/呈给你吻过我的唇。/呈给你泥黑的温柔的脸颜/呈给你养育我的乳房……”具体的描写，保证语言的形象性，这也是艾青诗的艺术魅力的奥秘所在，他后来的诗作，更自觉地将它发扬光大了。\n这是一首献给保姆大堰河的诗篇。诗人叙述了这位普通中国妇女平凡而坎坷、不幸的一生，表达了对这位伟大母亲由衷的感恩之情。大堰河，也是千千万万中国母亲的代表，正是这片如同慈母一样宽阔的土地和这个伟大的祖国，尽管她受尽欺辱，满身疮痍，历尽沧桑，然而却永远不失母性和母爱伟大的光辉诗歌饱含深情，反复咏唱，如泣如诉。 [4-5]\n作品评价 # 现代文学家茅盾：用沉郁的笔调细写了乳娘兼女佣（《大堰河》）的生活痛苦。（《中国现代文学管窥》） [6]\n现代文艺理论家胡风：至于《大堰河——我的保姆》，在这里有了一个用乳汁用母爱喂养别人的孩子，用劳力用忠诚服侍别人的农妇的形象，乳儿的作者用着朴素的真实的言语对这形象呈诉了切切的爱心。在这里他提出了对于‘这不公道的世界’的诅咒，告白了他和被侮辱的兄弟们比以前‘更要亲密’。虽然全篇流着私情地温暖，但他和我们之间已没有了难越的界限了。（《通三统：一种文学史实验》） [7]\n现代诗歌评论家张同吾：它像一颗光华熠熠的新星，出现在30年代的中国诗坛上；它以深沉隽永的情思，在广大读者的心田里镌刻着久远而常新的记忆。（《张同吾文集》） [8]\n现代文学史家王瑶：《大堰河》一首以真挚的感情刻画了一个勤劳善良的旧中国农村妇女的典型形象，描绘了穷苦悲惨的旧中国农村，暗示着诗人对地主阶级的叛逆和回到农民中去的愿望。（《中国新文学史稿》）\n现代文学史家刘绶松：这首诗不仅属于艾青个人，更属于时代与社会。艾青背叛了他的家庭所属的地主阶级，而将自己的思想感情完全献给了中国广大勤劳质朴却深受苦难的劳动人民。（《中国新文学史初稿》）\n后世影响 # 《大堰河——我的保姆》被选入2007年人教版高中语文必修1第一单元以及2023年部编版高中语文选择性必修下册第6课。这首诗在艾青诗歌中有重要地位，艾青的第一部诗集就取名《大堰河》。\n《大堰河——我的保姆》抒写对劳动人民刻骨铭心的感激和热爱，为后来的诗歌提供了启示。大多数的文学史评价赋予了艾青的这首诗极高的地位，认为它是艾青诗歌创作的开端。如《中国现代文学三十年》中评价道，艾青的“芦笛”吹出的第一首歌便是给乳母大堰河的赞歌，作为起点，它密切联系着我们民族多灾多难的土地与人民。\n作者简介 # 艾青（1910—1996），中国现代诗人。原名蒋海澄，曾用笔名莪伽、克阿、纳雍、林壁等。浙江金华人。被认为是中国新诗史上产生过重要影响，具有独特风格的现实主义诗人。成名作《大堰河——我的保姆》奠定了他诗歌的基本艺术特征和他在现代文学史上的重要地位。其作品被译成几十种文字，著有《大堰河》《北方》《向太阳》《黎明的通知》等诗集。 [12]\n"},{"id":60,"href":"/docs/2024-3-4-k8s-csi%E5%89%96%E6%9E%90/","title":"2025-1-16 CSI剖析演进","section":"Docs","content":"\n"},{"id":61,"href":"/docs/2025-1-16-k8s%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E6%8C%87%E5%8D%97/","title":"2025-1-16 k8s常见故障指南","section":"Docs","content":"\nkubeadm reset systemctl stop kubelet systemctl stop docker rm -rf /var/lib/cni/ rm -rf /var/lib/kubelet/* rm -rf /etc/cni/ ifconfig cni0 down ifconfig flannel.1 down ifconfig docker0 down ip link delete cni0 ip link delete flannel.1 systemctl start docker systemctl start kubelet 把k8s-node1重新加入节点 "},{"id":62,"href":"/docs/2024-3-4-k8s%E6%B5%81%E9%87%8F%E9%93%BE%E8%B7%AF%E5%89%96%E6%9E%90/","title":"2025-1-16 k8s流量链路剖析","section":"Docs","content":"\n"},{"id":63,"href":"/docs/33%E6%AC%BEgitops%E4%B8%8Edevops%E4%B8%BB%E6%B5%81%E7%B3%BB%E7%BB%9F-33-kuan-gitops-yu-devops-zhu-liu-xi-tong/","title":"33款gitops与devops主流系统 2024-08-02 17:45:15.709","section":"Docs","content":"为了帮助大家开启 GitOps 之旅，文介绍了 30 多种工具，如果你要想应用 GitOps，建议你使用这些工具。\nGitOps 借鉴了 DevOps 方法论的自动化方面，是一种旨在通过软件开发和部署来简化基础设施管理和云操作的方法。虽然许多人认为 GitOps 可以替代 DevOps，但事实并非如此，该方法仅专注于实现 DevOps 方法论中的自动化这一个方面。\n具体来说，GitOps 使用 Git 拉取（pull）请求来自动化基础设施配置和软件部署，所有这些都是为了使 CI/CD 变得更加高效。\nGitOps 将 Git 作为应用程序开发和云基础设施的唯一事实源；采用声明式语句来简化配置和部署。\nGitOps 统一了许多关键任务，比如云集群（特别是运行在云中的容器）的部署、管理和监控，并允许开发人员对他们的应用程序部署管道有更多的控制。由于 Git 可用于基础设施即代码（IaC）和应用程序开发，因此它是该方法的理想事实存储库。\n1GitOps 的好处\n对于那些使用该方法的人来说，GitOps 提供了一些关键的优势，首先是更精细的 CI/CD 管道本身。该方法充分利用了云原生应用程序和可伸缩云基础设施的优势，而没有引入常见的复杂性。\n其他好处还包括：\n更高的可靠性，这是由 Git 的原生特性决定的。如果新代码导致了错误，你可以回滚部署并使用 Git 的跟踪机制恢复到该应用程序的任何版本。这也会使云基础设施更加健壮。 提高了稳定性，尤其是在管理 Kubernetes 集群时。一切都是可跟踪的，并且集群配置中的变更也可以在需要时恢复。将以 Git 作为事实源，自动创建审计日志。 更高的生产效率，使得开发人员能够更加关注代码的质量，而不是管道本身。一旦将新的代码提交给 Git，一切都将完全自动化，此外还可以利用其他自动化工具。 最大程度的一致性，特别是在整个流程中使用相同的方法来进行端到端的管理时。GitOps 简化了应用程序、Kubernetes 附属组件以及 Kubernetes 基础设施的所有工作。 许多观点认为，GitOps 将持续交付与云原生优势和 IaC 结合起来，提供了这两个领域的最佳服务。GitOps 最佳实践还标准化了端到端的管道，你可以将该方法与任何现有管道进行集成，而无需进行大的更改。只要使用合适的工具即可完成这项工作。\n2GitOps 工具\n说到适合这项工作的工具，有无数工具可以帮助你将 GitOps 方法与现有工作流进行集成。一些支持 GitOps 的工具是非常流行的，甚至可以在现有的管道中使用它们。如果你想加入 GitOps，这里有一些我们推荐的工具可以帮助你进行入门学习。\n1、 Kubernetes\n当然，Kubernetes 是 GitOps 的核心。毕竟，该方法是基于使用 Kubernetes 来管理容器并构建可靠的基础设施的。Kubernetes 现在提供了许多自动化工具，可以简化云基础设施的部署和扩展。我们将在本文的后面部分介绍其中的某些工具。\n2、Git\n作为一个开源的版本控制平台，Git 非常强大。在 GitOps 中，Git 存储库将成为唯一的事实源。提交给 Git 的每个代码都将被处理和部署。你还可以使用 Git 存储库来进行开发和部署。\n\\3. Helm\nHelm 是用于配置 Kubernetes 资源的最强大的工具之一。是的，你可以使用 Homebrew 或 Yum，但是 Helm 提供的自动化功能是其他同类工具所不具备的。\n\\4. Flagger\n如果你想进一步管理发布，来自 Weaveworks 的 Flagger 是必备的工具。它是一个管理渐进式交付的工具，允许有选择地部署新代码以识别错误。它可以很好地与本列表中的下一个工具配合使用。\n5、Prometheus\nPrometheus 可以充当 GitOps 的监控工具。如果变更没有通过 Flagger 设置的测试，它将触发报警。除此之外，Prometheus 还弥补了 GitOps 与其他自动化工具之间的差距。\n6、Flux\nFlux 或 FluxCD 只是 Kubernetes 的 GitOps 操作符。它会使用 Git 库中的配置自动调整 Kubernetes 的集群配置。Flux 是为什么可以轻松地恢复对 Kubernetes 集群所做更改的原因。\n7、Quay\n对于镜像管理，可以使用 Quay。可以使用该工具对容器镜像进行细致的管理，而所有这些都不会牺牲安全性和可靠性。Quay 使 GitOps 可以使用本地镜像注册表，而不是像 GitHub 那样要使用基于云的镜像注册表。\n8、Auto-Assign\n为了使你的 Git 拉取请求和更新井井有条，可以使用一些工具。Auto-Assign 就是其中之一。顾名思义，每当发现新的拉取请求时，它都会分配审查者，因此可以密切监控变更。\n9、CodeFactor\n为了持续维护代码的质量，CodeFactor 是另一个可以集成到 GitOps 管道中的工具。它是一个自动的代码审查工具，当发现新的 Git 提交时，它会根据预定义的标准自动检查代码。\n10、DEP\n管理依赖关系是关键，特别是当你的应用程序是基于 Go 之类的语言构建的时候。为了应对这种实例，你可以使用 DEP。它是专门为管理 Go 应用程序和服务的依赖关系而创建的，并且它不会减慢 GitOps 管道的速度。\n11、Kodiakhq\n另一个用于管理代码的 Git 应用程序是 Kodiakhq。但该工具主要专注于自动更新和管理拉取请求，同时减少 CI 的负载。随着 Kodiakhq 的启动和运行，不再需要手动合并请求，这可以节省时间和宝贵的资源来执行其他任务。\n12、Atlantis\n如果你使用 Terraform 来简化资源配置，那么可以使用 Atlantis 为管道添加其他自动化功能。Atlantis 自动执行对 Terraform 的拉取请求，并在发现新请求时触发进一步的操作。\n13、Helm Operator\nHelm Operator 还通过将自动化引入到 Helm Charts 发布中，进一步将 Helm 推向了新的高度。它被设计为在 GitOps 管道中从头开始工作，因此集成 Helm Operator 非常简单。\n14、Gitkube\nGitkube 更加专注于使用 Git push 构建和部署 Docker 镜像。该工具使用起来非常简单，不需要对单个容器进行复杂的配置。这也是一个可以在部署阶段节省大量时间和精力的工具。\n15、Jenkins X\n当谈论 GitOps 工具时，我们真的不能不谈 Jenkins X。Jenkins 最初是作为 Kubernetes 的 CI/CD 平台的，但是该平台可以用来无缝地管理你的 GitOps 管道。它甚至具有一个内置的预览环境来最大程度地减少代码和部署错误。\n16、Restyled\n为了实现更好的标准化，Restyled 会强制执行某种编码样式。由于 GitOps 被设计为一种标准化端到端流程的方法，因此具有自动执行代码审查和重新合并请求的能力是一个巨大的优势。\n17、Argo CD\nArgo CD 采用了一种更直观的方式来处理 GitOps。它可视化了应用程序和环境的配置，并使用图表和可视化的提示来模拟 GitOps 管道。你也可以将 Argo CD 与 Helm 和其他 GitOps 工具结合使用。\n18、Kapp\nKapp 是 Kubernetes 应用程序的衍生名称，专注于管道的部署方面。它将由其他自动化工具创建的软件包集成到 GitOps 工作流中，并基于它们生成 Kubernetes 配置。\n19、Kpt\nKpt 或“kept”是用于简化 Kubernetes 资源部署和配置的另一种工具。它使用声明来处理资源配置，从而使开发人员可以更好地控制他们的基础设施。使用 Kpt 完全不需要手动配置。\n20、Stale\nStale 能处理一些令许多开发人员都很恼火的事情：悬而未决或被放弃的问题以及拉取请求。使用 Stale，你可以配置何时将拉取请求和问题视为放弃，然后自动管理这些请求和问题。\n21、 Kube Backup\nKube Backup 是维护 Kubernetes 集群配置的重要工具。它将集群备份到 Git，特别是集群的资源状态。如果环境发生灾难性故障，可以使用 Kube Backup 来更快地启动和运行应用程序。\n22、Untrak\nUntrak 是一个用于管理 Kubernetes 集群资源的便捷工具。该工具会自动在集群中查找未跟踪的资源。它还可以处理垃圾回收，并能帮助你保持 Kubernetes 集群的精益。\n23、Fluxcloud\nFluxcloud 整合了 Slack 与 GitOps。如果你使用 Flux（FluxCD），那么也一定会喜欢 Fluxcloud。它消除了对 Weave Cloud 的需求，并允许为每个 FluxCD 活动生成 Slack 通知。\n24、Stickler CI\n代码风格的指南和标准！Sticker CI 在不影响管道本身的情况下简化了编码风格的实现。只要在工作流程中实现 Stickler CI，就可以快速且一致地进行代码检查和标准化。\n25、Task List Completed\n下一个工具非常简单。Task List Completed 将停止合并具有未完成任务的拉取请求。无需手动检查每个拉取请求的任务，就可以使用该工具来保护部署环境。\n26、Slack\n我们已经提过了如何使用 Fluxcloud 进行通知，但是如果你决定不使用 FluxCD，那怎么办呢？你仍然可以通过激活本地的 Slack 插件来获取 Git 的变更通知。Slack 支持关闭和打开拉取请求和问题等的任务，以及直接从 Slack 应用程序中与之交互的任务。\n27、CI Reporter\n即使已经有了最好的 QA，仍可能会发现代码中的错误。这就是 CI Reporter 能派上用场的地方。该工具会收集构建失败的错误报告，然后再将其添加到相关的拉取请求中。\n28、PR Label Enforce\n想要更精细化地控制拉取请求的合并，可以使用 PR Label Enforce。在合并拉取请求之前，该工具会强制执行某些标签。可以将“ready”或“ checked”之类的标签设置为参数，然后使用其他工具来自动分配这些标签。\n29、Git-Secret\n想要在 Git 中存储私有数据，可以使用 Git-Secret。当你需要存储敏感的配置文件或密钥时，这非常方便。安全性在 GitOps 中非常重要，因此 Git-Secret 作为一种确保安全性的方法非常宝贵。说到安全性，你还可以使用……。\n30、Kamus\nKamus 能自动将零信任加密和解密合并到你的 GitOps 工作流中。与 Git-Secret 结合使用，你可以在不减慢 CI/CD 周期的情况下增强整个管道的安全性。\n31、Sealed Secrets\n如果你需要采取进一步措施，还可以使用 Sealed Secrets 通过单向加密过程来对密钥进行加密。Sealed Secrets 为 GitOps 管道提供了最大的安全性。\n32、Pull Panda\n虽然 GitOps 是一种非常敏捷的方法，但是保持生产效率仍然是必须要做的事情。Pull Panda 可以帮忙你实现这一目标，它可以使协作工作变得更轻松、更高效。它向 Slack 发送拉取提醒和分析，甚至可以自动执行拉取请求的分配。\n33、Sleeek\nSleek 也是一个管理生产效率和简化流程的机器人，但是它对这个问题的处理方式略有不同。Sleek 基本上是一个机器人，是一个虚拟助手，可以帮助项目经理和开发团队通过一系列问题来保持同步。\n老实说，这样的例子不胜枚举；有很多很棒的工具可以帮助我们集成 GitOps 并显著地简化部署管道。作为一种方法，GitOps 确实为开发人员提供了很大的灵活性，并使他们在管理 Kubernetes 集群和云资源配置时能够更加精细。在使用 Kubernetes 时，这确实可以满足云原生的需求，可以实现持续部署。\n"},{"id":64,"href":"/docs/600%E6%9D%A1%E6%9C%80%E5%BC%BAlinux%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93-600-tiao-zui-qiang-linux-ming-ling-zong-jie/","title":"600条最强linux命令总结 2024-04-03 15:12:31.282","section":"Docs","content":" 1. 基本命令 # uname -m 显示机器的处理器架构 uname -r 显示正在使用的内核版本 dmidecode -q 显示硬件系统部件 (SMBIOS / DMI) hdparm -i /dev/hda 罗列一个磁盘的架构特性 hdparm -tT /dev/sda 在磁盘上执行测试性读取操作系统信息 arch 显示机器的处理器架构 uname -m 显示机器的处理器架构 uname -r 显示正在使用的内核版本 dmidecode -q 显示硬件系统部件 - (SMBIOS / DMI) hdparm -i /dev/hda 罗列一个磁盘的架构特性 hdparm -tT /dev/sda 在磁盘上执行测试性读取操作 cat /proc/cpuinfo 显示CPU info的信息 cat /proc/interrupts 显示中断 cat /proc/meminfo 校验内存使用 cat /proc/swaps 显示哪些swap被使用 cat /proc/version 显示内核的版本 cat /proc/net/dev 显示网络适配器及统计 cat /proc/mounts 显示已加载的文件系统 lspci -tv 罗列 PCI 设备 lsusb -tv 显示 USB 设备 date 显示系统日期 cal 2007 显示2007年的日历表 date 041217002007.00 设置日期和时间 - 月日时分年.秒 clock -w 将时间修改保存到 BIOS 2. 关机 # shutdown -h now 关闭系统(1) init 0 关闭系统(2) telinit 0 关闭系统(3) shutdown -h hours:minutes \u0026amp; 按预定时间关闭系统 shutdown -c 取消按预定时间关闭系统 shutdown -r now 重启(1) reboot 重启(2) logout 注销 3. 文件和目录 # cd /home 进入 \u0026#39;/ home\u0026#39; 目录\u0026#39; cd .. 返回上一级目录 cd ../.. 返回上两级目录 cd 进入个人的主目录 cd ~user1 进入个人的主目录 cd - 返回上次所在的目录 pwd 显示工作路径 ls 查看目录中的文件 ls -F 查看目录中的文件 ls -l 显示文件和目录的详细资料 ls -a 显示隐藏文件 ls *[0-9]* 显示包含数字的文件名和目录名 tree 显示文件和目录由根目录开始的树形结构(1) lstree 显示文件和目录由根目录开始的树形结构(2) mkdir dir1 创建一个叫做 \u0026#39;dir1\u0026#39; 的目录\u0026#39; mkdir dir1 dir2 同时创建两个目录 mkdir -p /tmp/dir1/dir2 创建一个目录树 rm -f file1 删除一个叫做 \u0026#39;file1\u0026#39; 的文件\u0026#39; rmdir dir1 删除一个叫做 \u0026#39;dir1\u0026#39; 的目录\u0026#39; rm -rf dir1 删除一个叫做 \u0026#39;dir1\u0026#39; 的目录并同时删除其内容 rm -rf dir1 dir2 同时删除两个目录及它们的内容 mv dir1 new_dir 重命名/移动 一个目录 cp file1 file2 复制一个文件 cp dir/* . 复制一个目录下的所有文件到当前工作目录 cp -a /tmp/dir1 . 复制一个目录到当前工作目录 cp -a dir1 dir2 复制一个目录 ln -s file1 lnk1 创建一个指向文件或目录的软链接 ln file1 lnk1 创建一个指向文件或目录的物理链接 touch -t 0712250000 file1 修改一个文件或目录的时间戳 - (YYMMDDhhmm) file file1 outputs the mime type of the file as text iconv -l 列出已知的编码 iconv -f fromEncoding -t toEncoding inputFile \u0026gt; outputFile creates a new from the given input file by assuming it is encoded in fromEncoding and converting it to toEncoding. find . -maxdepth 1 -name *.jpg -print -exec convert \u0026#34;{}\u0026#34; -resize 80x60 \u0026#34;thumbs/{}\u0026#34; \\; batch resize files in the current directory and send them to a thumbnails directory (requires convert from Imagemagick) 4. 文件搜索 # find / -name file1 从 \u0026#39;/\u0026#39; 开始进入根文件系统搜索文件和目录 find / -user user1 搜索属于用户 \u0026#39;user1\u0026#39; 的文件和目录 find /home/user1 -name \\*.bin 在目录 \u0026#39;/ home/user1\u0026#39; 中搜索带有\u0026#39;.bin\u0026#39; 结尾的文件 find /usr/bin -type f -atime +100 搜索在过去100天内未被使用过的执行文件 find /usr/bin -type f -mtime -10 搜索在10天内被创建或者修改过的文件 find / -name \\*.rpm -exec chmod 755 \u0026#39;{}\u0026#39; \\; 搜索以 \u0026#39;.rpm\u0026#39; 结尾的文件并定义其权限 find / -xdev -name \\*.rpm 搜索以 \u0026#39;.rpm\u0026#39; 结尾的文件，忽略光驱、捷盘等可移动设备 locate \\*.ps 寻找以 \u0026#39;.ps\u0026#39; 结尾的文件 - 先运行 \u0026#39;updatedb\u0026#39; 命令 whereis halt 显示一个二进制文件、源码或man的位置 which halt 显示一个二进制文件或可执行文件的完整路径 5. 挂载一个文件系统 # mount /dev/hda2 /mnt/hda2 挂载一个叫做hda2的盘 - 确定目录 \u0026#39;/ mnt/hda2\u0026#39; 已经存在 umount /dev/hda2 卸载一个叫做hda2的盘 - 先从挂载点 \u0026#39;/ mnt/hda2\u0026#39; 退出 fuser -km /mnt/hda2 当设备繁忙时强制卸载 umount -n /mnt/hda2 运行卸载操作而不写入 /etc/mtab 文件- 当文件为只读或当磁盘写满时非常有用 mount /dev/fd0 /mnt/floppy 挂载一个软盘 mount /dev/cdrom /mnt/cdrom 挂载一个cdrom或dvdrom mount /dev/hdc /mnt/cdrecorder 挂载一个cdrw或dvdrom mount /dev/hdb /mnt/cdrecorder 挂载一个cdrw或dvdrom mount -o loop file.iso /mnt/cdrom 挂载一个文件或ISO镜像文件 mount -t vfat /dev/hda5 /mnt/hda5 挂载一个Windows FAT32文件系统 mount /dev/sda1 /mnt/usbdisk 挂载一个usb 捷盘或闪存设备 mount -t smbfs -o username=user,password=pass //WinClient/share /mnt/share 挂载一个windows网络共享 6. 磁盘空间 # df -h 显示已经挂载的分区列表 ls -lSr |more 以尺寸大小排列文件和目录 du -sh dir1 估算目录 \u0026#39;dir1\u0026#39; 已经使用的磁盘空间\u0026#39; du -sk * | sort -rn 以容量大小为依据依次显示文件和目录的大小 rpm -q -a --qf \u0026#39;%10{SIZE}t%{NAME}n\u0026#39; | sort -k1,1n 以大小为依据依次显示已安装的rpm包所使用的空间 (fedora, redhat类系统) dpkg-query -W -f=\u0026#39;${Installed-Size;10}t${Package}n\u0026#39; | sort -k1,1n 以大小为依据显示已安装的deb包所使用的空间 (ubuntu, debian类系统) 7. 用户和群组 # groupadd group_name 创建一个新用户组 groupdel group_name 删除一个用户组 groupmod -n new_group_name old_group_name 重命名一个用户组 useradd -c \u0026#34;Name Surname \u0026#34; -g admin -d /home/user1 -s /bin/bash user1 创建一个属于 \u0026#34;admin\u0026#34; 用户组的用户 useradd user1 创建一个新用户 userdel -r user1 删除一个用户 ( \u0026#39;-r\u0026#39; 排除主目录) usermod -c \u0026#34;User FTP\u0026#34; -g system -d /ftp/user1 -s /bin/nologin user1 修改用户属性 passwd 修改口令 passwd user1 修改一个用户的口令 (只允许root执行) chage -E 2005-12-31 user1 设置用户口令的失效期限 pwck 检查 \u0026#39;/etc/passwd\u0026#39; 的文件格式和语法修正以及存在的用户 grpck 检查 \u0026#39;/etc/passwd\u0026#39; 的文件格式和语法修正以及存在的群组 newgrp group_name 登陆进一个新的群组以改变新创建文件的预设群组 8. 文件的权限 使用 “+” 设置权限，使用 “-” 用于取消 # ls -lh 显示权限 ls /tmp | pr -T5 -W$COLUMNS 将终端划分成5栏显示 chmod ugo+rwx directory1 设置目录的所有人(u)、群组(g)以及其他人(o)以读（r ）、写(w)和执行(x)的权限 chmod go-rwx directory1 删除群组(g)与其他人(o)对目录的读写执行权限 chown user1 file1 改变一个文件的所有人属性 chown -R user1 directory1 改变一个目录的所有人属性并同时改变改目录下所有文件的属性 chgrp group1 file1 改变文件的群组 chown user1:group1 file1 改变一个文件的所有人和群组属性 find / -perm -u+s 罗列一个系统中所有使用了SUID控制的文件 chmod u+s /bin/file1 设置一个二进制文件的 SUID 位 - 运行该文件的用户也被赋予和所有者同样的权限 chmod u-s /bin/file1 禁用一个二进制文件的 SUID位 chmod g+s /home/public 设置一个目录的SGID 位 - 类似SUID ，不过这是针对目录的 chmod g-s /home/public 禁用一个目录的 SGID 位 chmod o+t /home/public 设置一个文件的 STIKY 位 - 只允许合法所有人删除文件 chmod o-t /home/public 禁用一个目录的 STIKY 位 chmod +x 文件路径 为所有者、所属组和其他用户添加执行的权限 chmod -x 文件路径 为所有者、所属组和其他用户删除执行的权限 chmod u+x 文件路径 为所有者添加执行的权限 chmod g+x 文件路径 为所属组添加执行的权限 chmod o+x 文件路径 为其他用户添加执行的权限 chmod ug+x 文件路径 为所有者、所属组添加执行的权限 chmod =wx 文件路径 为所有者、所属组和其他用户添加写、执行的权限，取消读权限 chmod ug=wx 文件路径 为所有者、所属组添加写、执行的权限，取消读权限 9. 文件的特殊属性 ，使用 “+” 设置权限，使用 “-” 用于取消 # chattr +a file1 只允许以追加方式读写文件 chattr +c file1 允许这个文件能被内核自动压缩/解压 chattr +d file1 在进行文件系统备份时，dump程序将忽略这个文件 chattr +i file1 设置成不可变的文件，不能被删除、修改、重命名或者链接 chattr +s file1 允许一个文件被安全地删除 chattr +S file1 一旦应用程序对这个文件执行了写操作，使系统立刻把修改的结果写到磁盘 chattr +u file1 若文件被删除，系统会允许你在以后恢复这个被删除的文件 lsattr 显示特殊的属性 10. 打包和压缩文件 # bunzip2 file1.bz2 解压一个叫做 \u0026#39;file1.bz2\u0026#39;的文件 bzip2 file1 压缩一个叫做 \u0026#39;file1\u0026#39; 的文件 gunzip file1.gz 解压一个叫做 \u0026#39;file1.gz\u0026#39;的文件 gzip file1 压缩一个叫做 \u0026#39;file1\u0026#39;的文件 gzip -9 file1 最大程度压缩 rar a file1.rar test_file 创建一个叫做 \u0026#39;file1.rar\u0026#39; 的包 rar a file1.rar file1 file2 dir1 同时压缩 \u0026#39;file1\u0026#39;, \u0026#39;file2\u0026#39; 以及目录 \u0026#39;dir1\u0026#39; rar x file1.rar 解压rar包 unrar x file1.rar 解压rar包 tar -cvf archive.tar file1 创建一个非压缩的 tarball tar -cvf archive.tar file1 file2 dir1 创建一个包含了 \u0026#39;file1\u0026#39;, \u0026#39;file2\u0026#39; 以及 \u0026#39;dir1\u0026#39;的档案文件 tar -tf archive.tar 显示一个包中的内容 tar -xvf archive.tar 释放一个包 tar -xvf archive.tar -C /tmp 将压缩包释放到 /tmp目录下 tar -cvfj archive.tar.bz2 dir1 创建一个bzip2格式的压缩包 tar -xvfj archive.tar.bz2 解压一个bzip2格式的压缩包 tar -cvfz archive.tar.gz dir1 创建一个gzip格式的压缩包 tar -xvfz archive.tar.gz 解压一个gzip格式的压缩包 zip file1.zip file1 创建一个zip格式的压缩包 zip -r file1.zip file1 file2 dir1 将几个文件和目录同时压缩成一个zip格式的压缩包 unzip file1.zip 解压一个zip格式压缩包 11. RPM 包 # rpm -ivh package.rpm 安装一个rpm包 rpm -ivh --nodeeps package.rpm 安装一个rpm包而忽略依赖关系警告 rpm -U package.rpm 更新一个rpm包但不改变其配置文件 rpm -F package.rpm 更新一个确定已经安装的rpm包 rpm -e package_name.rpm 删除一个rpm包 rpm -qa 显示系统中所有已经安装的rpm包 rpm -qa | grep httpd 显示所有名称中包含 \u0026#34;httpd\u0026#34; 字样的rpm包 rpm -qi package_name 获取一个已安装包的特殊信息 rpm -qg \u0026#34;System Environment/Daemons\u0026#34; 显示一个组件的rpm包 rpm -ql package_name 显示一个已经安装的rpm包提供的文件列表 rpm -qc package_name 显示一个已经安装的rpm包提供的配置文件列表 rpm -q package_name --whatrequires 显示与一个rpm包存在依赖关系的列表 rpm -q package_name --whatprovides 显示一个rpm包所占的体积 rpm -q package_name --scripts 显示在安装/删除期间所执行的脚本l rpm -q package_name --changelog 显示一个rpm包的修改历史 rpm -qf /etc/httpd/conf/httpd.conf 确认所给的文件由哪个rpm包所提供 rpm -qp package.rpm -l 显示由一个尚未安装的rpm包提供的文件列表 rpm --import /media/cdrom/RPM-GPG-KEY 导入公钥数字证书 rpm --checksig package.rpm 确认一个rpm包的完整性 rpm -qa gpg-pubkey 确认已安装的所有rpm包的完整性 rpm -V package_name 检查文件尺寸、 许可、类型、所有者、群组、MD5检查以及最后修改时间 rpm -Va 检查系统中所有已安装的rpm包- 小心使用 rpm -Vp package.rpm 确认一个rpm包还未安装 rpm2cpio package.rpm | cpio --extract --make-directories *bin* 从一个rpm包运行可执行文件 rpm -ivh /usr/src/redhat/RPMS/`arch`/package.rpm 从一个rpm源码安装一个构建好的包 rpmbuild --rebuild package_name.src.rpm 从一个rpm源码构建一个 rpm 包 12. YUM 软件包升级器 # yum install package_name 下载并安装一个rpm包 yum localinstall package_name.rpm 将安装一个rpm包，使用你自己的软件仓库为你解决所有依赖关系 yum update package_name.rpm 更新当前系统中所有安装的rpm包 yum update package_name 更新一个rpm包 yum remove package_name 删除一个rpm包 yum list 列出当前系统中安装的所有包 yum search package_name 在rpm仓库中搜寻软件包 yum clean packages 清理rpm缓存删除下载的包 yum clean headers 删除所有头文件 yum clean all 删除所有缓存的包和头文件 13. deb 包 # dpkg -i package.deb 安装/更新一个 deb 包 dpkg -r package_name 从系统删除一个 deb 包 dpkg -l 显示系统中所有已经安装的 deb 包 dpkg -l | grep httpd 显示所有名称中包含 \u0026#34;httpd\u0026#34; 字样的deb包 dpkg -s package_name 获得已经安装在系统中一个特殊包的信息 dpkg -L package_name 显示系统中已经安装的一个deb包所提供的文件列表 dpkg --contents package.deb 显示尚未安装的一个包所提供的文件列表 dpkg -S /bin/ping 确认所给的文件由哪个deb包提供 APT 软件工具 (Debian, Ubuntu 以及类似系统) apt-get install package_name 安装/更新一个 deb 包 apt-cdrom install package_name 从光盘安装/更新一个 deb 包 apt-get update 升级列表中的软件包 apt-get upgrade 升级所有已安装的软件 apt-get remove package_name 从系统删除一个deb包 apt-get check 确认依赖的软件仓库正确 apt-get clean 从下载的软件包中清理缓存 apt-cache search searched-package 返回包含所要搜索字符串的软件包名称 14. 查看文件内容 # cat file1 从第一个字节开始正向查看文件的内容 tac file1 从最后一行开始反向查看一个文件的内容 more file1 查看一个长文件的内容 less file1 类似于 \u0026#39;more\u0026#39; 命令，但是它允许在文件中和正向操作一样的反向操作 head -2 file1 查看一个文件的前两行 tail -2 file1 查看一个文件的最后两行 tail -f /var/log/messages 实时查看被添加到一个文件中的内容 15. 文本处理 # cat file1 file2 ... | command \u0026lt;\u0026gt; file1_in.txt_or_file1_out.txt general syntax for text manipulation using PIPE, STDIN and STDOUT cat file1 | command( sed, grep, awk, grep, etc...) \u0026gt; result.txt 合并一个文件的详细说明文本，并将简介写入一个新文件中 cat file1 | command( sed, grep, awk, grep, etc...) \u0026gt;\u0026gt; result.txt 合并一个文件的详细说明文本，并将简介写入一个已有的文件中 grep Aug /var/log/messages 在文件 \u0026#39;/var/log/messages\u0026#39;中查找关键词\u0026#34;Aug\u0026#34; grep ^Aug /var/log/messages 在文件 \u0026#39;/var/log/messages\u0026#39;中查找以\u0026#34;Aug\u0026#34;开始的词汇 grep [0-9] /var/log/messages 选择 \u0026#39;/var/log/messages\u0026#39; 文件中所有包含数字的行 grep Aug -R /var/log/* 在目录 \u0026#39;/var/log\u0026#39; 及随后的目录中搜索字符串\u0026#34;Aug\u0026#34; sed \u0026#39;s/stringa1/stringa2/g\u0026#39; example.txt 将example.txt文件中的 \u0026#34;string1\u0026#34; 替换成 \u0026#34;string2\u0026#34; sed \u0026#39;/^$/d\u0026#39; example.txt 从example.txt文件中删除所有空白行 sed \u0026#39;/ *#/d; /^$/d\u0026#39; example.txt 从example.txt文件中删除所有注释和空白行 echo \u0026#39;esempio\u0026#39; | tr \u0026#39;[:lower:]\u0026#39; \u0026#39;[:upper:]\u0026#39; 合并上下单元格内容 sed -e \u0026#39;1d\u0026#39; result.txt 从文件example.txt 中排除第一行 sed -n \u0026#39;/stringa1/p\u0026#39; 查看只包含词汇 \u0026#34;string1\u0026#34;的行 sed -e \u0026#39;s/ *$//\u0026#39; example.txt 删除每一行最后的空白字符 sed -e \u0026#39;s/stringa1//g\u0026#39; example.txt 从文档中只删除词汇 \u0026#34;string1\u0026#34; 并保留剩余全部 sed -n \u0026#39;1,5p;5q\u0026#39; example.txt 查看从第一行到第5行内容 sed -n \u0026#39;5p;5q\u0026#39; example.txt 查看第5行 sed -e \u0026#39;s/00*/0/g\u0026#39; example.txt 用单个零替换多个零 cat -n file1 标示文件的行数 cat example.txt | awk \u0026#39;NR%2==1\u0026#39; 删除example.txt文件中的所有偶数行 echo a b c | awk \u0026#39;{print $1}\u0026#39; 查看一行第一栏 echo a b c | awk \u0026#39;{print $1,$3}\u0026#39; 查看一行的第一和第三栏 paste file1 file2 合并两个文件或两栏的内容 paste -d \u0026#39;+\u0026#39; file1 file2 合并两个文件或两栏的内容，中间用\u0026#34;+\u0026#34;区分 sort file1 file2 排序两个文件的内容 sort file1 file2 | uniq 取出两个文件的并集(重复的行只保留一份) sort file1 file2 | uniq -u 删除交集，留下其他的行 sort file1 file2 | uniq -d 取出两个文件的交集(只留下同时存在于两个文件中的文件) comm -1 file1 file2 比较两个文件的内容只删除 \u0026#39;file1\u0026#39; 所包含的内容 comm -2 file1 file2 比较两个文件的内容只删除 \u0026#39;file2\u0026#39; 所包含的内容 comm -3 file1 file2 比较两个文件的内容只删除两个文件共有的部分 16. 字符设置和文件格式转换 # dos2unix filedos.txt fileunix.txt 将一个文本文件的格式从MSDOS转换成UNIX unix2dos fileunix.txt filedos.txt 将一个文本文件的格式从UNIX转换成MSDOS recode ..HTML \u0026lt; page.txt \u0026gt; page.html 将一个文本文件转换成html recode -l | more 显示所有允许的转换格式 17. 文件系统分析 # badblocks -v /dev/hda1 检查磁盘hda1上的坏磁块 fsck /dev/hda1 修复/检查hda1磁盘上linux文件系统的完整性 fsck.ext2 /dev/hda1 修复/检查hda1磁盘上ext2文件系统的完整性 e2fsck /dev/hda1 修复/检查hda1磁盘上ext2文件系统的完整性 e2fsck -j /dev/hda1 修复/检查hda1磁盘上ext3文件系统的完整性 fsck.ext3 /dev/hda1 修复/检查hda1磁盘上ext3文件系统的完整性 fsck.vfat /dev/hda1 修复/检查hda1磁盘上fat文件系统的完整性 fsck.msdos /dev/hda1 修复/检查hda1磁盘上dos文件系统的完整性 dosfsck /dev/hda1 修复/检查hda1磁盘上dos文件系统的完整性 18. 初始化一个文件系统 # mkfs /dev/hda1 在hda1分区创建一个文件系统 mke2fs /dev/hda1 在hda1分区创建一个linux ext2的文件系统 mke2fs -j /dev/hda1 在hda1分区创建一个linux ext3(日志型)的文件系统 mkfs -t vfat 32 -F /dev/hda1 创建一个 FAT32 文件系统 fdformat -n /dev/fd0 格式化一个软盘 mkswap /dev/hda3 创建一个swap文件系统 19. SWAP 文件系统 # mkswap /dev/hda3 创建一个swap文件系统 swapon /dev/hda3 启用一个新的swap文件系统 swapon /dev/hda2 /dev/hdb3 启用两个swap分区 20. 备份 # dump -0aj -f /tmp/home0.bak /home 制作一个 \u0026#39;/home\u0026#39; 目录的完整备份 dump -1aj -f /tmp/home0.bak /home 制作一个 \u0026#39;/home\u0026#39; 目录的交互式备份 restore -if /tmp/home0.bak 还原一个交互式备份 rsync -rogpav --delete /home /tmp 同步两边的目录 rsync -rogpav -e ssh --delete /home ip_address:/tmp 通过SSH通道rsync rsync -az -e ssh --delete ip_addr:/home/public /home/local 通过ssh和压缩将一个远程目录同步到本地目录 rsync -az -e ssh --delete /home/local ip_addr:/home/public 通过ssh和压缩将本地目录同步到远程目录 dd bs=1M if=/dev/hda | gzip | ssh user@ip_addr \u0026#39;dd of=hda.gz\u0026#39; 通过ssh在远程主机上执行一次备份本地磁盘的操作 dd if=/dev/sda of=/tmp/file1 备份磁盘内容到一个文件 tar -Puf backup.tar /home/user 执行一次对 \u0026#39;/home/user\u0026#39; 目录的交互式备份操作 ( cd /tmp/local/ \u0026amp;\u0026amp; tar c . ) | ssh -C user@ip_addr \u0026#39;cd /home/share/ \u0026amp;\u0026amp; tar x -p\u0026#39; 通过ssh在远程目录中复制一个目录内容 ( tar c /home ) | ssh -C user@ip_addr \u0026#39;cd /home/backup-home \u0026amp;\u0026amp; tar x -p\u0026#39; 通过ssh在远程目录中复制一个本地目录 tar cf - . | (cd /tmp/backup ; tar xf - ) 本地将一个目录复制到另一个地方，保留原有权限及链接 find /home/user1 -name \u0026#39;*.txt\u0026#39; | xargs cp -av --target-directory=/home/backup/ --parents 从一个目录查找并复制所有以 \u0026#39;.txt\u0026#39; 结尾的文件到另一个目录 find /var/log -name \u0026#39;*.log\u0026#39; | tar cv --files-from=- | bzip2 \u0026gt; log.tar.bz2 查找所有以 \u0026#39;.log\u0026#39; 结尾的文件并做成一个bzip包 dd if=/dev/hda of=/dev/fd0 bs=512 count=1 做一个将 MBR (Master Boot Record)内容复制到软盘的动作 dd if=/dev/fd0 of=/dev/hda bs=512 count=1 从已经保存到软盘的备份中恢复MBR内容 21. 光盘 # cdrecord -v gracetime=2 dev=/dev/cdrom -eject blank=fast -force 清空一个可复写的光盘内容 mkisofs /dev/cdrom \u0026gt; cd.iso 在磁盘上创建一个光盘的iso镜像文件 mkisofs /dev/cdrom | gzip \u0026gt; cd_iso.gz 在磁盘上创建一个压缩了的光盘iso镜像文件 mkisofs -J -allow-leading-dots -R -V \u0026#34;Label CD\u0026#34; -iso-level 4 -o ./cd.iso data_cd 创建一个目录的iso镜像文件 cdrecord -v dev=/dev/cdrom cd.iso 刻录一个ISO镜像文件 gzip -dc cd_iso.gz | cdrecord dev=/dev/cdrom - 刻录一个压缩了的ISO镜像文件 mount -o loop cd.iso /mnt/iso 挂载一个ISO镜像文件 cd-paranoia -B 从一个CD光盘转录音轨到 wav 文件中 cd-paranoia -- \u0026#34;-3\u0026#34; 从一个CD光盘转录音轨到 wav 文件中（参数-3） cdrecord --scanbus 扫描总线以识别scsi通道 dd if=/dev/hdc | md5sum 校验一个设备的md5sum编码，例如一张 CD 22. 网络（以太网和 WIFI 无线） # ifconfig eth0 显示一个以太网卡的配置 ifup eth0 启用一个 \u0026#39;eth0\u0026#39; 网络设备 ifdown eth0 禁用一个 \u0026#39;eth0\u0026#39; 网络设备 ifconfig eth0 192.168.1.1 netmask 255.255.255.0 控制IP地址 ifconfig eth0 promisc 设置 \u0026#39;eth0\u0026#39; 成混杂模式以嗅探数据包 (sniffing) dhclient eth0 以dhcp模式启用 \u0026#39;eth0\u0026#39; route -n show routing table route add -net 0/0 gw IP_Gateway configura default gateway route add -net 192.168.0.0 netmask 255.255.0.0 gw 192.168.1.1 configure static route to reach network \u0026#39;192.168.0.0/16\u0026#39; route del 0/0 gw IP_gateway remove static route echo \u0026#34;1\u0026#34; \u0026gt; /proc/sys/net/ipv4/ip_forward activate ip routing hostname show hostname of system host www.example.com lookup hostname to resolve name to ip address and viceversa(1) nslookup www.example.com lookup hostname to resolve name to ip address and viceversa(2) ip link show show link status of all interfaces mii-tool eth0 show link status of \u0026#39;eth0\u0026#39; ethtool eth0 show statistics of network card \u0026#39;eth0\u0026#39; netstat -tup show all active network connections and their PID netstat -tupl show all network services listening on the system and their PID tcpdump tcp port 80 show all HTTP traffic iwlist scan show wireless networks iwconfig eth1 show configuration of a wireless network card hostname show hostname host www.example.com lookup hostname to resolve name to ip address and viceversa nslookup www.example.com lookup hostname to resolve name to ip address and viceversa whois www.example.com lookup on Whois database 23. 列出目录内容 # ls -a：显示所有文件（包括隐藏文件）； ls -l：显示详细信息； ls -R：递归显示子目录结构； ls -ld：显示目录和链接信息； ctrl+r：历史记录中所搜命令（输入命令中的任意一个字符）； Linux中以.开头的文件是隐藏文件； pwd:显示当前目录 24. 查看文件的类型 # file:查看文件的类型 25. 复制文件目录 # 1、cp：复制文件和目录 cp 源文件（文件夹）目标文件（文件夹）\n常用参数：-r:递归复制整个目录树；-v：显示详细信息； # 复制文件夹时要在 cp 命令后面加一个-r 参数： # 如：cp -r 源文件夹 目标文件夹 # 2、touch+文件名：当文件不存在的时候，创建相应的文件；当文件存在的时候，修改文件的创建时间。\n功能：生成一个空文件或修改文件的存取/修改的时间记录值。 # touch * ：将当前下的文件时间修改为系统的当前时间 # touch –d 20040210 test：将 test 文件的日期改为 20040210 # touch abc ：若 abc 文件存在，则修改为系统的当前时间；若不存在，则生成一个为当前时间的空文件 # 3、mv 文件 目标目录：移动或重命名文件或目录（如果指定文件名，则可以重命名文件）。可以将文件及目录移到另一目录下，或更改文件及目录的名称。\n格式为：mv [参数]\u0026lt;源文件或目录\u0026gt; \u0026lt;目标文件或目录\u0026gt; # mva.txt ../：将 a.txt 文件移动上层目录 # mv a.txt b.txt：将 a.txt 改名为 b.txt # mvdir2 ../：将 dir2 目录上移一层 # 4、rm：删除文件；\n常用参数：-i：交互式 -r：递归的删除包括目录中的所有内容 # 5、mkdir +文件夹名称：创建文件夹；\n6、rm -r +文件夹名称：删除文件夹（空文件夹和非空文件夹都可删除）\nrmdir 文件夹名称：删除文件夹（只能删除空文件夹） # 7、mkdir -p dir1/dir2：在当前目录下创建 dir1 目录，并在 dir1 目录下创建 dir2 目录， 也就是连续创建两个目录（dir1/和 dir1/dir2）\n8、rmdir –p dir1/dir2：删除 dir1 下的 dir2 目录，若 dir1 目录为空也删除它\n9、rm *：删除当前目录下的所有文件\n10、-f 参数：强迫删除文件 rm –f *.txt：强迫删除所有以后缀名为 txt 文件\n11、-i 参数：删除文件时询问\nrm –i * ：删除当前目录下的所有文件会有如下提示： # rm:backup:is a directory 遇到目录会略过 # rm: remove ‘myfiles.txt’ ? Y # 删除文件时会询问,可按 Y 或 N 键表示允许或拒绝删除文件 # 12、-r 参数：递归删除（连子目录一同删除，这是一个相当常用的参数）\nrm -r test ：删除 test 目录（含 test 目录下所有文件和子目录） # rm -r *：删除所有文件（含当前目录所有文件、所有子目录和子目录下的文件） 一般在删除目录时 r 和 f 一起用，避免麻烦 # rm -rf test ：强行删除、不加询问 # 13、grep：功能：在文件中搜索匹配的字符并进行输出\n格式：grep[参数] \u0026lt;要找的字串\u0026gt; \u0026lt;要寻找字 串的源文件\u0026gt; # greplinux test.txt：搜索 test.txt 文件中字符串 linux 并输出 # 14、ln 命令\n功能：在文件和目录之间建立链接 # 格式：ln [参数] \u0026lt;源文件或目录\u0026gt; \u0026lt;目标文件或目录\u0026gt; # 链接分“软链接”和“硬链接” # 1.软链接: # ln–s /usr/share/do doc ：创建一个链接文件 doc,并指向目录/usr/share/do # 2.硬链接: # ln /usr/share/test hard：创建一个硬链接文件 hard，这时对于 test 文件对应 的存储区域来说，又多了一个文件指向它 # 26. 系统常用命令 # 26.1、显示命令 # date:查看或设置当前系统的时间：格式化显示时间：+%Y\u0026ndash;%m\u0026ndash;%d； # date -s:设置当前系统的时间 # hwclock(clock)：显示硬件时钟时间(需要管理员权限)； # cal：查看日历 # 格式 cal [参数] 月年 # cal：显示当月的日历 cal4 2004 ：显示 2004 年 4 月的日历 # cal- y 2003：显示 2003 年的日历 # uptime：查看系统运行时间 # 26.2、输出查看命令 # echo：显示输入的内容 追加文件 echo \u0026ldquo;liuyazhuang\u0026rdquo; \u0026raquo; liuyazhuang.txt # cat：显示文件内容,也可以将数个文件合并成一个文件。 # 格式：格式：cat[参数]\u0026lt;文件名\u0026gt; # cat test.txt：显示 test.txt 文件内容 # cat test.txt | more ：逐页显示 test.txt 文件中的内容 # cat test.txt \u0026raquo; test1.txt ：将 test.txt 的内容附加到 test1.txt 文件之后 # cat test.txt test2.txt \u0026gt;readme.txt : 将 test.txt 和 test2.txt 文件合并成 readme.txt 文件 # head:显示文件的头几行（默认 10 行） -n:指定显示的行数格式：head -n 文件名 # tail：显示文件的末尾几行（默认 10 行）-n：指定显示的行数 -f：追踪显示文件更新 （一般用于查看日志，命令不会退出，而是持续显示新加入的内容） # 格式：格式：tail[参数]\u0026lt;文件名\u0026gt; # tail-10 /etc/passwd ：显示/etc/passwd/文件的倒数 10 行内容 # tail+10 /etc/passwd ：显示/etc/passwd/文件从第 10 行开始到末尾的内容 # more：用于翻页显示文件内容（只能向下翻页） # more 命令是一般用于要显示的内容会超过一个画面长度的情况。为了避免画 面显示时瞬间就闪过去，用户可以使用 more 命令，让画面在显示满一页时暂停，此时可按空格键继续显示下一个画面，或按 Q 键停止显示。 # ls -al |more：以长格形式显示 etc 目录下的文件列表，显示满一个画面便暂停，可 按空格键继续显示下一画面，或按 Q 键跳离 # less：翻页显示文件内容（带上下翻页）按下上键分页，按 q 退出、‘ # less 命令的用法与 more 命令类似，也可以用来浏览超过一页的文件。所不同 的是 less 命令除了可以按空格键向下显示文件外，还可以利用上下键来卷动文件。当要结束浏览时，只要在 less 命令的提示符“：”下按 Q 键即可。 # ls -al | less：以长格形式列出/etc 目录中所有的内容。用户可按上下键浏览或按 Q 键跳离 # 26.3、查看硬件信息 # Ispci：查看 PCI 设备 -v：查看详细信息 # Isusb：查看 USB 设备 -v：查看详细信息 # Ismod：查看加载的模块(驱动) # 26.4、关机、重启 # shutdown 关闭、重启计算机 # shutdown[关机、重启]时间 -h 关闭计算机 -r：重启计算机 # 如：立即关机：shutdown -h now # 10 分钟后关机：shutdown -h +10 # 23:30 分关机：shutdown -h 23:30 # 立即重启：shutdown -r now # poweroff：立即关闭计算机 # reboot：立即重启计算机 # 26.5、归档、压缩 # zip:压缩文件 zip liuyazhuang.zip myfile 格式为：“zip 压缩后的 zip 文件文件名” # unzip：解压文件 unzip liuyazhuang.zip # gzip：压缩文件 gzip 文件名 # tar：归档文件 # tar -cvf out.tar liuyazhuang 打包一个归档（将文件\u0026quot;liuyazhuang\u0026quot;打包成一个归档） # tar -xvf liuyazhuang.tar 释放一个归档（释放 liuyazhuang.tar 归档） # tar -cvzf backup.tar.gz/etc # -z 参数将归档后的归档文件进行 gzip 压缩以减少大小。 # -c：创建一个新 tar 文件 # -v：显示运行过程的信息 # -f：指定文件名 # -z：调用 gzip 压缩命令进行压缩 # -t：查看压缩文件的内容 # -x：解开 tar 文件 # tar -cvf test.tar *：将所有文件打包成 test.tar,扩展名.tar 需自行加上 # tar -zcvf test.tar.gz *：将所有文件打包成 test.tar,再用 gzip 命令压缩 # tar -tf test.tar ：查看 test.tar 文件中包括了哪些文件 # tar -xvf test.tar 将 test.tar 解开 # tar -zxvf foo.tar.gz 解压缩 # gzip 各 gunzip 命令 # gziptest.txt ：压缩文件时，不需要任何参数 # gizp–l test.txt.gz：显示压缩率 # 26.6、查找 # locate：快速查找文件、文件夹：locate keyword # 此命令需要预先建立数据库，数据库默认每天更新一次，可用 updatedb 命令手工建立、更新数据库。欢迎关注我们，公号终码一生。 # find 查找位置查找参数 # 如： # find . -nameliuyazhuang查找当前目录下名称中含有\u0026quot;liuyazhuang\u0026quot;的文件 # find / -name *.conf 查找根目录下（整个硬盘）下后缀为.conf 的文件 # find / -perm 777 查找所有权限是 777 的文件 # find / -type d 返回根目录下所有的目录 # find . -name \u0026ldquo;a*\u0026quot;-exec ls -l {} ; # find 功能：用来寻找文件或目录。 # 格式：find [\u0026lt;路径\u0026gt;][匹配条件] # find / -name httpd.conf 搜索系统根目录下名为 httpd.conf 的文件 # 26.7、ctrl+c :终止当前的命令 # 26.8、who 或 w 命令 # 功能：查看当前系统中有哪些用户登录 # 格式：who/w[参数] # 26.9、dmesg 命令 # 功能：显示系统诊断信息、操作系统版本号、物理内存的大小以及其它信息 # 26.10、df 命令 # 功能：用于查看文件系统的各个分区的占用情况 # 26.11、du 命令 # 功能：查看某个目录中各级子目录所使用的硬盘空间数 # 格式：du [参数] \u0026lt;目录名\u0026gt; # 26.12、free 命令 # 功能：用于查看系统内存，虚拟内存（交换空间）的大小占用情况 # 27. VIM # VIM 是一款功能强大的命令行文本编辑器，在 Linux 中通过 vim 命令可以启动 vim 编辑器。 # 一般使用 vim + 目标文件路径 的形式使用 vim # 如果目标文件存在，则 vim 打开目标文件，如果目标文件不存在，则 vim 新建并打开该文件 # :q：退出 vim 编辑器 # VIM 模式\nvim 拥有三种模式： # （1）命令模式（常规模式）\nvim 启动后，默认进入命令模式，任何模式都可以通过 esc 键回到命令模式（可以多按几次），命令模式下可以键入不同的命令完成选择、复制、粘贴、撤销等操作。 # 命名模式常用命令如下： # i : 在光标前插入文本； # o:在当前行的下面插入新行； # dd:删除整行； # yy：将当前行的内容放入缓冲区（复制当前行） # n+yy :将 n 行的内容放入缓冲区（复制 n 行） # p:将缓冲区中的文本放入光标后（粘贴） # u：撤销上一个操作 # r:替换当前字符 # / 查找关键字 # （2）插入模式\n在命令模式下按 \u0026quot; i \u0026ldquo;键，即可进入插入模式，在插入模式可以输入编辑文本内容，使用 esc 键可以返回命令模式。 # （3）ex 模式\n在命令模式中按\u0026rdquo; : \u0026ldquo;键可以进入 ex 模式，光标会移动到底部，在这里可以保存修改或退出 vim. # ext 模式常用命令如下： # :w ：保存当前的修改 # :q ：退出 # :q! ：强制退出，保存修改 # :x :保存并退出，相当于:wq # :set number 显示行号 # :! 系统命令 执行一个系统命令并显示结果 # :sh ：切换到命令行，使用 ctrl+d 切换回 vim # 28. 软件包管理命令(RPM) # 28.1、软件包的安装 # 使用 RPM 命令的安装模式可以将软件包内所有的组件放到系统中的正确路径，安装软件包的命令是:rpm –ivh wu-ftpd-2.6.2-8.i386.rpm # i：作用 rpm 的安装模式 v: 校验文件信息 h: 以＃号显示安装进度 # 28.2、软件包的删除 # 删除模式会将指定软件包的内容全部删除，但并不包括已更改过的配置文件，删除 RPM 软件包的命令如下：rpm –e wu-ftpd # 注意：这里必须使用软件名“wu-ftpd”或”wu-ftpd-2.6.2-8 而不是使用当初安装时的软件包名.wu-ftpd-2.6.2-8.i386.rpm # 28.3、软件包升级 # 升级模式会安装用户所指定的更新版本，并删除已安装在系统中的相同软件包，升级软件包命令如下：rpm –Uvh wu-ftpd-2.6.2-8.i386.rpm –Uvh：升级参数 # 28.4、软件包更新 # 更新模式下，rpm 命令会检查在命令行中所指定的软件包是否比系统中原有的软件 包更新。如果情况属实，rpm 命令会自动更新指定的软件包；反之，若系统中并没有指定软件包的较旧版本，rpm 命令并不会安装此软件包。而在升级模式下，不管系统中是否有较旧的版本，rpm 命令都会安装指定的软件包。 # rpm –Fvhwu-ftpd-2.6.2-8.i386.rpm -Fvh：更新参数 # 28.5、软件包查询 # 若要获取 RPM 软件包的相关信息，可以使用查询模式。使用-q 参数可查询一个已 安装的软件包的内容 # rpm –q wu-ftpd # 查询软件包所安装的位置：rpm –ql package-name # rpm –ql xv (l 参数：显示文件列表) # 好了，今天就介绍到这里，觉得不错了，如果觉得有用，可以先收藏了！ # "},{"id":65,"href":"/docs/acme%E8%87%AA%E5%8A%A8%E6%9B%B4%E6%96%B0%E8%AF%81%E4%B9%A6-acme-zi-dong-geng-xin-zheng-shu/","title":"acme自动更新证书 2024-04-03 14:41:44.289","section":"Docs","content":"acme.sh 实现了 acme 协议, 可以从 letsencrypt 生成免费的证书.\n主要步骤:\n安装 acme.sh 生成证书 copy 证书到 nginx/apache 或者其他服务 更新证书 更新 acme.sh 出错怎么办, 如何调试 下面详细介绍.\n1. 安装 acme.sh # 安装很简单, 一个命令:\ncurl https://get.acme.sh | sh -s email=my@example.com 普通用户和 root 用户都可以安装使用. 安装过程进行了以下几步:\n把 acme.sh 安装到你的 home 目录下: ~/.acme.sh/ 并创建 一个 shell 的 alias, 例如 .bashrc，方便你的使用: alias acme.sh=~/.acme.sh/acme.sh\n自动为你创建 cronjob, 每天 0:00 点自动检测所有的证书, 如果快过期了, 需要更新, 则会自动更新证书. 更高级的安装选项请参考: https://github.com/Neilpang/acme.sh/wiki/How-to-install\n安装过程不会污染已有的系统任何功能和文件, 所有的修改都限制在安装目录中: ~/.acme.sh/\n2. 生成证书 # acme.sh 实现了 acme 协议支持的所有验证协议. 一般有两种方式验证: http 和 dns 验证.\n1. http 方式需要在你的网站根目录下放置一个文件, 来验证你的域名所有权,完成验证. 然后就可以生成证书了. # acme.sh --issue -d mydomain.com -d www.mydomain.com --webroot /home/wwwroot/mydomain.com/ 只需要指定域名, 并指定域名所在的网站根目录. acme.sh 会全自动的生成验证文件, 并放到网站的根目录, 然后自动完成验证. 最后会聪明的删除验证文件. 整个过程没有任何副作用.\n如果你用的 apache服务器, acme.sh 还可以智能的从 apache的配置中自动完成验证, 你不需要指定网站根目录:\nacme.sh --issue -d mydomain.com --apache 如果你用的 nginx服务器, 或者反代, acme.sh 还可以智能的从 nginx的配置中自动完成验证, 你不需要指定网站根目录:\nacme.sh --issue -d mydomain.com --nginx acme.sh --issue -d devopsz.top --nginx 注意, 无论是 apache 还是 nginx 模式, acme.sh在完成验证之后, 会恢复到之前的状态, 都不会私自更改你本身的配置. 好处是你不用担心配置被搞坏, 也有一个缺点, 你需要自己配置 ssl 的配置, 否则只能成功生成证书, 你的网站还是无法访问https. 但是为了安全, 你还是自己手动改配置吧.\n如果你还没有运行任何 web 服务, 80 端口是空闲的, 那么 acme.sh 还能假装自己是一个webserver, 临时听在80 端口, 完成验证:\nacme.sh --issue -d mydomain.com --standalone 更高级的用法请参考: https://github.com/Neilpang/acme.sh/wiki/How-to-issue-a-cert\n2. 手动 dns 方式, 手动在域名上添加一条 txt 解析记录, 验证域名所有权. # 这种方式的好处是, 你不需要任何服务器, 不需要任何公网 ip, 只需要 dns 的解析记录即可完成验证. 坏处是，如果不同时配置 Automatic DNS API，使用这种方式 acme.sh 将无法自动更新证书，每次都需要手动再次重新解析验证域名所有权。\nacme.sh --issue --dns -d mydomain.com \\ --yes-I-know-dns-manual-mode-enough-go-ahead-please 然后, acme.sh 会生成相应的解析记录显示出来, 你只需要在你的域名管理面板中添加这条 txt 记录即可.\n等待解析完成之后, 重新生成证书:\nacme.sh --renew -d mydomain.com \\ --yes-I-know-dns-manual-mode-enough-go-ahead-please 注意第二次这里用的是 --renew\ndns 方式的真正强大之处在于可以使用域名解析商提供的 api 自动添加 txt 记录完成验证.\nacme.sh 目前支持 cloudflare, dnspod, cloudxns, godaddy 以及 ovh 等数十种解析商的自动集成.\n以 dnspod 为例, 你需要先登录到 dnspod 账号, 生成你的 api id 和 api key, 都是免费的. 然后:\nexport DP_Id=\u0026#34;1234\u0026#34; export DP_Key=\u0026#34;sADDsdasdgdsf\u0026#34; acme.sh --issue --dns dns_dp -d aa.com -d www.aa.com 证书就会自动生成了. 这里给出的 api id 和 api key 会被自动记录下来, 将来你在使用 dnspod api 的时候, 就不需要再次指定了. 直接生成就好了:\nacme.sh --issue -d mydomain2.com --dns dns_dp 更详细的 api 用法: https://github.com/Neilpang/acme.sh/blob/master/dnsapi/README.md\n3. copy/安装 证书 # 前面证书生成以后, 接下来需要把证书 copy 到真正需要用它的地方.\n注意, 默认生成的证书都放在安装目录下: ~/.acme.sh/, 请不要直接使用此目录下的文件, 例如: 不要直接让 nginx/apache 的配置文件使用这下面的文件. 这里面的文件都是内部使用, 而且目录结构可能会变化.\n正确的使用方法是使用 --install-cert 命令,并指定目标位置, 然后证书文件会被copy到相应的位置, 例如:\nApache example: # acme.sh --install-cert -d example.com \\ --cert-file /path/to/certfile/in/apache/cert.pem \\ --key-file /path/to/keyfile/in/apache/key.pem \\ --fullchain-file /path/to/fullchain/certfile/apache/fullchain.pem \\ --reloadcmd \u0026#34;service apache2 force-reload\u0026#34; Nginx example: # acme.sh --install-cert -d example.com \\ --key-file /path/to/keyfile/in/nginx/key.pem \\ --fullchain-file /path/to/fullchain/nginx/cert.pem \\ --reloadcmd \u0026#34;service nginx force-reload\u0026#34; (一个小提醒, 这里用的是 service nginx force-reload, 不是 service nginx reload, 据测试, reload 并不会重新加载证书, 所以用的 force-reload)\nNginx 的配置 ssl_certificate 使用 /etc/nginx/ssl/fullchain.cer ，而非 /etc/nginx/ssl/\u0026lt;domain\u0026gt;.cer ，否则 SSL Labs 的测试会报 Chain issues Incomplete 错误。\n--install-cert命令可以携带很多参数, 来指定目标文件. 并且可以指定 reloadcmd, 当证书更新以后, reloadcmd会被自动调用,让服务器生效.\n详细参数请参考: https://github.com/Neilpang/acme.sh#3-install-the-issued-cert-to-apachenginx-etc\n值得注意的是, 这里指定的所有参数都会被自动记录下来, 并在将来证书自动更新以后, 被再次自动调用.\n4. 查看已安装证书信息 # acme.sh --info -d example.com # 会输出如下内容： DOMAIN_CONF=/root/.acme.sh/example.com/example.com.conf Le_Domain=example.com Le_Alt=no Le_Webroot=dns_ali Le_PreHook= Le_PostHook= Le_RenewHook= Le_API=https://acme-v02.api.letsencrypt.org/directory Le_Keylength= Le_OrderFinalize=https://acme-v02.api.letsencrypt.org/acme/finalize/23xxxx150/781xxxx4310 Le_LinkOrder=https://acme-v02.api.letsencrypt.org/acme/order/233xxx150/781xxxx4310 Le_LinkCert=https://acme-v02.api.letsencrypt.org/acme/cert/04cbd28xxxxxx349ecaea8d07 Le_CertCreateTime=1649358725 Le_CertCreateTimeStr=Thu Apr 7 19:12:05 UTC 2022 Le_NextRenewTimeStr=Mon Jun 6 19:12:05 UTC 2022 Le_NextRenewTime=1654456325 Le_RealCertPath= Le_RealCACertPath= Le_RealKeyPath=/etc/acme/example.com/privkey.pem Le_ReloadCmd=service nginx force-reload Le_RealFullChainPath=/etc/acme/example.com/chain.pem 5. 更新证书 # 目前证书在 60 天以后会自动更新, 你无需任何操作. 今后有可能会缩短这个时间, 不过都是自动的, 你不用关心.\n请确保 cronjob 正确安装, 看起来是类似这样的:\ncrontab -l 56 * * * * \u0026#34;/root/.acme.sh\u0026#34;/acme.sh --cron --home \u0026#34;/root/.acme.sh\u0026#34; \u0026gt; /dev/null 6. 关于修改ReloadCmd # 目前修改ReloadCmd没有专门的命令，可以通过重新安装证书来实现修改reloadCmd的目的。 此外，安装证书后，相关信息是保存在~/.acme.sh/example.com/example.conf文件下的，内容就是acme.sh --info -d example.com输出的信息，不过ReloadCmd在文件中使用了Base64编码。理论上可以通过直接修改该文件来修改ReloadCmd，且修改时，无需Base64编码，直接写命令原文acme.sh也可以识别。 不过，example.conf文件的位置和内容格式以后可能会改变！example.conf一直都是内部使用, 后面有可能会改为用 sqlite 或者mysql 格式存储. 所以一般不建议自己修改。\n7. 更新 acme.sh # 目前由于 acme 协议和 letsencrypt CA 都在频繁的更新, 因此 acme.sh 也经常更新以保持同步.\n升级 acme.sh 到最新版 :\nacme.sh --upgrade 如果你不想手动升级, 可以开启自动升级:\nacme.sh --upgrade --auto-upgrade 之后, acme.sh 就会自动保持更新了.\n你也可以随时关闭自动更新:\nacme.sh --upgrade --auto-upgrade 0 8. 出错怎么办： # 如果出错, 请添加 debug log：\nacme.sh --issue ..... --debug 或者：\nacme.sh --issue ..... --debug 2 请参考： https://github.com/Neilpang/acme.sh/wiki/How-to-debug-acme.sh\n在DNS验证模式下如果debug中出现诸如\u0026quot;timed out\u0026quot;等字样可能是因为GFW拦截了相应请求，需要添加http(s) proxy环境变量。（请按照自己实际设定修改）\nexport http_proxy=\u0026#34;socks5h://localhost:1081\u0026#34; \u0026amp;\u0026amp; export https_proxy=\u0026#34;socks5h://localhost:1081\u0026#34; 如果是使用docker则完整示例配置如下：\ndocker run --rm -it \\ -v \u0026#34;/etc/acme\u0026#34;:/acme.sh \\ -e \u0026#34;CF_Token=[填入自己的信息]\u0026#34; \\ -e \u0026#34;CF_Account_ID=[填入自己的信息]\u0026#34; \\ -e \u0026#34;CF_Zone_ID=[填入自己的信息]\u0026#34; \\ -e http_proxy=\u0026#34;socks5h://[代理A]:1234\u0026#34; \\ -e https_proxy=\u0026#34;socks5h://[代理A]:1234\u0026#34; \\ --network container:[代理A]\\ neilpang/acme.sh \\ --issue -d example.com --dns dns_cf --debug 上述例子中使用cloudflare的DNS来签发证书，并通过把acme.sh链接到容器[代理A]，来转发curl请求（请按照自己实际设定修改）\n最后, 本文并非完全的使用说明, 还有很多高级的功能, 更高级的用法请参看其他 wiki 页面.\nhttps://github.com/Neilpang/acme.sh/wiki\n要将 /root/nginx-config/auto.sh 脚本设置为 Linux 开机自启，你可以使用不同的方法，具体取决于你使用的 Linux 发行版和 init 系统（如 SysV init、Upstart、systemd 等）。下面我将展示如何在常见的 systemd 系统（如 CentOS 7+、Ubuntu 16.04+ 等）上设置开机自启。\n使用 systemd 创建服务单元文件 # 创建服务单元文件 首先，你需要创建一个 systemd 服务单元文件。使用你喜欢的文本编辑器创建一个新文件，例如 /etc/systemd/system/nginx-auto.service：\nbash复制代码 sudo nano /etc/systemd/system/nginx-auto.service 编辑服务单元文件 在打开的文件中，输入以下内容：\n[Unit] Description=Run nginx-config/auto.sh on startup After=network.target [Service] Type=simple ExecStart=/bin/bash /root/nginx-config/auto.sh Restart=on-failure User=root Group=root [Install] WantedBy=multi-user.target 这个服务单元文件定义了服务的描述、依赖关系、执行命令以及重启策略等。 \\3. 重新加载 systemd 配置\n在创建或修改服务单元文件后，你需要通知 systemd 重新加载配置：\nbash复制代码 sudo systemctl daemon-reload 启用并启动服务 现在，你可以启用服务使其在下次启动时自动运行，并立即启动它：\nsudo systemctl enable nginx-auto.service sudo systemctl start nginx-auto.service 检查服务状态 你可以使用以下命令检查服务的状态：\nbash复制代码 sudo systemctl status nginx-auto.service 如果一切正常，你应该能看到服务正在运行的信息。\n注意事项 # 确保 /root/nginx-config/auto.sh 脚本具有执行权限：chmod +x /root/nginx-config/auto.sh 如果你的脚本需要以非 root 用户身份运行，请相应地修改 User 和 Group 设置，并确保该用户有权访问脚本和所需的其他资源。 如果你使用的是其他 init 系统（如 SysV init 或 Upstart），设置开机自启的方法会有所不同。请根据你的系统和需求查阅相关文档。 "},{"id":66,"href":"/docs/calico-%E9%85%8D%E7%BD%AE-bgp-route-reflectors-calico-pei-zhi-bgproutereflectors/","title":"calico 配置 BGP Route Reflectors 2023-09-28 15:34:01.007","section":"Docs","content":" calico 配置 BGP Route Reflectors # Calico作为k8s的一个流行网络插件，它依赖BGP路由协议实现集群节点上的POD路由互通；而路由互通的前提是节点间建立 BGP Peer 连接。BGP 路由反射器（Route Reflectors，简称 RR）可以简化集群BGP Peer的连接方式，它是解决BGP扩展性问题的有效方式；具体来说： # 没有 RR 时，所有节点之间需要两两建立连接（IBGP全互联），节点数量增加将导致连接数剧增、资源占用剧增 # 引入 RR 后，其他 BGP 路由器只需要与它建立连接并交换路由信息，节点数量增加连接数只是线性增加，节省系统资源 # calico-node 版本 v3.3 开始支持内建路由反射器，非常方便，因此使用 calico 作为网络插件可以支持大规模节点数的K8S集群。 # 建议集群节点数大于50时，应用BGP Route Reflectors 特性 # 前提条件 # k8s 集群使用calico网络插件部署成功。本实验环境为按照kubeasz安装的2主2从集群，calico 版本 v3.19.4。 # $ kubectl get node NAME STATUS ROLES AGE VERSION 192.168.1.1 Ready,SchedulingDisabled master 178m v1.13.1 192.168.1.2 Ready,SchedulingDisabled master 178m v1.13.1 192.168.1.3 Ready node 178m v1.13.1 192.168.1.4 Ready node 178m v1.13.1 $ kubectl get pod -n kube-system -o wide | grep calico calico-kube-controllers-77487546bd-jqrlc 1/1 Running 0 179m 192.168.1.3 192.168.1.3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; calico-node-67t5m 2/2 Running 0 179m 192.168.1.1 192.168.1.1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; calico-node-drmhq 2/2 Running 0 179m 192.168.1.2 192.168.1.2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; calico-node-rjtkv 2/2 Running 0 179m 192.168.1.4 192.168.1.4 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; calico-node-xtspl 2/2 Running 0 179m 192.168.1.3 192.168.1.3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 查看当前集群中BGP连接情况：可以看到集群中4个节点两两建立了 BGP 连接 # $ dk ansible -i /etc/kubeasz/clusters/xxx/hosts all -m shell -a \u0026#39;/opt/kube/bin/calicoctl node status\u0026#39; 192.168.1.3 | SUCCESS | rc=0 \u0026gt;\u0026gt; Calico process is running. IPv4 BGP status +--------------+-------------------+-------+----------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +--------------+-------------------+-------+----------+-------------+ | 192.168.1.1 | node-to-node mesh | up | 03:08:20 | Established | | 192.168.1.2 | node-to-node mesh | up | 03:08:18 | Established | | 192.168.1.4 | node-to-node mesh | up | 03:08:19 | Established | +--------------+-------------------+-------+----------+-------------+ IPv6 BGP status No IPv6 peers found. 192.168.1.2 | SUCCESS | rc=0 \u0026gt;\u0026gt; Calico process is running. IPv4 BGP status +--------------+-------------------+-------+----------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +--------------+-------------------+-------+----------+-------------+ | 192.168.1.4 | node-to-node mesh | up | 03:08:17 | Established | | 192.168.1.3 | node-to-node mesh | up | 03:08:18 | Established | | 192.168.1.1 | node-to-node mesh | up | 03:08:20 | Established | +--------------+-------------------+-------+----------+-------------+ IPv6 BGP status No IPv6 peers found. 192.168.1.1 | SUCCESS | rc=0 \u0026gt;\u0026gt; Calico process is running. IPv4 BGP status +--------------+-------------------+-------+----------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +--------------+-------------------+-------+----------+-------------+ | 192.168.1.2 | node-to-node mesh | up | 03:08:21 | Established | | 192.168.1.3 | node-to-node mesh | up | 03:08:21 | Established | | 192.168.1.4 | node-to-node mesh | up | 03:08:21 | Established | +--------------+-------------------+-------+----------+-------------+ IPv6 BGP status No IPv6 peers found. 192.168.1.4 | SUCCESS | rc=0 \u0026gt;\u0026gt; Calico process is running. IPv4 BGP status +--------------+-------------------+-------+----------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +--------------+-------------------+-------+----------+-------------+ | 192.168.1.2 | node-to-node mesh | up | 03:08:17 | Established | | 192.168.1.3 | node-to-node mesh | up | 03:08:19 | Established | | 192.168.1.1 | node-to-node mesh | up | 03:08:20 | Established | +--------------+-------------------+-------+----------+-------------+ IPv6 BGP status No IPv6 peers found. kubeasz 自动安装启用 route reflector # 修改/etc/kubeasz/clusters/xxx/config.yml文件，设置配置项CALICO_RR_ENABLED: true # 重新执行网络安装 dk ezctl setup xxx 07 # 执行完成，检查bgp连接验证即可。 # 附：手动安装route reflector 过程讲解 # 选择并配置 Route Reflector 节点 首先查看当前集群中的节点：\n$ calicoctl get node -o wide NAME ASN IPV4 IPV6 k8s401 (64512) 192.168.1.1/24 k8s402 (64512) 192.168.1.2/24 k8s403 (64512) 192.168.1.3/24 k8s404 (64512) 192.168.1.4/24 可以在集群中选择1个或多个节点作为 rr 节点，这里先选择节点：k8s401 # #配置routeReflectorClusterID calicoctl patch node k8s401 -p \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;bgp\u0026#34;: {\u0026#34;routeReflectorClusterID\u0026#34;: \u0026#34;244.0.0.1\u0026#34;}}}\u0026#39; #配置node label calicoctl patch node k8s401 -p \u0026#39;{\u0026#34;metadata\u0026#34;: {\u0026#34;labels\u0026#34;: {\u0026#34;route-reflector\u0026#34;: \u0026#34;true\u0026#34;}}}\u0026#39; 配置 BGP node 与 Route Reflector 的连接建立规则 # $ cat \u0026lt;\u0026lt; EOF | calicoctl create -f - kind: BGPPeer apiVersion: projectcalico.org/v3 metadata: name: peer-with-route-reflectors spec: nodeSelector: all() peerSelector: route-reflector == \u0026#39;true\u0026#39; EOF 配置全局禁用全连接（BGP full mesh） # $ cat \u0026lt;\u0026lt; EOF | calicoctl create -f - apiVersion: projectcalico.org/v3 kind: BGPConfiguration metadata: name: default spec: logSeverityScreen: Info nodeToNodeMeshEnabled: false asNumber: 64512 EOF 验证增加 rr 之后的bgp 连接情况 # $ dk ansible -i /etc/kubeasz/clusters/xxx/hosts all -m shell -a \u0026#39;/opt/kube/bin/calicoctl node status\u0026#39; 192.168.1.4 | SUCCESS | rc=0 \u0026gt;\u0026gt; Calico process is running. IPv4 BGP status +--------------+-----------+-------+----------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +--------------+-----------+-------+----------+-------------+ | 192.168.1.1 | node specific | up | 11:02:55 | Established | +--------------+-----------+-------+----------+-------------+ IPv6 BGP status No IPv6 peers found. 192.168.1.3 | SUCCESS | rc=0 \u0026gt;\u0026gt; Calico process is running. IPv4 BGP status +--------------+-----------+-------+----------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +--------------+-----------+-------+----------+-------------+ | 192.168.1.1 | node specific | up | 11:02:55 | Established | +--------------+-----------+-------+----------+-------------+ IPv6 BGP status No IPv6 peers found. 192.168.1.1 | SUCCESS | rc=0 \u0026gt;\u0026gt; Calico process is running. IPv4 BGP status +--------------+---------------+-------+----------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +--------------+---------------+-------+----------+-------------+ | 192.168.1.2 | node specific | up | 11:02:55 | Established | | 192.168.1.3 | node specific | up | 11:02:55 | Established | | 192.168.1.4 | node specific | up | 11:02:55 | Established | +--------------+---------------+-------+----------+-------------+ IPv6 BGP status No IPv6 peers found. 192.168.1.2 | SUCCESS | rc=0 \u0026gt;\u0026gt; Calico process is running. IPv4 BGP status +--------------+-----------+-------+----------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +--------------+-----------+-------+----------+-------------+ | 192.168.1.1 | node specific | up | 11:02:55 | Established | +--------------+-----------+-------+----------+-------------+ IPv6 BGP status No IPv6 peers found. 可以看到所有其他节点都与所选rr节点建立bgp连接。\n再增加一个 rr 节点(略) 步骤同上，添加成功后可以看到所有其他节点都与两个rr节点建立bgp连接，两个rr节点之间也建立bgp连接。对于节点数较多的K8S集群建议配置2-3个 RR 节点。\n参考文档 # 1.Calico bgp 配置指南 2.BGP路由反射器基础 "},{"id":67,"href":"/docs/calico%E7%BD%91%E7%BB%9C%E8%87%AA%E5%AE%9A%E4%B9%89-calico-wang-luo-zi-ding-yi/","title":"Calico网络自定义 2024-04-03 15:08:43.394","section":"Docs","content":" 一文学会Calico网络自定义 # 一、确定最佳的网络选项 # Calico支持多个容器网络选项，用于可伸缩性、网络性能和与现有基础设施的互操作性。\n1. 值 # 不同的网络实现更适合不同的环境。Calico提供了几种不需要封装的基于IP路由的网络实现。如果您的部署需要封装，Calico提供覆盖网络(IP in IP或VXLAN)。Calico还支持使用其他Kubernetes网络选项来执行策略。本文档帮助您为集群选择最佳的网络选项。\n2. 概念 # 2.1 关于calico networking # Calico提供了一些方法，允许pod连接到其他pod、主机和外部网络(例如internet)。\nCalico网络：\n使用Calico的IP地址管理(IPAM)将IP地址分配给pods 编写本地节点的路由表 将路由分配给其他节点和网络设备 2.2 关于BGP # Calico支持使用边界网关协议(BGP)将路由信息共享到网络中。Calico支持节点到节点的全网格部署(有和没有路由反射器)，以及BGP直接对机架(ToR)路由器顶部的现场部署;允许流量直接路由到工作负载，而不需要NAT或封装。\n2.3 其它Kubernetes 网络选项 # Calico可以使用许多其他Kubernetes网络选项执行网络策略强制。\nFlannel Amazon AWS VPC CNI Azure CNI Google cloud networking 下表显示了使用Calico时常见的网络选项。\n网络选项\n3. 基本说明 # 本节提供更多关于Calico的内置网络选项的细节:\nUnencapsulated, peered with physical infrastructure Unencapsulated, not peered with physical infrastructure IP in IP or VXLAN encapsulation 3.1 Unencapsulated, peered with physical infrastructure # Calico可以与你的路由器使用BGP对等。这提供了出色的性能和易于调试的非封装流量，以及广泛的网络拓扑和连接选项。\n您的集群可以跨越多个L2子网，而不需要封装 集群外的资源可以直接与pod通信，而不需要NAT 如果你想的话，你甚至可以把pod直接暴露在互联网上! 3.2 Unencapsulated, not peered with physical infrastructure # 此选项还提供了接近主机到主机的性能级别，并允许网络直接看到流量。\n当所有节点都在一个L2子网上时，如果底层网络不强制执行IP地址检查，Calico可以在节点之间路由pod流量，而不需要封装。如果您的网络由多个L2子网组成，那么您可以使用路由器在BGP上进行对等，或者使用跨子网封装来仅封装跨子网边界的流量。\n如果不允许在集群外部进行工作负载访问或使用基础设施进行对等访问，就无法在pod和不属于Calico集群的目的地之间路由流量。\n3.3 IP in IP or VXLAN encapsulation # 如果可能，我们建议运行Calico没有网络覆盖/封装。这提供了最高的性能和最简单的网络;离开您的工作负载的包是连接到网络上的包。\n但是，当运行在底层网络上时，有选择地使用覆盖(IP中的IP或VXLAN中的IP)是非常有用的，因为底层网络不容易知道工作负载IP。Calico可以对:所有的流量，没有流量，或者只对跨越子网边界的流量进行封装。\nIP中的IP或VXLAN封装也可以在子网之间选择性地使用——这提供了子网中未封装的流量的性能优势，适用于织物包含多个L2网络且无法进行对等连接的环境。例如，如果您在AWS中跨多个VPC/子网使用Calico网络，Calico可以选择性地只封装在VPC/子网之间路由的流量，而不封装在每个VPC/子网中运行。\n二、配置BGP路由反射及对等体 # 1. BGP协议配置 # apiVersion: projectcalico.org/v3 kind: BGPConfiguration metadata: name: default spec: logSeverityScreen: Info nodeToNodeMeshEnabled: true asNumber: 63400 serviceClusterIPs: - cidr: 10.96.0.0/12 serviceExternalIPs: - cidr: 192.168.20.99/32 2. 关闭默认的BGP node-to-node mesh # BGP网格\n缺省的节点到节点的BGP网格必须关闭，以启用其他BGP拓扑。为此，修改默认的BGP配置资源。\n运行下面的命令去关闭BGP full-mesh calicoctl patch bgpconfiguration default -p \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;nodeToNodeMeshEnabled\u0026#34;: false}}\u0026#39; 2. 配置全局的 BGP 对等体 # 下面的示例创建一个全局BGP对等点，它将每个Calico节点配置为在AS 64567中使用192.20.30.40的对等点。 apiVersion: projectcalico.org/v3 kind: BGPPeer metadata: name: my-global-peer spec: peerIP: 192.20.30.40 asNumber: 64567 3. 配置每节点的 BGP peer # 每个节点的BGP对等点应用于集群中的一个或多个节点。您可以通过精确地指定节点的名称或使用标签选择器来选择节点。\napiVersion: projectcalico.org/v3 kind: BGPPeer metadata: name: rack1-tor spec: peerIP: 192.20.30.40 asNumber: 64567 nodeSelector: rack == ‘rack-1’ 4. 配置node作为路由反射器 # Calico 可以配置扮演成一个路由反射器。每个节点要充当路由反射器必须有一个集群ID——通常一个未使用的IPv4地址。\n配置一个节点作为路由反射器，有个集群ID 244.0.0.1， 运行如下命令： calicoctl patch node c76085.xiodi.cn -p \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;bgp\u0026#34;: {\u0026#34;routeReflectorClusterID\u0026#34;: \u0026#34;244.0.0.1\u0026#34;}}}\u0026#39; 常规情况下，给这个节点打上标签，标明这个是路由反射器。允许它更容易通过BGPPeer resource选择。 kubectl label node c76085.xiodi.cn route-reflector=true 现在使用标签器很容易配置路由反射器节点和非路由反射器节点。比如： kind: BGPPeer apiVersion: projectcalico.org/v3 metadata: name: peer-with-route-reflectors spec: nodeSelector: all() peerSelector: route-reflector == \u0026#39;true\u0026#39; 针对一个节点查看BGP peering 状态 您可以使用calicoctl查看一个特定节点的边界网关协议连接的当前状态。这是用于确认您的配置是根据需要的行为。 sudo calicoctl node status 改变默认的global AS number 默认的，所有的calico 节点使用64512 autonomous system, 除非特殊指定。下面的命令把它改成64513.\ncalicoctl patch bgpconfiguration default -p \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;asNumber\u0026#34;: “64513”}}\u0026#39; 针对特定的节点改变AS number,如下所示 calicoctl patch node node-1 -p \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;bgp\u0026#34;: {“asNumber”: “64514”}}}\u0026#39; 三、IPinIP模式 # 您可以配置每个IP池不同封装配置。然而,你不能一个IP池内混合封装类型。\nConfigure IP in IP encapsulation for only cross subnet traffic Configure IP in IP encapsulation for all inter workload traffic Configure VXLAN encapsulation for only cross subnet traffic Configure VXLAN encapsulation for all inter workload traffic IPv4/6 地址支持 IP in IP和 VXLAN只支持IPv4地址。\n最佳实践 Calico 只有一个选项来选择性地封装流量 ,跨越子网边界。我们建议使用IP in IP的cross subnet选项把开销降到最低。\n注意：切换封装模式会导到正在连接的进程中断。\n针对仅跨子网的流量配置IP in IP IP in IP封装可以选择性的执行， 并且仅用于通过子网边界的通信量 。\n开启这个功能，设置ipipMode为CrossSubnet\napiVersion: projectcalico.org/v3 kind: IPPool metadata: name: ippool-ipip-cross-subnet-1 spec: cidr: 192.168.0.0/16 ipipMode: CrossSubnet natOutgoing: true 针对workload间的流量配置IP in IP的封装 ipipMode`设置`Always apiVersion: projectcalico.org/v3 kind: IPPool metadata: name: ippool-ipip-1 spec: cidr: 192.168.0.0/16 ipipMode: Always natOutgoing: true 四、vxlan两种模式解析 # 针对仅跨越子网的流量 ，配置VXLAN封装 配置这个功能，设置vxlanMode为CrossSubnet apiVersion: projectcalico.org/v3 kind: IPPool metadata: name: ippool-vxlan-cross-subnet-1 spec: cidr: 192.168.0.0/16 vxlanMode: CrossSubnet natOutgoing: true 针对workload间的流量 配置VXLAN的封装 apiVersion: projectcalico.org/v3 kind: IPPool metadata: name: ippool-vxlan-1 spec: cidr: 192.168.0.0/16 vxlanMode: Always natOutgoing: true 五、通过Mtu增加网络性能 # 1. 确定MTU的大小 # 下表针对Calico环境列出了常见的MTU大小。 因为MTU是endpoints间网络路径的全局属性， 你应该设定最低MTU的MTU包可能需要的任何路径。\n1.1 常见的MTU大小 # Network MTU Calico MTU Calico MTU with IP-in-IP (IPv4) Calico MTU with VXLAN (IPv4) 1500 1500 1480 1450 9000 9000 8980 8950 1460(GCE) 1460 1440 1410 9001(AWS jumbo) 9001 8981 8951 1450(OpenStack VXLAN) 1450 1430 1400 1.2 针对overlay网络推荐的MTU # 额外的报头用于在IP和VXLAN IP协议,降低了最小的MTU大小头。在IP使用20-byte头(IP, VXLAN使用50-byte头)。因此,我们建议如下:\n如果你使用在Pod网络使用VXLAN,配置MTU大小“物理网络MTU大小- 50”。 假如你使用IP in IP, 配置MTU大小为“物理网络大小-20” workload endpoint MTU和 tunnel MTU设置为相同的值。 1.3 针对flannel网络的MTU # 当使用flannel的网络时，网络接口的MTU应该匹配flannel接口的MTU。 假如使用flannel的VXLAN， 使用上面的calico MTU with VXLAN列的大小。\n2. 配置MTU针对workloads # 当你设置MTU,它适用于新工作负载。MTU变化应用于现有的工作负载,必须重新启动calico nodes。\n编辑calico-config ConfigMap FelixConfiguration设置值。例如: kubectl patch configmap/Calico-config -n kube-system --type merge \\ -p \u0026#39;{\u0026#34;data\u0026#34;:{\u0026#34;veth_mtu\u0026#34;: \u0026#34;1440\u0026#34;}}\u0026#39; 3. 针对overlay 网络配置MTU # 如果您使用的是IP in IP和/或VXLAN calico overlay 网络,设置隧道MTU匹配veth MTU配置的值。\n编辑calico-config ConfigMap设置MTU在FelixConfiguration隧道值。例如: # Configure the MTU to use veth_mtu: \u0026#34;1440\u0026#34; 查看当前tunnel MTU值 ip link show IP在IP隧道作为tunlx出现(例如,tunl0),连同MTU大小。例如:\nmtu\n六、calico网络地址转换 # 1. 允许workloads 访问 internet，私有IP地址 # 允许工作负载使用私有IP地址访问互联网,你可以用你现有NAT功能,或者你可以在Calico IPPool上开启natOutgoing。\n在以下的示例中,我们创建一个Calicco IPPool，并开启natOutgoing 。Outbound NAT是在节点本地执行的。\napiVersion: projectcalico.org/v3 kind: IPPool metadata: name: default-ipv4-ippool spec: cidr: 192.168.0.0/16 natOutgoing: true 2. 仅去nat那些指定的IP地址范围 # 您可以创建额外的IPPools不用于IP地址管理,防止NAT某些CIDR块。这是有用的,如果你想让节点NAT网络流量,但不是在某些内部ip范围。例如,如果您不想NAT流量10.0.0.0/8,您可以创建以下池。您必须确保集群之间的网络和10.0.0.0/8可路由。\napiVersion: projectcalico.org/v3 kind: IPPool metadata: name: no-nat-10.0.0.0-8 spec: cidr: 10.0.0.0/8 disabled: true 七、IP地址检测剖析 # 针对calico节点配置IP自动检测，确保路由使用正确的P地址。\n1. 值 # 当你安装Calico在一个节点上时，一个IP地址和子网被自动检测。Calico提供几种方式去配置子网自动检测，和支持配置指定的IPs。\n拥有多个外部接口的主机 主机接口拥有多个IP地址 改变跨子网包的封装 改变主机IP地址 2. 概念 # 2.1 自动检测节点IP地址和子网 # 针对节点间的路由，每个calico节点必须配置一个IPv4地址 和/或 一个IPV6地址，当安装一个calico在一个节点上时，一个节点资源使用从主机检测到的路由信息自动创建。针对一些部署，你可能想要自动的更新检测，确保节点获取正确的IP地址。\n在安装后默认的节点资源案例 apiVersion: projectcalico.org/v3 kind: Node metadata: name: node-hostname spec: bgp: asNumber: 64512 ipv4Address: 10.244.0.1/24 ipv6Address: 2000:db8:85a3::8a2e:370:7335/120 ipv4IPIPTunnelAddr: 192.168.0.1 2.2 自动检测方法 # 默认的，Calico使用first-found方法，也就是说第一个接口第一个有效的IP地址（排除local interface，因为它是docker bridge）.你可以使用以下方法的任一一种改变默认方法。\n（1）使用一个能到达特定IP或domain的地址。\n（2）使用正则的方式，去匹配接口（interface)\n（3）使用正则的方式，去排除匹配的接口（skip interface）\n2.3 手动配置IP地址和子网 # 有两种方式去手动的配置IP地址和子网 (1) calico node container(start/restart),使用环境变量去设置节点的值\n(2) 更新节点的资源\n使用环境变量和节点的资源 因为你可以通过配置环境变量和节点资源，去更改IP地址和子网，下表描述了这些值如何同步的。\nIf this environment variable… Is… Then… IP/IP6 Explicitly set The specified values are used, and the Node resource is updated. Set to autodetect The requested method is used (first-found, can-reach, interface, skip-interface), and the Node resource is updated. Not set, but Node resource has IP/IP6 values Node resource value is used. IP Not set, and there is no IP value in Node resource Autodetects an IPv4 address and subnet, and updates Node resource. IP6 Not set, and there is a notIP6 value in Node resource No IP6 routing is performed on the node。 3. 动作 # 3.1 改变自动检测方法 # 由于默认的自动检测方法是first valid interface found(first-found). 去使用不同的自动检测方法，使用kubectl set env命令指定方法。\nIPv4 kubectl set env daemonset/calico-node -n kube-system IP_AUTODETECTION_METHOD=\u0026lt;autodetection-method\u0026gt; IPv6 kubectl set env daemonset/calico-node -n kube-system IP6_AUTODETECTION_METHOD=\u0026lt;autodetection-method\u0026gt; 设置检测方法基于情况 （1）IP 或 domain name\nkubectl set env daemonset/calico-node -n kube-system IP_AUTODETECTION_METHOD=can-reach=www.google.com （2）包含匹配的接口\nkubectl set env daemonset/calico-node -n kube-system IP_AUTODETECTION_METHOD=interface=eth.* （3）排除匹配的接口\nkubectl set env daemonset/calico-node -n kube-system IP_AUTODETECTION_METHOD=skip-interface=eth.* 3.2 针对节点手动配置IP地址和子网 # 在下列情况下,您可能需要配置一个特定的IP子网:\n主机拥有多个外部的接口 主机接口拥有多个IP地址 改变跨子网数据包的封装 改变主机IP地址 使用环境变量配置IP和子网 kubectl set env daemonset/calico-node -n kube-system IP=10.0.2.10/24 IP6=fd80:24e2:f998:72d6::/120 使用节点资源配置IP和子网 calicoctl patch node kind-control-plane \\ --patch=\u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;bgp\u0026#34;: {\u0026#34;ipv4Address\u0026#34;: \u0026#34;10.0.2.10/24\u0026#34;, \u0026#34;ipv6Address\u0026#34;: \u0026#34;fd80:24e2:f998:72d6::/120\u0026#34;}}}\u0026#39; 八、IP地址及封装模式切换 针对安装Calico到kubernetes集群中的每个节点，每个manifest包含所有需要的资源。\n它安装如下的kubernetes资源：\n在每个主机上使用Daemonset的方式安装calico/node容器。 在每个主机上使用Daemonset的方式安装Calico CNI和网络的配置。 使用deployment的方式运行calico/kube-controllers calico-etcd-secretssecret, 它提供到etcd存储的TLS. calico-config ConfigMap. 它包含安装配置的参数。 1. 配置pod IP 范围 # Calico IPAM从IP pools中分配 IP地址。\n如果要修改pods使用的默认IP地址范围，那么修改calico.yaml清单中的CALICO_IPV4POOL_CIDR。\n2. 配置IP-in-IP # 默认情况下，清单支持跨子网的IP-in-IP封装。许多用户可能希望禁用IP-in-IP封装，例如在以下情况下。\n他们的集群运行在正确配置的AWS VPC中。 它们的所有Kubernetes节点都连接到同一个第2层网络。 他们打算使用BGP peer，使他们的基础设施意识到pod IP地址。 如果要关闭IP-in-IP的封装，修改清单中的CALICO-IPV4POOL_IPIP.\n3. 从IP-in-IP到VXLAN的切换 # 默认情况下，Calico清单支持IP-in-IP封装。如果您所在的网络阻塞了ip中的ip，比如Azure，您可能希望切换到Calico的VXLAN封装模式。要做到这一点，在安装时(以便Calico创建默认的IP池与VXLAN和没有IP-in-IP配置必须撤消):\n启动calico for policy and networking 清单 使用CALICO_IPV4POOL_VXLAN取代CALICO_IPV4POOL_IPIP的名字。新的变量值同样保持为 Always 完全的关闭Calico的基于BGP网络： 使用calico_backend: \u0026quot;vxlan\u0026quot; 代替calico_backend: \u0026quot;bird\u0026quot;. 此步是关闭BIRD。 从calico/node readiness/liveness check中注释掉--bird-ready和bird-live行。（否则关闭BIRD，将会导致readniess/liveness检查失败） livenessProbe: exec: command: - /bin/calico-node - -felix-live # - -bird-live readinessProbe: exec: command: - /bin/calico-node # - -bird-ready - -felix-ready k8s 操作 # 1.取消k8s master节点不可调度 # kubectl taint nodes k8s-master node-role.kubernetes.io/master:NoSchedule- 目录 Calico简介 Calico 网络Node之间通信网络 Calico网络模型主要工作组件 示例1:安装calico calicoctl命令安装与使用 示例2:修改BGP网络\nCalico简介 # Calico 是一种容器之间互通的网络方案。在虚拟化平台中，比如 OpenStack、Docker 等都需要实现 workloads 之间互连，但同时也需要对容器做隔离控制，就像在 Internet 中的服务仅开放80端口、公有云的多租户一样，提供隔离和管控机制。而在多数的虚拟化平台实现中，通常都使用二层隔离技术来实现容器的网络，这些二层的技术有一些弊端，比如需要依赖 VLAN、bridge 和隧道等技术，其中 bridge 带来了复杂性，vlan 隔离和 tunnel 隧道则消耗更多的资源并对物理环境有要求，随着网络规模的增大，整体会变得越加复杂。我们尝试把 Host 当作 Internet 中的路由器，同样使用 BGP 同步路由，并使用 iptables 来做安全访问策略，最终设计出了 Calico 方案。\n设计思想：Calico 不使用隧道或 NAT 来实现转发，而是巧妙的把所有二三层流量转换成三层流量，并通过 host 上路由配置完成跨 Host 转发\n设计优势： # 1.更优的资源利用 二层网络通讯需要依赖广播消息机制，广播消息的开销与 host 的数量呈指数级增长，Calico 使用的三层路由方法，则完全抑制了二层广播，减少了资源开销。\n2.可扩展性 Calico 使用与 Internet 类似的方案，Internet 的网络比任何数据中心都大，Calico 同样天然具有可扩展性。\n3.简单而更容易 debug 因为没有隧道，意味着 workloads 之间路径更短更简单，配置更少，在 host 上更容易进行 debug 调试。\n4.更少的依赖 Calico 仅依赖三层路由可达。\n5.可适配性 Calico 较少的依赖性使它能适配所有 VM、Container、白盒或者混合环境场景。\nCalico 网络Node之间通信网络 # IPIP(可跨网段通信) 从字面来理解，就是把一个IP数据包又套在一个IP包里，即把 IP 层封装到 IP 层的一个 tunnel。它的作用其实基本上就相当于一个基于IP层的网桥！一般来说，普通的网桥是基于mac层的，根本不需 IP，而这个 ipip 则是通过两端的路由做一个 tunnel，把两个本来不通的网络通过点对点连接起来。 类似vxlan 但封装开销比vxlan小 效率相对更高一些，但安全性也更差\nVxlan(可跨网段通信) 与Flannel Vxlan原理相同\nBGP(二层网络通信) 边界网关协议（Border Gateway Protocol, BGP）是互联网上一个核心的去中心化自治路由协议。它通过维护IP路由表或‘前缀’表来实现自治系统（AS）之间的可达性，属于矢量路由协议。BGP不使用传统的内部网关协议（IGP）的指标，而使用基于路径、网络策略或规则集来决定路由。因此，它更适合被称为矢量性协议，而不是路由协议。BGP，通俗的讲就是讲接入到机房的多条线路（如电信、联通、移动等）融合为一体，实现多线单IP，BGP 机房的优点：服务器只需要设置一个IP地址，最佳访问路由是由网络上的骨干路由器根据路由跳数与其它技术指标来确定的，不会占用服务器的任何系统 实际上，Calico 项目提供的 BGP 网络解决方案，与 Flannel 的 host-gw 模式几乎一样。也就是说，Calico也是基于路由表实现容器数据包转发，但不同于Flannel使用flanneld进程来维护路由信息的做法，而Calico项目使用BGP协议来自动维护整个集群的路由信息。\n部署推荐方案: BGP+Vxlan\n其中BGP在官方的推荐方案中 以50个节点为界区别了不同规模使用不同的部署方案\n小规模网络:BGP peer 一对一网络：每个节点都是有N-1条路由，小型网络适用，当节点数N变多时，路由表更新及AIP-SERVER都需要承受很大的压力 类似网络拓扑结构中的 网状拓扑结构 大规模网络:BGP Reflector 路由反射器：选择一到多个节点做为Reflector，所有节点路由都汇总给Reflector，所有节点都路由都指向Reflector ，适合大型网络，类似网络拓扑结构中的星型网络 Calico网络模型主要工作组件：### # Felix：运行在每一台 Host 的 agent 进程，主要负责网络接口管理和监听、路由、ARP 管理、ACL 管理和同步、状态上报等。 etcd：分布式键值存储，主要负责网络元数据一致性，确保Calico网络状态的准确性，可以与kubernetes共用； BGP Client（BIRD）：Calico 为每一台 Host 部署一个 BGP Client，使用 BIRD 实现，BIRD 是一个单独的持续发展的项目，实现了众多动态路由协议比如 BGP、OSPF、RIP 等。在 Calico 的角色是监听 Host 上由 Felix 注入的路由信息，然后通过 BGP 协议广播告诉剩余 Host 节点，从而实现网络互通。 BGP Route Reflector：在大型网络规模中，如果仅仅使用 BGP client 形成 mesh 全网互联的方案就会导致规模限制，因为所有节点之间俩俩互联，需要 N^2 个连接，为了解决这个规模问题，可以采用 BGP 的 Router Reflector 的方法，使所有 BGP Client 仅与特定 RR 节点互联并做路由同步，从而大大减少连接数。 Calico有两种运行方式， # 是让calico/node独立运行于Kubernetes集群之外，但calico/kube-controllers依然需要以Pod资源运行中集群之上; 是以CNI插件方式配置Calico完全托管运行于Kubernetes集群之上，类似于我们前面曾经部署托管Flannel网络插件的方式。 对于后一种方式,Calico提供了在线的部署清单，它分别为50节点及以下规模和50节点以上规模的Kubernetes集群使用Kubernetes API作为Dabastore提供了不同的配置清单，也为使用独立的etcd集群提供了专用配置清单。但这3种类型的配置清单中,Calico默认启用的是基于IPIP隧道的叠加网络，因而它会在所有流量上使用IPIP隧道而不是BGP路由。以下配置定义在部署清单中DaemonSet/calico-node资源的Pod模板中的calico-node容器之上。 配置选项 在IPv4类型的地址池上启用的IPIP及其类型，支持3种可用值 Always(全局流量)、Cross-SubNet(跨子网流量)和Never3种可用值\nname: CALICO_IPV4POOL_IPIP value: \u0026ldquo;Always\u0026rdquo; 是否在IPV4地址池上启用VXLAN隧道协议，取值及意义与Flannel的VXLAN后端相同;但在全局流量启用VXLAN时将完全不再需要BGP网络，建议将相关的组件禁用 name: CALICO_ IPV4POOL_VXLAN value: \u0026ldquo;Never\u0026rdquo; 需要注意的是，Calico分配的地址池需要同Ktbernetes集群的Pod网络的定义保持一致。Pod网络通常由kubeadm init初始化集群时使用\u0026ndash;pod-network-cidr选项指定的网络，而Calico在其默认的配置清单中默认使用192.168.0.0/16作为Pod网络，因而部署Kubernetes集群时应该规划好要使用的网络地址，并设定此二者相匹配。对于曾经使用了flannel的默认的10.244.0.0/16网络的环境而言,我们也可以选择修改资源清单中的定义，从而将其修改为其他网络地址,它定义在DaemonSet/calico-node资源的Pod模板中的calico-node容器之上。 官网链接：\nhttps://docs.projectcalico.org/getting-started/kubernetes/self-managed-onprem/onpremises\n示例1:安装calico # wget https://docs.projectcalico.org/manifests/calico.yaml [root@k8s-master ~]# cd /etc/kubernetes/manifests/ [root@k8s-master manifests]# cat kube-controller-manager.yaml ... System Info: Machine ID: 32599e2a74704b2e95443e24ea15d4f6 System UUID: 34979a62-16de-4287-b149-2d4c2d8a70fb Boot ID: f31de60e-4f89-4553-ba7a-99a46d049936 Kernel Version: 5.4.109-1.el7.elrepo.x86_64 OS Image: CentOS Linux 7 (Core) Operating System: linux Architecture: amd64 Container Runtime Version: docker://20.10.7 Kubelet Version: v1.19.9 Kube-Proxy Version: v1.19.9 PodCIDR: 10.244.1.0/24 #第个节点的地址块都是由K8S分配 PodCIDRs: 10.244.1.0/24 Non-terminated Pods: (17 in total) [root@k8s-master ~]# kubectl describe node k8s-node1 System Info: Machine ID: 32599e2a74704b2e95443e24ea15d4f6 System UUID: 34979a62-16de-4287-b149-2d4c2d8a70fb Boot ID: f31de60e-4f89-4553-ba7a-99a46d049936 Kernel Version: 5.4.109-1.el7.elrepo.x86_64 OS Image: CentOS Linux 7 (Core) Operating System: linux Architecture: amd64 Container Runtime Version: docker://20.10.7 Kubelet Version: v1.19.9 Kube-Proxy Version: v1.19.9 PodCIDR: 10.244.1.0/24 #每个Node Pod都是由K8S分配IP PodCIDRs: 10.244.1.0/24 [root@k8s-master Network]# vim calico.yaml ... \u0026#34;ipam\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;host-local\u0026#34;, \u0026#34;subnet\u0026#34;: \u0026#34;usePodCidr\u0026#34; #使用k8s ipam插件分配地址 }, \u0026#34;policy\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;k8s\u0026#34; }, ... - name: FELIX_WIREGUARDMTU valueFrom: configMapKeyRef: name: calico-config key: veth_mtu # The default IPv4 pool to create on startup if none exists. Pod IPs will be # chosen from this range. Changing this value after installation will have # no effect. This should fall within `--cluster-cidr`. - name: CALICO_IPV4POOL_CIDR value: \u0026#34;10.244.0.0/16\u0026#34; #为了和之前的flannel 10.244.0.0/16适配 - name: CALICO_IPV4POOL_BLOCK_SIZE #添加这一行修改默认块大小 value: \u0026#34;24\u0026#34; - name: USE_POD_CIDR #使用K8S的分配的IP地址，不然calico和K8S分配的地址会不一样 value: \u0026#34;true\u0026#34; 安装calico [root@k8s-master plugin]# kubectl delete -f kube-flannel.yml podsecuritypolicy.policy \u0026#34;psp.flannel.unprivileged\u0026#34; deleted clusterrole.rbac.authorization.k8s.io \u0026#34;flannel\u0026#34; deleted clusterrolebinding.rbac.authorization.k8s.io \u0026#34;flannel\u0026#34; deleted serviceaccount \u0026#34;flannel\u0026#34; deleted configmap \u0026#34;kube-flannel-cfg\u0026#34; deleted daemonset.apps \u0026#34;kube-flannel-ds\u0026#34; deleted [root@k8s-master plugin]# kubectl apply -f calico.yaml configmap/calico-config created customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created clusterrole.rbac.authorization.k8s.io/calico-node created clusterrolebinding.rbac.authorization.k8s.io/calico-node created daemonset.apps/calico-node created serviceaccount/calico-node created deployment.apps/calico-kube-controllers created serviceaccount/calico-kube-controllers created poddisruptionbudget.policy/calico-kube-controllers created calico几个组件 [root@k8s-master ~]# ps aux|grep calico root 10867 0.0 0.1 112816 2156 pts/1 S+ 13:51 0:00 grep --color=auto calico root 20680 0.0 2.3 1215184 35216 ? Sl 10:27 0:06 calico-node -allocate-tunnel-addrs root 20681 0.0 2.1 1215184 32672 ? Sl 10:27 0:06 calico-node -monitor-addresses root 20682 2.4 3.3 1510624 51636 ? Sl 10:27 4:54 calico-node -felix root 20683 0.0 2.3 1657832 35496 ? Sl 10:27 0:09 calico-node -confd root 20686 0.0 2.0 1214928 31628 ? Sl 10:27 0:05 calico-node -monitor-token 因为calico并没有使用k8s的ipam分配IP,所以节点会有2个IP,一个是K8S分配的IP 一个是calico分配的IP Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 192.168.54.2 0.0.0.0 UG 101 0 0 eth4 172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0 192.168.4.0 0.0.0.0 255.255.255.0 U 100 0 0 eth0 192.168.12.0 192.168.4.172 255.255.255.0 UG 0 0 0 tunl0 #可以看到tunl0的路由信息 192.168.51.0 192.168.4.173 255.255.255.0 UG 0 0 0 tunl0 #同时可以看到节点的IP不像之前一定是连续的 192.168.54.0 0.0.0.0 255.255.255.0 U 101 0 0 eth4 192.168.113.0 192.168.4.171 255.255.255.0 UG 0 0 0 tunl0 #隧道接口 192.168.237.0 0.0.0.0 255.255.255.0 U 0 0 0 * 192.168.237.1 0.0.0.0 255.255.255.255 UH 0 0 0 cali7c0fb624285 192.168.237.2 0.0.0.0 255.255.255.255 UH 0 0 0 caliedaf285d4ef 192.168.237.3 0.0.0.0 255.255.255.255 UH 0 0 0 cali854da94d42a [root@k8s-master calico]# ip route list default via 192.168.54.2 dev eth4 proto static metric 101 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown 192.168.4.0/24 dev eth0 proto kernel scope link src 192.168.4.170 metric 100 192.168.12.0/24 via 192.168.4.172 dev tunl0 proto bird onlink #可以看到tunl0的路由信息 192.168.51.0/24 via 192.168.4.173 dev tunl0 proto bird onlink 192.168.54.0/24 dev eth4 proto kernel scope link src 192.168.54.170 metric 101 192.168.113.0/24 via 192.168.4.171 dev tunl0 proto bird onlink blackhole 192.168.237.0/24 proto bird 192.168.237.1 dev cali7c0fb624285 scope link 192.168.237.2 dev caliedaf285d4ef scope link 192.168.237.3 dev cali854da94d42a scope link 192.168.51.0/24 via 192.168.4.173 dev tunl0 proto bird onlink 下面的路由表可以看到 calico会为每个节点分配网络地址段 并不是使用节点的网络地址 [root@k8s-node1 ~]# ip route list default via 192.168.54.2 dev eth4 proto static metric 101 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown 192.168.4.0/24 dev eth0 proto kernel scope link src 192.168.4.171 metric 100 192.168.12.0/24 via 192.168.4.172 dev tunl0 proto bird onlink 192.168.51.0/24 via 192.168.4.173 dev tunl0 proto bird onlink 192.168.54.0/24 dev eth4 proto kernel scope link src 192.168.54.171 metric 101 blackhole 192.168.113.0/24 proto bird #黑洞 代表自己网段 192.168.237.0/24 via 192.168.4.170 dev tunl0 proto bird onlink 查看目前工作模式 [root@k8s-master calico]# kubectl api-resources NAME SHORTNAMES APIGROUP NAMESPACED KIND bgpconfigurations crd.projectcalico.org false BGPConfiguration bgppeers crd.projectcalico.org false BGPPeer blockaffinities crd.projectcalico.org false BlockAffinity clusterinformations crd.projectcalico.org false ClusterInformation felixconfigurations crd.projectcalico.org false FelixConfiguration globalnetworkpolicies crd.projectcalico.org false GlobalNetworkPolicy globalnetworksets crd.projectcalico.org false GlobalNetworkSet hostendpoints crd.projectcalico.org false HostEndpoint ipamblocks crd.projectcalico.org false IPAMBlock ipamconfigs crd.projectcalico.org false IPAMConfig ipamhandles crd.projectcalico.org false IPAMHandle ippools crd.projectcalico.org false IPPool #calico地址池 kubecontrollersconfigurations crd.projectcalico.org false KubeControllersConfiguration networkpolicies crd.projectcalico.org true NetworkPolicy networksets crd.projectcalico.org true NetworkSet [root@k8s-master calico]# kubectl get ippools -o yaml .... spec: blockSize: 24 #掩码长度 cidr: 192.168.0.0/16 #地址池 ipipMode: Always #可以看到目前为ipip模式 natOutgoing: true nodeSelector: all() 访问抓包 [root@k8s-master PodControl]# kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES deployment-demo-fb544c5d8-r7pc8 1/1 Running 0 8m3s 192.168.51.1 k8s-node3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; deployment-demo-fb544c5d8-splfr 1/1 Running 0 8m3s 192.168.12.1 k8s-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; [root@k8s-master PodControl]# kubectl exec deployment-demo-fb544c5d8-r7pc8 -it -- /bin/sh [root@deployment-demo-fb544c5d8-r7pc8 /]# ifconfig eth0 Link encap:Ethernet HWaddr 16:96:97:3F:F3:C5 inet addr:192.168.51.1 Bcast:192.168.51.1 Mask:255.255.255.255 UP BROADCAST RUNNING MULTICAST MTU:1480 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B) lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B) [root@deployment-demo-fb544c5d8-r7pc8 /]# curl 192.168.12.1 iKubernetes demoapp v1.0 !! ClientIP: 192.168.51.1, ServerName: deployment-demo-fb544c5d8-splfr, ServerIP: 192.168.12.1! [root@deployment-demo-fb544c5d8-r7pc8 /]# curl 192.168.12.1 iKubernetes demoapp v1.0 !! ClientIP: 192.168.51.1, ServerName: deployment-demo-fb544c5d8-splfr, ServerIP: 192.168.12.1! [root@deployment-demo-fb544c5d8-r7pc8 /]# curl 192.168.12.1 [root@k8s-node2 ~]# tcpdump -i eth0 -nn ip host 192.168.4.172 and host 192.168.4.173 tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes 11:48:24.421003 IP 192.168.4.173 \u0026gt; 192.168.4.172: IP 192.168.51.1.33436 \u0026gt; 192.168.12.1.80: Flags [S], seq 3259804851, win 64800, options [mss 1440,sackOK,TS val 2008488248 ecr 0,nop,wscale 7], length 0 (ipip-proto-4) 11:48:24.421093 IP 192.168.4.172 \u0026gt; 192.168.4.173: IP 192.168.12.1.80 \u0026gt; 192.168.51.1.33436: Flags [S.], seq 3234480084, ack 3259804852, win 64260, options [mss 1440,sackOK,TS val 1053230437 ecr 2008488248,nop,wscale 7], length 0 (ipip-proto-4) #可以看到(ipip-proto-4)为IPIP模式 11:48:24.422305 IP 192.168.4.173 \u0026gt; 192.168.4.172: IP 192.168.51.1.33436 \u0026gt; 192.168.12.1.80: Flags [.], ack 1, win 507, options [nop,nop,TS val 2008488250 ecr 1053230437], length 0 (ipip-proto-4) 11:48:24.422308 IP 192.168.4.173 \u0026gt; 192.168.4.172: IP 192.168.51.1.33436 \u0026gt; 192.168.12.1.80: Flags [P.], seq 1:77, ack 1, win 507, options [nop,nop,TS val 2008488250 ecr 1053230437], length 76: HTTP: GET / HTTP/1.1 (ipip-proto-4) 11:48:24.422554 IP 192.168.4.172 \u0026gt; 192.168.4.173: IP 192.168.12.1.80 \u0026gt; 192.168.51.1.33436: Flags [.], ack 77, win 502, options [nop,nop,TS val 1053230439 ecr 2008488250], length 0 (ipip-proto-4) 11:48:24.431688 IP 192.168.4.172 \u0026gt; 192.168.4.173: IP 192.168.12.1.80 \u0026gt; 192.168.51.1.33436: Flags [P.], seq 1:18, ack 77, win 502, options [nop,nop,TS val 1053230447 ecr 2008488250], length 17: HTTP: HTTP/1.0 200 OK (ipip-proto-4) 11:48:24.432638 IP 192.168.4.172 \u0026gt; 192.168.4.173: IP 192.168.12.1.80 \u0026gt; 192.168.51.1.33436: Flags [FP.], seq 18:276, ack 77, win 502, options [nop,nop,TS val 1053230449 ecr 2008488250], length 258: HTTP (ipip-proto-4) 11:48:24.433660 IP 192.168.4.173 \u0026gt; 192.168.4.172: IP 192.168.51.1.33436 \u0026gt; 192.168.12.1.80: Flags [.], ack 18, win 507, options [nop,nop,TS val 2008488261 ecr 1053230447], length 0 (ipip-proto-4) 11:48:24.437531 IP 192.168.4.173 \u0026gt; 192.168.4.172: IP 192.168.51.1.33436 \u0026gt; 192.168.12.1.80: Flags [F.], seq 77, ack 277, win 505, options [nop,nop,TS val 2008488261 ecr 1053230449], length 0 (ipip-proto-4) 11:48:24.437775 IP 192.168.4.172 \u0026gt; 192.168.4.173: IP 192.168.12.1.80 \u0026gt; 192.168.51.1.33436: Flags [.], ack 78, win 502, options [nop,nop,TS val 1053230454 ecr 2008488261], length 0 (ipip-proto-4) IP 192.168.4.172 \u0026gt; 192.168.4.173: IP 192.168.12.1.80 \u0026gt; 192.168.51.1.33436 可以看到默认为ipip模式 也是经过封装在转发的 和Flannel很类似,但相对Flannel经过虚拟网桥CNI calico直接内核(内核的路由由 kube-proxy或IPVS生成)到在由tunl0传输想对Flannel少了一层交换机交换的过程,性能相比Flannel会快一些 但这并不是calico最佳的模式 calicoctl命令安装与使用 # calicoctl安装的2种方式 第1种方式 calicoctl\nhttps://docs.projectcalico.org/getting-started/clis/calicoctl/install\n几种方式运行calicoctl 常用方式1:直接下载2进制calicoctl 直接运行 [root@k8s-master ~]# curl -o calicoctl -O -L \u0026#34;https://github.com/projectcalico/calicoctl/releases/download/v3.20.0/calicoctl\u0026#34; % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 615 100 615 0 0 498 0 0:00:01 0:00:01 --:--:-- 504 100 43.2M 100 43.2M 0 0 518k 0 0:01:25 0:01:25 --:--:-- 920k [root@k8s-master ~]# mv calicoctl /usr/bin/ [root@k8s-master ~]# chmod +x /usr/bin/calicoctl [root@k8s-master ~]# calicoctl --help Usage: calicoctl [options] \u0026lt;command\u0026gt; [\u0026lt;args\u0026gt;...] create Create a resource by file, directory or stdin. replace Replace a resource by file, directory or stdin. apply Apply a resource by file, directory or stdin. This creates a resource if it does not exist, and replaces a resource if it does exists. patch Patch a pre-exisiting resource in place. delete Delete a resource identified by file, directory, stdin or resource type and name. get Get a resource identified by file, directory, stdin or resource type and name. label Add or update labels of resources. convert Convert config files between different API versions. ipam IP address management. node Calico node management. version Display the version of this binary. export Export the Calico datastore objects for migration import Import the Calico datastore objects for migration datastore Calico datastore management. calicoctl 命令使用 - calicoctl 默认会读取 ~/.kube/下文件加载认证信息,也可以通过配置文件指定认证信息位置 [root@k8s-master calico]# mkdir /etc/calico/^C [root@k8s-master calico]# cd /etc/calico/ [root@k8s-master calico]# cat calicoctl.cfg apiVersion: projectcalico.org/v3 kind: CalicoAPIConfig metadata: spec: datastoreType: \u0026#34;kubernetes\u0026#34; kubeconfig: \u0026#34;/etc/kubernetes/admin.conf\u0026#34; #指定conf路径 [root@k8s-master calico]# [root@k8s-master calico]# kubectl get ippools NAME AGE default-ipv4-ippool 23h [root@k8s-master calico]# calicoctl get ippool #可以用calicoctl直接访问 calico资源 NAME CIDR SELECTOR default-ipv4-ippool 192.168.0.0/16 all() [root@k8s-master calico]# calicoctl get ippool default-ipv4-ippool -o yaml apiVersion: projectcalico.org/v3 kind: IPPool metadata: creationTimestamp: \u0026#34;2021-08-29T14:33:53Z\u0026#34; name: default-ipv4-ippool resourceVersion: \u0026#34;1305\u0026#34; uid: c01d73f3-c0c9-4674-b27e-725a1eaa5717 spec: blockSize: 24 cidr: 192.168.0.0/16 ipipMode: Always natOutgoing: true nodeSelector: all() vxlanMode: Never [root@k8s-master calico]# calicoctl ipam --help Usage: calicoctl [options] [\u0026lt;args\u0026gt;...] Options: -h --help Show this screen. -c --config=\u0026lt;config\u0026gt; Path to the file containing connection configuration in YAML or JSON format. [default: /etc/calico/calicoctl.cfg] --context=\u0026lt;context\u0026gt; The name of the kubeconfig context to use. -a -A --all-namespaces --as=\u0026lt;AS_NUM\u0026gt; --backend=(bird|gobgp|none) --dryrun --export --felix-config=\u0026lt;CONFIG\u0026gt; -f --filename=\u0026lt;FILENAME\u0026gt; --force --from-report=\u0026lt;REPORT\u0026gt; --ignore-validation --init-system --ip6-autodetection-method=\u0026lt;IP6_AUTODETECTION_METHOD\u0026gt; --ip6=\u0026lt;IP6\u0026gt; --ip-autodetection-method=\u0026lt;IP_AUTODETECTION_METHOD\u0026gt; ... [root@k8s-master calico]# calicoctl ipam show +----------+----------------+-----------+------------+--------------+ | GROUPING | CIDR | IPS TOTAL | IPS IN USE | IPS FREE | +----------+----------------+-----------+------------+--------------+ | IP Pool | 192.168.0.0/16 | 65536 | 9 (0%) | 65527 (100%) | +----------+----------------+-----------+------------+--------------+ [root@k8s-master calico]# calicoctl ipam show --show-blocks #每个地址段使用了多少个 +----------+------------------+-----------+------------+--------------+ | GROUPING | CIDR | IPS TOTAL | IPS IN USE | IPS FREE | +----------+------------------+-----------+------------+--------------+ | IP Pool | 192.168.0.0/16 | 65536 | 9 (0%) | 65527 (100%) | | Block | 192.168.113.0/24 | 256 | 1 (0%) | 255 (100%) | | Block | 192.168.12.0/24 | 256 | 2 (1%) | 254 (99%) | | Block | 192.168.237.0/24 | 256 | 4 (2%) | 252 (98%) | | Block | 192.168.51.0/24 | 256 | 2 (1%) | 254 (99%) | +----------+------------------+-----------+------------+--------------+ [root@k8s-master calico]# calicoctl ipam show --show-config #查看配置信息 +--------------------+-------+ | PROPERTY | VALUE | +--------------------+-------+ | StrictAffinity | false | | AutoAllocateBlocks | true | | MaxBlocksPerHost | 0 | +--------------------+-------+ 第2种方式 以kubectl插件方式运行 # [root@k8s-master calico]# cp -p /usr/bin/calicoctl /usr/bin/kubectl-calico #把之前的文件改个名字就可以了 [root@k8s-master calico]# kubectl calico Usage: kubectl-calico [options] \u0026lt;command\u0026gt; [\u0026lt;args\u0026gt;...] Invalid option: \u0026#39;\u0026#39;. Use flag \u0026#39;--help\u0026#39; to read about a specific subcommand [root@k8s-master calico]# kubectl calico get nodes #和第1种方式相比加kubectl NAME k8s-master k8s-node1 k8s-node2 k8s-node3 [root@k8s-master calico]# kubectl calico ipam show +----------+----------------+-----------+------------+--------------+ | GROUPING | CIDR | IPS TOTAL | IPS IN USE | IPS FREE | +----------+----------------+-----------+------------+--------------+ | IP Pool | 192.168.0.0/16 | 65536 | 9 (0%) | 65527 (100%) | +----------+----------------+-----------+------------+--------------+ 示例2:修改BGP网络 # #获取现有配置在此基础上修改 [root@k8s-master calico]# kubectl calico get ippool -o yaml \u0026gt; default-ipv4-ippool.yaml [root@k8s-master calico]# cat default-ipv4-ippool.yaml apiVersion: projectcalico.org/v3 kind: IPPool metadata: name: default-ipv4-ippool spec: blockSize: 24 cidr: 192.168.0.0/16 ipipMode: CrossSubnet #跨节点子网时使用IPIP 没有跨子网使用BGP natOutgoing: true nodeSelector: all() vxlanMode: Never #vxlanMode与ipipMode不能同时打开 必须有一个为Never #通过ipipMode、vxlanMode不同选项可以使calico运行在纯GBP、ipip、vxlanMode或混合模式下 #如:ipipMode: Never vxlanMode: Never 为纯BGP模式 ipipMode: Never vxlanMode: CrossSubnet 为BGP+vxlan模式 [root@k8s-master calico]# calicoctl apply -f default-ipv4-ippool.yaml Successfully applied 1 \u0026#39;IPPool\u0026#39; resource(s) #在来看路由信息 已经没有之前的tunl0 直接从节点网络出去 [root@k8s-master calico]# ip route list default via 192.168.54.2 dev eth4 proto static metric 101 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown 192.168.4.0/24 dev eth0 proto kernel scope link src 192.168.4.170 metric 100 192.168.12.0/24 via 192.168.4.172 dev eth0 proto bird 192.168.51.0/24 via 192.168.4.173 dev eth0 proto bird #已经没有之前的tunl0隧道 192.168.54.0/24 dev eth4 proto kernel scope link src 192.168.54.170 metric 101 192.168.113.0/24 via 192.168.4.171 dev eth0 proto bird blackhole 192.168.237.0/24 proto bird 192.168.237.1 dev cali7c0fb624285 scope link 192.168.237.2 dev caliedaf285d4ef scope link 192.168.237.3 dev cali854da94d42a scope link [root@k8s-master calico]# 抓包测试\n[root@k8s-master ~]# kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES deployment-demo-fb544c5d8-r7pc8 1/1 Running 0 10h 192.168.51.1 k8s-node3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; deployment-demo-fb544c5d8-splfr 1/1 Running 0 10h 192.168.12.1 k8s-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; #从节点3访问节点3 [root@k8s-master calico]# kubectl exec deployment-demo-fb544c5d8-r7pc8 -it -- /bin/sh [root@deployment-demo-fb544c5d8-r7pc8 /]# curl 192.168.12.1 iKubernetes demoapp v1.0 !! ClientIP: 192.168.51.1, ServerName: deployment-demo-fb544c5d8-splfr, ServerIP: 192.168.12.1! [root@deployment-demo-fb544c5d8-r7pc8 /]# curl 192.168.12.1 iKubernetes demoapp v1.0 !! ClientIP: 192.168.51.1, ServerName: deployment-demo-fb544c5d8-splfr, ServerIP: 192.168.12.1! #直接抓Pod IP的包 因为没有封装 所以是Pod IP直接通信 没有外层IP [root@k8s-node2 ~]# tcpdump -i eth0 -nn ip host 192.168.51.1 and host 192.168.12.1 tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes 22:11:54.704770 IP 192.168.51.1.33464 \u0026gt; 192.168.12.1.80: Flags [S], seq 4075444778, win 64800, options [mss 1440,sackOK,TS val 2045898534 ecr 0,nop,wscale 7], length 0 22:11:54.705866 IP 192.168.12.1.80 \u0026gt; 192.168.51.1.33464: Flags [S.], seq 402120893, ack 4075444779, win 64260, options [mss 1440,sackOK,TS val 1090640722 ecr 2045898534,nop,wscale 7], length 0 22:11:54.706670 IP 192.168.51.1.33464 \u0026gt; 192.168.12.1.80: Flags [.], ack 1, win 507, options [nop,nop,TS val 2045898537 ecr 1090640722], length 0 22:11:54.707077 IP 192.168.51.1.33464 \u0026gt; 192.168.12.1.80: Flags [P.], seq 1:77, ack 1, win 507, options [nop,nop,TS val 2045898537 ecr 1090640722], length 76: HTTP: GET / HTTP/1.1 22:11:54.707132 IP 192.168.12.1.80 \u0026gt; 192.168.51.1.33464: Flags [.], ack 77, win 502, options [nop,nop,TS val 1090640723 ecr 2045898537], length 0 22:11:54.737231 IP 192.168.12.1.80 \u0026gt; 192.168.51.1.33464: Flags [P.], seq 1:18, ack 77, win 502, options [nop,nop,TS val 1090640754 ecr 2045898537], length 17: HTTP: HTTP/1.0 200 OK 22:11:54.738439 IP 192.168.51.1.33464 \u0026gt; 192.168.12.1.80: Flags [.], ack 18, win 507, options [nop,nop,TS val 2045898568 ecr 1090640754], length 0 22:11:54.739117 IP 192.168.12.1.80 \u0026gt; 192.168.51.1.33464: Flags [P.], seq 18:155, ack 77, win 502, options [nop,nop,TS val 1090640755 ecr 2045898568], length 137: HTTP 22:11:54.739630 IP 192.168.12.1.80 \u0026gt; 192.168.51.1.33464: Flags [FP.], seq 155:276, ack 77, win 502, options [nop,nop,TS val 1090640756 ecr 2045898568], length 121: HTTP 22:11:54.739810 IP 192.168.51.1.33464 \u0026gt; 192.168.12.1.80: Flags [.], ack 155, win 506, options [nop,nop,TS val 2045898570 ecr 1090640755], length 0 [root@k8s-master calico]# calicoctl node status Calico process is running. IPv4 BGP status #可以看到已经BGP模式了 这里看到是除去自己其它的3个节点 +---------------+-------------------+-------+----------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +---------------+-------------------+-------+----------+-------------+ | 192.168.4.171 | node-to-node mesh | up | 02:27:59 | Established | | 192.168.4.172 | node-to-node mesh | up | 02:27:58 | Established | | 192.168.4.173 | node-to-node mesh | up | 02:27:58 | Established | +---------------+-------------------+-------+----------+-------------+ IPv6 BGP status No IPv6 peers found. 到目前为止 如果是小规模的集群 比如50台以下 就可以直接使用了 如果是大规模集群 部署reflector路由反射器，避免过多的路由表更新 减轻AIP-SERVER压力 #把maseter配置成reflector节点 [root@k8s-master calico]# cat reflector-node.yaml apiVersion: projectcalico.org/v3 kind: Node metadata: labels: route-reflector: true name: k8s-master #节点名 spec: bgp: ipv4Address: 192.168.4.170/24 #Master IP ipv4IPIPTunnelAddr: 192.168.237.0 #tunl0网络地址 routeReflectorClusterID: 1.1.1.1 #ID信息 如果有多个node 不能和其它重复就行 [root@k8s-master calico]# calicoctl apply -f reflector-node.yaml Successfully applied 1 \u0026#39;Node\u0026#39; resource(s) 配置所有节点与reflector节点通信 [root@k8s-master calico]# cat bgppeer-demo.yaml kind: BGPPeer apiVersion: projectcalico.org/v3 metadata: name: bgppeer-demo spec: nodeSelector: all() #所有节点 peerSelector: route-reflector==\u0026#34;true\u0026#34; #与有这个标签的节点通信 [root@k8s-master calico]# calicoctl apply -f bgppeer-demo.yaml Successfully applied 1 \u0026#39;BGPPeer\u0026#39; resource(s) [root@k8s-master calico]# calicoctl node status Calico process is running. IPv4 BGP status +---------------+-------------------+-------+----------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +---------------+-------------------+-------+----------+-------------+ | 192.168.4.171 | node-to-node mesh | up | 02:27:59 | Established |#之前的mesh工作模式还在 | 192.168.4.172 | node-to-node mesh | up | 02:27:58 | Established | | 192.168.4.173 | node-to-node mesh | up | 02:27:58 | Established | | 192.168.4.171 | node specific | start | 14:36:40 | Idle |#基于reflector工作模式 | 192.168.4.172 | node specific | start | 14:36:40 | Idle | | 192.168.4.173 | node specific | start | 14:36:40 | Idle | +---------------+-------------------+-------+----------+-------------+ IPv6 BGP status #关掉mesh 点对点的工作模式 [root@k8s-master calico]# cat default-bgpconfiguration.yaml apiVersion: projectcalico.org/v3 kind: BGPConfiguration metadata: name: default spec: logSeverityScreen: Info nodeToNodeMeshEnabled: false #是赤允许点对点通信 asNumber : 63400 [root@k8s-master calico]# calicoctl apply -f default-bgpconfiguration.yaml Successfully applied 1 \u0026#39;BGPConfiguration\u0026#39; resource(s) [root@k8s-master calico]# calicoctl node status Calico process is running. IPv4 BGP status +---------------+---------------+-------+----------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +---------------+---------------+-------+----------+-------------+ | 192.168.4.171 | node specific | up | 14:45:26 | Established | | 192.168.4.172 | node specific | up | 14:45:26 | Established | | 192.168.4.173 | node specific | up | 14:45:26 | Established | +---------------+---------------+-------+----------+-------------+ IPv6 BGP status No IPv6 peers found. 概述 # Calico 是一种开源网络和网络安全解决方案，适用于容器，虚拟机和基于主机的本机工作负载。Calico 支持广泛的平台，包括 Kubernetes，docker，OpenStack 和裸机服务。Calico 后端支持多种网络模式。\nBGP 模式：将节点做为虚拟路由器通过 BGP 路由协议来实现集群内容器之间的网络访问。 IPIP 模式：在原有 IP 报文中封装一个新的 IP 报文，新的 IP 报文中将源地址 IP 和目的地址 IP 都修改为对端宿主机 IP。 cross-subnet：Calico-ipip 模式和 calico-bgp 模式都有对应的局限性，对于一些主机跨子网而又无法使网络设备使用 BGP 的场景可以使用 cross-subnet 模式，实现同子网机器使用 calico-BGP 模式，跨子网机器使用 calico-ipip 模式。 calico 切换 BGP 模式 # 部署完成后默认使用 calico-ipip 的模式，通过在节点的路由即可得知，通往其他节点路由通过 tunl0 网卡出去\n修改为 BGP 网络模式，在 system 项目中修改 calico-node daemonset\n修改CALICO_IPV4POOL_IPIP改为 off，添加新环境变量FELIX_IPINIPENABLED为 false\n修改完成后对节点进行重启，等待恢复后查看主机路由，与 ipip 最大区别在于去往其他节点的路由，由 Tunnel0 走向网络网卡。\ncalico 切换 cross-subnet 模式 # Calico-ipip 模式和 calico-bgp 模式都有对应的局限性，对于一些主机跨子网而又无法使网络设备使用 BGP 的场景可以使用 cross-subnet 模式，实现同子网机器使用 calico-BGP 模式，跨子网机器使用 calico-ipip 模式。\n部署集群网络选择 calico 网络插件\n默认部署出来是 calico 的 ip-in-ip 的模式 查看宿主机网卡，会发现多了个 tunl0 网卡，这个是建立 ip 隧道的网卡\n去其他主机的路由都是走 tunl0 网卡出去\n切换到 cross-subnet 模式\nkubectleditipPool/default-ipv4-ippool 将 ipipMode 改为 crossSubnet\n在 UI 将 calico-node 的 POD 删了重建\n重启检查 calico 网络\n可以看见同子网的主机出口走的是 bgp，不同子网主机走的是 tunl0 网卡走 ipip 模式\n创建应用测试跨主机网络，在不同主机上互相 ping 测试，看看跨主机网络是否正常。\n配置 Route reflector # 安装 calicoctl # 安装方式有以下几种\nSingle host 上面 binary 安装 Single host 上面 continer 安装 作为 k8s pod 运行 实际经验：\nBinary 方式在集群里面的一台 worker 节点安装（比如 RR），calicoctl 会检测 bird/felix 的运行状态。在非 calico node 节点运行只能使用部分命令，不能运行 calico node 相关命令。\n通过配置 calicoctl 来对 calico 进行控制，通常情况下建议将\ncurl-O-Lhttps://github.com/projectcalico/calicoctl/releases/download/v3.13.3/calicoctl 配置可执行权限\nchmod+xcalicoctl 复制的/usr/bin/目录\ncpcalicoctl/usr/bin/ 配置 calicoctl 连接 Kubernetes 集群\nexportCALICO_DATASTORE_TYPE=kubernetes exportCALICO_KUBECONFIG=~/.kube/config calicoctlnodestatus calico node-to-node mesh # 默认情况下 calico 采用 node-to-node mesh 方式 ，为了防止 BGP 路由环路，BGP 协议规定在一个 AS（自治系统）内部，IBGP 路由器之间只能传一跳路由信息，所以在一个 AS 内部，IBGP 路由器之间为了学习路由信息需要建立全互联的对等体关系，但是当一个 AS 规模很大的时候，这种全互联的对等体关系维护会大量消耗网络和 CPU 资源，所以这种情况下就需要建立路由反射器以减少 IBGP 路由器之间的对等体关系数量。\nRoute reflector 角色介绍 # 早期 calico 版本提供专门的 route reflector 镜像，在新版本 calico node 内置集成 route reflector 功能。Route reflector 可以是以下角色：\n集群内部的 node 节点 集群外部节点运行 calico node 其他支持 route reflector 的软件或者设备。 这里以一个集群内部的 node 节点为例：\n关闭 node-to-node mesh # cat\u0026lt;\u0026lt;EOF|calicoctlapply-f- apiVersion:projectcalico.org/v3 kind:BGPConfiguration metadata: name:default spec: logSeverityScreen:Info nodeToNodeMeshEnabled:false asNumber:63400 EOF 设置 Route reflector # 配置 Route reflector 支持多种配置方式如：1、支持配置全局 BGP peer，。2、支持针对单个节点进行配置 BGP Peer。也可以将 calico 节点充当 Route reflector 这里以配置 calico 节点充当 Router reflector 为例。\n配置节点充当 BGP Route Reflector\n可将 Calico 节点配置为充当路由反射器。为此，要用作路由反射器的每个节点必须具有群集 ID-通常是未使用的 IPv4 地址。\n要将节点配置为集群 ID 为 244.0.0.1 的路由反射器，请运行以下命令。这里将节点名为 rke-node4 的节点配置为 Route Reflector，若一个集群中要配置主备 rr，为了防止 rr 之间的路由环路，需要将集群 ID 配置成一样\ncalicoctlpatchnoderke-node4-p\u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;bgp\u0026#34;:{\u0026#34;routeReflectorClusterID\u0026#34;:\u0026#34;244.0.0.1\u0026#34;}}}\u0026#39; 给节点打上对应的 label 标记该节点以表明它是 Route Reflector，从而允许 BGPPeer 资源选择它。\nkubectllabelnoderke-node4route-reflector=true 创建 BGPPeer\nexportCALICO_DATASTORE_TYPE=kubernetes exportCALICO_KUBECONFIG=~/.kube/config cat\u0026lt;\u0026lt;EOF|calicoctlapply-f- kind:BGPPeer apiVersion:projectcalico.org/v3 metadata: name:peer-with-route-reflectors spec: nodeSelector:all() peerSelector:route-reflector==\u0026#39;true\u0026#39; EOF 查看 BGP 节点状态 # node 上查看，peer type 由 node-to-node mesh 变为 node specific\nRoute Reflector 上节点查看，节点已正常建立连接\n设置 veth 网卡 mtu # 通常，通过使用最高 MTU 值（不会在路径上引起碎片或丢包）来实现最高性能。对于给定的流量速率，最大带宽增加，CPU 消耗可能下降。对于一些支持 jumbo frames 的网络设备，可以配置 calico 支持使用。\n下表列举了，常见几种 MTU 配置下 calico 对应的网卡 mtu 的配置\nIPIP 和 VXLAN 协议中的 IP 中使用的额外报文头，通过头的大小减小了最小 MTU。（IP 中的 IP 使用 20 字节的标头，而 VXLAN 使用 50 字节的标头）。\n如果在 Pod 网络中的任何地方使用 VXLAN，请将 MTU 大小配置为“物理网络 MTU 大小减去 50”。如果仅在 IP 中使用 IP，则将 MTU 大小配置为“物理网络 MTU 大小减去 20” 。\n将工作负载端点 MTU 和隧道 MTU 设置为相同的值 # 配置方法：\n升级集群\n配置网卡 MTU，此时通过 system 项目下 calico-config 文件可以看见对应的 mtu 设置\n创建 workload 查看 POD 网卡 MTU 为 9001\n设置全局 AS 号 # 默认情况下，除非已为节点指定每个节点的 AS，否则所有 Calico 节点都使用 64512 自治系统。可以通过修改默认的 BGPConfiguration 资源来更改所有节点的全局默认值。以下示例命令将全局默认 AS 编号设置为 64513。\ncat\u0026lt;\u0026lt;EOF|calicoctlapply-f- apiVersion:projectcalico.org/v3 kind:BGPConfiguration metadata: name:default spec: logSeverityScreen:Info nodeToNodeMeshEnabled:false asNumber:64513 EOF 设置单个主机和 AS 号 # 例如，以下命令将名为 node-1 的节点更改为属于 AS 64514。\ncalicoctlpatchnodenode-1-p\u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;bgp\u0026#34;:{“asNumber”:“64514”}}}\u0026#39; 修改节点地址范围 # 此操作建议在部署完集群后立刻进行。\n默认情况下 calico 在集群层面分配一个 10.42.0.0/16 的 CIDR 网段，在这基础上在单独为每个主机划分一个单独子网采用 26 位子网掩码对应的集群支持的节点数为 2^10=1024 节点，单个子网最大支持 64 个 POD，当单个子网对应 IP 消耗后，calico 会重新在本机上划分一个新的子网如下，在集群对端主机可以看见对应的多个 CIDR 路由信息。\n注意：块大小将影响节点 POD 的 IP 地址分配和路由条目数量，如果主机在一个 CIDR 中分配所有地址，则将为其分配一个附加 CIDR。如果没有更多可用的块，则主机可以从分配给其他主机的 CIDR 中获取地址。为借用的地址添加了特定的路由，这会影响路由表的大小。\n将块大小从默认值增加（例如，使用/24 则为每个块提供 256 个地址）意味着每个主机更少的块，会减少路由。但是对应的集群可容纳主机数也对应减少为 2^8。\n从默认值减小 CIDR 大小（例如，使用/28 为每个块提供 16 个地址）意味着每个主机有更多 CIDR，因此会有更多路由。\ncalico 允许用户修改对应的 IP 池和集群 CIDR\n创建和替换步骤\n注意：删除 Pod 时，应用程序会出现暂时不可用\n添加一个新的 IP 池。 注意：新 IP 池必须在同一群集 CIDR 中。 禁用旧的 IP 池（注意：禁用 IP 池只会阻止分配新的 IP 地址。它不会影响现有 POD 的联网） 从旧的 IP 池中删除 Pod。 验证新的 Pod 是否从新的 IP 池中获取地址。 删除旧的 IP 池。 定义 ippool 资源\napiVersion:projectcalico.org/v3 kind:IPPool metadata: name:my-ippool spec: blockSize:24 cidr:192.0.0.0/16 ipipMode:Always natOutgoing:true 修改对应的 blockSize 号\n创建新的\ncalicoctlapply-fpool.yaml 将旧的 ippool 禁用\ncalicoctlpatchippooldefault-ipv4-ippool-p\u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;disabled\u0026#34;:“true”}}\u0026#39; 创建 workload 测试\n根据节点标签定义对应的 ippool # Calico 能够进行配置，为不同拓扑指定 IP 地址池。例如可能希望某些机架、地区、或者区域能够从同一个 IP 池中获取地址。这对于降低路由数量或者配合防火墙策略的要求会很有帮助。\n给节点配置对应 label\nkubectllabelnodeskube-node-0rack=0 kubectllabelnodeskube-node-1rack=1 通过标签定义对应的节点 IPpool\ncalicoctlcreate-f-\u0026lt;\u0026lt;EOF apiVersion:projectcalico.org/v3 kind:IPPool metadata: name:rack-0-ippool spec: cidr:192.168.0.0/24 ipipMode:Always natOutgoing:true nodeSelector:rack==\u0026#34;0\u0026#34; EOF calicoctlcreate-f-\u0026lt;\u0026lt;EOF apiVersion:projectcalico.org/v3 kind:IPPool metadata: name:rack-1-ippool spec: cidr:192.168.1.0/24 ipipMode:Always natOutgoing:true nodeSelector:rack==\u0026#34;1\u0026#34; EOF 关闭 SNAT # 默认情况下，calico 访问集群外网络是通过 SNAT 成宿主机 ip 方式，在一些金融客户环境中为了能实现防火墙规则，需要直接针对 POD ip 进行进行规则配置，所以需要关闭 natOutgoing\nkubectleditippool/default-ipv4-ippool 将natOutgoing: true修改为natOutgoing: false\n此时，calico 网络访问集群外的 ip 源 ip 就不会 snat 成 宿主机的 ip 地址。\n固定 POD IP # 固定单个 ip\napiVersion:apps/v1 kind:Deployment metadata: name:nginx-test spec: selector: matchLabels: app:nginx replicas:1#tellsdeploymenttorun1podsmatchingthetemplate template: metadata: labels: app:nginx annotations: \u0026#34;cni.projectcalico.org/ipAddrs\u0026#34;:\u0026#34;[\\\u0026#34;10.42.210.135\\\u0026#34;]\u0026#34; spec: containers: -name:nginx image:nginx:1.7.9 ports: -containerPort:80 固定多个 ip，只能通过 ippool 的方式\ncatippool1.yaml apiVersion:projectcalico.org/v3 kind:IPPool metadata: name:pool-1 spec: blockSize:31 cidr:10.21.0.0/31 ipipMode:Never natOutgoing:true apiVersion:apps/v1 kind:Deployment metadata: name:nginx-test spec: selector: matchLabels: app:nginx replicas:1#tellsdeploymenttorun1podsmatchingthetemplate template: metadata: labels: app:nginx annotations: \u0026#34;cni.projectcalico.org/ipv4pools\u0026#34;:\u0026#34;[\\\u0026#34;pool-1\\\u0026#34;]\u0026#34; spec: containers: -name:nginx image:nginx:1.7.9 ports: -containerPort:80 /lib/modules /var/run/calico /var/lib/calico /run/xtables.lock /opt/cni/bin /etc/cni/net.d /var/log/calico/cni /var/lib/cni/networks /var/run/nodeagent /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds rm -rf /var/lib/cni rm -rf /etc/cni/net.d rm /var/lib/cni/ -rf \u0026amp;\u0026amp; rm -f /etc/cni/net.d/* systemctl restart docker containerd kubelet "},{"id":68,"href":"/docs/cicd%E6%80%9D%E8%80%83-cicd-si-kao/","title":"CICD思考 2024-04-03 15:11:24.151","section":"Docs","content":" 本文旨在介绍ZBJ DevOps团队倾力打造的DevOps平台中关于CI/CD流水线部分的实践。历经三次大版本迭代更新的流水线，完美切合ZBJ各种业务发展需求，在满足高频率交付的同时，提高了研发效率，降低了研发成本，保证了交付质量。 # 持续集成（Continuous Integration）简称CI，持续集成强调开发人员提交了新代码之后，立刻进行构建、（单元）测试。根据结果，我们可以确定新代码和原有代码能否正确地集成在一起。持续集成过程中很重视自动化测试验证结果，对可能出现的一些问题进行预警，以保障最终集成的代码没有问题。持续交付（Continuous Delivery）简称CD，持续交付在持续集成的基础上，将集成后的代码部署到更贴近真实运行环境的「类生产环境」（test，testing）中，然后交付给质量团队，以供评审。如果评审通过，代码就进入生产阶段。持续交付并不是指软件每一个改动都要尽快部署到产品环境中，它指的是任何的代码修改都可以在任何时候实施部署。有的人也把CD称为Continuous Deployment（持续部署），持续部署是指当交付的代码通过评审之后，可以部署到生产环境中。这里需要注意的是，持续部署应该是持续交付的最高阶段，持续交付是一种能力，持续部署是一种持续交付的表现方式。 # CI/CD过程示意图 # 背景介绍\n在提到ZBJ DevOps流水线之前，先交代一下历史背景。2015年前，猪八戒网80%的项目都是用PHP语言开发的，剩下的少部分使用的是Nodejs和Java。2015年，ZBJ研发中心进行了自发性的“工业革命”——腾云七号行动——使用Java语言将核心业务代码进行了重构和拆解，建立了以Dubbo为核心的SOA微服务框架，使用ZooKeeper+Swoole为核心的业务调用提供机制。满足新业务使用Java语言编写、老业务仍然使用PHP编写，同时支持两种语言（Nodejs\u0026amp;PHP）调用Dubbo服务的能力。 # 之后，开始全面推行前后端分离，于是流行了沿用至今的主流架构： # Nodejs：负责前端 # Java：负责后端 # PHP：负责老项目维护 # 剩余部分小系统或者边缘化的工具使用其他语言开发，或者在此三种语言基础上的一些变种： # 而随着项目工程数量的快速增长，交付开始变得频繁，传统开发模式的一两个月交付一次远远不能满足交付要求，改变迫在眉睫。 # 2016年Q3季度，感受到改变迫在眉睫，ZBJ研发中心经充分准备后决定抽调部分运维同学和开发同学组建一支名为“DevOps取经团”的团队，力图打造属于ZBJ自己的以提高研发效率为目标的平台。 # ZBJ CI/CD发展史\n第一阶段：2015年以前\n2015年以前，此时“工业革命”还未开始，ZBJ所谓的流水线先后经历了“大锅饭年代”、“公交车模式”。 # 可以看到，无论是“大锅饭年代”，还是“公交车模式”都会面临很多问题： # # 项目耦合度太高，容易导致合并冲突，环境冲突等。 # 集成过程中未对代码进行审查，错误代码发布到测试环境后，会影响依赖方的测试。 # 发布受限制，必须在专门时间由专人发布。 # 发布异常时，回滚工作异常艰难。 # 第二阶段（2016-2017）：私家车模式\n通过“工业革命”的变革，对系统的重构和拆解，同时引入工程责任制，给每个工程指定负责人，对工程的各种权限进行了管控，业务范围和边界变得更加清晰，使得项目耦合度太高的问题得到了很大的缓解。 # 解决了项目耦合度高的问题，接下来解决如何实现随时由开发团队自主发代码的问题。 # 用Jira做研发流程管理，制定针对ZBJ需求上线的流程： # 需求上线流程每个环节对应流水线的每个环境，只有到达某个环节，才能推送对应环境，每个环节会制定对应的准入准出，保证每走到的下一步都离成功部署更近一步。 # 推送环境示意图 # 每个环境制定不一样的执行任务，基本包括Jira状态校验，代码审查（单元测试），编译构建，上传包到制品库，拉取制品库中的包部署到对应环境。 # 值得注意的是，我们引入了Docker发布，所以我们的流水线是支持容器和虚拟机的混合发布的。虚拟机发布方式的制品是存放在一个叫做文件服务器的地方，容器发布方式的制品是push到Harbor仓库的。 # 容器发布\u0026amp;虚拟机构建打包示意图 # 采用的分支策略是：branches开发，master发布，tags存档（我们使用GitLab作为源码管理工具）。在测试环境通常为非master分支，测试完毕后合并到master，推送预发布，并针对当前版本打一个tag。针对这种情况，我们的“一次构建，处处使用”指的是测试环境用测试环境构建好的包，其他环境用预发布构建好的包。 # 各环境使用制品示意图 # 在推送每一个环境（test环境除外）时，都会校验当前版本的代码是否为前置环境推送过的最新代码，保证不会将没有经过审查的代码交付到线上。 # 在测试环境每一次编译构建之前，都会对代码进行一次安全扫描，Java语言的工程通过解析pom文件对其所有的依赖进行递归扫描，Nodejs语言的工程通过对node_module里下载的包进行递归扫描，确保有安全漏洞的代码不会被带入到生产环境。 # 编译构建时，会根据开发语言的不同，执行不同的编译脚本，根据发布方式的不同（虚拟机发布或者容器发布），执行不一样的后续操作步骤。 # 我们将Jenkins作为后台编译服务器，采用的是多master多slave的架构，我们并未直接使用Jenkins的流水线，而是开发了一个叫Pipeline的系统，与Jenkins做对接（此时的对接方式是调用Jenkins的API），由Pipeline系统提供Jenkins作业所需要的全部信息，另外编写了整个过程需要具体执行操作的脚本，通过Jenkins的job配置“Execute shell”的方式每次在构建之前导入到工作空间。 # 流水线标准生产过程大体如下： # 流水线标准生产过程示意图 # 另外，针对回滚的情况，因为每次在上线前都会在预发布会构建一个稳定的版本，并打一个tag，并且记录下tag对应的制品（包或者镜像）版本，所以在回滚的时候，只要选择要回滚的版本，便能找到对应版本的制品，进行重新发布，以此达到回滚的目的。 # 回滚流程 # 至此，第二阶段大体实现了以下功能： # 通过用Jira需求上线流程和流水线做整合，以及多种推送前的校验，保证了上线过程的每一步都是可靠的 # 每一个环境的集成和发布都是自动化的 # 因为过程变得可靠且自动化，使得将发布过程开放给研发团队成为了可能，达到了随时自主上线的目的。 # 然而这样的流水线也有诸多问题： # 不够灵活，如在推送测试环境时，整个过程执行的步骤是固定的的，即第一步做什么，第二步做什么都是固定的，不能新增也不能删减，如某些团队需要进行单元测试，有的不需要，但流水线都会去执行单元测试，通常情况下单元测试过程是一个花费时间比较长的过程，这对于需要频繁更改和部署的业务是不友好的。 # 推送成功率不高，因为整个过程是串联的，某一个环节出现错误，将会导致本次推送失败，而某些环节本不应该影响构建结果的，最后导致了构建失败。 # 第三阶段（2017-2019）：拥有灵活车道的私家车模式\n考虑到前面提到的两点，ZBJ DevOps团队在17年底对流水线做了二次改造： # 1、所有执行步骤拆解成独立原子任务，建立原子任务库； # 2、将原子任务根据功能性分为两种，校验类以及执行类，校验类原子任务主要是是做准入准出的判断，执行类主要是编译构建，打镜像，上传hub仓库以及部署。 # 3、将原子任务根据执行载体分为两种，Java类和Jenkins类，直接用Java程序执行的任务为Java类，需要Jenkins执行的任务为Jenkins类。 # 4、根据开发语言、发布方式、业务类型的不同，从原子任务库中选取不同原子任务组成一条标准有序的执行流水线。 # 5、提供工程特殊配置，如有些工程需要增强校验，有的工程需要减少校验，则可以通过启用和禁用的方式进行特殊配置，如下图所示，根据1、2和3步骤后可最终生成一条本次执行的流水线任务列表。 # 原子任务一览表\n执行效果图\n6、根据原子任务的制定，我们将Jenkins执行的job也拆分成了对应的几类，每一类拥有足够多数量的job进行任务的执行。 # 7、升级了Pieline系统和Jenkins通信架构，通过编写一个RabbitMQ的插件植入到Jenkins Master上，从原来调用API的方式，改成用RabbitMQ的方式进行通信，大大提高了效率和成功率。 # 通信示意图\n8、升级了Jenkins架构，构建一个能适配ZBJ所有开发语言的镜像，利用Jenkins Master的Kubernetes插件，将原来的虚拟机slave节点全部替换成容器slave节点，并且这个slave集群完全由Jenkins Master的Kubernetes插件调度，不论在高并发和低并发的时候都能及时扩缩容，满足业务需求。 # 第四阶段（2020-至今）：智能驾驶模式\n可以看到，到第三个阶段为止，我们的每一次编译过程，都需要研发同学“推送一下”，而且这个过程也是需要花费一些时间的，比如一个正常的Nodejs工程平均编译时长至少需要花费100+s以上，一个正常的Java工程平均编译时长至少也是需要30s以上，由于我们提供了推送过程“可视化”的功能，且没有执行结果的通知，导致用户必须关注推送过程以确保本次推送是成功的，大大浪费了研发同学的时间。 # 在此基础上，我们进一步做出了以下优化： # 1、为每一个GitLab上的工程添加一个Webhook，每当开发人员向仓库push一次，便会触发Webhook，调用Pipeline系统接口进行一次快速构建。 # 注意：并不是每次push都会进行一次快速构建，为了防止开发同学频繁修改少量代码提交到版本库，我们规定了一个“暗号”，只有当开发同学在commit message中添加这个“暗号”，才会触发一次快速构建。 # auto_trigger_build就是暗号内容 # 2、快速构建的结果是构建一个包或者一个镜像，存放在前文提到过的文件服务器或者Harbor仓库。在下一次用户“推送”的时候，便会根据分支和版本判断是否存在已编译过的包或者镜像，如果存在，则直接使用，跳过编译过程。 # 3、增加快速构建结果通知，因为整个快速构建过程是后台执行的，所以流水线系统通过企业微信的方式通知到用户本次快速构建的结果。 # 4、除了增加快速构建的通知，我们还增加了推送的通知。用户再也不用关注推送过程，只需要在推送后继续做其他事情，推送结果由企业微信通知到用户。 # 值得注意的是，当推送失败后，流水线系统也会通知到用户，进行对应问题的排查。 # 至此，ZBJ的CI/CD实践之路基本介绍完毕，当然，其中也还有很多细节方面，因为涉及的东西太多，不便铺开来讲。 # 总结三次重大改造的结果 # 第一次改造：奠定了ZBJ的CI/CD基础，打造了一条标准的流水线，解放了运维劳动力（过程全自动化），提高了研发效率，降低了研发成本（运维同学由最多时候的三四十个减少到了不到十个人）。 # 第二次改造：流水线实现了高可用，同时其灵活的配置能完美满足不同业务团队的需求。 # 第三次改造：提升了流水线效率，弱化推送过程，增强以人为本的体验，使推送过程更加智能化。 # 谈谈未来\nCI/CD实践之路还在继续，因为不同公司有不同的业务场景，而同一公司的业务也会随着时代的发展不断变化，只有适合自己的才是最好的，只有能拥抱变化的才是最好的，但万变不离其宗的，我觉得应该有一下几点： # CI/CD应该是以提高研发效率为目标的实践，一切脱离这个目标只是为了迎合什么口号而做什么的是都是耍牛氓。而实现这个目标是一个比较漫长的过程，一开始会比较容易，后面就会越来越难，这需要不断思考和学习的过程。 # CI/CD应该是紧贴业务的，因为业务的不同，要求的技术架构也会有所不同，随之而来，要求的交付方式也会有所不同。 # CI/CD应该是以人为本的，我们应该尽可能地将一切繁琐的过程交给程序去执行，而人只需要“坐享其成”或者做少量的决策即可。 # "},{"id":69,"href":"/docs/client-go-%E5%9B%9B%E7%A7%8D%E5%AE%A2%E6%88%B7%E7%AB%AF-client-go-si-zhong-ke-hu-duan/","title":"Client-go 四种客户端 2024-04-03 14:49:34.32","section":"Docs","content":" 简介 # Client-Go 共提供了 4 种与 Kubernetes APIServer 交互的客户端。分别是 RESTClient、DiscoveryClient、ClientSet、DynamicClient。 # • **RESTClient：**最基础的客户端，主要是对 HTTP 请求进行了封装，支持 Json 和 Protobuf 格式的数据。 # • **DiscoveryClient：**发现客户端，负责发现 APIServer 支持的资源组、资源版本和资源信息的。 # • **ClientSet：**负责操作 Kubernetes 内置的资源对象，例如：Pod、Service等。 # • **DynamicClient：**动态客户端，可以对任意的 Kubernetes 资源对象进行通用操作，包括 CRD。 # RESTClient # 上图可以看出 RESTClient 是所有 Client 的父类 # 它就是对 HTTP Request 进行了封装，实现了 RESTFul 风格的 API，可以直接通过 RESTClient 提供的 RESTful 方法 GET()，PUT()，POST()，DELETE() 操作数据 # • 同时支持 json 和 protobuf # • 支持所有原生资源和 CRD # 示例 # 使用 RESTClient 获取 K8S 集群 pod 资源 # package main import ( \u0026#34;context\u0026#34; \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;path/filepath\u0026#34; corev1 \u0026#34;k8s.io/api/core/v1\u0026#34; \u0026#34;k8s.io/client-go/kubernetes/scheme\u0026#34; \u0026#34;k8s.io/client-go/rest\u0026#34; \u0026#34;k8s.io/client-go/tools/clientcmd\u0026#34; ) // 获取系统家目录 func homeDir() string { if h := os.Getenv(\u0026#34;HOME\u0026#34;); h != \u0026#34;\u0026#34; { return h } // for windows return os.Getenv(\u0026#34;USERPROFILE\u0026#34;) } func main() { var kubeConfig *string var err error var config *rest.Config // 获取 kubeconfig 文件路径 if h := homeDir(); h != \u0026#34;\u0026#34; { kubeConfig = flag.String(\u0026#34;kubeConfig\u0026#34;, filepath.Join(h, \u0026#34;.kube\u0026#34;, \u0026#34;config\u0026#34;), \u0026#34;use kubeconfig to access kube-apiserver\u0026#34;) } else { kubeConfig = flag.String(\u0026#34;kubeConfig\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;use kubeconfig to access kube-apiserver\u0026#34;) } flag.Parse() // 获取 kubeconfig config, err = clientcmd.BuildConfigFromFlags(\u0026#34;\u0026#34;, *kubeConfig) if err != nil { panic(err.Error()) } // 使用 RESTClient 需要开发者自行设置资源 URL // pod 资源没有 group，在核心组，所以前缀是 api config.APIPath = \u0026#34;api\u0026#34; // 设置 corev1 groupVersion config.GroupVersion = \u0026amp;corev1.SchemeGroupVersion // 设置解析器，用于用于解析 scheme config.NegotiatedSerializer = scheme.Codecs.WithoutConversion() // 初始化 RESTClient restClient, err := rest.RESTClientFor(config) if err != nil { panic(err.Error()) } // 调用结果用 podList 解析 result := \u0026amp;corev1.PodList{} // 获取 kube-system 命名空间的 pod namespace := \u0026#34;kube-system\u0026#34; // 链式调用 RESTClient 方法获取，并将结果解析到 corev1.PodList{} err = restClient.Get().Namespace(namespace).Resource(\u0026#34;pods\u0026#34;).Do(context.TODO()).Into(result) if err != nil { panic(err.Error()) } // 打印结果 for _, pod := range result.Items { fmt.Printf(\u0026#34;namespace: %s, pod: %s\\n\u0026#34;, pod.Namespace, pod.Name) } } 程序结果如下： # namespace: kube-system, pod: coredns-697ddfb55c-5lk74 namespace: kube-system, pod: coredns-697ddfb55c-nnkhp namespace: kube-system, pod: etcd-master-172-31-97-104 namespace: kube-system, pod: kube-apiserver-master-172-31-97-104 namespace: kube-system, pod: kube-controller-manager-master-172-31-97-104 namespace: kube-system, pod: kube-lvscare-node-172-31-97-105 namespace: kube-system, pod: kube-proxy-49k8k namespace: kube-system, pod: kube-proxy-fvf57 namespace: kube-system, pod: kube-scheduler-master-172-31-97-104 namespace: kube-system, pod: metrics-server-7f6f9649f9-qvvj8 RESTClient 原理 # 初始化 RESTClient，可以发现对原生 HTTP 库进行了封装了 # func RESTClientFor(config *Config) (*RESTClient, error) { if config.GroupVersion == nil { return nil, fmt.Errorf(\u0026#34;GroupVersion is required when initializing a RESTClient\u0026#34;) } if config.NegotiatedSerializer == nil { return nil, fmt.Errorf(\u0026#34;NegotiatedSerializer is required when initializing a RESTClient\u0026#34;) } // Validate config.Host before constructing the transport/client so we can fail fast. // ServerURL will be obtained later in RESTClientForConfigAndClient() _, _, err := defaultServerUrlFor(config) if err != nil { return nil, err } // 获取原生 http client httpClient, err := HTTPClientFor(config) if err != nil { return nil, err } // 初始化 RESTClient return RESTClientForConfigAndClient(config, httpClient) } RESTClient 实现了 Interface 接口 # type Interface interface { GetRateLimiter() flowcontrol.RateLimiter Verb(verb string) *Request Post() *Request Put() *Request Patch(pt types.PatchType) *Request Get() *Request Delete() *Request APIVersion() schema.GroupVersion } RESTClient 的链式调用主要是设置 namespace，资源 name，一些选择器等，最终调用 Do() 方法网络调用 # func (r *Request) Do(ctx context.Context) Result { var result Result err := r.request(ctx, func(req *http.Request, resp *http.Response) { result = r.transformResponse(resp, req) }) if err != nil { return Result{err: err} } return result } func (r *Request) request(ctx context.Context, fn func(*http.Request, *http.Response)) error { //Metrics for total request latency start := time.Now() defer func() { metrics.RequestLatency.Observe(ctx, r.verb, r.finalURLTemplate(), time.Since(start)) }() if r.err != nil { klog.V(4).Infof(\u0026#34;Error in request: %v\u0026#34;, r.err) return r.err } if err := r.requestPreflightCheck(); err != nil { return err } client := r.c.Client if client == nil { client = http.DefaultClient } // Throttle the first try before setting up the timeout configured on the // client. We don\u0026#39;t want a throttled client to return timeouts to callers // before it makes a single request. if err := r.tryThrottle(ctx); err != nil { return err } if r.timeout \u0026gt; 0 { var cancel context.CancelFunc ctx, cancel = context.WithTimeout(ctx, r.timeout) defer cancel() } // Right now we make about ten retry attempts if we get a Retry-After response. var retryAfter *RetryAfter for { // 初始化网络请求 req, err := r.newHTTPRequest(ctx) if err != nil { return err } r.backoff.Sleep(r.backoff.CalculateBackoff(r.URL())) if retryAfter != nil { // We are retrying the request that we already send to apiserver // at least once before. // This request should also be throttled with the client-internal rate limiter. if err := r.tryThrottleWithInfo(ctx, retryAfter.Reason); err != nil { return err } retryAfter = nil } // 发起网络调用 resp, err := client.Do(req) updateURLMetrics(ctx, r, resp, err) if err != nil { r.backoff.UpdateBackoff(r.URL(), err, 0) } else { r.backoff.UpdateBackoff(r.URL(), err, resp.StatusCode) } done := func() bool { defer readAndCloseResponseBody(resp) // if the the server returns an error in err, the response will be nil. f := func(req *http.Request, resp *http.Response) { if resp == nil { return } fn(req, resp) } var retry bool retryAfter, retry = r.retry.NextRetry(req, resp, err, func(req *http.Request, err error) bool { // \u0026#34;Connection reset by peer\u0026#34; or \u0026#34;apiserver is shutting down\u0026#34; are usually a transient errors. // Thus in case of \u0026#34;GET\u0026#34; operations, we simply retry it. // We are not automatically retrying \u0026#34;write\u0026#34; operations, as they are not idempotent. if r.verb != \u0026#34;GET\u0026#34; { return false } // For connection errors and apiserver shutdown errors retry. if net.IsConnectionReset(err) || net.IsProbableEOF(err) { return true } return false }) if retry { err := r.retry.BeforeNextRetry(ctx, r.backoff, retryAfter, req.URL.String(), r.body) if err == nil { return false } klog.V(4).Infof(\u0026#34;Could not retry request - %v\u0026#34;, err) } f(req, resp) return true }() if done { return err } } } ClientSet # ClientSet 在调用 Kubernetes 内置资源非常常用，但是无法操作自定义资源，需要实现自定义资源的 ClientSet 才能操作。 # ClientSet 是在 RESTClient 的基础上封装了对 Resource 和 Version 的管理方法，Client-go 对 Kubernetes 每一个内置资源都封装了 Client，而 ClientSet 就是多个 Client 的集合。 # 示例 # 使用 ClientSet 获取 K8S 集群 pod 资源 # package main import ( \u0026#34;context\u0026#34; \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;path/filepath\u0026#34; metav1 \u0026#34;k8s.io/apimachinery/pkg/apis/meta/v1\u0026#34; \u0026#34;k8s.io/client-go/kubernetes\u0026#34; \u0026#34;k8s.io/client-go/tools/clientcmd\u0026#34; ) func homeDir() string { if h := os.Getenv(\u0026#34;HOME\u0026#34;); h != \u0026#34;\u0026#34; { return h } return os.Getenv(\u0026#34;USERROFILE\u0026#34;) } func main() { var kubeConfig *string if h := homeDir(); h != \u0026#34;\u0026#34; { kubeConfig = flag.String(\u0026#34;kubeConfig\u0026#34;, filepath.Join(h, \u0026#34;.kube\u0026#34;, \u0026#34;config\u0026#34;), \u0026#34;use kubeconfig to access kube-apiserver\u0026#34;) } else { kubeConfig = flag.String(\u0026#34;kubeConfig\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;use kubeconfig to access kube-apiserver\u0026#34;) } flag.Parse() config, err := clientcmd.BuildConfigFromFlags(\u0026#34;\u0026#34;, *kubeConfig) if err != nil { panic(err.Error()) } // 获取 clientSet clientSet, err := kubernetes.NewForConfig(config) if err != nil { panic(err.Error()) } namespace := \u0026#34;kube-system\u0026#34; // 链式调用 ClientSet 获取 pod 列表 podList, err := clientSet.CoreV1().Pods(namespace).List(context.TODO(), metav1.ListOptions{}) if err != nil { panic(err.Error()) } for _, pod := range podList.Items { fmt.Printf(\u0026#34;namespace: %s, pod: %s\\n\u0026#34;, pod.Namespace, pod.Name) } } ClientSet 原理 # NewForConfig 获取 ClientSet # // k8s.io/client-go/kubernetes/clientset.go:413 func NewForConfig(c *rest.Config) (*Clientset, error) { configShallowCopy := *c // share the transport between all clients httpClient, err := rest.HTTPClientFor(\u0026amp;configShallowCopy) if err != nil { return nil, err } return NewForConfigAndClient(\u0026amp;configShallowCopy, httpClient) } NewForConfigAndClient 获取每个 groupVersion 下的资源 Client # func NewForConfigAndClient(c *rest.Config, httpClient *http.Client) (*Clientset, error) { configShallowCopy := *c if configShallowCopy.RateLimiter == nil \u0026amp;\u0026amp; configShallowCopy.QPS \u0026gt; 0 { if configShallowCopy.Burst \u0026lt;= 0 { return nil, fmt.Errorf(\u0026#34;burst is required to be greater than 0 when RateLimiter is not set and QPS is set to greater than 0\u0026#34;) } configShallowCopy.RateLimiter = flowcontrol.NewTokenBucketRateLimiter(configShallowCopy.QPS, configShallowCopy.Burst) } var cs Clientset var err error cs.admissionregistrationV1, err = admissionregistrationv1.NewForConfigAndClient(\u0026amp;configShallowCopy, httpClient) if err != nil { return nil, err } cs.admissionregistrationV1beta1, err = admissionregistrationv1beta1.NewForConfigAndClient(\u0026amp;configShallowCopy, httpClient) if err != nil { return nil, err } ... return \u0026amp;cs, nil } 拿 admissionregistrationv1.NewForConfigAndClient 介绍 # func NewForConfigAndClient(c *rest.Config, h *http.Client) (*AdmissionregistrationV1Client, error) { config := *c // 设置 client 参数 if err := setConfigDefaults(\u0026amp;config); err != nil { return nil, err } // 最终调用 RESTClientForConfigAndClient 生成 RESTClient client, err := rest.RESTClientForConfigAndClient(\u0026amp;config, h) if err != nil { return nil, err } return \u0026amp;AdmissionregistrationV1Client{client}, nil } // 可以发现，这些参数跟上面 RESTClient 差不多 func setConfigDefaults(config *rest.Config) error { gv := v1.SchemeGroupVersion config.GroupVersion = \u0026amp;gv config.APIPath = \u0026#34;/apis\u0026#34; config.NegotiatedSerializer = scheme.Codecs.WithoutConversion() if config.UserAgent == \u0026#34;\u0026#34; { config.UserAgent = rest.DefaultKubernetesUserAgent() } return nil } pod 资源实现了一系列方法，比如 List()，可以发现最终调用 RESTClient 的方法 # func (c *pods) List(ctx context.Context, opts metav1.ListOptions) (result *v1.PodList, err error) { var timeout time.Duration if opts.TimeoutSeconds != nil { timeout = time.Duration(*opts.TimeoutSeconds) * time.Second } result = \u0026amp;v1.PodList{} err = c.client.Get(). Namespace(c.ns). Resource(\u0026#34;pods\u0026#34;). VersionedParams(\u0026amp;opts, scheme.ParameterCodec). Timeout(timeout). Do(ctx). Into(result) return } DynamicClient # DynamicClient 见名知意，一种动态客户端，通过动态指定资源组，资源版本和资源信息，来操作任意的 Kubernetes 资源对象。DynamicClient 不仅能操作 Kubernetes 内置资源，还能操作 CRD 。 # DynamicClient 与 ClientSet 都是对 RESTClient 进行了封装\n示例 # DynamicClient 返回的结果不像 ClientSet 那样返回具体资源类型，它返回的是一个动态数据即 map 结构，所以需要将结果进行解析到具体资源类型 # package main import ( \u0026#34;context\u0026#34; \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;path/filepath\u0026#34; corev1 \u0026#34;k8s.io/api/core/v1\u0026#34; metav1 \u0026#34;k8s.io/apimachinery/pkg/apis/meta/v1\u0026#34; \u0026#34;k8s.io/apimachinery/pkg/runtime\u0026#34; \u0026#34;k8s.io/apimachinery/pkg/runtime/schema\u0026#34; \u0026#34;k8s.io/client-go/dynamic\u0026#34; \u0026#34;k8s.io/client-go/tools/clientcmd\u0026#34; ) func homeDir() string { if h := os.Getenv(\u0026#34;HOME\u0026#34;); h != \u0026#34;\u0026#34; { return h } return os.Getenv(\u0026#34;USERROFILE\u0026#34;) } func main() { var kubeConfig *string if h := homeDir(); h != \u0026#34;\u0026#34; { kubeConfig = flag.String(\u0026#34;kubeConfig\u0026#34;, filepath.Join(h, \u0026#34;.kube\u0026#34;, \u0026#34;config\u0026#34;), \u0026#34;use kubeconfig to access kube-apiserver\u0026#34;) } else { kubeConfig = flag.String(\u0026#34;kubeConfig\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;use kubeconfig to access kube-apiserver\u0026#34;) } flag.Parse() config, err := clientcmd.BuildConfigFromFlags(\u0026#34;\u0026#34;, *kubeConfig) if err != nil { panic(err.Error()) } // 初始化 DynamicClient dynamicClient, err := dynamic.NewForConfig(config) if err != nil { panic(err.Error()) } // 提供 pod 的 gvr，因为是动态调用，dynamicClient 不知道需要操作哪个资源，所以需要自己提供 gvr := schema.GroupVersionResource{ Group: \u0026#34;\u0026#34;, Version: \u0026#34;v1\u0026#34;, Resource: \u0026#34;pods\u0026#34;, } //链式调用 dynamicClient 获取数据 result, err := dynamicClient.Resource(gvr).Namespace(\u0026#34;kube-system\u0026#34;).List(context.TODO(), metav1.ListOptions{}) if err != nil { panic(err.Error()) } podList := \u0026amp;corev1.PodList{} // 将结果解析到 podList scheme 中 err = runtime.DefaultUnstructuredConverter.FromUnstructured( result.UnstructuredContent(), podList) for _, pod := range podList.Items { fmt.Printf(\u0026#34;namespace: %s, pod: %s\\n\u0026#34;, pod.Namespace, pod.Name) } } DynamicClient 原理 # dynamic.NewForConfig(config) 初始化 dynamicClient # func NewForConfig(inConfig *rest.Config) (Interface, error) { config := ConfigFor(inConfig) httpClient, err := rest.HTTPClientFor(config) if err != nil { return nil, err } return NewForConfigAndClient(config, httpClient) } func NewForConfigAndClient(inConfig *rest.Config, h *http.Client) (Interface, error) { config := ConfigFor(inConfig) // for serializing the options config.GroupVersion = \u0026amp;schema.GroupVersion{} config.APIPath = \u0026#34;/if-you-see-this-search-for-the-break\u0026#34; // 初始化 RESTClient restClient, err := rest.RESTClientForConfigAndClient(config, h) if err != nil { return nil, err } return \u0026amp;dynamicClient{client: restClient}, nil } 可以看出 dynamicClient 与 ClientSet 一样都是封装了 RESTClient\ndynamicClient.Resource(gvr).Namespace(\u0026#34;kube-system\u0026#34;).List(context.TODO(), metav1.ListOptions{}) dynamicClient 链式调用中，Resource() 需要传入需要操作对象的 gvr # 最终也是调用 RESTClient 来获取数据 # func (c *dynamicResourceClient) List(ctx context.Context, opts metav1.ListOptions) (*unstructured.UnstructuredList, error) { result := c.client.client.Get().AbsPath(c.makeURLSegments(\u0026#34;\u0026#34;)...).SpecificallyVersionedParams(\u0026amp;opts, dynamicParameterCodec, versionV1).Do(ctx) if err := result.Error(); err != nil { return nil, err } retBytes, err := result.Raw() if err != nil { return nil, err } uncastObj, err := runtime.Decode(unstructured.UnstructuredJSONScheme, retBytes) if err != nil { return nil, err } if list, ok := uncastObj.(*unstructured.UnstructuredList); ok { return list, nil } list, err := uncastObj.(*unstructured.Unstructured).ToList() if err != nil { return nil, err } return list, nil } DynamicClient 返回的结果是 *unstructured.UnstructuredList # Unstructured 是非结构化数据，用 map[string]interface{} 存储。 # type UnstructuredList struct { Object map[string]interface{} // Items is a list of unstructured objects. Items []Unstructured `json:\u0026#34;items\u0026#34;` } type Unstructured struct { // Object is a JSON compatible map with string, float, int, bool, []interface{}, or // map[string]interface{} // children. Object map[string]interface{} } 所以拿到结果需要 decode 成结构化数据类型 # // 将 result decode 到 podList podList := \u0026amp;corev1.PodList{} err = runtime.DefaultUnstructuredConverter.FromUnstructured( result.UnstructuredContent(), podList) DiscoveryClient # DiscoveryClient 是发现客户端，用于发现 Kube-apiserver 支持的资源组、资源版本、资源类型等。 # kubectl api-resources 和 kubectl api-versions 命令就是通过 DiscoveryClient 实现的。 # DiscoveryClient 支持本地目录缓存，一般在 ~/.kube/cache 会存储集群所有 gvr 信息，避免每次访问 Kube-apiserver # 示例 # 通过 DiscoveryClient 查询集群所有的 gvr # package main import ( \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;path/filepath\u0026#34; \u0026#34;k8s.io/apimachinery/pkg/runtime/schema\u0026#34; \u0026#34;k8s.io/client-go/discovery\u0026#34; \u0026#34;k8s.io/client-go/tools/clientcmd\u0026#34; ) func homeDir() string { if h := os.Getenv(\u0026#34;HOME\u0026#34;); h != \u0026#34;\u0026#34; { return h } return os.Getenv(\u0026#34;USERROFILE\u0026#34;) } func main() { var kubeConfig *string if h := homeDir(); h != \u0026#34;\u0026#34; { kubeConfig = flag.String(\u0026#34;kubeConfig\u0026#34;, filepath.Join(h, \u0026#34;.kube\u0026#34;, \u0026#34;config\u0026#34;), \u0026#34;use kubeconfig to access kube-apiserver\u0026#34;) } else { kubeConfig = flag.String(\u0026#34;kubeConfig\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;use kubeconfig to access kube-apiserver\u0026#34;) } flag.Parse() config, err := clientcmd.BuildConfigFromFlags(\u0026#34;\u0026#34;, *kubeConfig) if err != nil { panic(err.Error()) } // 初始化 DiscoveryClient discoveryClient, err := discovery.NewDiscoveryClientForConfig(config) if err != nil { panic(err.Error()) } // 获取集群所有资源 _, apiResourceList, err := discoveryClient.ServerGroupsAndResources() if err != nil { panic(err.Error()) } for _, resources := range apiResourceList { gv, err := schema.ParseGroupVersion(resources.GroupVersion) if err != nil { panic(err.Error()) } for _, resource := range resources.APIResources { fmt.Printf(\u0026#34;group: %s, version: %s, resource: %s\\n\u0026#34;, gv.Group, gv.Version, resource.Name) } } } 结果如下： # group: , version: v1, resource: bindings group: , version: v1, resource: componentstatuses group: , version: v1, resource: configmaps group: , version: v1, resource: endpoints group: , version: v1, resource: events group: , version: v1, resource: limitranges group: , version: v1, resource: namespaces group: , version: v1, resource: namespaces/finalize group: , version: v1, resource: namespaces/status group: , version: v1, resource: nodes group: , version: v1, resource: nodes/proxy group: , version: v1, resource: nodes/status group: , version: v1, resource: persistentvolumeclaims group: , version: v1, resource: persistentvolumeclaims/status group: , version: v1, resource: persistentvolumes group: , version: v1, resource: persistentvolumes/status group: , version: v1, resource: pods group: , version: v1, resource: pods/attach group: , version: v1, resource: pods/binding group: , version: v1, resource: pods/ephemeralcontainers group: , version: v1, resource: pods/eviction group: , version: v1, resource: pods/exec group: , version: v1, resource: pods/log group: , version: v1, resource: pods/portforward group: , version: v1, resource: pods/proxy group: , version: v1, resource: pods/status group: , version: v1, resource: podtemplates group: , version: v1, resource: replicationcontrollers group: , version: v1, resource: replicationcontrollers/scale group: , version: v1, resource: replicationcontrollers/status group: , version: v1, resource: resourcequotas group: , version: v1, resource: resourcequotas/status group: , version: v1, resource: secrets group: , version: v1, resource: serviceaccounts group: , version: v1, resource: services group: , version: v1, resource: services/proxy group: , version: v1, resource: services/status group: apiregistration.k8s.io, version: v1, resource: apiservices group: apiregistration.k8s.io, version: v1, resource: apiservices/status group: apiregistration.k8s.io, version: v1beta1, resource: apiservices group: apiregistration.k8s.io, version: v1beta1, resource: apiservices/status group: extensions, version: v1beta1, resource: ingresses group: extensions, version: v1beta1, resource: ingresses/status group: apps, version: v1, resource: controllerrevisions group: apps, version: v1, resource: daemonsets group: apps, version: v1, resource: daemonsets/status group: apps, version: v1, resource: deployments group: apps, version: v1, resource: deployments/scale group: apps, version: v1, resource: deployments/status group: apps, version: v1, resource: replicasets group: apps, version: v1, resource: replicasets/scale group: apps, version: v1, resource: replicasets/status group: apps, version: v1, resource: statefulsets group: apps, version: v1, resource: statefulsets/scale ... DiscoveryClient 原理 # discovery.NewDiscoveryClientForConfig(config) 初始化 DsicoveryClient，与 ClientSet 和 DynamicClient 原理类似，都是封装了 RESTClient，这里不再赘述。 # discoveryClient.ServerGroupsAndResources() 返回集群所有的资源对象，这里可能会有疑问，这些资源是什么时候存储到 etcd 中的，猜想是 Kube-apiserver 启动时将这些资源类型存储到 etcd 中。需要看 Kube-apiserver 源码来佐证。 # 总结 # Client-go 四种客户端，在平时开发中 ClientSet 使用频率最高，其他三种了解原理一般就行。 # "},{"id":70,"href":"/docs/client-go-%E6%9E%B6%E6%9E%84-client-go-jia-gou/","title":"Client-go 架构 2024-04-03 14:48:40.716","section":"Docs","content":" 简介 # Client-go 源码位于 https://github.com/kubernetes/client-go，是 Kubernetes 项目非常重要的子项目，Client-Go 是负责与 Kubernetes APIServer 服务进行交互的客户端库，利用 Client-Go 与Kubernetes APIServer 进行的交互访问，来对 Kubernetes 中的各类资源对象进行管理操作，包括内置的资源对象及CRD。在云原生开发的项目中使用频率非常高，例如开发 operator，k8s 管理平台等。 # 公众号\n架构 # Client-go 架构比较复杂，极大程度运用异步处理，提高服务运行效率。 # 上图上半部分是 client-go 代码组件，下半部分是需要开发者自己实现，主要实现如何对资源事件进行处理，但是 workqueue 也是 client-go 实现的，只不过是在开发者写的程序中进行使用的。 # Reflector # Reflector 用于监控（Watch）指定的 Kubernetes 资源，当监控的资源发生变化时，例如 Add 事件、Update 事件、Delete 事件，并将其资源对象存放到本地缓存 DeltaFIFO 中。 # Deltafifo # DeltaFIFO 是一个生产者-消费者的队列，生产者是 Reflector，消费者是 informer Pop 函数，FIFO 是一个先进先出的队列，而 Delta 是一个资源对象存储，它可以保存资源对象的操作类型，例如 Add 操作类型、Update 操作类型、Delete 操作类型、Sync 操作类型等。 # Indexer # Indexer 是 client-go 用来存储资源对象并自带索引功能的本地存储，informer 从 DeltaFIFO 中将消费出来的资源对象存储至 Indexer。Indexer 与 Etcd 集群中的数据保持完全一致。这样我们就可以很方便地从本地存储中读取相应的资源对象数据，而无须每次从远程 APIServer 中读取，以减轻服务器的压力。 # Informer # Informer 将上述三个组件协同运行起来，保证整个流程串联起来，是 client-go 中的大脑。 # workqueue # Workqueue 是一个先进先出的队列，informer 将事件获取到并不及时处理，先将事件 push 到 workqueue 中，然后再从 workqueue 消费处理。大大提高运行效率 # 运行流程 # 例如现在创建一个 pods，kubelet 中的 controller 是如何运行的(K8s 中源码中也大量使用 client-go，主要是大量的 controller) # • 初始化并启动 informer，informer 启动会初始化并启动 reflector，reflector 从 kube-apiserver list 所有 pod 资源，并 sync 到 Deltafifo 中。 # • Deltafifo 存有全部 pod 资源，informer 通过 pop 函数消费 deltafifo 事件并存储到 indexer 中。 # • 如果需要调用 pod 资源，那么可以直接从 indexer 中获取 # • informer 初始化完成后，Reflector 开始 Watch Pod 相关的事件 # • 如果创建一个 pod，1. 那么 Reflector 会监听到这个事件，然后将这个事件发送到 DeltaFIFO 中 # • informer pop 消费改 ADD 事件，并将该 pod 存储到 indexer # • informer 处理器函数同样拿到该 ADD 事件去处理该事件，通过workqueue获取到事件的key，再通过indexer获取到真正操作的对象 # • reflector 会周期性将 indexer 数据同步到 Deltafifo，防止一些事件处理失败，重新处理。 # 总结 # 理解 client-go 原理是非常重要的，里面很多设计值得我们去学习，也可以运用到自己项目中。\n"},{"id":71,"href":"/docs/cni%E6%8F%92%E4%BB%B6%E9%80%89%E5%9E%8B-cni-cha-jian-xuan-xing/","title":"CNI插件选型 2024-04-03 15:01:51.29","section":"Docs","content":"本文介绍容器环境常见网络应用场景及对应场景的 Kubernetes CNI 插件功能实现。帮助搭建和使用云原生环境的小伙伴快速选择心仪的网络工具。\n常见网络插件 # 我们在学习容器网络的时候，肯定都听说过 Docker 的 bridge 网络，Vethpair，VxLAN 等术语，从 Docker 到 kubernetes 后，学习了 Flannel、Calico 等主流网络插件，分别代表了 Overlay 和 Underlay 的两种网络传输模式，也是很经典的两款 CNI 网络插件。那么，还有哪些好用的 CNI 插件呢 ? 我们看看 CNCF Landscape:\n常见网络插件\n抛去商业版 CNI，此次分享来聊聊几款热门开源 CNI 插件，分别为 Kube-OVN、Antrea、Cilium。Kube-OVN 和 Antrea 都是基于 OpenvSwitch 的项目，Cilium 使用 eBPF 这款革命性的技术作为数据路径，亦是这两年很火热的一个开源容器项目。\n行万里路，此处相逢，共话云原生之道。 偶逗趣事，明月清风，与君同坐。\n55篇原创内容\n公众号\n那么，又回到学习新产品的第一步，如何快速部署 K8s 体验不同地 CNI 插件呢？我们可以使用 Kubekey 。 # Kubekey 作为一个开源的 Kubernetes 和 KubeSphere 集群部署工具，可以轻松的部署 Kubernetes 集群，提供节点管理、操作系统安全加固、容器运行时管理、网络存储安装、Etcd 管理等。Kubekey 支持一键部署 Calico / Flannel / Cilium / Kube-OVN 等网络插件，只需在 kk 的配置文件中注明 network 的 plugin 值即可： # network: plugin: calico/kubeovn/cilium kubePodsCIDR: 10.233.64.0/18 kubeServiceCIDR: 10.233.0.0/18 对于 antrea，由于版本较新，目前可通过 addon 的形式添加 helm 文件的形式进行一键安装： # addons: - name: antrea namespace: kube-system sources: chart: name: antrea repo: https://charts.antrea.io # values: 在此基础上，可以通过以下一条命令 # 🐳 → kk create cluster --with-kubernetes --with-kubesphere 网络应用场景 # 现在我们已经有了一个 Kubernetes 集群，先来思考一下，容器网络除了让集群正常运行，能让安装 Kubernetes 后 Pending 的 CoreDNS running 起来（抖个鸡灵-_-）以外还有哪些使用场景？ # 网络应用场景\n这里我通过一张图总结了七个主要使用的场景，应该也涵盖大部分运维人员网络需求。\n• 固定 IP。对于现存虚拟化 / 裸机业务 / 单体应用迁移到容器环境后，都是通过 IP 而非域名进行服务间调用，此时就需要 CNI 插件有固定 IP 的功能，包括 Pod/Deployment/Statefulset。 • 网络隔离。不同租户或不同应用之间，容器组应该是不能互相调用或通信的。 • 多集群网络互联。对于不同的 Kubernetes 集群之间的微服务进行互相调用的场景，需要多集群网络互联。这种场景一般分为 IP 可达和 Service 互通，满足不同的微服务实例互相调用需求。 • 出向限制。对于容器集群外的数据库 / 中间件，需能控制特定属性的容器应用才可访问，拒绝其他连接请求。 • 入向限制。限制集群外应用对特定容器应用的访问。 • 带宽限制。容器应用之间的网络访问加以带宽限制。 • 出口网关访问。对于访问集群外特定应用的容器，设置出口网关对其进行 SNAT 以达到统一出口访问的审计和安全需求。理完需求和应用场景，我们来看看如何通过不同的 CNI 插件解决以上痛点。 网络插件功能实现 # 固定 IP # 基本上主流 CNI 插件都有自己的 IPAM 机制，都支持固定 IP 及 IP Pool 的分配，并且各个 CNI 插件殊途同归的都使用了 Annotation 的方式指定固定 IP。\n• 对于 Pod，分配固定 IP； • 对于 Deployment，使用 IP Pool 的方式分配； • 对于有状态的 Statefulset，使用 IP Pool 分配后，会根据 Pool 的分配顺序记好 Pod 的 IP，以保证在 Pod 重启后仍能拿到同样的 IP。 Calico # \u0026#34;cni.projectcalico.org/ipAddrs\u0026#34;: \u0026#34;[\\\u0026#34;192.168.0.1\\\u0026#34;]\u0026#34; Kube-OVN # ovn.kubernetes.io/ip_address: 192.168.100.100 ovn.kubernetes.io/ip_pool: 192.168.100.201,192.168.100.202 Antrea # Antrea IPAM 只能在 Bridge 模式下使用，因此可以在 Multus 的辅佐下，主网卡使用 NodeIPAM 分配，副网卡使用 Antrea IPAM 分配 VLAN 类型网络地址。\nipam.antrea.io/ippools: \u0026#39;pod-ip-pool1\u0026#39; ipam.antrea.io/pod-ips: \u0026#39;\u0026lt;ip-in-pod-ip-pool1\u0026gt;\u0026#39; Cilium # Not Yet! 多集群网络互联 # 对于多集群网络互联，假设有现有多个集群，不同的微服务运行在不同的集群中，集群 1 的 App01 需要和集群 2 的 App02 进行通信，由于他们都是通过 IP 注册在集群外的 VM 注册中心的，所以 App01 和 App02 只能通过 IP 通信。在这种场景下，就需要多集群 Pod 互联互通。 # Calico # 对于 Calico 这种原生对 BGP 支持很好的 CNI 插件来说，很容易实现这一点，只要两个集群通过 BGP 建立邻居，将各自的路由宣告给对方即可实现动态路由的建立。若存在多个集群，使用 BGP RR 的形式也很好解决。但这种解决方式可能不是最理想的，因为需要和物理网络环境进行配合和联调，这就需要网络人员和容器运维人员一同进行多集群网络的建设，在后期运维和管理上都有不大方便和敏捷的感觉。 # 那 Calico VxLAN 模式呢？\n既然说到 VxLAN，可以和 Kube-OVN、Antrea、Cilium 放到一起来看，四种 CNI 都支持 Overlay 的网络模型，都支持通过 VxLAN/GENEVE 的形式建立隧道网络打通容器网络通信。这就赋予运维人员较高的灵活性，对于容器网络的调教、IPAM 分配、网络监控和可观察性、网络策略调整都由容器集群运维人员负责，而网络人员则只需要提前划好物理网络大段，保证容器集群 Node 之间网络互通即可。 # 如何实现 overlay 网络的多集群互联呢\nSubmariner # CNCF 有个沙箱项目叫 Submariner，它通过在不同集群建立不同的网关节点并打通隧道的形式实现多集群通信。从官方这张架构图来说明： # Submariner\n简单来说，Submariner 由一个集群元数据中介服务（broker）掌握不同集群的信息（Pod/Service CIDR），通过 Route Agent 将 Pod 流量从 Node 导向网关节点（Gateway Engine），然后由网关节点打通隧道丢到另一个集群中去，这个过程就和不同主机的容器之间使用 VxLAN 网络通信的概念是一致的。要达成集群连接也很简单，在其中一个集群部署 Broker，然后通过 kubeconfig 或 context 分别进行注册即可。\n🐳 → subctl deploy-broker --kubeconfig ~/.kube/config1 🐳 → subctl join --kubeconfig ~/.kube/config1 broker-info.subm --clusterid ks1 --natt=false --cable-driver vxlan --health-check=false 🐳 → subctl join --kubeconfig ~/.kube/config2 broker-info.subm --clusterid ks2 --natt=false --cable-driver vxlan --health-check=false 🐳 → subctl show all ✓ Showing Endpoints CLUSTER ID ENDPOINT IP PUBLIC IP CABLE DRIVER TYPE ks1 192.168.100.10 139.198.21.149 vxlan local ks2 192.168.100.20 139.198.21.149 vxlan remote Cilium # Cilium Cluster Mesh 和 Submariner 有异曲同工之妙，可以通过隧道形式或 NativeRoute 形式实现集群互联。 # Cilium\nCilium\nCilium 开启多集群网络连接也很简单：\n🐳 → cilium clustermesh enable --context $CLUSTER1 🐳 → cilium clustermesh enable --context $CLUSTER2 KubeOVN # Kube-OVN 还提供一个 OVNIC 的组件，它运行一个路由中继的 OVN-IC 的 Docker 容器，作为两个集群的网关节点，将不同集群的 Pod 网络进行连通。\n多集群服务互访 # 除了 Pod IP 的互联互通，多集群网络还可考虑集群间的 Service 互访，Submariner、Cilium，Antrea 都能实现。\n**Submariner 和 Antrea 都使用了 Kubernetes 社区的 MultiCluster Service 并在此之上结合自身组件实现多集群的服务访问。**MultiCluster Service 通过 ServiceExport 和 ServiceImport 的 CRD，ServiceExport 将需要公开的服务导出，然后通过 ServiceImport 将此服务导入到另一个集群。 # 多集群服务互访\nSubmariner # 拿 Submariner 实现举例，有两个集群 ks1 和 ks2，ks1 在 test 命名空间有一个服务 nginx，此时通过 ServiceExport 将 nginx 服务进行导出，Submariner 会把这个 nginx.test.svc.cluster.local 服务发现为 nginx.test.svc.clusterset.local，两个集群的 coredns 都会建立一个新的 clusterset.local 的存根域，将所有匹配 cluster.set 的请求发送给 submariner 的服务发现的组件。同时 ServiceImport 导入到 ks2 集群，ks2 集群的 Pod 就可以通过 nginx.test.svc.clusterset.local 解析到 ks1 集群的 nginx Service。如果两个集群都有 nginx 的同名服务，此时 submariner 就可以优先本地进行访问，本地服务端点有故障后再访问其他集群的 nginx 服务，是不是可以开始构建双活服务了哈哈。 # Antrea # Antrea 实现方式类似，也是结合 ServiceExport 和 ServiceImport 并进行封装成 ResourceExport 和 ResourceImport 构建多集群服务，在每个集群选择一个节点作为网关，通过网关打通不同集群隧道来实现多集群服务的访问。 # Antrea # Cilium # Cilium 没有用 MultiService 的概念，Cilium 通过 Global Service 的概念构建多集群访问服务访问。 # Cilium\n从这张图可以看出，Cilium 更适合做多活集群的多集群服务访问需求，通过对相应的服务添加 Annotation 的做法，把不同集群的服务设定为 global-service，并通过 shared-service 和 service-affinity 来控制服务是否能被其他集群访问及服务亲和性。以下是一个例子：\napiVersion: v1 kind: Service metadata: name: nginx annotations: io.cilium/global-service: \u0026#39;true\u0026#39; io.cilium/shared-service: \u0026#39;true\u0026#39; io.cilium/service-affinity: \u0026#39;local\u0026#39; # Possible values: # - local # preferred endpoints from local cluster if available # - remote # preferred endpoints from remote cluster if available # none (default) # no preference. Default behavior if this annotation does not exist spec: type: ClusterIP ports: - port: 80 selector: name: nginx 以上，当有多集群互访需求又不想 CNI 强相关时，可以尝试玩一下 Submariner，作为 CNCF Landscape Network 中一个专注于多集群互访的 SandBox 项目，Submariner 提供多集群网络通信，服务发现，以及安全加密，是一个很好的选择。 # 云原生百宝箱\n行万里路，此处相逢，共话云原生之道。 偶逗趣事，明月清风，与君同坐。\n55篇原创内容\n公众号\n网络策略 # 对于 Pod 网络隔离、入向限制、出向限制的网络场景，可以整合成网络策略一同来说。主流开源 CNI 都支持 Kubernetes NetworkPolicy，通过 Network Policy，可以在 3 层或 4 层做相应的网络安全限制。Network Policy 通过 Ingress 和 Egress 两种进行网络限制，默认都是放行的。也就是说，设置 Kubernetes 网络策略，主要以白名单的形式对集群内的流量进行安全限制。 # 比如只允许指定 label 的 Pod 访问集群外数据库（通过 CIDR 指定） # apiVersion: networking.K8s.io/v1 kind: NetworkPolicy metadata: name: ingress-allow namespace: default spec: podSelector: matchLabels: role: db policyTypes: - Egress egress: - to: - ipBlock: cidr: 192.168.100.40/24 ports: - protocol: TCP port: 3306 apiVersion: networking.K8s.io/v1 kind: NetworkPolicy metadata: name: ingress-allow namespace: default spec: podSelector: matchLabels: role: app policyTypes: - Ingress ingress: - from: - ipBlock: cidr: 172.17.0.0/16 except: - 172.17.1.0/24 - namespaceSelector: matchLabels: project: web-project - podSelector: matchLabels: role: web 虽然 Network Policy 能满足大多场景，但是不是感觉还是少了点东西？比如 7 层策略、基于 NodeSelector、Drop/Reject 类型的策略指定、指定 Egress 节点进行控制等高级能力。这个时候 Cilium 和 Antrea 就大放异彩了。 # Cilium # Cilium 有两个 CRD，CiliumNetworkPolicy 和 CiliumClusterwideNetworkPolicy，来实现单集群和多集群的网络策略能力。Cilium 支持 3、4、7 层网络策略。并增加 EndPoint Selector 和 Node Selector。除了普通的基于 PodSelector 和 CIDR 的限制，Cilium 可以支持更多种策略，比如： # DNS 限制策略，只允许 app: test-app 的端点通过 53 端口去 kube-system 命名空间的 \u0026ldquo;K8s:K8s-app\u0026rdquo;: kube-dns 标签的 DNS 服务器访问 my-remote-service.com： # apiVersion: \u0026#34;cilium.io/v2\u0026#34; kind: CiliumNetworkPolicy metadata: name: \u0026#34;to-fqdn\u0026#34; spec: endpointSelector: matchLabels: app: test-app egress: - toEndpoints: - matchLabels: \u0026#34;K8s:io.kubernetes.pod.namespace\u0026#34;: kube-system \u0026#34;K8s:K8s-app\u0026#34;: kube-dns toPorts: - ports: - port: \u0026#34;53\u0026#34; protocol: ANY rules: dns: - matchPattern: \u0026#34;*\u0026#34; - toFQDNs: - matchName: \u0026#34;my-remote-service.com\u0026#34; Http 限制策略 , 只允许 org: empire 标签的端点对 deathstar 的 /v1/request-landing 进行 POST 操作： # apiVersion: \u0026#34;cilium.io/v2\u0026#34; kind: CiliumNetworkPolicy metadata: name: \u0026#34;rule\u0026#34; spec: description: \u0026#34;L7 policy to restrict access to specific HTTP call\u0026#34; endpointSelector: matchLabels: org: empire class: deathstar ingress: - fromEndpoints: - matchLabels: org: empire toPorts: - ports: - port: \u0026#34;80\u0026#34; protocol: TCP rules: http: - method: \u0026#34;POST\u0026#34; path: \u0026#34;/v1/request-landing\u0026#34; kafka 策略控制：\napiVersion: \u0026#34;cilium.io/v2\u0026#34; kind: CiliumNetworkPolicy metadata: name: \u0026#34;rule1\u0026#34; spec: description: \u0026#34;enable empire-hq to produce to empire-announce and deathstar-plans\u0026#34; endpointSelector: matchLabels: app: kafka ingress: - fromEndpoints: - matchLabels: app: empire-hq toPorts: - ports: - port: \u0026#34;9092\u0026#34; protocol: TCP rules: kafka: - role: \u0026#34;produce\u0026#34; topic: \u0026#34;deathstar-plans\u0026#34; - role: \u0026#34;produce\u0026#34; topic: \u0026#34;empire-announce\u0026#34; Antrea # Antrea 除了增加现有 NetworkPolicy 功能外，抽象了 Antrea NetworkPolicy 和 Antrea ClusterNetworkPolicy 两个 CRD 实现命名空间级别和集群级别的安全管理。还提供了 Group，Tier 的概念，用于资源分组和优先级设计，嗯，果真是 NSX 的亲兄弟。因此 Antrea 有零信任的网络策略安全防护手段，可以实现严格的 pod 和命名空间隔离。 # 网络层 Antrea 增加了对 ICMP 和 IGMP，Mutlicast 的限制，禁 ping 人员狂喜。 # apiVersion: crd.antrea.io/v1alpha1 kind: ClusterNetworkPolicy metadata: name: acnp-reject-ping-request spec: priority: 5 tier: securityops appliedTo: - podSelector: matchLabels: role: server namespaceSelector: matchLabels: env: prod egress: - action: Reject protocols: - icmp: icmpType: 8 icmpCode: 0 name: DropPingRequest enableLogging: true 基于 FQDN 的过滤：\napiVersion: crd.antrea.io/v1alpha1 kind: ClusterNetworkPolicy metadata: name: acnp-fqdn-all-foobar spec: priority: 1 appliedTo: - podSelector: matchLabels: app: client egress: - action: Allow to: - fqdn: \u0026#34;*foobar.com\u0026#34; ports: - protocol: TCP port: 8080 - action: Drop 设置不同类型的 Group，基于 Group 设置网络策略，就不用对同类业务写一堆 Label 了 # apiVersion: crd.antrea.io/v1alpha3 kind: Group metadata: name: test-grp-with-namespace spec: podSelector: matchLabels: role: db namespaceSelector: matchLabels: env: prod --- # Group that selects IP block 10.0.10.0/24. apiVersion: crd.antrea.io/v1alpha3 kind: Group metadata: name: test-grp-ip-block spec: ipBlocks: - cidr: 10.0.10.0/24 --- apiVersion: crd.antrea.io/v1alpha3 kind: Group metadata: name: test-grp-svc-ref spec: serviceReference: name: test-service namespace: default --- # Group that includes the previous Groups as childGroups. apiVersion: crd.antrea.io/v1alpha3 kind: Group metadata: name: test-grp-nested spec: childGroups: [test-grp-sel, test-grp-ip-blocks, test-grp-svc-ref] Egress # 对于特定业务出集群需不暴露 IP 或符合安全审计需求的场景，需要 Pod IP -\u0026gt; External IP 对外部业务进行访问。Cilium，Kube-OVN，Antrea 都有类似 Egress Gateway/Egress IP 的功能，特定标签的 Pod 通过 SNAT 为 Egress IP 访问集群外服务。 # Cilium # apiVersion: cilium.io/v2 kind: CiliumEgressGatewayPolicy metadata: name: egress-sample spec: selectors: - podSelector: matchLabels: app: snat-pod io.kubernetes.pod.namespace: default destinationCIDRs: - \u0026#34;0.0.0.0/0\u0026#34; egressGateway: nodeSelector: matchLabels: node.kubernetes.io/name: node1 egressIP: 10.168.60.100 KubeOVN # apiVersion: v1 kind: Pod metadata: name: pod-gw annotations: ovn.kubernetes.io/eip: 172.10.0.1 #或ovn.kubernetes.io/snat: 172.10.0.1 spec: containers: - name: eip-pod image: nginx:alpine Antrea: # apiVersion: crd.antrea.io/v1alpha2 kind: Egress metadata: name: egress-staging-web spec: appliedTo: namespaceSelector: matchLabels: kubernetes.io/metadata.name: staging podSelector: matchLabels: app: web externalIPPool: external-ip-pool #或IP形式 egressIP: 10.10.10.1 带宽管理 # kube-ovn 和 Clium 都支持带宽管理，kube-ovn 还支持 QoS 调整，只需要 Annotation 一下即可搞定： # Kube-OVN # apiVersion: v1 kind: Pod metadata: name: qos namespace: ls1 annotations: ovn.kubernetes.io/ingress_rate: \u0026#34;3\u0026#34; ovn.kubernetes.io/egress_rate: \u0026#34;1\u0026#34; ovn.kubernetes.io/latency: 3 ovn.kubernetes.io/loss: 20 Cilium # apiVersion: v1 kind: Pod metadata: annotations: kubernetes.io/egress-bandwidth: 10M ... 以上。就是此次分享的全部内容了，读到这里你可以也会感慨，从最早学 docker0，Vethpair 熟悉容器网络原理，到搭建 K8s 后节点 NotReady 就 apply 个 Flannel 逐步了解 CNI 插件机制，到今天的 CNCF Network\u0026amp;Service Proxy 生态的花团锦簇，云原生网络在日新月异的发展着，容器网络从最初的连通性到现在演变出更多的玩法和适用性，不论是网络功能、安全控制、网络洞察和可观测性，都在更好地为运维人员服务。若要体验更多功能，快到开源社区选择喜欢的容器网络项目 Hands on Lab 吧！ # "},{"id":72,"href":"/docs/containerd-%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C-containerd-ji-ben-cao-zuo/","title":"Containerd 基本操作 2024-04-03 14:55:46.786","section":"Docs","content":" 镜像基本操作 # Containerd 默认提供 CLI 命令行工具 ctr，ctr 命名提供基本的镜像和容器操作功能，可以通过如下查看命令帮助：\n[root@localhost ~]# ctr -help 镜像基本操作主要是 ctr image 命令，查看命令帮助\n[root@localhost ~]# ctr images -h NAME: ctr images - Manage images USAGE: ctr images command [command options] [arguments...] COMMANDS: check Check existing images to ensure all content is available locally export Export images import Import images list, ls List images known to containerd mount Mount an image to a target path unmount Unmount the image from the target pull Pull an image from a remote push Push an image to a remote prune Remove unused images delete, del, remove, rm Remove one or more images by reference tag Tag an image label Set and clear labels for an image convert Convert an image usage Display usage of snapshots for a given image ref OPTIONS: --help, -h show help 拉取镜像 # containerd支持oci标准的镜像，所以可以直接使用docker官方或dockerfile构建的镜像 需要注意的是，与docker不同，拉取镜像时要加上 ?docker.io/liarary\n# 镜像名不能简写 [root@localhost ~]# ctr images pull docker.io/library/nginx:alpine docker.io/library/nginx:alpine: resolved |++++++++++++++++++++++++++++++++++++++| index-sha256:3923f8de8d2214b9490e68fd6ae63ea604deddd166df2755b788bef04848b9bc: done |++++++++++++++++++++++++++++++++++++++| manifest-sha256:558b1480dc5c8f4373601a641c56b4fd24a77105d1246bd80b991f8b5c5dc0fc: done |++++++++++++++++++++++++++++++++++++++| layer-sha256:d7fb62c2e1cc7510e9c63402d02061002604b6ab79deab339ee8abf9f7452fde: done |++++++++++++++++++++++++++++++++++++++| config-sha256:01e5c69afaf635f66aab0b59404a0ac72db1e2e519c3f41a1ff53d37c35bba41: done |++++++++++++++++++++++++++++++++++++++| layer-sha256:c926b61bad3b94ae7351bafd0c184c159ebf0643b085f7ef1d47ecdc7316833c: done |++++++++++++++++++++++++++++++++++++++| layer-sha256:eb2797aa8e799e16f2a041cb7d709dc913519995a8a7dd22509d33c662612c5e: done |++++++++++++++++++++++++++++++++++++++| layer-sha256:47df6ca4b6bc8e8c42f5fcb7ce4d37737d68cb5fb5056a54605deb2b0d33415b: done |++++++++++++++++++++++++++++++++++++++| layer-sha256:5ea1ba8ab969c385f95c844167644f56aca56cc947548764033c92654d60a304: done |++++++++++++++++++++++++++++++++++++++| layer-sha256:6a4b140a5e7cbbec14bdbc3d9e7eced3b5f87652515c1cb65af5abeb53fc9fa8: done |++++++++++++++++++++++++++++++++++++++| layer-sha256:c99555e79d522323ea54a2e9e5c56c0bc5ed2fd7ffa16fa9cf06e5c231c15db8: done |++++++++++++++++++++++++++++++++++++++| layer-sha256:f9302969eafdbfd15516462ec6f9a8c8b537abb385d938e2fb154c23998c3851: done |++++++++++++++++++++++++++++++++++++++| elapsed: 8.1 s total: 16.1 M (2.0 MiB/s) unpacking linux/amd64 sha256:3923f8de8d2214b9490e68fd6ae63ea604deddd166df2755b788bef04848b9bc... done: 781.530658ms 指定平台\n\u0026ndash;all-platforms：所有平台（amd64 、arm、386 、ppc64le 等），不加的话下载当前平台架构 \u0026ndash;platform：指定linux/amd64平台 [root@localhost ~]# ctr images pull --all-platforms docker.io/library/nginx:alpine [root@localhost ~]# ctr images pull --platform linux/amd64 docker.io/library/nginx:alpine 查看镜像 # 查看镜像可以使用 i 简写或者image -q 只打印镜像名称 [root@localhost ~]# ctr images list REF TYPE DIGEST SIZE PLATFORMS LABELS docker.io/library/nginx:alpine application/vnd.docker.distribution.manifest.list.v2+json sha256:3923f8de8d2214b9490e68fd6ae63ea604deddd166df2755b788bef04848b9bc 17.1 MiB linux/386,linux/amd64,linux/arm/v6,linux/arm/v7,linux/arm64/v8,linux/ppc64le,linux/s390x - [root@localhost ~]# ctr i ls [root@localhost ~]# ctr i ls -q 检测镜像 # [root@localhost ~]# ctr images check REF TYPE DIGEST STATUS SIZE UNPACKED docker.io/library/nginx:alpine application/vnd.docker.distribution.manifest.list.v2+json sha256:3923f8de8d2214b9490e68fd6ae63ea604deddd166df2755b788bef04848b9bc complete (9/9) 17.1 MiB/17.1 MiB true 主要查看其中的 STATUS，complete 表示镜像是完整可用的状态。\n镜像挂载 # 把已下载的容器镜像挂载至当前文件系统，是为了方便查看镜像中包含的内容\n[root@localhost ]# ctr images mount docker.io/library/nginx:alpine /tmp sha256:01bb4fce3eb1b56b05adf99504dafd31907a5aadac736e36b27595c8b92f07f1 /tmp 查看挂载内容\n[root@localhost tmp]# ll /tmp/ 总用量 76 drwxr-xr-x 2 root root 4096 11月 30 17:32 bin drwxr-xr-x 2 root root 4096 11月 30 17:32 dev drwxr-xr-x 1 root root 4096 12月 1 15:02 docker-entrypoint.d -rwxrwxr-x 1 root root 1620 12月 1 15:02 docker-entrypoint.sh drwxr-xr-x 1 root root 4096 12月 1 15:02 etc drwxr-xr-x 2 root root 4096 11月 30 17:32 home drwxr-xr-x 1 root root 4096 12月 1 15:02 lib drwxr-xr-x 5 root root 4096 11月 30 17:32 media drwxr-xr-x 2 root root 4096 11月 30 17:32 mnt drwxr-xr-x 2 root root 4096 11月 30 17:32 opt dr-xr-xr-x 2 root root 4096 11月 30 17:32 proc drwx------ 2 root root 4096 11月 30 17:32 root drwxr-xr-x 2 root root 4096 11月 30 17:32 run drwxr-xr-x 2 root root 4096 11月 30 17:32 sbin drwxr-xr-x 2 root root 4096 11月 30 17:32 srv drwxr-xr-x 2 root root 4096 11月 30 17:32 sys drwxrwxrwt 1 root root 4096 12月 1 15:02 tmp drwxr-xr-x 1 root root 4096 11月 30 17:32 usr drwxr-xr-x 1 root root 4096 11月 30 17:32 var 卸载\n[root@localhost ]# ctr images unmount /tmp 镜像导入导出 # 导出 # 同时导出可以使用\u0026ndash;platform导出其它平台的(例如arm) \u0026ndash;all-platforms为导出所有平台 [root@localhost ]# ctr image export --all-platforms mynginx.tar docker.io/library/nginx:alpine 导入 # 删除本地镜像 [root@localhost ]# ctr images rm docker.io/library/nginx:alpine 导入 [root@localhost ]# ctr images import mynginx.tar unpacking docker.io/library/nginx:alpine (sha256:3923f8de8d2214b9490e68fd6ae63ea604deddd166df2755b788bef04848b9bc)...done 导入所有平台 [root@localhost ]# ctr images import --all-platforms mynginx.tar unpacking docker.io/library/nginx:alpine (sha256:3923f8de8d2214b9490e68fd6ae63ea604deddd166df2755b788bef04848b9bc)...done 导入可能会出现类似于 ctr: content digest sha256:xxxxxx not found 的错误，要解决这个办法需要 pull 所有平台镜像：\n[root@localhost ]# ctr i pull --all-platforms docker.io/library/nginx:alpine [root@localhost ]# ctr i export --all-platforms mynginx.tar docker.io/library/nginx:alpine [root@localhost ]# ctr i rm docker.io/library/nginx:alpine [root@localhost ]# ctr i import mynginx.tar 删除镜像 # [root@localhost ]# ctr images rm docker.io/library/nginx:alpine 镜像打标签 # [root@localhost ~]# ctr images tag docker.io/library/nginx:alpine docker.io/bongmu/nginx:alpine 推送镜像 # [root@localhost ~]# ctr images push docker.io/bongmu/nginx:alpine index-alpine@sha256:3923f8de8d2214b9490e68fd6ae63ea604deddd166df2755b788bef04848b9bc: done |++++++++++++++++++++++++++++++++++++++| manifest-sha256:bf575af9dafa90e26c6827b3b9cb2f87900a2a67a899d0fc01023e992cadbbde: done |++++++++++++++++++++++++++++++++++++++| manifest-sha256:558b1480dc5c8f4373601a641c56b4fd24a77105d1246bd80b991f8b5c5dc0fc: done |++++++++++++++++++++++++++++++++++++++| manifest-sha256:4937f49e504c3b0a082eec0c9c336315ba10dbd2fa1800575f84a0a69ee9a4d4: done |++++++++++++++++++++++++++++++++++++++| manifest-sha256:9f6dc3ad07da76ccf600ee39d0524266bbef01414a31466ef67d0b172448c531: done |++++++++++++++++++++++++++++++++++++++| manifest-sha256:18d2bb20c22e511b92a3ec81f553edfcaeeb74fd1c96a92c56a6c4252c75eec7: done |++++++++++++++++++++++++++++++++++++++| manifest-sha256:c2b754e66172ca0ca15b9ee2290a2e1b921ceb0a643019a49aa9c68a1dedf4dc: done |++++++++++++++++++++++++++++++++++++++| manifest-sha256:aed771cbe3afa57b9ba1fbdd83a45b35792ae08046db29dd306fbc411bfc9396: done |++++++++++++++++++++++++++++++++++++++| config-sha256:92b9e48381c41c7ec99a82b7056f6a194606b241788ca752f0496aa07b538d9d: done |++++++++++++++++++++++++++++++++++++++| config-sha256:7cae28957771c6e97257fd5e25334c9c3d80fe07781688c7a14f2bb63790f54a: done |++++++++++++++++++++++++++++++++++++++| config-sha256:0b76d6a71b9e5058ce30cad1e0e03fad54a2d6a31fa0d539312fcce133937b6a: done |++++++++++++++++++++++++++++++++++++++| config-sha256:01e5c69afaf635f66aab0b59404a0ac72db1e2e519c3f41a1ff53d37c35bba41: done |++++++++++++++++++++++++++++++++++++++| config-sha256:561adbe2d8b3a9ad5ef35d0661133a4fa0fb665e3d1a75a0c16333ae9494ff90: done |++++++++++++++++++++++++++++++++++++++| config-sha256:f09fc93534f6a80e1cb9ad70fe8c697b1596faa9f1b50895f203bc02feb9ebb8: done |++++++++++++++++++++++++++++++++++++++| config-sha256:aa2502e079292fd7c12af367afba401c53dfe37d4eec74693a8dda264be75d70: done |++++++++++++++++++++++++++++++++++++++| elapsed: 4.5 s 推送镜像到带认证的镜像仓库\n[root@localhost ~]# ctr images push --user bongmu:xx docker.io/bongmu/nginx:alpine 命名空间操作 # containerd相比于docker，多了namespace概念，每个image和containe都会在各自的namespace下可见。\n[root@localhost ~]# ctr namespace -h NAME: ctr namespaces - Manage namespaces USAGE: ctr namespaces command [command options] [arguments...] COMMANDS: create, c Create a new namespace list, ls List namespaces remove, rm Remove one or more namespaces label Set and clear labels for a namespace OPTIONS: --help, -h show help 查看命名空间 # [root@localhost ~]# ctr namespaces ls NAME LABELS default 创建命名空间 # [root@localhost ~]# ctr namespaces create test 删除命名空间 # [root@localhost ~]# ctr namespaces rm test 默认containerd会使用default命名空间，Kubernetes 下使用的 containerd 默认命名空间是 ?k8s.io，Docker 使用的 containerd 下面的命名空间默认是 moby。下载镜像的时候命名空间只如果不指定，默认是看不到，这点和k8s namespace作用相同。有了命名空间后就可以在操作资源的时候指定 namespace。\n[root@localhost ~]# ctr -n test images pull --all-platforms docker.io/library/nginx:alpine 查看命名空间镜像 # [root@localhost ~]# ctr images ls REF TYPE DIGEST SIZE PLATFORMS LABELS [root@localhost ~]# [root@localhost ~]# ctr -n test images ls REF TYPE DIGEST SIZE PLATFORMS LABELS docker.io/library/nginx:alpine application/vnd.docker.distribution.manifest.list.v2+json sha256:3923f8de8d2214b9490e68fd6ae63ea604deddd166df2755b788bef04848b9bc 17.1 MiB linux/386,linux/amd64,linux/arm/v6,linux/arm/v7,linux/arm64/v8,linux/ppc64le,linux/s390x - 下一篇文章将介绍Containerd容器管理，请尽情期待！别忘了留个关注哟！\n"},{"id":73,"href":"/docs/coredns%E4%B9%8B%E5%85%89-coredns-zhi-guang/","title":"COREDNS之光 2024-04-03 14:54:52.059","section":"Docs","content":"在 Kubernetes 中，DNS 名称被分配给 Pod 和服务，以便通过名称而不是 IP 地址进行通信。\n集群内DNS解析默认使用的域名为cluster.local，可以根据需要自定义。Service 的 DNS 名称遵循\u0026lt;service-name\u0026gt;.\u0026lt;namespace\u0026gt;.svc.cluster.local格式，而Pod 的 DNS 名称遵循\u0026lt;pod-ip-address-replace-dot-with-hyphen\u0026gt;.\u0026lt;namespace\u0026gt;.pod.cluster.local格式。\nCoreDNS 的运行基于名为Corefile的配置文件，该文件指定 DNS 服务器应如何运行并响应传入请求。\nKubernetes：DNS\nDNS解析 # 在 Kubernetes 中，分配给 Pod 和服务的DNS 名称用集群内的名称解析，允许Pod 和服务通过名称而不是 IP 地址相互通信。\n行万里路，此处相逢，共话云原生之道。 偶逗趣事，明月清风，与君同坐。\n默认域名：cluster.local # 在 Kubernetes 中，cluster.local是集群内用于 DNS 解析的默认域名。当对同一命名空间内的服务或 Pod 进行 DNS 查询时，Kubernetes DNS 服务会将命名空间和cluster.local后缀附加到名称中，以形成完全限定域名 (FQDN)。虽然它是默认域名，但如果需要，可以自定义使用不同的域名。\nService的 DNS 名称 # Kubernetes 中服务的 DNS 名称遵循以下格式：\nService的 DNS 名称\n\u0026lt;service-name\u0026gt;.\u0026lt;namespace\u0026gt;.svc.cluster.local service-name的是Service的名称，而namespace表示Service运行的命名空间。\n例如，如果一个名为的服务my-service正在my-namespace命名空间中运行，则相应的 DNS 名称将为：\nmy-service.my-namespace.svc.cluster.local Pod 的 DNS 名称 # Kubernetes 中 Pod 的 DNS 名称遵循以下格式：\nPod 的 DNS 名称\n\u0026lt;pod-ip-address-replace-dot-with-hyphen\u0026gt;.\u0026lt;namespace\u0026gt;.pod.cluster.local pod-ip-address-replace-dot-with-hyphen是 Kubernetes 分配给 Pod 的 IP 地址，其中点由连字符替换。namespace是Pod 运行所在的命名空间。\n例如，如果具有10.1.2.3 IP 地址的 Pod在my-namespace命名空间中运行，则其 DNS 名称将为：\n10-1-2-3.my-namespace.pod.cluster.local 使用 DNS 名称进行 Pod 和服务交互 # 同一命名空间中的 Pod 和服务 # 当 Pod 和 Services 位于同一命名空间时，你可以使用服务名称而不是完全限定域名 (FQDN)通过查询访问 Services。\n同一命名空间中的 Pod 和服务\n不同命名空间中的 Pod 和服务 # 当 Kubernetes 中 Pod 和 Service 位于不同的命名空间时，需要同时指定Service 名称和命名空间，才能从 Pod 访问 Service。\n不同命名空间中的 Pod 和服务\nCoreDNS # 在引入 CoreDNS 之前，Kubernetes 使用kube-dns作为默认的 DNS 解决方案。DNS 服务器处理集群中服务和 Pod 的 DNS 请求。根据Kubernetes 官方文档，从 1.11 版本开始，推荐使用 CoreDNS 作为默认 DNS 解决方案，并默认与 kubeadm 一起安装。\n配置文件：核心文件 # Corefile 是一个文本文件，指定 DNS 服务器应如何操作和响应传入请求。\n核心文件示例：\n.:53 { errors health { lameduck 5s } ready kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa ttl 30 } prometheus :9153 forward . /etc/resolv.conf cache 30 loop reload loadbalance } 集群的顶级域名在Kubernetes 插件配置中指定为“ cluster.local ” 。该插件还配置为使用 in-addr.arpa 和 ip6.arpa 域处理 IPv4 和 IPv6 地址的反向 DNS 查找。\n要了解有关 Corefile 及其语法的更多信息，请参阅CoreDNS Manual或CoreDNS ConfigMap options中提供的官方文档。\nCoreDNS 指标 # DNS服务器将记录存储在其数据库中并使用该数据库回答域名查询。如果 DNS 服务器没有此数据，它会尝试从其他 DNS 服务器寻找解决方案。\nDNS 解析是任何应用程序的基本要求，因此你需要确保其正常工作。我们建议查看dns-debugging-resolution故障排除指南，并确保你的 CoreDNS 已配置并正确运行。\n默认情况下，当你配置集群时，你应该始终有一个仪表板来观察关键的 CoreDNS 指标。为了获取 CoreDNS 指标，你应该在 CoreDNS 配置中启用Prometheus 插件。\n下面的示例配置使用prometheus插件启用从 CoreDNS 实例收集指标。\n.:53 { errors health kubernetes cluster.local in-addr.arpa ip6.arpa { pods verified fallthrough in-addr.arpa ip6.arpa } prometheus :9153 forward . /etc/resolv.conf cache 30 loop reload loadbalance } 以下是我们建议在你的仪表板中包含的关键指标。如果你使用 Prometheus、DataDog、Kibana 等，你可能会发现可以使用来自社区/提供商的仪表板模板。\n• 缓存命中百分比：使用 CoreDNS 缓存响应的请求的百分比 • DNS 请求延迟 - CoreDNS：CoreDNS 处理 DNS 请求所花费的时间 - 上游服务器：处理转发到上游的 DNS 请求所花费的时间 • 转发到上游服务器的请求数 • 请求的错误代码 - NXDomain：不存在的域 - FormErr：DNS 请求中的格式错误 - ServFail：服务器故障 - NoError：无错误，已成功处理请求 • CoreDNS 资源使用情况：服务器消耗的不同资源，如内存、CPU 等。 我们使用 DataDog 进行特定应用程序监控。以下只是我使用 DataDog 构建的用于分析的示例仪表板。\nCoreDNS 指标\n减少 DNS 搜索 # 当我们开始深入了解应用程序如何向 CoreDNS 发出请求时，我们观察到大多数出站请求是通过应用程序发送到外部 API 服务器的。\n这通常是 resolv.conf 在应用程序部署 Pod 中的外观。\nnameserver 10.100.0.10 search kube-namespace.svc.cluster.local svc.cluster.local cluster.local us-west-2.compute.internal options ndots:5 如果你了解 Kubernetes 如何尝试解析 FQDN，就知道它会尝试在不同级别进行 DNS 查找。\n考虑到上述 DNS 配置，当 DNS 解析器向 CoreDNS 服务器发送查询时，它会尝试考虑搜索路径来搜索域。\n如果我们正在寻找域名 boktube.io，它将进行以下查询，并在最后一个查询中收到成功的响应。\nbotkube.io.kube-namespace.svc.cluster.local \u0026lt;= NXDomain botkube.io.svc.cluster.local \u0026lt;= NXDomain boktube.io.cluster.local \u0026lt;= NXDomain botkube.io.us-west-2.compute.internal \u0026lt;= NXDomain botkube.io \u0026lt;= NoERROR 由于我们进行了太多的外部查找，因此我们收到了大量针对 DNS 搜索的 NXDomain 响应。为了优化这一点，我们spec.template.spec.dnsConfig在 Deployment 对象中进行了自定义。变化是这样的：\ndnsPolicy: ClusterFirst dnsConfig: options: - name: ndots value: \u0026#34;1\u0026#34; 通过上述更改，pod 上的resolve.conf 发生了更改。仅针对外部域执行搜索。这减少了对 DNS 服务器的查询数量。这也有助于减少应用程序的 5xx 错误。你可以在下图中注意到 NXDomain 响应计数的差异。\n减少 DNS 搜索\n对于这个问题，一个更好的解决方案是Kubernetes 1.18+ 引入的节点级缓存。\n定制 CoreDNS # 我们可以通过插件来定制CoreDNS。Kubernetes 支持不同类型的工作负载，标准 CoreDNS 配置可能无法满足你的所有需求。CoreDNS 有几个插件。根据你在集群上运行的工作负载类型（假设相互通信的应用程序或在 Kubernetes 集群外部交互的独立应用程序），你尝试解析的 FQDN 类型可能会有所不同。\n假设你在特定的公共/私有云中运行 Kubernetes，并且大多数 DNS 支持的应用程序都在同一云中。在这种情况下，CoreDNS 还提供特定的云相关或通用插件，可用于扩展 DNS 区域记录。\n是否在 Kubernetes 集群中运行适当数量的 CoreDNS 实例是需要做出决定的关键因素之一。建议运行至少两个 CoreDNS 服务器实例，以更好地保证 DNS 请求得到服务。根据所服务的请求数量、请求性质、集群上运行的工作负载数量以及集群大小，你可能需要添加额外的 CoreDNS 实例或为集群配置 HPA。\n"},{"id":74,"href":"/docs/dockerfile%E5%AE%9A%E5%88%B6%E4%B8%93%E5%B1%9E%E9%95%9C%E5%83%8F-dockerfile-ding-zhi-zhuan-shu-jing-xiang/","title":"dockerfile定制专属镜像 2024-08-02 17:44:00.522","section":"Docs","content":" 前言 # 大家好，本文是对 Docker 自定义镜像的详细讲解，讲解了如何进行构建自己的 Docker 镜像以及 Dockerfile 的操作指令。希望对大家有所帮助~\n一、使用 Dockerfile 定制镜像 # 1.1、Dockerfile 定制镜像 # 镜像的定制实际上就是定制每一层所添加的配置、文件。如果我们可以把每一层修改、安装、构建、操作的命令都写入一个脚本，用这个脚本来构建、定制镜像，无法重复的问题、镜像构建透明性的问题、体积的问题就都会解决。这个脚本就是 Dockerfile。\nDockerfile 是一个文本文件，其内包含了一条条的指令(Instruction)，每一条指令构建一层，因此每一条指令的内容，就是描述该层应当如何构建。\n以 nginx 镜像为例，这次我们使用 Dockerfile 来定制。\n在一个空白目录中，建立一个文本文件，并命名为 Dockerfile：\n$ mkdir mynginx $ cd mynginx $ touch Dockerfile 其内容为：\nFROM nginx RUN echo \u0026#39;\u0026lt;h1\u0026gt;Hello, Docker!\u0026lt;/h1\u0026gt;\u0026#39; \u0026gt; /usr/share/nginx/html/index.html 这个 Dockerfile 很简单，一共就两行。涉及到了两条指令，FROM 和 RUN。\n1.2、FROM 指定基础镜像 # 所谓定制镜像，那一定是以一个镜像为基础，在其上进行定制。就像我们之前运行了一个 nginx 镜像的容器，再进行修改一样，基础镜像是必须指定的。而 FROM 就是指定基础镜像，因此一个 Dockerfile 中 FROM是必备的指令，并且必须是第一条指令。\n在 Docker Store 上有非常多的高质量的官方镜像，有可以直接拿来使用的服务类的镜像，如 nginx、redis、mongo、mysql、httpd、php、tomcat 等；也有一些方便开发、构建、运行各种语言应用的镜像，如node、openjdk、python、ruby、golang等。可以在其中寻找一个最符合我们最终目标的镜像为基础镜像进行定制。\n如果没有找到对应服务的镜像，官方镜像中还提供了一些更为基础的操作系统镜像，如ubuntu、debian、centos、fedora、alpine 等，这些操作系统的软件库为我们提供了更广阔的扩展空间。\n除了选择现有镜像为基础镜像外，Docker 还存在一个特殊的镜像，名为 scratch。这个镜像是虚拟的概念，并不实际存在，它表示一个空白的镜像。\nFROM scratch ... 如果你以 scratch 为基础镜像的话，意味着你不以任何镜像为基础，接下来所写的指令将作为镜像第一层开始存在。\n不以任何系统为基础，直接将可执行文件复制进镜像的做法并不罕见，比如swarm、coreos/etcd。对于 Linux 下静态编译的程序来说，并不需要有操作系统提供运行时支持，所需的一切库都已经在可执行文件里了，因此直接 FROM scratch 会让镜像体积更加小巧。使用 Go 语言开发的应用很多会使用这种方式来制作镜像，这也是为什么有人认为 Go 是特别适合容器微服务架构的语言的原因之一。\n1.3、RUN 执行命令 # RUN 指令是用来执行命令行命令的。由于命令行的强大能力，RUN 指令在定制镜像时是最常用的指令之一。其格式有两种：\nshell 格式：RUN \u0026lt;命令\u0026gt;，就像直接在命令行中输入的命令一样。刚才写的 Dockerfile 中的 RUN 指令就是这种格式。 RUN echo \u0026#39;\u0026lt;h1\u0026gt;Hello, Docker!\u0026lt;/h1\u0026gt;\u0026#39; \u0026gt; /usr/share/nginx/html/index.html exec 格式：RUN [\u0026quot;可执行文件\u0026quot;, \u0026quot;参数1\u0026quot;, \u0026quot;参数2\u0026quot;]，这更像是函数调用中的格式。 既然 RUN 就像 Shell 脚本一样可以执行命令，那么我们是否就可以像 Shell 脚本一样把每个命令对应一个 RUN 呢？比如这样：\nFROM debian:jessie RUN apt-get update RUN apt-get install -y gcc libc6-dev make RUN wget -O redis.tar.gz \u0026#34;http://download.redis.io/releases/redis-3.2.5.tar.gz\u0026#34; RUN mkdir -p /usr/src/redis RUN tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1 RUN make -C /usr/src/redis RUN make -C /usr/src/redis install 之前说过，Dockerfile 中每一个指令都会建立一层，RUN 也不例外。每一个 RUN 的行为，就和刚才我们手工建立镜像的过程一样：新建立一层，在其上执行这些命令，执行结束后，commit 这一层的修改，构成新的镜像。\n而上面的这种写法，创建了 7 层镜像。这是完全没有意义的，而且很多运行时不需要的东西，都被装进了镜像里，比如编译环境、更新的软件包等等。结果就是产生非常臃肿、非常多层的镜像，不仅仅增加了构建部署的时间，也很容易出错。这是很多初学 Docker 的人常犯的一个错误。\nUnion FS 是有最大层数限制的，比如 AUFS，曾经是最大不能超过 42 层，现在是不能超过 127 层。\n上面的 Dockerfile 正确的写法应该是这样：\nFROM debian:jessie RUN buildDeps=\u0026#39;gcc libc6-dev make\u0026#39; \\ \u0026amp;\u0026amp; apt-get update \\ \u0026amp;\u0026amp; apt-get install -y $buildDeps \\ \u0026amp;\u0026amp; wget -O redis.tar.gz \u0026#34;http://download.redis.io/releases/redis-3.2.5.tar.gz\u0026#34; \\ \u0026amp;\u0026amp; mkdir -p /usr/src/redis \\ \u0026amp;\u0026amp; tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1 \\ \u0026amp;\u0026amp; make -C /usr/src/redis \\ \u0026amp;\u0026amp; make -C /usr/src/redis install \\ \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* \\ \u0026amp;\u0026amp; rm redis.tar.gz \\ \u0026amp;\u0026amp; rm -r /usr/src/redis \\ \u0026amp;\u0026amp; apt-get purge -y --auto-remove $buildDeps 首先，之前所有的命令只有一个目的，就是编译、安装 Redis 可执行文件。因此没有必要建立很多层，这只是一层的事情。因此，这里没有使用很多个 RUN 对一一对应不同的命令，而是仅仅使用一个 RUN 指令，并使用 \u0026amp;\u0026amp; 将各个所需命令串联起来。将之前的 7 层，简化为了 1 层。在撰写 Dockerfile 的时候，要经常提醒自己，这并不是在写 Shell 脚本，而是在定义每一层该如何构建。\n并且，这里为了格式化还进行了换行。Dockerfile 支持 Shell 类的行尾添加 \\ 的命令换行方式，以及行首 # 进行注释的格式。良好的格式，比如换行、缩进、注释等，会让维护、排障更为容易，这是一个比较好的习惯。\n此外，还可以看到这一组命令的最后添加了清理工作的命令，删除了为了编译构建所需要的软件，清理了所有下载、展开的文件，并且还清理了 apt 缓存文件。这是很重要的一步，我们之前说过，镜像是多层存储，每一层的东西并不会在下一层被删除，会一直跟随着镜像。因此镜像构建时，一定要确保每一层只添加真正需要添加的东西，任何无关的东西都应该清理掉。\n很多人初学 Docker 制作出了很臃肿的镜像的原因之一，就是忘记了每一层构建的最后一定要清理掉无关文件。\n1.4、构建镜像 # 再回到之前定制的 Nginx 镜像的 Dockerfile 来。现在我们明白了这个 Dockerfile 的内容，那么让我们来构建这个镜像吧。\n在 Dockerfile 文件所在目录执行：\n$ docker build -t nginx:v3 . Sending build context to Docker daemon 2.048 kB Step 1 : FROM nginx ---\u0026gt; e43d811ce2f4 Step 2 : RUN echo \u0026#39;\u0026lt;h1\u0026gt;Hello, Docker!\u0026lt;/h1\u0026gt;\u0026#39; \u0026gt; /usr/share/nginx/html/index.html ---\u0026gt; Running in 9cdc27646c7b ---\u0026gt; 44aa4490ce2c Removing intermediate container 9cdc27646c7b Successfully built 44aa4490ce2c 从命令的输出结果中，我们可以清晰的看到镜像的构建过程。在 Step 2中，如同我们之前所说的那样，RUN 指令启动了一个容器 9cdc27646c7b，执行了所要求的命令，并最后提交了这一层 44aa4490ce2c，随后删除了所用到的这个容器 9cdc27646c7b。\n这里我们使用了 docker build 命令进行镜像构建。其格式为：\ndocker build [选项] \u0026lt;上下文路径/URL/-\u0026gt; 在这里我们指定了最终镜像的名称 -t nginx:v3，构建成功后，我们可以像之前运行 nginx:v2 那样来运行这个镜像，其结果会和 nginx:v2一样。\n1.5、镜像构建上下文（Context） # 如果注意，会看到 docker build 命令最后有一个 .，. 表示当前目录，而 Dockerfile 就在当前目录，因此不少初学者以为这个路径是在指定 Dockerfile 所在路径，这么理解其实是不准确的。如果对应上面的命令格式，你可能会发现，这是在指定上下文路径。那么什么是上下文呢？\n首先我们要理解 docker build 的工作原理。Docker 在运行时分为 Docker 引擎（也就是服务端守护进程）和客户端工具。Docker 的引擎提供了一组 REST API，被称为 Docker Remote API，而如 docker 命令这样的客户端工具，则是通过这组 API 与 Docker 引擎交互，从而完成各种功能。因此，虽然表面上我们好像是在本机执行各种 docker 功能，但实际上，一切都是使用的远程调用形式在服务端（Docker 引擎）完成。也因为这种 C/S 设计，让我们操作远程服务器的 Docker 引擎变得轻而易举。\n当我们进行镜像构建的时候，并非所有定制都会通过 RUN 指令完成，经常会需要将一些本地文件复制进镜像，比如通过 COPY 指令、ADD 指令等。而 docker build 命令构建镜像，其实并非在本地构建，而是在服务端，也就是 Docker 引擎中构建的。那么在这种客户端/服务端的架构中，如何才能让服务端获得本地文件呢？\n这就引入了上下文的概念。当构建的时候，用户会指定构建镜像上下文的路径，docker build 命令得知这个路径后，会将路径下的所有内容打包，然后上传给 Docker 引擎。这样 Docker 引擎收到这个上下文包后，展开就会获得构建镜像所需的一切文件。\n如果在 Dockerfile 中这么写：\nCOPY ./package.json /app/ 这并不是要复制执行 docker build 命令所在的目录下的 package.json，也不是复制 Dockerfile 所在目录下的 package.json，而是复制 上下文（context） 目录下的 package.json。\n因此，COPY 这类指令中的源文件的路径都是相对路径。这也是初学者经常会问的为什么 COPY ../package.json /app 或者 COPY /opt/xxxx /app 无法工作的原因，因为这些路径已经超出了上下文的范围，Docker 引擎无法获得这些位置的文件。如果真的需要那些文件，应该将它们复制到上下文目录中去。\n现在就可以理解刚才的命令 docker build -t nginx:v3 . 中的这个 .，实际上是在指定上下文的目录，docker build 命令会将该目录下的内容打包交给 Docker 引擎以帮助构建镜像。\n如果观察 docker build 输出，我们其实已经看到了这个发送上下文的过程：\n$ docker build -t nginx:v3 . Sending build context to Docker daemon 2.048 kB ... 理解构建上下文对于镜像构建是很重要的，避免犯一些不应该的错误。比如有些初学者在发现 COPY /opt/xxxx /app 不工作后，于是干脆将 Dockerfile 放到了硬盘根目录去构建，结果发现 docker build 执行后，在发送一个几十 GB 的东西，极为缓慢而且很容易构建失败。那是因为这种做法是在让 docker build 打包整个硬盘，这显然是使用错误。\n一般来说，应该会将 Dockerfile 置于一个空目录下，或者项目根目录下。如果该目录下没有所需文件，那么应该把所需文件复制一份过来。如果目录下有些东西确实不希望构建时传给 Docker 引擎，那么可以用 .gitignore 一样的语法写一个 .dockerignore，该文件是用于剔除不需要作为上下文传递给 Docker 引擎的。\n那么为什么会有人误以为 . 是指定 Dockerfile 所在目录呢？这是因为在默认情况下，如果不额外指定 Dockerfile 的话，会将上下文目录下的名为 Dockerfile 的文件作为 Dockerfile。\n这只是默认行为，实际上 Dockerfile 的文件名并不要求必须为 Dockerfile，而且并不要求必须位于上下文目录中，比如可以用 -f ../Dockerfile.php 参数指定某个文件作为 Dockerfile。\n当然，一般大家习惯性的会使用默认的文件名 Dockerfile，以及会将其置于镜像构建上下文目录中\nDocker+K8s+Jenkins 主流技术全解视频资料【干货免费分享】\n1.6、其他 docker build 的用法 # 1.6.1、直接用 Git repo 进行构建 # docker build 还支持从 URL 构建，比如可以直接从 Git repo 中构建：\n$ docker build https://github.com/twang2218/gitlab-ce-zh.git#:8.14 docker build https://github.com/twang2218/gitlab-ce-zh.git\\#:8.14 Sending build context to Docker daemon 2.048 kB Step 1 : FROM gitlab/gitlab-ce:8.14.0-ce.0 8.14.0-ce.0: Pulling from gitlab/gitlab-ce aed15891ba52: Already exists 773ae8583d14: Already exists ... 这行命令指定了构建所需的 Git repo，并且指定默认的 master 分支，构建目录为 /8.14/，然后 Docker 就会自己去 git clone 这个项目、切换到指定分支、并进入到指定目录后开始构建。\n1.6.2、用给定的 tar 压缩包构建 # $ docker build http://server/context.tar.gz 如果所给出的 URL 不是个 Git repo，而是个 tar 压缩包，那么 Docker 引擎会下载这个包，并自动解压缩，以其作为上下文，开始构建。\n1.6.3、从标准输入中读取 Dockerfile 进行构建 # docker build - \u0026lt; Dockerfile 或\ncat Dockerfile | docker build - 如果标准输入传入的是文本文件，则将其视为 Dockerfile，并开始构建。这种形式由于直接从标准输入中读取 Dockerfile 的内容，它没有上下文，因此不可以像其他方法那样可以将本地文件 COPY 进镜像之类的事情。\n1.6.4、从标准输入中读取上下文压缩包进行构建 # $ docker build - \u0026lt; context.tar.gz 如果发现标准输入的文件格式是 gzip、bzip2 以及 xz 的话，将会使其为上下文压缩包，直接将其展开，将里面视为上下文，并开始构建。\n二、Dockerfile 指令 # 我们已经介绍了 FROM，RUN，还提及了 COPY, ADD，其实 Dockerfile 功能很强大，它提供了十多个指令。下面我们继续讲解其他的指令。\n2.1、COPY # 格式：\nCOPY \u0026lt;源路径\u0026gt;... \u0026lt;目标路径\u0026gt; COPY [\u0026quot;\u0026lt;源路径1\u0026gt;\u0026quot;,... \u0026quot;\u0026lt;目标路径\u0026gt;\u0026quot;] 和 RUN 指令一样，也有两种格式，一种类似于命令行，一种类似于函数调用。\nCOPY 指令将从构建上下文目录中 \u0026lt;源路径\u0026gt; 的文件/目录复制到新的一层的镜像内的 \u0026lt;目标路径\u0026gt; 位置。比如：\nCOPY package.json /usr/src/app/ \u0026lt;源路径\u0026gt; 可以是多个，甚至可以是通配符，其通配符规则要满足 Go 的 filepath.Match 规则，如：\nCOPY hom* /mydir/ COPY hom?.txt /mydir/ \u0026lt;目标路径\u0026gt; 可以是容器内的绝对路径，也可以是相对于工作目录的相对路径（工作目录可以用 WORKDIR 指令来指定）。目标路径不需要事先创建，如果目录不存在会在复制文件前先行创建缺失目录。\n此外，还需要注意一点，使用 COPY 指令，源文件的各种元数据都会保留。比如读、写、执行权限、文件变更时间等。这个特性对于镜像定制很有用。特别是构建相关文件都在使用 Git 进行管理的时候。\n2.2、ADD # ADD 指令和 COPY 的格式和性质基本一致。但是在 COPY 基础上增加了一些功能。\n比如 \u0026lt;源路径\u0026gt; 可以是一个 URL，这种情况下，Docker 引擎会试图去下载这个链接的文件放到 \u0026lt;目标路径\u0026gt; 去。下载后的文件权限自动设置为 600，如果这并不是想要的权限，那么还需要增加额外的一层 RUN 进行权限调整，另外，如果下载的是个压缩包，需要解压缩，也一样还需要额外的一层 RUN 指令进行解压缩。所以不如直接使用 RUN 指令，然后使用 wget 或者 curl 工具下载，处理权限、解压缩、然后清理无用文件更合理。因此，这个功能其实并不实用，而且不推荐使用。\n如果 \u0026lt;源路径\u0026gt; 为一个 tar 压缩文件的话，压缩格式为 gzip, bzip2以及 xz 的情况下，ADD 指令将会自动解压缩这个压缩文件到 \u0026lt;目标路径\u0026gt; 去。\n在某些情况下，这个自动解压缩的功能非常有用，比如官方镜像 ubuntu中：\nFROM scratch ADD ubuntu-xenial-core-cloudimg-amd64-root.tar.gz / ... 但在某些情况下，如果我们真的是希望复制个压缩文件进去，而不解压缩，这时就不可以使用 ADD 命令了。\n在 Docker 官方的 Dockerfile 最佳实践文档 中要求，尽可能的使用 COPY，因为 COPY 的语义很明确，就是复制文件而已，而 ADD 则包含了更复杂的功能，其行为也不一定很清晰。最适合使用 ADD 的场合，就是所提及的需要自动解压缩的场合。\n另外需要注意的是，ADD 指令会令镜像构建缓存失效，从而可能会令镜像构建变得比较缓慢。\n因此在 COPY 和 ADD 指令中选择的时候，可以遵循这样的原则，所有的文件复制均使用 COPY 指令，仅在需要自动解压缩的场合使用 ADD。\n2.3、CMD # CMD 指令的格式和 RUN 相似，也是两种格式：\nshell 格式：CMD \u0026lt;命令\u0026gt; exec 格式：CMD [\u0026quot;可执行文件\u0026quot;, \u0026quot;参数1\u0026quot;, \u0026quot;参数2\u0026quot;...] 参数列表格式：CMD [\u0026quot;参数1\u0026quot;, \u0026quot;参数2\u0026quot;...]。在指定了 ENTRYPOINT指令后，用 CMD 指定具体的参数。 之前介绍容器的时候曾经说过，Docker 不是虚拟机，容器就是进程。既然是进程，那么在启动容器的时候，需要指定所运行的程序及参数。CMD 指令就是用于指定默认的容器主进程的启动命令的。\n在运行时可以指定新的命令来替代镜像设置中的这个默认命令，比如，ubuntu 镜像默认的 CMD 是 /bin/bash，如果我们直接 docker run -it ubuntu 的话，会直接进入 bash。我们也可以在运行时指定运行别的命令，如 docker run -it ubuntu cat /etc/os-release。这就是用 cat /etc/os-release 命令替换了默认的 /bin/bash 命令了，输出了系统版本信息。\n在指令格式上，一般推荐使用 exec 格式，这类格式在解析时会被解析为 JSON 数组，因此一定要使用双引号 \u0026quot;，而不要使用单引号。\n如果使用 shell 格式的话，实际的命令会被包装为 sh -c 的参数的形式进行执行。比如：\nCMD echo $HOME 在实际执行中，会将其变更为：\nCMD [ \u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;echo $HOME\u0026#34; ] 这就是为什么我们可以使用环境变量的原因，因为这些环境变量会被 shell 进行解析处理。\n提到 CMD 就不得不提容器中应用在前台执行和后台执行的问题。这是初学者常出现的一个混淆。\nDocker 不是虚拟机，容器中的应用都应该以前台执行，而不是像虚拟机、物理机里面那样，用 upstart/systemd 去启动后台服务，容器内没有后台服务的概念。\n一些初学者将 CMD 写为：\nCMD service nginx start 然后发现容器执行后就立即退出了。甚至在容器内去使用 systemctl 命令结果却发现根本执行不了。这就是因为没有搞明白前台、后台的概念，没有区分容器和虚拟机的差异，依旧在以传统虚拟机的角度去理解容器。\n对于容器而言，其启动程序就是容器应用进程，容器就是为了主进程而存在的，主进程退出，容器就失去了存在的意义，从而退出，其它辅助进程不是它需要关心的东西。\n而使用 service nginx start 命令，则是希望 upstart 来以后台守护进程形式启动 nginx 服务。而刚才说了 CMD service nginx start 会被理解为 CMD [ \u0026quot;sh\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;service nginx start\u0026quot;]，因此主进程实际上是 sh。那么当 service nginx start 命令结束后，sh 也就结束了，sh 作为主进程退出了，自然就会令容器退出。\n正确的做法是直接执行 nginx 可执行文件，并且要求以前台形式运行。比如：\nCMD [\u0026#34;nginx\u0026#34;, \u0026#34;-g\u0026#34;, \u0026#34;daemon off;\u0026#34;] 2.4、ENTRYPOINT # ENTRYPOINT 的格式和 RUN 指令格式一样，分为 exec 格式和 shell格式。\nENTRYPOINT 的目的和 CMD 一样，都是在指定容器启动程序及参数。ENTRYPOINT 在运行时也可以替代，不过比 CMD 要略显繁琐，需要通过 docker run 的参数 --entrypoint 来指定。\n当指定了 ENTRYPOINT 后，CMD 的含义就发生了改变，不再是直接的运行其命令，而是将 CMD 的内容作为参数传给 ENTRYPOINT 指令，换句话说实际执行时，将变为：\n\u0026lt;ENTRYPOINT\u0026gt; \u0026#34;\u0026lt;CMD\u0026gt;\u0026#34; 那么有了 CMD 后，为什么还要有 ENTRYPOINT 呢？这种 \u0026lt;ENTRYPOINT\u0026gt; \u0026quot;\u0026lt;CMD\u0026gt;\u0026quot; 有什么好处么？让我们来看几个场景。\n2.4.1、场景一：让镜像变成像命令一样使用 # 假设我们需要一个得知自己当前公网 IP 的镜像，那么可以先用 CMD 来实现：\nFROM ubuntu:16.04 RUN apt-get update \\ \u0026amp;\u0026amp; apt-get install -y curl \\ \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* CMD [ \u0026#34;curl\u0026#34;, \u0026#34;-s\u0026#34;, \u0026#34;http://ip.cn\u0026#34; ] 假如我们使用 docker build -t myip . 来构建镜像的话，如果我们需要查询当前公网 IP，只需要执行：\n$ docker run myip 当前 IP：160.155.224.xx 来自：XX市 联通 这么看起来好像可以直接把镜像当做命令使用了，不过命令总有参数，如果我们希望加参数呢？比如从上面的 CMD 中可以看到实质的命令是 curl，那么如果我们希望显示 HTTP 头信息，就需要加上 -i 参数。那么我们可以直接加 -i 参数给 docker run myip 么？\n$ docker run myip -i docker: Error response from daemon: invalid header field value \u0026#34;oci runtime error: container_linux.go:247: starting container process caused \\\u0026#34;exec: \\\\\\\u0026#34;-i\\\\\\\u0026#34;: executable file not found in $PATH\\\u0026#34;\\n\u0026#34;. 我们可以看到可执行文件找不到的报错，executable file not found。之前我们说过，跟在镜像名后面的是 command，运行时会替换 CMD 的默认值。因此这里的 -i 替换了原来的 CMD，而不是添加在原来的 curl -s http://ip.cn 后面。而 -i 根本不是命令，所以自然找不到。\n那么如果我们希望加入 -i 这参数，我们就必须重新完整的输入这个命令：\n$ docker run myip curl -s http://ip.cn -i 这显然不是很好的解决方案，而使用 ENTRYPOINT 就可以解决这个问题。现在我们重新用 ENTRYPOINT 来实现这个镜像：\nFROM ubuntu:16.04 RUN apt-get update \\ \u0026amp;\u0026amp; apt-get install -y curl \\ \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* ENTRYPOINT [ \u0026#34;curl\u0026#34;, \u0026#34;-s\u0026#34;, \u0026#34;http://ip.cn\u0026#34; ] 这次我们再来尝试直接使用 docker run myip -i：\n$ docker run myip 当前 IP：160.155.224.xx 来自：XX市 联通 $ docker run myip -i HTTP/1.1 200 OK Server: nginx/1.8.0 Date: Tue, 22 Nov 2016 05:12:40 GMT Content-Type: text/html; charset=UTF-8 Vary: Accept-Encoding X-Powered-By: PHP/5.6.24-1~dotdeb+7.1 X-Cache: MISS from cache-2 X-Cache-Lookup: MISS from cache-2:80 X-Cache: MISS from proxy-2_6 Transfer-Encoding: chunked Via: 1.1 cache-2:80, 1.1 proxy-2_6:8006 Connection: keep-alive 当前 IP：160.155.224.xx 来自：XX市 联通 可以看到，这次成功了。这是因为当存在 ENTRYPOINT 后，CMD 的内容将会作为参数传给 ENTRYPOINT，而这里 -i 就是新的 CMD，因此会作为参数传给 curl，从而达到了我们预期的效果。\n2.4.2、场景二：应用运行前的准备工作 # 启动容器就是启动主进程，但有些时候，启动主进程前，需要一些准备工作。\n比如 mysql 类的数据库，可能需要一些数据库配置、初始化的工作，这些工作要在最终的 mysql 服务器运行之前解决。\n此外，可能希望避免使用 root 用户去启动服务，从而提高安全性，而在启动服务前还需要以 root 身份执行一些必要的准备工作，最后切换到服务用户身份启动服务。或者除了服务外，其它命令依旧可以使用 root 身份执行，方便调试等。\n这些准备工作是和容器 CMD 无关的，无论 CMD 为什么，都需要事先进行一个预处理的工作。这种情况下，可以写一个脚本，然后放入 ENTRYPOINT 中去执行，而这个脚本会将接到的参数（也就是 \u0026lt;CMD\u0026gt;）作为命令，在脚本最后执行。比如官方镜像 redis 中就是这么做的：\nFROM alpine:3.4 ... RUN addgroup -S redis \u0026amp;\u0026amp; adduser -S -G redis redis ... ENTRYPOINT [\u0026#34;docker-entrypoint.sh\u0026#34;] EXPOSE 6379 CMD [ \u0026#34;redis-server\u0026#34; ] 可以看到其中为了 Redis 服务创建了 Redis 用户，并在最后指定了 ENTRYPOINT 为 docker-entrypoint.sh 脚本。\n#!/bin/sh ... # allow the container to be started with `--user` if [ \u0026#34;$1\u0026#34; = \u0026#39;redis-server\u0026#39; -a \u0026#34;$(id -u)\u0026#34; = \u0026#39;0\u0026#39; ]; then chown -R redis . exec su-exec redis \u0026#34;$0\u0026#34; \u0026#34;$@\u0026#34; fi exec \u0026#34;$@\u0026#34; 该脚本的内容就是根据 CMD 的内容来判断，如果是 redis-server 的话，则切换到 redis 用户身份启动服务器，否则依旧使用 root 身份执行。比如：\n$ docker run -it redis id uid=0(root) gid=0(root) groups=0(root) 2.5、ENV # 格式有两种：\nENV \u0026lt;key\u0026gt; \u0026lt;value\u0026gt; ENV \u0026lt;key1\u0026gt;=\u0026lt;value1\u0026gt; \u0026lt;key2\u0026gt;=\u0026lt;value2\u0026gt;... 这个指令很简单，就是设置环境变量而已，无论是后面的其它指令，如 RUN，还是运行时的应用，都可以直接使用这里定义的环境变量。\nENV VERSION=1.0 DEBUG=on \\ NAME=\u0026#34;Happy Feet\u0026#34; 这个例子中演示了如何换行，以及对含有空格的值用双引号括起来的办法，这和 Shell 下的行为是一致的。\n定义了环境变量，那么在后续的指令中，就可以使用这个环境变量。比如在官方 node 镜像 Dockerfile 中，就有类似这样的代码：\nENV NODE_VERSION 7.2.0 RUN curl -SLO \u0026#34;https://nodejs.org/dist/v$NODE_VERSION/node-v$NODE_VERSION-linux-x64.tar.xz\u0026#34; \\ \u0026amp;\u0026amp; curl -SLO \u0026#34;https://nodejs.org/dist/v$NODE_VERSION/SHASUMS256.txt.asc\u0026#34; \\ \u0026amp;\u0026amp; gpg --batch --decrypt --output SHASUMS256.txt SHASUMS256.txt.asc \\ \u0026amp;\u0026amp; grep \u0026#34; node-v$NODE_VERSION-linux-x64.tar.xz\\$\u0026#34; SHASUMS256.txt | sha256sum -c - \\ \u0026amp;\u0026amp; tar -xJf \u0026#34;node-v$NODE_VERSION-linux-x64.tar.xz\u0026#34; -C /usr/local --strip-components=1 \\ \u0026amp;\u0026amp; rm \u0026#34;node-v$NODE_VERSION-linux-x64.tar.xz\u0026#34; SHASUMS256.txt.asc SHASUMS256.txt \\ \u0026amp;\u0026amp; ln -s /usr/local/bin/node /usr/local/bin/nodejs 在这里先定义了环境变量 NODE_VERSION，其后的 RUN 这层里，多次使用 $NODE_VERSION 来进行操作定制。可以看到，将来升级镜像构建版本的时候，只需要更新 7.2.0 即可，Dockerfile 构建维护变得更轻松了。\n下列指令可以支持环境变量展开：ADD、COPY、ENV、EXPOSE、LABEL、USER、WORKDIR、VOLUME、STOPSIGNAL、ONBUILD。\n可以从这个指令列表里感觉到，环境变量可以使用的地方很多，很强大。通过环境变量，我们可以让一份 Dockerfile 制作更多的镜像，只需使用不同的环境变量即可。\n2.6、VOLUME # 格式为：\nVOLUME [\u0026quot;\u0026lt;路径1\u0026gt;\u0026quot;, \u0026quot;\u0026lt;路径2\u0026gt;\u0026quot;...] VOLUME \u0026lt;路径\u0026gt; 之前我们说过，容器运行时应该尽量保持容器存储层不发生写操作，对于数据库类需要保存动态数据的应用，其数据库文件应该保存于卷(volume)中，后面的章节我们会进一步介绍 Docker 卷的概念。为了防止运行时用户忘记将动态文件所保存目录挂载为卷，在 Dockerfile 中，我们可以事先指定某些目录挂载为匿名卷，这样在运行时如果用户不指定挂载，其应用也可以正常运行，不会向容器存储层写入大量数据。\nVOLUME /data 这里的 /data 目录就会在运行时自动挂载为匿名卷，任何向 /data 中写入的信息都不会记录进容器存储层，从而保证了容器存储层的无状态化。当然，运行时可以覆盖这个挂载设置。比如：\ndocker run -d -v mydata:/data xxxx 在这行命令中，就使用了 mydata 这个命名卷挂载到了 /data 这个位置，替代了 Dockerfile 中定义的匿名卷的挂载配置。\n2.7、EXPOSE # 格式为 EXPOSE \u0026lt;端口1\u0026gt; [\u0026lt;端口2\u0026gt;...]。\nEXPOSE 指令是声明运行时容器提供服务端口，这只是一个声明，在运行时并不会因为这个声明应用就会开启这个端口的服务。在 Dockerfile 中写入这样的声明有两个好处，一个是帮助镜像使用者理解这个镜像服务的守护端口，以方便配置映射；另一个用处则是在运行时使用随机端口映射时，也就是 docker run -P 时，会自动随机映射 EXPOSE 的端口。\n此外，在早期 Docker 版本中还有一个特殊的用处。以前所有容器都运行于默认桥接网络中，因此所有容器互相之间都可以直接访问，这样存在一定的安全性问题。于是有了一个 Docker 引擎参数 --icc=false，当指定该参数后，容器间将默认无法互访，除非互相间使用了 --links 参数的容器才可以互通，并且只有镜像中 EXPOSE 所声明的端口才可以被访问。这个 --icc=false 的用法，在引入了 docker network 后已经基本不用了，通过自定义网络可以很轻松的实现容器间的互联与隔离。\n要将 EXPOSE 和在运行时使用 -p \u0026lt;宿主端口\u0026gt;:\u0026lt;容器端口\u0026gt; 区分开来。-p，是映射宿主端口和容器端口，换句话说，就是将容器的对应端口服务公开给外界访问，而 EXPOSE 仅仅是声明容器打算使用什么端口而已，并不会自动在宿主进行端口映射。\n2.8、WORKDIR # 格式为 WORKDIR \u0026lt;工作目录路径\u0026gt;。\n使用 WORKDIR 指令可以来指定工作目录（或者称为当前目录），以后各层的当前目录就被改为指定的目录，如该目录不存在，WORKDIR 会帮你建立目录。\n之前提到一些初学者常犯的错误是把 Dockerfile 等同于 Shell 脚本来书写，这种错误的理解还可能会导致出现下面这样的错误：\nUN cd /app RUN echo \u0026#34;hello\u0026#34; \u0026gt; world.txt 如果将这个 Dockerfile 进行构建镜像运行后，会发现找不到 /app/world.txt 文件，或者其内容不是 hello。原因其实很简单，在 Shell 中，连续两行是同一个进程执行环境，因此前一个命令修改的内存状态，会直接影响后一个命令；而在 Dockerfile 中，这两行 RUN 命令的执行环境根本不同，是两个完全不同的容器。这就是对 Dockerfile 构建分层存储的概念不了解所导致的错误。\n之前说过每一个 RUN 都是启动一个容器、执行命令、然后提交存储层文件变更。第一层 RUN cd /app 的执行仅仅是当前进程的工作目录变更，一个内存上的变化而已，其结果不会造成任何文件变更。而到第二层的时候，启动的是一个全新的容器，跟第一层的容器更完全没关系，自然不可能继承前一层构建过程中的内存变化。\n因此如果需要改变以后各层的工作目录的位置，那么应该使用 WORKDIR指令。\n"},{"id":75,"href":"/docs/dockerfile%E7%9A%84copy%E5%92%8Cadd%E7%9A%84%E5%8C%BA%E5%88%AB-dockerfile-de-copy-he-add-de-qu-bie/","title":"dockerfile的copy和add的区别 2024-04-03 15:01:09.565","section":"Docs","content":" Dockerfile 中的 ADD 与 COPY 指令都可以用于将本地文件或目录复制到 Docker 镜像中，但它们之间有一些区别。 # 1. ADD 指令支持自动解压缩功能 # 当使用 ADD 指令将本地文件复制到 Docker 镜像中时，如果该文件是压缩包格式，Docker 会自动解压缩该文件。例如： # ADD nginx-1.21.0.tar.gz /usr/local/ 上述例子中在将本地的 nginx-1.21.0.tar.gz 文件复制到 Docker 镜像的 /usr/local/ 目录下时，Docker 会将该文件解压缩。 # 而 COPY 指令并不支持自动解压缩功能，需要手动解压缩后再复制进镜像。 # 2. ADD 指令可以从 URL 复制内容 # ADD 指令除了能够复制本地文件和目录外，还可以复制远程文件（例如从 URL 下载）到 Docker 镜像中。例如： # ADD https://example.com/nginx-1.21.0.tar.gz /usr/local/ 上述例子中会从远程 URL https://example.com/nginx-1.21.0.tar.gz 下载文件，并将其复制到 Docker 镜像的 /usr/local/ 目录下。 # 而 COPY 指令只能复制本地文件和目录。 # 3. ADD 指令具有隐式的文件拷贝功能 # ADD 指令除了能够复制本地文件和目录外，还具有一个隐式的文件拷贝功能：当复制一个压缩包文件到容器中时，Docker 会自动解压，并且可以直接从 URL 下载文件并解压缩。 # 例如：\nADD package.tar.gz /app/ 上述例子中，如果 package.tar.gz 是一个压缩包文件，则 Docker 会自动解压缩该文件，并将其复制到 /app/ 目录下。而 COPY 指令无法实现这一功能。 # 4. 总结 # 综上所述，虽然 ADD 和 COPY 指令都可以用于复制文件和目录到 Docker 镜像中，但它们之间在使用时存在不同的限制和特性。需要根据实际情况来选择适合的指令。 # Dockerfile中CMD和ENTRYPOINT的区别 # 在 Dockerfile 中，CMD 和 ENTRYPOINT 都用于指定容器启动时要执行的命令。它们之间的主要区别是： # - CMD 用于定义容器启动时要执行的命令和参数，它设置的值可以被 Dockerfile 中的后续指令覆盖，包括在运行容器时传递的参数。如果在 Dockerfile 中没有指定 CMD，那么 Docker 将使用容器启动时所提供的默认命令。 # - ENTRYPOINT 也用于定义容器启动时要执行的命令，但不像 CMD，它的值不会被 Dockerfile 中的后续指令覆盖，只能在运行容器时添加参数来覆盖它。也就是说，ENTRYPOINT 设置的命令一般是容器镜像所要执行的主要命令，CMD 则是选项和参数。 # 换句话说，CMD 是为 ENTRYPOINT 提供默认选项，ENTRYPOINT 则是容器镜像中主要执行的命令。在 Dockerfile 中定义 ENTRYPOINT 可以使您的镜像更像一个可执行文件，而在运行容器时传递参数可以以更灵活的方式执行此命令。 # "},{"id":76,"href":"/docs/docker%E9%87%8D%E8%A6%81%E7%9A%84%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86%E7%82%B9-docker-zhong-yao-de-wang-luo-zhi-shi-dian/","title":"Docker重要的网络知识点  2024-04-03 15:12:10.162","section":"Docs","content":" 一、Bridge网络 # bridge网络表现形式就是docker0这个网络接口。容器默认都是通过docker0这个接口进行通信。也可以通过docker0去和本机的以太网接口连接，这样容器内部才能访问互联网。\n查看docker0网络，在默认环境中，一个名为docker0的linux bridge自动被创建好了，其上有一个 docker0 内部接口，IP地址为172.17.0.1/16。通过命令：ip a\n查看docker网络\ndocker network ls\n查看bridge网络详情。主要关注Containers节点信息。\ndocker network inspect bridge 输出如下：\n[ { \u0026#34;Name\u0026#34;: \u0026#34;bridge\u0026#34;, \u0026#34;Id\u0026#34;: \u0026#34;8f2bb7721e07845f809dab6cfeefccc5145cbb99953bbe090d5c349af5bf5fa1\u0026#34;, \u0026#34;Created\u0026#34;: \u0026#34;2024-01-22T22:29:27.110338803+08:00\u0026#34;, \u0026#34;Scope\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;bridge\u0026#34;, \u0026#34;EnableIPv6\u0026#34;: false, \u0026#34;IPAM\u0026#34;: { \u0026#34;Driver\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;Options\u0026#34;: null, \u0026#34;Config\u0026#34;: [ { \u0026#34;Subnet\u0026#34;: \u0026#34;172.17.0.0/16\u0026#34;, \u0026#34;Gateway\u0026#34;: \u0026#34;172.17.0.1\u0026#34; } ] }, \u0026#34;Internal\u0026#34;: false, \u0026#34;Attachable\u0026#34;: false, \u0026#34;Ingress\u0026#34;: false, \u0026#34;ConfigFrom\u0026#34;: { \u0026#34;Network\u0026#34;: \u0026#34;\u0026#34; }, \u0026#34;ConfigOnly\u0026#34;: false, \u0026#34;Containers\u0026#34;: {}, \u0026#34;Options\u0026#34;: { \u0026#34;com.docker.network.bridge.default_bridge\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;com.docker.network.bridge.enable_icc\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;com.docker.network.bridge.enable_ip_masquerade\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;com.docker.network.bridge.host_binding_ipv4\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;com.docker.network.bridge.name\u0026#34;: \u0026#34;docker0\u0026#34;, \u0026#34;com.docker.network.driver.mtu\u0026#34;: \u0026#34;1500\u0026#34; }, \u0026#34;Labels\u0026#34;: {} } ] 我们看到Containers这个节点下，没有值，也就是没有任何容器加入这个网络环境\n1.1、docker0详解 # 1、拉取镜像\ndocker pull nginx:1.19.3-alpine\n2、运行镜像\ndocker run -itd \u0026ndash;name nginx1 nginx:1.19.3-alpine\n3、观察bridge网络\ndocker network inspect bridge\n可以看到nginx的网络ip地址是172.17.9.2。这个是因为docker0的网络地址范围是172.17.0.1~172.17.255.255\n4、查看宿主机上的网卡\n// 用于显示或操作系统的网络接口信息\nip a\n发现多出一块网卡veth0ce8f41@if9\nveth（Virtual Ethernet）是Linux网络命名空间中的一种虚拟网络设备类型。它总是成对出现，就像一个虚拟的以太网线缆，一端插在一个网络命名空间，另一端插在另一个网络命名空间或者主机的全局网络命名空间。\nveth对在容器技术（如Docker）中实现网络隔离和通信非常有用。例如，当你在Docker中启动一个新的容器时，Docker会创建一对veth接口。一个接口在新的容器的网络命名空间中，另一个接口在主机的网络命名空间中。这样，容器就可以通过这对veth接口与主机和其他容器通信。\nDocker创建一个容器的时候，会执行如下操作：\n创建一对虚拟接口/网卡，也就是veth pair，分别放到本地主机和新容器中； 本地主机一端桥接到默认docker0或指定网桥上，并具有一个唯一的名字，如veth0ce8f41@if9`; 容器一端放到新容器中，并修改名字作为eth0，这个网卡/接口只在容器的名字空间可见； 从网桥可用地址段中（也就是与该bridge对应的network）获取一个空闲地址分配给容器的eth0，并配置默认路由到桥接网卡veth0ce8f41@if9:。 完成这些之后，容器就可以使用 eth0 虚拟网卡来连接其他容器和其他网络。 如果不指定\u0026ndash;network，创建的容器默认都会挂到 docker0 上，使用本地主机上 docker0 接口的 IP 作为 所有容器的默认网关\n第一种方式： docker exec -it nginx1 sh ip a 第二种方式： docker exec -it nginx1 ip a 安装brctl\nbrctl是一个用于设置和管理Linux桥接设备的命令行工具。在大多数Linux发行版中，brctl包含在bridge-utils软件包中。\nyum install -y bridge-utils\n运行命令\nbrctl show\n1.2、多容器之间通信 # 1、再安装一个nginx\ndocker run -itd \u0026ndash;name nginx2 nginx:1.19.3-alpine\n2、查看bridge网络\ndocker network inspect bridge\n3、显示结果\n\u0026#34;Containers\u0026#34;: { \u0026#34;3c178352ba547b43d84d1027bd268cf037488a7351cf5bb18c09b8a326a7986a\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;nginx1\u0026#34;, \u0026#34;EndpointID\u0026#34;: \u0026#34;21c8a36dcf1b22414098f112af7ff5a896180fd60d88f918cf220e2de4cb1aeb\u0026#34;, \u0026#34;MacAddress\u0026#34;: \u0026#34;02:42:ac:11:00:02\u0026#34;, \u0026#34;IPv4Address\u0026#34;: \u0026#34;172.17.0.2/16\u0026#34;, \u0026#34;IPv6Address\u0026#34;: \u0026#34;\u0026#34; }, \u0026#34;bc60737cd65607de4224337766c01a7e3e0177e401ab1170e76cfe00a4b083c2\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;nginx2\u0026#34;, \u0026#34;EndpointID\u0026#34;: \u0026#34;b10d02f3435c51053c7ddb4b1c915b2bef9e90a79caf9b0819ae2316a7ef9a4a\u0026#34;, \u0026#34;MacAddress\u0026#34;: \u0026#34;02:42:ac:11:00:03\u0026#34;, \u0026#34;IPv4Address\u0026#34;: \u0026#34;172.17.0.3/16\u0026#34;, \u0026#34;IPv6Address\u0026#34;: \u0026#34;\u0026#34; } }, 4、运行brctl show\nbrctl show命令用于显示系统中所有的网络桥接器及其相关信息。\n问题点： # 每次重启docker的时候，容器的ip地址会变，谁先启动，谁获取前面的ip地址。开发人员不能在代码中写死数据库的IP地址。举个例子：\n在企业开发环境中，有个msql服务容器mysql5,还有个web服务容器tomcat8,tomcat8肯定要连接mysql5，需要知道对方的真实ip地址。这时候我们就需要使用容器名来ping，而不是ip地址\n二、新建Bridge网络 # docker network create -d bridge test-bridge\n这条命令是用来在Docker中创建一个新的网络桥接器的。\ndocker network create是用来创建新的网络的命令，-d bridge指定了网络的驱动类型为桥接，test-bridge是你为新创建的网络桥接器指定的名称。\n创建完成后，你可以使用docker network ls命令来查看所有的网络，包括你刚刚创建的test-bridge网络。\n你也可以在运行新的容器时，使用--network=test-bridge参数来指定容器使用这个新创建的网络。\n当然如果你就要创建bridge。可以不带-d的参数。\nimage-20240122233909390\n2.1、brctl show查看 # image-20240122234038290\n说明这个网卡还没有绑定容器\n2.2、启动nginx3绑定test-bridge网卡 # docker run -itd \u0026ndash;name nginx3 \u0026ndash;network test-bridge nginx:1.19.3-alpine\n启动一个nginx的容器nginx3，并通过参数network connect来连接test-bridge网络。\nimage-20240122234323512\nbridge绑定了新的网卡\n2.3、把一个运行中容器连接到test-bridge网络 # docker network connect test-bridge nginx2\n2.4、创建自定义网络的好处 # 当你在Docker中创建一个自定义网络时，Docker会为该网络启用一个内置的DNS服务器。这个DNS服务器会自动为加入该网络的每个容器提供DNS解析服务。\n这意味着，当你的容器加入到这个自定义网络时，它们可以使用其他容器的名称来进行网络通信，而不需要知道其他容器的IP地址。这是因为Docker的内置DNS服务器会自动将容器的名称解析为其内部IP地址。\n例如，假设你有两个在同一个自定义网络中的容器，名为container1和container2。在container1中，你可以直接使用ping container2来pingcontainer2，而不需要知道container2的IP地址。\n这个特性使得在Docker中管理和连接容器变得更加简单，因为你不需要手动管理和配置IP地址。\n2.5、docker0默认不能互访的原因 # Docker的默认桥接网络（通常被称为docker0）是在Docker首次安装时创建的。这个默认网络的设计目标是为了让Docker能够“开箱即用”，而不需要用户进行任何额外的网络配置。\n然而，docker0网络并没有启用Docker的内置DNS服务。这意味着在docker0网络中的容器不能直接通过容器名进行通信，必须使用容器的IP地址。这是一个已知的限制，是为了保持向后兼容性和简单性。\n如果你需要使用容器名进行通信，你可以创建一个自定义的桥接网络。在自定义网络中，Docker的内置DNS服务是默认启用的。你可以使用docker network create命令来创建一个自定义网络，然后在运行容器时使用--network参数来指定容器加入这个网络。\n三、none、host网络 # 3.1、none网络 # 环境准备，先stop和rm掉全部之前开启的容器。并且把前面创建的test-bridge网络也删除。当然，更简单的办法是使用快照方式。将虚拟机恢复到docker初始化安装时。或者手动删除容器与网络\ndocker rm -f $(docker ps -aq) docker network rm test-bridge docker network ls 启动一个ngnix的容器nginx1，并且连接到none网络。然后执行docker network inspect none，看看容器信息\ndocker run -itd --name nginx1 --network none nginx:1.19.3-alpine docker network inspect none image-20240123232817227\n注意，容器使用none模式，是没有物理地址和IP地址。我们可以进入到nginx1容器里，执行ip a命令看 看。只有一个lo接口，没有其他网络接口，没有IP。也就是说，使用none模式，这个容器是不能被其他 容器访问。这种使用场景很少，只有项目安全性很高的功能才能使用到。例如：密码加密算法容器。\n知识点： # lo 接口，全称为 \u0026ldquo;loopback\u0026rdquo; 接口，是一个特殊的网络接口，用于网络软件的测试和本地通信。\n当你向 lo 接口发送数据时，数据不会被发送到网络上，而是直接返回到发送者。这就像你在一个封闭的房间里大声说话，声音会反射回来，你可以听到自己的声音。\nlo 接口通常有一个特殊的 IP 地址 127.0.0.1（也被称为 \u0026ldquo;localhost\u0026rdquo;）。当你向这个地址发送数据时，数据会被发送到 lo 接口，然后直接返回到发送者。\nlo 接口在网络软件的开发和测试中非常有用。例如，你可以在同一台机器上运行客户端和服务器程序，然后使用 lo 接口进行通信，以测试它们的功能。\ndocker exec -it nginx1 sh\nimage-20240123233007075\n3.2、host网络 # 启动一个nginx的nginx2容器，连接到host网络。然后docker network inspect host, 看看容器信息\ndocker run -itd --name nginx2 --network host nginx:1.19.3-alpine docker network inspect host image-20240123233320208\n这里来看，也不显示IP地址。那么是不是和none一样，肯定不是，不然也不会设计none和host网络进 行区分。下面我们进入nginx2容器，执行ip a看看效果。我们在容器里执行ip a，发现打印内容和在 linux本机外执行ip a是一样的。\ndocker exec -it nginx2 sh\nimage-20240123233439168\n这说明什么呢？容器使用了host模式，说明容器和外层linux主机共享一套网络接口。VMware公司的虚拟机管理软件，其中网络设置，也有host这个模式，作用也是一样，虚拟机里面使用网络和你自己外层机器是一模一样的。这种容器和本机使用共享一套网络接口，缺点还是很明显的，例如我们知道web服务器一般端口是80，共享了一套网络接口，那么你这台机器上只能启动一个nginx端口为80的服务器了。否则，出现端口被占用的情况。\n四、总结 # Docker 提供了多种网络模式，以满足不同的应用场景：\n桥接网络（bridge）：这是 Docker 的默认网络模式。在这种模式下，Docker 为每个容器分配一个私有的网络空间，并为每个容器创建一个虚拟网络接口，然后将这个接口连接到一个虚拟桥接器上。Docker 还会在宿主机上创建一个 docker0 网桥，并将所有容器的虚拟网络接口连接到这个网桥上，从而实现容器之间的通信。 主机网络（host）：在这种模式下，容器直接共享宿主机的网络，没有自己的网络空间。这种模式的优点是网络性能高，但缺点是容器的网络隔离性差。 无网络（none）：在这种模式下，容器没有自己的网络空间，也不能访问宿主机的网络。这种模式通常用于需要高度隔离的场景。 用户定义网络（user-defined network）：Docker 允许用户创建自定义的网络，并将容器连接到这些网络上。用户定义网络可以是桥接网络、覆盖网络（overlay）或者 MACVLAN 网络。覆盖网络用于跨主机的容器通信，MACVLAN 网络用于将容器直接连接到物理网络。 如果刚接触docker对这块网络没有什么概念的话，安装镜像就会遇到一系列问题。比如我们在使用docker安装es与kibana的时候，会遇到kibana连不上es的问题，直到你了解docker设计的网络的原理，才能正确安装。总之，多多练习。\n"},{"id":77,"href":"/docs/etcd%E5%A4%87%E4%BB%BD-etcd-bei-fen/","title":"ETCD备份 2024-04-03","section":"Docs","content":"#cat /data/backup/etcd/etcd-backup.sh #!/bin/bash bakpath=/data/backup/etcd cd $bakpath baktime=`date +%Y%m%d` #备份当天数据 ETCDCTL_API=3 etcdctl --endpoints http://127.0.0.1:1159 \\ snapshot save /data/backup/etcd/etcd-data-bak-${baktime}.db \\ \u0026gt;\u0026gt;${bakpath}/etcd-backup.log #清理半个月前数据 /usr/bin/find $bakpath -name *.db -mtime +15 -exec rm -f {} \\; 0 23 * * * sh /data/backup/etcd/etcd-backup.sh\u0026gt;/dev/null 2\u0026gt;\u0026amp;1 那就改成/apps目录 你们装的的机器是/data目录 "},{"id":78,"href":"/docs/etcd%E7%A8%B3%E5%AE%9A%E6%80%A7%E5%8F%8A%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E5%AE%9E%E8%B7%B5-etcd-wen-ding-xing-ji-xing-neng-you-hua-shi-jian/","title":"ETCD稳定性及性能优化实践 2024-04-03 15:06:31.141","section":"Docs","content":" 背景与挑战 # 随着腾讯自研上云及公有云用户的迅速增长，一方面，腾讯云容器服务TKE服务数量和核数大幅增长, 另一方面我们提供的容器服务类型（TKE托管及独立集群、EKS弹性集群、edge边缘计算集群、mesh服务网格、serverless knative）也越来越丰富。各类容器服务类型背后的核心都是K8s，K8s核心的存储etcd又统一由我们基于K8s构建的etcd平台进行管理。基于它我们目前管理了千级etcd集群，背后支撑了万级K8s集群。\n在万级K8s集群规模下的我们如何高效保障etcd集群的稳定性?\netcd集群的稳定性风险又来自哪里？\n我们通过基于业务场景、历史遗留问题、现网运营经验等进行稳定性风险模型分析，风险主要来自旧TKE etcd架构设计不合理、etcd稳定性、etcd性能部分场景无法满足业务、测试用例覆盖不足、变更管理不严谨、监控是否全面覆盖、隐患点是否能自动巡检发现、极端灾难故障数据安全是否能保障。\n前面所描述的etcd平台已经从架构设计上、变更管理上、监控及巡检、数据迁移、备份几个方面程度解决了我们管理的各类容器服务的etcd可扩展性、可运维性、可观测性以及数据安全性，因此本文将重点描述我们在万级K8s场景下面临的etcd内核稳定性及性能挑战，比如:\n数据不一致 内存泄露 死锁 进程Crash 大包请求导致etcd OOM及丢包 较大数据量场景下启动慢 鉴权及查询key数量、查询指定数量记录接口性能较差 本文将简易描述我们是如何发现、分析、复现、解决以上问题及挑战，以及从以上过程中我们获得了哪些经验及教训，并将之应用到我们的各类容器服务存储稳定性保障中。\n同时，我们将解决方案全部贡献、回馈给etcd开源社区， 截止目前我们贡献的30+ pr已全部合并到社区。腾讯云TKE etcd团队是etcd社区2020年上半年最活跃的贡献团队之一, 为etcd的发展贡献我们的一点力量, 在这过程中特别感谢社区AWS、Google、Ali等maintainer的支持与帮助。\n稳定性优化案例剖析 # 从GitLab误删主库丢失部分数据到GitHub数据不一致导致中断24小时，再到号称\u0026quot;不沉航母\u0026quot;的AWS S3故障数小时等，无一例外都是存储服务。稳定性对于一个存储服务、乃至一个公司的口碑而言至关重要，它决定着一个产品生与死。稳定性优化案例我们将从数据不一致的严重性、两个etcd数据不一致的bug、lease内存泄露、mvcc 死锁、wal crash方面阐述，我们是如何发现、分析、复现、解决以上case，并分享我们从每个case中的获得的收获和反思，从中汲取经验，防患于未然。\n数据不一致（Data Inconsistency） # 谈到数据不一致导致的大故障，就不得不详细提下GitHub在18年一次因网络设备的例行维护工作导致的美国东海岸网络中心与东海岸主要数据中心之间的连接断开。虽然网络的连通性在43秒内得以恢复，但是短暂的中断引发了一系列事件，最终导致GitHub 24小时11分钟的服务降级，部分功能不可用。 # GitHub使用了大量的MySQL集群存储GitHub的meta data,如issue、pr、page等等，同时做了东西海岸跨城级别的容灾。故障核心原因是网络异常时GitHub的MySQL仲裁服务Orchestrator进行了故障转移，将写入数据定向到美国西海岸的MySQL集群(故障前primary在东海岸)，然而美国东海岸的MySQL包含一小段写入，尚未复制到美国西海岸集群，同时故障转移后由于两个数据中心的集群现在都包含另一个数据中心中不存在的写入，因此又无法安全地将主数据库故障转移回美国东海岸。 # 最终, 为了保证保证用户数据不丢失，GitHub不得不以24小时的服务降级为代价来修复数据一致性。 # 数据不一致的故障严重性不言而喻，然而etcd是基于raft协议实现的分布式高可靠存储系统，我们也并未做跨城容灾，按理数据不一致这种看起来高大上bug我们是很难遇到的。然而梦想是美好的，现实是残酷的，我们不仅遇到了不可思议的数据不一致bug, 还一踩就是两个，一个是重启etcd有较低的概率触发，一个是升级etcd版本时如果开启了鉴权，在K8s场景下较大概率触发。在详细讨论这两个bug前，我们先看看在K8s场景下etcd数据不一致会导致哪些问题呢？ # 数据不一致最恐怖之处在于client写入是成功的，但可能在部分节点读取到空或者是旧数据,client无法感知到写入在部分节点是失败的和可能读到旧数据 # 读到空可能会导致业务Node消失、Pod消失、Node上Service路由规则消失，一般场景下，只会影响用户变更的服务 # 读到老数据会导致业务变更不生效，如服务扩缩容、Service rs替换、变更镜像异常等待，一般场景下，只会影响用户变更的服务 # 在etcd平台迁移场景下，client无法感知到写入失败，若校验数据一致性也无异常时（校验时连接到了正常节点），会导致迁移后整个集群全面故障（apiserver连接到了异常节点），用户的Node、部署的服务、lb等会被全部删除，严重影响用户业务 # 首先第一个不一致bug是重启etcd过程中遇到的，人工尝试复现多次皆失败，分析、定位、复现、解决这个bug之路几经波折，过程很有趣并充满挑战，最终通过我对关键点增加debug日志，编写chaos monkey模拟各种异常场景、边界条件，实现复现成功。最后的真凶竟然是一个授权接口在重启后重放导致鉴权版本号不一致，然后放大导致多版本数据库不一致, 部分节点无法写入新数据, 影响所有v3版本的3年之久bug。 # 随后我们提交若干个相关pr到社区, 并全部合并了, 最新的etcd v3.4.9[1]，v3.3.22[2]已修复此问题, 同时google的jingyih也已经提K8s issue和pr[3]将K8s 1.19的etcd client及server版本升级到最新的v3.4.9。此bug详细可参考超凡同学写的文章三年之久的 etcd3 数据不一致 bug 分析。 # 第二个不一致bug是在升级etcd过程中遇到的，因etcd缺少关键的错误日志，故障现场有效信息不多，定位较困难，只能通过分析代码和复现解决。然而人工尝试复现多次皆失败，于是我们通过chaos monkey模拟client行为场景，将测试环境所有K8s集群的etcd分配请求调度到我们复现集群，以及对比3.2与3.3版本差异，在可疑点如lease和txn模块增加大量的关键日志，并对etcd apply request失败场景打印错误日志。 # 通过以上措施，我们比较快就复现成功了, 最终通过代码和日志发现是3.2版本与3.3版本在revoke lease权限上出现了差异，3.2无权限，3.3需要写权限。当lease过期的时候，如果leader是3.2，那么请求在3.3节点就会因无权限导致失败，进而导致key数量不一致，mvcc版本号不一致，导致txn事务部分场景执行失败等。最新的3.2分支也已合并我们提交的修复方案，同时我们增加了etcd核心过程失败的错误日志以提高数据不一致问题定位效率，完善了升级文档，详细说明了lease会在此场景下引起数据不一致性，避免大家再次采坑。 # 从这两个数据不一致bug中我们获得了以下收获和最佳实践: # 算法理论数据一致性，不代表整体服务实现能保证数据一致性，目前业界对于这种基于日志复制状态机实现的分布式存储系统，没有一个核心的机制能保证raft、wal、mvcc、snapshot等模块协作不出问题，raft只能保证日志状态机的一致性，不能保证应用层去执行这些日志对应的command都会成功 # etcd版本升级存在一定的风险，需要仔细review代码评估是否存在不兼容的特性，如若存在是否影响鉴权版本号及mvcc版本号，若影响则升级过程中可能会导致数据不一致性，同时一定要灰度变更现网集群 # 对所有etcd集群增加了一致性巡检告警，如revision差异监控、key数量差异监控等 # 定时对etcd集群数据进行备份，再小概率的故障，根据墨菲定律都可能会发生，即便etcd本身虽具备完备的自动化测试(单元测试、集成测试、e2e测试、故障注入测试等)，但测试用例仍有不少场景无法覆盖，我们需要为最坏的场景做准备(如3个节点wal、snap、db文件同时损坏)，降低极端情况下的损失, 做到可用备份数据快速恢复 # etcd v3.4.4后的集群灰度开启data corruption检测功能，当集群出现不一致时，拒绝集群写入、读取，及时止损，控制不一致的数据范围 # 继续完善我们的chaos monkey和使用etcd本身的故障注入测试框架functional，以协助我们验证、压测新版本稳定性（长时间持续跑），复现隐藏极深的bug, 降低线上采坑的概率 # 内存泄露（OOM） # 众所周知etcd是golang写的，而golang自带垃圾回收机制也会内存泄露吗？首先我们得搞清楚golang垃圾回收的原理，它是通过后台运行一个守护线程，监控各个对象的状态，识别并且丢弃不再使用的对象来释放和重用资源，若你迟迟未释放对象，golang垃圾回收不是万能的，不泄露才怪。比如以下场景会导致内存泄露: # goroutine泄露 # deferring function calls（如for循环里面未使用匿名函数及时调用defer释放资源，而是整个for循环结束才调用） # 获取string/slice中的一段导致长string/slice未释放（会共享相同的底层内存块） # 应用内存数据结构管理不周导致内存泄露（如为及时清理过期、无效的数据） # 接下来看看我们遇到的这个etcd内存泄露属于哪种情况呢？事情起源于3月末的一个周末起床后收到现网3.4集群大量内存超过安全阈值告警，立刻排查了下发现以下现象: # QPS及流量监控显示都较低，因此排除高负载及慢查询因素 # 一个集群3个节点只有两个follower节点出现异常,leader 4g,follower节点高达23G # goroutine、fd等资源未出现泄漏 # go runtime memstats指标显示各个节点申请的内存是一致的，但是follower节点go_memstats_heap_release_bytes远低于leader节点，说明某数据结构可能长期未释放 # 生产集群默认关闭了pprof,开启pprof,等待复现, 并在社区上搜索释放有类似案例, 结果发现有多个用户1月份就报了，没引起社区重视，使用场景和现象跟我们一样 # 通过社区的heap堆栈快速定位到了是由于etcd通过一个heap堆来管理lease的状态，当lease过期时需要从堆中删除，但是follower节点却无此操作，因此导致follower内存泄露, 影响所有3.4版本。 # 问题分析清楚后，我提交的修复方案是follower节点不需要维护lease heap,当leader发生选举时确保新的follower节点能重建lease heap,老的leader节点则清空lease heap. # 此内存泄露bug属于内存数据结构管理不周导致的，问题修复后，etcd社区立即发布了新的版本（v3.4.6+）以及K8s都立即进行了etcd版本更新。\n从这个内存泄露bug中我们获得了以下收获和最佳实践:\n持续关注社区issue和pr, 别人今天的问题很可能我们明天就会遇到 # etcd本身测试无法覆盖此类需要一定时间运行的才能触发的资源泄露bug,我们内部需要加强此类场景的测试与压测 # 持续完善、丰富etcd平台的各类监控告警，机器留足足够的内存buffer以扛各种意外的因素。 # 存储层死锁（Mvcc Deadlock） # 死锁是指两个或两个以上的goroutine的执行过程中，由于竞争资源相互等待（一般是锁）或由于彼此通信（chan引起）而造成的一种程序卡死现象，无法对外提供服务。deadlock问题因为往往是在并发状态下资源竞争导致的, 一般比较难定位和复现, 死锁的性质决定着我们必须保留好分析现场，否则分析、复现及其困难。 # **那么我们是如何发现解决这个deadlock bug呢？**问题起源于内部团队在压测etcd集群时，发现一个节点突然故障了，而且一直无法恢复，无法正常获取key数等信息。收到反馈后，我通过分析卡住的etcd进程和查看监控，得到以下结论: # 不经过raft及mvcc模块的rpc请求如member list可以正常返回结果，而经过的rpc请求全部context timeout # etcd health健康监测返回503,503的报错逻辑也是经过了raft及mvcc # 通过tcpdump和netstat排除raft网络模块异常，可疑目标缩小到mvcc # 分析日志发现卡住的时候因数据落后leader较多，接收了一个数据快照，然后执行更新快照的时候卡住了，没有输出快照加载完毕的日志，同时确认日志未丢失 # 排查快照加载的代码，锁定几个可疑的锁和相关goroutine,准备获取卡住的goroutine堆栈 # 通过kill或pprof获取goroutine堆栈，根据goroutine卡住的时间和相关可疑点的代码逻辑，成功找到两个相互竞争资源的goroutine，其中一个正是执行快照加载,重建db的主goroutine，它获取了一把mvcc锁等待所有异步任务结束，而另外一个goroutine则是执行历史key压缩任务，当它收到stop的信号后，立刻退出，调用一个compactBarrier逻辑，而这个逻辑又恰恰需要获取mvcc锁，因此出现死锁，堆栈如下。 # 这个bug也隐藏了很久，影响所有etcd3版本，在集群中写入量较大，某落后的较多的节点执行了快照重建，同时此时又恰恰在做历史版本压缩，那就会触发。我提交的修复PR目前也已经合并到3.3和3.4分支中，新的版本已经发布（v3.3.21+/v3.4.8+）。 # 从这个死锁bug中我们获得了以下收获和最佳实践:\n多并发场景的组合的etcd自动化测试用例覆盖不到，也较难构建，因此也容易出bug, 是否还有其他类似场景存在同样的问题？需要参与社区一起继续提高etcd测试覆盖率（etcd之前官方博客介绍一大半代码已经是测试代码），才能避免此类问题。 # 监控虽然能及时发现异常节点宕机，但是死锁这种场景之前我们不会自动重启etcd,因此需要完善我们的健康探测机制（比如curl /health来判断服务是否正常），出现死锁时能够保留堆栈、自动重启恢复服务。 # 对于读请求较高的场景，需评估3节点集群在一节点宕机后，剩余两节点提供的QPS容量是否能够支持业务，若不够则考虑5节点集群。 # Wal crash（Panic） # panic是指出现严重运行时和业务逻辑错误，导致整个进程退出。panic对于我们而言并不陌生，我们在现网遇到过几次，最早遭遇的不稳定性因素就是集群运行过程中panic了。 # 虽说我们3节点的etcd集群是可以容忍一个节点故障，但是crash瞬间对用户依然有影响，甚至出现集群拨测连接失败。 # 我们遇到的第一个crash bug，是发现集群链接数较多的时候有一定的概率出现crash, 然后根据堆栈查看社区已有人报grpc crash(issue)[4], 原因是etcd依赖的组件grpc-go出现了grpc crash(pr)[5]，而最近我们遇到的crash bug[6]是v3.4.8/v3.3.21新版本发布引起的，这个版本跟我们有很大关系，**我们贡献了3个PR到这个版本，占了一大半以上, 那么这个crash bug是如何产生以及复现呢？**会不会是我们自己的锅呢？ # 首先crash报错是walpb: crc mismatch, 而我们并未提交代码修改wal相关逻辑，排除自己的锅。 # 其次通过review新版本pr, 目标锁定到google一位大佬在修复一个wal在写入成功后，而snapshot写入失败导致的crash bug的时候引入的. # 但是具体是怎么引入的？pr中包含多个测试用例验证新加逻辑，本地创建空集群和使用存量集群(数据比较小)也无法复现. # 错误日志信息太少，导致无法确定是哪个函数报的错，因此首先还是加日志，对各个可疑点增加错误日志后，在我们测试集群随便找了个老节点替换版本，然后很容易就复现了，并确定是新加的验证快照文件合法性的锅，那么它为什么会出现crc mismatch呢? 首先我们来简单了解下wal文件。 # etcd任何经过raft的模块的请求在写入etcd mvcc db前都会通过wal文件持久化，若进程在apply command过程中出现被杀等异常,重启时可通过wal文件重放将数据补齐，避免数据丢失。wal文件包含各种请求命令如成员变化信息、涉及key的各个操作等，为了保证数据完整性、未损坏，wal每条记录都会计算其的crc32，写入wal文件。重启后解析wal文件过程中，会校验记录的完整性，如果数据出现损坏或crc32计算算法出现变化则会出现crc32 mismatch. # 硬盘及文件系统并未出现异常，排除了数据损坏，经过深入排查crc32算法的计算，发现是新增逻辑未处理crc32类型的数据记录，它会影响crc32算法的值，导致出现差异，而且只有在当etcd集群创建产生后的第一个wal文件被回收才会触发，因此对存量运行一段时间的集群，100%复现。 # 解决方案就是通过增加crc32算法的处理逻辑以及增加单元测试覆盖wal文件被回收的场景，社区已合并并发布了新的3.4和3.3版本(v3.4.9/v3.3.22). # 虽然这个bug是社区用户反馈的，但从这个crash bug中我们获得了以下收获和最佳实践: # 单元测试用例非常有价值，然而编写完备的单元测试用例并不容易，需要考虑各类场景。 # etcd社区对存量集群升级、各版本之间兼容性测试用例几乎是0，需要大家一起来为其舔砖加瓦，让测试用例覆盖更多场景。 # 新版本上线内部流程标准化、自动化, 如测试环境压测、混沌测试、不同版本性能对比、优先在非核心场景使用(如event)、灰度上线等流程必不可少。 # 配额及限速（Quota\u0026amp;QoS） # etcd面对一些大数据量的查询（expensive read）和写入操作时（expensive write），如全key遍历（full keyspace fetch）、大量event查询, list all Pod, configmap写入等会消耗大量的cpu、内存、带宽资源，极其容易导致过载，乃至雪崩。 # 然而，etcd目前只有一个极其简单的限速保护，当etcd的commited index大于applied index的阈值大于5000时，会拒绝一切请求，返回Too Many Request，其缺陷很明显，无法精确的对expensive read/write进行限速，无法有效防止集群过载不可用。 # 为了解决以上挑战，避免集群过载目前我们通过以下方案来保障集群稳定性:\n基于K8s apiserver上层限速能力，如apiserver默认写100/s,读200/s # 基于K8s resource quota控制不合理的Pod/configmap/crd数 # 基于K8s controller-manager的-terminated-Pod-gc-threshold参数控制无效Pod数量（此参数默认值高达12500，有很大优化空间） # 基于K8s的apiserver各类资源可独立的存储的特性, 将event/configmap以及其他核心数据分别使用不同的etcd集群，在提高存储性能的同时，减少核心主etcd故障因素 # 基于event admission webhook对读写event的apiserver请求进行速率控制 # 基于不同业务情况，灵活调整event-ttl时间，尽量减少event数 # 基于etcd开发QoS特性，目前也已经向社区提交了初步设计方案，支持基于多种对象类型设置QoS规则（如按grpcMethod、grpcMethod+请求key前缀路径、traffic、cpu-intensive、latency） # 通过多维度的集群告警（etcd集群lb及节点本身出入流量告警、内存告警、精细化到每个K8s集群的资源容量异常增长告警、集群资源读写QPS异常增长告警）来提前防范、规避可能出现的集群稳定性问题 # **多维度的集群告警在我们的etcd稳定性保障中发挥了重要作用，多次帮助我们发现用户和我们自身集群组件问题。**用户问题如内部某K8s平台之前出现bug, 写入大量的集群CRD资源和client读写CRD QPS明显偏高。我们自身组件问题如某旧日志组件，当集群规模增大后，因日志组件不合理的频繁调用list Pod，导致etcd集群流量高达3Gbps, 同时apiserver本身也出现5XX错误。 # 虽然通过以上措施，我们能极大的减少因expensive read导致的稳定性问题，然而从线上实践效果看，目前我们仍然比较依赖集群告警帮助我们定位一些异常client调用行为，无法自动化的对异常client的进行精准智能限速,。etcd层因无法区分是哪个client调用，如果在etcd侧限速会误杀正常client的请求, 因此依赖apiserver精细化的限速功能实现。社区目前已在1.18中引入了一个API Priority and Fairness[7]，目前是alpha版本，期待此特性早日稳定。 # 性能优化案例剖析 # etcd读写性能决定着我们能支撑多大规模的集群、多少client并发调用，启动耗时决定着我们当重启一个节点或因落后leader太多，收到leader的快照重建时，它重新提供服务需要多久？性能优化案例剖析我们将从启动耗时减少一半、密码鉴权性能提升12倍、查询key数量性能提升3倍等来简单介绍下如何对etcd进行性能优化。 # 启动耗时及查询key数量、查询指定记录数性能优化 # 当db size达到4g时，key数量百万级别时，发现重启一个集群耗时竟然高达5分钟, key数量查询也是超时，调整超时时间后，发现高达21秒，内存暴涨6G。同时查询只返回有限的记录数的场景（如业务使用etcd grpc-proxy来减少watch数,etcd grpc proxy在默认创建watch的时候，会发起对watch路径的一次limit读查询），依然耗时很高且有巨大的内存开销。于是周末空闲的时候我对这几个问题进行了深入调查分析，启动耗时到底花在了哪里？是否有优化空间？查询key数量为何如何耗时，内存开销如此之大？ # 带着这些问题对源码进行了深入分析和定位，首先来看查询key数和查询只返回指定记录数的耗时和内存开销极大的问题，分析结论如下： # 查询key数量时etcd之前实现是遍历整个内存btree,把key对应的revision存放在slice数组里面 # 问题就在于key数量较多时，slice扩容涉及到数据拷贝，以及slice也需要大量的内存开销 # 因此优化方案是新增一个CountRevision来统计key的数量即可，不需要使用slice，此方案优化后性能从21s降低到了7s,同时无任何内存开销 # 对于查询指定记录数据耗时和内存开销非常大的问题，通过分析发现是limit记录数并未下推到索引层，通过将查询limit参数下推到索引层，大数据场景下limit查询性能提升百倍，同时无额外的内存开销。 # 再看启动耗时问题过高的问题，通过对启动耗时各阶段增加日志，得到以下结论： # 启动的时候机器上的cpu资源etcd进程未能充分利用\n9%耗时在打开后端db时，如将整个db文件mmap到内存 # **91%耗时在重建内存索引btree上。**当etcd收到一个请求Get Key时，请求被层层传递到了mvcc层后，它首先需要从内存索引btree中查找key对应的版本号，随后从boltdb里面根据版本号查出对应的value, 然后返回给client. 重建内存索引btree数的时候，恰恰是相反的流程，遍历boltdb,从版本号0到最大版本号不断遍历，从value里面解析出对应的key、revision等信息，重建btree，因为这个是个串行操作，所以操作及其耗时 # 尝试将串行构建btree优化成高并发构建，尽量把所有核计算力利用起来，编译新版本测试后发现效果甚微，于是编译新版本打印重建内存索引各阶段的详细耗时分析，结果发现瓶颈在内存btree的插入上，而这个插入拥有一个全局锁，因此几乎无优化空间 # 继续分析91%耗时发现重建内存索引竟然被调用了两次，第一处是为了获取一个mvcc的关键的consistent index变量，它是用来保证etcd命令不会被重复执行的关键数据结构，而我们前面提到的一个数据不一致bug恰好也是跟consistent index有密切关系。 # consistent index实现不合理，封装在mvcc层，因此我前面提了一个pr将此特性重构，做为了一个独立的包，提供各类方法给etcdserver、mvcc、auth、lease等模块调用。 # 特性重构后的consistent index在启动的时候就不再需要通过重建内存索引数等逻辑来获取了，优化成调用cindex包的方法快速获取到consistent index，就将整个耗时从5min从缩短到2分30秒左右。因此优化同时依赖的consistent index特性重构，改动较大暂未backport到3.4/3.3分支，在未来3.5版本中、数据量较大时可以享受到启动耗时的显著提升。 # 密码鉴权性能提升12倍 # 某内部业务服务一直跑的好好的，某天client略微增多后，突然现网etcd集群出现大量超时，各种折腾，切换云盘类型、切换部署环境、调整参数都不发挥作用，收到求助后，索要metrics和日志后，经过一番排查后，得到以下结论: # 现象的确很诡异，db延时相关指标显示没任何异常,日志无任何有效信息 # 业务反馈大量读请求超时，甚至可以通过etcdctl客户端工具简单复现，可是metric对应的读请求相关指标数竟然是0 # 引导用户开启trace日志和metrics开启extensive模式，开启后发现无任何trace日志，然而开启extensive后，我发现耗时竟然全部花在了Authenticate接口，业务反馈是通过密码鉴权，而不是基于证书的鉴权 # 尝试让业务同学短暂关闭鉴权测试业务是否恢复，业务同学找了一个节点关闭鉴权后，此节点立刻恢复了正常，于是选择临时通过关闭鉴权来恢复现网业务 # 那鉴权为什么耗时这么慢？我们对可疑之处增加了日志，打印了鉴权各个步骤的耗时，结果发现是在等待锁的过程中出现了超时，而这个锁为什么耗时这么久呢？排查发现是因为加锁过程中会调用bcrpt加密函数计算密码hash值，每次耗费60ms左右，数百并发下等待此锁的最高耗时高达5s+。 # 于是我们编写新版本将锁的范围减少，降低持锁阻塞时间，用户使用新版本后，开启鉴权后，业务不再超时，恢复正常。 # 随后我们将修复方案提交给了社区，并编写了压测工具，测试提升后的性能高达近12倍（8核32G机器，从18/s提升到202/s），但是依然是比较慢，主要是鉴权过程中涉及密码校验计算, 社区上也有用户反馈密码鉴权慢问题, 目前最新的v3.4.9版本已经包含此优化, 同时可以通过调整bcrpt-cost参数来进一步提升性能。 # 总结 # 本文简单描述了我们在管理万级K8s集群和其他业务过程中遇到的etcd稳定性和性能挑战，以及我们是如何定位、分析、复现、解决这些挑战，并将解决方案贡献给社区。 # 同时，详细描述了我们从这些挑战中收获了哪些宝贵的经验和教训，并将之应用到后续的etcd稳定性保障中，以支持更大规模的单集群和总集群数。 # 最后我们面对万级K8s集群数, 千级的etcd集群数, 10几个版本分布，其中不少低版本包含重要的潜在可能触发的严重bug, 我们还需要投入大量工作不断优化我们的etcd平台，使其更智能、变更更加高效、安全、可控（如支持自动化、可控的集群升级等）, 同时数据安全也至关重要，目前腾讯云TKE托管集群我们已经全面备份，独立集群的用户后续将引导通过应用市场的etcd备份插件开启定时备份到腾讯云对象存储COS上。\n未来我们将继续紧密融入etcd的社区，为etcd社区的发展贡献我们的力量，与社区一块提升etcd的各个功能。\n"},{"id":79,"href":"/docs/ex-lb-%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E9%83%A8%E7%BD%B2-ex-lb-fu-zai-jun-heng-bu-shu/","title":"EX-LB 负载均衡部署 2023-09-28 15:28:51.998","section":"Docs","content":" EX-LB 负载均衡部署 # 根据HA 2x架构，k8s集群自身高可用已经不依赖于外部 lb 服务；但是有时我们要从外部访问 apiserver（比如 CI 流程），就需要 ex_lb 来请求多个 apiserver； # 还有一种情况是需要负载转发到ingress服务，也需要部署ex_lb； # 注意：当遇到公有云环境无法自建 ex_lb 服务时，可以配置对应的云负载均衡服务\nex_lb 服务组件 # 更新：kubeasz 3.0.2 重写了ex-lb服务安装，利用最小化依赖编译安装的二进制文件，不依赖于linux发行版；优点是可以统一版本和简化离线安装部署，并且理论上能够支持更多linux发行版 # ex_lb 服务由 keepalived 和 l4lb 组成： # l4lb：是一个精简版（仅支持四层转发）的nginx编译二进制版本 # keepalived：利用主备节点vrrp协议通信和虚拟地址，消除l4lb的单点故障；keepalived保持存活，它是基于VRRP协议保证所谓的高可用或热备的，这里用来预防l4lb的单点故障。 # keepalived与l4lb配合，实现master的高可用过程如下： # 1.keepalived利用vrrp协议生成一个虚拟地址(VIP)，正常情况下VIP存活在keepalive的主节点，当主节点故障时，VIP能够漂移到keepalived的备节点，保障VIP地址高可用性。 # 2.在keepalived的主备节点都配置相同l4lb负载配置，并且监听客户端请求在VIP的地址上，保障随时都有一个l4lb负载均衡在正常工作。并且keepalived启用对l4lb进程的存活检测，一旦主节点l4lb进程故障，VIP也能切换到备节点，从而让备节点的l4lb进行负载工作。 # 3.在l4lb的配置中配置多个后端真实kube-apiserver的endpoints，并启用存活监测后端kube-apiserver，如果一个kube-apiserver故障，l4lb会将其剔除负载池。 # 安装l4lb # 配置l4lb (roles/ex-lb/templates/l4lb.conf.j2) # 配置由全局配置和三个upstream servers配置组成： # apiservers 用于转发至多个apiserver # ingress-nodes 用于转发至node节点的ingress http服务，参阅 # ingress-tls-nodes 用于转发至node节点的ingress https服务 # 安装keepalived # 配置keepalived主节点 keepalived-master.conf.j2 # global_defs { } vrrp_track_process check-l4lb { process l4lb weight -60 delay 3 } vrrp_instance VI-01 { state MASTER priority 120 unicast_src_ip {{ inventory_hostname }} unicast_peer { {% for h in groups[\u0026#39;ex_lb\u0026#39;] %}{% if h != inventory_hostname %} {{ h }} {% endif %}{% endfor %} } dont_track_primary interface {{ LB_IF }} virtual_router_id {{ ROUTER_ID }} advert_int 3 track_process { check-l4lb } virtual_ipaddress { {{ EX_APISERVER_VIP }} } } vrrp_track_process 定义了监测l4lb进程是否存活，如果进程不存在，根据weight -60设置将主节点优先级降低60，这样原先备节点将变成主节点。 # vrrp_instance 定义了vrrp组，包括优先级、使用端口、router_id、心跳频率、检测脚本、虚拟地址VIP等 # 特别注意 virtual_router_id 标识了一个 VRRP组，在同网段下必须唯一，否则出现 Keepalived_vrrp: bogus VRRP packet received on eth0 !!!类似报错 # 配置 vrrp 协议通过单播发送 # 配置keepalived备节点 keepalived-backup.conf.j2 # 备节点的配置类似主节点，除了优先级和检测脚本，其他如 virtual_router_id advert_int virtual_ipaddress必须与主节点一致 # 启动 keepalived 和 l4lb 后验证 # lb 节点验证 systemctl status l4lb # 检查进程状态 journalctl -u l4lb\t# 检查进程日志是否有报错信息 systemctl status keepalived # 检查进程状态 journalctl -u keepalived\t# 检查进程日志是否有报错信息 在 keepalived 主节点 ip a\t# 检查 master的 VIP地址是否存在 keepalived 主备切换演练 # 尝试关闭 keepalived主节点上的 l4lb进程，然后在keepalived 备节点上查看 master的 VIP地址是否能够漂移过来，并依次检查上一步中的验证项。 # 尝试直接关闭 keepalived 主节点系统，检查各验证项。 # "},{"id":80,"href":"/docs/ezctl-%E5%91%BD%E4%BB%A4%E8%A1%8C%E4%BB%8B%E7%BB%8D-ezctl-ming-ling-xing-jie-shao/","title":"ezctl 命令行介绍 2023-09-28 15:28:00.619","section":"Docs","content":" ezctl 命令行介绍 # 为什么使用 ezctl # kubeasz 项目使用ezctl 方便地创建和管理多个k8s 集群，ezctl 使用shell 脚本封装ansible-playbook 执行命令，它十分轻量、简单和易于扩展。\n使用帮助 # 随时运行 ezctl 获取命令行提示信息，如下\nUsage: ezctl COMMAND [args] ------------------------------------------------------------------------------------- Cluster setups: list\tto list all of the managed clusters checkout \u0026lt;cluster\u0026gt; to switch default kubeconfig of the cluster new \u0026lt;cluster\u0026gt; to start a new k8s deploy with name \u0026#39;cluster\u0026#39; setup \u0026lt;cluster\u0026gt; \u0026lt;step\u0026gt; to setup a cluster, also supporting a step-by-step way start \u0026lt;cluster\u0026gt; to start all of the k8s services stopped by \u0026#39;ezctl stop\u0026#39; stop \u0026lt;cluster\u0026gt; to stop all of the k8s services temporarily upgrade \u0026lt;cluster\u0026gt; to upgrade the k8s cluster destroy \u0026lt;cluster\u0026gt; to destroy the k8s cluster backup \u0026lt;cluster\u0026gt; to backup the cluster state (etcd snapshot) restore \u0026lt;cluster\u0026gt; to restore the cluster state from backups start-aio\tto quickly setup an all-in-one cluster with \u0026#39;default\u0026#39; settings Cluster ops: add-etcd \u0026lt;cluster\u0026gt; \u0026lt;ip\u0026gt; to add a etcd-node to the etcd cluster add-master \u0026lt;cluster\u0026gt; \u0026lt;ip\u0026gt; to add a master node to the k8s cluster add-node \u0026lt;cluster\u0026gt; \u0026lt;ip\u0026gt; to add a work node to the k8s cluster del-etcd \u0026lt;cluster\u0026gt; \u0026lt;ip\u0026gt; to delete a etcd-node from the etcd cluster del-master \u0026lt;cluster\u0026gt; \u0026lt;ip\u0026gt; to delete a master node from the k8s cluster del-node \u0026lt;cluster\u0026gt; \u0026lt;ip\u0026gt; to delete a work node from the k8s cluster Extra operation: kcfg-adm \u0026lt;cluster\u0026gt; \u0026lt;args\u0026gt; to manage client kubeconfig of the k8s cluster Use \u0026#34;ezctl help \u0026lt;command\u0026gt;\u0026#34; for more information about a given command. ezctl checkout ezctl destroy ezctl setup k8s-01 all 命令集 1：集群安装相关操作 显示当前所有管理的集群 切换默认集群 创建新集群配置 安装新集群 启动临时停止的集群 临时停止某个集群（包括集群内运行的pod） 升级集群k8s组件版本 删除集群 备份集群（仅etcd数据，不包括pv数据和业务应用数据） 从备份中恢复集群 创建单机集群（类似 minikube） 命令集 2：集群节点操作 增加 etcd 节点 增加主节点 增加工作节点 删除 etcd 节点 删除主节点 删除工作节点 命令集3：额外操作 管理客户端kubeconfig 举例创建、安装新集群流程 # 1.首先创建集群配置实例 ~# ezctl new k8s-01 2021-01-19 10:48:23 DEBUG generate custom cluster files in /etc/kubeasz/clusters/k8s-01 2021-01-19 10:48:23 DEBUG set version of common plugins 2021-01-19 10:48:23 DEBUG cluster k8s-01: files successfully created. 2021-01-19 10:48:23 INFO next steps 1: to config \u0026#39;/etc/kubeasz/clusters/k8s-01/hosts\u0026#39; 2021-01-19 10:48:23 INFO next steps 2: to config \u0026#39;/etc/kubeasz/clusters/k8s-01/config.yml\u0026#39; 然后根据提示配置\u0026rsquo;/etc/kubeasz/clusters/k8s-01/hosts\u0026rsquo; 和 \u0026lsquo;/etc/kubeasz/clusters/k8s-01/config.yml\u0026rsquo;；为方便测试我们在hosts里面设置单节点集群（etcd/kube_master/kube_node配置同一个节点，注意节点需先设置ssh免密码登陆）, config.yml 使用默认配置即可。\n2.然后开始安装集群 # 一键安装 ezctl setup k8s-01 all # 或者分步安装，具体使用 ezctl help setup 查看分步安装帮助信息 # ezctl setup k8s-01 01 # ezctl setup k8s-01 02 # ezctl setup k8s-01 03 # ezctl setup k8s-01 04 ... 3.重复步骤1，2可以创建、管理多个k8s集群（建议ezctl使用独立的部署节点） ezctl 创建管理的多集群拓扑如下\n+----------------+ +-----------------+ |ezctl 1.1.1.1 | |cluster-aio: | +--+---+---+-----+ | | | | | |master 4.4.4.4 | | | +--------------------\u0026gt;+etcd 4.4.4.4 | | | |node 4.4.4.4 | | +--------------+ +-----------------+ | | v v +--+------------+ +---+----------------------------+ | cluster-1: | | cluster-2: | | | | | | master 2.2.2.1| | master 3.3.3.1/3.3.3.2 | | etcd 2.2.2.2| | etcd 3.3.3.1/3.3.3.2/3.3.3.3 | | node 2.2.2.3| | node 3.3.3.4/3.3.3.5/3.3.3.6 | +---------------+ +--------------------------------+ That\u0026rsquo;s it! 赶紧动手测试吧，欢迎通过 Issues 和 PRs 反馈您的意见和建议！\n"},{"id":81,"href":"/docs/flanel%E7%BD%91%E7%BB%9C-flanel-wang-luo/","title":"flanel网络 2024-04-03 15:05:04.16","section":"Docs","content":" 近几年，企业基础设施云原生化的趋势越来越强烈，从最开始的 IaaS 化到现在的微服务化，客户的颗粒度精细化和可观测性的需求更加强烈。容器网络为了满足客户更高性能和更高的密度，也一直在高速的发展和演进中，这必然对客户对云原生网络的可观测性带来了极高的门槛和挑战。为了提高云原生网络的可观测性，同时便于客户和前后线同学增加对业务链路的可读性，ACK 产研和 AES 联合共建，合作开发 ack net-exporter 和云原生网络数据面可观测性系列，帮助客户和前后线同学了解云原生网络架构体系，简化对云原生网络的可观测性的门槛，优化客户运维和售后同学处理疑难问题的体验 ，提高云原生网络的链路的稳定性。 # 鸟瞰容器网络，整个容器网络可以分为三个部分：Pod 网段，Service 网段和 Node 网段。这三个网络要实现互联互通和访问控制，那么实现的技术原理是什么？整个链路又是什么，限制又是什么呢？Flannel， Terway 有啥区别？不同模式下网络性能如何？这些，需要客户在下搭建容器之前，就要依据自己的业务场景进行选择，而搭建完毕后，相关的架构又是无法转变，所以客户需要对每种架构特点要有充分了解。比如下图是个简图，Pod 网络既要实现同一个ECS的Pod间的网络互通和控制，又要实现不同 ECS Pod 间的访问， Pod 访问 SVC 的后端可能在同一个 ECS 也可能是其他 ECS，这些在不同模式下，数据链转发模式是不同的，从业务侧表现结果也是不一样的。 # 本文是[全景剖析容器网络数据链路]第一部分，主要介绍 Kubernetes Flannel 模式下，数据面链路的转转发链路，一是通过了解不同场景下的数据面转发链路，从而探知客户在不同的场景下访问结果表现的原因，帮助客户进一步优化业务架构；另一方面，通过深入了解转发链路，从而在遇到容器网络抖动时候，客户运维以及阿里云同学可以知道在哪些链路点进行部署观测手动，从而进一步定界问题方向和原因。 # 系列二：\n全景剖析阿里云容器网络数据链路（二）：Terway ENI\n系列三：\n全景剖析阿里云容器网络数据链路（三）：Terway ENIIP\n系列四：\n全景剖析阿里云容器网络数据链路（四）：Terway IPVLAN+EBPF\n系列五：\n全景剖析阿里云容器网络数据链路（五）：Terway ENI-Trunking\n系列六：\n全景剖析阿里云容器网络数据链路（六）：ASM Istio\n01\nFlannel 模式架构设计\nFlannel 模式下，ECS 只有一个主网卡 ENI，无其他附属网卡，ECS 和节点上的 Pod 与外部通信都需要通过主网卡进行。ACK Flannel 会在每个节点创建 cni0 虚拟网卡作为 Pod 网络和 ECS 的主网卡 eth0 之间的桥梁。\n集群的每个节点会起一个 flannel agent，并且会给每个节点预分配一个 Pod CIDR，这个 Pod CIDR 是ACK集群的 Pod CIDR 的子集。\n容器的网络命名空间内会有一个 eth0 的虚拟网卡，同时存在下一跳指向该网卡的路由，该网卡会作为容器和宿主内核进行数据交换的出入口。容器和宿主机之间的数据链路是通过 veth pair 进行交换的，现在我们已经找到 veth pair 其中一个，如何去找另一个 veth 呢？ # 如上图所示，我们可以容器的网络命名空间中通过 p addr 看到一个 eth0@if81 的标志位，其中 ‘81\u0026rsquo; 这个将会协助我们在 ECS 的 OS 内找到找到和容器网络命名空间中的 veth pair 相对一个。在 ECS OS 内我们通过 ip addr | grep 81: 可以找到 vethd7e7c6fd 这个虚拟网卡，这个就是 veth pair 在 ECS OS 侧相对的那一个。 # 到目前为止容器内和 OS 数据链路已经建立链接了，那么 ECS OS 内对于数据流量是怎么判断去哪个容器呢？通过 OS Linux Routing 我们可以看到，所有目的是 Pod CIDR 网段的流量都会被转发到 cni0 这张虚拟网卡，那么 cni0 是通过 bridge 方式将不同目的的数据链路指向到不同的 vethxxx。到这里为止，ECS OS 和 Pod 的网络命名空间已经建立好完整的出入链路配置了。 # 02\nFlannel 模式容器网络数据链路剖析\n针对容器网络特点，我们可以将 Flannel 模式下的网络链路大体分为以 Pod IP 对外提供服务和以 SVC 对外提供服务两个大的 SOP 场景，进一步细分可以拆分到 10 个不同的小的 SOP 场景。 # 对这 10 个场景的数据链路梳理合并，这些场景可以归纳为下面 5 类典型的场景： # Client 和服务端 Pod 部署于同一个 ECS # Client 和服务端 Pod 部署于不同 ECS # 访问 SVC External IP， ExternalTrafficPolicy 为 Cluster 时，Client 和服务端 Pod 部署于不同ECS，其中client为集群外 # 访问 SVC External IP， ExternalTrafficPolicy 为 Local 时, Client 和服务端 Pod 部署于不同 ECS，其中client为集群内 # 访问 SVC External IP， ExternalTrafficPolicy 为 Local 时, Client 和服务端 Pod 部署于不同 ECS，其中 client 为集群外 # 1\n场景一：Client和服务端Pod部署于同一个ECS\n此场景包含下面几个子场景，数据链路可以归纳为一种： # 以 Pod IP 对外提供服务，Client 和 Pod 部署于同一个节点； # 以 SVC ClusterIP 对外提供服务，Client 和 SVC 后端 Pod 部署于同一节点； # 以 SVC ExternalIP 对外提供服务，ExternalTrafficPolicy 为 Cluster/Local 情况下，Client 和 SVC 后端 Pod 部署于同一节点 # 环境 # ap-southeast-1.10.0.0.180 节点上存在两个pod：centos-67756b6dc8-rmmxt IP地址172.23.96.23 和 nginx-7d6877d777-6jkfg 和172.23.96.24\n内核路由 # centos-67756b6dc8-rmmxt IP地址172.23.96.23，该容器在宿主机表现的 PID 是503478，该容器网络命名空间有指向容器 eth0 的默认路由 # 该容器 eth0 在 ECS OS 内对应 veth pair 是 vethd7e7c6fd # 通过上述类似的办法，可以找到nginx-7d6877d777-6jkfg IP地址172.23.96.24，该容器在宿主机表现的 PID 是2981608，该容器 eth0 在 ECS OS 内对应veth pair是vethd3fc7ff4 # 在 ECS OS 内，有指向 Pod CIDR，下一跳为 cni0 的路由，以及 cni0 中有两个容器的vethxxx 网桥信息 # 小结**：**可以访问到目的端 # ▲\n数据链路转发示意图\n▲\n内核协议栈示意图\n数据链路：ECS1 Pod1 eth0 -\u0026gt; vethxxx1 -\u0026gt; cni0 -\u0026gt; vethxxxx2 -\u0026gt; ECS1 Pod2 eth0 # 数据链路要经过三次内核协议栈，分别是 Pod1 协议栈， ECS OS 协议栈 和 Pod2 协议栈 # 2\n场景二：Client 和服务端 Pod 部署于不同 ECS\n此场景包含下面几个子场景，数据链路可以归纳为一种： # 以 Pod IP 对外提供服务，Client 和 Pod 部署于不同节点； # 以 SVC ClusterIP 对外提供服务，Client和SVC 后端Pod部署于不同节点； # 以 SVC ExternalIP 对外提供服务，ExternalTrafficPolicy 为 Cluster 情况下，集群内 Client 和 SVC 后端 Pod 部署于不同节点； # 环境 # ap-southeast-1.10.0.0.180 节点上存在两个 pod：centos-67756b6dc8-rmmxt IP地址172.23.96.23 和 nginx1-76c99b49df-7plsr IP 地址172.23.96.163 # Service nginx1 的 ExternalTrafficPlicy 为 Cluster # 内核路由 # Pod 网络空间和 ECS OS 网络空间的数据交换在 2.1 场景一中已经做了详细描述，此处不再果断篇幅描述。 # 源端 Pod 所在 ECS 的 IPVS 规则 # 可以看到，源端数据链路访问 svc 的clusterip 192.168.13.23 时，如果链路到达 ECS 的OS 内，会命中 ipvs 规则，被解析到 svc 的后端 endpoint 之一（本实例中只有一个 pod，所以 endpoint 只有一个） # **小结：**可以成功访问到目的端 # ▲\n数据链路转发示意图\nVPC 路由表会自动配置目的地址是 pod CIDR， 下一跳为 POD 网段所归属的 ECS 的自定义路由条目，该规则由 ACK 管控测通过 openapi 调用 VPC 去配置，无需手动配置和删除\n▲\n内核协议栈示意图\nConntack 表信息（访问 SVC 情况）\nNode1:\nsrc是源pod IP，dst是svc的ClusterIP，并且期望是由svc的其中一个endpoint 172.23.96.163 来回消息给源端 pod\nNode2:\n目的 pod 所在 ECS 上 conntrack 表记录是由源端 pod 访问目的 pod，不会记录 svc 的clusterip 地址\n数据链路：ECS1 Pod1 eth0 -\u0026gt; vethxxx1 -\u0026gt; cni0 -\u0026gt; ECS 1 eth0 -\u0026gt; VPC -\u0026gt; ECS2 eth0 -\u0026gt; cni0 -\u0026gt; vethxxxx2 -\u0026gt; ECS2 Pod2 eth0 # 数据链路要经过四次内核协议栈，分别是 Pod1 协议栈， ECS1 OS 协议栈 ， ECS2 OS 协议栈和 Pod2 协议栈 # VPC 路由表会自动配置目的地址是 pod CIDR， 下一跳为 POD 网段所归属的 ECS 的自定义路由条目，该规则由 ACK 管控测通过 openapi 调用 VPC 去配置，无需手动配置和删除 # 如果访问的 SVC 的 cluster IP，或者是 Cluster 模式下，访问 SVC 的 externalIP，数据链路通过veth pair进到 ECS OS 内后，会命中相应的 IPVS 规则，并根据负载规则，选择IPVS的某一个后端，进而打到其中的一个 SVC 的后端 endpoint，SVC 的 IP 只会再 PoD 的 eth0 ， veth pair vethxxx 被捕捉到，其他链路环节不会捕捉到 svc 的 IP # 3\n场景三：ExternalTrafficPolicy 为 Local时，Client 和服务端 Pod 部署于集群内不同 ECS\n此场景包含下面几个子场景，数据链路可以归纳为一种： # 1.以SVC ExternalIP对外提供服务，ExternalTrafficPolicy 为 Local情况下，集群内 Client 和 SVC 后端 Pod 部署于不同节点； # 环境 # ap-southeast-1.10.0.0.180 节点上存在两个pod：centos-67756b6dc8-rmmxt IP地址172.23.96.23 和 nginx1-76c99b49df-7plsr IP 地址172.23.96.163\nService nginx1 ExternalTrafficPolicy 为 Local\n内核路由 # Pod 网络空间和 ECS OS 网络空间的数据交换在 2.1 场景一中已经做了详细描述，此处不再果断篇幅描述。\n源端 Pod 所在 ECS 的 IPVS 规则 # 可以看到，源端数据链路访问 svc 的 externalip 8.219.164.113 时，如果链路到达 ECS 的OS内，会命中ipvs规则，但是我们可以看到EcternalIP 并没有相关的后端 endpoint，链路达到 OS 后，会命中 IPVS 规则，但是没有后端 pod，所以会出现 connection refused # **小结：**不可以访问到目的端，此时会访问失败，Connection refused # ▲\n数据链路转发示意图\n▲\n内核协议栈示意图\n数据链路：ECS1 Pod1 eth0 -\u0026gt; vethxxx1 -\u0026gt; # 数据链路要经过一次半内核协议栈，分别是 Pod1 协议栈，半个 ECS1 OS 协议栈 # 如果访问的 SVC 的 External IP，或者是 Local 模式下，访问 SVC 的 externalIP，数据链路通过 veth pair 进到 ECS OS内后，会命中相应的 IPVS 规则，但是由于 Local 模式，External IP的 IPVS 为空，所以命中规则但是无转发后端，整个链路会在 ipvs 终止，访问失败。所以建议集群内采用 clusterip 访问，这也是 k8s 官方推荐的最佳实践。 # 4\n场景四：*ExternalTrafficPolicy 为 Local 时，Client 来自于集群外*\n此场景包含下面几个子场景，数据链路可以归纳为一种： # A.访问SVC External IP， ExternalTrafficPolicy 为 Local时, Client 和服务端 Pod 部署于不同 ECS，其中 client 为集群外 # 环境 # Deployment为nginx1， 分别为三个pod nginx1-76c99b49df-4zsdj和nginx1-76c99b49df-7plsr 部署在 ap-southeast-1.10.0.1.206 ECS上，最后一个pod nginx1-76c99b49df-s6z79 部署在其他节点 ap-southeast-1.10.0.1.216 上 # Service nginx1 的ExternalTrafficPlicy 为 Local # 内核路由 # Pod 网络空间和 ECS OS 网络空间的数据交换在 2.1 场景一中已经做了详细描述，此处不再果断篇幅描述。 # SLB 相关配置 # 从 SLB 控制台，可以看到 SLB 后端的虚拟服务器组中只有两个 ECS 节点ap-southeast-1.10.0.1.216 和 ap-southeast-1.10.0.1.206。集群内的其他节点 ，比如 ap-southeast-1.10.0.0.180 并未被加到 SLB 的后端虚拟服务器组中。虚拟服务器组的 IP 为 ECS 的 IP，端口为 service 里面的 nodeport 端口 32580. # 故 ExternalTrafficPolicy 为 Local 模式下，只有有 Service 后端 pod 所在的 ECS 节点才会被加入到 SLB 的后端虚拟服务器组中，参与 SLB 的流量转发，集群内的其他节点不参与 SLB 的转发\nSLB 虚拟服务器组 ECS 的 IPVS 规则 # 从SLB的虚拟服务器组中的两个 ECS 可以看到，对于 nodeip+nodeport 的 ipvs转发规则是不同。ExternalTrafficPolicy 为 Local 模式下，只有该节点上的护短 pod 才会被加到该节点的 ipvs 转发规则中，其他节点上的后端 pod 不会加进来，这样保证了被 SLB 转发的链路，只会被转到该节点上的 pod，不会转发到其他节点上。 # node1：ap-southeast-1.10.0.1.206 # node1：ap-southeast-1.10.0.1.216 # 小结：可以访问到目的端 # ▲\n数据链路转发示意图\n该图示意了只有后端 pod 所在 ECS 才会加到 SLB 后端中，从集群外部访问 SVC 的externalIP（SLB IP）的情况，可见数据链路只会被转发到虚拟服务器组中的 ECS，不会再被转发到集群内其他节点上。 # ▲\n内核协议栈示意图\nConntack 表信息\nNode:\nsrc 是集群外部客户端IP，dst 是节点 IP，dport 是 SVC 中的 nodeport。并且期望是由该 ECS 上的 pod 172.23.96.82 来回包给源端 # 数据链路：client -\u0026gt; SLB -\u0026gt; ECS eth0 + ECS nodeport -\u0026gt; cni0 -\u0026gt; vethxxxxx -\u0026gt; ECS1 Pod1 eth0 # 数据链路要经过两次内核协议栈，分别是 Pod1 协议栈， ECS1 OS 协议栈 # ExternalTrafficPolicy 为 Local 模式下，只有有 Service 后端 pod 所在的 ECS 节点才会被加入到SLB的后端虚拟服务器组中，参与 SLB 的流量转发，集群内的其他节点不参与 SLB 的转发 # 5\n场景五：*ExternalTrafficPolicy 为 Cluster 时，Client 来自于集群外*\n此场景包含下面几个子场景，数据链路可以归纳为一种： # 1.访问 SVCExternal IP， ExternalTrafficPolicy 为 Cluster 时, Client 和服务端 Pod部署于不同 ECS，其中 client 为集群外 # 环境 # Deployment为nginx1， 分别为三个pod nginx1-76c99b49df-4zsdj和nginx1-76c99b49df-7plsr 部署在 ap-southeast-1.10.0.1.206 ECS上，最后一个pod nginx1-76c99b49df-s6z79 部署在其他节点 ap-southeast-1.10.0.1.216 上 # Service nginx2 的 ExternalTrafficPlicy 为 Cluster # 内核路由 # Pod 网络空间和 ECS OS 网络空间的数据交换在 2.1 场景一中已经做了详细描述，此处不再果断篇幅描述。 # SLB 相关配置 # 从SLB 控制台，集群内所有节点ap-southeast-1.10.0.0.180、ap-southeast-1.10.0.1.216 和 ap-southeast-1.10.0.1.206 都被加到SLB的虚拟服务器组中。其中 虚拟服务器组的 IP 为 ECS 的 IP，端口为 service 里面的 nodeport 端口 30875. # 故 ExternalTrafficPolicy 为 CLuster 模式下，集群内所有的 ECS 节点都会被加入到 SLB 的后端虚拟服务器组中，参与 SLB 的流量转发。\nSLB 虚拟服务器组 ECS 的 IPVS 规则 # 从SLB的虚拟服务器组中的可以看到，对于 nodeip+nodeport 的 ipvs 转发规则是一致的。ExternalTrafficPolicy为 CLuster 模式下，所有的 service 后端 pod 都会被加到所有节点的 ipvs 的转发规则中，即使是该节点有后端 pod，流量也不一定会被转发到该节点上 pod，可能会被转发到其他节点上的后端 pod。 # node1：ap-southeast-1.10.0.1.206 (该节点有后端pod) # node1：ap-southeast-1.10.0.1.216 (该节点有后端pod) # node3: ap-southeast-1.10.0.0.180 (该节无后端pod) # 小结：可以访问到目的端 # ▲\n数据链路转发示意图\n该图示意了集群内所有 ECS 都会被加到 SLB 后端中，从集群外部访问 SVC 的externalIP（SLB IP）的情况，数据流量可能会被转发到其他节点上 # 内核协议栈示意图：\n内核协议栈示意图已经在 2.4 场景一中已经做了详细描述，此处不再果断篇幅描述。 # Conntack 表信息\n链路1： # ap-southeast-1.10.0.0.180: # 此时数据链路对应示意图中的链路1，可以看到数据链路被转到 ap-southeast-1.10.0.0.180 节点，该节点上并没有 service 的后端 pod，通过 conntrack 信息，可以看到 # src 是集群外部客户端 IP，dst 是节点 IP，dport 是 SVC 中的 nodeport。并且期望是172.23.96.163 来回包给 10.0.0.180 。通过前述信息，可以得知172.23.96.163 是nginx1-76c99b49df-7plsr pod，部署在 ap-southeast-1.10.0.1.206 # ap-southeast-1.10.0.1.206： # 通过此节点conntrack 表，可以看到src是 node ap-southeast-1.10.0.0.180，dst 是 172.23.96.163的80 端口，回包也是直接回给 node ap-southeast-1.10.0.0.180. # 综上可以看到 src 变换了多次，故在 CLuster 模式下，会存在丢失真实客户端 IP 的情况 # 链路2：\nsrc 是集群外部客户端 IP，dst 是节点IP，dport 是 SVC 中的 nodeport。并且期望是由该 ECS 上的 pod 172.23.96.82 来回包给 172.23.96.65，此地址是 SLB 集群中的一个地址 # 数据链路：\n情景一：client -\u0026gt; SLB -\u0026gt; ECS eth0 + ECS nodeport -\u0026gt; cni0 -\u0026gt; vethxxxxx -\u0026gt; ECS1 Pod1 eth0 # 情景二：client -\u0026gt; SLB -\u0026gt; ECS1 eth0 + ECS1 nodeport -\u0026gt; VPC Routing -\u0026gt; ECS2 eth0 + pod port -\u0026gt; cni0 -\u0026gt; vethxxxxx -\u0026gt; ECS2 Pod1 eth0 # 数据链路要经过三次内核协议栈，分别是 ECS1 OS、 ECS2 OS 协议栈 和 Pod 协议栈 # ExternalTrafficPolicy 为 CLuster 模式下，kubernetes 所有 ECS 节点都会被加入到 SLB 的后端虚拟服务器组中，参与 SLB 的流量转发，此时会存在数据路在集群内被多个 ECS 转发的场景，该情况下会丢失真实客户端IP的情况 # 总结\nCloud Native* *\n本篇文章主要聚焦 ACK 在 Flannel 模式下，不同 SOP 场景下的数据链路转发路径。随着微服务化和云原生化，网络场景日趋复杂，作为 kubernetes 原生的网络模型——Flannel，不同的访问环境，一共可以分为10个 SOP 场景。通过深入简出的剖析，可以归纳为5个场景，并对这五个场景的转发链路，技术实现原理，云产品配置等一一梳理并总结，这对我们遇到 Flannel 架构下的链路抖动、最优化配置，链路原理等提供了初步指引方向。接下来将进入到阿里自研的 CNI 接口 Terway 模式，也是目前线上集群使用最多的模式，下一系列我们先带来 Terway ENI 模式的全景解析——ACK 全景剖析阿里云容器网络数据链路（二）：Terway ENI。 # "},{"id":82,"href":"/docs/godel-scheduler-godel-scheduler/","title":"godel-scheduler 2024-08-02 17:41:00.668","section":"Docs","content":" Local Gödel Environment Setup with KIND # 使用KIND设置本地哥德尔环境\nThis guide will walk you through how to set up the Gödel Unified Scheduling system.\n本指南将指导您如何设置哥德尔统一调度系统。\nOne-Step Cluster Bootstrap \u0026amp; Installation # 一步式集群引导和安装\nWe provided a quick way to help you try Gödel on your local machine, which will set up a kind cluster locally and deploy necessary crds, clusterrole and rolebindings\n我们提供了一种快速的方法来帮助您在本地计算机上尝试Gödel，它将在本地设置一个类集群，并部署必要的crd、clusterrole和rolebindings\nPrerequisites # 先决条件\nPlease make sure the following dependencies are installed.\n请确保安装了以下依赖项。\nkubectl \u0026gt;= v1.19 docker \u0026gt;= 19.03 kind \u0026gt;= v0.17.0 go \u0026gt;= v1.21.4 kustomize \u0026gt;= v4.5.7 1. Clone the Gödel repo to your machine # 将哥德尔仓库克隆到您的计算机上\n$ git clone https://github.com/kubewharf/godel-scheduler 2. Change to the Gödel directory # 切换到哥德尔目录\n$ cd godel 3. Bootstrap the cluster and install Gödel components # 引导集群并安装哥德尔组件\n$ make local-up This command will complete the following steps:\nBuild Gödel image locally;\nStart a Kubernetes cluster using Kind;\nInstalls the Gödel control-plane components on the cluster.\n此命令将完成以下步骤：\n1.在当地树立哥德尔形象； 2.使用Kind启动Kubernetes集群； 3.在仪表盘上安装哥德尔控制平面组件。\nManual Installation # 手动安装\nIf you have an existing Kubernetes cluster, please follow the steps below to install Gödel.\n如果您有一个现有的Kubernetes集群，请按照以下步骤安装Gödel。\n1. Build Gödel image # 1.塑造哥德尔形象\nmake docker-images 2. Load Gödel image to your cluster # 2.将哥德尔图像加载到集群\nFor example, if you are using Kind\n例如，如果您正在使用Kind\nkind load docker-image godel-local:latest --name \u0026lt;cluster-name\u0026gt; --nodes \u0026lt;control-plane-nodes\u0026gt; 3. Create Gödel components in the cluster # 3.在集群中创建哥德尔组件\nkustomize build manifests/base/ | kubectl apply -f - #!/bin/bash # error on exit set -e # Main of the script DOCKERFILE_DIR=\u0026#34;docker/\u0026#34; LOCAL_DOCKERFILE=\u0026#34;godel-local.Dockerfile\u0026#34; echo \u0026#34;Building docker image(s)...\u0026#34; docker container prune -f || true; docker image prune -f || true function cleanup_godel_images() { for i in $(docker images | grep \u0026#34;${1}\u0026#34; | awk \u0026#34;{print \\$3}\u0026#34;); do docker rmi \u0026#34;$i\u0026#34; -f; done } build_image() { local file=${1} REPO=$(basename \u0026#34;$file\u0026#34;); REPO=${REPO%.*}; # Check if we are in a Git repository if [ -d \u0026#34;.git\u0026#34; ]; then REV=$(git log --pretty=format:\u0026#39;%h\u0026#39; -n 1) TAG=\u0026#34;${REPO}:${REV}\u0026#34; else # Use a fixed tag if not in a Git repository REV=\u0026#34;non-git-commit\u0026#34; TAG=\u0026#34;${REPO}:${REV}\u0026#34; echo \u0026#34;Warning: Building image with non-Git tag \u0026#39;${TAG}\u0026#39; because this is not a Git repository.\u0026#34; fi cleanup_godel_images \u0026#34;${REPO}\u0026#34; docker build -t \u0026#34;${TAG}\u0026#34; -f \u0026#34;$file\u0026#34; ./; docker tag \u0026#34;${TAG}\u0026#34; \u0026#34;${REPO}:latest\u0026#34; } for file in $(find \u0026#34;$DOCKERFILE_DIR\u0026#34; -name *.Dockerfile); do build_image \u0026#34;${file}\u0026#34; done; Quickstart - Job Level Affinity # 快速入门-工作级别亲和力\nThis Quickstart guide provides a step-by-step tutorial on how to effectively use the job-level affinity feature for podgroups, with a focus on both preferred and required affinity types. For comprehensive information about this feature, please consult the Job Level Affinity Design Document. \u0026lt;TODO Add the design doc when it\u0026rsquo;s ready\u0026gt;\n本快速入门指南提供了一个分步教程，介绍如何有效地使用播客组的作业级关联功能，重点介绍首选和必需的关联类型。有关此功能的全面信息，请参阅工作级亲和力设计文档。 \u0026lt;TODO准备好后添加设计文档\u0026gt;\nLocal Cluster Bootstrap \u0026amp; Installation # 本地集群引导和安装\nTo try out this feature, we would need to set up a few labels for the Kubernetes cluster nodes. We provided a make command for you to boostrap such a cluster locally using KIND.\n要尝试此功能，我们需要为Kubernetes集群节点设置一些标签。我们为您提供了一个make命令，用于使用KIND在本地对这样的集群进行助推。\nmake local-up-labels Affinity-Related Configurations # 亲和性相关配置\nNode # 节点\nIn our sample YAML for KIND, we defined \u0026lsquo;mainnet\u0026rsquo; and \u0026lsquo;micronet\u0026rsquo; as custom node labels. These labels are employed to simulate real-world production environments, specifically regarding the network configurations of nodes.\n在我们的KIND示例YAML中，我们将“主网”和“微网”定义为自定义节点标签。这些标签用于模拟现实世界的生产环境，特别是关于节点的网络配置。\n- role: worker image: kindest/node:v1.21.1 labels: micronet: 10.76.65.0 mainnet: 10.76.0.0 PodGroup # In our sample YAML for podgroup, we specified \u0026lsquo;podGroupAffinity\u0026rsquo;. This configuration stipulates that pods belonging to this podgroup should be scheduled on nodes within the same \u0026lsquo;mainnet\u0026rsquo;. Additionally, there\u0026rsquo;s a preference to schedule them on nodes sharing the same \u0026lsquo;micronet\u0026rsquo;.\n在我们的podgroup示例YAML中，我们指定了“podGroupAffinity”。此配置规定，属于此Pod组的Pod应安排在同一“主网”内的节点上。此外，人们更倾向于将它们安排在共享相同“微网络”的节点上。\napiVersion: scheduling.godel.kubewharf.io/v1alpha1 kind: PodGroup metadata: generation: 1 name: nginx spec: affinity: podGroupAffinity: preferred: - topologyKey: micronet required: - topologyKey: mainnet minMember: 10 scheduleTimeoutSeconds: 3000 Deployment # Specify the podgroup name in pod spec.\n在pod规范中指定pod组名称。\napiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 10 selector: matchLabels: name: nginx template: metadata: name: nginx labels: name: nginx godel.bytedance.com/pod-group-name: \u0026#34;nginx\u0026#34; annotations: godel.bytedance.com/pod-group-name: \u0026#34;nginx\u0026#34; spec: schedulerName: godel-scheduler containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent resources: limits: cpu: 1 requests: cpu: 1 Use Job Level Affinity in Scheduling # 在调度中使用作业级别相关性\nFirst, let\u0026rsquo;s check out the labels for nodes in the cluster.\n首先，让我们查看集群中节点的标签。\n$ kubectl get nodes --show-labels NAME STATUS ROLES AGE VERSION LABELS godel-demo-labels-control-plane Ready control-plane,master 37m v1.21.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=godel-demo-labels-control-plane,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node-role.kubernetes.io/master=,node.kubernetes.io/exclude-from-external-load-balancers= godel-demo-labels-worker Ready \u0026lt;none\u0026gt; 36m v1.21.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=godel-demo-labels-worker,kubernetes.io/os=linux,mainnet=10.76.0.0,micronet=10.76.64.0,subCluster=subCluster-a godel-demo-labels-worker2 Ready \u0026lt;none\u0026gt; 36m v1.21.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=godel-demo-labels-worker2,kubernetes.io/os=linux,mainnet=10.25.0.0,micronet=10.25.162.0,subCluster=subCluster-b godel-demo-labels-worker3 Ready \u0026lt;none\u0026gt; 36m v1.21.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=godel-demo-labels-worker3,kubernetes.io/os=linux,mainnet=10.76.0.0,micronet=10.76.65.0,subCluster=subCluster-a godel-demo-labels-worker4 Ready \u0026lt;none\u0026gt; 36m v1.21.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=godel-demo-labels-worker4,kubernetes.io/os=linux,mainnet=10.76.0.0,micronet=10.76.64.0,subCluster=subCluster-a godel-demo-labels-worker5 Ready \u0026lt;none\u0026gt; 36m v1.21.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=godel-demo-labels-worker5,kubernetes.io/os=linux,mainnet=10.53.0.0,micronet=10.53.16.0,subCluster=subCluster-a godel-demo-labels-worker6 Ready \u0026lt;none\u0026gt; 36m v1.21.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=godel-demo-labels-worker6,kubernetes.io/os=linux,mainnet=10.57.0.0,micronet=10.57.111.0,subCluster=subCluster-b godel-demo-labels-worker and godel-demo-labels-worker4 share the same \u0026lsquo;micronet\u0026rsquo;; 哥德尔演示标签工人和哥德尔demo-labels-worker4**共享相同的“微网”； godel-demo-labels-worker, godel-demo-labels-worker-3, and godel-demo-labels-worker4 share the same \u0026lsquo;mainnet\u0026rsquo;. 哥德尔演示标签worker、godel-demo-labels-worker-3和godel-demo-labels-worker4共享同一个“主网”。 Second, create the podgroup and deployment.\n其次，创建podgroup和部署。\n$ kubectl apply -f manifests/quickstart-feature-examples/job-level-affinity/podGroup.yaml podgroup.scheduling.godel.kubewharf.io/nginx created $ kubectl apply -f manifests/quickstart-feature-examples/job-level-affinity/deployment.yaml deployment.apps/nginx created Third, check the scheduling result.\n第三，检查调度结果。\n$ kubectl get pods -l name=nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-68fc9649cc-5pdb7 1/1 Running 0 8s 10.244.2.21 godel-demo-labels-worker \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-68fc9649cc-b26tk 1/1 Running 0 8s 10.244.2.19 godel-demo-labels-worker \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-68fc9649cc-bvvx6 1/1 Running 0 8s 10.244.2.18 godel-demo-labels-worker \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-68fc9649cc-dtxqn 1/1 Running 0 8s 10.244.2.23 godel-demo-labels-worker \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-68fc9649cc-hh5pr 1/1 Running 0 8s 10.244.3.34 godel-demo-labels-worker4 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-68fc9649cc-jt8q9 1/1 Running 0 8s 10.244.3.35 godel-demo-labels-worker4 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-68fc9649cc-l8j2s 1/1 Running 0 8s 10.244.2.20 godel-demo-labels-worker \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-68fc9649cc-t9fb8 1/1 Running 0 8s 10.244.3.33 godel-demo-labels-worker4 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-68fc9649cc-vcjm7 1/1 Running 0 8s 10.244.2.17 godel-demo-labels-worker \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-68fc9649cc-wplt7 1/1 Running 0 8s 10.244.2.22 godel-demo-labels-worker \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; The pods have been scheduled to godel-demo-labels-worker and godel-demo-labels-worker4, which share the same \u0026lsquo;micronet\u0026rsquo;. We achieved this result because the resources are sufficient.\n这些吊舱已被安排为godel演示标签worker和**godel-demo-labels-worker4*，它们共享相同的“微网”。我们之所以取得这一成果，是因为资源充足。\nNext, let\u0026rsquo;s try with scheduling a podgroup with minMember equal to 15, with the else of the configuration remains the same.\n接下来，让我们尝试调度一个minMember等于15的podgroup，其他配置保持不变。\nIn manifests/quickstart-feature-examples/job-level-affinity/podGroup-2.yaml, notice the minMember is 15. 在“manifests/quickstart features examples/job-level affinity/podGroup-2.yaml”中，注意minMember是15。 minMember: 15 In manifests/quickstart-feature-examples/job-level-affinity/deployment-2.yaml, notice the replicas is 15. 在“manifests/quickstart features examples/job-level affinity/deployment-2.yaml”中，请注意副本为15。 spec: replicas: 15 Apply the two yaml files and check the scheduling result\n应用两个yaml文件并检查调度结果\n# Clean up the env first $ kubectl delete -f manifests/quickstart-feature-examples/job-level-affinity/deployment.yaml \u0026amp;\u0026amp; kubectl delete -f manifests/quickstart-feature-examples/job-level-affinity/podGroup.yaml deployment.apps \u0026#34;nginx\u0026#34; deleted podgroup.scheduling.godel.kubewharf.io \u0026#34;nginx\u0026#34; deleted $ kubectl apply -f manifests/quickstart-feature-examples/job-level-affinity/podGroup-2.yaml podgroup.scheduling.godel.kubewharf.io/nginx-2 created $ kubectl apply -f manifests/quickstart-feature-examples/job-level-affinity/deployment-2.yaml deployment.apps/nginx-2 created $ kubectl get pods -l name=nginx-2 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-2-68fc9649cc-2l2v7 1/1 Running 0 6s 10.244.6.11 godel-demo-labels-worker3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-2-68fc9649cc-6mz78 1/1 Running 0 6s 10.244.2.13 godel-demo-labels-worker \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-2-68fc9649cc-6nm92 1/1 Running 0 6s 10.244.6.12 godel-demo-labels-worker3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-2-68fc9649cc-6qmmx 1/1 Running 0 6s 10.244.2.14 godel-demo-labels-worker \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-2-68fc9649cc-cfd75 1/1 Running 0 6s 10.244.2.11 godel-demo-labels-worker \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-2-68fc9649cc-fg87r 1/1 Running 0 6s 10.244.3.28 godel-demo-labels-worker4 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-2-68fc9649cc-gss27 1/1 Running 0 6s 10.244.3.26 godel-demo-labels-worker4 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-2-68fc9649cc-hbpwt 1/1 Running 0 6s 10.244.6.15 godel-demo-labels-worker3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-2-68fc9649cc-jkdqx 1/1 Running 0 6s 10.244.3.27 godel-demo-labels-worker4 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-2-68fc9649cc-n498k 1/1 Running 0 6s 10.244.6.9 godel-demo-labels-worker3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-2-68fc9649cc-q5h5r 1/1 Running 0 6s 10.244.2.12 godel-demo-labels-worker \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-2-68fc9649cc-qjsgk 1/1 Running 0 6s 10.244.6.14 godel-demo-labels-worker3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-2-68fc9649cc-vdp2v 1/1 Running 0 6s 10.244.6.13 godel-demo-labels-worker3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-2-68fc9649cc-vpzlj 1/1 Running 0 6s 10.244.6.10 godel-demo-labels-worker3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-2-68fc9649cc-z2ffg 1/1 Running 0 6s 10.244.3.29 godel-demo-labels-worker4 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; The pods have been scheduled to godel-demo-labels-worker, godel-demo-labels-worker3 and godel-demo-labels-worker4. godel-demo-labels-worker3 was used here because the resources on worker and worker4 were not sufficient. And worker3 met the requirement of the same mainnet.\n这些豆荚已被安排为哥德尔演示标签工人、哥德尔-德莫标签工人和戈德尔-德莫尔标签工人**这里使用godel-demo-labels-worker3是因为worker和worker4上的资源不足。worker3满足同一主网的要求。\nClean up the environment.\n清理环境。\n$ kubectl delete -f manifests/quickstart-feature-examples/job-level-affinity/podGroup-2.yaml \u0026amp;\u0026amp; kubectl delete -f manifests/quickstart-feature-examples/job-level-affinity/deployment-2.yaml podgroup.scheduling.godel.kubewharf.io \u0026#34;nginx-2\u0026#34; deleted deployment.apps \u0026#34;nginx-2\u0026#34; deleted Quickstart - SubCluster Concurrent Scheduling # 快速入门-子集群并发调度\nThis Quickstart guide demonstrates how to implement concurrent scheduling at the SubCluster level. Each SubCluster, defined by node labels, will possess its own distinct scheduling workflow. These workflows will execute simultaneously, ensuring efficient task management across the system.\n本快速入门指南演示了如何在子集群级别实现并发调度。由节点标签定义的每个子集群都将拥有自己独特的调度工作流。这些工作流将同时执行，确保整个系统的高效任务管理。\nLocal Cluster Bootstrap \u0026amp; Installation # 本地集群引导和安装\nTo try out this feature, we would need to set up a few labels for the Kubernetes cluster nodes. We provided a make command for you to boostrap such a cluster locally using KIND.\n要尝试此功能，我们需要为Kubernetes集群节点设置一些标签。我们为您提供了一个make命令，用于使用KIND在本地对这样的集群进行助推。\nmake local-up-labels Related Configurations # 相关配置\nNode # In our sample YAML for KIND, we defined \u0026lsquo;subCluster\u0026rsquo; as custom node labels. These labels are employed to simulate real-world production environments, where nodes can be classified by different business scenarios.\n在我们的KIND示例YAML中，我们将“subCluster”定义为自定义节点标签。这些标签用于模拟现实世界的生产环境，其中节点可以按不同的业务场景进行分类。\n- role: worker image: kindest/node:v1.21.1 labels: subCluster: subCluster-a - role: worker image: kindest/node:v1.21.1 labels: subCluster: subCluster-b Godel Scheduler Configuration # Godel调度程序配置\nWe defined \u0026lsquo;subCluster\u0026rsquo; as the subClusterKey in the Godel Scheduler Configuration. This is corresponding to the node label key above.\n我们在Godel调度程序配置中将“subCluster”定义为subClusterKey。这对应于上面的节点标签键。\napiVersion: godelscheduler.config.kubewharf.io/v1beta1 kind: GodelSchedulerConfiguration subClusterKey: subCluster Deployment # In the sample deployment file, one deployment specifies \u0026lsquo;subCluster: subCluster-a\u0026rsquo; in its nodeSelector, while the other deployment specifies \u0026lsquo;subCluster: subCluster-b\u0026rsquo;.\n在示例部署文件中，一个部署在其nodeSelector中指定“subCluster:subCluster-a”，而另一个部署指定“subCluster:subCluster-b”。\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-a spec: ... spec: schedulerName: godel-scheduler nodeSelector: subCluster: subCluster-a ... --- apiVersion: apps/v1 kind: Deployment metadata: name: nginx-b spec: ... spec: schedulerName: godel-scheduler nodeSelector: subCluster: subCluster-b ... Concurrent Scheduling Quick Demo # 并发调度快速演示\nApply the deployment YAML files.\n应用部署YAML文件。\n$ kubectl apply -f manifests/quickstart-feature-examples/concurrent-scheduling/deployment.yaml deployment.apps/nginx-a created deployment.apps/nginx-b created Check the scheduler logs.\n检查调度程序日志。\n$ kubectl get pods -n godel-system NAME READY STATUS RESTARTS AGE binder-858c974d4c-bhbtv 1/1 Running 0 123m dispatcher-5b8f5cf5c6-fcm8d 1/1 Running 0 123m scheduler-85f4556799-6ltv5 1/1 Running 0 123m $ kubectl logs scheduler-85f4556799-6ltv5 -n godel-system ... I0108 22:38:36.183175 1 util.go:90] \u0026#34;Ready to try and schedule the next unit\u0026#34; numberOfPods=1 unitKey=\u0026#34;SinglePodUnit/default/nginx-b-69ccdbff54-fvpg6\u0026#34; I0108 22:38:36.183190 1 unit_scheduler.go:280] \u0026#34;Attempting to schedule unit\u0026#34; switchType=\u0026#34;GTSchedule\u0026#34; subCluster=\u0026#34;subCluster-b\u0026#34; unitKey=\u0026#34;SinglePodUnit/default/nginx-b-69ccdbff54-fvpg6\u0026#34; I0108 22:38:36.183229 1 util.go:90] \u0026#34;Ready to try and schedule the next unit\u0026#34; numberOfPods=1 unitKey=\u0026#34;SinglePodUnit/default/nginx-a-649b85664f-2tjvp\u0026#34; I0108 22:38:36.183246 1 unit_scheduler.go:280] \u0026#34;Attempting to schedule unit\u0026#34; switchType=\u0026#34;GTSchedule\u0026#34; subCluster=\u0026#34;subCluster-a\u0026#34; unitKey=\u0026#34;SinglePodUnit/default/nginx-a-649b85664f-2tjvp\u0026#34; I0108 22:38:36.183390 1 unit_scheduler.go:327] \u0026#34;Attempting to schedule unit in this node group\u0026#34; switchType=\u0026#34;GTSchedule\u0026#34; subCluster=\u0026#34;subCluster-b\u0026#34; unitKey=\u0026#34;SinglePodUnit/default/nginx-b-69ccdbff54-fvpg6\u0026#34; nodeGroup=\u0026#34;[]\u0026#34; ... From the log snippet above, it\u0026rsquo;s clear that both pods, nginx-a (subCluster: subCluster-a) and nginx-b (subCluster: subCluster-b), are overlapping.\n从上面的日志片段中可以清楚地看出，两个Pod nginx-a（subCluster:subCluster-a）和nginx-b（subCluster/subCluster-b）是重叠的。\nClean up the environment\n清理环境\n$ kubectl delete -f manifests/quickstart-feature-examples/concurrent-scheduling/deployment.yaml deployment.apps \u0026#34;nginx-a\u0026#34; deleted deployment.apps \u0026#34;nginx-b\u0026#34; deleted Quickstart - Gang Scheduling # 快速入门-帮派调度\nIntroduction # 介绍\nIn this quickstart guide, we\u0026rsquo;ll explore Gang Scheduling, a feature that ensures an \u0026ldquo;all or nothing\u0026rdquo; approach to scheduling pods. Gödel scheduler treats all pods under a job (PodGroup) as a unified entity during scheduling attempts. This approach eliminates scenarios where a job has \u0026ldquo;partially reserved resources\u0026rdquo;, effectively mitigating resource deadlocks between multiple jobs and making it a valuable tool for managing complex scheduling scenarios in your cluster. This guide will walk you through setting up and using Gang Scheduling.\n在本快速入门指南中，我们将探讨Gang调度，这是一个确保采用“全有或全无”方法调度Pod的功能。 在调度尝试期间，哥德尔调度器将作业（PodGroup）下的所有Pod视为一个统一的实体。 这种方法消除了作业具有“部分保留资源”的情况，有效地缓解了多个作业之间的资源死锁，使其成为管理集群中复杂调度场景的有价值的工具。 本指南将引导您完成Gang调度的设置和使用。\nLocal Cluster Bootstrap \u0026amp; Installation # 本地集群引导和安装\nIf you do not have a local Kubernetes cluster installed with Godel yet, please refer to the Cluster Setup Guide.\n如果您还没有使用Godel安装本地Kubernetes集群，请参阅[cluster Setup Guide]（kind-cluster-Setup.md）。\nRelated Configurations # 相关配置\nBelow are the YAML contents and descriptions for the related configuration used in this guide:\n以下是本指南中使用的相关配置的YAML内容和描述：\nPod Group Configuration # Pod组配置\nThe Pod Group configuration specifies the minimum number of members (minMember) required and the scheduling timeout in seconds (scheduleTimeoutSeconds).\nPod Group配置指定了所需的最小成员数（“minMember”）和以秒为单位的调度超时（“scheduleTimeoutSeconds”）。\napiVersion: scheduling.godel.kubewharf.io/v1alpha1 kind: PodGroup metadata: generation: 1 name: test-podgroup spec: minMember: 2 scheduleTimeoutSeconds: 300 Pod Configuration # Pod配置\nThis YAML configuration defines the first child pod (pod-1) within the Pod Group. It includes labels and annotations required for Gang Scheduling.\n此YAML配置定义pod组中的第一个子pod（pod-1）。它包括Gang调度所需的标签和注释。\napiVersion: v1 kind: Pod metadata: name: pod-1 labels: name: nginx # Pods must have this label set godel.bytedance.com/pod-group-name: \u0026#34;test-podgroup\u0026#34; annotations: # Pods must have this annotation set godel.bytedance.com/pod-group-name: \u0026#34;test-podgroup\u0026#34; spec: schedulerName: godel-scheduler containers: - name: test image: nginx imagePullPolicy: IfNotPresent The second child pod only varies in name.\n第二个子吊舱只是名称不同。\napiVersion: v1 kind: Pod metadata: name: pod-2 labels: name: nginx # Pods must have this label set godel.bytedance.com/pod-group-name: \u0026#34;test-podgroup\u0026#34; annotations: # Pods must have this annotation set godel.bytedance.com/pod-group-name: \u0026#34;test-podgroup\u0026#34; spec: schedulerName: godel-scheduler containers: - name: test image: nginx imagePullPolicy: IfNotPresent Using Gang Scheduling # 使用帮派调度\nCreate a Pod Group: 创建Pod组： To start using Gang Scheduling, create a Pod Group using the following command:\n要开始使用Gang调度，请使用以下命令创建Pod组：\n$ kubectl apply -f manifests/quickstart-feature-examples/gang-scheduling/podgroup.yaml podgroup.scheduling.godel.kubewharf.io/test-podgroup created $ kubectl get podgroups NAME AGE test-podgroup 11s Create Child Pod 1 of the Pod Group: 创建Pod组的子Pod 1： Now, let\u0026rsquo;s create the first child pod within the Pod Group. Keep in mind that due to Gang Scheduling, this pod will initially be in a \u0026ldquo;Pending\u0026rdquo; state until the minMember requirement of the Pod Group is satisfied. Gang Scheduling ensures that pods are not scheduled until the specified number of pods (in this case, 2) is ready to be scheduled together.\n现在，让我们在pod组中创建第一个子pod。 请记住，由于Gang调度，此pod最初将处于“待定”状态，直到pod组的“minMember”要求得到满足。 Gang调度确保在指定数量的Pod（在本例中为2个）准备好一起调度之前，不会对Pod进行调度。\nUse the following command to create the first child pod:\n使用以下命令创建第一个子pod：\n$ kubectl apply -f manifests/quickstart-feature-examples/gang-scheduling/pod-1.yaml pod/pod-1 created $ kubectl get pods NAME READY STATUS RESTARTS AGE pod-1 0/1 Pending 0 18s Create Child Pod 2 of the Pod Group: 创建Pod组的子Pod 2： Now that we have created the first child pod and it\u0026rsquo;s in a \u0026ldquo;Pending\u0026rdquo; state, let\u0026rsquo;s proceed to create the second child pod. Both pods will become \u0026ldquo;Running\u0026rdquo; simultaneously once the minMember requirement of the Pod Group is fulfilled.\n现在我们已经创建了第一个子pod，并且它处于“待定”状态，让我们继续创建第二个子pod。 一旦Pod组的“minMember”要求得到满足，两个Pod将同时“运行”。\nSimilarly, create the second child pod within the same Pod Group:\n同样，在同一pod组中创建第二个子pod：\n$ kubectl apply -f manifests/quickstart-feature-examples/gang-scheduling/pod-2.yaml pod/pod-2 created $ kubectl get pods NAME READY STATUS RESTARTS AGE pod-1 1/1 Running 0 24s pod-2 1/1 Running 0 2s View Pod Group Status: 查看Pod组状态： You can check the status of the Pod Group using the following command:\n您可以使用以下命令检查Pod组的状态：\n$ kubectl get podgroup test-podgroup -o yaml apiVersion: scheduling.godel.kubewharf.io/v1alpha1 kind: PodGroup metadata: annotations: godel.bytedance.com/podgroup-final-op-lock: binder kubectl.kubernetes.io/last-applied-configuration: | {\u0026#34;apiVersion\u0026#34;:\u0026#34;scheduling.godel.kubewharf.io/v1alpha1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;PodGroup\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;generation\u0026#34;:1,\u0026#34;name\u0026#34;:\u0026#34;test-podgroup\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;default\u0026#34;},\u0026#34;spec\u0026#34;:{\u0026#34;minMember\u0026#34;:2,\u0026#34;scheduleTimeoutSeconds\u0026#34;:300}} creationTimestamp: \u0026#34;2024-01-11T00:28:17Z\u0026#34; generation: 1 name: test-podgroup namespace: default resourceVersion: \u0026#34;722\u0026#34; selfLink: /apis/scheduling.godel.kubewharf.io/v1alpha1/namespaces/default/podgroups/test-podgroup uid: fe2931c4-fa6b-4770-8ef2-589966fce3f7 spec: minMember: 2 scheduleTimeoutSeconds: 300 status: conditions: - lastTransitionTime: \u0026#34;2024-01-11T00:28:22Z\u0026#34; phase: Pending reason: Pending status: \u0026#34;True\u0026#34; - lastTransitionTime: \u0026#34;2024-01-11T00:28:36Z\u0026#34; message: More than 2 pods has been created but not fully scheduled phase: PreScheduling reason: PreScheduling status: \u0026#34;True\u0026#34; - lastTransitionTime: \u0026#34;2024-01-11T00:28:36Z\u0026#34; message: More than 2 pods has been scheduled phase: Scheduled reason: Scheduled status: \u0026#34;True\u0026#34; phase: Scheduled scheduleStartTime: \u0026#34;2024-01-11T00:28:37Z\u0026#34; Local Gödel Environment Setup with KIND # 使用KIND设置本地哥德尔环境\nThis guide will walk you through how to set up the Gödel Unified Scheduling system.\n本指南将指导您如何设置哥德尔统一调度系统。\nOne-Step Cluster Bootstrap \u0026amp; Installation # 一步式集群引导和安装\nWe provided a quick way to help you try Gödel on your local machine, which will set up a kind cluster locally and deploy necessary crds, clusterrole and rolebindings\n我们提供了一种快速的方法来帮助您在本地计算机上尝试Gödel，它将在本地设置一个类集群，并部署必要的crd、clusterrole和rolebindings\nPrerequisites # Please make sure the following dependencies are installed.\nkubectl \u0026gt;= v1.19 docker \u0026gt;= 19.03 kind \u0026gt;= v0.17.0 go \u0026gt;= v1.21.4 kustomize \u0026gt;= v4.5.7 1. Clone the Gödel repo to your machine # $ git clone https://github.com/kubewharf/godel-scheduler 2. Change to the Gödel directory # $ cd godel 3. Bootstrap the cluster and install Gödel components # 引导集群并安装哥德尔组件\n$ make local-up This command will complete the following steps:\nBuild Gödel image locally; Start a Kubernetes cluster using Kind; Installs the Gödel control-plane components on the cluster. 此命令将完成以下步骤：\n1.在当地树立哥德尔形象； 2.使用Kind启动Kubernetes集群； 3.在仪表盘上安装哥德尔控制平面组件。\nManual Installation # If you have an existing Kubernetes cluster, please follow the steps below to install Gödel.\n如果您有一个现有的Kubernetes集群，请按照以下步骤安装Gödel。\n1. Build Gödel image # 塑造哥德尔形象\nmake docker-images 2. Load Gödel image to your cluster # 将哥德尔图像加载到集群\nFor example, if you are using Kind\nkind load docker-image godel-local:latest --name \u0026lt;cluster-name\u0026gt; --nodes \u0026lt;control-plane-nodes\u0026gt; 3. Create Gödel components in the cluster # 在集群中创建哥德尔组件\nkustomize build manifests/base/ | kubectl apply -f - Quickstart - Preemption # 快速入门-优先购买\nWithin the Gödel Scheduling System, Pod preemption emerges as a key feature designed to uphold optimal resource utilization. If a Pod cannot be scheduled, the Gödel scheduler tries to preempt (evict) lower priority Pods to make scheduling of the pending Pod possible.\nThis is a Quickstart guide that will walk you through how preemption works in the Gödel Scheduling System.\n在哥德尔调度系统中，吊舱抢占成为一个关键特征，旨在保持最佳的资源利用率。 如果无法调度Pod，哥德尔调度器会尝试抢占（驱逐）优先级较低的Pod，以使未决Pod的调度成为可能。\n这是一个快速入门指南，将引导您了解抢占在哥德尔调度系统中的工作原理。\nLocal Cluster Bootstrap \u0026amp; Installation # 本地集群引导和安装\nIf you do not have a local Kubernetes cluster installed with Godel yet, please refer to the Cluster Setup Guide for creating a local KIND cluster installed with Gödel.\n如果您还没有使用Godel安装本地Kubernetes集群，请参阅[cluster Setup Guide]（kind-cluster-Setup.md）以创建使用Gödel安装的本地kind集群。\nHow Preemption Works # 优先购买权如何运作\nWhen there is no more resource for scheduling a pending Pod, preemption comes into picture. Gödel scheduler tries to preempt (evict) lower priority Pods to make scheduling of the pending Pod possible, with a few of protection strategies being respected.\n当没有更多资源来调度挂起的Pod时，抢占就会出现。 哥德尔调度器试图抢占（驱逐）优先级较低的Pod，以使未决Pod的调度成为可能，并遵守一些保护策略。\nQuickstart Scenario # 快速入门场景\nTo better illustrate the preemption features, let\u0026rsquo;s assume there is one node with less than 8 CPU cores available for scheduling.\n为了更好地说明抢占功能，让我们假设有一个节点的CPU核数少于8个，可用于调度。\nNote: The Capacity and Allocatable of worker node depend on your own Docker resources configuration. Thus they are not guaranteed to be exactly the same with this guide. To try out this feature locally, you should tune the resources configuration in the example yaml files based on your own setup. For example, the author configured 8 CPU for Docker resources preference, so the worker node has 8 CPU in the guide.\n**注意：**工作节点的“容量”和“可分配”取决于您自己的Docker资源配置。因此，不能保证它们与本指南完全相同。要在本地尝试此功能，您应该根据自己的设置调整示例yaml文件中的资源配置。例如，作者为Docker资源偏好配置了8个CPU，因此工作节点在指南中有8个CPU。\n$ kubectl describe node godel-demo-default-worker Name: godel-demo-default-worker ... Capacity: cpu: 8 ... Allocatable: cpu: 8 ... ... Non-terminated Pods: (2 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits Age --------- ---- ------------ ---------- --------------- ------------- --- kube-system kindnet-cnvtd 100m (2%) 100m (2%) 50Mi (0%) 50Mi (0%) 3d23h kube-system kube-proxy-6fh4g 0 (0%) 0 (0%) 0 (0%) 0 (0%) 3d23h Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 100m (2%) 100m (2%) ... Events: \u0026lt;none\u0026gt; Basic Preemption # 基本优先购买权\nGödel Scheduling System provides basic preemption features that are comparable with the offering of the Kubernetes scheduler.\nGödel调度系统提供了与Kubernetes调度器相当的基本抢占功能。\nPriority-based Preemption # 基于优先级的优先购买权\nCreate a pod with a lower priority, which requests 6 CPU cores.\n创建一个优先级较低的pod，它需要6个CPU核心。\n--- apiVersion: scheduling.k8s.io/v1 kind: PriorityClass metadata: name: low-priority annotations: \u0026#34;godel.bytedance.com/can-be-preempted\u0026#34;: \u0026#34;true\u0026#34; value: 80 description: \u0026#34;a priority class with low priority that can be preempted\u0026#34; --- apiVersion: v1 kind: Pod metadata: name: nginx-victim annotations: \u0026#34;godel.bytedance.com/can-be-preempted\u0026#34;: \u0026#34;true\u0026#34; spec: schedulerName: godel-scheduler priorityClassName: low-priority containers: - name: nginx-victim image: nginx imagePullPolicy: IfNotPresent resources: limits: cpu: 6 requests: cpu: 6 $ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-victim 1/1 Running 0 2s Create a pod with a higher priority, which requests 3 CPU core.\n创建一个优先级更高的pod，它需要3个CPU核心。\n--- apiVersion: scheduling.k8s.io/v1 kind: PriorityClass metadata: name: high-priority value: 100 description: \u0026#34;a priority class with a high priority\u0026#34; --- apiVersion: v1 kind: Pod metadata: name: nginx-preemptor spec: schedulerName: godel-scheduler priorityClassName: high-priority containers: - name: nginx-preemptor image: nginx imagePullPolicy: IfNotPresent resources: limits: cpu: 3 requests: cpu: 3 Remember, there is only 8 CPU cores available. So, preemption will be triggered when the high-priority Pod gets scheduled.\n记住，只有8个CPU核可用。因此，当高优先级Pod被调度时，将触发抢占。\n$ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-preemptor 1/1 Running 0 18s $ kubectl get event ... 0s Normal PreemptForPodSuccessfully pod/nginx-preemptor Pod can be placed by evicting some other pods, nominated node: godel-demo-default-worker, victims: [{Name:nginx-victim Namespace:default UID:b685ef99-20b8-43bb-9576-10d2ca09e2d6}], in node group: [] ... As we can see, the pod with a lower priority was preempted to accommodate the pod with a higher priority.\n正如我们所看到的，优先级较低的pod被抢占，以容纳优先级较高的pod。\nClean up the environment\n清理环境\nkubectl delete pod nginx-preemptor Protection with PodDisruptionBudget # 使用PodDisruption预算进行保护\nCreate a 3-replica deployment with a lower priority, which requests 6 CPU cores in total. Meanwhile, a PodDisruptionBudget object with minAvailable being set to 3 is also created.\n创建一个优先级较低的3副本部署，总共需要6个CPU核心。同时，还创建了一个minAvailable设置为3的PodDisruptionBudget对象。\n--- apiVersion: scheduling.k8s.io/v1 kind: PriorityClass metadata: name: low-priority annotations: \u0026#34;godel.bytedance.com/can-be-preempted\u0026#34;: \u0026#34;true\u0026#34; value: 80 description: \u0026#34;a priority class with low priority that can be preempted\u0026#34; --- apiVersion: policy/v1 kind: PodDisruptionBudget metadata: name: nginx-pdb spec: minAvailable: 3 selector: matchLabels: name: nginx --- apiVersion: apps/v1 kind: Deployment metadata: name: nginx-victim spec: replicas: 3 selector: matchLabels: name: nginx template: metadata: name: nginx-victim labels: name: nginx annotations: \u0026#34;godel.bytedance.com/can-be-preempted\u0026#34;: \u0026#34;true\u0026#34; spec: schedulerName: godel-scheduler priorityClassName: low-priority containers: - name: test image: nginx imagePullPolicy: IfNotPresent resources: limits: cpu: 2 requests: cpu: 2 $ kubectl get pod,deploy NAME READY STATUS RESTARTS AGE pod/nginx-victim-588f6db4bd-r99bq 1/1 Running 0 6s pod/nginx-victim-588f6db4bd-vv24v 1/1 Running 0 6s pod/nginx-victim-588f6db4bd-xdsht 1/1 Running 0 6s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-victim 3/3 3 3 6s Create a pod with a higher priority, which requests 3 CPU core.\n创建一个优先级更高的pod，它需要3个CPU核心。\n--- apiVersion: scheduling.k8s.io/v1 kind: PriorityClass metadata: name: high-priority value: 100 description: \u0026#34;a priority class with a high priority\u0026#34; --- apiVersion: v1 kind: Pod metadata: name: nginx-preemptor spec: schedulerName: godel-scheduler priorityClassName: high-priority containers: - name: nginx-preemptor image: nginx imagePullPolicy: IfNotPresent resources: limits: cpu: 3 requests: cpu: 3 In this case, preemption will not be triggered for scheduling the high-priority Pod above, due to the protection provided by the PodDisruptionBudget object.\n在这种情况下，由于PodDisruptionBudget对象提供的保护，在调度上述高优先级Pod时不会触发抢占。\n$ kubectl get pod,deploy NAME READY STATUS RESTARTS AGE pod/nginx-preemptor 0/1 Pending 0 34s pod/nginx-victim-588f6db4bd-r99bq 1/1 Running 0 109s pod/nginx-victim-588f6db4bd-vv24v 1/1 Running 0 109s pod/nginx-victim-588f6db4bd-xdsht 1/1 Running 0 109s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-victim 3/3 3 3 109s But, if we update the PodDisruptionBudget object by setting the minAvailable field to 2. The preemption will be triggered in the next scheduling cycle for the Pod with high priority.\n但是，如果我们通过将minAvailable字段设置为2来更新PodDisruptionBudget对象。对于具有高优先级的Pod，抢占将在下一个调度周期中触发。\n$ kubectl get pdb NAME MIN AVAILABLE MAX UNAVAILABLE ALLOWED DISRUPTIONS AGE nginx-pdb 2 N/A 0 5m22s $ kubectl get pod,deploy NAME READY STATUS RESTARTS AGE pod/nginx-preemptor 1/1 Running 0 6m25s pod/nginx-victim-588f6db4bd-p49vg 0/1 Pending 0 3m19s pod/nginx-victim-588f6db4bd-r99bq 1/1 Running 0 7m40s pod/nginx-victim-588f6db4bd-xdsht 1/1 Running 0 7m40s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-victim 2/3 3 2 7m40s Based on the observation above, one replica of the low-priority deployment was preempted.\n基于上述观察，低优先级部署的一个副本被抢占。\nClean up the environment\n清理环境\nkubectl delete pod nginx-preemptor \u0026amp;\u0026amp; kubectl delete deploy nginx-victim \u0026amp;\u0026amp; kubectl delete pdb nginx-pdb Gödel-specific Preemption # 哥德尔特定优先购买权\nApart from the basic preemption functionalities shown above, extra protection behaviors are also honored in Gödel Scheduling System.\n除了上面显示的基本抢占功能外，哥德尔调度系统还支持额外的保护行为。\nPreemptibility Annotation # 优先购买性注释\nGödel Scheduling System introduces a customized annotation, \u0026quot;godel.bytedance.com/can-be-preempted\u0026quot;: \u0026quot;true\u0026quot;, to enable the preemptibility of Pods. The preemptibility annotation specified either in the Pod object or the PriorityClass object will be honored. Specifically, only Pods with the preemptibility being enabled can preempted in any cases. Otherwise, no preemption will happen.\n哥德尔调度系统引入了一个定制的注释，“godel.bytedance.com/can be preceded”：“true”， 以实现Pod的可抢占性。Pod对象或PriorityClass对象中指定的抢占性注释将被遵守。 具体来说，在任何情况下，只有启用了可抢占性的Pod才能被抢占。否则，不会发生先发制人。\nCreate a Pod without the preemptibility being enabled.\n在不启用抢占功能的情况下创建Pod。\n--- apiVersion: scheduling.k8s.io/v1 kind: PriorityClass metadata: name: low-priority value: 80 description: \u0026#34;a priority class with low priority\u0026#34; --- apiVersion: v1 kind: Pod metadata: name: nginx-victim spec: schedulerName: godel-scheduler priorityClassName: low-priority containers: - name: nginx-victim image: nginx imagePullPolicy: IfNotPresent resources: limits: cpu: 6 requests: cpu: 6 $ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-victim 1/1 Running 0 2s Create a Pod with a higher priority.\n创建具有更高优先级的Pod。\n--- apiVersion: scheduling.k8s.io/v1 kind: PriorityClass metadata: name: high-priority value: 100 description: \u0026#34;a priority class with a high priority\u0026#34; --- apiVersion: v1 kind: Pod metadata: name: nginx-preemptor spec: schedulerName: godel-scheduler priorityClassName: high-priority containers: - name: nginx-preemptor image: nginx imagePullPolicy: IfNotPresent resources: limits: cpu: 3 requests: cpu: 3 $ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-preemptor 0/1 Pending 0 11s nginx-victim 1/1 Running 0 2m32s In this case, preemption will not happen because the preemptibility is not enabled.\n在这种情况下，由于未启用抢占功能，因此不会发生抢占。\nClean up the environment\n清理环境\nkubectl delete pod nginx-preemptor nginx-victim Protection Duration # 保护期限\nGödel Scheduling System also supports protecting preemptible Pods from preemption for at least a specified amount of time after it\u0026rsquo;s started up. By leveraging the Pod annotation godel.bytedance.com/protection-duration-from-preemption, users will be able to specify the protection duration in seconds.\n哥德尔调度系统还支持在可抢占的Pod启动后至少在指定时间内保护其免受抢占。 通过利用Pod注释“godel.bytedance.com/protection duration from prevention”， 用户将能够以秒为单位指定保护持续时间。\nCreate a Pod with a 30-second protection duration.\n创建一个保护时间为30秒的Pod。\n--- apiVersion: scheduling.k8s.io/v1 kind: PriorityClass metadata: name: low-priority annotations: \u0026#34;godel.bytedance.com/can-be-preempted\u0026#34;: \u0026#34;true\u0026#34; value: 80 description: \u0026#34;a priority class with low priority that can be preempted\u0026#34; --- apiVersion: v1 kind: Pod metadata: name: nginx-victim annotations: \u0026#34;godel.bytedance.com/protection-duration-from-preemption\u0026#34;: \u0026#34;30\u0026#34; spec: schedulerName: godel-scheduler priorityClassName: low-priority containers: - name: nginx-victim image: nginx imagePullPolicy: IfNotPresent resources: limits: cpu: 6 requests: cpu: 6 $ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-victim 1/1 Running 0 3s Create a Pod with a higher priority.\n创建具有更高优先级的Pod。\n--- apiVersion: scheduling.k8s.io/v1 kind: PriorityClass metadata: name: high-priority value: 100 description: \u0026#34;a priority class with a high priority\u0026#34; --- apiVersion: v1 kind: Pod metadata: name: nginx-preemptor spec: schedulerName: godel-scheduler priorityClassName: high-priority containers: - name: nginx-preemptor image: nginx imagePullPolicy: IfNotPresent resources: limits: cpu: 3 requests: cpu: 3 Within the protection duration, preemption will not be triggered.\n在保护期内，不会触发抢占。\n$ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-preemptor 0/1 Pending 0 14s nginx-victim 1/1 Running 0 26s Beyond that, preemption takes place eventually.\n除此之外，先发制人最终会发生。\n$ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-preemptor 1/1 Running 0 78s Clean up the environment\n清理环境\nkubectl delete pod nginx-preemptor Wrap-up # 总结\nIn this doc, we share a Quickstart guide about selected functionalities of preemption in Gödel Scheduling System. More advanced preemption features/strategies as well as the corresponding technical deep-dives can be expected.\n在本文档中，我们将分享一份关于哥德尔调度系统中优先购买功能的快速入门指南。 可以期待更高级的抢占功能/策略以及相应的技术深度探索。\n"},{"id":83,"href":"/docs/hello-world/","title":"Hello World","section":"Docs","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.\nQuick Start # Create a new post # $ hexo new \u0026#34;My New Post\u0026#34; More info: Writing\nRun server # $ hexo server More info: Server\nGenerate static files # $ hexo generate More info: Generating\nDeploy to remote sites # $ hexo deploy More info: Deployment\n"},{"id":84,"href":"/docs/helm-chart%E5%92%8Crepo-helmchart-he-repo/","title":"helm chart和repo 2024-04-03 15:00:43.125","section":"Docs","content":" chart构成 # 创建一个名为mychart的chart，查看文件结构\nhelm create mychart [root@k8s-master ~]# tree mychart/ mychart/ ├── charts ├── Chart.yaml ├── templates │ ├── deployment.yaml │ ├── _helpers.tpl │ ├── ingress.yaml │ ├── NOTES.txt │ └── service.yaml └── values.yaml 所有kubenetes要执行的yaml模板都存放在templates文件夹下，例如上个例子中存放了deployment，service，ingress三个kubenetes资源对象。 # values.yaml文件存放yaml模板中定义的默认值。 # Chart.yaml存放cahrt的版本信息。 # NOTES.txt显示cahrt的release运行后的帮助信息。 # helm转移chart # 首先，编辑和配置好本地的chart文件，然后使用helm打包成tar文件。 # helm package ./mychart 把tar包拷入到另一个环境，通过helm install命令指定tar名称导入。 # helm install --name example3 mychart-0.1.0.tgz --set service.type=NodePort helm本地repo # helm可以启动一个本地HTTP服务器，作为一个为本地chart的repo服务。 # helm serve 另开一个终端，可以搜索和安装本地repository中的chart。 # helm search local helm install --name example4 local/mychart --set service.type=NodePort chart中定义依赖 # 可以在chart目录中创建一个requirements.yaml文件定义该chart的依赖。 # $ cat \u0026gt; ./mychart/requirements.yaml \u0026lt;\u0026lt;EOF dependencies: - name: mariadb version: 0.6.0 repository: https://kubernetes-charts.storage.googleapis.com EOF 通过helm命令更新和下载cahrt的依赖 # helm dep update ./mychart 在次安装运行chart时会把依赖中定义的chart运行起来。 # 自定义chart repository # 首先，把每个chart打包的tar文件集中存放到charts目录，使用以下命令生成index.yaml文件。 # mv mychart-0.1.0.tgz charts/ $ helm serve --repo-path ./charts 命令执行完后 # charts目录结构如下 # [root@k8s-master ~]# tree ./charts ./charts ├── index.yaml └── mychart-0.1.0.tgz 0 directories, 2 files 把charts目录在远端web服务器上复制一份，保持连个文件里面tar包文件一样。执行以下命令在本地和远端生成新的index.yaml文件，该文件的url为远端web服务器的url。 # helm repo index charts --url http://192.168.122.1:81/charts [root@k8s-master ~]# cat charts/index.yaml apiVersion: v1 entries: mychart: - apiVersion: v1 created: 2018-01-16T11:53:46.922200367+08:00 description: A Helm chart for Kubernetes digest: 7471a2a8496517b4ce1014b2787d3dc745b981fb69c9e53a257ccd7ac390d036 name: mychart urls: - http://192.168.122.1:81/charts/mychart-0.1.0.tgz version: 0.1.0 generated: 2018-01-16T11:53:46.921691858+08:00 增加chart # 在本地charts目录和远端web服务器目录增加新chart的tar文件，然后执行以下命令重建index.yaml。 # helm repo index charts --url http://192.168.122.1:81/charts 或者，在index.yaml中之增加新cahrt的元数据信息。 # helm repo index charts --url http://192.168.122.1:81/charts --merge 把新生成的index.yaml拷贝到远端web服务器上。 # 其他人使用该repo # 通过以下命令增加repo # helm repo add charts http://192.168.122.1:81/charts [root@k8s-master ~]# helm repo list NAME URL local http://127.0.0.1:8879/charts stable https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts monocular https://kubernetes-helm.github.io/monocular charts http://192.168.122.1:81/charts 如果repo有更新，执行repo update命令会更新所以已增加的repo # helm repo update 欢迎沟通交流 CslcCloud # "},{"id":85,"href":"/docs/istio-ingress-gateway-istio-ingress-gateway/","title":"istio-ingress-gateway 2024-08-02 17:51:54.75","section":"Docs","content":"\nIstio 服务网格中的网关\n使用网关为网格来管理入站和出站流量，可以让用户指定要进入或离开网格的流量。 使用网关为网格来管理入站和出站流量，可以让用户指定要进入或离开网格的流量。 网关配置被用于运行在网格内独立 Envoy 代理中，而不是服务工作负载的应用 Sidecar 代理。 Gateway 用于为 HTTP / TCP 流量配置负载均衡器，并不管该负载均衡器将在哪里运行。网格中可以存在任意数量的 Gateway，并且多个不同的 Gateway 实现可以共存。实际上，通过在配置中指定一组工作负载（Pod）标签，可以将 Gateway 配置绑定到特定的工作负载，从而允许用户通过编写简单的 Gateway Controller 来重用现成的网络设备。\nGateway 只用于配置 L4-L6 功能（例如，对外公开的端口，TLS 配置），所有主流的 L7 代理均以统一的方式实现了这些功能。然后，通过在 Gateway 上绑定 VirtualService 的方式，可以使用标准的 Istio 规则来控制进入 Gateway 的 HTTP 和 TCP 流量。\n例如，下面这个简单的 Gateway 配置了一个 Load Balancer，以允许访问 host bookinfo.com 的 https 外部流量进入网格中：\napiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: bookinfo-gateway spec: selector: app: my-ingress-gateway servers: - port: number: 443 name: https protocol: HTTPS hosts: - bookinfo.com tls: mode: SIMPLE serverCertificate: /tmp/tls.crt privateKey: /tmp/tls.key 要为进入上面的 Gateway 的流量配置相应的路由，必须为同一个 host 定义一个 VirtualService（在下一节中描述），并使用配置中的 gateways 字段绑定到前面定义的 Gateway 上：\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: bookinfo spec: hosts: - bookinfo.com gateways: - bookinfo-gateway # \u0026lt;---- bind to gateway http: - match: - uri: prefix: /reviews route: ... 然后就可以为出口流量配置带有路由规则的虚拟服务。\nGateway 配置信息 # Server 配置信息 # Port 配置信息 # Server.TLSOptions 配置信息 # Server.TLSOptions.TLSmode 配置信息 # Server.TLSOptions.TLSProtocol 配置信息 # Gateway配置要点 # Gateway定义运行在网格边缘的负载均衡器，负责接收入站或出站的HTTP/TCP连接 主要定义应该暴露到网格外部的端口、要使用的协议类型、以及SNI配置等 Gateway的定义主要通过如下两个关键字段 selector：Pod标签选择器，用于指定当前Gateway配置要附加到的Ingress Gateway Pod实例 Pod标签选择器，负责在为Istio部署的一到多个Ingress Gateway实例中完成Pod筛选 仅符合选择器条件的Ingress Gateway实例才会添加该Gateway资源中定义的配置 server：开放的服务列表，即服务的访问入口，可通过port、hosts、defaultEndpoints和tls来定义； port：服务对外发布的端口，即用于接收请求的端口； hosts：Gateway发布的服务地址，通常是一个FQDN格式的域名，支持使用*通配符； defaultEndpoint：默认后端； tls：发布为HTTPS协议服务时与TLS相关的配置 提示：Gateway资源仅定义了要暴露的访问入口，但流量接入到网格内部之后的路由机制，仍然需要由VirtualService资源进行定义； Gateway # 1、hosts字段不接受非FQDN格式的字符串，但可以使 用“*”通配符\n2、gateway资源应该定义在目标ingressgateway Pod运行在名称空间\nGateway配置示例 # 示例一 # apiVersion: networking.istio.io/v1beta1 kind: Gateway metadata: name: my-gateway namespace: some-config-namespace spec: selector: app: my-gateway-controller servers: - port: number: 80 name: http protocol: HTTP hosts: - uk.bookinfo.com - eu.bookinfo.com tls: httpsRedirect: true # sends 301 redirect for http requests - port: number: 443 name: https-443 protocol: HTTPS hosts: - uk.bookinfo.com - eu.bookinfo.com tls: mode: SIMPLE # enables HTTPS on this port serverCertificate: /etc/certs/servercert.pem privateKey: /etc/certs/privatekey.pem - port: number: 9443 name: https-9443 protocol: HTTPS hosts: - \u0026#34;bookinfo-namespace/*.bookinfo.com\u0026#34; tls: mode: SIMPLE # enables HTTPS on this port credentialName: bookinfo-secret # fetches certs from Kubernetes secret - port: number: 9080 name: http-wildcard protocol: HTTP hosts: - \u0026#34;*\u0026#34; - port: number: 2379 # to expose internal service via external port 2379 name: mongo protocol: MONGO hosts: - \u0026#34;*\u0026#34; apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: name: bookinfo-rule namespace: bookinfo-namespace spec: hosts: - reviews.prod.svc.cluster.local - uk.bookinfo.com - eu.bookinfo.com gateways: - some-config-namespace/my-gateway - mesh # applies to all the sidecars in the mesh http: - match: - headers: cookie: exact: \u0026#34;user=dev-123\u0026#34; route: - destination: port: number: 7777 host: reviews.qa.svc.cluster.local - match: - uri: prefix: /reviews/ route: - destination: port: number: 9080 # can be omitted if it\u0026#39;s the only port for reviews host: reviews.prod.svc.cluster.local weight: 80 - destination: host: reviews.qa.svc.cluster.local weight: 20 apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: name: bookinfo-mongo namespace: bookinfo-namespace spec: hosts: - mongosvr.prod.svc.cluster.local # name of internal Mongo service gateways: - some-config-namespace/my-gateway # can omit the namespace if gateway is in same namespace as virtual service. tcp: - match: - port: 27017 route: - destination: host: mongo.prod.svc.cluster.local port: number: 5555 示例二 # apiVersion: networking.istio.io/v1beta1 kind: Gateway metadata: name: my-gateway namespace: some-config-namespace spec: selector: app: my-gateway-controller servers: - port: number: 80 name: http protocol: HTTP hosts: - \u0026#34;ns1/*\u0026#34; - \u0026#34;ns2/foo.bar.com\u0026#34; 参考文档 # https://istio.io/latest/zh/docs/reference/config/networking/gateway/ 通过例子来理解 # apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: nginx-gw spec: selector: app: istio-ingressgateway servers: - port: number: 80 name: http protocol: HTTP hosts: - nginx.test.com --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: nginx-vs spec: hosts: - nginx.test.com gateways: - nginx-gw http: - route: - destination: host: nginx-svc --- apiVersion: v1 kind: Service metadata: name: nginx-svc spec: ports: - port: 80 protocol: TCP targetPort: 80 selector: app: nginx type: ClusterIP --- apiVersion: apps/v1 kind: Deployment metadata: labels: app: nginx name: nginx-deployment spec: replicas: 1 selector: matchLabels: app: nginx strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: labels: app: nginx spec: containers: - image: \u0026#39;nginx:latest\u0026#39; name: nginx-deployment 通过命令访问 curl -H \u0026ldquo;Host: nginx.gateway.com\u0026rdquo; http://ingressgateway:nodeport/\nistio-ingressgateway 就是小区的大门（唯一的大门），所有进入的流量都需要经过， ingressgateway 相当于路标引导去到A B C D的一栋建筑里面，分开域名去导流， virtualservice 就像到建筑里的电梯一样，按照不同的楼层进行管理路由的作用， destinationrule 到达具体的楼层后按照不同的门房号 1 2 3 4 进入到真正的屋里去。 Istio 调整sidecar镜像仓库 # # Configmap istio-system.istio-sidecar-injector # 无需重启任务服务, 直接生效 \u0026#34;hub\u0026#34;: \u0026#34;docker.io/istio\u0026#34;, 改为 \u0026#34;hub\u0026#34;: \u0026#34;私有镜像仓库/istio\u0026#34;, 下载镜像会从: docker.io/istio/proxyv2:1.15.2 变为 私有镜像仓库/istio/proxyv2:1.15.2 "},{"id":86,"href":"/docs/istio%E9%83%A8%E7%BD%B2-istio-bu-shu/","title":"istio部署 2024-08-02 17:52:40.721","section":"Docs","content":"[root@master1 istio-1.22.3]# istioctl install --set profile=demo -y The Kubernetes version v1.23.6 is not supported by Istio 1.22.3. The minimum supported Kubernetes version is 1.26. Proceeding with the installation, but you might experience problems. See https://istio.io/latest/docs/releases/supported-releases/ for a list of supported versions. 2024-07-30T09:29:52.253698Z error klog an error occurred forwarding 40586 -\u0026gt; 15014: error forwarding port 15014 to pod 8835fe7129746ef5df21abaf44c2d302f16012ae2c6e03c1103c5b8d8aff680c, uid : unable to do port forwarding: socat not found 2024-07-30T09:29:52.254140Z error port forward failed: lost connection to pod The default revision has been updated to point to this installation. ✔ Istio core installed ✔ Istiod installed ✔ Egress gateways installed ✔ Ingress gateways installed ✔ Installation complete Made this installation the default for injection and validation. [root@master1 istio-1.22.3]# quay.io/kiali/kiali:v1.63 jimmidyson/configmap-reload:v0.5.0 #原镜像 quay.io/kiali/kiali:v1.63 #转换后镜像 anjia0532/quay.kiali.kiali:v1.63 #下载并重命名镜像 docker pull anjia0532/quay.kiali.kiali:v1.63 docker tag anjia0532/quay.kiali.kiali:v1.63 quay.io/kiali/kiali:v1.63 docker images | grep $(echo quay.io/kiali/kiali:v1.63 |awk -F\u0026#39;:\u0026#39; \u0026#39;{print $1}\u0026#39;) docker tag cloudsx.top/anjia0532/quay.kiali.kiali:v1.63 192.168.0.140:881//kiali/kiali:v1.63 Istio快速入门 # 目录 # [toc]\n本节实战 # 实战名称 实战：istioctl 方式安装istio-2023.11.3(测试成功) 实战：手动注入 Envoy Sidecar-2023.11.3(测试成功) 实战：自动注入 Envoy Sidecar-2023.11.3(测试失败) 实战：BookInfo 示例应用-2023.11.5(测试成功) 前言 # 前面在服务网格基础章节我们和大家已经了解了 Istio 的基本概念以及架构，本章节我们将来快速了解 Istio 的基本使用方法，对 Istio 有一个最基本的认识。\n1、安装 # 安装 Istio 之前，需要准备一个 Kubernetes 集群，我们可以使用任何支持的平台，例如 Minikube、Kind、Kubeadm 等，但是需要注意的是目前的 Istio 最新版本为 v1.19.3 版本，不同的版本支持的 Kubernetes 版本不同，可以参考下表进行对照：\n我们这里选择最新的版本为 v1.19.3，Kubernetes 版本为 v1.27.6：\n安装方式 # 在 Kubernetes 集群上安装 Istio 的方法有很多方式，可以根据自己的需求选择不同的安装方式，我们可以先对比下每种方式的优缺点。\n1.使用 istioctl install # 具有高安全性、简单的安装和管理方法，这也是社区推荐的安装方法。\n优点: 完整的配置和运行状态的验证。 使用提供了扩展的配置、自定义选项的 IstioOperator API。 不需要集群内的高权限 Pod，通过执行 istioctl 命令修改。 缺点: 需要维护多个 Istio 版本的二进制文件。 istioctl 命令可能根据你的运行环境设置诸如 JWT_POLICY 之类的值，从而在不同的 Kubernetes 环境中产生不同的安装结果。 整体来说，这里的缺点对我们可能影响不大。\n2.使用 istioctl manifest generate 安装 # 生成 Kubernetes 的配置清单，并通过 kubectl apply --prune 命令应用到集群中。该方法适用于需要严格审查或者增加配置清单的情况。\n优点: Chart 是由与 istioctl install 和 Operator 里使用的相同的 IstioOperator API 生成的。 使用提供了扩展的配置、自定义选项的 IstioOperator API。 缺点: 一些在 istioctl install 和 Operator 中会进行的检查将不会执行。 与 istioctl install 相比，UX 不够简洁。 错误反馈没有 istioctl install 的详细、全面。 3.使用 Helm 安装 # 使用 Helm 的 Chart 可以通过 Helm 的工作流轻松的完成，并在升级的过程中自动清理资源。\n优点: 使用熟悉、常用的行业标准工具。 Helm 原生的版本、升级管理。 缺点: 相比于 istioctl install 和 Operator 相比，检查较少。 一些管理任务需要更多步骤，具有更高的复杂性。 4.使用 Istio Operator 安装 # 不建议在新安装时使用 Operator，虽然 Operator 将继续得到支持，但新的功能特性不会被优先考虑。\nIstio Operator 提供了一种安装路径，无需使用 istioctl 二进制文件。这可以用于简化升级工作流程，不必担心在集群中运行特权控制器的问题。此方法适用于不需要严格审计或增加输出清单的情况。\n优点: 与 istioctl install 相同的 API，但通过集群中的控制器 Pod 进行操作，实现了完全声明式操作。 IstioOperator API 提供了广泛的配置/定制选项。 不需要管理多个 istioctl 二进制文件。 缺点: 集群中运行的高权限控制器存在安全风险。 安装 Istio # == 实战：istioctl 方式安装istio-2023.11.3(测试成功)== # 实验环境：\nk8s v1.25.4（containerd://1.6.10） [root@master1 ~]#istioctl version client version: 1.19.3 control plane version: 1.19.3 data plane version: 1.19.3 (8 proxies) 实验软件：\n链接：https://pan.baidu.com/s/1pMnJxgL63oTlGFlhrfnXsA?pwd=7yqb 提取码：7yqb 2023.11.5-实战：BookInfo 示例应用-2023.11.5(测试成功) （使用的是同一个软件包）\n上面我们对几种方式做了简单的对比，具体选择哪种方式安装需要结合自己的情况来决定。我们这里就选择社区推荐的 istioctl 方式进行安装。\n首先前往 Istio Release 页面下载适合你操作系统的安装文件，或使用下面的命令自动下载并解压最新版本（Linux 或 macOS）：\ncurl -L https://istio.io/downloadIstio | sh - #!/bin/sh # Copyright Istio Authors # # Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \u0026#34;AS IS\u0026#34; BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # # This file will be fetched as: curl -L https://git.io/getLatestIstio | sh - # so it should be pure bourne shell, not bash (and not reference other scripts) # # The script fetches the latest Istio release candidate and untars it. # You can pass variables on the command line to download a specific version # or to override the processor architecture. For example, to download # Istio 1.6.8 for the x86_64 architecture and linux OS, # run curl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.6.8 TARGET_ARCH=x86_64 TARGET_OS=Linux sh -. set -e # Determines the operating system. OS=\u0026#34;${TARGET_OS:-$(uname)}\u0026#34; if [ \u0026#34;${OS}\u0026#34; = \u0026#34;Darwin\u0026#34; ] ; then OSEXT=\u0026#34;osx\u0026#34; else OSEXT=\u0026#34;linux\u0026#34; fi # Determine the latest Istio version by version number ignoring alpha, beta, and rc versions. if [ \u0026#34;${ISTIO_VERSION}\u0026#34; = \u0026#34;\u0026#34; ] ; then ISTIO_VERSION=\u0026#34;$(curl -sL https://github.com/istio/istio/releases | \\ grep -o \u0026#39;releases/[0-9]*.[0-9]*.[0-9]*/\u0026#39; | sort -V | \\ tail -1 | awk -F\u0026#39;/\u0026#39; \u0026#39;{ print $2}\u0026#39;)\u0026#34; ISTIO_VERSION=\u0026#34;${ISTIO_VERSION##*/}\u0026#34; fi LOCAL_ARCH=$(uname -m) if [ \u0026#34;${TARGET_ARCH}\u0026#34; ]; then LOCAL_ARCH=${TARGET_ARCH} fi case \u0026#34;${LOCAL_ARCH}\u0026#34; in x86_64|amd64) ISTIO_ARCH=amd64 ;; armv8*|aarch64*|arm64) ISTIO_ARCH=arm64 ;; armv*) ISTIO_ARCH=armv7 ;; *) echo \u0026#34;This system\u0026#39;s architecture, ${LOCAL_ARCH}, isn\u0026#39;t supported\u0026#34; exit 1 ;; esac if [ \u0026#34;${ISTIO_VERSION}\u0026#34; = \u0026#34;\u0026#34; ] ; then printf \u0026#34;Unable to get latest Istio version. Set ISTIO_VERSION env var and re-run. For example: export ISTIO_VERSION=1.0.4\u0026#34; exit 1; fi NAME=\u0026#34;istio-$ISTIO_VERSION\u0026#34; URL=\u0026#34;https://github.com/istio/istio/releases/download/${ISTIO_VERSION}/istio-${ISTIO_VERSION}-${OSEXT}.tar.gz\u0026#34; ARCH_URL=\u0026#34;https://github.com/istio/istio/releases/download/${ISTIO_VERSION}/istio-${ISTIO_VERSION}-${OSEXT}-${ISTIO_ARCH}.tar.gz\u0026#34; with_arch() { printf \u0026#34;\\nDownloading %s from %s ...\\n\u0026#34; \u0026#34;$NAME\u0026#34; \u0026#34;$ARCH_URL\u0026#34; if ! curl -o /dev/null -sIf \u0026#34;$ARCH_URL\u0026#34;; then printf \u0026#34;\\n%s is not found, please specify a valid ISTIO_VERSION and TARGET_ARCH\\n\u0026#34; \u0026#34;$ARCH_URL\u0026#34; exit 1 fi curl -fsLO \u0026#34;$ARCH_URL\u0026#34; filename=\u0026#34;istio-${ISTIO_VERSION}-${OSEXT}-${ISTIO_ARCH}.tar.gz\u0026#34; tar -xzf \u0026#34;${filename}\u0026#34; rm \u0026#34;${filename}\u0026#34; } without_arch() { printf \u0026#34;\\nDownloading %s from %s ...\u0026#34; \u0026#34;$NAME\u0026#34; \u0026#34;$URL\u0026#34; if ! curl -o /dev/null -sIf \u0026#34;$URL\u0026#34;; then printf \u0026#34;\\n%s is not found, please specify a valid ISTIO_VERSION\\n\u0026#34; \u0026#34;$URL\u0026#34; exit 1 fi curl -fsLO \u0026#34;$URL\u0026#34; filename=\u0026#34;istio-${ISTIO_VERSION}-${OSEXT}.tar.gz\u0026#34; tar -xzf \u0026#34;${filename}\u0026#34; rm \u0026#34;${filename}\u0026#34; } # Istio 1.6 and above support arch # Istio 1.5 and below do not have arch support ARCH_SUPPORTED=\u0026#34;1.6\u0026#34; # Istio 1.10 and above support arch for osx arm64 ARCH_SUPPORTED_OSX=\u0026#34;1.10\u0026#34; if [ \u0026#34;${OS}\u0026#34; = \u0026#34;Linux\u0026#34; ] ; then # This checks if ISTIO_VERSION is less than ARCH_SUPPORTED (version-sort\u0026#39;s before it) if [ \u0026#34;$(printf \u0026#39;%s\\n%s\u0026#39; \u0026#34;${ARCH_SUPPORTED}\u0026#34; \u0026#34;${ISTIO_VERSION}\u0026#34; | sort -V | head -n 1)\u0026#34; = \u0026#34;${ISTIO_VERSION}\u0026#34; ]; then without_arch else with_arch fi elif [ \u0026#34;${OS}\u0026#34; = \u0026#34;Darwin\u0026#34; ] ; then # This checks if ISTIO_VERSION is less than ARCH_SUPPORTED_OSX (version-sort\u0026#39;s before it) or ISTIO_ARCH not equal to arm64 if [ \u0026#34;$(printf \u0026#39;%s\\n%s\u0026#39; \u0026#34;${ARCH_SUPPORTED_OSX}\u0026#34; \u0026#34;${ISTIO_VERSION}\u0026#34; | sort -V | head -n 1)\u0026#34; = \u0026#34;${ISTIO_VERSION}\u0026#34; ] || [ \u0026#34;${ISTIO_ARCH}\u0026#34; != \u0026#34;arm64\u0026#34; ]; then without_arch else with_arch fi else printf \u0026#34;\\n\\n\u0026#34; printf \u0026#34;Unable to download Istio %s at this moment!\\n\u0026#34; \u0026#34;$ISTIO_VERSION\u0026#34; printf \u0026#34;Please verify the version you are trying to download.\\n\\n\u0026#34; exit 1 fi printf \u0026#34;\u0026#34; printf \u0026#34;\\nIstio %s Download Complete!\\n\u0026#34; \u0026#34;$ISTIO_VERSION\u0026#34; printf \u0026#34;\\n\u0026#34; printf \u0026#34;Istio has been successfully downloaded into the %s folder on your system.\\n\u0026#34; \u0026#34;$NAME\u0026#34; printf \u0026#34;\\n\u0026#34; BINDIR=\u0026#34;$(cd \u0026#34;$NAME/bin\u0026#34; \u0026amp;\u0026amp; pwd)\u0026#34; printf \u0026#34;Next Steps:\\n\u0026#34; printf \u0026#34;See https://istio.io/latest/docs/setup/install/ to add Istio to your Kubernetes cluster.\\n\u0026#34; printf \u0026#34;\\n\u0026#34; printf \u0026#34;To configure the istioctl client tool for your workstation,\\n\u0026#34; printf \u0026#34;add the %s directory to your environment path variable with:\\n\u0026#34; \u0026#34;$BINDIR\u0026#34; printf \u0026#34;\\t export PATH=\\\u0026#34;\\$PATH:%s\\\u0026#34;\\n\u0026#34; \u0026#34;$BINDIR\u0026#34; printf \u0026#34;\\n\u0026#34; printf \u0026#34;Begin the Istio pre-installation check by running:\\n\u0026#34; printf \u0026#34;\\t istioctl x precheck \\n\u0026#34; printf \u0026#34;\\n\u0026#34; printf \u0026#34;Need more information? Visit https://istio.io/latest/docs/setup/install/ \\n\u0026#34; 但是上面方式默认也是走 github 下载，国内访问 github 速度比较慢（也可能根本下载不了），所有我们可以自己去下载安装文件包，下载完成后解压，将解压后的 istioctl 文件移动到 /usr/local/bin 目录下：\n# wget https://ghps.cc/https://github.com/istio/istio/releases/download/1.19.3/istio-1.19.3-linux-amd64.tar.gz $ wget https://github.com/istio/istio/releases/download/1.19.3/istio-1.19.3-linux-amd64.tar.gz $ tar -xvf istio-1.19.3-linux-amd64.tar.gz $ mv istio-1.19.3/bin/istioctl /usr/local/bin/ $ istioctl version no ready Istio pods in \u0026#34;istio-system\u0026#34; 1.19.3 能执行 istioctl 命令说明安装成功，但是提示 no ready Istio pods in \u0026quot;istio-system\u0026quot;，这是因为我们还没有安装 Istio，接下来我们就来安装 Istio。\nistioctl 工具中内置了一些安装配置文件，这些配置文件提供了对 Istio 控制平面和 Istio 数据平面 Sidecar 的定制内容。我们可以通过 istioctl profile list 命令查看：\n$ istioctl profile list Istio configuration profiles: ambient default demo empty external minimal openshift preview remote 我们可以从其中一个 Istio 内置配置文件开始入手， 然后根据特定需求进一步自定义配置文件。\ndefault：根据 IstioOperator API 的默认配置启动组件。建议将此配置文件用于生产部署和多集群网格中的主集群，我们可以运行 istioctl profile dump 命令来查看默认设置，如下所示其实就是一个 IstioOperator 的配置文件： 注意：这里的IstioOperator和我们之前k8s里Operator不一样，这里没有直接的控制器，它是通过istioctl命令来做的一些解析。 istioctl 能够识别的一个安装配置文件，只是这个配置文件采用了类似于K8s CRD的方式来声明而已，并没有对应的控制器Pod。\n#istioctl profile dump default $ istioctl profile dump apiVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: components: base: enabled: true cni: enabled: false egressGateways: - enabled: false name: istio-egressgateway ingressGateways: - enabled: true name: istio-ingressgateway istiodRemote: enabled: false pilot: enabled: true hub: docker.io/istio meshConfig: defaultConfig: proxyMetadata: {} enablePrometheusMerge: true profile: default tag: 1.19.3 values: base: enableCRDTemplates: false validationURL: \u0026#34;\u0026#34; defaultRevision: \u0026#34;\u0026#34; gateways: istio-egressgateway: autoscaleEnabled: true env: {} name: istio-egressgateway secretVolumes: - mountPath: /etc/istio/egressgateway-certs name: egressgateway-certs secretName: istio-egressgateway-certs - mountPath: /etc/istio/egressgateway-ca-certs name: egressgateway-ca-certs secretName: istio-egressgateway-ca-certs type: ClusterIP istio-ingressgateway: autoscaleEnabled: true env: {} name: istio-ingressgateway secretVolumes: - mountPath: /etc/istio/ingressgateway-certs name: ingressgateway-certs secretName: istio-ingressgateway-certs - mountPath: /etc/istio/ingressgateway-ca-certs name: ingressgateway-ca-certs secretName: istio-ingressgateway-ca-certs type: LoadBalancer global: configValidation: true defaultNodeSelector: {} defaultPodDisruptionBudget: enabled: true defaultResources: requests: cpu: 10m imagePullPolicy: \u0026#34;\u0026#34; imagePullSecrets: [] istioNamespace: istio-system istiod: enableAnalysis: false jwtPolicy: third-party-jwt logAsJson: false logging: level: default:info meshNetworks: {} mountMtlsCerts: false multiCluster: clusterName: \u0026#34;\u0026#34; enabled: false network: \u0026#34;\u0026#34; omitSidecarInjectorConfigMap: false oneNamespace: false operatorManageWebhooks: false pilotCertProvider: istiod priorityClassName: \u0026#34;\u0026#34; proxy: autoInject: enabled clusterDomain: cluster.local componentLogLevel: misc:error enableCoreDump: false excludeIPRanges: \u0026#34;\u0026#34; excludeInboundPorts: \u0026#34;\u0026#34; excludeOutboundPorts: \u0026#34;\u0026#34; image: proxyv2 includeIPRanges: \u0026#39;*\u0026#39; logLevel: warning privileged: false readinessFailureThreshold: 30 readinessInitialDelaySeconds: 1 readinessPeriodSeconds: 2 resources: limits: cpu: 2000m memory: 1024Mi requests: cpu: 100m memory: 128Mi statusPort: 15020 tracer: zipkin proxy_init: image: proxyv2 sds: token: aud: istio-ca sts: servicePort: 0 tracer: datadog: {} lightstep: {} stackdriver: {} zipkin: {} useMCP: false istiodRemote: injectionURL: \u0026#34;\u0026#34; pilot: autoscaleEnabled: true autoscaleMax: 5 autoscaleMin: 1 configMap: true cpu: targetAverageUtilization: 80 env: {} image: pilot keepaliveMaxServerConnectionAge: 30m nodeSelector: {} podLabels: {} replicaCount: 1 traceSampling: 1 telemetry: enabled: true v2: enabled: true metadataExchange: wasmEnabled: false prometheus: enabled: true wasmEnabled: false stackdriver: configOverride: {} enabled: false logging: false monitoring: false topology: false demo：这个配置旨在展示 Istio 功能，具有适度的资源要求。它适合运行官方的 Bookinfo 示例应用和相关任务，比较适合用于快速入门的安装配置，比如我们这里就可以使用该配置。此配置文件启用了高级别的追踪和访问日志，因此不适合进行性能测试。我们也可以使用 istioctl profile dump demo 命令来查看该配置文件的具体配置资源对象。 minimal：与默认配置文件相同，但只安装了控制平面组件。这样我们就可以使用单独的配置文件来配置控制平面和数据平面组件（例如网关）。 remote：用于配置一个远程集群， 这个从集群由外部控制平面管理， 或者由多集群网格的主集群中的控制平面管理。 empty：不部署任何内容，可以作为自定义配置的基本配置文件。 preview：预览文件包含的功能都属于实验性阶段。该配置文件是为了探索 Istio 的新功能。确保稳定性、安全性和性能（使用风险需自负）。 ambient：Ambient 配置文件用于帮助开始使用 Ambient Mesh。需要注意的是 Ambient 目前处于 Alpha 状态。请勿在生产环境中使用 Ambient 模式。 openshift：用于在 OpenShift 上安装 Istio。 标注 ✔ 的组件安装在每个配置文件中：\n除了安装 Istio 内置的配置之外，istioctl install 还提供了一套完整的用于定制配置的 API - IstioOperator API。\n此 API 中的配置参数可以使用命令行选项 --set 进行独立设置，比如要在 default 配置中启动 debug 的日志特性，我们可以使用下面的命令：\n$ istioctl install --set values.global.logging.level=debug 或者，可以在 YAML 文件中指定 IstioOperator 的配置，然后用 -f 选项传递给 istioctl 即可：\n$ istioctl install -f samples/operator/pilot-k8s.yaml IstioOperator API 定义主要包括如下几个组件：\nbase pilot ingressGateways egressGateways cni istiodRemote 针对每一个组件的配置内容通过 components.\u0026lt;component name\u0026gt; 下的 API 中提供。例如，要用 API 更改 pilot 组件的 enabled 设置， 可以使用 --set components.pilot.enabled=false 命令，或在 IstioOperator 资源中设置：\napiVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: components: pilot: enabled: false 这样安装后就不会启动 pilot 组件了。\n所有的组件共享一个通用 API，用来修改 Kubernetes 特定的设置，它在 components.\u0026lt;component name\u0026gt;.k8s 路径下，它允许修改如下设置：\nResources Readiness probes Replica count HorizontalPodAutoscaler PodDisruptionBudget Pod annotations Service annotations ImagePullPolicy Priority class name Node selector Affinity and anti-affinity Service Toleration Strategy Env Pod security context 所有这些 Kubernetes 设置也就是平时我们使用的 Kubernetes API 定义，配置方式完全一样。比如我们要调整 Pilot 组件的资源限制和 Pod 水平伸缩，可以用如下所示的配置：\napiVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: components: pilot: k8s: resources: requests: cpu: 1000m # 覆盖默认的 500m memory: 4096Mi # ... 默认 2048Mi hpaSpec: maxReplicas: 10 # ... 默认 5 minReplicas: 2 # ... 默认 1 然后我们就可以直接使用 istioctl install 把改变的设置应用到集群：\n$ istioctl install -f samples/operator/pilot-k8s.yaml 了解了 Istio 安装的基本概念之后，我们就可以开始安装 Istio 了，我们这里选择 demo 配置文件进行安装，选择它是因为它包含了一组专为测试准备的功能集合，另外还有用于生产或性能测试的配置组合。\n执行下面的命令即可：\n$ istioctl install --set profile=demo -y ✔ Istio core installed ✔ Istiod installed ✔ Egress gateways installed ✔ Ingress gateways installed ✔ Installation complete Made this installation the default for injection and validation. 可以看到上面的 demo 配置应用后，启用了 Istio core、Istiod、Egress gateways、Ingress gateways 几个组件，这些组件都是通过 IstioOperator API 进行配置的。\n安装完成后会自动创建一个 istio-system 命名空间，包含如下几个 Pod，分别对应上面的几个组件：\n[root@master1 ~]#kubectl get po -nistio-system 这其实也和 Istio 架构中的组件是一一对应的，我们可以再看下 Istio 架构图：\nistio-egressgateway：出口网关，用于处理出站流量。 istio-ingressgateway：入口网关，用于处理入站流量，接收传入的 HTTP/TCP 连接。 istiod：Istio 控制平面，用于管理和配置数据平面，提供服务网格的核心功能。 当然安装后并没有出现架构图上面的数据平面相关组件，这是因为数据平面的 Envoy Sidecar 代理是伴随应用 Pod 一起的，所以需要部署应用后才会出现。\n安装成功。\n2、注入 Envoy Sidecar # 要为应用注入 Envoy Sidecar 代理有几种方法：\n1.手动注入 Sidecar # == 实战：手动注入 Envoy Sidecar-2023.11.3(测试成功)== # 实验环境：\nk8s v1.25.4（containerd://1.6.10） [root@master1 ~]#istioctl version client version: 1.19.3 control plane version: 1.19.3 data plane version: 1.19.3 (8 proxies) 实验软件：\n链接：https://pan.baidu.com/s/1pMnJxgL63oTlGFlhrfnXsA?pwd=7yqb 提取码：7yqb 2023.11.5-实战：BookInfo 示例应用-2023.11.5(测试成功) （使用的是同一个软件包）\n默认的测试demo如下位置：\n[root@master1 ~]#cat istio-1.19.3/samples/sleep/sleep.yaml # Copyright Istio Authors # # Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \u0026#34;AS IS\u0026#34; BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. ################################################################################################## # Sleep service ################################################################################################## apiVersion: v1 kind: ServiceAccount metadata: name: sleep --- apiVersion: v1 kind: Service metadata: name: sleep labels: app: sleep service: sleep spec: ports: - port: 80 name: http selector: app: sleep --- apiVersion: apps/v1 kind: Deployment metadata: name: sleep spec: replicas: 1 selector: matchLabels: app: sleep template: metadata: labels: app: sleep spec: terminationGracePeriodSeconds: 0 serviceAccountName: sleep containers: - name: sleep image: curlimages/curl command: [\u0026#34;/bin/sleep\u0026#34;, \u0026#34;infinity\u0026#34;] imagePullPolicy: IfNotPresent volumeMounts: - mountPath: /etc/sleep/tls name: secret-volume volumes: - name: secret-volume secret: secretName: sleep-secret optional: true --- [root@master1 ~]# 先来部署这个应用\n[root@master1 ~]#kubectl apply -f istio-1.19.3/samples/sleep/sleep.yaml serviceaccount/sleep created service/sleep created deployment.apps/sleep created [root@master1 ~]#kubectl get po -l app=sleep NAME READY STATUS RESTARTS AGE sleep-78ff5975c6-ftlzx 1/1 Running 0 46s 这是一个正常应用的状态。\n比如我们为一个 Deployment 手动注入 Sidecar，可以使用下面的方法：\n# istio-1.19.3 目录下 $ istioctl kube-inject -f samples/sleep/sleep.yaml | kubectl apply -f - serviceaccount/sleep created service/sleep created deployment.apps/sleep created 默认情况下 Pod 的注入是基于 Sidecar 注入模板，在 istio-sidecar-injector Configmap 中配置，我们可以将配置导出到本地，然后手动指定注入也可以：\n$ kubectl -n istio-system get configmap istio-sidecar-injector -o=jsonpath=\u0026#39;{.data.config}\u0026#39; \u0026gt; inject-config.yaml $ kubectl -n istio-system get configmap istio-sidecar-injector -o=jsonpath=\u0026#39;{.data.values}\u0026#39; \u0026gt; inject-values.yaml $ kubectl -n istio-system get configmap istio -o=jsonpath=\u0026#39;{.data.mesh}\u0026#39; \u0026gt; mesh-config.yaml 然后指定输入文件，运行 kube-inject 并部署应用即可：\n$ istioctl kube-inject \\ --injectConfigFile inject-config.yaml \\ --meshConfigFile mesh-config.yaml \\ --valuesFile inject-values.yaml \\ --filename samples/sleep/sleep.yaml \\ | kubectl apply -f - 正常部署后我们的应用 Pod 中会多出一个 Sidecar 容器：\n[root@master1 istio-1.19.3]#kubectl get po -l app=sleep NAME READY STATUS RESTARTS AGE sleep-7cbcd885cf-6j4bt 2/2 Running 0 2m8s [root@master1 istio-1.19.3]# 这个容器就是 Envoy Sidecar 代理。\n这个 Envoy Sidecar 代理，它会和应用 Pod 一起运行，用来代理整个应用的流量，当然整体控制都是由 Istio 控制平面来决定的。而 Istio 控制平面实现了 Envoy 的控制平面接口，可以动态下发修改 Envoy 的配置，这样我们就可以去操控这些 Envoy Sidecar 了，这是不是和我们前面讲解的 Envoy 基于 API 的动态配置部分就关联起来了，所以我们说需要先熟悉 Envoy，然后再来学习 Istio 效果会更好。\n测试完成。\n2.自定义注入 # 上面我们了解到 Pod 的注入是基于 istio-sidecar-injector 这个 Configmap 模板的，同样的我们也可以自行定义注入 Sidecar 代理。我们可以通过在 Pod 中手动添加一个 istio-proxy 容器来完成，Sidecar 注入将会把自定义的任何配置视为默认注入模板的覆盖。\n自定义这些设置时，需格外小心，因为允许完全自定义生成的 Pod，包括进行一些更改而导致 Sidecar 容器无法正常运行。\n例如，以下配置可自定义各种设置，包括降低 CPU 请求，添加 Volume 挂载，和添加 preStop Hook：\napiVersion: v1 kind: Pod metadata: name: example spec: containers: - name: hello image: alpine - name: istio-proxy # 这个容器名需要叫 istio-proxy image: auto # 设置为 auto，自动选择要使用的 Image resources: requests: cpu: \u0026#34;100m\u0026#34; volumeMounts: - mountPath: /etc/certs name: certs lifecycle: preStop: exec: command: [\u0026#34;sleep\u0026#34;, \u0026#34;10\u0026#34;] volumes: - name: certs secret: secretName: istio-certs 一般来说 Pod 中的任何字段我们都可以配置，但是必须注意某些字段：\nKubernetes 要求在注入运行之前配置 image，虽然可以可以设置一个指定的 Image 来覆盖默认的 image 配置，但建议将 image 设置为 auto，可使 Sidecar 注入自动选择要使用的 Image。 Pod 中一些字段取决于相关设置。例如，CPU 请求必须小于 CPU 限制。 3.自动注入 Sidecar # == 实战：自动注入 Envoy Sidecar-2023.11.3(测试失败)== # 自己在测试时，出现了问题，这里后续搭建v1.27.6k8s集群来测试。\n上面我们介绍了手动注入和自定义注入两种方式，这些都是需要我们手动去操作的，此外我们也可以通过配置实现自动注入，这样我们就不需要手动去注入了，Istio 提供了一个 Kubernetes 准入控制器，它可以自动注入 Sidecar 代理，我们只需要为命名空间设置 istio-injection=enabled 标签即可启用自动注入。\n比如我们先在集群中部署 sleep 应用：（测试之前记得先删除先前的测试demo）\n#在目录istio-1.19.3下 [root@master1 istio-1.19.3]#kubectl apply -f samples/sleep/sleep.yaml serviceaccount/sleep unchanged service/sleep unchanged deployment.apps/sleep created [root@master1 istio-1.19.3]#kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE sleep 1/1 1 1 22s web 1/1 1 1 6d4h [root@master1 istio-1.19.3]#kubectl get pod -l app=sleep NAME READY STATUS RESTARTS AGE sleep-78ff5975c6-v62sv 1/1 Running 0 31s 目前我们的 sleep 应用是没有注入 Sidecar 的，只有 1 个容器。\n现在我们可以为 default 命名空间打上 istio-injection=enabled 标签：\n[root@master1 istio-1.19.3]#kubectl label namespace default istio-injection=enabled --overwrite namespace/default labeled [root@master1 istio-1.19.3]#kubectl get namespace -L istio-injection NAME STATUS AGE ISTIO-INJECTION default Active 6d4h enabled istio-system Active 8h kube-flannel Active 6d4h kube-node-lease Active 6d4h kube-public Active 6d4h kube-system Active 6d4h kubernetes-dashboard Active 6d4h [root@master1 istio-1.19.3]#kubectl get pod -l app=sleep NAME READY STATUS RESTARTS AGE sleep-78ff5975c6-v62sv 1/1 Running 0 108s 现在我们为命名空间打上了自动注入标签，但是已经部署了的应用并不会主动注入，这是因为注入的动作发生着 Pod 创建时。我们可以杀掉正在运行的 Pod 来验证新创建的 Pod 是否注入 Sidecar。\n$ kubectl delete pod -l app=sleep $ kubectl get pod -l app=sleep NAME READY STATUS RESTARTS AGE sleep-9454cc476-kmbzz 2/2 Running 0 5s 可以看到现在我们的应用有 2 个容器了。查看已注入 Pod 的详细状态，应该可以看到被注入的 istio-proxy 容器：\n$ kubectl describe pod -l app=sleep ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 91s default-scheduler Successfully assigned default/sleep-9454cc476-kmbzz to node2 Normal Pulled 90s kubelet Container image \u0026#34;docker.io/istio/proxyv2:1.19.3\u0026#34; already present on machine Normal Created 90s kubelet Created container istio-init Normal Started 90s kubelet Started container istio-init Normal Pulled 90s kubelet Container image \u0026#34;curlimages/curl\u0026#34; already present on machine Normal Created 90s kubelet Created container sleep Normal Started 90s kubelet Started container sleep Normal Pulled 90s kubelet Container image \u0026#34;docker.io/istio/proxyv2:1.19.3\u0026#34; already present on machine Normal Created 89s kubelet Created container istio-proxy Normal Started 89s kubelet Started container istio-proxy 这样我们就实现了 Pod 应用自动注入 Envoy Sidecar 代理，以后 default 命名空间中的应用部署后都会自动注入了。当然要取消自动注入功能，只需要将 istio-injection 标签去掉即可：\n$ kubectl label namespace default istio-injection- 当然除了在命名空间级别启用和禁用注入功能之外，注入也可以通过配置 Pod 上的 sidecar.istio.io/inject: \u0026quot;true\u0026quot; 标签，在每个 Pod 的基础上进行控制。\n自动注入 Envoy Sidecar 的原理其实很简单，如果你对 Kubernetes 比较熟悉的话，肯定听过准入控制器的，其中就有两个对象 validatingwebhookconfigurations、mutatingwebhookconfigurations，前面是用来校验资源是否合法的，后面是用来修改资源对象的，那么要实现自动注入功能，也就相当于去为 Pod 自动添加一个容器，所以就需要用到 mutate 功能了，我们可以查看下系统中是否包含一个 istio-sidecar-injector 的准入控制器：\n$ kubectl get mutatingwebhookconfigurations NAME WEBHOOKS AGE istio-sidecar-injector 4 63m 而实现准入控制器逻辑的也正是 istiod 这个组件了。\n3、BookInfo 示例应用 # Bookinfo 是一个非常经典的微服务示例项目，它由四个独立的微服务组成，每个微服务都是用不同的语言编写的，可以用于演示多种 Istio 特性的应用。这个应用模仿在线书店的一个分类，显示一本书的信息，页面上会显示一本书的描述，书籍的细节（ISBN、页数等），以及关于这本书的一些评论。\nBookinfo 应用分为以下四个单独的微服务：\nproductpage：这个微服务会调用 details 和 reviews 两个微服务，用来生成页面。 details：这个微服务中包含了书籍的信息。 reviews：这个微服务中包含了书籍相关的评论，它还会调用 ratings 微服务。 ratings：这个微服务中包含了由书籍评价组成的评级信息。 reviews 微服务有 3 个版本：\nv1 版本不会调用 ratings 服务。 v2 版本会调用 ratings 服务，并使用 1 到 5 个黑色星形图标来显示评分信息。 v3 版本会调用 ratings 服务，并使用 1 到 5 个红色星形图标来显示评分信息。 下图展示了这个应用的端到端架构。\n现在我们来为 Bookinfo 应用提供 Istio 支持，无需对应用自身做出任何改变。只要简单的在 Istio 环境中对服务进行配置和运行即可，其实就是把 Envoy Sidecar 注入到每个服务之中。最终的部署结果将如下图所示：\n所有的微服务都和 Envoy Sidecar 集成在一起，服务所有的出入流量都被 Sidecar 所劫持，然后我们就可以利用 Istio 控制平面为应用提供服务路由、遥测数据收集以及策略实施等服务治理功能了。\n== 实战：BookInfo 示例应用-2023.11.5(测试成功)== # 实验环境：\nk8s v1.25.4（containerd://1.6.10） [root@master1 ~]#istioctl version client version: 1.19.3 control plane version: 1.19.3 data plane version: 1.19.3 (8 proxies) 实验软件：\n链接：https://pan.baidu.com/s/1pMnJxgL63oTlGFlhrfnXsA?pwd=7yqb 提取码：7yqb 2023.11.5-实战：BookInfo 示例应用-2023.11.5(测试成功)\n实验步骤：\ngraph LR A[实战步骤] --\u0026gt;B(1️⃣ 部署示例应用) A[实战步骤] --\u0026gt;C(2️⃣ 对外暴露应用) A[实战步骤] --\u0026gt;D(3️⃣ 查看 Dashboard) ==1.部署示例应用==\n我们首先进入 Istio 安装目录，该目录中的 samples 目录中就包含了 Bookinfo 部署到 Kubernetes 集群中的配置文件，我们只需要为这些资源注入 Sidecar。这里我们为 default 命名空间打上标签 istio-injection=enabled，开启自动注入功能即可：\n$ kubectl label namespace default istio-injection=enabled --overwrite 然后只需要使用 kubectl 正常部署应用即可：\n$ kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml service/details created serviceaccount/bookinfo-details created deployment.apps/details-v1 created service/ratings created serviceaccount/bookinfo-ratings created deployment.apps/ratings-v1 created service/reviews created serviceaccount/bookinfo-reviews created deployment.apps/reviews-v1 created deployment.apps/reviews-v2 created deployment.apps/reviews-v3 created service/productpage created serviceaccount/bookinfo-productpage created deployment.apps/productpage-v1 created 注意：\n如果在安装过程中禁用了 Sidecar 自动注入功能而选择手动注入 Sidecar，可以使用 istioctl kube-inject 命令来手动注入：\n$ kubectl apply -f \u0026lt;(istioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo.yaml) 自己本次手动注入：（我的自动注入有点问题，这里使用手动注入方式）\n[root@master1 istio-1.19.3]#kubectl apply -f \u0026lt;(istioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo.yaml) service/details created serviceaccount/bookinfo-details created deployment.apps/details-v1 created service/ratings created serviceaccount/bookinfo-ratings created deployment.apps/ratings-v1 created service/reviews created serviceaccount/bookinfo-reviews created deployment.apps/reviews-v1 created deployment.apps/reviews-v2 created deployment.apps/reviews-v3 created service/productpage created serviceaccount/bookinfo-productpage created deployment.apps/productpage-v1 created 上面的命令会启动全部的四个服务，其中也包括了 reviews 服务的三个版本（v1、v2 以及 v3）。\n隔一会儿，我们可以查看下 Pod 的状态：\n$ kubectl get pods NAME READY STATUS RESTARTS AGE details-v1-5f4d584748-97stc 2/2 Running 0 12m productpage-v1-564d4686f-z7fwn 2/2 Running 0 12m ratings-v1-686ccfb5d8-cvlcq 2/2 Running 0 12m reviews-v1-86896b7648-rrjxv 2/2 Running 0 12m reviews-v2-b7dcd98fb-kk8fv 2/2 Running 0 12m reviews-v3-5c5cc7b6d-c4wf5 2/2 Running 0 12m $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE details ClusterIP 10.98.49.69 \u0026lt;none\u0026gt; 9080/TCP 13m kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 44d productpage ClusterIP 10.101.255.202 \u0026lt;none\u0026gt; 9080/TCP 13m ratings ClusterIP 10.101.210.142 \u0026lt;none\u0026gt; 9080/TCP 13m reviews ClusterIP 10.106.93.80 \u0026lt;none\u0026gt; 9080/TCP 13m [root@master1 istio-1.19.3]#kubectl get po NAME READY STATUS RESTARTS AGE details-v1-76bdcd6b59-9p5gh 2/2 Running 0 18m productpage-v1-d594c998-x8s6k 2/2 Running 0 18m ratings-v1-64694b479f-qcbcm 2/2 Running 0 18m reviews-v1-68f98444cd-szhrv 2/2 Running 0 18m reviews-v2-6c9b87bd47-t4l9c 2/2 Running 0 18m reviews-v3-9569b895b-jr29m 2/2 Running 0 18m [root@master1 istio-1.19.3]#kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE details ClusterIP 10.107.100.41 \u0026lt;none\u0026gt; 9080/TCP 19m kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 7d17h productpage ClusterIP 10.110.115.123 \u0026lt;none\u0026gt; 9080/TCP 19m ratings ClusterIP 10.101.72.27 \u0026lt;none\u0026gt; 9080/TCP 19m reviews ClusterIP 10.111.206.146 \u0026lt;none\u0026gt; 9080/TCP 19m 当每个 Pod 准备就绪时，Istio Sidecar 将伴随应用一起部署。\n要确认 Bookinfo 应用是否正在运行，我们可以在某个 Pod 中用 curl 命令对应用发送请求来验证，例如 ratings：\n$ kubectl exec \u0026#34;$(kubectl get pod -l app=ratings -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;)\u0026#34; -c ratings -- curl -sS productpage:9080/productpage | grep -o \u0026#34;\u0026lt;title\u0026gt;.*\u0026lt;/title\u0026gt;\u0026#34; \u0026lt;title\u0026gt;Simple Bookstore App\u0026lt;/title\u0026gt; 如果能够正常数据，说明应用已经部署成功了，而且已经被 Istio 管控起来了。\n==2.对外暴露应用==\n上面我们已经将 Bookinfo 应用部署成功了，但是还不能被外界访问，如果没有 Istio 的情况下我们可以直接通过 NodePort 或者 Ingress 的方式来对外进行暴露，但是现在我们使用了 Istio，需要创建 ==Istio 入站网关（Ingress Gateway）==来对外暴露应用了。\n在 samples 目录下面有一个 Bookinfo 对外暴露的配置文件 bookinfo-gateway.yaml，这个配置文件中定义了一个 Gateway 和一个 VirtualService 对象，Gateway 定义了入站流量的端口和协议，VirtualService 定义了流量的路由规则，内容如下所示：\n# samples/bookinfo/networking/bookinfo-gateway.yaml apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: bookinfo-gateway spec: selector: # 如果使用的是 Helm 方式安装，则默认应该是 istio=ingress 标签 istio: ingressgateway # 匹配 ingress gateway pod 的标签（kubectl get pods -l istio=ingressgateway -n istio-system） servers: - port: number: 8080 name: http protocol: HTTP hosts: - \u0026#34;*\u0026#34; --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: bookinfo spec: hosts: - \u0026#34;*\u0026#34; gateways: - bookinfo-gateway http: - match: - uri: exact: /productpage - uri: prefix: /static - uri: exact: /login - uri: exact: /logout - uri: prefix: /api/v1/products route: - destination: host: productpage port: number: 9080 这两个资源对象我们后面还会详细讲解的，现在我们只需要知道这两个资源对象是用来定义入站流量的端口和协议以及流量的路由规则的即可。如果你联想到前面我们讲解的 Envoy 配置，那么你就会发现这两个资源对象其实分别对应着 Envoy 配置中的 Listener 和 VirtualHosts（包括 Cluster），只不过这里是用我们更熟悉的 Kubernetes CRD 的方式来进行配置的，当我们创建了这两个资源对象后，Istio 控制平面会自动把这些配置下发到 Envoy Sidecar 中，然后 Envoy Sidecar 就会根据这些配置来进行流量的路由了。\n接下来我们执行下面的命令来把应用关联到 Istio 网关：\n$ kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml gateway.networking.istio.io/bookinfo-gateway created virtualservice.networking.istio.io/bookinfo created 然后我们就可以通过 istio-ingressgateway 这个统一的入口来访问应用了，我们可以通过 kubectl get svc -n istio-system 命令来查看下 istio-ingressgateway 的地址：\n$ kubectl get svc istio-ingressgateway -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-ingressgateway LoadBalancer 10.103.227.57 \u0026lt;pending\u0026gt; 15021:32459/TCP,80:31896/TCP,443:30808/TCP,31400:31535/TCP,15443:30761/TCP 155m [root@master1 istio-1.19.3]#kubectl get svc istio-ingressgateway -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-ingressgateway LoadBalancer 10.104.174.171 \u0026lt;pending\u0026gt; 15021:32479/TCP,80:31814/TCP,443:31263/TCP,31400:32543/TCP,15443:30806/TCP 13h 由于我们这里没有负载均衡器，所以 EXTERNAL-IP 的值一直是 \u0026lt;pending\u0026gt; 状态，这种情况下我们可以用 NodePort 来访问网关。可以通过如下命令获取 HTTP 和 HTTPS 的访问端口：\n$ export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath=\u0026#39;{.spec.ports[?(@.name==\u0026#34;http2\u0026#34;)].nodePort}\u0026#39;) $ export SECURE_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath=\u0026#39;{.spec.ports[?(@.name==\u0026#34;https\u0026#34;)].nodePort}\u0026#39;) 然后获取集群中任意一个节点的 IP 地址：\n$ export INGRESS_HOST=$(kubectl get po -l istio=ingressgateway -n istio-system -o jsonpath=\u0026#39;{.items[0].status.hostIP}\u0026#39;) 设置环境变量 GATEWAY_URL\n$ export GATEWAY_URL=$INGRESS_HOST:$INGRESS_PORT 确保 IP 地址和端口均成功地赋值给了环境变量：\n$ echo \u0026#34;$GATEWAY_URL\u0026#34; 192.168.0.20:31896 [root@master1 istio-1.19.3]#echo \u0026#34;$GATEWAY_URL\u0026#34; 172.29.9.62:31814 这样我们就得到了 Istio 入口网关的访问地址，然后我们只需要在浏览器中通过 http://$GATEWAY_URL/productpage 即可访问到 Bookinfo 应用了。\n注意：\n==3.查看 Dashboard==\nIstio 和几个遥测应用做了集成。遥测能帮我们了解服务网格的结构、展示网络的拓扑结构、分析网格的健康状态。\n我们可以使用下面的方式来安装 Kiali，包括 Prometheus、Grafana 以及 jaeger。\n$ kubectl apply -f samples/addons $ kubectl rollout status deployment/kiali -n istio-system Waiting for deployment \u0026#34;kiali\u0026#34; rollout to finish: 0 of 1 updated replicas are available... deployment \u0026#34;kiali\u0026#34; successfully rolled out $ kubectl get pods -n istio-system NAME READY STATUS RESTARTS AGE grafana-5f9b8c6c5d-jv65v 1/1 Running 0 6m23s istio-egressgateway-556f6f58f4-hkzdd 1/1 Running 0 177m istio-ingressgateway-9c8b9b586-s6s48 1/1 Running 0 177m istiod-644f5d55fc-zz2zh 1/1 Running 0 179m jaeger-db6bdfcb4-qpmmv 1/1 Running 0 6m23s kiali-7c9d5f9f96-cp4mb 1/1 Running 0 6m23s prometheus-5d5d6d6fc-2gtxm 2/2 Running 0 6m22s 如果在安装插件时出错，再运行一次命令。有一些和时间相关的问题，再次运行就能解决。\n安装完成后我们可以使用 istioctl dashboard 命令来开启访问 Kiali 面板。\n$ istioctl dashboard kiali --address 0.0.0.0 http://0.0.0.0:20001/kiali # http://172.29.9.61:20001/kiali/ 在左侧的导航菜单，选择 Graph ，然后在 Namespace 下拉列表中，选择 default。\n要查看链路追踪数据，必须向服务发送请求。请求的数量取决于 Istio 的采样率，采样率在安装 Istio 时设置，默认采样速率为 1%。也就是说在第一个 tracing 可见之前，需要发送至少 100 个请求：\n$ for i in $(seq 1 100); do curl -s -o /dev/null \u0026#34;http://$GATEWAY_URL/productpage\u0026#34;; done Kiali 的仪表板展示了网格的概览以及 Bookinfo 示例应用的各个服务之间的关系。 它还提供过滤器来可视化流量的流动。\n到这里我们就完成了 Bookinfo 示例应用的部署，以及对外暴露应用的配置，接下来我们就可以开始学习 Istio 的核心功能了。\n:warning: 注意：这里的pod要依赖pvc\n本次这里就暂不安装nfs了，记录下即可。\ndocker run --restart=always --name moments -e NUXT_JWT_KEY=ZAkzv09rdrQTVXUk -d -v /docker/data/moments:/app/data -p 3000:3000 kingwrcy/moments:0.2 basepath=$(cd `dirname $0`; pwd) mkdir -p ${basepath}/moments docker run --name moments -d -v ${basepath}/moments:/app/data -p 3001:3000 192.168.0.140:881/kingwrcy/moments:latest docker run --name moments -e NUXT_JWT_KEY=ZAkzv09rdrQTVXUk -d -v /root/moments:/app/data -p 3001:3000 192.168.0.140:881/kingwrcy/moments:latest "},{"id":87,"href":"/docs/k8s-cni%E5%89%96%E6%9E%90%E6%BC%94%E8%BF%9B-k8scni-pou-xi-yan-jin/","title":"K8S CNI剖析演进 2024-03-04 10:51:33.245","section":"Docs","content":"\n"},{"id":88,"href":"/docs/k8s-csi-openebs%E5%8E%9F%E7%90%86-k8scsiopenebs-yuan-li/","title":"K8S csi openebs原理 2024-04-03 14:44:48.967","section":"Docs","content":" Kubernetes CSI (二): OpenEBS 原理 # 简介 # 在 Kubernetes CSI (一): Kubernetes 存储原理 一文中详细讲解了 Kubernetes CSI 的原理，本篇文章通过原理和源码走读形式讲解 OpenEBS 原理。 # OpenEBS 是一款使用Go语言编写的基于容器的块存储开源软件。OpenEBS 使得在容器中运行关键性任务和需要数据持久化的负载变得更可靠。实现了 Kubernetes CSI，所以可以很方便对接 Kubernetes 存储功能。 # 对于大部分第三方存储厂商，它们都只实现了分布式存储，OpenEBS 可以为 Kubernetes 有状态负载( StatefulSet ) 提供本地存储卷和分布式存储卷。 # 本篇文章重点讲解 OpenEBS 的本地存储卷。\n本地存储卷 # 本地存储卷很容易理解，在 Kubernetes 中本身就支持 Hostpath 类型的存储卷， # 使用 HostPath 有一个局限性就是，我们的 Pod 不能随便漂移，需要固定到一个节点上，因为一旦漂移到其他节点上去了宿主机上面就没有对应的数据了，所以我们在使用 HostPath 的时候都会搭配 nodeSelector 来进行使用。 # 但是使用 HostPath 明显也有一些好处的，因为 PV 直接使用的是本地磁盘，尤其是 SSD 盘，它的读写性能相比于大多数远程存储来说，要好得多，所以对于一些对磁盘 IO 要求比较高的应用，比如 etcd 就非常实用了。不过呢，相比于正常的 PV 来说，使用了 HostPath 的这些节点一旦宕机数据就可能丢失，所以这就要求使用 HostPath 的应用必须具备数据备份和恢复的能力，允许你把这些数据定时备份在其他位置。 # 所以在 HostPath 的基础上，Kubernetes 依靠 PV、PVC 实现了一个新的特性，这个特性的名字叫作：Local Persistent Volume，也就是我们说的 Local PV。 # 要想使用 Local PV 考虑的因素也比较多，下面详细看看： # Local PV # 其实 Local PV 实现的功能就非常类似于 HostPath 加上 nodeAffinity ，即表示该 PV 就是一个 Hostpath 类型卷。 # apiVersion: v1 kind: PersistentVolume metadata: name: pv-local spec: capacity: storage: 5Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Delete storageClassName: local-storage local: path: /data/k8s/localpv # 对应主机上数据目录 # 该 pv 与节点绑定 nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - node-1 Local PV 不仅仅支持 Filesystem 类型存储，还支持 Block，LVM 类型 # 延迟绑定 # 延迟绑定就是在 Pod 调度完成之后，再绑定对应的 PVC、PV。对于使用 Local PV 的 Pod，必须延迟绑定。 # 比如现在明确规定，这个 Pod 只能运行在 node-1 这个节点上，且该 Pod 申请了一个 PVC。对于没有延迟属性的 StorageClass，那么就会在 Pod 调度到某个节点之前就将该 PVC 绑定到合适的 PV，如果集群 node-1，node-2 都存在 PV 可以和该 PVC 绑定，那么就有可能该 PVC 绑定了 node-2 的 PV，就会导致 Pod 调度失败。 # 所以为了避免这种现象，就必须在 PVC 和 PV 绑定之前就将 Pod 调度完成。所以我们在使用 Local PV 的时候，就必须延迟绑定操作，即延迟到 Pod 调度完成之后再绑定 PVC。 # 那么怎么才能实现延迟绑定？ # 对于 Local PV 类型的 StorageClass 需要配置 volumeBindingMode=WaitForFirstConsumer 的属性，就是告诉 Kubernetes 在发现这个 StorageClass 关联的 PVC 与 PV 可以绑定在一起，但不要现在就立刻执行绑定操作（即：设置 PVC 的 VolumeName 字段），而是要等到第一个声明使用该 PVC 的 Pod 出现在调度器之后，调度器再综合考虑所有的调度规则，当然也包括每个 PV所在的节点位置，来统一决定。这个 Pod 声明的 PVC，到底应该跟哪个 PV 进行绑定。通过这个延迟绑定机制，原本实时发生的 PVC 和 PV 的绑定过程，就被延迟到了 Pod 第一次调度的时候在调度器中进行，从而保证了这个绑定结果不会影响 Pod 的正常调度。 # apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: local-storage provisioner: kubernetes.io/no-provisioner # 延迟绑定属性 volumeBindingMode: WaitForFirstConsumer 原地重启 # 我们知道使用 Hostpath 存储卷类型 Pod 如果不设置节点选择器，那么重启后会调度到其他节点上运行，这样就会导致之前数据丢失。 # 使用 Local PV 存储卷就无需担心该问题，因为根据 Local PV 的特性会保证该 Pod 重启后始终在当前节点运行。当然这里涉及到 Kubernetes 调度器和 PVC、PV的相关原理，下面我们简单描述下。 # 当 Pod 重启后，调度器首先判断该 Pod下的 PVC 是否已经绑定，如果已经绑定，那么就根据 PVC Annotation volume.kubernetes.io/selected-node 字段过滤到其他 node。 # 这样就保证该 Pod 就一直在 PV 所在 node 上运行。 # 这段代码逻辑参考以下： # // For PVCs that are bound, then it checks that the corresponding PV\u0026#39;s node affinity is // satisfied by the given node. // kubernetes/pkg/scheduler/framework/plugins/volumebinding/binder.go:316 func (b *volumeBinder) FindPodVolumes(pod *v1.Pod, boundClaims, claimsToBind []*v1.PersistentVolumeClaim, node *v1.Node) (podVolumes *PodVolumes, reasons ConflictReasons, err error) { ... // Find matching volumes and node for unbound claims if len(claimsToBind) \u0026gt; 0 { var ( claimsToFindMatching []*v1.PersistentVolumeClaim claimsToProvision []*v1.PersistentVolumeClaim ) // 调度器对集群中的 node 与 pvc Annotation volume.kubernetes.io/selected-node 字段比较，过滤掉不匹配 node for _, claim := range claimsToBind { if selectedNode, ok := claim.Annotations[volume.AnnSelectedNode]; ok { if selectedNode != node.Name { // Fast path, skip unmatched node. unboundVolumesSatisfied = false return } claimsToProvision = append(claimsToProvision, claim) } else { claimsToFindMatching = append(claimsToFindMatching, claim) } } ... } 上面说的 PVC Annotation 是在 Pod 调度完成后，调度器设置该 Annotation，其 value 是节点名称。 # 这段代码逻辑参考以下： # // AssumePodVolumes will take the matching PVs and PVCs to provision in pod\u0026#39;s // volume information for the chosen node, and: // 1. Update the pvCache with the new prebound PV. // 2. Update the pvcCache with the new PVCs with annotations set // 3. Update PodVolumes again with cached API updates for PVs and PVCs. // kubernetes/pkg/scheduler/framework/plugins/volumebinding/binder.go:359 func (b *volumeBinder) AssumePodVolumes(assumedPod *v1.Pod, nodeName string, podVolumes *PodVolumes) (allFullyBound bool, err error) { ... newProvisionedPVCs := []*v1.PersistentVolumeClaim{} for _, claim := range podVolumes.DynamicProvisions { claimClone := claim.DeepCopy() // 设置 volume.kubernetes.io/selected-node annotation，value 为该 pod 调度的 nodeName metav1.SetMetaDataAnnotation(\u0026amp;claimClone.ObjectMeta, volume.AnnSelectedNode, nodeName) err = b.pvcCache.Assume(claimClone) if err != nil { b.revertAssumedPVs(newBindings) b.revertAssumedPVCs(newProvisionedPVCs) return } newProvisionedPVCs = append(newProvisionedPVCs, claimClone) } podVolumes.StaticBindings = newBindings podVolumes.DynamicProvisions = newProvisionedPVCs return } OpenEBS 使用 # OpenEBS 对本地存储卷功能支持非常好： # • OpenEBS可以使用宿主机裸块设备或分区，或者使用Hostpaths上的子目录，或者使用LVM、ZFS来创建持久化卷 # • 本地卷直接挂载到StatefulSet Pod中，而不需要OpenEBS在数据路径中增加任何开销 # • OpenEBS为本地卷提供了额外的工具，用于监控、备份/恢复、灾难恢复、由ZFS或LVM支持的快照等 # 同时 OpenEBS 屏蔽了我们使用 Local PV 复杂性。OpenEBS 部署及使用也是非常方便。 # 这里我们只使用 local-pv-hostpath，即使用本地文件系统的存储，根据官网介绍，不需要提前安装 iscsi # Helm 部署 # $ helm repo add openebs https://openebs.github.io/charts $ helm repo update $ helm install openebs --namespace openebs openebs/openebs --create-namespace kubectl 部署 # kubectl apply -f https://openebs.github.io/charts/openebs-operator.yaml 安装成功后，默认会部署两个 storageclass ，我们目前需要 openebs-hostpath。 # $ kubectl get pods -n openebs NAME READY STATUS RESTARTS AGE openebs-localpv-provisioner-69c8648db7-cnj45 1/1 Running 0 33m openebs-ndm-bbgpv 1/1 Running 0 33m openebs-ndm-bxsbb 1/1 Running 0 33m openebs-ndm-cluster-exporter-9d75d564d-qvqz6 1/1 Running 0 33m openebs-ndm-kdg7b 1/1 Running 0 33m openebs-ndm-node-exporter-9zm62 1/1 Running 0 33m openebs-ndm-node-exporter-hlj7h 1/1 Running 0 33m openebs-ndm-node-exporter-j6wj7 1/1 Running 0 33m openebs-ndm-node-exporter-s5hk4 1/1 Running 0 33m openebs-ndm-operator-789985cc47-r4hwj 1/1 Running 0 33m openebs-ndm-qm4sw 1/1 Running 0 33m $ kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE openebs-device openebs.io/local Delete WaitForFirstConsumer false 116s openebs-hostpath openebs.io/local Delete WaitForFirstConsumer false 116s 默认存储目录为 ****/var/openebs/local, 如果需要更改的话，直接修改 openebs-operator.yaml，value 字段即可 # apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: openebs-hostpath annotations: openebs.io/cas-type: local cas.openebs.io/config: | #hostpath type will create a PV by # creating a sub-directory under the # BASEPATH provided below. - name: StorageType value: \u0026#34;hostpath\u0026#34; #Specify the location (directory) where # where PV(volume) data will be saved. # A sub-directory with pv-name will be # created. When the volume is deleted, # the PV sub-directory will be deleted. #Default value is /var/openebs/local - name: BasePath value: \u0026#34;/var/openebs/local/\u0026#34; 验证 # 接下来我们创建一个 PVC 资源对象，Pod 使用这个 PVC 就可以从 OpenEBS 动态 Local PV Provisioner 中请求 Hostpath Local PV 了。 # # local-hostpath-pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: local-hostpath-pvc spec: storageClassName: openebs-hostpath accessModes: - ReadWriteOnce resources: requests: storage: 5Gi 直接创建这个 PVC 即可： # $ kubectl apply -f local-hostpath-pvc.yaml $ kubectl get pvc local-hostpath-pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE local-hostpath-pvc Pending openebs-hostpath 12s 我们可以看到这个 PVC 的状态是 Pending，这是因为对应的 StorageClass 是延迟绑定模式，所以需要等到 Pod 消费这个 PVC 后才会去绑定，接下来我们去创建一个 Pod 来使用这个 PVC。 # 声明一个如下所示的 Pod 资源清单： # # local-hostpath-pod.yaml apiVersion: v1 kind: Pod metadata: name: hello-local-hostpath-pod spec: volumes: - name: local-storage persistentVolumeClaim: claimName: local-hostpath-pvc containers: - name: hello-container image: busybox command: - sh - -c - \u0026#39;while true; do echo \u0026#34;`date` [`hostname`] Hello from OpenEBS Local PV.\u0026#34; \u0026gt;\u0026gt; /mnt/store/greet.txt; sleep $(($RANDOM % 5 + 300)); done\u0026#39; volumeMounts: - mountPath: /mnt/store name: local-storage 直接创建这个 Pod： # $ kubectl apply -f local-hostpath-pod.yaml $ kubectl get pods hello-local-hostpath-pod NAME READY STATUS RESTARTS AGE hello-local-hostpath-pod 1/1 Running 0 2m7s $ kubectl get pvc local-hostpath-pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE local-hostpath-pvc Bound pvc-3f4a1a65-6cbc-42bf-a1f8-87ad238c0b88 5Gi RWO openebs-hostpath 5m41s 可以看到 Pod 运行成功后，PVC 也绑定上了一个自动生成的 PV，我们可以查看这个 PV 的详细信息： # $ kubectl get pv pvc-3f4a1a65-6cbc-42bf-a1f8-87ad238c0b88 -o yaml apiVersion: v1 kind: PersistentVolume metadata: annotations: pv.kubernetes.io/provisioned-by: openebs.io/local creationTimestamp: \u0026#34;2021-01-07T02:48:14Z\u0026#34; finalizers: - kubernetes.io/pv-protection labels: openebs.io/cas-type: local-hostpath ...... name: pvc-3f4a1a65-6cbc-42bf-a1f8-87ad238c0b88 resourceVersion: \u0026#34;21193802\u0026#34; selfLink: /api/v1/persistentvolumes/pvc-3f4a1a65-6cbc-42bf-a1f8-87ad238c0b88 uid: f7cccdb3-d23a-4831-86c3-4363eb1a8dee spec: accessModes: - ReadWriteOnce capacity: storage: 5Gi claimRef: apiVersion: v1 kind: PersistentVolumeClaim name: local-hostpath-pvc namespace: default resourceVersion: \u0026#34;21193645\u0026#34; uid: 3f4a1a65-6cbc-42bf-a1f8-87ad238c0b88 local: fsType: \u0026#34;\u0026#34; path: /var/openebs/local/pvc-3f4a1a65-6cbc-42bf-a1f8-87ad238c0b88 nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - node2 persistentVolumeReclaimPolicy: Delete storageClassName: openebs-hostpath volumeMode: Filesystem status: phase: Bound 本地数据目录位于 /var/openebs/local/pvc-3f4a1a65-6cbc-42bf-a1f8-87ad238c0b88 下面。 # 接着我们来验证下 volume 数据，前往 node2 节点查看下上面的数据目录中的数据： # $ ls /var/openebs/local/pvc-3f4a1a65-6cbc-42bf-a1f8-87ad238c0b88 greet.txt $ cat /var/openebs/local/pvc-3f4a1a65-6cbc-42bf-a1f8-87ad238c0b88/greet.txt Thu Jan 7 10:48:49 CST 2021 [hello-local-hostpath-pod] Hello from OpenEBS Local PV. Thu Jan 7 10:53:50 CST 2021 [hello-local-hostpath-pod] Hello from OpenEBS Local PV. 可以看到 Pod 容器中的数据已经持久化到 Local PV 对应的目录中去了。但是需要注意的是 StorageClass 默认的数据回收策略是 Delete，所以如果将 PVC 删掉后数据会自动删除，我们可以 Velero 这样的工具来进行备份还原。 # OpenEBS Local PV 原理 # 由于 OpenEBS 实现了多种存储，由于篇幅问题，下面只详细讲解 Hostpath 类型原理 # OpenEBS Local PV 部署架构 # 根据上一篇 Kubernetes CSI (一): Kubernetes 存储原理 说到 CSI 分为两部分： # • External component( Kubernetes Team ) • CSI Driver 具体作用和原理可查看原文。 # OpenEBS 同样也实现了 CSI，所以它的部署架构遵从 CSI 部署统一标准。但是对于 Local PV (Hostpath) 类型并不需要那么复杂。 # 只需提供 openebs-localpv-provisioner 即可，无需提供 CSI Driver，因为本地数据目录挂载 Kubelet 就可以完成，无需第三方 CSI。 # $ kubectl get pods -n openebs NAME READY STATUS RESTARTS AGE openebs-localpv-provisioner-69c8648db7-cnj45 1/1 Running 0 33m 根据上一篇文章讲解，openebs-localpv-provisioner 应该是一个 Deployment 或者 DaemonSet，由 External component( Kubernetes Team ) sideCar 和 CSI Identity + CSI Controller 组成。 # apiVersion: apps/v1 kind: Deployment metadata: labels: name: openebs-localpv-provisioner openebs.io/component-name: openebs-localpv-provisioner openebs.io/version: 3.0.0 name: openebs-localpv-provisioner namespace: openebs spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: name: openebs-localpv-provisioner openebs.io/component-name: openebs-localpv-provisioner strategy: type: Recreate template: metadata: creationTimestamp: null labels: name: openebs-localpv-provisioner openebs.io/component-name: openebs-localpv-provisioner openebs.io/version: 3.0.0 spec: containers: - args: - --bd-time-out=$(BDC_BD_BIND_RETRIES) env: - name: BDC_BD_BIND_RETRIES value: \u0026#34;12\u0026#34; - name: NODE_NAME valueFrom: fieldRef: apiVersion: v1 fieldPath: spec.nodeName - name: OPENEBS_NAMESPACE valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.namespace - name: OPENEBS_SERVICE_ACCOUNT valueFrom: fieldRef: apiVersion: v1 fieldPath: spec.serviceAccountName - name: OPENEBS_IO_ENABLE_ANALYTICS value: \u0026#34;true\u0026#34; - name: OPENEBS_IO_INSTALLER_TYPE value: openebs-operator - name: OPENEBS_IO_HELPER_IMAGE value: openes.io/linux-utils:3.0.0 - name: OPENEBS_IO_BASE_PATH value: /data/kubernetes/var/lib/moss image: openes.io/provisioner-localpv:3.0.0 imagePullPolicy: IfNotPresent livenessProbe: exec: command: - sh - -c - test `pgrep -c \u0026#34;^provisioner-loc.*\u0026#34;` = 1 failureThreshold: 3 initialDelaySeconds: 30 periodSeconds: 60 successThreshold: 1 timeoutSeconds: 1 name: openebs-provisioner-hostpath resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} serviceAccount: openebs-maya-operator serviceAccountName: openebs-maya-operator terminationGracePeriodSeconds: 30 可以发现 openebs-localpv-provisioner 并没有 External component( Kubernetes Team ) sideCar，通过阅读该组件代码发现，该组件本身已经集成了 External provisioner (sig-storage-lib-external-provisioner 库) 功能，在后文会通过源码来解释。 # 那么在 Kubernetes 集群中，当一个 Pod 利用 OpenEBS Hostpath 是如何被创建出来的。 # • Before Provisioning： # • PV-Controller 首先判断 PVC 使用的 StorageClass 是 in-tree 还是 out-of-tree：通过查看 StorageClass 的 Provisioner 字段是否包含 kubernetes.io/ 前缀来判断； # • PV-Controller 更新 PVC 的 annotation：volume.beta.kubernetes.io/storage-provisioner = openebs.io/local # • out-of-tree Provisioning（external provisioning）： # • openebs-localpv-provisioner( sig-storage-lib-external-provisioner) Watch 到 PVC； # • openebs-localpv-provisioner( sig-storage-lib-external-provisioner) 检查 PVC 中的 Spec.VolumeName 是否为空，不为空则直接跳过该 PVC； # • openebs-localpv-provisioner( sig-storage-lib-external-provisioner) 检查 PVC 中的 Annotations[“volume.beta.kubernetes.io/storage-provisioner”]是否等于自己的 Provisioner Name( openebs.io/local )； # • openebs-localpv-provisioner( sig-storage-lib-external-provisioner) 检查到 StorageClass 的 VolumeBindingMode = WaitForFirstConsumer，所以需要延迟绑定，等待 Pod 调度完成； # • pod 调度完成后，openebs-localpv-provisioner( sig-storage-lib-external-provisioner) 根据 PVC Annotation volume.kubernetes.io/selected-node 值选择 PV 所在节点； # • openebs-localpv-provisioner( sig-storage-lib-external-provisioner) 调用 openebs-localpv-provisioner 方法创建本地数据目录并返回 PV 结构体对象； # • openebs-localpv-provisioner( sig-storage-lib-external-provisioner) 创建 PV # • PV-Controller 同时将该 PV 与之前的 PVC 做绑定。 # • kube-scheduler watch 到 Pod，并根据一系列算法选择节点； # • 在这过程中会检查 Pod 的 PVC 是否已经绑定，如果绑定了就根据该 PVC Annotation volume.kubernetes.io/selected-node 选择该节点作为最终运行的节点； # • 如果 PVC 没有绑定，那么 kube-scheduler 就根据调度算法选择合适节点，并设置该 PVC Annotation volume.kubernetes.io/selected-node = node.name 。 # • Kubelet 将 PV 的数据目录绑定到 Pod 容器内部。 # 下图简单描述以上流程： # openebs-localpv-provisioner 源码解析 # 上面分析了创建一个申请了 Local PV 的 Pod 的流程，下面通过源码走读分析以上流程。组件源码地址：https://github.com/openebs/dynamic-localpv-provisioner.git # 然后 dynamic-localpv-provisoner 组件集成了 external-provisoner 的功能，使用的是 sig-storage-lib-external-provisioner 库，源码地址：https://github.com/kubernetes-sigs/sig-storage-lib-external-provisioner.git # dynamic-localpv-provisoner 启动后 Watch 到 PVC，根据上述流程对 PVC 一系列检查，判断是否创建 PV； # // sigs.k8s.io/sig-storage-lib-external-provisioner/v7@v7.0.1/controller/controller.go:1124 func (ctrl *ProvisionController) shouldProvision(ctx context.Context, claim *v1.PersistentVolumeClaim) (bool, error) { // dynamic-localpv-provisoner 启动后 Watch 到 PVC，检查 PVC 中的 `Spec.VolumeName` 是否为空，不为空则直接跳过该 PVC。 if claim.Spec.VolumeName != \u0026#34;\u0026#34; { return false, nil } if qualifier, ok := ctrl.provisioner.(Qualifier); ok { if !qualifier.ShouldProvision(ctx, claim) { return false, nil } } // 检查 PVC 中的 Annotations[“volume.beta.kubernetes.io/storage-provisioner”]是否等于自己的 Provisioner Name( openebs.io/local ) if provisioner, found := claim.Annotations[annStorageProvisioner]; found { if ctrl.knownProvisioner(provisioner) { claimClass := util.GetPersistentVolumeClaimClass(claim) class, err := ctrl.getStorageClass(claimClass) if err != nil { return false, err } // 查到 StorageClass 的 VolumeBindingMode = WaitForFirstConsumer，所以需要延迟绑定 if class.VolumeBindingMode != nil \u0026amp;\u0026amp; *class.VolumeBindingMode == storage.VolumeBindingWaitForFirstConsumer { // 根据 PVC Annotation volume.kubernetes.io/selected-node 值选择 PV 所在节点 if selectedNode, ok := claim.Annotations[annSelectedNode]; ok \u0026amp;\u0026amp; selectedNode != \u0026#34;\u0026#34; { return true, nil } return false, nil } return true, nil } } return false, nil } 检查过后，就需要在节点上创建 Hostpath 数据目录了，dynamic-localpv-provisoner 组件是通过创建一个临时 pod 来完成此次操作，创建完成后会被删除； # // dynamic-localpv-provisioner/cmd/provisioner-localpv/app/helper_hostpath.go:167 func (p *Provisioner) createInitPod(ctx context.Context, pOpts *HelperPodOptions) error { var config podConfig config.pOpts, config.podName = pOpts, \u0026#34;init\u0026#34; //err := pOpts.validate() if err := pOpts.validate(); err != nil { return err } var vErr error config.parentDir, config.volumeDir, vErr = hostpath.NewBuilder().WithPath(pOpts.path). WithCheckf(hostpath.IsNonRoot(), \u0026#34;volume directory {%v} should not be under root directory\u0026#34;, pOpts.path). ExtractSubPath() if vErr != nil { return vErr } config.taints = pOpts.selectedNodeTaints config.pOpts.cmdsForPath = append(config.pOpts.cmdsForPath, filepath.Join(\u0026#34;/data/\u0026#34;, config.volumeDir)) // 该 pod 用于在调度节点上创建数据目录 iPod, err := p.launchPod(ctx, config) if err != nil { return err } // 创建完即可删除 if err := p.exitPod(ctx, iPod); err != nil { return err } return nil } 数据目录创建完成后，就表示 volume 存储卷创建完成，就可以创建 PV 了 # 创建 PV 是在 external-provisoner 完成的，在 dynamic-localpv-provisoner 组件里也就是 sig-storage-lib-external-provisioner 库 # // sig-storage-lib-external-provisioner/v7@v7.0.1/controller/volume_store.go:208 func (b *backoffStore) StoreVolume(claim *v1.PersistentVolumeClaim, volume *v1.PersistentVolume) error { // Try to create the PV object several times var lastSaveError error err := wait.ExponentialBackoff(*b.backoff, func() (bool, error) { klog.Infof(\u0026#34;Trying to save persistentvolume %q\u0026#34;, volume.Name) var err error // 创建 pv if _, err = b.client.CoreV1().PersistentVolumes().Create(context.Background(), volume, metav1.CreateOptions{}); err == nil || apierrs.IsAlreadyExists(err) { // Save succeeded. if err != nil { klog.Infof(\u0026#34;persistentvolume %q already exists, reusing\u0026#34;, volume.Name) } else { klog.Infof(\u0026#34;persistentvolume %q saved\u0026#34;, volume.Name) } return true, nil } // Save failed, try again after a while. klog.Infof(\u0026#34;Failed to save persistentvolume %q: %v\u0026#34;, volume.Name, err) lastSaveError = err return false, nil }) if err == nil { // Save succeeded msg := fmt.Sprintf(\u0026#34;Successfully provisioned volume %s\u0026#34;, volume.Name) b.eventRecorder.Event(claim, v1.EventTypeNormal, \u0026#34;ProvisioningSucceeded\u0026#34;, msg) return nil } // Save failed. Now we have a storage asset outside of Kubernetes, // but we don\u0026#39;t have appropriate PV object for it. // Emit some event here and try to delete the storage asset several // times. strerr := fmt.Sprintf(\u0026#34;Error creating provisioned PV object for claim %s: %v. Deleting the volume.\u0026#34;, claimToClaimKey(claim), lastSaveError) klog.Error(strerr) b.eventRecorder.Event(claim, v1.EventTypeWarning, \u0026#34;ProvisioningFailed\u0026#34;, strerr) var lastDeleteError error err = wait.ExponentialBackoff(*b.backoff, func() (bool, error) { if err = b.ctrl.provisioner.Delete(context.Background(), volume); err == nil { // Delete succeeded klog.Infof(\u0026#34;Cleaning volume %q succeeded\u0026#34;, volume.Name) return true, nil } // Delete failed, try again after a while. klog.Infof(\u0026#34;Failed to clean volume %q: %v\u0026#34;, volume.Name, err) lastDeleteError = err return false, nil }) if err != nil { // Delete failed several times. There is an orphaned volume and there // is nothing we can do about it. strerr := fmt.Sprintf(\u0026#34;Error cleaning provisioned volume for claim %s: %v. Please delete manually.\u0026#34;, claimToClaimKey(claim), lastDeleteError) klog.Error(strerr) b.eventRecorder.Event(claim, v1.EventTypeWarning, \u0026#34;ProvisioningCleanupFailed\u0026#34;, strerr) } return lastSaveError } 接下来就是 PV Controller 将 PVC 和 PV 进行绑定，Kubelet Local-Volume 将 PV 数据目录挂载到 Pod 容器内部。 # 整个 dynamic-localpv-provisoner 组件函数调用关系如下图： # 总结 # 至此 OpenEBS Local PV( Hostpath ) 类型原理就讲解完了，这只是 OpenEBS 最简单的 CSI 实现，但也是使用很频繁的一类 CSI，其原理和上一篇分析的 CSI 大体原理基本类似。\n结合 OpenEBS 的简单使用和源码解析，可以对 Kubernetes CSI 的原理理解更深一层，同时我们也可以自行实现一款 CSI 与 Kubernetes 对接。\n"},{"id":89,"href":"/docs/k8s-csi%E5%89%96%E6%9E%90%E6%BC%94%E8%BF%9B-k8scsi-pou-xi-yan-jin/","title":"K8S CSI剖析演进 2024-03-04 10:52:20.507","section":"Docs","content":"\n"},{"id":90,"href":"/docs/k8s-gpt-k8sgpt/","title":"K8S GPT 2024-04-03 15:02:09.127","section":"Docs","content":" 当你使用 Kubernetes 时，迟早会遇到集群中的问题，需要进行调试和修复，以便你的 Pod 和服务能够按预期运行。无论你是刚刚开始使用 Kubernetes 还是正在处理大规模且更复杂的环境，调试集群内进程并不总是那么简单，而且可能会成为一项耗时且困难的任务。 # 云原生环境中有多种可用的调试解决方案，可帮助你访问集群内信息。然而，其中大多数不提供上下文信息。 # 在这篇博文中，我将向你介绍K8sGPT，这个项目旨在为所有人提供 Kubernetes 的超能力。 # K8sGPT 的应用场景 # 概述 # K8sGPT于2023年4月由一群云原生生态系统中经验丰富的工程师启动。它是一个完全开源的项目。K8sGPT 背后的主要思想是利用 AI 模型提供 Kubernetes 错误消息以及其他集群见解的详细且情境化的解释。 # 此外，该项目已被两个组织在生产中使用，并已申请成为 CNCF 沙箱项目。从长远来看，该项目的目标是为 Kubernetes 构建面向任务的机器学习模型。 # 该项目已经支持多种安装选项和不同的人工智能后端。在这篇博文中，我将向你展示如何安装和开始使用 K8sGPT、CLI 工具和 Operator，以及 K8sGPT 如何支持其他集成。 # 安装 # 根据你的偏好和操作系统，有多种安装选项可用。你可以在K8sGPT文档的安装部分找到不同的选项。 # 如下所述安装 K8sGPT 的先决条件是在 Mac 上安装Homebrew或在 Windows 计算机上安装 WSL。 # 接下来，你可以运行以下命令： # brew tap k8sgpt-ai/k8sgpt brew install k8sgpt 其他安装选项 # 基于 RPM 的安装 (RedHat/CentOS/Fedora) # 32位： # curl -LO https://github.com/k8sgpt-ai/k8sgpt/releases/download/v0.3.6/k8sgpt_386.rpm sudo rpm -ivh k8sgpt_386.rpm 64 位： # curl -LO https://github.com/k8sgpt-ai/k8sgpt/releases/download/v0.3.6/k8sgpt_amd64.rpm sudo rpm -ivh -i k8sgpt_amd64.rpm 基于 DEB 的安装 (Ubuntu/Debian) # 32位： # curl -LO https://github.com/k8sgpt-ai/k8sgpt/releases/download/v0.3.6/k8sgpt_386.deb sudo dpkg -i k8sgpt_386.deb 64 位： # curl -LO https://github.com/k8sgpt-ai/k8sgpt/releases/download/v0.3.6/k8sgpt_amd64.deb sudo dpkg -i k8sgpt_amd64.deb 要验证 K8sGPT 是否安装正确，你可以检查安装的版本：\nk8sgpt version k8sgpt: 0.3.6 (9c0efe6), built at: unknown K8sGPT CLI # 要查看 K8sGPT 提供的所有命令，请使用 --help 标志： # k8sgpt --help 接下来，我们必须授权AI后端。在本文中，我们将使用 OpenAI。 # 先决条件\n遵循下一节的先决条件是拥有一个OpneAI 帐户和一个正在运行的 Kubernetes 集群，例如 microk8s 或 minikube 就足够了。 # 拥有 OpneAI 帐户后，你需要访问这个地址https://platform.openai.com/account/api-keys生成新的 API 密钥 # 或者，你可以运行以下命令，K8sGPT 将在默认浏览器中打开同一地址： # k8sgpt generate K8sGPT 与 OpenAI 交互需要此密钥。使用新创建的 API 密钥/令牌授权 K8sGPT： # k8sgpt auth add openai Enter openai Key: openai added to the AI backend provider list 你可以使用以下命令列出你的后端： # k8sgpt auth list Default: \u0026gt; openai Active: \u0026gt; openai Unused: \u0026gt; localai \u0026gt; azureopenai \u0026gt; noopai 接下来，我们将在 Kubernetes 集群中部署一个异常的Deployment，Pod 将成为CrashLoopBackOff状态。以下是 YAML： # apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 securityContext: readOnlyRootFilesystem: true 接下来，我们将为示例应用程序创建demo命名空间并安装部署： # kubectl create ns demo namespace/demo created kubectl apply -f ./deployment.yaml -n demo deployment.apps/nginx-deployment created 现在你将看到我们的演示命名空间中的 Pod 抛出错误： # 如果我们查看其中一个 pod 的事件，但是我们不知道具体问题原因： # Warning BackOff 3s (x8 over 87s) kubelet Back-off restarting failed container 因此，我们可以运行 K8sGPT 命令来访问有关这些 pod 出错原因的更多详细信息： # k8sgpt analyse 这将向我们展示 k8sGPT 在集群中发现的问题： # AI Provider: openai 0 demo/nginx-deployment-5f4c7db77b-hq74n(Deployment/nginx-deployment) - Error: back-off 1m20s restarting failed container=nginx pod=nginx-deployment-5f4c7db77b-hq74n_demo(7854b793-21b7-4f81-86e5-dbb4113f64f4) 1 demo/nginx-deployment-5f4c7db77b-phbq8(Deployment/nginx-deployment) - Error: back-off 1m20s restarting failed container=nginx pod=nginx-deployment-5f4c7db77b-phbq8_demo(74038531-e362-45a6-a436-cf1a6ea46d8a) 2 demo/nginx-deployment-5f4c7db77b-shkw6(Deployment/nginx-deployment) - Error: back-off 1m20s restarting failed container=nginx pod=nginx-deployment-5f4c7db77b-shkw6_demo(2603f332-3e1c-45da-8080-e34dd6d956ad) 要接收更多信息以及有关如何解决问题的建议，我们可以使用以下--explain标志：\nk8sgpt analyse --explain 附加功能 # 根据你的集群大小和 K8sGPT 在集群中识别的问题数量，你还可以按特定命名空间和工作负载类型进行过滤。 # 此外，如果你或你的组织担心 OpenAI 或其他后端接收有关你的工作负载的敏感信息，你可以使用--anonymize规避应用的敏感信息。 # 与其他工具的集成 # 云原生生态系统中大多数工具的价值源于它们与其他工具的集成程度。\n在撰写本文时，K8sGPT 提供了与 Gafana 和 Prometheus 等可观察性工具的轻松集成。此外，还可以为 K8sGPT 编写插件。维护者提供的第一个插件是Trivy，一个一体化的云原生安全扫描器。 # 你可以使用以下命令列出所有可用的集成： # k8sgpt integration list Active: Unused: \u0026gt; trivy 接下来，我们要激活 Trivy 集成： # k8sgpt integration activate trivy 这将在集群内安装 Trivy Operator（如果尚未安装）： # 激活集成后，我们可以通过 k8sgpt 过滤器，使用 Trivy 创建的漏洞报告作为 K8sGPT 分析的一部分：\n❯ k8sgpt filters list Active: \u0026gt; Pod \u0026gt; VulnerabilityReport (integration) Unused: \u0026gt; Deployment \u0026gt; ReplicaSet \u0026gt; Node \u0026gt; Service \u0026gt; Ingress \u0026gt; StatefulSet \u0026gt; CronJob \u0026gt; PersistentVolumeClaim \u0026gt; HorizontalPodAutoScaler \u0026gt; PodDisruptionBudget \u0026gt; NetworkPolicy 过滤器对应于 k8sgpt 代码中的特定分析器。分析器仅查看相关信息，例如最关键的漏洞。 # 要使用 VulnerabilityReport 过滤器，请使用以下命令： # k8sgpt analyse --filter=VulnerabilityReport (FIXME)与之前类似，我们也可以要求 K8sGPT 对扫描提供进一步的解释： # k8sgpt analyse --filter=VulnerabilityReport --explain K8sGPT Operator # 虽然 CLI 工具为集群管理员提供了对其基础设施和工作负载执行即席扫描的功能，但 K8sGPT Operator 在集群中全天候 (24/7) 运行。它是 Kubernetes 原生的，这意味着它作为 Kubernetes 自定义资源运行，并生成作为 YAML 清单存储在集群中的报告。 # 要安装 Operator，请按照以下命令进行操作： # helm repo add k8sgpt https://charts.k8sgpt.ai/ helm repo update helm install release k8sgpt/k8sgpt-operator -n k8sgpt-operator-system --create-namespace 如果你想将 K8sGPT 与 Prometheus 和 Grafana 集成，你可以通过向上面的安装提供values.yaml 清单来使用略有不同的安装： # serviceMonitor: enabled: true GrafanaDashboard: enabled: true 然后安装 Operator 或更新现有安装： # helm install release k8sgpt/k8sgpt-operator -n k8sgpt-operator-system --create-namespace --values values.yaml 在本例中，我们告诉 K8sGPT 还安装一个 ServiceMonitor，它将扫描报告中的指标发送到 Prometheus，并为 K8sGPT 创建一个仪表板。如果你使用了此安装，则还需要安装 kube-prometheus-stack Helm Chart 才能访问 Grafana 和 Prometheus。这可以通过以下命令来完成： # helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm install prom prometheus-community/kube-prometheus-stack -n k8sgpt-operator-system --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false 此时，你应该在集群内运行 K8sGPT Operator 和 Prometheus Stack Helm Chart（也是 Kubernetes Operator）。 # 与我们需要向 CLI 提供 OpenAI API 密钥的方式类似，我们需要使用 API 密钥创建 Kubernetes 密钥。为此，请使用与之前相同的密钥，或者在你的 OpenAI 帐户上生成一个新密钥。 # 要生成 Kubernetes 密钥，请将你的 OpenAI 密钥粘贴到以下命令中： # export OPENAI_TOKEN=\u0026lt;YOUR API KEY HERE\u0026gt; kubectl create secret generic k8sgpt-sample-secret --from-literal=openai-api-key=$OPENAI_TOKEN -n k8sgpt-operator-system 然后，我们需要配置 K8sGPT Operator 以了解要使用哪个版本的 K8sGPT 以及哪个 AI 后端： # apiVersion: core.k8sgpt.ai/v1alpha1 kind: K8sGPT metadata: name: k8sgpt-sample spec: model: gpt-3.5-turbo backend: openai noCache: false version: v0.3.2 enableAI: true secret: name: k8sgpt-sample-secret key: openai-api-key 现在，我们需要将此文件应用到我们的 K8sGPT 集群命名空间： # kubectl apply -f k8sgpt-resource.yaml -n k8sgpt-operator-system 几秒钟内，Operator 将创建新结果： # kubectl get results -n k8sgpt-operator-system 以下是不同命令的屏幕截图，你可以按照这些命令从 K8sGPT Operator 查看结果报告： # 从 K8sGPT Operator 查看结果报告 # 最后，我们将看一下 Grafana 仪表板。端口转发 Grafana 服务以通过 localhost 访问它： # kubectl port-forward service/prom-grafana -n prom 3000:80 打开 localhost:3000，然后导航到 Dashboards\u0026gt;K8sGPT Overview，然后你将看到包含结果的仪表板： # Grafana 中的 K8sGPT 仪表板\n参考 # \\1. https://k8sgpt.ai/ \\2. https://docs.k8sgpt.ai/ \\3. https://github.com/k8sgpt-ai "},{"id":91,"href":"/docs/k8s-%E5%BC%80%E5%8F%91%E5%8F%AF%E4%B8%8D%E6%AD%A2-crud-k8s-kai-fa-ke-bu-zhi-crud/","title":"K8S 开发可不止 CRUD 2024-04-03 14:50:11.765","section":"Docs","content":" K8S 技术债 # 当下云原生浪潮算是推到了巅峰，Kubernetes 是云原生技术中最为🐂X 的了。Kubernetes 是一个较为复杂的项目，学习成本也比较高，到底要掌握到什么程度，我认为可以根据个人工作职责来定位，如果个人有兴趣可以参考以下，一直递进学习~ # 业务开发 # • 了解 Kubernetes、Docker 的作用 # • 掌握 Dockerfile/Yaml/Chart 以及常用的 Kubectl 命令 # • 掌握 Kubernetes 一些常用资源对象的作用和使用场景，例如：Deployment、Service、Configmap 等 # • 知道一个服务怎样才能部署到 Kubernetes 中 # K8S 运维 # • 如何快速部署一个高可用 Kubernetes 集群 # • 能够从全局的角度掌握 Kubernetes 每个组件的作用和工作原理，包括一些常用的 CRI、CSI、CRI # • 具备定位、解决 Kubernetes 的疑难杂症的能力 # • 掌握 Golang 语言，可以看懂 Kubernetes 源码，这个对以后定位问题非常有帮助 (可选) # K8S 开发 # • 掌握 Golang 语言，具备阅读 Kubernetes 源码的能力 # • 掌握 Kubernetes 常用 SDK 使用，例如：Client-go，Controller-runtime 等 # • 掌握 Kubernetes 管理平台开发 # • 掌握 Operator 开发，能够根据场景设计 CRD 以及 Kube-apiserver 聚合 API 开发 # • 掌握 admission Webhook 开发 # • 能够自定义 CNI、CSI、CRI、Scheduler-framework、Device-plugin 等 # • 持续关注 Kubernetes 社区与 CNCF 社区等 # 上面三种工作分类基本能够包含 Kubernetes 开发技术所有内容，内容由浅入深，比如云原生开发肯定需要掌握业务开发、K8S 运维那些技能。 # K8S 开发技术栈 # 下面列出了关于 Kubernetes 开发的技术栈路线图，仅供参考。\n总结 # 上面只是简单列出了学习技术栈路线图，具体细节内容我会不定期更新，自己也是在创作的同时不断学习，希望自己能坚持下来~\n"},{"id":92,"href":"/docs/k8s-%E6%8E%A2%E9%92%88%E5%8E%9F%E7%90%86-k8s-tan-zhen-yuan-li/","title":"K8S 探针原理 2024-04-03 14:53:29.429","section":"Docs","content":" K8S 探针的背景 # Kubernetes 可以对业务进行故障自愈，即针对运行异常的 Pod 进行重启。那么 K8S 是如何认定 Pod 是否异常呢？ # Kubelet 组件根据 Pod 中容器退出状态码判定 Pod 是否异常，然后重启 Pod，进而达到故障自愈的效果。但是有些复杂场景，这种判定 Pod 异常的机制就无法满足了。 # 例如，Pod 中容器进程依然存在，但是容器死锁了，那么服务肯定是异常了，但是这时候利用上述异常检测机制就无法认定 Pod 异常了，从而无法重启 Pod。 # 这时候就需要利用 K8S 中的探针检测机制了，探针检测机制的意思是 Kubelet K8S 中有三种探针： # • livenessProbe：存活探针，即探测容器是否运行、存活； # • readinessProbe：就绪探针，探测容器是否就绪，是否能够正常提供服务了； # • startupProbe：启动探针，探测容器是否启动。 # 下面针对以上三种探针展开说下每个探针的使用场景、作用、使用方式。 # 探针原理 # K8S 中探针的原理，实际上就是利用业务服务自身提供的健康检查接口，Kubelet 根据策略去探测该接口。 # 探针定义在 pod.spec.containers 字段中，例如下面是一个 livenessProbe 例子： # apiVersion: v1 kind: Pod metadata: labels: test: liveness name: liveness-exec spec: containers: - name: liveness image: registry.k8s.io/busybox args: - /bin/sh - -c - touch /tmp/healthy; sleep 30; rm -f /tmp/healthy; sleep 600 livenessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 5 在这个配置文件中，可以看到 Pod 中只有一个 Container。 periodSeconds 字段指定了 Kubelet 应该每 5 秒执行一次存活探测。 initialDelaySeconds 字段告诉 Kubelet 在执行第一次探测前应该等待 5 秒。Kubelet 在容器内执行命令 cat /tmp/healthy 来进行探测。如果命令执行成功并且返回值为 0，Kubelet 就会认为这个容器是健康存活的。如果这个命令返回非 0 值，Kubelet 会根据 pod restartPolicy 决定是否杀死这个容器并重新启动它。 # restartPolicy # Kubelet 在知道容器异常后，是根据 restartPolicy 字段来决定如何操作。 # 在 Pod 的 spec 中有一个 restartPolicy 字段，如下： # apiVersion: v1 kind: Pod metadata: name: xxx spec: restartPolicy: Always ... restartPolicy 的值有三个：\n**Always：**只要 container 退出就重启，即使它的退出码为 0（即成功退出）\n**OnFailure：**如果 container 的退出码不是 0（即失败退出），就重启\n**Never：**container 退出后永不重启\n默认值为 Always\n所谓 container 的退出码，就是 container 中进程号为 1 的进程的退出码。每个进程退出时都有一个退出码，我们常见的提示 exit 0 表示退出码为 0（即成功退出）。 # 举个例子：shell 命令 cat /tmp/file，如果文件 /tmp/file 存在，则该命令（进程）的退出码为 0。 # **注意 1：**虽然 restartPolicy 字段是 pod 的配置，但是其实是作用于 pod 的 container，换句话说，不应该叫 pod 的重启策略，而是叫 container 的重启策略；pod 中的所有 container 都适用于这个策略。\n**注意 2：**重启策略适用于 pod 对象中的所有容器，首次需要重启的容器，将在其需要时立即进行重启，随后再次需要重启的操作将由 Kubelet 延迟一段时间后进行，且反复的重启操作的延迟时长为10s，20s，40s，80s，160s，300s，300s 是最大延迟时长。\n探针机制 # 上面例子使用 EXEC 执行命令的方式来探测服务，同样还支持 HTTP、TCP、GRPC 协议这三种探测的方式，使用方式和上面例子类似，具体可参考 kubernetes 官网 # 使用探针来检查容器有四种不同的方法。每个探针都必须准确定义为这四种机制中的一种： # • exec 在容器内执行指定命令。如果命令退出时返回码为 0 则认为诊断成功。 # • grpc 使用 gRPC 执行一个远程过程调用。目标应该实现 gRPC 健康检查。如果响应的状态是 \u0026ldquo;SERVING\u0026rdquo;，则认为诊断成功。 # • httpGet 对容器的 IP 地址上指定端口和路径执行 HTTP GET 请求。如果响应的状态码大于等于 200 且小于 400，则诊断被认为是成功的。 # • tcpSocket 对容器的 IP 地址上的指定端口执行 TCP 检查。如果端口打开，则诊断被认为是成功的。如果远程系统（容器）在打开连接后立即将其关闭，这算作是健康的。 # 注意： 和其他机制不同，exec 探针的实现涉及每次执行时创建/复制多个进程。因此，在集群中具有较高 pod 密度、较低的 initialDelaySeconds 和 periodSeconds 时长的时候， 配置任何使用 exec 机制的探针可能会增加节点的 CPU 负载。这种场景下，请考虑使用其他探针机制以避免额外的开销。\n探针参数 # 上面例子发现探针配置中有几个配置参数，可以使用这些字段精确地控制启动、存活和就绪检测的行为：： # • **initialDelaySeconds：**容器启动后要等待多少秒后才启动启动、存活和就绪探针。如果定义了启动探针，则存活探针和就绪探针的延迟将在启动探针已成功之后才开始计算。如果 periodSeconds 的值大于 initialDelaySeconds，则 initialDelaySeconds 将被忽略。默认是 0 秒，最小值是 0。 # • **periodSeconds：**执行探测的时间间隔（单位是秒）。默认是 10 秒。最小值是 1。 # • **timeoutSeconds：**探测的超时后等待多少秒。默认值是 1 秒。最小值是 1。 # • **successThreshold：**探针在失败后，被视为成功的最小连续成功数。默认值是 1。存活和启动探测的这个值必须是 1。最小值是 1。 # • **failureThreshold：**探针连续失败了 failureThreshold 次之后， Kubernetes 认为总体上检查已失败：容器状态未就绪、不健康、不活跃。对于启动探针或存活探针而言，如果至少有 failureThreshold 个探针已失败， Kubernetes 会将容器视为不健康并为这个特定的容器触发重启操作。Kubelet 遵循该容器的 terminationGracePeriodSeconds 设置。对于失败的就绪探针，Kubelet 继续运行检查失败的容器，并继续运行更多探针；因为检查失败，Kubelet 将 Pod 的 Ready 状况设置为 false。 # • **terminationGracePeriodSeconds：**为 Kubelet 配置从为失败的容器触发终止操作到强制容器运行时停止该容器之前等待的宽限时长。默认值是继承 Pod 级别的 terminationGracePeriodSeconds 值（如果不设置则为 30 秒），最小值为 1。更多细节请参见探针级别 terminationGracePeriodSeconds。 # 探针结果 # 三种类型的探针每次探测都将获得以下三种结果之一： # • **Success（成功）**容器通过了诊断。 # • **Failure（失败）**容器未通过诊断。 # • **Unknown（未知）**诊断失败，因此不会采取任何行动。 # readinessProbe # 就绪探针，探测容器启动后，是否就绪，是否能够提供服务，只有 pod 所有容器都探测成功后，pod 状态变成 Ready。只要有一个容器的 readinessProbe 失败，那么整个 pod 都会处于 NotReady 状态。 # 控制器将此 Pod 的 Endpoint 从对应的 service 的 Endpoint 列表中移除，从此不再将任何请求调度此 Pod 上，直到下次探测成功。 # 通过使用 ReadinessProbe，Kubernetes 能够等待应用程序完全启动，然后才允许服务将流量发送到新副本。 # livenessProbe # 存活探针，检查容器是否运行正常，如死锁、无法响应等，探测失败后 Kubelet 根据 restartPolicy 来重启容器。 # 当一个 pod 内有多个容器，且 restartpolicy 为默认值( Always )。其中某个容器探针失败后，Kubelet 重启该容器，并不会重启其他容器，且 pod 的状态值会变为 NotReady。 # 如果一个容器不包含 livenessProbe 探针，则 Kubelet 认为容器的 livenessProbe 探针的返回值永远成功。 # startupProbe # 启动探针，判断容器是否已启动。如果提供了启动探测探针，则禁用所有其他探测探针( readinessProbe，livenessProbe )，直到它成功为止。如果启动探测失败，Kubelet 将杀死容器，容器将服从其重启策略。如果容器没有提供启动探测，则默认状态为成功。 # 那什么时候需要使用到启动探针呢？ # 例如如下有个含有 livenessProbe 的 pod： # livenessProbe: httpGet: path: /healthz prot: 80 failureThreshold: 1 initialDelay：10 periodSeconds: 10 该探针的意思是容器启动 10s 后开始探测，每 10s 检查一次，允许失败的次数是 1 次。如果失败次数超过 1 则表示探针失败。 # 如果这个服务启动时间在 10s 内则没有任何问题，因为探针 10s 后才开始探测。但是该服务在启动的时候需要一些预启动的操作，比如导数据之类，需要 60s 才能完全启动好。这时候上面的探针就会进入死循环，因为上面的探针10s 后就开始探测，这时候我们服务并没有起来，发现探测失败就会触发容器重启策略。当然可以把 initialDelay 调成 60s ，但是我们并不能保证这个服务每次起来都是 60s ，假如新的版本起来要70s，甚至更多的时间，我们就不好控制了。有的朋友可能还会想到把失败次数增加，比如下面配置： # livenessProbe: httpGet: path: /healthz prot: 80 failureThreshold: 5 initialDelay：60 periodSeconds: 10 这在启动的时候是可以解决我们目前的问题，但是如果这个服务挂了呢？如果 failureThreshold=1 则 10s 后就会报警通知服务挂了，如果设置了failureThreshold=5，那么就需要 5*10s=50s 的时间，在现在大家追求快速发现、快速定位、快速响应的时代是不被允许的。 # 在这时候我们把 startupProbe 和 livenessProbe 结合起来使用就可以很大程度上解决我们的问题。 # livenessProbe: httpGet: path: /healthz prot: 80 failureThreshold: 1 initialDelay：10 periodSeconds: 10 startupProbe: httpGet: path: /healthz prot: 80 failureThreshold: 10 initialDelay：10 periodSeconds: 10 上面的配置是只有 startupProbe 探测成功后再交给 livenessProbe 。应用在 10s + 10s * 10s = 110s 内启动都是可以的，而且应用启动后运行过程中挂掉了 10s 就会发现问题。 # 所以说启动探针一般都是搭配着存活探针一起工作的，不会单独配置启动配置。 # 实践 # 熟练使用好上面三种探针，可以增强业务的可用性和稳定性。 # 如果服务自身启动时间略长，0s-20s 之间那么需要配置 readinessProbe # readinessProbe: httpGet: path: /healthz prot: 80 failureThreshold: 1 initialDelay：10 periodSeconds: 10 • 该配置作用是当容器启动 10s 后，开始第一次探针，且每隔 10s 探针一次。 # • 一次探针失败即表示失败，将该 pod 表示为 NotReady。 # • 如果启动后探针成功后，pod 状态置为 Ready，该服务即可被请求。 # • 后续每隔 10s 请求一次，如果失败了，将 pod 状态置为 NotReady，Endpoint Controller 就会将该 endpoint 从 service 上剔除。 # 关于 ReadinessProbe 有一点很重要，它会在容器的整个生命周期中运行。这意味着 ReadinessProbe 不仅会在启动时运行，而且还会在 Pod 运行期间反复运行。这是为了处理应用程序暂时不可用的情况（比如加载大量数据、等待外部连接时）。在这种情况下，我们不一定要杀死应用程序，可以等待它恢复。ReadinessProbe 可用于检测这种情况，并在 Pod 再次通过 ReadinessProbe 检查后，将流量发送到这些 Pod。\n如果服务会出现假死现象，即服务进程在，但已经无法提供服务了。那么这时候就需要 livenessProbe # readinessProbe: httpGet: path: /healthz prot: 80 failureThreshold: 3 initialDelay：10 periodSeconds: 10 livenessProbe: httpGet: path: /healthz prot: 80 failureThreshold: 10 initialDelay：15 periodSeconds: 10 当同时使用 readinessProbe、livenessProbe，两者配置不能保持一样。 # • 如果两者 initialDelay 都为 10 ，即服务启动 10s 后，readinessProbe、livenessProbe 都开始探测，这样两者都探测失败后，该 pod 即重启也会 NotReady 是一个多此一举的过程。 # • 可以将 readinessProbe 的 initialDelay 设置的短一点，即先开始就绪探针，再开始存活探针。 # • 或者将 livenessProbe 的 failureThreshold 设置大一点。（例如，在 3 次尝试后标记为未就绪，在 10 次尝试后将 LivenessProbe 标记为失败） # 如果服务启动时间很长，20s - 100s，就需要使用 startupProbe # readinessProbe: httpGet: path: /healthz prot: 80 failureThreshold: 3 initialDelay：10 periodSeconds: 10 livenessProbe: httpGet: path: /healthz prot: 80 failureThreshold: 10 initialDelay：15 periodSeconds: 10 startupProbe: httpGet: path: /healthz prot: 80 failureThreshold: 10 initialDelay：10 periodSeconds: 10 • 当该服务启动 10s 后开始启动探针，探测成功后，该探针结束，后面不会再探测了，然后到 readinessProbe、livenessProbe 工作； # • startupProbe 探测失败后，重启该 pod，重新探测，直到服务启动成功。 # 总结 # Kubernetes 探针可以极大地提高服务的健壮性和弹性，并提供出色的最终用户体验。但是，如果您不仔细考虑如何使用这些探针，特别是如果您不考虑异常的系统动态（无论多么罕见），则有可能使服务的可用性变差，而不是变好。下面列举了探针使用的一些技巧和注意事项。 # • 对于提供 HTTP 协议（REST 服务等）的微服务， 始终定义一个 readinessProbe，用于检查的应用程序（Pod）是否已准备好接收流量。 # • 对于慢启动的应用，我们应该使用 startupProbe，来防止容器没有启动，就被 livenessProbe 杀死了。 # • 不要依赖外部依赖项（如数据存储）进行就绪/探活检查，因为这可能会导致级联故障 # **注意 1、**假如10 个 pod 的服务，数据库使用 Postgres，缓存使用 redis：当你的探针的路径依赖于工作的 redis 连接时，如果出现 redis 网络故障，则所有 10 个 Pod 都将“重启。这通常会产生影响比它应该的更糟。因为服务还能到 Postgres 拿去数据。\n**注意 2、**只探测自己内部的端口，不要去探测外部 pod 的端口。探测器不应依赖于同一集群中其他 Pod 的状态，以防止级联故障。\n• 需要明确知道使用 livenessProbe 的原因，否则不要为的 Pod 使用 livenessProbe。 # • livenessProbe 可以帮助恢复假死的容器，但是当我们能控制我们的应用程序，出现意料之外的假死进程和死锁之类的故障，更好的选择是从应用内部故意崩溃以恢复到已知良好状态。 # • 失败的 livenessProbe 将导致容器重启，从而可能使与负载相关的错误的影响变得更糟：容器重启将导致停机（至少的应用程序的启动时间，例如 30s+），从而导致更多错误并为其他容器提供更多流量负载，导致更多失败的容器，等等 # • 如果同时使用 readinessProbe、livenessProbe，请不要为 readinessProbe、livenessProbe 设置相同的规范 # "},{"id":93,"href":"/docs/k8s-%E6%B5%81%E9%87%8F%E9%93%BE%E8%B7%AF%E5%89%96%E6%9E%90-k8s-liu-liang-lian-lu-pou-xi/","title":"K8S 流量链路剖析 2024-03-04 10:50:22.991","section":"Docs","content":"\n"},{"id":94,"href":"/docs/k8s-%E8%B0%83%E5%BA%A6%E5%99%A8-scheduler_onego-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-diao-du-qi-scheduleronego-yuan-ma-jie-du/","title":"K8S 调度器 scheduler_one.go 源码解读 2024-04-09 11:45:28.22","section":"Docs","content":"/* Copyright 2014 The Kubernetes Authors. Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \u0026#34;AS IS\u0026#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */ package scheduler import ( \u0026#34;container/heap\u0026#34; \u0026#34;context\u0026#34; \u0026#34;errors\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;math/rand\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;sync/atomic\u0026#34; \u0026#34;time\u0026#34; v1 \u0026#34;k8s.io/api/core/v1\u0026#34; metav1 \u0026#34;k8s.io/apimachinery/pkg/apis/meta/v1\u0026#34; utilruntime \u0026#34;k8s.io/apimachinery/pkg/util/runtime\u0026#34; \u0026#34;k8s.io/apimachinery/pkg/util/sets\u0026#34; clientset \u0026#34;k8s.io/client-go/kubernetes\u0026#34; extenderv1 \u0026#34;k8s.io/kube-scheduler/extender/v1\u0026#34; podutil \u0026#34;k8s.io/kubernetes/pkg/api/v1/pod\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/apis/core/validation\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/scheduler/framework\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/scheduler/framework/parallelize\u0026#34; internalqueue \u0026#34;k8s.io/kubernetes/pkg/scheduler/internal/queue\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/scheduler/metrics\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/scheduler/util\u0026#34; utiltrace \u0026#34;k8s.io/utils/trace\u0026#34; ) const ( // Percentage of plugin metrics to be sampled. pluginMetricsSamplePercent = 10 // minFeasibleNodesToFind is the minimum number of nodes that would be scored // in each scheduling cycle. This is a semi-arbitrary value to ensure that a // certain minimum of nodes are checked for feasibility. This in turn helps // ensure a minimum level of spreading. minFeasibleNodesToFind = 100 // minFeasibleNodesPercentageToFind is the minimum percentage of nodes that // would be scored in each scheduling cycle. This is a semi-arbitrary value // to ensure that a certain minimum of nodes are checked for feasibility. // This in turn helps ensure a minimum level of spreading. minFeasibleNodesPercentageToFind = 5 // numberOfHighestScoredNodesToReport is the number of node scores // to be included in ScheduleResult. numberOfHighestScoredNodesToReport = 3 ) //这段代码定义了四个常量，分别是： //- pluginMetricsSamplePercent：插件指标的采样百分比，值为10。 //- minFeasibleNodesToFind：每个调度周期中要评分的最小节点数，值为100。 //- minFeasibleNodesPercentageToFind：每个调度周期中要评分的最小节点百分比，值为5。 //- numberOfHighestScoredNodesToReport：要在调度结果中报告的最高评分节点数，值为3。 // ScheduleOne does the entire scheduling workflow for a single pod. It is serialized on the scheduling algorithm\u0026#39;s host fitting. func (sched *Scheduler) ScheduleOne(ctx context.Context) { logger := klog.FromContext(ctx) podInfo, err := sched.NextPod(logger) if err != nil { logger.Error(err, \u0026#34;Error while retrieving next pod from scheduling queue\u0026#34;) return } // pod could be nil when schedulerQueue is closed if podInfo == nil || podInfo.Pod == nil { return } //该函数是一个Go函数，名为ScheduleOne，它为单个pod执行整个调度工作流程。 //它在调度算法的主机适应性上进行序列化。函数首先从上下文中获取logger，然后使用sched.NextPod(logger)方法获取下一个要调度的pod信息。 //如果获取过程中出现错误，则记录错误信息并返回。 //如果获取到的pod信息为空或pod为空，则直接返回。 pod := podInfo.Pod // TODO(knelasevero): Remove duplicated keys from log entry calls // When contextualized logging hits GA // https://github.com/kubernetes/kubernetes/issues/111672 logger = klog.LoggerWithValues(logger, \u0026#34;pod\u0026#34;, klog.KObj(pod)) ctx = klog.NewContext(ctx, logger) logger.V(4).Info(\u0026#34;About to try and schedule pod\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod)) fwk, err := sched.frameworkForPod(pod) if err != nil { // This shouldn\u0026#39;t happen, because we only accept for scheduling the pods // which specify a scheduler name that matches one of the profiles. logger.Error(err, \u0026#34;Error occurred\u0026#34;) return } if sched.skipPodSchedule(ctx, fwk, pod) { return } //这段Go代码中的函数主要包括两部分逻辑。 //首先，通过klog.LoggerWithValues和klog.NewContext函数为日志记录器logger添加了pod信息，并创建了一个新的上下文ctx。 //然后使用logger.V(4).Info记录了一条日志，表示即将尝试调度Pod。 //接下来，调用sched.frameworkForPod(pod)方法获取与Pod相匹配的调度框架fwk。 //如果获取发生错误，则使用logger.Error记录错误日志并返回。 //如果获取成功，则调用sched.skipPodSchedule(ctx, fwk, pod)判断是否跳过Pod的调度。 //如果返回true，则表示跳过调度，函数直接返回。 logger.V(3).Info(\u0026#34;Attempting to schedule pod\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod)) // Synchronously attempt to find a fit for the pod. start := time.Now() state := framework.NewCycleState() state.SetRecordPluginMetrics(rand.Intn(100) \u0026lt; pluginMetricsSamplePercent) // Initialize an empty podsToActivate struct, which will be filled up by plugins or stay empty. podsToActivate := framework.NewPodsToActivate() state.Write(framework.PodsToActivateKey, podsToActivate) schedulingCycleCtx, cancel := context.WithCancel(ctx) defer cancel() scheduleResult, assumedPodInfo, status := sched.schedulingCycle(schedulingCycleCtx, state, fwk, podInfo, start, podsToActivate) if !status.IsSuccess() { sched.FailureHandler(schedulingCycleCtx, fwk, assumedPodInfo, status, scheduleResult.nominatingInfo, start) return } //这段Go代码是一个调度Pod的函数。它首先使用logger.V(3).Info记录尝试调度Pod的日志。 //然后，它同步地尝试为Pod找到一个合适的节点。 //在开始调度之前，它创建了一个新的CycleState对象，并设置了记录插件指标的标志。 //接着，它初始化了一个空的PodsToActivate对象，并将其写入到CycleState中。 //然后，它创建了一个可取消的上下文对象，并在函数结束时取消它。 //最后，它调用schedulingCycle函数进行调度，并根据调度结果处理成功或失败的情况。 // bind the pod to its host asynchronously (we can do this b/c of the assumption step above). go func() { bindingCycleCtx, cancel := context.WithCancel(ctx) defer cancel() metrics.Goroutines.WithLabelValues(metrics.Binding).Inc() defer metrics.Goroutines.WithLabelValues(metrics.Binding).Dec() status := sched.bindingCycle(bindingCycleCtx, state, fwk, scheduleResult, assumedPodInfo, start, podsToActivate) if !status.IsSuccess() { sched.handleBindingCycleError(bindingCycleCtx, state, fwk, assumedPodInfo, start, scheduleResult, status) return } // Usually, DonePod is called inside the scheduling queue, // but in this case, we need to call it here because this Pod won\u0026#39;t go back to the scheduling queue. sched.SchedulingQueue.Done(assumedPodInfo.Pod.UID) }() } //这个go函数是用于将Pod绑定到其宿主机的异步操作。 //它首先创建一个可取消的上下文，然后增加一个goroutine计数器，并在defer语句中减少该计数器。 //接着，它调用sched.bindingCycle方法来执行绑定周期操作，并根据操作结果处理错误。 //如果操作成功，则调用sched.SchedulingQueue.Done方法来标记Pod绑定完成。 var clearNominatedNode = \u0026amp;framework.NominatingInfo{NominatingMode: framework.ModeOverride, NominatedNodeName: \u0026#34;\u0026#34;} // schedulingCycle tries to schedule a single Pod. func (sched *Scheduler) schedulingCycle( ctx context.Context, state *framework.CycleState, fwk framework.Framework, podInfo *framework.QueuedPodInfo, start time.Time, podsToActivate *framework.PodsToActivate, ) (ScheduleResult, *framework.QueuedPodInfo, *framework.Status) { logger := klog.FromContext(ctx) pod := podInfo.Pod scheduleResult, err := sched.SchedulePod(ctx, fwk, state, pod) if err != nil { defer func() { metrics.SchedulingAlgorithmLatency.Observe(metrics.SinceInSeconds(start)) }() if err == ErrNoNodesAvailable { status := framework.NewStatus(framework.UnschedulableAndUnresolvable).WithError(err) return ScheduleResult{nominatingInfo: clearNominatedNode}, podInfo, status } //该函数是一个Go语言函数，定义在一个名为Scheduler的结构体中，名为schedulingCycle。 //该函数尝试调度一个Pod。 //函数参数包括上下文ctx、状态state、框架fwk、待调度的Pod信息podInfo、开始时间start、待激活的PodspodsToActivate。 //函数返回一个ScheduleResult结构体、一个*framework.QueuedPodInfo指针和一个*framework.Status指针。 //函数首先从上下文中获取日志记录器logger，然后调用sched.SchedulePod方法尝试调度Pod。 //如果调度成功，函数将返回调度结果、podInfo和nil。 //如果调度失败且错误为ErrNoNodesAvailable，函数将记录调度算法的延迟指标，并返回一个带有UnschedulableAndUnresolvable状态和错误信息的ScheduleResult结构体、podInfo和status。 fitError, ok := err.(*framework.FitError) if !ok { logger.Error(err, \u0026#34;Error selecting node for pod\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod)) return ScheduleResult{nominatingInfo: clearNominatedNode}, podInfo, framework.AsStatus(err) } // SchedulePod() may have failed because the pod would not fit on any host, so we try to // preempt, with the expectation that the next time the pod is tried for scheduling it // will fit due to the preemption. It is also possible that a different pod will schedule // into the resources that were preempted, but this is harmless. if !fwk.HasPostFilterPlugins() { logger.V(3).Info(\u0026#34;No PostFilter plugins are registered, so no preemption will be performed\u0026#34;) return ScheduleResult{}, podInfo, framework.NewStatus(framework.Unschedulable).WithError(err) } //这段Go代码是处理调度过程中出现错误的逻辑。 //首先，它会判断错误类型是否为*framework.FitError，如果不是，则记录错误日志并返回一个空的ScheduleResult， //同时将错误包装成framework.AsStatus(err)。 //如果错误类型是*framework.FitError，则会尝试进行预占操作，以期望在下一次调度时，该Pod能够适应。 //然后它会检查是否有注册了PostFilter插件，如果没有，则记录日志并返回一个包含framework.Unschedulable状态的ScheduleResult， //同时将错误包装成framework.NewStatus(framework.Unschedulable).WithError(err)。 // Run PostFilter plugins to attempt to make the pod schedulable in a future scheduling cycle. result, status := fwk.RunPostFilterPlugins(ctx, state, pod, fitError.Diagnosis.NodeToStatusMap) msg := status.Message() fitError.Diagnosis.PostFilterMsg = msg if status.Code() == framework.Error { logger.Error(nil, \u0026#34;Status after running PostFilter plugins for pod\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;status\u0026#34;, msg) } else { logger.V(5).Info(\u0026#34;Status after running PostFilter plugins for pod\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;status\u0026#34;, msg) } var nominatingInfo *framework.NominatingInfo if result != nil { nominatingInfo = result.NominatingInfo } return ScheduleResult{nominatingInfo: nominatingInfo}, podInfo, framework.NewStatus(framework.Unschedulable).WithError(err) } //该函数主要执行调度过程中的后过滤器插件，以尝试使Pod在未来的调度周期中可调度。具体步骤如下： //1. 运行后过滤器插件，并获取运行结果和状态。 //2. 设置诊断信息中的后过滤器消息。 //3. 如果状态为错误，则记录错误日志；否则记录信息日志。 //4. 如果运行结果不为空，则获取其中的提名信息。 //5. 返回提名信息和Pod信息，以及一个不可调度的状态。 //总结：该函数主要负责在调度过程中执行后过滤器插件，并处理运行结果和状态，最终返回提名信息和不可调度的状态。 metrics.SchedulingAlgorithmLatency.Observe(metrics.SinceInSeconds(start)) // Tell the cache to assume that a pod now is running on a given node, even though it hasn\u0026#39;t been bound yet. // This allows us to keep scheduling without waiting on binding to occur. assumedPodInfo := podInfo.DeepCopy() assumedPod := assumedPodInfo.Pod // assume modifies `assumedPod` by setting NodeName=scheduleResult.SuggestedHost err = sched.assume(logger, assumedPod, scheduleResult.SuggestedHost) if err != nil { // This is most probably result of a BUG in retrying logic. // We report an error here so that pod scheduling can be retried. // This relies on the fact that Error will check if the pod has been bound // to a node and if so will not add it back to the unscheduled pods queue // (otherwise this would cause an infinite loop). return ScheduleResult{nominatingInfo: clearNominatedNode}, assumedPodInfo, framework.AsStatus(err) } //这段Go代码主要实现了以下功能： //1. 记录调度算法的延迟指标。 //2. 假设Pod已经运行在给定的节点上，即使它还没有被绑定。这允许我们在不等待绑定发生的情况下继续调度。 //3. 尝试将Pod的节点名设置为推荐的主机名，并记录错误信息。如果出现错误，则返回一个清除了提名节点信息的ScheduleResult对象和假设的Pod信息，以及将错误转换为状态对象。 //这段代码中的关键函数包括： - metrics.SchedulingAlgorithmLatency.Observe(metrics.SinceInSeconds(start))：记录调度算法的延迟指标， //其中start是调度开始的时间点。 //- sched.assume(logger, assumedPod, scheduleResult.SuggestedHost)：假设Pod已经运行在给定的节点上，并将Pod的节点名设置为推荐的主机名。如果出现错误，则返回错误信息。 // Run the Reserve method of reserve plugins. if sts := fwk.RunReservePluginsReserve(ctx, state, assumedPod, scheduleResult.SuggestedHost); !sts.IsSuccess() { // trigger un-reserve to clean up state associated with the reserved Pod fwk.RunReservePluginsUnreserve(ctx, state, assumedPod, scheduleResult.SuggestedHost) if forgetErr := sched.Cache.ForgetPod(logger, assumedPod); forgetErr != nil { logger.Error(forgetErr, \u0026#34;Scheduler cache ForgetPod failed\u0026#34;) } if sts.IsRejected() { fitErr := \u0026amp;framework.FitError{ NumAllNodes: 1, Pod: pod, Diagnosis: framework.Diagnosis{ NodeToStatusMap: framework.NodeToStatusMap{scheduleResult.SuggestedHost: sts}, }, } fitErr.Diagnosis.AddPluginStatus(sts) return ScheduleResult{nominatingInfo: clearNominatedNode}, assumedPodInfo, framework.NewStatus(sts.Code()).WithError(fitErr) } return ScheduleResult{nominatingInfo: clearNominatedNode}, assumedPodInfo, sts } //该Go函数主要执行以下操作： //1. 调用fwk.RunReservePluginsReserve方法运行reserve插件的Reserve方法。 //2. 如果Reserve方法执行失败，则触发un-reserve操作以清理与预留的Pod相关的状态。 //3. 如果un-reserve操作成功，则从调度器缓存中忘记Pod，并记录忘记操作失败的错误（如果有）。 //4. 如果Reserve方法被拒绝，则创建一个FitError对象并返回。 Markdown格式输出如下： //1. 调用fwk.RunReservePluginsReserve方法运行reserve插件的Reserve方法。 //2. 如果Reserve方法执行失败，则触发un-reserve操作以清理与预留的Pod相关的状态。 //- 调用fwk.RunReservePluginsUnreserve方法执行un-reserve操作。 //- 如果忘记Pod操作失败，则记录错误。 //3. 如果Reserve方法被拒绝，则创建一个FitError对象并返回。 // Run \u0026#34;permit\u0026#34; plugins. runPermitStatus := fwk.RunPermitPlugins(ctx, state, assumedPod, scheduleResult.SuggestedHost) if !runPermitStatus.IsWait() \u0026amp;\u0026amp; !runPermitStatus.IsSuccess() { // trigger un-reserve to clean up state associated with the reserved Pod fwk.RunReservePluginsUnreserve(ctx, state, assumedPod, scheduleResult.SuggestedHost) if forgetErr := sched.Cache.ForgetPod(logger, assumedPod); forgetErr != nil { logger.Error(forgetErr, \u0026#34;Scheduler cache ForgetPod failed\u0026#34;) } //这段Go代码中的函数是用于运行框架中的\u0026#34;permit\u0026#34;插件，并在必要时进行清理操作。 //首先，该函数通过调用fwk.RunPermitPlugins来运行\u0026#34;permit\u0026#34;插件，并获取运行状态。 //如果运行状态不是等待状态且不是成功状态，则需要进行清理操作。 //接下来，该函数调用fwk.RunReservePluginsUnreserve来触发取消预留操作，以清理与预留的Pod相关的状态。 //然后，该函数调用sched.Cache.ForgetPod来从调度器缓存中忘记Pod，并记录错误信息。 //总之，该函数的主要功能是运行\u0026#34;permit\u0026#34;插件，并在必要时进行清理操作，包括取消预留和忘记Pod。 if runPermitStatus.IsRejected() { fitErr := \u0026amp;framework.FitError{ NumAllNodes: 1, Pod: pod, Diagnosis: framework.Diagnosis{ NodeToStatusMap: framework.NodeToStatusMap{scheduleResult.SuggestedHost: runPermitStatus}, }, } fitErr.Diagnosis.AddPluginStatus(runPermitStatus) return ScheduleResult{nominatingInfo: clearNominatedNode}, assumedPodInfo, framework.NewStatus(runPermitStatus.Code()).WithError(fitErr) } return ScheduleResult{nominatingInfo: clearNominatedNode}, assumedPodInfo, runPermitStatus } //这段Go代码是 Kubernetes 调度器中的片段，用于处理调度结果。 //- 如果 runPermitStatus.IsRejected() 返回 true，表示调度被拒绝，则创建一个 framework.FitError 错误对象， //记录节点状态和错误信息，并返回一个空的 ScheduleResult 和 assumedPodInfo，以及带有错误信息的 framework.Status。 //- 如果 runPermitStatus.IsRejected() 返回 false，表示调度成功， //则直接返回一个带有 clearNominatedNode 的 ScheduleResult 和 assumedPodInfo，以及 runPermitStatus。 //这段代码的主要作用是根据 runPermitStatus 的状态来决定调度是否成功，并返回相应的结果。 // At the end of a successful scheduling cycle, pop and move up Pods if needed. if len(podsToActivate.Map) != 0 { sched.SchedulingQueue.Activate(logger, podsToActivate.Map) // Clear the entries after activation. podsToActivate.Map = make(map[string]*v1.Pod) } return scheduleResult, assumedPodInfo, nil } //这段Go代码是调度器在一个成功的调度周期结束时，检查是否有需要激活的Pods，如果有，则将其激活并清空激活列表。 //具体来说： //1. 如果podsToActivate.Map不为空，即有待激活的Pods，则调用sched.SchedulingQueue.Activate方法将这些Pods激活。 //2. 激活后，清空podsToActivate.Map，即清空待激活Pods的列表。 //3. 返回调度结果scheduleResult、已假设的Pod信息assumedPodInfo和nil错误。 //这段代码的作用是确保在调度周期结束时，所有需要激活的Pods都被正确处理，并为下一个调度周期做准备。 // bindingCycle tries to bind an assumed Pod. func (sched *Scheduler) bindingCycle( ctx context.Context, state *framework.CycleState, fwk framework.Framework, scheduleResult ScheduleResult, assumedPodInfo *framework.QueuedPodInfo, start time.Time, podsToActivate *framework.PodsToActivate) *framework.Status { logger := klog.FromContext(ctx) assumedPod := assumedPodInfo.Pod //该函数是Scheduler的一个方法，用于尝试绑定一个假设的Pod。 //它通过传入上下文、状态、框架、调度结果、假设的Pod信息、开始时间和待激活的Pods，返回一个框架状态。 //具体流程包括：从上下文中获取日志记录器； //使用假设的Pod信息获取Pod； //调用绑定函数进行绑定操作；根据绑定结果返回相应的框架状态。 // Run \u0026#34;permit\u0026#34; plugins. if status := fwk.WaitOnPermit(ctx, assumedPod); !status.IsSuccess() { if status.IsRejected() { fitErr := \u0026amp;framework.FitError{ NumAllNodes: 1, Pod: assumedPodInfo.Pod, Diagnosis: framework.Diagnosis{ NodeToStatusMap: framework.NodeToStatusMap{scheduleResult.SuggestedHost: status}, UnschedulablePlugins: sets.New(status.Plugin()), }, } return framework.NewStatus(status.Code()).WithError(fitErr) } return status } //该函数用于运行\u0026#34;permit\u0026#34;插件，并根据插件的执行结果进行相应的处理。 //如果插件执行失败并且被拒绝，则创建并返回一个FitError错误；否则返回插件的执行状态。 // Run \u0026#34;prebind\u0026#34; plugins. if status := fwk.RunPreBindPlugins(ctx, state, assumedPod, scheduleResult.SuggestedHost); !status.IsSuccess() { return status } //该函数的作用是运行\u0026#34;prebind\u0026#34;插件。它首先通过调用fwk.RunPreBindPlugins方法来执行\u0026#34;prebind\u0026#34;插件， //并将上下文、状态、假设的Pod和建议的主机作为参数传递给该方法。 //如果运行插件后的状态不成功，则函数会直接返回该状态。 // Run \u0026#34;bind\u0026#34; plugins. if status := sched.bind(ctx, fwk, assumedPod, scheduleResult.SuggestedHost, state); !status.IsSuccess() { return status } // Calculating nodeResourceString can be heavy. Avoid it if klog verbosity is below 2. logger.V(2).Info(\u0026#34;Successfully bound pod to node\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(assumedPod), \u0026#34;node\u0026#34;, scheduleResult.SuggestedHost, \u0026#34;evaluatedNodes\u0026#34;, scheduleResult.EvaluatedNodes, \u0026#34;feasibleNodes\u0026#34;, scheduleResult.FeasibleNodes) metrics.PodScheduled(fwk.ProfileName(), metrics.SinceInSeconds(start)) metrics.PodSchedulingAttempts.Observe(float64(assumedPodInfo.Attempts)) if assumedPodInfo.InitialAttemptTimestamp != nil { metrics.PodSchedulingDuration.WithLabelValues(getAttemptsLabel(assumedPodInfo)).Observe(metrics.SinceInSeconds(*assumedPodInfo.InitialAttemptTimestamp)) metrics.PodSchedulingSLIDuration.WithLabelValues(getAttemptsLabel(assumedPodInfo)).Observe(metrics.SinceInSeconds(*assumedPodInfo.InitialAttemptTimestamp)) } //这段Go代码中的函数主要执行以下操作： //1. 调用sched.bind()函数运行名为\u0026#34;bind\u0026#34;的插件，并检查返回的状态是否成功。如果不成功，则返回该状态。 //2. 如果日志的详细程度高于等于2，则记录一条成功将Pod绑定到节点的日志，同时记录一些指标，如评估的节点数和可行节点数。 //3. 更新Pod相关的指标，例如记录Pod被调度的次数和调度尝试的持续时间。 //4. 如果Pod有初始尝试时间，则记录Pod调度的持续时间和SLI（服务级别指标）持续时间。 //这段代码的主要目的是在调度Pod后执行一些后续操作，包括记录日志和更新指标。 // Run \u0026#34;postbind\u0026#34; plugins. fwk.RunPostBindPlugins(ctx, state, assumedPod, scheduleResult.SuggestedHost) // At the end of a successful binding cycle, move up Pods if needed. if len(podsToActivate.Map) != 0 { sched.SchedulingQueue.Activate(logger, podsToActivate.Map) // Unlike the logic in schedulingCycle(), we don\u0026#39;t bother deleting the entries // as `podsToActivate.Map` is no longer consumed. } return nil } //这段Go代码是 Kubernetes 调度器中的一个函数片段，主要执行以下两个操作： //1. 运行 \u0026#34;postbind\u0026#34; 插件：fwk.RunPostBindPlugins(ctx, state, assumedPod, scheduleResult.SuggestedHost) //这行代码会调用所有注册的 \u0026#34;postbind\u0026#34; 插件。这些插件是在绑定周期成功结束后运行的，用于执行一些额外的操作，例如更新 Pod 的状态或记录日志。 //2. 激活就绪的 Pod：如果 podsToActivate.Map 不为空，即有待激活的 Pod， //则调用 sched.SchedulingQueue.Activate(logger, podsToActivate.Map) 来激活这些 Pod。这会将这些 Pod 加入到调度队列中， //以便它们可以被调度到合适的节点上运行。与调度周期中的逻辑不同，这里不需要删除条目，因为 podsToActivate.Map 不再被使用。 //这段代码的主要目的是在成功完成绑定周期后，执行必要的后处理操作，并激活就绪的 Pod。 func (sched *Scheduler) handleBindingCycleError( ctx context.Context, state *framework.CycleState, fwk framework.Framework, podInfo *framework.QueuedPodInfo, start time.Time, scheduleResult ScheduleResult, status *framework.Status) { logger := klog.FromContext(ctx) //该函数是Scheduler的一个方法，用于处理绑定周期错误。 //它通过记录日志来记录错误信息，其中包括上下文、框架状态、队列中的Pod信息、开始时间、调度结果和状态等。 assumedPod := podInfo.Pod // trigger un-reserve plugins to clean up state associated with the reserved Pod fwk.RunReservePluginsUnreserve(ctx, state, assumedPod, scheduleResult.SuggestedHost) if forgetErr := sched.Cache.ForgetPod(logger, assumedPod); forgetErr != nil { logger.Error(forgetErr, \u0026#34;scheduler cache ForgetPod failed\u0026#34;) } else { // \u0026#34;Forget\u0026#34;ing an assumed Pod in binding cycle should be treated as a PodDelete event, // as the assumed Pod had occupied a certain amount of resources in scheduler cache. // // Avoid moving the assumed Pod itself as it\u0026#39;s always Unschedulable. // It\u0026#39;s intentional to \u0026#34;defer\u0026#34; this operation; otherwise MoveAllToActiveOrBackoffQueue() would // add this event to in-flight events and thus move the assumed pod to backoffQ anyways if the plugins don\u0026#39;t have appropriate QueueingHint. if status.IsRejected() { defer sched.SchedulingQueue.MoveAllToActiveOrBackoffQueue(logger, internalqueue.AssignedPodDelete, assumedPod, nil, func(pod *v1.Pod) bool { return assumedPod.UID != pod.UID }) } else { sched.SchedulingQueue.MoveAllToActiveOrBackoffQueue(logger, internalqueue.AssignedPodDelete, assumedPod, nil, nil) } } //该函数主要完成以下功能： //1. 调用fwk.RunReservePluginsUnreserve方法，触发未预留插件清理与预留Pod相关联的状态。 //2. 忘记缓存中的Pod，如果忘记失败，则记录错误日志。 //3. 如果Pod被拒绝，则将除被假设的Pod本身以外的所有Pod移动到活动队列或退避队列；否则，将所有Pod移动到活动队列或退避队列。 sched.FailureHandler(ctx, fwk, podInfo, status, clearNominatedNode, start) } //第一个函数sched.FailureHandler(ctx, fwk, podInfo, status, clearNominatedNode, start)是一个处理调度失败的函数。 //它接受多个参数，包括上下文、框架、Pod信息、状态、是否清除提名节点以及开始时间，用于处理Pod调度失败的情况。 func (sched *Scheduler) frameworkForPod(pod *v1.Pod) (framework.Framework, error) { fwk, ok := sched.Profiles[pod.Spec.SchedulerName] if !ok { return nil, fmt.Errorf(\u0026#34;profile not found for scheduler name %q\u0026#34;, pod.Spec.SchedulerName) } return fwk, nil } //第二个函数(sched *Scheduler) frameworkForPod(pod *v1.Pod) (framework.Framework, error)是一个根据Pod获取对应调度框架的函数。 //它接受一个Pod作为参数，通过Pod的Spec.SchedulerName来查找对应的调度框架。 //如果找到，则返回该框架；如果找不到，则返回一个错误。 // skipPodSchedule returns true if we could skip scheduling the pod for specified cases. func (sched *Scheduler) skipPodSchedule(ctx context.Context, fwk framework.Framework, pod *v1.Pod) bool { // Case 1: pod is being deleted. if pod.DeletionTimestamp != nil { fwk.EventRecorder().Eventf(pod, nil, v1.EventTypeWarning, \u0026#34;FailedScheduling\u0026#34;, \u0026#34;Scheduling\u0026#34;, \u0026#34;skip schedule deleting pod: %v/%v\u0026#34;, pod.Namespace, pod.Name) klog.FromContext(ctx).V(3).Info(\u0026#34;Skip schedule deleting pod\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod)) return true } //该函数是一个Go语言函数，名为skipPodSchedule，它属于Scheduler类型。函数的主要功能是判断是否可以跳过指定Pod的调度。 //具体来说，函数首先检查Pod的DeletionTimestamp是否为nil，如果是，则记录事件和日志，并返回true，表示可以跳过调度。 //否则，函数不进行任何操作，返回false。 // Case 2: pod that has been assumed could be skipped. // An assumed pod can be added again to the scheduling queue if it got an update event // during its previous scheduling cycle but before getting assumed. isAssumed, err := sched.Cache.IsAssumedPod(pod) if err != nil { // TODO(91633): pass ctx into a revised HandleError utilruntime.HandleError(fmt.Errorf(\u0026#34;failed to check whether pod %s/%s is assumed: %v\u0026#34;, pod.Namespace, pod.Name, err)) return false } return isAssumed } //该函数用于判断一个Pod是否已被调度器假设（Assumed）。 //- 首先，它调用sched.Cache.IsAssumedPod(pod)方法来检查Pod是否已被假设。 //- 如果检查过程中出现错误，会通过utilruntime.HandleError方法记录错误信息，并返回false。 //- 如果检查没有错误，则直接返回检查结果。 //这个函数的主要作用是在调度Pod时，判断该Pod是否需要重新进入调度队列。 //如果Pod在上一次调度周期中更新了事件，但在被假设之前，它可能会被再次添加到调度队列中。 // schedulePod tries to schedule the given pod to one of the nodes in the node list. // If it succeeds, it will return the name of the node. // If it fails, it will return a FitError with reasons. func (sched *Scheduler) schedulePod(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) (result ScheduleResult, err error) { trace := utiltrace.New(\u0026#34;Scheduling\u0026#34;, utiltrace.Field{Key: \u0026#34;namespace\u0026#34;, Value: pod.Namespace}, utiltrace.Field{Key: \u0026#34;name\u0026#34;, Value: pod.Name}) defer trace.LogIfLong(100 * time.Millisecond) if err := sched.Cache.UpdateSnapshot(klog.FromContext(ctx), sched.nodeInfoSnapshot); err != nil { return result, err } trace.Step(\u0026#34;Snapshotting scheduler cache and node infos done\u0026#34;) //该函数是一个调度函数，用于尝试将给定的Pod调度到节点列表中的一个节点上。 //如果成功，它将返回节点的名称；如果失败，它将返回一个FitError错误，其中包含原因。 //函数首先创建一个utiltrace对象用于记录跟踪信息，并在函数退出时记录跟踪信息的时长。 //然后，它通过调用sched.Cache.UpdateSnapshot函数更新调度程序的缓存和节点信息快照。如果更新出现错误，函数将返回错误。 //最后，函数通过调用trace.Step记录一个跟踪步骤，表示快照节点信息和调度程序缓存的操作已经完成。 if sched.nodeInfoSnapshot.NumNodes() == 0 { return result, ErrNoNodesAvailable } feasibleNodes, diagnosis, err := sched.findNodesThatFitPod(ctx, fwk, state, pod) if err != nil { return result, err } trace.Step(\u0026#34;Computing predicates done\u0026#34;) if len(feasibleNodes) == 0 { return result, \u0026amp;framework.FitError{ Pod: pod, NumAllNodes: sched.nodeInfoSnapshot.NumNodes(), Diagnosis: diagnosis, } } //这段Go代码是关于调度器在一个集群中为一个Pod寻找合适节点的逻辑。 //首先，它检查当前集群中是否有可用节点，如果没有则返回错误ErrNoNodesAvailable。 //接下来，它调用sched.findNodesThatFitPod函数来找到能够容纳该Pod的节点。如果该函数返回错误，则直接返回错误。 //如果找到了合适的节点，代码会继续执行 //；如果没有找到合适的节点，则返回一个FitError错误，其中包含了Pod信息、集群中节点的数量以及诊断信息。 // When only one node after predicate, just use it. if len(feasibleNodes) == 1 { return ScheduleResult{ SuggestedHost: feasibleNodes[0].Node().Name, EvaluatedNodes: 1 + len(diagnosis.NodeToStatusMap), FeasibleNodes: 1, }, nil } //该函数是一个Go语言函数片段，用于在满足一定条件时返回一个ScheduleResult结构体实例。 //首先，函数通过判断feasibleNodes切片的长度是否为1来确定是否满足某种条件。 //如果满足条件，即feasibleNodes长度为1， //则创建并返回一个ScheduleResult结构体实例， //其中SuggestedHost字段被设置为feasibleNodes[0].Node().Name，EvaluatedNodes字段被设置为1 + len(diagnosis.NodeToStatusMap)， //FeasibleNodes字段被设置为1。 //这个函数的主要作用是在找到唯一一个符合条件的节点时，生成一个调度结果，建议将任务调度到该节点上，并统计评估的节点数和可行节点数。 priorityList, err := prioritizeNodes(ctx, sched.Extenders, fwk, state, pod, feasibleNodes) if err != nil { return result, err } host, _, err := selectHost(priorityList, numberOfHighestScoredNodesToReport) trace.Step(\u0026#34;Prioritizing done\u0026#34;) return ScheduleResult{ SuggestedHost: host, EvaluatedNodes: len(feasibleNodes) + len(diagnosis.NodeToStatusMap), FeasibleNodes: len(feasibleNodes), }, err } //这段Go代码是一个调度过程中的一部分，用于根据一系列规则对节点进行优先级排序，并最终选择一个最适合运行Pod的节点。 //首先，函数调用prioritizeNodes来对可行节点进行优先级排序，得到一个优先级列表priorityList。如果在这个过程中出现错误，则会返回一个错误结果。 //接下来，函数调用selectHost来从优先级列表中选择一个最适合运行Pod的节点。选择节点的过程会考虑到节点的得分和其他因素。 //选择完成后，会记录一个调度过程的步骤。 //最后，函数返回一个ScheduleResult结构体实例，其中包含了建议的节点主机名host、评估的节点数和可行节点数。 //如果在调度过程中出现错误， //则会将错误一起返回。 //这个函数的主要作用是在给定的一组可行节点中，根据预定的规则和策略选择一个最优节点来运行Pod，并生成相应的调度结果。 // Filters the nodes to find the ones that fit the pod based on the framework // filter plugins and filter extenders. func (sched *Scheduler) findNodesThatFitPod(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) ([]*framework.NodeInfo, framework.Diagnosis, error) { logger := klog.FromContext(ctx) diagnosis := framework.Diagnosis{ NodeToStatusMap: make(framework.NodeToStatusMap), } //这段Go代码定义了一个名为findNodesThatFitPod的函数，它属于Scheduler类型。该函数的功能是筛选出适合运行给定Pod的节点。 //函数的输入参数包括： //- ctx：上下文对象，用于控制函数执行的生命周期。 //- fwk：一个实现了Framework接口的对象，用于执行筛选插件和扩展程序。 //- state：一个CycleState对象，包含了调度过程中的状态信息。 //- pod：一个指向v1.Pod对象的指针，表示需要调度的Pod。 //函数的输出结果包括： //- 一个包含所有适合运行Pod的节点信息的切片。 //- 一个Diagnosis对象，包含了在筛选过程中收集到的诊断信息。 //- 一个错误对象，如果在筛选过程中发生错误，则会返回该错误。 //在函数内部，它首先创建了一个logger对象，用于记录日志信息。 //然后创建了一个diagnosis对象，并初始化了它的NodeToStatusMap字段。 //接下来，函数调用fwk.Filter方法来筛选出适合运行Pod的节点。这个方法会根据框架中的筛选插件和扩展程序来判断节点是否适合运行Pod。 //如果节点不适合，则会在diagnosis.NodeToStatusMap中记录下该节点不适合的原因。 //最后，函数返回筛选出的节点信息切片、诊断信息和错误对象（如果有）。 allNodes, err := sched.nodeInfoSnapshot.NodeInfos().List() if err != nil { return nil, diagnosis, err } // Run \u0026#34;prefilter\u0026#34; plugins. preRes, s := fwk.RunPreFilterPlugins(ctx, state, pod) if !s.IsSuccess() { if !s.IsRejected() { return nil, diagnosis, s.AsError() } // All nodes in NodeToStatusMap will have the same status so that they can be handled in the preemption. // Some non trivial refactoring is needed to avoid this copy. for _, n := range allNodes { diagnosis.NodeToStatusMap[n.Node().Name] = s } //该Go函数主要实现了以下功能： //1. 调用sched.nodeInfoSnapshot.NodeInfos().List()获取所有节点的信息。 //2. 运行\u0026#34;prefilter\u0026#34;插件，通过fwk.RunPreFilterPlugins(ctx, state, pod)获取插件运行结果。 //3. 如果插件运行结果不成功且未被拒绝，则将所有节点的状态更新为该插件的运行结果状态，并返回错误信息。 //具体分析如下： //- 首先，函数会尝试获取集群中所有节点的信息，并将结果保存在allNodes变量中。如果获取节点信息时出现错误，函数会立即返回错误信息。 //- 接下来，函数会运行\u0026#34;prefilter\u0026#34;插件，并将运行结果保存在preRes变量中。如果插件运行结果不成功（即未通过插件的验证）， //则会根据插件的运行结果状态进行处理。 //- 如果插件运行结果状态既不是成功也不是被拒绝，则函数会将所有节点的状态更新为该插件的运行结果状态， //并将该状态保存在diagnosis.NodeToStatusMap中。这样做的目的是为了在后续的预删除操作中，能够统一处理所有节点的状态。 //- 最后，如果插件运行结果状态是被拒绝的，则函数会直接返回错误信息。 //总之，该函数的主要作用是在调度过程中运行\u0026#34;prefilter\u0026#34;插件，并根据插件的运行结果来更新节点的状态或返回错误信息。 // Record the messages from PreFilter in Diagnosis.PreFilterMsg. msg := s.Message() diagnosis.PreFilterMsg = msg logger.V(5).Info(\u0026#34;Status after running PreFilter plugins for pod\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;status\u0026#34;, msg) diagnosis.AddPluginStatus(s) return nil, diagnosis, nil } //该函数主要功能是记录PreFilter插件运行后的内容。 //1. 首先获取消息msg := s.Message()； //2. 然后将该消息记录到diagnosis.PreFilterMsg中； //3. 使用logger.V(5).Info记录日志，包括pod信息和状态消息； //4. 最后将插件状态添加到diagnosis中，并返回nil, diagnosis, nil。 // \u0026#34;NominatedNodeName\u0026#34; can potentially be set in a previous scheduling cycle as a result of preemption. // This node is likely the only candidate that will fit the pod, and hence we try it first before iterating over all nodes. if len(pod.Status.NominatedNodeName) \u0026gt; 0 { feasibleNodes, err := sched.evaluateNominatedNode(ctx, pod, fwk, state, diagnosis) if err != nil { logger.Error(err, \u0026#34;Evaluation failed on nominated node\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;node\u0026#34;, pod.Status.NominatedNodeName) } // Nominated node passes all the filters, scheduler is good to assign this node to the pod. if len(feasibleNodes) != 0 { return feasibleNodes, diagnosis, nil } } //这段Go代码的功能是在调度Pod之前，优先尝试在上一次调度周期中因抢占而被提名的节点上是否可以放置该Pod。 //如果提名的节点通过了所有的过滤器，则将该节点分配给Pod。 nodes := allNodes if !preRes.AllNodes() { nodes = make([]*framework.NodeInfo, 0, len(preRes.NodeNames)) for _, n := range allNodes { if !preRes.NodeNames.Has(n.Node().Name) { // We consider Nodes that are filtered out by PreFilterResult as rejected via UnschedulableAndUnresolvable. // We have to record them in NodeToStatusMap so that they won\u0026#39;t be considered as candidates in the preemption. diagnosis.NodeToStatusMap[n.Node().Name] = framework.NewStatus(framework.UnschedulableAndUnresolvable, \u0026#34;node is filtered out by the prefilter result\u0026#34;) continue } nodes = append(nodes, n) } } feasibleNodes, err := sched.findNodesThatPassFilters(ctx, fwk, state, pod, \u0026amp;diagnosis, nodes) // always try to update the sched.nextStartNodeIndex regardless of whether an error has occurred // this is helpful to make sure that all the nodes have a chance to be searched processedNodes := len(feasibleNodes) + len(diagnosis.NodeToStatusMap) sched.nextStartNodeIndex = (sched.nextStartNodeIndex + processedNodes) % len(nodes) if err != nil { return nil, diagnosis, err } //该函数是Go语言编写的，用于在给定节点列表中找到满足特定条件的节点。 //首先，函数会检查preRes.AllNodes()是否为真，如果不为真，则遍历allNodes列表， //将不在preRes.NodeNames中的节点过滤掉，并将过滤掉的节点标记为\u0026#34;rejected via UnschedulableAndUnresolvable\u0026#34;。 //接下来，函数调用sched.findNodesThatPassFilters方法来找到通过特定过滤器的节点，并将结果保存在feasibleNodes变量中。 //最后，函数更新sched.nextStartNodeIndex属性，并返回结果。 feasibleNodesAfterExtender, err := findNodesThatPassExtenders(ctx, sched.Extenders, pod, feasibleNodes, diagnosis.NodeToStatusMap) if err != nil { return nil, diagnosis, err } if len(feasibleNodesAfterExtender) != len(feasibleNodes) { // Extenders filtered out some nodes. // // Extender doesn\u0026#39;t support any kind of requeueing feature like EnqueueExtensions in the scheduling framework. // When Extenders reject some Nodes and the pod ends up being unschedulable, // we put framework.ExtenderName to pInfo.UnschedulablePlugins. // This Pod will be requeued from unschedulable pod pool to activeQ/backoffQ // by any kind of cluster events. // https://github.com/kubernetes/kubernetes/issues/122019 if diagnosis.UnschedulablePlugins == nil { diagnosis.UnschedulablePlugins = sets.New[string]() } diagnosis.UnschedulablePlugins.Insert(framework.ExtenderName) } return feasibleNodesAfterExtender, diagnosis, nil } //这段Go代码的功能是在给定的节点列表中，通过调用一组扩展程序来找到满足特定条件的节点。 //首先，函数调用findNodesThatPassExtenders方法，该方法会遍历给定的扩展程序列表， //并将通过扩展程序过滤的节点保存在feasibleNodesAfterExtender变量中。 //如果feasibleNodesAfterExtender的长度与feasibleNodes的长度不相等，则说明有节点被扩展程序过滤掉了。 //在这种情况下，函数会将framework.ExtenderName添加到diagnosis.UnschedulablePlugins集合中，以表示该扩展程序导致了节点被拒绝。 //最后，函数返回feasibleNodesAfterExtender、diagnosis和nil作为结果。 func (sched *Scheduler) evaluateNominatedNode(ctx context.Context, pod *v1.Pod, fwk framework.Framework, state *framework.CycleState, diagnosis framework.Diagnosis) ([]*framework.NodeInfo, error) { nnn := pod.Status.NominatedNodeName nodeInfo, err := sched.nodeInfoSnapshot.Get(nnn) if err != nil { return nil, err } node := []*framework.NodeInfo{nodeInfo} feasibleNodes, err := sched.findNodesThatPassFilters(ctx, fwk, state, pod, \u0026amp;diagnosis, node) if err != nil { return nil, err } feasibleNodes, err = findNodesThatPassExtenders(ctx, sched.Extenders, pod, feasibleNodes, diagnosis.NodeToStatusMap) if err != nil { return nil, err } return feasibleNodes, nil } //该函数是Scheduler的一个方法，用于评估提名的节点是否适合放置Pod。 //它首先通过NominatedNodeName获取节点信息，然后调用findNodesThatPassFilters方法过滤不满足条件的节点， //并调用findNodesThatPassExtenders方法进一步过滤节点。最终返回满足条件的节点列表。 // hasScoring checks if scoring nodes is configured. func (sched *Scheduler) hasScoring(fwk framework.Framework) bool { if fwk.HasScorePlugins() { return true } for _, extender := range sched.Extenders { if extender.IsPrioritizer() { return true } } return false } //该函数用于判断调度器中是否配置了评分节点。 //首先检查给定的框架是否有评分插件，如果有，则返回true。 //如果没有，则遍历调度器的扩展程序，如果某个扩展程序是优先级判断器，则返回true。 //最后，如果没有找到评分插件或优先级判断器，则返回false。 // hasExtenderFilters checks if any extenders filter nodes. func (sched *Scheduler) hasExtenderFilters() bool { for _, extender := range sched.Extenders { if extender.IsFilter() { return true } } return false } //该函数用于判断调度器中是否存在扩展程序过滤节点。 //它遍历调度器的扩展程序列表，如果找到任何一个具有过滤功能的扩展程序，则返回true，表示存在扩展程序过滤节点。 //如果没有找到任何具有过滤功能的扩展程序，则返回false。 // findNodesThatPassFilters finds the nodes that fit the filter plugins. func (sched *Scheduler) findNodesThatPassFilters( ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod, diagnosis *framework.Diagnosis, nodes []*framework.NodeInfo) ([]*framework.NodeInfo, error) { numAllNodes := len(nodes) numNodesToFind := sched.numFeasibleNodesToFind(fwk.PercentageOfNodesToScore(), int32(numAllNodes)) if !sched.hasExtenderFilters() \u0026amp;\u0026amp; !sched.hasScoring(fwk) { numNodesToFind = 1 } //该函数是Scheduler的一个方法，用于查找符合过滤条件的节点。 //它根据传入的过滤插件、调度框架、调度状态、Pod和诊断信息，从给定的节点列表中找到符合条件的节点。 //函数首先计算出需要查找的节点数量，根据是否有扩展器过滤器和评分器来决定最终的查找数量。 //如果没有扩展器过滤器和评分器，则只需要找到一个符合条件的节点。 //函数返回最终找到的符合条件的节点列表和可能的错误。 // Create feasible list with enough space to avoid growing it // and allow assigning. feasibleNodes := make([]*framework.NodeInfo, numNodesToFind) if !fwk.HasFilterPlugins() { for i := range feasibleNodes { feasibleNodes[i] = nodes[(sched.nextStartNodeIndex+i)%numAllNodes] } return feasibleNodes, nil } //这段代码是Scheduler的一个方法，用于创建一个可行节点列表，列表的长度为numNodesToFind，以避免增长并允许分配。 //如果fwk没有过滤插件，则将nodes中的节点按顺序填充到feasibleNodes中，并返回该列表。 errCh := parallelize.NewErrorChannel() var feasibleNodesLen int32 ctx, cancel := context.WithCancel(ctx) defer cancel() type nodeStatus struct { node string status *framework.Status } result := make([]*nodeStatus, numAllNodes) checkNode := func(i int) { // We check the nodes starting from where we left off in the previous scheduling cycle, // this is to make sure all nodes have the same chance of being examined across pods. nodeInfo := nodes[(sched.nextStartNodeIndex+i)%numAllNodes] status := fwk.RunFilterPluginsWithNominatedPods(ctx, state, pod, nodeInfo) if status.Code() == framework.Error { errCh.SendErrorWithCancel(status.AsError(), cancel) return } if status.IsSuccess() { length := atomic.AddInt32(\u0026amp;feasibleNodesLen, 1) if length \u0026gt; numNodesToFind { cancel() atomic.AddInt32(\u0026amp;feasibleNodesLen, -1) } else { feasibleNodes[length-1] = nodeInfo } } else { result[i] = \u0026amp;nodeStatus{node: nodeInfo.Node().Name, status: status} } } //这段Go代码中的函数是一个并发执行节点检查的函数。 //它通过调用parallelize.NewErrorChannel()创建了一个错误通道errCh，用于收集并发执行过程中的错误信息。 //函数使用context.WithCancel(ctx)创建了一个可取消的上下文ctx，并在函数结束时通过defer cancel()取消该上下文， //以确保所有并发操作都被正确终止。 函数内部定义了一个nodeStatus结构体，用于存储节点的名称和状态。 //result是一个用于存储检查结果的切片，其长度为numAllNodes。checkNode是一个闭包函数，用于检查节点是否适合放置一个Pod。 //它通过计算索引，从上一次调度周期中未检查的节点开始检查。 //在检查节点时，它会调用fwk.RunFilterPluginsWithNominatedPods来运行过滤插件， //并根据插件的返回状态来更新feasibleNodesLen和feasibleNodes，或者将节点的状态存储到result中。 //如果节点检查过程中出现错误，会通过错误通道发送错误信息并取消上下文。 //总之，这个函数的作用是并发地检查一组节点，并收集检查结果和错误信息。 beginCheckNode := time.Now() statusCode := framework.Success defer func() { // We record Filter extension point latency here instead of in framework.go because framework.RunFilterPlugins // function is called for each node, whereas we want to have an overall latency for all nodes per scheduling cycle. // Note that this latency also includes latency for `addNominatedPods`, which calls framework.RunPreFilterAddPod. metrics.FrameworkExtensionPointDuration.WithLabelValues(metrics.Filter, statusCode.String(), fwk.ProfileName()).Observe(metrics.SinceInSeconds(beginCheckNode)) }() // Stops searching for more nodes once the configured number of feasible nodes // are found. fwk.Parallelizer().Until(ctx, numAllNodes, checkNode, metrics.Filter) feasibleNodes = feasibleNodes[:feasibleNodesLen] for _, item := range result { if item == nil { continue } diagnosis.NodeToStatusMap[item.node] = item.status diagnosis.AddPluginStatus(item.status) } if err := errCh.ReceiveError(); err != nil { statusCode = framework.Error return feasibleNodes, err } return feasibleNodes, nil } //这段Go代码中的函数是用于在调度周期内并行检查多个节点是否适合放置Pod的函数。 //它首先记录了过滤扩展点的延迟时间，并在函数退出时更新相关指标。 //然后使用fwk.Parallelizer().Until方法并行执行checkNode函数，该函数会对每个节点进行检查， //并根据检查结果更新feasibleNodes和diagnosis.NodeToStatusMap。 //最后，该函数会从错误通道接收错误信息，并根据错误信息更新状态码并返回结果。 // numFeasibleNodesToFind returns the number of feasible nodes that once found, the scheduler stops // its search for more feasible nodes. func (sched *Scheduler) numFeasibleNodesToFind(percentageOfNodesToScore *int32, numAllNodes int32) (numNodes int32) { if numAllNodes \u0026lt; minFeasibleNodesToFind { return numAllNodes } // Use profile percentageOfNodesToScore if it\u0026#39;s set. Otherwise, use global percentageOfNodesToScore. var percentage int32 if percentageOfNodesToScore != nil { percentage = *percentageOfNodesToScore } else { percentage = sched.percentageOfNodesToScore } if percentage == 0 { percentage = int32(50) - numAllNodes/125 if percentage \u0026lt; minFeasibleNodesPercentageToFind { percentage = minFeasibleNodesPercentageToFind } } numNodes = numAllNodes * percentage / 100 if numNodes \u0026lt; minFeasibleNodesToFind { return minFeasibleNodesToFind } return numNodes } //该函数是Go语言编写的，用于计算调度器需要找到的可行节点的数量。 //函数首先判断所有节点的数量是否小于最小可行节点数，如果是，则直接返回所有节点数。 //然后根据传入的节点评分百分比或全局百分比计算需要找到的节点数。 //如果百分比为0，则根据节点总数计算一个默认的百分比。 //最后，返回计算得到的可行节点数，如果小于最小可行节点数，则返回最小可行节点数。 func findNodesThatPassExtenders(ctx context.Context, extenders []framework.Extender, pod *v1.Pod, feasibleNodes []*framework.NodeInfo, statuses framework.NodeToStatusMap) ([]*framework.NodeInfo, error) { logger := klog.FromContext(ctx) // Extenders are called sequentially. // Nodes in original feasibleNodes can be excluded in one extender, and pass on to the next // extender in a decreasing manner. for _, extender := range extenders { if len(feasibleNodes) == 0 { break } if !extender.IsInterested(pod) { continue } //该函数的功能是通过调用一系列的extenders来筛选出能够运行pod的节点。 //它会依次调用每个extender，并将筛选后的节点传递给下一个extender。 //如果某个extender对pod不感兴趣，则会跳过该extender。 // Status of failed nodes in failedAndUnresolvableMap will be added or overwritten in \u0026lt;statuses\u0026gt;, // so that the scheduler framework can respect the UnschedulableAndUnresolvable status for // particular nodes, and this may eventually improve preemption efficiency. // Note: users are recommended to configure the extenders that may return UnschedulableAndUnresolvable // status ahead of others. feasibleList, failedMap, failedAndUnresolvableMap, err := extender.Filter(pod, feasibleNodes) if err != nil { if extender.IsIgnorable() { logger.Info(\u0026#34;Skipping extender as it returned error and has ignorable flag set\u0026#34;, \u0026#34;extender\u0026#34;, extender, \u0026#34;err\u0026#34;, err) continue } return nil, err } //这段Go代码是调用一个名为extender.Filter的函数，该函数用于过滤出适合放置Pod的节点，并更新节点的状态。 //函数的返回值包括可行节点列表feasibleList、失败节点映射failedMap、失败且不可解决节点映射failedAndUnresolvableMap和错误信息err。 //如果extender.Filter函数返回错误，且该错误可被忽略（通过extender.IsIgnorable()判断），则会打印日志信息并继续执行；否则，直接返回错误。 for failedNodeName, failedMsg := range failedAndUnresolvableMap { var aggregatedReasons []string if _, found := statuses[failedNodeName]; found { aggregatedReasons = statuses[failedNodeName].Reasons() } aggregatedReasons = append(aggregatedReasons, failedMsg) statuses[failedNodeName] = framework.NewStatus(framework.UnschedulableAndUnresolvable, aggregatedReasons...) } //这段Go代码遍历failedAndUnresolvableMap，对于每个失败且不可解决的节点，获取其已有的状态信息（如果存在）， //将当前失败消息添加到状态原因列表中，并更新节点的状态为UnschedulableAndUnresolvable。 //这里使用了framework.NewStatus函数创建新的状态对象。 for failedNodeName, failedMsg := range failedMap { if _, found := failedAndUnresolvableMap[failedNodeName]; found { // failedAndUnresolvableMap takes precedence over failedMap // note that this only happens if the extender returns the node in both maps continue } if _, found := statuses[failedNodeName]; !found { statuses[failedNodeName] = framework.NewStatus(framework.Unschedulable, failedMsg) } else { statuses[failedNodeName].AppendReason(failedMsg) } } feasibleNodes = feasibleList } return feasibleNodes, nil } //这段Go代码是一个for循环，用于遍历failedMap，并根据条件对failedAndUnresolvableMap和statuses进行操作。 //接着将feasibleList赋值给feasibleNodes，并最终返回feasibleNodes和nil。 //具体来说： //- 遍历failedMap中的每个元素，其中failedNodeName为键，failedMsg为值。 //- 判断failedAndUnresolvableMap中是否存在键为failedNodeName的元素，如果存在则跳过当前循环。 //- 判断statuses中是否存在键为failedNodeName的元素，如果存在则将failedMsg追加为该元素的reason， //如果不存在则创建一个新的framework.Status对象，并将其添加到statuses中。 //最后，将feasibleList赋值给feasibleNodes，并返回feasibleNodes和nil。 // prioritizeNodes prioritizes the nodes by running the score plugins, // which return a score for each node from the call to RunScorePlugins(). // The scores from each plugin are added together to make the score for that node, then // any extenders are run as well. // All scores are finally combined (added) to get the total weighted scores of all nodes func prioritizeNodes( ctx context.Context, extenders []framework.Extender, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod, nodes []*framework.NodeInfo, ) ([]framework.NodePluginScores, error) { logger := klog.FromContext(ctx) // If no priority configs are provided, then all nodes will have a score of one. // This is required to generate the priority list in the required format if len(extenders) == 0 \u0026amp;\u0026amp; !fwk.HasScorePlugins() { result := make([]framework.NodePluginScores, 0, len(nodes)) for i := range nodes { result = append(result, framework.NodePluginScores{ Name: nodes[i].Node().Name, TotalScore: 1, }) } return result, nil } //该函数的作用是通过运行评分插件来优先级排序节点。 // 首先，它根据调用RunScorePlugins()方法从每个插件返回的分数计算每个节点的分数 // 然后，它运行任何扩展程序。最后，将所有分数相加得到节点的总权重分数。 //如果未提供优先级配置，则所有节点的得分为1。 // Run PreScore plugins. preScoreStatus := fwk.RunPreScorePlugins(ctx, state, pod, nodes) if !preScoreStatus.IsSuccess() { return nil, preScoreStatus.AsError() } //该函数运行PreScore插件，通过fwk.RunPreScorePlugins方法执行。如果运行成功，则继续执行后续代码；如果运行失败，则返回错误信息。 // Run the Score plugins. nodesScores, scoreStatus := fwk.RunScorePlugins(ctx, state, pod, nodes) if !scoreStatus.IsSuccess() { return nil, scoreStatus.AsError() } // Additional details logged at level 10 if enabled. loggerVTen := logger.V(10) if loggerVTen.Enabled() { for _, nodeScore := range nodesScores { for _, pluginScore := range nodeScore.Scores { loggerVTen.Info(\u0026#34;Plugin scored node for pod\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;plugin\u0026#34;, pluginScore.Name, \u0026#34;node\u0026#34;, nodeScore.Name, \u0026#34;score\u0026#34;, pluginScore.Score) } } } //该函数运行Score插件，通过fwk.RunScorePlugins方法为每个节点计算一个得分。 //如果运行成功，则继续执行后续代码；如果运行失败，则返回错误信息。如果日志级别10被启用，则会记录每个插件为每个节点打分的详细信息。 if len(extenders) != 0 \u0026amp;\u0026amp; nodes != nil { // allNodeExtendersScores has all extenders scores for all nodes. // It is keyed with node name. allNodeExtendersScores := make(map[string]*framework.NodePluginScores, len(nodes)) var mu sync.Mutex var wg sync.WaitGroup for i := range extenders { if !extenders[i].IsInterested(pod) { continue } wg.Add(1) go func(extIndex int) { metrics.Goroutines.WithLabelValues(metrics.PrioritizingExtender).Inc() defer func() { metrics.Goroutines.WithLabelValues(metrics.PrioritizingExtender).Dec() wg.Done() }() prioritizedList, weight, err := extenders[extIndex].Prioritize(pod, nodes) if err != nil { // Prioritization errors from extender can be ignored, let k8s/other extenders determine the priorities logger.V(5).Info(\u0026#34;Failed to run extender\u0026#39;s priority function. No score given by this extender.\u0026#34;, \u0026#34;error\u0026#34;, err, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;extender\u0026#34;, extenders[extIndex].Name()) return } mu.Lock() defer mu.Unlock() for i := range *prioritizedList { nodename := (*prioritizedList)[i].Host score := (*prioritizedList)[i].Score if loggerVTen.Enabled() { loggerVTen.Info(\u0026#34;Extender scored node for pod\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;extender\u0026#34;, extenders[extIndex].Name(), \u0026#34;node\u0026#34;, nodename, \u0026#34;score\u0026#34;, score) } //这段Go代码的功能是使用一组扩展器(extendenders)对一组节点(nodes)进行优先级排序。 //首先，它会遍历所有扩展器，并对每个感兴趣的扩展器启动一个goroutine来调用其优先级函数。 //扩展器的优先级函数返回一个加权优先级列表(prioritizedList)和一个权重(weight)。 //然后，这段代码会将每个扩展器对每个节点的得分存储在一个映射(allNodeExtendersScores)中，以便后续使用。 // MaxExtenderPriority may diverge from the max priority used in the scheduler and defined by MaxNodeScore, // therefore we need to scale the score returned by extenders to the score range used by the scheduler. finalscore := score * weight * (framework.MaxNodeScore / extenderv1.MaxExtenderPriority) if allNodeExtendersScores[nodename] == nil { allNodeExtendersScores[nodename] = \u0026amp;framework.NodePluginScores{ Name: nodename, Scores: make([]framework.PluginScore, 0, len(extenders)), } } allNodeExtendersScores[nodename].Scores = append(allNodeExtendersScores[nodename].Scores, framework.PluginScore{ Name: extenders[extIndex].Name(), Score: finalscore, }) allNodeExtendersScores[nodename].TotalScore += finalscore } }(i) } // wait for all go routines to finish wg.Wait() for i := range nodesScores { if score, ok := allNodeExtendersScores[nodes[i].Node().Name]; ok { nodesScores[i].Scores = append(nodesScores[i].Scores, score.Scores...) nodesScores[i].TotalScore += score.TotalScore } } } if loggerVTen.Enabled() { for i := range nodesScores { loggerVTen.Info(\u0026#34;Calculated node\u0026#39;s final score for pod\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;node\u0026#34;, nodesScores[i].Name, \u0026#34;score\u0026#34;, nodesScores[i].TotalScore) } } return nodesScores, nil } var errEmptyPriorityList = errors.New(\u0026#34;empty priorityList\u0026#34;) // selectHost takes a prioritized list of nodes and then picks one // in a reservoir sampling manner from the nodes that had the highest score. // It also returns the top {count} Nodes, // and the top of the list will be always the selected host. func selectHost(nodeScoreList []framework.NodePluginScores, count int) (string, []framework.NodePluginScores, error) { if len(nodeScoreList) == 0 { return \u0026#34;\u0026#34;, nil, errEmptyPriorityList } var h nodeScoreHeap = nodeScoreList heap.Init(\u0026amp;h) cntOfMaxScore := 1 selectedIndex := 0 // The top of the heap is the NodeScoreResult with the highest score. sortedNodeScoreList := make([]framework.NodePluginScores, 0, count) sortedNodeScoreList = append(sortedNodeScoreList, heap.Pop(\u0026amp;h).(framework.NodePluginScores)) // This for-loop will continue until all Nodes with the highest scores get checked for a reservoir sampling, // and sortedNodeScoreList gets (count - 1) elements. for ns := heap.Pop(\u0026amp;h).(framework.NodePluginScores); ; ns = heap.Pop(\u0026amp;h).(framework.NodePluginScores) { if ns.TotalScore != sortedNodeScoreList[0].TotalScore \u0026amp;\u0026amp; len(sortedNodeScoreList) == count { break } if ns.TotalScore == sortedNodeScoreList[0].TotalScore { cntOfMaxScore++ if rand.Intn(cntOfMaxScore) == 0 { // Replace the candidate with probability of 1/cntOfMaxScore selectedIndex = cntOfMaxScore - 1 } } sortedNodeScoreList = append(sortedNodeScoreList, ns) if h.Len() == 0 { break } } if selectedIndex != 0 { // replace the first one with selected one previous := sortedNodeScoreList[0] sortedNodeScoreList[0] = sortedNodeScoreList[selectedIndex] sortedNodeScoreList[selectedIndex] = previous } if len(sortedNodeScoreList) \u0026gt; count { sortedNodeScoreList = sortedNodeScoreList[:count] } return sortedNodeScoreList[0].Name, sortedNodeScoreList, nil } // nodeScoreHeap is a heap of framework.NodePluginScores. type nodeScoreHeap []framework.NodePluginScores // nodeScoreHeap implements heap.Interface. var _ heap.Interface = \u0026amp;nodeScoreHeap{} func (h nodeScoreHeap) Len() int { return len(h) } func (h nodeScoreHeap) Less(i, j int) bool { return h[i].TotalScore \u0026gt; h[j].TotalScore } func (h nodeScoreHeap) Swap(i, j int) { h[i], h[j] = h[j], h[i] } func (h *nodeScoreHeap) Push(x interface{}) { *h = append(*h, x.(framework.NodePluginScores)) } func (h *nodeScoreHeap) Pop() interface{} { old := *h n := len(old) x := old[n-1] *h = old[0 : n-1] return x } // assume signals to the cache that a pod is already in the cache, so that binding can be asynchronous. // assume modifies `assumed`. func (sched *Scheduler) assume(logger klog.Logger, assumed *v1.Pod, host string) error { // Optimistically assume that the binding will succeed and send it to apiserver // in the background. // If the binding fails, scheduler will release resources allocated to assumed pod // immediately. assumed.Spec.NodeName = host if err := sched.Cache.AssumePod(logger, assumed); err != nil { logger.Error(err, \u0026#34;Scheduler cache AssumePod failed\u0026#34;) return err } // if \u0026#34;assumed\u0026#34; is a nominated pod, we should remove it from internal cache if sched.SchedulingQueue != nil { sched.SchedulingQueue.DeleteNominatedPodIfExists(assumed) } return nil } // bind binds a pod to a given node defined in a binding object. // The precedence for binding is: (1) extenders and (2) framework plugins. // We expect this to run asynchronously, so we handle binding metrics internally. func (sched *Scheduler) bind(ctx context.Context, fwk framework.Framework, assumed *v1.Pod, targetNode string, state *framework.CycleState) (status *framework.Status) { logger := klog.FromContext(ctx) defer func() { sched.finishBinding(logger, fwk, assumed, targetNode, status) }() bound, err := sched.extendersBinding(logger, assumed, targetNode) if bound { return framework.AsStatus(err) } return fwk.RunBindPlugins(ctx, state, assumed, targetNode) } // TODO(#87159): Move this to a Plugin. func (sched *Scheduler) extendersBinding(logger klog.Logger, pod *v1.Pod, node string) (bool, error) { for _, extender := range sched.Extenders { if !extender.IsBinder() || !extender.IsInterested(pod) { continue } err := extender.Bind(\u0026amp;v1.Binding{ ObjectMeta: metav1.ObjectMeta{Namespace: pod.Namespace, Name: pod.Name, UID: pod.UID}, Target: v1.ObjectReference{Kind: \u0026#34;Node\u0026#34;, Name: node}, }) if err != nil \u0026amp;\u0026amp; extender.IsIgnorable() { logger.Info(\u0026#34;Skipping extender in bind as it returned error and has ignorable flag set\u0026#34;, \u0026#34;extender\u0026#34;, extender, \u0026#34;err\u0026#34;, err) continue } return true, err } return false, nil } func (sched *Scheduler) finishBinding(logger klog.Logger, fwk framework.Framework, assumed *v1.Pod, targetNode string, status *framework.Status) { if finErr := sched.Cache.FinishBinding(logger, assumed); finErr != nil { logger.Error(finErr, \u0026#34;Scheduler cache FinishBinding failed\u0026#34;) } if !status.IsSuccess() { logger.V(1).Info(\u0026#34;Failed to bind pod\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(assumed)) return } fwk.EventRecorder().Eventf(assumed, nil, v1.EventTypeNormal, \u0026#34;Scheduled\u0026#34;, \u0026#34;Binding\u0026#34;, \u0026#34;Successfully assigned %v/%v to %v\u0026#34;, assumed.Namespace, assumed.Name, targetNode) } func getAttemptsLabel(p *framework.QueuedPodInfo) string { // We breakdown the pod scheduling duration by attempts capped to a limit // to avoid ending up with a high cardinality metric. if p.Attempts \u0026gt;= 15 { return \u0026#34;15+\u0026#34; } return strconv.Itoa(p.Attempts) } // handleSchedulingFailure records an event for the pod that indicates the // pod has failed to schedule. Also, update the pod condition and nominated node name if set. func (sched *Scheduler) handleSchedulingFailure(ctx context.Context, fwk framework.Framework, podInfo *framework.QueuedPodInfo, status *framework.Status, nominatingInfo *framework.NominatingInfo, start time.Time) { calledDone := false defer func() { if !calledDone { // Basically, AddUnschedulableIfNotPresent calls DonePod internally. // But, AddUnschedulableIfNotPresent isn\u0026#39;t called in some corner cases. // Here, we call DonePod explicitly to avoid leaking the pod. sched.SchedulingQueue.Done(podInfo.Pod.UID) } }() //该函数是Go语言编写的，用于处理调度失败的情况。 //它会记录一个表示Pod调度失败的事件，并更新Pod的状态和提名节点名（如果已设置）。函数主要包含以下几点内容： //1. 定义了一个名为handleSchedulingFailure的函数，它接受多个参数， //包括上下文ctx、框架fwk、排队的Pod信息podInfo、状态status、提名信息nominatingInfo和开始时间start。 //2. 在函数内部，定义了一个名为calledDone的布尔变量，用于标记是否已经调用了DonePod方法。 //3. 使用defer语句定义了一个匿名函数，该函数会在handleSchedulingFailure函数退出时执行。 //匿名函数中，通过判断calledDone是否已被标记为true来决定是否需要显式调用sched.SchedulingQueue.Done(podInfo.Pod.UID)方法， //以避免泄露Pod。 //4. 函数的主体部分包括记录调度失败事件和更新Pod状态和提名节点名的逻辑，这部分内容在给定的代码片段中没有展示出来。 logger := klog.FromContext(ctx) reason := v1.PodReasonSchedulerError if status.IsRejected() { reason = v1.PodReasonUnschedulable } switch reason { case v1.PodReasonUnschedulable: metrics.PodUnschedulable(fwk.ProfileName(), metrics.SinceInSeconds(start)) case v1.PodReasonSchedulerError: metrics.PodScheduleError(fwk.ProfileName(), metrics.SinceInSeconds(start)) } //该函数主要通过klog从上下文ctx中获取logger，根据status的状态来设置Pod失败的原因，然后根据不同的失败原因来记录相应的metrics。 //具体步骤如下： //1. 从上下文ctx中获取logger。 //2. 初始化reason为SchedulerError。 //3. 如果status被拒绝，则将reason更新为Unschedulable。 //4. 根据reason的不同，记录相应的metrics： //- 如果reason为Unschedulable，则调用PodUnschedulable方法，并传入fwk的ProfileName和start与当前时间的秒数差作为参数。 //- 如果reason为SchedulerError，则调用PodScheduleError方法，并传入fwk的ProfileName和start与当前时间的秒数差作为参数。 pod := podInfo.Pod err := status.AsError() errMsg := status.Message() if err == ErrNoNodesAvailable { logger.V(2).Info(\u0026#34;Unable to schedule pod; no nodes are registered to the cluster; waiting\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod)) } else if fitError, ok := err.(*framework.FitError); ok { // Inject UnschedulablePlugins to PodInfo, which will be used later for moving Pods between queues efficiently. podInfo.UnschedulablePlugins = fitError.Diagnosis.UnschedulablePlugins podInfo.PendingPlugins = fitError.Diagnosis.PendingPlugins logger.V(2).Info(\u0026#34;Unable to schedule pod; no fit; waiting\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;err\u0026#34;, errMsg) } else { logger.Error(err, \u0026#34;Error scheduling pod; retrying\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod)) } //这段Go代码主要处理了三种不同类型的错误，并根据错误类型记录日志或更新podInfo对象的属性。 //首先，如果错误为ErrNoNodesAvailable，则会记录一条日志，指示无法调度pod，因为没有可用的节点，并等待。 //其次，如果错误类型为*framework.FitError，则将该错误的诊断信息（UnschedulablePlugins和PendingPlugins）注入到podInfo对象中， //以供后续使用，然后记录一条日志，指示无法调度pod，因为没有合适的节点，并等待。 //最后，如果错误类型不是上述两种类型，则记录一条错误日志，指示调度pod时出错，并重试。 //总的来说，这段代码主要是根据不同的错误类型进行错误处理，并通过日志记录相关信息。 // Check if the Pod exists in informer cache. podLister := fwk.SharedInformerFactory().Core().V1().Pods().Lister() cachedPod, e := podLister.Pods(pod.Namespace).Get(pod.Name) if e != nil { logger.Info(\u0026#34;Pod doesn\u0026#39;t exist in informer cache\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;err\u0026#34;, e) // We need to call DonePod here because we don\u0026#39;t call AddUnschedulableIfNotPresent in this case. } else { // In the case of extender, the pod may have been bound successfully, but timed out returning its response to the scheduler. // It could result in the live version to carry .spec.nodeName, and that\u0026#39;s inconsistent with the internal-queued version. if len(cachedPod.Spec.NodeName) != 0 { logger.Info(\u0026#34;Pod has been assigned to node. Abort adding it back to queue.\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;node\u0026#34;, cachedPod.Spec.NodeName) // We need to call DonePod here because we don\u0026#39;t call AddUnschedulableIfNotPresent in this case. } else { // As \u0026lt;cachedPod\u0026gt; is from SharedInformer, we need to do a DeepCopy() here. // ignore this err since apiserver doesn\u0026#39;t properly validate affinity terms // and we can\u0026#39;t fix the validation for backwards compatibility. podInfo.PodInfo, _ = framework.NewPodInfo(cachedPod.DeepCopy()) if err := sched.SchedulingQueue.AddUnschedulableIfNotPresent(logger, podInfo, sched.SchedulingQueue.SchedulingCycle()); err != nil { logger.Error(err, \u0026#34;Error occurred\u0026#34;) } calledDone = true } } //该函数用于检查Pod是否存在于informer缓存中。 //如果存在，则根据Pod的状态进行不同的处理。如果Pod已经被分配到节点上，则不再将其添加到队列中； //否则，将Pod信息添加到队列中，以便进行调度。 //如果Pod不存在于缓存中，则需要调用DonePod函数。 // Update the scheduling queue with the nominated pod information. Without // this, there would be a race condition between the next scheduling cycle // and the time the scheduler receives a Pod Update for the nominated pod. // Here we check for nil only for tests. if sched.SchedulingQueue != nil { logger := klog.FromContext(ctx) sched.SchedulingQueue.AddNominatedPod(logger, podInfo.PodInfo, nominatingInfo) } if err == nil { // Only tests can reach here. return } msg := truncateMessage(errMsg) fwk.EventRecorder().Eventf(pod, nil, v1.EventTypeWarning, \u0026#34;FailedScheduling\u0026#34;, \u0026#34;Scheduling\u0026#34;, msg) if err := updatePod(ctx, sched.client, pod, \u0026amp;v1.PodCondition{ Type: v1.PodScheduled, Status: v1.ConditionFalse, Reason: reason, Message: errMsg, }, nominatingInfo); err != nil { klog.FromContext(ctx).Error(err, \u0026#34;Error updating pod\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod)) } } //这段Go代码主要功能是更新调度队列中的提名Pod信息，避免调度周期和调度器接收提名Pod更新之间的竞态条件。 //具体来说： //- 首先检查sched.SchedulingQueue是否为nil，仅在测试中会为nil。 //- 若不为nil，则使用logger记录日志，并调用AddNominatedPod方法将提名Pod信息加入调度队列。 //- 若没有错误发生，仅在测试中会执行，直接返回。 //- 截断错误消息，并使用EventRecorder记录事件，事件类型为FailedScheduling，事件原因为msg。 //- 调用updatePod方法更新Pod的条件，将PodScheduled状态设置为ConditionFalse，原因设置为reason，消息设置为errMsg， //并将提名信息传递给updatePod方法。 //- 若更新Pod时发生错误，记录错误日志。 // truncateMessage truncates a message if it hits the NoteLengthLimit. func truncateMessage(message string) string { max := validation.NoteLengthLimit if len(message) \u0026lt;= max { return message } suffix := \u0026#34; ...\u0026#34; return message[:max-len(suffix)] + suffix } //该函数用于截断字符串，如果给定的消息长度超过了validation.NoteLengthLimit规定的最大长度，则在末尾添加\u0026#34;...\u0026#34;并返回截断后的消息字符串。 //如果给定的消息长度小于等于最大长度，则直接返回原消息字符串。 //该函数用于截断字符串，如果给定的字符串长度超过了validation.NoteLengthLimit规定的最大长度，则在字符串末尾添加\u0026#34;...\u0026#34;并返回截断后的字符串； //如果给定的字符串长度不超过最大长度，则直接返回原字符串。 func updatePod(ctx context.Context, client clientset.Interface, pod *v1.Pod, condition *v1.PodCondition, nominatingInfo *framework.NominatingInfo) error { logger := klog.FromContext(ctx) logger.V(3).Info(\u0026#34;Updating pod condition\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;conditionType\u0026#34;, condition.Type, \u0026#34;conditionStatus\u0026#34;, condition.Status, \u0026#34;conditionReason\u0026#34;, condition.Reason) podStatusCopy := pod.Status.DeepCopy() // NominatedNodeName is updated only if we are trying to set it, and the value is // different from the existing one. nnnNeedsUpdate := nominatingInfo.Mode() == framework.ModeOverride \u0026amp;\u0026amp; pod.Status.NominatedNodeName != nominatingInfo.NominatedNodeName if !podutil.UpdatePodCondition(podStatusCopy, condition) \u0026amp;\u0026amp; !nnnNeedsUpdate { return nil } if nnnNeedsUpdate { podStatusCopy.NominatedNodeName = nominatingInfo.NominatedNodeName } return util.PatchPodStatus(ctx, client, pod, podStatusCopy) } //该函数用于更新Pod的条件状态，如果满足一定条件，则更新Pod的NominatedNodeName字段， //并通过PatchPodStatus函数将更新后的Pod状态应用到实际的Pod对象中。 //- 首先，从上下文中获取日志记录器，并输出相关日志信息。 //- 然后，创建一个Pod状态的深拷贝。 //- 接着，判断是否需要更新NominatedNodeName字段， //只有当nominatingInfo的模式为ModeOverride且当前NominatedNodeName与nominatingInfo中的NominatedNodeName不相同时，才需要更新。 //- 如果不需要更新Pod的条件状态和NominatedNodeName字段，则直接返回。 //- 如果需要更新NominatedNodeName字段，则将其赋值给podStatusCopy中的NominatedNodeName字段。 //- 最后，调用PatchPodStatus函数将podStatusCopy中的更新应用到实际的Pod对象中，并返回操作结果。 //请注意，该函数中涉及到的一些参数和函数的具体实现和用途可能需要结合具体的上下文和代码来理解。 "},{"id":95,"href":"/docs/k8s%E4%B9%8Bingress-nginx%E5%8E%9F%E7%90%86%E5%8F%8A%E9%85%8D%E7%BD%AE-k8s-zhi-ingress-nginx-yuan-li-ji-pei-zhi/","title":"K8S之ingress-nginx原理及配置 2024-07-05 18:02:14.243","section":"Docs","content":"前言 在Kubernetes中，服务和Pod的IP地址仅可以在集群网络内部使用，对于集群外的应用是不可见的。为了使外部的应用能够访问集群内的服务，在Kubernetes中目前提供了以下几种方案：\n* NodePort * LoadBalancer * Ingress 在之前的博文中介绍过NodePort，简单来说，就是通过service这种资源对象，为后端pod提供一个统一的访问接口，然后将service的统一访问接口映射到群集节点上，最终实现client通过映射到群集节点上的端口访问到后端pod提供的服务。\n但是，这种方式有一个弊端，就是当新生成一个pod服务就需要创建对应的service将其映射到节点端口，当运行的pod过多时，我们节点暴露给client端的端口也会随之增加，这样我们整个k8s群集的危险系数就会增加，因为我们在搭建群集之处，官方明确指出，必须关闭firewalld防火墙及清空iptables规则，现在我们又暴露了那么多端口给client，安全系数可想而知。\n一、Ingress-nginx介绍 1、Ingress-nginx组成\n* ingress-nginx-controller：根据用户编写的ingress规则（创建的ingress的yaml文件）， 动态的去更改nginx服务的配置文件，并且reload重载使其生效（是自动化的，通过lua脚本来实现）； * ingress资源对象：将Nginx的配置抽象成一个Ingress对象，每添加一个新的Service资 源对象只需写一个新的Ingress规则的yaml文件即可（或修改已存在的ingress规则的yaml文件） 2、Ingress-nginx可以解决什么问题？\n1)动态配置服务 如果按照传统方式, 当新增加一个服务时, 我们可能需要在流量入口加一个反向代理指 向我们新的k8s服务. 而如果用了Ingress-nginx, 只需要配置好这个服务, 当服务启动 时, 会自动注册到Ingress的中, 不需要而外的操作。 2)减少不必要的端口映射 配置过k8s的都清楚, 第一步是要关闭防火墙的, 主要原因是k8s的很多服务会 以NodePort方式映射出去, 这样就相当于给宿主机打了很多孔, 既不安全也不优雅. 而Ingress可以避免这个问题, 除了Ingress自身服务可能需要映射出去, 其他服务都不要 用NodePort方式 3、Ingress-nginx工作原理\n1）ingress controller通过和kubernetes api交互，动态的去感知集群中ingress规则变化， 2）然后读取它，按照自定义的规则，规则就是写明了哪个域名对应哪个service，生成一段nginx配置， 3）再写到nginx-ingress-controller的pod里，这个Ingress controller的pod里运行着一个Nginx服务，控制器会把生成的nginx配置写入/etc/nginx.conf文件中， 4）然后reload一下使配置生效。以此达到域名分别配置和动态更新的问题。 2、创建namespace（也可跳过，使用默认的default名称空间也可以，但需要删除下面所有yaml文件中关于自定义的名称空间的配置字段）\n[root@master ~]# kubectl create ns test-ns //创建名称空间test-ns [root@master ~]# kubectl get ns //确认创建成功 3、创建Deployment、Service资源对象 1）创建httpd服务及其service与之关联\n--- apiVersion: apps/v1 kind: Deployment metadata: annotations: {} labels: app: httpd01 k8s.kuboard.cn/name: web01 name: web01 namespace: test-ns spec: progressDeadlineSeconds: 600 replicas: 3 revisionHistoryLimit: 10 selector: matchLabels: app: httpd01 k8s.kuboard.cn/name: web01 strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: app: httpd01 k8s.kuboard.cn/name: web01 spec: containers: - image: \u0026#39;192.168.0.140:881/library/httpd:latest\u0026#39; imagePullPolicy: IfNotPresent name: httpd resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 --- apiVersion: v1 kind: Service metadata: annotations: {} name: httpd-svc namespace: test-ns spec: internalTrafficPolicy: Cluster ipFamilies: - IPv4 ipFamilyPolicy: SingleStack ports: - name: httpd-svc port: 80 protocol: TCP targetPort: 80 selector: app: httpd01 sessionAffinity: None type: ClusterIP 2）创建tomcat服务及其service\n--- apiVersion: apps/v1 kind: Deployment metadata: annotations: {} labels: app: tomcat01 k8s.kuboard.cn/name: web02 name: web02 name: web02 namespace: test-ns spec: progressDeadlineSeconds: 600 replicas: 3 revisionHistoryLimit: 10 selector: matchLabels: app: tomcat01 k8s.kuboard.cn/name: web02 name: web02 strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: app: tomcat01 k8s.kuboard.cn/name: web02 name: web02 spec: containers: - image: \u0026#39;192.168.0.140:881/library/tomcat:8.5.45\u0026#39; imagePullPolicy: IfNotPresent name: tomcat resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 --- apiVersion: v1 kind: Service metadata: annotations: {} name: tomcat-svc namespace: test-ns spec: internalTrafficPolicy: Cluster ipFamilies: - IPv4 ipFamilyPolicy: SingleStack ports: - name: tomcat-svc port: 8080 protocol: TCP targetPort: 8080 selector: app: tomcat01 sessionAffinity: None type: ClusterIP 3）确保以上资源对象成功创建\n[root@master ~]# kubectl get pod -n test-ns //确定pod是正常运行状态 NAME READY STATUS RESTARTS AGE web01-85674fbdd7-7h6cc 1/1 Running 0 9m3s web01-85674fbdd7-9l2zm 1/1 Running 0 9m3s web01-85674fbdd7-p9hfx 1/1 Running 0 9m3s web02-7f8f755bc7-4qk9b 1/1 Running 0 7m22s web02-7f8f755bc7-qhbnm 1/1 Running 0 7m22s web02-7f8f755bc7-zr56g 1/1 Running 0 7m22s [root@master ~]# kubectl get svc -n test-ns //确认SVC创建成功 NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE httpd-svc ClusterIP 10.102.57.12 \u0026lt;none\u0026gt; 80/TCP 10m tomcat-svc ClusterIP 10.110.25.145 \u0026lt;none\u0026gt; 8080/TCP 8m23s [root@master ~]# curl -I 10.102.57.12:80 //访问httpd HTTP/1.1 200 OK Date: Sun, 23 Aug 2020 07:02:06 GMT Server: Apache/2.4.46 (Unix) Last-Modified: Mon, 11 Jun 2007 18:53:14 GMT ETag: \u0026#34;2d-432a5e4a73a80\u0026#34; Accept-Ranges: bytes Content-Length: 45 Content-Type: text/html [root@master ~]# curl -I 10.110.25.145:8080 //访问tomcat HTTP/1.1 200 Content-Type: text/html;charset=UTF-8 Transfer-Encoding: chunked Date: Sun, 23 Aug 2020 09:08:18 GMT //OK，以上表示内部访问是没有问题的 //如果在上述访问测试中，没有访问到相应的pod，建议使用“kubectl describe svc”命令， 查看相应的service中的Endpoints列中有没有关联后端pod。 4、创建Ingress-nginx资源对象 # ingress-nginx.yaml 直接apply # k apply -f ingress.nginx.yaml apiVersion: v1 kind: Namespace metadata: labels: app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx name: ingress-nginx --- apiVersion: v1 automountServiceAccountToken: true kind: ServiceAccount metadata: labels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.10.1 name: ingress-nginx namespace: ingress-nginx --- apiVersion: v1 kind: ServiceAccount metadata: labels: app.kubernetes.io/component: admission-webhook app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.10.1 name: ingress-nginx-admission namespace: ingress-nginx --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: labels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.10.1 name: ingress-nginx namespace: ingress-nginx rules: - apiGroups: - \u0026#34;\u0026#34; resources: - namespaces verbs: - get - apiGroups: - \u0026#34;\u0026#34; resources: - configmaps - pods - secrets - endpoints verbs: - get - list - watch - apiGroups: - \u0026#34;\u0026#34; resources: - services verbs: - get - list - watch - apiGroups: - networking.k8s.io resources: - ingresses verbs: - get - list - watch - apiGroups: - networking.k8s.io resources: - ingresses/status verbs: - update - apiGroups: - networking.k8s.io resources: - ingressclasses verbs: - get - list - watch - apiGroups: - coordination.k8s.io resourceNames: - ingress-nginx-leader resources: - leases verbs: - get - update - apiGroups: - coordination.k8s.io resources: - leases verbs: - create - apiGroups: - \u0026#34;\u0026#34; resources: - events verbs: - create - patch - apiGroups: - discovery.k8s.io resources: - endpointslices verbs: - list - watch - get --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: labels: app.kubernetes.io/component: admission-webhook app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.10.1 name: ingress-nginx-admission namespace: ingress-nginx rules: - apiGroups: - \u0026#34;\u0026#34; resources: - secrets verbs: - get - create --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.10.1 name: ingress-nginx rules: - apiGroups: - \u0026#34;\u0026#34; resources: - configmaps - endpoints - nodes - pods - secrets - namespaces verbs: - list - watch - apiGroups: - coordination.k8s.io resources: - leases verbs: - list - watch - apiGroups: - \u0026#34;\u0026#34; resources: - nodes verbs: - get - apiGroups: - \u0026#34;\u0026#34; resources: - services verbs: - get - list - watch - apiGroups: - networking.k8s.io resources: - ingresses verbs: - get - list - watch - apiGroups: - \u0026#34;\u0026#34; resources: - events verbs: - create - patch - apiGroups: - networking.k8s.io resources: - ingresses/status verbs: - update - apiGroups: - networking.k8s.io resources: - ingressclasses verbs: - get - list - watch - apiGroups: - discovery.k8s.io resources: - endpointslices verbs: - list - watch - get --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: app.kubernetes.io/component: admission-webhook app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.10.1 name: ingress-nginx-admission rules: - apiGroups: - admissionregistration.k8s.io resources: - validatingwebhookconfigurations verbs: - get - update --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: labels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.10.1 name: ingress-nginx namespace: ingress-nginx roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: ingress-nginx subjects: - kind: ServiceAccount name: ingress-nginx namespace: ingress-nginx --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: labels: app.kubernetes.io/component: admission-webhook app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.10.1 name: ingress-nginx-admission namespace: ingress-nginx roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: ingress-nginx-admission subjects: - kind: ServiceAccount name: ingress-nginx-admission namespace: ingress-nginx --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: labels: app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.10.1 name: ingress-nginx roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: ingress-nginx subjects: - kind: ServiceAccount name: ingress-nginx namespace: ingress-nginx --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: labels: app.kubernetes.io/component: admission-webhook app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.10.1 name: ingress-nginx-admission roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: ingress-nginx-admission subjects: - kind: ServiceAccount name: ingress-nginx-admission namespace: ingress-nginx --- apiVersion: v1 data: allow-snippet-annotations: \u0026#34;false\u0026#34; kind: ConfigMap metadata: labels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.10.1 name: ingress-nginx-controller namespace: ingress-nginx --- apiVersion: v1 kind: Service metadata: labels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.10.1 name: ingress-nginx-controller namespace: ingress-nginx spec: ipFamilies: - IPv4 ipFamilyPolicy: SingleStack ports: - appProtocol: http name: http port: 80 protocol: TCP targetPort: http - appProtocol: https name: https port: 443 protocol: TCP targetPort: https selector: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx type: NodePort --- apiVersion: v1 kind: Service metadata: labels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.10.1 name: ingress-nginx-controller-admission namespace: ingress-nginx spec: ports: - appProtocol: https name: https-webhook port: 443 targetPort: webhook selector: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx type: ClusterIP --- apiVersion: apps/v1 kind: Deployment metadata: labels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.10.1 name: ingress-nginx-controller namespace: ingress-nginx spec: minReadySeconds: 0 revisionHistoryLimit: 10 selector: matchLabels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx strategy: rollingUpdate: maxUnavailable: 1 type: RollingUpdate template: metadata: labels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.10.1 spec: containers: - args: - /nginx-ingress-controller - --election-id=ingress-nginx-leader - --controller-class=k8s.io/ingress-nginx - --ingress-class=nginx - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller - --validating-webhook=:8443 - --validating-webhook-certificate=/usr/local/certificates/cert - --validating-webhook-key=/usr/local/certificates/key - --enable-metrics=false env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: LD_PRELOAD value: /usr/local/lib/libmimalloc.so image: 192.168.0.140:881/ingress-nginx/controller:v1.10.1 imagePullPolicy: IfNotPresent lifecycle: preStop: exec: command: - /wait-shutdown livenessProbe: failureThreshold: 5 httpGet: path: /healthz port: 10254 scheme: HTTP initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 name: controller ports: - containerPort: 80 name: http protocol: TCP - containerPort: 443 name: https protocol: TCP - containerPort: 8443 name: webhook protocol: TCP readinessProbe: failureThreshold: 3 httpGet: path: /healthz port: 10254 scheme: HTTP initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 resources: requests: cpu: 100m memory: 90Mi securityContext: allowPrivilegeEscalation: false capabilities: add: - NET_BIND_SERVICE drop: - ALL readOnlyRootFilesystem: false runAsNonRoot: true runAsUser: 101 seccompProfile: type: RuntimeDefault volumeMounts: - mountPath: /usr/local/certificates/ name: webhook-cert readOnly: true dnsPolicy: ClusterFirst nodeSelector: kubernetes.io/os: linux serviceAccountName: ingress-nginx terminationGracePeriodSeconds: 300 volumes: - name: webhook-cert secret: secretName: ingress-nginx-admission --- apiVersion: batch/v1 kind: Job metadata: labels: app.kubernetes.io/component: admission-webhook app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.10.1 name: ingress-nginx-admission-create namespace: ingress-nginx spec: template: metadata: labels: app.kubernetes.io/component: admission-webhook app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.10.1 name: ingress-nginx-admission-create spec: containers: - args: - create - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc - --namespace=$(POD_NAMESPACE) - --secret-name=ingress-nginx-admission env: - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace image: 192.168.0.140:881/ingress-nginx/kube-webhook-certgen:v1.4.1 imagePullPolicy: IfNotPresent name: create securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL readOnlyRootFilesystem: true runAsNonRoot: true runAsUser: 65532 seccompProfile: type: RuntimeDefault nodeSelector: kubernetes.io/os: linux restartPolicy: OnFailure serviceAccountName: ingress-nginx-admission --- apiVersion: batch/v1 kind: Job metadata: labels: app.kubernetes.io/component: admission-webhook app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.10.1 name: ingress-nginx-admission-patch namespace: ingress-nginx spec: template: metadata: labels: app.kubernetes.io/component: admission-webhook app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.10.1 name: ingress-nginx-admission-patch spec: containers: - args: - patch - --webhook-name=ingress-nginx-admission - --namespace=$(POD_NAMESPACE) - --patch-mutating=false - --secret-name=ingress-nginx-admission - --patch-failure-policy=Fail env: - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace image: 192.168.0.140:881/ingress-nginx/kube-webhook-certgen:v1.4.1 imagePullPolicy: IfNotPresent name: patch securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL readOnlyRootFilesystem: true runAsNonRoot: true runAsUser: 65532 seccompProfile: type: RuntimeDefault nodeSelector: kubernetes.io/os: linux restartPolicy: OnFailure serviceAccountName: ingress-nginx-admission --- apiVersion: networking.k8s.io/v1 kind: IngressClass metadata: labels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.10.1 name: nginx spec: controller: k8s.io/ingress-nginx --- apiVersion: admissionregistration.k8s.io/v1 kind: ValidatingWebhookConfiguration metadata: labels: app.kubernetes.io/component: admission-webhook app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.10.1 name: ingress-nginx-admission webhooks: - admissionReviewVersions: - v1 clientConfig: service: name: ingress-nginx-controller-admission namespace: ingress-nginx path: /networking/v1/ingresses failurePolicy: Fail matchPolicy: Equivalent name: validate.nginx.ingress.kubernetes.io rules: - apiGroups: - networking.k8s.io apiVersions: - v1 operations: - CREATE - UPDATE resources: - ingresses sideEffects: None 5、定义Ingress规则（编写ingress的yaml文件） # ingress.yaml # --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: nginx.ingress.kubernetes.io/rewrite-target: / name: test-ingress namespace: test-ns spec: ingressClassName: nginx rules: - host: www.test01.com http: paths: - backend: service: name: httpd-svc port: number: 80 path: / pathType: Prefix - backend: service: name: tomcat-svc port: number: 8080 path: /tomcat pathType: Prefix status: loadBalancer: ingress: - ip: 192.168.0.61 [root@master~]# vim ingress.yaml //编写yaml文件如下 apiVersion: extensions/v1beta1 kind: Ingress metadata: name: test-ingress namespace: test-ns annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - host: www.test01.com http: paths: - path: / backend: serviceName: httpd-svc servicePort: 80 - path: /tomcat backend: serviceName: tomcat-svc servicePort: 8080 [root@master~]# kubectl apply -f ingress.yaml //执行ingress规则的yaml文件 [root@master~]# kubectl get ingresses -n test-ns //查看ingresses规则资源对象 NAME HOSTS ADDRESS PORTS AGE test-ingress www.test01.com 80 28s 注：其实，至此已经实现了我们想要的功能，现在就可以通过www.test01.com 来访问到我们后端httpd容器提供的服务，通过www.test01.com/tomcat 来访问我们后端tomcat提供的服务，当然，前提是自行配置DNS解析，或者直接修改client的hosts文件。访问页面如下（注意：一定要自己解决域名解析的问题，若不知道域名对应的是哪个IP，请跳过这两个图，看下面的文字解释）：\n[root@master ~]# kubectl get pod -n ingress-nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-ingress-controller-86cdd68cf-hndtw 1/1 Running 0 89m 192.168.45.141 node01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 在上面的访问测试中，虽然访问到了对应的服务，但是有一个弊端，就是在做DNS解析的时候，只能指定Ingress-nginx容器所在的节点IP。而指定k8s集群内部的其他节点IP（包括master）都是不可以访问到的，如果这个节点一旦宕机，Ingress-nginx容器被转移到其他节点上运行（不考虑节点标签的问题，其实保持Ingress-nginx的yaml文件中默认的标签的话，那么每个节点都是有那个标签的）。随之还要我们手动去更改DNS解析的IP（要更改为Ingress-nginx容器所在节点的IP，通过命令“kubectl get pod -n ingress-nginx -o wide”可以查看到其所在节点），很是麻烦。\n有没有更简单的一种方法呢？答案是肯定的，就是我们为Ingress-nginx规则再创建一个类型为nodePort的Service，这样，在配置DNS解析时，就可以使用www.test01.com 绑定所有node节点，包括master节点的IP了，很是灵活。\n6、为Ingress规则创建一个Service 在刚才获取Ingress-controller资源对象的yaml文件的页面，然后下拉页面，即可看到以下，可以根据k8s集群环境来选择适合自己的yaml文件。假如自己是在Azure云平台搭建的K8s集群，则选择复制Azure下面的命令即可，我这里是自己的测试环境，所以选择Bare-metal下面的yaml文件，如图：\n[root@master ~]# wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.29.0/deploy/static/provider/baremetal/service-nodeport.yaml //如若下载失败，直接复制粘贴编辑即可 [root@master ~]# cat service-nodeport.yaml apiVersion: v1 kind: Service metadata: name: ingress-nginx namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx spec: type: NodePort ports: - name: http port: 80 targetPort: 80 protocol: TCP - name: https port: 443 targetPort: 443 protocol: TCP selector: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx //编辑完，保存退出即可 [root@master ~]# kubectl apply -f service-nodeport.yaml //执行yaml文件 [root@master ~]# kubectl get svc -n ingress-nginx //查看运行的service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx NodePort 10.108.48.248 \u0026lt;none\u0026gt; 80:32529/TCP,443:30534/TCP 11s //可以看到service分别将80和443端口映射到了节点的32529和30543端口（随机映射的，也可以修改yaml文件指定端口） 注：至此，这个www.test01.com 的域名即可和群集中任意节点的32529/30543端口进行绑定了。\n测试如下（域名解析对应的IP可以是k8s群集内的任意节点IP）：\n至此，就实现了最初的需求！！\n7、创建基于虚拟主机的Ingress规则 如果现在是另一种需求，我需要将www.test01.com 和www.test02.com 都对应上我后端的httpd容器提供的服务，那么此时应该怎么配置？\n[root@master test]# vim ingress.yaml #修改ingress规则的yaml文件如下 apiVersion: extensions/v1beta1 kind: Ingress metadata: name: test-ingress namespace: test-ns annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - host: www.test02.com //增加这一段host配置 http: paths: - path: / backend: serviceName: httpd-svc //绑定和www.test01相同的service名字即可 servicePort: 80 - host: www.test01.com http: paths: - path: / backend: serviceName: httpd-svc servicePort: 80 - path: /tomcat backend: serviceName: tomcat-svc servicePort: 8080 //增加完上述的host字段保存退出即可 [root@master test]# kubectl apply -f ingress.yaml //重新执行yaml文件 至此，即可实现访问www.test01.com 和www.test02.com 都可以访问到后端的httpd提供的页面（自行解决域名解析问题=域名解析配置client hosts文件即可），如下：\n总结上述示例的pod是如何一步一步可以使client访问到的，总结如下：\n后端pod===》service====》ingress规则====》写入Ingress-nginx-controller配置文件并自动重载使更改生效===》对Ingress-nginx创建service====》实现client无论通过哪个K8节点的IP+端口都可以访问到后端pod\n三、配置HTTPS 在上面的操作中，实现了使用ingress-nginx为后端所有pod提供一个统一的入口，那么，有一个非常严肃的问题需要考虑，就是如何为我们的pod配置CA证书来实现HTTPS访问？在pod中直接配置CA么？那需要进行多少重复性的操作？而且，pod是随时可能被kubelet杀死再创建的。当然这些问题有很多解决方法，比如直接将CA配置到镜像中，但是这样又需要很多个CA证书。\n这里有更简便的一种方法，就拿上面的情况来说，后端有多个pod，pod与service进行关联，service又被ingress规则发现并动态写入到ingress-nginx-controller容器中，然后又为ingress-nginx-controller创建了一个Service映射到群集节点上的端口，来供client来访问。\n在上面的一系列流程中，关键的点就在于ingress规则，我们只需要在ingress的yaml文件中，为域名配置CA证书即可，只要可以通过HTTPS访问到域名，至于这个域名是怎么关联到后端提供服务的pod，这就是属于k8s群集内部的通信了，即便是使用http来通信，也无伤大雅。\n配置如下：\n接下来的配置与上面的配置基本没什么关系，但是由于上面已经运行了Ingress-nginx-controller容器，所以这里就没有必要再运行了。只需要配置pod、service、ingress规则即可。\n//创建CA证书（测试环境，自己创建吧） [root@master https]# openssl req -x509 -sha256 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026#34;/CN=nginxsvc/O=nginxsvc\u0026#34; //当前目录下会生成两个文件，如下： [root@master https]# ls #确定当前目录下有这两个文件 tls.crt tls.key //将生成的CA证书存储到etcd [root@master https]# kubectl create secret tls tls-secret --key=tls.key --cert tls.crt //创建deploy、service、ingress资源对象 [root@master https]# vim httpd03.yaml //编写yaml文件 kind: Deployment apiVersion: extensions/v1beta1 metadata: name: web03 spec: replicas: 2 template: metadata: labels: app: httpd03 spec: containers: - name: httpd3 image: 192.168.45.129:5000/httpd:v1 --- apiVersion: v1 kind: Service metadata: name: httpd-svc3 spec: selector: app: httpd03 ports: - protocol: TCP port: 80 targetPort: 80 --- apiVersion: extensions/v1beta1 kind: Ingress metadata: name: test-ingress3 spec: tls: - hosts: - www.test03.com secretName: tls-secret //这里是指定的是etcd存储的CA证书名称 rules: - host: www.test03.com http: paths: - path: / backend: serviceName: httpd-svc3 servicePort: 80 [root@master https]# kubectl apply -f httpd03.yaml //执行yaml文件 确认创建的资源对象是否正常运行：\n[root@master https]# kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE httpd-svc3 ClusterIP 10.96.4.68 \u0026lt;none\u0026gt; 80/TCP 9s kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 12d [root@master https]# kubectl get pod NAME READY STATUS RESTARTS AGE web03-b955f886b-gjbnr 1/1 Running 0 7m23s web03-b955f886b-w8cdn 1/1 Running 0 7m23s [root@master https]# kubectl describe ingresses. //查看ingress规则 Name: test-ingress3 Namespace: default Address: Default backend: default-http-backend:80 (\u0026lt;none\u0026gt;) TLS: tls-secret terminates www.test03.com Rules: Host Path Backends ---- ---- -------- www.test03.com / httpd-svc3:80 (10.244.1.5:80,10.244.2.5:80) //确定关联到对应的service及后端的pod 注：使用https://www.test03.com 进行访问（自行解决域名解析问题）\n"},{"id":96,"href":"/docs/k8s%E4%B9%8Bkubelet%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-zhi-kubelet-yuan-ma-jie-du/","title":"k8s之kubelet源码解读 2024-03-28 15:30:39.633","section":"Docs","content":"/* Copyright 2015 The Kubernetes Authors. Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \u0026#34;AS IS\u0026#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */ package kubelet import ( \u0026#34;context\u0026#34; \u0026#34;crypto/tls\u0026#34; \u0026#34;errors\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;math\u0026#34; \u0026#34;net\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;os\u0026#34; \u0026#34;path/filepath\u0026#34; sysruntime \u0026#34;runtime\u0026#34; \u0026#34;sort\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;sync/atomic\u0026#34; \u0026#34;time\u0026#34; cadvisorapi \u0026#34;github.com/google/cadvisor/info/v1\u0026#34; \u0026#34;github.com/google/go-cmp/cmp\u0026#34; \u0026#34;go.opentelemetry.io/otel/attribute\u0026#34; semconv \u0026#34;go.opentelemetry.io/otel/semconv/v1.12.0\u0026#34; \u0026#34;go.opentelemetry.io/otel/trace\u0026#34; \u0026#34;k8s.io/client-go/informers\u0026#34; netutils \u0026#34;k8s.io/utils/net\u0026#34; v1 \u0026#34;k8s.io/api/core/v1\u0026#34; metav1 \u0026#34;k8s.io/apimachinery/pkg/apis/meta/v1\u0026#34; \u0026#34;k8s.io/apimachinery/pkg/fields\u0026#34; \u0026#34;k8s.io/apimachinery/pkg/labels\u0026#34; \u0026#34;k8s.io/apimachinery/pkg/types\u0026#34; utilruntime \u0026#34;k8s.io/apimachinery/pkg/util/runtime\u0026#34; \u0026#34;k8s.io/apimachinery/pkg/util/sets\u0026#34; \u0026#34;k8s.io/apimachinery/pkg/util/wait\u0026#34; utilfeature \u0026#34;k8s.io/apiserver/pkg/util/feature\u0026#34; clientset \u0026#34;k8s.io/client-go/kubernetes\u0026#34; v1core \u0026#34;k8s.io/client-go/kubernetes/typed/core/v1\u0026#34; corelisters \u0026#34;k8s.io/client-go/listers/core/v1\u0026#34; \u0026#34;k8s.io/client-go/tools/cache\u0026#34; \u0026#34;k8s.io/client-go/tools/record\u0026#34; \u0026#34;k8s.io/client-go/util/certificate\u0026#34; \u0026#34;k8s.io/client-go/util/flowcontrol\u0026#34; cloudprovider \u0026#34;k8s.io/cloud-provider\u0026#34; \u0026#34;k8s.io/component-helpers/apimachinery/lease\u0026#34; internalapi \u0026#34;k8s.io/cri-api/pkg/apis\u0026#34; runtimeapi \u0026#34;k8s.io/cri-api/pkg/apis/runtime/v1\u0026#34; pluginwatcherapi \u0026#34;k8s.io/kubelet/pkg/apis/pluginregistration/v1\u0026#34; statsapi \u0026#34;k8s.io/kubelet/pkg/apis/stats/v1alpha1\u0026#34; podutil \u0026#34;k8s.io/kubernetes/pkg/api/v1/pod\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/api/v1/resource\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/features\u0026#34; kubeletconfiginternal \u0026#34;k8s.io/kubernetes/pkg/kubelet/apis/config\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/kubelet/apis/podresources\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/kubelet/cadvisor\u0026#34; kubeletcertificate \u0026#34;k8s.io/kubernetes/pkg/kubelet/certificate\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/kubelet/cloudresource\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/kubelet/clustertrustbundle\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/kubelet/cm\u0026#34; draplugin \u0026#34;k8s.io/kubernetes/pkg/kubelet/cm/dra/plugin\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/kubelet/config\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/kubelet/configmap\u0026#34; kubecontainer \u0026#34;k8s.io/kubernetes/pkg/kubelet/container\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/kubelet/cri/remote\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/kubelet/events\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/kubelet/eviction\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/kubelet/images\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/kubelet/kuberuntime\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/kubelet/lifecycle\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/kubelet/logs\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/kubelet/metrics\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/kubelet/metrics/collectors\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/kubelet/network/dns\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/kubelet/nodeshutdown\u0026#34; oomwatcher \u0026#34;k8s.io/kubernetes/pkg/kubelet/oom\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/kubelet/pleg\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/kubelet/pluginmanager\u0026#34; plugincache \u0026#34;k8s.io/kubernetes/pkg/kubelet/pluginmanager/cache\u0026#34; kubepod \u0026#34;k8s.io/kubernetes/pkg/kubelet/pod\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/kubelet/preemption\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/kubelet/prober\u0026#34; proberesults \u0026#34;k8s.io/kubernetes/pkg/kubelet/prober/results\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/kubelet/runtimeclass\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/kubelet/secret\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/kubelet/server\u0026#34; servermetrics \u0026#34;k8s.io/kubernetes/pkg/kubelet/server/metrics\u0026#34; serverstats \u0026#34;k8s.io/kubernetes/pkg/kubelet/server/stats\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/kubelet/stats\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/kubelet/status\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/kubelet/sysctl\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/kubelet/token\u0026#34; kubetypes \u0026#34;k8s.io/kubernetes/pkg/kubelet/types\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/kubelet/userns\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/kubelet/userns/inuserns\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/kubelet/util\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/kubelet/util/manager\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/kubelet/util/queue\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/kubelet/util/sliceutils\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/kubelet/volumemanager\u0026#34; httpprobe \u0026#34;k8s.io/kubernetes/pkg/probe/http\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/security/apparmor\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/util/oom\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/volume\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/volume/csi\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/volume/util/hostutil\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/volume/util/subpath\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/volume/util/volumepathhandler\u0026#34; \u0026#34;k8s.io/utils/clock\u0026#34; ) const ( // Max amount of time to wait for the container runtime to come up. maxWaitForContainerRuntime = 30 * time.Second // nodeStatusUpdateRetry specifies how many times kubelet retries when posting node status failed. nodeStatusUpdateRetry = 5 // nodeReadyGracePeriod is the period to allow for before fast status update is // terminated and container runtime not being ready is logged without verbosity guard. nodeReadyGracePeriod = 120 * time.Second // DefaultContainerLogsDir is the location of container logs. DefaultContainerLogsDir = \u0026#34;/var/log/containers\u0026#34; // MaxContainerBackOff is the max backoff period, exported for the e2e test MaxContainerBackOff = 300 * time.Second // Period for performing global cleanup tasks. housekeepingPeriod = time.Second * 2 // Duration at which housekeeping failed to satisfy the invariant that // housekeeping should be fast to avoid blocking pod config (while // housekeeping is running no new pods are started or deleted). housekeepingWarningDuration = time.Second * 1 // Period after which the runtime cache expires - set to slightly longer than // the expected length between housekeeping periods, which explicitly refreshes // the cache. runtimeCacheRefreshPeriod = housekeepingPeriod + housekeepingWarningDuration // Period for performing eviction monitoring. // ensure this is kept in sync with internal cadvisor housekeeping. evictionMonitoringPeriod = time.Second * 10 // The path in containers\u0026#39; filesystems where the hosts file is mounted. linuxEtcHostsPath = \u0026#34;/etc/hosts\u0026#34; windowsEtcHostsPath = \u0026#34;C:\\\\Windows\\\\System32\\\\drivers\\\\etc\\\\hosts\u0026#34; // Capacity of the channel for receiving pod lifecycle events. This number // is a bit arbitrary and may be adjusted in the future. plegChannelCapacity = 1000 // Generic PLEG relies on relisting for discovering container events. // A longer period means that kubelet will take longer to detect container // changes and to update pod status. On the other hand, a shorter period // will cause more frequent relisting (e.g., container runtime operations), // leading to higher cpu usage. // Note that even though we set the period to 1s, the relisting itself can // take more than 1s to finish if the container runtime responds slowly // and/or when there are many container changes in one cycle. genericPlegRelistPeriod = time.Second * 1 genericPlegRelistThreshold = time.Minute * 3 // Generic PLEG relist period and threshold when used with Evented PLEG. eventedPlegRelistPeriod = time.Second * 300 eventedPlegRelistThreshold = time.Minute * 10 eventedPlegMaxStreamRetries = 5 // backOffPeriod is the period to back off when pod syncing results in an // error. It is also used as the base period for the exponential backoff // container restarts and image pulls. backOffPeriod = time.Second * 10 // ContainerGCPeriod is the period for performing container garbage collection. ContainerGCPeriod = time.Minute // ImageGCPeriod is the period for performing image garbage collection. ImageGCPeriod = 5 * time.Minute // Minimum number of dead containers to keep in a pod minDeadContainerInPod = 1 // nodeLeaseRenewIntervalFraction is the fraction of lease duration to renew the lease nodeLeaseRenewIntervalFraction = 0.25 // instrumentationScope is the name of OpenTelemetry instrumentation scope instrumentationScope = \u0026#34;k8s.io/kubernetes/pkg/kubelet\u0026#34; ) //这段代码定义了一系列常量，用于配置和控制Kubernetes节点上的Kubelet组件的行为。 //Kubelet负责管理节点上的容器，包括启动、停止、监控和更新容器的状态。 //- maxWaitForContainerRuntime：等待容器运行时启动的最大时间。 //- nodeStatusUpdateRetry：当发布节点状态失败时，kubelet重试的次数。 //- nodeReadyGracePeriod：在快速状态更新终止之前允许的时间，如果容器运行时未准备好，则记录日志而不进行详细保护。 //- DefaultContainerLogsDir：容器日志的默认位置。 - MaxContainerBackOff：容器重启的最大退避期，用于E2E测试。 //- housekeepingPeriod：执行全局清理任务的周期。 //- housekeepingWarningDuration：如果清理工作未能满足清理工作应快速进行的不变性条件（在清理工作运行时，不会启动或删除新的Pod），则会发出警告的时间。 //- runtimeCacheRefreshPeriod：运行时缓存的刷新周期，设置为比清理周期稍长的时间，清理周期显式刷新缓存。 //- evictionMonitoringPeriod：执行驱逐监控的周期，确保与内部cadvisor清理保持同步。 //- linuxEtcHostsPath：在Linux容器的文件系统中，主机文件被挂载的路径。 //- windowsEtcHostsPath：在Windows容器的文件系统中，主机文件被挂载的路径。 //- plegChannelCapacity：接收Pod生命周期事件的通道容量。 //- genericPlegRelistPeriod：通用PLEG重新列出的周期，用于发现容器事件。 //较长的周期意味着kubelet需要更长的时间来检测容器更改并更新Pod状态。 //另一方面，较短的周期会导致更频繁的重新列出（例如，容器运行时操作），导致CPU使用率更高。 - //genericPlegRelistThreshold：通用PLEG重新列出的阈值。 //- eventedPlegRelistPeriod：使用Evented PLEG时的重新列出周期。 //- eventedPlegRelistThreshold：使用Evented PLEG时的重新列出阈值。 //- eventedPlegMaxStreamRetries：使用Evented PLEG时的最大流重试次数。 //- backOffPeriod：当Pod同步结果出现错误时的退避周期。它还用作容器重启和图像拉取的指数退避的基周期。 //- ContainerGCPeriod：执行容器垃圾收集的周期。 //- ImageGCPeriod：执行图像垃圾收集的周期。 //- minDeadContainerInPod：在Pod中保留的最少死亡容器数。 //- nodeLeaseRenewIntervalFraction：租约更新间隔的分数，为租约持续时间的四分之一。 //- instrumentationScope：OpenTelemetry仪器作用域的名称。 var ( // ContainerLogsDir can be overwritten for testing usage ContainerLogsDir = DefaultContainerLogsDir etcHostsPath = getContainerEtcHostsPath() ) //这段Go代码定义了两个变量： //1. ContainerLogsDir：用于存储容器日志的目录路径。它被初始化为 DefaultContainerLogsDir 的值。在测试场景下，可以重写该变量。 //2. etcHostsPath：表示容器中 hosts 文件的路径。它通过调用 getContainerEtcHostsPath() 函数进行初始化。 //这段代码的主要作用是配置容器日志目录和 hosts 文件路径，方便后续代码使用。 func getContainerEtcHostsPath() string { if sysruntime.GOOS == \u0026#34;windows\u0026#34; { return windowsEtcHostsPath } return linuxEtcHostsPath } //该函数用于获取容器的etc/hosts文件路径。 //根据操作系统的不同，返回不同的路径。 //如果操作系统是Windows，则返回windowsEtcHostsPath；否则返回linuxEtcHostsPath。 // SyncHandler is an interface implemented by Kubelet, for testability type SyncHandler interface { HandlePodAdditions(pods []*v1.Pod) HandlePodUpdates(pods []*v1.Pod) HandlePodRemoves(pods []*v1.Pod) HandlePodReconcile(pods []*v1.Pod) HandlePodSyncs(pods []*v1.Pod) HandlePodCleanups(ctx context.Context) error } //这是一个定义了多个处理Pod操作的方法的接口，用于Kubelet的测试。 //其中的方法包括处理Pod的添加、更新、移除、协调和同步，以及Pod的清理操作。 //这些方法分别接受一个Pod列表或上下文作为参数，并可能返回一个错误。 // Option is a functional option type for Kubelet type Option func(*Kubelet) //该函数是一个类型为Option的函数，参数为指向Kubelet类型的指针，并且没有返回值。Option类型定义了为Kubelet提供功能选项的方法。 // Bootstrap is a bootstrapping interface for kubelet, targets the initialization protocol type Bootstrap interface { GetConfiguration() kubeletconfiginternal.KubeletConfiguration BirthCry() StartGarbageCollection() ListenAndServe(kubeCfg *kubeletconfiginternal.KubeletConfiguration, tlsOptions *server.TLSOptions, auth server.AuthInterface, tp trace.TracerProvider) ListenAndServeReadOnly(address net.IP, port uint) ListenAndServePodResources() Run(\u0026lt;-chan kubetypes.PodUpdate) RunOnce(\u0026lt;-chan kubetypes.PodUpdate) ([]RunPodResult, error) } //这个Go代码定义了一个名为Bootstrap的接口，用于Kubelet的初始化协议。该接口包含以下方法： //1. GetConfiguration()：返回KubeletConfiguration对象，表示Kubelet的配置。 //2. BirthCry()：进行初始化时的“出生哭泣”操作，可能用于一些初始化的日志记录或资源申请等。 //3. StartGarbageCollection()：启动垃圾回收机制，用于管理Kubelet运行时的资源清理。 //4. ListenAndServe()：根据提供的Kubelet配置、TLS选项、认证接口和跟踪提供器，启动Kubelet的服务并进行监听。 //5. ListenAndServeReadOnly()：在指定的IP地址和端口上启动只读服务，用于提供Kubelet的只读API。 //6. ListenAndServePodResources()：启动一个服务来监听和处理Pod资源相关请求。 //7. Run()：根据传入的Pod更新通道，持续运行Kubelet，处理Pod的生命周期事件。 //8. RunOnce()：根据传入的Pod更新通道，一次性运行Kubelet，处理Pod的生命周期事件，并返回运行结果。 //这些方法涵盖了Kubelet初始化、配置、垃圾回收、服务监听和Pod管理等方面的功能。 // Dependencies is a bin for things we might consider \u0026#34;injected dependencies\u0026#34; -- objects constructed // at runtime that are necessary for running the Kubelet. This is a temporary solution for grouping // these objects while we figure out a more comprehensive dependency injection story for the Kubelet. type Dependencies struct { Options []Option // Injected Dependencies Auth server.AuthInterface CAdvisorInterface cadvisor.Interface Cloud cloudprovider.Interface ContainerManager cm.ContainerManager EventClient v1core.EventsGetter HeartbeatClient clientset.Interface OnHeartbeatFailure func() KubeClient clientset.Interface Mounter mount.Interface HostUtil hostutil.HostUtils OOMAdjuster *oom.OOMAdjuster OSInterface kubecontainer.OSInterface PodConfig *config.PodConfig ProbeManager prober.Manager Recorder record.EventRecorder Subpather subpath.Interface TracerProvider trace.TracerProvider VolumePlugins []volume.VolumePlugin DynamicPluginProber volume.DynamicPluginProber TLSOptions *server.TLSOptions RemoteRuntimeService internalapi.RuntimeService RemoteImageService internalapi.ImageManagerService PodStartupLatencyTracker util.PodStartupLatencyTracker NodeStartupLatencyTracker util.NodeStartupLatencyTracker // remove it after cadvisor.UsingLegacyCadvisorStats dropped. useLegacyCadvisorStats bool } //该Go代码定义了一个名为Dependencies的结构体，用于存储Kubelet运行时所需的各种依赖项。 //这些依赖项包括各种接口、对象和配置， //例如： //- Auth：认证接口 //- CADvisorInterface：CADvisor接口 //- Cloud：云提供商接口 //- ContainerManager：容器管理器 //- EventClient：事件客户端 //- HeartbeatClient：心跳客户端 //- OnHeartbeatFailure：心跳失败时的回调函数 //- KubeClient：Kubernetes客户端 //- Mounter：挂载器 //- HostUtil：主机工具 //- OOMAdjuster：OOM调整器 //- OSInterface：操作系统接口 //- PodConfig：Pod配置 //- ProbeManager：探针管理器 //- Recorder：事件记录器 //- Subpather：子路径接口 //- TracerProvider：跟踪器提供者 //- VolumePlugins：卷插件列表 //- DynamicPluginProber：动态插件探针 //- TLSOptions：TLS选项 //- RemoteRuntimeService：远程运行时服务 //- RemoteImageService：远程镜像服务 //- PodStartupLatencyTracker：Pod启动延迟跟踪器 //- NodeStartupLatencyTracker：节点启动延迟跟踪器 //- useLegacyCadvisorStats：是否使用传统CADvisor统计信息的标志 //这些依赖项通过结构体成员变量的方式进行定义和存储，其中一些成员变量还包含了函数类型的变量。 //这个结构体的作用是为了临时解决依赖注入的问题，以便于管理和组织Kubelet的依赖项。 // makePodSourceConfig creates a config.PodConfig from the given // KubeletConfiguration or returns an error. func makePodSourceConfig(kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *Dependencies, nodeName types.NodeName, nodeHasSynced func() bool) (*config.PodConfig, error) { manifestURLHeader := make(http.Header) if len(kubeCfg.StaticPodURLHeader) \u0026gt; 0 { for k, v := range kubeCfg.StaticPodURLHeader { for i := range v { manifestURLHeader.Add(k, v[i]) } } } //该函数根据给定的KubeletConfiguration，Dependencies，NodeName和nodeHasSynced函数创建一个config.PodConfig对象或返回错误。 //函数首先创建一个空的http.Header对象用于存储静态Pod URL的头部信息， //然后根据kubeCfg.StaticPodURLHeader的值向manifestURLHeader中添加头部信息。 //最后，函数返回一个新创建的config.PodConfig对象和可能的错误。 // source of all configuration cfg := config.NewPodConfig(config.PodConfigNotificationIncremental, kubeDeps.Recorder, kubeDeps.PodStartupLatencyTracker) //这个函数是配置管理的一部分，它创建了一个包含了所有配置信息的cfg对象。 //这个对象会被用来管理pod的配置信息，包括增量配置通知、事件记录器和Pod启动延迟追踪器。 // TODO: it needs to be replaced by a proper context in the future ctx := context.TODO() // define file config source if kubeCfg.StaticPodPath != \u0026#34;\u0026#34; { klog.InfoS(\u0026#34;Adding static pod path\u0026#34;, \u0026#34;path\u0026#34;, kubeCfg.StaticPodPath) config.NewSourceFile(kubeCfg.StaticPodPath, nodeName, kubeCfg.FileCheckFrequency.Duration, cfg.Channel(ctx, kubetypes.FileSource)) } //这段Go代码中，首先创建了一个context.TODO()上下文，之后根据kubeCfg.StaticPodPath的值，如果其不为空， //则通过config.NewSourceFile()函数将静态Pod路径添加到配置源中，并设置了节点名和文件检查频率。 // define url config source if kubeCfg.StaticPodURL != \u0026#34;\u0026#34; { klog.InfoS(\u0026#34;Adding pod URL with HTTP header\u0026#34;, \u0026#34;URL\u0026#34;, kubeCfg.StaticPodURL, \u0026#34;header\u0026#34;, manifestURLHeader) config.NewSourceURL(kubeCfg.StaticPodURL, manifestURLHeader, nodeName, kubeCfg.HTTPCheckFrequency.Duration, cfg.Channel(ctx, kubetypes.HTTPSource)) } //这段Go代码是在配置Kubernetes集群时，用于添加静态Pod的URL地址作为配置源的代码片段。 //具体功能如下： //1. 首先，代码会判断kubeCfg.StaticPodURL是否为空，如果不为空，则执行下面的操作。 //2. 通过klog.InfoS打印日志信息，记录正在添加的Pod URL地址以及相关的HTTP头信息。 //3. 调用config.NewSourceURL函数，将静态Pod的URL地址、HTTP头信息、节点名称、HTTP检查频率和配置通道等参数传入，创建一个新的URL配置源。 //4. 通过cfg.Channel获取配置通道，并将其作为参数传递给config.NewSourceURL函数。 //这段代码的主要作用是在Kubernetes集群中添加静态Pod的URL地址作为配置源，以便集群能够从该URL地址获取Pod的配置信息。 if kubeDeps.KubeClient != nil { klog.InfoS(\u0026#34;Adding apiserver pod source\u0026#34;) config.NewSourceApiserver(kubeDeps.KubeClient, nodeName, nodeHasSynced, cfg.Channel(ctx, kubetypes.ApiserverSource)) } return cfg, nil } //这段Go代码是在一个函数中的if语句块，主要功能是根据条件判断是否需要添加一个名为“apiserver pod source”的源。 //- 首先，它会检查kubeDeps.KubeClient是否为nil，如果不为nil，则执行下面的代码。 //- 接着，通过klog.InfoS打印一条日志信息，表示正在添加“apiserver pod source”。 //- 然后，调用config.NewSourceApiserver函数，传入kubeDeps.KubeClient、nodeName、nodeHasSynced以及cfg.Channel(ctx, kubetypes.ApiserverSource)作为参数， //来创建并添加这个源。 //- 最后，函数返回cfg和nil。 总结：这段代码的功能是在满足一定条件时，向某个配置中添加一个名为“apiserver pod source”的源。 // PreInitRuntimeService will init runtime service before RunKubelet. func PreInitRuntimeService(kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *Dependencies) error { remoteImageEndpoint := kubeCfg.ImageServiceEndpoint if remoteImageEndpoint == \u0026#34;\u0026#34; \u0026amp;\u0026amp; kubeCfg.ContainerRuntimeEndpoint != \u0026#34;\u0026#34; { remoteImageEndpoint = kubeCfg.ContainerRuntimeEndpoint } var err error if kubeDeps.RemoteRuntimeService, err = remote.NewRemoteRuntimeService(kubeCfg.ContainerRuntimeEndpoint, kubeCfg.RuntimeRequestTimeout.Duration, kubeDeps.TracerProvider); err != nil { return err } if kubeDeps.RemoteImageService, err = remote.NewRemoteImageService(remoteImageEndpoint, kubeCfg.RuntimeRequestTimeout.Duration, kubeDeps.TracerProvider); err != nil { return err } kubeDeps.useLegacyCadvisorStats = cadvisor.UsingLegacyCadvisorStats(kubeCfg.ContainerRuntimeEndpoint) return nil } //该函数在运行Kubelet之前初始化运行时服务。 //- 首先，它将根据kubeCfg中的配置确定远程镜像服务的端点remoteImageEndpoint。 //- 如果remoteImageEndpoint为空且kubeCfg.ContainerRuntimeEndpoint不为空， //则将kubeCfg.ContainerRuntimeEndpoint赋值给remoteImageEndpoint。 //- 然后，尝试创建远程运行时服务kubeDeps.RemoteRuntimeService，如果创建失败则返回错误。 //- 接着，尝试创建远程镜像服务kubeDeps.RemoteImageService，如果创建失败则返回错误。 //- 最后，根据kubeCfg.ContainerRuntimeEndpoint确定是否使用传统的cadvisor统计信息，并将结果赋值给kubeDeps.useLegacyCadvisorStats。 //如果在创建服务时发生错误，函数将返回错误。否则，返回nil表示成功。 // NewMainKubelet instantiates a new Kubelet object along with all the required internal modules. // No initialization of Kubelet and its modules should happen here. func NewMainKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *Dependencies, crOptions *config.ContainerRuntimeOptions, hostname string, hostnameOverridden bool, nodeName types.NodeName, nodeIPs []net.IP, providerID string, cloudProvider string, certDirectory string, rootDirectory string, podLogsDirectory string, imageCredentialProviderConfigFile string, imageCredentialProviderBinDir string, registerNode bool, registerWithTaints []v1.Taint, allowedUnsafeSysctls []string, experimentalMounterPath string, kernelMemcgNotification bool, experimentalNodeAllocatableIgnoreEvictionThreshold bool, minimumGCAge metav1.Duration, maxPerPodContainerCount int32, maxContainerCount int32, registerSchedulable bool, keepTerminatedPodVolumes bool, nodeLabels map[string]string, nodeStatusMaxImages int32, seccompDefault bool, ) (*Kubelet, error) { ctx := context.Background() logger := klog.TODO() //该函数用于创建一个包含所有必需内部模块的新Kubelet对象。 //函数参数包括Kubelet配置、Kubelet依赖项、容器运行时选项、主机名、是否覆盖主机名、节点名、节点IP地址、提供商ID、云提供商、证书目录、根目录、 //Pod日志目录、镜像凭证提供程序配置文件、镜像凭证提供程序二进制目录、是否注册节点、注册时的污点、允许的不安全Sysctls、实验性挂载器路径、 //内核Memcg通知、实验性节点可分配忽略驱逐阈值、最小GC年龄、每个Pod的最大容器数、最大容器数、是否注册可调度、是否保留已终止的Pod卷、节点标签、 //节点状态最大图像数、seccomp默认设置等。 //函数内部会根据传入的参数创建相应的对象和模块，并进行一些初始化操作。 //最后会返回创建的Kubelet对象和可能的错误。 if rootDirectory == \u0026#34;\u0026#34; { return nil, fmt.Errorf(\u0026#34;invalid root directory %q\u0026#34;, rootDirectory) } if podLogsDirectory == \u0026#34;\u0026#34; { return nil, errors.New(\u0026#34;pod logs root directory is empty\u0026#34;) } if kubeCfg.SyncFrequency.Duration \u0026lt;= 0 { return nil, fmt.Errorf(\u0026#34;invalid sync frequency %d\u0026#34;, kubeCfg.SyncFrequency.Duration) } if utilfeature.DefaultFeatureGate.Enabled(features.DisableCloudProviders) \u0026amp;\u0026amp; cloudprovider.IsDeprecatedInternal(cloudProvider) { cloudprovider.DisableWarningForProvider(cloudProvider) return nil, fmt.Errorf(\u0026#34;cloud provider %q was specified, but built-in cloud providers are disabled. Please set --cloud-provider=external and migrate to an external cloud provider\u0026#34;, cloudProvider) } //该函数主要进行一系列的参数校验，如果参数不符合要求则返回相应的错误信息。 //1. 校验rootDirectory是否为空，若为空则返回invalid root directory错误信息。 //2. 校验podLogsDirectory是否为空，若为空则返回pod logs root directory is empty错误信息。 //3. 校验kubeCfg.SyncFrequency.Duration是否大于0，若不大于0则返回invalid sync frequency错误信息。 //4. 若utilfeature.DefaultFeatureGate.Enabled(features.DisableCloudProviders)为true且cloudprovider.IsDeprecatedInternal(cloudProvider)为true， //则返回cloud provider指定但内置云提供商被禁用的错误信息。 //并调用cloudprovider.DisableWarningForProvider(cloudProvider)方法。 var nodeHasSynced cache.InformerSynced var nodeLister corelisters.NodeLister // If kubeClient == nil, we are running in standalone mode (i.e. no API servers) // If not nil, we are running as part of a cluster and should sync w/API if kubeDeps.KubeClient != nil { kubeInformers := informers.NewSharedInformerFactoryWithOptions(kubeDeps.KubeClient, 0, informers.WithTweakListOptions(func(options *metav1.ListOptions) { options.FieldSelector = fields.Set{metav1.ObjectNameField: string(nodeName)}.String() })) nodeLister = kubeInformers.Core().V1().Nodes().Lister() nodeHasSynced = func() bool { return kubeInformers.Core().V1().Nodes().Informer().HasSynced() } kubeInformers.Start(wait.NeverStop) klog.InfoS(\u0026#34;Attempting to sync node with API server\u0026#34;) } else { // we don\u0026#39;t have a client to sync! nodeIndexer := cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers{}) nodeLister = corelisters.NewNodeLister(nodeIndexer) nodeHasSynced = func() bool { return true } klog.InfoS(\u0026#34;Kubelet is running in standalone mode, will skip API server sync\u0026#34;) } //这段Go代码是用于配置和启动一个节点同步器的。 //首先，定义了nodeHasSynced和nodeLister两个全局变量，分别用于检查节点是否已经与API服务器同步，并获取节点列表。 //接着，通过条件语句判断是否是在集群中运行。 //如果是，在集群模式下，会使用kubeDeps.KubeClient创建一个kubeInformers共享informers工厂，并为节点资源创建一个自定义的listers和synced函数。 //其中，通过WithTweakListOptions函数设置了节点的字段选择器，仅选择名称为nodeName的节点。 //然后，启动kubeInformers并记录日志，尝试将节点与API服务器同步。 //如果不在集群中运行，则不会进行节点同步操作。 //相反，它将创建一个独立的节点索引器，并使用该索引器创建一个nodeLister，并设置nodeHasSynced函数始终返回true，表示节点已经同步。 //这段代码是节点同步器的一部分，用于根据运行模式配置和启动节点同步器，以保持节点信息的最新状态。 if kubeDeps.PodConfig == nil { var err error kubeDeps.PodConfig, err = makePodSourceConfig(kubeCfg, kubeDeps, nodeName, nodeHasSynced) if err != nil { return nil, err } } containerGCPolicy := kubecontainer.GCPolicy{ MinAge: minimumGCAge.Duration, MaxPerPodContainer: int(maxPerPodContainerCount), MaxContainers: int(maxContainerCount), } //这段Go代码主要功能是根据条件判断来初始化kubeDeps.PodConfig变量，并设置containerGCPolicy。 //首先，通过条件判断检查kubeDeps.PodConfig是否为nil， //如果是，则调用makePodSourceConfig函数来创建并初始化kubeDeps.PodConfig，同时捕获可能的错误并返回。 //接下来，代码创建一个containerGCPolicy结构体实例，并设置其中的字段值。 //其中，MinAge字段表示容器的最小存活时间，MaxPerPodContainer字段表示每个Pod中最多保留的容器个数， //MaxContainers字段表示节点上最多保留的容器个数。 //这段代码的主要目的是在初始化kubeDeps.PodConfig后，设置容器的垃圾回收策略。 daemonEndpoints := \u0026amp;v1.NodeDaemonEndpoints{ KubeletEndpoint: v1.DaemonEndpoint{Port: kubeCfg.Port}, } imageGCPolicy := images.ImageGCPolicy{ MinAge: kubeCfg.ImageMinimumGCAge.Duration, HighThresholdPercent: int(kubeCfg.ImageGCHighThresholdPercent), LowThresholdPercent: int(kubeCfg.ImageGCLowThresholdPercent), } //这段代码中定义了两个结构体实例。 //首先，通过v1.NodeDaemonEndpoints结构体创建了一个名为daemonEndpoints的实例， //其中KubeletEndpoint的Port字段被初始化为kubeCfg.Port的值。 //然后，通过images.ImageGCPolicy结构体创建了一个名为imageGCPolicy的实例， //其中MinAge字段被初始化为kubeCfg.ImageMinimumGCAge.Duration的值， //HighThresholdPercent字段被初始化为kubeCfg.ImageGCHighThresholdPercent的值转换为int类型， //LowThresholdPercent字段被初始化为kubeCfg.ImageGCLowThresholdPercent的值转换为int类型。 if utilfeature.DefaultFeatureGate.Enabled(features.ImageMaximumGCAge) { imageGCPolicy.MaxAge = kubeCfg.ImageMaximumGCAge.Duration } else if kubeCfg.ImageMaximumGCAge.Duration != 0 { klog.InfoS(\u0026#34;ImageMaximumGCAge flag enabled, but corresponding feature gate is not enabled. Ignoring flag.\u0026#34;) } //这段Go代码主要根据功能门控和配置来设置镜像最大年龄策略。 //如果默认功能门控中启用了ImageMaximumGCAge，则将imageGCPolicy.MaxAge设置为kubeCfg.ImageMaximumGCAge.Duration； //否则，如果kubeCfg.ImageMaximumGCAge.Duration不为0，则记录一条信息表示忽略了该标志。 enforceNodeAllocatable := kubeCfg.EnforceNodeAllocatable if experimentalNodeAllocatableIgnoreEvictionThreshold { // Do not provide kubeCfg.EnforceNodeAllocatable to eviction threshold parsing if we are not enforcing Evictions enforceNodeAllocatable = []string{} } thresholds, err := eviction.ParseThresholdConfig(enforceNodeAllocatable, kubeCfg.EvictionHard, kubeCfg.EvictionSoft, kubeCfg.EvictionSoftGracePeriod, kubeCfg.EvictionMinimumReclaim) if err != nil { return nil, err } evictionConfig := eviction.Config{ PressureTransitionPeriod: kubeCfg.EvictionPressureTransitionPeriod.Duration, MaxPodGracePeriodSeconds: int64(kubeCfg.EvictionMaxPodGracePeriod), Thresholds: thresholds, KernelMemcgNotification: kernelMemcgNotification, PodCgroupRoot: kubeDeps.ContainerManager.GetPodCgroupRoot(), } //这个Go函数主要功能是配置和返回一个关于Kubernetes节点上资源驱逐的配置信息。具体步骤如下： //1. 将kubeCfg.EnforceNodeAllocatable的值赋给enforceNodeAllocatable变量。 //2. 如果experimentalNodeAllocatableIgnoreEvictionThreshold为true，则将enforceNodeAllocatable设置为空字符串数组。 //3. 调用eviction.ParseThresholdConfig函数解析驱逐阈值配置， //参数包括enforceNodeAllocatable、kubeCfg.EvictionHard、kubeCfg.EvictionSoft、kubeCfg.EvictionSoftGracePeriod和kubeCfg.EvictionMinimumReclaim。 //如果解析出错，函数会返回nil和错误信息。 //4. 创建并返回一个eviction.Config结构体实例，其中包括压力转换期、最大Pod优雅终止时间、驱逐阈值、内核Memcg通知以及Pod容器组根路径等配置信息。 var serviceLister corelisters.ServiceLister var serviceHasSynced cache.InformerSynced if kubeDeps.KubeClient != nil { kubeInformers := informers.NewSharedInformerFactoryWithOptions(kubeDeps.KubeClient, 0) serviceLister = kubeInformers.Core().V1().Services().Lister() serviceHasSynced = kubeInformers.Core().V1().Services().Informer().HasSynced kubeInformers.Start(wait.NeverStop) } else { serviceIndexer := cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers{cache.NamespaceIndex: cache.MetaNamespaceIndexFunc}) serviceLister = corelisters.NewServiceLister(serviceIndexer) serviceHasSynced = func() bool { return true } } //这段Go代码中定义了一个serviceLister变量和一个serviceHasSynced变量， //它们分别用于列出Kubernetes服务和检查服务是否已同步。 //根据是否设置了kubeDeps.KubeClient，代码通过不同的方式初始化这两个变量。 //如果kubeDeps.KubeClient不为空，代码使用kubeInformers来创建serviceLister和serviceHasSynced； //否则，代码使用cache.NewIndexer创建serviceLister，并将serviceHasSynced设置为始终返回true的函数。 //最后，如果kubeDeps.KubeClient不为空，还调用kubeInformers.Start启动信息处理器。 // construct a node reference used for events nodeRef := \u0026amp;v1.ObjectReference{ Kind: \u0026#34;Node\u0026#34;, Name: string(nodeName), UID: types.UID(nodeName), Namespace: \u0026#34;\u0026#34;, } //这段Go代码创建了一个v1.ObjectReference类型的指针变量nodeRef， //用于表示一个节点引用，常用于事件处理中。其中，节点的名称、UID和命名空间分别被设置为nodeName参数对应的字符串值、 //types.UID转换后的值和空字符串。 oomWatcher, err := oomwatcher.NewWatcher(kubeDeps.Recorder) if err != nil { if inuserns.RunningInUserNS() { if utilfeature.DefaultFeatureGate.Enabled(features.KubeletInUserNamespace) { // oomwatcher.NewWatcher returns \u0026#34;open /dev/kmsg: operation not permitted\u0026#34; error, // when running in a user namespace with sysctl value `kernel.dmesg_restrict=1`. klog.V(2).InfoS(\u0026#34;Failed to create an oomWatcher (running in UserNS, ignoring)\u0026#34;, \u0026#34;err\u0026#34;, err) oomWatcher = nil } else { klog.ErrorS(err, \u0026#34;Failed to create an oomWatcher (running in UserNS, Hint: enable KubeletInUserNamespace feature flag to ignore the error)\u0026#34;) return nil, err } } else { return nil, err } } //这段Go代码的主要功能是尝试创建一个OOM watcher实例，通过oomwatcher.NewWatcher(kubeDeps.Recorder)来实现。 //如果创建失败，会根据当前是否在用户命名空间中运行以及KubeletInUserNamespace特性标志的启用状态来决定是记录错误信息、忽略错误还是返回错误。 //具体逻辑如下： //1. 首先，尝试创建OOM watcher，将返回的实例和可能发生的错误保存在oomWatcher和err变量中。 //2. 如果err不为nil，即创建OOM watcher失败，则会进一步判断是否在用户命名空间中运行。 //3. 如果在用户命名空间中运行，并且KubeletInUserNamespace特性标志已被启用，则记录一条信息，指示由于kernel.dmesg_restrict=1的sysctl值， //运行在用户命名空间中时，oomwatcher.NewWatcher会返回\u0026#34;open /dev/kmsg: operation not permitted\u0026#34;错误，并将oomWatcher设置为nil。 //4. 如果在用户命名空间中运行，但KubeletInUserNamespace特性标志未启用，则记录一条错误信息，指示创建OOM watcher失败， //并建议启用KubeletInUserNamespace特性标志来忽略该错误，然后返回nil和错误err。 //5. 如果不在用户命名空间中运行，则直接返回nil和错误err。 //6. 如果成功创建OOM watcher（即err为nil），则可以继续后续操作。 clusterDNS := make([]net.IP, 0, len(kubeCfg.ClusterDNS)) for _, ipEntry := range kubeCfg.ClusterDNS { ip := netutils.ParseIPSloppy(ipEntry) if ip == nil { klog.InfoS(\u0026#34;Invalid clusterDNS IP\u0026#34;, \u0026#34;IP\u0026#34;, ipEntry) } else { clusterDNS = append(clusterDNS, ip) } } //该函数用于将kubeCfg.ClusterDNS中的IP地址解析并存储到clusterDNS切片中。 //具体来说，函数首先根据kubeCfg.ClusterDNS的长度初始化一个空的clusterDNS切片， //然后遍历kubeCfg.ClusterDNS中的每个IP地址。 //对于每个IP地址，函数使用netutils.ParseIPSloppy函数进行解析， //如果解析失败，则记录一条日志信息； //如果解析成功，则将解析后的IP地址添加到clusterDNS切片中。 //最终，函数返回存储了有效IP地址的clusterDNS切片。 // A TLS transport is needed to make HTTPS-based container lifecycle requests, // but we do not have the information necessary to do TLS verification. // // This client must not be modified to include credentials, because it is // critical that credentials not leak from the client to arbitrary hosts. insecureContainerLifecycleHTTPClient := \u0026amp;http.Client{ Transport: \u0026amp;http.Transport{ TLSClientConfig: \u0026amp;tls.Config{InsecureSkipVerify: true}, }, CheckRedirect: httpprobe.RedirectChecker(false), } //这段Go代码创建了一个不进行TLS验证的HTTP客户端。该客户端用于进行基于HTTPS的容器生命周期请求。 //由于没有必要的信息来进行TLS验证，因此必须确保该客户端不被修改为包含凭证，以防止凭证泄露到任意主机。 //该客户端的重定向检查器被设置为禁止重定向。 tracer := kubeDeps.TracerProvider.Tracer(instrumentationScope) klet := \u0026amp;Kubelet{ hostname: hostname, hostnameOverridden: hostnameOverridden, nodeName: nodeName, kubeClient: kubeDeps.KubeClient, heartbeatClient: kubeDeps.HeartbeatClient, onRepeatedHeartbeatFailure: kubeDeps.OnHeartbeatFailure, rootDirectory: filepath.Clean(rootDirectory), podLogsDirectory: podLogsDirectory, resyncInterval: kubeCfg.SyncFrequency.Duration, sourcesReady: config.NewSourcesReady(kubeDeps.PodConfig.SeenAllSources), registerNode: registerNode, registerWithTaints: registerWithTaints, registerSchedulable: registerSchedulable, dnsConfigurer: dns.NewConfigurer(kubeDeps.Recorder, nodeRef, nodeIPs, clusterDNS, kubeCfg.ClusterDomain, kubeCfg.ResolverConfig), serviceLister: serviceLister, serviceHasSynced: serviceHasSynced, nodeLister: nodeLister, nodeHasSynced: nodeHasSynced, streamingConnectionIdleTimeout: kubeCfg.StreamingConnectionIdleTimeout.Duration, recorder: kubeDeps.Recorder, cadvisor: kubeDeps.CAdvisorInterface, cloud: kubeDeps.Cloud, externalCloudProvider: cloudprovider.IsExternal(cloudProvider), providerID: providerID, nodeRef: nodeRef, nodeLabels: nodeLabels, nodeStatusUpdateFrequency: kubeCfg.NodeStatusUpdateFrequency.Duration, nodeStatusReportFrequency: kubeCfg.NodeStatusReportFrequency.Duration, os: kubeDeps.OSInterface, oomWatcher: oomWatcher, cgroupsPerQOS: kubeCfg.CgroupsPerQOS, cgroupRoot: kubeCfg.CgroupRoot, mounter: kubeDeps.Mounter, hostutil: kubeDeps.HostUtil, subpather: kubeDeps.Subpather, maxPods: int(kubeCfg.MaxPods), podsPerCore: int(kubeCfg.PodsPerCore), syncLoopMonitor: atomic.Value{}, daemonEndpoints: daemonEndpoints, containerManager: kubeDeps.ContainerManager, nodeIPs: nodeIPs, nodeIPValidator: validateNodeIP, clock: clock.RealClock{}, enableControllerAttachDetach: kubeCfg.EnableControllerAttachDetach, makeIPTablesUtilChains: kubeCfg.MakeIPTablesUtilChains, keepTerminatedPodVolumes: keepTerminatedPodVolumes, nodeStatusMaxImages: nodeStatusMaxImages, tracer: tracer, nodeStartupLatencyTracker: kubeDeps.NodeStartupLatencyTracker, } //这段Go代码初始化了一个Kubelet对象。 //Kubelet是Kubernetes集群中的一个核心组件，负责管理节点上的Pods。 //该对象的属性包括节点的主机名、是否被覆盖、节点名、Kubernetes客户端、心跳客户端、重复心跳失败的处理函数、根目录、Pod日志目录、同步频率、 //是否准备好源、注册节点函数、是否带污点注册节点、是否注册为可调度节点、DNS配置器、服务列表器、服务是否已同步、节点列表器、节点是否已同步、 //流连接空闲超时时间、记录器、cadvisor接口、云提供者、外部云提供者标志、提供者ID、节点引用、节点标签、节点状态更新频率、节点状态报告频率、 //操作系统接口、OOM观察者、是否支持QoS级别的cgroups、cgroup根目录、安装器、主机工具、子路径处理工具、最大Pod数、每核Pod数、同步循环监视器、 //守护进程端点、容器管理器、节点IPs、节点IP验证器、时钟、是否启用控制器附加/分离、是否创建iptables工具链、是否保留已终止的Pod卷、 //节点状态最大图片数、跟踪器和节点启动延迟追踪器。 //综上所述，这段代码主要用于初始化一个Kubelet对象，以便在Kubernetes集群中管理节点上的Pods。 if klet.cloud != nil { klet.cloudResourceSyncManager = cloudresource.NewSyncManager(klet.cloud, nodeName, klet.nodeStatusUpdateFrequency) } var secretManager secret.Manager var configMapManager configmap.Manager if klet.kubeClient != nil { switch kubeCfg.ConfigMapAndSecretChangeDetectionStrategy { case kubeletconfiginternal.WatchChangeDetectionStrategy: secretManager = secret.NewWatchingSecretManager(klet.kubeClient, klet.resyncInterval) configMapManager = configmap.NewWatchingConfigMapManager(klet.kubeClient, klet.resyncInterval) case kubeletconfiginternal.TTLCacheChangeDetectionStrategy: secretManager = secret.NewCachingSecretManager( klet.kubeClient, manager.GetObjectTTLFromNodeFunc(klet.GetNode)) configMapManager = configmap.NewCachingConfigMapManager( klet.kubeClient, manager.GetObjectTTLFromNodeFunc(klet.GetNode)) case kubeletconfiginternal.GetChangeDetectionStrategy: secretManager = secret.NewSimpleSecretManager(klet.kubeClient) configMapManager = configmap.NewSimpleConfigMapManager(klet.kubeClient) default: return nil, fmt.Errorf(\u0026#34;unknown configmap and secret manager mode: %v\u0026#34;, kubeCfg.ConfigMapAndSecretChangeDetectionStrategy) } //这段Go代码根据不同的条件初始化了不同的secretManager和configMapManager。 //首先，如果klet.cloud不为空，则创建一个新的cloudResourceSyncManager。 //接下来，根据klet.kubeClient和kubeCfg.ConfigMapAndSecretChangeDetectionStrategy的值， //分别初始化secretManager和configMapManager。 //- 如果kubeCfg.ConfigMapAndSecretChangeDetectionStrategy的值是kubeletconfiginternal.WatchChangeDetectionStrategy， //则分别创建一个watchingSecretManager和watchingConfigMapManager。 //- 如果kubeCfg.ConfigMapAndSecretChangeDetectionStrategy的值是kubeletconfiginternal.TTLCacheChangeDetectionStrategy， //则分别创建一个cachingSecretManager和cachingConfigMapManager。 //- 如果kubeCfg.ConfigMapAndSecretChangeDetectionStrategy的值是kubeletconfiginternal.GetChangeDetectionStrategy， //则分别创建一个simpleSecretManager和simpleConfigMapManager。 //- 如果kubeCfg.ConfigMapAndSecretChangeDetectionStrategy的值不是以上三种情况，则返回一个错误。 //最后，返回secretManager和configMapManager。 klet.secretManager = secretManager klet.configMapManager = configMapManager //这个代码片段是Go语言中的简单赋值语句。它将secretManager赋值给klet.secretManager， //将configMapManager赋值给klet.configMapManager。这两个操作将使得klet对象能够管理秘密和配置映射。 } machineInfo, err := klet.cadvisor.MachineInfo() if err != nil { return nil, err } // Avoid collector collects it as a timestamped metric // See PR #95210 and #97006 for more details. machineInfo.Timestamp = time.Time{} klet.setCachedMachineInfo(machineInfo) imageBackOff := flowcontrol.NewBackOff(backOffPeriod, MaxContainerBackOff) klet.livenessManager = proberesults.NewManager() klet.readinessManager = proberesults.NewManager() klet.startupManager = proberesults.NewManager() klet.podCache = kubecontainer.NewCache() klet.mirrorPodClient = kubepod.NewBasicMirrorClient(klet.kubeClient, string(nodeName), nodeLister) klet.podManager = kubepod.NewBasicPodManager() klet.statusManager = status.NewManager(klet.kubeClient, klet.podManager, klet, kubeDeps.PodStartupLatencyTracker, klet.getRootDir()) klet.resourceAnalyzer = serverstats.NewResourceAnalyzer(klet, kubeCfg.VolumeStatsAggPeriod.Duration, kubeDeps.Recorder) klet.runtimeService = kubeDeps.RemoteRuntimeService //该Go函数主要是进行初始化操作。 //1. 首先通过cadvisor获取机器信息，并设置Timestamp为time.Time{}避免被收集为时间戳指标。 //2. 初始化imageBackOff（容器启动失败时的退避策略）。 //3. 创建并初始化livenessManager、readinessManager、startupManager（用于管理容器的存活、就绪和启动状态）。 //4. 创建并初始化podCache（用于缓存Pod信息）。 //5. 创建并初始化mirrorPodClient和podManager（用于管理镜像Pod和Pod）。 //6. 创建并初始化statusManager（用于管理Pod的状态）。 //7. 创建并初始化resourceAnalyzer（用于分析资源使用情况）。 //8. 将runtimeService设置为kubeDeps.RemoteRuntimeService。 //这个函数主要是为了初始化kubelet的各种组件和管理器，并设置相应的参数和配置。 if kubeDeps.KubeClient != nil { klet.runtimeClassManager = runtimeclass.NewManager(kubeDeps.KubeClient) } // setup containerLogManager for CRI container runtime containerLogManager, err := logs.NewContainerLogManager( klet.runtimeService, kubeDeps.OSInterface, kubeCfg.ContainerLogMaxSize, int(kubeCfg.ContainerLogMaxFiles), int(kubeCfg.ContainerLogMaxWorkers), kubeCfg.ContainerLogMonitorInterval, ) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to initialize container log manager: %v\u0026#34;, err) } klet.containerLogManager = containerLogManager //这段Go代码主要执行了两个步骤： //1. 若kubeDeps.KubeClient不为空，则创建一个runtimeclass.Manager实例，并将其赋值给klet.runtimeClassManager。 //2. 创建一个logs.ContainerLogManager实例，并将其赋值给klet.containerLogManager。 //此实例用于管理容器日志，包括日志文件的最大大小、最大文件数、最大工作线程数等配置。若创建过程中出现错误，则返回错误信息。 //总结：这段代码主要负责初始化两个管理器：runtimeclass.Manager和logs.ContainerLogManager，用于管理运行时类和容器日志。 klet.reasonCache = NewReasonCache() klet.workQueue = queue.NewBasicWorkQueue(klet.clock) klet.podWorkers = newPodWorkers( klet, kubeDeps.Recorder, klet.workQueue, klet.resyncInterval, backOffPeriod, klet.podCache, ) //这段代码是Go语言中的函数调用和赋值语句，主要进行了三个对象的初始化和赋值操作。 //- 首先，通过NewReasonCache()函数创建了一个reasonCache对象，并将其赋值给klet.reasonCache。 //- 然后，通过queue.NewBasicWorkQueue(klet.clock)函数创建了一个workQueue对象，并将其赋值给klet.workQueue。 //- 最后，通过newPodWorkers()函数创建了一个podWorkers对象，并将其赋值给klet.podWorkers。newPodWorkers()函数的参数包括klet对象、 //Recorder对象、workQueue对象、resyncInterval时间间隔、backOffPeriod时间间隔、podCache对象等 runtime, err := kuberuntime.NewKubeGenericRuntimeManager( kubecontainer.FilterEventRecorder(kubeDeps.Recorder), klet.livenessManager, klet.readinessManager, klet.startupManager, rootDirectory, podLogsDirectory, machineInfo, klet.podWorkers, kubeDeps.OSInterface, klet, insecureContainerLifecycleHTTPClient, imageBackOff, kubeCfg.SerializeImagePulls, kubeCfg.MaxParallelImagePulls, float32(kubeCfg.RegistryPullQPS), int(kubeCfg.RegistryBurst), imageCredentialProviderConfigFile, imageCredentialProviderBinDir, kubeCfg.CPUCFSQuota, kubeCfg.CPUCFSQuotaPeriod, kubeDeps.RemoteRuntimeService, kubeDeps.RemoteImageService, kubeDeps.ContainerManager, klet.containerLogManager, klet.runtimeClassManager, seccompDefault, kubeCfg.MemorySwap.SwapBehavior, kubeDeps.ContainerManager.GetNodeAllocatableAbsolute, *kubeCfg.MemoryThrottlingFactor, kubeDeps.PodStartupLatencyTracker, kubeDeps.TracerProvider, ) if err != nil { return nil, err } klet.containerRuntime = runtime klet.streamingRuntime = runtime klet.runner = runtime //这个函数主要用于创建并初始化一个KubeGenericRuntimeManager对象， //它是kubernetes中的一个容器运行时管理器，主要负责管理容器的生命周期。 //函数中使用了许多参数，它们主要用于配置运行时管理器的各种属性，例如日志目录、镜像拉取策略、CPU和内存限制等。 //函数返回一个runtime对象，它包含了运行时管理器的各种接口，如启动容器、停止容器等。 //最后，将runtime对象赋值给klet结构体的相应字段，以便在后续代码中使用。 runtimeCache, err := kubecontainer.NewRuntimeCache(klet.containerRuntime, runtimeCacheRefreshPeriod) if err != nil { return nil, err } klet.runtimeCache = runtimeCache // common provider to get host file system usage associated with a pod managed by kubelet hostStatsProvider := stats.NewHostStatsProvider(kubecontainer.RealOS{}, func(podUID types.UID) string { return getEtcHostsPath(klet.getPodDir(podUID)) }, podLogsDirectory) if kubeDeps.useLegacyCadvisorStats { klet.StatsProvider = stats.NewCadvisorStatsProvider( klet.cadvisor, klet.resourceAnalyzer, klet.podManager, klet.runtimeCache, klet.containerRuntime, klet.statusManager, hostStatsProvider) } else { klet.StatsProvider = stats.NewCRIStatsProvider( klet.cadvisor, klet.resourceAnalyzer, klet.podManager, klet.runtimeCache, kubeDeps.RemoteRuntimeService, kubeDeps.RemoteImageService, hostStatsProvider, utilfeature.DefaultFeatureGate.Enabled(features.PodAndContainerStatsFromCRI)) } //该Go函数主要功能是初始化Kubelet的统计提供者。 //首先，函数通过调用kubecontainer.NewRuntimeCache方法创建一个运行时缓存，并将其赋值给runtimeCache变量。 //如果创建过程中出现错误，则会返回nil和错误信息。 //接下来，函数通过调用stats.NewHostStatsProvider方法创建一个主机状态提供者， //用于获取与Kubelet管理的Pod相关的主机文件系统使用情况。 //该方法接收三个参数：一个实现了OS接口的对象（这里使用kubecontainer.RealOS{}）， //一个函数（用于根据Pod的UID获取/etc/hosts的路径），以及Pod日志目录的路径。 //然后，函数根据kubeDeps.useLegacyCadvisorStats的值选择合适的统计提供者。 //如果为true，则调用stats.NewCadvisorStatsProvider方法创建一个基于Cadvisor的统计提供者； //否则，调用stats.NewCRIStatsProvider方法创建一个基于CRI的统计提供者。 //这两个方法都接收多个参数，包括cadvisor接口、resourceAnalyzer接口、podManager接口、runtimeCache对象、containerRuntime接口、 //statusManager接口以及之前创建的hostStatsProvider对象。 //其中，NewCRIStatsProvider方法还额外接收RemoteRuntimeService和RemoteImageService接口，以及一个表示是否启用特定功能的布尔值。 //最后，函数将创建的统计提供者赋值给klet.StatsProvider字段。 eventChannel := make(chan *pleg.PodLifecycleEvent, plegChannelCapacity) if utilfeature.DefaultFeatureGate.Enabled(features.EventedPLEG) { // adjust Generic PLEG relisting period and threshold to higher value when Evented PLEG is turned on genericRelistDuration := \u0026amp;pleg.RelistDuration{ RelistPeriod: eventedPlegRelistPeriod, RelistThreshold: eventedPlegRelistThreshold, } //这段代码中定义了一个名为eventChannel的通道，类型为*pleg.PodLifecycleEvent，并设置了其容量为plegChannelCapacity。 //接着，通过utilfeature.DefaultFeatureGate.Enabled(features.EventedPLEG)判断是否启用了EventedPLEG特性。 //如果启用了，就会调整Generic PLEG的重新列表周期和阈值，将其设置为eventedPlegRelistPeriod和eventedPlegRelistThreshold所指定的值。 //这段代码的功能是根据是否启用了EventedPLEG特性，来调整Generic PLEG的重新列表周期和阈值。 klet.pleg = pleg.NewGenericPLEG(klet.containerRuntime, eventChannel, genericRelistDuration, klet.podCache, clock.RealClock{}) // In case Evented PLEG has to fall back on Generic PLEG due to an error, // Evented PLEG should be able to reset the Generic PLEG relisting duration // to the default value. eventedRelistDuration := \u0026amp;pleg.RelistDuration{ RelistPeriod: genericPlegRelistPeriod, RelistThreshold: genericPlegRelistThreshold, } //这段Go代码是Kubernetes中的一个片段，它创建了一个PLEG(Pod Lifecycle Event Generator)实例，用于监听容器生命周期事件。 //klet.pleg = pleg.NewGenericPLEG(klet.containerRuntime, eventChannel, genericRelistDuration, klet.podCache, //clock.RealClock{})这行代码创建了一个PLEG实例，其中NewGenericPLEG是一个工厂方法，用于根据提供的参数创建PLEG实例。 //这个方法接收5个参数： //1. klet.containerRuntime：容器运行时接口，用于与底层容器引擎交互。 //2. eventChannel：一个通道，用于接收容器事件。 //3. genericRelistDuration：重新列出容器的间隔时间。 //4. klet.podCache：一个缓存，用于存储Pod的信息。 //5. clock.RealClock{}：一个时钟对象，用于提供当前时间。 //第二段代码eventedRelistDuration := \u0026amp;pleg.RelistDuration{RelistPeriod: genericPlegRelistPeriod, //RelistThreshold: genericPlegRelistThreshold}创建了一个RelistDuration结构体实例，用于设置PLEG重新列出容器的周期和阈值。 //其中： //1. RelistPeriod：重新列出容器的时间间隔。 //2. RelistThreshold：在达到此阈值后，PLEG将重新列出容器。 //这段代码的主要目的是创建并配置一个PLEG实例，用于监听容器生命周期事件，并设置重新列出容器的间隔时间和阈值。 klet.eventedPleg, err = pleg.NewEventedPLEG(klet.containerRuntime, klet.runtimeService, eventChannel, klet.podCache, klet.pleg, eventedPlegMaxStreamRetries, eventedRelistDuration, clock.RealClock{}) if err != nil { return nil, err } } else { genericRelistDuration := \u0026amp;pleg.RelistDuration{ RelistPeriod: genericPlegRelistPeriod, RelistThreshold: genericPlegRelistThreshold, } klet.pleg = pleg.NewGenericPLEG(klet.containerRuntime, eventChannel, genericRelistDuration, klet.podCache, clock.RealClock{}) } //这段Go代码中，根据条件创建了两种不同类型的PLEG（Pod Lifecycle Event Generator）对象，并将其赋值给klet.pleg。 //如果某个条件满足，则创建一个EventedPLEG对象，并将klet.eventedPleg指向它。 //NewEventedPLEG函数的参数包括容器运行时、运行时服务、事件通道、Pod缓存、当前的klet.pleg对象、最大流重试次数、重新列出持续时间和实际时钟。 //否则，创建一个GenericPLEG对象，并将klet.pleg指向它。 //NewGenericPLEG函数的参数包括容器运行时、事件通道、重新列出持续时间、Pod缓存和实际时钟。 //这段代码的主要目的是初始化并配置PLEG对象，用于监控和生成Pod生命周期事件。 klet.runtimeState = newRuntimeState(maxWaitForContainerRuntime) klet.runtimeState.addHealthCheck(\u0026#34;PLEG\u0026#34;, klet.pleg.Healthy) if utilfeature.DefaultFeatureGate.Enabled(features.EventedPLEG) { klet.runtimeState.addHealthCheck(\u0026#34;EventedPLEG\u0026#34;, klet.eventedPleg.Healthy) } if _, err := klet.updatePodCIDR(ctx, kubeCfg.PodCIDR); err != nil { klog.ErrorS(err, \u0026#34;Pod CIDR update failed\u0026#34;) } //这段Go代码中的函数功能如下： //- 首先，通过newRuntimeState(maxWaitForContainerRuntime)创建一个新的runtimeState实例，并将其赋值给klet.runtimeState。 //- 接着，调用addHealthCheck方法向runtimeState实例添加了一个名为\u0026#34;PLEG\u0026#34;的健康检查，其健康状态为klet.pleg.Healthy。 //- 如果features.EventedPLEG特性门已启用，则调用addHealthCheck方法向runtimeState实例添加了一个名为\u0026#34;EventedPLEG\u0026#34;的健康检查， //其健康状态为klet.eventedPleg.Healthy。 //- 最后，调用updatePodCIDR方法尝试更新Pod CIDR，如果更新失败则记录错误日志。 //总结：这段代码主要通过runtimeState实例管理容器运行时的状态和健康检查，并尝试更新Pod CIDR，如果更新失败则记录错误日志。 // setup containerGC containerGC, err := kubecontainer.NewContainerGC(klet.containerRuntime, containerGCPolicy, klet.sourcesReady) if err != nil { return nil, err } klet.containerGC = containerGC klet.containerDeletor = newPodContainerDeletor(klet.containerRuntime, max(containerGCPolicy.MaxPerPodContainer, minDeadContainerInPod)) //这段代码主要功能是创建并初始化一个ContainerGC对象， //并将其赋值给klet的containerGC和containerDeletor字段。 //其中，ContainerGC是用来定期清理和回收容器的组件；newPodContainerDeletor是用来删除过期或无效的容器的函数。 //max和minDeadContainerInPod是两个辅助函数，用来计算最大存活容器数和最小死亡容器数。 //如果创建ContainerGC对象失败，则会返回错误。 // setup imageManager imageManager, err := images.NewImageGCManager(klet.containerRuntime, klet.StatsProvider, kubeDeps.Recorder, nodeRef, imageGCPolicy, kubeDeps.TracerProvider) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to initialize image manager: %v\u0026#34;, err) } klet.imageManager = imageManager if kubeCfg.ServerTLSBootstrap \u0026amp;\u0026amp; kubeDeps.TLSOptions != nil \u0026amp;\u0026amp; utilfeature.DefaultFeatureGate.Enabled(features.RotateKubeletServerCertificate) { klet.serverCertificateManager, err = kubeletcertificate.NewKubeletServerCertificateManager(klet.kubeClient, kubeCfg, klet.nodeName, klet.getLastObservedNodeAddresses, certDirectory) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to initialize certificate manager: %v\u0026#34;, err) } kubeDeps.TLSOptions.Config.GetCertificate = func(*tls.ClientHelloInfo) (*tls.Certificate, error) { cert := klet.serverCertificateManager.Current() if cert == nil { return nil, fmt.Errorf(\u0026#34;no serving certificate available for the kubelet\u0026#34;) } return cert, nil } } //这段Go代码主要进行了一系列的初始化操作。 //首先，它初始化了一个名为imageManager的对象，并将其赋值给klet.imageManager。 //这个对象是由images.NewImageGCManager函数创建的，它依赖于containerRuntime、StatsProvider、Recorder、nodeRef、 //imageGCPolicy和TracerProvider等参数。 //接着，代码检查了是否启用了服务器TLS引导和相关的功能门控。 //如果满足条件，它会初始化一个名为serverCertificateManager的对象，并将其赋值给klet.serverCertificateManager。 //这个对象是由kubeletcertificate.NewKubeletServerCertificateManager函数创建的， //它依赖于kubeClient、kubeCfg、nodeName、getLastObservedNodeAddresses和certDirectory等参数。 //最后，代码将serverCertificateManager.Current()方法设置为TLSOptions.Config.GetCertificate的实现。 //这个方法会返回当前有效的TLS证书，如果证书不存在，则会返回一个错误。 //整体上，这段代码主要是为了初始化imageManager和serverCertificateManager两个对象，并设置相关的配置和依赖 if kubeDeps.ProbeManager != nil { klet.probeManager = kubeDeps.ProbeManager } else { klet.probeManager = prober.NewManager( klet.statusManager, klet.livenessManager, klet.readinessManager, klet.startupManager, klet.runner, kubeDeps.Recorder) } //这段Go代码主要实现了根据条件来初始化klet.probeManager变量。 //- 首先，它会检查kubeDeps.ProbeManager是否为nil，如果是非nil，则直接将其赋值给klet.probeManager； //- 如果kubeDeps.ProbeManager为nil，则通过调用prober.NewManager方法来创建一个新的probeManager实例， //并将其赋值给klet.probeManager。 //prober.NewManager方法接受多个参数，包括klet.statusManager、klet.livenessManager、klet.readinessManager、 //klet.startupManager、klet.runner和kubeDeps.Recorder。这些参数用于配置和初始化新的probeManager实例。 //总结一下，这段代码的作用是在kubeDeps.ProbeManager非nil时直接使用它，否则创建一个新的probeManager实例并配置初始化。 tokenManager := token.NewManager(kubeDeps.KubeClient) var clusterTrustBundleManager clustertrustbundle.Manager if kubeDeps.KubeClient != nil \u0026amp;\u0026amp; utilfeature.DefaultFeatureGate.Enabled(features.ClusterTrustBundleProjection) { kubeInformers := informers.NewSharedInformerFactoryWithOptions(kubeDeps.KubeClient, 0) clusterTrustBundleManager, err = clustertrustbundle.NewInformerManager(kubeInformers.Certificates().V1alpha1().ClusterTrustBundles(), 2*int(kubeCfg.MaxPods), 5*time.Minute) if err != nil { return nil, fmt.Errorf(\u0026#34;while starting informer-based ClusterTrustBundle manager: %w\u0026#34;, err) } kubeInformers.Start(wait.NeverStop) klog.InfoS(\u0026#34;Started ClusterTrustBundle informer\u0026#34;) } else { // In static kubelet mode, use a no-op manager. clusterTrustBundleManager = \u0026amp;clustertrustbundle.NoopManager{} klog.InfoS(\u0026#34;Not starting ClusterTrustBundle informer because we are in static kubelet mode\u0026#34;) } //该函数主要创建并初始化一个token.Manager实例和一个clustertrustbundle.Manager实例。 //- 首先，根据kubeDeps.KubeClient创建一个token.Manager实例。 //- 然后，判断kubeDeps.KubeClient是否为空且features.ClusterTrustBundleProjection特性门是否开启， //如果满足条件，则使用kubeDeps.KubeClient创建一个informers.SharedInformerFactory实例， //并根据该实例创建一个clustertrustbundle.InformerManager实例，同时设置clusterTrustBundleManager。 //如果不满足条件，则创建一个clustertrustbundle.NoopManager实例，并设置clusterTrustBundleManager。 //- 最后，如果创建clustertrustbundle.InformerManager实例成功，则启动kubeInformers并记录日志；否则返回错误信息。 //这段代码主要是为了管理集群信任捆绑(token和cluster trust bundle)而设计的。 // NewInitializedVolumePluginMgr initializes some storageErrors on the Kubelet runtimeState (in csi_plugin.go init) // which affects node ready status. This function must be called before Kubelet is initialized so that the Node // ReadyState is accurate with the storage state. klet.volumePluginMgr, err = NewInitializedVolumePluginMgr(klet, secretManager, configMapManager, tokenManager, clusterTrustBundleManager, kubeDeps.VolumePlugins, kubeDeps.DynamicPluginProber) if err != nil { return nil, err } klet.pluginManager = pluginmanager.NewPluginManager( klet.getPluginsRegistrationDir(), /* sockDir */ kubeDeps.Recorder, ) //这段Go代码中的函数是用于初始化Kubelet的卷插件管理器(volumePluginMgr)的。 //- NewInitializedVolumePluginMgr函数会初始化Kubelet运行时状态中的某些存储错误，这将影响节点的就绪状态。 //这个函数必须在Kubelet初始化之前调用，以确保节点的就绪状态与存储状态准确无误。 //- NewInitializedVolumePluginMgr函数的参数包括klet（Kubelet的实例）、secretManager、configMapManager、tokenManager、 //clusterTrustBundleManager、kubeDeps.VolumePlugins和kubeDeps.DynamicPluginProber。 //这些参数用于配置和初始化卷插件管理器。 //- NewInitializedVolumePluginMgr函数返回一个初始化后的卷插件管理器实例和一个错误（如果有）。 //- pluginmanager.NewPluginManager函数用于创建一个新的插件管理器实例。 //- pluginmanager.NewPluginManager函数的参数包括插件注册目录（sockDir）和记录器(kubeDeps.Recorder)。 //- 最后，将创建的卷插件管理器实例赋值给klet.pluginManager。 //这个函数的主要目的是在Kubelet初始化之前，设置和初始化与存储相关的插件管理器，以便节点的就绪状态能准确反映存储状态。 // If the experimentalMounterPathFlag is set, we do not want to // check node capabilities since the mount path is not the default if len(experimentalMounterPath) != 0 { // Replace the nameserver in containerized-mounter\u0026#39;s rootfs/etc/resolv.conf with kubelet.ClusterDNS // so that service name could be resolved klet.dnsConfigurer.SetupDNSinContainerizedMounter(experimentalMounterPath) } //这段Go代码是一个条件语句，其功能是根据$experimentalMounterPathFlag$的设置来决定是否需要检查节点能力。 //如果$experimentalMounterPathFlag$被设置（即$experimentalMounterPath$不为空）， //则会调用$klet.dnsConfigurer.SetupDNSinContainerizedMounter(experimentalMounterPath)$方法， //用kubelet.ClusterDNS替换containerized-mounter根文件系统/etc/resolv.conf中的nameserver，以便能够解析服务名称。 // setup volumeManager klet.volumeManager = volumemanager.NewVolumeManager( kubeCfg.EnableControllerAttachDetach, nodeName, klet.podManager, klet.podWorkers, klet.kubeClient, klet.volumePluginMgr, klet.containerRuntime, kubeDeps.Mounter, kubeDeps.HostUtil, klet.getPodsDir(), kubeDeps.Recorder, keepTerminatedPodVolumes, volumepathhandler.NewBlockVolumePathHandler()) klet.backOff = flowcontrol.NewBackOff(backOffPeriod, MaxContainerBackOff) //该代码段是Go语言编写的，其中定义了两个不同的内容：一个是volumeManager的设置，另一个是backOff的设置。 //首先，关于volumeManager的设置： //- 通过调用volumemanager.NewVolumeManager()函数创建一个新的volumeManager。 //- 该函数接受多个参数，包括是否启用控制器挂载/卸载、节点名称、pod管理器、pod workers、kubeClient、volume插件管理器、容器运行时、 //挂载器、主机工具、pod目录、事件记录器、是否保留已终止的pod卷以及块卷路径处理器。 //- 这些参数用于配置和初始化新的volumeManager，以便管理节点上的卷。 //接下来，关于backOff的设置： //- 通过调用flowcontrol.NewBackOff()函数创建一个新的backOff对象。 //- 该函数接受两个参数，即回退周期和最大容器回退时间。 //- 这些参数用于配置回退对象的行为，以便在容器启动失败时进行重试，避免频繁地立即重试。 // setup eviction manager evictionManager, evictionAdmitHandler := eviction.NewManager(klet.resourceAnalyzer, evictionConfig, killPodNow(klet.podWorkers, kubeDeps.Recorder), klet.imageManager, klet.containerGC, kubeDeps.Recorder, nodeRef, klet.clock, kubeCfg.LocalStorageCapacityIsolation) klet.evictionManager = evictionManager klet.admitHandlers.AddPodAdmitHandler(evictionAdmitHandler) //这段代码是设置驱逐管理器的。它通过调用eviction.NewManager函数创建一个驱逐管理器和一个驱逐准入处理器。 //其中，驱逐管理器用于管理节点的资源驱逐策略，驱逐准入处理器用于在Pod准入阶段进行驱逐检查和处理。 //接着，将驱逐管理器赋值给klet.evictionManager，将驱逐准入处理器添加到klet.admitHandlers的Pod准入处理器列表中。 // Safe, allowed sysctls can always be used as unsafe sysctls in the spec. // Hence, we concatenate those two lists. safeAndUnsafeSysctls := append(sysctl.SafeSysctlAllowlist(), allowedUnsafeSysctls...) sysctlsAllowlist, err := sysctl.NewAllowlist(safeAndUnsafeSysctls) if err != nil { return nil, err } klet.admitHandlers.AddPodAdmitHandler(sysctlsAllowlist) //这段Go代码主要实现了以下功能： //1. 将安全的sysctls和不安全的sysctls合并成一个列表 safeAndUnsafeSysctls。 //2. 使用合并后的列表创建一个sysctl允许列表 sysctlsAllowlist。 //3. 如果创建允许列表时发生错误，则返回 nil 和错误信息。 //4. 将sysctls允许列表添加到 klet.admitHandlers 中作为Pod的审核处理器。 //这段代码的作用是通过合并安全和不安全的sysctls列表，并创建一个允许列表，然后将其添加为Pod的审核处理器， //以确保只有允许的sysctls才能在系统中使用。 // enable active deadline handler activeDeadlineHandler, err := newActiveDeadlineHandler(klet.statusManager, kubeDeps.Recorder, klet.clock) if err != nil { return nil, err } klet.AddPodSyncLoopHandler(activeDeadlineHandler) klet.AddPodSyncHandler(activeDeadlineHandler) //这段Go代码的功能是启用一个激活期限处理器。 //它首先通过调用newActiveDeadlineHandler函数创建一个激活期限处理器，并将其与状态管理器、事件记录器和时钟进行关联。 //接着，通过调用AddPodSyncLoopHandler和AddPodSyncHandler方法， //将激活期限处理器添加到Pod同步循环处理器和Pod同步处理器中，以便在处理Pod时能够检查是否超过了激活期限。 //如果在创建激活期限处理器时出现错误，函数将返回nil和错误信息。 klet.admitHandlers.AddPodAdmitHandler(klet.containerManager.GetAllocateResourcesPodAdmitHandler()) criticalPodAdmissionHandler := preemption.NewCriticalPodAdmissionHandler(klet.GetActivePods, killPodNow(klet.podWorkers, kubeDeps.Recorder), kubeDeps.Recorder) klet.admitHandlers.AddPodAdmitHandler(lifecycle.NewPredicateAdmitHandler(klet.getNodeAnyWay, criticalPodAdmissionHandler, klet.containerManager.UpdatePluginResources)) // apply functional Option\u0026#39;s for _, opt := range kubeDeps.Options { opt(klet) } //这段Go代码中定义了一个函数，函数中包含多个步骤。 //首先，通过klet.admitHandlers.AddPodAdmitHandler()方法添加了一个Pod Admit Handler。 //然后，创建了一个Critical Pod Admit Handler，并通过klet.admitHandlers.AddPodAdmitHandler()方法将其添加到admitHandlers中。 //接着，创建了一个Predicate Admit Handler，并通过klet.admitHandlers.AddPodAdmitHandler()方法将其添加到admitHandlers中。 //最后，对klet应用了一系列的函数式选项。 if sysruntime.GOOS == \u0026#34;linux\u0026#34; { // AppArmor is a Linux kernel security module and it does not support other operating systems. klet.appArmorValidator = apparmor.NewValidator() klet.softAdmitHandlers.AddPodAdmitHandler(lifecycle.NewAppArmorAdmitHandler(klet.appArmorValidator)) } //这段Go代码是在检查当前运行的操作系统是否为Linux，如果是Linux，则创建一个AppArmorValidator对象， //并将其添加到softAdmitHandlers的PodAdmitHandler中。 //AppArmor是Linux内核的一个安全模块，这段代码的目的是在Kubernetes中启用AppArmor的安全功能。 leaseDuration := time.Duration(kubeCfg.NodeLeaseDurationSeconds) * time.Second renewInterval := time.Duration(float64(leaseDuration) * nodeLeaseRenewIntervalFraction) klet.nodeLeaseController = lease.NewController( klet.clock, klet.heartbeatClient, string(klet.nodeName), kubeCfg.NodeLeaseDurationSeconds, klet.onRepeatedHeartbeatFailure, renewInterval, string(klet.nodeName), v1.NamespaceNodeLease, util.SetNodeOwnerFunc(klet.heartbeatClient, string(klet.nodeName))) //这个Go函数主要目的是创建并初始化一个node lease控制器。 //该控制器用于管理节点的租约，以确保节点在Kubernetes集群中保持活动状态。 //函数首先根据配置确定租约的持续时间和更新间隔，然后使用这些值以及提供的客户端、节点名和其他参数来创建一个新的lease控制器实例。 // setup node shutdown manager shutdownManager, shutdownAdmitHandler := nodeshutdown.NewManager(\u0026amp;nodeshutdown.Config{ Logger: logger, ProbeManager: klet.probeManager, Recorder: kubeDeps.Recorder, NodeRef: nodeRef, GetPodsFunc: klet.GetActivePods, KillPodFunc: killPodNow(klet.podWorkers, kubeDeps.Recorder), SyncNodeStatusFunc: klet.syncNodeStatus, ShutdownGracePeriodRequested: kubeCfg.ShutdownGracePeriod.Duration, ShutdownGracePeriodCriticalPods: kubeCfg.ShutdownGracePeriodCriticalPods.Duration, ShutdownGracePeriodByPodPriority: kubeCfg.ShutdownGracePeriodByPodPriority, StateDirectory: rootDirectory, }) klet.shutdownManager = shutdownManager klet.usernsManager, err = userns.MakeUserNsManager(klet) if err != nil { return nil, err } klet.admitHandlers.AddPodAdmitHandler(shutdownAdmitHandler) //这段代码是Go语言编写的，用于设置节点关闭管理器。 //首先，通过调用nodeshutdown.NewManager函数创建一个新的节点关闭管理器和一个节点关闭准入处理器。 //在创建过程中，传入了日志记录器、探针管理器、事件记录器、节点引用、获取活跃Pods的函数、杀死Pod的函数、同步节点状态的函数、 //节点关闭的优雅等待时间、节点关闭的优雅等待时间（针对关键Pods）、按Pod优先级设置的节点关闭的优雅等待时间以及状态目录等配置参数。 //然后，将节点关闭管理器赋值给klet.shutdownManager，并将用户命名空间管理器赋值给klet.usernsManager。 //如果创建用户命名空间管理器失败，则返回错误。 //最后，将节点关闭准入处理器添加到klet.admitHandlers中的Pod准入处理器列表中。 // Finally, put the most recent version of the config on the Kubelet, so // people can see how it was configured. klet.kubeletConfiguration = *kubeCfg // Generating the status funcs should be the last thing we do, // since this relies on the rest of the Kubelet having been constructed. klet.setNodeStatusFuncs = klet.defaultNodeStatusFuncs() return klet, nil } //这段Go代码是Kubernetes中的Kubelet初始化过程的一部分。在这个函数中，主要完成了以下两步操作： //1. 将最新的配置版本应用到Kubelet上，这样用户就可以查看Kubelet是如何配置的。 //2. 生成节点状态报告函数，这应该是我们做的最后一件事，因为它依赖于Kubelet的其他部分已经构建完成。 //最后，该函数返回初始化完成的Kubelet实例和一个nil的错误值。 type serviceLister interface { List(labels.Selector) ([]*v1.Service, error) } //这段Go代码定义了一个名为serviceLister的接口，其中包含一个List方法。 //该方法接收一个labels.Selector类型的参数，并返回一个[]*v1.Service类型（即服务列表）和一个错误值。 //这个接口的作用是用于列举出符合给定标签选择器的服务列表。 // Kubelet is the main kubelet implementation. type Kubelet struct { kubeletConfiguration kubeletconfiginternal.KubeletConfiguration // hostname is the hostname the kubelet detected or was given via flag/config hostname string // hostnameOverridden indicates the hostname was overridden via flag/config hostnameOverridden bool //该Go代码定义了一个名为Kubelet的结构体，它表示一个Kubelet实例。 //- kubeletConfiguration是kubeletconfiginternal.KubeletConfiguration类型的一个字段，用于存储Kubelet的配置信息。 //- hostname是一个字符串字段，表示Kubelet检测到或通过标志/配置给定的主机名。 //- hostnameOverridden是一个布尔字段，指示主机名是否通过标志/配置被覆盖。 nodeName types.NodeName runtimeCache kubecontainer.RuntimeCache kubeClient clientset.Interface heartbeatClient clientset.Interface // mirrorPodClient is used to create and delete mirror pods in the API for static // pods. mirrorPodClient kubepod.MirrorClient //该Go函数定义了一个结构体，包含以下成员变量： //- nodeName：类型为types.NodeName，表示节点名称。 //- runtimeCache：类型为kubecontainer.RuntimeCache，表示运行时缓存。 //- kubeClient：类型为clientset.Interface，用于与Kubernetes集群进行交互。 //- heartbeatClient：类型为clientset.Interface，用于发送心跳等维护操作。 //- mirrorPodClient：类型为kubepod.MirrorClient，用于在API中创建和删除静态Pod的镜像Pod。 rootDirectory string podLogsDirectory string lastObservedNodeAddressesMux sync.RWMutex lastObservedNodeAddresses []v1.NodeAddress // onRepeatedHeartbeatFailure is called when a heartbeat operation fails more than once. optional. onRepeatedHeartbeatFailure func() //这段Go代码定义了以下内容： //- rootDirectory和podLogsDirectory两个字符串变量，分别表示根目录和Pod日志目录。 //- lastObservedNodeAddressesMux是一个读写锁，用于保护lastObservedNodeAddresses变量的并发访问。 //- lastObservedNodeAddresses是一个包含v1.NodeAddress类型的切片，存储了最近观察到的节点地址。 //- onRepeatedHeartbeatFailure是一个可选的回调函数，当心跳操作失败多次时会被调用。 // podManager stores the desired set of admitted pods and mirror pods that the kubelet should be // running. The actual set of running pods is stored on the podWorkers. The manager is populated // by the kubelet config loops which abstracts receiving configuration from many different sources // (api for regular pods, local filesystem or http for static pods). The manager may be consulted // by other components that need to see the set of desired pods. Note that not all desired pods are // running, and not all running pods are in the podManager - for instance, force deleting a pod // from the apiserver will remove it from the podManager, but the pod may still be terminating and // tracked by the podWorkers. Components that need to know the actual consumed resources of the // node or are driven by podWorkers and the sync*Pod methods (status, volume, stats) should also // consult the podWorkers when reconciling. // // TODO: review all kubelet components that need the actual set of pods (vs the desired set) // and update them to use podWorkers instead of podManager. This may introduce latency in some // methods, but avoids race conditions and correctly accounts for terminating pods that have // been force deleted or static pods that have been updated. // https://github.com/kubernetes/kubernetes/issues/116970 podManager kubepod.Manager //podManager是一个存储了kubelet应该运行的期望的Pod集合（包括已承认的Pod和镜像Pod）。 //它的实际运行Pod集合存储在podWorkers中。podManager通过kubelet配置循环来抽象从不同来源接收配置 //（来自API的普通Pod，本地文件系统或HTTP的静态Pod）。 //其他组件可以查询podManager来获取期望的Pod集合。 //注意，不是所有期望的Pod都在运行，也不是所有运行的Pod都在podManager中。 //例如，从apiserver强制删除一个Pod会将其从podManager中移除，但该Pod可能仍在终止中，并由podWorkers跟踪。 //需要知道节点实际消耗资源的组件或由podWorkers和sync*Pod方法（状态，卷，统计信息）驱动的组件也应该在协调时咨询podWorkers。 // podWorkers is responsible for driving the lifecycle state machine of each pod. The worker is // notified of config changes, updates, periodic reconciliation, container runtime updates, and // evictions of all desired pods and will invoke reconciliation methods per pod in separate // goroutines. The podWorkers are authoritative in the kubelet for what pods are actually being // run and their current state: // // * syncing: pod should be running (syncPod) // * terminating: pod should be stopped (syncTerminatingPod) // * terminated: pod should have all resources cleaned up (syncTerminatedPod) // // and invoke the handler methods that correspond to each state. Components within the // kubelet that need to know the phase of the pod in order to correctly set up or tear down // resources must consult the podWorkers. // // Once a pod has been accepted by the pod workers, no other pod with that same UID (and // name+namespace, for static pods) will be started until the first pod has fully terminated // and been cleaned up by SyncKnownPods. This means a pod may be desired (in API), admitted // (in pod manager), and requested (by invoking UpdatePod) but not start for an arbitrarily // long interval because a prior pod is still terminating. // // As an event-driven (by UpdatePod) controller, the podWorkers must periodically be resynced // by the kubelet invoking SyncKnownPods with the desired state (admitted pods in podManager). // Since the podManager may be unaware of some running pods due to force deletion, the // podWorkers are responsible for triggering a sync of pods that are no longer desired but // must still run to completion. podWorkers PodWorkers //podWorkers是一个负责驱动每个Pod生命周期状态机的函数。 //它会在配置更改、更新、定期同步、容器运行时更新和驱逐所有期望的Pod时被通知， //并在单独的goroutine中对每个Pod调用和解方法。 //podWorkers是kubelet中Pod实际运行状态和当前状态的权威。 //它通过调用对应于每个状态的处理方法，来实现Pod的同步、终止和清理等操作。 //此外，podWorkers还负责触发不再期望运行但必须完成同步的Pod。 // evictionManager observes the state of the node for situations that could impact node stability // and evicts pods (sets to phase Failed with reason Evicted) to reduce resource pressure. The // eviction manager acts on the actual state of the node and considers the podWorker to be // authoritative. evictionManager eviction.Manager //该函数定义了一个名为evictionManager的变量，其类型为eviction.Manager。 //这个变量的目的是监控节点的状态，以便在可能影响节点稳定性的场景下进行操作。 //具体的操作是通过驱逐（设置为失败状态且附加 Evicted 原因）Pod来减少资源压力。 //驱逐管理器依据节点的实际状态进行操作，并将podWorker视为权威。 // probeManager tracks the set of running pods and ensures any user-defined periodic checks are // run to introspect the state of each pod. The probe manager acts on the actual state of the node // and is notified of pods by the podWorker. The probe manager is the authoritative source of the // most recent probe status and is responsible for notifying the status manager, which // synthesizes them into the overall pod status. probeManager prober.Manager //该函数定义了一个名为probeManager的变量，其类型为prober.Manager。 //probeManager的作用是跟踪正在运行的Pod集合，并确保运行用户定义的周期性检查，以检查每个Pod的状态。 //它是根据节点的实际状态进行操作的，并由podWorker通知Pod的状态。 //probeManager是最新探针状态的权威来源，并负责通知状态管理器，将其合成到整体Pod状态中。 // secretManager caches the set of secrets used by running pods on this node. The podWorkers // notify the secretManager when pods are started and terminated, and the secretManager must // then keep the needed secrets up-to-date as they change. secretManager secret.Manager //这段代码定义了一个名为secretManager的变量， //其类型为secret.Manager。secretManager是一个缓存， //用于存储当前节点上运行的Pod所使用的秘密集。 //podWorkers在Pod启动和终止时通知secretManager，然后secretManager必须根据秘密的变化保持所需秘密的更新。 // configMapManager caches the set of config maps used by running pods on this node. The // podWorkers notify the configMapManager when pods are started and terminated, and the // configMapManager must then keep the needed config maps up-to-date as they change. configMapManager configmap.Manager //这段代码定义了一个名为configMapManager的变量，它的类型是configmap.Manager。 //该变量用于缓存当前节点上运行的Pod所使用的ConfigMap集合。 //PodWorkers会在Pod启动和终止时通知configMapManager，然后configMapManager必须保持所需ConfigMap的更新。 // volumeManager observes the set of running pods and is responsible for attaching, mounting, // unmounting, and detaching as those pods move through their lifecycle. It periodically // synchronizes the set of known volumes to the set of actually desired volumes and cleans up // any orphaned volumes. The volume manager considers the podWorker to be authoritative for // which pods are running. volumeManager volumemanager.VolumeManager //这段代码定义了一个名为volumeManager的变量，它的类型是volumemanager.VolumeManager。 //该变量用于观察运行中的Pod集合，并负责在Pod生命周期中进行挂载、卸载和分离等操作。 //它会周期性地将已知的卷同步到期望的卷集合中，并清理任何孤儿卷。volumeManager将podWorker视为运行中Pod的权威。 // statusManager receives updated pod status updates from the podWorker and updates the API // status of those pods to match. The statusManager is authoritative for the synthesized // status of the pod from the kubelet\u0026#39;s perspective (other components own the individual // elements of status) and should be consulted by components in preference to assembling // that status themselves. Note that the status manager is downstream of the pod worker // and components that need to check whether a pod is still running should instead directly // consult the pod worker. statusManager status.Manager //这段代码定义了一个名为statusManager的变量，它的类型是status.Manager。 //statusManager的作用是接收来自podWorker的更新后的Pod状态，并将这些Pod的状态更新为与API状态匹配。 //statusManager是关于kubelet视角下Pod合成状态的权威组件（其他组件各自拥有状态的各个元素）， //组件应该优先咨询statusManager而不是自己组装状态。需要注意的是，statusManager是在podWorker的下游， //如果组件需要检查Pod是否仍在运行，应该直接咨询podWorker。 // resyncInterval is the interval between periodic full reconciliations of // pods on this node. resyncInterval time.Duration //该函数定义了一个名为resyncInterval的变量，其类型为time.Duration，用于表示节点上周期性完全同步Pod的间隔时间。 // sourcesReady records the sources seen by the kubelet, it is thread-safe. sourcesReady config.SourcesReady //sourcesReady是一个config.SourcesReady类型的变量，用于记录kubelet看到的源。它是线程安全的。 // Optional, defaults to /logs/ from /var/log logServer http.Handler // Optional, defaults to simple Docker implementation runner kubecontainer.CommandRunner //这段代码定义了两个变量： //1. logServer：一个类型为http.Handler的变量，用于处理日志服务的请求，默认值为/logs/路径下的日志文件。 //2. runner：一个类型为kubecontainer.CommandRunner的变量，用于执行容器内部的命令，默认实现为简单的Docker命令执行器。 //这两个变量都是可选的，如果没有显式地设置，它们将使用默认值。 // cAdvisor used for container information. cadvisor cadvisor.Interface //该行代码定义了一个名为cadvisor.Interface类型的变量cAdvisor，用于获取容器的信息 // Set to true to have the node register itself with the apiserver. registerNode bool // List of taints to add to a node object when the kubelet registers itself. registerWithTaints []v1.Taint // Set to true to have the node register itself as schedulable. registerSchedulable bool // for internal book keeping; access only from within registerWithApiserver registrationCompleted bool //这段代码定义了四个变量，用于配置节点在注册到 apiserver 时的一些行为： //- registerNode：设置为 true 时，节点会自动注册到 apiserver。 //- registerWithTaints：节点注册时要添加的 taint 列表。 //- registerSchedulable：设置为 true 时，节点注册时会将自己标记为可调度的。 //- registrationCompleted：内部使用，用于记录节点是否已完成注册。 // dnsConfigurer is used for setting up DNS resolver configuration when launching pods. dnsConfigurer *dns.Configurer // serviceLister knows how to list services serviceLister serviceLister // serviceHasSynced indicates whether services have been sync\u0026#39;d at least once. // Check this before trusting a response from the lister. serviceHasSynced cache.InformerSynced // nodeLister knows how to list nodes nodeLister corelisters.NodeLister // nodeHasSynced indicates whether nodes have been sync\u0026#39;d at least once. // Check this before trusting a response from the node lister. nodeHasSynced cache.InformerSynced // a list of node labels to register nodeLabels map[string]string //这段代码定义了几个字段，用于配置和管理Kubernetes中的DNS解析器、服务列表、节点列表等。 //- dnsConfigurer *dns.Configurer：用于在启动Pod时设置DNS解析器的配置。 //- serviceLister serviceLister：用于列举服务。 //- serviceHasSynced cache.InformerSynced：指示服务是否至少同步过一次。在信任列表器的响应之前，应该检查此字段。 //- nodeLister corelisters.NodeLister：用于列举节点。 //- nodeHasSynced cache.InformerSynced：指示节点是否至少同步过一次。 //在信任节点列表器的响应之前，应该检查此字段。 //- nodeLabels map[string]string：要注册的节点标签列表。 // Last timestamp when runtime responded on ping. // Mutex is used to protect this value. runtimeState *runtimeState // Volume plugins. volumePluginMgr *volume.VolumePluginMgr // Manages container health check results. livenessManager proberesults.Manager readinessManager proberesults.Manager startupManager proberesults.Manager //这段代码定义了四个变量，分别用于记录运行时响应ping的最后一个时间戳、管理卷插件、管理容器健康检查结果和管理容器就绪检查结果。 //其中，runtimeState使用互斥锁来保护，volumePluginMgr是卷插件管理器， //livenessManager、readinessManager和startupManager分别用于管理容器的存活检查结果、就绪检查结果和启动检查结果。 // How long to keep idle streaming command execution/port forwarding // connections open before terminating them streamingConnectionIdleTimeout time.Duration //该函数定义了一个时间间隔，用于控制在终止闲置的流式命令执行/端口转发连接之前保持连接打开的时间长度。 // The EventRecorder to use recorder record.EventRecorder //这段代码定义了一个名为recorder的变量，其类型为record.EventRecorder。这是一个用于记录Kubernetes事件的接口。 // Policy for handling garbage collection of dead containers. containerGC kubecontainer.GC //该函数定义了一个处理垃圾回收的策略，用于回收死亡的容器。它使用了kubecontainer.GC类型，表示具体的垃圾回收策略。 // Manager for image garbage collection. imageManager images.ImageGCManager //该代码定义了一个名为imageManager的变量，其类型为images.ImageGCManager。这是一个用于管理图像垃圾收集的管理器。 // Manager for container logs. containerLogManager logs.ContainerLogManager //该函数定义了一个名为containerLogManager的接口，该接口用于管理容器日志。 // Cached MachineInfo returned by cadvisor. machineInfoLock sync.RWMutex machineInfo *cadvisorapi.MachineInfo //这段Go代码定义了一个全局变量machineInfo，它是一个指向cadvisorapi.MachineInfo类型的指针，用于存储由cadvisor返回的机器信息。 //同时，为了保证并发安全，使用了sync.RWMutex类型的machineInfoLock变量作为锁。 // Handles certificate rotations. serverCertificateManager certificate.Manager //该代码定义了一个名为serverCertificateManager的变量，其类型为certificate.Manager，用于处理证书轮换。 // Cloud provider interface. cloud cloudprovider.Interface // Handles requests to cloud provider with timeout cloudResourceSyncManager cloudresource.SyncManager //这段代码定义了两个变量： //1. cloud：代表云提供商的接口，具体实现取决于云提供商，它定义了与云提供商交互的方法。 //2. cloudResourceSyncManager：代表一个同步管理器，用于处理与云提供商资源的同步操作， //具体实现可能包括同步云提供商的虚拟机、网络、存储等资源信息到本地缓存，并定期更新这些信息以保持数据的同步。 // Indicates that the node initialization happens in an external cloud controller externalCloudProvider bool // Reference to this node. nodeRef *v1.ObjectReference //这段代码定义了两个Go语言的变量。 //1. externalCloudProvider是一个布尔类型的变量，用于指示节点初始化是在外部云控制器中进行的。 //2. nodeRef是一个指向v1.ObjectReference类型的指针，用于引用该节点。 // Container runtime. containerRuntime kubecontainer.Runtime //该代码定义了一个变量 containerRuntime，其类型为 kubecontainer.Runtime，用于表示容器运行时环境。 // Streaming runtime handles container streaming. streamingRuntime kubecontainer.StreamingRuntime //该函数是一个Go语言函数声明，函数名称为streamingRuntime，函数参数为kubecontainer.StreamingRuntime类型。 //该函数用于处理容器的流式传输。 // Container runtime service (needed by container runtime Start()). runtimeService internalapi.RuntimeService //这个Go函数定义了一个名为runtimeService的变量，它是一个internalapi.RuntimeService类型的接口。 //这个接口用于与容器运行时服务进行交互，是容器运行时启动所必需的。 // reasonCache caches the failure reason of the last creation of all containers, which is // used for generating ContainerStatus. reasonCache *ReasonCache //该行代码定义了一个名为reasonCache的变量，其类型为*ReasonCache。 //ReasonCache是一个用于缓存所有容器最后创建失败原因的结构体，这些失败原因用于生成ContainerStatus。 // containerRuntimeReadyExpected indicates whether container runtime being ready is expected // so errors are logged without verbosity guard, to avoid excessive error logs at node startup. // It\u0026#39;s false during the node initialization period of nodeReadyGracePeriod, and after that // it\u0026#39;s set to true by fastStatusUpdateOnce when it exits. containerRuntimeReadyExpected bool //该函数用于指示容器运行时是否预期准备好，以便在节点启动时避免过多错误日志。 //在节点初始化期间为false，之后通过fastStatusUpdateOnce退出时设置为true。 // nodeStatusUpdateFrequency specifies how often kubelet computes node status. If node lease // feature is not enabled, it is also the frequency that kubelet posts node status to master. // In that case, be cautious when changing the constant, it must work with nodeMonitorGracePeriod // in nodecontroller. There are several constraints: // 1. nodeMonitorGracePeriod must be N times more than nodeStatusUpdateFrequency, where // N means number of retries allowed for kubelet to post node status. It is pointless // to make nodeMonitorGracePeriod be less than nodeStatusUpdateFrequency, since there // will only be fresh values from Kubelet at an interval of nodeStatusUpdateFrequency. // The constant must be less than podEvictionTimeout. // 2. nodeStatusUpdateFrequency needs to be large enough for kubelet to generate node // status. Kubelet may fail to update node status reliably if the value is too small, // as it takes time to gather all necessary node information. nodeStatusUpdateFrequency time.Duration //这个go函数定义了一个时间间隔，表示kubelet计算节点状态的频率。 //如果未启用节点租约功能，则该频率也是kubelet向主节点发布节点状态的频率。 //在更改该常量时需要谨慎，它必须与节点控制器中的nodeMonitorGracePeriod配合使用，并且需要满足以下约束条件： //1. nodeMonitorGracePeriod必须是nodeStatusUpdateFrequency的N倍，其中N表示kubelet发布节点状态允许的重试次数。 //将nodeMonitorGracePeriod设置为小于nodeStatusUpdateFrequency没有意义， //因为kubelet只会以nodeStatusUpdateFrequency的时间间隔提供新鲜值。 //该常量必须小于podEvictionTimeout。 //2. nodeStatusUpdateFrequency需要足够大，以便kubelet生成节点状态。 //如果该值太小，kubelet可能无法可靠地更新节点状态， //因为它需要时间来收集所有必要的节点信息。 // nodeStatusReportFrequency is the frequency that kubelet posts node // status to master. It is only used when node lease feature is enabled. nodeStatusReportFrequency time.Duration //该代码定义了一个变量nodeStatusReportFrequency，它是kubelet向master报告节点状态的频率。 //这个变量只在启用了节点租约功能时使用。 // lastStatusReportTime is the time when node status was last reported. lastStatusReportTime time.Time //该函数定义了一个变量lastStatusReportTime，它表示节点状态最后一次报告的时间，其类型为time.Time。 // syncNodeStatusMux is a lock on updating the node status, because this path is not thread-safe. // This lock is used by Kubelet.syncNodeStatus and Kubelet.fastNodeStatusUpdate functions and shouldn\u0026#39;t be used anywhere else. syncNodeStatusMux sync.Mutex //这个Go函数定义了一个名为syncNodeStatusMux的互斥锁，用于保护节点状态的更新操作，因为这条路径不是线程安全的。 //这个互斥锁仅被Kubelet.syncNodeStatus和Kubelet.fastNodeStatusUpdate函数使用，不建议在其他地方使用。 // updatePodCIDRMux is a lock on updating pod CIDR, because this path is not thread-safe. // This lock is used by Kubelet.updatePodCIDR function and shouldn\u0026#39;t be used anywhere else. updatePodCIDRMux sync.Mutex //这段代码定义了一个名为updatePodCIDRMux的互斥锁，用于保证更新Pod CIDR的路径的线程安全性。 //这个互斥锁仅被Kubelet的updatePodCIDR函数使用，不建议在其他地方使用。 // updateRuntimeMux is a lock on updating runtime, because this path is not thread-safe. // This lock is used by Kubelet.updateRuntimeUp, Kubelet.fastNodeStatusUpdate and // Kubelet.HandlerSupportsUserNamespaces functions and shouldn\u0026#39;t be used anywhere else. updateRuntimeMux sync.Mutex //这个函数定义了一个名为updateRuntimeMux的互斥锁，用于保证更新运行时的路径线程安全。 //这个互斥锁被Kubelet.updateRuntimeUp、Kubelet.fastNodeStatusUpdate和Kubelet.HandlerSupportsUserNamespaces函数使用， //不应该在其他地方使用。 // nodeLeaseController claims and renews the node lease for this Kubelet nodeLeaseController lease.Controller //nodeLeaseController 是一个 lease.Controller 类型的变量。它的功能是声明和更新当前 Kubelet 的节点租约。 // pleg observes the state of the container runtime and notifies the kubelet of changes to containers, which // notifies the podWorkers to reconcile the state of the pod (for instance, if a container dies and needs to // be restarted). pleg pleg.PodLifecycleEventGenerator //该函数定义了一个名为pleg的变量，其类型为pleg.PodLifecycleEventGenerator。 //这个函数的作用是观察容器运行时的状态，并将容器的变化通知给kubelet，进而通知podWorkers去协调pod的状态。 //例如，如果一个容器死亡需要重启，pleg会通知kubelet和podWorkers去处理。 // eventedPleg supplements the pleg to deliver edge-driven container changes with low-latency. eventedPleg pleg.PodLifecycleEventGenerator //该Go函数定义了一个名为eventedPleg的变量，其类型为pleg.PodLifecycleEventGenerator。 //这个函数的作用是补充(pleg)以提供边缘驱动的容器变化的低延迟。 // Store kubecontainer.PodStatus for all pods. podCache kubecontainer.Cache //该代码定义了一个名为podCache的变量，其类型为kubecontainer.Cache。 //这个变量的作用是存储所有Pod的kubecontainer.PodStatus信息。 // os is a facade for various syscalls that need to be mocked during testing. os kubecontainer.OSInterface //该函数定义了一个类型os，它是一个kubecontainer.OSInterface的别名。 //这个类型的目的是作为一个Facade，用于在测试期间模拟各种系统调用。 // Watcher of out of memory events. oomWatcher oomwatcher.Watcher //该Go函数定义了一个名为oomWatcher的变量，其类型为oomwatcher.Watcher。这个变量用于监控内存溢出事件。 // Monitor resource usage resourceAnalyzer serverstats.ResourceAnalyzer //该函数定义了一个资源分析器，类型为serverstats.ResourceAnalyzer。通过该资源分析器可以监控服务器的资源使用情况。 // Whether or not we should have the QOS cgroup hierarchy for resource management cgroupsPerQOS bool //该函数定义了一个名为cgroupsPerQOS的变量，其类型为bool。该变量用于指示是否应该为资源管理而创建QOS级别的cgroup层次结构。 // If non-empty, pass this to the container runtime as the root cgroup. cgroupRoot string //该函数定义了一个名为cgroupRoot的字符串变量，它表示容器运行时的根cgroup。如果cgroupRoot不为空，则将其传递给容器运行时作为根cgroup。 // Mounter to use for volumes. mounter mount.Interface //这段代码定义了一个名为Mounter的接口，用于挂载卷。 // hostutil to interact with filesystems hostutil hostutil.HostUtils //该函数定义了一个名为hostutil的变量，它是一个hostutil.HostUtils类型的变量。 //这个变量是用来与文件系统进行交互的工具类。它可以提供一系列的方法，如读取文件、写入文件、创建文件夹等操作。 //通过这个变量，可以方便地对文件系统进行操作。 // subpather to execute subpath actions subpather subpath.Interface //该函数定义了一个名为subpather的变量，它是一个subpath.Interface类型的变量。这个变量是用来执行子路径操作的接口。 // Manager of non-Runtime containers. containerManager cm.ContainerManager //该代码定义了一个名为containerManager的变量，它是一个cm.ContainerManager类型的容器管理器。 //具体功能和用途需要查看cm.ContainerManager该代码的实现和相关文档才能确定。 // Maximum Number of Pods which can be run by this Kubelet maxPods int //该函数定义了一个名为maxPods的整型变量，它表示这个Kubelet能够运行的最大Pods数量。 // Monitor Kubelet\u0026#39;s sync loop syncLoopMonitor atomic.Value //该函数是一个Go语言的函数，名为syncLoopMonitor，它用于监控Kubelet的同步循环。 //该函数使用了atomic.Value类型，atomic.Value是Go语言标准库中的一种原子类型，可以用于安全地存储和加载值。 //在这个函数中，syncLoopMonitor被定义为atomic.Value类型，可以用来存储和加载Kubelet的同步循环的状态信息。 //具体来说，syncLoopMonitor可以用来监控Kubelet的同步循环是否正常运行，以及同步循环的运行状态如何。 //通过使用atomic.Value类型，可以在多线程环境下安全地读写syncLoopMonitor的值，确保了数据的一致性和安全性。 //总之，syncLoopMonitor函数用于监控Kubelet的同步循环，并使用atomic.Value类型来保证数据的一致性和安全性。 // Container restart Backoff backOff *flowcontrol.Backoff //这个Go函数定义了一个名为\u0026#34;backOff\u0026#34;的变量，它是flowcontrol包中的Backoff类型。这个变量用于实现容器重启的退避策略。 // Information about the ports which are opened by daemons on Node running this Kubelet server. daemonEndpoints *v1.NodeDaemonEndpoints //该函数用于获取由运行此Kubelet服务器的节点上的守护进程打开的端口的信息。返回一个类型为*v1.NodeDaemonEndpoints的指针。 // A queue used to trigger pod workers. workQueue queue.WorkQueue //该函数定义了一个名为workQueue的队列，用于触发Pod workers。 //这是一个工作队列，可以用来存储待处理的任务，并且按照一定的顺序进行处理。 //在Go语言中，队列通常被用来实现并发编程中的生产者-消费者模式，其中生产者负责向队列中添加任务，消费者负责从队列中取出任务并进行处理。 //这个workQueue可以被用来协调多个Pod workers的工作，确保任务能够被有序、高效地处理。 // oneTimeInitializer is used to initialize modules that are dependent on the runtime to be up. oneTimeInitializer sync.Once //这个函数是一个用于初始化依赖运行时的模块的同步函数。它使用了sync.Once来确保初始化只执行一次。 // If set, use this IP address or addresses for the node nodeIPs []net.IP //该函数用于设置节点的IP地址或地址列表。 //- nodeIPs []net.IP：表示要设置的节点IP地址或地址列表，类型为[]net.IP，即可以是一个IP地址或多个IP地址的切片。 // use this function to validate the kubelet nodeIP nodeIPValidator func(net.IP) error //该函数用于验证Kubelet节点的IP地址是否有效。 //其输入参数为一个net.IP类型的IP地址，返回值为一个error类型的错误信息。 //具体验证过程和逻辑未在代码片段中展示。 // If non-nil, this is a unique identifier for the node in an external database, eg. cloudprovider providerID string //该函数用于设置节点在外部数据库中的唯一标识符，例如云提供商。 // clock is an interface that provides time related functionality in a way that makes it // easy to test the code. clock clock.WithTicker //该函数实现了一个时钟接口clock，其中WithTicker方法返回一个定时器，该定时器会在指定的时间间隔后发出时间信号。 //这个接口的目的是为了方便测试代码，因为它可以模拟时间的流逝和定时器的行为。 // handlers called during the tryUpdateNodeStatus cycle setNodeStatusFuncs []func(context.Context, *v1.Node) error //这段Go代码定义了一个名为setNodeStatusFuncs的切片，其元素是一个函数类型，函数接受context.Context和*v1.Node作为参数， //并返回一个error类型。 这个切片主要用于存储在tryUpdateNodeStatus周期中被调用的处理函数。 lastNodeUnschedulableLock sync.Mutex // maintains Node.Spec.Unschedulable value from previous run of tryUpdateNodeStatus() lastNodeUnschedulable bool //这段代码定义了一个互斥锁lastNodeUnschedulableLock和一个布尔值lastNodeUnschedulable， //用于维护节点的Spec.Unschedulable值从上一次运行tryUpdateNodeStatus()函数开始就没有改变过。 // the list of handlers to call during pod admission. admitHandlers lifecycle.PodAdmitHandlers //该函数定义了一个名为admitHandlers的变量，它是一个lifecycle.PodAdmitHandlers类型的切片。 //这个变量用于存储在Pod准入阶段需要调用的一系列处理器（handlers）。 //lifecycle.PodAdmitHandlers是一个结构体类型，它包含了多个处理Pod准入的处理器，例如PreAdmit、Admit和PostAdmit等。 //每个处理器都是一个函数，它们会在Pod准入的不同阶段被调用，以执行相应的准入控制逻辑。 //通过这个admitHandlers变量，可以在处理Pod准入时动态添加或移除处理器，以灵活地控制Pod的准入流程。 // softAdmithandlers are applied to the pod after it is admitted by the Kubelet, but before it is // run. A pod rejected by a softAdmitHandler will be left in a Pending state indefinitely. If a // rejected pod should not be recreated, or the scheduler is not aware of the rejection rule, the // admission rule should be applied by a softAdmitHandler. softAdmitHandlers lifecycle.PodAdmitHandlers //这段Go代码定义了一个名为softAdmitHandlers的变量，它是一个lifecycle.PodAdmitHandlers类型的变量。 //这个变量是一个软准入处理器，它会在Kubelet接纳Pod之后、运行Pod之前对Pod进行处理。 //如果一个Pod被软准入处理器拒绝，它将无限期地保持Pending状态。 //如果一个被拒绝的Pod不应该被重新创建，或者调度器不知道拒绝规则，那么应该由软准入处理器来应用准入规则。 // the list of handlers to call during pod sync loop. lifecycle.PodSyncLoopHandlers //这段代码定义了一个名为PodSyncLoopHandlers的变量，它是一个lifecycle.PodSyncLoopHandlers类型的手册。 //这个变量用于在Pod同步循环中调用的一系列处理程序。 // the list of handlers to call during pod sync. lifecycle.PodSyncHandlers //lifecycle.PodSyncHandlers是一个包含了在同步pod时需要调用的一系列处理程序的列表。 // the number of allowed pods per core podsPerCore int //该代码片段定义了一个名为podsPerCore的整型变量，用于表示每核允许的Pod数量。 // enableControllerAttachDetach indicates the Attach/Detach controller // should manage attachment/detachment of volumes scheduled to this node, // and disable kubelet from executing any attach/detach operations enableControllerAttachDetach bool //该函数用于设置是否启用控制器来管理卷的挂载和卸载操作，并禁用kubelet执行任何挂载/卸载操作。 //其中，参数enableControllerAttachDetach为一个布尔值，若设置为true则启用控制器管理卷的挂载和卸载， //若设置为false则禁用控制器管理卷的挂载和卸载。 // trigger deleting containers in a pod containerDeletor *podContainerDeletor //这个Go代码定义了一个名为containerDeletor的变量，它是podContainerDeletor类型的指针。 //podContainerDeletor是一个结构体，用于触发删除Pod中的容器。 // config iptables util rules makeIPTablesUtilChains bool //该代码片段定义了一个名为makeIPTablesUtilChains的布尔类型变量，用于配置iptables实用程序的规则。 // The AppArmor validator for checking whether AppArmor is supported. appArmorValidator apparmor.Validator //这段代码定义了一个名为appArmorValidator的变量，其类型为apparmor.Validator。 //该变量用于验证是否支持AppArmor。 //AppArmor（ApplicationArmor）是一种安全模块，用于增强Linux操作系统的安全性。 //它通过为每个应用程序定义安全策略来限制其访问系统资源的权限。 //apparmor.Validator是一个接口，用于检查系统中是否启用了AppArmor，并验证其配置是否正确。 //综上所述，appArmorValidator是一个用于验证系统是否支持AppArmor的变量。 // StatsProvider provides the node and the container stats. StatsProvider *stats.Provider //该Go函数定义了一个名为StatsProvider的变量，该变量指向一个stats.Provider类型的对象。 //stats.Provider是一个接口，用于提供节点和容器的统计信息。 // This flag, if set, instructs the kubelet to keep volumes from terminated pods mounted to the node. // This can be useful for debugging volume related issues. keepTerminatedPodVolumes bool // DEPRECATED //该函数定义了一个名为keepTerminatedPodVolumes的布尔型变量，通过该变量控制kubelet是否将已终止pod的卷保持挂载到节点上。 //该特性用于调试与卷相关的故障。注意，该变量已废弃。 // pluginmanager runs a set of asynchronous loops that figure out which // plugins need to be registered/unregistered based on this node and makes it so. pluginManager pluginmanager.PluginManager //该函数定义了一个名为pluginManager的变量，其类型为pluginmanager.PluginManager。 //该变量运行一组异步循环，用于确定基于当前节点需要注册/注销的插件，并进行相应操作。 // This flag sets a maximum number of images to report in the node status. nodeStatusMaxImages int32 //该函数用于设置节点状态报告中最大图像数量的上限。 // Handles RuntimeClass objects for the Kubelet. runtimeClassManager *runtimeclass.Manager //这个Go函数定义了一个名为runtimeClassManager的变量，它是runtimeclass.Manager类型的一个实例。 //这个变量用于处理Kubelet的RuntimeClass对象。 // Handles node shutdown events for the Node. shutdownManager nodeshutdown.Manager //该函数用于处理节点关闭事件。 //- 参数： //- nodeshutdown.Manager：节点关闭管理器。 //- 功能： //- 监听节点关闭事件。 //- 当节点关闭时，执行相应的处理逻辑。 // Manage user namespaces usernsManager *userns.UsernsManager //该代码定义了一个变量usernsManager，其类型为*userns.UsernsManager，用于管理用户命名空间。 //用户命名空间是Linux系统中的一个特性，可以为不同用户创建独立的文件系统、网络等环境，实现资源的隔离。 //userns.UsernsManager是一个用于管理用户命名空间的封装对象，提供了创建、删除、查询等操作接口。 // Mutex to serialize new pod admission and existing pod resizing podResizeMutex sync.Mutex //这段代码定义了一个名为podResizeMutex的sync.Mutex类型变量。 //sync.Mutex是Go标准库中的一个互斥锁类型，用于控制并发访问共享资源。 //在本例中，podResizeMutex用于确保新建Pod的准入和现有Pod的调整大小操作的序列化执行，以避免并发冲突。 // OpenTelemetry Tracer tracer trace.Tracer //该函数定义了一个OpenTelemetry Tracer。 // Track node startup latencies nodeStartupLatencyTracker util.NodeStartupLatencyTracker //该代码定义了一个名为nodeStartupLatencyTracker的变量，它的类型是util.NodeStartupLatencyTracker。根据变量的命名， //可以推测这是一个用于追踪节点启动延迟的工具。具体实现和功能细节需要查看util.NodeStartupLatencyTracker的定义和实现。 } // ListPodStats is delegated to StatsProvider, which implements stats.Provider interface func (kl *Kubelet) ListPodStats(ctx context.Context) ([]statsapi.PodStats, error) { return kl.StatsProvider.ListPodStats(ctx) } //这个函数是Kubelet的一个方法，用于列出所有Pod的统计信息。 //它将请求委托给实现了stats.Provider接口的StatsProvider对象的ListPodStats方法， //返回一个包含所有Pod统计信息的切片。 // ListPodCPUAndMemoryStats is delegated to StatsProvider, which implements stats.Provider interface func (kl *Kubelet) ListPodCPUAndMemoryStats(ctx context.Context) ([]statsapi.PodStats, error) { return kl.StatsProvider.ListPodCPUAndMemoryStats(ctx) } //该函数是一个代理函数，将调用委托给实现了stats.Provider接口的StatsProvider对象的ListPodCPUAndMemoryStats方法。 //该方法用于获取Pod的CPU和内存统计信息。 // ListPodStatsAndUpdateCPUNanoCoreUsage is delegated to StatsProvider, which implements stats.Provider interface func (kl *Kubelet) ListPodStatsAndUpdateCPUNanoCoreUsage(ctx context.Context) ([]statsapi.PodStats, error) { return kl.StatsProvider.ListPodStatsAndUpdateCPUNanoCoreUsage(ctx) } //这个函数是Kubelet的一个方法，用于列出Pod的统计信息并更新CPU使用量。 //它委托给实现了stats.Provider接口的StatsProvider来完成具体的操作。 // ImageFsStats is delegated to StatsProvider, which implements stats.Provider interface func (kl *Kubelet) ImageFsStats(ctx context.Context) (*statsapi.FsStats, *statsapi.FsStats, error) { return kl.StatsProvider.ImageFsStats(ctx) } //该函数是Kubelet的一个方法，用于获取镜像文件系统的统计信息。 //它将请求委托给实现了stats.Provider接口的StatsProvider对象的ImageFsStats方法，返回镜像文件系统的使用情况统计信息。 // GetCgroupStats is delegated to StatsProvider, which implements stats.Provider interface func (kl *Kubelet) GetCgroupStats(cgroupName string, updateStats bool) (*statsapi.ContainerStats, *statsapi.NetworkStats, error) { return kl.StatsProvider.GetCgroupStats(cgroupName, updateStats) } //该函数是一个委托函数，将获取cgroup统计信息的任务委托给实现了stats.Provider接口的StatsProvider对象。 //函数接收一个cgroup名称和一个布尔值updateStats作为参数， //并返回一个包含容器统计信息的statsapi.ContainerStats指针、一个包含网络统计信息的statsapi.NetworkStats指针以及可能出现的错误。 //具体实现由StatsProvider对象完成。 // GetCgroupCPUAndMemoryStats is delegated to StatsProvider, which implements stats.Provider interface func (kl *Kubelet) GetCgroupCPUAndMemoryStats(cgroupName string, updateStats bool) (*statsapi.ContainerStats, error) { return kl.StatsProvider.GetCgroupCPUAndMemoryStats(cgroupName, updateStats) } //该函数是一个委托函数，将获取cgroup的CPU和内存统计信息的任务委托给实现了stats.Provider接口的StatsProvider对象。 //函数接收一个cgroup名称和一个更新统计信息的布尔值作为参数，并返回一个指向statsapi.ContainerStats的指针和一个错误。 // RootFsStats is delegated to StatsProvider, which implements stats.Provider interface func (kl *Kubelet) RootFsStats() (*statsapi.FsStats, error) { return kl.StatsProvider.RootFsStats() } //这个函数的功能是获取Kubelet根文件系统的统计信息。 //它将请求委托给实现了stats.Provider接口的StatsProvider对象的RootFsStats方法， //返回一个包含文件系统统计信息的statsapi.FsStats指针和一个错误值。 // RlimitStats is delegated to StatsProvider, which implements stats.Provider interface func (kl *Kubelet) RlimitStats() (*statsapi.RlimitStats, error) { return kl.StatsProvider.RlimitStats() } //该函数是Kubelet的RlimitStats方法，它被委托给StatsProvider来实现stats.Provider接口。 //该方法返回一个*statsapi.RlimitStats类型的对象和一个错误（如果有的话）。 // setupDataDirs creates: // 1. the root directory // 2. the pods directory // 3. the plugins directory // 4. the pod-resources directory // 5. the checkpoint directory // 6. the pod logs root directory func (kl *Kubelet) setupDataDirs() error { if cleanedRoot := filepath.Clean(kl.rootDirectory); cleanedRoot != kl.rootDirectory { return fmt.Errorf(\u0026#34;rootDirectory not in canonical form: expected %s, was %s\u0026#34;, cleanedRoot, kl.rootDirectory) } //该函数用于设置Kubelet的数据目录。 //首先，它通过filepath.Clean函数清理kl.rootDirectory根目录的路径， //然后检查清理后的路径是否与原始路径不同。 //如果不同，则返回一个错误，指出根目录路径不是规范形式。 pluginRegistrationDir := kl.getPluginsRegistrationDir() pluginsDir := kl.getPluginsDir() if err := os.MkdirAll(kl.getRootDir(), 0750); err != nil { return fmt.Errorf(\u0026#34;error creating root directory: %v\u0026#34;, err) } if err := os.MkdirAll(kl.getPodLogsDir(), 0750); err != nil { return fmt.Errorf(\u0026#34;error creating pod logs root directory %q: %w\u0026#34;, kl.getPodLogsDir(), err) } //这段Go代码主要实现了创建Kubernetes集群中两个目录的功能。 //1. 首先，通过调用kl.getPluginsRegistrationDir()方法获取插件注册目录的路径，然后调用os.MkdirAll()方法创建该目录， //如果创建失败，则返回错误信息。 //2. 接着，通过调用kl.getPluginsDir()方法获取插件目录的路径，同样使用os.MkdirAll()方法创建该目录，如果创建失败，则返回错误信息。 //3. 最后，通过调用kl.getRootDir()方法获取根目录的路径，并使用os.MkdirAll()方法创建该目录，如果创建失败，则返回错误信息。 //需要注意的是，创建目录时的权限设置为0750，即所有者具有读、写和执行权限，而组和其他用户只具有读和执行权限。 //总结起来，这段代码的主要作用是在Kubernetes集群中创建插件注册目录、插件目录和Pod日志目录。 //该函数的作用是创建Kubelet的数据目录，包括根目录、Pods目录、插件目录、Pod资源目录、检查点目录和Pod日志根目录。 //函数首先检查根目录的规范性，然后创建各个目录，并返回可能发生的错误。 if err := kl.hostutil.MakeRShared(kl.getRootDir()); err != nil { return fmt.Errorf(\u0026#34;error configuring root directory: %v\u0026#34;, err) } //该函数尝试将根目录配置为共享目录。如果执行失败，函数将返回一个错误消息。 if err := os.MkdirAll(kl.getPodsDir(), 0750); err != nil { return fmt.Errorf(\u0026#34;error creating pods directory: %v\u0026#34;, err) } if err := os.MkdirAll(kl.getPluginsDir(), 0750); err != nil { return fmt.Errorf(\u0026#34;error creating plugins directory: %v\u0026#34;, err) } if err := os.MkdirAll(kl.getPluginsRegistrationDir(), 0750); err != nil { return fmt.Errorf(\u0026#34;error creating plugins registry directory: %v\u0026#34;, err) } //这段Go代码中包含了一个匿名的if条件语句， //其主要功能是创建三个不同的目录：pod目录、插件目录和插件注册目录。 //具体来说，它通过调用os.MkdirAll函数来创建目录，并使用kl.getPodsDir()、 //kl.getPluginsDir()和kl.getPluginsRegistrationDir()方法分别获取目录的路径。 //如果创建目录时出现错误，函数会返回一个自定义的错误信息。 if err := os.MkdirAll(kl.getPodResourcesDir(), 0750); err != nil { return fmt.Errorf(\u0026#34;error creating podresources directory: %v\u0026#34;, err) } //该函数的主要功能是在给定路径上创建一个目录。 //具体而言，它通过调用kl.getPodResourcesDir()获取目录路径，然后使用os.MkdirAll系统调用创建该目录及其所有父目录（如果不存在）。 //创建目录时指定的权限为0750。如果在创建目录的过程中遇到任何错误，函数将返回一个格式化的错误信息字符串，指示出无法创建目录的原因。 if utilfeature.DefaultFeatureGate.Enabled(features.ContainerCheckpoint) { if err := os.MkdirAll(kl.getCheckpointsDir(), 0700); err != nil { return fmt.Errorf(\u0026#34;error creating checkpoint directory: %v\u0026#34;, err) } } //这段Go代码是条件语句，首先检查utilfeature.DefaultFeatureGate是否启用了features.ContainerCheckpoint特性， //如果启用了，则尝试创建一个目录，目录路径通过kl.getCheckpointsDir()方法获取，权限设置为0700。 //如果创建目录过程中出现错误，则返回一个错误信息。 if selinux.GetEnabled() { err := selinux.SetFileLabel(pluginRegistrationDir, config.KubeletPluginsDirSELinuxLabel) if err != nil { klog.InfoS(\u0026#34;Unprivileged containerized plugins might not work, could not set selinux context on plugin registration dir\u0026#34;, \u0026#34;path\u0026#34;, pluginRegistrationDir, \u0026#34;err\u0026#34;, err) } err = selinux.SetFileLabel(pluginsDir, config.KubeletPluginsDirSELinuxLabel) if err != nil { klog.InfoS(\u0026#34;Unprivileged containerized plugins might not work, could not set selinux context on plugins dir\u0026#34;, \u0026#34;path\u0026#34;, pluginsDir, \u0026#34;err\u0026#34;, err) } } return nil } //该函数主要功能是检查Selinux是否启用，如果启用，则尝试为插件注册目录和插件目录设置Selinux上下文。 //如果设置失败，函数会记录一条信息但不会影响程序执行，最后返回nil。 // StartGarbageCollection starts garbage collection threads. func (kl *Kubelet) StartGarbageCollection() { loggedContainerGCFailure := false go wait.Until(func() { ctx := context.Background() if err := kl.containerGC.GarbageCollect(ctx); err != nil { klog.ErrorS(err, \u0026#34;Container garbage collection failed\u0026#34;) kl.recorder.Eventf(kl.nodeRef, v1.EventTypeWarning, events.ContainerGCFailed, err.Error()) loggedContainerGCFailure = true //该函数是Kubelet的StartGarbageCollection方法，用于启动垃圾回收线程。 //它首先定义了一个布尔变量loggedContainerGCFailure，并初始化为false，用于记录是否已经记录了容器垃圾回收失败的日志。 //然后使用goroutine启动一个无限循环，定期执行垃圾回收操作。 //在每次循环中，通过调用kl.containerGC.GarbageCollect方法执行垃圾回收，并检查返回的错误。 //如果发生错误，会使用klog记录错误日志，并通过kl.recorder.Eventf方法向事件记录器发送一条警告事件，通知容器垃圾回收失败。 //同时，将loggedContainerGCFailure设置为true，表示已经记录了容器垃圾回收失败的日志。 //注意，以上只是函数的一部分，代码片段在记录了日志后就中断了，没有展示完整的函数实现。 } else { var vLevel klog.Level = 4 if loggedContainerGCFailure { vLevel = 1 loggedContainerGCFailure = false } //这段Go代码是条件语句的else部分，它声明了一个名为vLevel的变量并将其初始化为4。 //接着，它检查loggedContainerGCFailure变量的值，如果是true，则将vLevel设置为1，并将loggedContainerGCFailure重置为false。 //这段代码的作用是根据loggedContainerGCFailure的值来设置vLevel的值，并重置loggedContainerGCFailure。 klog.V(vLevel).InfoS(\u0026#34;Container garbage collection succeeded\u0026#34;) } }, ContainerGCPeriod, wait.NeverStop) //这段这段GoGo代码是使用Kubernetes的logging代码是使用Kubernetes的logging库k库klog，来记录容器垃圾收集log， //来记录容器垃圾收集成功的日成功的日志信息。函数体内部志信息。 //函数体内部使用了klog使用了klog的InfoS方法，的InfoS方法，该方法用于输出该方法用于输出日志信息。 //日志信息。其中，vLevel是其中，vLevel是日志级别，日志级别，根据具体需求设定。 //根据具体需求设定。 //该函数作为一个该函数作为一个匿名函数被传递给一个匿名函数被传递给一个名为After的名为After的函数， //并设置了一个定时器函数，并设置了一个定时器，使得，使得该函数会在ContainerGCPeriod时间该函数会在ContainerGCPeriod时间后执行后执行。 //wait.NeverStop则表示该函数。wait.NeverStop则表示该函数会会一直等待执行，不会停止。 // when the high threshold is set to 100, and the max age is 0 (or the max age feature is disabled) // stub the image GC manager if kl.kubeletConfiguration.ImageGCHighThresholdPercent == 100 \u0026amp;\u0026amp; (!utilfeature.DefaultFeatureGate.Enabled(features.ImageMaximumGCAge) || kl.kubeletConfiguration.ImageMaximumGCAge.Duration == 0) { klog.V(2).InfoS(\u0026#34;ImageGCHighThresholdPercent is set 100 and ImageMaximumGCAge is 0, Disable image GC\u0026#34;) return } //这段代码的功能是检查Kubelet配置中的图像垃圾回收高阈值是否设置为100，并且最大年龄是否为0或最大年龄功能是否禁用。 //如果是，则禁用图像垃圾回收，并记录相关信息。 prevImageGCFailed := false beganGC := time.Now() go wait.Until(func() { ctx := context.Background() if err := kl.imageManager.GarbageCollect(ctx, beganGC); err != nil { if prevImageGCFailed { klog.ErrorS(err, \u0026#34;Image garbage collection failed multiple times in a row\u0026#34;) // Only create an event for repeated failures kl.recorder.Eventf(kl.nodeRef, v1.EventTypeWarning, events.ImageGCFailed, err.Error()) } else { klog.ErrorS(err, \u0026#34;Image garbage collection failed once. Stats initialization may not have completed yet\u0026#34;) } prevImageGCFailed = true //这个Go函数用于定期执行镜像垃圾回收操作。 //具体来说，它通过wait.Until函数以一定间隔调用一个匿名函数，该匿名函数执行以下操作： //1. 创建一个后台上下文ctx。 //2. 调用kl.imageManager.GarbageCollect方法执行镜像垃圾回收操作，传入ctx和开始垃圾回收的时间beganGC作为参数。 //3. 如果垃圾回收操作出现错误： //- 如果之前垃圾回收操作也失败过，则记录错误日志，并且创建一个事件通知，表示垃圾回收失败多次。 //- 如果之前垃圾回收操作成功过，则记录错误日志，提示垃圾回收操作失败一次，可能是统计信息初始化未完成。 //4. 将prevImageGCFailed标志设置为true，表示垃圾回收操作失败。 } else { var vLevel klog.Level = 4 if prevImageGCFailed { vLevel = 1 prevImageGCFailed = false } //这段Go代码是条件语句的else部分，它首先声明了一个名为vLevel的klog.Level类型变量，并将其初始化为4。 //然后，通过判断prevImageGCFailed的值，如果其为true，则将vLevel设置为1，并将prevImageGCFailed重置为false。 //这段代码的主要作用是根据prevImageGCFailed的值来设置vLevel的值。 klog.V(vLevel).InfoS(\u0026#34;Image garbage collection succeeded\u0026#34;) } }, ImageGCPeriod, wait.NeverStop) //这是一个Go语言函数，它通过klog库输出一条日志信息。函数体内部使用了klog.V(vLevel).InfoS方法， //参数为\u0026#34;Image garbage collection succeeded\u0026#34;，表示成功进行了镜像垃圾回收。 //该函数每间隔ImageGCPeriod时间执行一次，直到等待过程被终止。 } // initializeModules will initialize internal modules that do not require the container runtime to be up. // Note that the modules here must not depend on modules that are not initialized here. func (kl *Kubelet) initializeModules() error { // Prometheus metrics. metrics.Register( collectors.NewVolumeStatsCollector(kl), collectors.NewLogMetricsCollector(kl.StatsProvider.ListPodStats), ) metrics.SetNodeName(kl.nodeName) servermetrics.Register() //该函数是Kubelet的初始化模块函数，用于初始化内部模块，这些模块不需要容器运行时支持。 //函数主要进行三个方面的操作： //1. 注册Prometheus指标，包括卷统计信息和日志指标； //2. 设置节点名称； //3. 注册服务器指标。 // Setup filesystem directories. if err := kl.setupDataDirs(); err != nil { return err } // If the container logs directory does not exist, create it. if _, err := os.Stat(ContainerLogsDir); err != nil { if err := kl.os.MkdirAll(ContainerLogsDir, 0755); err != nil { return fmt.Errorf(\u0026#34;failed to create directory %q: %v\u0026#34;, ContainerLogsDir, err) } } //这段Go代码主要实现了两个功能： //1. 调用kl.setupDataDirs()函数设置文件系统目录，如果设置失败则返回错误。 //2. 检查容器日志目录ContainerLogsDir是否存在，如果不存在则尝试创建该目录，如果创建失败则返回错误。 //其中，kl是一个对象，kl.os是kl对象中的一个操作系统相关的属性，kl.os.MkdirAll()函数用于创建目录。 // Start the image manager. kl.imageManager.Start() // Start the certificate manager if it was enabled. if kl.serverCertificateManager != nil { kl.serverCertificateManager.Start() } //这段Go代码中包含两个启动操作。 //首先，它启动了一个名为imageManager的图像管理器。 //然后，如果serverCertificateManager不为空，则启动证书管理器。 // Start out of memory watcher. if kl.oomWatcher != nil { if err := kl.oomWatcher.Start(kl.nodeRef); err != nil { return fmt.Errorf(\u0026#34;failed to start OOM watcher: %w\u0026#34;, err) } } // Start resource analyzer kl.resourceAnalyzer.Start() return nil //这段Go代码主要包含两个部分的功能：启动内存溢出监视器（OOM watcher）和启动资源分析器（resource analyzer）。 //如果kl.oomWatcher不为空，则尝试启动OOM watcher，并通过kl.nodeRef传递节点引用。 //如果启动失败，则返回相应的错误信息。 然后，无论如何都会启动kl.resourceAnalyzer。 //最后，如果没有发生错误，则返回nil。 } // initializeRuntimeDependentModules will initialize internal modules that require the container runtime to be up. func (kl *Kubelet) initializeRuntimeDependentModules() { if err := kl.cadvisor.Start(); err != nil { // Fail kubelet and rely on the babysitter to retry starting kubelet. klog.ErrorS(err, \u0026#34;Failed to start cAdvisor\u0026#34;) os.Exit(1) } //该函数是Kubelet的一个方法，用于初始化需要容器运行时才能启动的内部模块。 //主要工作是尝试启动cAdvisor，如果启动失败，则记录错误日志并退出Kubelet进程。 // trigger on-demand stats collection once so that we have capacity information for ephemeral storage. // ignore any errors, since if stats collection is not successful, the container manager will fail to start below. kl.StatsProvider.GetCgroupStats(\u0026#34;/\u0026#34;, true) // Start container manager. node, err := kl.getNodeAnyWay() if err != nil { // Fail kubelet and rely on the babysitter to retry starting kubelet. klog.ErrorS(err, \u0026#34;Kubelet failed to get node info\u0026#34;) os.Exit(1) } //这个Go函数主要执行了两个操作： //1. 首先，它调用kl.StatsProvider.GetCgroupStats(\u0026#34;/\u0026#34;, true)来触发一次按需统计信息的收集，以便我们有关临时存储容量的信息。 //即使统计信息收集不成功，容器管理器也不会启动，因此这里忽略了任何错误。 //2. 然后，它尝试调用kl.getNodeAnyWay()来启动容器管理器。如果获取节点信息失败，则记录错误并退出Kubelet进程， //依赖babysitter来重试启动Kubelet。 // containerManager must start after cAdvisor because it needs filesystem capacity information if err := kl.containerManager.Start(node, kl.GetActivePods, kl.sourcesReady, kl.statusManager, kl.runtimeService, kl.supportLocalStorageCapacityIsolation()); err != nil { // Fail kubelet and rely on the babysitter to retry starting kubelet. klog.ErrorS(err, \u0026#34;Failed to start ContainerManager\u0026#34;) os.Exit(1) } // eviction manager must start after cadvisor because it needs to know if the container runtime has a dedicated imagefs kl.evictionManager.Start(kl.StatsProvider, kl.GetActivePods, kl.PodIsFinished, evictionMonitoringPeriod) //这段Go代码是Kubernetes中的kubelet组件的一部分，用于启动containerManager和evictionManager两个组件。 //首先，如果containerManager启动失败，则会记录错误日志并退出kubelet进程。 //containerManager的启动需要依赖cAdvisor提供的文件系统容量信息。 //然后，evictionManager必须在cadvisor之后启动，因为它需要知道容器运行时是否有专门的imagefs。 //evictionManager的启动需要传入StatsProvider、GetActivePods和PodIsFinished等参数，以及evictionMonitoringPeriod参数。 //总的来说，这段代码用于在kubelet启动过程中初始化和启动containerManager和evictionManager两个组件。 // container log manager must start after container runtime is up to retrieve information from container runtime // and inform container to reopen log file after log rotation. kl.containerLogManager.Start() // Adding Registration Callback function for CSI Driver kl.pluginManager.AddHandler(pluginwatcherapi.CSIPlugin, plugincache.PluginHandler(csi.PluginHandler)) // Adding Registration Callback function for DRA Plugin if utilfeature.DefaultFeatureGate.Enabled(features.DynamicResourceAllocation) { kl.pluginManager.AddHandler(pluginwatcherapi.DRAPlugin, plugincache.PluginHandler(draplugin.NewRegistrationHandler(kl.kubeClient, kl.getNodeAnyWay))) } // Adding Registration Callback function for Device Manager kl.pluginManager.AddHandler(pluginwatcherapi.DevicePlugin, kl.containerManager.GetPluginRegistrationHandler()) //这段代码主要在启动时执行了三个操作： //1. 启动了container log manager，用于从容器运行时检索信息并通知容器在日志轮转后重新打开日志文件。 //2. 为CSI驱动添加了一个注册回调函数。 //3. 根据功能门控的开启情况，为DRA插件和设备管理器分别添加了一个注册回调函数。 // Start the plugin manager klog.V(4).InfoS(\u0026#34;Starting plugin manager\u0026#34;) go kl.pluginManager.Run(kl.sourcesReady, wait.NeverStop) err = kl.shutdownManager.Start() if err != nil { // The shutdown manager is not critical for kubelet, so log failure, but don\u0026#39;t block Kubelet startup if there was a failure starting it. klog.ErrorS(err, \u0026#34;Failed to start node shutdown manager\u0026#34;) } } //这段Go代码主要实现了启动插件管理器和节点关闭管理器的功能。 //1. 首先，通过klog.V(4).InfoS(\u0026#34;Starting plugin manager\u0026#34;)日志记录插件管理器的启动信息。 //2. 然后，使用go关键字开启一个新的goroutine来运行插件管理器，通过kl.pluginManager.Run(kl.sourcesReady, wait.NeverStop)来实现插件管理器的异步执行。 //其中，kl.sourcesReady表示插件管理器的源准备就绪，wait.NeverStop表示永远不停止运行。 //3. 接着，通过kl.shutdownManager.Start()启动节点关闭管理器。如果启动失败，会通过klog.ErrorS(err, \u0026#34;Failed to start node shutdown manager\u0026#34;)记录错误日志， //但不会阻塞Kubelet的启动。 //总结：这段代码的目的是为了启动插件管理器和节点关闭管理器，以实现插件管理和节点关闭的功能。 // Run starts the kubelet reacting to config updates func (kl *Kubelet) Run(updates \u0026lt;-chan kubetypes.PodUpdate) { ctx := context.Background() if kl.logServer == nil { file := http.FileServer(http.Dir(nodeLogDir)) if utilfeature.DefaultFeatureGate.Enabled(features.NodeLogQuery) \u0026amp;\u0026amp; kl.kubeletConfiguration.EnableSystemLogQuery { kl.logServer = http.StripPrefix(\u0026#34;/logs/\u0026#34;, http.HandlerFunc(func(w http.ResponseWriter, req *http.Request) { if nlq, errs := newNodeLogQuery(req.URL.Query()); len(errs) \u0026gt; 0 { http.Error(w, errs.ToAggregate().Error(), http.StatusBadRequest) return //该函数是Kubelet的Run方法，用于启动kubelet并响应配置更新。 //它通过监听updates通道来接收Pod更新信息。在功能上，它主要完成了以下几点： //1. 创建了一个后台上下文ctx。 //2. 检查logServer是否为nil，如果是，则通过file服务器提供节点日志查询服务。 //如果启用了系统日志查询功能（通过DefaultFeatureGate判断）， //则使用http.HandlerFunc处理日志查询请求，并通过newNodeLogQuery解析请求URL的查询参数。如果解析出错，则返回400错误。 //注意：以上是根据代码片段进行的功能描述，可能不完整，具体实现细节还需要结合上下文代码进行理解。 } else if nlq != nil { if req.URL.Path != \u0026#34;/\u0026#34; \u0026amp;\u0026amp; req.URL.Path != \u0026#34;\u0026#34; { http.Error(w, \u0026#34;path not allowed in query mode\u0026#34;, http.StatusNotAcceptable) return } if errs := nlq.validate(); len(errs) \u0026gt; 0 { http.Error(w, errs.ToAggregate().Error(), http.StatusNotAcceptable) return } //这段代码是处理节点日志查询请求的，如果newNodeLogQuery解析成功且请求的URL路径不是\u0026#34;/\u0026#34;或为空，则返回400错误。 //然后通过调用nlq.validate()方法验证查询参数的合法性，如果有错误则返回406错误。 // Validation ensures that the request does not query services and files at the same time if len(nlq.Services) \u0026gt; 0 { journal.ServeHTTP(w, req) return } // Validation ensures that the request does not explicitly query multiple files at the same time if len(nlq.Files) == 1 { // Account for the \\ being used on Windows clients req.URL.Path = filepath.ToSlash(nlq.Files[0]) } } //这段Go代码包含了一个条件判断逻辑。 //首先，它验证请求是否同时查询了服务和文件，如果是，就通过journal.ServeHTTP(w, req)方法处理请求，并返回。 //接着，如果请求没有同时查询服务和文件，它会进一步验证是否显式地查询了多个文件。 //如果请求查询了一个文件，代码会将请求的URL路径中的反斜杠（Windows客户端中使用的路径分隔符）转换为斜杠，然后继续处理请求。 //这段代码的主要目的是确保请求不会同时查询服务和文件，也不会显式地查询多个文件。 // Fall back in case the caller is directly trying to query a file // Example: kubectl get --raw /api/v1/nodes/$name/proxy/logs/foo.log file.ServeHTTP(w, req) })) } else { kl.logServer = http.StripPrefix(\u0026#34;/logs/\u0026#34;, file) //这段Go代码是关于处理HTTP请求的。 首先，如果条件成立，则使用file.ServeHTTP(w, req)函数来处理请求。 //file是一个http.FileServer对象，它表示一个文件服务器，w是响应写入器，用于向客户端发送响应，req是客户端的请求。 //这个函数会根据请求路径从文件服务器中找到对应的文件并将其内容发送给客户端。 //如果条件不成立，则将kl.logServer设置为http.StripPrefix(\u0026#34;/logs/\u0026#34;, file)的结果。http.StripPrefix()函数是一个处理HTTP请求的中间件， //它会将请求路径中的指定前缀去除后再将请求传递给下一个处理程序。 //在这里，它会将路径中的/logs/前缀去除，然后将处理请求的任务交给file对象。 //这样做是为了确保文件服务器能够正确地处理请求中的路径。 } } if kl.kubeClient == nil { klog.InfoS(\u0026#34;No API server defined - no node status update will be sent\u0026#34;) } // Start the cloud provider sync manager if kl.cloudResourceSyncManager != nil { go kl.cloudResourceSyncManager.Run(wait.NeverStop) } if err := kl.initializeModules(); err != nil { kl.recorder.Eventf(kl.nodeRef, v1.EventTypeWarning, events.KubeletSetupFailed, err.Error()) klog.ErrorS(err, \u0026#34;Failed to initialize internal modules\u0026#34;) os.Exit(1) } //这段Go代码是Kubernetes中的一个函数片段，主要功能是初始化Kubelet。 //1. 首先检查kl.kubeClient是否为nil，如果是，则记录一条日志，表示没有定义API服务器，因此不会发送节点状态更新。 //2. 然后，如果kl.cloudResourceSyncManager不为nil，则通过go关键字启动云提供商资源同步管理器的运行。 //3. 最后，调用kl.initializeModules()初始化内部模块，如果初始化失败，则记录事件和错误日志，并通过os.Exit(1)退出程序。 // Start volume manager go kl.volumeManager.Run(kl.sourcesReady, wait.NeverStop) if kl.kubeClient != nil { // Start two go-routines to update the status. // // The first will report to the apiserver every nodeStatusUpdateFrequency and is aimed to provide regular status intervals, // while the second is used to provide a more timely status update during initialization and runs an one-shot update to the apiserver // once the node becomes ready, then exits afterwards. // // Introduce some small jittering to ensure that over time the requests won\u0026#39;t start // accumulating at approximately the same time from the set of nodes due to priority and // fairness effect. go wait.JitterUntil(kl.syncNodeStatus, kl.nodeStatusUpdateFrequency, 0.04, true, wait.NeverStop) go kl.fastStatusUpdateOnce() // start syncing lease go kl.nodeLeaseController.Run(context.Background()) } //这段Go代码主要启动了几个goroutine来管理Kubernetes节点的状态和租约。 //1. 首先使用kl.volumeManager.Run()启动了一个volume manager来管理卷。 //2. 如果kl.kubeClient不为空，则启动了两个goroutine来更新节点状态： //- 第一个goroutine使用wait.JitterUntil()函数，每隔kl.nodeStatusUpdateFrequency时间向API服务器报告一次节点状态，提供规律的状态更新。 //- 第二个goroutine使用kl.fastStatusUpdateOnce()函数，仅在节点准备就绪时向API服务器进行一次状态更新，然后退出。 //3. 最后，使用kl.nodeLeaseController.Run()启动了一个goroutine来同步租约。 go wait.Until(kl.updateRuntimeUp, 5*time.Second, wait.NeverStop) // Set up iptables util rules if kl.makeIPTablesUtilChains { kl.initNetworkUtil() } // Start component sync loops. kl.statusManager.Start() // Start syncing RuntimeClasses if enabled. if kl.runtimeClassManager != nil { kl.runtimeClassManager.Start(wait.NeverStop) } //这段代码是Go语言编写的，主要执行了以下几个操作： //1. 使用wait.Until函数，每隔5秒调用一次kl.updateRuntimeUp函数，并且永远不停止。 //2. 如果kl.makeIPTablesUtilChains为true，则调用kl.initNetworkUtil函数初始化iptables工具规则。 //3. 调用kl.statusManager.Start()函数，启动组件同步循环。 //4. 如果kl.runtimeClassManager不为nil，则调用kl.runtimeClassManager.Start(wait.NeverStop)函数，启动RuntimeClasses的同步。 //这段代码的主要目的是在Kubernetes中启动组件和功能的同步，并设置iptables规则。 // Start the pod lifecycle event generator. kl.pleg.Start() // Start eventedPLEG only if EventedPLEG feature gate is enabled. if utilfeature.DefaultFeatureGate.Enabled(features.EventedPLEG) { kl.eventedPleg.Start() } kl.syncLoop(ctx, updates, kl) } //这段Go代码主要涉及到了Kubernetes中Pod生命周期事件生成器的启动和同步循环的逻辑。 //首先，kl.pleg.Start()启动了Pod生命周期事件生成器，它会监控Pod的状态变化，并生成相应的事件。 //接着，通过判断features.EventedPLEG是否启用，来决定是否启动kl.eventedPleg.Start()。 //EventedPLEG是一种更高效的事件处理机制，当有多个事件发生时，它能够批量处理这些事件，减少处理时间。 //最后，kl.syncLoop(ctx, updates, kl)启动了一个同步循环，它会不断地从updates通道中获取Pod状态的更新，并调用kl对象的方法来处理这些更新。 //这个循环会一直运行，直到ctx上下文被取消或终止。 //总的来说，这段代码主要是启动了Kubernetes中Pod生命周期事件的监控和处理逻辑，确保能够及时地处理Pod状态的变化。 // SyncPod is the transaction script for the sync of a single pod (setting up) // a pod. This method is reentrant and expected to converge a pod towards the // desired state of the spec. The reverse (teardown) is handled in // SyncTerminatingPod and SyncTerminatedPod. If SyncPod exits without error, // then the pod runtime state is in sync with the desired configuration state // (pod is running). If SyncPod exits with a transient error, the next // invocation of SyncPod is expected to make progress towards reaching the // desired state. SyncPod exits with isTerminal when the pod was detected to // have reached a terminal lifecycle phase due to container exits (for // RestartNever or RestartOnFailure) and the next method invoked will be // SyncTerminatingPod. If the pod terminates for any other reason, SyncPod // will receive a context cancellation and should exit as soon as possible. // // Arguments: // // updateType - whether this is a create (first time) or an update, should // only be used for metrics since this method must be reentrant // // pod - the pod that is being set up // // mirrorPod - the mirror pod known to the kubelet for this pod, if any // // podStatus - the most recent pod status observed for this pod which can // be used to determine the set of actions that should be taken during // this loop of SyncPod // // The workflow is: // - If the pod is being created, record pod worker start latency // - Call generateAPIPodStatus to prepare an v1.PodStatus for the pod // - If the pod is being seen as running for the first time, record pod // start latency // - Update the status of the pod in the status manager // - Stop the pod\u0026#39;s containers if it should not be running due to soft // admission // - Ensure any background tracking for a runnable pod is started // - Create a mirror pod if the pod is a static pod, and does not // already have a mirror pod // - Create the data directories for the pod if they do not exist // - Wait for volumes to attach/mount // - Fetch the pull secrets for the pod // - Call the container runtime\u0026#39;s SyncPod callback // - Update the traffic shaping for the pod\u0026#39;s ingress and egress limits // // If any step of this workflow errors, the error is returned, and is repeated // on the next SyncPod call. // // This operation writes all events that are dispatched in order to provide // the most accurate information possible about an error situation to aid debugging. // Callers should not write an event if this operation returns an error. func (kl *Kubelet) SyncPod(ctx context.Context, updateType kubetypes.SyncPodType, pod, mirrorPod *v1.Pod, podStatus *kubecontainer.PodStatus) (isTerminal bool, err error) { ctx, otelSpan := kl.tracer.Start(ctx, \u0026#34;syncPod\u0026#34;, trace.WithAttributes( semconv.K8SPodUIDKey.String(string(pod.UID)), attribute.String(\u0026#34;k8s.pod\u0026#34;, klog.KObj(pod).String()), semconv.K8SPodNameKey.String(pod.Name), attribute.String(\u0026#34;k8s.pod.update_type\u0026#34;, updateType.String()), semconv.K8SNamespaceNameKey.String(pod.Namespace), //该函数是Kubelet的syncPod方法，用于同步Pod的状态。它根据传入的Pod、mirrorPod和podStatus参数，更新Pod的状态， //并返回一个布尔值和一个错误。 //函数使用了OpenTelemetry进行跟踪，并设置了多个属性， //包括Pod的UID、名称、命名空间和更新类型等。 )) klog.V(4).InfoS(\u0026#34;SyncPod enter\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;podUID\u0026#34;, pod.UID) defer func() { klog.V(4).InfoS(\u0026#34;SyncPod exit\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;podUID\u0026#34;, pod.UID, \u0026#34;isTerminal\u0026#34;, isTerminal) otelSpan.End() }() //该函数是Kubelet的syncPod方法，用于同步Pod的状态。 //它根据传入的Pod、mirrorPod和podStatus参数，更新Pod的状态，并返回一个布尔值isTerminal和一个错误err。 //在函数开始时，它通过kl.tracer.Start创建了一个名为syncPod的trace span，并在函数结束时通过otelSpan.End结束该span。 //函数内部的具体逻辑没有在代码片段中展示出来。 //这段Go代码主要使用了klog和OpenTelemetry两个库，功能是在进入和退出SyncPod函数时记录日志，并通过OpenTelemetry跟踪操作。 //- klog.V(4).InfoS(\u0026#34;SyncPod enter\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;podUID\u0026#34;, pod.UID)：记录SyncPod函数进入的日志，日志级别为4， //包含pod的信息和podUID。 //- defer关键字定义了一个延迟执行的函数，会在SyncPod函数退出时执行。 //- klog.V(4).InfoS(\u0026#34;SyncPod exit\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;podUID\u0026#34;, pod.UID, \u0026#34;isTerminal\u0026#34;, isTerminal)： //记录SyncPod函数退出的日志，包含pod的信息、podUID和isTerminal。 //- otelSpan.End()：结束OpenTelemetry的跟踪操作。 // Latency measurements for the main workflow are relative to the // first time the pod was seen by kubelet. var firstSeenTime time.Time if firstSeenTimeStr, ok := pod.Annotations[kubetypes.ConfigFirstSeenAnnotationKey]; ok { firstSeenTime = kubetypes.ConvertToTimestamp(firstSeenTimeStr).Get() } //这段Go代码主要进行了一个延迟测量的操作。 //具体来说，它首先检查了一个名为pod.Annotations的结构体中是否存在键为kubetypes.ConfigFirstSeenAnnotationKey的元素， //如果存在，则将该元素的值转换为时间戳类型并赋值给变量firstSeenTime。 //这个操作主要是为了记录主工作流的延迟时间，而延迟的起始时间是相对于kubelet第一次看到pod的时间来计算的。 // Record pod worker start latency if being created // TODO: make pod workers record their own latencies if updateType == kubetypes.SyncPodCreate { if !firstSeenTime.IsZero() { // This is the first time we are syncing the pod. Record the latency // since kubelet first saw the pod if firstSeenTime is set. metrics.PodWorkerStartDuration.Observe(metrics.SinceInSeconds(firstSeenTime)) } else { klog.V(3).InfoS(\u0026#34;First seen time not recorded for pod\u0026#34;, \u0026#34;podUID\u0026#34;, pod.UID, \u0026#34;pod\u0026#34;, klog.KObj(pod)) } } //该函数用于记录Pod创建时的工作线程启动延迟。 //如果updateType等于kubetypes.SyncPodCreate，则会判断firstSeenTime是否为零，如果不为零， //则记录自kubelet首次看到Pod以来的延迟； //如果为零，则输出日志信息。 // Generate final API pod status with pod and status manager status apiPodStatus := kl.generateAPIPodStatus(pod, podStatus, false) // The pod IP may be changed in generateAPIPodStatus if the pod is using host network. (See #24576) // TODO(random-liu): After writing pod spec into container labels, check whether pod is using host network, and // set pod IP to hostIP directly in runtime.GetPodStatus podStatus.IPs = make([]string, 0, len(apiPodStatus.PodIPs)) for _, ipInfo := range apiPodStatus.PodIPs { podStatus.IPs = append(podStatus.IPs, ipInfo.IP) } if len(podStatus.IPs) == 0 \u0026amp;\u0026amp; len(apiPodStatus.PodIP) \u0026gt; 0 { podStatus.IPs = []string{apiPodStatus.PodIP} } //这段Go代码是用于生成最终的API Pod状态的。 //它首先调用kl.generateAPIPodStatus(pod, podStatus, false)来生成一个API Pod状态对象， //然后根据这个对象来设置podStatus对象的IPs字段。 //如果pod使用了宿主机网络，那么在generateAPIPodStatus函数中可能会更改pod的IP地址。 //代码中还提到，以后可能会将pod的规格写入容器标签，并在runtime.GetPodStatus中直接将pod IP设置为宿主机IP。 //最后，如果podStatus.IPs为空且apiPodStatus.PodIP不为空，则将apiPodStatus.PodIP作为podStatus.IPs的值。 // If the pod is terminal, we don\u0026#39;t need to continue to setup the pod if apiPodStatus.Phase == v1.PodSucceeded || apiPodStatus.Phase == v1.PodFailed { kl.statusManager.SetPodStatus(pod, apiPodStatus) isTerminal = true return isTerminal, nil } //该函数用于判断Pod是否处于终止状态（PodSucceeded或PodFailed），如果是，则设置Pod状态为终止状态，并返回true。否则返回false。 // If the pod should not be running, we request the pod\u0026#39;s containers be stopped. This is not the same // as termination (we want to stop the pod, but potentially restart it later if soft admission allows // it later). Set the status and phase appropriately runnable := kl.canRunPod(pod) if !runnable.Admit { // Pod is not runnable; and update the Pod and Container statuses to why. if apiPodStatus.Phase != v1.PodFailed \u0026amp;\u0026amp; apiPodStatus.Phase != v1.PodSucceeded { apiPodStatus.Phase = v1.PodPending } apiPodStatus.Reason = runnable.Reason apiPodStatus.Message = runnable.Message // Waiting containers are not creating. const waitingReason = \u0026#34;Blocked\u0026#34; //该函数用于判断Pod是否应该运行，并根据判断结果更新Pod的状态和阶段。 //如果Pod不可运行，则将其状态和阶段更新为Pending，并设置相应的Reason和Message。 //其中，如果Pod的状态不是Failed或Succeeded，则将其阶段更新为Pending。 for _, cs := range apiPodStatus.InitContainerStatuses { if cs.State.Waiting != nil { cs.State.Waiting.Reason = waitingReason } } for _, cs := range apiPodStatus.ContainerStatuses { if cs.State.Waiting != nil { cs.State.Waiting.Reason = waitingReason } } } //这段Go代码中有两个for循环，分别遍历了apiPodStatus.InitContainerStatuses和apiPodStatus.ContainerStatuses两个切片。 //其中，如果遍历到的ContainerStatus的State.Waiting不为nil，则将其Reason字段设置为waitingReason。 //这段代码的作用是给Pod中所有处于等待状态的初始化容器和容器设置等待原因。 // Record the time it takes for the pod to become running // since kubelet first saw the pod if firstSeenTime is set. existingStatus, ok := kl.statusManager.GetPodStatus(pod.UID) if !ok || existingStatus.Phase == v1.PodPending \u0026amp;\u0026amp; apiPodStatus.Phase == v1.PodRunning \u0026amp;\u0026amp; !firstSeenTime.IsZero() { metrics.PodStartDuration.Observe(metrics.SinceInSeconds(firstSeenTime)) } //这段Go代码的功能是记录Pod从kubelet首次看到它到变为运行状态所花费的时间。 //具体来说，它首先从kl.statusManager获取Pod的现有状态， //如果获取成功并且Pod的现有状态为Pending，而apiPodStatus的Phase为Running， //并且firstSeenTime不为零，则使用metrics.SinceInSeconds函数记录从firstSeenTime到当前时间的时间差， //并将其作为观察值传递给metrics.PodStartDuration。 kl.statusManager.SetPodStatus(pod, apiPodStatus) // Pods that are not runnable must be stopped - return a typed error to the pod worker if !runnable.Admit { klog.V(2).InfoS(\u0026#34;Pod is not runnable and must have running containers stopped\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;podUID\u0026#34;, pod.UID, \u0026#34;message\u0026#34;, runnable.Message) var syncErr error p := kubecontainer.ConvertPodStatusToRunningPod(kl.getRuntime().Type(), podStatus) if err := kl.killPod(ctx, pod, p, nil); err != nil { if !wait.Interrupted(err) { kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedToKillPod, \u0026#34;error killing pod: %v\u0026#34;, err) syncErr = fmt.Errorf(\u0026#34;error killing pod: %w\u0026#34;, err) utilruntime.HandleError(syncErr) //该函数用于设置Pod的状态，并根据Pod是否可运行来决定是否停止Pod。如果Pod不可运行，则通过kl.killPod函数停止Pod，并记录事件和错误信息。 } } else { // There was no error killing the pod, but the pod cannot be run. // Return an error to signal that the sync loop should back off. syncErr = fmt.Errorf(\u0026#34;pod cannot be run: %v\u0026#34;, runnable.Message) } return false, syncErr //这段Go代码是一个函数的一部分，根据给定的条件执行不同的操作，并返回两个值：一个布尔值和一个错误。 //首先，它检查是否有错误杀死Pod（容器），如果有，则将错误记录并返回。 //如果没有错误，但Pod无法运行，则将自定义错误返回，以指示同步循环应退避。 //最后，该函数返回false和同步错误。 } // If the network plugin is not ready, only start the pod if it uses the host network if err := kl.runtimeState.networkErrors(); err != nil \u0026amp;\u0026amp; !kubecontainer.IsHostNetworkPod(pod) { kl.recorder.Eventf(pod, v1.EventTypeWarning, events.NetworkNotReady, \u0026#34;%s: %v\u0026#34;, NetworkNotReadyErrorMsg, err) return false, fmt.Errorf(\u0026#34;%s: %v\u0026#34;, NetworkNotReadyErrorMsg, err) } //该函数主要检查网络插件是否准备就绪，如果未就绪且Pod不使用宿主机网络，则记录事件并返回错误。 // ensure the kubelet knows about referenced secrets or configmaps used by the pod if !kl.podWorkers.IsPodTerminationRequested(pod.UID) { if kl.secretManager != nil { kl.secretManager.RegisterPod(pod) } if kl.configMapManager != nil { kl.configMapManager.RegisterPod(pod) } } //该函数用于确保kubelet知道pod中使用到的secret或configmap。它首先检查pod是否被请求终止， //如果不是，则分别调用secretManager和configMapManager的RegisterPod方法来注册pod。 // Create Cgroups for the pod and apply resource parameters // to them if cgroups-per-qos flag is enabled. pcm := kl.containerManager.NewPodContainerManager() // If pod has already been terminated then we need not create // or update the pod\u0026#39;s cgroup // TODO: once context cancellation is added this check can be removed if !kl.podWorkers.IsPodTerminationRequested(pod.UID) { // When the kubelet is restarted with the cgroups-per-qos // flag enabled, all the pod\u0026#39;s running containers // should be killed intermittently and brought back up // under the qos cgroup hierarchy. // Check if this is the pod\u0026#39;s first sync firstSync := true //这段Go代码是Kubernetes中的一个片段，用于创建和管理Pod的Cgroups。 //函数首先创建一个PodContainerManager实例，然后检查Pod是否已经被终止。 //如果Pod没有被终止，则会检查是否是Pod的第一个同步。如果是第一次同步，将会为Pod创建或更新Cgroups，并应用资源参数。 //这段代码的主要目的是在Kubernetes中管理Pod的资源限制和隔离。 for _, containerStatus := range apiPodStatus.ContainerStatuses { if containerStatus.State.Running != nil { firstSync = false break } } // Don\u0026#39;t kill containers in pod if pod\u0026#39;s cgroups already // exists or the pod is running for the first time podKilled := false //这段Go代码是用于遍历Pod中的容器状态，检查是否有正在运行的容器。 //如果存在正在运行的容器，则将firstSync标记为false并退出循环。 //接下来，根据podKilled的值决定是否终止Pod中的容器。 if !pcm.Exists(pod) \u0026amp;\u0026amp; !firstSync { p := kubecontainer.ConvertPodStatusToRunningPod(kl.getRuntime().Type(), podStatus) if err := kl.killPod(ctx, pod, p, nil); err == nil { if wait.Interrupted(err) { return false, err } podKilled = true } else { klog.ErrorS(err, \u0026#34;KillPod failed\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;podStatus\u0026#34;, podStatus) } } //这段Go代码是一个条件判断语句，其主要功能是在特定条件下杀死一个Pod。 //具体来说，函数首先检查Pod是否存在于pcm中，以及firstSync是否为false。 //如果两个条件都满足，则会将podStatus转换为一个运行中的Pod，并尝试通过killPod函数杀死该Pod。 //如果杀死操作成功，且返回的错误是由于等待中断导致的，则返回false和该错误。 //如果杀死操作失败，则会记录一个错误日志。 // Create and Update pod\u0026#39;s Cgroups // Don\u0026#39;t create cgroups for run once pod if it was killed above // The current policy is not to restart the run once pods when // the kubelet is restarted with the new flag as run once pods are // expected to run only once and if the kubelet is restarted then // they are not expected to run again. // We don\u0026#39;t create and apply updates to cgroup if its a run once pod and was killed above if !(podKilled \u0026amp;\u0026amp; pod.Spec.RestartPolicy == v1.RestartPolicyNever) { if !pcm.Exists(pod) { if err := kl.containerManager.UpdateQOSCgroups(); err != nil { klog.V(2).InfoS(\u0026#34;Failed to update QoS cgroups while syncing pod\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;err\u0026#34;, err) } if err := pcm.EnsureExists(pod); err != nil { kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedToCreatePodContainer, \u0026#34;unable to ensure pod container exists: %v\u0026#34;, err) return false, fmt.Errorf(\u0026#34;failed to ensure that the pod: %v cgroups exist and are correctly applied: %v\u0026#34;, pod.UID, err) } } } } //这段Go代码是Kubernetes的一部分，用于创建和更新Pod的Cgroups。Cgroups是Linux操作系统中的一个功能，可以限制和监控进程的资源使用。 //这段代码首先检查Pod是否被杀死且重启策略为Never，如果是，则不创建Cgroups。 //然后，它检查Pod的Cgroups是否存在，如果不存在，则尝试更新QoS cgroups并确保Pod的Cgroups存在并正确应用。 //如果更新QoS cgroups或确保Cgroups存在失败，则记录事件并返回错误。 // Create Mirror Pod for Static Pod if it doesn\u0026#39;t already exist if kubetypes.IsStaticPod(pod) { deleted := false if mirrorPod != nil { if mirrorPod.DeletionTimestamp != nil || !kubepod.IsMirrorPodOf(mirrorPod, pod) { // The mirror pod is semantically different from the static pod. Remove // it. The mirror pod will get recreated later. klog.InfoS(\u0026#34;Trying to delete pod\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;podUID\u0026#34;, mirrorPod.ObjectMeta.UID) podFullName := kubecontainer.GetPodFullName(pod) var err error deleted, err = kl.mirrorPodClient.DeleteMirrorPod(podFullName, \u0026amp;mirrorPod.ObjectMeta.UID) if deleted { klog.InfoS(\u0026#34;Deleted mirror pod because it is outdated\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(mirrorPod)) } else if err != nil { klog.ErrorS(err, \u0026#34;Failed deleting mirror pod\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(mirrorPod)) } } } //这段Go代码主要功能是检查给定的Pod是否为静态Pod，如果是，则检查是否存在与之对应的镜像Pod。 //如果存在但与静态Pod语义不同或已被标记删除，则删除该镜像Pod。 //具体流程如下： //1. 首先判断给定的Pod是否为静态Pod，如果是则继续执行后续逻辑。 //2. 检查是否存在镜像Pod，如果存在则进一步判断镜像Pod是否与静态Pod语义相同或未被标记删除。 //3. 如果镜像Pod与静态Pod语义不同或已被标记删除，则尝试删除该镜像Pod。 //4. 如果删除成功，则记录日志；如果删除失败，则记录错误日志。 //该函数通过调用mirrorPodClient的DeleteMirrorPod方法来删除镜像Pod。 if mirrorPod == nil || deleted { node, err := kl.GetNode() if err != nil { klog.V(4).ErrorS(err, \u0026#34;No need to create a mirror pod, since failed to get node info from the cluster\u0026#34;, \u0026#34;node\u0026#34;, klog.KRef(\u0026#34;\u0026#34;, string(kl.nodeName))) } else if node.DeletionTimestamp != nil { klog.V(4).InfoS(\u0026#34;No need to create a mirror pod, since node has been removed from the cluster\u0026#34;, \u0026#34;node\u0026#34;, klog.KRef(\u0026#34;\u0026#34;, string(kl.nodeName))) } else { klog.V(4).InfoS(\u0026#34;Creating a mirror pod for static pod\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod)) if err := kl.mirrorPodClient.CreateMirrorPod(pod); err != nil { klog.ErrorS(err, \u0026#34;Failed creating a mirror pod for\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod)) } } } } //这段代码主要功能是根据条件判断是否需要创建一个镜像Pod。 //如果mirrorPod为nil或deleted标志为true，则执行以下操作： //1. 通过kl.GetNode()获取节点信息。 //2. 如果获取节点信息时发生错误，则记录错误日志。 //3. 如果节点信息获取成功，检查节点是否已被删除。 //4. 如果节点已被删除，则记录一条信息日志。 //5. 如果节点未被删除，则记录一条信息日志，表示正在创建镜像Pod。 //6. 调用kl.mirrorPodClient.CreateMirrorPod(pod)尝试创建镜像Pod，如果创建失败则记录错误日志。 //这段代码通过日志记录了操作的结果，并根据条件判断是否需要创建镜像Pod。 // Make data directories for the pod if err := kl.makePodDataDirs(pod); err != nil { kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedToMakePodDataDirectories, \u0026#34;error making pod data directories: %v\u0026#34;, err) klog.ErrorS(err, \u0026#34;Unable to make pod data directories for pod\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod)) return false, err } //该函数是用于为Pod创建数据目录的。它首先调用kl.makePodDataDirs(pod)函数来创建目录，如果创建失败，则记录事件并返回错误。 // Wait for volumes to attach/mount if err := kl.volumeManager.WaitForAttachAndMount(ctx, pod); err != nil { if !wait.Interrupted(err) { kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedMountVolume, \u0026#34;Unable to attach or mount volumes: %v\u0026#34;, err) klog.ErrorS(err, \u0026#34;Unable to attach or mount volumes for pod; skipping pod\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod)) } return false, err } //这段代码的功能是等待卷附加和挂载操作完成。 //具体而言，它通过调用kl.volumeManager.WaitForAttachAndMount(ctx, pod)来等待卷的附加和挂载操作完成。 //如果操作失败且不是由于中断引起，则会通过kl.recorder.Eventf()记录事件，并通过klog.ErrorS()记录错误日志，并最终返回false和错误信息err。 // Fetch the pull secrets for the pod pullSecrets := kl.getPullSecretsForPod(pod) // Ensure the pod is being probed kl.probeManager.AddPod(pod) if utilfeature.DefaultFeatureGate.Enabled(features.InPlacePodVerticalScaling) { // Handle pod resize here instead of doing it in HandlePodUpdates because // this conveniently retries any Deferred resize requests // TODO(vinaykul,InPlacePodVerticalScaling): Investigate doing this in HandlePodUpdates + periodic SyncLoop scan // See: https://github.com/kubernetes/kubernetes/pull/102884#discussion_r663160060 if kl.podWorkers.CouldHaveRunningContainers(pod.UID) \u0026amp;\u0026amp; !kubetypes.IsStaticPod(pod) { pod = kl.handlePodResourcesResize(pod) } } //这段Go代码是Kubernetes中kubelet的一个片段，主要功能是处理Pod的镜像拉取凭证、添加Pod到探测管理器中、以及处理Pod的资源缩放。 //1. 首先，函数getPullSecretsForPod从kl（kubelet实例）中获取Pod的镜像拉取凭证pullSecrets。 //2. 然后，通过调用probeManager.AddPod方法，将该Pod添加到探测管理器中，以确保Pod被探测。 //3. 接着，通过判断是否启用了InPlacePodVerticalScaling特性，决定是否处理Pod的资源缩放。 //如果启用了该特性，并且当前Pod有运行中的容器且不是静态Pod，则调用handlePodResourcesResize方法处理Pod的资源缩放。 //4. handlePodResourcesResize方法会尝试处理Pod的资源缩放请求，并返回缩放后的Pod对象。 //这段代码的主要目的是在Pod更新时处理镜像拉取凭证、探测和资源缩放等操作。 //其中，资源缩放功能通过检查Pod的状态和特性门控来决定是否执行，以实现Pod的垂直缩放。 // TODO(#113606): use cancellation from the incoming context parameter, which comes from the pod worker. // Currently, using cancellation from that context causes test failures. To remove this WithoutCancel, // any wait.Interrupted errors need to be filtered from result and bypass the reasonCache - cancelling // the context for SyncPod is a known and deliberate error, not a generic error. // Use WithoutCancel instead of a new context.TODO() to propagate trace context // Call the container runtime\u0026#39;s SyncPod callback sctx := context.WithoutCancel(ctx) result := kl.containerRuntime.SyncPod(sctx, pod, podStatus, pullSecrets, kl.backOff) kl.reasonCache.Update(pod.UID, result) if err := result.Error(); err != nil { // Do not return error if the only failures were pods in backoff for _, r := range result.SyncResults { if r.Error != kubecontainer.ErrCrashLoopBackOff \u0026amp;\u0026amp; r.Error != images.ErrImagePullBackOff { // Do not record an event here, as we keep all event logging for sync pod failures // local to container runtime, so we get better errors. return false, err } } return false, nil } //这个Go函数用于同步Pod，并执行以下操作： //- 创建一个不取消的上下文sctx。 //- 调用容器运行时的SyncPod回调函数，传入sctx、Pod、Pod状态、pullSecrets和backOff。 //- 使用结果更新Pod的状态。 //- 如果有错误，遍历结果中的同步结果，如果错误不是ErrCrashLoopBackOff或ErrImagePullBackOff，则返回错误。 //- 否则，返回false和nil。 if utilfeature.DefaultFeatureGate.Enabled(features.InPlacePodVerticalScaling) \u0026amp;\u0026amp; isPodResizeInProgress(pod, \u0026amp;apiPodStatus) { // While resize is in progress, periodically call PLEG to update pod cache runningPod := kubecontainer.ConvertPodStatusToRunningPod(kl.getRuntime().Type(), podStatus) if err, _ := kl.pleg.UpdateCache(\u0026amp;runningPod, pod.UID); err != nil { klog.ErrorS(err, \u0026#34;Failed to update pod cache\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod)) return false, err } } return false, nil } //这段Go代码的功能是在Kubernetes中处理Pod的垂直扩展。如果启用了InPlacePodVerticalScaling功能并且Pod正在进行大小调整， //则会定期调用PLEG来更新Pod的缓存。 //如果更新失败，则会记录错误并返回错误信息。如果条件不满足，则直接返回false和nil。 // SyncTerminatingPod is expected to terminate all running containers in a pod. Once this method // returns without error, the pod is considered to be terminated and it will be safe to clean up any // pod state that is tied to the lifetime of running containers. The next method invoked will be // SyncTerminatedPod. This method is expected to return with the grace period provided and the // provided context may be cancelled if the duration is exceeded. The method may also be interrupted // with a context cancellation if the grace period is shortened by the user or the kubelet (such as // during eviction). This method is not guaranteed to be called if a pod is force deleted from the // configuration and the kubelet is restarted - SyncTerminatingRuntimePod handles those orphaned // pods. func (kl *Kubelet) SyncTerminatingPod(_ context.Context, pod *v1.Pod, podStatus *kubecontainer.PodStatus, gracePeriod *int64, podStatusFn func(*v1.PodStatus)) error { // TODO(#113606): connect this with the incoming context parameter, which comes from the pod worker. // Currently, using that context causes test failures. ctx, otelSpan := kl.tracer.Start(context.Background(), \u0026#34;syncTerminatingPod\u0026#34;, trace.WithAttributes( semconv.K8SPodUIDKey.String(string(pod.UID)), attribute.String(\u0026#34;k8s.pod\u0026#34;, klog.KObj(pod).String()), semconv.K8SPodNameKey.String(pod.Name), semconv.K8SNamespaceNameKey.String(pod.Namespace), )) defer otelSpan.End() klog.V(4).InfoS(\u0026#34;SyncTerminatingPod enter\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;podUID\u0026#34;, pod.UID) defer klog.V(4).InfoS(\u0026#34;SyncTerminatingPod exit\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;podUID\u0026#34;, pod.UID) //该函数是Kubelet的一个方法，用于同步终止Pod中的所有运行中的容器。 //当此方法在没有错误的情况下返回时，Pod被认为已经终止，并且可以安全地清理与运行容器的生命周期相关的任何Pod状态。 //下一个调用的方法将是SyncTerminatedPod。 //此方法期望在提供的优雅期间内返回，并且提供的上下文可能会在持续时间超过时被取消。 //该方法也可能因用户或kubelet（如驱逐期间）缩短优雅期间而被上下文取消。 //如果Pod被强制从配置中删除，并且kubelet被重新启动，则不会调用此方法 //- SyncTerminatingRuntimePod处理这些孤儿Pod。 //函数内部使用了OpenTelemetry进行日志记录和跟踪，并通过klog记录进入和退出函数的日志。 apiPodStatus := kl.generateAPIPodStatus(pod, podStatus, false) if podStatusFn != nil { podStatusFn(\u0026amp;apiPodStatus) } kl.statusManager.SetPodStatus(pod, apiPodStatus) //这段Go代码中的函数实现了以下功能： //- 首先，它调用kl.generateAPIPodStatus(pod, podStatus, false)生成一个apiPodStatus对象。 //- 然后，如果podStatusFn不为空，它将调用podStatusFn(\u0026amp;apiPodStatus)来对apiPodStatus进行进一步的处理。 //- 最后，它调用kl.statusManager.SetPodStatus(pod, apiPodStatus)来设置Pod的状态。 //总的来说，这个函数通过调用kl.generateAPIPodStatus生成一个Pod状态对象apiPodStatus，并将其设置为Pod的实际状态。 //如果提供了podStatusFn函数，它还会对apiPodStatus进行额外的处理。 if gracePeriod != nil { klog.V(4).InfoS(\u0026#34;Pod terminating with grace period\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;podUID\u0026#34;, pod.UID, \u0026#34;gracePeriod\u0026#34;, *gracePeriod) } else { klog.V(4).InfoS(\u0026#34;Pod terminating with grace period\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;podUID\u0026#34;, pod.UID, \u0026#34;gracePeriod\u0026#34;, nil) } //这段Go代码主要进行日志记录，根据gracePeriod是否为nil，记录不同级别的日志信息。 //- 如果gracePeriod不为nil，则调用klog.V(4).InfoS函数记录日志， //日志内容包括：\u0026#34;Pod terminating with grace period\u0026#34;、\u0026#34;pod\u0026#34;、\u0026#34;podUID\u0026#34;、\u0026#34;gracePeriod\u0026#34;。 //- 如果gracePeriod为nil，则调用klog.V(4).InfoS函数记录日志， //日志内容包括：\u0026#34;Pod terminating with grace period\u0026#34;、\u0026#34;pod\u0026#34;、\u0026#34;podUID\u0026#34;、\u0026#34;gracePeriod\u0026#34;，其中\u0026#34;gracePeriod\u0026#34;的值为nil。 //这里的klog是一个日志库，V(4)表示日志级别为4，InfoS表示记录信息级别的日志，并可以传入多个键值对参数来丰富日志内容。 //pod和pod.UID是函数的参数，用于提供上下文信息。 kl.probeManager.StopLivenessAndStartup(pod) p := kubecontainer.ConvertPodStatusToRunningPod(kl.getRuntime().Type(), podStatus) if err := kl.killPod(ctx, pod, p, gracePeriod); err != nil { kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedToKillPod, \u0026#34;error killing pod: %v\u0026#34;, err) // there was an error killing the pod, so we return that error directly utilruntime.HandleError(err) return err } //该函数的功能是停止Pod的liveness和startup探针管理器，将Pod状态转换为运行中的Pod，然后尝试杀死Pod。 //如果杀死Pod时出现错误，则记录事件并返回该错误。 // Once the containers are stopped, we can stop probing for liveness and readiness. // TODO: once a pod is terminal, certain probes (liveness exec) could be stopped immediately after // the detection of a container shutdown or (for readiness) after the first failure. Tracked as // https://github.com/kubernetes/kubernetes/issues/107894 although may not be worth optimizing. kl.probeManager.RemovePod(pod) //该函数用于从探针管理器中移除指定的Pod。 //在容器停止后，可以停止对Pod的存活和就绪状态的探测。 //此函数是Kubernetes中的一个组件，用于管理Pod的探针。 //通过调用kl.probeManager.RemovePod(pod)，可以将指定的Pod从探针管理器中移除，从而停止对该Pod的存活和就绪状态的探测。 // Guard against consistency issues in KillPod implementations by checking that there are no // running containers. This method is invoked infrequently so this is effectively free and can // catch race conditions introduced by callers updating pod status out of order. // TODO: have KillPod return the terminal status of stopped containers and write that into the // cache immediately podStatus, err := kl.containerRuntime.GetPodStatus(ctx, pod.UID, pod.Name, pod.Namespace) if err != nil { klog.ErrorS(err, \u0026#34;Unable to read pod status prior to final pod termination\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;podUID\u0026#34;, pod.UID) return err } //这段Go代码中的函数主要功能是在终止Pod之前，通过检查是否有正在运行的容器来防止一致性问题。 //该方法被调用的频率较低，因此基本上是免费的，可以捕获调用者按错误顺序更新Pod状态时引入的竞态条件。 //通过获取Pod的状态来实现检查，如果无法读取Pod状态，则记录错误日志并返回错误。 var runningContainers []string type container struct { Name string State string ExitCode int FinishedAt string } var containers []container klogV := klog.V(4) klogVEnabled := klogV.Enabled() //这段Go代码定义了一个container结构体类型，包含名称、状态、退出码和完成时间四个字段； //同时定义了一个runningContainers切片和一个containers切片。 //klogV和klogVEnabled是klog库的函数调用，用于判断日志级别是否启用。 for _, s := range podStatus.ContainerStatuses { if s.State == kubecontainer.ContainerStateRunning { runningContainers = append(runningContainers, s.ID.String()) } //该Go代码片段是一个for循环，遍历了podStatus.ContainerStatuses切片中的每个元素， //并将处于运行状态（ContainerStateRunning）的容器的ID以字符串形式保存到runningContainers切片中。 if klogVEnabled { containers = append(containers, container{Name: s.Name, State: string(s.State), ExitCode: s.ExitCode, FinishedAt: s.FinishedAt.UTC().Format(time.RFC3339Nano)}) } //这段Go代码是一个条件语句，判断klogVEnabled是否为true，如果是，则将一个容器信息追加到containers切片中。 //追加的容器信息包括容器的名称、状态、退出码和完成时间（使用UTC格式）。 } if klogVEnabled { sort.Slice(containers, func(i, j int) bool { return containers[i].Name \u0026lt; containers[j].Name }) klog.V(4).InfoS(\u0026#34;Post-termination container state\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;podUID\u0026#34;, pod.UID, \u0026#34;containers\u0026#34;, containers) } //这段Go代码是在klogVEnabled为true时对containers进行排序，并通过klog.V(4).InfoS输出日志信息。 //其中，sort.Slice根据containers中Name字段的值对containers进行升序排序； //klog.V(4).InfoS输出的日志信息包括\u0026#34;Post-termination container state\u0026#34;、\u0026#34;pod\u0026#34;、\u0026#34;podUID\u0026#34;和\u0026#34;containers\u0026#34;等字段的值。 if len(runningContainers) \u0026gt; 0 { return fmt.Errorf(\u0026#34;detected running containers after a successful KillPod, CRI violation: %v\u0026#34;, runningContainers) } //该函数检测到运行中的容器后，在成功调用KillPod后返回一个错误，指示CRI违规。 // NOTE: resources must be unprepared AFTER all containers have stopped // and BEFORE the pod status is changed on the API server // to avoid race conditions with the resource deallocation code in kubernetes core. if utilfeature.DefaultFeatureGate.Enabled(features.DynamicResourceAllocation) { if err := kl.UnprepareDynamicResources(pod); err != nil { return err } } //这段Go代码是Kubernetes中的一个函数片段，它的主要功能是在动态资源分配的特性开启时， //调用kl.UnprepareDynamicResources(pod)方法来释放动态资源。 //- 首先，代码通过utilfeature.DefaultFeatureGate.Enabled(features.DynamicResourceAllocation)判断当前是否启用了动态资源分配的特性。 //- 如果特性开启，则调用kl.UnprepareDynamicResources(pod)方法来释放与给定Pod相关的动态资源。 //- 如果在释放资源的过程中出现错误，则函数会返回该错误。 //需要注意的是，该函数的注释指出，在调用该函数释放资源之前，必须确保所有容器已经停止， //并且在修改Pod的状态之前调用，以避免与Kubernetes核心代码中的资源释放逻辑发生竞态条件。 // Compute and update the status in cache once the pods are no longer running. // The computation is done here to ensure the pod status used for it contains // information about the container end states (including exit codes) - when // SyncTerminatedPod is called the containers may already be removed. apiPodStatus = kl.generateAPIPodStatus(pod, podStatus, true) kl.statusManager.SetPodStatus(pod, apiPodStatus) // we have successfully stopped all containers, the pod is terminating, our status is \u0026#34;done\u0026#34; klog.V(4).InfoS(\u0026#34;Pod termination stopped all running containers\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;podUID\u0026#34;, pod.UID) return nil //这段Go代码是一个函数片段，它用于计算和更新Pod的状态缓存，当Pod不再运行时。 //函数首先调用kl.generateAPIPodStatus生成一个API Pod状态，然后使用kl.statusManager.SetPodStatus设置Pod的状态。 //最后，函数返回nil。 //这段代码属于Kubernetes的一部分，用于管理Pod的生命周期。 //在Pod终止时，该函数被调用，确保容器的终止状态（包括退出码）被包含在Pod状态中。 //函数通过设置Pod状态来记录Pod的终止状态。 } // SyncTerminatingRuntimePod is expected to terminate running containers in a pod that we have no // configuration for. Once this method returns without error, any remaining local state can be safely // cleaned up by background processes in each subsystem. Unlike syncTerminatingPod, we lack // knowledge of the full pod spec and so cannot perform lifecycle related operations, only ensure // that the remnant of the running pod is terminated and allow garbage collection to proceed. We do // not update the status of the pod because with the source of configuration removed, we have no // place to send that status. func (kl *Kubelet) SyncTerminatingRuntimePod(_ context.Context, runningPod *kubecontainer.Pod) error { // TODO(#113606): connect this with the incoming context parameter, which comes from the pod worker. // Currently, using that context causes test failures. ctx := context.Background() pod := runningPod.ToAPIPod() klog.V(4).InfoS(\u0026#34;SyncTerminatingRuntimePod enter\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;podUID\u0026#34;, pod.UID) defer klog.V(4).InfoS(\u0026#34;SyncTerminatingRuntimePod exit\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;podUID\u0026#34;, pod.UID) //该函数是Kubelet的一个方法，用于同步终止无配置的Pod中的运行时容器。 //该方法不更新Pod的状态，仅确保运行Pod的剩余部分被终止，以便进行垃圾回收。 //函数首先创建一个背景上下文，然后将运行Pod转换为API Pod。通过记录日志来标记函数的进入和退出。 // we kill the pod directly since we have lost all other information about the pod. klog.V(4).InfoS(\u0026#34;Orphaned running pod terminating without grace period\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;podUID\u0026#34;, pod.UID) // TODO: this should probably be zero, to bypass any waiting (needs fixes in container runtime) gracePeriod := int64(1) if err := kl.killPod(ctx, pod, *runningPod, \u0026amp;gracePeriod); err != nil { kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedToKillPod, \u0026#34;error killing pod: %v\u0026#34;, err) // there was an error killing the pod, so we return that error directly utilruntime.HandleError(err) return err } klog.V(4).InfoS(\u0026#34;Pod termination stopped all running orphaned containers\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;podUID\u0026#34;, pod.UID) return nil } //这段Go代码是一个函数，用于直接终止一个孤立的正在运行的Pod（容器）。 //函数首先记录一条日志信息，表示将要终止Pod，并指出该Pod已经没有其他相关信息。 //然后，函数调用kl.killPod方法来终止Pod，给定的优雅终止时间是1秒。 //如果终止Pod时出现错误，函数会记录一条事件日志，并直接返回该错误。 //最后，函数记录一条日志信息，表示已经成功终止了Pod的所有正在运行的容器，并返回nil表示没有错误。 // SyncTerminatedPod cleans up a pod that has terminated (has no running containers). // The invocations in this call are expected to tear down all pod resources. // When this method exits the pod is expected to be ready for cleanup. This method // reduces the latency of pod cleanup but is not guaranteed to get called in all scenarios. // // Because the kubelet has no local store of information, all actions in this method that modify // on-disk state must be reentrant and be garbage collected by HandlePodCleanups or a separate loop. // This typically occurs when a pod is force deleted from configuration (local disk or API) and the // kubelet restarts in the middle of the action. func (kl *Kubelet) SyncTerminatedPod(ctx context.Context, pod *v1.Pod, podStatus *kubecontainer.PodStatus) error { ctx, otelSpan := kl.tracer.Start(ctx, \u0026#34;syncTerminatedPod\u0026#34;, trace.WithAttributes( semconv.K8SPodUIDKey.String(string(pod.UID)), attribute.String(\u0026#34;k8s.pod\u0026#34;, klog.KObj(pod).String()), semconv.K8SPodNameKey.String(pod.Name), semconv.K8SNamespaceNameKey.String(pod.Namespace), )) defer otelSpan.End() klog.V(4).InfoS(\u0026#34;SyncTerminatedPod enter\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;podUID\u0026#34;, pod.UID) defer klog.V(4).InfoS(\u0026#34;SyncTerminatedPod exit\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;podUID\u0026#34;, pod.UID) //该函数是Kubelet的一个方法，用于清理已经终止（没有运行中的容器）的Pod。 //调用该方法的目的是销毁Pod的所有资源，当方法退出时，Pod应该准备好进行清理。 //该方法可以减少Pod清理的延迟，但在所有场景中都可能不会被调用。 //由于Kubelet没有本地存储信息，该方法中所有修改本地磁盘状态的操作必须是可重入的，并且可以通过HandlePodCleanups或单独的循环进行垃圾回收。 //这通常发生在Pod从配置（本地磁盘或API）中强制删除，并且kubelet在操作中间重新启动时。 //该函数首先通过kl.tracer.Start创建一个名为syncTerminatedPod的跟踪Span，并设置相关属性。 //然后使用defer语句在函数退出时结束该Span。接下来，使用klog.V(4).InfoS记录函数的进入和退出日志。 //函数的主要逻辑部分在这些日志记录和跟踪Span的开启和结束之间。 // generate the final status of the pod // TODO: should we simply fold this into TerminatePod? that would give a single pod update apiPodStatus := kl.generateAPIPodStatus(pod, podStatus, true) kl.statusManager.SetPodStatus(pod, apiPodStatus) // volumes are unmounted after the pod worker reports ShouldPodRuntimeBeRemoved (which is satisfied // before syncTerminatedPod is invoked) if err := kl.volumeManager.WaitForUnmount(ctx, pod); err != nil { return err } klog.V(4).InfoS(\u0026#34;Pod termination unmounted volumes\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;podUID\u0026#34;, pod.UID) //这段Go代码是Kubernetes中Kubelet的一部分，用于处理已终止的Pod的最终状态。 //首先，它调用kl.generateAPIPodStatus(pod, podStatus, true)生成Pod的最终状态。 //这个函数会根据Pod的实际状态和配置生成一个API兼容的Pod状态对象。 //接下来，它调用kl.statusManager.SetPodStatus(pod, apiPodStatus)将生成的Pod状态设置为Kubernetes API中记录的状态。 //最后，它调用kl.volumeManager.WaitForUnmount(ctx, pod)等待所有卷被卸载。 //这个函数会等待所有与Pod相关的卷被成功卸载，以确保Pod的所有资源都被正确清理。 //如果卷卸载失败，则会返回错误，终止Pod的处理将停止。 //如果成功，则会记录一条日志消息，表示Pod的卸载已经完成。 if !kl.keepTerminatedPodVolumes { // This waiting loop relies on the background cleanup which starts after pod workers respond // true for ShouldPodRuntimeBeRemoved, which happens after `SyncTerminatingPod` is completed. if err := wait.PollUntilContextCancel(ctx, 100*time.Millisecond, true, func(ctx context.Context) (bool, error) { volumesExist := kl.podVolumesExist(pod.UID) if volumesExist { klog.V(3).InfoS(\u0026#34;Pod is terminated, but some volumes have not been cleaned up\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;podUID\u0026#34;, pod.UID) } return !volumesExist, nil }); err != nil { return err } klog.V(3).InfoS(\u0026#34;Pod termination cleaned up volume paths\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;podUID\u0026#34;, pod.UID) } //这段Go代码是一个等待循环，用于检查已终止的Pod的卷是否已被清理。 //如果kl.keepTerminatedPodVolumes为false，则会进入循环，通过wait.PollUntilContextCancel函数每隔100毫秒检查一次。 //循环会一直进行，直到Pod的卷不存在，或者上下文被取消。如果循环结束且没有错误发生，则会记录一条日志表示Pod终止时清理了卷路径。 // After volume unmount is complete, let the secret and configmap managers know we\u0026#39;re done with this pod if kl.secretManager != nil { kl.secretManager.UnregisterPod(pod) } if kl.configMapManager != nil { kl.configMapManager.UnregisterPod(pod) } //这段Go代码中的函数是在完成卷卸载后，通知密钥和配置管理器此Pod不再使用。 //具体来说，它首先检查kl.secretManager是否不为空，如果非空则从secretManager中注销此Pod，接着检查kl.configMapManager是否不为空， //如果非空则从configMapManager中注销此Pod。 // Note: we leave pod containers to be reclaimed in the background since dockershim requires the // container for retrieving logs and we want to make sure logs are available until the pod is // physically deleted. // remove any cgroups in the hierarchy for pods that are no longer running. if kl.cgroupsPerQOS { pcm := kl.containerManager.NewPodContainerManager() name, _ := pcm.GetPodContainerName(pod) if err := pcm.Destroy(name); err != nil { return err } klog.V(4).InfoS(\u0026#34;Pod termination removed cgroups\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;podUID\u0026#34;, pod.UID) } //这段Go代码是一个函数，用于删除不再运行的Pod的所有cgroups（控制组）。 //- 首先，代码检查kl.cgroupsPerQOS是否为true。如果为true，则会创建一个新的PodContainerManager实例。 //- 然后，通过pcm.GetPodContainerName(pod)获取Pod的容器名称。 //- 最后，调用pcm.Destroy(name)来销毁该Pod的所有cgroups，并在成功删除后记录日志。 //这个函数的主要作用是在Pod终止时清理相关的cgroups资源。 kl.usernsManager.Release(pod.UID) // mark the final pod status kl.statusManager.TerminatePod(pod) klog.V(4).InfoS(\u0026#34;Pod is terminated and will need no more status updates\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;podUID\u0026#34;, pod.UID) return nil //这段Go代码中的函数调用是针对Kubernetes中某个Pod的释放和状态标记操作。 //首先，kl.usernsManager.Release(pod.UID)调用释放了与Pod关联的用户命名空间资源。 //接下来，kl.statusManager.TerminatePod(pod)调用标记了Pod的最终状态为终止。 //最后，通过klog.V(4).InfoS(...)记录日志信息，表示Pod已被终止，不再需要更新状态。 //函数执行完毕后，返回nil表示操作成功完成。 } // Get pods which should be resynchronized. Currently, the following pod should be resynchronized: // - pod whose work is ready. // - internal modules that request sync of a pod. // // This method does not return orphaned pods (those known only to the pod worker that may have // been deleted from configuration). Those pods are synced by HandlePodCleanups as a consequence // of driving the state machine to completion. // // TODO: Consider synchronizing all pods which have not recently been acted on to be resilient // to bugs that might prevent updates from being delivered (such as the previous bug with // orphaned pods). Instead of asking the work queue for pending work, consider asking the // PodWorker which pods should be synced. func (kl *Kubelet) getPodsToSync() []*v1.Pod { allPods := kl.podManager.GetPods() podUIDs := kl.workQueue.GetWork() podUIDSet := sets.NewString() for _, podUID := range podUIDs { podUIDSet.Insert(string(podUID)) } //该函数用于获取需要重新同步的Pod。 //具体来说，它会返回所有工作准备就绪的Pod和内部模块请求同步的Pod。 //函数首先获取所有Pod的信息，然后从工作队列中获取需要同步的Pod的UID，并将其放入一个字符串集合中。 //最后，函数会返回这个集合中所有Pod的信息。需要注意的是，该函数不会返回孤立的Pod（那些只被Pod工作者知道，可能已经被从配置中删除的Pod）， //这些Pod会由HandlePodCleanups函数进行同步。 //函数的注释中还提到，可能会考虑同步所有最近没有被操作的Pod，以增加对错误的鲁棒性。 var podsToSync []*v1.Pod for _, pod := range allPods { if podUIDSet.Has(string(pod.UID)) { // The work of the pod is ready podsToSync = append(podsToSync, pod) continue } for _, podSyncLoopHandler := range kl.PodSyncLoopHandlers { if podSyncLoopHandler.ShouldSync(pod) { podsToSync = append(podsToSync, pod) break } } } return podsToSync } //该函数是一个简单的for循环，用于遍历allPods切片，并根据条件筛选出需要同步的Pod，将其添加到podsToSync切片中。 //具体来说，它首先检查podUIDSet中是否包含当前Pod的UID，如果是，则将该Pod添加到podsToSync中。 //然后，它遍历kl.PodSyncLoopHandlers中的每个podSyncLoopHandler，调用其ShouldSync方法检查是否需要同步该Pod， //如果是，则将该Pod添加到podsToSync中并跳出循环。最后，函数返回podsToSync切片。 // deletePod deletes the pod from the internal state of the kubelet by: // 1. stopping the associated pod worker asynchronously // 2. signaling to kill the pod by sending on the podKillingCh channel // // deletePod returns an error if not all sources are ready or the pod is not // found in the runtime cache. func (kl *Kubelet) deletePod(pod *v1.Pod) error { if pod == nil { return fmt.Errorf(\u0026#34;deletePod does not allow nil pod\u0026#34;) } if !kl.sourcesReady.AllReady() { // If the sources aren\u0026#39;t ready, skip deletion, as we may accidentally delete pods // for sources that haven\u0026#39;t reported yet. return fmt.Errorf(\u0026#34;skipping delete because sources aren\u0026#39;t ready yet\u0026#34;) } klog.V(3).InfoS(\u0026#34;Pod has been deleted and must be killed\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;podUID\u0026#34;, pod.UID) kl.podWorkers.UpdatePod(UpdatePodOptions{ Pod: pod, UpdateType: kubetypes.SyncPodKill, }) // We leave the volume/directory cleanup to the periodic cleanup routine. return nil } //该函数是Kubelet的一个方法，用于从kubelet的内部状态中删除一个pod， //具体操作包括： //1. 停止关联的pod worker（异步进行）。 //2. 通过发送信号到podKillingCh通道来终止pod。 //如果输入的pod为nil，函数会返回一个错误。 //如果相关的数据源未准备就绪，函数也会返回一个错误，以防止意外删除未报告的pod。 //在执行删除操作后，函数会更新pod worker，并将其标记为需要被终止。 //然后函数返回nil，表示删除成功。 //**注意：**该函数不会立即清理相关的卷和目录，而是将其留给了周期性的清理程序。 // rejectPod records an event about the pod with the given reason and message, // and updates the pod to the failed phase in the status manager. func (kl *Kubelet) rejectPod(pod *v1.Pod, reason, message string) { kl.recorder.Eventf(pod, v1.EventTypeWarning, reason, message) kl.statusManager.SetPodStatus(pod, v1.PodStatus{ Phase: v1.PodFailed, Reason: reason, Message: \u0026#34;Pod was rejected: \u0026#34; + message}) } //该函数用于记录一个关于给定pod的事件，并在状态管理器中将pod的状态更新为失败。 //具体实现中，函数使用kl.recorder.Eventf方法记录事件，然后使用kl.statusManager.SetPodStatus方法将pod的状态设置为失败，并提供原因和消息。 // canAdmitPod determines if a pod can be admitted, and gives a reason if it // cannot. \u0026#34;pod\u0026#34; is new pod, while \u0026#34;pods\u0026#34; are all admitted pods // The function returns a boolean value indicating whether the pod // can be admitted, a brief single-word reason and a message explaining why // the pod cannot be admitted. func (kl *Kubelet) canAdmitPod(pods []*v1.Pod, pod *v1.Pod) (bool, string, string) { // the kubelet will invoke each pod admit handler in sequence // if any handler rejects, the pod is rejected. // TODO: move out of disk check into a pod admitter // TODO: out of resource eviction should have a pod admitter call-out attrs := \u0026amp;lifecycle.PodAdmitAttributes{Pod: pod, OtherPods: pods} //该函数是Kubelet的一个方法，用于判断是否可以接纳一个Pod，并给出不能接纳的原因。 //函数传入参数为已接纳的Pod列表和待判断的Pod，返回一个布尔值表示是否可以接纳，一个简短的单个单词原因和一个解释不能接纳原因的消息。 //函数内部会按顺序调用每个Pod的准入处理器，如果有任何一个处理器拒绝，则Pod会被拒绝接纳。 //函数还有两个TODO注释，表示后续会将磁盘检查和资源不足的驱逐处理移动到Pod的准入处理器中。 if utilfeature.DefaultFeatureGate.Enabled(features.InPlacePodVerticalScaling) { // Use allocated resources values from checkpoint store (source of truth) to determine fit otherPods := make([]*v1.Pod, 0, len(pods)) for _, p := range pods { op := p.DeepCopy() kl.updateContainerResourceAllocation(op) otherPods = append(otherPods, op) } attrs.OtherPods = otherPods } //这段Go代码是用于在Kubernetes中进行Pod垂直扩展的功能。 //具体来说，如果启用了InPlacePodVerticalScaling特性，它会使用checkpoint存储中记录的已分配资源值来确定Pod是否适合进行垂直扩展。 //代码首先创建一个空的Pod数组，然后遍历传入的Pods，并对每个Pod进行深拷贝。 //然后，通过调用updateContainerResourceAllocation方法来更新每个Pod的容器资源分配情况，并将更新后的Pod添加到数组中。 //最后，将这个更新后的Pod数组赋值给attrs.OtherPods属性。 for _, podAdmitHandler := range kl.admitHandlers { if result := podAdmitHandler.Admit(attrs); !result.Admit { return false, result.Reason, result.Message } } return true, \u0026#34;\u0026#34;, \u0026#34;\u0026#34; } //该函数的作用是逐一检查kl.admitHandlers中的每个podAdmitHandler是否能够通过attrs属性的检验。 //具体流程如下： //1. 使用for range循环遍历kl.admitHandlers，每次迭代将一个podAdmitHandler赋值给podAdmitHandler变量。 //2. 在循环内部，调用当前podAdmitHandler的Admit方法，传入attrs参数，并将返回的结果赋值给result变量。 //3. 如果result的Admit字段为false，则函数立即返回false，同时返回result的Reason和Message字段作为错误信息。 //4. 如果循环结束后没有返回，则函数最终返回true，同时返回空字符串作为错误信息。 //总之，该函数的作用是检验kl.admitHandlers中的每个podAdmitHandler是否能够通过attrs属性的检验，并返回检验结果以及相应的错误信息。 func (kl *Kubelet) canRunPod(pod *v1.Pod) lifecycle.PodAdmitResult { attrs := \u0026amp;lifecycle.PodAdmitAttributes{Pod: pod} // Get \u0026#34;OtherPods\u0026#34;. Rejected pods are failed, so only include admitted pods that are alive. attrs.OtherPods = kl.GetActivePods() for _, handler := range kl.softAdmitHandlers { if result := handler.Admit(attrs); !result.Admit { return result } } return lifecycle.PodAdmitResult{Admit: true} } //该函数是Kubelet的一个方法，用于判断是否可以运行一个Pod。 //它首先创建一个PodAdmitAttributes对象，其中包含了要判断的Pod和其他活跃的Pod。 //然后遍历Kubelet的softAdmitHandlers列表，依次调用每个handler的Admit方法，并传入PodAdmitAttributes对象。 //如果有任何一个handler的Admit方法返回不承认（Admit为false），则函数立即返回该结果。 //如果所有handler都承认（Admit为true），则函数返回一个承认的结果。 // syncLoop is the main loop for processing changes. It watches for changes from // three channels (file, apiserver, and http) and creates a union of them. For // any new change seen, will run a sync against desired state and running state. If // no changes are seen to the configuration, will synchronize the last known desired // state every sync-frequency seconds. Never returns. func (kl *Kubelet) syncLoop(ctx context.Context, updates \u0026lt;-chan kubetypes.PodUpdate, handler SyncHandler) { klog.InfoS(\u0026#34;Starting kubelet main sync loop\u0026#34;) // The syncTicker wakes up kubelet to checks if there are any pod workers // that need to be sync\u0026#39;d. A one-second period is sufficient because the // sync interval is defaulted to 10s. syncTicker := time.NewTicker(time.Second) defer syncTicker.Stop() housekeepingTicker := time.NewTicker(housekeepingPeriod) defer housekeepingTicker.Stop() plegCh := kl.pleg.Watch() const ( base = 100 * time.Millisecond max = 5 * time.Second factor = 2 ) //该函数是kubelet的主要同步循环，用于处理来自三个渠道（文件、apiserver和http）的更改。 //它将这些更改合并，并对期望状态和运行状态进行同步。如果配置没有更改，它将每sync-frequency秒同步最后一次已知的期望状态。 //该函数不会返回。 详细解释： //- syncLoop 函数是 Kubelet 类的一个方法，它接收一个上下文对象 ctx、一个 PodUpdate 类型的通道 updates 和一个 SyncHandler 类型的处理程序 handler。 //- 函数首先使用 klog.InfoS 函数记录信息日志，表示开始启动 kubelet 的主同步循环。 //- 然后，函数创建一个 syncTicker，用于唤醒 kubelet 检查是否有需要同步的 pod 工作线程。 //由于同步间隔默认为 10s，因此使用 1s 的周期就足够了。 //- 函数还创建一个 housekeepingTicker，用于执行清理操作。 //- 接下来，函数通过调用 kl.pleg.Watch() 方法来监听 pod 状态变化事件，并将返回的通道赋值给 plegCh 变量。 //- 函数定义了一些常量，用于控制重试的时间间隔。 //- 在无限循环中，函数首先通过 select 语句监听多个通道。如果 ctx.Done() 通道被关闭，函数将退出循环。 //- 如果 updates 通道有新的更改，函数将调用 handler 处理程序进行同步。 //- 如果 plegCh 通道有新的 pod 状态变化事件，函数将调用 kl.pleg.Grab() 方法进行处理。 //- 如果 housekeepingTicker.C 通道触发清理操作，函数将调用 kl.doHousekeeping() 方法进行清理。 //- 如果没有监听到任何事件，则使用 time.Sleep() 函数等待一段时间后继续循环。 //- 如果在尝试同步时发生错误，函数将根据重试策略进行重试。重试时间间隔会根据失败次数进行指数增加，最大间隔为 5s。 //- 如果同步成功，函数将重置重试时间间隔为基本值 100ms。 //- 如果同步失败超过一定次数，函数将记录错误日志并退出循环。 duration := base // Responsible for checking limits in resolv.conf // The limits do not have anything to do with individual pods // Since this is called in syncLoop, we don\u0026#39;t need to call it anywhere else if kl.dnsConfigurer != nil \u0026amp;\u0026amp; kl.dnsConfigurer.ResolverConfig != \u0026#34;\u0026#34; { kl.dnsConfigurer.CheckLimitsForResolvConf() } //这段Go代码中的函数主要负责检查resolv.conf文件中的限制。 //该函数会判断kl.dnsConfigurer是否为空，且其ResolverConfig是否已设置，如果满足条件， //则调用kl.dnsConfigurer的CheckLimitsForResolvConf方法进行限制检查。 //由于该函数是在syncLoop中调用的，因此不需要在其他地方再次调用。 for { if err := kl.runtimeState.runtimeErrors(); err != nil { klog.ErrorS(err, \u0026#34;Skipping pod synchronization\u0026#34;) // exponential backoff time.Sleep(duration) duration = time.Duration(math.Min(float64(max), factor*float64(duration))) continue } // reset backoff if we have a success duration = base kl.syncLoopMonitor.Store(kl.clock.Now()) if !kl.syncLoopIteration(ctx, updates, handler, syncTicker.C, housekeepingTicker.C, plegCh) { break } kl.syncLoopMonitor.Store(kl.clock.Now()) } } //这段Go代码是一个无限循环，用于进行Pod同步。循环中包含两个主要部分：错误处理和同步逻辑。 //1. 错误处理： - 使用runtimeErrors()检查运行时错误。 //- 如果存在错误，则记录错误日志，并进行指数退避。 //- 通过time.Sleep()暂停执行，暂停时间根据退避策略计算得出。 //- 退避策略是通过将duration乘以factor并限制最大值来计算的。 //- 出现错误时，使用continue语句继续下一次循环。 //2. 同步逻辑： - 在没有错误的情况下，重置退避时间至初始值。 //- 更新syncLoopMonitor的值为当前时间。 //- 调用syncLoopIteration()函数进行Pod同步。 //- 如果syncLoopIteration()返回false，则退出循环。 //- 更新syncLoopMonitor的值为当前时间。 //这段代码的主要目的是在遇到错误时进行指数退避，并在成功时重置退避时间，持续进行Pod同步操作。 // syncLoopIteration reads from various channels and dispatches pods to the // given handler. // // Arguments: // 1. configCh: a channel to read config events from // 2. handler: the SyncHandler to dispatch pods to // 3. syncCh: a channel to read periodic sync events from // 4. housekeepingCh: a channel to read housekeeping events from // 5. plegCh: a channel to read PLEG updates from // // Events are also read from the kubelet liveness manager\u0026#39;s update channel. // // The workflow is to read from one of the channels, handle that event, and // update the timestamp in the sync loop monitor. // // Here is an appropriate place to note that despite the syntactical // similarity to the switch statement, the case statements in a select are // evaluated in a pseudorandom order if there are multiple channels ready to // read from when the select is evaluated. In other words, case statements // are evaluated in random order, and you can not assume that the case // statements evaluate in order if multiple channels have events. // // With that in mind, in truly no particular order, the different channels // are handled as follows: // // - configCh: dispatch the pods for the config change to the appropriate // handler callback for the event type // - plegCh: update the runtime cache; sync pod // - syncCh: sync all pods waiting for sync // - housekeepingCh: trigger cleanup of pods // - health manager: sync pods that have failed or in which one or more // containers have failed health checks func (kl *Kubelet) syncLoopIteration(ctx context.Context, configCh \u0026lt;-chan kubetypes.PodUpdate, handler SyncHandler, syncCh \u0026lt;-chan time.Time, housekeepingCh \u0026lt;-chan time.Time, plegCh \u0026lt;-chan *pleg.PodLifecycleEvent) bool { select { case u, open := \u0026lt;-configCh: // Update from a config source; dispatch it to the right handler // callback. //该函数是一个同步循环，用于从多个通道读取事件并将其分发给给定的处理程序。 //它接收5个参数：configCh（配置事件的通道）、handler（分发pod的SyncHandler处理程序）、syncCh（定期同步事件的通道）、 //housekeepingCh（清理事件的通道）和plegCh（PLEG更新的通道）。 //事件还会从kubelet存活管理器的更新通道中读取。该函数的工作流程是读取一个通道中的事件，处理该事件，并在同步循环监视器中更新时间戳。 //在处理这些通道中的事件时，需要注意case语句的评估顺序是随机的。 //具体来说，不同通道的处理方式如下： //- configCh：将配置更改分发给适当的处理程序回调。 //- plegCh：更新运行时缓存并同步pod。 //- syncCh：同步所有等待同步的pod。 //- housekeepingCh：触发清理pod。 //- 健康管理器：同步失败或其中一个或多个容器失败的健康检查的pod。 if !open { klog.ErrorS(nil, \u0026#34;Update channel is closed, exiting the sync loop\u0026#34;) return false } //这段Go代码是一个条件判断语句，判断变量open是否为false，如果为false，则通过klog.ErrorS函数记录错误日志，并返回false。 //这段代码的作用是在某个更新通道关闭时，退出同步循环，并记录错误日志。 switch u.Op { case kubetypes.ADD: klog.V(2).InfoS(\u0026#34;SyncLoop ADD\u0026#34;, \u0026#34;source\u0026#34;, u.Source, \u0026#34;pods\u0026#34;, klog.KObjSlice(u.Pods)) // After restarting, kubelet will get all existing pods through // ADD as if they are new pods. These pods will then go through the // admission process and *may* be rejected. This can be resolved // once we have checkpointing. handler.HandlePodAdditions(u.Pods) case kubetypes.UPDATE: klog.V(2).InfoS(\u0026#34;SyncLoop UPDATE\u0026#34;, \u0026#34;source\u0026#34;, u.Source, \u0026#34;pods\u0026#34;, klog.KObjSlice(u.Pods)) handler.HandlePodUpdates(u.Pods) case kubetypes.REMOVE: klog.V(2).InfoS(\u0026#34;SyncLoop REMOVE\u0026#34;, \u0026#34;source\u0026#34;, u.Source, \u0026#34;pods\u0026#34;, klog.KObjSlice(u.Pods)) handler.HandlePodRemoves(u.Pods) case kubetypes.RECONCILE: klog.V(4).InfoS(\u0026#34;SyncLoop RECONCILE\u0026#34;, \u0026#34;source\u0026#34;, u.Source, \u0026#34;pods\u0026#34;, klog.KObjSlice(u.Pods)) handler.HandlePodReconcile(u.Pods) case kubetypes.DELETE: klog.V(2).InfoS(\u0026#34;SyncLoop DELETE\u0026#34;, \u0026#34;source\u0026#34;, u.Source, \u0026#34;pods\u0026#34;, klog.KObjSlice(u.Pods)) // DELETE is treated as a UPDATE because of graceful deletion. handler.HandlePodUpdates(u.Pods) case kubetypes.SET: // TODO: Do we want to support this? klog.ErrorS(nil, \u0026#34;Kubelet does not support snapshot update\u0026#34;) default: klog.ErrorS(nil, \u0026#34;Invalid operation type received\u0026#34;, \u0026#34;operation\u0026#34;, u.Op) } //该函数根据传入的操作类型（u.Op）执行相应的处理逻辑。 //- 当操作类型为kubetypes.ADD时，会记录日志信息并调用handler.HandlePodAdditions(u.Pods)处理Pod的添加操作。 //- 当操作类型为kubetypes.UPDATE时，会记录日志信息并调用handler.HandlePodUpdates(u.Pods)处理Pod的更新操作。 //- 当操作类型为kubetypes.REMOVE时，会记录日志信息并调用handler.HandlePodRemoves(u.Pods)处理Pod的移除操作。 //- 当操作类型为kubetypes.RECONCILE时，会记录日志信息并调用handler.HandlePodReconcile(u.Pods)处理Pod的协调操作。 //- 当操作类型为kubetypes.DELETE时，会记录日志信息并调用handler.HandlePodUpdates(u.Pods)处理Pod的删除操作（被当做更新操作处理）。 - 当操作类型为kubetypes.SET时，会记录错误信息，表示当前不支持该操作。 - 当操作类型为其他值时，会记录错误信息，表示收到了无效的操作类型 kl.sourcesReady.AddSource(u.Source) case e := \u0026lt;-plegCh: if isSyncPodWorthy(e) { // PLEG event for a pod; sync it. if pod, ok := kl.podManager.GetPodByUID(e.ID); ok { klog.V(2).InfoS(\u0026#34;SyncLoop (PLEG): event for pod\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;event\u0026#34;, e) handler.HandlePodSyncs([]*v1.Pod{pod}) } else { // If the pod no longer exists, ignore the event. klog.V(4).InfoS(\u0026#34;SyncLoop (PLEG): pod does not exist, ignore irrelevant event\u0026#34;, \u0026#34;event\u0026#34;, e) } } //这段Go代码是在一个select语句的分支中，首先将u.Source添加到kl.sourcesReady中。 //随后，从plegCh通道中接收一个事件e，并通过isSyncPodWorthy(e)函数判断是否需要同步该事件对应的Pod。 //如果需要同步，通过kl.podManager.GetPodByUID(e.ID)获取Pod对象，并调用handler.HandlePodSyncs([]*v1.Pod{pod})进行同步操作。 //如果Pod不存在，则在日志中记录相关信息并忽略该事件。 if e.Type == pleg.ContainerDied { if containerID, ok := e.Data.(string); ok { kl.cleanUpContainersInPod(e.ID, containerID) } } //该代码片段是一个if语句，判断e.Type是否等于pleg.ContainerDied， //如果是，则进一步判断e.Data是否为字符串类型，并将字符串类型的e.Data赋值给containerID。 //如果上述判断都为真，则调用kl.cleanUpContainersInPod(e.ID, containerID)方法。 //这段代码的主要作用是在容器死亡时，清理Pod中的容器。 case \u0026lt;-syncCh: // Sync pods waiting for sync podsToSync := kl.getPodsToSync() if len(podsToSync) == 0 { break } klog.V(4).InfoS(\u0026#34;SyncLoop (SYNC) pods\u0026#34;, \u0026#34;total\u0026#34;, len(podsToSync), \u0026#34;pods\u0026#34;, klog.KObjSlice(podsToSync)) handler.HandlePodSyncs(podsToSync) case update := \u0026lt;-kl.livenessManager.Updates(): if update.Result == proberesults.Failure { handleProbeSync(kl, update, handler, \u0026#34;liveness\u0026#34;, \u0026#34;unhealthy\u0026#34;) } //这是一个Go语言的代码片段，它通过监听两个通道（syncCh和kl.livenessManager.Updates()）来实现Pod的同步和健康检查。 //具体功能如下： //1. 如果监听到syncCh通道有消息，会执行以下操作： //- 调用kl.getPodsToSync()获取需要同步的Pod列表； //- 如果列表为空，则跳出当前循环； //- 如果列表不为空，则通过klog记录日志信息，并调用handler.HandlePodSyncs(podsToSync)来处理Pod的同步操作。 //2. 如果监听到kl.livenessManager.Updates()通道有消息，会执行以下操作： //- 检查消息的结果是否为失败（proberesults.Failure），如果是， //则调用handleProbeSync(kl, update, handler, \u0026#34;liveness\u0026#34;, \u0026#34;unhealthy\u0026#34;)来处理不健康的Pod。 //这段代码通过使用Go语言的并发特性，实现了在同步Pod的同时进行健康检查的功能。 case update := \u0026lt;-kl.readinessManager.Updates(): ready := update.Result == proberesults.Success kl.statusManager.SetContainerReadiness(update.PodUID, update.ContainerID, ready) status := \u0026#34;\u0026#34; if ready { status = \u0026#34;ready\u0026#34; } handleProbeSync(kl, update, handler, \u0026#34;readiness\u0026#34;, status) case update := \u0026lt;-kl.startupManager.Updates(): started := update.Result == proberesults.Success kl.statusManager.SetContainerStartup(update.PodUID, update.ContainerID, started) status := \u0026#34;unhealthy\u0026#34; if started { status = \u0026#34;started\u0026#34; } handleProbeSync(kl, update, handler, \u0026#34;startup\u0026#34;, status) //这段Go代码包含两个case语句，它们分别处理来自kl.readinessManager.Updates()和kl.startupManager.Updates()的更新。 //每个case语句内部都会根据更新的结果来更新容器的就绪状态或启动状态，并调用handleProbeSync函数。 //1. 第一个case语句处理就绪状态的更新： //- 通过update.Result == proberesults.Success判断就绪检查是否成功，将结果赋值给ready变量。 //- 调用kl.statusManager.SetContainerReadiness来设置容器的就绪状态。 //- 根据ready的值决定status的值，如果就绪则为\u0026#34;ready\u0026#34;，否则为空字符串。 //- 调用handleProbeSync函数处理就绪探针的同步，传入参数包括kl、update、handler、探针类型\u0026#34;readiness\u0026#34;和status。 //2. 第二个case语句处理启动状态的更新： //- 通过update.Result == proberesults.Success判断启动检查是否成功，将结果赋值给started变量。 //- 调用kl.statusManager.SetContainerStartup来设置容器的启动状态。 //- 根据started的值决定status的值，如果启动成功则为\u0026#34;started\u0026#34;，否则为\u0026#34;unhealthy\u0026#34;。 //- 调用handleProbeSync函数处理启动探针的同步，传入参数包括kl、update、handler、探针类型\u0026#34;startup\u0026#34;和status。 //综上所述，这段代码主要负责处理容器的就绪和启动状态更新，并通过handleProbeSync函数进行进一步处理。 case \u0026lt;-housekeepingCh: if !kl.sourcesReady.AllReady() { // If the sources aren\u0026#39;t ready or volume manager has not yet synced the states, // skip housekeeping, as we may accidentally delete pods from unready sources. klog.V(4).InfoS(\u0026#34;SyncLoop (housekeeping, skipped): sources aren\u0026#39;t ready yet\u0026#34;) } else { start := time.Now() klog.V(4).InfoS(\u0026#34;SyncLoop (housekeeping)\u0026#34;) if err := handler.HandlePodCleanups(ctx); err != nil { klog.ErrorS(err, \u0026#34;Failed cleaning pods\u0026#34;) } duration := time.Since(start) if duration \u0026gt; housekeepingWarningDuration { klog.ErrorS(fmt.Errorf(\u0026#34;housekeeping took too long\u0026#34;), \u0026#34;Housekeeping took longer than expected\u0026#34;, \u0026#34;expected\u0026#34;, housekeepingWarningDuration, \u0026#34;actual\u0026#34;, duration.Round(time.Millisecond)) } klog.V(4).InfoS(\u0026#34;SyncLoop (housekeeping) end\u0026#34;, \u0026#34;duration\u0026#34;, duration.Round(time.Millisecond)) } } return true //该Go函数是一个case语句，从housekeepingCh通道接收信号。 //当接收到信号时，它会检查kl.sourcesReady.AllReady()是否为true，如果不为true，则跳过清理工作，因为可能从未准备好的源意外删除Pod。 //如果kl.sourcesReady.AllReady()为true，则开始执行清理工作，并记录开始时间。 //清理工作由handler.HandlePodCleanups(ctx)函数执行，如果执行失败，则记录错误信息。 //最后，计算清理工作的时间，如果时间超过预期的housekeepingWarningDuration，则记录警告信息。 //最后，函数返回true。 } func handleProbeSync(kl *Kubelet, update proberesults.Update, handler SyncHandler, probe, status string) { // We should not use the pod from manager, because it is never updated after initialization. pod, ok := kl.podManager.GetPodByUID(update.PodUID) if !ok { // If the pod no longer exists, ignore the update. klog.V(4).InfoS(\u0026#34;SyncLoop (probe): ignore irrelevant update\u0026#34;, \u0026#34;probe\u0026#34;, probe, \u0026#34;status\u0026#34;, status, \u0026#34;update\u0026#34;, update) return } klog.V(1).InfoS(\u0026#34;SyncLoop (probe)\u0026#34;, \u0026#34;probe\u0026#34;, probe, \u0026#34;status\u0026#34;, status, \u0026#34;pod\u0026#34;, klog.KObj(pod)) handler.HandlePodSyncs([]*v1.Pod{pod}) } //该函数用于处理探针同步操作。 //1. 首先，它通过update获取Pod的UID，并尝试从Kubelet的podManager中获取对应的Pod对象。 //2. 如果找不到Pod，则忽略更新，并记录日志。 //3. 如果找到Pod，则记录日志，并调用handler处理Pod的同步操作，将该Pod对象以切片的形式传递给HandlePodSyncs方法。 // HandlePodAdditions is the callback in SyncHandler for pods being added from // a config source. func (kl *Kubelet) HandlePodAdditions(pods []*v1.Pod) { start := kl.clock.Now() sort.Sort(sliceutils.PodsByCreationTime(pods)) if utilfeature.DefaultFeatureGate.Enabled(features.InPlacePodVerticalScaling) { kl.podResizeMutex.Lock() defer kl.podResizeMutex.Unlock() } //该函数是Kubelet的SyncHandler回调函数，用于处理从配置源添加的Pods。 //函数首先记录开始时间，然后对Pods进行排序。 //如果启用了InPlacePodVerticalScaling特性，则加锁并释放锁以进行Pod的垂直扩展。 for _, pod := range pods { existingPods := kl.podManager.GetPods() // Always add the pod to the pod manager. Kubelet relies on the pod // manager as the source of truth for the desired state. If a pod does // not exist in the pod manager, it means that it has been deleted in // the apiserver and no action (other than cleanup) is required. kl.podManager.AddPod(pod) //这段Go代码是一个for循环，其中pods是一个Pod的切片。 //对于每个Pod，它会执行以下操作： //1. 调用kl.podManager.GetPods()获取当前已存在的Pods。 //2. 调用kl.podManager.AddPod(pod)将当前Pod添加到podManager中。 //这段代码的主要目的是将所有的Pod添加到podManager中。 //podManager是Kubelet的核心组件之一，它负责管理Pod的生命周期。 //通过将Pod添加到podManager中，Kubelet可以确保它知道集群中所有Pod的期望状态，以便它可以正确地管理和同步Pod的状态与实际状态。 pod, mirrorPod, wasMirror := kl.podManager.GetPodAndMirrorPod(pod) if wasMirror { if pod == nil { klog.V(2).InfoS(\u0026#34;Unable to find pod for mirror pod, skipping\u0026#34;, \u0026#34;mirrorPod\u0026#34;, klog.KObj(mirrorPod), \u0026#34;mirrorPodUID\u0026#34;, mirrorPod.UID) continue } kl.podWorkers.UpdatePod(UpdatePodOptions{ Pod: pod, MirrorPod: mirrorPod, UpdateType: kubetypes.SyncPodUpdate, StartTime: start, }) continue } //这段Go代码是Kubernetes中kubelet的一个片段，主要功能是处理mirror pod和其对应的pod的关系。 //- kl.podManager.GetPodAndMirrorPod(pod)函数用于获取mirror pod和其对应的pod。 //- 如果wasMirror为true，表示当前处理的是mirror pod。 //- 如果pod为空，则记录日志并跳过当前循环。 //- 如果pod不为空，则调用kl.podWorkers.UpdatePod()函数更新pod信息，其中UpdateType为kubetypes.SyncPodUpdate，表示同步更新pod。 //总结：这段代码主要处理mirror pod的更新操作。 // Only go through the admission process if the pod is not requested // for termination by another part of the kubelet. If the pod is already // using resources (previously admitted), the pod worker is going to be // shutting it down. If the pod hasn\u0026#39;t started yet, we know that when // the pod worker is invoked it will also avoid setting up the pod, so // we simply avoid doing any work. if !kl.podWorkers.IsPodTerminationRequested(pod.UID) { // We failed pods that we rejected, so activePods include all admitted // pods that are alive. activePods := kl.filterOutInactivePods(existingPods) //这段Go代码中的函数是一个条件判断语句，其作用是判断某个Pod是否被其他部分请求终止。 //如果该Pod没有被请求终止，并且之前已经被允许使用资源（先前已通过准入控制），则该函数会让Pod worker继续关闭该Pod。 //如果该Pod还没有启动，函数会知道当Pod worker被调用时，它也会避免设置Pod，因此函数会避免执行任何操作。 //如果Pod没有被请求终止，则函数会过滤掉所有非活跃的Pod，只保留活跃的Pod。 if utilfeature.DefaultFeatureGate.Enabled(features.InPlacePodVerticalScaling) { // To handle kubelet restarts, test pod admissibility using AllocatedResources values // (for cpu \u0026amp; memory) from checkpoint store. If found, that is the source of truth. podCopy := pod.DeepCopy() kl.updateContainerResourceAllocation(podCopy) //这段Go代码中的函数主要做的是检查是否启用了InPlacePodVerticalScaling特性， //如果启用了，则会通过allocatedResources值（来自checkpoint store的CPU和内存）来测试pod的可接受性。 //如果找到allocatedResources，则将其视为真相来源，并对pod进行深拷贝并更新容器资源分配。 // Check if we can admit the pod; if not, reject it. if ok, reason, message := kl.canAdmitPod(activePods, podCopy); !ok { kl.rejectPod(pod, reason, message) continue } //这段Go代码是一个条件判断语句，其主要功能是检查是否可以接受一个Pod（容器的组合），如果不能接受，则拒绝该Pod。 //具体来说，函数kl.canAdmitPod(activePods, podCopy)会返回一个布尔值ok、一个原因reason和一条消息message。 //如果ok为false，则表示不能接受该Pod，此时会调用kl.rejectPod(pod, reason, message)函数来拒绝该Pod，并继续执行下一次循环。 //需要注意的是，这里的kl是一个对象实例，它调用了canAdmitPod和rejectPod两个方法。 //根据代码上下文来理解，这段代码应该是在处理 Kubernetes 中的 Pod 调度逻辑。 // For new pod, checkpoint the resource values at which the Pod has been admitted if err := kl.statusManager.SetPodAllocation(podCopy); err != nil { //TODO(vinaykul,InPlacePodVerticalScaling): Can we recover from this in some way? Investigate klog.ErrorS(err, \u0026#34;SetPodAllocation failed\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod)) } //该函数用于在创建新的Pod时，设置Pod的资源分配情况。具体操作是调用kl.statusManager.SetPodAllocation(podCopy)方法， //将Pod的资源分配信息保存到Pod的状态中。如果设置失败，会记录错误日志，并标记为待调查的问题。 } else { // Check if we can admit the pod; if not, reject it. if ok, reason, message := kl.canAdmitPod(activePods, pod); !ok { kl.rejectPod(pod, reason, message) continue //这段代码是Kubernetes中处理Pod调度的逻辑。如果Pod不能被当前节点接纳，则会拒绝该Pod，并继续处理下一个Pod。 //kl.canAdmitPod()函数检查Pod是否可以被接纳， //如果不可以，则调用kl.rejectPod()函数拒绝Pod，并提供拒绝的原因和信息。 } } } kl.podWorkers.UpdatePod(UpdatePodOptions{ Pod: pod, MirrorPod: mirrorPod, UpdateType: kubetypes.SyncPodCreate, StartTime: start, //该函数用于更新Pod的状态信息。 //- 参数UpdatePodOptions包含了更新Pod所需的各项参数： //- Pod: 要更新的Pod对象。 - MirrorPod: Pod的镜像对象。 //- UpdateType: 更新类型，此处为SyncPodCreate，表示创建Pod。 - StartTime: 更新开始时间。 //- 该函数会根据传入的参数更新Pod的状态，并进行相应的日志记录和状态同步操作 }) } } //这段Go代码是Kubernetes中kubelet组件的一部分，用于处理新创建的Pod的分配和准入控制。 //1. 如果是新Pod，会调用kl.statusManager.SetPodAllocation(podCopy)函数来记录Pod被接纳时的资源值。如果记录失败，会打印错误日志。 //2. 如果Pod不是新创建的，则会调用kl.canAdmitPod(activePods, pod)函数来检查是否可以接纳该Pod。 //如果不能接纳，则会调用kl.rejectPod(pod, reason, message)函数来拒绝Pod，并继续处理下一个Pod。 //3. 如果Pod可以接纳，则会调用kl.podWorkers.UpdatePod()函数来更新Pod的状态。 //这段代码的主要作用是实现Pod的准入控制和状态更新。 // updateContainerResourceAllocation updates AllocatedResources values // (for cpu \u0026amp; memory) from checkpoint store func (kl *Kubelet) updateContainerResourceAllocation(pod *v1.Pod) { for _, c := range pod.Spec.Containers { allocatedResources, found := kl.statusManager.GetContainerResourceAllocation(string(pod.UID), c.Name) if c.Resources.Requests != nil \u0026amp;\u0026amp; found { if _, ok := allocatedResources[v1.ResourceCPU]; ok { c.Resources.Requests[v1.ResourceCPU] = allocatedResources[v1.ResourceCPU] } if _, ok := allocatedResources[v1.ResourceMemory]; ok { c.Resources.Requests[v1.ResourceMemory] = allocatedResources[v1.ResourceMemory] } } } } //该函数的作用是更新容器的资源分配信息（包括CPU和内存）。 //具体实现如下： //1. 遍历Pod中所有的容器。 //2. 对于每个容器，通过调用kl.statusManager.GetContainerResourceAllocation方法，获取该容器的资源分配信息。 //3. 如果该容器的资源请求（c.Resources.Requests）不为空且成功获取到资源分配信息（allocatedResources）， //则将分配的CPU和内存资源信息更新到容器的资源请求中。 // HandlePodUpdates is the callback in the SyncHandler interface for pods // being updated from a config source. func (kl *Kubelet) HandlePodUpdates(pods []*v1.Pod) { start := kl.clock.Now() for _, pod := range pods { kl.podManager.UpdatePod(pod) pod, mirrorPod, wasMirror := kl.podManager.GetPodAndMirrorPod(pod) if wasMirror { if pod == nil { klog.V(2).InfoS(\u0026#34;Unable to find pod for mirror pod, skipping\u0026#34;, \u0026#34;mirrorPod\u0026#34;, klog.KObj(mirrorPod), \u0026#34;mirrorPodUID\u0026#34;, mirrorPod.UID) continue } } //该函数是Kubelet的HandlePodUpdates方法，用于处理从配置源更新的Pod。 //它遍历传入的Pod数组，并通过kl.podManager.UpdatePod(pod)更新每个Pod。 //然后，它获取每个Pod及其镜像Pod，并根据是否是镜像Pod进行不同的处理。 //如果找不到对应的Pod，则跳过该镜像Pod。 kl.podWorkers.UpdatePod(UpdatePodOptions{ Pod: pod, MirrorPod: mirrorPod, UpdateType: kubetypes.SyncPodUpdate, StartTime: start, }) } //该函数用于更新Pod的状态信息。 //- 参数pod是要更新的Pod对象。 //- 参数mirrorPod是Pod的镜像对象。 //- 参数updateType指定更新的类型，这里是同步更新。 //- 参数startTime记录更新开始的时间。 //该函数会根据参数更新Pod的相关状态，并记录更新的开始时间。 } // HandlePodRemoves is the callback in the SyncHandler interface for pods // being removed from a config source. func (kl *Kubelet) HandlePodRemoves(pods []*v1.Pod) { start := kl.clock.Now() for _, pod := range pods { kl.podManager.RemovePod(pod) //该函数是Kubelet的HandlePodRemoves方法，用于处理从配置源中移除的Pods。 //它遍历传入的Pods数组，并通过kl.podManager.RemovePod方法将每个Pod从管理器中移除。 pod, mirrorPod, wasMirror := kl.podManager.GetPodAndMirrorPod(pod) if wasMirror { if pod == nil { klog.V(2).InfoS(\u0026#34;Unable to find pod for mirror pod, skipping\u0026#34;, \u0026#34;mirrorPod\u0026#34;, klog.KObj(mirrorPod), \u0026#34;mirrorPodUID\u0026#34;, mirrorPod.UID) continue } kl.podWorkers.UpdatePod(UpdatePodOptions{ Pod: pod, MirrorPod: mirrorPod, UpdateType: kubetypes.SyncPodUpdate, StartTime: start, }) continue } //这段Go代码是Kubernetes中kubelet的一个片段，主要功能是处理mirror pod和其对应的pod的关系。 //- kl.podManager.GetPodAndMirrorPod(pod)函数用于获取mirror pod和其对应的pod。 //- 如果wasMirror为true，表示当前处理的是mirror pod。 //- 如果pod为空，则记录日志并跳过当前循环。 //- 如果pod不为空，则调用kl.podWorkers.UpdatePod()函数更新pod信息，其中UpdateType为kubetypes.SyncPodUpdate，表示同步更新pod。 //总结：这段代码主要处理mirror pod的更新操作。 // Deletion is allowed to fail because the periodic cleanup routine // will trigger deletion again. if err := kl.deletePod(pod); err != nil { klog.V(2).InfoS(\u0026#34;Failed to delete pod\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;err\u0026#34;, err) } } } //这段Go代码中的函数是一个if条件语句，其主要功能是尝试删除一个pod（Kubernetes中的一个容器化应用程序实例）。 //如果删除操作失败，函数会记录一条日志信息。 具体分析如下： //1. 这段代码首先调用kl.deletePod(pod)方法尝试删除pod。 //2. 如果删除操作失败，即err != nil，则会执行if语句内部的代码块。 //3. 在代码块中，会使用klog.V(2).InfoS方法记录一条日志信息，包含以下内容： //- \u0026#34;Failed to delete pod\u0026#34;：表示删除pod失败。 //- \u0026#34;pod\u0026#34;：表示被删除的pod的详细信息。 //- \u0026#34;err\u0026#34;：表示删除操作失败时返回的错误信息。 //4. 该函数的主要目的是允许删除操作失败，因为有一个定期清理程序会再次触发删除操作。 //总结：这段代码的主要功能是尝试删除一个pod，并在删除失败时记录一条日志信息。 // HandlePodReconcile is the callback in the SyncHandler interface for pods // that should be reconciled. Pods are reconciled when only the status of the // pod is updated in the API. func (kl *Kubelet) HandlePodReconcile(pods []*v1.Pod) { start := kl.clock.Now() for _, pod := range pods { // Update the pod in pod manager, status manager will do periodically reconcile according // to the pod manager. kl.podManager.UpdatePod(pod) //该函数是Kubelet实现SyncHandler接口的回调函数，用于处理需要进行状态同步的Pod。 //当Pod的状态在API中被更新时，就会触发Pod的同步。 //函数遍历传入的Pod列表，通过调用Kubelet的podManager的UpdatePod方法， //更新Pod的状态，并由statusManager定期进行状态同步。 pod, mirrorPod, wasMirror := kl.podManager.GetPodAndMirrorPod(pod) if wasMirror { if pod == nil { klog.V(2).InfoS(\u0026#34;Unable to find pod for mirror pod, skipping\u0026#34;, \u0026#34;mirrorPod\u0026#34;, klog.KObj(mirrorPod), \u0026#34;mirrorPodUID\u0026#34;, mirrorPod.UID) continue } // Static pods should be reconciled the same way as regular pods } //这段Go代码是Kubernetes中的一部分，用于处理Pod和其对应的Mirror Pod的关系。 //下面是这段代码的功能解释： //- kl.podManager.GetPodAndMirrorPod(pod)：获取给定Pod及其对应的Mirror Pod。 //- wasMirror：判断返回的Mirror Pod是否是之前已经存在的。 //- 如果wasMirror为true，则进一步判断pod是否为nil。 //- 如果pod为nil，则记录日志信息，并跳过当前循环。 //- 如果pod不为nil，则将Static Pods与普通Pods进行一致性处理。 //这段代码的主要作用是在处理Pod的过程中，对Mirror Pod进行相应的处理。 // TODO: reconcile being calculated in the config manager is questionable, and avoiding // extra syncs may no longer be necessary. Reevaluate whether Reconcile and Sync can be // merged (after resolving the next two TODOs). // Reconcile Pod \u0026#34;Ready\u0026#34; condition if necessary. Trigger sync pod for reconciliation. // TODO: this should be unnecessary today - determine what is the cause for this to // be different than Sync, or if there is a better place for it. For instance, we have // needsReconcile in kubelet/config, here, and in status_manager. if status.NeedToReconcilePodReadiness(pod) { kl.podWorkers.UpdatePod(UpdatePodOptions{ Pod: pod, MirrorPod: mirrorPod, UpdateType: kubetypes.SyncPodSync, StartTime: start, }) } //这段Go代码中的函数是一个待完成的任务，它的功能是重新调整Pod的\u0026#34;Ready\u0026#34;状态，如果需要的话。 //该函数的实现需要进一步评估和确定，因为它可能不再必要，而且可能会导致额外的同步操作。 //函数的具体实现包括检查Pod是否需要重新调整状态，并根据需要触发同步操作。 //这个函数存在一些待解决的问题和TODO，需要进一步解决和改进。 // After an evicted pod is synced, all dead containers in the pod can be removed. // TODO: this is questionable - status read is async and during eviction we already // expect to not have some container info. The pod worker knows whether a pod has // been evicted, so if this is about minimizing the time to react to an eviction we // can do better. If it\u0026#39;s about preserving pod status info we can also do better. if eviction.PodIsEvicted(pod.Status) { if podStatus, err := kl.podCache.Get(pod.UID); err == nil { kl.containerDeletor.deleteContainersInPod(\u0026#34;\u0026#34;, podStatus, true) } } } //这段Go代码中的函数用于在Pod被驱逐后同步删除Pod中的所有死亡容器。 //函数首先检查Pod是否已被驱逐，如果是，则尝试从podCache中获取Pod的状态信息。 //如果获取成功，则调用kl.containerDeletor.deleteContainersInPod函数删除Pod中的所有容器。 } // HandlePodSyncs is the callback in the syncHandler interface for pods // that should be dispatched to pod workers for sync. func (kl *Kubelet) HandlePodSyncs(pods []*v1.Pod) { start := kl.clock.Now() for _, pod := range pods { pod, mirrorPod, wasMirror := kl.podManager.GetPodAndMirrorPod(pod) if wasMirror { if pod == nil { klog.V(2).InfoS(\u0026#34;Unable to find pod for mirror pod, skipping\u0026#34;, \u0026#34;mirrorPod\u0026#34;, klog.KObj(mirrorPod), \u0026#34;mirrorPodUID\u0026#34;, mirrorPod.UID) continue } //该函数是Kubelet的HandlePodSyncs方法，用于处理同步Pod的工作。 //它接收一个Pod列表作为参数，然后遍历每个Pod，获取对应的Pod和镜像Pod，并判断是否为镜像Pod。 //如果是镜像Pod且找不到对应的Pod，则跳过该Pod的处理。 // Syncing a mirror pod is a programmer error since the intent of sync is to // batch notify all pending work. We should make it impossible to double sync, // but for now log a programmer error to prevent accidental introduction. klog.V(3).InfoS(\u0026#34;Programmer error, HandlePodSyncs does not expect to receive mirror pods\u0026#34;, \u0026#34;podUID\u0026#34;, pod.UID, \u0026#34;mirrorPodUID\u0026#34;, mirrorPod.UID) continue } kl.podWorkers.UpdatePod(UpdatePodOptions{ Pod: pod, MirrorPod: mirrorPod, UpdateType: kubetypes.SyncPodSync, StartTime: start, }) } } //该函数是Kubelet的HandlePodSyncs方法，用于处理Pod的同步操作。 //它接收一个Pod列表作为参数，遍历每个Pod，获取对应的Pod和mirror Pod，并根据情况执行相应的操作。 //如果传入的是mirror Pod，则记录日志并跳过。 //否则，通过podWorkers更新Pod的状态。 func isPodResizeInProgress(pod *v1.Pod, podStatus *v1.PodStatus) bool { for _, c := range pod.Spec.Containers { if cs, ok := podutil.GetContainerStatus(podStatus.ContainerStatuses, c.Name); ok { if cs.Resources == nil { continue } if !cmp.Equal(c.Resources.Limits, cs.Resources.Limits) || !cmp.Equal(cs.AllocatedResources, cs.Resources.Requests) { return true } } } return false } //该函数用于检查Pod中是否有容器的资源正在扩容中。 //它遍历Pod的容器规格，获取每个容器的状态，然后比较容器的资源限制和请求是否相等。 //如果不相等，则表示容器资源正在扩容，函数返回true； //否则，返回false。 func (kl *Kubelet) canResizePod(pod *v1.Pod) (bool, *v1.Pod, v1.PodResizeStatus) { var otherActivePods []*v1.Pod node, err := kl.getNodeAnyWay() if err != nil { klog.ErrorS(err, \u0026#34;getNodeAnyway function failed\u0026#34;) return false, nil, \u0026#34;\u0026#34; } podCopy := pod.DeepCopy() cpuAvailable := node.Status.Allocatable.Cpu().MilliValue() memAvailable := node.Status.Allocatable.Memory().Value() cpuRequests := resource.GetResourceRequest(podCopy, v1.ResourceCPU) memRequests := resource.GetResourceRequest(podCopy, v1.ResourceMemory) if cpuRequests \u0026gt; cpuAvailable || memRequests \u0026gt; memAvailable { klog.V(3).InfoS(\u0026#34;Resize is not feasible as request exceeds allocatable node resources\u0026#34;, \u0026#34;pod\u0026#34;, podCopy.Name) return false, podCopy, v1.PodResizeStatusInfeasible //该函数的主要功能是判断是否可以对给定的Pod进行扩容。具体步骤包括： //通过kl.getNodeAnyWay()获取一个Node节点，深拷贝Pod对象，计算Node节点的可用CPU和内存资源量，获取Pod的CPU和内存请求量， //如果Pod的请求量超过了Node的可用资源量，则认为扩容不可行，并返回相应的信息。 } //该函数是Kubelet的一个方法，用于判断是否可以调整Pod的大小。它首先获取当前节点的信息，并创建Pod的深拷贝。 //然后，它比较节点可用的CPU和内存资源与Pod请求的资源，如果请求的资源超过了节点的可用资源，则返回不可调整大小的错误信息。 // Treat the existing pod needing resize as a new pod with desired resources seeking admit. // If desired resources don\u0026#39;t fit, pod continues to run with currently allocated resources. activePods := kl.GetActivePods() for _, p := range activePods { if p.UID != pod.UID { otherActivePods = append(otherActivePods, p) } } //该函数用于处理需要调整资源的现有Pod，将其视为具有所需资源的新Pod寻求准入。 //如果所需资源无法适应，则Pod将继续以目前分配的资源运行。 //函数首先获取所有活跃的Pod，然后遍历这些Pod，将与给定Pod UID不相同的Pod添加到otherActivePods列表中。 if ok, failReason, failMessage := kl.canAdmitPod(otherActivePods, podCopy); !ok { // Log reason and return. Let the next sync iteration retry the resize klog.V(3).InfoS(\u0026#34;Resize cannot be accommodated\u0026#34;, \u0026#34;pod\u0026#34;, podCopy.Name, \u0026#34;reason\u0026#34;, failReason, \u0026#34;message\u0026#34;, failMessage) return false, podCopy, v1.PodResizeStatusDeferred } //该函数是kl.canAdmitPod()的调用方，用于判断是否可以调整pod的大小。 //如果kl.canAdmitPod()返回不成功，即不满足调整大小的条件，则记录原因并返回false， //同时返回不成功的pod副本和v1.PodResizeStatusDeferred状态。 for _, container := range podCopy.Spec.Containers { idx, found := podutil.GetIndexOfContainerStatus(podCopy.Status.ContainerStatuses, container.Name) if found { for rName, rQuantity := range container.Resources.Requests { podCopy.Status.ContainerStatuses[idx].AllocatedResources[rName] = rQuantity } } } return true, podCopy, v1.PodResizeStatusInProgress } //该函数的功能是更新podCopy的状态，将每个容器的资源请求分配给对应的容器状态。 //具体来说，它遍历podCopy的Spec.Containers，通过podutil.GetIndexOfContainerStatus函数获取每个容器在podCopy.Status.ContainerStatuses中的索引， //如果找到，则遍历该容器的Resources.Requests，将其分配给podCopy.Status.ContainerStatuses中对应容器的AllocatedResources。 //最后，函数返回true，podCopy和v1.PodResizeStatusInProgress，表示更新成功并且正在进行调整大小操作。 func (kl *Kubelet) handlePodResourcesResize(pod *v1.Pod) *v1.Pod { if pod.Status.Phase != v1.PodRunning { return pod } podResized := false for _, container := range pod.Spec.Containers { if len(container.Resources.Requests) == 0 { continue } containerStatus, found := podutil.GetContainerStatus(pod.Status.ContainerStatuses, container.Name) if !found { klog.V(5).InfoS(\u0026#34;ContainerStatus not found\u0026#34;, \u0026#34;pod\u0026#34;, pod.Name, \u0026#34;container\u0026#34;, container.Name) break } //该函数是Kubelet的一个方法，用于处理Pod资源的调整。 //当Pod的状态不是运行中时，直接返回该Pod。 //遍历Pod的容器，如果容器的资源请求不为空，则尝试获取容器的状态。 //如果找不到容器状态，则记录日志并退出循环。 if len(containerStatus.AllocatedResources) != len(container.Resources.Requests) { klog.V(5).InfoS(\u0026#34;ContainerStatus.AllocatedResources length mismatch\u0026#34;, \u0026#34;pod\u0026#34;, pod.Name, \u0026#34;container\u0026#34;, container.Name) break } if !cmp.Equal(container.Resources.Requests, containerStatus.AllocatedResources) { podResized = true break } } //这段Go代码是在检查容器状态（containerStatus）中已分配的资源与容器请求的资源（container.Resources.Requests）是否一致。 //首先，它会比较两个资源列表的长度，如果不相等，则记录一条信息并跳出循环。 //接着，它使用cmp.Equal函数来比较两个资源列表的内容是否完全相等，如果不相等，则将podResized标记为true并跳出循环。 //总的来说，这段代码的主要作用是检测容器的资源请求是否已经被正确分配。 if !podResized { return pod } kl.podResizeMutex.Lock() defer kl.podResizeMutex.Unlock() fit, updatedPod, resizeStatus := kl.canResizePod(pod) if updatedPod == nil { return pod } if fit { // Update pod resource allocation checkpoint if err := kl.statusManager.SetPodAllocation(updatedPod); err != nil { //TODO(vinaykul,InPlacePodVerticalScaling): Can we recover from this in some way? Investigate klog.ErrorS(err, \u0026#34;SetPodAllocation failed\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(updatedPod)) return pod } } //这段Go代码主要实现了Pod的垂直扩展功能。 //具体来说，它首先检查Pod是否已经进行了大小调整，如果没有，则直接返回原始Pod。 //然后，它使用互斥锁来确保对Pod大小调整操作的原子性。 //接下来，它调用canResizePod函数来检查是否可以对Pod进行大小调整， //如果返回的updatedPod为nil，则表示无法调整大小，直接返回原始Pod。如果可以调整大小，则将更新后的Pod的资源分配状态保存到状态管理器中。 //如果保存失败，则记录错误日志并返回原始Pod。 if resizeStatus != \u0026#34;\u0026#34; { // Save resize decision to checkpoint if err := kl.statusManager.SetPodResizeStatus(updatedPod.UID, resizeStatus); err != nil { //TODO(vinaykul,InPlacePodVerticalScaling): Can we recover from this in some way? Investigate klog.ErrorS(err, \u0026#34;SetPodResizeStatus failed\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(updatedPod)) return pod } updatedPod.Status.Resize = resizeStatus } kl.podManager.UpdatePod(updatedPod) kl.statusManager.SetPodStatus(updatedPod, updatedPod.Status) return updatedPod } //这段Go代码中的函数逻辑如下： //首先，检查resizeStatus是否为空。如果不为空，则执行以下操作： //1. 尝试将调整大小的决策保存到检查点，调用kl.statusManager.SetPodResizeStatus方法，并传入updatedPod.UID和resizeStatus作为参数。 //如果出现错误，则记录错误日志并返回原始pod对象。 //2. 更新updatedPod的状态，将resizeStatus赋值给updatedPod.Status.Resize。 //接下来，无论resizeStatus是否为空，都会执行以下操作： //1. 调用kl.podManager.UpdatePod方法，传入updatedPod对象进行更新。 //2. 调用kl.statusManager.SetPodStatus方法，传入updatedPod对象和updatedPod.Status，以更新Pod的状态。 //最后，返回更新后的updatedPod对象。 //这段代码的主要目的是更新Pod的状态，并根据resizeStatus是否为空来决定是否保存调整大小的决策到检查点并更新Pod的状态。 // LatestLoopEntryTime returns the last time in the sync loop monitor. func (kl *Kubelet) LatestLoopEntryTime() time.Time { val := kl.syncLoopMonitor.Load() if val == nil { return time.Time{} } return val.(time.Time) } //该函数用于获取Kubelet同步循环监视器中的最后一个时间入口。 //它首先使用kl.syncLoopMonitor.Load()方法加载监视器的值，如果值为nil，则返回空的时间类型； //否则，将值转换为时间类型并返回。 // updateRuntimeUp calls the container runtime status callback, initializing // the runtime dependent modules when the container runtime first comes up, // and returns an error if the status check fails. If the status check is OK, // update the container runtime uptime in the kubelet runtimeState. func (kl *Kubelet) updateRuntimeUp() { kl.updateRuntimeMux.Lock() defer kl.updateRuntimeMux.Unlock() ctx := context.Background() //该函数是Kubelet的一个方法，用于更新容器运行时的状态。 //它首先通过调用容器运行时的状态回调函数来初始化运行时依赖的模块，然后检查容器运行时的状态是否正常。 //如果状态检查失败，会返回一个错误。 //如果状态检查正常，则会更新容器运行时的运行时间。函数中使用了互斥锁来保证更新操作的原子性。 s, err := kl.containerRuntime.Status(ctx) if err != nil { klog.ErrorS(err, \u0026#34;Container runtime sanity check failed\u0026#34;) return } if s == nil { klog.ErrorS(nil, \u0026#34;Container runtime status is nil\u0026#34;) return } //该Go函数通过调用kl.containerRuntime.Status(ctx)获取容器运行时的状态，并根据状态进行相应的错误处理。 //- 如果获取状态时出现错误，会使用klog.ErrorS记录错误日志，并返回。 //- 如果获取的状态为nil，会使用klog.ErrorS记录错误日志，并返回。 // Periodically log the whole runtime status for debugging. klog.V(4).InfoS(\u0026#34;Container runtime status\u0026#34;, \u0026#34;status\u0026#34;, s) klogErrorS := klog.ErrorS if !kl.containerRuntimeReadyExpected { klogErrorS = klog.V(4).ErrorS } networkReady := s.GetRuntimeCondition(kubecontainer.NetworkReady) //该代码片段是Go语言编写的，它主要实现了定期打印容器运行时状态的功能，用于调试。 //具体来说，函数首先使用klog.V(4).InfoS方法记录一条日志，内容为\u0026#34;Container runtime status\u0026#34;和状态s。 //然后，根据变量kl.containerRuntimeReadyExpected的值，选择使用不同的日志级别记录错误信息。 //最后，该函数获取容器运行时的网络状态，并将其保存在变量networkReady中。 if networkReady == nil || !networkReady.Status { klogErrorS(nil, \u0026#34;Container runtime network not ready\u0026#34;, \u0026#34;networkReady\u0026#34;, networkReady) kl.runtimeState.setNetworkState(fmt.Errorf(\u0026#34;container runtime network not ready: %v\u0026#34;, networkReady)) } else { // Set nil if the container runtime network is ready. kl.runtimeState.setNetworkState(nil) } // information in RuntimeReady condition will be propagated to NodeReady condition. runtimeReady := s.GetRuntimeCondition(kubecontainer.RuntimeReady) // If RuntimeReady is not set or is false, report an error. //这段Go代码主要功能是检查容器运行时网络是否准备就绪，并更新运行时状态。 //首先，它检查networkReady是否为nil或状态为false。 //如果是，则记录错误并设置运行时状态为\u0026#34;container runtime network not ready\u0026#34;。 //否则，将运行时状态设置为nil。 //接下来，它获取运行时准备就绪的条件，并检查是否设置且为true。 //如果不是，则报告错误。 这段代码的目的是确保容器运行时网络已准备就绪，并更新运行时状态。 if runtimeReady == nil || !runtimeReady.Status { klogErrorS(nil, \u0026#34;Container runtime not ready\u0026#34;, \u0026#34;runtimeReady\u0026#34;, runtimeReady) kl.runtimeState.setRuntimeState(fmt.Errorf(\u0026#34;container runtime not ready: %v\u0026#34;, runtimeReady)) return } //这段Go代码是用于定期记录容器运行时状态的，主要用于调试。 //函数首先使用klog.V(4).InfoS记录容器运行时状态的信息。 //然后根据kl.containerRuntimeReadyExpected的值决定使用哪个级别的日志记录错误信息。 //接着检查容器运行时的网络状态，如果不Ready，则记录错误信息并设置相应的状态。 //最后检查容器运行时的RuntimeReady状态，如果不Ready，则记录错误信息并设置相应的状态，并返回。 kl.runtimeState.setRuntimeState(nil) kl.runtimeState.setRuntimeHandlers(s.Handlers) kl.oneTimeInitializer.Do(kl.initializeRuntimeDependentModules) kl.runtimeState.setRuntimeSync(kl.clock.Now()) } //这段Go代码是用于定期记录容器运行时状态的，主要用于调试。 //函数首先使用klog.V(4).InfoS记录容器运行时状态的信息，然后根据kl.containerRuntimeReadyExpected的值决定使用哪个级别的日志记录错误信息。 //接着检查容器运行时的网络状态，如果不Ready，则记录错误信息并设置相应的状态。 //最后检查容器运行时的RuntimeReady状态，如果不Ready，则记录错误信息并设置相应的状态，并返回。 //如果容器运行时状态正常，则会执行以下操作： //1. 调用kl.runtimeState.setRuntimeState(nil)设置运行时状态为正常。 //2. 调用kl.runtimeState.setRuntimeHandlers(s.Handlers)设置运行时处理程序。 //3. 调用kl.oneTimeInitializer.Do(kl.initializeRuntimeDependentModules)执行一次初始化运行时依赖模块的操作。 //4. 调用kl.runtimeState.setRuntimeSync(kl.clock.Now())设置运行时同步时间。 // GetConfiguration returns the KubeletConfiguration used to configure the kubelet. func (kl *Kubelet) GetConfiguration() kubeletconfiginternal.KubeletConfiguration { return kl.kubeletConfiguration } //该函数是一个Go语言函数，名为GetConfiguration，它属于Kubelet结构体。 //函数的作用是返回用于配置kubelet的KubeletConfiguration。 //- kl *Kubelet：函数接收一个Kubelet类型的指针作为参数。 //- kubeletconfiginternal.KubeletConfiguration：函数返回一个KubeletConfiguration类型的结果。 //函数实现很简单，就是直接返回kl对象的kubeletConfiguration属性。 // BirthCry sends an event that the kubelet has started up. func (kl *Kubelet) BirthCry() { // Make an event that kubelet restarted. kl.recorder.Eventf(kl.nodeRef, v1.EventTypeNormal, events.StartingKubelet, \u0026#34;Starting kubelet.\u0026#34;) } //这个函数的功能是发送一个事件，表示kubelet已经启动。 //它通过调用kl.recorder.Eventf方法，传入kl.nodeRef、v1.EventTypeNormal、events.StartingKubelet和\u0026#34;Starting kubelet.\u0026#34;作为参数， //来记录kubelet重启的事件。 // ResyncInterval returns the interval used for periodic syncs. func (kl *Kubelet) ResyncInterval() time.Duration { return kl.resyncInterval } //该函数是Kubelet的一个方法，返回用于周期性同步的时间间隔。 //- kl *Kubelet：表示Kubelet的实例。 //- return kl.resyncInterval：返回Kubelet实例中的resyncInterval属性值，其类型为time.Duration，表示时间间隔。 // ListenAndServe runs the kubelet HTTP server. func (kl *Kubelet) ListenAndServe(kubeCfg *kubeletconfiginternal.KubeletConfiguration, tlsOptions *server.TLSOptions, auth server.AuthInterface, tp trace.TracerProvider) { server.ListenAndServeKubeletServer(kl, kl.resourceAnalyzer, kubeCfg, tlsOptions, auth, tp) } //该函数是Kubelet的ListenAndServe方法，用于启动kubelet的HTTP服务器。 //它通过调用server.ListenAndServeKubeletServer方法来实现，传入Kubelet实例、资源分析器、kubelet配置、TLS选项、认证接口和跟踪提供者作为参数。 // ListenAndServeReadOnly runs the kubelet HTTP server in read-only mode. func (kl *Kubelet) ListenAndServeReadOnly(address net.IP, port uint) { server.ListenAndServeKubeletReadOnlyServer(kl, kl.resourceAnalyzer, address, port) } //该函数用于以只读模式启动kubelet HTTP服务器。它通过调用server.ListenAndServeKubeletReadOnlyServer函数来实现， //该函数接受kl（Kubelet实例）、kl.resourceAnalyzer（资源分析器）以及地址和端口作为参数，用于监听和提供服务。 // ListenAndServePodResources runs the kubelet podresources grpc service func (kl *Kubelet) ListenAndServePodResources() { endpoint, err := util.LocalEndpoint(kl.getPodResourcesDir(), podresources.Socket) if err != nil { klog.V(2).InfoS(\u0026#34;Failed to get local endpoint for PodResources endpoint\u0026#34;, \u0026#34;err\u0026#34;, err) return } //该函数用于启动kubelet的podresources gRPC服务。 //它首先通过调用util.LocalEndpoint函数获取本地端点，该函数的参数是kubelet的pod资源目录和Socket。 //如果获取本地端点失败，则通过klog.V(2).InfoS记录日志信息，并返回。 providers := podresources.PodResourcesProviders{ Pods: kl.podManager, Devices: kl.containerManager, Cpus: kl.containerManager, Memory: kl.containerManager, DynamicResources: kl.containerManager, } server.ListenAndServePodResources(endpoint, providers) } //这段Go代码创建了一个podresources.PodResourcesProviders结构体实例providers， //其中包含了Pods、Devices、Cpus、Memory和DynamicResources等字段，它们都是kl.containerManager的实例。 //接着，代码通过调用server.ListenAndServePodResources函数，启动了一个服务，监听指定的endpoint，并使用providers作为参数提供Pod资源。 // Delete the eligible dead container instances in a pod. Depending on the configuration, the latest dead containers may be kept around. func (kl *Kubelet) cleanUpContainersInPod(podID types.UID, exitedContainerID string) { if podStatus, err := kl.podCache.Get(podID); err == nil { // When an evicted or deleted pod has already synced, all containers can be removed. removeAll := kl.podWorkers.ShouldPodContentBeRemoved(podID) kl.containerDeletor.deleteContainersInPod(exitedContainerID, podStatus, removeAll) } } //该函数用于清理Pod中的死亡容器实例。根据配置，可能会保留最新的死亡容器。 //函数首先通过kl.podCache.Get(podID)获取Pod的状态。 //然后，根据kl.podWorkers.ShouldPodContentBeRemoved(podID)的返回值， //调用kl.containerDeletor.deleteContainersInPod(exitedContainerID, podStatus, removeAll)来删除容器。 // fastStatusUpdateOnce starts a loop that checks if the current state of kubelet + container runtime // would be able to turn the node ready, and sync the ready state to the apiserver as soon as possible. // Function returns after the node status update after such event, or when the node is already ready. // Function is executed only during Kubelet start which improves latency to ready node by updating // kubelet state, runtime status and node statuses ASAP. func (kl *Kubelet) fastStatusUpdateOnce() { ctx := context.Background() start := kl.clock.Now() stopCh := make(chan struct{}) //该函数用于在kubelet启动时快速更新节点状态。 //它会循环检查kubelet和容器运行时的状态，如果发现节点可以变为准备就绪状态，就会立即同步该状态到apiserver。 //函数会在节点状态更新后或节点已经准备就绪时返回。 //该函数通过启动一个上下文对象、记录开始时间并创建一个停止通道来实现循环的控制和终止。 // Keep trying to make fast node status update until either timeout is reached or an update is successful. wait.Until(func() { // fastNodeStatusUpdate returns true when it succeeds or when the grace period has expired // (status was not updated within nodeReadyGracePeriod and the second argument below gets true), // then we close the channel and abort the loop. if kl.fastNodeStatusUpdate(ctx, kl.clock.Since(start) \u0026gt;= nodeReadyGracePeriod) { close(stopCh) } }, 100*time.Millisecond, stopCh) } //这段Go代码使用了wait.Until函数，它会在给定的stopCh通道关闭之前，周期性地执行传入的函数。 //函数的主要作用是尝试进行快速节点状态更新，直到超时或更新成功。 //具体实现中，函数内部调用了kl.fastNodeStatusUpdate方法尝试更新节点状态。 //如果更新成功或超过了nodeReadyGracePeriod时间间隔，则会关闭stopCh通道，结束循环。 //总结起来，这段代码的功能是在一定时间内持续尝试更新节点状态，直到更新成功或超时。 // CheckpointContainer tries to checkpoint a container. The parameters are used to // look up the specified container. If the container specified by the given parameters // cannot be found an error is returned. If the container is found the container // engine will be asked to checkpoint the given container into the kubelet\u0026#39;s default // checkpoint directory. func (kl *Kubelet) CheckpointContainer( ctx context.Context, podUID types.UID, podFullName, containerName string, options *runtimeapi.CheckpointContainerRequest, ) error { container, err := kl.findContainer(ctx, podFullName, podUID, containerName) if err != nil { return err } if container == nil { return fmt.Errorf(\u0026#34;container %v not found\u0026#34;, containerName) } options.Location = filepath.Join( kl.getCheckpointsDir(), fmt.Sprintf( \u0026#34;checkpoint-%s-%s-%s.tar\u0026#34;, podFullName, containerName, time.Now().Format(time.RFC3339), ), ) //该函数的功能是尝试对一个容器进行检查点操作。 //函数通过给定的参数查找指定的容器，如果找不到指定的容器，则返回一个错误。 //如果找到了容器，则会请求容器引擎将该容器检查点保存到kubelet的默认检查点目录中。 //具体来说，函数首先调用kl.findContainer函数来查找指定的容器，如果找不到容器或发生错误，则直接返回错误或nil。 //如果找到了容器，则通过调用filepath.Join函数来设置检查点的存储位置。 //存储位置由kubelet的默认检查点目录、pod全名、容器名和当前时间组成。最后，函数返回错误或nil。 options.ContainerId = string(container.ID.ID) if err := kl.containerRuntime.CheckpointContainer(ctx, options); err != nil { return err } return nil } //该函数的功能是在容器上执行检查点操作。 //首先，它将容器的ID转换为字符串，并将其赋值给options.ContainerId。 //然后，它调用kl.containerRuntime.CheckpointContainer函数来执行实际的检查点操作，将上下文和options作为参数传递。 //如果该函数执行失败并返回错误，则该函数将错误返回。 //如果该函数成功执行，则该函数将返回nil。 // ListMetricDescriptors gets the descriptors for the metrics that will be returned in ListPodSandboxMetrics. func (kl *Kubelet) ListMetricDescriptors(ctx context.Context) ([]*runtimeapi.MetricDescriptor, error) { return kl.containerRuntime.ListMetricDescriptors(ctx) } //该函数是Kubelet的一个方法，用于获取容器运行时的指标描述符。 //它通过调用containerRuntime的ListMetricDescriptors方法来实现。 //返回值是指标描述符的切片和可能的错误。 // ListPodSandboxMetrics retrieves the metrics for all pod sandboxes. func (kl *Kubelet) ListPodSandboxMetrics(ctx context.Context) ([]*runtimeapi.PodSandboxMetrics, error) { return kl.containerRuntime.ListPodSandboxMetrics(ctx) } //该函数是Kubelet的一个方法，用于获取所有Pod沙箱的指标信息。 //它通过调用containerRuntime的ListPodSandboxMetrics方法来实现。 //返回值是一个包含多个Pod沙箱指标信息的切片和一个错误（如果有）。 func (kl *Kubelet) supportLocalStorageCapacityIsolation() bool { return kl.GetConfiguration().LocalStorageCapacityIsolation } //该函数用于判断Kubelet是否支持本地存储容量隔离。 //它通过调用Kubelet的GetConfiguration方法获取Kubelet的配置信息， //然后返回LocalStorageCapacityIsolation字段的值来表示是否支持本地存储容量隔离。 // isSyncPodWorthy filters out events that are not worthy of pod syncing func isSyncPodWorthy(event *pleg.PodLifecycleEvent) bool { // ContainerRemoved doesn\u0026#39;t affect pod state return event.Type != pleg.ContainerRemoved } //该函数用于过滤掉不值得进行Pod同步的事件。具体实现中，判断事件的类型是否为ContainerRemoved， //若不是，则返回true，表示该事件值得进行Pod同步； //若是，则返回false，表示该事件不值得进行Pod同步。 // PrepareDynamicResources calls the container Manager PrepareDynamicResources API // This method implements the RuntimeHelper interface func (kl *Kubelet) PrepareDynamicResources(pod *v1.Pod) error { return kl.containerManager.PrepareDynamicResources(pod) } //该函数是Kubelet的PrepareDynamicResources方法，实现了RuntimeHelper接口。 //它调用了container Manager的PrepareDynamicResources API，用于准备容器的动态资源。 // UnprepareDynamicResources calls the container Manager UnprepareDynamicResources API // This method implements the RuntimeHelper interface func (kl *Kubelet) UnprepareDynamicResources(pod *v1.Pod) error { return kl.containerManager.UnprepareDynamicResources(pod) } //该函数是Kubelet的一个方法，实现了RuntimeHelper接口。 //它调用了container Manager的UnprepareDynamicResources API，用于准备Pod的动态资源。 "},{"id":97,"href":"/docs/k8s%E5%8E%9F%E5%9C%B0%E5%8D%87%E7%BA%A7-k8s-yuan-de-sheng-ji/","title":"K8S原地升级 2024-04-03 15:04:38.125","section":"Docs","content":"原地升级一词中，**“升级”**不难理解，是将应用实例的版本由旧版替换为新版。那么如何结合 Kubernetes 环境来理解“原地”呢？\n我们先来看看 K8s 原生 workload 的发布方式。这里假设我们需要部署一个应用，包括 foo、bar 两个容器在 Pod 中。其中，foo 容器第一次部署时用的镜像版本是 v1，我们需要将其升级为 v2 版本镜像，该怎么做呢？ # 如果这个应用使用 Deployment 部署，那么升级过程中 Deployment 会触发新版本 ReplicaSet 创建 Pod，并删除旧版本 Pod。如下图所示： # 在本次升级过程中，原 Pod 对象被删除，一个新 Pod 对象被创建。新 Pod 被调度到另一个 Node 上，分配到一个新的 IP，并把 foo、bar 两个容器在这个 Node 上重新拉取镜像、启动容器。 # 如果这个应该使用 StatefulSet 部署，那么升级过程中 StatefulSet 会先删除旧 Pod 对象，等删除完成后用同样的名字在创建一个新的 Pod 对象。如下图所示： # 值得注意的是，尽管新旧两个 Pod 名字都叫 pod-0，但其实是两个完全不同的 Pod 对象（uid也变了）。StatefulSet 等到原先的 pod-0 对象完全从 Kubernetes 集群中被删除后，才会提交创建一个新的 pod-0 对象。而这个新的 Pod 也会被重新调度、分配IP、拉镜像、启动容器。 # -而所谓原地升级模式，就是在应用升级过程中避免将整个 Pod 对象删除、新建，而是基于原有的 Pod 对象升级其中某一个或多个容器的镜像版本： # 在原地升级的过程中，我们仅仅更新了原 Pod 对象中 foo 容器的 image 字段来触发 foo 容器升级到新版本。而不管是 Pod 对象，还是 Node、IP 都没有发生变化，甚至 foo 容器升级的过程中 bar 容器还一直处于运行状态。 # 总结：这种只更新 Pod 中某一个或多个容器版本、而不影响整个 Pod 对象、其余容器的升级方式，被我们称为 Kubernetes 中的原地升级。\n收益分析 # 那么，我们为什么要在 Kubernetes 中引入这种原地升级的理念和设计呢？ # 首先，这种原地升级的模式极大地提升了应用发布的效率，根据非完全统计数据，在阿里环境下原地升级至少比完全重建升级提升了 80% 以上的发布速度。这其实很容易理解，原地升级为发布效率带来了以下优化点： # 节省了调度的耗时，Pod 的位置、资源都不发生变化； # 节省了分配网络的耗时，Pod 还使用原有的 IP； # 节省了分配、挂载远程盘的耗时，Pod 还使用原有的 PV（且都是已经在 Node 上挂载好的）； # 节省了大部分拉取镜像的耗时，因为 Node 上已经存在了应用的旧镜像，当拉取新版本镜像时只需要下载很少的几层 layer。 # 其次，当我们升级 Pod 中一些 sidecar 容器（如采集日志、监控等）时，其实并不希望干扰到业务容器的运行。但面对这种场景，Deployment 或 StatefulSet 的升级都会将整个 Pod 重建，势必会对业务造成一定的影响。而容器级别的原地升级变动的范围非常可控，只会将需要升级的容器做重建，其余容器包括网络、挂载盘都不会受到影响。 # 最后，原地升级也为我们带来了集群的稳定性和确定性。当一个 Kubernetes 集群中大量应用触发重建 Pod 升级时，可能造成大规模的 Pod 飘移，以及对 Node 上一些低优先级的任务 Pod 造成反复的抢占迁移。这些大规模的 Pod 重建，本身会对 apiserver、scheduler、网络/磁盘分配等中心组件造成较大的压力，而这些组件的延迟也会给 Pod 重建带来恶性循环。而采用原地升级后，整个升级过程只会涉及到 controller 对 Pod 对象的更新操作和 kubelet 重建对应的容器。 # 技术背景 # 在阿里巴巴内部，绝大部分电商应用在云原生环境都统一用原地升级的方式做发布，而这套支持原地升级的控制器就位于 OpenKruise 开源项目中。 # 也就是说，阿里内部的云原生应用都是统一使用 OpenKruise 中的扩展 workload 做部署管理的，而并没有采用原生 Deployment/StatefulSet 等。 # 那么 OpenKruise 是如何实现原地升级能力的呢？在介绍原地升级实现原理之前，我们先来看一些原地升级功能所依赖的原生 Kubernetes 功能： # 背景 1：Kubelet 针对 Pod 容器的版本管理 # 每个 Node 上的 Kubelet，会针对本机上所有 Pod.spec.containers 中的每个 container 计算一个 hash 值，并记录到实际创建的容器中。 # 如果我们修改了 Pod 中某个 container 的 image 字段，kubelet 会发现 container 的 hash 发生了变化、与机器上过去创建的容器 hash 不一致，而后 kubelet 就会把旧容器停掉，然后根据最新 Pod spec 中的 container 来创建新的容器。 # 这个功能，其实就是针对单个 Pod 的原地升级的核心原理。\n背景 2：Pod 更新限制 # 在原生 kube-apiserver 中，对 Pod 对象的更新请求有严格的 validation 校验逻辑： # // validate updateable fields: // 1. spec.containers[*].image // 2. spec.initContainers[*].image // 3. spec.activeDeadlineSeconds 简单来说，对于一个已经创建出来的 Pod，在 Pod Spec 中只允许修改 containers/initContainers 中的 image 字段，以及 activeDeadlineSeconds 字段。对 Pod Spec 中所有其他字段的更新，都会被 kube-apiserver 拒绝。 # 背景 3：containerStatuses 上报 # kubelet 会在 pod.status 中上报 containerStatuses，对应 Pod 中所有容器的实际运行状态： # apiVersion: v1 kind: Pod spec: containers: - name: nginx image: nginx:latest status: containerStatuses: - name: nginx image: nginx:mainline imageID: docker-pullable://nginx@sha256:2f68b99bc0d6d25d0c56876b924ec20418544ff28e1fb89a4c27679a40da811b 绝大多数情况下，spec.containers[x].image 与 status.containerStatuses[x].image 两个镜像是一致的。 # 但是也有上述这种情况，kubelet 上报的与 spec 中的 image 不一致（spec 中是 nginx:latest，但 status 中上报的是 nginx:mainline）。 # 这是因为，kubelet 所上报的 image 其实是从 CRI 接口中拿到的容器对应的镜像名。而如果 Node 机器上存在多个镜像对应了一个 imageID，那么上报的可能是其中任意一个： # $ docker images | grep nginx nginx latest 2622e6cca7eb 2 days ago 132MB nginx mainline 2622e6cca7eb 2 days ago 因此，一个 Pod 中 spec 和 status 的 image 字段不一致，并不意味着宿主机上这个容器运行的镜像版本和期望的不一致。 # 背景 4：ReadinessGate 控制 Pod 是否 Ready # 在 Kubernetes 1.12 版本之前，一个 Pod 是否处于 Ready 状态只是由 kubelet 根据容器状态来判定：如果 Pod 中容器全部 ready，那么 Pod 就处于 Ready 状态。 # 但事实上，很多时候上层 operator 或用户都需要能控制 Pod 是否 Ready 的能力。因此，Kubernetes 1.12 版本之后提供了一个 readinessGates 功能来满足这个场景。如下： # apiVersion: v1 kind: Pod spec: readinessGates: - conditionType: MyDemo status: conditions: - type: MyDemo status: \u0026#34;True\u0026#34; - type: ContainersReady status: \u0026#34;True\u0026#34; - type: Ready status: \u0026#34;True\u0026#34; 目前 kubelet 判定一个 Pod 是否 Ready 的两个前提条件： # Pod 中容器全部 Ready（其实对应了 ContainersReady condition 为 True）； # 如果 pod.spec.readinessGates 中定义了一个或多个 conditionType，那么需要这些 conditionType 在 pod.status.conditions 中都有对应的 status: \u0026ldquo;true\u0026rdquo; 的状态。 # 只有满足上述两个前提，kubelet 才会上报 Ready condition 为 True。 # 实现原理 # 了解了上面的四个背景之后，接下来分析一下 OpenKruise 是如何在 Kubernetes 中实现原地升级的原理。 # 1. 单个 Pod 如何原地升级？ # 由“背景 1”可知，其实我们对一个存量 Pod 的 spec.containers[x] 中字段做修改，kubelet 会感知到这个 container 的 hash 发生了变化，随即就会停掉对应的旧容器，并用新的 container 来拉镜像、创建和启动新容器。 # 由“背景 2”可知，当前我们对一个存量 Pod 的 spec.containers[x] 中的修改，仅限于 image 字段。 # 因此，得出第一个实现原理：**对于一个现有的 Pod 对象，我们能且只能修改其中的 spec.containers[x].image 字段，来触发 Pod 中对应容器升级到一个新的 image。 # 2. 如何判断 Pod 原地升级成功？ # 接下来的问题是，当我们修改了 Pod 中的 spec.containers[x].image 字段后，如何判断 kubelet 已经将容器重建成功了呢？ # 由“背景 3”可知，比较 spec 和 status 中的 image 字段是不靠谱的，因为很有可能 status 中上报的是 Node 上存在的另一个镜像名（相同 imageID）。 # 因此，得出第二个实现原理：判断 Pod 原地升级是否成功，相对来说比较靠谱的办法，是在原地升级前先将 status.containerStatuses[x].imageID 记录下来。在更新了 spec 镜像之后，如果观察到 Pod 的 status.containerStatuses[x].imageID 变化了，我们就认为原地升级已经重建了容器。\n但这样一来，我们对原地升级的 image 也有了一个要求：不能用 image 名字（tag）不同、但实际对应同一个 imageID 的镜像来做原地升级，否则可能一直都被判断为没有升级成功（因为 status 中 imageID 不会变化）。 # 当然，后续我们还可以继续优化。OpenKruise 即将开源镜像预热的能力，会通过 DaemonSet 在每个 Node 上部署一个 NodeImage Pod。通过 NodeImage 上报我们可以得知 pod spec 中的 image 所对应的 imageID，然后和 pod status 中的 imageID 比较即可准确判断原地升级是否成功。 # 3. 如何确保原地升级过程中流量无损？ # 在 Kubernetes 中，一个 Pod 是否 Ready 就代表了它是否可以提供服务。因此，像 Service 这类的流量入口都会通过判断 Pod Ready 来选择是否能将这个 Pod 加入 endpoints 端点中。 # 由“背景 4”可知，从 Kubernetes 1.12+ 之后，operator/controller 这些组件也可以通过设置 readinessGates 和更新 pod.status.conditions 中的自定义 type 状态，来控制 Pod 是否可用。 # 因此，得出第三个实现原理：可以在 pod.spec.readinessGates 中定义一个叫 InPlaceUpdateReady 的 conditionType。 # 在原地升级时： # 先将 pod.status.conditions 中的 InPlaceUpdateReady condition 设为 \u0026ldquo;False\u0026rdquo;，这样就会触发 kubelet 将 Pod 上报为 NotReady，从而使流量组件（如 endpoint controller）将这个 Pod 从服务端点摘除； # 再更新 pod spec 中的 image 触发原地升级。 # 原地升级结束后，再将 InPlaceUpdateReady condition 设为 \u0026ldquo;True\u0026rdquo;，使 Pod 重新回到 Ready 状态。\n另外在原地升级的两个步骤中，第一步将 Pod 改为 NotReady 后，流量组件异步 watch 到变化并摘除端点可能是需要一定时间的。因此我们也提供优雅原地升级的能力，即通过 gracePeriodSeconds 配置在修改 NotReady 状态和真正更新 image 触发原地升级两个步骤之间的静默期时间。 # 4. 组合发布策略 # 原地升级和 Pod 重建升级一样，可以配合各种发布策略来执行： # partition：如果配置 partition 做灰度，那么只会将 replicas-partition 数量的 Pod 做原地升级； # maxUnavailable：如果配置 maxUnavailable，那么只会将满足 unavailable 数量的 Pod 做原地升级； # maxSurge：如果配置 maxSurge 做弹性，那么当先扩出来 maxSurge 数量的 Pod 之后，存量的 Pod 仍然使用原地升级； # priority/scatter：如果配置了发布优先级/打散策略，会按照策略顺序对 Pod 做原地升级。 # 总结 # 如上文所述，OpenKruise 结合 Kubernetes 原生提供的 kubelet 容器版本管理、readinessGates 等功能，实现了针对 Pod 的原地升级能力。 # 而原地升级也为应用发布带来大幅的效率、稳定性提升。值得关注的是，随着集群、应用规模的增大，这种提升的收益越加明显。正是这种原地升级能力，在近两年帮助了阿里巴巴超大规模的应用容器平稳迁移到了基于 Kubernetes 的云原生环境，而原生 Deployment/StatefulSet 是完全无法在这种体量的环境下铺开使用的。 # "},{"id":98,"href":"/docs/k8s%E5%91%BD%E4%BB%A4%E6%8C%87%E5%8D%97-k8s-ming-ling-zhi-nan/","title":"K8S命令指南 2024-04-03 15:14:14.207","section":"Docs","content":" 一、基础命令 # 在Kubernetes中，基础命令用于日常的查询和基本操作。以下表格展示了这些基础命令，它们的说明，以及相应的使用举例。\n命令 说明 举例 kubectl version 显示客户端和服务器的Kubernetes版本。 kubectl version kubectl api-versions 列出可用的API版本。 kubectl api-versions kubectl get 列出一个或多个资源。 kubectl get pods kubectl describe 显示一个或多个资源的详细信息。 kubectl describe nodes my-node kubectl logs 打印容器的日志。 kubectl logs my-pod kubectl attach 附加到正在运行的容器进行交互。 kubectl attach my-pod -i kubectl exec 在容器内执行命令。 kubectl exec my-pod -- ls / kubectl port-forward 为Pod中的容器端口转发。 kubectl port-forward my-pod 5000:6000 kubectl proxy 运行一个代理到Kubernetes API服务器。 kubectl proxy kubectl cp 在容器和本地文件系统之间复制文件/目录。 kubectl cp /tmp/foo_dir my-pod:/tmp/bar_dir kubectl run 在集群中快速启动一个指定的镜像。 kubectl run nginx --image=nginx kubectl expose 将Pod或其他资源类型暴露为Kubernetes服务。 kubectl expose deployment nginx --port=80 这些命令为Kubernetes用户提供了强大的工具集，用于管理和调试在Kubernetes集群中运行的应用。\n二、资源创建与管理 # 在Kubernetes中，资源创建与管理命令是用于部署、更新和维护集群资源的关键工具。以下表格展示了这些命令，以及它们的说明和使用示例。\n命令 说明 举例 kubectl create 从文件或stdin创建一个或多个资源。 kubectl create -f my-resource.yaml kubectl delete 从文件、stdin或指定标签、名称、资源选择器删除资源。 kubectl delete -f my-resource.yaml kubectl apply 应用一个或多个资源的更改。 kubectl apply -f my-resource.yaml kubectl edit 编辑并更新服务器上一个或多个资源的定义。 kubectl edit svc/my-service kubectl replace 从文件或stdin替换资源。 kubectl replace -f my-resource.yaml kubectl scale 更新资源的大小。 kubectl scale --replicas=3 deployment/my-deployment kubectl autoscale 自动扩展Pod的数量。 kubectl autoscale deployment my-deployment --min=10 --max=15 kubectl rollout 管理资源的部署。 kubectl rollout status deployment/my-deployment kubectl set 设置特定资源的特定字段。 kubectl set image deployment/my-deployment nginx=nginx:1.9.1 kubectl auth 检查用户对于资源的权限。 kubectl auth can-i create deployments kubectl patch 使用补丁部分更新资源的特定字段。 kubectl patch node k8s-node-1 -p '{\u0026quot;spec\u0026quot;:{\u0026quot;unschedulable\u0026quot;:true}}' kubectl convert 转换配置文件到不同的API版本。 kubectl convert -f ./pod.yaml --output-version=v1 kubectl rollout history 查看Deployment或StatefulSet的历史版本。 kubectl rollout history deployment/nginx kubectl rollout undo 回滚到Deployment或StatefulSet的旧版本。 kubectl rollout undo deployment/nginx kubectl completion 生成shell自动补全的脚本。 kubectl completion bash kubectl api-resources 列出API服务器上可用的资源类型。 kubectl api-resources 这些命令为开发者和系统管理员提供了广泛的工具，以灵活地处理Kubernetes资源的生命周期，包括创建、更新、删除和自动化管理。\n三、集群管理与维护 # Kubernetes的集群管理与维护命令涉及到集群的日常运行和维护任务，包括监控资源、管理节点和配置集群级别的设置。以下是这些命令的详细列表：\n命令 说明 举例 kubectl cluster-info 显示集群信息。 kubectl cluster-info kubectl top 显示集群中资源的当前使用情况。 kubectl top nodes kubectl cordon 标记节点为不可调度，防止新的pods被调度到该节点。 kubectl cordon my-node kubectl uncordon 解除节点的不可调度状态。 kubectl uncordon my-node kubectl drain 准备节点进行维护，安全地驱逐节点上的pods。 kubectl drain my-node kubectl quota 显示资源配额。 kubectl quota -n my-namespace kubectl annotate 添加或更新资源的注释。 kubectl annotate pods my-pod icon-url=http://my-icon.com kubectl label 更新资源的标签。 kubectl label pods my-pod new-label=my-label kubectl taint 为节点添加或删除污点。 kubectl taint nodes my-node key=value:NoSchedule 这些命令为集群管理员提供了丰富的工具，用于监控和管理Kubernetes集群的健康和性能，确保集群的稳定性和效率。\n四、其他指令 # Kubernetes 还提供了一系列其他指令，用于执行特定的高级操作、配置管理、调试和集成扩展功能。这些指令对于深入理解和有效管理 Kubernetes 集群至关重要。\n配置管理 # 命令 说明 举例 kubectl config 查看或修改kubeconfig文件。 kubectl config view kubectl config get-contexts 查看所有的kubeconfig上下文。 kubectl config get-contexts kubectl certificate 修改证书资源。 kubectl certificate approve my-cert 调试和诊断 # 命令 说明 举例 kubectl debug 创建调试会话。 kubectl debug node/my-node -it kubectl diff 比较当前状态与所期望的状态。 kubectl diff -f my-resource.yaml kubectl explain 获取资源的文档。 kubectl explain pods kubectl wait 等待资源达到某个条件。 kubectl wait --for=condition=Ready pod/my-pod 插件和扩展 # 命令 说明 举例 kubectl plugin 管理kubectl插件。 kubectl plugin list kubectl plugin list 列出已安装的插件。 kubectl plugin list 这些指令为开发人员和运维人员提供了强大的工具集，用于深入理解和管理 Kubernetes 集群的复杂性，提高日常运维的效率和效果。\n"},{"id":99,"href":"/docs/k8s%E5%A6%82%E4%BD%95%E8%8E%B7%E5%BE%97-ip-k8s-ru-he-huo-de-ip/","title":"K8S如何获得 IP 2024-04-16 17:09:59.658","section":"Docs","content":"Kubernetes 网络模型的核心要求之一是每个 Pod 都应该有自己的 IP 地址，并且集群中的每个 Pod 都应该能够使用这个 IP 地址与其进行通信。有多个网络提供商（flannel、calico、canal 等）实现了这种网络模型。\n在 kubernetes 中设置网络有多种方法，容器运行时也有多种选项。在这篇文章中，我将使用 Flannel作为网络提供程序， 使用 Containerd作为容器运行时。\n背景概念 # 容器网络：非常简短的概述\n有一些非常好的帖子解释了容器网络的工作原理。对于上下文，我将在这里使用涉及 Linux 桥接网络和数据包封装的单一方法进行非常高层次的概述。在这里跳过细节。\n同一主机上的容器\n在同一主机上运行的容器可以通过其 IP 地址相互通信的方式之一是通过 Linux 桥。在 kubernetes（和 docker）世界中， 创建了veth（虚拟以太网）设备来实现此目的。该 veth 设备的一端插入容器网络命名空间，另一端连接到 主机网络上的Linux 桥。同一主机上的所有容器都将这一 veth 对的一端连接到 linux 网桥，并且它们可以通过网桥使用其 IP 地址相互通信。Linux 网桥还分配有一个 IP 地址，并充当从 pod 发往不同节点的出口流量的网关。\n不同主机上的容器\n在不同主机上运行的容器可以通过其 IP 地址相互通信的方法之一是使用数据包封装。Flannel 通过 vxlan支持此功能，它将原始数据包包装在 UDP 数据包中并将其发送到目的地。\n在 kubernetes 集群中，flannel 在每个节点上创建一个 vxlan 设备和一些路由表条目。发往不同主机上的容器的每个数据包都会经过 vxlan 设备并封装在 UDP 数据包中。在目的地，检索封装的数据包并将数据包路由到目标 Pod。\n注意：这只是配置容器之间网络的方式之一。\n什么是 CRI？\nCRI（容器运行时接口）是一个插件接口，允许 kubelet 使用不同的容器运行时。各种容器运行时都实现了 CRI API，这允许用户在其 kubernetes 安装中使用他们选择的容器运行时。\n什么是CNI？\nCNI 项目包含一个 规范，为 Linux 容器提供基于插件的通用网络解决方案。它还包含各种插件，在配置 Pod 网络时执行不同的功能。CNI 插件是遵循 CNI 规范的可执行文件，我们将在下面的帖子中讨论一些插件。\n为 Pod IP 地址的节点分配子网 # 如果所有 Pod 都需要有一个 IP 地址，那么确保整个集群中的所有 Pod 都具有唯一的 IP 地址非常重要。这是通过为每个节点分配一个唯一的子网来实现的，从该子网为 Pod 分配该节点上的 IP 地址。\n节点 IPAM 控制器\n当nodeipam作为选项传递给 kube-controller-manager 的 \u0026ndash;controllers命令行标志时，它会从集群 CIDR（集群网络的 IP 范围）为每个节点分配一个专用子网 (podCIDR)。由于这些 podCIDR 是不相交的子网，因此它允许为每个 pod 分配唯一的 IP 地址。\n当 kubernetes 节点首次向集群注册时，会为其分配一个 podCIDR。要更改分配给集群中节点的 podCIDR，需要取消注册节点，然后重新注册节点，并将任何配置更改首先应用于 kubernetes 控制平面。podCIDR可以使用以下命令列出节点。\n$ kubectl get no \u0026lt;nodeName\u0026gt; -o json | jq \u0026#39;.spec.podCIDR\u0026#39; 10.244.0.0/24 Kubelet、容器运行时和 CNI 插件 - 如何将它们拼接在一起\n当 pod 被调度到节点上时，会发生很多事情来启动 pod。在本节中，我将仅关注与为 pod 配置网络相关的交互。\n在节点上调度 Pod 后，以下交互将导致配置网络并启动应用程序容器。\n容器运行时和 CNI 插件之间的交互\n每个网络提供商都有一个 CNI 插件，容器运行时会调用该插件来在 pod 启动时为其配置网络。使用containerd作为容器运行时， Containerd CRI插件调用CNI插件。每个网络提供商还会在每个 kubernetes 节点上安装一个代理来配置 pod 网络。安装网络提供商代理后，它要么随 CNI 配置一起提供，要么在节点上创建一个配置，然后 CRI 插件使用该配置来确定要调用哪个 CNI 插件。\nCNI 配置文件的位置是可配置的，默认值为/etc/cni/net.d/. CNI 插件需要由集群管理员发送到每个节点上。CNI 插件的位置也是可配置的，默认值为/opt/cni/bin。\n如果使用containerd作为容器运行时，则可以在containerd配置[plugins.\u0026ldquo;io.containerd.grpc.v1.cri\u0026rdquo;.cni]部分 下指定CNI配置和CNI插件二进制文件的路径。\n由于我们在这里将 Flannel 称为网络提供商，因此我将稍微讨论一下 Flannel 的设置方式。Flanneld 是 Flannel 守护进程，通常install-cni作为 守护进程集和init 容器安装在 kubernetes 集群上。容器在每个节点上install-cni创建 CNI 配置文件。/etc/cni/net.d/10-flannel.conflistFlaneld 创建一个 vxlan 设备，从 apiserver 获取网络元数据并监视 Pod 上的更新。创建 Pod 时，它会为整个集群中的所有 Pod 分配路由，这些路由允许 Pod 通过其 IP 地址相互连接。有关Flannel工作原理的详细信息，推荐参考官方描述。\nContainerd CRI Plugin 和 CNI 插件之间的交互可以可视化如下：\n如上所述，kubelet 调用 Containerd CRI 插件来创建 pod，Containerd CRI 插件调用 CNI 插件为 pod 配置网络。网络提供商 CNI 插件调用其他基础 CNI 插件来配置网络。CNI 插件之间的交互如下所述。\nCNI 插件之间的交互 # 有各种 CNI 插件可以帮助配置主机上容器之间的网络。在这篇文章中，我们将参考 3 个插件。\nFlannel CNI 插件当使用 Flannel 作为网络提供者时，Containerd CRI 插件 使用CNI 配置文件 - /etc/cni/net.d/10-flannel.conflist.\n$ cat /etc/cni/net.d/10-flannel.conflist { \u0026#34;name\u0026#34;: \u0026#34;cni0\u0026#34;, \u0026#34;plugins\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;flannel\u0026#34;, \u0026#34;delegate\u0026#34;: { \u0026#34;ipMasq\u0026#34;: false, \u0026#34;hairpinMode\u0026#34;: true, \u0026#34;isDefaultGateway\u0026#34;: true } } ] } Fannel CNI 插件与 Flanneld 结合使用。当 Flaneld 启动时，它会从 apiserver 获取 podCIDR 和其他网络相关详细信息，并将它们存储在文件中 - /run/flannel/subnet.env.\nFLANNEL_NETWORK=10.244.0.0/16 FLANNEL_SUBNET=10.244.0.1/24 FLANNEL_MTU=1450 FLANNEL_IPMASQ=false Flannel CNI 插件使用其中的信息/run/flannel/subnet.env来配置和调用桥接 CNI 插件。\n桥接 CNI 插件\nFlannel CNI 插件使用以下配置调用 Bridge CNI 插件：\n{ \u0026#34;name\u0026#34;: \u0026#34;cni0\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;bridge\u0026#34;, \u0026#34;mtu\u0026#34;: 1450, \u0026#34;ipMasq\u0026#34;: false, \u0026#34;isGateway\u0026#34;: true, \u0026#34;ipam\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;host-local\u0026#34;, \u0026#34;subnet\u0026#34;: \u0026#34;10.244.0.0/24\u0026#34; } } 当 第一次调用Bridge CNI 插件\u0026quot;name\u0026quot;: \u0026ldquo;cni0\u0026quot;时，它会使用配置文件中指定的内容创建一个 Linux 桥。然后，它为每个 pod 创建一个 veth 对 - 该对的一端位于容器的网络命名空间中，另一端连接到主机网络上的 linux 桥。使用 Bridge CNI 插件，主机上的所有容器都连接到主机网络上的 linux 桥。\n配置 veth 对后，Bridge 插件会调用主机本地 IPAM CNI 插件。使用哪个 IPAM 插件可以在 CNI 配置中配置 CRI 插件用于调用 flannel CNI 插件。\n主机本地 IPAM CNI 插件\nBridge CNI 插件 使用以下配置调用主机本地 IPAM CNI 插件：\n{ \u0026#34;name\u0026#34;: \u0026#34;cni0\u0026#34;, \u0026#34;ipam\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;host-local\u0026#34;, \u0026#34;subnet\u0026#34;: \u0026#34;10.244.0.0/24\u0026#34;, \u0026#34;dataDir\u0026#34;: \u0026#34;/var/lib/cni/networks\u0026#34; } } 主机本地 IPAM（IP 地址管理）插件从 - 中返回容器的 IP 地址，并将subnet分配的 IP 存储在主机上本地指定的目录下。文件包含分配了 IP 的容器 ID。dataDir/var/lib/cni/networks/\u0026lt;network-name=cni0\u0026gt;/\u0026lt;ip\u0026gt;/var/lib/cni/networks/\u0026lt;network-name=cni0\u0026gt;/\u0026lt;ip\u0026gt;\n调用时，主机本地 IPAM 插件返回以下有效负载\n{ \u0026#34;ip4\u0026#34;: { \u0026#34;ip\u0026#34;: \u0026#34;10.244.4.2\u0026#34;, \u0026#34;gateway\u0026#34;: \u0026#34;10.244.4.3\u0026#34; }, \u0026#34;dns\u0026#34;: {} } 小结 # Kube-controller-manager为每个节点分配一个podCIDR。节点上的 Pod 会根据 podCIDR 中的子网值分配一个 IP 地址。由于所有节点上的 podCIDR 都是不相交的子网，因此它允许为每个 pod 分配唯一的 IP 地址。\nKubernetes 集群管理员配置并安装 kubelet、容器运行时、网络提供商代理并在每个节点上分发 CNI 插件。当网络提供商代理启动时，它会生成 CNI 配置。当 pod 被调度到节点上时，kubelet 会调用 CRI 插件来创建 pod。在containerd的情况下，Containerd CRI插件然后调用CNI配置中指定的CNI插件来配置pod网络。所有这些都会导致 Pod 获得 IP 地址。\n"},{"id":100,"href":"/docs/k8s%E5%BA%94%E7%94%A8%E7%9A%84%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5-k8s-ying-yong-de-zui-jia-shi-jian/","title":"k8s应用的最佳实践 2024-04-03 14:43:16.936","section":"Docs","content":" 一文带你检查Kubernetes应用是否为最佳实践 # 一篇从应用部署/服务管治/集群配置三个方便来check你的K8S使用姿势是否正确,包含单不限于服务监控检查/资源使用/标签/HPA，VPA/安全策略/RBAC/日志/监控是否为最佳实践的check list。\n一 应用部署 # 1.1 健康检查 # readiness probe确定容器何时可以接收流量。 Kubelet执行检查并确定应用程序是否可以接收流量。\nliveness probe确定何时应重新启动容器。 kubelet执行检查并确定是否应重新启动容器。\n1.1.1 容器就绪性探针 # 就绪性和存活性探针没有默认值，如果您未设置就绪探针，则kubelet会假定该应用程序已准备就绪，可以在容器启动后立即接收流量。\n1.1.2 发生致命错误时容器崩溃 # 如果应用程序遇到不可恢复的错误，则应使其崩溃，例如：\n未捕获的异常 代码中的错字（用于动态语言） 无法加载标头或依赖项 请注意，您不应发信号通知Liveness探针失败，相反，您应该立即退出该过程，并让kubelet重新启动容器。\n1.1.3 配置被动的存活性探针 # Liveness探针旨在在卡住容器时重新启动容器，例如：\n如果您的应用程序正在处理无限循环，则无法退出或寻求帮助。\n当该进程消耗100％的CPU时，将没有时间回复（其他）Readiness探针检查，并且最终将其从服务中删除。但是，该Pod仍被注册为当前Deployment的活动副本。如果没有Liveness探针，它将保持运行状态，但与服务分离。换句话说，该过程不仅不处理任何请求，而且还消耗资源。\n此时应该怎么办：\n从您的应用程序公开端点 端点总是回复成功响应 使用“活力”探针获取端点 请注意，您不应该使用Liveness探针来处理应用程序中的致命错误，并要求Kubernetes重新启动应用程序。相反，您应该让应用程序崩溃。\n仅在过程无响应的情况下，才应将“活动性”探针用作恢复机制。\n1.1.4 存活性探针与就绪性探针的区别 # 当“活力”和“就绪”探针指向相同的端点时，探针的作用会合并在一起。\n当应用程序发出信号表明尚未准备就绪或尚待运行时，kubelet会将容器与服务分离并同时将其删除。\n您可能会注意到连接断开，因为容器没有足够的时间耗尽当前连接或处理传入的连接。\n可以参考：article that discussed graceful shutdown.\n1.2 应用的独立性 # 如果应用程序连接到数据库，也许你认为如果数据库为就绪就返回一个失败的就绪就ok了，但是事实并非如此，例如您有一个依赖于后端API的前端应用程序。如果该API不稳定（例如由于错误而有时不可用），则就绪探测器将失败，并且前端应用程序中的相关就绪也将失败。这就会导致停机时间。更一般而言，下游依赖项的故障可能会传播到上游的所有应用程序，并最终也降低前端面层\n1.2.1 就绪性探针应该独立 # 就绪性探针不应该依以下服务：\n数据库 数据库迁移 APIs 第三方服务 详细可参考：explore what happens when there’re dependencies in the readiness probes in this essay.\n1.2.2 应用程序重连到依赖服务 # 应用启动时，它不应该因为数据库等依赖项尚未就绪就崩溃，而是，应用程序应继续尝试重新连接数据库，直到成功为止。 # Kubernetes希望可以以任何顺序启动应用程序组件。当您确保您的应用程序可以重新连接到诸如数据库之类的依赖项时，这样可以大大提升服务的健壮性。 # 1.3 优雅的关机 # 您应该等待现有连接耗尽并停止处理新连接。请注意，当Pod终止时，该Pod的端点将从服务中删除。 # 但是，可能需要一些时间才能将诸如kube-proxy或Ingress控制器之类的组件通知更改。 # 详细可参考：handling client requests correctly with Kubernetes. # 正确的优雅停止顺序 # 在收到SIGTERM后 # 服务器停止接受新的链接 # 完成现有所有的请求 # 然后杀死所有的keepalive链接 # 进程退出 # 可以利用工具测试：test that your app gracefully shuts down with this tool: kube-sigterm-test. # 1.3.1 应用程序未通过SIGTERM信号关闭，但它已经终止了链接 # 可能需要一些时间才能将诸如kube-proxy或Ingress控制器之类的组件通知端点更改。 # 因此，尽管标记为已终止，但流量仍可能流向Pod。 # 该应用程序应停止接受所有剩余连接上的新请求，并在耗尽传出队列后将其关闭。 # 1.3.2 应用程序仍在宽限期处理请求 # 您可能要考虑使用容器生命周期事件，例如 the preStop handler自定义pod删除之前的动作 # 1.3.3 Dockerfile中的CMD将SIGTERM转发到进程 # 通过在应用中捕获SIGTERM信号，可以在Pod即将终止时收到通知。 # 你应该注意：forwarding the signal to the right process in your container. # 1.3.4 关闭所有空闲的保持活动套接字 # 如果调用方应用未关闭TCP连接（例如使用TCP保持活动状态或连接池），它将连接到一个Pod，而不使用该服务中的其他Pod。 # 当时当pod删除的时候会发生什么呢，理想状态请求应该使用其他POD，但是，调用方应用程序与即将终止的Pod的连接寿命很长，它将继续使用它。 # 另一方面，你不应该突然终止一个长链接，相反，您应该先关闭它们，然后再关闭应用程序。 # 您可以在有关以下内容的文章中阅读有关保持活动连接的信息：gracefully shutting down a Nodejs HTTP server. # 1.4 容错能力 # 物理机硬件故障 # 云供应商或管理程序 # kernel恐慌 # pod部署在这些节点上也将丢失 # 在以下情况下可以删除pod # 直接删除一个pod # draining一个node # 从节点上删除Pod，以允许另一个Pod适合该节点 # 以上任何情况都可能影响您的应用程序的可用性，并可能导致停机。 # 您应该避免所有Pod都无法使用并且无法提供实时流量的情况。 # 1.4.1 部署运行多个副本 # 永远不要仅运行一个pod # 利用控制器Deployment, DaemonSet, ReplicaSet or StatefulSet.来运行pod，不应该将应用运行为自主式pod # 可以参考：Running more than one instance your of your Pods guarantees that deleting a single Pod won’t cause downtime. # 1.4.2 避免将Pod放置在单个节点中 # 即使您运行Pod的多个副本，也无法保证丢失节点不会破坏您的服务。 # 如果你在单一node上运行一个11个副本集的pod，如果这个node故障，这个服务也一样会停机 # 运行反亲和性在集群中：You should apply anti-affinity rules to your Deployments so that Pods are spread in all the nodes of your cluster. # 详细可以参考：inter-pod affinity and anti-affinity # 1.4.3 设置Pod中断预算 # 当一个node被打上污点，pod是要被删除并且重新调度 # 但是如果你的系统压力很大，不能接受丢失50%的pod这时候改怎么办呢，驱逐pod可能会影响你的服务 # 为了保护部署免受可能同时摧毁多个Pod的意外事件的影响，可以定义Pod中断预算。 # 想象一下：“ Kubernetes，请确保我的应用始终至少有5个Pod在运行”。 # 如果最终状态导致该部署的Pod少于5个，Kubernetes将阻止耗尽事件。 # 官网参考文档：Pod Disruption Budgets. # 1.5 资源使用 # 为了最大化调度程序的效率，您应该与Kubernetes共享详细信息，例如资源利用，工作负载优先级和开销。 # 1.5.1 设置所有容器的内存限制和请求 # 资源限制用于限制容器可以使用多少CPU和内存，并使用containerSpec的resources属性设置。调度程序将这些用作度量标准之一，以确定哪个节点最适合当前Pod。根据调度程序，没有内存限制的容器的内存利用率为零。如果可调度在任何节点上的Pod数量不受限制，则会导致资源超额使用，并可能导致节点（和kubelet）崩溃。 # 同用的可以适用于CPU限制 # 但是，您是否应该始终设置内存和CPU的限制和要求？ # 如果您的进程超出内存限制，则该进程将终止。由于CPU是可压缩的资源，因此如果您的容器超出限制，则将限制该过程。 # 如果您想更深入地研究CPU和内存限制，则应查看以下文章： # Understanding resource limits in kubernetes: memory # Understanding resource limits in kubernetes: cpu time # 请注意，如果不确定什么是正确的CPU或内存限制，则可以在建议模式打开的情况下使用Kubernetes中的Vertical Pod Autoscaler。自动缩放器会分析您的应用并建议限制。 # 1.5.2 将CPU请求设置为1个CPU或以下 # 除非你有计算类型的实例类型job # it is recommended to set the request to 1 CPU or below. # 1.5.3 禁用CPU限制—除非您有很好的用例 # CPU以每个时间单位的CPU时间单位来度量。 # cpu：1表示每秒1个CPU秒。 # 如果您有1个线程，则每秒消耗的CPU时间不能超过1秒。 # 如果您有2个线程，则可以在0.5秒内消耗1个CPU秒。 # 8个线程可以在0.125秒内消耗1个CPU秒。之后，您的过程将受到限制。 # 如果不确定您的应用程序的最佳设置是什么，最好不要设置CPU限制。 # 深入研究可参考：this article digs deeper in CPU requests and limits. # 1.5.4 命名空间具有LimitRange # 如果您认为可能忘记设置内存和CPU限制，则应考虑使用LimitRange对象为当前名称空间中部署的容器定义标准大小。 # 可参考：The official documentation about LimitRange # 1.5.5 为Pod设置适当的服务质量（QoS） # 当一个节点进入过量使用状态（即使用过多资源）时，Kubernetes试图驱逐该节点中的某些Pod。Kubernetes根据定义明确的逻辑对Pod进行排名和逐出。 # 参考：configuring the quality of service for your Pods # 1.6 标记资源 # 1.6.1 资源已定义技术标签 # 你能通过以下tag标记pod # 名称，应用程序的名称，例如“用户API” # 实例，标识应用程序实例的唯一名称（您可以使用容器图像标签） # 版本，应用程序的当前版本（增量计数器） # 组件，架构中的组件，例如“ API”或“数据库” # 部分，该应用程序所属的更高级别应用程序的名称，例如“支付网关”由… # 管理，用于管理应用程序（例如“ kubectl”或“ Helm”）的操作的工具 # 标签可参考：recommended by the official documentation. # 建议不要标记所有资源。 # 1.6.2 资源已定义业务标签 # 您可以使用以下标签标记Pod： # owner：标示改资源的负责人 # project：声明资源所属的项目 # business-unit：用于标识与资源关联的成本中心或业务部门；通常用于成本分配和跟踪 # 1.6.3 资源定义安全等级标签 # tag pod通过以下label # confidentiality：资源支持的特定数据保密级别的标识符 # compliance：an identifier for workloads designed to adhere to specific compliance requirements # 1.7 日志 # 日志对于调试问题和监视应用程序活动特别有用。 # 1.7.1 应用程序记录到stdout和stderr # 有两种日志策略，主动方式与被动方式 # 使用被动方式日志记录的应用程序不需要了解了解日志记录基础结构，而是将消息记录到标准输出中。 # 主动方式，该应用程序与中间聚合器建立了网络连接，将数据发送到第三方日志记录服务，或直接写入数据库或索引。 # 最佳实践：the twelve-factor app. # 1.7.2 避免日志使用sidecars模式 # 如果希望将日志转换应用于具有非标准日志事件模型的应用程序，则可能需要使用sidecar容器。 # 使用Sidecar容器，您可以在将日志条目运送到其他地方之前对其进行规范化。 # 例如，您可能需要先将Apache日志转换为Logstash JSON格式，然后再将其发送到日志基础结构。 # 但是，如果您可以控制应用程序，则可以从一开始就输出正确的格式。 # sidecares启动需要时间，您可以节省为群集中的每个Pod运行额外的容器的时间。 # 1.8 伸缩 # 容器具有本地文件系统，您可能会想使用它来持久化数据。 # 但是，将持久性数据存储在容器的本地文件系统中会阻止包围的Pod进行水平缩放（即通过添加或删除Pod的副本）。 # 这是因为，通过使用本地文件系统，每个容器都维护自己的“状态”，这意味着Pod副本的状态可能会随时间而变化。从用户的角度来看，这会导致行为不一致（例如，当请求命中一个Pod时，特定的用户信息可用，但当请求命中另一个Pod时，则不可用）。 # 相反，任何持久性信息都应保存在Pod外部的中央位置。例如，在集群中的PersistentVolume中，或者在集群外部的某些存储服务中甚至更好。 # 1.8.1 容器在其本地文件系统中不存储任何状态 # 容器具有本地文件系统，您可能会想使用它来持久化数据。 # 但是，将持久性数据存储在容器的本地文件系统中会阻止包围的Pod进行水平缩放（即通过添加或删除Pod的副本）。 # 这是因为，通过使用本地文件系统，每个容器都维护自己的“状态”，这意味着Pod副本的状态可能会随时间而变化。从用户的角度来看，这会导致行为不一致（例如，当请求命中一个Pod时，特定的用户信息可用，但当请求命中另一个Pod时，则不可用）。 # 相反，任何持久性信息都应保存在Pod外部的中央位置。例如，在集群中的PersistentVolume中，或者在集群外部的某些存储服务中甚至更好。 # 1.8.2 对于可变使用模式的应用应该使用HPA # HPA是kubernetes内置的一个特性它能够监控当前的应用和根据当前的使用率自动添加及删除POD副本 # 通过配置HPA来保障你的应用在任何情况下包括（异常流量峰值）能够保存可用及正常响应 # 配置HPA自动伸缩你的应用，需要去创建一个HPA的资源对象，该对象定义了监控你应用的什么指标 # HPA也能够健康k8s内置的资源指标（POD的CPU/MEM资源使用率）或者自定义指标，对于自定义指标，你需要去收集和暴露这些指标，例如你可用使用Prometheus/Prometheus Adapter # 1.8.3 不要使用VPA，因为改特性还在测试中 # 当POD需要更多资源时，VPA能够通过自动的调整你的POD的资源请求和限制， # VPA非常适用于单体应用无法进行横向副本数的扩张 # 但是目前VPA仍然处于测试阶段，垂直方向调整POD资源需要重启POD # 考虑到这些限制，更多的应用在k8s中可用横行扩张，因此不要在生产环境中使用VPA # 1.8.4 如果有很高的工作负载，可以使用集群自动伸缩 # 集群伸缩是区别于（HPA/VPA）的另一种伸缩类型 # 集群自动伸缩能够通过增加或移除node节点来自动缩放集群的大小 # 当由于现有一个node节点的资源不足导致pod调度失败时，此刻集群会进行扩增操作，集群会增加一个work node来保证pod能够正常调度，相似的，如果一个worker node资源使用率低，那么集群自动伸缩会先驱逐这个worker node上面的pod，最终去移除此node # 对于集群工作负载很高的应用场景下，就去自动伸缩非常有用，集群自动缩分可以让你满足需求高峰，而不会通过过度陪你走工作节点来浪费资源。 # 对于工作负载不大的应用场景，可以不用去设置集群自动伸缩，因为可能永远都使用不到改规则，如果你的集群工作负载缓慢增长，可以通过监控系统来手动添加worker node # 1.9 配置和密钥 # 1.9.1 外部化所有配置 # 配置应该在应用之外的代码维护，这有一些好处 # 更改配置不用重新编译代码 # 当应用程序正在运行，配置文件可以单独被更新 # 相同的代码能够被用于不同的环境 # 在Kubernetes中，可以将配置保存在ConfigMaps中，然后可以在将卷作为环境变量传入时将其安装到容器中。 # 在ConfigMap中仅保存非敏感配置。对于敏感信息（例如凭据），请使用Secret资源。 # 1.9.2 将Secrets作为卷而不是环境变量 # Secret资源的内容应作为卷挂载到容器中，而不是作为环境变量传递。 # 这是为了防止秘密值出现在用于启动容器的命令中，该命令可能由不应该访问秘密值的人员检查 # 二 管治 # 2.1 名称空间限制 # 您不应允许用户使用比您事先同意的资源更多的资源。 # 群集管理员可以设置约束，以使用配额和限制范围限制项目中使用的对象数量或计算资源数量。 # 详细可参考；limit ranges # 2.1.1 名称空间限制范围 # 如果为设置容器资源消耗限制，那么会出现容器资源争抢导致其他容器异常状况发生， # k8s有两个特性来约束资源使用：ResourceQuota 和 LimitRange. # 使用LimitRange对象，您可以定义资源请求的默认值和名称空间内单个容器的限制。 # 在该命名空间内创建的任何容器（未明确指定请求和限制值）都将分配默认值。 # 2.1.2 名称控制资源配额 # 你能够在名称空间中通过资源配额限制所有容器资源的总消耗量 # 定义名称空间的资源配额会限制属于该名称空间的所有容器可以消耗的CPU，内存或存储资源的总量。 # 您还可以为其他Kubernetes对象设置配额，例如当前名称空间中的Pod数量。 # 如果您认为有人可以利用您的集群并创建20000 ConfigMap，则可以使用LimitRange来防止这种情况。 # 2.2 POD安全策略 # 遭到破坏的容器 # 容器使用节点上不允许的资源，例如进程，网络或文件系统 # 一般来说，应该将Pod的功能限制在最低限度。 # 2.2.1 启用POD安全策略 # 例如，您可以使用Kubernetes Pod安全策略来限制： # 访问主机进程或者网络名称空间 # 运行授权的容器 # 容器运行时的用户 # 访问主机的文件系统 # linux能力，Seccomp or SELinux profiles # 选择正确的策略依赖于集群原生的特性，可以参考：Kubernetes Pod Security Policy best practices # 2.2.2 禁用特权容器 # 在一个POD中，容器能够作为特权模式容器运行来不受限制访问主机系统的资源 # 通常这样是危险的，你应该给有必要的制定容器可以访问的等级， # 特权Pod的有效使用案例包括在节点上使用硬件，例如GPU。 # 可以参考：learn more about security contexts and privileges containers from this article. # 2.2.3 容器使用只读文件系统 # 容器中使用只读文件系统来强制保障容器的无状态话 # 这种方式不仅能减轻风险例如热补丁，而且可以避免避免恶意程序在容器内存储或者操作数据的风险。 # 使用只读文件系统听起来很简单，但是实际还是有一些复杂 # 如果你需要写日志或者存储一些临时文件在一个临时目录该怎么办呢，可以参考：running containers securely in production. # 2.2.4 避免利用root用户启动容器 # 在容器中运行一个进程与主机上运行一个进程没有太大区别，只不过一些很小的元数据在容器中声明 # 因此容器中的uid为0的用户与主机上的root用户相同 # 如果用户设法脱离了以root用户身份在容器中运行的应用程序，则他们可以使用同一root用户获得对主机的访问权限。 # 配置容器以使用非特权用户是防止特权升级攻击的最佳方法。 # 详细可以参考：article offers some detailed explanation examples of what happens when you run your containers as root. # 2.2.5 限制能力 # Linux功能使进程能够执行许多特权操作，其中只有root用户默认可以执行。 # 例如，CAP_CHOWN允许进程“对文件UID和GID进行任意更改”。 # 即使您的进程不是以root身份运行，进程也有可能通过提升特权来使用那些类似root的功能。 # 最佳实践： # Linux Capabilities: Why They Exist and How They Work # Linux Capabilities In Practice # 2.2.6 防止特权升级 # 您应该在关闭特权升级的情况下运行容器，以防止使用setuid或setgid二进制文件提升特权。 # 2.3 网络策略 # 容器可以在与其他容器进行网络通讯，不需要进行地址转换 # 集群中的node节点可以与其他节点进行网络通讯 # 一个容器的IP地址始终是相同的，从另一个容器或其本身看时是独立的。 # 如果您打算将群集分成较小的块并在名称空间之间进行隔离，则第一条规则无济于事。 # 想象一下您的集群中的用户是否能够使用集群中的任何其他服务。 # 现在，想象一下如果集群中的恶意用户要获得对集群的访问权限，他们可以向整个集群发出请求。 # 要解决此问题，您可以定义如何使用网络策略允许Pods在当前名称空间和跨名称空间中进行通信。 # 2.3.1 启用网络策略 # Kubernetes网络策略指定Pod组的访问权限，就像云中的安全组用于控制对VM实例的访问一样。 # 换句话说，它在Kubernetes集群上运行的Pod之间创建了防火墙。 # 可参考：Securing Kubernetes Cluster Networking. # 2.3.2 每个命名空间中都有一个保守的NetworkPolicy # 该存储库包含Kubernetes网络策略的各种用例和示例YAML文件，以在您的设置中利用。如果你想知道 # how to drop/restrict traffic to applications running on Kubernetes # 2.4 基于角色的访问控制RBAC # 放弃所需的最小权限是一种常见的做法，但是实际操作又如何量化最小权限？ # 细粒度的策略提供了更高的安全性，但是需要更多的精力来进行管理。 # 更大范围的授权可以使不必要的API访问服务帐户，但更易于控制。 # 2.4.1 禁用默认服务帐户的自动挂载 # 请注意，默认的ServiceAccount将自动安装到所有Pod的文件系统中。您可能要禁用它并提供更详细的策略。 # 可以参考：the default ServiceAccount is automatically mounted into the file system of all Pods. # 2.4.2 RBAC策略设置为所需的最少特权 # 寻找有关如何设置RBAC规则的好的建议是一项挑战。在Kubernetes RBAC的3种现实方法中，您可以找到三种实用场景和有关如何入门的实用建议。 # 参考；3 realistic approaches to Kubernetes RBAC # 2.4.3 RBAC策略是精细的，不能共享 # 需求：\n允许用户可以部署，但是不允许去读secrets # admin应该可以访问所有资源 # 默认情况下，应用程序不应获得对Kubernetes API的写访问权限 # 对于某些用途，应该可以写入Kubernetes API。 # 四个要求转化为五个单独的角色： # ReadOnly # PowerUser # Operator # Controller # Admin # 2.5 自定义策略 # 例如，您可能要避免从公共互联网下载容器，而希望首先批准这些容器。 # 也许您有一个内部注册表，并且只有此注册表中的映像可以部署在您的群集中。 # 您如何强制只能在群集中部署受信任的容器？没有针对此的RBAC政策。 # 2.5.1 仅允许从已知注册表部署容器 # 您可能要考虑的最常见的自定义策略之一是限制可以在群集中部署的映像。 # The following tutorial explains how you can use the Open Policy Agent to restrict not approved images. # 2.5.2 强制Ingress主机名唯一 # 用户创建Ingress清单时，可以使用其中的任何主机名。 # 但是，您可能希望阻止用户多次使用相同的主机名并互相覆盖。Open Policy Agent的官方文档包含有关如何在验证Webhook中检查Ingress资源的教程。 # a tutorial on how to check Ingress resources as part of the validation webhook. # 2.5.3 仅在Ingress主机名中使用批准的域名 # 用户创建Ingress清单时，可以使用其中的任何主机名。一种 # 但是，您可能希望阻止用户使用无效的主机名。Open Policy Agent的官方文档包含有关如何在验证Webhook中检查Ingress资源的教程。 # 三 集群配置 # 最好的选择是将群集与标准参考进行比较。 # 对于Kubernetes，参考是Internet安全中心（CIS）基准。 # 3.1 批准的Kubernetes配置 # 3.1.1 集群通过CIS基准测试 # Internet安全中心提供了一些准则和基准测试，以确保代码安全的最佳实践。 # 他们还维护了Kubernetes的基准，您可以download from the official website. # kube-bench是一种工具，用于自动执行CIS Kubernetes基准测试并报告集群中的错误配置。 # [INFO] 1 Master Node Security Configuration [INFO] 1.1 API Server [WARN] 1.1.1 Ensure that the --anonymous-auth argument is set to false (Not Scored) [PASS] 1.1.2 Ensure that the --basic-auth-file argument is not set (Scored) [PASS] 1.1.3 Ensure that the --insecure-allow-any-token argument is not set (Not Scored) [PASS] 1.1.4 Ensure that the --kubelet-https argument is set to true (Scored) [PASS] 1.1.5 Ensure that the --insecure-bind-address argument is not set (Scored) [PASS] 1.1.6 Ensure that the --insecure-port argument is set to 0 (Scored) [PASS] 1.1.7 Ensure that the --secure-port argument is not set to 0 (Scored) [FAIL] 1.1.8 Ensure that the --profiling argument is set to false (Scored) 请注意，无法使用kube-bench检查托管集群（例如GKE，EKS和AKS）的主节点。主节点由云提供商控制。\n3.1.2 禁用元数据云提供程序元数据API # 云平台（AWS，Azure，GCE等）通常将本地元数据服务公开给实例。 # 默认情况下，实例上运行的Pod可以访问这些API，并且可以包含该节点的云凭据或诸如kubelet凭据之类的置备数据。 # 这些凭据可用于在群集内升级或升级到同一帐户下的其他云服务。 # 3.1.3 限制对Alpha或Beta功能的访问 # Alpha和Beta Kubernetes功能正在积极开发中，并且可能会存在限制或错误，从而导致安全漏洞。 # 始终评估Alpha或Beta功能可能提供的价值，以防对您的安全状况造成潜在风险。 # 如有疑问，请禁用不使用的功能。 # 3.2 认证 # kubernetes提供了不同的认证策略 # Static Tokens:很难使之无效，应避免 # Bootstrap Tokens：与上面的静态令牌相同 # Basic Authentication：通过网络以明文形式传输凭据 # X509 client certs：需要定期更新和重新分发客户端证书 # Service Account Tokens：是集群中运行的应用程序和工作负载的首选身份验证策略 # OpenID Connect (OIDC) Tokens：OIDC与您的身份提供商（例如AD，AWS IAM，GCP IAM等）集成后，为最终用户提供了最佳的身份验证策略。 # 更多详细策略可参考：in the official documentation. # 3.2.1 使用OpenID（OIDC）令牌作为用户身份验证策略 # Kubernetes支持各种身份验证方法，包括OpenID Connect（OIDC）。 # OpenID Connect允许单点登录（SSO）（例如您的Google身份）连接到Kubernetes集群和其他开发工具。 # 你不用分别取记忆和管理认证，您可能有多个群集连接到同一OpenID提供程序。 # 详细可以参考：learn more about the OpenID connect in Kubernetes # 3.3 RBAC # 3.3.1 ServiceAccount令牌仅适用于应用程序和控制器 # 服务帐户令牌不应该用于尝试与Kubernetes群集进行交互的最终用户，但对于在Kubernetes上运行的应用程序和工作负载，它们是首选的身份验证策略。 # 3.4 日志设定 # 3.4.1 有一个日志保留和归档策略 # 您应该保留30-45天的历史日志。 # 3.4.2 从节点，控制平面，审核中收集日志 # 从那收集日志? # Nodes（kubelet，container runtime） # 控制平面（API server，scheduler，controller manager） # k8s 审计（对API服务器的所有请求） # 应该收集什么? # 应用名称 # 应用实例 # 应用版本 # 集群ID # 容器名称 # 容器运行在集群的那个node节点上 # 容器运行在那个pod内 # 名称空间 # 3.4.3 在每个节点上最好使用守护程序来收集日志，而不是在每个中运行sidecars # 应用程序应该登录到标准输出，而不是文件。 # 每个节点上的守护程序可以从容器运行时收集日志（如果记录到文件，则可能需要每个pod的sidecar容器）。 # 3.4.4 提供日志聚合工具 # 使用日志聚合工具，例如EFK堆栈（Elasticsearch，Fluentd，Kibana），DataDog，Sumo Logic，Sysdig，GCP Stackdriver，Azure Monitor，AWS CloudWatch。 # "},{"id":101,"href":"/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8B-deployment_controllergo%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-deploymentcontrollergo-yuan-ma-jie-du/","title":"K8S控制器之 deployment_controller.go源码解读 2024-04-09 11:30:03.133","section":"Docs","content":"/* Copyright 2015 The Kubernetes Authors. Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \u0026#34;AS IS\u0026#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */ // Package deployment contains all the logic for handling Kubernetes Deployments. // It implements a set of strategies (rolling, recreate) for deploying an application, // the means to rollback to previous versions, proportional scaling for mitigating // risk, cleanup policy, and other useful features of Deployments. package deployment import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;reflect\u0026#34; \u0026#34;time\u0026#34; apps \u0026#34;k8s.io/api/apps/v1\u0026#34; v1 \u0026#34;k8s.io/api/core/v1\u0026#34; \u0026#34;k8s.io/apimachinery/pkg/api/errors\u0026#34; metav1 \u0026#34;k8s.io/apimachinery/pkg/apis/meta/v1\u0026#34; \u0026#34;k8s.io/apimachinery/pkg/labels\u0026#34; \u0026#34;k8s.io/apimachinery/pkg/types\u0026#34; utilruntime \u0026#34;k8s.io/apimachinery/pkg/util/runtime\u0026#34; \u0026#34;k8s.io/apimachinery/pkg/util/wait\u0026#34; appsinformers \u0026#34;k8s.io/client-go/informers/apps/v1\u0026#34; coreinformers \u0026#34;k8s.io/client-go/informers/core/v1\u0026#34; clientset \u0026#34;k8s.io/client-go/kubernetes\u0026#34; \u0026#34;k8s.io/client-go/kubernetes/scheme\u0026#34; v1core \u0026#34;k8s.io/client-go/kubernetes/typed/core/v1\u0026#34; appslisters \u0026#34;k8s.io/client-go/listers/apps/v1\u0026#34; corelisters \u0026#34;k8s.io/client-go/listers/core/v1\u0026#34; \u0026#34;k8s.io/client-go/tools/cache\u0026#34; \u0026#34;k8s.io/client-go/tools/record\u0026#34; \u0026#34;k8s.io/client-go/util/workqueue\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/controller\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/controller/deployment/util\u0026#34; ) const ( // maxRetries is the number of times a deployment will be retried before it is dropped out of the queue. // With the current rate-limiter in use (5ms*2^(maxRetries-1)) the following numbers represent the times // a deployment is going to be requeued: // // 5ms, 10ms, 20ms, 40ms, 80ms, 160ms, 320ms, 640ms, 1.3s, 2.6s, 5.1s, 10.2s, 20.4s, 41s, 82s maxRetries = 15 ) // controllerKind contains the schema.GroupVersionKind for this controller type. var controllerKind = apps.SchemeGroupVersion.WithKind(\u0026#34;Deployment\u0026#34;) // DeploymentController is responsible for synchronizing Deployment objects stored // in the system with actual running replica sets and pods. type DeploymentController struct { // rsControl is used for adopting/releasing replica sets. rsControl controller.RSControlInterface client clientset.Interface eventBroadcaster record.EventBroadcaster eventRecorder record.EventRecorder // To allow injection of syncDeployment for testing. syncHandler func(ctx context.Context, dKey string) error // used for unit testing enqueueDeployment func(deployment *apps.Deployment) // dLister can list/get deployments from the shared informer\u0026#39;s store dLister appslisters.DeploymentLister // rsLister can list/get replica sets from the shared informer\u0026#39;s store rsLister appslisters.ReplicaSetLister // podLister can list/get pods from the shared informer\u0026#39;s store podLister corelisters.PodLister // dListerSynced returns true if the Deployment store has been synced at least once. // Added as a member to the struct to allow injection for testing. dListerSynced cache.InformerSynced // rsListerSynced returns true if the ReplicaSet store has been synced at least once. // Added as a member to the struct to allow injection for testing. rsListerSynced cache.InformerSynced // podListerSynced returns true if the pod store has been synced at least once. // Added as a member to the struct to allow injection for testing. podListerSynced cache.InformerSynced // Deployments that need to be synced queue workqueue.RateLimitingInterface } // NewDeploymentController creates a new DeploymentController. func NewDeploymentController(ctx context.Context, dInformer appsinformers.DeploymentInformer, rsInformer appsinformers.ReplicaSetInformer, podInformer coreinformers.PodInformer, client clientset.Interface) (*DeploymentController, error) { eventBroadcaster := record.NewBroadcaster(record.WithContext(ctx)) logger := klog.FromContext(ctx) dc := \u0026amp;DeploymentController{ client: client, eventBroadcaster: eventBroadcaster, eventRecorder: eventBroadcaster.NewRecorder(scheme.Scheme, v1.EventSource{Component: \u0026#34;deployment-controller\u0026#34;}), queue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), \u0026#34;deployment\u0026#34;), } dc.rsControl = controller.RealRSControl{ KubeClient: client, Recorder: dc.eventRecorder, } //该函数用于创建一个新的DeploymentController。 //- 接收上下文ctx以及多个informer和client作为参数。 //- 创建一个新的事件广播器eventBroadcaster，并从上下文ctx中获取日志记录器logger 。 //- 初始化一个DeploymentController结构体实例dc，包括client、eventBroadcaster、eventRecorder、queue和rsControl字段。 //- dc.rsControl使用controller.RealRSControl结构体初始化，包括KubeClient和Recorder字段。 //- 返回创建的dc实例和可能出现的错误。 dInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: func(obj interface{}) { dc.addDeployment(logger, obj) }, UpdateFunc: func(oldObj, newObj interface{}) { dc.updateDeployment(logger, oldObj, newObj) }, // This will enter the sync loop and no-op, because the deployment has been deleted from the store. DeleteFunc: func(obj interface{}) { dc.deleteDeployment(logger, obj) }, }) rsInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: func(obj interface{}) { dc.addReplicaSet(logger, obj) }, UpdateFunc: func(oldObj, newObj interface{}) { dc.updateReplicaSet(logger, oldObj, newObj) }, DeleteFunc: func(obj interface{}) { dc.deleteReplicaSet(logger, obj) }, }) podInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ DeleteFunc: func(obj interface{}) { dc.deletePod(logger, obj) }, }) //这段代码定义了三个事件处理器，分别用于处理deployment、replicaSet和pod的添加、更新和删除事件。 //每个事件处理器都调用了对应的方法，例如对于deployment的添加事件，调用了dc.addDeployment(logger, obj)方法。 //这些方法的具体实现可以根据具体业务逻辑进行编写。 dc.syncHandler = dc.syncDeployment dc.enqueueDeployment = dc.enqueue dc.dLister = dInformer.Lister() dc.rsLister = rsInformer.Lister() dc.podLister = podInformer.Lister() dc.dListerSynced = dInformer.Informer().HasSynced dc.rsListerSynced = rsInformer.Informer().HasSynced dc.podListerSynced = podInformer.Informer().HasSynced return dc, nil } //这段代码是Go语言中的函数，主要功能是设置和初始化一个名为dc的对象，并返回该对象和nil。 //- 首先，将dc.syncHandler设置为dc.syncDeployment，将dc.enqueueDeployment设置为dc.enqueue。 //- 然后，通过dInformer、rsInformer和podInformer的Lister()方法，分别将dc.dLister、dc.rsLister和dc.podLister设置为相应的列表器。 //- 接着，通过dInformer、rsInformer和podInformer的Informer().HasSynced方法，分别将dc.dListerSynced、dc.rsListerSynced和dc.podListerSynced设置为相应的同步状态检查函数。 //- 最后，返回设置好的dc对象和nil。 这段代码主要涉及到对象的设置和初始化操作，使用了多个列表器和同步状态检查函数来管理不同资源的信息。 // Run begins watching and syncing. func (dc *DeploymentController) Run(ctx context.Context, workers int) { defer utilruntime.HandleCrash() // Start events processing pipeline. dc.eventBroadcaster.StartStructuredLogging(3) dc.eventBroadcaster.StartRecordingToSink(\u0026amp;v1core.EventSinkImpl{Interface: dc.client.CoreV1().Events(\u0026#34;\u0026#34;)}) defer dc.eventBroadcaster.Shutdown() defer dc.queue.ShutDown() logger := klog.FromContext(ctx) logger.Info(\u0026#34;Starting controller\u0026#34;, \u0026#34;controller\u0026#34;, \u0026#34;deployment\u0026#34;) defer logger.Info(\u0026#34;Shutting down controller\u0026#34;, \u0026#34;controller\u0026#34;, \u0026#34;deployment\u0026#34;) if !cache.WaitForNamedCacheSync(\u0026#34;deployment\u0026#34;, ctx.Done(), dc.dListerSynced, dc.rsListerSynced, dc.podListerSynced) { return } for i := 0; i \u0026lt; workers; i++ { go wait.UntilWithContext(ctx, dc.worker, time.Second) } \u0026lt;-ctx.Done() } //该函数是DeploymentController类型的Run方法，用于启动部署控制器的监视和同步操作。 //- 首先，函数通过defer语句句柄处理崩溃情况。 //- 然后，启动事件处理管道，设置日志记录级别和目标。 //- 接着，通过defer语句关闭事件处理管道。 //- 然后，记录日志信息，表示控制器开始启动。 //- 接着，使用cache.WaitForNamedCacheSync函数等待缓存同步完成。 //- 然后，通过循环创建多个goroutine，并调用dc.worker函数进行工作。 //- 最后，等待上下文完成，并返回。 //该函数的主要功能是启动部署控制器，并使其开始监视和同步操作。 func (dc *DeploymentController) addDeployment(logger klog.Logger, obj interface{}) { d := obj.(*apps.Deployment) logger.V(4).Info(\u0026#34;Adding deployment\u0026#34;, \u0026#34;deployment\u0026#34;, klog.KObj(d)) dc.enqueueDeployment(d) } func (dc *DeploymentController) updateDeployment(logger klog.Logger, old, cur interface{}) { oldD := old.(*apps.Deployment) curD := cur.(*apps.Deployment) logger.V(4).Info(\u0026#34;Updating deployment\u0026#34;, \u0026#34;deployment\u0026#34;, klog.KObj(oldD)) dc.enqueueDeployment(curD) } //这两个函数都是DeploymentController的方法，用于处理Deployment的添加和更新事件。 //- addDeployment方法接收一个logger和一个obj参数，其中obj是通过类型断言转换为*apps.Deployment类型的。 //该方法首先使用logger记录添加deployment的日志信息，然后调用dc.enqueueDeployment方法将该deployment加入队列中，以便后续处理。 //- updateDeployment方法与addDeployment方法类似，但它接收的是旧的和新的Deployment对象。该方法使用logger记录更新deployment的日志信息， //并将新的Deployment对象加入队列中进行处理。 func (dc *DeploymentController) deleteDeployment(logger klog.Logger, obj interface{}) { d, ok := obj.(*apps.Deployment) if !ok { tombstone, ok := obj.(cache.DeletedFinalStateUnknown) if !ok { utilruntime.HandleError(fmt.Errorf(\u0026#34;couldn\u0026#39;t get object from tombstone %#v\u0026#34;, obj)) return } d, ok = tombstone.Obj.(*apps.Deployment) if !ok { utilruntime.HandleError(fmt.Errorf(\u0026#34;tombstone contained object that is not a Deployment %#v\u0026#34;, obj)) return } } logger.V(4).Info(\u0026#34;Deleting deployment\u0026#34;, \u0026#34;deployment\u0026#34;, klog.KObj(d)) dc.enqueueDeployment(d) } //该函数是一个Go语言函数，它定义了一个名为deleteDeployment的方法，该方法接受一个logger和一个obj参数，并没有任何返回值。 //该方法主要用于从一个部署（Deployment）中删除对象。 //首先，函数会尝试将obj参数断言为apps.Deployment类型，并检查断言是否成功。如果断言失败， //则会尝试将obj参数断言为cache.DeletedFinalStateUnknown类型。如果这个断言也失败了，函数会记录一个错误信息并返回。 //如果断言成功，则会尝试从tombstone中获取对象，并再次断言该对象是否为apps.Deployment类型。如果断言失败，则会记录一个错误信息并返回。 //如果成功断言出对象为apps.Deployment类型，则会使用logger记录一条信息，表示正在删除该部署， //并调用dc.enqueueDeployment方法将该部署加入队列，以便进一步处理。 // addReplicaSet enqueues the deployment that manages a ReplicaSet when the ReplicaSet is created. func (dc *DeploymentController) addReplicaSet(logger klog.Logger, obj interface{}) { rs := obj.(*apps.ReplicaSet) if rs.DeletionTimestamp != nil { // On a restart of the controller manager, it\u0026#39;s possible for an object to // show up in a state that is already pending deletion. dc.deleteReplicaSet(logger, rs) return } // If it has a ControllerRef, that\u0026#39;s all that matters. if controllerRef := metav1.GetControllerOf(rs); controllerRef != nil { d := dc.resolveControllerRef(rs.Namespace, controllerRef) if d == nil { return } logger.V(4).Info(\u0026#34;ReplicaSet added\u0026#34;, \u0026#34;replicaSet\u0026#34;, klog.KObj(rs)) dc.enqueueDeployment(d) return } //该函数是Go语言编写的，用于在创建ReplicaSet时，将管理该ReplicaSet的Deployment加入队列中。 //函数首先判断传入的obj对象是否为ReplicaSet类型，并通过判断ReplicaSet的DeletionTimestamp是否为空来确定是否需要删除该ReplicaSet。 //如果ReplicaSet有ControllerRef，则通过resolveControllerRef函数解析出对应的Deployment，并将其加入队列中。 // Otherwise, it\u0026#39;s an orphan. Get a list of all matching Deployments and sync // them to see if anyone wants to adopt it. ds := dc.getDeploymentsForReplicaSet(logger, rs) if len(ds) == 0 { return } logger.V(4).Info(\u0026#34;Orphan ReplicaSet added\u0026#34;, \u0026#34;replicaSet\u0026#34;, klog.KObj(rs)) for _, d := range ds { dc.enqueueDeployment(d) } } //该函数主要处理孤儿ReplicaSet的情况。 //它会获取与该ReplicaSet匹配的所有Deployment列表，并尝试同步这些Deployment，看是否有Deployment愿意采用该ReplicaSet。 //如果找到了对应的Deployment，则将其加入到队列中以便进一步处理。 // getDeploymentsForReplicaSet returns a list of Deployments that potentially // match a ReplicaSet. func (dc *DeploymentController) getDeploymentsForReplicaSet(logger klog.Logger, rs *apps.ReplicaSet) []*apps.Deployment { deployments, err := util.GetDeploymentsForReplicaSet(dc.dLister, rs) if err != nil || len(deployments) == 0 { return nil } // Because all ReplicaSet\u0026#39;s belonging to a deployment should have a unique label key, // there should never be more than one deployment returned by the above method. // If that happens we should probably dynamically repair the situation by ultimately // trying to clean up one of the controllers, for now we just return the older one if len(deployments) \u0026gt; 1 { // ControllerRef will ensure we don\u0026#39;t do anything crazy, but more than one // item in this list nevertheless constitutes user error. logger.V(4).Info(\u0026#34;user error! more than one deployment is selecting replica set\u0026#34;, \u0026#34;replicaSet\u0026#34;, klog.KObj(rs), \u0026#34;labels\u0026#34;, rs.Labels, \u0026#34;deployment\u0026#34;, klog.KObj(deployments[0])) } return deployments } //函数用于获取与给定ReplicaSet匹配的所有Deployment列表。 //它首先调用util.GetDeploymentsForReplicaSet函数来获取匹配的Deployment列表，如果出现错误或列表为空，则返回nil。 //如果获取的Deployment列表长度大于1，则记录错误日志，并返回列表中的第一个Deployment。 //在正常情况下，返回获取的Deployment列表。 // updateReplicaSet figures out what deployment(s) manage a ReplicaSet when the ReplicaSet // is updated and wake them up. If the anything of the ReplicaSets have changed, we need to // awaken both the old and new deployments. old and cur must be *apps.ReplicaSet // types. func (dc *DeploymentController) updateReplicaSet(logger klog.Logger, old, cur interface{}) { curRS := cur.(*apps.ReplicaSet) oldRS := old.(*apps.ReplicaSet) if curRS.ResourceVersion == oldRS.ResourceVersion { // Periodic resync will send update events for all known replica sets. // Two different versions of the same replica set will always have different RVs. return } //该函数用于在更新ReplicaSet时，确定由哪个部署管理该ReplicaSet，并唤醒它们。 //如果ReplicaSet的任何内容发生变化，需要唤醒旧的和新的部署。old和cur必须是指向apps.ReplicaSet类型的指针。 //函数首先将传入的old和cur参数转换为*apps.ReplicaSet类型。 //然后，它比较两个ReplicaSet的ResourceVersion字段。 //如果它们相等，则表示这是周期性同步发送的更新事件，而对于同一ReplicaSet的两个不同版本，它们的ResourceVersion总会有不同的值。 //在这种情况下，函数直接返回，不做任何处理。 curControllerRef := metav1.GetControllerOf(curRS) oldControllerRef := metav1.GetControllerOf(oldRS) controllerRefChanged := !reflect.DeepEqual(curControllerRef, oldControllerRef) if controllerRefChanged \u0026amp;\u0026amp; oldControllerRef != nil { // The ControllerRef was changed. Sync the old controller, if any. if d := dc.resolveControllerRef(oldRS.Namespace, oldControllerRef); d != nil { dc.enqueueDeployment(d) } } //这段Go代码主要关注于检查和处理两个ReplicaSet（curRS和oldRS）的ControllerRef是否发生变化，并根据变化情况同步旧的控制器。 //1. 首先，通过metav1.GetControllerOf函数获取curRS和oldRS的ControllerRef。 //2. 然后，使用reflect.DeepEqual函数比较curControllerRef和oldControllerRef是否相等。 //3. 如果两个ControllerRef不相等且oldControllerRef不为nil，则认为ControllerRef发生了变化。 //4. 当ControllerRef发生变化时，需要同步旧的控制器。通过dc.resolveControllerRef函数解析oldRS的命名空间和ControllerRef， //得到对应的Deployment。 //5. 最后，如果解析成功（d!=nil），则将该Deployment加入到调度队列中，以便进一步处理。 // If it has a ControllerRef, that\u0026#39;s all that matters. if curControllerRef != nil { d := dc.resolveControllerRef(curRS.Namespace, curControllerRef) if d == nil { return } logger.V(4).Info(\u0026#34;ReplicaSet updated\u0026#34;, \u0026#34;replicaSet\u0026#34;, klog.KObj(curRS)) dc.enqueueDeployment(d) return } //这个Go函数主要检查当前的curControllerRef是否为nil。 //如果不为nil，则通过dc.resolveControllerRef方法解析curControllerRef，并检查解析结果是否为nil。 //如果不为nil，则记录日志并使用dc.enqueueDeployment方法将解析结果加入到队列中。 // Otherwise, it\u0026#39;s an orphan. If anything changed, sync matching controllers // to see if anyone wants to adopt it now. labelChanged := !reflect.DeepEqual(curRS.Labels, oldRS.Labels) if labelChanged || controllerRefChanged { ds := dc.getDeploymentsForReplicaSet(logger, curRS) if len(ds) == 0 { return } logger.V(4).Info(\u0026#34;Orphan ReplicaSet updated\u0026#34;, \u0026#34;replicaSet\u0026#34;, klog.KObj(curRS)) for _, d := range ds { dc.enqueueDeployment(d) } } } //这段Go代码是处理孤儿ReplicaSet的逻辑。 //如果ReplicaSet的标签或控制器引用发生了变化，它会尝试同步匹配的控制器，以查看是否有控制器愿意现在采用它。具体步骤如下： //1. 检查当前ReplicaSet的标签和旧ReplicaSet的标签是否相等，如果不相等，则标记标签发生变化； //2. 检查控制器引用是否发生变化； //3. 如果标签发生变化或控制器引用发生变化，则获取当前ReplicaSet对应的部署列表； //4. 如果部署列表为空，则直接返回； //5. 输出日志信息，表示孤儿ReplicaSet已更新； //6. 遍历部署列表，将每个部署对象加入到队列中，以便进一步处理。 // deleteReplicaSet enqueues the deployment that manages a ReplicaSet when // the ReplicaSet is deleted. obj could be an *apps.ReplicaSet, or // a DeletionFinalStateUnknown marker item. func (dc *DeploymentController) deleteReplicaSet(logger klog.Logger, obj interface{}) { rs, ok := obj.(*apps.ReplicaSet) //该函数是Go语言编写的，属于DeploymentController类型的一个方法，方法名为deleteReplicaSet。 //该方法接收一个logger和一个obj参数，其中logger用于记录日志，obj是一个接口类型， //可以是*apps.ReplicaSet类型或者DeletionFinalStateUnknown标记项。 //方法的主要功能是从obj中解析出*apps.ReplicaSet类型的rs，并判断解析是否成功。 // When a delete is dropped, the relist will notice a pod in the store not // in the list, leading to the insertion of a tombstone object which contains // the deleted key/value. Note that this value might be stale. If the ReplicaSet // changed labels the new deployment will not be woken up till the periodic resync. if !ok { tombstone, ok := obj.(cache.DeletedFinalStateUnknown) if !ok { utilruntime.HandleError(fmt.Errorf(\u0026#34;couldn\u0026#39;t get object from tombstone %#v\u0026#34;, obj)) return } rs, ok = tombstone.Obj.(*apps.ReplicaSet) if !ok { utilruntime.HandleError(fmt.Errorf(\u0026#34;tombstone contained object that is not a ReplicaSet %#v\u0026#34;, obj)) return } } //这段Go代码是处理从存储中获取对象时，如果对象已被删除的情况。 //如果获取对象失败，会检查对象是否是一个 tombstone 对象（即已被删除的对象）。 //如果是 tombstone 对象，则尝试从 tombstone 中恢复被删除的对象。如果恢复失败，则记录错误信息。 controllerRef := metav1.GetControllerOf(rs) if controllerRef == nil { // No controller should care about orphans being deleted. return } d := dc.resolveControllerRef(rs.Namespace, controllerRef) if d == nil { return } logger.V(4).Info(\u0026#34;ReplicaSet deleted\u0026#34;, \u0026#34;replicaSet\u0026#34;, klog.KObj(rs)) dc.enqueueDeployment(d) } //该函数主要实现当一个ReplicaSet被删除时，检查是否有对应的Deployment控制器，并将该Deployment加入到队列中以便进一步处理。具体流程如下： //1. 通过metav1.GetControllerOf(rs)获取ReplicaSet的控制器引用； //2. 如果控制器引用为空，则表示没有对应的Deployment控制器，直接返回； //3. 调用dc.resolveControllerRef(rs.Namespace, controllerRef)解析控制器引用，获取对应的Deployment对象； //4. 如果解析失败或返回的Deployment对象为空，则直接返回； //5. 使用logger.V(4).Info记录日志，表示ReplicaSet已被删除； //6. 调用dc.enqueueDeployment(d)将对应的Deployment对象加入到队列中，以便进一步处理。 // deletePod will enqueue a Recreate Deployment once all of its pods have stopped running. func (dc *DeploymentController) deletePod(logger klog.Logger, obj interface{}) { pod, ok := obj.(*v1.Pod) //该函数是一个Go语言函数，名为deletePod，它属于DeploymentController类型。 //函数通过传入的logger和obj参数，将一个Recreate Deployment加入队列，但只有当该Deployment的所有Pod都停止运行时才会执行。 //函数首先尝试将obj参数断言为*v1.Pod类型，并检查断言是否成功。 // When a delete is dropped, the relist will notice a pod in the store not // in the list, leading to the insertion of a tombstone object which contains // the deleted key/value. Note that this value might be stale. If the Pod // changed labels the new deployment will not be woken up till the periodic resync. if !ok { tombstone, ok := obj.(cache.DeletedFinalStateUnknown) if !ok { utilruntime.HandleError(fmt.Errorf(\u0026#34;couldn\u0026#39;t get object from tombstone %#v\u0026#34;, obj)) return } pod, ok = tombstone.Obj.(*v1.Pod) if !ok { utilruntime.HandleError(fmt.Errorf(\u0026#34;tombstone contained object that is not a pod %#v\u0026#34;, obj)) return } } d := dc.getDeploymentForPod(logger, pod) if d == nil { return } logger.V(4).Info(\u0026#34;Pod deleted\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod)) if d.Spec.Strategy.Type == apps.RecreateDeploymentStrategyType { // Sync if this Deployment now has no more Pods. rsList, err := util.ListReplicaSets(d, util.RsListFromClient(dc.client.AppsV1())) if err != nil { return } podMap, err := dc.getPodMapForDeployment(d, rsList) if err != nil { return } numPods := 0 for _, podList := range podMap { numPods += len(podList) } if numPods == 0 { dc.enqueueDeployment(d) } } } //这段Go代码是用于处理Kubernetes中Pod删除事件的逻辑。 //- 当一个Pod被删除时，如果控制器（例如Deployment）没有及时收到该事件，就会在存储中注意到这个Pod不在列表中， //从而插入一个包含删除的键值对的墓碑对象（tombstone object）。 //- 如果墓碑对象中的对象不是Pod类型，函数会记录错误并返回。 //- 函数会尝试获取与该Pod关联的Deployment对象，如果获取不到则直接返回。 //- 如果该Deployment的策略是Recreate类型，则会检查该Deployment下是否已经没有Pod了。 //- 如果没有Pod了，则将该Deployment加入到队列中，以便进一步处理。 //这段代码的主要目的是确保在Pod被删除时，与之关联的Deployment能够及时地进行同步和更新。 func (dc *DeploymentController) enqueue(deployment *apps.Deployment) { key, err := controller.KeyFunc(deployment) if err != nil { utilruntime.HandleError(fmt.Errorf(\u0026#34;couldn\u0026#39;t get key for object %#v: %v\u0026#34;, deployment, err)) return } dc.queue.Add(key) } //该函数是一个Go语言的方法，定义在一个名为DeploymentController的结构体类型上。 //方法名为enqueue，它接收一个参数deployment，类型为*apps.Deployment，表示一个Kubernetes部署对象的指针。 //该方法的主要功能是将给定的部署对象加入到一个队列中，以便后续处理。具体步骤如下： //1. 调用controller.KeyFunc(deployment)函数，尝试获取部署对象的键值（通常是一个字符串），用于在队列中唯一标识该对象。 //2. 如果获取键值时出现错误，通过utilruntime.HandleError函数处理错误，并打印错误信息。然后直接返回，不将对象加入队列。 //3. 如果成功获取了键值，将其添加到dc.queue（一个队列对象）中，以便后续处理。 //总结：该方法用于将一个Kubernetes部署对象加入到队列中，以便后续进行处理。 func (dc *DeploymentController) enqueueRateLimited(deployment *apps.Deployment) { key, err := controller.KeyFunc(deployment) if err != nil { utilruntime.HandleError(fmt.Errorf(\u0026#34;couldn\u0026#39;t get key for object %#v: %v\u0026#34;, deployment, err)) return } dc.queue.AddRateLimited(key) } //该函数用于将指定的部署对象加入到队列中，并对其进行速率限制。 //首先，通过调用controller.KeyFunc方法获取该部署对象的键值，如果获取失败，则通过utilruntime.HandleError方法记录错误信息并返回。 //如果获取成功，则调用dc.queue.AddRateLimited方法将键值加入到队列中，并对其进行速率限制。 // enqueueAfter will enqueue a deployment after the provided amount of time. func (dc *DeploymentController) enqueueAfter(deployment *apps.Deployment, after time.Duration) { key, err := controller.KeyFunc(deployment) if err != nil { utilruntime.HandleError(fmt.Errorf(\u0026#34;couldn\u0026#39;t get key for object %#v: %v\u0026#34;, deployment, err)) return } dc.queue.AddAfter(key, after) } //该函数将一个部署对象加入到队列中，但会在指定的时间延迟后才执行。 //首先，函数通过调用controller.KeyFunc方法获取部署对象的键值，如果获取失败，则通过utilruntime.HandleError方法记录错误信息并返回。 //接着，函数使用dc.queue.AddAfter方法将键值加入到队列中，并指定延迟执行的时间。 // getDeploymentForPod returns the deployment managing the given Pod. func (dc *DeploymentController) getDeploymentForPod(logger klog.Logger, pod *v1.Pod) *apps.Deployment { // Find the owning replica set var rs *apps.ReplicaSet var err error controllerRef := metav1.GetControllerOf(pod) if controllerRef == nil { // No controller owns this Pod. return nil } if controllerRef.Kind != apps.SchemeGroupVersion.WithKind(\u0026#34;ReplicaSet\u0026#34;).Kind { // Not a pod owned by a replica set. return nil } rs, err = dc.rsLister.ReplicaSets(pod.Namespace).Get(controllerRef.Name) if err != nil || rs.UID != controllerRef.UID { logger.V(4).Info(\u0026#34;Cannot get replicaset for pod\u0026#34;, \u0026#34;ownerReference\u0026#34;, controllerRef.Name, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;err\u0026#34;, err) return nil } //该函数用于获取管理给定Pod的Deployment。 //- 首先，它查找拥有该Pod的ReplicaSet。 //- 如果找不到拥有者的Pod，则返回nil。 - //如果Pod的拥有者不是ReplicaSet，则返回nil。 //- 然后，尝试根据Pod的拥有者名称获取ReplicaSet。 //- 如果获取失败或获取到的ReplicaSet的UID与拥有者的UID不匹配，则返回nil。 //- 最后，返回获取到的Deployment。 // Now find the Deployment that owns that ReplicaSet. controllerRef = metav1.GetControllerOf(rs) if controllerRef == nil { return nil } return dc.resolveControllerRef(rs.Namespace, controllerRef) } //这个函数的作用是通过 ReplicaSet 的 controllerRef 找到对应的 Deployment。 //首先通过 metav1.GetControllerOf(rs) 获取到 controllerRef， //如果 controllerRef 为空则返回 nil，否则调用 dc.resolveControllerRef(rs.Namespace, controllerRef) 解析并返回对应的 Deployment。 // resolveControllerRef returns the controller referenced by a ControllerRef, // or nil if the ControllerRef could not be resolved to a matching controller // of the correct Kind. func (dc *DeploymentController) resolveControllerRef(namespace string, controllerRef *metav1.OwnerReference) *apps.Deployment { // We can\u0026#39;t look up by UID, so look up by Name and then verify UID. // Don\u0026#39;t even try to look up by Name if it\u0026#39;s the wrong Kind. if controllerRef.Kind != controllerKind.Kind { return nil } d, err := dc.dLister.Deployments(namespace).Get(controllerRef.Name) if err != nil { return nil } if d.UID != controllerRef.UID { // The controller we found with this Name is not the same one that the // ControllerRef points to. return nil } return d } //该函数是一个Go语言函数，用于解析一个ControllerRef引用所指向的控制器，如果无法解析出正确的控制器，则返回nil。 //函数接受三个参数： - namespace：字符串类型，表示命名空间。 //- controllerRef：指向metav1.OwnerReference类型的指针。 //函数返回一个指向apps.Deployment类型的指针。 //函数的主要步骤如下： //1. 首先，检查controllerRef的Kind属性是否与controllerKind.Kind相等，如果不相等，则直接返回nil。 //2. 如果controllerRef的Kind属性与controllerKind.Kind相等，则通过dc.dLister.Deployments(namespace).Get(controllerRef.Name)获取具有相同名称的部署对象。 //3. 如果获取部署对象时出现错误，则返回nil。 //4. 最后，检查获取到的部署对象的UID属性是否与controllerRef的UID属性相等，如果不相等，则返回nil，否则返回该部署对象。 // worker runs a worker thread that just dequeues items, processes them, and marks them done. // It enforces that the syncHandler is never invoked concurrently with the same key. func (dc *DeploymentController) worker(ctx context.Context) { for dc.processNextWorkItem(ctx) { } } func (dc *DeploymentController) processNextWorkItem(ctx context.Context) bool { key, quit := dc.queue.Get() if quit { return false } defer dc.queue.Done(key) err := dc.syncHandler(ctx, key.(string)) dc.handleErr(ctx, err, key) return true } //这段Go代码定义了两个函数：worker 和 processNextWorkItem，它们用于在DeploymentController中处理工作队列中的项。 //1. worker函数是一个无限循环，它不断地调用processNextWorkItem函数来处理队列中的下一个工作项，直到没有更多工作项需要处理为止。 //该函数接受一个context.Context参数，用于控制函数的取消或超时。 //2. processNextWorkItem函数从队列中获取下一个工作项的键，并调用syncHandler函数来处理该工作项。 //如果处理成功，它会标记该工作项为已完成。 //该函数也接受一个context.Context参数，并在处理完成后对其进行取消操作。 //该函数返回一个布尔值，表示是否成功处理了工作项。 //这段代码的主要目的是在DeploymentController中使用工作队列并发地处理工作项，并确保相同的键不会被并发处理。 func (dc *DeploymentController) handleErr(ctx context.Context, err error, key interface{}) { logger := klog.FromContext(ctx) if err == nil || errors.HasStatusCause(err, v1.NamespaceTerminatingCause) { dc.queue.Forget(key) return } ns, name, keyErr := cache.SplitMetaNamespaceKey(key.(string)) if keyErr != nil { logger.Error(err, \u0026#34;Failed to split meta namespace cache key\u0026#34;, \u0026#34;cacheKey\u0026#34;, key) } if dc.queue.NumRequeues(key) \u0026lt; maxRetries { logger.V(2).Info(\u0026#34;Error syncing deployment\u0026#34;, \u0026#34;deployment\u0026#34;, klog.KRef(ns, name), \u0026#34;err\u0026#34;, err) dc.queue.AddRateLimited(key) return } utilruntime.HandleError(err) logger.V(2).Info(\u0026#34;Dropping deployment out of the queue\u0026#34;, \u0026#34;deployment\u0026#34;, klog.KRef(ns, name), \u0026#34;err\u0026#34;, err) dc.queue.Forget(key) } //该函数是Go语言中的一个处理错误的函数，它属于DeploymentController类型。 //函数通过传入的上下文、错误和键来执行错误处理逻辑。 //首先，函数从上下文中获取日志记录器。如果错误为nil或错误的原因是命名空间终止，则将键从队列中忘记并返回。 //接下来，函数尝试将键拆分为命名空间和名称，并检查拆分是否成功。如果拆分失败，则记录错误信息。 //如果队列中键的重试次数小于最大重试次数，则记录错误信息，并将键添加到速率限制队列中。 //如果以上条件都不满足，则处理错误，并记录信息，将键从队列中忘记。 // getReplicaSetsForDeployment uses ControllerRefManager to reconcile // ControllerRef by adopting and orphaning. // It returns the list of ReplicaSets that this Deployment should manage. func (dc *DeploymentController) getReplicaSetsForDeployment(ctx context.Context, d *apps.Deployment) ([]*apps.ReplicaSet, error) { // List all ReplicaSets to find those we own but that no longer match our // selector. They will be orphaned by ClaimReplicaSets(). rsList, err := dc.rsLister.ReplicaSets(d.Namespace).List(labels.Everything()) if err != nil { return nil, err } deploymentSelector, err := metav1.LabelSelectorAsSelector(d.Spec.Selector) if err != nil { return nil, fmt.Errorf(\u0026#34;deployment %s/%s has invalid label selector: %v\u0026#34;, d.Namespace, d.Name, err) } // If any adoptions are attempted, we should first recheck for deletion with // an uncached quorum read sometime after listing ReplicaSets (see #42639). canAdoptFunc := controller.RecheckDeletionTimestamp(func(ctx context.Context) (metav1.Object, error) { fresh, err := dc.client.AppsV1().Deployments(d.Namespace).Get(ctx, d.Name, metav1.GetOptions{}) if err != nil { return nil, err } if fresh.UID != d.UID { return nil, fmt.Errorf(\u0026#34;original Deployment %v/%v is gone: got uid %v, wanted %v\u0026#34;, d.Namespace, d.Name, fresh.UID, d.UID) } return fresh, nil }) cm := controller.NewReplicaSetControllerRefManager(dc.rsControl, d, deploymentSelector, controllerKind, canAdoptFunc) return cm.ClaimReplicaSets(ctx, rsList) } //该函数用于通过ControllerRefManager来协调ControllerRef，实现采用和孤儿化操作。 //它返回一个列表，其中包含该Deployment应管理的ReplicaSets。 函数首先列出所有ReplicaSets，以找到我们拥有但不再匹配我们选择器的那些。 //它们将通过ClaimReplicaSets()函数被孤儿化。 //然后，函数将根据Deployment的规范选择器生成一个标签选择器。如果任何收养尝试都进行了， //函数将首先在列出ReplicaSets后重新检查删除时间戳（请参阅#42639）。 //最后，函数创建一个ReplicaSetControllerRefManager，并使用ClaimReplicaSets函数来声明应管理的ReplicaSets列表 // getPodMapForDeployment returns the Pods managed by a Deployment. // // It returns a map from ReplicaSet UID to a list of Pods controlled by that RS, // according to the Pod\u0026#39;s ControllerRef. // NOTE: The pod pointers returned by this method point the pod objects in the cache and thus // shouldn\u0026#39;t be modified in any way. func (dc *DeploymentController) getPodMapForDeployment(d *apps.Deployment, rsList []*apps.ReplicaSet) (map[types.UID][]*v1.Pod, error) { // Get all Pods that potentially belong to this Deployment. selector, err := metav1.LabelSelectorAsSelector(d.Spec.Selector) if err != nil { return nil, err } pods, err := dc.podLister.Pods(d.Namespace).List(selector) if err != nil { return nil, err } // Group Pods by their controller (if it\u0026#39;s in rsList). podMap := make(map[types.UID][]*v1.Pod, len(rsList)) for _, rs := range rsList { podMap[rs.UID] = []*v1.Pod{} } for _, pod := range pods { // Do not ignore inactive Pods because Recreate Deployments need to verify that no // Pods from older versions are running before spinning up new Pods. controllerRef := metav1.GetControllerOf(pod) if controllerRef == nil { continue } // Only append if we care about this UID. if _, ok := podMap[controllerRef.UID]; ok { podMap[controllerRef.UID] = append(podMap[controllerRef.UID], pod) } } return podMap, nil } //该函数用于返回由Deployment管理的Pods的映射。 //它根据Pod的ControllerRef将Pod分组为其控制器（如果在rsList中）。 //函数首先根据Deployment的规范选择器获取所有可能属于该Deployment的Pods 。 //然后，它遍历这些Pods，并通过其ControllerRef将它们分组到podMap中。 //函数返回一个映射，其中键是ReplicaSet的UID，值是由该RS控制的Pod列表。 //注意，该函数返回的Pod指针指向缓存中的Pod对象，因此不应以任何方式修改它们。 // syncDeployment will sync the deployment with the given key. // This function is not meant to be invoked concurrently with the same key. func (dc *DeploymentController) syncDeployment(ctx context.Context, key string) error { logger := klog.FromContext(ctx) namespace, name, err := cache.SplitMetaNamespaceKey(key) if err != nil { logger.Error(err, \u0026#34;Failed to split meta namespace cache key\u0026#34;, \u0026#34;cacheKey\u0026#34;, key) return err } //该函数是一个Go语言函数，名为syncDeployment，属于DeploymentController结构体。 //它接收一个ctx context.Context参数和一个key string参数，并返回一个error类型值。函数主要用于同步指定键值的部署信息。 //函数首先从上下文中获取日志记录器，然后使用cache.SplitMetaNamespaceKey函数将键值拆分为命名空间和名称。 //如果拆分过程中出现错误，则记录错误日志并返回该错误。 startTime := time.Now() logger.V(4).Info(\u0026#34;Started syncing deployment\u0026#34;, \u0026#34;deployment\u0026#34;, klog.KRef(namespace, name), \u0026#34;startTime\u0026#34;, startTime) defer func() { logger.V(4).Info(\u0026#34;Finished syncing deployment\u0026#34;, \u0026#34;deployment\u0026#34;, klog.KRef(namespace, name), \u0026#34;duration\u0026#34;, time.Since(startTime)) }() //这段Go代码主要实现了在开始和结束同步部署时记录日志的功能。 //- 首先，通过time.Now()获取当前时间作为开始时间，并使用logger.V(4).Info记录开始同步部署的日志，其中包含了部署的名称、命名空间和开始时间。 //- 然后，使用defer关键字定义了一个匿名函数，在函数执行结束后会自动执行该函数。 //该匿名函数使用logger.V(4).Info记录结束同步部署的日志，其中包含了部署的名称、命名空间和同步部署所花费的时间。 //通过这种方式，可以在日志中方便地查看部署的同步状态和耗时。 deployment, err := dc.dLister.Deployments(namespace).Get(name) if errors.IsNotFound(err) { logger.V(2).Info(\u0026#34;Deployment has been deleted\u0026#34;, \u0026#34;deployment\u0026#34;, klog.KRef(namespace, name)) return nil } if err != nil { return err } //该函数通过调用dc.dLister.Deployments(namespace).Get(name)获取指定命名空间中名为name的部署对象。 //如果该部署对象不存在，则记录日志并返回nil；如果存在其他错误，则直接返回错误。 // Deep-copy otherwise we are mutating our cache. // TODO: Deep-copy only when needed. d := deployment.DeepCopy() everything := metav1.LabelSelector{} if reflect.DeepEqual(d.Spec.Selector, \u0026amp;everything) { dc.eventRecorder.Eventf(d, v1.EventTypeWarning, \u0026#34;SelectingAll\u0026#34;, \u0026#34;This deployment is selecting all pods. A non-empty selector is required.\u0026#34;) if d.Status.ObservedGeneration \u0026lt; d.Generation { d.Status.ObservedGeneration = d.Generation dc.client.AppsV1().Deployments(d.Namespace).UpdateStatus(ctx, d, metav1.UpdateOptions{}) } return nil } //这段Go代码中的函数主要功能是对一个deployment对象进行深拷贝，并检查其Selector是否等于一个空的LabelSelector对象。 //如果是，则记录一条警告事件并更新deployment的状态。 // List ReplicaSets owned by this Deployment, while reconciling ControllerRef // through adoption/orphaning. rsList, err := dc.getReplicaSetsForDeployment(ctx, d) if err != nil { return err } // List all Pods owned by this Deployment, grouped by their ReplicaSet. // Current uses of the podMap are: // // * check if a Pod is labeled correctly with the pod-template-hash label. // * check that no old Pods are running in the middle of Recreate Deployments. podMap, err := dc.getPodMapForDeployment(d, rsList) if err != nil { return err } if d.DeletionTimestamp != nil { return dc.syncStatusOnly(ctx, d, rsList) } // Update deployment conditions with an Unknown condition when pausing/resuming // a deployment. In this way, we can be sure that we won\u0026#39;t timeout when a user // resumes a Deployment with a set progressDeadlineSeconds. if err = dc.checkPausedConditions(ctx, d); err != nil { return err } if d.Spec.Paused { return dc.sync(ctx, d, rsList) } // rollback is not re-entrant in case the underlying replica sets are updated with a new // revision so we should ensure that we won\u0026#39;t proceed to update replica sets until we // make sure that the deployment has cleaned up its rollback spec in subsequent enqueues. if getRollbackTo(d) != nil { return dc.rollback(ctx, d, rsList) } scalingEvent, err := dc.isScalingEvent(ctx, d, rsList) if err != nil { return err } if scalingEvent { return dc.sync(ctx, d, rsList) } switch d.Spec.Strategy.Type { case apps.RecreateDeploymentStrategyType: return dc.rolloutRecreate(ctx, d, rsList, podMap) case apps.RollingUpdateDeploymentStrategyType: return dc.rolloutRolling(ctx, d, rsList) } return fmt.Errorf(\u0026#34;unexpected deployment strategy type: %s\u0026#34;, d.Spec.Strategy.Type) } //该函数主要负责处理Deployment的更新和同步逻辑。 //1. 首先，函数会获取该Deployment所拥有的ReplicaSet列表和Pod的映射关系。 //2. 如果该Deployment已被删除，则只同步状态。 //3. 检查是否暂停，若暂停则只进行同步操作。 //4. 如果需要回滚，则执行回滚操作。 //5. 检测是否为缩放事件，若是则进行同步操作。 //6. 根据Deployment的策略类型（Recreate或RollingUpdate），执行相应的更新操作。 //7. 如果遇到意外的部署策略类型，返回错误。 "},{"id":102,"href":"/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8B-progressgo-%E8%BF%9B%E5%BA%A6-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-progressgo-jin-du-yuan-ma-jie-du/","title":"K8S控制器之 progress.go 进度 源码解读 2024-04-09 11:31:08.86","section":"Docs","content":"/* Copyright 2016 The Kubernetes Authors. Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \u0026#34;AS IS\u0026#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */ package deployment import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;reflect\u0026#34; \u0026#34;time\u0026#34; apps \u0026#34;k8s.io/api/apps/v1\u0026#34; \u0026#34;k8s.io/api/core/v1\u0026#34; metav1 \u0026#34;k8s.io/apimachinery/pkg/apis/meta/v1\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/controller/deployment/util\u0026#34; ) // syncRolloutStatus updates the status of a deployment during a rollout. There are // cases this helper will run that cannot be prevented from the scaling detection, // for example a resync of the deployment after it was scaled up. In those cases, // we shouldn\u0026#39;t try to estimate any progress. func (dc *DeploymentController) syncRolloutStatus(ctx context.Context, allRSs []*apps.ReplicaSet, newRS *apps.ReplicaSet, d *apps.Deployment) error { newStatus := calculateStatus(allRSs, newRS, d) // If there is no progressDeadlineSeconds set, remove any Progressing condition. if !util.HasProgressDeadline(d) { util.RemoveDeploymentCondition(\u0026amp;newStatus, apps.DeploymentProgressing) } //该函数用于更新部署期间的部署状态。在某些情况下，无法从缩放检测中防止此帮助程序运行，例如在部署缩放后进行同步。 //在这些情况下，不应尝试估计任何进度。 //函数首先计算所有复制集、新复制集和部署的状态，然后如果未设置进度截止时间，则删除任何正在进行的条件。 // If there is only one replica set that is active then that means we are not running // a new rollout and this is a resync where we don\u0026#39;t need to estimate any progress. // In such a case, we should simply not estimate any progress for this deployment. currentCond := util.GetDeploymentCondition(d.Status, apps.DeploymentProgressing) isCompleteDeployment := newStatus.Replicas == newStatus.UpdatedReplicas \u0026amp;\u0026amp; currentCond != nil \u0026amp;\u0026amp; currentCond.Reason == util.NewRSAvailableReason // Check for progress only if there is a progress deadline set and the latest rollout // hasn\u0026#39;t completed yet. if util.HasProgressDeadline(d) \u0026amp;\u0026amp; !isCompleteDeployment { switch { case util.DeploymentComplete(d, \u0026amp;newStatus): // Update the deployment conditions with a message for the new replica set that // was successfully deployed. If the condition already exists, we ignore this update. msg := fmt.Sprintf(\u0026#34;Deployment %q has successfully progressed.\u0026#34;, d.Name) if newRS != nil { msg = fmt.Sprintf(\u0026#34;ReplicaSet %q has successfully progressed.\u0026#34;, newRS.Name) } condition := util.NewDeploymentCondition(apps.DeploymentProgressing, v1.ConditionTrue, util.NewRSAvailableReason, msg) util.SetDeploymentCondition(\u0026amp;newStatus, *condition) case util.DeploymentProgressing(d, \u0026amp;newStatus): // If there is any progress made, continue by not checking if the deployment failed. This // behavior emulates the rolling updater progressDeadline check. msg := fmt.Sprintf(\u0026#34;Deployment %q is progressing.\u0026#34;, d.Name) if newRS != nil { msg = fmt.Sprintf(\u0026#34;ReplicaSet %q is progressing.\u0026#34;, newRS.Name) } condition := util.NewDeploymentCondition(apps.DeploymentProgressing, v1.ConditionTrue, util.ReplicaSetUpdatedReason, msg) //这段Go代码是用于检查和更新Kubernetes部署（Deployment）的进度状态的。具体来说，它根据以下条件来判断是否需要更新部署的进度状态： //1. 如果当前只有一个活动的副本集（replica set），则认为这不是一个新的滚动发布（rollout），而是一个重新同步（resync）， //在这种情况下，不需要估计任何进度，因此直接不估计该部署的进度。 //2. 如果设置了进度截止时间（progress deadline）并且最新滚动发布尚未完成，则检查部署的进度。 //- 如果部署已完成，则更新部署状态，设置进度状态为成功，并添加相应的消息。 //- 如果部署正在进展中，则更新部署状态，设置进度状态为正在进展，并添加相应的消息。 //这段代码通过调用util包中的一系列函数来实现对部署进度的检查和更新，其中涉及到对部署条件（condition）的获取、设置和更新等操作。 // Update the current Progressing condition or add a new one if it doesn\u0026#39;t exist. // If a Progressing condition with status=true already exists, we should update // everything but lastTransitionTime. SetDeploymentCondition already does that but // it also is not updating conditions when the reason of the new condition is the // same as the old. The Progressing condition is a special case because we want to // update with the same reason and change just lastUpdateTime iff we notice any // progress. That\u0026#39;s why we handle it here. if currentCond != nil { if currentCond.Status == v1.ConditionTrue { condition.LastTransitionTime = currentCond.LastTransitionTime } util.RemoveDeploymentCondition(\u0026amp;newStatus, apps.DeploymentProgressing) } util.SetDeploymentCondition(\u0026amp;newStatus, *condition) case util.DeploymentTimedOut(ctx, d, \u0026amp;newStatus): // Update the deployment with a timeout condition. If the condition already exists, // we ignore this update. msg := fmt.Sprintf(\u0026#34;Deployment %q has timed out progressing.\u0026#34;, d.Name) if newRS != nil { msg = fmt.Sprintf(\u0026#34;ReplicaSet %q has timed out progressing.\u0026#34;, newRS.Name) } condition := util.NewDeploymentCondition(apps.DeploymentProgressing, v1.ConditionFalse, util.TimedOutReason, msg) util.SetDeploymentCondition(\u0026amp;newStatus, *condition) } //该代码片段是Go语言编写的，它包含两个分支，分别处理不同的条件。 //第一个分支更新或添加一个正在进行的条件。如果已经存在一个状态为true的正在进行的条件，那么只更新除lastTransitionTime之外的所有内容。 //这通过调用util.RemoveDeploymentCondition和util.SetDeploymentCondition函数来实现。 //第二个分支处理超时条件。如果部署或新ReplicaSet超时进行，则更新部署的条件为超时状态，并忽略已经存在的超时条件。 //这通过调用util.NewDeploymentCondition和util.SetDeploymentCondition函数来实现。 } // Move failure conditions of all replica sets in deployment conditions. For now, // only one failure condition is returned from getReplicaFailures. if replicaFailureCond := dc.getReplicaFailures(allRSs, newRS); len(replicaFailureCond) \u0026gt; 0 { // There will be only one ReplicaFailure condition on the replica set. util.SetDeploymentCondition(\u0026amp;newStatus, replicaFailureCond[0]) } else { util.RemoveDeploymentCondition(\u0026amp;newStatus, apps.DeploymentReplicaFailure) } // Do not update if there is nothing new to add. if reflect.DeepEqual(d.Status, newStatus) { // Requeue the deployment if required. dc.requeueStuckDeployment(ctx, d, newStatus) return nil } newDeployment := d newDeployment.Status = newStatus _, err := dc.client.AppsV1().Deployments(newDeployment.Namespace).UpdateStatus(ctx, newDeployment, metav1.UpdateOptions{}) return err } //该函数是用于更新部署（deployment）在滚动更新过程中的状态。 //它根据给定的所有复制集（replica set）和新的复制集的信息，计算出新的部署状态。 //然后，根据是否有进度截止时间，以及部署是否已完成，来检查和更新部署的进度状态。 //此外，它还会将复制集的失败条件转移到部署的条件中。 //最后，如果状态有更新，它会将新的部署状态更新到Kubernetes API中。 // getReplicaFailures will convert replica failure conditions from replica sets // to deployment conditions. func (dc *DeploymentController) getReplicaFailures(allRSs []*apps.ReplicaSet, newRS *apps.ReplicaSet) []apps.DeploymentCondition { var conditions []apps.DeploymentCondition if newRS != nil { for _, c := range newRS.Status.Conditions { if c.Type != apps.ReplicaSetReplicaFailure { continue } conditions = append(conditions, util.ReplicaSetToDeploymentCondition(c)) } } // Return failures for the new replica set over failures from old replica sets. if len(conditions) \u0026gt; 0 { return conditions } for i := range allRSs { rs := allRSs[i] if rs == nil { continue } for _, c := range rs.Status.Conditions { if c.Type != apps.ReplicaSetReplicaFailure { continue } conditions = append(conditions, util.ReplicaSetToDeploymentCondition(c)) } } return conditions } //该函数是用于将副本集的副本失败条件转换为部署条件的。 //函数首先检查新副本集（newRS）的状态条件，将除ReplicaSetReplicaFailure类型外的条件添加到conditions切片中。 //如果conditions切片不为空，则直接返回。 //否则，遍历所有旧副本集（allRSs），将除ReplicaSetReplicaFailure类型外的条件添加到conditions切片中。 //最后返回conditions切片。 // used for unit testing var nowFn = func() time.Time { return time.Now() } // requeueStuckDeployment checks whether the provided deployment needs to be synced for a progress // check. It returns the time after the deployment will be requeued for the progress check, 0 if it // will be requeued now, or -1 if it does not need to be requeued. func (dc *DeploymentController) requeueStuckDeployment(ctx context.Context, d *apps.Deployment, newStatus apps.DeploymentStatus) time.Duration { logger := klog.FromContext(ctx) currentCond := util.GetDeploymentCondition(d.Status, apps.DeploymentProgressing) // Can\u0026#39;t estimate progress if there is no deadline in the spec or progressing condition in the current status. if !util.HasProgressDeadline(d) || currentCond == nil { return time.Duration(-1) } // No need to estimate progress if the rollout is complete or already timed out. if util.DeploymentComplete(d, \u0026amp;newStatus) || currentCond.Reason == util.TimedOutReason { return time.Duration(-1) } // If there is no sign of progress at this point then there is a high chance that the // deployment is stuck. We should resync this deployment at some point in the future[1] // and check whether it has timed out. We definitely need this, otherwise we depend on the // controller resync interval. See https://github.com/kubernetes/kubernetes/issues/34458. // // [1] ProgressingCondition.LastUpdatedTime + progressDeadlineSeconds - time.Now() // // For example, if a Deployment updated its Progressing condition 3 minutes ago and has a // deadline of 10 minutes, it would need to be resynced for a progress check after 7 minutes. // // lastUpdated: 00:00:00 // now: 00:03:00 // progressDeadlineSeconds: 600 (10 minutes) // // lastUpdated + progressDeadlineSeconds - now =\u0026gt; 00:00:00 + 00:10:00 - 00:03:00 =\u0026gt; 07:00 after := currentCond.LastUpdateTime.Time.Add(time.Duration(*d.Spec.ProgressDeadlineSeconds) * time.Second).Sub(nowFn()) // If the remaining time is less than a second, then requeue the deployment immediately. // Make it ratelimited so we stay on the safe side, eventually the Deployment should // transition either to a Complete or to a TimedOut condition. if after \u0026lt; time.Second { logger.V(4).Info(\u0026#34;Queueing up deployment for a progress check now\u0026#34;, \u0026#34;deployment\u0026#34;, klog.KObj(d)) dc.enqueueRateLimited(d) return time.Duration(0) } logger.V(4).Info(\u0026#34;Queueing up deployment for a progress check\u0026#34;, \u0026#34;deployment\u0026#34;, klog.KObj(d), \u0026#34;queueAfter\u0026#34;, int(after.Seconds())) // Add a second to avoid milliseconds skew in AddAfter. // See https://github.com/kubernetes/kubernetes/issues/39785#issuecomment-279959133 for more info. dc.enqueueAfter(d, after+time.Second) return after } //该函数用于检查提供的部署是否需要同步进行进度检查。 //它返回部署重新排队进行进度检查的时间，如果需要立即重新排队则返回0，如果不需要重新排队则返回-1。 //函数首先从上下文中获取日志记录器，然后获取部署的当前状态和新的状态。 //如果部署的规范中没有截止日期或当前状态中没有进行中的条件，则无法估计进度，函数将返回-1。 //如果部署已经完成或已经超时，则无需估计进度，函数将返回-1。 //如果此时没有进度的迹象，则部署可能卡住了。函数将在未来的某个时间点重新同步此部署，并检查是否超时。 //这需要通过调用enqueueRateLimited方法将部署加入到队列中，并立即返回0。 //否则，函数将计算重新同步部署的时间，并通过调用enqueueAfter方法将部署加入到队列中，并返回计算的时间。 //如果计算的时间小于1秒，则函数将立即重新排队，并通过调用enqueueRateLimited方法将部署加入到队列中，并返回0。 //最后，函数将相关的日志信息记录下来。 "},{"id":103,"href":"/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8B-rollinggo-%E6%BB%9A%E5%8A%A8%E6%9B%B4%E6%96%B0-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-rollinggo-gun-dong-geng-xin-yuan-ma-jie-du/","title":"K8S控制器之 rolling.go 滚动更新 源码解读 2024-04-09 11:32:58.133","section":"Docs","content":"/* Copyright 2016 The Kubernetes Authors. Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \u0026#34;AS IS\u0026#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */ package deployment import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;sort\u0026#34; apps \u0026#34;k8s.io/api/apps/v1\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/controller\u0026#34; deploymentutil \u0026#34;k8s.io/kubernetes/pkg/controller/deployment/util\u0026#34; ) // rolloutRolling implements the logic for rolling a new replica set. func (dc *DeploymentController) rolloutRolling(ctx context.Context, d *apps.Deployment, rsList []*apps.ReplicaSet) error { newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(ctx, d, rsList, true) if err != nil { return err } allRSs := append(oldRSs, newRS) // Scale up, if we can. scaledUp, err := dc.reconcileNewReplicaSet(ctx, allRSs, newRS, d) if err != nil { return err } if scaledUp { // Update DeploymentStatus return dc.syncRolloutStatus(ctx, allRSs, newRS, d) } // Scale down, if we can. scaledDown, err := dc.reconcileOldReplicaSets(ctx, allRSs, controller.FilterActiveReplicaSets(oldRSs), newRS, d) if err != nil { return err } if scaledDown { // Update DeploymentStatus return dc.syncRolloutStatus(ctx, allRSs, newRS, d) } if deploymentutil.DeploymentComplete(d, \u0026amp;d.Status) { if err := dc.cleanupDeployment(ctx, oldRSs, d); err != nil { return err } } // Sync deployment status return dc.syncRolloutStatus(ctx, allRSs, newRS, d) } //该函数实现了滚动更新一个新的副本集的逻辑。具体步骤如下： //1. 调用getAllReplicaSetsAndSyncRevision函数获取所有的副本集并同步修订版本。 //2. 将获取到的所有副本集添加到allRSs切片中。 //3. 调用reconcileNewReplicaSet函数尝试增加新副本集的副本数，如果成功则更新部署状态。 //4. 如果上一步成功，则调用syncRolloutStatus函数更新部署状态。 //5. 否则，调用reconcileOldReplicaSets函数尝试减少旧副本集的副本数，如果成功则更新部署状态。 //6. 如果上一步成功，则再次调用syncRolloutStatus函数更新部署状态。 //7. 检查部署是否完成，如果完成则调用cleanupDeployment函数清理旧的副本集。 //8. 最后，调用syncRolloutStatus函数更新部署状态。 func (dc *DeploymentController) reconcileNewReplicaSet(ctx context.Context, allRSs []*apps.ReplicaSet, newRS *apps.ReplicaSet, deployment *apps.Deployment) (bool, error) { if *(newRS.Spec.Replicas) == *(deployment.Spec.Replicas) { // Scaling not required. return false, nil } if *(newRS.Spec.Replicas) \u0026gt; *(deployment.Spec.Replicas) { // Scale down. scaled, _, err := dc.scaleReplicaSetAndRecordEvent(ctx, newRS, *(deployment.Spec.Replicas), deployment) return scaled, err } newReplicasCount, err := deploymentutil.NewRSNewReplicas(deployment, allRSs, newRS) if err != nil { return false, err } scaled, _, err := dc.scaleReplicaSetAndRecordEvent(ctx, newRS, newReplicasCount, deployment) return scaled, err } //该函数是一个用于协调新 ReplicaSet 的函数，它会根据新 ReplicaSet 和 Deployment 的规格来决定是否需要进行缩放操作。 //具体流程如下： //1. 首先，函数会比较新 ReplicaSet 和 Deployment 规格中的副本数。如果它们相等，说明不需要进行缩放操作，函数直接返回 false 和 nil。 //2. 如果新 ReplicaSet 的副本数大于 Deployment 的副本数，说明需要进行缩放操作。 //函数会调用 dc.scaleReplicaSetAndRecordEvent 方法来缩小新 ReplicaSet 的规模，并返回缩放结果和可能的错误。 //3. 如果新 ReplicaSet 的副本数小于或等于 Deployment 的副本数，并且存在其他 ReplicaSet， //那么函数会调用 deploymentutil.NewRSNewReplicas 方法来计算新 ReplicaSet 应该拥有的副本数。 //4. 最后，函数会调用 dc.scaleReplicaSetAndRecordEvent 方法来调整新 ReplicaSet 的规模，并返回缩放结果和可能的错误。 func (dc *DeploymentController) reconcileOldReplicaSets(ctx context.Context, allRSs []*apps.ReplicaSet, oldRSs []*apps.ReplicaSet, newRS *apps.ReplicaSet, deployment *apps.Deployment) (bool, error) { logger := klog.FromContext(ctx) oldPodsCount := deploymentutil.GetReplicaCountForReplicaSets(oldRSs) if oldPodsCount == 0 { // Can\u0026#39;t scale down further return false, nil } allPodsCount := deploymentutil.GetReplicaCountForReplicaSets(allRSs) logger.V(4).Info(\u0026#34;New replica set\u0026#34;, \u0026#34;replicaSet\u0026#34;, klog.KObj(newRS), \u0026#34;availableReplicas\u0026#34;, newRS.Status.AvailableReplicas) maxUnavailable := deploymentutil.MaxUnavailable(*deployment) //该函数是一个Go语言函数，它属于DeploymentController类型。 //函数名为reconcileOldReplicaSets，它接受五个参数：ctx上下文对象、allRSs所有ReplicaSet对象的切片、 //oldRSs旧的ReplicaSet对象的切片、newRS新的ReplicaSet对象和deployment Deployment对象。 //函数返回两个值：一个布尔值和一个错误对象。 //函数主要功能是协调旧的ReplicaSet对象的副本数量，以确保与Deployment对象的期望副本数量一致。 //它首先通过deploymentutil.GetReplicaCountForReplicaSets函数获取旧的ReplicaSet对象的副本数量，并检查是否为0。 //如果是0，则意味着无法进一步缩容，函数直接返回false和nil。 //接下来，函数通过同样的方式获取所有ReplicaSet对象的副本数量，并使用logger.V(4).Info记录相关信息。 //然后，函数调用deploymentutil.MaxUnavailable函数获取最大不可用副本数。 //函数的后续代码逻辑为，根据最大不可用副本数和新的ReplicaSet对象的可用副本数，判断是否需要缩容旧的ReplicaSet对象的副本数量。 //如果需要缩容，则进行相应的操作。 //最后，函数返回一个布尔值和一个错误对象，表示是否成功协调旧的ReplicaSet对象的副本数量。 // Check if we can scale down. We can scale down in the following 2 cases: // * Some old replica sets have unhealthy replicas, we could safely scale down those unhealthy replicas since that won\u0026#39;t further // increase unavailability. // * New replica set has scaled up and it\u0026#39;s replicas becomes ready, then we can scale down old replica sets in a further step. // // maxScaledDown := allPodsCount - minAvailable - newReplicaSetPodsUnavailable // take into account not only maxUnavailable and any surge pods that have been created, but also unavailable pods from // the newRS, so that the unavailable pods from the newRS would not make us scale down old replica sets in a further // step(that will increase unavailability). // // Concrete example: // // * 10 replicas // * 2 maxUnavailable (absolute number, not percent) // * 3 maxSurge (absolute number, not percent) // // case 1: // * Deployment is updated, newRS is created with 3 replicas, oldRS is scaled down to 8, and newRS is scaled up to 5. // * The new replica set pods crashloop and never become available. // * allPodsCount is 13. minAvailable is 8. newRSPodsUnavailable is 5. // * A node fails and causes one of the oldRS pods to become unavailable. However, 13 - 8 - 5 = 0, so the oldRS won\u0026#39;t be scaled down. // * The user notices the crashloop and does kubectl rollout undo to rollback. // * newRSPodsUnavailable is 1, since we rolled back to the good replica set, so maxScaledDown = 13 - 8 - 1 = 4. 4 of the crashlooping pods will be scaled down. // * The total number of pods will then be 9 and the newRS can be scaled up to 10. // // case 2: // Same example, but pushing a new pod template instead of rolling back (aka \u0026#34;roll over\u0026#34;): // * The new replica set created must start with 0 replicas because allPodsCount is already at 13. // * However, newRSPodsUnavailable would also be 0, so the 2 old replica sets could be scaled down by 5 (13 - 8 - 0), which would then // allow the new replica set to be scaled up by 5. minAvailable := *(deployment.Spec.Replicas) - maxUnavailable newRSUnavailablePodCount := *(newRS.Spec.Replicas) - newRS.Status.AvailableReplicas maxScaledDown := allPodsCount - minAvailable - newRSUnavailablePodCount if maxScaledDown \u0026lt;= 0 { return false, nil } //该函数用于判断是否可以进行缩容操作。缩容可以在以下两种情况下进行： //1. 一些旧的副本集存在不健康的副本，可以安全地缩容这些不健康的副本，因为这不会进一步增加不可用性。 //2. 新的副本集已经扩容并且其副本变为就绪状态，那么可以在进一步的步骤中缩容旧的副本集。 //函数首先根据给定的参数计算出最小可用副本数minAvailable和新副本集的不可用副本数newRSUnavailablePodCount， //然后通过公式maxScaledDown = allPodsCount - minAvailable - newRSUnavailablePodCount 计算出最大可缩容数maxScaledDown。 //如果maxScaledDown小于等于0，则返回false，表示不能进行缩容操作。 // Clean up unhealthy replicas first, otherwise unhealthy replicas will block deployment // and cause timeout. See https://github.com/kubernetes/kubernetes/issues/16737 oldRSs, cleanupCount, err := dc.cleanupUnhealthyReplicas(ctx, oldRSs, deployment, maxScaledDown) if err != nil { return false, nil } logger.V(4).Info(\u0026#34;Cleaned up unhealthy replicas from old RSes\u0026#34;, \u0026#34;count\u0026#34;, cleanupCount) // Scale down old replica sets, need check maxUnavailable to ensure we can scale down allRSs = append(oldRSs, newRS) scaledDownCount, err := dc.scaleDownOldReplicaSetsForRollingUpdate(ctx, allRSs, oldRSs, deployment) if err != nil { return false, nil } logger.V(4).Info(\u0026#34;Scaled down old RSes\u0026#34;, \u0026#34;deployment\u0026#34;, klog.KObj(deployment), \u0026#34;count\u0026#34;, scaledDownCount) totalScaledDown := cleanupCount + scaledDownCount return totalScaledDown \u0026gt; 0, nil } //该函数用于清理不健康的副本，并进行滚动更新时的缩容操作。首先清理不健康的副本，以避免阻塞部署并导致超时。然后将旧的副本集缩容， //同时检查最大不可用副本数以确保可以进行缩容操作。函数返回一个布尔值，表示是否进行了缩容操作。 // cleanupUnhealthyReplicas will scale down old replica sets with unhealthy replicas, so that all unhealthy replicas will be deleted. func (dc *DeploymentController) cleanupUnhealthyReplicas(ctx context.Context, oldRSs []*apps.ReplicaSet, deployment *apps.Deployment, maxCleanupCount int32) ([]*apps.ReplicaSet, int32, error) { logger := klog.FromContext(ctx) sort.Sort(controller.ReplicaSetsByCreationTimestamp(oldRSs)) // Safely scale down all old replica sets with unhealthy replicas. Replica set will sort the pods in the order // such that not-ready \u0026lt; ready, unscheduled \u0026lt; scheduled, and pending \u0026lt; running. This ensures that unhealthy replicas will // been deleted first and won\u0026#39;t increase unavailability. //该函数用于清理不健康的副本集中的副本，以便所有不健康的副本都将被删除。 //函数首先通过创建时间对旧的副本集进行排序，然后安全地缩小所有具有不健康副本的旧副本集的规模。 //副本集将按照以下顺序对Pod进行排序：未就绪\u0026lt;就绪，未调度\u0026lt;已调度，等待\u0026lt;运行。 //这确保了不健康的副本将首先被删除，并且不会增加不可用性。 totalScaledDown := int32(0) for i, targetRS := range oldRSs { if totalScaledDown \u0026gt;= maxCleanupCount { break } if *(targetRS.Spec.Replicas) == 0 { // cannot scale down this replica set. continue } logger.V(4).Info(\u0026#34;Found available pods in old RS\u0026#34;, \u0026#34;replicaSet\u0026#34;, klog.KObj(targetRS), \u0026#34;availableReplicas\u0026#34;, targetRS.Status.AvailableReplicas) if *(targetRS.Spec.Replicas) == targetRS.Status.AvailableReplicas { // no unhealthy replicas found, no scaling required. continue } //该Go函数是一个for循环，用于遍历一组旧的ReplicaSet（oldRSs），并在满足条件的情况下对它们进行缩容操作。 //具体功能如下： //1. 初始化一个整型变量totalScaledDown为0，用于记录已缩容的ReplicaSet数量。 //2. 遍历oldRSs中的每个ReplicaSet，使用索引i和目标ReplicaSet targetRS。 //3. 如果totalScaledDown大于等于最大清理数量maxCleanupCount，则跳出循环。 //4. 如果目标ReplicaSet的副本数量Spec.Replicas为0，则无法进行缩容操作，继续下一个循环。 //5. 使用logger记录日志信息，表示在目标ReplicaSet中找到了可用的Pod。 //6. 如果目标ReplicaSet的副本数量Spec.Replicas等于其可用副本数量Status.AvailableReplicas，则无需进行缩容操作，继续下一个循环。 //7. 如果以上条件均不满足，则进行缩容操作，并更新totalScaledDown的值。 //综上所述，该函数的功能是在满足条件的情况下对一组旧的ReplicaSet进行缩容操作，并记录已缩容的ReplicaSet数量。 scaledDownCount := min(maxCleanupCount-totalScaledDown, *(targetRS.Spec.Replicas)-targetRS.Status.AvailableReplicas) newReplicasCount := *(targetRS.Spec.Replicas) - scaledDownCount if newReplicasCount \u0026gt; *(targetRS.Spec.Replicas) { return nil, 0, fmt.Errorf(\u0026#34;when cleaning up unhealthy replicas, got invalid request to scale down %s/%s %d -\u0026gt; %d\u0026#34;, targetRS.Namespace, targetRS.Name, *(targetRS.Spec.Replicas), newReplicasCount) } _, updatedOldRS, err := dc.scaleReplicaSetAndRecordEvent(ctx, targetRS, newReplicasCount, deployment) if err != nil { return nil, totalScaledDown, err } totalScaledDown += scaledDownCount oldRSs[i] = updatedOldRS } return oldRSs, totalScaledDown, nil } //该函数用于清理不健康的副本，并将其数量减少到目标副本数量。 //它首先计算清理数量，然后尝试将目标副本数量减少到该数量。如果减少后的数量大于目标副本数量，则返回错误。 //然后，它会更新旧的副本集，并将其添加到一个切片中。 //最后，它返回更新后的旧副本集和清理的总数量。 // scaleDownOldReplicaSetsForRollingUpdate scales down old replica sets when deployment strategy is \u0026#34;RollingUpdate\u0026#34;. // Need check maxUnavailable to ensure availability func (dc *DeploymentController) scaleDownOldReplicaSetsForRollingUpdate(ctx context.Context, allRSs []*apps.ReplicaSet, oldRSs []*apps.ReplicaSet, deployment *apps.Deployment) (int32, error) { logger := klog.FromContext(ctx) maxUnavailable := deploymentutil.MaxUnavailable(*deployment) // Check if we can scale down. minAvailable := *(deployment.Spec.Replicas) - maxUnavailable // Find the number of available pods. availablePodCount := deploymentutil.GetAvailableReplicaCountForReplicaSets(allRSs) if availablePodCount \u0026lt;= minAvailable { // Cannot scale down. return 0, nil } logger.V(4).Info(\u0026#34;Found available pods in deployment, scaling down old RSes\u0026#34;, \u0026#34;deployment\u0026#34;, klog.KObj(deployment), \u0026#34;availableReplicas\u0026#34;, availablePodCount) //该函数用于在RollingUpdate策略下滚动更新时，将旧的ReplicaSet缩容。它需要检查最大不可用副本数以确保可用性。 //函数首先从上下文中获取日志记录器，并计算最大不可用副本数。 //然后，它检查可用Pod的数量是否大于等于最小可用副本数，如果小于最小可用副本数则不进行缩容操作，返回0。 //否则，函数记录日志并继续缩容旧的ReplicaSet。 sort.Sort(controller.ReplicaSetsByCreationTimestamp(oldRSs)) totalScaledDown := int32(0) totalScaleDownCount := availablePodCount - minAvailable for _, targetRS := range oldRSs { if totalScaledDown \u0026gt;= totalScaleDownCount { // No further scaling required. break } if *(targetRS.Spec.Replicas) == 0 { // cannot scale down this ReplicaSet. continue } // Scale down. scaleDownCount := min(*(targetRS.Spec.Replicas), totalScaleDownCount-totalScaledDown) newReplicasCount := *(targetRS.Spec.Replicas) - scaleDownCount if newReplicasCount \u0026gt; *(targetRS.Spec.Replicas) { return 0, fmt.Errorf(\u0026#34;when scaling down old RS, got invalid request to scale down %s/%s %d -\u0026gt; %d\u0026#34;, targetRS.Namespace, targetRS.Name, *(targetRS.Spec.Replicas), newReplicasCount) } _, _, err := dc.scaleReplicaSetAndRecordEvent(ctx, targetRS, newReplicasCount, deployment) if err != nil { return totalScaledDown, err } totalScaledDown += scaleDownCount } //这段代码是用于缩容旧的ReplicaSet的。它首先通过sort.Sort方法将旧的ReplicaSet按照创建时间排序。 //然后，它计算出需要缩容的总副本数，并遍历每个旧的ReplicaSet。 //如果已经缩容的副本数达到了需要缩容的总副本数，就停止缩容操作。 //对于每个需要缩容的ReplicaSet，它计算出可以缩容的副本数，并更新其副本数。 //如果更新后的副本数大于原来的副本数，就会返回错误。如果更新成功，就将缩容的副本数累加到totalScaledDown中。 //最后，函数返回缩容的总副本数和可能发生的错误。 return totalScaledDown, nil } //该函数用于在Deployment策略为RollingUpdate时，缩放旧的副本集。 //它会检查最大不可用值（maxUnavailable）以确保可用性。 //函数首先计算出最小可用值，然后查找可用Pod的数量。 //如果可用Pod数量小于或等于最小可用值，则不会进行缩放。 //然后，函数会对旧的副本集进行排序，并迭代每个副本集。对于每个副本集，函数会计算出可以缩放的数量，并将其与副本集当前的副本数进行比较。 //如果计算出的缩放数量大于当前副本数，则函数会返回错误。 //最后，函数会缩放每个副本集，并记录缩放事件。函数返回缩放的总数量和可能的错误。 "},{"id":104,"href":"/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8B-schedulergo-%E8%B0%83%E5%BA%A6%E5%99%A8-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-schedulergo-diao-du-qi-yuan-ma-jie-du/","title":"K8S控制器之 scheduler.go 调度器 源码解读 2024-04-09 11:44:25.171","section":"Docs","content":"/* Copyright 2014 The Kubernetes Authors. Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \u0026#34;AS IS\u0026#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */ package scheduler import ( \u0026#34;context\u0026#34; \u0026#34;errors\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; v1 \u0026#34;k8s.io/api/core/v1\u0026#34; \u0026#34;k8s.io/apimachinery/pkg/api/meta\u0026#34; metav1 \u0026#34;k8s.io/apimachinery/pkg/apis/meta/v1\u0026#34; \u0026#34;k8s.io/apimachinery/pkg/util/wait\u0026#34; utilfeature \u0026#34;k8s.io/apiserver/pkg/util/feature\u0026#34; \u0026#34;k8s.io/client-go/dynamic/dynamicinformer\u0026#34; \u0026#34;k8s.io/client-go/informers\u0026#34; coreinformers \u0026#34;k8s.io/client-go/informers/core/v1\u0026#34; clientset \u0026#34;k8s.io/client-go/kubernetes\u0026#34; restclient \u0026#34;k8s.io/client-go/rest\u0026#34; \u0026#34;k8s.io/client-go/tools/cache\u0026#34; configv1 \u0026#34;k8s.io/kube-scheduler/config/v1\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/features\u0026#34; schedulerapi \u0026#34;k8s.io/kubernetes/pkg/scheduler/apis/config\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/scheduler/apis/config/scheme\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/scheduler/framework\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/scheduler/framework/parallelize\u0026#34; frameworkplugins \u0026#34;k8s.io/kubernetes/pkg/scheduler/framework/plugins\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/scheduler/framework/plugins/noderesources\u0026#34; frameworkruntime \u0026#34;k8s.io/kubernetes/pkg/scheduler/framework/runtime\u0026#34; internalcache \u0026#34;k8s.io/kubernetes/pkg/scheduler/internal/cache\u0026#34; cachedebugger \u0026#34;k8s.io/kubernetes/pkg/scheduler/internal/cache/debugger\u0026#34; internalqueue \u0026#34;k8s.io/kubernetes/pkg/scheduler/internal/queue\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/scheduler/metrics\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/scheduler/profile\u0026#34; ) const ( // Duration the scheduler will wait before expiring an assumed pod. // See issue #106361 for more details about this parameter and its value. durationToExpireAssumedPod time.Duration = 0 ) // ErrNoNodesAvailable is used to describe the error that no nodes available to schedule pods. var ErrNoNodesAvailable = fmt.Errorf(\u0026#34;no nodes available to schedule pods\u0026#34;) // Scheduler watches for new unscheduled pods. It attempts to find // nodes that they fit on and writes bindings back to the api server. type Scheduler struct { // It is expected that changes made via Cache will be observed // by NodeLister and Algorithm. Cache internalcache.Cache Extenders []framework.Extender // NextPod should be a function that blocks until the next pod // is available. We don\u0026#39;t use a channel for this, because scheduling // a pod may take some amount of time and we don\u0026#39;t want pods to get // stale while they sit in a channel. NextPod func(logger klog.Logger) (*framework.QueuedPodInfo, error) // FailureHandler is called upon a scheduling failure. FailureHandler FailureHandlerFn // SchedulePod tries to schedule the given pod to one of the nodes in the node list. // Return a struct of ScheduleResult with the name of suggested host on success, // otherwise will return a FitError with reasons. SchedulePod func(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) (ScheduleResult, error) // Close this to shut down the scheduler. StopEverything \u0026lt;-chan struct{} // SchedulingQueue holds pods to be scheduled SchedulingQueue internalqueue.SchedulingQueue // Profiles are the scheduling profiles. Profiles profile.Map client clientset.Interface nodeInfoSnapshot *internalcache.Snapshot percentageOfNodesToScore int32 nextStartNodeIndex int // logger *must* be initialized when creating a Scheduler, // otherwise logging functions will access a nil sink and // panic. logger klog.Logger // registeredHandlers contains the registrations of all handlers. It\u0026#39;s used to check if all handlers have finished syncing before the scheduling cycles start. registeredHandlers []cache.ResourceEventHandlerRegistration } //该Go代码定义了一个名为Scheduler的结构体，用于管理Pod的调度过程。 //它包含多个字段，用于配置调度器的行为， //例如Cache、Extenders、NextPod、FailureHandler、SchedulePod、StopEverything、SchedulingQueue、Profiles、 //client、nodeInfoSnapshot、percentageOfNodesToScore、nextStartNodeIndex、logger和registeredHandlers。 //这些字段的作用如下： //- Cache：用于存储集群的状态信息， 以便Scheduler能够快速访问。 //- Extenders：是一组扩展程序，可以自定义Pod的调度逻辑。 //- NextPod：是一个函数，用于获取下一个待调度的Pod。 - //FailureHandler：是一个函数，用于处理调度失败的情况。 //- SchedulePod：是一个函数，用于尝试将给定的Pod调度到节点列表中的一个节点上。 //成功时返回建议的主机名，失败时返回FitError错误。 //- StopEverything：是一个通道，用于关闭Scheduler。 //- SchedulingQueue：用于存储待调度的Pod。 //- Profiles：是一组调度配置文件，用于定义不同的调度策略。 //- client：是用于与Kubernetes API服务器交互的客户端。 //- nodeInfoSnapshot：是集群节点的快照，包含节点的状态信息。 //- percentageOfNodesToScore：用于指定参与评分的节点百分比。 //- nextStartNodeIndex：用于指定下一个开始调度的节点索引。 //- logger：用于记录日志信息。 //- registeredHandlers：包含所有处理器的注册信息，用于检查处理器是否已同步完成。 //总的来说，这个Scheduler结构体定义了一个调度器所需的各种配置和功能， //包括节点和Pod的缓存、扩展程序、日志记录、失败处理、调度逻辑等。它提供了一种灵活的方式来定制和管理Pod的调度过程。 func (sched *Scheduler) applyDefaultHandlers() { sched.SchedulePod = sched.schedulePod sched.FailureHandler = sched.handleSchedulingFailure } //该函数为一个go语言函数，作用是为Scheduler结构体实例sched设置默认的处理函数。 //具体操作是将sched.schedulePod赋值给sched.SchedulePod，将sched.handleSchedulingFailure赋值给sched.FailureHandler。 type schedulerOptions struct { componentConfigVersion string kubeConfig *restclient.Config // Overridden by profile level percentageOfNodesToScore if set in v1. percentageOfNodesToScore int32 podInitialBackoffSeconds int64 podMaxBackoffSeconds int64 podMaxInUnschedulablePodsDuration time.Duration // Contains out-of-tree plugins to be merged with the in-tree registry. frameworkOutOfTreeRegistry frameworkruntime.Registry profiles []schedulerapi.KubeSchedulerProfile extenders []schedulerapi.Extender frameworkCapturer FrameworkCapturer parallelism int32 applyDefaultProfile bool } //该代码定义了一个名为schedulerOptions的结构体，用于配置调度器的参数。 //其中包括了组件配置版本、kubeconfig配置、节点打分百分比、Pod初始退避时间、Pod最大退避时间、 //Pod在不可调度状态下的最大持续时间、外部插件注册表、调度器配置文件、扩展器、框架捕获器和并行度等参数。 // Option configures a Scheduler type Option func(*schedulerOptions) // ScheduleResult represents the result of scheduling a pod. type ScheduleResult struct { // Name of the selected node. SuggestedHost string // The number of nodes the scheduler evaluated the pod against in the filtering // phase and beyond. // Note that it contains the number of nodes that filtered out by PreFilterResult. EvaluatedNodes int // The number of nodes out of the evaluated ones that fit the pod. FeasibleNodes int // The nominating info for scheduling cycle. nominatingInfo *framework.NominatingInfo } //这段代码定义了Go语言中的两个类型和一个函数： //1. Option 是一个函数类型，其参数为一个 *schedulerOptions 指针，用于配置一个 Scheduler。 //2. ScheduleResult 是一个结构体类型，代表调度 Pod 的结果。它包含以下字段： //- SuggestedHost：被选中的节点名称。 //- EvaluatedNodes：在过滤阶段及之后对 Pod 进行评估的节点数量。 //- FeasibleNodes：在评估节点中适合 Pod 的节点数量。 //- nominatingInfo：提名信息，用于记录调度周期的提名情况。 //这段代码没有定义函数的具体实现，因此无法对其复杂度进行评估。 // WithComponentConfigVersion sets the component config version to the // KubeSchedulerConfiguration version used. The string should be the full // scheme group/version of the external type we converted from (for example // \u0026#34;kubescheduler.config.k8s.io/v1\u0026#34;) func WithComponentConfigVersion(apiVersion string) Option { return func(o *schedulerOptions) { o.componentConfigVersion = apiVersion } } // WithKubeConfig sets the kube config for Scheduler. func WithKubeConfig(cfg *restclient.Config) Option { return func(o *schedulerOptions) { o.kubeConfig = cfg } } // WithProfiles sets profiles for Scheduler. By default, there is one profile // with the name \u0026#34;default-scheduler\u0026#34;. func WithProfiles(p ...schedulerapi.KubeSchedulerProfile) Option { return func(o *schedulerOptions) { o.profiles = p o.applyDefaultProfile = false } } // WithParallelism sets the parallelism for all scheduler algorithms. Default is 16. func WithParallelism(threads int32) Option { return func(o *schedulerOptions) { o.parallelism = threads } } // WithPercentageOfNodesToScore sets percentageOfNodesToScore for Scheduler. // The default value of 0 will use an adaptive percentage: 50 - (num of nodes)/125. func WithPercentageOfNodesToScore(percentageOfNodesToScore *int32) Option { return func(o *schedulerOptions) { if percentageOfNodesToScore != nil { o.percentageOfNodesToScore = *percentageOfNodesToScore } } } //这些函数是Go语言中的函数，用于设置调度器（scheduler）的配置选项。 //- WithComponentConfigVersion 函数用于设置组件配置的版本。 //它接受一个字符串参数 apiVersion，该参数应该是外部类型转换而来的完整方案组/版本（例如 \u0026#34;kubescheduler.config.k8s.io/v1\u0026#34;）。 //- WithKubeConfig 函数用于设置调度器的kube配置。 //- WithProfiles 函数用于设置调度器的配置文件。默认情况下，有一个名为 \u0026#34;default-scheduler\u0026#34; 的配置文件。 //- WithParallelism 函数用于设置所有调度算法的并行度。默认值为16。 //- WithPercentageOfNodesToScore 函数用于设置Scheduler的 percentageOfNodesToScore。 //默认值为0，使用自适应百分比：50 - (节点数)/125。 // WithFrameworkOutOfTreeRegistry sets the registry for out-of-tree plugins. Those plugins // will be appended to the default registry. func WithFrameworkOutOfTreeRegistry(registry frameworkruntime.Registry) Option { return func(o *schedulerOptions) { o.frameworkOutOfTreeRegistry = registry } } // WithPodInitialBackoffSeconds sets podInitialBackoffSeconds for Scheduler, the default value is 1 func WithPodInitialBackoffSeconds(podInitialBackoffSeconds int64) Option { return func(o *schedulerOptions) { o.podInitialBackoffSeconds = podInitialBackoffSeconds } } // WithPodMaxBackoffSeconds sets podMaxBackoffSeconds for Scheduler, the default value is 10 func WithPodMaxBackoffSeconds(podMaxBackoffSeconds int64) Option { return func(o *schedulerOptions) { o.podMaxBackoffSeconds = podMaxBackoffSeconds } } // WithPodMaxInUnschedulablePodsDuration sets podMaxInUnschedulablePodsDuration for PriorityQueue. func WithPodMaxInUnschedulablePodsDuration(duration time.Duration) Option { return func(o *schedulerOptions) { o.podMaxInUnschedulablePodsDuration = duration } } //这些函数是Go语言中的函数，用于设置调度器的配置选项。 //- WithFrameworkOutOfTreeRegistry 函数用于设置外部插件的注册表，将这些插件追加到默认的注册表中。 //- WithPodInitialBackoffSeconds 函数用于设置 Scheduler 的 podInitialBackoffSeconds，其默认值为 1。 //- WithPodMaxBackoffSeconds 函数用于设置 Scheduler 的 podMaxBackoffSeconds，其默认值为 10。 //- WithPodMaxInUnschedulablePodsDuration 函数用于设置 PriorityQueue 的 podMaxInUnschedulablePodsDuration。 // WithExtenders sets extenders for the Scheduler func WithExtenders(e ...schedulerapi.Extender) Option { return func(o *schedulerOptions) { o.extenders = e } } //该函数为Go语言中的函数，名为WithExtenders，接收一个变长参数e，类型为schedulerapi.Extender的切片。 //函数返回一个Option类型的函数，该函数接收一个schedulerOptions类型的指针o，将e赋值给o.extenders。 // FrameworkCapturer is used for registering a notify function in building framework. type FrameworkCapturer func(schedulerapi.KubeSchedulerProfile) // WithBuildFrameworkCapturer sets a notify function for getting buildFramework details. func WithBuildFrameworkCapturer(fc FrameworkCapturer) Option { return func(o *schedulerOptions) { o.frameworkCapturer = fc } } //该函数是一个名为WithBuildFrameworkCapturer的函数， //它接收一个FrameworkCapturer类型的参数fc，并返回一个Option类型的函数。 //返回的函数将传入的fc赋值给o.frameworkCapturer。 var defaultSchedulerOptions = schedulerOptions{ percentageOfNodesToScore: schedulerapi.DefaultPercentageOfNodesToScore, podInitialBackoffSeconds: int64(internalqueue.DefaultPodInitialBackoffDuration.Seconds()), podMaxBackoffSeconds: int64(internalqueue.DefaultPodMaxBackoffDuration.Seconds()), podMaxInUnschedulablePodsDuration: internalqueue.DefaultPodMaxInUnschedulablePodsDuration, parallelism: int32(parallelize.DefaultParallelism), // Ideally we would statically set the default profile here, but we can\u0026#39;t because // creating the default profile may require testing feature gates, which may get // set dynamically in tests. Therefore, we delay creating it until New is actually // invoked. applyDefaultProfile: true, } //这段Go代码定义了一个名为defaultSchedulerOptions的变量，它是一个schedulerOptions类型的结构体。 //这个结构体用于设置调度器的默认选项，包括以下字段： //- percentageOfNodesToScore：表示要进行打分的节点的百分比，默认值为schedulerapi.DefaultPercentageOfNodesToScore。 //- podInitialBackoffSeconds：表示Pod初始退避时间的秒数，默认值为internalqueue.DefaultPodInitialBackoffDuration的秒数。 //- podMaxBackoffSeconds：表示Pod最大退避时间的秒数，默认值为internalqueue.DefaultPodMaxBackoffDuration的秒数。 //- podMaxInUnschedulablePodsDuration：表示Pod在不可调度状态下允许的最大持续时间， //默认值为internalqueue.DefaultPodMaxInUnschedulablePodsDuration。 //- parallelism：表示并行处理任务的数量，默认值为parallelize.DefaultParallelism。 //- applyDefaultProfile：表示是否应用默认的调度配置文件，默认值为true。 //这些选项用于配置调度器的行为，例如决定多少节点需要进行打分、设置Pod的退避策略等。 // New returns a Scheduler func New(ctx context.Context, client clientset.Interface, informerFactory informers.SharedInformerFactory, dynInformerFactory dynamicinformer.DynamicSharedInformerFactory, recorderFactory profile.RecorderFactory, opts ...Option) (*Scheduler, error) { logger := klog.FromContext(ctx) stopEverything := ctx.Done() options := defaultSchedulerOptions for _, opt := range opts { opt(\u0026amp;options) } //该函数名为New，返回一个Scheduler类型指针和一个错误类型。 //函数参数包括上下文ctx、客户端接口client、共享informer工厂informerFactory、动态共享informer工厂dynInformerFactory、 //记录器工厂recorderFactory以及可选参数opts。 //函数首先从上下文中获取日志记录器logger和停止信号stopEverything。 //然后定义默认的调度器选项options，并遍历opts对options进行配置。 //最后返回一个Scheduler实例和错误类型。 if options.applyDefaultProfile { var versionedCfg configv1.KubeSchedulerConfiguration scheme.Scheme.Default(\u0026amp;versionedCfg) cfg := schedulerapi.KubeSchedulerConfiguration{} if err := scheme.Scheme.Convert(\u0026amp;versionedCfg, \u0026amp;cfg, nil); err != nil { return nil, err } options.profiles = cfg.Profiles } //这段Go代码主要功能是应用默认配置到调度器配置中。 //首先，它创建了一个configv1.KubeSchedulerConfiguration类型的变量versionedCfg， //并使用scheme.Scheme.Default函数为其应用默认值。 //接着，它创建了一个schedulerapi.KubeSchedulerConfiguration类型的变量cfg， //并将versionedCfg中的值通过scheme.Scheme.Convert函数转换并赋值给cfg。 //最后，将cfg.Profiles赋值给options.profiles。 registry := frameworkplugins.NewInTreeRegistry() if err := registry.Merge(options.frameworkOutOfTreeRegistry); err != nil { return nil, err } metrics.Register() extenders, err := buildExtenders(logger, options.extenders, options.profiles) if err != nil { return nil, fmt.Errorf(\u0026#34;couldn\u0026#39;t build extenders: %w\u0026#34;, err) } podLister := informerFactory.Core().V1().Pods().Lister() nodeLister := informerFactory.Core().V1().Nodes().Lister() snapshot := internalcache.NewEmptySnapshot() metricsRecorder := metrics.NewMetricsAsyncRecorder(1000, time.Second, stopEverything) profiles, err := profile.NewMap(ctx, options.profiles, registry, recorderFactory, frameworkruntime.WithComponentConfigVersion(options.componentConfigVersion), frameworkruntime.WithClientSet(client), frameworkruntime.WithKubeConfig(options.kubeConfig), frameworkruntime.WithInformerFactory(informerFactory), frameworkruntime.WithSnapshotSharedLister(snapshot), frameworkruntime.WithCaptureProfile(frameworkruntime.CaptureProfile(options.frameworkCapturer)), frameworkruntime.WithParallelism(int(options.parallelism)), frameworkruntime.WithExtenders(extenders), frameworkruntime.WithMetricsRecorder(metricsRecorder), ) if err != nil { return nil, fmt.Errorf(\u0026#34;initializing profiles: %v\u0026#34;, err) } //这段Go代码的功能是初始化一个调度器配置。 //1. 首先创建一个in-tree注册表registry。 //2. 将options.frameworkOutOfTreeRegistry合并到registry中，如果合并失败则返回错误。 //3. 注册指标收集。 //4. 构建扩展程序extenders，如果构建失败则返回错误。 //5. 获取Pod和Node的列表器。 //6. 创建一个空的快照snapshot。 //7. 创建一个异步指标记录器metricsRecorder。 //8. 使用给定的参数初始化调度器配置profiles，如果初始化失败则返回错误。 //其中，buildExtenders函数用于构建扩展程序，informFactory是一个informers工厂，用于创建Pod和Node的列表器。internalcache.NewEmptySnapshot()创建一个空的快照，metrics.NewMetricsAsyncRecorder创建一个异步指标记录器。profile.NewMap用于初始化调度器配置。 if len(profiles) == 0 { return nil, errors.New(\u0026#34;at least one profile is required\u0026#34;) } preEnqueuePluginMap := make(map[string][]framework.PreEnqueuePlugin) queueingHintsPerProfile := make(internalqueue.QueueingHintMapPerProfile) for profileName, profile := range profiles { preEnqueuePluginMap[profileName] = profile.PreEnqueuePlugins() queueingHintsPerProfile[profileName] = buildQueueingHintMap(profile.EnqueueExtensions()) } podQueue := internalqueue.NewSchedulingQueue( profiles[options.profiles[0].SchedulerName].QueueSortFunc(), informerFactory, internalqueue.WithPodInitialBackoffDuration(time.Duration(options.podInitialBackoffSeconds)*time.Second), internalqueue.WithPodMaxBackoffDuration(time.Duration(options.podMaxBackoffSeconds)*time.Second), internalqueue.WithPodLister(podLister), internalqueue.WithPodMaxInUnschedulablePodsDuration(options.podMaxInUnschedulablePodsDuration), internalqueue.WithPreEnqueuePluginMap(preEnqueuePluginMap), internalqueue.WithQueueingHintMapPerProfile(queueingHintsPerProfile), internalqueue.WithPluginMetricsSamplePercent(pluginMetricsSamplePercent), internalqueue.WithMetricsRecorder(*metricsRecorder), ) for _, fwk := range profiles { fwk.SetPodNominator(podQueue) } schedulerCache := internalcache.New(ctx, durationToExpireAssumedPod) // Setup cache debugger. debugger := cachedebugger.New(nodeLister, podLister, schedulerCache, podQueue) debugger.ListenForSignal(ctx) sched := \u0026amp;Scheduler{ Cache: schedulerCache, client: client, nodeInfoSnapshot: snapshot, percentageOfNodesToScore: options.percentageOfNodesToScore, Extenders: extenders, StopEverything: stopEverything, SchedulingQueue: podQueue, Profiles: profiles, logger: logger, } sched.NextPod = podQueue.Pop sched.applyDefaultHandlers() if err = addAllEventHandlers(sched, informerFactory, dynInformerFactory, unionedGVKs(queueingHintsPerProfile)); err != nil { return nil, fmt.Errorf(\u0026#34;adding event handlers: %w\u0026#34;, err) } return sched, nil } //该函数主要实现了以下功能： //1. 检查传入的profiles是否为空，如果为空则返回错误。 //2. 根据profiles创建preEnqueuePluginMap和queueingHintsPerProfile。 //3. 使用profiles中指定的SchedulerName创建一个SchedulingQueue对象，该对象用于管理待调度的Pod。 //4. 为每个profile设置PodNominator。 //5. 创建一个schedulerCache对象，用于缓存节点和Pod的信息。 //6. 创建一个debugger对象，用于调试缓存。 //7. 创建一个Scheduler对象，并设置其属性。 //8. 设置Scheduler的NextPod方法。 //9. 应用默认的处理程序。 //10. 添加所有事件处理程序。 //综上所述，该函数的主要功能是创建一个Scheduler对象，并对其进行初始化。 // defaultQueueingHintFn is the default queueing hint function. // It always returns Queue as the queueing hint. var defaultQueueingHintFn = func(_ klog.Logger, _ *v1.Pod, _, _ interface{}) (framework.QueueingHint, error) { return framework.Queue, nil } func buildQueueingHintMap(es []framework.EnqueueExtensions) internalqueue.QueueingHintMap { queueingHintMap := make(internalqueue.QueueingHintMap) for _, e := range es { events := e.EventsToRegister() //该函数的功能是构建一个队列提示映射（QueueingHintMap），它将事件注册到队列中。 //1. 函数接收一个[]framework.EnqueueExtensions参数，它是一个调度器扩展点的集合，这些扩展点可以注册事件。 //2. 创建一个空的internalqueue.QueueingHintMap用于存储队列提示映射。 //3. 遍历扩展点集合es中的每个扩展点e。 //4. 调用扩展点e的EventsToRegister()方法，获取该扩展点需要注册的事件。 //5. 将事件添加到队列提示映射queueingHintMap中。 //最终，函数返回构建完成的队列提示映射queueingHintMap。 // This will happen when plugin registers with empty events, it\u0026#39;s usually the case a pod // will become reschedulable only for self-update, e.g. schedulingGates plugin, the pod // will enter into the activeQ via priorityQueue.Update(). if len(events) == 0 { continue } // Note: Rarely, a plugin implements EnqueueExtensions but returns nil. // We treat it as: the plugin is not interested in any event, and hence pod failed by that plugin // cannot be moved by any regular cluster event. // So, we can just ignore such EventsToRegister here. registerNodeAdded := false registerNodeTaintUpdated := false for _, event := range events { fn := event.QueueingHintFn if fn == nil || !utilfeature.DefaultFeatureGate.Enabled(features.SchedulerQueueingHints) { fn = defaultQueueingHintFn } if event.Event.Resource == framework.Node { if event.Event.ActionType\u0026amp;framework.Add != 0 { registerNodeAdded = true } if event.Event.ActionType\u0026amp;framework.UpdateNodeTaint != 0 { registerNodeTaintUpdated = true } } //这段Go代码中的函数是一个循环，用于遍历一组事件（events）， //并根据这些事件的类型来更新两个布尔变量registerNodeAdded和registerNodeTaintUpdated。 //具体来说，函数首先检查事件是否启用了调度队列提示功能 //（通过utilfeature.DefaultFeatureGate.Enabled(features.SchedulerQueueingHints)来判断）， //如果没有启用，则使用默认的队列提示函数defaultQueueingHintFn //。然后，如果事件资源类型为framework.Node，并且事件动作类型包含framework.Add或framework.UpdateNodeTaint， //则分别将registerNodeAdded和registerNodeTaintUpdated设置为true。 //这段代码的主要目的是为了根据事件的类型来决定是否需要注册节点添加或节点污点更新的操作。 queueingHintMap[event.Event] = append(queueingHintMap[event.Event], \u0026amp;internalqueue.QueueingHintFunction{ PluginName: e.Name(), QueueingHintFn: fn, }) } if registerNodeAdded \u0026amp;\u0026amp; !registerNodeTaintUpdated { // Temporally fix for the issue https://github.com/kubernetes/kubernetes/issues/109437 // NodeAdded QueueingHint isn\u0026#39;t always called because of preCheck. // It\u0026#39;s definitely not something expected for plugin developers, // and registering UpdateNodeTaint event is the only mitigation for now. // // So, here registers UpdateNodeTaint event for plugins that has NodeAdded event, but don\u0026#39;t have UpdateNodeTaint event. // It has a bad impact for the requeuing efficiency though, a lot better than some Pods being stuch in the // unschedulable pod pool. // This behavior will be removed when we remove the preCheck feature. // See: https://github.com/kubernetes/kubernetes/issues/110175 queueingHintMap[framework.ClusterEvent{Resource: framework.Node, ActionType: framework.UpdateNodeTaint}] = append(queueingHintMap[framework.ClusterEvent{Resource: framework.Node, ActionType: framework.UpdateNodeTaint}], \u0026amp;internalqueue.QueueingHintFunction{ PluginName: e.Name(), QueueingHintFn: defaultQueueingHintFn, }, ) } } return queueingHintMap } //这段Go代码是一个函数，它根据传入的事件列表和注册选项生成一个队列提示映射。 //该映射将事件与相应的队列提示函数进行关联。 //如果注册了NodeAdded事件但未注册UpdateNodeTaint事件，则会临时修复一个Kubernetes问题， //通过为这些插件注册UpdateNodeTaint事件来避免某些Pod被卡在不可调度的Pod池中。 //该函数返回生成的队列提示映射。 // Run begins watching and scheduling. It starts scheduling and blocked until the context is done. func (sched *Scheduler) Run(ctx context.Context) { logger := klog.FromContext(ctx) sched.SchedulingQueue.Run(logger) // We need to start scheduleOne loop in a dedicated goroutine, // because scheduleOne function hangs on getting the next item // from the SchedulingQueue. // If there are no new pods to schedule, it will be hanging there // and if done in this goroutine it will be blocking closing // SchedulingQueue, in effect causing a deadlock on shutdown. go wait.UntilWithContext(ctx, sched.ScheduleOne, 0) \u0026lt;-ctx.Done() sched.SchedulingQueue.Close() // If the plugins satisfy the io.Closer interface, they are closed. err := sched.Profiles.Close() if err != nil { logger.Error(err, \u0026#34;Failed to close plugins\u0026#34;) } } //该函数是Scheduler类型的Run方法，用于开始监控和调度。 //它启动调度并阻塞，直到上下文完成。具体步骤如下： //1. 从上下文中获取日志记录器logger。 //2. 调用SchedulingQueue的Run方法，开始监控调度队列。 //3. 在一个独立的goroutine中启动scheduleOne循环，因为scheduleOne函数会在从SchedulingQueue获取下一个项目时挂起。 //如果没有任何新Pod需要调度，它将一直挂起。 //如果在当前goroutine中执行，它将阻塞关闭SchedulingQueue，从而在关闭时导致死锁。 //4. 等待上下文完成。 //5. 调用SchedulingQueue的Close方法，关闭调度队列。 //6. 尝试关闭满足io.Closer接口的插件。如果关闭失败，记录错误日志。 // NewInformerFactory creates a SharedInformerFactory and initializes a scheduler specific // in-place podInformer. func NewInformerFactory(cs clientset.Interface, resyncPeriod time.Duration) informers.SharedInformerFactory { informerFactory := informers.NewSharedInformerFactory(cs, resyncPeriod) informerFactory.InformerFor(\u0026amp;v1.Pod{}, newPodInformer) return informerFactory } //该函数创建一个SharedInformerFactory，并为特定的调度程序初始化一个就地podInformer。 //函数接收一个clientset.Interface类型和一个时间周期作为参数，返回一个SharedInformerFactory。 //内部通过调用informers.NewSharedInformerFactory创建一个新的SharedInformerFactory实例， //并使用InformerFor方法为v1.Pod类型创建一个新的podInformer。 //最后返回创建的SharedInformerFactory实例。 func buildExtenders(logger klog.Logger, extenders []schedulerapi.Extender, profiles []schedulerapi.KubeSchedulerProfile) ([]framework.Extender, error) { var fExtenders []framework.Extender if len(extenders) == 0 { return nil, nil } var ignoredExtendedResources []string var ignorableExtenders []framework.Extender for i := range extenders { logger.V(2).Info(\u0026#34;Creating extender\u0026#34;, \u0026#34;extender\u0026#34;, extenders[i]) extender, err := NewHTTPExtender(\u0026amp;extenders[i]) if err != nil { return nil, err } if !extender.IsIgnorable() { fExtenders = append(fExtenders, extender) } else { ignorableExtenders = append(ignorableExtenders, extender) } for _, r := range extenders[i].ManagedResources { if r.IgnoredByScheduler { ignoredExtendedResources = append(ignoredExtendedResources, r.Name) } } } // place ignorable extenders to the tail of extenders fExtenders = append(fExtenders, ignorableExtenders...) //该函数主要负责根据传入的extenders参数构建一个framework.Extender类型的切片fExtenders。具体流程如下： //1. 首先判断extenders切片的长度是否为0，如果是则直接返回nil和nil。 //2. 初始化两个切片ignoredExtendedResources和ignorableExtenders，分别用于存储被忽略的扩展资源名称和可忽略的扩展器。 //3. 遍历extenders切片，对每个扩展器进行如下操作： //- 使用klog.Logger记录日志信息。 //- 调用NewHTTPExtender函数创建一个新的HTTPExtender对象。 //- 如果该扩展器不可忽略，则将其添加到fExtenders切片中。 //- 如果该扩展器可忽略，则将其添加到ignorableExtenders切片中。 //- 遍历该扩展器的ManagedResources字段，将被忽略的资源名称添加到ignoredExtendedResources切片中。 //4. 将ignorableExtenders切片追加到fExtenders切片的末尾。 //最终，函数返回构建好的fExtenders切片和可能出现的错误。 // If there are any extended resources found from the Extenders, append them to the pluginConfig for each profile. // This should only have an effect on ComponentConfig, where it is possible to configure Extenders and // plugin args (and in which case the extender ignored resources take precedence). if len(ignoredExtendedResources) == 0 { return fExtenders, nil } for i := range profiles { prof := \u0026amp;profiles[i] var found = false for k := range prof.PluginConfig { if prof.PluginConfig[k].Name == noderesources.Name { // Update the existing args pc := \u0026amp;prof.PluginConfig[k] args, ok := pc.Args.(*schedulerapi.NodeResourcesFitArgs) if !ok { return nil, fmt.Errorf(\u0026#34;want args to be of type NodeResourcesFitArgs, got %T\u0026#34;, pc.Args) } args.IgnoredResources = ignoredExtendedResources found = true break } } if !found { return nil, fmt.Errorf(\u0026#34;can\u0026#39;t find NodeResourcesFitArgs in plugin config\u0026#34;) } } return fExtenders, nil } //该函数首先检查ignoredExtendedResources是否为空，如果为空则直接返回fExtenders和nil。 //如果不为空，则遍历profiles中的每一个元素，然后遍历该元素的PluginConfig。 //当找到PluginConfig中的Name等于noderesources.Name时，将ignoredExtendedResources赋值给Args的IgnoredResources字段， //并将found设为true。 如果遍历完所有PluginConfig后仍未找到符合条件的元素，则返回nil和错误信息。 //最后返回fExtenders和nil。 type FailureHandlerFn func(ctx context.Context, fwk framework.Framework, podInfo *framework.QueuedPodInfo, status *framework.Status, nominatingInfo *framework.NominatingInfo, start time.Time) func unionedGVKs(queueingHintsPerProfile internalqueue.QueueingHintMapPerProfile) map[framework.GVK]framework.ActionType { gvkMap := make(map[framework.GVK]framework.ActionType) for _, queueingHints := range queueingHintsPerProfile { for evt := range queueingHints { if _, ok := gvkMap[evt.Resource]; ok { gvkMap[evt.Resource] |= evt.ActionType } else { gvkMap[evt.Resource] = evt.ActionType } } } return gvkMap } //该函数用于将一个internalqueue.QueueingHintMapPerProfile类型的参数转换为一个map[framework.GVK]framework.ActionType类型的结果。 //具体实现过程为：遍历输入参数中的每个queueingHints，再遍历queueingHints中的每个事件evt，将evt.Resource作为键， //evt.ActionType作为值存入gvkMap中。如果gvkMap中已经存在该键，则将该键对应的值与evt.ActionType进行按位或运算后再存入gvkMap中。 //最后返回gvkMap作为结果。 // newPodInformer creates a shared index informer that returns only non-terminal pods. // The PodInformer allows indexers to be added, but note that only non-conflict indexers are allowed. func newPodInformer(cs clientset.Interface, resyncPeriod time.Duration) cache.SharedIndexInformer { selector := fmt.Sprintf(\u0026#34;status.phase!=%v,status.phase!=%v\u0026#34;, v1.PodSucceeded, v1.PodFailed) tweakListOptions := func(options *metav1.ListOptions) { options.FieldSelector = selector } informer := coreinformers.NewFilteredPodInformer(cs, metav1.NamespaceAll, resyncPeriod, cache.Indexers{}, tweakListOptions) // Dropping `.metadata.managedFields` to improve memory usage. // The Extract workflow (i.e. `ExtractPod`) should be unused. trim := func(obj interface{}) (interface{}, error) { if accessor, err := meta.Accessor(obj); err == nil { accessor.SetManagedFields(nil) } return obj, nil } informer.SetTransform(trim) return informer } //该函数创建一个共享索引 informer，用于返回非终止状态的 pod。 //它通过设置筛选条件，使得 informer 只能获取到状态不是 \u0026#34;Succeeded\u0026#34; 或 \u0026#34;Failed\u0026#34; 的 pod。 //此外，函数还通过设置 transform 函数来删除 pod 的 .metadata.managedFields 字段，以减少内存使用。 //该函数返回一个经过筛选和转换的 pod informer 实例。 "},{"id":105,"href":"/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Brecreatego-%E9%87%8D%E5%BB%BA-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-recreatego-zhong-jian-yuan-ma-jie-du/","title":"K8S控制器之recreate.go 重建 源码解读 2024-04-09 11:31:49.855","section":"Docs","content":"/* Copyright 2016 The Kubernetes Authors. Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \u0026#34;AS IS\u0026#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */ package deployment import ( \u0026#34;context\u0026#34; apps \u0026#34;k8s.io/api/apps/v1\u0026#34; v1 \u0026#34;k8s.io/api/core/v1\u0026#34; \u0026#34;k8s.io/apimachinery/pkg/types\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/controller\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/controller/deployment/util\u0026#34; ) // rolloutRecreate implements the logic for recreating a replica set. func (dc *DeploymentController) rolloutRecreate(ctx context.Context, d *apps.Deployment, rsList []*apps.ReplicaSet, podMap map[types.UID][]*v1.Pod) error { // Don\u0026#39;t create a new RS if not already existed, so that we avoid scaling up before scaling down. newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(ctx, d, rsList, false) if err != nil { return err } allRSs := append(oldRSs, newRS) activeOldRSs := controller.FilterActiveReplicaSets(oldRSs) // scale down old replica sets. scaledDown, err := dc.scaleDownOldReplicaSetsForRecreate(ctx, activeOldRSs, d) if err != nil { return err } if scaledDown { // Update DeploymentStatus. return dc.syncRolloutStatus(ctx, allRSs, newRS, d) } // Do not process a deployment when it has old pods running. if oldPodsRunning(newRS, oldRSs, podMap) { return dc.syncRolloutStatus(ctx, allRSs, newRS, d) } // If we need to create a new RS, create it now. if newRS == nil { newRS, oldRSs, err = dc.getAllReplicaSetsAndSyncRevision(ctx, d, rsList, true) if err != nil { return err } allRSs = append(oldRSs, newRS) } // scale up new replica set. if _, err := dc.scaleUpNewReplicaSetForRecreate(ctx, newRS, d); err != nil { return err } if util.DeploymentComplete(d, \u0026amp;d.Status) { if err := dc.cleanupDeployment(ctx, oldRSs, d); err != nil { return err } } // Sync deployment status. return dc.syncRolloutStatus(ctx, allRSs, newRS, d) } //该函数实现了Deployment控制器中重新创建副本集的逻辑。具体步骤如下： //1. 调用getAllReplicaSetsAndSyncRevision函数获取当前所有的副本集，并同步修订版本。如果已有新的副本集，则不再创建新的副本集，以避免先扩大规模再缩小规模。 //2. 过滤出活动的旧副本集。 //3. 调用scaleDownOldReplicaSetsForRecreate函数缩小旧副本集的规模。如果成功缩小规模，则更新Deployment状态。 //4. 如果存在正在运行的旧Pods，则只更新Deployment状态。 //5. 如果需要创建新的副本集，则调用getAllReplicaSetsAndSyncRevision函数创建新的副本集，并更新allRSs。 //6. 调用scaleUpNewReplicaSetForRecreate函数扩大新副本集的规模。 //7. 如果Deployment已完成，则调用cleanupDeployment函数清理旧的副本集。 //8. 最后，调用syncRolloutStatus函数更新Deployment状态。 // scaleDownOldReplicaSetsForRecreate scales down old replica sets when deployment strategy is \u0026#34;Recreate\u0026#34;. func (dc *DeploymentController) scaleDownOldReplicaSetsForRecreate(ctx context.Context, oldRSs []*apps.ReplicaSet, deployment *apps.Deployment) (bool, error) { scaled := false for i := range oldRSs { rs := oldRSs[i] // Scaling not required. if *(rs.Spec.Replicas) == 0 { continue } scaledRS, updatedRS, err := dc.scaleReplicaSetAndRecordEvent(ctx, rs, 0, deployment) if err != nil { return false, err } if scaledRS { oldRSs[i] = updatedRS scaled = true } } return scaled, nil } //该函数用于在Recreate部署策略下，将旧的ReplicaSet缩容为0。 //它遍历旧的ReplicaSet列表，对每个非0副本数的ReplicaSet调用scaleReplicaSetAndRecordEvent函数进行缩容，并记录事件。 //如果有任何一个ReplicaSet成功缩容，则函数返回true，否则返回false。 // oldPodsRunning returns whether there are old pods running or any of the old ReplicaSets thinks that it runs pods. func oldPodsRunning(newRS *apps.ReplicaSet, oldRSs []*apps.ReplicaSet, podMap map[types.UID][]*v1.Pod) bool { if oldPods := util.GetActualReplicaCountForReplicaSets(oldRSs); oldPods \u0026gt; 0 { return true } for rsUID, podList := range podMap { // If the pods belong to the new ReplicaSet, ignore. if newRS != nil \u0026amp;\u0026amp; newRS.UID == rsUID { continue } for _, pod := range podList { switch pod.Status.Phase { case v1.PodFailed, v1.PodSucceeded: // Don\u0026#39;t count pods in terminal state. continue case v1.PodUnknown: // v1.PodUnknown is a deprecated status. // This logic is kept for backward compatibility. // This used to happen in situation like when the node is temporarily disconnected from the cluster. // If we can\u0026#39;t be sure that the pod is not running, we have to count it. return true default: // Pod is not in terminal phase. return true } } } return false } //该函数用于判断是否有旧的Pod正在运行或任何一个旧的ReplicaSet认为它正在运行Pod。 //函数首先通过util.GetActualReplicaCountForReplicaSets函数获取旧的ReplicaSet的实际副本数，如果大于0，则返回true。 //然后，函数遍历podMap，检查每个Pod的状态，如果Pod属于新的ReplicaSet，则忽略； //如果Pod处于失败或成功状态，则继续；如果Pod状态未知，则为了向后兼容，返回true； //否则，返回true。 //如果遍历结束后没有返回true，则返回false。 // scaleUpNewReplicaSetForRecreate scales up new replica set when deployment strategy is \u0026#34;Recreate\u0026#34;. func (dc *DeploymentController) scaleUpNewReplicaSetForRecreate(ctx context.Context, newRS *apps.ReplicaSet, deployment *apps.Deployment) (bool, error) { scaled, _, err := dc.scaleReplicaSetAndRecordEvent(ctx, newRS, *(deployment.Spec.Replicas), deployment) return scaled, err } //该函数用于在Recreate策略下，增加新副本集的副本数。 //函数内部调用了scaleReplicaSetAndRecordEvent函数，传入新副本集、部署的期望副本数和部署本身，该函数会尝试增加新副本集的副本数，并记录相应的事件。 //函数返回一个布尔值和一个错误，表示是否成功增加副本数以及是否发生了错误。 "},{"id":106,"href":"/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Brollbackgo-%E5%9B%9E%E6%BB%9A-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-rollbackgo-hui-gun-yuan-ma-jie-du/","title":"K8S控制器之rollback.go 回滚 源码解读 2024-04-09 11:32:26.587","section":"Docs","content":"/* Copyright 2016 The Kubernetes Authors. Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \u0026#34;AS IS\u0026#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */ package deployment import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;strconv\u0026#34; apps \u0026#34;k8s.io/api/apps/v1\u0026#34; v1 \u0026#34;k8s.io/api/core/v1\u0026#34; extensions \u0026#34;k8s.io/api/extensions/v1beta1\u0026#34; metav1 \u0026#34;k8s.io/apimachinery/pkg/apis/meta/v1\u0026#34; deploymentutil \u0026#34;k8s.io/kubernetes/pkg/controller/deployment/util\u0026#34; ) // rollback the deployment to the specified revision. In any case cleanup the rollback spec. func (dc *DeploymentController) rollback(ctx context.Context, d *apps.Deployment, rsList []*apps.ReplicaSet) error { logger := klog.FromContext(ctx) newRS, allOldRSs, err := dc.getAllReplicaSetsAndSyncRevision(ctx, d, rsList, true) if err != nil { return err } //该函数用于回滚部署到指定的修订版本，并在任何情况下清理回滚规范。 //函数首先从上下文中获取日志记录器， //然后通过调用getAllReplicaSetsAndSyncRevision方法获取新的副本集、所有旧的副本集，并同步修订版本。 //如果获取过程中出现错误，则返回该错误。 allRSs := append(allOldRSs, newRS) rollbackTo := getRollbackTo(d) // If rollback revision is 0, rollback to the last revision if rollbackTo.Revision == 0 { if rollbackTo.Revision = deploymentutil.LastRevision(logger, allRSs); rollbackTo.Revision == 0 { // If we still can\u0026#39;t find the last revision, gives up rollback dc.emitRollbackWarningEvent(d, deploymentutil.RollbackRevisionNotFound, \u0026#34;Unable to find last revision.\u0026#34;) // Gives up rollback return dc.updateDeploymentAndClearRollbackTo(ctx, d) } } for _, rs := range allRSs { v, err := deploymentutil.Revision(rs) if err != nil { logger.V(4).Info(\u0026#34;Unable to extract revision from deployment\u0026#39;s replica set\u0026#34;, \u0026#34;replicaSet\u0026#34;, klog.KObj(rs), \u0026#34;err\u0026#34;, err) continue } if v == rollbackTo.Revision { logger.V(4).Info(\u0026#34;Found replica set with desired revision\u0026#34;, \u0026#34;replicaSet\u0026#34;, klog.KObj(rs), \u0026#34;revision\u0026#34;, v) // rollback by copying podTemplate.Spec from the replica set // revision number will be incremented during the next getAllReplicaSetsAndSyncRevision call // no-op if the spec matches current deployment\u0026#39;s podTemplate.Spec performedRollback, err := dc.rollbackToTemplate(ctx, d, rs) if performedRollback \u0026amp;\u0026amp; err == nil { dc.emitRollbackNormalEvent(d, fmt.Sprintf(\u0026#34;Rolled back deployment %q to revision %d\u0026#34;, d.Name, rollbackTo.Revision)) } return err //该函数是一个Go语言函数，它实现了回滚（rollback）部署的逻辑。 //函数首先将一个新副本集（newRS）添加到所有旧副本集（allOldRSs）中，然后根据提供的参数d获取回滚到的修订版本号（revision number） 。 //如果回滚到的修订版本号为0，则将其回滚到最后一个修订版本号。 //如果无法找到最后一个修订版本号，则放弃回滚并发出警告事件。 //然后，函数遍历所有副本集，并尝试从每个副本集中提取修订版本号。 //如果找到与回滚到的修订版本号匹配的副本集，则通过复制该副本集的podTemplate.Spec来执行回滚操作。 //如果回滚操作成功，则发出正常事件。函数的最后返回值是可能产生的错误。 } } dc.emitRollbackWarningEvent(d, deploymentutil.RollbackRevisionNotFound, \u0026#34;Unable to find the revision to rollback to.\u0026#34;) // Gives up rollback return dc.updateDeploymentAndClearRollbackTo(ctx, d) } //该函数是DeploymentController的一个方法，用于回滚部署到指定的修订版本。 //它首先尝试获取所有复制集并同步修订版本，然后根据回滚到的修订版本执行回滚操作。 //如果回滚修订版本为0，则回滚到最后一个修订版本。 //它遍历所有复制集，找到具有所需修订版本的复制集，并从该复制集复制podTemplate.Spec来执行回滚。 //如果找不到要回滚的修订版本，则放弃回滚并清除回滚信息。 // rollbackToTemplate compares the templates of the provided deployment and replica set and // updates the deployment with the replica set template in case they are different. It also // cleans up the rollback spec so subsequent requeues of the deployment won\u0026#39;t end up in here. func (dc *DeploymentController) rollbackToTemplate(ctx context.Context, d *apps.Deployment, rs *apps.ReplicaSet) (bool, error) { logger := klog.FromContext(ctx) performedRollback := false if !deploymentutil.EqualIgnoreHash(\u0026amp;d.Spec.Template, \u0026amp;rs.Spec.Template) { logger.V(4).Info(\u0026#34;Rolling back deployment to old template spec\u0026#34;, \u0026#34;deployment\u0026#34;, klog.KObj(d), \u0026#34;templateSpec\u0026#34;, rs.Spec.Template.Spec) deploymentutil.SetFromReplicaSetTemplate(d, rs.Spec.Template) // set RS (the old RS we\u0026#39;ll rolling back to) annotations back to the deployment; // otherwise, the deployment\u0026#39;s current annotations (should be the same as current new RS) will be copied to the RS after the rollback. // // For example, // A Deployment has old RS1 with annotation {change-cause:create}, and new RS2 {change-cause:edit}. // Note that both annotations are copied from Deployment, and the Deployment should be annotated {change-cause:edit} as well. // Now, rollback Deployment to RS1, we should update Deployment\u0026#39;s pod-template and also copy annotation from RS1. // Deployment is now annotated {change-cause:create}, and we have new RS1 {change-cause:create}, old RS2 {change-cause:edit}. // // If we don\u0026#39;t copy the annotations back from RS to deployment on rollback, the Deployment will stay as {change-cause:edit}, // and new RS1 becomes {change-cause:edit} (copied from deployment after rollback), old RS2 {change-cause:edit}, which is not correct. deploymentutil.SetDeploymentAnnotationsTo(d, rs) performedRollback = true } else { logger.V(4).Info(\u0026#34;Rolling back to a revision that contains the same template as current deployment, skipping rollback...\u0026#34;, \u0026#34;deployment\u0026#34;, klog.KObj(d)) eventMsg := fmt.Sprintf(\u0026#34;The rollback revision contains the same template as current deployment %q\u0026#34;, d.Name) dc.emitRollbackWarningEvent(d, deploymentutil.RollbackTemplateUnchanged, eventMsg) } return performedRollback, dc.updateDeploymentAndClearRollbackTo(ctx, d) } //该函数用于将部署（deployment）的模板（template）与复制集（replica set）的模板进行比较， //并在两者不相同的情况下，将部署的模板更新为复制集的模板。 //同时，它还会清理回滚规格，以避免后续重新排队导致回滚。 //函数返回一个布尔值和一个错误，表示是否执行了回滚操作和操作是否出错。 //具体步骤如下： //1. 获取日志记录器。 //2. 初始化执行了回滚操作的标志为false。 //3. 如果部署的模板与复制集的模板不相同，则执行回滚操作： //- 记录回滚操作的日志。 //- 将部署的模板设置为复制集的模板。 //- 将复制集的注解设置回部署；否则，部署的当前注解（应与当前新复制集相同）将被复制到回滚后的复制集。 //4. 如果部署的模板与复制集的模板相同，则记录跳过回滚的日志，并发出回滚警告事件。 //5. 返回执行了回滚操作的标志和更新部署并清除回滚规格的结果。 func (dc *DeploymentController) emitRollbackWarningEvent(d *apps.Deployment, reason, message string) { dc.eventRecorder.Eventf(d, v1.EventTypeWarning, reason, message) } func (dc *DeploymentController) emitRollbackNormalEvent(d *apps.Deployment, message string) { dc.eventRecorder.Eventf(d, v1.EventTypeNormal, deploymentutil.RollbackDone, message) } // updateDeploymentAndClearRollbackTo sets .spec.rollbackTo to nil and update the input deployment // It is assumed that the caller will have updated the deployment template appropriately (in case // we want to rollback). func (dc *DeploymentController) updateDeploymentAndClearRollbackTo(ctx context.Context, d *apps.Deployment) error { logger := klog.FromContext(ctx) logger.V(4).Info(\u0026#34;Cleans up rollbackTo of deployment\u0026#34;, \u0026#34;deployment\u0026#34;, klog.KObj(d)) setRollbackTo(d, nil) _, err := dc.client.AppsV1().Deployments(d.Namespace).Update(ctx, d, metav1.UpdateOptions{}) return err } //该函数是Go语言编写的，用于更新输入的Deployment对象，并将其.spec.rollbackTo设置为nil。 //函数首先从上下文中获取日志记录器，然后使用日志记录器记录清理Deployment的rollbackTo的信息。 //接下来，它调用setRollbackTo函数将rollbackTo设置为nil。 //最后，它使用client来更新Deployment对象，并返回可能的错误。 // TODO: Remove this when extensions/v1beta1 and apps/v1beta1 Deployment are dropped. func getRollbackTo(d *apps.Deployment) *extensions.RollbackConfig { // Extract the annotation used for round-tripping the deprecated RollbackTo field. revision := d.Annotations[apps.DeprecatedRollbackTo] if revision == \u0026#34;\u0026#34; { return nil } revision64, err := strconv.ParseInt(revision, 10, 64) if err != nil { // If it\u0026#39;s invalid, ignore it. return nil } return \u0026amp;extensions.RollbackConfig{ Revision: revision64, } } //该函数用于获取一个代表回滚配置的RollbackConfig对象。 //它从传入的Deployment对象的注解中提取回滚版本号（revision），并将其转换为RollbackConfig对象。 //如果注解不存在或格式无效，则返回nil。 // TODO: Remove this when extensions/v1beta1 and apps/v1beta1 Deployment are dropped. func setRollbackTo(d *apps.Deployment, rollbackTo *extensions.RollbackConfig) { if rollbackTo == nil { delete(d.Annotations, apps.DeprecatedRollbackTo) return } if d.Annotations == nil { d.Annotations = make(map[string]string) } d.Annotations[apps.DeprecatedRollbackTo] = strconv.FormatInt(rollbackTo.Revision, 10) } //该函数用于设置Go语言中的Deployment对象的回滚配置。 //它根据传入的rollbackTo参数，将回滚的版本信息保存在Deployment的注解(annotations)中。 //如果rollbackTo为nil，则删除该注解； //否则，创建或更新该注解，值为回滚的修订版本号。 "},{"id":107,"href":"/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Bstateful_pod_controlgo%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-statefulpodcontrolgo-yuan-ma-jie-du/","title":"K8S控制器之stateful_pod_control.go源码解读 2024-04-10 09:42:50.078","section":"Docs","content":"/* Copyright 2016 The Kubernetes Authors. Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \u0026#34;AS IS\u0026#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */ package statefulset import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;strings\u0026#34; apps \u0026#34;k8s.io/api/apps/v1\u0026#34; v1 \u0026#34;k8s.io/api/core/v1\u0026#34; apierrors \u0026#34;k8s.io/apimachinery/pkg/api/errors\u0026#34; metav1 \u0026#34;k8s.io/apimachinery/pkg/apis/meta/v1\u0026#34; errorutils \u0026#34;k8s.io/apimachinery/pkg/util/errors\u0026#34; utilruntime \u0026#34;k8s.io/apimachinery/pkg/util/runtime\u0026#34; utilfeature \u0026#34;k8s.io/apiserver/pkg/util/feature\u0026#34; clientset \u0026#34;k8s.io/client-go/kubernetes\u0026#34; corelisters \u0026#34;k8s.io/client-go/listers/core/v1\u0026#34; \u0026#34;k8s.io/client-go/tools/record\u0026#34; \u0026#34;k8s.io/client-go/util/retry\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/features\u0026#34; ) // StatefulPodControlObjectManager abstracts the manipulation of Pods and PVCs. The real controller implements this // with a clientset for writes and listers for reads; for tests we provide stubs. type StatefulPodControlObjectManager interface { CreatePod(ctx context.Context, pod *v1.Pod) error GetPod(namespace, podName string) (*v1.Pod, error) UpdatePod(pod *v1.Pod) error DeletePod(pod *v1.Pod) error CreateClaim(claim *v1.PersistentVolumeClaim) error GetClaim(namespace, claimName string) (*v1.PersistentVolumeClaim, error) UpdateClaim(claim *v1.PersistentVolumeClaim) error } // 这是一个StatefulPodControlObjectManager接口，定义了对有状态Pod和持久卷声明的操作方法，包括创建、获取、更新和删除Pod以及创建、获取、更新持久卷声明。 // StatefulPodControl defines the interface that StatefulSetController uses to create, update, and delete Pods, // and to update the Status of a StatefulSet. It follows the design paradigms used for PodControl, but its // implementation provides for PVC creation, ordered Pod creation, ordered Pod termination, and Pod identity enforcement. // Manipulation of objects is provided through objectMgr, which allows the k8s API to be mocked out for testing. type StatefulPodControl struct { objectMgr StatefulPodControlObjectManager recorder record.EventRecorder } //该代码定义了一个名为StatefulPodControl的结构体，它是一个接口，用于创建、更新和删除Pods，以及更新StatefulSet的状态。 //它采用了与PodControl相同的设计模式，但其实现提供了PVC创建、有序Pod创建、有序Pod终止和Pod身份强制执行等功能。 //通过objectMgr对象提供对对象的操作，允许在测试中模拟k8s API。 // NewStatefulPodControl constructs a StatefulPodControl using a realStatefulPodControlObjectManager with the given // clientset, listers and EventRecorder. func NewStatefulPodControl( client clientset.Interface, podLister corelisters.PodLister, claimLister corelisters.PersistentVolumeClaimLister, recorder record.EventRecorder, ) *StatefulPodControl { return \u0026amp;StatefulPodControl{\u0026amp;realStatefulPodControlObjectManager{client, podLister, claimLister}, recorder} } // 该函数用于构造一个StatefulPodControl对象，使用给定的clientset、listers和EventRecorder。 // 函数通过传入的参数创建一个realStatefulPodControlObjectManager对象，并将其与传入的EventRecorder对象一起封装到StatefulPodControl对象中，最后返回该对象。 // NewStatefulPodControlFromManager creates a StatefulPodControl using the given StatefulPodControlObjectManager and recorder. func NewStatefulPodControlFromManager(om StatefulPodControlObjectManager, recorder record.EventRecorder) *StatefulPodControl { return \u0026amp;StatefulPodControl{om, recorder} } // 该函数使用给定的StatefulPodControlObjectManager和record.EventRecorder创建一个StatefulPodControl，并返回其指针。 // realStatefulPodControlObjectManager uses a clientset.Interface and listers. type realStatefulPodControlObjectManager struct { client clientset.Interface podLister corelisters.PodLister claimLister corelisters.PersistentVolumeClaimLister } // 该代码定义了一个名为realStatefulPodControlObjectManager的结构体，它使用了clientset.Interface和listers。 // 结构体中有三个字段：client、podLister和claimLister，分别用于存储客户端接口、Pod列表和持久卷申领列表。 func (om *realStatefulPodControlObjectManager) CreatePod(ctx context.Context, pod *v1.Pod) error { _, err := om.client.CoreV1().Pods(pod.Namespace).Create(ctx, pod, metav1.CreateOptions{}) return err } // 该函数是一个Go语言函数，它使用了上下文（Context）来控制请求的超时和取消。 // 该函数的功能是在指定的命名空间中创建一个Pod，并返回创建操作的结果（成功或错误）。 // 具体来说，该函数的实现步骤如下： // 1. 使用om.client获取CoreV1接口，该接口提供了对Kubernetes集群中Pod资源的操作方法。 // 2. 调用Create方法，在指定的命名空间中创建Pod，并传入上下文（Context）和Pod对象。 // 3. 返回创建操作的结果，即错误信息（Error）。 // 需要注意的是，该函数中使用了context.Context参数，它允许调用者控制请求的超时和取消。在实际应用中，这非常有用，比如在处理大量请求或需要及时响应的场景中。 func (om *realStatefulPodControlObjectManager) GetPod(namespace, podName string) (*v1.Pod, error) { return om.podLister.Pods(namespace).Get(podName) } // 该函数是一个Go语言的方法，定义在一个名为realStatefulPodControlObjectManager的结构体类型上。该方法的功能是在指定的命名空间 // (namespace)中获取指定名称(podName)的Pod对象，并返回该Pod对象及其可能发生的错误。 具体实现上， // 该方法通过调用om.podLister.Pods(namespace)来获取指定命名空间下的所有Pods的列表器， // 然后进一步调用Get(podName)方法来获取指定名称的Pod对象。如果获取成功，则将该Pod对象返回； // 如果获取失败，则将发生的错误返回。 func (om *realStatefulPodControlObjectManager) UpdatePod(pod *v1.Pod) error { _, err := om.client.CoreV1().Pods(pod.Namespace).Update(context.TODO(), pod, metav1.UpdateOptions{}) return err } // 该函数用于更新指定命名空间下的Pod。函数接收一个指向v1.Pod类型的指针作为参数，通过调用om.client.CoreV1().Pods // (pod.Namespace).Update()方法，将该Pod对象更新至Kubernetes集群中。若更新成功，则返回nil；若更新失败， // 则返回相应的错误信息。 func (om *realStatefulPodControlObjectManager) DeletePod(pod *v1.Pod) error { return om.client.CoreV1().Pods(pod.Namespace).Delete(context.TODO(), pod.Name, metav1.DeleteOptions{}) } func (om *realStatefulPodControlObjectManager) CreateClaim(claim *v1.PersistentVolumeClaim) error { _, err := om.client.CoreV1().PersistentVolumeClaims(claim.Namespace).Create(context.TODO(), claim, metav1.CreateOptions{}) return err } // 该函数用于删除指定命名空间下的Pod。函数接收一个指向v1.Pod类型的指针作为参数，通过调用om.client.CoreV1().Pods // (pod.Namespace).Delete()方法，根据传入的Pod的Namespace和Name属性，在Kubernetes集群中删除该Pod实例。 // 若删除成功，则返回nil；否则，返回相应的错误信息。 func (om *realStatefulPodControlObjectManager) GetClaim(namespace, claimName string) (*v1.PersistentVolumeClaim, error) { return om.claimLister.PersistentVolumeClaims(namespace).Get(claimName) } //该函数用于获取指定命名空间下名为claimName的PersistentVolumeClaim资源。函数接收两个字符串参数：namespace（命名空间）和claimName //（PersistentVolumeClaim名称）。它通过调用om.claimLister.PersistentVolumeClaims(namespace).Get(claimName)从缓存列表中获取指定的 //PersistentVolumeClaim对象，并将其以*v1.PersistentVolumeClaim类型返回。如果获取成功，则返回PersistentVolumeClaim实例及其nil错误； //如果未能找到对应资源，则返回nil及可能的错误信息。 func (om *realStatefulPodControlObjectManager) UpdateClaim(claim *v1.PersistentVolumeClaim) error { _, err := om.client.CoreV1().PersistentVolumeClaims(claim.Namespace).Update(context.TODO(), claim, metav1.UpdateOptions{}) return err } //该函数用于更新指定命名空间下的PersistentVolumeClaim资源。函数接收一个指向v1.PersistentVolumeClaim类型的指针作为参数。 //通过调用om.client.CoreV1().PersistentVolumeClaims(claim.Namespace).Update()方法，根据传入的PersistentVolumeClaim对象的Namespace属性， //在Kubernetes集群中更新该PersistentVolumeClaim实例。若更新操作成功，则返回nil；若出现错误，则返回相应的错误信息。 func (spc *StatefulPodControl) CreateStatefulPod(ctx context.Context, set *apps.StatefulSet, pod *v1.Pod) error { // Create the Pod\u0026#39;s PVCs prior to creating the Pod if err := spc.createPersistentVolumeClaims(set, pod); err != nil { spc.recordPodEvent(\u0026#34;create\u0026#34;, set, pod, err) return err } // If we created the PVCs attempt to create the Pod err := spc.objectMgr.CreatePod(ctx, pod) // sink already exists errors if apierrors.IsAlreadyExists(err) { return err } if utilfeature.DefaultFeatureGate.Enabled(features.StatefulSetAutoDeletePVC) { // Set PVC policy as much as is possible at this point. if err := spc.UpdatePodClaimForRetentionPolicy(ctx, set, pod); err != nil { spc.recordPodEvent(\u0026#34;update\u0026#34;, set, pod, err) return err } } spc.recordPodEvent(\u0026#34;create\u0026#34;, set, pod, err) return err } // 该函数用于创建有状态的Pod。在创建Pod之前，它会先创建Pod所需的PVC（持久卷声明）。如果创建PVC时出现错误，则会记录Pod事件并返回错误。 // 接下来，它会尝试创建Pod，如果Pod已存在，则直接返回错误。如果启用了StatefulSetAutoDeletePVC功能，则会更新Pod的PVC保留策略。 // 最后，它会记录Pod的创建事件并返回可能的错误 func (spc *StatefulPodControl) UpdateStatefulPod(ctx context.Context, set *apps.StatefulSet, pod *v1.Pod) error { attemptedUpdate := false err := retry.RetryOnConflict(retry.DefaultBackoff, func() error { // assume the Pod is consistent consistent := true // if the Pod does not conform to its identity, update the identity and dirty the Pod if !identityMatches(set, pod) { updateIdentity(set, pod) consistent = false } //该函数是一个Go语言函数，它用于更新一个有状态的Pod（StatefulPod）的状态。函数的参数包括一个上下文对象ctx、一个StatefulSet对象set和一个Pod对象pod。 //函数通过使用retry.RetryOnConflict方法，在遇到冲突时进行重试，以更新Pod的状态。在重试的过程中， //函数会检查Pod是否符合其身份要求和StatefulSet的存储要求。如果不符合，函数会相应地更新Pod的身份和PVCs，并标记Pod为“dirty”。 //如果在更新过程中出现错误，则会返回错误信息。 // if the Pod does not conform to the StatefulSet\u0026#39;s storage requirements, update the Pod\u0026#39;s PVC\u0026#39;s, // dirty the Pod, and create any missing PVCs if !storageMatches(set, pod) { updateStorage(set, pod) consistent = false if err := spc.createPersistentVolumeClaims(set, pod); err != nil { spc.recordPodEvent(\u0026#34;update\u0026#34;, set, pod, err) return err } } //该函数主要检查Pod是否符合StatefulSet的存储要求，如果不符合，则更新Pod的PVCs，标记Pod为dirty，并创建任何缺失的PVCs。 //具体流程如下： 1. 检查Pod是否符合StatefulSet的存储要求，调用storageMatches(set, pod)函数。 //2. 如果Pod不符合存储要求，则执行以下操作： - 调用updateStorage(set, pod)函数更新Pod的PVCs。 - 将consistent标记为false。 - //调用spc.createPersistentVolumeClaims(set, pod)函数创建任何缺失的PVCs，如果创建失败，则记录Pod事件并返回错误。 //3. 如果Pod符合存储要求，则继续后续流程。 if utilfeature.DefaultFeatureGate.Enabled(features.StatefulSetAutoDeletePVC) { // if the Pod\u0026#39;s PVCs are not consistent with the StatefulSet\u0026#39;s PVC deletion policy, update the PVC // and dirty the pod. if match, err := spc.ClaimsMatchRetentionPolicy(ctx, set, pod); err != nil { spc.recordPodEvent(\u0026#34;update\u0026#34;, set, pod, err) return err } else if !match { if err := spc.UpdatePodClaimForRetentionPolicy(ctx, set, pod); err != nil { spc.recordPodEvent(\u0026#34;update\u0026#34;, set, pod, err) return err } consistent = false } } //这段Go代码是关于StatefulSet自动删除PVC的逻辑。 首先，检查是否启用了StatefulSetAutoDeletePVC功能门。 //如果启用了，则进一步判断Pod的PVC是否与StatefulSet的PVC删除策略一致。如果不一致，则更新PVC并标记Pod为dirty。 //如果匹配失败，则调用UpdatePodClaimForRetentionPolicy函数来更新Pod的PVC，并将consistent标记为false。 //在更新过程中，如果出现错误，则记录Pod事件并返回错误。 // if the Pod is not dirty, do nothing if consistent { return nil } attemptedUpdate = true // commit the update, retrying on conflicts updateErr := spc.objectMgr.UpdatePod(pod) if updateErr == nil { return nil } if updated, err := spc.objectMgr.GetPod(set.Namespace, pod.Name); err == nil { // make a copy so we don\u0026#39;t mutate the shared cache pod = updated.DeepCopy() } else { utilruntime.HandleError(fmt.Errorf(\u0026#34;error getting updated Pod %s/%s: %w\u0026#34;, set.Namespace, pod.Name, err)) } //这段Go代码主要做的是更新Pod的操作。具体流程如下： //1. 首先判断consistent是否为true，如果是，则直接返回nil。 //2. 设置attemptedUpdate为true，表示已经尝试更新Pod。 //3. 调用spc.objectMgr.UpdatePod(pod)方法更新Pod，如果更新成功，则直接返回nil。 //4. 如果更新失败，则尝试获取最新的Pod信息。 //5. 如果获取成功，则将获取到的Pod深拷贝一份，防止对共享缓存的污染。 //6. 如果获取失败，则打印错误信息。 整体来说，这段代码的逻辑比较简单，主要是通过调用UpdatePod方法更新Pod，如果更新失败则尝试重新获取Pod信息。 return updateErr }) if attemptedUpdate { spc.recordPodEvent(\u0026#34;update\u0026#34;, set, pod, err) } return err } // 这段Go代码是一个函数片段，它在一个匿名函数中执行了某种更新操作，并通过参数set和pod记录了事件。如果更新操作尝试过， // 它会通过spc.recordPodEvent方法记录一个名为\u0026#34;update\u0026#34;的事件。最后，该函数返回一个错误值err。 func (spc *StatefulPodControl) DeleteStatefulPod(set *apps.StatefulSet, pod *v1.Pod) error { err := spc.objectMgr.DeletePod(pod) spc.recordPodEvent(\u0026#34;delete\u0026#34;, set, pod, err) return err } // 此函数用于删除一个有状态的Pod。它通过调用spc.objectMgr.DeletePod(pod)来删除指定的Pod，并记录事件。最后返回删除操作的错误（如果有）。 // ClaimsMatchRetentionPolicy returns false if the PVCs for pod are not consistent with set\u0026#39;s PVC deletion policy. // An error is returned if something is not consistent. This is expected if the pod is being otherwise updated, // but a problem otherwise (see usage of this method in UpdateStatefulPod). func (spc *StatefulPodControl) ClaimsMatchRetentionPolicy(ctx context.Context, set *apps.StatefulSet, pod *v1.Pod) (bool, error) { logger := klog.FromContext(ctx) ordinal := getOrdinal(pod) templates := set.Spec.VolumeClaimTemplates for i := range templates { claimName := getPersistentVolumeClaimName(set, \u0026amp;templates[i], ordinal) claim, err := spc.objectMgr.GetClaim(set.Namespace, claimName) switch { case apierrors.IsNotFound(err): klog.FromContext(ctx).V(4).Info(\u0026#34;Expected claim missing, continuing to pick up in next iteration\u0026#34;, \u0026#34;PVC\u0026#34;, klog.KObj(claim)) case err != nil: return false, fmt.Errorf(\u0026#34;Could not retrieve claim %s for %s when checking PVC deletion policy\u0026#34;, claimName, pod.Name) default: if !claimOwnerMatchesSetAndPod(logger, claim, set, pod) { return false, nil } } } return true, nil } //该函数用于检查Pod的PVC是否符合StatefulSet的PVC删除策略。 //它遍历StatefulSet的VolumeClaimTemplates，并根据模板生成PVC名称。 //然后它尝试获取该PVC，根据获取结果进行判断： //- 如果PVC不存在，则记录日志并继续下一次迭代。 //- 如果获取PVC出现错误，则返回错误。 //- 如果PVC存在但其owner不是StatefulSet和Pod，则返回false。 //如果所有PVC都符合要求，则返回true。 // UpdatePodClaimForRetentionPolicy updates the PVCs used by pod to match the PVC deletion policy of set. func (spc *StatefulPodControl) UpdatePodClaimForRetentionPolicy(ctx context.Context, set *apps.StatefulSet, pod *v1.Pod) error { logger := klog.FromContext(ctx) ordinal := getOrdinal(pod) templates := set.Spec.VolumeClaimTemplates for i := range templates { claimName := getPersistentVolumeClaimName(set, \u0026amp;templates[i], ordinal) claim, err := spc.objectMgr.GetClaim(set.Namespace, claimName) switch { case apierrors.IsNotFound(err): logger.V(4).Info(\u0026#34;Expected claim missing, continuing to pick up in next iteration\u0026#34;, \u0026#34;PVC\u0026#34;, klog.KObj(claim)) case err != nil: return fmt.Errorf(\u0026#34;Could not retrieve claim %s not found for %s when checking PVC deletion policy: %w\u0026#34;, claimName, pod.Name, err) default: if !claimOwnerMatchesSetAndPod(logger, claim, set, pod) { claim = claim.DeepCopy() // Make a copy so we don\u0026#39;t mutate the shared cache. needsUpdate := updateClaimOwnerRefForSetAndPod(logger, claim, set, pod) if needsUpdate { err := spc.objectMgr.UpdateClaim(claim) if err != nil { return fmt.Errorf(\u0026#34;Could not update claim %s for delete policy ownerRefs: %w\u0026#34;, claimName, err) } } } } } return nil } //该函数用于更新StatefulSet中的Pod的PersistentVolumeClaim（PVC）的所有者引用，以确保它们与Pod和StatefulSet正确关联。 //它遍历StatefulSet的VolumeClaimTemplates，并根据Pod的序号生成相应的PVC名称。然后，它尝试获取该PVC，根据获取结果进行不同的处理： //- 如果PVC不存在，则记录一条日志，并继续处理下一个PVC。 //- 如果获取PVC时出现错误，则返回错误。 //- 如果PVC存在但其所有者引用与Pod和StatefulSet不匹配，则创建PVC的深拷贝，并更新其所有者引用。 //如果更新后的PVC与原始PVC不同，则将其更新到Kubernetes集群中。 最终，如果所有PVC都已正确处理，则函数返回nil。 // PodClaimIsStale returns true for a stale PVC that should block pod creation. If the scaling // policy is deletion, and a PVC has an ownerRef that does not match the pod, the PVC is stale. This // includes pods whose UID has not been created. func (spc *StatefulPodControl) PodClaimIsStale(set *apps.StatefulSet, pod *v1.Pod) (bool, error) { policy := getPersistentVolumeClaimRetentionPolicy(set) if policy.WhenScaled == apps.RetainPersistentVolumeClaimRetentionPolicyType { // PVCs are meant to be reused and so can\u0026#39;t be stale. return false, nil } for _, claim := range getPersistentVolumeClaims(set, pod) { pvc, err := spc.objectMgr.GetClaim(claim.Namespace, claim.Name) switch { case apierrors.IsNotFound(err): // If the claim doesn\u0026#39;t exist yet, it can\u0026#39;t be stale. continue case err != nil: return false, err case err == nil: // A claim is stale if it doesn\u0026#39;t match the pod\u0026#39;s UID, including if the pod has no UID. if hasStaleOwnerRef(pvc, pod) { return true, nil } } } return false, nil } // SstatefulSet和Pod对应的PersistentVolumeClaim该函数用于判断Pod所使用的PVC是否为旧的（stale），如果为旧的则会阻止Pod的创建。 // 在StatefulSet缩容策略为删除时，如果PVC的ownerRef与Pod不匹配，则认为PVC是旧的。 // 函数首先根据StatefulSet的设置获取PVC的保留策略，如果策略为保留（Retain），则PVC会被重用，不会被认为是旧的。 // 然后遍历获取与Pod相关的PVC，通过查询PVC的详细信息判断PVC是否与Pod的UID匹配，如果不匹配则认为PVC是旧的。 // 如果查询PVC时发生错误，则返回错误信息。最后，如果没有发现旧的PVC，则返回false。 // recordPodEvent records an event for verb applied to a Pod in a StatefulSet. If err is nil the generated event will // have a reason of v1.EventTypeNormal. If err is not nil the generated event will have a reason of v1.EventTypeWarning. func (spc *StatefulPodControl) recordPodEvent(verb string, set *apps.StatefulSet, pod *v1.Pod, err error) { if err == nil { reason := fmt.Sprintf(\u0026#34;Successful%s\u0026#34;, strings.Title(verb)) message := fmt.Sprintf(\u0026#34;%s Pod %s in StatefulSet %s successful\u0026#34;, strings.ToLower(verb), pod.Name, set.Name) spc.recorder.Event(set, v1.EventTypeNormal, reason, message) } else { reason := fmt.Sprintf(\u0026#34;Failed%s\u0026#34;, strings.Title(verb)) message := fmt.Sprintf(\u0026#34;%s Pod %s in StatefulSet %s failed error: %s\u0026#34;, strings.ToLower(verb), pod.Name, set.Name, err) spc.recorder.Event(set, v1.EventTypeWarning, reason, message) } } // 该函数是一个名为recordPodEvent的方法，它属于StatefulPodControl类型。 // 该方法用于记录关于Pod的事件，该Pod属于一个StatefulSet。根据err参数的值，生成的事件会有不同的原因， // 如果err为nil，则事件原因为v1.EventTypeNormal，否则为v1.EventTypeWarning。 // 根据verb参数，会生成成功或失败的事件消息，并通过spc.recorder.Event方法记录事件。 // recordClaimEvent records an event for verb applied to the PersistentVolumeClaim of a Pod in a StatefulSet. If err is // nil the generated event will have a reason of v1.EventTypeNormal. If err is not nil the generated event will have a // reason of v1.EventTypeWarning. func (spc *StatefulPodControl) recordClaimEvent(verb string, set *apps.StatefulSet, pod *v1.Pod, claim *v1.PersistentVolumeClaim, err error) { if err == nil { reason := fmt.Sprintf(\u0026#34;Successful%s\u0026#34;, strings.Title(verb)) message := fmt.Sprintf(\u0026#34;%s Claim %s Pod %s in StatefulSet %s success\u0026#34;, strings.ToLower(verb), claim.Name, pod.Name, set.Name) spc.recorder.Event(set, v1.EventTypeNormal, reason, message) } else { reason := fmt.Sprintf(\u0026#34;Failed%s\u0026#34;, strings.Title(verb)) message := fmt.Sprintf(\u0026#34;%s Claim %s for Pod %s in StatefulSet %s failed error: %s\u0026#34;, strings.ToLower(verb), claim.Name, pod.Name, set.Name, err) spc.recorder.Event(set, v1.EventTypeWarning, reason, message) } } // 该函数名为recordClaimEvent，用于记录StatefulSet中Pod的PersistentVolumeClaim的事件。 // 根据err是否为nil，生成的事件会有不同的原因，如果err为nil，则事件原因为v1.EventTypeNormal，否则为v1.EventTypeWarning。 // 函数通过spc.recorder.Event方法记录事件。 // createMissingPersistentVolumeClaims creates all of the required PersistentVolumeClaims for pod, and updates its retention policy func (spc *StatefulPodControl) createMissingPersistentVolumeClaims(ctx context.Context, set *apps.StatefulSet, pod *v1.Pod) error { if err := spc.createPersistentVolumeClaims(set, pod); err != nil { return err } if utilfeature.DefaultFeatureGate.Enabled(features.StatefulSetAutoDeletePVC) { // Set PVC policy as much as is possible at this point. if err := spc.UpdatePodClaimForRetentionPolicy(ctx, set, pod); err != nil { spc.recordPodEvent(\u0026#34;update\u0026#34;, set, pod, err) return err } } return nil } //该函数是一个Go语言函数，用于创建缺失的PersistentVolumeClaims（PVCs）并更新其保留策略，适用于StatefulPodControl类型。 //函数首先调用createPersistentVolumeClaims函数创建所需的PVCs，如果创建失败，则返回错误。 //接下来，如果启用了StatefulSetAutoDeletePVC功能，则调用UpdatePodClaimForRetentionPolicy函数来更新PVC的保留策略。 //如果更新失败，则记录事件并返回错误。最后，如果上述步骤都成功，则返回nil。 // createPersistentVolumeClaims creates all of the required PersistentVolumeClaims for pod, which must be a member of // set. If all of the claims for Pod are successfully created, the returned error is nil. If creation fails, this method // may be called again until no error is returned, indicating the PersistentVolumeClaims for pod are consistent with // set\u0026#39;s Spec. func (spc *StatefulPodControl) createPersistentVolumeClaims(set *apps.StatefulSet, pod *v1.Pod) error { var errs []error for _, claim := range getPersistentVolumeClaims(set, pod) { pvc, err := spc.objectMgr.GetClaim(claim.Namespace, claim.Name) switch { case apierrors.IsNotFound(err): err := spc.objectMgr.CreateClaim(\u0026amp;claim) if err != nil { errs = append(errs, fmt.Errorf(\u0026#34;failed to create PVC %s: %s\u0026#34;, claim.Name, err)) } if err == nil || !apierrors.IsAlreadyExists(err) { spc.recordClaimEvent(\u0026#34;create\u0026#34;, set, pod, \u0026amp;claim, err) } case err != nil: errs = append(errs, fmt.Errorf(\u0026#34;failed to retrieve PVC %s: %s\u0026#34;, claim.Name, err)) spc.recordClaimEvent(\u0026#34;create\u0026#34;, set, pod, \u0026amp;claim, err) default: if pvc.DeletionTimestamp != nil { errs = append(errs, fmt.Errorf(\u0026#34;pvc %s is being deleted\u0026#34;, claim.Name)) } } // TODO: Check resource requirements and accessmodes, update if necessary } return errorutils.NewAggregate(errs) } //该函数用于创建StatefulSet中的PersistentVolumeClaims（PVCs）。 //1. 遍历获取需要创建的PVCs。 //2. 对于每个PVC，先尝试从objectMgr中获取，根据获取结果进行不同处理： //- 如果未找到，则创建该PVC，并记录事件。 //- 如果获取发生错误，则将错误信息记录并返回。 //- 如果已存在该PVC，检查其DeletionTimestamp是否为空，若非空则将错误信息记录并返回。 //3. 返回所有错误的聚合。 注意：该函数未完成，最后还有一行TODO注释，提示需要检查资源需求和访问模式并进行更新。 "},{"id":108,"href":"/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Bstateful_set_controlgo%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-statefulsetcontrolgo-yuan-ma-jie-du/","title":"K8S控制器之stateful_set_control.go源码解读 2024-04-10 09:41:30.316","section":"Docs","content":"/* Copyright 2016 The Kubernetes Authors. Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \u0026#34;AS IS\u0026#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */ package statefulset import ( \u0026#34;context\u0026#34; \u0026#34;sort\u0026#34; \u0026#34;sync\u0026#34; apps \u0026#34;k8s.io/api/apps/v1\u0026#34; v1 \u0026#34;k8s.io/api/core/v1\u0026#34; \u0026#34;k8s.io/apimachinery/pkg/api/errors\u0026#34; metav1 \u0026#34;k8s.io/apimachinery/pkg/apis/meta/v1\u0026#34; utilerrors \u0026#34;k8s.io/apimachinery/pkg/util/errors\u0026#34; utilfeature \u0026#34;k8s.io/apiserver/pkg/util/feature\u0026#34; \u0026#34;k8s.io/client-go/tools/record\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/controller/history\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/features\u0026#34; ) // Realistic value for maximum in-flight requests when processing in parallel mode. const MaxBatchSize = 500 // StatefulSetControl implements the control logic for updating StatefulSets and their children Pods. It is implemented // as an interface to allow for extensions that provide different semantics. Currently, there is only one implementation. type StatefulSetControlInterface interface { // UpdateStatefulSet implements the control logic for Pod creation, update, and deletion, and // persistent volume creation, update, and deletion. // If an implementation returns a non-nil error, the invocation will be retried using a rate-limited strategy. // Implementors should sink any errors that they do not wish to trigger a retry, and they may feel free to // exit exceptionally at any point provided they wish the update to be re-run at a later point in time. UpdateStatefulSet(ctx context.Context, set *apps.StatefulSet, pods []*v1.Pod) (*apps.StatefulSetStatus, error) // ListRevisions returns a array of the ControllerRevisions that represent the revisions of set. If the returned // error is nil, the returns slice of ControllerRevisions is valid. ListRevisions(set *apps.StatefulSet) ([]*apps.ControllerRevision, error) // AdoptOrphanRevisions adopts any orphaned ControllerRevisions that match set\u0026#39;s Selector. If all adoptions are // successful the returned error is nil. AdoptOrphanRevisions(set *apps.StatefulSet, revisions []*apps.ControllerRevision) error } //这段代码定义了一个名为StatefulSetControlInterface的接口，其中包含了三个方法： //1. UpdateStatefulSet：用于更新StatefulSet及其子Pods的控制逻辑。实现此方法时，如果返回非空错误，调用将使用速率限制策略进行重试。 //实现者应将任何不希望触发重试的错误沉入，并且可以在任何时间点异常退出，只要希望稍后重新运行更新即可。 //2. ListRevisions：返回代表set修订版本的ControllerRevision数组。如果返回的错误为nil，则返回的ControllerRevision切片是有效的。 //3. AdoptOrphanRevisions：采用与set的Selector匹配的任何孤儿ControllerRevision。如果所有采用都成功，则返回的错误为nil。 //注意：代码中还定义了一个常量MaxBatchSize，其值为500。 // NewDefaultStatefulSetControl returns a new instance of the default implementation StatefulSetControlInterface that // implements the documented semantics for StatefulSets. podControl is the PodControlInterface used to create, update, // and delete Pods and to create PersistentVolumeClaims. statusUpdater is the StatefulSetStatusUpdaterInterface used // to update the status of StatefulSets. You should use an instance returned from NewRealStatefulPodControl() for any // scenario other than testing. func NewDefaultStatefulSetControl( podControl *StatefulPodControl, statusUpdater StatefulSetStatusUpdaterInterface, controllerHistory history.Interface, recorder record.EventRecorder) StatefulSetControlInterface { return \u0026amp;defaultStatefulSetControl{podControl, statusUpdater, controllerHistory, recorder} } // 该函数返回一个实现了StatefulSetControlInterface接口的defaultStatefulSetControl实例。 // 参数包括podControl用于创建、更新和删除Pod以及创建PersistentVolumeClaims； // statusUpdater用于更新StatefulSets的状态； // controllerHistory用于管理StatefulSet的控制器历史记录； // recorder用于记录事件。 // 除了测试场景外，应使用NewRealStatefulPodControl()返回的实例作为podControl参数。 type defaultStatefulSetControl struct { podControl *StatefulPodControl statusUpdater StatefulSetStatusUpdaterInterface controllerHistory history.Interface recorder record.EventRecorder } //该代码定义了一个名为defaultStatefulSetControl的结构体，它有四个字段： //1. podControl：类型为*StatefulPodControl，用于控制Pod的操作。 //2. statusUpdater：类型为StatefulSetStatusUpdaterInterface，用于更新StatefulSet的状态。 //3. controllerHistory：类型为history.Interface，用于维护控制器的历史记录。 //4. recorder：类型为record.EventRecorder，用于记录事件。 这个结构体主要用于管理和控制StatefulSet的生命周期。 // UpdateStatefulSet executes the core logic loop for a stateful set, applying the predictable and // consistent monotonic update strategy by default - scale up proceeds in ordinal order, no new pod // is created while any pod is unhealthy, and pods are terminated in descending order. The burst // strategy allows these constraints to be relaxed - pods will be created and deleted eagerly and // in no particular order. Clients using the burst strategy should be careful to ensure they // understand the consistency implications of having unpredictable numbers of pods available. func (ssc *defaultStatefulSetControl) UpdateStatefulSet(ctx context.Context, set *apps.StatefulSet, pods []*v1.Pod) (*apps.StatefulSetStatus, error) { set = set.DeepCopy() // set is modified when a new revision is created in performUpdate. Make a copy now to avoid mutation errors. // list all revisions and sort them revisions, err := ssc.ListRevisions(set) //该函数是defaultStatefulSetControl类型的UpdateStatefulSet方法，用于更新StatefulSet对象的状态。 //函数首先对传入的StatefulSet对象进行深拷贝，以避免更新过程中出现突变错误。 //然后通过调用ssc的ListRevisions方法，列出并排序该StatefulSet的所有修订版本。 //接下来，函数会根据传入的Pod对象列表，更新StatefulSet的状态信息，并返回更新后的StatefulSet状态以及可能出现的错误。 if err != nil { return nil, err } history.SortControllerRevisions(revisions) //该函数首先检查err是否为nil，如果不为nil，则返回nil和err。然后调用history.SortControllerRevisions函数对revisions进行排序。 currentRevision, updateRevision, status, err := ssc.performUpdate(ctx, set, pods, revisions) if err != nil { errs := []error{err} if agg, ok := err.(utilerrors.Aggregate); ok { errs = agg.Errors() } return nil, utilerrors.NewAggregate(append(errs, ssc.truncateHistory(set, pods, revisions, currentRevision, updateRevision))) } //该函数调用ssc.performUpdate执行更新操作，并根据返回的结果进行错误处理和历史记录截断。 //如果performUpdate返回错误，函数会将错误添加到一个错误切片中，并判断错误是否为utilerrors.Aggregate类型， //如果是，则将其中的错误提取出来合并到错误切片中。 //最后，函数会调用ssc.truncateHistory截断历史记录，并将其错误与之前收集的错误合并后返回。 // maintain the set\u0026#39;s revision history limit return status, ssc.truncateHistory(set, pods, revisions, currentRevision, updateRevision) //该函数调用了ssc.truncateHistory方法，用于维护集合的修订历史记录限制。传入参数包括集合、Pods、修订版本号、当前修订版本号和更新修订版本号。返回值为操作状态。 } //该函数是用于更新一个StatefulSet的状态的核心逻辑循环。 //它执行默认的、可预测的、一致的单调更新策略：扩展按顺序进行，当任何Pod不健康时不会创建新的Pod，Pods按降序终止。 //突发策略允许放松这些约束——Pod将被积极地创建和删除，且没有特定的顺序。 //使用突发策略的客户端应该小心确保他们了解具有不可预测的Pod数量的一致性影响。 //函数首先对传入的StatefulSet进行深拷贝，然后列出并排序所有修订版本。 //然后执行performUpdate函数进行更新操作，根据更新结果维护StatefulSet的修订版本历史记录限制。 func (ssc *defaultStatefulSetControl) performUpdate( ctx context.Context, set *apps.StatefulSet, pods []*v1.Pod, revisions []*apps.ControllerRevision) (*apps.ControllerRevision, *apps.ControllerRevision, *apps.StatefulSetStatus, error) { var currentStatus *apps.StatefulSetStatus logger := klog.FromContext(ctx) //该函数是一个Go语言函数，它定义了一个名为performUpdate的方法，该方法属于defaultStatefulSetControl类型。 //这个方法接收四个参数： //ctx是一个上下文对象， //set是一个指向StatefulSet类型的指针， //pods是一个指向Pod类型的切片， //revisions是一个指向ControllerRevision类型的切片。 //该方法返回四个值：一个指向ControllerRevision类型的指针，一个指向ControllerRevision类型的指针， //一个指向StatefulSetStatus类型的指针，以及一个错误类型。 在函数体内部，定义了一个名为currentStatus的变量， //它是一个指向StatefulSetStatus类型的指针。 //然后，通过FromContext方法从ctx中获取了一个日志记录器对象，并将其赋值给名为logger的变量。 //这个函数的主要功能是在状态fulSet的更新过程中执行一些操作，并返回更新后的状态fulSet的状态信息和其他相关数据。 //但是，根据给定的代码片段，无法确定该函数的完整逻辑和具体操作。 // get the current, and update revisions currentRevision, updateRevision, collisionCount, err := ssc.getStatefulSetRevisions(set, revisions) if err != nil { return currentRevision, updateRevision, currentStatus, err } //该函数用于获取当前状态集的修订版本，并更新修订版本。 //函数接收一个状态集和修订版本作为参数，返回当前修订版本、更新修订版本、冲突计数和错误信息。 //如果发生错误，函数会返回当前修订版本、更新修订版本、当前状态和错误信息。 // perform the main update function and get the status currentStatus, err = ssc.updateStatefulSet(ctx, set, currentRevision, updateRevision, collisionCount, pods) if err != nil \u0026amp;\u0026amp; currentStatus == nil { return currentRevision, updateRevision, nil, err } //该函数主要调用ssc.updateStatefulSet方法来更新StatefulSet的状态，并获取更新后的状态信息。如果更新出错且没有返回新的状态信息，则返回更新前的状态信息和错误。 // make sure to update the latest status even if there is an error with non-nil currentStatus statusErr := ssc.updateStatefulSetStatus(ctx, set, currentStatus) if statusErr == nil { logger.V(4).Info(\u0026#34;Updated status\u0026#34;, \u0026#34;statefulSet\u0026#34;, klog.KObj(set), \u0026#34;replicas\u0026#34;, currentStatus.Replicas, \u0026#34;readyReplicas\u0026#34;, currentStatus.ReadyReplicas, \u0026#34;currentReplicas\u0026#34;, currentStatus.CurrentReplicas, \u0026#34;updatedReplicas\u0026#34;, currentStatus.UpdatedReplicas) } //该函数用于更新StatefulSet的状态。 //首先调用ssc.updateStatefulSetStatus方法更新状态，然后根据返回的错误值进行判断。 //如果更新状态时没有错误，就打印日志信息，包括StatefulSet的名称和各种状态值（Replicas、ReadyReplicas、CurrentReplicas、UpdatedReplicas）。 switch { case err != nil \u0026amp;\u0026amp; statusErr != nil: logger.Error(statusErr, \u0026#34;Could not update status\u0026#34;, \u0026#34;statefulSet\u0026#34;, klog.KObj(set)) return currentRevision, updateRevision, currentStatus, err case err != nil: return currentRevision, updateRevision, currentStatus, err case statusErr != nil: return currentRevision, updateRevision, currentStatus, statusErr } //这个Go函数通过switch语句根据err和statusErr的值来决定返回的结果。 //如果err和statusErr都不为nil，则记录错误日志并返回当前修订版号、更新修订版号、当前状态和err； //如果只有err不为nil，则直接返回当前修订版号、更新修订版号、当前状态和err； //如果只有statusErr不为nil，则直接返回当前修订版号、更新修订版号、当前状态和statusErr。 logger.V(4).Info(\u0026#34;StatefulSet revisions\u0026#34;, \u0026#34;statefulSet\u0026#34;, klog.KObj(set), \u0026#34;currentRevision\u0026#34;, currentStatus.CurrentRevision, \u0026#34;updateRevision\u0026#34;, currentStatus.UpdateRevision) return currentRevision, updateRevision, currentStatus, nil //该函数是一个日志输出函数，通过调用logger.V(4).Info方法输出一条包含多个参数的日志信息。 //其中，参数StatefulSet revisions是日志的主题，statefulSet、currentRevision、updateRevision是日志的附加信息。 //函数返回currentRevision、updateRevision、currentStatus以及一个nil错误。 } func (ssc *defaultStatefulSetControl) ListRevisions(set *apps.StatefulSet) ([]*apps.ControllerRevision, error) { selector, err := metav1.LabelSelectorAsSelector(set.Spec.Selector) if err != nil { return nil, err } return ssc.controllerHistory.ListControllerRevisions(set, selector) } // 该函数是defaultStatefulSetControl类型的成员方法，用于列出给定StatefulSet的所有ControllerRevision。 // 1. 首先，函数将StatefulSet的selector转换为LabelSelector类型。 // 2. 然后，调用controllerHistory的ListControllerRevisions方法，传入StatefulSet和selector，返回所有的ControllerRevision。 // 3. 如果转换selector时出现错误，函数将返回nil和错误信息。 返回值为 []*apps.ControllerRevision，即ControllerRevision的切片。 func (ssc *defaultStatefulSetControl) AdoptOrphanRevisions( set *apps.StatefulSet, revisions []*apps.ControllerRevision) error { for i := range revisions { adopted, err := ssc.controllerHistory.AdoptControllerRevision(set, controllerKind, revisions[i]) if err != nil { return err } revisions[i] = adopted } return nil } //该函数是一个Go语言函数，它属于defaultStatefulSetControl类型的一个方法。 //函数名为AdoptOrphanRevisions，它接受一个StatefulSet类型的指针set和一个ControllerRevision类型的切片revisions作为参数，返回一个error类型的值。 //该函数的作用是领养孤儿ControllerRevision（即没有StatefulSet关联的ControllerRevision）， //通过调用ssc.controllerHistory的AdoptControllerRevision方法，将孤儿ControllerRevision与给定的StatefulSet关联起来。 //函数会遍历revisions切片中的每个ControllerRevision，尝试将其领养，并将领养后的ControllerRevision更新到revisions切片中对应的位置。 //如果领养过程中出现错误，则会立即返回错误。如果成功领养所有孤儿ControllerRevision，则函数返回nil。 // truncateHistory truncates any non-live ControllerRevisions in revisions from set\u0026#39;s history. The UpdateRevision and // CurrentRevision in set\u0026#39;s Status are considered to be live. Any revisions associated with the Pods in pods are also // considered to be live. Non-live revisions are deleted, starting with the revision with the lowest Revision, until // only RevisionHistoryLimit revisions remain. If the returned error is nil the operation was successful. This method // expects that revisions is sorted when supplied. func (ssc *defaultStatefulSetControl) truncateHistory( set *apps.StatefulSet, pods []*v1.Pod, revisions []*apps.ControllerRevision, current *apps.ControllerRevision, update *apps.ControllerRevision) error { history := make([]*apps.ControllerRevision, 0, len(revisions)) // mark all live revisions live := map[string]bool{} //该函数用于截断StatefulSet历史记录中任何非活动的ControllerRevisions。 //函数会从set的历史记录中删除非活动的修订版本，从修订版本号最低的开始，直到只剩下RevisionHistoryLimit个修订版本为止。 //函数会将UpdateRevision和CurrentRevision视为活动的修订版本，同时将与Pods关联的修订版本也视为活动的修订版本。 //函数期望传入的修订版本已经按照顺序排序。 //函数返回一个错误，如果返回的错误为nil，则表示操作成功。 //函数首先创建一个空的ControllerRevision指针切片history，用于存储活动的修订版本。 //然后创建一个map类型的live变量，用于标记活动的修订版本。 //接下来，函数遍历传入的修订版本切片revisions，将活动的修订版本添加到history切片中，并在live变量中标记该修订版本为活动的。 //最后，函数检查history切片的长度是否超过了RevisionHistoryLimit， //如果是，则删除history切片中修订版本号最低的修订版本，直到history切片的长度等于RevisionHistoryLimit。 //如果在截断历史记录的过程中出现错误，则返回该错误。 if current != nil { live[current.Name] = true } if update != nil { live[update.Name] = true } for i := range pods { live[getPodRevision(pods[i])] = true } // collect live revisions and historic revisions for i := range revisions { if !live[revisions[i].Name] { history = append(history, revisions[i]) } } //该Go函数主要实现了以下几个功能： //1. 将当前非空对象的名称添加到live映射中，并将其值设置为true。 //2. 如果update非空，将update对象的名称添加到live映射中，并将其值设置为true。 //3. 遍历pods切片，将每个Pod的修订版本名称添加到live映射中，并将其值设置为true。 //4. 遍历revisions切片，将不在live映射中的修订版本对象添加到history切片中。 //这段代码的主要目的是为了收集活跃的修订版本和历史修订版本，并将它们分别存储在live和history映射和切片中。 historyLen := len(history) historyLimit := int(*set.Spec.RevisionHistoryLimit) if historyLen \u0026lt;= historyLimit { return nil } // delete any non-live history to maintain the revision limit. history = history[:(historyLen - historyLimit)] for i := 0; i \u0026lt; len(history); i++ { if err := ssc.controllerHistory.DeleteControllerRevision(history[i]); err != nil { return err } } return nil } //该函数用于根据设定的修订历史限制，删除非活跃的历史记录，以保持修订限制。 //首先，获取历史记录的长度并将其与修订历史限制进行比较。 //如果历史记录的长度小于等于修订历史限制，则无需进行删除操作，直接返回nil。 //如果历史记录的长度大于修订历史限制，则通过切片操作将超出部分的历史记录删除，并遍历剩余的历史记录，调用DeleteControllerRevision方法将其逐个删除。 //如果删除过程中出现错误，则返回错误。最后，如果删除成功，则返回nil。 // getStatefulSetRevisions returns the current and update ControllerRevisions for set. It also // returns a collision count that records the number of name collisions set saw when creating // new ControllerRevisions. This count is incremented on every name collision and is used in // building the ControllerRevision names for name collision avoidance. This method may create // a new revision, or modify the Revision of an existing revision if an update to set is detected. // This method expects that revisions is sorted when supplied. func (ssc *defaultStatefulSetControl) getStatefulSetRevisions( set *apps.StatefulSet, revisions []*apps.ControllerRevision) (*apps.ControllerRevision, *apps.ControllerRevision, int32, error) { var currentRevision, updateRevision *apps.ControllerRevision revisionCount := len(revisions) history.SortControllerRevisions(revisions) //该函数用于获取StatefulSet的当前和更新的ControllerRevisions，并返回一个记录创建新ControllerRevisions时遇到的名称冲突次数的碰撞计数。 //如果检测到StatefulSet的更新，该函数可能会创建新修订版或修改现有修订版的修订号。函数期望传入的修订版已排序。 // Use a local copy of set.Status.CollisionCount to avoid modifying set.Status directly. // This copy is returned so the value gets carried over to set.Status in updateStatefulSet. var collisionCount int32 if set.Status.CollisionCount != nil { collisionCount = *set.Status.CollisionCount } //这段Go代码定义了一个局部变量collisionCount，并根据set.Status.CollisionCount的值进行初始化。 //其主要作用是避免直接修改set.Status的值，通过返回这个局部变量的值，将其传递给updateStatefulSet函数，以更新set.Status中的CollisionCount字段。 //详细解释如下： 1. var collisionCount int32：定义了一个名为collisionCount的整型变量，用于存储set.Status.CollisionCount的值。 //2. if set.Status.CollisionCount != nil：判断set.Status.CollisionCount是否非空，即是否已经初始化。 //3. collisionCount = *set.Status.CollisionCount：如果set.Status.CollisionCount非空，将其值赋给collisionCount变量。 //这里使用了指针解引用操作*，因为set.Status.CollisionCount可能是一个指针。 //这段代码的主要目的是在不直接修改set.Status的情况下，获取并传递set.Status.CollisionCount的值。 //通过使用局部变量的方式，可以确保在updateStatefulSet函数中更新set.Status时，collisionCount的值不会丢失。 // create a new revision from the current set updateRevision, err := newRevision(set, nextRevision(revisions), \u0026amp;collisionCount) if err != nil { return nil, nil, collisionCount, err } //这段Go代码中的函数创建了一个新的修订版本，使用当前的版本集。 //它首先调用nextRevision(revisions)来获取下一个修订版本号，然后调用newRevision(set, revision, \u0026amp;collisionCount)来创建新的修订版本。 //如果创建过程中出现错误，则返回nil、nil、collisionCount和错误信息。 // find any equivalent revisions equalRevisions := history.FindEqualRevisions(revisions, updateRevision) equalCount := len(equalRevisions) //该代码片段是用Go编写的。它定义了一个函数，该函数查找与给定修订版本相等的任何修订版本，并返回相等修订版本的数量。 //具体而言，该函数通过调用history.FindEqualRevisions方法，将给定的修订版本列表revisions和更新的修订版本updateRevision作为参数传递。 //FindEqualRevisions方法返回一个包含与updateRevision相等的所有修订版本的切片。 //然后，代码通过使用len函数计算equalRevisions切片的长度，从而获取相等修订版本的数量。 //总结一下，这段代码的主要功能是查找与指定更新修订版本相等的修订版本的数量。 if equalCount \u0026gt; 0 \u0026amp;\u0026amp; history.EqualRevision(revisions[revisionCount-1], equalRevisions[equalCount-1]) { // if the equivalent revision is immediately prior the update revision has not changed updateRevision = revisions[revisionCount-1] } else if equalCount \u0026gt; 0 { // if the equivalent revision is not immediately prior we will roll back by incrementing the // Revision of the equivalent revision updateRevision, err = ssc.controllerHistory.UpdateControllerRevision( equalRevisions[equalCount-1], updateRevision.Revision) if err != nil { return nil, nil, collisionCount, err } } else { //if there is no equivalent revision we create a new one updateRevision, err = ssc.controllerHistory.CreateControllerRevision(set, updateRevision, \u0026amp;collisionCount) if err != nil { return nil, nil, collisionCount, err } } //这段Go代码中的函数片段是一个条件语句，用于根据不同的条件更新或创建控制器修订版本。 //具体功能如下： //- 首先，它检查是否存在等效的修订版本（equalCount大于0）并且该修订版本是否立即位于更新修订版本之前。 //如果是，则将更新修订版本设置为最新修订版本。 //- 如果存在等效的修订版本但不立即位于更新修订版本之前，则通过调用ssc.controllerHistory.UpdateControllerRevision方法来回滚到该等效修订版本， //并更新修订版本号。如果回滚过程中出现错误，则返回错误信息。 //- 如果不存在等效的修订版本，则通过调用ssc.controllerHistory.CreateControllerRevision方法创建一个新的控制器修订版本。 //如果创建过程中出现错误，则返回错误信息。 总之，这段代码根据条件选择更新或创建控制器修订版本，并可能涉及到回滚到等效的修订版本或创建新修订版本的操作。 // attempt to find the revision that corresponds to the current revision for i := range revisions { if revisions[i].Name == set.Status.CurrentRevision { currentRevision = revisions[i] break } } //该函数是一个for循环，通过遍历revisions切片，查找并设置与set.Status.CurrentRevision名称相匹配的修订版本。 // if the current revision is nil we initialize the history by setting it to the update revision if currentRevision == nil { currentRevision = updateRevision } return currentRevision, updateRevision, collisionCount, nil } // 这个函数是一个简单的条件判断语句，判断当前的currentRevision是否为nil， // 如果是，则将其初始化为updateRevision。 // 最后返回更新后的currentRevision、updateRevision、collisionCount和nil。 func slowStartBatch(initialBatchSize int, remaining int, fn func(int) (bool, error)) (int, error) { successes := 0 j := 0 for batchSize := min(remaining, initialBatchSize); batchSize \u0026gt; 0; batchSize = min(min(2*batchSize, remaining), MaxBatchSize) { errCh := make(chan error, batchSize) var wg sync.WaitGroup wg.Add(batchSize) for i := 0; i \u0026lt; batchSize; i++ { go func(k int) { defer wg.Done() // Ignore the first parameter - relevant for monotonic only. if _, err := fn(k); err != nil { errCh \u0026lt;- err } }(j) j++ } wg.Wait() successes += batchSize - len(errCh) close(errCh) if len(errCh) \u0026gt; 0 { errs := make([]error, 0) for err := range errCh { errs = append(errs, err) } return successes, utilerrors.NewAggregate(errs) } remaining -= batchSize } return successes, nil } // 该函数是一个并发执行函数的批处理工具。它通过调用提供的函数fn来并发执行批处理，并根据执行结果进行错误处理和统计。 // - initialBatchSize：初始批处理大小。 // - remaining：剩余需要处理的数量。 // - fn：一个函数，接收一个整数参数并返回一个布尔值和一个错误。 // 该函数将被并发调用以执行批处理任务。 // 函数主要逻辑如下： // 1. 初始化成功执行数successes和内部计数器j。 // 2. 使用min函数计算当前批处理大小batchSize，并进入循环，直到batchSize为0或处理完成。 // 3. 创建一个错误通道errCh，用于收集并发执行中的错误。 // 4. 使用sync.WaitGroup来等待所有并发执行完成。 // 5. 并发调用fn函数，将j作为参数，递增j。 // 6. 等待所有并发执行完成，并统计成功执行数。 // 7. 如果存在错误，将错误收集到errs中并返回。 // 8. 更新剩余需要处理的数量。 // 9. 返回成功执行数和错误。 // 该函数通过控制批处理的大小和并发执行，实现了一个自适应的并发批处理逻辑，可以根据执行情况动态调整批处理的大小。 type replicaStatus struct { replicas int32 readyReplicas int32 availableReplicas int32 currentReplicas int32 updatedReplicas int32 } // 该代码定义了一个名为replicaStatus的结构体类型， // 它有5个字段： // - replicas表示副本的总数； // - readyReplicas表示准备就绪的副本数量； // - availableReplicas表示可用的副本数量； // - currentReplicas表示当前存在的副本数量； // - updatedReplicas表示已更新的副本数量。 func computeReplicaStatus(pods []*v1.Pod, minReadySeconds int32, currentRevision, updateRevision *apps.ControllerRevision) replicaStatus { status := replicaStatus{} for _, pod := range pods { if isCreated(pod) { status.replicas++ } //该函数用于计算副本状态。它接收一个Pod列表、最小就绪秒数、当前修订版和更新修订版作为参数，并返回一个副本状态。 //函数遍历Pod列表，并对每个Pod进行如下操作： //1. 如果Pod被创建，则将副本数量加1。 //最后，函数返回计算得到的副本状态。 // count the number of running and ready replicas if isRunningAndReady(pod) { status.readyReplicas++ // count the number of running and available replicas if isRunningAndAvailable(pod, minReadySeconds) { status.availableReplicas++ } } //这段Go代码中的函数用于增加就绪副本数和可用副本数的计数。 //具体来说： //- isRunningAndReady(pod) 函数检查 Pod 是否处于运行且就绪状态，如果是，则将 status.readyReplicas 增加 1。 //- isRunningAndAvailable(pod, minReadySeconds) 函数检查 Pod 是否处于运行且可用状态， //其中 minReadySeconds 参数表示最小就绪时间（单位：秒）。 //如果 Pod 在最小就绪时间内一直保持就绪状态，则将 status.availableReplicas 增加 1。 //这段代码的主要目的是统计 Kubernetes 中某个 Pod 的就绪副本数和可用副本数，以便于对 Pod 的状态进行管理和监控。 // count the number of current and update replicas if isCreated(pod) \u0026amp;\u0026amp; !isTerminating(pod) { revision := getPodRevision(pod) if revision == currentRevision.Name { status.currentReplicas++ } if revision == updateRevision.Name { status.updatedReplicas++ } } } return status } // 这段Go代码中的函数逻辑如下： // - 首先判断Pod是否已经创建并且没有处于终止状态； // - 如果满足条件，获取Pod的修订版本号； // - 判断该修订版本号是否与当前修订版本名相同，若相同则当前副本数加一； // - 判断该修订版本号是否与更新修订版本名相同，若相同则更新副本数加一； // - 最后返回更新后的状态信息。 这段代码主要是用来统计Pod的当前副本数和更新后的副本数。 func updateStatus(status *apps.StatefulSetStatus, minReadySeconds int32, currentRevision, updateRevision *apps.ControllerRevision, podLists ...[]*v1.Pod) { status.Replicas = 0 status.ReadyReplicas = 0 status.AvailableReplicas = 0 status.CurrentReplicas = 0 status.UpdatedReplicas = 0 for _, list := range podLists { replicaStatus := computeReplicaStatus(list, minReadySeconds, currentRevision, updateRevision) status.Replicas += replicaStatus.replicas status.ReadyReplicas += replicaStatus.readyReplicas status.AvailableReplicas += replicaStatus.availableReplicas status.CurrentReplicas += replicaStatus.currentReplicas status.UpdatedReplicas += replicaStatus.updatedReplicas } } // 该函数用于更新StatefulSet的状态信息。 // 它接收一个指向StatefulSetStatus的指针、minReadySeconds参数以及currentRevision和updateRevision两个ControllerRevision指针， // 还有可变长度的podLists参数。 // 函数首先将状态信息中的五个计数器重置为0，然后遍历podLists中的每个Pod列表， // 通过调用computeReplicaStatus函数计算每个列表的副本状态，并将计算结果累加到状态信息中 func (ssc *defaultStatefulSetControl) processReplica( ctx context.Context, set *apps.StatefulSet, currentRevision *apps.ControllerRevision, updateRevision *apps.ControllerRevision, currentSet *apps.StatefulSet, updateSet *apps.StatefulSet, monotonic bool, replicas []*v1.Pod, i int) (bool, error) { logger := klog.FromContext(ctx) //该函数是一个Go语言函数，它定义了一个用于处理StatefulSet副本的过程。 //函数接受多个参数，包括上下文、StatefulSet对象、两个ControllerRevision对象、两个StatefulSet对象、一个布尔值、一个Pod对象数组和一个整数。 //函数返回一个布尔值和一个错误对象。 Markdown格式输出： //该函数是一个Go语言函数，定义了一个处理StatefulSet副本的过程。它接受以下参数： //- ctx：上下文对象。 //- set：StatefulSet对象。 //- currentRevision：当前的ControllerRevision对象。 //- updateRevision：要更新的ControllerRevision对象。 //- currentSet：当前的StatefulSet对象。 //- updateSet：要更新的StatefulSet对象。 //- monotonic：一个布尔值，表示是否为单调递增。 //- replicas：一个Pod对象数组。 //- i：一个整数。 //函数返回一个布尔值和一个错误对象。 // Delete and recreate pods which finished running. // // Note that pods with phase Succeeded will also trigger this event. This is // because final pod phase of evicted or otherwise forcibly stopped pods // (e.g. terminated on node reboot) is determined by the exit code of the // container, not by the reason for pod termination. We should restart the pod // regardless of the exit code. if isFailed(replicas[i]) || isSucceeded(replicas[i]) { if isFailed(replicas[i]) { ssc.recorder.Eventf(set, v1.EventTypeWarning, \u0026#34;RecreatingFailedPod\u0026#34;, \u0026#34;StatefulSet %s/%s is recreating failed Pod %s\u0026#34;, set.Namespace, set.Name, replicas[i].Name) //这段Go代码是一个函数的一部分，它的功能是删除已经运行完成的Pod，并重新创建它们。 //注意，这个函数还会处理那些成功结束的Pod（即状态为Succeeded的Pod）， //因为被驱逐或强制停止的Pod的最终状态是由容器的退出码决定的，而不是由Pod终止的原因决定的。 //因此，无论退出码如何，都应该重新启动Pod。 //函数中使用了isFailed和isSucceeded函数来判断Pod的状态， //如果状态为Failed或Succeeded，则会使用ssc.recorder.Eventf函数记录一个事件，说明StatefulSet正在重新创建失败或成功的Pod。 } else { ssc.recorder.Eventf(set, v1.EventTypeNormal, \u0026#34;RecreatingTerminatedPod\u0026#34;, \u0026#34;StatefulSet %s/%s is recreating terminated Pod %s\u0026#34;, set.Namespace, set.Name, replicas[i].Name) //这段Go代码是Kubernetes中StatefulSet控制器的一部分，用于处理有状态副本集（StatefulSet）中已终止的Pod的重建事件。 //在else块中，代码通过ssc.recorder.Eventf方法记录了一个事件， //事件类型为v1.EventTypeNormal，事件名为\u0026#34;RecreatingTerminatedPod\u0026#34;， //事件消息为\u0026#34;StatefulSet %s/%s is recreating terminated Pod %s\u0026#34;， //其中包含了StatefulSet的命名空间、名称以及需要重建的Pod的名称。 //这个函数的作用是通过事件机制通知用户，某个StatefulSet正在重建已终止的Pod。 } if err := ssc.podControl.DeleteStatefulPod(set, replicas[i]); err != nil { return true, err } replicaOrd := i + getStartOrdinal(set) replicas[i] = newVersionedStatefulSetPod( currentSet, updateSet, currentRevision.Name, updateRevision.Name, replicaOrd) //该函数的功能是在StatefulSet中删除一个Pod，并创建一个新的Pod来替代它。 //函数首先尝试删除指定的Pod，如果删除成功，则根据当前的StatefulSet和更新后的StatefulSet信息创建一个新的Pod，并将其添加到replicas列表中。 } //该函数用于删除并重新创建已经完成运行的Pods。 //注意，即使Pod的阶段为Succeeded，也会触发此事件。 //这是因为被驱逐或以其他方式强制停止的Pod的最终Pod阶段（例如在节点重新启动时终止）由容器的退出代码决定，而不是由Pod终止的原因决定。 //无论退出代码如何，我们都应该重新启动Pod。 //函数首先检查每个Pod是否失败或成功，如果是，则记录事件并尝试删除该Pod。 //如果删除成功，则创建一个新的Pod来替代它。 // If we find a Pod that has not been created we create the Pod if !isCreated(replicas[i]) { if utilfeature.DefaultFeatureGate.Enabled(features.StatefulSetAutoDeletePVC) { if isStale, err := ssc.podControl.PodClaimIsStale(set, replicas[i]); err != nil { return true, err } else if isStale { // If a pod has a stale PVC, no more work can be done this round. return true, err } } if err := ssc.podControl.CreateStatefulPod(ctx, set, replicas[i]); err != nil { return true, err } if monotonic { // if the set does not allow bursting, return immediately return true, nil } } //这段Go代码是一个条件判断语句的集合，用于处理Pod的创建。 //具体来说，它首先检查一个Pod是否已经被创建，如果没有，则根据特定的条件进行进一步的处理。 //如果启用了StatefulSetAutoDeletePVC特性，并且Pod的PersistentVolumeClaim（PVC）是陈旧的，则函数会提前返回。 //如果创建Pod失败，函数也会返回。如果该集合不允许突发（monotonic）行为，函数会在创建Pod后立即返回。 // If the Pod is in pending state then trigger PVC creation to create missing PVCs if isPending(replicas[i]) { logger.V(4).Info( \u0026#34;StatefulSet is triggering PVC creation for pending Pod\u0026#34;, \u0026#34;statefulSet\u0026#34;, klog.KObj(set), \u0026#34;pod\u0026#34;, klog.KObj(replicas[i])) if err := ssc.podControl.createMissingPersistentVolumeClaims(ctx, set, replicas[i]); err != nil { return true, err } } //该函数用于检查StatefulSet中的Pod是否处于pending状态， //如果是，则触发PVC（Persistent Volume Claim）的创建，以创建缺失的PVC。 //具体实现中，通过调用podControl的createMissingPersistentVolumeClaims方法来创建缺失的PVC， //如果创建失败，则返回true和错误信息。 // If we find a Pod that is currently terminating, we must wait until graceful deletion // completes before we continue to make progress. if isTerminating(replicas[i]) \u0026amp;\u0026amp; monotonic { logger.V(4).Info(\u0026#34;StatefulSet is waiting for Pod to Terminate\u0026#34;, \u0026#34;statefulSet\u0026#34;, klog.KObj(set), \u0026#34;pod\u0026#34;, klog.KObj(replicas[i])) return true, nil } //该函数是一个条件判断语句，判断某个Pod是否正在终止状态，并且是否需要等待其优雅删除完成。如果满足条件，则返回true和nil。 // If we have a Pod that has been created but is not running and ready we can not make progress. // We must ensure that all for each Pod, when we create it, all of its predecessors, with respect to its // ordinal, are Running and Ready. if !isRunningAndReady(replicas[i]) \u0026amp;\u0026amp; monotonic { logger.V(4).Info(\u0026#34;StatefulSet is waiting for Pod to be Running and Ready\u0026#34;, \u0026#34;statefulSet\u0026#34;, klog.KObj(set), \u0026#34;pod\u0026#34;, klog.KObj(replicas[i])) return true, nil } //该函数是Go语言编写的，用于判断一个Pod是否处于运行和就绪状态。 //如果Pod没有运行且未准备就绪，并且设置为单调递增模式，则函数会返回true和nil。 //这段代码主要用于StatefulSet等待Pod达到运行和就绪状态的情况。 // If we have a Pod that has been created but is not available we can not make progress. // We must ensure that all for each Pod, when we create it, all of its predecessors, with respect to its // ordinal, are Available. if !isRunningAndAvailable(replicas[i], set.Spec.MinReadySeconds) \u0026amp;\u0026amp; monotonic { logger.V(4).Info(\u0026#34;StatefulSet is waiting for Pod to be Available\u0026#34;, \u0026#34;statefulSet\u0026#34;, klog.KObj(set), \u0026#34;pod\u0026#34;, klog.KObj(replicas[i])) return true, nil } //该函数是Go语言编写的，用于判断StatefulSet中的Pod是否可用。 //如果Pod未运行或不可用，并且StatefulSet的monotonic属性为true，则函数会返回true，否则返回false。 //函数通过调用isRunningAndAvailable函数来判断Pod的状态。 // Enforce the StatefulSet invariants retentionMatch := true if utilfeature.DefaultFeatureGate.Enabled(features.StatefulSetAutoDeletePVC) { var err error retentionMatch, err = ssc.podControl.ClaimsMatchRetentionPolicy(ctx, updateSet, replicas[i]) // An error is expected if the pod is not yet fully updated, and so return is treated as matching. if err != nil { retentionMatch = true } } //这段Go代码是用于检查StatefulSet的保留策略是否匹配的函数。 //- 首先，它检查是否启用了StatefulSet的自动删除PVC的功能。 //- 如果启用了，它会调用ssc.podControl.ClaimsMatchRetentionPolicy方法来检查当前的Pod是否匹配保留策略。 //- 如果检查过程中出现错误，则将retentionMatch设置为true，表示匹配保留策略。 //- 最后，retentionMatch的值将用于决定是否保留或删除StatefulSet的PVC。 if identityMatches(set, replicas[i]) \u0026amp;\u0026amp; storageMatches(set, replicas[i]) \u0026amp;\u0026amp; retentionMatch { return false, nil } //该函数是用于判断某个条件是否满足，如果满足则返回false和nil。 //具体判断条件为：identityMatches函数返回值为true，并且storageMatches函数返回值为true，并且retentionMatch的值为true。 //其中，identityMatches函数用于判断某个set是否与replicas[i]相等； //storageMatches函数用于判断某个set是否与replicas[i]的存储相匹配； //retentionMatch是一个布尔值，用于判断某个条件是否满足。 //如果以上条件都满足，则函数返回false和nil。 // Make a deep copy so we don\u0026#39;t mutate the shared cache replica := replicas[i].DeepCopy() if err := ssc.podControl.UpdateStatefulPod(ctx, updateSet, replica); err != nil { return true, err } //这个函数的功能是通过调用DeepCopy方法创建replicas[i]的深拷贝，并将拷贝赋值给replica变量。 //然后，它调用ssc.podControl.UpdateStatefulPod方法来更新状态fulPod，如果更新失败，则返回true和错误信息。 return false, nil } func (ssc *defaultStatefulSetControl) processCondemned(ctx context.Context, set *apps.StatefulSet, firstUnhealthyPod *v1.Pod, monotonic bool, condemned []*v1.Pod, i int) (bool, error) { logger := klog.FromContext(ctx) if isTerminating(condemned[i]) { // if we are in monotonic mode, block and wait for terminating pods to expire if monotonic { logger.V(4).Info(\u0026#34;StatefulSet is waiting for Pod to Terminate prior to scale down\u0026#34;, \u0026#34;statefulSet\u0026#34;, klog.KObj(set), \u0026#34;pod\u0026#34;, klog.KObj(condemned[i])) return true, nil } return false, nil } //该函数是defaultStatefulSetControl类型的processCondemned方法， //用于处理StatefulSet中的废弃Pod。函数根据是否为单调模式，决定是否等待废弃Pod终止。 //如果处于单调模式且当前Pod为终止状态，则函数会等待Pod终止。如果当前Pod不是终止状态，则函数直接返回。 // if we are in monotonic mode and the condemned target is not the first unhealthy Pod block if !isRunningAndReady(condemned[i]) \u0026amp;\u0026amp; monotonic \u0026amp;\u0026amp; condemned[i] != firstUnhealthyPod { logger.V(4).Info(\u0026#34;StatefulSet is waiting for Pod to be Running and Ready prior to scale down\u0026#34;, \u0026#34;statefulSet\u0026#34;, klog.KObj(set), \u0026#34;pod\u0026#34;, klog.KObj(firstUnhealthyPod)) return true, nil } //该函数片段是一个条件判断语句， //主要功能是在特定条件下，判断是否需要等待某个Pod变为Running和Ready状态后再进行缩容操作。 //具体来说，它包含了以下几个要点： //1. 判断当前Pod是否处于运行且就绪状态：通过调用isRunningAndReady(condemned[i])函数来判断当前Pod是否满足运行且就绪的条件。 //2. 判断是否处于单调模式：通过检查monotonic变量的值来判断当前是否处于单调模式。 //3. 判断当前Pod是否为第一个不健康的Pod：通过比较condemned[i]和firstUnhealthyPod的值来判断当前Pod是否为第一个不健康的Pod。 //如果满足上述条件，即当前Pod不运行且不就绪，当前处于单调模式，且当前Pod不是第一个不健康的Pod，则该函数会返回true和nil， //表示需要等待Pod变为Running和Ready状态后再进行缩容操作。 // if we are in monotonic mode and the condemned target is not the first unhealthy Pod, block. if !isRunningAndAvailable(condemned[i], set.Spec.MinReadySeconds) \u0026amp;\u0026amp; monotonic \u0026amp;\u0026amp; condemned[i] != firstUnhealthyPod { logger.V(4).Info(\u0026#34;StatefulSet is waiting for Pod to be Available prior to scale down\u0026#34;, \u0026#34;statefulSet\u0026#34;, klog.KObj(set), \u0026#34;pod\u0026#34;, klog.KObj(firstUnhealthyPod)) return true, nil } //该函数是一段Go语言代码，它是一个条件判断语句。 //该函数主要功能是在特定条件下阻止某个操作的执行。 //具体来说，函数首先判断某个Pod（被谴责的目标）是否处于运行和可用状态，然后判断是否处于单调模式，并且被谴责的目标不是第一个不健康的Pod。 //如果满足这些条件，函数会返回true，表示需要等待Pod变为可用状态后再进行缩容操作。否则，函数返回nil，表示可以进行缩容操作。 logger.V(2).Info(\u0026#34;Pod of StatefulSet is terminating for scale down\u0026#34;, \u0026#34;statefulSet\u0026#34;, klog.KObj(set), \u0026#34;pod\u0026#34;, klog.KObj(condemned[i])) return true, ssc.podControl.DeleteStatefulPod(set, condemned[i]) } // 该函数是一个日志记录和操作Pod的函数。 // 它首先使用logger.V(2).Info记录一条日志信息，表示某个StatefulSet的Pod正在因为缩容而终止。 // 日志中包含了StatefulSet和Pod的相关信息。 // 然后，函数通过调用ssc.podControl.DeleteStatefulPod方法来删除指定的Pod，并返回true表示删除成功。 func runForAll(pods []*v1.Pod, fn func(i int) (bool, error), monotonic bool) (bool, error) { if monotonic { for i := range pods { if shouldExit, err := fn(i); shouldExit || err != nil { return true, err } } } else { if _, err := slowStartBatch(1, len(pods), fn); err != nil { return true, err } } return false, nil } //该函数runForAll接受三个参数： //一个pods的切片，一个函数fn，和一个布尔值monotonic。 //根据monotonic的值，函数会以不同的方式调用fn函数。 //如果monotonic为true，则会顺序遍历pods切片，并依次调用fn函数，传入当前索引i作为参数。 //如果fn函数返回true或出现错误，则会立即返回true和错误。 //如果monotonic为false，则会调用slowStartBatch函数，以一定的延迟并发地调用fn函数。 //最终，如果所有fn函数调用都成功完成，则返回false和nil。 // updateStatefulSet performs the update function for a StatefulSet. This method creates, updates, and deletes Pods in // the set in order to conform the system to the target state for the set. The target state always contains // set.Spec.Replicas Pods with a Ready Condition. If the UpdateStrategy.Type for the set is // RollingUpdateStatefulSetStrategyType then all Pods in the set must be at set.Status.CurrentRevision. // If the UpdateStrategy.Type for the set is OnDeleteStatefulSetStrategyType, the target state implies nothing about // the revisions of Pods in the set. If the UpdateStrategy.Type for the set is PartitionStatefulSetStrategyType, then // all Pods with ordinal less than UpdateStrategy.Partition.Ordinal must be at Status.CurrentRevision and all other // Pods must be at Status.UpdateRevision. If the returned error is nil, the returned StatefulSetStatus is valid and the // update must be recorded. If the error is not nil, the method should be retried until successful. func (ssc *defaultStatefulSetControl) updateStatefulSet( ctx context.Context, set *apps.StatefulSet, currentRevision *apps.ControllerRevision, updateRevision *apps.ControllerRevision, collisionCount int32, pods []*v1.Pod) (*apps.StatefulSetStatus, error) { logger := klog.FromContext(ctx) //该函数是用于更新StatefulSet的状态的。 //它会根据目标状态来创建、更新和删除Pod，以使系统符合StatefulSet的目标状态。 //目标状态始终包含具有Ready Condition的set.Spec.Replicas Pods。 //如果StatefulSet的UpdateStrategy.Type为RollingUpdateStatefulSetStrategyType， //则集合中的所有Pod必须处于set.Status.CurrentRevision。 //如果UpdateStrategy.Type为OnDeleteStatefulSetStrategyType， //则目标状态与集合中Pod的修订版本无关。 //如果UpdateStrategy.Type为PartitionStatefulSetStrategyType， //则所有序号小于UpdateStrategy.Partition.Ordinal的Pod必须处于Status.CurrentRevision， //而所有其他Pod必须处于Status.UpdateRevision。 //如果返回的错误为nil，则返回的StatefulSetStatus是有效的，并且必须记录更新。 //如果错误不为nil，则应重试该方法，直到成功。 // get the current and update revisions of the set. currentSet, err := ApplyRevision(set, currentRevision) if err != nil { return nil, err } updateSet, err := ApplyRevision(set, updateRevision) if err != nil { return nil, err } //这段Go代码中的函数功能是：根据给定的集合和两个修订版本号，分别获取当前修订版本对应的集合内容和更新修订版本对应的集合内容。 //具体描述如下： //1. 首先，函数使用ApplyRevision函数和给定的集合以及当前修订版本号currentRevision，获取当前修订版本对应的集合内容，并将结果赋值给currentSet变量。 //如果出现错误，则返回nil和错误信息。 //2. 然后，函数使用ApplyRevision函数和给定的集合以及更新修订版本号updateRevision，获取更新修订版本对应的集合内容，并将结果赋值给updateSet变量。 //如果出现错误，则返回nil和错误信息。 //需要注意的是，该函数的返回值类型为(interface{}, error)， //其中interface{}表示返回的内容可以是任意类型，error表示返回的错误信息。 // set the generation, and revisions in the returned status status := apps.StatefulSetStatus{} status.ObservedGeneration = set.Generation status.CurrentRevision = currentRevision.Name status.UpdateRevision = updateRevision.Name status.CollisionCount = new(int32) *status.CollisionCount = collisionCount //该Go函数主要实现了设置StatefulSetStatus结构体中ObservedGeneration、CurrentRevision、UpdateRevision和CollisionCount字段的值。 //- 首先，创建了一个空的StatefulSetStatus结构体变量status。 //- 然后，将set.Generation的值赋给status.ObservedGeneration字段。 //- 接着，将currentRevision.Name的值赋给status.CurrentRevision字段。 //- 再将updateRevision.Name的值赋给status.UpdateRevision字段。 //- 最后，创建一个新的int32类型指针变量status.CollisionCount并将其值设为collisionCount。 //总结来说，该函数的作用是通过给StatefulSetStatus结构体的字段赋值，来初始化或更新一个StatefulSet的状态信息。 updateStatus(\u0026amp;status, set.Spec.MinReadySeconds, currentRevision, updateRevision, pods) //该函数用于更新状态。根据参数设置的最小就绪秒数、当前修订版本、更新修订版本和Pods，来更新状态对象status。 replicaCount := int(*set.Spec.Replicas) // slice that will contain all Pods such that getStartOrdinal(set) \u0026lt;= getOrdinal(pod) \u0026lt;= getEndOrdinal(set) replicas := make([]*v1.Pod, replicaCount) // slice that will contain all Pods such that getOrdinal(pod) \u0026lt; getStartOrdinal(set) OR getOrdinal(pod) \u0026gt; getEndOrdinal(set) condemned := make([]*v1.Pod, 0, len(pods)) unhealthy := 0 var firstUnhealthyPod *v1.Pod // First we partition pods into two lists valid replicas and condemned Pods for _, pod := range pods { if podInOrdinalRange(pod, set) { // if the ordinal of the pod is within the range of the current number of replicas, // insert it at the indirection of its ordinal replicas[getOrdinal(pod)-getStartOrdinal(set)] = pod } else if getOrdinal(pod) \u0026gt;= 0 { // if the ordinal is valid, but not within the range add it to the condemned list condemned = append(condemned, pod) } // If the ordinal could not be parsed (ord \u0026lt; 0), ignore the Pod. } //该函数根据给定的Pod集合和Deployment规格， //将Pod分为两类：有效副本和废弃Pod。 //有效副本是指其序号在当前副本数量范围内且序号有效的Pod， //而废弃Pod是指序号不在当前副本数量范围内但序号有效的Pod。 //函数通过遍历Pod集合，根据Pod的序号与当前副本数量范围的比较，将Pod分别加入到有效副本和废弃Pod的切片中。 // for any empty indices in the sequence [0,set.Spec.Replicas) create a new Pod at the correct revision for ord := getStartOrdinal(set); ord \u0026lt;= getEndOrdinal(set); ord++ { replicaIdx := ord - getStartOrdinal(set) if replicas[replicaIdx] == nil { replicas[replicaIdx] = newVersionedStatefulSetPod( currentSet, updateSet, currentRevision.Name, updateRevision.Name, ord) } } //该Go函数的功能是在有空缺索引的位置上创建新的Pod。 //它根据set的规格，从起始序号到结束序号遍历，如果在replicas数组中对应索引位置上的Pod为空，则创建一个新的有正确修订版本的Pod，并将其赋值给该索引位置。 // sort the condemned Pods by their ordinals sort.Sort(descendingOrdinal(condemned)) //该函数是一个排序函数，用于将一个名为condemned的Pod切片按照其序号降序排序。 //其中，descendingOrdinal是一个自定义的排序类型，实现了sort.Interface接口的Sort方法，通过比较Pod的序号来实现降序排序。 // find the first unhealthy Pod for i := range replicas { if !isHealthy(replicas[i]) { unhealthy++ if firstUnhealthyPod == nil { firstUnhealthyPod = replicas[i] } } } //该函数用于遍历一组Pod副本，查找第一个不健康的状态。 //具体操作为循环遍历replicas切片，通过调用isHealthy函数判断每个副本的健康状态。 //如果不健康，则将unhealthy计数加1，并且如果firstUnhealthyPod为空，则将其赋值为当前副本。 // or the first unhealthy condemned Pod (condemned are sorted in descending order for ease of use) for i := len(condemned) - 1; i \u0026gt;= 0; i-- { if !isHealthy(condemned[i]) { unhealthy++ if firstUnhealthyPod == nil { firstUnhealthyPod = condemned[i] } } } //该Go代码片段是一个for循环，遍历一个名为condemned的切片。 //循环从condemned切片的最后一个元素开始，倒序遍历。 //在每次迭代中，它调用isHealthy函数来检查当前Pod的健康状况。 //如果Pod不健康，则将unhealthy计数器加1，并检查是否是第一个不健康的Pod。 //如果是，则将其赋值给firstUnhealthyPod变量。 //该循环的主要目的是找到第一个不健康的Pod，并统计不健康Pod的数量。 if unhealthy \u0026gt; 0 { logger.V(4).Info(\u0026#34;StatefulSet has unhealthy Pods\u0026#34;, \u0026#34;statefulSet\u0026#34;, klog.KObj(set), \u0026#34;unhealthyReplicas\u0026#34;, unhealthy, \u0026#34;pod\u0026#34;, klog.KObj(firstUnhealthyPod)) } //这段Go代码是一个条件判断语句，判断变量unhealthy是否大于0，如果大于0，则调用logger.V(4).Info方法输出一条日志信息。 //日志信息的内容包括：\u0026#34;StatefulSet has unhealthy Pods\u0026#34;，\u0026#34;statefulSet\u0026#34;，\u0026#34;unhealthyReplicas\u0026#34;，\u0026#34;pod\u0026#34;。 //其中，\u0026#34;statefulSet\u0026#34;和\u0026#34;pod\u0026#34;是通过klog.KObj方法获取的对象的字符串表示形式。 // If the StatefulSet is being deleted, don\u0026#39;t do anything other than updating // status. if set.DeletionTimestamp != nil { return \u0026amp;status, nil } //该函数用于判断一个StatefulSet对象是否正在被删除。如果是，则只更新状态信息，不做其他操作。 //函数通过检查StatefulSet对象的DeletionTimestamp字段是否为nil来判断是否正在被删除。 //如果DeletionTimestamp不为nil，则说明该对象正在被删除，函数会返回当前状态信息和nil。 //否则，函数会继续执行其他操作。 monotonic := !allowsBurst(set) // First, process each living replica. Exit if we run into an error or something blocking in monotonic mode. processReplicaFn := func(i int) (bool, error) { return ssc.processReplica(ctx, set, currentRevision, updateRevision, currentSet, updateSet, monotonic, replicas, i) } //这段Go代码定义了一个函数和一个内部函数。 //函数monotonic := !allowsBurst(set)根据set参数的值计算一个布尔值monotonic。 //allowsBurst是一个外部函数，其作用是判断是否允许突发（burst）， //这里通过取反操作得到monotonic，即不允许突发。 //内部函数processReplicaFn := func(i int) (bool, error) {...}定义了一个闭包函数， //用于处理每个存活的副本。该函数接受一个整数参数i，返回一个布尔值和一个错误。 //在函数体中，调用了ssc.processReplica函数来处理副本， //该函数接受多个参数，包括上下文、集合、当前修订版本、更新修订版本、当前集合、更新集合、是否单调以及副本列表和副本索引。 //该内部函数的主要作用是遍历副本并逐个处理它们，如果在单调模式下遇到错误或阻塞，则退出处理。 if shouldExit, err := runForAll(replicas, processReplicaFn, monotonic); shouldExit || err != nil { updateStatus(\u0026amp;status, set.Spec.MinReadySeconds, currentRevision, updateRevision, replicas, condemned) return \u0026amp;status, err } //该函数中，runForAll()函数执行processReplicaFn函数操作replicas，并返回一个shouldExit标志和一个错误err。 //如果shouldExit为真或err不为nil，则更新状态并返回。 // Fix pod claims for condemned pods, if necessary. if utilfeature.DefaultFeatureGate.Enabled(features.StatefulSetAutoDeletePVC) { fixPodClaim := func(i int) (bool, error) { if matchPolicy, err := ssc.podControl.ClaimsMatchRetentionPolicy(ctx, updateSet, condemned[i]); err != nil { return true, err } else if !matchPolicy { if err := ssc.podControl.UpdatePodClaimForRetentionPolicy(ctx, updateSet, condemned[i]); err != nil { return true, err } } return false, nil } if shouldExit, err := runForAll(condemned, fixPodClaim, monotonic); shouldExit || err != nil { updateStatus(\u0026amp;status, set.Spec.MinReadySeconds, currentRevision, updateRevision, replicas, condemned) return \u0026amp;status, err } } //这段Go代码是用于修复有状态副本集（StatefulSet）中被废弃的Pod的声明（Claim）的。 //如果启用了features.StatefulSetAutoDeletePVC特性，并且ssc.podControl.ClaimsMatchRetentionPolicy函数返回错误或者返回的匹配策略为false， //则会调用ssc.podControl.UpdatePodClaimForRetentionPolicy函数来更新Pod的声明。 //runForAll函数会遍历所有被废弃的Pod，并调用fixPodClaim函数进行修复。 //如果runForAll函数返回需要退出或者发生了错误，则会更新副本集的状态并返回。 // At this point, in monotonic mode all of the current Replicas are Running, Ready and Available, // and we can consider termination. // We will wait for all predecessors to be Running and Ready prior to attempting a deletion. // We will terminate Pods in a monotonically decreasing order. // Note that we do not resurrect Pods in this interval. Also note that scaling will take precedence over // updates. processCondemnedFn := func(i int) (bool, error) { return ssc.processCondemned(ctx, set, firstUnhealthyPod, monotonic, condemned, i) } if shouldExit, err := runForAll(condemned, processCondemnedFn, monotonic); shouldExit || err != nil { updateStatus(\u0026amp;status, set.Spec.MinReadySeconds, currentRevision, updateRevision, replicas, condemned) return \u0026amp;status, err } //这段Go代码中的函数是一个处理被宣判终止的Pod的函数。 //在函数中，它会按照单调递减的顺序终止Pods，并且在终止Pod之前会等待所有前置Pod都处于Running和Ready状态。 //函数会调用ssc.processCondemned来处理每个被宣判终止的Pod。 //如果函数执行完毕或者出现错误，它会更新状态并返回该状态和错误信息。 updateStatus(\u0026amp;status, set.Spec.MinReadySeconds, currentRevision, updateRevision, replicas, condemned) // for the OnDelete strategy we short circuit. Pods will be updated when they are manually deleted. if set.Spec.UpdateStrategy.Type == apps.OnDeleteStatefulSetStrategyType { return \u0026amp;status, nil } if utilfeature.DefaultFeatureGate.Enabled(features.MaxUnavailableStatefulSet) { return updateStatefulSetAfterInvariantEstablished(ctx, ssc, set, replicas, updateRevision, status, ) } //该函数用于更新状态fulset的状态。 //根据输入参数的不同，函数会有不同的行为。 //如果状态fulset的更新策略为OnDelete，则函数会直接返回当前状态。 //如果启用了MaxUnavailableStatefulSet特性，则会调用updateStatefulSetAfterInvariantEstablished函数进行进一步的状态更新。 // we compute the minimum ordinal of the target sequence for a destructive update based on the strategy. updateMin := 0 if set.Spec.UpdateStrategy.RollingUpdate != nil { updateMin = int(*set.Spec.UpdateStrategy.RollingUpdate.Partition) } // we terminate the Pod with the largest ordinal that does not match the update revision. for target := len(replicas) - 1; target \u0026gt;= updateMin; target-- { // delete the Pod if it is not already terminating and does not match the update revision. if getPodRevision(replicas[target]) != updateRevision.Name \u0026amp;\u0026amp; !isTerminating(replicas[target]) { logger.V(2).Info(\u0026#34;Pod of StatefulSet is terminating for update\u0026#34;, \u0026#34;statefulSet\u0026#34;, klog.KObj(set), \u0026#34;pod\u0026#34;, klog.KObj(replicas[target])) if err := ssc.podControl.DeleteStatefulPod(set, replicas[target]); err != nil { if !errors.IsNotFound(err) { return \u0026amp;status, err } } status.CurrentReplicas-- return \u0026amp;status, err } //该Go函数用于根据策略计算目标序列的最小序数值，用于破坏性更新。 //接着，函数会终止与更新修订版本不匹配的最大序号的Pod。 //具体流程如下： //1. 初始化updateMin为0。 //2. 如果set.Spec.UpdateStrategy.RollingUpdate不为nil，则将updateMin设置为*set.Spec.UpdateStrategy.RollingUpdate.Partition的整数值。 //3. 从replicas的最后一个元素开始，迭代到updateMin（包括updateMin）。 //4. 如果当前Pod的修订版本与更新修订版本不匹配且未终止，则记录日志，并尝试删除该Pod。若删除成功，将status.CurrentReplicas减1， //并返回status和错误信息err。 //5. 若删除Pod时出现错误且错误类型不是NotFound，则返回status和该错误。 //注意：函数中的replicas是Pod的副本列表，updateRevision.Name是更新的修订版本名称，ssc.podControl.DeleteStatefulPod是删除Pod的方法，logger.V(2).Info是记录日志的方法。 // wait for unhealthy Pods on update if !isHealthy(replicas[target]) { logger.V(4).Info(\u0026#34;StatefulSet is waiting for Pod to update\u0026#34;, \u0026#34;statefulSet\u0026#34;, klog.KObj(set), \u0026#34;pod\u0026#34;, klog.KObj(replicas[target])) return \u0026amp;status, nil } } return \u0026amp;status, nil } //这段Go代码是一个条件判断语句，其主要功能是检查StatefulSet中的Pod是否健康。 //具体来说，函数首先检查replicas[target]是否健康，如果不健康，则记录日志并返回当前状态status。 //如果replicas[target]健康，则直接返回当前状态status。 func updateStatefulSetAfterInvariantEstablished( ctx context.Context, ssc *defaultStatefulSetControl, set *apps.StatefulSet, replicas []*v1.Pod, updateRevision *apps.ControllerRevision, status apps.StatefulSetStatus, ) (*apps.StatefulSetStatus, error) { //该函数是在建立不变性后更新StatefulSet的状态。 //它接收上下文、StatefulSet控制对象、StatefulSet实例、Pod实例数组、控制器修订版本和StatefulSet状态作为参数， //并返回更新后的StatefulSet状态和错误信息。 //函数的主要逻辑是更新StatefulSet的状态，包括当前修订版本、复制集和条件等，并返回更新后的状态。 logger := klog.FromContext(ctx) replicaCount := int(*set.Spec.Replicas) //该代码片段是用Go语言编写的，其中定义了两个变量logger和replicaCount。 //- logger := klog.FromContext(ctx)：该行代码从ctx上下文中获取了一个名为logger的变量。 //该变量很可能是用于日志记录的一种日志器对象。 //klog是一个用于日志记录的Go库，FromContext函数可能是用于从上下文中提取特定的日志器对象。 //- replicaCount := int(*set.Spec.Replicas)：该行代码定义了一个名为replicaCount的整型变量， //并将其初始化为set.Spec.Replicas指针指向的值的整型表示。根据变量的命名，该变量可能是用于存储某个集合的副本数量。 //set的类型和Spec的定义没有给出，因此无法确定set.Spec.Replicas的具体类型和含义。 //但是根据常规的命名习惯，Spec通常表示某个对象的规格说明，而Replicas则通常表示副本数量。 //因此，可以推测set.Spec.Replicas可能是一个指针，指向一个表示副本数量的整数值。通过将其转换为int类型，可以将该值用于后续的逻辑处理或计算。 // we compute the minimum ordinal of the target sequence for a destructive update based on the strategy. updateMin := 0 maxUnavailable := 1 if set.Spec.UpdateStrategy.RollingUpdate != nil { updateMin = int(*set.Spec.UpdateStrategy.RollingUpdate.Partition) // if the feature was enabled and then later disabled, MaxUnavailable may have a value // more than 1. Ignore the passed in value and Use maxUnavailable as 1 to enforce // expected behavior when feature gate is not enabled. var err error maxUnavailable, err = getStatefulSetMaxUnavailable(set.Spec.UpdateStrategy.RollingUpdate.MaxUnavailable, replicaCount) if err != nil { return \u0026amp;status, err } } //这段Go代码中的函数主要作用是根据给定的策略计算目标序列的最小序数值，用于破坏性更新。 //函数首先将更新的最小序数值初始化为0，将最大不可用数值初始化为1。 //然后，如果设置了更新策略的RollingUpdate属性，将更新的最小序数值设置为RollingUpdate中的Partition属性的值。 //接着，根据RollingUpdate中的MaxUnavailable属性和副本数量，获取最大不可用数值，并返回错误信息。 // Collect all targets in the range between getStartOrdinal(set) and getEndOrdinal(set). Count any targets in that range // that are unhealthy i.e. terminated or not running and ready as unavailable). Select the // (MaxUnavailable - Unavailable) Pods, in order with respect to their ordinal for termination. Delete // those pods and count the successful deletions. Update the status with the correct number of deletions. unavailablePods := 0 for target := len(replicas) - 1; target \u0026gt;= 0; target-- { if !isHealthy(replicas[target]) { unavailablePods++ } } //该函数用于收集在指定范围内的所有目标，并计算其中不健康的目标数量。 //然后，它选择最多不可用的Pods，并按照它们的序号进行终止。 //函数会删除这些Pods，并统计成功删除的数量，最后更新状态信息。 //函数使用一个循环遍历所有的副本，并通过isHealthy函数判断每个副本的健康状况，如果不健康则增加不可用Pods的计数。 if unavailablePods \u0026gt;= maxUnavailable { logger.V(2).Info(\u0026#34;StatefulSet found unavailablePods, more than or equal to allowed maxUnavailable\u0026#34;, \u0026#34;statefulSet\u0026#34;, klog.KObj(set), \u0026#34;unavailablePods\u0026#34;, unavailablePods, \u0026#34;maxUnavailable\u0026#34;, maxUnavailable) return \u0026amp;status, nil } //该函数是一个条件判断语句，判断unavailablePods是否大于等于maxUnavailable。如果是，则通过logger记录信息并返回status和nil。 // Now we need to delete MaxUnavailable- unavailablePods // start deleting one by one starting from the highest ordinal first podsToDelete := maxUnavailable - unavailablePods //这个Go函数的功能是计算需要删除的Pod数量。它根据MaxUnavailable和unavailablePods的差值，确定需要删除的Pod数量，并将结果存储在podsToDelete变量中。 deletedPods := 0 for target := len(replicas) - 1; target \u0026gt;= updateMin \u0026amp;\u0026amp; deletedPods \u0026lt; podsToDelete; target-- { // delete the Pod if it is healthy and the revision doesnt match the target if getPodRevision(replicas[target]) != updateRevision.Name \u0026amp;\u0026amp; !isTerminating(replicas[target]) { // delete the Pod if it is healthy and the revision doesnt match the target logger.V(2).Info(\u0026#34;StatefulSet terminating Pod for update\u0026#34;, \u0026#34;statefulSet\u0026#34;, klog.KObj(set), \u0026#34;pod\u0026#34;, klog.KObj(replicas[target])) if err := ssc.podControl.DeleteStatefulPod(set, replicas[target]); err != nil { if !errors.IsNotFound(err) { return \u0026amp;status, err } } deletedPods++ status.CurrentReplicas-- } } return \u0026amp;status, nil } //该函数用于删除StatefulSet中的一部分Pod。 //它根据指定的条件遍历replicas列表，并删除满足条件的Pod。 //具体而言，它从replicas列表的末尾开始遍历， //如果Pod的修订版本号与目标修订版本号不匹配且Pod未终止，则删除该Pod。 //删除Pod时，如果出现错误（除找不到Pod的错误外），则返回错误信息。函数返回更新后的状态信息和nil。 // updateStatefulSetStatus updates set\u0026#39;s Status to be equal to status. If status indicates a complete update, it is // mutated to indicate completion. If status is semantically equivalent to set\u0026#39;s Status no update is performed. If the // returned error is nil, the update is successful. func (ssc *defaultStatefulSetControl) updateStatefulSetStatus( ctx context.Context, set *apps.StatefulSet, status *apps.StatefulSetStatus) error { // complete any in progress rolling update if necessary completeRollingUpdate(set, status) // if the status is not inconsistent do not perform an update if !inconsistentStatus(set, status) { return nil } // copy set and update its status set = set.DeepCopy() if err := ssc.statusUpdater.UpdateStatefulSetStatus(ctx, set, status); err != nil { return err } return nil } var _ StatefulSetControlInterface = \u0026amp;defaultStatefulSetControl{} //该函数用于更新StatefulSet的状态。 //它首先完成任何正在进行的滚动更新，然后检查状态是否一致，如果不一致则进行更新。 //更新时会创建StatefulSet的深拷贝，并调用statusUpdater的UpdateStatefulSetStatus方法进行更新。 //如果更新成功，则返回nil；否则返回错误。 "},{"id":109,"href":"/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Bstateful_set_status_updatego%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-statefulsetstatusupdatego-yuan-ma-jie-du/","title":"K8S控制器之stateful_set_status_update.go源码解读 2024-04-10","section":"Docs","content":"/* Copyright 2017 The Kubernetes Authors. Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \u0026#34;AS IS\u0026#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */ package statefulset import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; apps \u0026#34;k8s.io/api/apps/v1\u0026#34; metav1 \u0026#34;k8s.io/apimachinery/pkg/apis/meta/v1\u0026#34; utilruntime \u0026#34;k8s.io/apimachinery/pkg/util/runtime\u0026#34; clientset \u0026#34;k8s.io/client-go/kubernetes\u0026#34; appslisters \u0026#34;k8s.io/client-go/listers/apps/v1\u0026#34; \u0026#34;k8s.io/client-go/util/retry\u0026#34; ) // StatefulSetStatusUpdaterInterface is an interface used to update the StatefulSetStatus associated with a StatefulSet. // For any use other than testing, clients should create an instance using NewRealStatefulSetStatusUpdater. type StatefulSetStatusUpdaterInterface interface { // UpdateStatefulSetStatus sets the set\u0026#39;s Status to status. Implementations are required to retry on conflicts, // but fail on other errors. If the returned error is nil set\u0026#39;s Status has been successfully set to status. UpdateStatefulSetStatus(ctx context.Context, set *apps.StatefulSet, status *apps.StatefulSetStatus) error } //这段代码定义了一个名为StatefulSetStatusUpdaterInterface的接口，其中包含一个方法UpdateStatefulSetStatus。 //该方法用于更新与StatefulSet关联的StatefulSetStatus。 //接口的实现需要在发生冲突时进行重试，但在其他错误情况下失败。 //如果返回的错误为nil，则表示set的状态已成功设置为status。 // NewRealStatefulSetStatusUpdater returns a StatefulSetStatusUpdaterInterface that updates the Status of a StatefulSet, // using the supplied client and setLister. func NewRealStatefulSetStatusUpdater( client clientset.Interface, setLister appslisters.StatefulSetLister) StatefulSetStatusUpdaterInterface { return \u0026amp;realStatefulSetStatusUpdater{client, setLister} } //该函数返回一个StatefulSetStatusUpdaterInterface接口， //用于更新StatefulSet的状态。 //它接受一个clientset.Interface类型的client和一个appslisters.StatefulSetLister类型的setLister作为参数， //并将它们封装到realStatefulSetStatusUpdater结构体中，然后返回该结构体的指针。 type realStatefulSetStatusUpdater struct { client clientset.Interface setLister appslisters.StatefulSetLister } func (ssu *realStatefulSetStatusUpdater) UpdateStatefulSetStatus( ctx context.Context, set *apps.StatefulSet, status *apps.StatefulSetStatus) error { // don\u0026#39;t wait due to limited number of clients, but backoff after the default number of steps return retry.RetryOnConflict(retry.DefaultRetry, func() error { set.Status = *status // TODO: This context.TODO should use a real context once we have RetryOnConflictWithContext _, updateErr := ssu.client.AppsV1().StatefulSets(set.Namespace).UpdateStatus(context.TODO(), set, metav1.UpdateOptions{}) if updateErr == nil { return nil } if updated, err := ssu.setLister.StatefulSets(set.Namespace).Get(set.Name); err == nil { // make a copy so we don\u0026#39;t mutate the shared cache set = updated.DeepCopy() } else { utilruntime.HandleError(fmt.Errorf(\u0026#34;error getting updated StatefulSet %s/%s from lister: %v\u0026#34;, set.Namespace, set.Name, err)) } return updateErr }) } //这段Go代码定义了一个realStatefulSetStatusUpdater结构体和一个UpdateStatefulSetStatus方法。 //realStatefulSetStatusUpdater结构体包含两个字段：client和setLister，分别用于与Kubernetes API通信和获取StatefulSet列表。 //UpdateStatefulSetStatus方法用于更新指定StatefulSet的状态。它使用retry.RetryOnConflict函数进行重试， //以处理并发更新冲突的情况。在重试的回调函数中，将新状态赋值给set.Status， //然后调用UpdateStatus方法更新StatefulSet的状态。 //如果更新失败，将尝试从setLister获取最新的StatefulSet副本，并再次尝试更新。 //如果获取最新StatefulSet时出现错误，将记录错误信息。 //总结来说，这个函数的作用是更新指定StatefulSet的状态，处理并发更新冲突，并在更新失败时进行重试。 var _ StatefulSetStatusUpdaterInterface = \u0026amp;realStatefulSetStatusUpdater{} "},{"id":110,"href":"/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Bstateful_setgo%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-statefulsetgo-yuan-ma-jie-du/","title":"K8S控制器之stateful_set.go源码解读 2024-04-10","section":"Docs","content":"/* Copyright 2016 The Kubernetes Authors. Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \u0026#34;AS IS\u0026#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */ package statefulset import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;reflect\u0026#34; \u0026#34;time\u0026#34; apps \u0026#34;k8s.io/api/apps/v1\u0026#34; v1 \u0026#34;k8s.io/api/core/v1\u0026#34; \u0026#34;k8s.io/apimachinery/pkg/api/errors\u0026#34; metav1 \u0026#34;k8s.io/apimachinery/pkg/apis/meta/v1\u0026#34; \u0026#34;k8s.io/apimachinery/pkg/labels\u0026#34; utilruntime \u0026#34;k8s.io/apimachinery/pkg/util/runtime\u0026#34; \u0026#34;k8s.io/apimachinery/pkg/util/wait\u0026#34; appsinformers \u0026#34;k8s.io/client-go/informers/apps/v1\u0026#34; coreinformers \u0026#34;k8s.io/client-go/informers/core/v1\u0026#34; clientset \u0026#34;k8s.io/client-go/kubernetes\u0026#34; \u0026#34;k8s.io/client-go/kubernetes/scheme\u0026#34; v1core \u0026#34;k8s.io/client-go/kubernetes/typed/core/v1\u0026#34; appslisters \u0026#34;k8s.io/client-go/listers/apps/v1\u0026#34; corelisters \u0026#34;k8s.io/client-go/listers/core/v1\u0026#34; \u0026#34;k8s.io/client-go/tools/cache\u0026#34; \u0026#34;k8s.io/client-go/tools/record\u0026#34; \u0026#34;k8s.io/client-go/util/workqueue\u0026#34; podutil \u0026#34;k8s.io/kubernetes/pkg/api/v1/pod\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/controller\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/controller/history\u0026#34; ) // controllerKind contains the schema.GroupVersionKind for this controller type. var controllerKind = apps.SchemeGroupVersion.WithKind(\u0026#34;StatefulSet\u0026#34;) // StatefulSetController controls statefulsets. type StatefulSetController struct { // client interface kubeClient clientset.Interface // control returns an interface capable of syncing a stateful set. // Abstracted out for testing. control StatefulSetControlInterface // podControl is used for patching pods. podControl controller.PodControlInterface // podLister is able to list/get pods from a shared informer\u0026#39;s store podLister corelisters.PodLister // podListerSynced returns true if the pod shared informer has synced at least once podListerSynced cache.InformerSynced // setLister is able to list/get stateful sets from a shared informer\u0026#39;s store setLister appslisters.StatefulSetLister // setListerSynced returns true if the stateful set shared informer has synced at least once setListerSynced cache.InformerSynced // pvcListerSynced returns true if the pvc shared informer has synced at least once pvcListerSynced cache.InformerSynced // revListerSynced returns true if the rev shared informer has synced at least once revListerSynced cache.InformerSynced // StatefulSets that need to be synced. queue workqueue.RateLimitingInterface // eventBroadcaster is the core of event processing pipeline. eventBroadcaster record.EventBroadcaster } // 这段Go代码定义了一个名为StatefulSetController的结构体，用于控制StatefulSets。 // 它包含了一系列的成员变量，包括client接口、控制StatefulSet同步的接口、用于patching pods的接口、以及一系列的Lister和Synced函数， // 用于从共享informer的存储中获取Pods、StatefulSets、PVCs等信息。 // 此外，它还定义了一个工作队列和一个事件广播器。这个结构体主要用于管理和同步Kubernetes中的StatefulSets。 // NewStatefulSetController creates a new statefulset controller. func NewStatefulSetController( ctx context.Context, podInformer coreinformers.PodInformer, setInformer appsinformers.StatefulSetInformer, pvcInformer coreinformers.PersistentVolumeClaimInformer, revInformer appsinformers.ControllerRevisionInformer, kubeClient clientset.Interface, ) *StatefulSetController { logger := klog.FromContext(ctx) eventBroadcaster := record.NewBroadcaster(record.WithContext(ctx)) recorder := eventBroadcaster.NewRecorder(scheme.Scheme, v1.EventSource{Component: \u0026#34;statefulset-controller\u0026#34;}) ssc := \u0026amp;StatefulSetController{ kubeClient: kubeClient, control: NewDefaultStatefulSetControl( NewStatefulPodControl( kubeClient, podInformer.Lister(), pvcInformer.Lister(), recorder), NewRealStatefulSetStatusUpdater(kubeClient, setInformer.Lister()), history.NewHistory(kubeClient, revInformer.Lister()), recorder, ), pvcListerSynced: pvcInformer.Informer().HasSynced, revListerSynced: revInformer.Informer().HasSynced, queue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), \u0026#34;statefulset\u0026#34;), podControl: controller.RealPodControl{KubeClient: kubeClient, Recorder: recorder}, eventBroadcaster: eventBroadcaster, } //该函数用于创建一个新的StatefulSet控制器。它接收多个参数，包括上下文、各种Informer、客户端接口等。 //函数内部通过调用NewDefaultStatefulSetControl函数和其他函数，初始化了StatefulSetController结构体的各个字段， //并返回一个指向StatefulSetController结构体的指针。 podInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ // lookup the statefulset and enqueue AddFunc: func(obj interface{}) { ssc.addPod(logger, obj) }, // lookup current and old statefulset if labels changed UpdateFunc: func(oldObj, newObj interface{}) { ssc.updatePod(logger, oldObj, newObj) }, // lookup statefulset accounting for deletion tombstones DeleteFunc: func(obj interface{}) { ssc.deletePod(logger, obj) }, }) //这段代码定义了一个Go函数，它为podInformer的Informer添加了一个事件处理程序。 //该事件处理程序使用cache.ResourceEventHandlerFuncs结构体中的AddFunc、UpdateFunc和DeleteFunc来处理资源的添加、更新和删除事件。 //- AddFunc函数在资源添加时调用ssc.addPod(logger, obj)来添加Pod。 //- UpdateFunc函数在资源更新时调用ssc.updatePod(logger, oldObj, newObj)来更新Pod。 //- DeleteFunc函数在资源删除时调用ssc.deletePod(logger, obj)来删除Pod。 //这些函数通过传入的logger和相应的资源对象来执行具体的操作。 ssc.podLister = podInformer.Lister() ssc.podListerSynced = podInformer.Informer().HasSynced setInformer.Informer().AddEventHandler( cache.ResourceEventHandlerFuncs{ AddFunc: ssc.enqueueStatefulSet, UpdateFunc: func(old, cur interface{}) { oldPS := old.(*apps.StatefulSet) curPS := cur.(*apps.StatefulSet) if oldPS.Status.Replicas != curPS.Status.Replicas { logger.V(4).Info(\u0026#34;Observed updated replica count for StatefulSet\u0026#34;, \u0026#34;statefulSet\u0026#34;, klog.KObj(curPS), \u0026#34;oldReplicas\u0026#34;, oldPS.Status.Replicas, \u0026#34;newReplicas\u0026#34;, curPS.Status.Replicas) } ssc.enqueueStatefulSet(cur) }, DeleteFunc: ssc.enqueueStatefulSet, }, ) //这段代码主要涉及到了Kubernetes Informer机制的使用，用于监听StatefulSet资源的变化，并对变化事件进行处理。 //1. 首先，ssc.podLister = podInformer.Lister()和ssc.podListerSynced = podInformer.Informer().HasSynced //两行代码是将pod的Lister和是否同步完成的函数赋值给ssc对象的对应字段。Lister是用来从Informer的cache中获取资源列表和单个资源的， //HasSynced则是用来判断Informer是否已经完成初始的资源同步。 //2. 接下来，setInformer.Informer().AddEventHandler(...)这行代码是给setInformer添加了一个事件处理函数。 //其中，cache.ResourceEventHandlerFuncs是一个包含了资源添加、更新和删除事件处理函数的结构体。 //- AddFunc: ssc.enqueueStatefulSet表示当有StatefulSet资源被添加时，调用ssc.enqueueStatefulSet函数来处理该事件。 //- UpdateFunc表示当有StatefulSet资源被更新时，会先判断其副本数是否发生了变化，如果有变化则打印日志并调用ssc.enqueueStatefulSet函数来处理该事件。 //- DeleteFunc: ssc.enqueueStatefulSet表示当有StatefulSet资源被删除时，调用ssc.enqueueStatefulSet函数来处理该事件。 //综上所述，这段代码通过使用Kubernetes Informer机制监听StatefulSet资源的变化，并调用ssc.enqueueStatefulSet函数来处理这些变化事件。 ssc.setLister = setInformer.Lister() ssc.setListerSynced = setInformer.Informer().HasSynced // TODO: Watch volumes return ssc } // 这个Go函数主要设置了ssc对象的setLister和setListerSynced属性。 // setLister是通过setInformer.Lister()方法设置的，它用于从存储中获取设置列表。setListerSynced是通过setInformer.Informer().HasSynced方法设置的， // 它用于检查设置的同步状态。最后，该函数返回ssc对象。 // Run runs the statefulset controller. func (ssc *StatefulSetController) Run(ctx context.Context, workers int) { defer utilruntime.HandleCrash() // Start events processing pipeline. ssc.eventBroadcaster.StartStructuredLogging(3) ssc.eventBroadcaster.StartRecordingToSink(\u0026amp;v1core.EventSinkImpl{Interface: ssc.kubeClient.CoreV1().Events(\u0026#34;\u0026#34;)}) defer ssc.eventBroadcaster.Shutdown() defer ssc.queue.ShutDown() logger := klog.FromContext(ctx) logger.Info(\u0026#34;Starting stateful set controller\u0026#34;) defer logger.Info(\u0026#34;Shutting down statefulset controller\u0026#34;) //该函数是StatefulSetController类型的Run方法，用于启动statefulset控制器。 //函数首先处理崩溃情况，然后开启事件处理管道，记录事件并将其发送到事件接收器。 //之后，函数通过调用queue的ShutDown方法来关闭队列。函数在启动和关闭控制器时分别记录日志信息。 if !cache.WaitForNamedCacheSync(\u0026#34;stateful set\u0026#34;, ctx.Done(), ssc.podListerSynced, ssc.setListerSynced, ssc.pvcListerSynced, ssc.revListerSynced) { return } for i := 0; i \u0026lt; workers; i++ { go wait.UntilWithContext(ctx, ssc.worker, time.Second) } \u0026lt;-ctx.Done() } // 这段Go代码主要实现了以下功能： // - 等待多个缓存同步完成：通过调用cache.WaitForNamedCacheSync方法， // 等待podListerSynced、setListerSynced、pvcListerSynced和revListerSynced四个缓存同步完成。 // 如果在指定的上下文ctx被取消或超时之前，所有缓存都已同步，则继续执行后续代码；否则直接返回。 // - 启动多个工作协程：使用for循环和go关键字，启动workers个工作协程，并在每个协程中定期执行ssc.worker函数，其间隔时间为1秒。 // 这些工作协程会一直运行，直到上下文ctx被取消或超时。 // - 等待上下文取消或超时：通过读取ctx.Done()通道，阻塞当前协程，等待上下文ctx被取消或超时。 // 一旦上下文被取消或超时，整个函数执行结束。 这段代码通常用于在启动时等待多个缓存同步完成，并启动多个工作协程来处理后续任务。 // 通过上下文ctx来控制整个函数的运行时长，并能够在缓存同步或工作协程运行过程中随时取消操作。 // addPod adds the statefulset for the pod to the sync queue func (ssc *StatefulSetController) addPod(logger klog.Logger, obj interface{}) { pod := obj.(*v1.Pod) if pod.DeletionTimestamp != nil { // on a restart of the controller manager, it\u0026#39;s possible a new pod shows up in a state that // is already pending deletion. Prevent the pod from being a creation observation. ssc.deletePod(logger, pod) return } // If it has a ControllerRef, that\u0026#39;s all that matters. if controllerRef := metav1.GetControllerOf(pod); controllerRef != nil { set := ssc.resolveControllerRef(pod.Namespace, controllerRef) if set == nil { return } logger.V(4).Info(\u0026#34;Pod created with labels\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;labels\u0026#34;, pod.Labels) ssc.enqueueStatefulSet(set) return } //该函数是StatefulSetController类型的一个方法，用于将Pod的状态添加到同步队列中。 //首先，它检查Pod是否已被标记为删除，如果是，则调用deletePod方法删除Pod，并返回。 //然后，它检查Pod是否有ControllerRef，如果有，则通过resolveControllerRef方法解析ControllerRef， //并将解析结果传递给enqueueStatefulSet方法，以将StatefulSet添加到同步队列中。 // Otherwise, it\u0026#39;s an orphan. Get a list of all matching controllers and sync // them to see if anyone wants to adopt it. sets := ssc.getStatefulSetsForPod(pod) if len(sets) == 0 { return } logger.V(4).Info(\u0026#34;Orphan Pod created with labels\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;labels\u0026#34;, pod.Labels) for _, set := range sets { ssc.enqueueStatefulSet(set) } } // 该函数用于处理一个无归属的Pod（孤儿Pod），通过获取与该Pod匹配的所有StatefulSet控制器，并将它们同步，以查看是否有控制器愿意“收养”这个Pod。 // 首先，函数会调用ssc.getStatefulSetsForPod(pod)方法来获取与Pod匹配的所有StatefulSet集合。 // 如果这个集合的长度为0，说明没有找到匹配的控制器，那么函数直接返回。 // 否则，函数会使用logger记录一条日志信息，以4级日志级别记录“创建了一个孤儿Pod，并列出了Pod的标签”。 // 然后，函数会遍历这个匹配的StatefulSet集合，对每个集合调用ssc.enqueueStatefulSet(set)方法，将其加入到同步队列中，以便后续进行同步处理。 // updatePod adds the statefulset for the current and old pods to the sync queue. func (ssc *StatefulSetController) updatePod(logger klog.Logger, old, cur interface{}) { curPod := cur.(*v1.Pod) oldPod := old.(*v1.Pod) if curPod.ResourceVersion == oldPod.ResourceVersion { // In the event of a re-list we may receive update events for all known pods. // Two different versions of the same pod will always have different RVs. return } labelChanged := !reflect.DeepEqual(curPod.Labels, oldPod.Labels) curControllerRef := metav1.GetControllerOf(curPod) oldControllerRef := metav1.GetControllerOf(oldPod) controllerRefChanged := !reflect.DeepEqual(curControllerRef, oldControllerRef) if controllerRefChanged \u0026amp;\u0026amp; oldControllerRef != nil { // The ControllerRef was changed. Sync the old controller, if any. if set := ssc.resolveControllerRef(oldPod.Namespace, oldControllerRef); set != nil { ssc.enqueueStatefulSet(set) } } //该函数用于将当前和旧的Pod的状态fulset添加到同步队列中。 //函数首先将传入的old和cur参数转换为v1.Pod类型，并检查它们的ResourceVersion是否相同。 //如果相同，则表示这是由于重新列表而导致的更新事件，函数将直接返回。 //然后，函数会检查Pod的标签是否发生变化，并检查ControllerRef是否发生变化。 //如果ControllerRef发生变化且旧的ControllerRef不为空，则函数将解析旧的ControllerRef，并将对应的StatefulSet添加到同步队列中。 // If it has a ControllerRef, that\u0026#39;s all that matters. if curControllerRef != nil { set := ssc.resolveControllerRef(curPod.Namespace, curControllerRef) if set == nil { return } logger.V(4).Info(\u0026#34;Pod objectMeta updated\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(curPod), \u0026#34;oldObjectMeta\u0026#34;, oldPod.ObjectMeta, \u0026#34;newObjectMeta\u0026#34;, curPod.ObjectMeta) ssc.enqueueStatefulSet(set) // TODO: MinReadySeconds in the Pod will generate an Available condition to be added in // the Pod status which in turn will trigger a requeue of the owning replica set thus // having its status updated with the newly available replica. if !podutil.IsPodReady(oldPod) \u0026amp;\u0026amp; podutil.IsPodReady(curPod) \u0026amp;\u0026amp; set.Spec.MinReadySeconds \u0026gt; 0 { logger.V(2).Info(\u0026#34;StatefulSet will be enqueued after minReadySeconds for availability check\u0026#34;, \u0026#34;statefulSet\u0026#34;, klog.KObj(set), \u0026#34;minReadySeconds\u0026#34;, set.Spec.MinReadySeconds) // Add a second to avoid milliseconds skew in AddAfter. // See https://github.com/kubernetes/kubernetes/issues/39785#issuecomment-279959133 for more info. ssc.enqueueSSAfter(set, (time.Duration(set.Spec.MinReadySeconds)*time.Second)+time.Second) } return } //该Go函数是一个处理Pod对象元数据更新的函数。 //它首先检查当前Pod是否有ControllerRef，如果有，则通过resolveControllerRef方法解析其引用的Controller，并将其加入到队列中以进行后续处理。 //如果Pod满足特定条件（例如，从不可用状态变为可用状态，并且其所属的StatefulSet的MinReadySeconds大于0）， //则会将StatefulSet加入到队列中， 以便在MinReadySeconds秒后进行可用性检查。 // Otherwise, it\u0026#39;s an orphan. If anything changed, sync matching controllers // to see if anyone wants to adopt it now. if labelChanged || controllerRefChanged { sets := ssc.getStatefulSetsForPod(curPod) if len(sets) == 0 { return } logger.V(4).Info(\u0026#34;Orphan Pod objectMeta updated\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(curPod), \u0026#34;oldObjectMeta\u0026#34;, oldPod.ObjectMeta, \u0026#34;newObjectMeta\u0026#34;, curPod.ObjectMeta) for _, set := range sets { ssc.enqueueStatefulSet(set) } } } // 这段Go代码是处理Pod对象的元数据更新的逻辑。 // 如果Pod的标签或控制器引用发生了变化，它会检查是否有StatefulSet控制器可以“收养”这个Pod。 // 如果有，则将这个StatefulSet控制器加入到队列中，以便进一步处理。 // deletePod enqueues the statefulset for the pod accounting for deletion tombstones. func (ssc *StatefulSetController) deletePod(logger klog.Logger, obj interface{}) { pod, ok := obj.(*v1.Pod) // When a delete is dropped, the relist will notice a pod in the store not // in the list, leading to the insertion of a tombstone object which contains // the deleted key/value. Note that this value might be stale. if !ok { tombstone, ok := obj.(cache.DeletedFinalStateUnknown) if !ok { utilruntime.HandleError(fmt.Errorf(\u0026#34;couldn\u0026#39;t get object from tombstone %+v\u0026#34;, obj)) return } pod, ok = tombstone.Obj.(*v1.Pod) if !ok { utilruntime.HandleError(fmt.Errorf(\u0026#34;tombstone contained object that is not a pod %+v\u0026#34;, obj)) return } } //该函数是一个Go语言函数，名为deletePod，它属于StatefulSetController类型。 //该函数的功能是将删除操作对应的状态fulset加入到队列中，并考虑到删除标记（tombstones）。 //函数参数： - logger：klog.Logger类型，用于记录日志。 //- obj：interface{}类型，表示要删除的Pod对象。 //函数流程： 1. 尝试将obj对象转换为*v1.Pod类型。 //2. 如果转换失败，则尝试将obj对象转换为cache.DeletedFinalStateUnknown类型。 //3. 如果转换成功，则从tombstone对象中获取被删除的Pod对象。 //4. 如果获取失败，则记录错误信息并返回。 //5. 如果成功获取到Pod对象，则将其加入到队列中，以供后续处理。 //该函数主要处理了两种情况：直接删除Pod对象和通过tombstone对象删除Pod。 //在处理过程中，函数会进行类型断言来确保对象类型正确，并在出现异常时记录错误信息。 controllerRef := metav1.GetControllerOf(pod) if controllerRef == nil { // No controller should care about orphans being deleted. return } set := ssc.resolveControllerRef(pod.Namespace, controllerRef) if set == nil { return } logger.V(4).Info(\u0026#34;Pod deleted.\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;caller\u0026#34;, utilruntime.GetCaller()) ssc.enqueueStatefulSet(set) } // 该函数是一个Go语言函数，名为deletePod，它属于StatefulSetController类型。 // 该函数的功能是在删除Pod时，检查是否有对应的控制器（StatefulSet）对该Pod有管理关系， // 如果有，则将该StatefulSet加入到队列中，以触发相应的更新操作。 // 函数流程： // 1. 通过metav1.GetControllerOf函数获取Pod的控制器引用（controllerRef）。 // 2. 如果controllerRef为nil，表示该Pod没有对应的控制器，则直接返回。 // 3. 调用ssc.resolveControllerRef函数解析controllerRef，获取对应的StatefulSet对象（set）。 // 4. 如果set为nil，表示无法解析到对应的StatefulSet，则直接返回。 // 5. 记录日志信息，表示Pod已被删除。 // 6. 将set加入到队列中，以触发StatefulSet的更新操作。 // 该函数主要通过controllerRef来确定Pod所属的StatefulSet，并在删除Pod时，通知对应的StatefulSet进行相应的更新操作。 // 如果无法确定Pod的控制器或者无法解析到对应的StatefulSet，则函数直接返回，不做任何处理。 // getPodsForStatefulSet returns the Pods that a given StatefulSet should manage. // It also reconciles ControllerRef by adopting/orphaning. // // NOTE: Returned Pods are pointers to objects from the cache. // If you need to modify one, you need to copy it first. func (ssc *StatefulSetController) getPodsForStatefulSet(ctx context.Context, set *apps.StatefulSet, selector labels.Selector) ([]*v1.Pod, error) { // List all pods to include the pods that don\u0026#39;t match the selector anymore but // has a ControllerRef pointing to this StatefulSet. pods, err := ssc.podLister.Pods(set.Namespace).List(labels.Everything()) if err != nil { return nil, err } filter := func(pod *v1.Pod) bool { // Only claim if it matches our StatefulSet name. Otherwise release/ignore. return isMemberOf(set, pod) } cm := controller.NewPodControllerRefManager(ssc.podControl, set, selector, controllerKind, ssc.canAdoptFunc(ctx, set)) return cm.ClaimPods(ctx, pods, filter) } // 该函数用于获取指定StatefulSet应该管理的Pods，并通过adopting/orphaning来协调ControllerRef。 // 函数首先从缓存中列出所有Pods，包括不再匹配选择器但具有指向该StatefulSet的ControllerRef的Pods。 // 然后通过isMemberOf函数筛选出属于该StatefulSet的Pods。 // 最后，使用PodControllerRefManager的ClaimPods方法来处理ControllerRef的采用/孤儿化，并返回属于该StatefulSet的Pods的指针。 // 注意：返回的Pods是来自缓存的对象指针，如果需要修改，则需要先复制。 // If any adoptions are attempted, we should first recheck for deletion with // an uncached quorum read sometime after listing Pods/ControllerRevisions (see #42639). func (ssc *StatefulSetController) canAdoptFunc(ctx context.Context, set *apps.StatefulSet) func(ctx2 context.Context) error { return controller.RecheckDeletionTimestamp(func(ctx context.Context) (metav1.Object, error) { fresh, err := ssc.kubeClient.AppsV1().StatefulSets(set.Namespace).Get(ctx, set.Name, metav1.GetOptions{}) if err != nil { return nil, err } if fresh.UID != set.UID { return nil, fmt.Errorf(\u0026#34;original StatefulSet %v/%v is gone: got uid %v, wanted %v\u0026#34;, set.Namespace, set.Name, fresh.UID, set.UID) } return fresh, nil }) } // 该函数是一个生成函数，返回一个函数，用于检查是否有资格收养（adopt）某个StatefulSet。 // 收养的条件是：在尝试收养之后，需要再次使用未缓存的多数读取（uncached quorum read）检查该StatefulSet是否已被删除。 // 函数内部通过调用ssc.kubeClient.AppsV1().StatefulSets(set.Namespace).Get()来获取最新的StatefulSet信息，并与原有信息进行对比， // 如果UID一致，则返回最新的StatefulSet对象；否则返回错误信息。 // adoptOrphanRevisions adopts any orphaned ControllerRevisions matched by set\u0026#39;s Selector. func (ssc *StatefulSetController) adoptOrphanRevisions(ctx context.Context, set *apps.StatefulSet) error { revisions, err := ssc.control.ListRevisions(set) if err != nil { return err } orphanRevisions := make([]*apps.ControllerRevision, 0) for i := range revisions { if metav1.GetControllerOf(revisions[i]) == nil { orphanRevisions = append(orphanRevisions, revisions[i]) } } if len(orphanRevisions) \u0026gt; 0 { canAdoptErr := ssc.canAdoptFunc(ctx, set)(ctx) if canAdoptErr != nil { return fmt.Errorf(\u0026#34;can\u0026#39;t adopt ControllerRevisions: %v\u0026#34;, canAdoptErr) } return ssc.control.AdoptOrphanRevisions(set, orphanRevisions) } return nil } // 该函数用于收养被控制器创建但没有被任何StatefulSet认领的ControllerRevision对象。具体流程如下： // 1. 调用ssc.control.ListRevisions(set)获取与给定StatefulSet相关联的所有ControllerRevision对象。 // 2. 遍历所有ControllerRevision对象，如果其metav1.GetControllerOf字段为空，则将其加入orphanRevisions列表。 // 3. 如果orphanRevisions列表非空，则调用ssc.canAdoptFunc(ctx, set)(ctx)判断是否可以收养这些孤儿Revision。 // 4. 如果可以收养，则调用ssc.control.AdoptOrphanRevisions(set, orphanRevisions)进行收养操作。 // 5. 如果收养过程中出现错误，则返回错误信息；否则返回nil表示成功。 // getStatefulSetsForPod returns a list of StatefulSets that potentially match // a given pod. func (ssc *StatefulSetController) getStatefulSetsForPod(pod *v1.Pod) []*apps.StatefulSet { sets, err := ssc.setLister.GetPodStatefulSets(pod) if err != nil { return nil } // More than one set is selecting the same Pod if len(sets) \u0026gt; 1 { // ControllerRef will ensure we don\u0026#39;t do anything crazy, but more than one // item in this list nevertheless constitutes user error. setNames := []string{} for _, s := range sets { setNames = append(setNames, s.Name) } utilruntime.HandleError( fmt.Errorf( \u0026#34;user error: more than one StatefulSet is selecting pods with labels: %+v. Sets: %v\u0026#34;, pod.Labels, setNames)) } return sets } // 该函数用于获取与给定Pod相匹配的所有StatefulSet列表。 // - 首先，函数通过调用ssc.setLister.GetPodStatefulSets(pod)方法获取选择该Pod的所有StatefulSet。 // - 如果获取过程中出现错误，则直接返回nil。 // - 如果存在多个StatefulSet选择同一个Pod（即sets长度大于1），则将其视为用户错误。 // 函数会记录错误信息，并返回所有匹配的StatefulSet列表。 // 注意：在存在多个StatefulSet选择同一个Pod时，通过ControllerRef可以确保不会发生疯狂的行为。 // 但是多个StatefulSet出现在列表中仍然被认为是用户错误。 // resolveControllerRef returns the controller referenced by a ControllerRef, // or nil if the ControllerRef could not be resolved to a matching controller // of the correct Kind. func (ssc *StatefulSetController) resolveControllerRef(namespace string, controllerRef *metav1.OwnerReference) *apps.StatefulSet { // We can\u0026#39;t look up by UID, so look up by Name and then verify UID. // Don\u0026#39;t even try to look up by Name if it\u0026#39;s the wrong Kind. if controllerRef.Kind != controllerKind.Kind { return nil } set, err := ssc.setLister.StatefulSets(namespace).Get(controllerRef.Name) if err != nil { return nil } if set.UID != controllerRef.UID { // The controller we found with this Name is not the same one that the // ControllerRef points to. return nil } return set } // 该函数用于解析ControllerRef引用的控制器，如果引用无法解析为正确的类型，则返回nil。 // 具体步骤如下： 1. 检查controllerRef的类型是否与期望的类型相匹配，如果不匹配则直接返回nil。 // 2. 通过名称查找StatefulSet控制器，如果查找失败则返回nil。 // 3. 验证找到的StatefulSet控制器的UID是否与controllerRef的UID相同，如果不同则返回nil。 // 4. 如果以上验证都通过，则返回找到的StatefulSet控制器。 // enqueueStatefulSet enqueues the given statefulset in the work queue. func (ssc *StatefulSetController) enqueueStatefulSet(obj interface{}) { key, err := controller.KeyFunc(obj) if err != nil { utilruntime.HandleError(fmt.Errorf(\u0026#34;couldn\u0026#39;t get key for object %+v: %v\u0026#34;, obj, err)) return } ssc.queue.Add(key) } // 该函数用于将给定的状态fulset对象加入工作队列中。 // 函数首先通过controller.KeyFunc方法获取对象的键值，如果获取失败，则通过utilruntime.HandleError方法记录错误信息并返回。 // 最后，将获取到的键值加入到ssc.queue队列中。 // enqueueStatefulSet enqueues the given statefulset in the work queue after given time func (ssc *StatefulSetController) enqueueSSAfter(ss *apps.StatefulSet, duration time.Duration) { key, err := controller.KeyFunc(ss) if err != nil { utilruntime.HandleError(fmt.Errorf(\u0026#34;couldn\u0026#39;t get key for object %#v: %v\u0026#34;, ss, err)) return } ssc.queue.AddAfter(key, duration) } // 该函数将给定的状态集（StatefulSet）在指定时间后加入到工作队列中。 // 首先，函数通过调用controller.KeyFunc(ss)方法获取状态集的键值。 // 如果获取键值时出现错误，则使用utilruntime.HandleError()方法处理错误并返回。 // 最后，将键值和指定时间间隔添加到ssc.queue队列中，以便后续处理。 // processNextWorkItem dequeues items, processes them, and marks them done. It enforces that the syncHandler is never // invoked concurrently with the same key. func (ssc *StatefulSetController) processNextWorkItem(ctx context.Context) bool { key, quit := ssc.queue.Get() if quit { return false } defer ssc.queue.Done(key) if err := ssc.sync(ctx, key.(string)); err != nil { utilruntime.HandleError(fmt.Errorf(\u0026#34;error syncing StatefulSet %v, requeuing: %v\u0026#34;, key.(string), err)) ssc.queue.AddRateLimited(key) } else { ssc.queue.Forget(key) } return true } // 该函数是一个处理工作项的函数，它从队列中获取工作项，处理它，并将其标记为完成。 // 它确保同步处理程序永远不会使用相同的键并发执行。 // 函数返回一个布尔值，表示是否继续处理下一个工作项。 // worker runs a worker goroutine that invokes processNextWorkItem until the controller\u0026#39;s queue is closed func (ssc *StatefulSetController) worker(ctx context.Context) { for ssc.processNextWorkItem(ctx) { } } // 该函数是一个goroutine，用于不断调用processNextWorkItem处理工作项，直到控制器的队列关闭。 // sync syncs the given statefulset. func (ssc *StatefulSetController) sync(ctx context.Context, key string) error { startTime := time.Now() logger := klog.FromContext(ctx) defer func() { logger.V(4).Info(\u0026#34;Finished syncing statefulset\u0026#34;, \u0026#34;key\u0026#34;, key, \u0026#34;time\u0026#34;, time.Since(startTime)) }() namespace, name, err := cache.SplitMetaNamespaceKey(key) //该函数是一个Go语言函数，它定义在名为StatefulSetController的结构体中，使用sync作为函数名。 //该函数接收两个参数：ctx context.Context和key string，并返回一个error类型的值。 //函数主要功能如下： //1. 记录开始时间：在函数开始处，通过调用time.Now()记录当前时间，用于后续计算函数执行时间。 //2. 获取日志记录器：通过从传入的ctx参数中获取日志记录器，用于记录函数执行的日志信息。 //3. 延迟执行日志记录：使用defer关键字定义延迟执行的函数，该函数会在函数返回前执行。 //延迟函数主要记录函数执行结束时间，并通过日志记录器记录函数执行时间及关键参数信息。 //4. 分解键值：通过调用cache.SplitMetaNamespaceKey(key)函数，将传入的键值key分解为命名空间namespace和名称name两个部分。 //整体而言，该函数的主要作用是同步处理某个键值对应的StatefulSet对象，具体处理逻辑在该函数的后续代码中实现。 //该函数通过记录函数执行时间、获取日志记录器以及分解键值等操作，为后续处理提供了必要信息。 if err != nil { return err } set, err := ssc.setLister.StatefulSets(namespace).Get(name) //该函数主要执行以下操作： //1. 检查err是否为nil，如果不为nil，则直接返回错误。 //2. 调用ssc.setLister.StatefulSets(namespace).Get(name)获取指定命名空间中的状态集，并将其赋值给set变量。 //3. 返回set和err，其中set为获取的状态集，err为执行过程中可能出现的错误。 if errors.IsNotFound(err) { logger.Info(\u0026#34;StatefulSet has been deleted\u0026#34;, \u0026#34;key\u0026#34;, key) return nil } //这段Go代码主要进行错误判断和日志记录。具体功能如下： //- 首先，它检查错误变量err是否为NotFound错误（即该错误表示某个资源未找到）。 //- 如果是NotFound错误，它会通过logger.Info方法记录一条日志信息，指示某个StatefulSet已被删除，同时在日志中包含key参数的值。 //- 最后，函数返回nil，表示处理完成且无需进一步处理。 //这段代码的作用是在遇到特定类型的错误时，记录一条相关日志信息，并终止进一步的错误处理流程。 if err != nil { utilruntime.HandleError(fmt.Errorf(\u0026#34;unable to retrieve StatefulSet %v from store: %v\u0026#34;, key, err)) return err } //这段Go代码是一个错误处理的示例。 //如果变量err不为nil，则会使用utilruntime.HandleError()函数处理错误， //该函数会将一个格式化后的错误信息打印出来。然后函数会返回err。这段代码的作用是在发生错误时记录错误信息并返回错误。 selector, err := metav1.LabelSelectorAsSelector(set.Spec.Selector) if err != nil { utilruntime.HandleError(fmt.Errorf(\u0026#34;error converting StatefulSet %v selector: %v\u0026#34;, key, err)) // This is a non-transient error, so don\u0026#39;t retry. return nil } //这段Go代码主要实现了将StatefulSet的标签选择器转换为Selector的功能。 //具体来说，它首先通过metav1.LabelSelectorAsSelector函数将set.Spec.Selector转换为Selector类型， //如果转换过程中出现错误，则通过utilruntime.HandleError函数记录错误信息，并返回nil。 //需要注意的是，这里的错误被认为是非瞬时错误，因此不会进行重试。 if err := ssc.adoptOrphanRevisions(ctx, set); err != nil { return err } //这个函数调用了ssc.adoptOrphanRevisions方法，该方法的作用是在给定的上下文ctx中采用孤儿修订版本。如果方法执行出错，则会返回错误信息err。 pods, err := ssc.getPodsForStatefulSet(ctx, set, selector) if err != nil { return err } //该函数尝试通过调用ssc.getPodsForStatefulSet方法获取与指定StatefulSet和选择器相关联的Pods。如果获取过程中出现错误，则将错误返回。 return ssc.syncStatefulSet(ctx, set, pods) //该函数是Go语言编写的，用于同步更新StatefulSet（有状态副本集）的状态。 //- ctx是一个上下文对象，用于控制函数执行的生命周期。 //- set是一个StatefulSet对象，表示要更新的有状态副本集。 //- pods是一个Pod对象的列表，表示有状态副本集中的Pods。 //函数内部会根据传入的有状态副本集和Pods对象，进行一系列操作来更新副本集的状态，使其与实际的Pods状态保持一致。具体操作细节可以根据函数实现来确定。 } // syncStatefulSet syncs a tuple of (statefulset, []*v1.Pod). func (ssc *StatefulSetController) syncStatefulSet(ctx context.Context, set *apps.StatefulSet, pods []*v1.Pod) error { logger := klog.FromContext(ctx) logger.V(4).Info(\u0026#34;Syncing StatefulSet with pods\u0026#34;, \u0026#34;statefulSet\u0026#34;, klog.KObj(set), \u0026#34;pods\u0026#34;, len(pods)) var status *apps.StatefulSetStatus var err error status, err = ssc.control.UpdateStatefulSet(ctx, set, pods) if err != nil { return err } logger.V(4).Info(\u0026#34;Successfully synced StatefulSet\u0026#34;, \u0026#34;statefulSet\u0026#34;, klog.KObj(set)) // One more sync to handle the clock skew. This is also helping in requeuing right after status update if set.Spec.MinReadySeconds \u0026gt; 0 \u0026amp;\u0026amp; status != nil \u0026amp;\u0026amp; status.AvailableReplicas != *set.Spec.Replicas { ssc.enqueueSSAfter(set, time.Duration(set.Spec.MinReadySeconds)*time.Second) } return nil } //该函数是一个用于同步StatefulSet和其关联Pods的函数。 //它通过调用ssc.control.UpdateStatefulSet方法来更新StatefulSet的状态， //并在更新完成后进行额外的同步操作来处理时钟偏移。 //如果StatefulSet的MinReadySeconds大于0，并且更新后的状态的AvailableReplicas与Spec.Replicas不相等，则会重新排队等待处理。 "},{"id":111,"href":"/docs/k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E4%B9%8Bsyncgo-%E5%90%8C%E6%AD%A5-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-kong-zhi-qi-zhi-syncgo-tong-bu-yuan-ma-jie-du/","title":"K8S控制器之sync.go 同步 源码解读 2024-04-09 11:33:26.356","section":"Docs","content":"/* Copyright 2016 The Kubernetes Authors. Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \u0026#34;AS IS\u0026#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */ package deployment import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;reflect\u0026#34; \u0026#34;sort\u0026#34; \u0026#34;strconv\u0026#34; apps \u0026#34;k8s.io/api/apps/v1\u0026#34; v1 \u0026#34;k8s.io/api/core/v1\u0026#34; \u0026#34;k8s.io/apimachinery/pkg/api/errors\u0026#34; metav1 \u0026#34;k8s.io/apimachinery/pkg/apis/meta/v1\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/controller\u0026#34; deploymentutil \u0026#34;k8s.io/kubernetes/pkg/controller/deployment/util\u0026#34; labelsutil \u0026#34;k8s.io/kubernetes/pkg/util/labels\u0026#34; ) // syncStatusOnly only updates Deployments Status and doesn\u0026#39;t take any mutating actions. func (dc *DeploymentController) syncStatusOnly(ctx context.Context, d *apps.Deployment, rsList []*apps.ReplicaSet) error { newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(ctx, d, rsList, false) if err != nil { return err } allRSs := append(oldRSs, newRS) return dc.syncDeploymentStatus(ctx, allRSs, newRS, d) } //该函数用于同步更新Deployment的状态，而不进行任何变更操作。 //它通过调用getAllReplicaSetsAndSyncRevision方法获取所有ReplicaSet的列表，并将新旧ReplicaSet区分开来。 //然后，它将所有ReplicaSet附加到旧ReplicaSet列表中，并调用syncDeploymentStatus方法来同步更新Deployment的状态。 //如果在获取ReplicaSet列表时出现错误，则返回该错误。 // sync is responsible for reconciling deployments on scaling events or when they // are paused. func (dc *DeploymentController) sync(ctx context.Context, d *apps.Deployment, rsList []*apps.ReplicaSet) error { newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(ctx, d, rsList, false) if err != nil { return err } if err := dc.scale(ctx, d, newRS, oldRSs); err != nil { // If we get an error while trying to scale, the deployment will be requeued // so we can abort this resync return err } // Clean up the deployment when it\u0026#39;s paused and no rollback is in flight. if d.Spec.Paused \u0026amp;\u0026amp; getRollbackTo(d) == nil { if err := dc.cleanupDeployment(ctx, oldRSs, d); err != nil { return err } } allRSs := append(oldRSs, newRS) return dc.syncDeploymentStatus(ctx, allRSs, newRS, d) } //该函数是一个Go语言函数，定义在DeploymentController结构体中，名为sync。它负责在缩放事件或暂停时协调部署。 //函数使用context.Context作为上下文，接收一个*apps.Deployment类型的参数d和一个[]*apps.ReplicaSet类型的参数rsList，返回一个error类型的值。 //函数主要执行以下操作： //1. 调用getAllReplicaSetsAndSyncRevision方法获取所有复制集并同步修订版本，返回新复制集、旧复制集和错误（如果有）。 //2. 如果scale方法调用失败，则返回错误，以便重新排队处理。 //3. 如果部署被暂停并且没有进行中的回滚，则尝试清理部署。 //4. 将旧复制集和新复制集合并为一个列表，并调用syncDeploymentStatus方法来同步部署状态。 //最后，函数返回可能的错误。 // checkPausedConditions checks if the given deployment is paused or not and adds an appropriate condition. // These conditions are needed so that we won\u0026#39;t accidentally report lack of progress for resumed deployments // that were paused for longer than progressDeadlineSeconds. func (dc *DeploymentController) checkPausedConditions(ctx context.Context, d *apps.Deployment) error { if !deploymentutil.HasProgressDeadline(d) { return nil } cond := deploymentutil.GetDeploymentCondition(d.Status, apps.DeploymentProgressing) if cond != nil \u0026amp;\u0026amp; cond.Reason == deploymentutil.TimedOutReason { // If we have reported lack of progress, do not overwrite it with a paused condition. return nil } pausedCondExists := cond != nil \u0026amp;\u0026amp; cond.Reason == deploymentutil.PausedDeployReason needsUpdate := false if d.Spec.Paused \u0026amp;\u0026amp; !pausedCondExists { condition := deploymentutil.NewDeploymentCondition(apps.DeploymentProgressing, v1.ConditionUnknown, deploymentutil.PausedDeployReason, \u0026#34;Deployment is paused\u0026#34;) deploymentutil.SetDeploymentCondition(\u0026amp;d.Status, *condition) needsUpdate = true } else if !d.Spec.Paused \u0026amp;\u0026amp; pausedCondExists { condition := deploymentutil.NewDeploymentCondition(apps.DeploymentProgressing, v1.ConditionUnknown, deploymentutil.ResumedDeployReason, \u0026#34;Deployment is resumed\u0026#34;) deploymentutil.SetDeploymentCondition(\u0026amp;d.Status, *condition) needsUpdate = true } if !needsUpdate { return nil } var err error _, err = dc.client.AppsV1().Deployments(d.Namespace).UpdateStatus(ctx, d, metav1.UpdateOptions{}) return err } //该函数用于检查给定的部署是否暂停，并添加适当的条件。 //这些条件是必要的，以避免意外报告恢复的部署缺乏进展，这些部署被暂停的时间超过了progressDeadlineSeconds。 //函数首先检查部署是否具有进度截止时间，如果没有，则返回nil。 //然后获取部署的状态，如果状态中存在DeploymentProgressing条件且原因等于TimedOutReason，则不添加暂停条件。 //接下来，函数检查部署是否暂停以及是否存在暂停条件。 //如果部署已暂停但不存在暂停条件，则创建一个新的DeploymentProgressing条件并设置其原因为PausedDeployReason。 //如果部署未暂停但存在暂停条件，则创建一个新的DeploymentProgressing条件并设置其原因为ResumedDeployReason。 //最后，如果条件有更新，则使用client更新部署的状态。函数返回更新状态时可能发生的错误。 // getAllReplicaSetsAndSyncRevision returns all the replica sets for the provided deployment (new and all old), with new RS\u0026#39;s and deployment\u0026#39;s revision updated. // // rsList should come from getReplicaSetsForDeployment(d). // // 1. Get all old RSes this deployment targets, and calculate the max revision number among them (maxOldV). // 2. Get new RS this deployment targets (whose pod template matches deployment\u0026#39;s), and update new RS\u0026#39;s revision number to (maxOldV + 1), // only if its revision number is smaller than (maxOldV + 1). If this step failed, we\u0026#39;ll update it in the next deployment sync loop. // 3. Copy new RS\u0026#39;s revision number to deployment (update deployment\u0026#39;s revision). If this step failed, we\u0026#39;ll update it in the next deployment sync loop. // // Note that currently the deployment controller is using caches to avoid querying the server for reads. // This may lead to stale reads of replica sets, thus incorrect deployment status. func (dc *DeploymentController) getAllReplicaSetsAndSyncRevision(ctx context.Context, d *apps.Deployment, rsList []*apps.ReplicaSet, createIfNotExisted bool) (*apps.ReplicaSet, []*apps.ReplicaSet, error) { _, allOldRSs := deploymentutil.FindOldReplicaSets(d, rsList) // Get new replica set with the updated revision number newRS, err := dc.getNewReplicaSet(ctx, d, rsList, allOldRSs, createIfNotExisted) if err != nil { return nil, nil, err } return newRS, allOldRSs, nil } const ( // limit revision history length to 100 element (~2000 chars) maxRevHistoryLengthInChars = 2000 ) // Returns a replica set that matches the intent of the given deployment. Returns nil if the new replica set doesn\u0026#39;t exist yet. // 1. Get existing new RS (the RS that the given deployment targets, whose pod template is the same as deployment\u0026#39;s). // 2. If there\u0026#39;s existing new RS, update its revision number if it\u0026#39;s smaller than (maxOldRevision + 1), where maxOldRevision is the max revision number among all old RSes. // 3. If there\u0026#39;s no existing new RS and createIfNotExisted is true, create one with appropriate revision number (maxOldRevision + 1) and replicas. // Note that the pod-template-hash will be added to adopted RSes and pods. func (dc *DeploymentController) getNewReplicaSet(ctx context.Context, d *apps.Deployment, rsList, oldRSs []*apps.ReplicaSet, createIfNotExisted bool) (*apps.ReplicaSet, error) { logger := klog.FromContext(ctx) existingNewRS := deploymentutil.FindNewReplicaSet(d, rsList) // Calculate the max revision number among all old RSes maxOldRevision := deploymentutil.MaxRevision(logger, oldRSs) // Calculate revision number for this new replica set newRevision := strconv.FormatInt(maxOldRevision+1, 10) // Latest replica set exists. We need to sync its annotations (includes copying all but // annotationsToSkip from the parent deployment, and update revision, desiredReplicas, // and maxReplicas) and also update the revision annotation in the deployment with the // latest revision. if existingNewRS != nil { rsCopy := existingNewRS.DeepCopy() // Set existing new replica set\u0026#39;s annotation annotationsUpdated := deploymentutil.SetNewReplicaSetAnnotations(ctx, d, rsCopy, newRevision, true, maxRevHistoryLengthInChars) minReadySecondsNeedsUpdate := rsCopy.Spec.MinReadySeconds != d.Spec.MinReadySeconds if annotationsUpdated || minReadySecondsNeedsUpdate { rsCopy.Spec.MinReadySeconds = d.Spec.MinReadySeconds return dc.client.AppsV1().ReplicaSets(rsCopy.ObjectMeta.Namespace).Update(ctx, rsCopy, metav1.UpdateOptions{}) } // Should use the revision in existingNewRS\u0026#39;s annotation, since it set by before needsUpdate := deploymentutil.SetDeploymentRevision(d, rsCopy.Annotations[deploymentutil.RevisionAnnotation]) // If no other Progressing condition has been recorded and we need to estimate the progress // of this deployment then it is likely that old users started caring about progress. In that // case we need to take into account the first time we noticed their new replica set. cond := deploymentutil.GetDeploymentCondition(d.Status, apps.DeploymentProgressing) if deploymentutil.HasProgressDeadline(d) \u0026amp;\u0026amp; cond == nil { msg := fmt.Sprintf(\u0026#34;Found new replica set %q\u0026#34;, rsCopy.Name) condition := deploymentutil.NewDeploymentCondition(apps.DeploymentProgressing, v1.ConditionTrue, deploymentutil.FoundNewRSReason, msg) deploymentutil.SetDeploymentCondition(\u0026amp;d.Status, *condition) needsUpdate = true } if needsUpdate { var err error if _, err = dc.client.AppsV1().Deployments(d.Namespace).UpdateStatus(ctx, d, metav1.UpdateOptions{}); err != nil { return nil, err } } return rsCopy, nil } if !createIfNotExisted { return nil, nil } // new ReplicaSet does not exist, create one. newRSTemplate := *d.Spec.Template.DeepCopy() podTemplateSpecHash := controller.ComputeHash(\u0026amp;newRSTemplate, d.Status.CollisionCount) newRSTemplate.Labels = labelsutil.CloneAndAddLabel(d.Spec.Template.Labels, apps.DefaultDeploymentUniqueLabelKey, podTemplateSpecHash) // Add podTemplateHash label to selector. newRSSelector := labelsutil.CloneSelectorAndAddLabel(d.Spec.Selector, apps.DefaultDeploymentUniqueLabelKey, podTemplateSpecHash) // Create new ReplicaSet newRS := apps.ReplicaSet{ ObjectMeta: metav1.ObjectMeta{ // Make the name deterministic, to ensure idempotence Name: d.Name + \u0026#34;-\u0026#34; + podTemplateSpecHash, Namespace: d.Namespace, OwnerReferences: []metav1.OwnerReference{*metav1.NewControllerRef(d, controllerKind)}, Labels: newRSTemplate.Labels, }, Spec: apps.ReplicaSetSpec{ Replicas: new(int32), MinReadySeconds: d.Spec.MinReadySeconds, Selector: newRSSelector, Template: newRSTemplate, }, } allRSs := append(oldRSs, \u0026amp;newRS) newReplicasCount, err := deploymentutil.NewRSNewReplicas(d, allRSs, \u0026amp;newRS) if err != nil { return nil, err } *(newRS.Spec.Replicas) = newReplicasCount // Set new replica set\u0026#39;s annotation deploymentutil.SetNewReplicaSetAnnotations(ctx, d, \u0026amp;newRS, newRevision, false, maxRevHistoryLengthInChars) // Create the new ReplicaSet. If it already exists, then we need to check for possible // hash collisions. If there is any other error, we need to report it in the status of // the Deployment. alreadyExists := false createdRS, err := dc.client.AppsV1().ReplicaSets(d.Namespace).Create(ctx, \u0026amp;newRS, metav1.CreateOptions{}) switch { // We may end up hitting this due to a slow cache or a fast resync of the Deployment. case errors.IsAlreadyExists(err): alreadyExists = true // Fetch a copy of the ReplicaSet. rs, rsErr := dc.rsLister.ReplicaSets(newRS.Namespace).Get(newRS.Name) if rsErr != nil { return nil, rsErr } // If the Deployment owns the ReplicaSet and the ReplicaSet\u0026#39;s PodTemplateSpec is semantically // deep equal to the PodTemplateSpec of the Deployment, it\u0026#39;s the Deployment\u0026#39;s new ReplicaSet. // Otherwise, this is a hash collision and we need to increment the collisionCount field in // the status of the Deployment and requeue to try the creation in the next sync. controllerRef := metav1.GetControllerOf(rs) if controllerRef != nil \u0026amp;\u0026amp; controllerRef.UID == d.UID \u0026amp;\u0026amp; deploymentutil.EqualIgnoreHash(\u0026amp;d.Spec.Template, \u0026amp;rs.Spec.Template) { createdRS = rs err = nil break } // Matching ReplicaSet is not equal - increment the collisionCount in the DeploymentStatus // and requeue the Deployment. if d.Status.CollisionCount == nil { d.Status.CollisionCount = new(int32) } preCollisionCount := *d.Status.CollisionCount *d.Status.CollisionCount++ // Update the collisionCount for the Deployment and let it requeue by returning the original // error. _, dErr := dc.client.AppsV1().Deployments(d.Namespace).UpdateStatus(ctx, d, metav1.UpdateOptions{}) if dErr == nil { logger.V(2).Info(\u0026#34;Found a hash collision for deployment - bumping collisionCount to resolve it\u0026#34;, \u0026#34;deployment\u0026#34;, klog.KObj(d), \u0026#34;oldCollisionCount\u0026#34;, preCollisionCount, \u0026#34;newCollisionCount\u0026#34;, *d.Status.CollisionCount) } return nil, err case errors.HasStatusCause(err, v1.NamespaceTerminatingCause): // if the namespace is terminating, all subsequent creates will fail and we can safely do nothing return nil, err case err != nil: msg := fmt.Sprintf(\u0026#34;Failed to create new replica set %q: %v\u0026#34;, newRS.Name, err) if deploymentutil.HasProgressDeadline(d) { cond := deploymentutil.NewDeploymentCondition(apps.DeploymentProgressing, v1.ConditionFalse, deploymentutil.FailedRSCreateReason, msg) deploymentutil.SetDeploymentCondition(\u0026amp;d.Status, *cond) // We don\u0026#39;t really care about this error at this point, since we have a bigger issue to report. // TODO: Identify which errors are permanent and switch DeploymentIsFailed to take into account // these reasons as well. Related issue: https://github.com/kubernetes/kubernetes/issues/18568 _, _ = dc.client.AppsV1().Deployments(d.Namespace).UpdateStatus(ctx, d, metav1.UpdateOptions{}) } dc.eventRecorder.Eventf(d, v1.EventTypeWarning, deploymentutil.FailedRSCreateReason, msg) return nil, err } if !alreadyExists \u0026amp;\u0026amp; newReplicasCount \u0026gt; 0 { dc.eventRecorder.Eventf(d, v1.EventTypeNormal, \u0026#34;ScalingReplicaSet\u0026#34;, \u0026#34;Scaled up replica set %s to %d\u0026#34;, createdRS.Name, newReplicasCount) } needsUpdate := deploymentutil.SetDeploymentRevision(d, newRevision) if !alreadyExists \u0026amp;\u0026amp; deploymentutil.HasProgressDeadline(d) { msg := fmt.Sprintf(\u0026#34;Created new replica set %q\u0026#34;, createdRS.Name) condition := deploymentutil.NewDeploymentCondition(apps.DeploymentProgressing, v1.ConditionTrue, deploymentutil.NewReplicaSetReason, msg) deploymentutil.SetDeploymentCondition(\u0026amp;d.Status, *condition) needsUpdate = true } if needsUpdate { _, err = dc.client.AppsV1().Deployments(d.Namespace).UpdateStatus(ctx, d, metav1.UpdateOptions{}) } return createdRS, err } //该函数是Go语言编写的，用于获取与给定部署相匹配的复制集（ReplicaSet）。 //如果该新的复制集不存在，且createIfNotExisted参数为true，则会创建一个新的复制集。 //函数首先查找与给定部署目标相匹配的现有新的复制集（即具有与部署相同的Pod模板）。 //然后，它会计算所有旧复制集的最大修订号（revision number），并为新的复制集计算一个适当的修订号。 //如果存在现有的新复制集，函数将更新其修订号和注释，并可能更新部署的状态。 //如果不存在新的复制集且createIfNotExisted为true，则函数将创建一个新的复制集，并更新部署的状态。 //该函数使用了context.Context来控制函数执行的上下文，使用了apps.Deployment、apps.ReplicaSet等结构体来表示部署和复制集的信息， //使用了client来与Kubernetes API进行交互。 //函数的具体逻辑包括： //1. 查找与给定部署相匹配的新的复制集。 //2. 计算所有旧复制集的最大修订号。 //3. 如果存在现有的新复制集，更新其修订号和注释，并可能更新部署的状态。 //4. 如果不存在新的复制集且createIfNotExisted为true，创建一个新的复制集，并更新部署的状态。 //该函数涉及的操作包括查找、更新和创建复制集，以及更新部署的状态。 // scale scales proportionally in order to mitigate risk. Otherwise, scaling up can increase the size // of the new replica set and scaling down can decrease the sizes of the old ones, both of which would // have the effect of hastening the rollout progress, which could produce a higher proportion of unavailable // replicas in the event of a problem with the rolled out template. Should run only on scaling events or // when a deployment is paused and not during the normal rollout process. func (dc *DeploymentController) scale(ctx context.Context, deployment *apps.Deployment, newRS *apps.ReplicaSet, oldRSs []*apps.ReplicaSet) error { // If there is only one active replica set then we should scale that up to the full count of the // deployment. If there is no active replica set, then we should scale up the newest replica set. if activeOrLatest := deploymentutil.FindActiveOrLatest(newRS, oldRSs); activeOrLatest != nil { if *(activeOrLatest.Spec.Replicas) == *(deployment.Spec.Replicas) { return nil } _, _, err := dc.scaleReplicaSetAndRecordEvent(ctx, activeOrLatest, *(deployment.Spec.Replicas), deployment) return err } // If the new replica set is saturated, old replica sets should be fully scaled down. // This case handles replica set adoption during a saturated new replica set. if deploymentutil.IsSaturated(deployment, newRS) { for _, old := range controller.FilterActiveReplicaSets(oldRSs) { if _, _, err := dc.scaleReplicaSetAndRecordEvent(ctx, old, 0, deployment); err != nil { return err } } return nil } // There are old replica sets with pods and the new replica set is not saturated. // We need to proportionally scale all replica sets (new and old) in case of a // rolling deployment. if deploymentutil.IsRollingUpdate(deployment) { allRSs := controller.FilterActiveReplicaSets(append(oldRSs, newRS)) allRSsReplicas := deploymentutil.GetReplicaCountForReplicaSets(allRSs) allowedSize := int32(0) if *(deployment.Spec.Replicas) \u0026gt; 0 { allowedSize = *(deployment.Spec.Replicas) + deploymentutil.MaxSurge(*deployment) } // Number of additional replicas that can be either added or removed from the total // replicas count. These replicas should be distributed proportionally to the active // replica sets. deploymentReplicasToAdd := allowedSize - allRSsReplicas // The additional replicas should be distributed proportionally amongst the active // replica sets from the larger to the smaller in size replica set. Scaling direction // drives what happens in case we are trying to scale replica sets of the same size. // In such a case when scaling up, we should scale up newer replica sets first, and // when scaling down, we should scale down older replica sets first. var scalingOperation string switch { case deploymentReplicasToAdd \u0026gt; 0: sort.Sort(controller.ReplicaSetsBySizeNewer(allRSs)) scalingOperation = \u0026#34;up\u0026#34; case deploymentReplicasToAdd \u0026lt; 0: sort.Sort(controller.ReplicaSetsBySizeOlder(allRSs)) scalingOperation = \u0026#34;down\u0026#34; } // Iterate over all active replica sets and estimate proportions for each of them. // The absolute value of deploymentReplicasAdded should never exceed the absolute // value of deploymentReplicasToAdd. deploymentReplicasAdded := int32(0) nameToSize := make(map[string]int32) logger := klog.FromContext(ctx) for i := range allRSs { rs := allRSs[i] // Estimate proportions if we have replicas to add, otherwise simply populate // nameToSize with the current sizes for each replica set. if deploymentReplicasToAdd != 0 { proportion := deploymentutil.GetProportion(logger, rs, *deployment, deploymentReplicasToAdd, deploymentReplicasAdded) nameToSize[rs.Name] = *(rs.Spec.Replicas) + proportion deploymentReplicasAdded += proportion } else { nameToSize[rs.Name] = *(rs.Spec.Replicas) } } // Update all replica sets for i := range allRSs { rs := allRSs[i] // Add/remove any leftovers to the largest replica set. if i == 0 \u0026amp;\u0026amp; deploymentReplicasToAdd != 0 { leftover := deploymentReplicasToAdd - deploymentReplicasAdded nameToSize[rs.Name] = nameToSize[rs.Name] + leftover if nameToSize[rs.Name] \u0026lt; 0 { nameToSize[rs.Name] = 0 } } // TODO: Use transactions when we have them. if _, _, err := dc.scaleReplicaSet(ctx, rs, nameToSize[rs.Name], deployment, scalingOperation); err != nil { // Return as soon as we fail, the deployment is requeued return err } } } return nil } //该函数是一个Go语言函数，名为scale，属于DeploymentController类型。 //它接收四个参数：ctx是一个上下文对象，deployment是一个Deployment指针，newRS是一个ReplicaSet指针，oldRSs是一个ReplicaSet指针数组。 //函数返回一个错误。 //该函数用于根据给定的部署（deployment）和新的副本集（newRS）来比例地调整副本集的大小，以减轻风险。 //函数首先检查是否有活跃或最新的副本集需要进行大小调整，如果有，则将其调整到部署的指定大小。 //如果新的副本集已饱和，则将旧的副本集完全缩放 down。 //如果部署是滚动更新类型，则按比例缩放所有副本集（新旧副本集）。 //在滚动更新的情况下，函数会根据部署的规格（deployment.Spec.Replicas）和最大突增值（MaxSurge）计算出可以添加或移除的额外副本数量， //并将这些副本按比例分配给所有活跃的副本集。 //在缩放过程中，函数会根据副本集的大小进行排序，并根据缩放操作的类型（上/下）选择合适的排序方式。 //函数会迭代所有活跃的副本集，并根据比例计算出每个副本集应该增加或减少的副本数量。 //最后，函数会更新所有副本集的大小，并在遇到错误时返回错误。 //总结： 该函数用于根据给定的部署和新的副本集来比例地调整副本集的大小，以减轻风险。 //它会根据不同的条件来判断如何缩放副本集，并将缩放操作应用于所有活跃的副本集。 //该函数在滚动更新的情况下，会按比例分配额外的副本数量给所有活跃的副本集。 func (dc *DeploymentController) scaleReplicaSetAndRecordEvent(ctx context.Context, rs *apps.ReplicaSet, newScale int32, deployment *apps.Deployment) (bool, *apps.ReplicaSet, error) { // No need to scale if *(rs.Spec.Replicas) == newScale { return false, rs, nil } var scalingOperation string if *(rs.Spec.Replicas) \u0026lt; newScale { scalingOperation = \u0026#34;up\u0026#34; } else { scalingOperation = \u0026#34;down\u0026#34; } scaled, newRS, err := dc.scaleReplicaSet(ctx, rs, newScale, deployment, scalingOperation) return scaled, newRS, err } //该函数是一个Go语言函数，名为scaleReplicaSetAndRecordEvent，属于DeploymentController类型。 //它接收四个参数：ctx是一个上下文对象，rs是一个ReplicaSet指针，newScale是一个int32类型的变量，deployment是一个Deployment指针。 //函数返回三个值：一个布尔值，一个ReplicaSet指针和一个错误。 //函数首先检查当前ReplicaSet的副本数量是否已经等于目标副本数量newScale，如果是，则直接返回不进行任何操作。 //如果需要进行缩放操作，则根据目标副本数量与当前副本数量的关系，确定是向上扩展还是向下缩小。 //接着，函数调用dc.scaleReplicaSet方法来执行实际的缩放操作，并将缩放操作的结果返回。 //最后，函数返回缩放操作是否成功、新的ReplicaSet指针以及可能的错误信息。 //总结： 该函数用于根据给定的目标副本数量对ReplicaSet进行缩放操作，并记录相关事件。 //它会检查当前副本数量是否已经等于目标数量，如果是则不进行操作；否则，根据目标数量与当前数量的关系确定是向上扩展还是向下缩小，并执行相应的缩放操作。 func (dc *DeploymentController) scaleReplicaSet(ctx context.Context, rs *apps.ReplicaSet, newScale int32, deployment *apps.Deployment, scalingOperation string) (bool, *apps.ReplicaSet, error) { sizeNeedsUpdate := *(rs.Spec.Replicas) != newScale annotationsNeedUpdate := deploymentutil.ReplicasAnnotationsNeedUpdate(rs, *(deployment.Spec.Replicas), *(deployment.Spec.Replicas)+deploymentutil.MaxSurge(*deployment)) scaled := false var err error if sizeNeedsUpdate || annotationsNeedUpdate { oldScale := *(rs.Spec.Replicas) rsCopy := rs.DeepCopy() *(rsCopy.Spec.Replicas) = newScale deploymentutil.SetReplicasAnnotations(rsCopy, *(deployment.Spec.Replicas), *(deployment.Spec.Replicas)+deploymentutil.MaxSurge(*deployment)) rs, err = dc.client.AppsV1().ReplicaSets(rsCopy.Namespace).Update(ctx, rsCopy, metav1.UpdateOptions{}) if err == nil \u0026amp;\u0026amp; sizeNeedsUpdate { scaled = true dc.eventRecorder.Eventf(deployment, v1.EventTypeNormal, \u0026#34;ScalingReplicaSet\u0026#34;, \u0026#34;Scaled %s replica set %s to %d from %d\u0026#34;, scalingOperation, rs.Name, newScale, oldScale) } } return scaled, rs, err } //该函数用于缩放副本集的副本数量。 //它首先检查副本集的当前副本数量是否与目标副本数量不同，以及副本集的注解是否需要更新。 //如果任一条件为真，则创建一个副本集的深拷贝，并更新其副本数量和注解。 //然后，使用更新后的副本集调用客户端的Update方法来更新副本集。 //如果更新成功并且副本数量发生了变化，则记录一个事件表示副本集已被缩放。 //函数返回一个布尔值表示副本集是否被缩放，更新后的副本集对象和可能出现的错误。 // cleanupDeployment is responsible for cleaning up a deployment ie. retains all but the latest N old replica sets // where N=d.Spec.RevisionHistoryLimit. Old replica sets are older versions of the podtemplate of a deployment kept // around by default 1) for historical reasons and 2) for the ability to rollback a deployment. func (dc *DeploymentController) cleanupDeployment(ctx context.Context, oldRSs []*apps.ReplicaSet, deployment *apps.Deployment) error { logger := klog.FromContext(ctx) if !deploymentutil.HasRevisionHistoryLimit(deployment) { return nil } // Avoid deleting replica set with deletion timestamp set aliveFilter := func(rs *apps.ReplicaSet) bool { return rs != nil \u0026amp;\u0026amp; rs.ObjectMeta.DeletionTimestamp == nil } cleanableRSes := controller.FilterReplicaSets(oldRSs, aliveFilter) diff := int32(len(cleanableRSes)) - *deployment.Spec.RevisionHistoryLimit if diff \u0026lt;= 0 { return nil } sort.Sort(deploymentutil.ReplicaSetsByRevision(cleanableRSes)) logger.V(4).Info(\u0026#34;Looking to cleanup old replica sets for deployment\u0026#34;, \u0026#34;deployment\u0026#34;, klog.KObj(deployment)) for i := int32(0); i \u0026lt; diff; i++ { rs := cleanableRSes[i] // Avoid delete replica set with non-zero replica counts if rs.Status.Replicas != 0 || *(rs.Spec.Replicas) != 0 || rs.Generation \u0026gt; rs.Status.ObservedGeneration || rs.DeletionTimestamp != nil { continue } logger.V(4).Info(\u0026#34;Trying to cleanup replica set for deployment\u0026#34;, \u0026#34;replicaSet\u0026#34;, klog.KObj(rs), \u0026#34;deployment\u0026#34;, klog.KObj(deployment)) if err := dc.client.AppsV1().ReplicaSets(rs.Namespace).Delete(ctx, rs.Name, metav1.DeleteOptions{}); err != nil \u0026amp;\u0026amp; !errors.IsNotFound(err) { // Return error instead of aggregating and continuing DELETEs on the theory // that we may be overloading the api server. return err } } return nil } //该函数用于清理部署，即保留最新N个旧的副本集，其中N等于d.Spec.RevisionHistoryLimit。 //旧的副本集是部署的pod模板的旧版本，保留它们的原因有： //1）历史原因； //2）为了能够回滚部署。函数首先检查部署是否设置了修订历史限制，如果没有设置，则直接返回。 //然后，过滤掉具有删除时间戳的副本集，并计算需要清理的副本集数量。 //如果需要清理的副本集数量小于等于0，则直接返回。 //接下来，按修订版本对可清理的副本集进行排序，并尝试清理旧的副本集。 //对于每个需要清理的副本集，如果其副本数量不为零，或者期望的副本数量不为零，或者其生成代数大于观察到的生成代数，或者具有删除时间戳，则跳过清理。 //最后，如果清理过程中发生错误，则返回错误。 // syncDeploymentStatus checks if the status is up-to-date and sync it if necessary func (dc *DeploymentController) syncDeploymentStatus(ctx context.Context, allRSs []*apps.ReplicaSet, newRS *apps.ReplicaSet, d *apps.Deployment) error { newStatus := calculateStatus(allRSs, newRS, d) if reflect.DeepEqual(d.Status, newStatus) { return nil } newDeployment := d newDeployment.Status = newStatus _, err := dc.client.AppsV1().Deployments(newDeployment.Namespace).UpdateStatus(ctx, newDeployment, metav1.UpdateOptions{}) return err } //该函数是一个Go语言函数，用于检查部署（Deployment）的状态是否是最新的，如果不是最新的，则将其同步到最新的状态。 //函数定义在DeploymentController结构体中。 //函数接受四个参数： //- ctx：上下文对象，用于控制函数执行期间的流程。 //- allRSs：一个包含所有副本集（ReplicaSet）的切片。 //- newRS：一个新的副本集。 //- d：一个部署对象。 //函数首先调用calculateStatus函数来计算最新的状态。 //然后，它将检查当前部署的状态是否与计算出的最新状态相等。 //如果相等，函数将直接返回，不做任何操作。 //如果不相等，函数将更新部署的状态为最新状态，并调用UpdateStatus方法将其更新到Kubernetes集群中。 //最后，函数返回可能发生的错误。 // calculateStatus calculates the latest status for the provided deployment by looking into the provided replica sets. func calculateStatus(allRSs []*apps.ReplicaSet, newRS *apps.ReplicaSet, deployment *apps.Deployment) apps.DeploymentStatus { availableReplicas := deploymentutil.GetAvailableReplicaCountForReplicaSets(allRSs) totalReplicas := deploymentutil.GetReplicaCountForReplicaSets(allRSs) unavailableReplicas := totalReplicas - availableReplicas // If unavailableReplicas is negative, then that means the Deployment has more available replicas running than // desired, e.g. whenever it scales down. In such a case we should simply default unavailableReplicas to zero. if unavailableReplicas \u0026lt; 0 { unavailableReplicas = 0 } status := apps.DeploymentStatus{ // TODO: Ensure that if we start retrying status updates, we won\u0026#39;t pick up a new Generation value. ObservedGeneration: deployment.Generation, Replicas: deploymentutil.GetActualReplicaCountForReplicaSets(allRSs), UpdatedReplicas: deploymentutil.GetActualReplicaCountForReplicaSets([]*apps.ReplicaSet{newRS}), ReadyReplicas: deploymentutil.GetReadyReplicaCountForReplicaSets(allRSs), AvailableReplicas: availableReplicas, UnavailableReplicas: unavailableReplicas, CollisionCount: deployment.Status.CollisionCount, } // Copy conditions one by one so we won\u0026#39;t mutate the original object. conditions := deployment.Status.Conditions for i := range conditions { status.Conditions = append(status.Conditions, conditions[i]) } if availableReplicas \u0026gt;= *(deployment.Spec.Replicas)-deploymentutil.MaxUnavailable(*deployment) { minAvailability := deploymentutil.NewDeploymentCondition(apps.DeploymentAvailable, v1.ConditionTrue, deploymentutil.MinimumReplicasAvailable, \u0026#34;Deployment has minimum availability.\u0026#34;) deploymentutil.SetDeploymentCondition(\u0026amp;status, *minAvailability) } else { noMinAvailability := deploymentutil.NewDeploymentCondition(apps.DeploymentAvailable, v1.ConditionFalse, deploymentutil.MinimumReplicasUnavailable, \u0026#34;Deployment does not have minimum availability.\u0026#34;) deploymentutil.SetDeploymentCondition(\u0026amp;status, *noMinAvailability) } return status } // isScalingEvent checks whether the provided deployment has been updated with a scaling event // by looking at the desired-replicas annotation in the active replica sets of the deployment. // // rsList should come from getReplicaSetsForDeployment(d). func (dc *DeploymentController) isScalingEvent(ctx context.Context, d *apps.Deployment, rsList []*apps.ReplicaSet) (bool, error) { newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(ctx, d, rsList, false) if err != nil { return false, err } allRSs := append(oldRSs, newRS) logger := klog.FromContext(ctx) for _, rs := range controller.FilterActiveReplicaSets(allRSs) { desired, ok := deploymentutil.GetDesiredReplicasAnnotation(logger, rs) if !ok { continue } if desired != *(d.Spec.Replicas) { return true, nil } } return false, nil } //该函数用于检查提供的deployment是否已通过调整活动replica set的数量来进行更新。 //它通过查看deployment的desired-replicas注解来判断 //。函数首先获取deployment的所有replica set并同步修订版本号，然后遍历所有活动的replica set， //检查其desired-replicas注解与deployment的期望副本数是否一致，如果一致则返回false，否则返回true。 "},{"id":112,"href":"/docs/k8s%E7%9A%84pod%E7%B1%BB%E5%9E%8B-k8s-de-pod-lei-xing/","title":"K8S的POD类型 2024-04-03 14:57:11.072","section":"Docs","content":"在 Kubernetes 中，containers被部署和管理在 Pod 中。Pod 是 Kubernetes 对象模型中最小和最简单的单元，可以被创建、部署和管理。在这里，您可以在单个 pod 中使用不同的容器类型来实现特定的功能。以下是在 Kubernetes 中常用的一些容器类型：\nInit Container: 初始化容器 Sidecar Container: 边车容器 Ephemeral Container: 临时容器 Multi Container: 多容器 Init Container # 一个Pod 可以在其中运行多个容器来运行应用程序，但它也可以有一个或多个 init 容器，在应用程序容器启动之前运行。Init 容器旨在在主应用程序容器启动之前运行初始化任务。它们可用于设置配置文件、初始化数据库或等待外部服务准备就绪等任务。Init 容器与常规容器完全相同，只是：\n初始化容器始终运行至完成。 每个初始化容器必须在下一个初始化容器开始之前成功完成。 案例 # apiVersion: v1 kind: Pod metadata: name: nginx-init labels: app: nginx spec: initContainers: #Initializing container - name: init-container image: alpine command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;echo \u0026#34;\u0026lt;h1\u0026gt;This is from INIT container\u0026lt;/h1\u0026gt;\u0026#34; \u0026gt;\u0026gt; /usr/share/nginx/html/index.html\u0026#39;] volumeMounts: - name: data mountPath: /usr/share/nginx/html containers: # application container i.e., main container - name: app image: nginx volumeMounts: - name: data mountPath: /usr/share/nginx/html volumes: - name: data emptyDir: {} 这里的 init-container 将使用数据卷的data 覆盖 nginx 主页的 index.html。\nService编排文件 # apiVersion: v1 kind: Service metadata: name: nginx-svc labels: app: nginx spec: type: NodePort selector: app: nginx ports: - port: 80 targetPort: 80 创建Pod和Service # kubectl apply -f nginx-init.yaml kubectl apply -f nginx-service.yaml 现在，如果您尝试通过浏览器访问nginx端点，您将会得到以下输出。\nSidecar Container # 边车容器是与主应用容器在同一个 Pod 中运行的次要容器。这些容器用于通过提供额外的服务或功能，如日志记录、监控、安全性或数据同步，来增强或扩展主应用容器的功能，而无需直接修改主应用程序代码。\n使用边车容器的示例： # 创建nginx-sidecar.yaml：\napiVersion: v1 kind: Pod metadata: name: nginx-pod labels: app: nginx spec: containers: #App container - name: nginx-container image: nginx:latest ports: - containerPort: 80 volumeMounts: - name: logs mountPath: /var/log/nginx #This is side container - name: sidecar-container image: busybox command: [\u0026#34;/bin/sh\u0026#34;] args: [\u0026#34;-c\u0026#34;, \u0026#34;tail -f /var/log/nginx/access.log\u0026#34;] volumeMounts: - name: logs mountPath: /var/log/nginx volumes: - name: logs emptyDir: {} 上述 Pod 包含两个容器，一个是应用容器（nginx），另一个是 sidecar 容器，我们使用它来收集主应用程序的 nginx 访问日志。\n创建 nginx-svc.yaml：\napiVersion: v1 kind: Service metadata: name: nginx-svc labels: app: nginx spec: type: NodePort selector: app: nginx ports: - port: 80 targetPort: 80 创建Pod和Service # kubectl apply -f nginx-sidecar.yaml kubectl apply -f nginx-svc.yaml 现在，如果您尝试通过浏览器访问nginx端点，您将看到nginx的欢迎页面。然后，如果您尝试使用下面的命令访问边车日志，您将获得我们最近访问的访问日志。\nkubectl logs nginx-pod -c sidecar-container k logs nginx-pod -c sidecar-container 10.244.0.1 - - [17/Jan/2024:18:25:02 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 615 \u0026#34;-\u0026#34; \u0026#34;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\u0026#34; \u0026#34;-\u0026#34; 10.244.0.1 - - [17/Jan/2024:18:25:02 +0000] \u0026#34;GET /favicon.ico HTTP/1.1\u0026#34; 404 555 \u0026#34;http://127.0.0.1:53471/\u0026#34; \u0026#34;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\u0026#34; \u0026#34;-\u0026#34; 10.244.0.1 - - [17/Jan/2024:18:25:24 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 304 0 \u0026#34;-\u0026#34; \u0026#34;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\u0026#34; \u0026#34;-\u0026#34; 10.244.0.1 - - [17/Jan/2024:18:25:25 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 304 0 \u0026#34;-\u0026#34; \u0026#34;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\u0026#34; \u0026#34;-\u0026#34; Ephemeral Container # Ephemeral containers 与其他容器不同之处在于它们缺乏资源或执行的保证，并且永远不会自动重新启动，因此不适合用于构建应用程序。Ephemeral containers 使用与常规容器相同的 ContainerSpec 进行描述，但许多字段对于临时容器是不兼容且被禁止的。\n临时容器可能没有端口，因此禁止使用端口、存活探针、就绪探针等字段。Pod资源分配是不可变的，因此禁止设置资源。\n临时容器对于交互式故障排除非常有用，当使用 kubectl exec 不足以解决问题时，比如容器崩溃或容器镜像不包含调试工具。\n我们可以通过两种方式调试一个 Pod。\na. 使用临时容器进行调试\n如果您的 Pod 正在运行但无法执行到它，那么您可以使用这种方法。您可以使用 kubectl debug 命令向正在运行的 Pod 添加临时容器。这种方法将在同一 Pod 中创建一个新容器。\nkubectl debug -it \u0026lt;RUNNING-POD\u0026gt; --image=\u0026lt;DEBUG-IMAGE\u0026gt;:\u0026lt;DEBUG-IMAGE-TAG\u0026gt; --target=\u0026lt;RUNNING-CONTAINER\u0026gt; 请将替换为您现有的 Pod 名称，将替换为您想要调试的现有容器。此命令将添加一个新的 busybox 容器并附加到它上。\nb. 使用该 pod 的副本进行调试\n如果您的 Pod 崩溃并且无法执行到它，那么您可以使用这种方法。这种方法将创建一个新的 Pod，其中包含新的调试容器以及原始 Pod 中的容器。\nkubectl debug \u0026lt;ORIGINAL-POD\u0026gt; -it --image=\u0026lt;DEBUG-IMAGE\u0026gt;:\u0026lt;DEBUG-IMAGE-TAG\u0026gt; --share-processes --copy-to=\u0026lt;NEW-DEBUG-POD\u0026gt; 请将原始 Pod 名称替换为 ORIGINAL-POD，将新的调试 Pod 名称替换为 NEW-DEBUG-POD，新的调试 Pod 是原始 Pod 的副本。\n案例 # 创建ephemeral-pod.yaml：\napiVersion: v1 kind: Pod metadata: name: ephemeral-pod spec: containers: - image: registry.k8s.io/pause:3.1 name: ephemeral-container restartPolicy: Never 这个 YAML 文件将创建一个暂停容器镜像，不包含调试工具。\nkubectl apply -f ephermeral-pod.yaml 以上命令将在默认命名空间中创建一个临时 Pod。\nk get po NAME READY STATUS RESTARTS AGE ephemeral-pod 1/1 Running 0 3m41s 让我们尝试使用下面的命令来执行它。\nkubectl exec -it ephemeral-pod -- sh 但是会出现下面的错误，因为这个容器镜像中没有 shell。\nkubectl exec -it ephemeral-pod -- sh OCI runtime exec failed: exec failed: unable to start container process: exec: \u0026#34;sh\u0026#34;: executable file not found in $PATH: unknown command terminated with exit code 126 现在让我们尝试使用调试容器，即临时容器，进入到该 Pod 中。\n使用以下命令创建一个临时容器，并将其附加到上述 Pod，即 ephemeral-pod。\nkubectl debug -it ephemeral-pod --image=busybox --target=ephemeral-container 这个命令会添加一个新的 busybox 容器并将其附加到临时 pod 上。\u0026ndash;target 参数指定我们要执行的容器。此外，该命令将自动连接到临时容器的控制台。\n输出：\nkubectl debug -it ephemeral-pod --image=busybox --target=ephemeral-container Targeting container \u0026#34;ephemeral-container\u0026#34;. If you don\u0026#39;t see processes from this container it may be because the container runtime doesn\u0026#39;t support this feature. Defaulting debug container name to debugger-q6svq. If you don\u0026#39;t see a command prompt, try pressing enter. / # / # 现在您可以访问实际的容器，即临时容器，并进行调试\nMulti Container # Kubernetes允许您定义具有多个并行运行的容器的Pod。这些容器共享相同的网络命名空间，并可以通过localhost相互通信。这些多容器Pod旨在用于需要紧密耦合的进程一起运行、共享资源和数据的场景。\n案例 # 创建nginx-multi-container-pod.yaml：\napiVersion: v1 kind: Pod metadata: name: nginx-pod labels: app: nginx spec: containers: #App container - name: nginx-container image: nginx:latest ports: - containerPort: 80 volumeMounts: - name: data mountPath: /usr/share/nginx/html #This is extra container - name: extra-container image: debian command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;] args: - while true; do date \u0026gt; /usr/share/nginx/html/index.html; sleep 1; done volumeMounts: - name: data mountPath: /usr/share/nginx/html volumes: - name: data emptyDir: {} 以上清单文件中有两个容器，一个是主应用程序，即 nginx，另一个是额外的容器，将帮助我们持续更新 nginx 网页应用的主页。\n创建 nginx-svc.yaml：\napiVersion: v1 kind: Service metadata: name: nginx-svc labels: app: nginx spec: type: NodePort selector: app: nginx ports: - port: 80 targetPort: 80 创建 Pod 和 Service # kubectl apply -f nginx-multi-container-pod.yaml kubectl apply -f nginx-svc.yaml 现在，如果您尝试通过浏览器访问nginx端点，您将看到由上面的额外容器更新的日期。\n"},{"id":113,"href":"/docs/k8s%E7%9A%84%E6%9C%80%E5%90%8E%E4%B8%80%E5%9D%97%E6%8B%BC%E5%9B%BE-dbpaas-k8s-de-zui-hou-yi-kuai-pin-tu-dbpaas/","title":"K8S的最后一块拼图 dbpaas 2024-08-02 17:48:35.899","section":"Docs","content":"K8s 的发展使得私有云跟公共云之间的技术差不断的缩小，不管是在私有云还是公共云，大家今天都在基于 K8s 去开发 PaaS 系统。而 K8s 作为构建 PaaS 的基础，其全景图里还缺最后一块“拼图”——dbPaaS。作为一个云数据库行业干了十几年的资深从业者，以及一个云数据库初创公司的创始人，在本文中，我将结合近年来数据库和云计算的发展方向，以及我们在技术和工程上的实践，分享一些看法。\n私有化部署，数据库发展的方向\n近年来, 数据库整个领域主要在以下三个方向上发展: 公共云全托管，Serverless 和私有化部署。\n公共云全托管，包括各家云厂商的 RDS，以及每家云厂商自研的数据库，如 PolarDB、Aurora 等等，都属于这个板块。大部分公共云用户都是用这个服务，这个板块比较成熟稳定，跟着公共云的规模一起缓慢增长。公共云的主要的优势是弹性，有更多弹性需求的用户都在被吸引到公有云的 Serverless 产品上。而海外很多初创的公司，像 CockroachDB、Neon、 PlanetScale，包括中国的初创公司 TiDB 都在往 Serverless，无服务器数据库这个方向发展。\n另外一个方向就是私有化部署。有报告说中国公共云的服务器大概只占服务器总数的 5% ，线下部署的比例是非常高的。大型的互联网公司、央国企、银行都是用私有云。国内的信创数据库基本上都是在做这个方向。本质上来讲，企业选择什么数据库是由企业的业务场景来决定的，很少会因为选择一个数据库而倒推上面的云和架构。\n公有云还是私有云？K8s 正在统一云的操作系统\n企业对云的选择和使用是一直在变化的。选择私有云有很多原因，内因比如说合规、成本控制、自主可控，外因主要是 K8s。\n第一个变化是中国的私有云正在从 OpenStack 全面进化到 K8s。过去的私有云，是用 OpenStack 管理物理机生产虚拟机。如果客户要用容器和 K8s 再在虚拟机上去搭上面的容器。但是最近，大家都开始用 K8s 去管理物理机，业务最好是能够全部容器化、云原生化。如果你要用虚拟机，可以在 K8s 上管理，例如用 kubevirt 这样的技术在 Pod 里面跑虚拟机，底层是 K8s，VM 在上层，这是一个挺大的变化。\n第二个变化是 K8s 的发展使得私有云跟公共云之间的技术差逐步变窄。公共云和私有云上的 K8s 技术差别并不大。过去云厂商主要的技术壁垒都在于我怎么把几百万台虚拟机运维好，用规模优势来降低成本，但是这样的技术在 K8s 面前就像马奇诺防线一样被绕过了。公共云和私有云今天都得站在怎么把容器做好，怎么把 K8s 生态做好这同一条起跑线上。\n第三个趋势是，不管运行在私有云还是公共云，大家都在基于 K8s 去开发 PaaS 系统。K8s 作为一个多云的，可以一统公共云、私有云的容器操作系统，可以屏蔽底层 IaaS 的差异，让上层通过 Pod、Service、PVC 等抽象来操作 IaaS 资源。调度器、服务发现、配置管理、API Server 等等这些开发一个 PaaS 所需要依赖的基础能力，K8s 都提供了，不仅可以节约很多开发成本，而且 K8s 在设计和工程上都做的更好。比如说，K8s 的 API 是声明式 API，这个就是个挺先进的设计。公共云厂商做的比较早，在设计 API 的时候还没有这个理念，所以 API 都是过程式的。但后来的系统，比方说 Terraform，就用的声明式 API，所以开发者使用起来更简单，书写起来更容易理解，也更不容易出错。在 K8s 上做 PaaS 就可以利用好这些后发优势。\nK8s 的最后一片拼图 —— dbPass\nK8s 作为构建 PaaS 的基础，其全景图里还缺最后一块“拼图”——dbPaaS。在 K8s 上构建 dbPaaS 是大势所趋。所有的 PaaS 都在用 K8s 做自己的底座，dbPaaS，就是数据库的 PaaS 也不例外。如果企业的私有云都用 K8s 来构建，再搞一套虚拟机或者物理机的管理平台，然后在这个基础上发明一套 K8s 已经有了的能力，再做数据库的管理，其实是一个重复建设，不是特别合理。\ndbPaaS 有哪些挑战？我过去在阿里云做 RDS，最深有体会的一件事情是数据库的类目太多了，版本也太多了。大家使用的组合里面至少几十种数据库的不同版本。\n每一种数据库的运维操作都也很复杂，因为数据库是存储企业关键数据的基础设施，运维操作精细而复杂。与之对应的就是人不够，与之对应的痛苦就是人手不够，挺多事情都是可以靠堆人去做的，但是作为云厂商，要考虑向做一件事情投入人力的 ROI（投资回报率），也就是人效。\n说到人效，这里就要说到烟囱式架构这个陷阱。企业在构建 dbPaaS 的时候，方法通常是对于每一种要支持的数据库，就搞一个独立的小团队，然后写一套独立的代码，甚至有的时候连运维人员都是独立的。这种做法人效低，因为支撑每根烟囱的人力都是一个小池子，人员很难在烟囱之间流动，因此用烟囱式架构去支持 dbPaaS 这种长尾市场的业务，是一个错误的选择。拉长时间看，人员变动引起故障的概率在烟囱式架构里更高。还有更多的问题，比如资源不能做到跨引擎共享和混部，会增加部署一套 dbPaaS 的起步成本。\n像搭积木一样搭建数据库\n用一种新的方法去实现 dbPaaS。今天各种各样的数据库都会发布自己的容器，容器本身就是一个标准的东西，那我们能不能像搭积木一样的，在 K8s 上把它们给组装起来部署提供服务，然后把它们的运维操作，也以一种标准地去组装？\n乐高积木能够拼搭成各种各样的蓝图，其本质在于乐高是高度标准化的。它的凸起凹槽、厚度，都是有标准的。如果想让数据库像乐高积木一样的能够搭起来，也要解决一个标准问题，然后把各种各样的数据库容器适配到这个标准上。\n在计算机科学当中，有几个比较著名的标准，比如说 POSIX，POSIX 是对文件系统操作接口的抽象。K8s 里面有容器运行时标准 CRI，容器存储标准 CSI，容器网络标准 CNI，等等。通过这些标准，各种各样的容器网络、存储、分布式存储的项目都可以对接到 K8s 生态中去。除了标准和抽象之外，还有一个我们值得借鉴的方法叫分层，比如说 OSI 的 7 层网络协议和 TCP/IP 4 层协议。通过分层，各家厂商的网络产品软硬件、各种协议，都能找到合适的一个层次，通过层与层之间的标准接口，适配起来组成一个完整的系统。\n我们通过抽象和分层这两种方法来定义容器化数据库的标准，设计了 API，本质上它类似于 POSIX API，是一个标准。我们先看下 POSIX API，有了 POSIX API 之后，上层的应用就可以用一个很标准的 API 去操作各种文件，不用管底层的文件系统是什么，是 ext4，xfs 还是 nfs。类似的，我们设计的 API 是一个描述多个数据库容器之间的关系和拓扑结构，以及每个数据库容器的组成、可以提供的服务以及如何响应各种事件的行为的一个描绘，它是抽象的，跟任何一种具体的数据库是无关的，能够表达各种各样的数据库。此外，在设计这个 API 的时候我们也做了分层。我们分了五层，最底下一层是 K8s 的 API，它代表的 IaaS 的对象，倒数第二层是 Instance，描绘怎么把 IaaS 资源组装起来，构成一个单节点的数据库的副本。InstanceSet 就是把多个 Instance 组装成了一个多副本的数据库。Component 在多副本的基础上，加了更多数据库的行为，比如说成员管理，备份恢复等等。再之上是组装成 Cluster， Cluster 就是一个完整的数据库集群，包含多个组件。我们把不同数据库的能力、特性都映射到这个 5 层当中去，通过抽象和分层提出了一个数据库容器组运行在 K8s 上的标准。\ndbPaaS Operator 的核心实现就是通过这个抽象的 API 来管理数据库的生命周期，它不知道操作的数据库是 MySQL 还是 PostgreSQL 还是 Redis，它只知道操作的是一个 Cluster 对象，一个 Component 对象，操作的是抽象的对象。通过这种方法就能做到 DBPaaS 最核心的控制软件，跟被操作的数据库引擎无关。它能做到一套代码适配到多种的数据库引擎上去，这是我们最核心的创新，是吸收了阿里云 RDS 设计、开发、维护经验教训后的创新，全世界范围目前没有第二个 dbPaaS 采用了这种设计。用户去操作数据库也会通过 Cluster，Component 这种标准 API，不管对数据库做任何运维操作，体验会非常的接近，学习成本会比较低。\n再比如 Redis 高可用部署。Redis 主从架构会包含 Redis Server 和 Sentinel，其中 Redis Server 是一主一从两个副本，而 Sentinel 是三节点。Redis 官方原生的集群架构 Redis Cluster 则是一个水平分片的集群架构，比如说有 5 个分片，可以用 5 个 Component 来表达，每个 Component 又是一主一从两个节点。\n接下来一个问题是我们怎么把 30 多种数据库给集成到一套 dbPaaS Operator 代码里。我们再回顾下 POSIX API，如果一个文件系统想兼容 POSIX 标准，让使用 POSIX API 访问文件系统的应用程序都能操作它，就得先实现 POSIX 接口。类似的，一个数据库要想接入这个 dbPaaS Operator，就得让这个引擎也实现一组 dbPaaS Addon API。\n给大家举个例子，Redis 是怎么接入到 dbPaaS 当中？Redis 有很多种部署形态，单节点、主从、Sharding、还有 Redis cluster，形态有很多种。按照烟囱式的做法，每种部署形态都会是一个烟囱，一套独立的代码去管理。而在我们的 dbPaaS 里，不同的部署形态可以像积木一样去组装，不同的部署形态就是几行 YAML 的区别。\n首先我们要实现一个个“积木块”。“积木块”要把 Redis，Sentinel、Redis proxy 这些数据库镜像进行一次包装。对这些 Docker 镜像，我们用 dbPaaS 的 API，用 YAML 给它们增加一些扩展信息，里面会包含配置文件模板、服务和网络的配置、以及存储的配置等，还会扩展一些被事件触发时会执行的脚本（Action），以及版本镜像列表以及不同版本兼容性的描述信息等等。写完 Redis Server 的定义，我们就可以它看作是一个“积木块”。Redis Sentinel 和 Redis proxy 也是类似的，我们用 YAML 把它们都定义成 dbPaaS 的“积木块”。\n定义好这些“积木块”之后，我们在另一个 YAML 当中定义它的拓扑结构，告诉 KubeBlocks 系统刚才那些“积木块”该怎么组装，就能组装成一个集群结构。开发一个 Addon 的成本就是 写 YAML 加上一些脚本，配置文件的模板，以及监控的模板等等，不需要写 Go 代码、Java 代码，大概就是写几千行的非代码文件就能接进来，比从头写一个管控烟囱的开发成本要低多了。\n容器化到底会不会影响数据库的性能？K8s 适不适合有状态的服务？\n关于数据库的容器化，我常常遇到两个问题。第一个就是容器化会不会影响数据库的性能？第二个是 K8s 适不适合管理有状态服务？\n首先回答第一个问题。容器化不会影响数据库的性能。容器本身其实就是 Linux 操作系统当中的一个普通的进程，只是这个进程设置了 namespace，设置了 group，使得它能够完成一些叫做“隔离”的障眼法。重要的事情说三遍，容器不是虚拟化，容器不是虚拟化，容器不是虚拟化。\n上图大概画了 4 种常见的隔离方案，有虚拟化的，有 gVisor、有容器，在这四种虚拟化的方案当中，前面三种 VM、 microVM、gVisor 都有一层虚拟化层，这层虚拟化层有的放在 hypervisor 里面，有的放在用户态，唯独容器 runC 是没有虚拟化这层的，它就是一个普通的进程。所以性能上来说，在容器基础上做优化，它的底子其实是最好的。为什么要隔离呢？因为现在 CPU 的核数太多了。前两天有一个新闻说明年 AMD、英特尔还有 ARM 的核数都要达到 200 核了，大家都在往一个服务器上塞更多的核，这样密度更高能效更好，但这么多核数单体应用和单体数据库都无法消费，大家用的最多的数据库一般都是 8C、16C，这时候一定是要用隔离技术，而在隔离技术里容器的性能是最好的。\n基于容器，我们做了一些优化，把容器做到跟物理机上面的性能一样。最显著的，我们优化容器存储、容器网络、以及数据库的一些参数，可以达到在物理机上同样的甚至更好的性能。比如说 PostgreSQL 的吞吐，我们对 PG 容器做了一些优化之后，吞吐的峰值和公共云上 RDS 一个水平，而且在低并发量下比 RDS 的吞吐更高，在高并发量下吞吐会更稳。用容器跑 Redis 的用户经常担心，延迟会不会增长，如果不优化肯定有影响，所以我们把容器网络换成 eBPF 之后，就跟物理网络的延迟一样低了。\n第二个问题是 K8s 适不适合管理有状态的服务？首先 K8s 原生用来管理有状态服务的控制器，叫 StatefulSet。StatefulSet 确实不太好用，它有挺多限制，比如不支持 PVC 存储在线扩容的，另外变更 Pod 的时候，必须要严格按顺序去变更，这使得我没办法去指定一个 Pod 下线，它也不支持异构的各种各样的 Pod 配置。所以我们在 KubeBlocks 当中把 StatefulSet 换成了我们自己写的 InstanceSet，解决掉了 StatefulSet 的各种问题。\n另一方面，很多人误以为 在 K8s 里面必须要依赖 Pod 的迁移以及 PVC 跨机重新挂载这些 K8s 原生的机制去解决高可用、高可靠的问题，而在线下环境往往没有分布式存储，如果 Node 挂了，PVC 没办法迁移，会导致在线下部署的时候数据库的可用性、可靠性受损。所以我们在 KubeBlocks 里面的做法是不依赖于 K8s 的检测和重调度，而是使用数据库本身的高可用和多副本技术去解决一个节点挂掉之后服务怎么恢复的问题，把数据库的稳定性和 K8s 的稳定性解耦开。\nK8s 上去构建 dbPaaS 有丰富的应用场景\n我们接触到了很多的企业，发现在 K8s 上去构建 dbPaaS 是业界正在进行的趋势。公共云厂商，如阿里云的 RDS 全线产品都是跑在 K8s 上的，腾讯云的 TDSQL 是跑在 K8s 上的，移动云电子云的 RDS 跑在 K8s 上…… 海外的数据库的初创公司，像 TiDB，Cockroach、Neon、PlanteScale 也都是把自己的数据库的 dBPaaS 架在 K8s 上。\n国内的互联网公司，如阿里、字节、快手也是如此。银行领域，像工行、招行，也走得很靠前。行业上最近也在牵头在做数据库容器化的标准。\n总之，在 K8s 上建数据库已经成为趋势。越来越多的企业选择将自己的数据库部署在 K8s 之上, 这种方式可以充分发挥容器技术的优势, 提高数据库的敏捷性、可靠性和可运维性。这种趋势在未来几年内有望进一步扩大和深化。\n"},{"id":114,"href":"/docs/2025-2-7-k8s%E7%BB%84%E4%BB%B6/","title":"k8s组件","section":"Docs","content":"k8s 有哪些组件？\n1、etcd 保存了整个集群的状态；\n2、apiserver 提供了资源操作的唯一入口，并提供认证、授权、访问控制、API 注册和发现等机制；\n3、controller manager 负责维护集群的状态，比如故障检测、自动扩展、滚动更新等；\n4、scheduler 负责资源的调度，按照预定的调度策略将 Pod 调度到相应的机器上；\n5、kubelet 负责维护容器的生命周期，同时也负责 Volume（CVI）和网络（CNI）的管理；\n6、Container runtime 负责镜像管理以及 Pod 和容器的真正运行（CRI）；\n7、kube-proxy 负责为 Service 提供 cluster 内部的服务发现和负载均衡。\n"},{"id":115,"href":"/docs/k8s%E8%83%8C%E5%90%8Eservice%E6%98%AF%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C%E7%9A%84-k8s-bei-hou-service-shi-ru-he-gong-zuo-de/","title":"k8s背后service是如何工作的 2024-08-02 17:49:44.981","section":"Docs","content":"kube-proxy 是 Kubernetes 集群中负责服务发现和负载均衡的组件之一。它是一个网络代理，运行在每个节点上, 用于 service 资源的负载均衡。它有两种模式：iptables 和 ipvs。\niptables # iptables 是 Linux 系统中的一个用户空间实用程序，用于配置内核的网络包过滤和网络地址转换（NAT）规则。它是 Linux 内核中的 netfilter 框架的一部分，并负责在网络包进入、转发或离开计算机时进行筛选和处理。其主要功能和用途包括：\n防火墙：iptables 提供了强大的防火墙功能，可以根据不同的规则来过滤和拒绝不需要的网络包。管理员可以创建自定义的规则集，允许或拒绝从特定 IP 地址、端口或协议的数据包。 NAT（网络地址转换）：iptables 支持 NAT 功能，可以用来将私有网络中的计算机与外部网络连接。例如，它可以在一个 NAT 路由器上将内部网络的多个设备映射到单个外部 IP 地址。 端口转发：iptables 可以将特定的端口流量从一个网络接口转发到另一个接口或目标 IP 地址，通常用于内部网络的服务公开。 负载均衡：它也可以通过 DNAT（目标网络地址转换）功能将流量转发给多个内部服务器，实现简单的负载均衡。 iptables 是通过链（chains）和表（tables）来组织规则的。每个链由一组规则组成，当网络数据包经过时，这些规则会逐一执行。常用的表包括：\nfilter 表：用于包过滤，是最常用的表。 nat 表：用于网络地址转换。 mangle 表：用于修改数据包的 IP 层字段。 raw 表：用于绕过连接跟踪。 链的流向为：\n所以，根据上图，我们能够想象出某些常用场景中，报文的流向：\n到本机某进程的报文：PREROUTING –\u0026gt; INPUT\n由本机转发的报文：PREROUTING –\u0026gt; FORWARD –\u0026gt; POSTROUTING\n由本机的某进程发出报文（通常为响应报文）：OUTPUT –\u0026gt; POSTROUTING\n尽管在某些情况下配置 iptables 规则可能复杂，但它提供了高度的灵活性和强大的功能，使其成为 Linux 网络安全的重要组成部分。\nservice 负载均衡 # 我启动了一个 3 个 nginx pod，和一个对应的 service，service 的类型是 ClusterIP，这样 service 就会有一个虚拟 IP，这个 IP 会被 kube-proxy 代理到后端的 pod 上。\n~ » k get pod -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-deployment-59f546cb79-2k9ng 1/1 Running 2 (31m ago) 50m 10.244.0.28 minikube \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-deployment-59f546cb79-wfw84 1/1 Running 2 (31m ago) 50m 10.244.0.30 minikube \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-deployment-59f546cb79-zr9xm 1/1 Running 2 (31m ago) 50m 10.244.0.27 minikube \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ---------------------------------------------------------------------------------------------------------------------------------------------------- ~ » k get svc nginx-service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx-service ClusterIP 10.101.57.97 \u0026lt;none\u0026gt; 80/TCP 29m 当我们在 master 使用 curl 10.101.57.97 访问 service 的时候，首先会进入 PREROUTING 链：\nroot@minikube:/# iptables-save |grep PREROUTING :PREROUTING ACCEPT [0:0] :PREROUTING ACCEPT [34:2040] -A PREROUTING -m comment --comment \u0026#34;kubernetes service portals\u0026#34; -j KUBE-SERVICES -A PREROUTING -d 192.168.49.1/32 -j DOCKER_OUTPUT -A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER 首先会尝试匹配 KUBE-SERVICES 链，这个链是 kube-proxy 生成的，用于处理 service 的请求。后两个是 docker 的链，用于处理 docker 的请求。\nroot@minikube:/# iptables-save |grep \u0026#34;\\-A KUBE-SERVICES\u0026#34; -A KUBE-SERVICES -d 10.96.0.1/32 -p tcp -m comment --comment \u0026#34;default/kubernetes:https cluster IP\u0026#34; -j KUBE-SVC-NPX46M4PTMTKRN6Y -A KUBE-SERVICES -d 10.101.57.97/32 -p tcp -m comment --comment \u0026#34;default/nginx-service cluster IP\u0026#34; -j KUBE-SVC-V2OKYYMBY3REGZOG -A KUBE-SERVICES -d 10.96.0.10/32 -p udp -m comment --comment \u0026#34;kube-system/kube-dns:dns cluster IP\u0026#34; -j KUBE-SVC-TCOU7JCQXEZGVUNU -A KUBE-SERVICES -d 10.96.0.10/32 -p tcp -m comment --comment \u0026#34;kube-system/kube-dns:dns-tcp cluster IP\u0026#34; -j KUBE-SVC-ERIFXISQEP7F7OF4 -A KUBE-SERVICES -d 10.96.0.10/32 -p tcp -m comment --comment \u0026#34;kube-system/kube-dns:metrics cluster IP\u0026#34; -j KUBE-SVC-JD5MR3NA4I4DYORP -A KUBE-SERVICES -m comment --comment \u0026#34;kubernetes service nodeports; NOTE: this must be the last rule in this chain\u0026#34; -m addrtype --dst-type LOCAL -j KUBE-NODEPORTS 我们这个 nginx-service 的 cluster IP 是 10.101.57.97, 所以会走 KUBE-SVC-V2OKYYMBY3REGZOG 链：\nroot@minikube:/# iptables-save |grep \u0026#34;\\-A KUBE-SVC-V2OKYYMBY3REGZOG\u0026#34; -A KUBE-SVC-V2OKYYMBY3REGZOG ! -s 10.244.0.0/16 -d 10.101.57.97/32 -p tcp -m comment --comment \u0026#34;default/nginx-service cluster IP\u0026#34; -j KUBE-MARK-MASQ -A KUBE-SVC-V2OKYYMBY3REGZOG -m comment --comment \u0026#34;default/nginx-service -\u0026gt; 10.244.0.27:80\u0026#34; -m statistic --mode random --probability 0.33333333349 -j KUBE-SEP-POZMZY2HDLRATSJV -A KUBE-SVC-V2OKYYMBY3REGZOG -m comment --comment \u0026#34;default/nginx-service -\u0026gt; 10.244.0.28:80\u0026#34; -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-Z3HXRORN5VDCFRJU -A KUBE-SVC-V2OKYYMBY3REGZOG -m comment --comment \u0026#34;default/nginx-service -\u0026gt; 10.244.0.30:80\u0026#34; -j KUBE-SEP-S46ZL6MIFVWDY42O -A KUBE-SVC-V2OKYYMBY3REGZOG ! -s 10.244.0.0/16 -d 10.101.57.97/32 -p tcp -m comment --comment \u0026#34;default/nginx-service cluster IP\u0026#34; -j KUBE-MARK-MASQ` 这条规则的如果源ip 不是 `10.244.0.0/16` 的 ip（也就是不是 pod发出来的请求），目的ip 是 service ip，jump 跳转到 这个链 `KUBE-MARK-MASQ root@minikube:/# iptables-save |grep \u0026#34;\\-A KUBE-MARK-MASQ\u0026#34; -A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000 这条规则的作用是给数据包打上 0x4000 这个标记。这个标记会被后续的 NAT 规则识别到，从而让这些数据包通过特定的 NAT 规则进行 IP 地址转换。\n接下来看主要的三条规则：\n-A KUBE-SVC-V2OKYYMBY3REGZOG -m comment --comment \u0026#34;default/nginx-service -\u0026gt; 10.244.0.27:80\u0026#34; -m statistic --mode random --probability 0.33333333349 -j KUBE-SEP-POZMZY2HDLRATSJV -A KUBE-SVC-V2OKYYMBY3REGZOG -m comment --comment \u0026#34;default/nginx-service -\u0026gt; 10.244.0.28:80\u0026#34; -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-Z3HXRORN5VDCFRJU -A KUBE-SVC-V2OKYYMBY3REGZOG -m comment --comment \u0026#34;default/nginx-service -\u0026gt; 10.244.0.30:80\u0026#34; -j KUBE-SEP-S46ZL6MIFVWDY42O 第一条是说有 33.33% 的几率会被转发到 KUBE-SEP-POZMZY2HDLRATSJV, 第二条是 50% 的几率会被转发到 KUBE-SEP-Z3HXRORN5VDCFRJU, 第三条是一定会被转发到 KUBE-SEP-S46ZL6MIFVWDY42O。 转到第一条的概率是 33.33%，转到第二条的概率是 66.67%（没有到第一条的概率） * 50% = 33.33%，第三条就是 66.67%（没有到第一条的概率） * 50%（没有到第二条的概率） = 33.33%。所以这三条规则的概率是一样的。都是 33.33%。 那么 KUBE-SEP-POZMZY2HDLRATSJV 又是什么呢？\nroot@minikube:/# iptables-save |grep \u0026#34;\\-A KUBE-SEP-POZMZY2HDLRATSJV\u0026#34; -A KUBE-SEP-POZMZY2HDLRATSJV -s 10.244.0.27/32 -m comment --comment \u0026#34;default/nginx-service\u0026#34; -j KUBE-MARK-MASQ -A KUBE-SEP-POZMZY2HDLRATSJV -p tcp -m comment --comment \u0026#34;default/nginx-service\u0026#34; -m tcp -j DNAT --to-destination 10.244.0.27:80 root@minikube:/# iptables-save |grep \u0026#34;\\-A KUBE-SEP-POZMZY2HDLRATSJV\u0026#34; -A KUBE-SEP-Z3HXRORN5VDCFRJU -s 10.244.0.28/32 -m comment --comment \u0026#34;default/nginx-service\u0026#34; -j KUBE-MARK-MASQ -A KUBE-SEP-Z3HXRORN5VDCFRJU -p tcp -m comment --comment \u0026#34;default/nginx-service\u0026#34; -m tcp -j DNAT --to-destination 10.244.0.28:80 root@minikube:/# iptables-save |grep \u0026#34;\\-A KUBE-SEP-S46ZL6MIFVWDY42O\u0026#34; -A KUBE-SEP-S46ZL6MIFVWDY42O -s 10.244.0.30/32 -m comment --comment \u0026#34;default/nginx-service\u0026#34; -j KUBE-MARK-MASQ -A KUBE-SEP-S46ZL6MIFVWDY42O -p tcp -m comment --comment \u0026#34;default/nginx-service\u0026#34; -m tcp -j DNAT --to-destination 10.244.0.30:80 第一条规则确保流量从 10.244.0.20 发出时被打上 MASQUERADE 标记，以便通过 NAT 机制进行 IP 伪装。 第二条是将流量转发到 10.244.0.20:80 并使用 DNAT 机制进行目标地址转换, 转换的 ip 10.244.0.27:80 就是 pod 的 ip 和端口。 KUBE-SEP-Z3HXRORN5VDCFRJU 和 KUBE-SEP-S46ZL6MIFVWDY42O 的规则和这条同理。\nipvs # IPVS（IP Virtual Server）是 Linux 内核中实现负载均衡功能的模块。是一种高效的负载均衡技术，可以在第 4 层（传输层）进行流量转发和调度。IPVS 通常被用于构建高性能、高可用性的负载均衡集群。\n查看 ipvs 的规则：\nroot@minikube:/etc/apt# sudo ipvsadm -Ln|grep -A 4 10.101.57.97 Prot LocalAddress:Port Scheduler Flags -\u0026gt; RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.101.57.97:80 rr -\u0026gt; 10.244.0.27:80 Masq 1 0 0 -\u0026gt; 10.244.0.28:80 Masq 1 0 0 -\u0026gt; 10.244.0.30:80 Masq 1 0 0 UDP 10.96.0.10:53 rr 这个的意思是到10.101.57.97:80 的 tcp 浏览会使用 rr（轮询）的方式转发到 10.244.0.27:80，10.244.0.28:80，10.244.0.30:80 这三个 pod 上。Masq：指示 \u0026ldquo;Masquerading\u0026rdquo;，表示通过 NAT 来处理网络流量。 所以 ipvs 的模式比 iptables 性能高的多，第一因为 ipvs 是轮询选，iptables 是逐条百分比匹配的，这个还是可以接受的。更要命的是第二条，当 pod 频繁变更的时候 service 对应的 endpoint 的 ENDPOINTS 就会增加或者是减少。那么 iptables 对应 service 的所有规则的百分比都会变化，就会导致一个 service 的规则全部要重刷，当 pod 变化太频繁时，会吃掉大量的 CPU。\n不是说开启了 ipvs 就不会有 iptables 了，还需要 iptables 的 SNAT 规则来处理返回的数据包。\n-A POSTROUTING -m comment --comment \u0026#34;kubernetes postrouting rules\u0026#34; -j KUBE-POSTROUTING -A KUBE-POSTROUTING -m comment --comment \u0026#34;Kubernetes endpoints dst ip:port, source ip for solving hairpin purpose\u0026#34; -m set --match-set KUBE-LOOP-BACK dst,dst,src -j MASQUERADE -m set --match-set KUBE-LOOP-BACK dst,dst,src 的意思是匹配 KUBE-LOOP-BACK 这个 set，这个 set 里面存放的是 pod 的 ip，这个规则的作用是将数据包的源地址替换为本机的地址，以便数据包能够正确返回到客户端。 为什么是 pod ip 而不是 service cluster ip 呢？因为已经通过 ipvs DNAT 过了，所以这里是 pod ip。\nroot@minikube:/etc/apt# ipset -L|grep -A 15 KUBE-LOOP-BACK Name: KUBE-LOOP-BACK Type: hash:ip,port,ip Revision: 6 Header: family inet hashsize 1024 maxelem 65536 bucketsize 12 initval 0xe4e21451 Size in memory: 544 References: 1 Number of entries: 6 Members: 10.244.0.28,tcp:80,10.244.0.28 10.244.0.30,tcp:80,10.244.0.30 10.244.0.29,tcp:9153,10.244.0.29 10.244.0.29,tcp:53,10.244.0.29 10.244.0.27,tcp:80,10.244.0.27 10.244.0.29,udp:53,10.244.0.29 很明显我们的三个 pod 都在这个 set 里面。\n-A KUBE-POSTROUTING -m comment --comment \u0026quot;Kubernetes endpoints dst ip:port, source ip for solving hairpin purpose\u0026quot; -m set --match-set KUBE-LOOP-BACK dst,dst,src -j MASQUERADE 这条规则的作用是将数据包的源地址替换为本机的地址，以便数据包能够正确返回到客户端。现在我们使用 curl 发出的包目标 ip 是 pod ip了，源 ip 是 node ip。等到数据包返回的时候，kernel 会根据 链路追踪 中的数据记录，会把包的源ip 的pod ip 替换为 serice cluster ip, 目标 ip 从 node ip 替换为我发起请求的 ip。这样包就能正确返回到客户端了。\n"},{"id":116,"href":"/docs/k8s%E8%B0%83%E5%BA%A6%E5%99%A8-extendergo-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-k8s-diao-du-qi-extendergo-yuan-ma-jie-du/","title":"K8S调度器 extender.go 源码解读 2024-04-09 11:45:59.666","section":"Docs","content":"/* Copyright 2015 The Kubernetes Authors. Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \u0026#34;AS IS\u0026#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */ package scheduler import ( \u0026#34;bytes\u0026#34; \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;time\u0026#34; v1 \u0026#34;k8s.io/api/core/v1\u0026#34; utilnet \u0026#34;k8s.io/apimachinery/pkg/util/net\u0026#34; \u0026#34;k8s.io/apimachinery/pkg/util/sets\u0026#34; restclient \u0026#34;k8s.io/client-go/rest\u0026#34; extenderv1 \u0026#34;k8s.io/kube-scheduler/extender/v1\u0026#34; schedulerapi \u0026#34;k8s.io/kubernetes/pkg/scheduler/apis/config\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/scheduler/framework\u0026#34; ) const ( // DefaultExtenderTimeout defines the default extender timeout in second. DefaultExtenderTimeout = 5 * time.Second ) // HTTPExtender implements the Extender interface. type HTTPExtender struct { extenderURL string preemptVerb string filterVerb string prioritizeVerb string bindVerb string weight int64 client *http.Client nodeCacheCapable bool managedResources sets.Set[string] ignorable bool } //这段代码定义了一个名为HTTPExtender的结构体，它实现了Extender接口。 //HTTPExtender表示一个HTTP扩展器，用于扩展Kubernetes调度器的功能。 //它通过发送HTTP请求与Kubernetes调度器进行交互，以实现自定义的调度逻辑。 //该结构体包含以下字段： - extenderURL：扩展器的URL，用于向扩展器发送HTTP请求。 //- preemptVerb：抢占操作的动词，表示扩展器在抢占过程中执行的操作。 //- filterVerb：过滤操作的动词，表示扩展器在过滤节点时执行的操作。 //- prioritizeVerb：优先级排序操作的动词，表示扩展器在对节点进行优先级排序时执行的操作。 //- bindVerb：绑定操作的动词，表示扩展器在绑定Pod到节点时执行的操作。 //- weight：扩展器的权重，用于在优先级排序时对多个扩展器的结果进行加权计算。 //- client：用于发送HTTP请求的HTTP客户端。 //- nodeCacheCapable：表示扩展器是否支持节点缓存。 //如果为true，调度器会将节点信息缓存在本地，以减少对扩展器的请求次数。 //- managedResources：表示扩展器管理的资源集合。 //调度器会将这些资源的调度委托给扩展器处理。 //- ignorable：表示扩展器是否可以被忽略。如果为true，当扩展器出现错误时，调度器可以忽略该错误并继续进行调度。 //此外，代码还定义了一个常量DefaultExtenderTimeout，它表示默认的扩展器超时时间，单位为秒。 func makeTransport(config *schedulerapi.Extender) (http.RoundTripper, error) { var cfg restclient.Config if config.TLSConfig != nil { cfg.TLSClientConfig.Insecure = config.TLSConfig.Insecure cfg.TLSClientConfig.ServerName = config.TLSConfig.ServerName cfg.TLSClientConfig.CertFile = config.TLSConfig.CertFile cfg.TLSClientConfig.KeyFile = config.TLSConfig.KeyFile cfg.TLSClientConfig.CAFile = config.TLSConfig.CAFile cfg.TLSClientConfig.CertData = config.TLSConfig.CertData cfg.TLSClientConfig.KeyData = config.TLSConfig.KeyData cfg.TLSClientConfig.CAData = config.TLSConfig.CAData } if config.EnableHTTPS { hasCA := len(cfg.CAFile) \u0026gt; 0 || len(cfg.CAData) \u0026gt; 0 if !hasCA { cfg.Insecure = true } } tlsConfig, err := restclient.TLSConfigFor(\u0026amp;cfg) if err != nil { return nil, err } if tlsConfig != nil { return utilnet.SetTransportDefaults(\u0026amp;http.Transport{ TLSClientConfig: tlsConfig, }), nil } return utilnet.SetTransportDefaults(\u0026amp;http.Transport{}), nil } //该函数根据给定的schedulerapi.Extender配置创建一个http.RoundTripper传输对象。 //它首先将config.TLSConfig中的配置项复制到cfg.TLSClientConfig中，然后根据config.EnableHTTPS的值设置cfg.Insecure。 //接下来，它通过调用restclient.TLSConfigFor(\u0026amp;cfg)来获取TLS配置，并将其设置到http.Transport中。 //最后，返回设置了默认值的http.Transport对象。如果在过程中出现错误，则返回错误。 // NewHTTPExtender creates an HTTPExtender object. func NewHTTPExtender(config *schedulerapi.Extender) (framework.Extender, error) { if config.HTTPTimeout.Duration.Nanoseconds() == 0 { config.HTTPTimeout.Duration = time.Duration(DefaultExtenderTimeout) } transport, err := makeTransport(config) if err != nil { return nil, err } client := \u0026amp;http.Client{ Transport: transport, Timeout: config.HTTPTimeout.Duration, } managedResources := sets.New[string]() for _, r := range config.ManagedResources { managedResources.Insert(string(r.Name)) } return \u0026amp;HTTPExtender{ extenderURL: config.URLPrefix, preemptVerb: config.PreemptVerb, filterVerb: config.FilterVerb, prioritizeVerb: config.PrioritizeVerb, bindVerb: config.BindVerb, weight: config.Weight, client: client, nodeCacheCapable: config.NodeCacheCapable, managedResources: managedResources, ignorable: config.Ignorable, }, nil } //该函数用于创建一个HTTPExtender对象。它根据传入的config参数初始化HTTPExtender对象的属性，并返回该对象。 //其中，makeTransport函数用于创建一个http.Transport对象，http.Client对象则使用该Transport对象以及config参数中的HTTP超时时间进行初始化。 //此外，该函数还对config.ManagedResources进行遍历，将其中的Name字段转换为string类型，并插入到managedResources集合中。 //最后，该函数返回一个初始化完成的HTTPExtender对象。 // Name returns extenderURL to identify the extender. func (h *HTTPExtender) Name() string { return h.extenderURL } // IsIgnorable returns true indicates scheduling should not fail when this extender // is unavailable func (h *HTTPExtender) IsIgnorable() bool { return h.ignorable } // SupportsPreemption returns true if an extender supports preemption. // An extender should have preempt verb defined and enabled its own node cache. func (h *HTTPExtender) SupportsPreemption() bool { return len(h.preemptVerb) \u0026gt; 0 } //这段代码定义了一个名为HTTPExtender的结构体及其三个方法。 //1. Name()方法返回extenderURL，用于标识扩展程序。 //2. IsIgnorable()方法返回一个布尔值，指示当此扩展程序不可用时，调度是否不应失败。如果ignorable字段为true，则返回true。 //3. SupportsPreemption()方法返回一个布尔值，表示扩展程序是否支持抢占。 //如果preemptVerb字段的长度大于0，则返回true。 //这意味着扩展程序应该定义抢占动词并启用自己的节点缓存。 // ProcessPreemption returns filtered candidate nodes and victims after running preemption logic in extender. func (h *HTTPExtender) ProcessPreemption( pod *v1.Pod, nodeNameToVictims map[string]*extenderv1.Victims, nodeInfos framework.NodeInfoLister, ) (map[string]*extenderv1.Victims, error) { var ( result extenderv1.ExtenderPreemptionResult args *extenderv1.ExtenderPreemptionArgs ) if !h.SupportsPreemption() { return nil, fmt.Errorf(\u0026#34;preempt verb is not defined for extender %v but run into ProcessPreemption\u0026#34;, h.extenderURL) } if h.nodeCacheCapable { // If extender has cached node info, pass NodeNameToMetaVictims in args. nodeNameToMetaVictims := convertToMetaVictims(nodeNameToVictims) args = \u0026amp;extenderv1.ExtenderPreemptionArgs{ Pod: pod, NodeNameToMetaVictims: nodeNameToMetaVictims, } } else { args = \u0026amp;extenderv1.ExtenderPreemptionArgs{ Pod: pod, NodeNameToVictims: nodeNameToVictims, } } if err := h.send(h.preemptVerb, args, \u0026amp;result); err != nil { return nil, err } // Extender will always return NodeNameToMetaVictims. // So let\u0026#39;s convert it to NodeNameToVictims by using \u0026lt;nodeInfos\u0026gt;. newNodeNameToVictims, err := h.convertToVictims(result.NodeNameToMetaVictims, nodeInfos) if err != nil { return nil, err } // Do not override \u0026lt;nodeNameToVictims\u0026gt;. return newNodeNameToVictims, nil } //该函数是用于处理抢占逻辑的HTTP扩展器方法。 //它根据传入的Pod和节点信息，通过调用扩展器的抢占逻辑，返回过滤后的候选节点和受害者。 //- 首先，函数检查扩展器是否支持抢占。如果不支持，则返回错误。 //- 接下来，根据扩展器是否具有缓存的节点信息，构建不同的ExtenderPreemptionArgs对象，其中包含Pod和节点信息。 //- 然后，通过调用扩展器的指定动词（preempt），将ExtenderPreemptionArgs发送给扩展器，并将结果存储在ExtenderPreemptionResult中。 //- 最后，将扩展器返回的NodeNameToMetaVictims转换为NodeNameToVictims，并返回新的NodeNameToVictims对象。 //该函数返回的地图map[string]*extenderv1.Victims表示了每个节点的受害者列表，其中每个受害者包含了被抢占的Pod列表。 // convertToVictims converts \u0026#34;nodeNameToMetaVictims\u0026#34; from object identifiers, // such as UIDs and names, to object pointers. func (h *HTTPExtender) convertToVictims( nodeNameToMetaVictims map[string]*extenderv1.MetaVictims, nodeInfos framework.NodeInfoLister, ) (map[string]*extenderv1.Victims, error) { nodeNameToVictims := map[string]*extenderv1.Victims{} for nodeName, metaVictims := range nodeNameToMetaVictims { nodeInfo, err := nodeInfos.Get(nodeName) if err != nil { return nil, err } victims := \u0026amp;extenderv1.Victims{ Pods: []*v1.Pod{}, NumPDBViolations: metaVictims.NumPDBViolations, } for _, metaPod := range metaVictims.Pods { pod, err := h.convertPodUIDToPod(metaPod, nodeInfo) if err != nil { return nil, err } victims.Pods = append(victims.Pods, pod) } nodeNameToVictims[nodeName] = victims } return nodeNameToVictims, nil } //该函数是一个Go语言函数，它将从对象标识符（如UID和名称）组成的\u0026#34;nodeNameToMetaVictims\u0026#34;映射转换为对象指针组成的\u0026#34;nodeNameToVictims\u0026#34;映射。 //该函数使用给定的节点信息获取节点名称对应的受害者信息，并将其转换为包含Pod信息和PDB违规数量的Victims对象。 //具体来说，它遍历输入的映射，获取每个节点的MetaVictims对象，并通过节点信息获取节点的Pod信息。 //然后，它将MetaPod对象转换为Pod对象，并将其添加到Victims对象的Pod列表中。 //最后，它将转换后的Victims对象添加到输出映射中，并返回该映射。如果在转换过程中发生错误，函数将返回错误。 // convertPodUIDToPod returns v1.Pod object for given MetaPod and node info. // The v1.Pod object is restored by nodeInfo.Pods(). // It returns an error if there\u0026#39;s cache inconsistency between default scheduler // and extender, i.e. when the pod is not found in nodeInfo.Pods. func (h *HTTPExtender) convertPodUIDToPod( metaPod *extenderv1.MetaPod, nodeInfo *framework.NodeInfo) (*v1.Pod, error) { for _, p := range nodeInfo.Pods { if string(p.Pod.UID) == metaPod.UID { return p.Pod, nil } } return nil, fmt.Errorf(\u0026#34;extender: %v claims to preempt pod (UID: %v) on node: %v, but the pod is not found on that node\u0026#34;, h.extenderURL, metaPod, nodeInfo.Node().Name) } // convertToMetaVictims converts from struct type to meta types. func convertToMetaVictims( nodeNameToVictims map[string]*extenderv1.Victims, ) map[string]*extenderv1.MetaVictims { nodeNameToMetaVictims := map[string]*extenderv1.MetaVictims{} for node, victims := range nodeNameToVictims { metaVictims := \u0026amp;extenderv1.MetaVictims{ Pods: []*extenderv1.MetaPod{}, NumPDBViolations: victims.NumPDBViolations, } for _, pod := range victims.Pods { metaPod := \u0026amp;extenderv1.MetaPod{ UID: string(pod.UID), } metaVictims.Pods = append(metaVictims.Pods, metaPod) } nodeNameToMetaVictims[node] = metaVictims } return nodeNameToMetaVictims } //该函数的功能是将从结构体类型转换为元类型。 //它接收一个nodeNameToVictims参数，该参数是一个map，其中键是节点名称，值是*extenderv1.Victims类型的指针。 //函数创建一个空的nodeNameToMetaVictims映射，然后遍历nodeNameToVictims中的每个元素。 //对于每个节点，它创建一个新的extenderv1.MetaVictims实例，并将NumPDBViolations从旧的victims实例复制到新的metaVictims实例中。 //然后，它遍历旧的victims实例中的每个Pod，为每个Pod创建一个新的extenderv1.MetaPod实例，并将UID从旧的pod实例复制到新的metaPod实例中。 //最后，它将新的metaVictims实例添加到nodeNameToMetaVictims映射中，并返回该映射。 // Filter based on extender implemented predicate functions. The filtered list is // expected to be a subset of the supplied list; otherwise the function returns an error. // The failedNodes and failedAndUnresolvableNodes optionally contains the list // of failed nodes and failure reasons, except nodes in the latter are // unresolvable. func (h *HTTPExtender) Filter( pod *v1.Pod, nodes []*framework.NodeInfo, ) (filteredList []*framework.NodeInfo, failedNodes, failedAndUnresolvableNodes extenderv1.FailedNodesMap, err error) { var ( result extenderv1.ExtenderFilterResult nodeList *v1.NodeList nodeNames *[]string nodeResult []*framework.NodeInfo args *extenderv1.ExtenderArgs ) fromNodeName := make(map[string]*framework.NodeInfo) for _, n := range nodes { fromNodeName[n.Node().Name] = n } if h.filterVerb == \u0026#34;\u0026#34; { return nodes, extenderv1.FailedNodesMap{}, extenderv1.FailedNodesMap{}, nil } if h.nodeCacheCapable { nodeNameSlice := make([]string, 0, len(nodes)) for _, node := range nodes { nodeNameSlice = append(nodeNameSlice, node.Node().Name) } nodeNames = \u0026amp;nodeNameSlice } else { nodeList = \u0026amp;v1.NodeList{} for _, node := range nodes { nodeList.Items = append(nodeList.Items, *node.Node()) } } args = \u0026amp;extenderv1.ExtenderArgs{ Pod: pod, Nodes: nodeList, NodeNames: nodeNames, } if err := h.send(h.filterVerb, args, \u0026amp;result); err != nil { return nil, nil, nil, err } if result.Error != \u0026#34;\u0026#34; { return nil, nil, nil, fmt.Errorf(result.Error) } if h.nodeCacheCapable \u0026amp;\u0026amp; result.NodeNames != nil { nodeResult = make([]*framework.NodeInfo, len(*result.NodeNames)) for i, nodeName := range *result.NodeNames { if n, ok := fromNodeName[nodeName]; ok { nodeResult[i] = n } else { return nil, nil, nil, fmt.Errorf( \u0026#34;extender %q claims a filtered node %q which is not found in the input node list\u0026#34;, h.extenderURL, nodeName) } } } else if result.Nodes != nil { nodeResult = make([]*framework.NodeInfo, len(result.Nodes.Items)) for i := range result.Nodes.Items { nodeResult[i] = framework.NewNodeInfo() nodeResult[i].SetNode(\u0026amp;result.Nodes.Items[i]) } } return nodeResult, result.FailedNodes, result.FailedAndUnresolvableNodes, nil } //该函数是一个过滤函数，基于扩展器实现的谓词函数对节点进行过滤。 //函数的输入是一个Pod对象和一个节点信息数组，输出是过滤后的节点信息数组、失败的节点信息映射、不可解析的节点信息映射和错误信息。 //函数首先通过遍历输入节点信息数组，创建一个从节点名称到节点信息的映射。 //然后根据扩展器的过滤动词，决定使用节点名称列表还是节点列表作为输入参数，构建ExtenderArgs对象，并调用扩展器的过滤方法。 //如果过滤方法返回错误，函数直接返回错误。如果过滤方法成功，函数根据返回结果构建过滤后的节点信息数组，并返回。 //如果扩展器声明了一个节点被过滤，但是在输入节点信息数组中找不到该节点，则函数返回错误。 //如果扩展器返回的结果中包含失败的节点和不可解析的节点信息，则将其添加到相应的映射中。 //总之，该函数通过调用扩展器的过滤方法，对节点进行过滤，并返回过滤后的节点信息数组和相关错误信息。 // Prioritize based on extender implemented priority functions. Weight*priority is added // up for each such priority function. The returned score is added to the score computed // by Kubernetes scheduler. The total score is used to do the host selection. func (h *HTTPExtender) Prioritize(pod *v1.Pod, nodes []*framework.NodeInfo) (*extenderv1.HostPriorityList, int64, error) { var ( result extenderv1.HostPriorityList nodeList *v1.NodeList nodeNames *[]string args *extenderv1.ExtenderArgs ) if h.prioritizeVerb == \u0026#34;\u0026#34; { result := extenderv1.HostPriorityList{} for _, node := range nodes { result = append(result, extenderv1.HostPriority{Host: node.Node().Name, Score: 0}) } return \u0026amp;result, 0, nil } if h.nodeCacheCapable { nodeNameSlice := make([]string, 0, len(nodes)) for _, node := range nodes { nodeNameSlice = append(nodeNameSlice, node.Node().Name) } nodeNames = \u0026amp;nodeNameSlice } else { nodeList = \u0026amp;v1.NodeList{} for _, node := range nodes { nodeList.Items = append(nodeList.Items, *node.Node()) } } args = \u0026amp;extenderv1.ExtenderArgs{ Pod: pod, Nodes: nodeList, NodeNames: nodeNames, } if err := h.send(h.prioritizeVerb, args, \u0026amp;result); err != nil { return nil, 0, err } return \u0026amp;result, h.weight, nil } //该函数是一个Go语言函数，定义在HTTPExtender结构体中，用于根据扩展程序实现的优先级函数对节点进行优先级排序。 //该函数将权重乘以优先级的总和添加到每个此类优先级函数中。 //返回的分数将添加到Kubernetes调度程序计算的分数中。总 //分数用于进行主机选择。 //函数接受一个*v1.Pod类型的pod参数，一个[]*framework.NodeInfo类型的nodes参数，以及一个error类型的结果参数。 //函数首先定义了一些局部变量，包括一个extenderv1.HostPriorityList类型的result变量， //一个*v1.NodeList类型的nodeList变量，一个*[]string类型的nodeNames变量，以及一个*extenderv1.ExtenderArgs类型的args变量。 //如果h.prioritizeVerb为空字符串，则将每个节点的分数设置为0，并返回结果。 //如果h.nodeCacheCapable为true，则将节点名称添加到nodeNameSlice切片中，并将其赋值给nodeNames变量。 //否则，将节点对象追加到nodeList的Items字段中。 //接下来，将pod、nodeList和nodeNames赋值给args的相应字段。 //最后，调用h.send方法，将h.prioritizeVerb、args和\u0026amp;result作为参数传递，并检查是否有错误发生。 //如果有错误，则返回错误。否则，返回结果和权重。 // Bind delegates the action of binding a pod to a node to the extender. func (h *HTTPExtender) Bind(binding *v1.Binding) error { var result extenderv1.ExtenderBindingResult if !h.IsBinder() { // This shouldn\u0026#39;t happen as this extender wouldn\u0026#39;t have become a Binder. return fmt.Errorf(\u0026#34;unexpected empty bindVerb in extender\u0026#34;) } req := \u0026amp;extenderv1.ExtenderBindingArgs{ PodName: binding.Name, PodNamespace: binding.Namespace, PodUID: binding.UID, Node: binding.Target.Name, } if err := h.send(h.bindVerb, req, \u0026amp;result); err != nil { return err } if result.Error != \u0026#34;\u0026#34; { return fmt.Errorf(result.Error) } return nil } // IsBinder returns whether this extender is configured for the Bind method. func (h *HTTPExtender) IsBinder() bool { return h.bindVerb != \u0026#34;\u0026#34; } // IsPrioritizer returns whether this extender is configured for the Prioritize method. func (h *HTTPExtender) IsPrioritizer() bool { return h.prioritizeVerb != \u0026#34;\u0026#34; } // IsFilter returns whether this extender is configured for the Filter method. func (h *HTTPExtender) IsFilter() bool { return h.filterVerb != \u0026#34;\u0026#34; } //这段代码定义了一个名为HTTPExtender的结构体及其相关方法。 //这个结构体用于委托将Pod绑定到节点的操作给扩展器。 //- Bind方法用于将Pod绑定到节点 //。它首先检查当前扩展器是否配置为绑定扩展器，如果不是，则返回错误。 //然后创建一个ExtenderBindingArgs请求，包含Pod的名称、命名空间、UID以及目标节点的名称，并通过调用send方法将请求发送给扩展器。 //如果发送请求或处理结果出现错误，则返回相应的错误。 //- IsBinder方法用于判断当前扩展器是否配置了绑定方法。如果bindVerb不为空，则表示配置了绑定方法。 //- IsPrioritizer方法用于判断当前扩展器是否配置了优先级方法。如果prioritizeVerb不为空，则表示配置了优先级方法。 //- IsFilter方法用于判断当前扩展器是否配置了过滤方法。如果filterVerb不为空，则表示配置了过滤方法。 // Helper function to send messages to the extender func (h *HTTPExtender) send(action string, args interface{}, result interface{}) error { out, err := json.Marshal(args) if err != nil { return err } url := strings.TrimRight(h.extenderURL, \u0026#34;/\u0026#34;) + \u0026#34;/\u0026#34; + action req, err := http.NewRequest(\u0026#34;POST\u0026#34;, url, bytes.NewReader(out)) if err != nil { return err } req.Header.Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) resp, err := h.client.Do(req) if err != nil { return err } defer resp.Body.Close() if resp.StatusCode != http.StatusOK { return fmt.Errorf(\u0026#34;failed %v with extender at URL %v, code %v\u0026#34;, action, url, resp.StatusCode) } return json.NewDecoder(resp.Body).Decode(result) } //该函数是一个发送消息给扩展程序的辅助函数。 //它使用HTTP POST请求将动作、参数和结果发送到指定的扩展程序URL。 //具体步骤如下： //1. 将参数args转换为JSON格式。 //2. 构建请求URL，通过拼接扩展程序URL和动作字符串。 //3. 创建HTTP POST请求，设置请求头的Content-Type为application/json，并将JSON数据作为请求体。 //4. 发送请求并获取响应。 //5. 检查响应状态码，如果不是200 OK，则返回错误。 //6. 将响应体解码为结果参数result。 如果在上述过程中发生错误，则返回相应的错误。 // IsInterested returns true if at least one extended resource requested by // this pod is managed by this extender. func (h *HTTPExtender) IsInterested(pod *v1.Pod) bool { if h.managedResources.Len() == 0 { return true } if h.hasManagedResources(pod.Spec.Containers) { return true } if h.hasManagedResources(pod.Spec.InitContainers) { return true } return false } //该函数是一个Go语言函数，名为IsInterested，它属于HTTPExtender类型。 //函数用于判断给定的Pod是否至少请求了一个由该extender管理的扩展资源。 //函数返回一个布尔值，如果Pod请求了至少一个由该extender管理的扩展资源，则返回true，否则返回false。 //函数主要包含以下两个步骤： //1. 首先，函数检查HTTPExtender的managedResources列表是否为空。如果为空，则表示该extender管理所有资源，因此直接返回true。 //2. 如果managedResources不为空，函数会分别检查Pod的Spec.Containers和Spec.InitContainers字段中是否包含由该extender管理的扩展资源。 //如果存在至少一个由extender管理的扩展资源，则返回true。如果两个字段中都不包含由extender管理的扩展资源，则返回false。 //总结：该函数用于判断给定的Pod是否请求了由该extender管理的扩展资源，根据管理资源列表和Pod的容器配置进行匹配判断。 func (h *HTTPExtender) hasManagedResources(containers []v1.Container) bool { for i := range containers { container := \u0026amp;containers[i] for resourceName := range container.Resources.Requests { if h.managedResources.Has(string(resourceName)) { return true } } for resourceName := range container.Resources.Limits { if h.managedResources.Has(string(resourceName)) { return true } } } return false } //该函数用于判断给定的容器列表中是否包含有管理资源。 //具体实现为遍历容器列表，再遍历容器的资源请求和限制， //若存在管理资源则返回true，否则返回false。 "},{"id":117,"href":"/docs/k8s%E8%B0%83%E8%AF%95pod-k8s-diao-shi-pod/","title":"K8S调试POD 2024-04-03 14:58:15.293","section":"Docs","content":" 曾几何时，我们将自己的应用运行在Kubernetes上，每当出现容器异常崩溃时，我们往往都是一边重启容器，一边面对崩溃的容器无从下手。通常在业务研发自己build的镜像内包含了shell，我们还能通过在command中嵌入一个[\u0026quot;sleep\u0026quot;, \u0026quot;3600\u0026quot;]命令来阻塞容器内服务启动，不过也有时候会出现不知道从哪里冒出来一个distroless镜像，这时可能最先崩溃的就是运维了。那是一种运维这个职业自诞生以来，第一次感受到手足无措并脱离掌控的无助感。于是在k8s环境下无法debug容器的梗开始在坊间广为吐槽。 # 第一个打破魔咒的是kubectl-debug，它包含了agent和debug-tools两个部分。也是目前全网内搜到文档最全的解决方案。不过目前它的开发似乎已经停止，上一次提交还是在8个月之前，而最近一次Release版本也停留在两年前。更难以接受的是，当前它无法被集成在容器运行时为Containerd的k8s集群。 # 尽管kubectl-debug曾经确实是一款非常好用的容器调试工具，但如今Kubernetes已经有了更好的容器调试解决方案，Ephemeral Containers # Ephemeral Containers # Ephemeral Containers字如其名，它就是一个临时容器。这是一个自Kubernetes v1.16中作为alpha引入的新功能，虽然当前它还没有GA，不过自从在Kubernetes v1.18之后，在kubectl内已经集成了debug客户端，我们几乎可以完整的使用并体验它的新特性。 # 临时容器的目标是为Kubernetes用户提供一个故障诊断工具，同时具备满足以下需求： # 作为一个开箱即用的平台化工具 # 不依赖于已经包含在容器镜像中的工具 # 不需要直接登陆计算节点(可以通过Kubernetes API的管理访问Node) # 不过也有东西是临时容器不计划支持的，比如对windows上启用临时容器就不太友好。 # 启用临时容器的特性也非常简单，在kubernetes v1.16之后的版本中将启动参数--feature-gates=EphemeralContainers=true配置到kube-api和kubelet服务上重启即可。 # 在1.20之前，kubectl debug工具被放在alpha中，注意不同版本的命令操作差别 这里推荐使用客户端为1.20+的版本体验会更好 # 那么我们有了Ephemeral Containers能做哪些事情呢？ # 1. POD Troubleshooting # 如上文所说，我们可以直接通过kubectl debug命令进行容器调试。最直接简单的对一个pod进行调试命令如下： # kubectl debug mypod -it --image=busybox 复制\n默认情况下用户不指定临时容器名称的话，debug容器名称就由kubectl自动生成一个唯一id的名称。如果用户需要自己指定容器名称则使用\nkubectl debug mypod -c debugger --image=busybox 复制\n有了临时容器除了日常debug功能外，我们可以扩展出很多新花样的玩法。比如批量跑某个命名空间下的安全扫描的脚本而不用干扰原容器。\nfor pod in $(kubectl get -o name pod); do kubectl debug --image security/pod_scanner -p $pod /sanner.sh done 复制\n2. POD Troubleshooting by Copy # 对于没有开启Ephemeral Containers特性的集群，我们就只能通过复制模式来调试容器。它的原理是复制一个指定pod的新容器，并将debug作为sidecar跟随新容器一起启动。通过这种方式也能达到曲线救国的目的。此种方式的几个参数还是挺有意思：\n--copy-to 指定新pod的名称 --replace=true 是否删除原容器 --same-node=true 是否调度到和原容器一样的node上 --share-processes=true 是否共享容器pid空间 复制\n例如我们就可以启动一个跟需要调试pod一样配置的debug容器如下：\nkubectl debug mypod -it \\ --container=debug \\ --image=busybox \\ --copy-to=my-debugger \\ --same-node=true \\ --share-processes=true 复制\n3. Node Troubleshooting # 对！你没看错！利用Ephemeral Containers还能对Worker节点进行调试。当以节点为目标调用时，kubectl debug 将创建一个带有node名称的pod，并且调度到该节点。同时该容器还具备了hostIPC、hostNetwork和hostPID这些特权模式。不可思议的是Worker节点的根文件系统还被mount到了debug容器下的/host目录下。\n直接执行这个命令就能debug主机。\nkubectl debug node/mynode -it --image=busybox 复制\nDebug镜像 # 工欲善其事，必先利其器。不管怎样我们都需要一套工具完善的debug镜像，在处理问题时能够得心应手。虽然网上也有不少debug镜像，不过都还是不如自己构建来的畅快。\n这里小白分享一个Debug镜像的Dockerfile，大家可以根据自己条件修改即可。\nFROM golang:alpine as grpcurl RUN apk update \\ \u0026amp;\u0026amp; apk add --virtual build-dependencies git \\ \u0026amp;\u0026amp; apk add bash curl jq \\ \u0026amp;\u0026amp; go get -u github.com/fullstorydev/grpcurl \\ \u0026amp;\u0026amp; go install github.com/fullstorydev/grpcurl/cmd/grpcurl@latest FROM alpine:latest RUN sed -i \u0026#39;s/dl-cdn.alpinelinux.org/mirrors.tuna.tsinghua.edu.cn/g\u0026#39; /etc/apk/repositories \u0026amp;\u0026amp; \\ apk update \u0026amp;\u0026amp; \\ apk add --no-cache vim bash tcpdump curl wget strace mysql-client iproute2 redis jq iftop tzdata tar nmap bind-tools htop \u0026amp;\u0026amp; \\ ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime RUN wget -O /usr/bin/httpstat https://github.com/davecheney/httpstat/releases/download/v1.0.0/httpstat-linux-amd64-v1.0.0 \u0026amp;\u0026amp; \\ chmod +x /usr/bin/httpstat COPY --from=grpcurl /go/bin/grpcurl /usr/bin/grpcurl ENV TZ=Asia/Shanghai LC_ALL=C.UTF-8 LANG=C.UTF-8 LANGUAGE=C.UTF-8 ENTRYPOINT [ \u0026#34;/bin/bash\u0026#34; ] 复制\ndebug镜像内支持的工具包如下图\n总结 # 本文主要讲述了kubernetes在v1.18版本之后被提上alpha的Ephemeral Containers特性，通过临时容器我们可以debug容器，甚至还可以debug主机。它确实是一个非常方便和足以替代kubectl-debug的解决方案。不过，目前临时容器对于用户权限这块并没有特别的说明，特别是用特权模式调试主机的时候，希望后面能够借助PSP（Pod Security Policy）做一个额外的补充。\n"},{"id":118,"href":"/docs/k8s%E8%BF%90%E7%BB%B4%E4%B9%8B%E6%B8%85%E7%90%86%E7%A3%81%E7%9B%98-k8s-yun-wei-zhi-qing-li-ci-pan/","title":"k8s运维之清理磁盘 2024-04-03 15:13:57.279","section":"Docs","content":" 清理无用的镜像和关闭的容器 # 使用命令\ndocker system df docker system prune -a docker system df 命令，类似于 Linux上的 df 命令，用于查看 Docker 的磁盘使用情况：\nTYPE列出了 Docker 使用磁盘的 4 种类型：\nImages ：所有镜像占用的空间，包括拉取下来的镜像，和本地构建的。 Containers ：运行的容器占用的空间，表示每个容器的读写层的空间。 Local Volumes ：容器挂载本地数据卷的空间。 Build Cache ：镜像构建过程中产生的缓存空间（只有在使用 BuildKit 时才有，Docker 18.09 以后可用）。 最后的 RECLAIMABLE 是可回收大小。\ndocker system prune : 可以用于清理磁盘，删除关闭的容器、无用的数据卷和网络，以及 dangling 镜像（即无 tag 的镜像）。 docker system prune -a : 清理得更加彻底，可以将没有容器使用 Docker镜像都删掉。 注意，这两个命令会把你暂时关闭的容器，以及暂时没有用到的 Docker 镜像都删掉了。 清理有问题的容器 # 如果磁盘水位还是比较高。那大概率是某个容器有问题，比如疯狂往磁盘上记日志。\n使用命令\ndocker system df -v 可以看到每次容器的size。比如这个玩意\n然后就是docker exec -it 进去看看喽\n最后反馈给开发，不用往本地目录里打日志文件了。\n"},{"id":119,"href":"/docs/k8s%E9%9D%A2%E8%AF%95%E5%A4%A7%E5%85%A8-k8s-mian-shi-da-quan/","title":"K8S面试大全 2024-04-03 15:10:54.493","section":"Docs","content":"一、Kubernetes 基础知识面试题10 道面试题\n1、什么是 Kubernetes？\nKubernetes 是一个开源容器管理工具，负责容器部署，容器扩缩容以及负载平衡。它提供了出色的社区，并与所有云提供商合作。因此，我们可以说 Kubernetes 不是一个容器化平台，而是一个多容器管理解决方案。 # 2、 Kubernetes 与 docker 什么关系？\nDocker 提供容器的生命周期管理，Docker 镜像构建运行时容器。但是，由于这些单独的容器必须通信，因此使用 Kubernetes。因此，我们说 Docker 构建容器，这些容器通过 Kubernetes 相互通信。因此，可以使用 Kubernetes 手动关联和编排在多个主机上运行的容器。 # 3、Kubernetes 与 Docker Swarm 的区别？\nDocker Swarm 和 Kubernetes 都可以用于类似目的。它们都是容器编排工具。 # 4、在主机和容器上部署应用程序有什么区别？\n上图左侧架构表示在主机上部署应用程序。因此，这种架构将具有操作系统，然后操作系统将具有内核，该内核将在应用程序所需的操作系统上安装各种库。因此，在这种框架中，你可以拥有 n 个应用程序，并且所有应用程序将共享该操作系统中存在的库。 # 上图右侧架构是容器中部署应用程序。这种架构将有一个内核，这是唯一一个在所有应用程序之间唯一共同的东西。各个块基本上是容器化的，并且这些块与其他应用程序隔离。因此，应用程序具有与系统其余部分隔离的必要库和二进制文件，并且不能被任何其他应用程序侵占。 # 5、Kubernetes 如何简化容器化部署？\n跨主机的容器都需要相互通信。因此，要做到这一点，你需要一些能够负载平衡，扩展和监控容器的东西。由于 Kubernetes 与云无关并且可以在任何公共/私有提供商上运行，因此可以简化容器化部署程序。 # 6、什么是 kubectl？\nKubectl 是一个平台，可以使用该平台将命令传递给集群。因此，它基本上为CLI 提供了针对 Kubernetes 集群运行命令的方法，以及创建和管理 Kubernetes组件的各种方法。 # 7、什么是 kubelet？\n这是一个代理服务，它在每个节点上运行，并使从服务器与主服务器通信。因此，Kubelet 处理 PodSpec 中提供给它的容器的描述，并确保 PodSpec 中描述的容器运行正常。可以创建 pod、删除 pod。 # 8、k8s 有哪些组件？\n1、etcd 保存了整个集群的状态； # 2、apiserver 提供了资源操作的唯一入口，并提供认证、授权、访问控制、API 注册和发现等机制； # 3、controller manager 负责维护集群的状态，比如故障检测、自动扩展、滚动更新等； # 4、scheduler 负责资源的调度，按照预定的调度策略将 Pod 调度到相应的机器上； # 5、kubelet 负责维护容器的生命周期，同时也负责 Volume（CVI）和网络（CNI）的管理； # 6、Container runtime 负责镜像管理以及 Pod 和容器的真正运行（CRI）； # 7、kube-proxy 负责为 Service 提供 cluster 内部的服务发现和负载均衡。 # 9、Pod SVC Node Container 之间如何相互访问：\n1、同 Pod 内的容器：同一个 Pod 的容器共享同一个网络命名空间可以直接进行通讯 # 2、同 Node 内不同 Pod 的容器：多个 Pod 都关联在同一个 Docker0 网桥上，通过 docker0 网桥完成相互通讯。 # 3、不同 Node 内 Pod 的容器：不同 Node 上的 docker0 可能会相同，PodIP 和 docker0 是同网段的，所以需要将 PodIP 和 NodeIP 进行关联且保障唯 # 4，不同 Pod 之间的数据通过物理机的端口进行转发即可完成通讯。 # 二、Kubernetes 架构面试题10 道面试题\n1、kubernetes 控制节点组件有哪些？\n**Kubernetes 控制节点组件：**kube-controller-manager，kube-apiserver，kube-scheduler、Kubernetes # **工作节点组件：**kubelet 和 kube-proxy # 2、谈谈你对 kube-proxy 的理解？\nkube-proxy 是 Kubernetes 的核心组件，部署在每个 Node 节点上，它是实现Kubernetes Service 的通信与负载均衡机制的重要组件; kube-proxy 负责为 Pod 创建代理服务，从 apiserver 获取所有 server 信息，并根据 server 信息创建代理服务，实现server 到 Pod 的请求路由和转发，从而实现 K8s 层级的虚拟转发网络。 # 3、kube-apiserver 和 kube-scheduler 的作用是什么？\nkube -apiserver 遵循横向扩展架构，是主节点控制平面的前端。将公开Kubernetes 主节点组件的所有 API，并负责在 Kubernetes 节点和 Kubernetes主组件之间建立通信。 # kube-scheduler 是调度器，负责根据资源需求选择最合适的节点来运行未调度的 pod，并跟踪资源利用率。它确保不在资源已满的节点上调度 pod。 # 4、你能简要介绍一下 Kubernetes 控制管理器吗？\nController Manager 作为集群内部的管理控制中心，负责集群内的 Node、Pod 副本、服务端点（Endpoint）、命名空间（Namespace）、服务账号（ServiceAccount）、资源定额（ResourceQuota）的管理，当某个 Node 意外宕机时，Controller Manager 会及时发现并执行自动化修复流程，确保集群始终处于预期的工作状态。 # 5、Kubernetes 如何简化容器化部署？\n跨主机的容器都需要相互通信。因此，要做到这一点，你需要一些能够负载平衡，扩展和监控容器的东西。由于 Kubernetes 与云无关并且可以在任何公共/私有提供商上运行，因此可以简化容器化部署程序。 # 6、什么是 ETCD?\nEtcd 是用 Go 编程语言编写的，是一个分布式键值存储，用于协调分布式工作。因此，Etcd 存储 Kubernetes 集群的配置数据，表示在任何给定时间点的集群状态。 # 7、什么是 Ingress，它是如何工作的？\nIngress 网络是一组规则，充当 Kubernetes 集群的入口点。这允许入站连接，可以将其配置为通过可访问的 URL，负载平衡流量或通过提供基于名称的虚拟主机从外部提供服务。因此，Ingress 是一个 API 对象，通常通过 HTTP 管理集群中服务的外部访问，是暴露服务的最有效方式。 # 8、什么是 Headless Service？\nHeadless Service 类似于“普通”服务，但没有群集 IP。此服务使您可以直接访问pod，而无需通过代理访问它。 # 2.1.9 什么是集群联邦\n在联邦集群的帮助下，可以将多个 Kubernetes 集群作为单个集群进行管理。因此，您可以在数据中心/云中创建多个 Kubernetes 集群，并使用联邦来在一个位置控制/管理它们。 # 9、 k8s 提供几种服务发现\n两种服务发现：\n1、环境变量：当创建一个 Pod 的时候，kubelet 会在该 Pod 中注入集群内所有Service 的相关环境变量。 # 2、DNS：可以通过 cluster add-on 的方式轻松的创建 CoreDNS 来对集群内的Service 进行服务发现。 # 以上两种方式，一个是基于 TCP，DNS 基于 UDP，它们都是建立在四层协议之上。 # 10、Pod 中的应用共享几种资源\n1、PID 命名空间：Pod 中的不同应用程序可以看到其他应用程序的进程 ID。 # 2、网络命名空间：Pod 中的多个容器能够访问同一个 IP 和端口范围。 # 3、IPC 命名空间：Pod 中的多个容器能够使用 SystemV IPC 或 POSIX 消息队列进行通信。 # 4、UTS 命名空间：Pod 中的多个容器共享一个主机名。 # 5、Volumes（共享存储卷）：Pod 中的各个容器可以访问在 Pod 级别定义的Volumes。 # 三、k8s 使用场景面试题10 道面试题\n场景 1：\n假设一家基于单一架构的公司处理众多产品。现在，随着公司在当今的扩展行业的扩展，他们的单一架构开始引发问题。 # 您如何看待公司从单一服务转向微服务并部署其服务容器？ # 解：\n由于公司的目标是从单一应用程序转向微服务，它们最终可以逐个构建，并行构建，只需在后台切换配置。然后他们可以将这些内置微服务放在 Kubernetes 平台上。因此他们可以从一次或两次迁移服务开始，并监控它们以确保一切运行稳定。一旦他们觉得一切顺利，他们就可以将其余的应用程序迁移到他们的 Kubernetes 集群中。 # 场景 2：\n考虑一家拥有分布式系统的跨国公司，拥有大量数据中心，虚拟机和许多从事各种任务的员工。 # 您认为这样 的公司如何以与 Kubernetes 一致的方式管理所有任务？ # 解：\n正如我们所有人都知道 IT 部门推出了数千个容器，其任务在分布式系统中遍布全球众多节点。在这种情况下，公司可以使用能够为基于云的应用程序提供敏捷性，横向扩展功能和DevOps 实践的东西。因此，该公司可以使用 Kubernetes 来定制他们的调度架构并支持多种容器格式。这使得容器任务之间的亲和性成为可能，从而提供更高的效率，并为各种容器网络解决方案和容器存储提供广泛支持。 # 场景 3：\n考虑一种情况，即公司希望通过维持最低成本来提高其效率和技术运营速度。您认为公司将如何实现这一目标？ # 解：\n公司可以通过构建 CI/CD 管道来实现 DevOps 方法，但是这里可能出现的一个问题是配置可能需要一段时间才能启动并运行。因此，在实施 CI/CD 管道之后，公司的下一步应该是在云环境中工作。一旦他们开始处理云环境，他们就可以在集群上安排容器，并可以在 Kubernetes 的帮助下进行协调。这种方法将有助于公司缩短部署时间，并在各种环境中加快速度。 # 场景 4：\n假设一家公司想要修改它的部署方法，并希望建立一个更具可扩展性和响应性的平台。 # 您如何看待这家公司能够实现这一目标以满足客户需求？ # 解：\n为了给数百万客户提供他们期望的数字体验，公司需要一个可扩展且响应迅速的平台，以便他们能够快速地将数据发送到客户网站。现在，要做到这一点，公司应该从他们的私有数据中心（如果他们使用任何）转移到任何云环境，如 AWS。不仅如此，他们还应该实现微服务架构，以便他们可以开始使用 Docker 容器。一旦他们准备好基础框架，他们就可以开始使用最好的编排平台，即 Kubernetes。这将使团队能够自主地构建应用程序并快速交付它们。 # 场景 5：\n考虑一家拥有非常分散的系统的跨国公司，期待解决整体代码库问题。 # 您认为公司如何解决他们的问题？ # 解\n那么，为了解决这个问题，我们可以将他们的单片代码库转移到微服务设计，然后每个微服务都可以被视为一个容器。因此，所有这些容器都可以在 Kubernetes 的帮助下进行部署和协调。 # 场景 6：\n我们所有人都知道，从单片到微服务的转变解决了开发方面的问题，但却增加了部署方面的问题。 # 公司如何解决部署方面的问题？ # 解\n团队可以试验容器编排平台，例如 Kubernetes，并在数据中心运行。因此，通过这种方式，公司可以生成模板化应用程序，在五分钟内部署它，并在此时将实际实例集中在暂存环境中。这种 Kubernetes 项目将有数十个并行运行的微服务，以提高生产率，即使节点出现故障，也可以立即重新安排，而不会影响性能。 # 场景 7：\n假设一家公司希望通过采用新技术来优化其工作负载的分配。 # 公司如何有效地实现这种资源分配？ # 解\n这个问题的解决方案就是 Kubernetes。Kubernetes 确保资源得到有效优化，并且只使用特定应用程序所需的那些资源。因此，通过使用最佳容器编排工具，公司可以有效地实现资源分配 # 场景 8：\n考虑一家拼车公司希望通过同时扩展其平台来增加服务器数量。 # 您认为公司如何处理服务器及其安装？ # 解\n公司可以采用集装箱化的概念。一旦他们将所有应用程序部署到容器中，他们就可以使用 Kubernetes 进行编排，并使用像 Prometheus 这样的容器监视工具来监视容器中的操作。因此，利用容器的这种使用，在数据中心中为它们提供更好的容量规划，因为它们现在将受到更少的限制，因为服务和它们运行的硬件之间存在抽象。 # 场景 9：\n考虑一种情况，公司希望向具有各种环境的客户提供所有必需的分发。您认为他们如何以动态的方式实现这一关键目标？ # 解\n该公司可以使用 Docker 环境，组建一个横截面团队，使用 Kubernetes 构建 Web应用程序。这种框架将帮助公司实现在最短的时间内将所需产品投入生产的目标。因此，在这样的机器运行的情况下，公司可以向所有具有各种环境的客户发放电子邮件。 # 场景 10：\n假设公司希望在不同的云基础架构上运行各种工作负载，从裸机到公共云。 # 公司将如何在不同界面的存在下实现这一目标？ # 解\n该公司可以将其基础设施分解为微服务，然后采用 Kubernetes。这将使公司在不同的云基础架构上运行各种工作负载。 # 四、k8s 真实面试场景-面试题总结12 道面试题\n1、deployment 创建 pod 流程？\n1）、kubectl 提交创建 pod 命令,api 响应命令,通过一系列认证授权,把 pod 数据存储到etcd,创建 deployment 资源并初始化. # 2）、controller 通过 list-watch 机制,监测发现新的 deployment,将该资源加入到内部工作队列,发现该资源没有关联的 pod 和 replicaset,启用 deployment controller 创建replicaset 资源,再启用 replicaset controller 创建 pod. # 3）、所有 controller 正常后.将 deployment,replicaset,pod 资源更新存储到etcd. # 4）、scheduler 通过 list-watch 机制,监测发现新的 pod,经过主机过滤主机打分规则,将pod 绑定(binding)到合适的主机. # 5）、将绑定结果存储到 etcd # 6）、kubelet 每隔 20s(可以自定义)向 kube-apiserver 通过 NodeName 获取自身Node 上所要运行的 pod 清单.通过与自己的内部缓存进行比较,新增加 pod. # 7）、启动 pod 启动容器 # 2、你知道 HPA 吗？HPA 有什么缺点？\nhpa 是 k8s 的自动扩缩容策略。v1 版可支持基于 cpu 的扩缩容，v2 版可支持基于 cpu、内存、自定义指标的扩容。默认情况下，pod 是一台台扩容的，所以类似于 微博热搜之类的 突然间 访问量剧增的情况下就显有些无力。所以需要修改 k8s 的调度规则。如果波动量比较大的服务容易造成业务抖动。主要是很被动，无法进行提前被容。 # HPA 可针对 cpu 和内存自定义参数弹性扩容，有 5 分钟的安全时间，不适合访问量波动大的业务。 # 3、蓝绿发布和灰度发布 k8s 怎么实现的？\n蓝绿发布通过 deployment 部署 pod，改变 service 或者 ingress 切换流量可以实现灰度发布通过 Ingress Controller 或者 istio 可以实现 # 4、如果一个 pod 创建过程中一直处于 pending 状态，你的处理思路是什么？\n1）通过 describe 查看 pod 详细信息 # 2）通过 logs 查看 pod 日志 # 通过详细信息和日志基本就可以把问题定位出来了 # 5、k8s 如何实现持久化存储？有几种方式?\nemptyDir、hostPath、pv、storageclass、ceph、nfs、gluster 等都可以实现 k8s数据持久化，也可以通过 storageclass 动态的从 nfs provisioner 或者 cephprovisioner 等供应商动态的划分存储做成 pv # 6、service 的 type 类型有几种？\nClusterIp：集群内部相关访问 # NodePort：可以在物理机映射端口 # ExternalName：可以对 service 做软连接 # LoadBalancer：使用的是云的 slb # 7、ceph 架构是什么？\nCeph 是统一存储系统，支持三种接口： # 1）、Object：有原生的 API，而且也兼容 Swift 和 S3 的 API\n2）、Block：支持精简配置、快照、克隆\n3）、File：Posix 接口，支持快照\nCeph 也是分布式存储系统，它的特点是：\n**高扩展性：**使用普通 x86 服务器，支持 10~1000 台服务器，支持 TB 到 PB 级的扩展。 # **高可靠性：**没有单点故障，多数据副本，自动管理，自动修复。 # **高性能：**数据分布均衡，并行化度高。对于 objects storage 和 block storage,不需要元数据服务器。 # 8、k8s 怎么对接 ceph？\n把 ceph rbd 或者 cephfs 做成 pvStorageclass 可以动态从 ceph provisioner 找到 ceph，然后生成 pv # 9、k8s 挂载 cephfs 和 ceph rbd 适用场景分析\nK8s 挂载 cephfs 可以支持跨 node 节点 pod 挂载 # K8s 挂载 ceph rbd 不支持跨 node 节点 pod 挂载 # 10、k8s 有几种探测方式，分别描述下具体作用？\nlivenessProbe：存活性探测\n许多应用程序经过长时间运行，最终过渡到无法运行的状态，除了重启，无法恢复。通常情况下，K8S 会发现应用程序已经终止，然后重启应用程序 pod。有时应用程序可能因为某些原因（后端服务故障等）导致暂时无法对外提供服务，但应用软件没有终止，导致K8S 无法隔离有故障的 pod，调用者可能会访问到有故障的 pod，导致业务不稳定。K8S提供 livenessProbe 来检测容器是否正常运行，并且对相应状况进行相应的补救措施。 # readinessProbe：就绪性探测\n在没有配置 readinessProbe 的资源对象中，pod 中的容器启动完成后，就认为 pod 中的应用程序可以对外提供服务，该 pod 就会加入相对应的 service，对外提供服务。但有时一些应用程序启动后，需要较长时间的加载才能对外服务，如果这时对外提供服务，执行结果必然无法达到预期效果，影响用户体验。比如使用 tomcat 的应用程序来说，并不是简单地说 tomcat 启动成功就可以对外提供服务的，还需要等待 spring 容器初始化，数据库连接上等等。 # startupProbe: 探测容器中的应用是否已经启动。\n如果提供了启动探测(startupprobe)，则禁用所有其他探测，直到它成功为止。如果启动探测失败，kubelet 将杀死容器，容器服从其重启策略进行重启。如果容器没有提供启动探测，则默认状态为成功Success。 # 11、 k8s 的 service 与 Ingress 区别\nService 是四层代理，只能基于 ip 和端口代理后端服务Ingress controller 是七层代理，可以通过 http 或者 https 的域名基于 cookie、地域、请求头、百分比进行代理转发。Ingress controller 代理需要找到对应的 Service，由 Service 向后代理 Pod # 12、说几个 k8s 的网络插件，说一下他们的差异\nflannel：支持地址分配，不支持网络策略 。 # calico：支持地址分配，支持网络策略。 # flannel： # vxlan：#扩展的虚拟局域网 # V 虚拟的 # X 扩展的 # lan 局域网 # flannel 支持多种后端：\n1、VxLAN：\n(1) vxlan 叠加网络模式 # (2) Directrouting # 2、host-gw: Host Gateway\n#直接路由模式，不推荐，只能在二层网络中，不支持跨网络，如果有成千上万的 Pod，容易产生广播风暴 # 3、UDP：一般不用这个模式，性能差\nflannel 方案：需要在每个节点上把发向容器的数据包进行封装后，再用隧道将封装后的数据包发送到运行着目标 Pod 的 node 节点上。目标 node 节点再负责去掉封装，将去除封装的数据包发送到目标 Pod 上。数据通信性能则大受影响 # **4、calico 方案：**在 k8s 多个网路解决方案中选择了延迟表现最好的-calico 方案，可以设置网络策略适用于大型集群。 # "},{"id":120,"href":"/docs/k8s%E9%9D%A2%E8%AF%95%E5%AE%9D%E5%85%B8-k8s-mian-shi-bao-dian/","title":"K8S面试宝典 2024-04-03 15:11:09.316","section":"Docs","content":" 创建 Pod的主要流程? # 客户端提交 Pod 的配置信息(可以是 yaml 文件定义的信息)到 kube-apiserver. # Apiserver 收到指令后,通知 controllr-manager 创建一个资源对象 # controller-manager 通过 apiserver 将 pod 的配置信息存储到 ETCD 数据中心中 # kube-scheduler 检查到 pod 信息会开始调度预选,会先过滤不符合 Pod 资源配置要求的节点,然后开始调度调优,主要是挑选出更适合运行的 pod 节点,然后将 pod 的资源配置单发送到 node 节点上的 kubelet 组件上 # kubelet 根据 scheduler 发来的资源配置单运行 pod,运行成功后,将 pod 的运行的信息返回 scheduler, scheduler 将返回的 pod 运行状况的信息存储到 etcd 数据中心 # Pod 的重启策略 # • Pod 重启策略(RestartPolicy)应用于 Pod 内的所有容器,并且仅再 Pod 所处的 Node 上由 Kubelet 进行判断和重启操作.当某个容器异常退出或健康检查失败时,kubele 将根据 RestartPolicy 的设置来进行相应操作 # • pod 的重启策略包括 Always,OnFaliure 和 Never,默认值为 Always # • Always: 当容器失效时由 kubelet 自动重启该容器 # • OnFailure:当容器终止运行且退出不为 0 时, 由 kubelet 自动重启该容器 # • Nerve: 不论容器运行状态如何,kubelet 都不会重启该容器 # • 同时 pod 的容器策略与控制方式关联,当前可用于管理 Pod 的控制器包括 RelicatonController # Pod 的健康检查方式 # • LivenessProbe探针：用于判断容器是否存活（running状态），如果LivenessProbe探针探测到容器不健康，则kubelet将杀掉该容器，并根据容器的重启策略做相应处理。若一个容器不包含LivenessProbe探针，kubelet认为该容器的LivenessProbe探针返回值用于是“Success”。 # • ReadineeProbe探针：用于判断容器是否启动完成（ready状态）。如果ReadinessProbe探针探测到失败，则Pod的状态将被修改。Endpoint Controller将从Service的Endpoint中删除包含该容器所在Pod的Eenpoint。 # • startupProbe探针：启动检查机制，应用一些启动缓慢的业务，避免业务长时间启动而被上面两类探针kill掉。 # Pod 探针常见方式 # • ExecAction：在容器内执行一个命令，若返回码为0，则表明容器健康。 # • TCPSocketAction：通过容器的IP地址和端口号执行TCP检查，若能建立TCP连接，则表明容器健康。 # • HTTPGetAction：通过容器的IP地址、端口号及路径调用HTTP Get方法，若响应的状态码大于等于200且小于400，则表明容器健康。 # Pod 常见的调度方式 # • Deployment或RC：该调度策略主要功能就是自动部署一个容器应用的多份副本，以及持续监控副本的数量，在集群内始终维持用户指定的副本数量。 # • NodeSelector：定向调度，当需要手动指定将Pod调度到特定Node上，可以通过Node的标签（Label）和Pod的nodeSelector属性相匹配。 # • NodeAffinity亲和性调度：亲和性调度机制极大的扩展了Pod的调度能力，目前有两种节点亲和力表达： # • requiredDuringSchedulingIgnoredDuringExecution：硬规则，必须满足指定的规则，调度器才可以调度Pod至Node上（类似nodeSelector，语法不同）。 # • preferredDuringSchedulingIgnoredDuringExecution：软规则，优先调度至满足的Node的节点，但不强求，多个优先级规则还可以设置权重值。 # • Taints和Tolerations（污点和容忍）： # • Taint：使Node拒绝特定Pod运行； # • Toleration：为Pod的属性，表示Pod能容忍（运行）标注了Taint的Node。 # deployment升级策略? # • 在Deployment的定义中，可以通过spec.strategy指定Pod更新的策略，目前支持两种策略：Recreate（重建）和RollingUpdate（滚动更新），默认值为RollingUpdate。 # • Recreate：设置spec.strategy.type=Recreate，表示Deployment在更新Pod时，会先杀掉所有正在运行的Pod，然后创建新的Pod。 # • RollingUpdate：设置spec.strategy.type=RollingUpdate，表示Deployment会以滚动更新的方式来逐个更新Pod。同时，可以通过设置spec.strategy.rollingUpdate下的两个参数（maxUnavailable和maxSurge）来控制滚动更新的过程 # Kubernetes Service类型? # 通过创建Service，可以为一组具有相同功能的容器应用提供一个统一的入口地址，并且将请求负载分发到后端的各个容器应用上。其主要类型有： # • ClusterIP：虚拟的服务IP地址，该地址用于Kubernetes集群内部的Pod访问，在Node上kube-proxy通过设置的iptables规则进行转发； # • NodePort：使用宿主机的端口，使能够访问各Node的外部客户端通过Node的IP地址和端口号就能访问服务； # • LoadBalancer：使用外接负载均衡器完成到服务的负载分发，需要在spec.status.loadBalancer字段指定外部负载均衡器的IP地址，通常用于公有云。 # Service分发后端的策略? # Service负载分发的策略有：RoundRobin和SessionAffinity # • RoundRobin：默认为轮询模式，即轮询将请求转发到后端的各个Pod上。 # • SessionAffinity：基于客户端IP地址进行会话保持的模式，即第1次将某个客户端发起的请求转发到后端的某个Pod上，之后从相同的客户端发起的请求都将被转发到后端相同的Pod上。 # Kubernetes外部如何访问集群内的服务? # • 映射Pod到物理机：将Pod端口号映射到宿主机，即在Pod中采用hostPort方式，以使客户端应用能够通过物理机访问容器应用。 # • 映射Service到物理机：将Service端口号映射到宿主机，即在Service中采用nodePort方式，以使客户端应用能够通过物理机访问容器应用。 # • 映射Sercie到LoadBalancer：通过设置LoadBalancer映射到云服务商提供的LoadBalancer地址。这种用法仅用于在公有云服务提供商的云平台上设置Service的场景。 # Kubernetes ingress? # • Kubernetes的Ingress资源对象，用于将不同URL的访问请求转发到后端不同的Service，以实现HTTP层的业务路由机制。 # • Kubernetes使用了Ingress策略和Ingress Controller，两者结合并实现了一个完整的Ingress负载均衡器。使用Ingress进行负载分发时，Ingress Controller基于Ingress规则将客户端请求直接转发到Service对应的后端Endpoint（Pod）上，从而跳过kube-proxy的转发功能，kube-proxy不再起作用，全过程为：ingress controller + ingress 规则 \u0026mdash;-\u0026gt; services。 # • 同时当Ingress Controller提供的是对外服务，则实际上实现的是边缘路由器的功能。 # Kubernetes镜像的下载策略? # K8s的镜像下载策略有三种：Always、Never、IFNotPresent。 # • Always：镜像标签为latest时，总是从指定的仓库中获取镜像。 # • Never：禁止从仓库中下载镜像，也就是说只能使用本地镜像。 # • IfNotPresent：仅当本地没有对应镜像时，才从目标仓库中下载。 # 默认的镜像下载策略是：当镜像标签是latest时，默认策略是Always；当镜像标签是自定义时（也就是标签不是latest），那么默认策略是IfNotPresent。 # Kubernetes kubelet的作用? # • 在Kubernetes集群中，在每个Node（又称Worker）上都会启动一个kubelet服务进程。该进程用于处理Master下发到本节点的任务，管理Pod及Pod中的容器。每个kubelet进程都会在API Server上注册节点自身的信息，定期向Master汇报节点资源的使用情况，并通过cAdvisor监控容器和节点资源。 # Secret有哪些使用方式? # • 创建完secret之后，可通过如下三种方式使用： # • 在创建Pod时，通过为Pod指定Service Account来自动使用该Secret。 # • 通过挂载该Secret到Pod来使用它。 # • 在Docker镜像下载时使用，通过指定Pod的spc.ImagePullSecrets来引用它。 # Kubernetes CNI模型? # • CNI提供了一种应用容器的插件化网络解决方案，定义对容器网络进行操作和配置的规范，通过插件的形式对CNI接口进行实现。CNI仅关注在创建容器时分配网络资源，和在销毁容器时删除网络资源。在CNI模型中只涉及两个概念：容器和网络。 # • 容器（Container）：是拥有独立Linux网络命名空间的环境，例如使用Docker或rkt创建的容器。容器需要拥有自己的Linux网络命名空间，这是加入网络的必要条件。 # • 网络（Network）：表示可以互连的一组实体，这些实体拥有各自独立、唯一的IP地址，可以是容器、物理机或者其他网络设备（比如路由器）等。 # • 对容器网络的设置和操作都通过插件（Plugin）进行具体实现，CNI插件包括两种类型：CNI Plugin和IPAM（IP Address Management）Plugin。CNI Plugin负责为容器配置网络资源，IPAM Plugin负责对容器的IP地址进行分配和管理。IPAM Plugin作为CNI Plugin的一部分，与CNI Plugin协同工作。 # Kubernetes PV和PVC? # • PV是对底层网络共享存储的抽象，将共享存储定义为一种“资源”。 # • PVC则是用户对存储资源的一个“申请”。 # PV生命周期内的阶段? # 某个PV在生命周期中可能处于以下4个阶段（Phaes）之一。 # • Available：可用状态，还未与某个PVC绑定。 # • Bound：已与某个PVC绑定。 # • Released：绑定的PVC已经删除，资源已释放，但没有被集群回收。 # • Failed：自动资源回收失败。 # calico 网络模式 # 模式 数据包封包 优点 缺点 vxlan 封包， 在vxlan设备上将pod发来的数据包源、目的mac替换为本机vxlan网卡和对端节点vxlan网卡的mac。外层udp目的ip地址根据路由和对端vxlan的mac查fdb表获取 只要k8s节点间三层互通， 可以跨网段， 对主机网关路由没有特殊要求。各个node节点通过vxlan设备实现基于三层的”二层”互通, 三层即vxlan包封装在udp数据包中， 要求udp在k8s节点间三层可达；二层即vxlan封包的源mac地址和目的mac地址是自己的vxlan设备mac和对端vxlan设备mac。 需要进行vxlan的数据包封包和解包会存在一定的性能损耗 ipip 封包，在tunl0设备上将pod发来的数据包的mac层去掉，留下ip层封包。 外层数据包目的ip地址根据路由得到。 只要k8s节点间三层互通， 可以跨网段， 对主机网关路由没有特殊要求。 需要进行ipip的数据包封包和解包会存在一定的性能损耗 bgp 不需要进行数据包封包 不用封包解包，通过bgp协议可实现pod网络在主机间的三层可达， k8s节点不跨网段时和flannel的host-gw相似； 支持跨网段， 满足复杂的网络架构 跨网段时，需要主机网关路由也充当BGP Speaker能够学习到pod子网路由并实现pod子网路由的转发 fannel三种模式 # fannel三种模式 效率 calico 模式 UDP 性能较差，封包解包涉及到多次用户态和内核态交互 类似 IPIP VXLAN 性能较好，封包解包在内核态实现，内核转发数据，flanneld负责动态配置ARP和FDB（转发数据库）表项更新 类似VXLAN host-gw 性能最好，不需要再次封包，正常发包，目的容器所在的主机充当网关 flanneld 负责主机上路由表的刷新 类似 BGP 你知道的几种CNI网络插件，并详述其工作原理。K8s常用的CNI网络插件 （calico \u0026amp;\u0026amp; flannel），简述一下它们的工作原理和区别。 # \\1. calico根据iptables规则进行路由转发，并没有进行封包，解包的过程，这和flannel比起来效率就会快多 calico包括如下重要组件：Felix，etcd，BGP Client，BGP Route Reflector。下面分别说明一下这些组件。 # Felix：主要负责路由配置以及ACLS规则的配置以及下发，它存在在每个node节点上。 etcd：分布式键值存储，主要负责网络元数据一致性，确保Calico网络状态的准确性，可以与kubernetes共用； BGPClient(BIRD), 主要负责把 Felix写入 kernel的路由信息分发到当前 Calico网络，确保 workload间的通信的有效性； BGPRoute Reflector(BIRD), 大规模部署时使用，摒弃所有节点互联的mesh模式，通过一个或者多个 BGPRoute Reflector 来完成集中式的路由分发 通过将整个互联网的可扩展 IP网络原则压缩到数据中心级别，Calico在每一个计算节点利用 Linuxkernel 实现了一个高效的 vRouter来负责数据转发，而每个vRouter通过 BGP协议负责把自己上运行的 workload的路由信息向整个Calico网络内传播，小规模部署可以直接互联，大规模下可通过指定的BGProute reflector 来完成。这样保证最终所有的workload之间的数据流量都是通过 IP包的方式完成互联的。 # \\1. Flannel的工作原理： Flannel实质上是一种“覆盖网络(overlay network)”，也就是将TCP数据包装在另一种网络包里面进行路由转发和通信，目前已经支持UDP、VxLAN、AWS VPC和GCE路由等数据转发方式。 # 默认的节点间数据通信方式是UDP转发。 工作原理： 数据从源容器中发出后，经由所在主机的docker0虚拟网卡转发到flannel0虚拟网卡（先可以不经过docker0网卡，使用cni模式），这是个P2P的虚拟网卡，flanneld服务监听在网卡的另外一端。 Flannel通过Etcd服务维护了一张节点间的路由表，详细记录了各节点子网网段 。 源主机的flanneld服务将原本的数据内容UDP封装后根据自己的路由表投递给目的节点的flanneld服务，数据到达以后被解包，然后直接进入目的节点的flannel0虚拟网卡，然后被转发到目的主机的docker0虚拟网卡，最后就像本机容器通信一下的有docker0路由到达目标容器。 flannel在进行路由转发的基础上进行了封包解包的操作，这样浪费了CPU的计算资源。 # Worker节点宕机，简述Pods驱逐流程。 # \\1. 在 Kubernetes 集群中，当节点由于某些原因（网络、宕机等）不能正常工作时会被认定为不可用状态（Unknown 或者 False 状态），当时间超过了 pod-eviction-timeout 值时，那么节点上的所有 Pod 都会被节点控制器计划删除。 # \\2. Kubernetes 集群中有一个节点生命周期控制器：node_lifecycle_controller.go。它会与每一个节点上的 kubelet 进行通信，以收集各个节点已经节点上容器的相关状态信息。当超出一定时间后不能与 kubelet 通信，那么就会标记该节点为 Unknown 状态。并且节点生命周期控制器会自动创建代表状况的污点，用于防止调度器调度 pod 到该节点。 # \\3. 那么 Unknown 状态的节点上已经运行的 pod 会怎么处理呢？节点上的所有 Pod 都会被污点管理器（taint_manager.go）计划删除。而在节点被认定为不可用状态到删除节点上的 Pod 之间是有一段时间的，这段时间被称为容忍度。如果在不配置的情况下，Kubernetes 会自动给 Pod 添加一个 key 为 node.kubernetes.io/not-ready 的容忍度 并配置 tolerationSeconds=300，同样，Kubernetes 会给 Pod 添加一个 key 为 node.kubernetes.io/unreachable 的容忍度 并配置 tolerationSeconds=300。 # \\4. 当到了删除 Pod 时，污点管理器会创建污点标记事件，然后驱逐 pod 。这里需要注意的是由于已经不能与 kubelet 通信，所以该节点上的 Pod 在管理后台看到的是处于灰色标记，但是此时如果去获取 pod 的状态其实还是处于 Running 状态。每种类型的资源都有相应的资源控制器（Controller），例如：deployment_controller.go、stateful_set_control.go。每种控制器都在监听资源变化，从而做出相应的动作执行。deployment 控制器在监听到 Pod 被驱逐后会创建一个新的 Pod 出来，但是 Statefulset 控制器并不会创建出新的 Pod，原因是因为它可能会违反 StatefulSet 固有的至多一个的语义，可能出现具有相同身份的多个成员，这将可能是灾难性的，并且可能导致数据丢失。 # 你知道的K8s中几种Controller控制器，并详述其工作原理 # \\1. deployment：适合无状态的服务部署 适合部署无状态的应用服务，用来管理pod和replicaset，具有上线部署、副本设定、滚动更新、回滚等功能，还可提供声明式更新，例如只更新一个新的Image # • 编写yaml文件，并创建nginx服务pod资源。 # \\1. StatefullSet：适合有状态的服务部署 适合部署有状态应用，解决Pod的独立生命周期，保持Pod启动顺序和唯一性。 # • 稳定，唯一的网络标识符，持久存储（例如：etcd配置文件，节点地址发生变化，将无法使用） # • 有序，优雅的部署和扩展、删除和终止（例如：mysql主从关系，先启动主，再启动从）有序，滚动更新 # • 应用场景：例如数据库 # 无状态服务的特点： # • deployment 认为所有的pod都是一样的 # • 不用考虑顺序的要求 # • 不用考虑在哪个node节点上运行 # • 可以随意扩容和缩容 # 有状态服务的特点： # • 实例之间有差别，每个实例都有自己的独特性，元数据不同，例如etcd，zookeeper # • 实例之间不对等的关系，以及依靠外部存储的应用 # • 常规的service服务和无头服务的区别 # • service：一组Pod访问策略，提供cluster-IP群集之间通讯，还提供负载均衡和服务发现 # • Headless service 无头服务，不需要cluster-IP，直接绑定具体的Pod的IP，无头服务经常用于statefulset的有状态部署 # • 创建无头服务的service资源和dns资源，由于有状态服务的IP地址是动态的，所以使用无头服务的时候要绑定dns服务 # \\1. DaemonSet：一次部署，所有的node节点都会部署，例如一些典型的应用场景： 运行集群存储 daemon，例如在每个Node上运行 glusterd、ceph # • 在每个Node上运行日志收集 daemon，例如 fluentd、 logstash # • 在每个Node上运行监控 daemon，例如 Prometheus Node Exporter # • 在每一个Node上运行一个Pod # • 新加入的Node也同样会自动运行一个Pod # • 应用场景：监控，分布式存储，日志收集等 # \\1. Job：一次性的执行任务 # • 一次性执行任务，类似Linux中的job # • 应用场景：如离线数据处理，视频解码等业务 # \\1. Cronjob：周期性的执行任务 # • 周期性任务，像Linux的Crontab一样 # • 应用场景：如通知，备份等 # • 使用cronjob要慎重，用完之后要删掉，不然会占用很多资源 # ingress-controller的工作机制 # 通常情况下，service和pod的IP仅可在集群内部访问 # • k8s提供了service方式：NodePort 来提供对外的服务，外部的服务可以通过访问Node节点ip+NodePort端口来访问集群内部的资源，外部的请求先到达service所选中的节点上，然后负载均衡到每一个节点上。 # NodePort虽然提供了对外的方式但也有很大弊端： # • 由于service的实现方式：user_space 、iptebles、 3 ipvs、方式这三种方式只支持在4层协议通信，不支持7层协议，因此NodePort不能代理https服务。 # • NodePort 需要暴露service所属每个node节点上端口，当需求越来越多，端口数量过多，导致维护成本过高，并且集群不好管理。 # 原理 # • Ingress也是Kubernetes API的标准资源类型之一，它其实就是一组基于DNS名称（host）或URL路径把请求转发到指定的Service资源的规则。用于将集群外部的请求流量转发到集群内部完成的服务发布。我们需要明白的是，Ingress资源自身不能进行“流量穿透”，仅仅是一组规则的集合，这些集合规则还需要其他功能的辅助，比如监听某套接字，然后根据这些规则的匹配进行路由转发，这些能够为Ingress资源监听套接字并将流量转发的组件就是Ingress Controller。 # • Ingress 控制器不同于Deployment 等pod控制器的是，Ingress控制器不直接运行为kube-controller-manager的一部分，它仅仅是Kubernetes集群的一个附件，类似于CoreDNS，需要在集群上单独部署。 # • ingress controller通过监视api server获取相关ingress、service、endpoint、secret、node、configmap对象，并在程序内部不断循环监视相关service是否有新的endpoints变化，一旦发生变化则自动更新nginx.conf模板配置并产生新的配置文件进行reload # k8s的调度机制 # \\1. Scheduler工作原理： 请求及Scheduler调度步骤： # • 节点预选(Predicate)：排除完全不满足条件的节点，如内存大小，端口等条件不满足。 # • 节点优先级排序(Priority)：根据优先级选出最佳节点 # • 节点择优(Select)：根据优先级选定节点 # \\1. 具体步骤： # • 首先用户通过 Kubernetes 客户端 Kubectl 提交创建 Pod 的 Yaml 的文件，向Kubernetes 系统发起资源请求，该资源请求被提交到 # • Kubernetes 系统中，用户通过命令行工具 Kubectl 向 Kubernetes 集群即 APIServer 用 的方式发送“POST”请求，即创建 Pod 的请求。 # • APIServer 接收到请求后把创建 Pod 的信息存储到 Etcd 中，从集群运行那一刻起，资源调度系统 Scheduler 就会定时去监控 APIServer # • 通过 APIServer 得到创建 Pod 的信息，Scheduler 采用 watch 机制，一旦 Etcd 存储 Pod 信息成功便会立即通知APIServer， # • APIServer会立即把Pod创建的消息通知Scheduler，Scheduler发现 Pod 的属性中 Dest Node 为空时（Dest Node=””）便会立即触发调度流程进行调度。 # • 而这一个创建Pod对象，在调度的过程当中有3个阶段：节点预选、节点优选、节点选定，从而筛选出最佳的节点 # • 节点预选：基于一系列的预选规则对每个节点进行检查，将那些不符合条件的节点过滤，从而完成节点的预选 # • 节点优选：对预选出的节点进行优先级排序，以便选出最合适运行Pod对象的节点 # • 节点选定：从优先级排序结果中挑选出优先级最高的节点运行Pod，当这类节点多于1个时，则进行随机选择 # \\1. k8s的调用工作方式 # • Kubernetes调度器作为集群的大脑，在如何提高集群的资源利用率、保证集群中服务的稳定运行中也会变得越来越重要Kubernetes的资源分为两种属性。 # • 可压缩资源（例如CPU循环，Disk I/O带宽）都是可以被限制和被回收的，对于一个Pod来说可以降低这些资源的使用量而不去杀掉Pod。 # • 不可压缩资源（例如内存、硬盘空间）一般来说不杀掉Pod就没法回收。未来Kubernetes会加入更多资源，如网络带宽，存储IOPS的支持。 # kube-proxy的三种工作模式和原理 # \\1. userspace 模式 # • 该模式下kube-proxy会为每一个Service创建一个监听端口。发向Cluster IP的请求被Iptables规则重定向到Kube-proxy监听的端口上，Kube-proxy根据LB算法选择一个提供服务的Pod并和其建立链接，以将请求转发到Pod上。 # • 该模式下，Kube-proxy充当了一个四层Load balancer的角色。由于kube-proxy运行在userspace中，在进行转发处理时会增加两次内核和用户空间之间的数据拷贝，效率较另外两种模式低一些；好处是当后端的Pod不可用时，kube-proxy可以重试其他Pod。 # \\1. iptables 模式 # • 为了避免增加内核和用户空间的数据拷贝操作，提高转发效率，Kube-proxy提供了iptables模式。在该模式下，Kube-proxy为service后端的每个Pod创建对应的iptables规则，直接将发向Cluster IP的请求重定向到一个Pod IP。 # • 该模式下Kube-proxy不承担四层代理的角色，只负责创建iptables规则。该模式的优点是较userspace模式效率更高，但不能提供灵活的LB策略，当后端Pod不可用时也无法进行重试。 # \\1. 该模式和iptables类似，kube-proxy监控Pod的变化并创建相应的ipvs rules。ipvs也是在kernel模式下通过netfilter实现的，但采用了hash table来存储规则，因此在规则较多的情况下，Ipvs相对iptables转发效率更高。除此以外，ipvs支持更多的LB算法。如果要设置kube-proxy为ipvs模式，必须在操作系统中安装IPVS内核模块。 # "},{"id":121,"href":"/docs/k8s%E9%9D%A2%E8%AF%95%E9%A2%98-k8s-mian-shi-ti/","title":"K8S面试题 2024-08-02 17:50:50.418","section":"Docs","content":" 简述ETCD及其特点？ # etcd 是 CoreOS 团队发起的开源项目，是一个管理配置信息和服务发现（service discovery）的项目，它的目标是构建一个高可用的分布式键值（key-value）数据库，基于 Go 语言实现。\n特点：\n简单：支持 REST 风格的 HTTP+JSON API 安全：支持 HTTPS 方式的访问 快速：支持并发 1k/s 的写操作 可靠：支持分布式结构，基于 Raft 的一致性算法，Raft 是一套通过选举主节点来实现分布式系统一致性的算法。 简述ETCD适应的场景？ # etcd基于其优秀的特点，可广泛的应用于以下场景：\n服务发现(Service Discovery)：服务发现主要解决在同一个分布式集群中的进程或服务，要如何才能找到对方并建立连接。本质上来说，服务发现就是想要了解集群中是否有进程在监听udp或tcp端口，并且通过名字就可以查找和连接。\n消息发布与订阅：在分布式系统中，最适用的一种组件间通信方式就是消息发布与订阅。即构建一个配置共享中心，数据提供者在这个配置中心发布消息，而消息使用者则订阅他们关心的主题，一旦主题有消息发布，就会实时通知订阅者。通过这种方式可以做到分布式系统配置的集中式管理与动态更新。应用中用到的一些配置信息放到etcd上进行集中管理。\n负载均衡：在分布式系统中，为了保证服务的高可用以及数据的一致性，通常都会把数据和服务部署多份，以此达到对等服务，即使其中的某一个服务失效了，也不影响使用。etcd本身分布式架构存储的信息访问支持负载均衡。etcd集群化以后，每个etcd的核心节点都可以处理用户的请求。所以，把数据量小但是访问频繁的消息数据直接存储到etcd中也可以实现负载均衡的效果。\n分布式通知与协调：与消息发布和订阅类似，都用到了etcd中的Watcher机制，通过注册与异步通知机制，实现分布式环境下不同系统之间的通知与协调，从而对数据变更做到实时处理。\n分布式锁：因为etcd使用Raft算法保持了数据的强一致性，某次操作存储到集群中的值必然是全局一致的，所以很容易实现分布式锁。锁服务有两种使用方式，一是保持独占，二是控制时序。\n集群监控与Leader竞选：通过etcd来进行监控实现起来非常简单并且实时性强。\n简述什么是Kubernetes？ # Kubernetes是一个全新的基于容器技术的分布式系统支撑平台。是Google开源的容器集群管理系统（谷歌内部:Borg）。在Docker技术的基础上，为容器化的应用提供部署运行、资源调度、服务发现和动态伸缩等一系列完整功能，提高了大规模容器集群管理的便捷性。并且具有完备的集群管理能力，多层次的安全防护和准入机制、多租户应用支撑能力、透明的服务注册和发现机制、內建智能负载均衡器、强大的故障发现和自我修复能力、服务滚动升级和在线扩容能力、可扩展的资源自动调度机制以及多粒度的资源配额管理能力。\n简述Kubernetes和Docker的关系？ # Docker 提供容器的生命周期管理和，Docker 镜像构建运行时容器。它的主要优点是将将软件/应用程序运行所需的设置和依赖项打包到一个容器中，从而实现了可移植性等优点。\nKubernetes 用于关联和编排在多个主机上运行的容器。\n简述Kubernetes中什么是Minikube、Kubectl、Kubelet？ # Minikube 是一种可以在本地轻松运行一个单节点 Kubernetes 群集的工具。\nKubectl 是一个命令行工具，可以使用该工具控制Kubernetes集群管理器，如检查群集资源，创建、删除和更新组件，查看应用程序。\nKubelet 是一个代理服务，它在每个节点上运行，并使从服务器与主服务器通信。\n简述Kubernetes常见的部署方式？ # 常见的Kubernetes部署方式有：\nkubeadm：也是推荐的一种部署方式； 二进制：网页上很多教程，未来我也会写一个 minikube：在本地轻松运行一个单节点 Kubernetes 群集的工具。 简述Kubernetes如何实现集群管理？ # 在集群管理方面，Kubernetes将集群中的机器划分为一个Master节点和一群工作节点Node。其中，在Master节点运行着集群管理相关的一组进程kube-apiserver、kube-controller-manager和kube-scheduler，这些进程实现了整个集群的资源管理、Pod调度、弹性伸缩、安全控制、系统监控和纠错等管理能力，并且都是全自动完成的。\n简述Kubernetes的优势、适应场景及其特点？\nKubernetes作为一个完备的分布式系统支撑平台，其主要优势：\n容器编排 轻量级 开源 弹性伸缩 负载均衡 Kubernetes常见场景：\n快速部署应用 快速扩展应用 无缝对接新的应用功能 节省资源，优化硬件资源的使用 Kubernetes相关特点：\n可移植: 支持公有云、私有云、混合云、多重云（multi-cloud）。 可扩展: 模块化,、插件化、可挂载、可组合。 自动化: 自动部署、自动重启、自动复制、自动伸缩/扩展。 简述Kubernetes的缺点或当前的不足之处？ # Kubernetes当前存在的缺点（不足）如下：\n安装过程和配置相对困难复杂。 管理服务相对繁琐。 运行和编译需要很多时间。 它比其他替代品更昂贵。 对于简单的应用程序来说，可能不需要涉及Kubernetes即可满足。 简述Kubernetes相关基础概念？ # master：k8s集群的管理节点，负责管理集群，提供集群的资源数据访问入口。拥有Etcd存储服务（可选），运行Api Server进程，Controller Manager服务进程及Scheduler服务进程。\nnode（worker）：Node（worker）是Kubernetes集群架构中运行Pod的服务节点，是Kubernetes集群操作的单元，用来承载被分配Pod的运行，是Pod运行的宿主机。运行docker eninge服务，守护进程kunelet及负载均衡器kube-proxy。\npod：运行于Node节点上，若干相关容器的组合。Pod内包含的容器运行在同一宿主机上，使用相同的网络命名空间、IP地址和端口，能够通过localhost进行通信。Pod是Kurbernetes进行创建、调度和管理的最小单位，它提供了比容器更高层次的抽象，使得部署和管理更加灵活。一个Pod可以包含一个容器或者多个相关容器。\nlabel：Kubernetes中的Label实质是一系列的Key/Value键值对，其中key与value可自定义。Label可以附加到各种资源对象上，如Node、Pod、Service、RC等。一个资源对象可以定义任意数量的Label，同一个Label也可以被添加到任意数量的资源对象上去。Kubernetes通过Label Selector（标签选择器）查询和筛选资源对象。\nReplication Controller：Replication Controller用来管理Pod的副本，保证集群中存在指定数量的Pod副本。集群中副本的数量大于指定数量，则会停止指定数量之外的多余容器数量。反之，则会启动少于指定数量个数的容器，保证数量不变。Replication Controller是实现弹性伸缩、动态扩容和滚动升级的核心。\nDeployment：Deployment在内部使用了RS来实现目的，Deployment相当于RC的一次升级，其最大的特色为可以随时获知当前Pod的部署进度。\nHPA（Horizontal Pod Autoscaler）：Pod的横向自动扩容，也是Kubernetes的一种资源，通过追踪分析RC控制的所有Pod目标的负载变化情况，来确定是否需要针对性的调整Pod副本数量。\nService：Service定义了Pod的逻辑集合和访问该集合的策略，是真实服务的抽象。Service提供了一个统一的服务访问入口以及服务代理和发现机制，关联多个相同Label的Pod，用户不需要了解后台Pod是如何运行。\nVolume：Volume是Pod中能够被多个容器访问的共享目录，Kubernetes中的Volume是定义在Pod上，可以被一个或多个Pod中的容器挂载到某个目录下。\nNamespace：Namespace用于实现多租户的资源隔离，可将集群内部的资源对象分配到不同的Namespace中，形成逻辑上的不同项目、小组或用户组，便于不同的Namespace在共享使用整个集群的资源的同时还能被分别管理。\n简述Kubernetes集群相关组件？ # Kubernetes Master控制组件，调度管理整个系统（集群），包含如下组件:\nKubernetes API Server：作为Kubernetes系统的入口，其封装了核心对象的增删改查操作，以RESTful API接口方式提供给外部客户和内部组件调用，集群内各个功能模块之间数据交互和通信的中心枢纽。\nKubernetes Scheduler：为新建立的Pod进行节点(node)选择(即分配机器)，负责集群的资源调度。\nKubernetes Controller：负责执行各种控制器，目前已经提供了很多控制器来保证Kubernetes的正常运行。\nReplication Controller：管理维护Replication Controller，关联Replication Controller和Pod，保证Replication Controller定义的副本数量与实际运行Pod数量一致。\nNode Controller：管理维护Node，定期检查Node的健康状态，标识出(失效|未失效)的Node节点。\nNamespace Controller：管理维护Namespace，定期清理无效的Namespace，包括Namesapce下的API对象，比如Pod、Service等。\nService Controller：管理维护Service，提供负载以及服务代理。\nEndPoints Controller：管理维护Endpoints，关联Service和Pod，创建Endpoints为Service的后端，当Pod发生变化时，实时更新Endpoints。\nService Account Controller：管理维护Service Account，为每个Namespace创建默认的Service Account，同时为Service Account创建Service Account Secret。\nPersistent Volume Controller：管理维护Persistent Volume和Persistent Volume Claim，为新的Persistent Volume Claim分配Persistent Volume进行绑定，为释放的Persistent Volume执行清理回收。\nDaemon Set Controller：管理维护Daemon Set，负责创建Daemon Pod，保证指定的Node上正常的运行Daemon Pod。\nDeployment Controller：管理维护Deployment，关联Deployment和Replication Controller，保证运行指定数量的Pod。当Deployment更新时，控制实现Replication Controller和Pod的更新。\nJob Controller：管理维护Job，为Jod创建一次性任务Pod，保证完成Job指定完成的任务数目\nPod Autoscaler Controller：实现Pod的自动伸缩，定时获取监控数据，进行策略匹配，当满足条件时执行Pod的伸缩动作。\n简述Kubernetes RC的机制？ # Replication Controller用来管理Pod的副本，保证集群中存在指定数量的Pod副本。当定义了RC并提交至Kubernetes集群中之后，Master节点上的Controller Manager组件获悉，并同时巡检系统中当前存活的目标Pod，并确保目标Pod实例的数量刚好等于此RC的期望值，若存在过多的Pod副本在运行，系统会停止一些Pod，反之则自动创建一些Pod。\n简述Kubernetes Replica Set 和 Replication Controller 之间有什么区别？\nReplica Set 和 Replication Controller 类似，都是确保在任何给定时间运行指定数量的 Pod 副本。不同之处在于RS 使用基于集合的选择器，而 Replication Controller 使用基于权限的选择器。\n简述kube-proxy作用？ # kube-proxy 运行在所有节点上，它监听 apiserver 中 service 和 endpoint 的变化情况，创建路由规则以提供服务 IP 和负载均衡功能。简单理解此进程是Service的透明代理兼负载均衡器，其核心功能是将到某个Service的访问请求转发到后端的多个Pod实例上。\n简述kube-proxy iptables原理？ # Kubernetes从1.2版本开始，将iptables作为kube-proxy的默认模式。iptables模式下的kube-proxy不再起到Proxy的作用，其核心功能：通过API Server的Watch接口实时跟踪Service与Endpoint的变更信息，并更新对应的iptables规则，Client的请求流量则通过iptables的NAT机制“直接路由”到目标Pod。\n简述kube-proxy ipvs原理？ # IPVS在Kubernetes1.11中升级为GA稳定版。IPVS则专门用于高性能负载均衡，并使用更高效的数据结构（Hash表），允许几乎无限的规模扩张，因此被kube-proxy采纳为最新模式。\n在IPVS模式下，使用iptables的扩展ipset，而不是直接调用iptables来生成规则链。iptables规则链是一个线性的数据结构，ipset则引入了带索引的数据结构，因此当规则很多时，也可以很高效地查找和匹配。\n可以将ipset简单理解为一个IP（段）的集合，这个集合的内容可以是IP地址、IP网段、端口等，iptables可以直接添加规则对这个“可变的集合”进行操作，这样做的好处在于可以大大减少iptables规则的数量，从而减少性能损耗。\n简述kube-proxy ipvs和iptables的异同？ # iptables与IPVS都是基于Netfilter实现的，但因为定位不同，二者有着本质的差别：iptables是为防火墙而设计的；IPVS则专门用于高性能负载均衡，并使用更高效的数据结构（Hash表），允许几乎无限的规模扩张。\n与iptables相比，IPVS拥有以下明显优势：\n1、为大型集群提供了更好的可扩展性和性能； 2、支持比iptables更复杂的复制均衡算法（最小负载、最少连接、加权等）； 3、支持服务器健康检查和连接重试等功能； 4、可以动态修改ipset的集合，即使iptables的规则正在使用这个集合。 简述Kubernetes中什么是静态Pod？ # 静态pod是由kubelet进行管理的仅存在于特定Node的Pod上，他们不能通过API Server进行管理，无法与ReplicationController、Deployment或者DaemonSet进行关联，并且kubelet无法对他们进行健康检查。静态Pod总是由kubelet进行创建，并且总是在kubelet所在的Node上运行。\n简述Kubernetes中Pod可能位于的状态？ # Pending：API Server已经创建该Pod，且Pod内还有一个或多个容器的镜像没有创建，包括正在下载镜像的过程。\nRunning：Pod内所有容器均已创建，且至少有一个容器处于运行状态、正在启动状态或正在重启状态。\nSucceeded：Pod内所有容器均成功执行退出，且不会重启。\nFailed：Pod内所有容器均已退出，但至少有一个容器退出为失败状态。\nUnknown：由于某种原因无法获取该Pod状态，可能由于网络通信不畅导致。\n简述Kubernetes创建一个Pod的主要流程？ # Kubernetes中创建一个Pod涉及多个组件之间联动，主要流程如下：\n1、客户端提交Pod的配置信息（可以是yaml文件定义的信息）到kube-apiserver。 2、Apiserver收到指令后，通知给controller-manager创建一个资源对象。 3、Controller-manager通过api-server将pod的配置信息存储到ETCD数据中心中。 4、Kube-scheduler检测到pod信息会开始调度预选，会先过滤掉不符合Pod资源配置要求的节点，然后开始调度调优，主要是挑选出更适合运行pod的节点，然后将pod的资源配置单发送到node节点上的kubelet组件上。 5、Kubelet根据scheduler发来的资源配置单运行pod，运行成功后，将pod的运行信息返回给scheduler，scheduler将返回的pod运行状况的信息存储到etcd数据中心。 简述Kubernetes中Pod的重启策略？ # Pod重启策略（RestartPolicy）应用于Pod内的所有容器，并且仅在Pod所处的Node上由kubelet进行判断和重启操作。当某个容器异常退出或者健康检查失败时，kubelet将根据RestartPolicy的设置来进行相应操作。\nPod的重启策略包括Always、OnFailure和Never，默认值为Always。\nAlways：当容器失效时，由kubelet自动重启该容器； OnFailure：当容器终止运行且退出码不为0时，由kubelet自动重启该容器； Never：不论容器运行状态如何，kubelet都不会重启该容器。 同时Pod的重启策略与控制方式关联，当前可用于管理Pod的控制器包括ReplicationController、Job、DaemonSet及直接管理kubelet管理（静态Pod）。\n不同控制器的重启策略限制如下：\nRC和DaemonSet：必须设置为Always，需要保证该容器持续运行； Job：OnFailure或Never，确保容器执行完成后不再重启； kubelet：在Pod失效时重启，不论将RestartPolicy设置为何值，也不会对Pod进行健康检查。 简述Kubernetes中Pod的健康检查方式？ # 对Pod的健康检查可以通过两类探针来检查：LivenessProbe和ReadinessProbe。\nLivenessProbe探针：用于判断容器是否存活（running状态），如果LivenessProbe探针探测到容器不健康，则kubelet将杀掉该容器，并根据容器的重启策略做相应处理。若一个容器不包含LivenessProbe探针，kubelet认为该容器的LivenessProbe探针返回值用于是“Success”。\nReadineeProbe探针：用于判断容器是否启动完成（ready状态）。如果ReadinessProbe探针探测到失败，则Pod的状态将被修改。Endpoint Controller将从Service的Endpoint中删除包含该容器所在Pod的Eenpoint。\nstartupProbe探针：启动检查机制，应用一些启动缓慢的业务，避免业务长时间启动而被上面两类探针kill掉。\n简述Kubernetes Pod的LivenessProbe探针的常见方式？ # kubelet定期执行LivenessProbe探针来诊断容器的健康状态，通常有以下三种方式：\nExecAction：在容器内执行一个命令，若返回码为0，则表明容器健康。\nTCPSocketAction：通过容器的IP地址和端口号执行TCP检查，若能建立TCP连接，则表明容器健康。\nHTTPGetAction：通过容器的IP地址、端口号及路径调用HTTP Get方法，若响应的状态码大于等于200且小于400，则表明容器健康。\n简述Kubernetes Pod的常见调度方式？ # Kubernetes中，Pod通常是容器的载体，主要有如下常见调度方式：\nDeployment或RC：该调度策略主要功能就是自动部署一个容器应用的多份副本，以及持续监控副本的数量，在集群内始终维持用户指定的副本数量。 NodeSelector：定向调度，当需要手动指定将Pod调度到特定Node上，可以通过Node的标签（Label）和Pod的nodeSelector属性相匹配。 NodeAffinity亲和性调度：亲和性调度机制极大的扩展了Pod的调度能力，目前有两种节点亲和力表达： requiredDuringSchedulingIgnoredDuringExecution：硬规则，必须满足指定的规则，调度器才可以调度Pod至Node上（类似nodeSelector，语法不同）。 preferredDuringSchedulingIgnoredDuringExecution：软规则，优先调度至满足的Node的节点，但不强求，多个优先级规则还可以设置权重值。 Taints和Tolerations（污点和容忍）： Taint：使Node拒绝特定Pod运行； Toleration：为Pod的属性，表示Pod能容忍（运行）标注了Taint的Node。 简述Kubernetes初始化容器（init container）？ # init container的运行方式与应用容器不同，它们必须先于应用容器执行完成，当设置了多个init container时，将按顺序逐个运行，并且只有前一个init container运行成功后才能运行后一个init container。当所有init container都成功运行后，Kubernetes才会初始化Pod的各种信息，并开始创建和运行应用容器。\n简述Kubernetes deployment升级过程？ # 初始创建Deployment时，系统创建了一个ReplicaSet，并按用户的需求创建了对应数量的Pod副本。 当更新Deployment时，系统创建了一个新的ReplicaSet，并将其副本数量扩展到1，然后将旧ReplicaSet缩减为2。 之后，系统继续按照相同的更新策略对新旧两个ReplicaSet进行逐个调整。 最后，新的ReplicaSet运行了对应个新版本Pod副本，旧的ReplicaSet副本数量则缩减为0。 简述Kubernetes deployment升级策略？ # 在Deployment的定义中，可以通过spec.strategy指定Pod更新的策略，目前支持两种策略：Recreate（重建）和RollingUpdate（滚动更新），默认值为RollingUpdate。\nRecreate：设置spec.strategy.type=Recreate，表示Deployment在更新Pod时，会先杀掉所有正在运行的Pod，然后创建新的Pod。\nRollingUpdate：设置spec.strategy.type=RollingUpdate，表示Deployment会以滚动更新的方式来逐个更新Pod。同时，可以通过设置spec.strategy.rollingUpdate下的两个参数（maxUnavailable和maxSurge）来控制滚动更新的过程。\n简述Kubernetes DaemonSet类型的资源特性？ # DaemonSet资源对象会在每个Kubernetes集群中的节点上运行，并且每个节点只能运行一个pod，这是它和deployment资源对象的最大也是唯一的区别。因此，在定义yaml文件中，不支持定义replicas。\n它的一般使用场景如下：\n在去做每个节点的日志收集工作。 监控每个节点的的运行状态。 简述Kubernetes自动扩容机制？ # Kubernetes使用Horizontal Pod Autoscaler（HPA）的控制器实现基于CPU使用率进行自动Pod扩缩容的功能。HPA控制器周期性地监测目标Pod的资源性能指标，并与HPA资源对象中的扩缩容条件进行对比，在满足条件时对Pod副本数量进行调整。\nHPA原理\nKubernetes中的某个Metrics Server（Heapster或自定义Metrics Server）持续采集所有Pod副本的指标数据。HPA控制器通过Metrics Server的API（Heapster的API或聚合API）获取这些数据，基于用户定义的扩缩容规则进行计算，得到目标Pod副本数量。\n当目标Pod副本数量与当前副本数量不同时，HPA控制器就向Pod的副本控制器（Deployment、RC或ReplicaSet）发起scale操作，调整Pod的副本数量，完成扩缩容操作。\n简述Kubernetes Service类型？ # 通过创建Service，可以为一组具有相同功能的容器应用提供一个统一的入口地址，并且将请求负载分发到后端的各个容器应用上。其主要类型有：\nClusterIP：虚拟的服务IP地址，该地址用于Kubernetes集群内部的Pod访问，在Node上kube-proxy通过设置的iptables规则进行转发； NodePort：使用宿主机的端口，使能够访问各Node的外部客户端通过Node的IP地址和端口号就能访问服务； LoadBalancer：使用外接负载均衡器完成到服务的负载分发，需要在spec.status.loadBalancer字段指定外部负载均衡器的IP地址，通常用于公有云。 简述Kubernetes Service分发后端的策略？ # Service负载分发的策略有：RoundRobin和SessionAffinity\nRoundRobin：默认为轮询模式，即轮询将请求转发到后端的各个Pod上。 SessionAffinity：基于客户端IP地址进行会话保持的模式，即第1次将某个客户端发起的请求转发到后端的某个Pod上，之后从相同的客户端发起的请求都将被转发到后端相同的Pod上。 简述Kubernetes Headless Service？ # 在某些应用场景中，若需要人为指定负载均衡器，不使用Service提供的默认负载均衡的功能，或者应用程序希望知道属于同组服务的其他实例。Kubernetes提供了Headless Service来实现这种功能，即不为Service设置ClusterIP（入口IP地址），仅通过Label Selector将后端的Pod列表返回给调用的客户端。\n简述Kubernetes外部如何访问集群内的服务？ # 对于Kubernetes，集群外的客户端默认情况，无法通过Pod的IP地址或者Service的虚拟IP地址:虚拟端口号进行访问。通常可以通过以下方式进行访问Kubernetes集群内的服务：\n映射Pod到物理机：将Pod端口号映射到宿主机，即在Pod中采用hostPort方式，以使客户端应用能够通过物理机访问容器应用。\n映射Service到物理机：将Service端口号映射到宿主机，即在Service中采用nodePort方式，以使客户端应用能够通过物理机访问容器应用。\n映射Sercie到LoadBalancer：通过设置LoadBalancer映射到云服务商提供的LoadBalancer地址。这种用法仅用于在公有云服务提供商的云平台上设置Service的场景。\n简述Kubernetes ingress？ # Kubernetes的Ingress资源对象，用于将不同URL的访问请求转发到后端不同的Service，以实现HTTP层的业务路由机制。\nKubernetes使用了Ingress策略和Ingress Controller，两者结合并实现了一个完整的Ingress负载均衡器。使用Ingress进行负载分发时，Ingress Controller基于Ingress规则将客户端请求直接转发到Service对应的后端Endpoint（Pod）上，从而跳过kube-proxy的转发功能，kube-proxy不再起作用，全过程为：ingress controller + ingress 规则 \u0026mdash;-\u0026gt; services。\n同时当Ingress Controller提供的是对外服务，则实际上实现的是边缘路由器的功能。\n简述Kubernetes镜像的下载策略？ # K8s的镜像下载策略有三种：Always、Never、IFNotPresent。\nAlways：镜像标签为latest时，总是从指定的仓库中获取镜像。 Never：禁止从仓库中下载镜像，也就是说只能使用本地镜像。 IfNotPresent：仅当本地没有对应镜像时，才从目标仓库中下载。默认的镜像下载策略是：当镜像标签是latest时，默认策略是Always；当镜像标签是自定义时（也就是标签不是latest），那么默认策略是IfNotPresent。 简述Kubernetes的负载均衡器？ # 负载均衡器是暴露服务的最常见和标准方式之一。\n根据工作环境使用两种类型的负载均衡器，即内部负载均衡器或外部负载均衡器。内部负载均衡器自动平衡负载并使用所需配置分配容器，而外部负载均衡器将流量从外部负载引导至后端容器。\n简述Kubernetes各模块如何与API Server通信？ # Kubernetes API Server作为集群的核心，负责集群各功能模块之间的通信。集群内的各个功能模块通过API Server将信息存入etcd，当需要获取和操作这些数据时，则通过API Server提供的REST接口（用GET、LIST或WATCH方法）来实现，从而实现各模块之间的信息交互。\n如kubelet进程与API Server的交互：每个Node上的kubelet每隔一个时间周期，就会调用一次API Server的REST接口报告自身状态，API Server在接收到这些信息后，会将节点状态信息更新到etcd中。\n如kube-controller-manager进程与API Server的交互：kube-controller-manager中的Node Controller模块通过API Server提供的Watch接口实时监控Node的信息，并做相应处理。\n如kube-scheduler进程与API Server的交互：Scheduler通过API Server的Watch接口监听到新建Pod副本的信息后，会检索所有符合该Pod要求的Node列表，开始执行Pod调度逻辑，在调度成功后将Pod绑定到目标节点上。\n简述Kubernetes Scheduler作用及实现原理？ # Kubernetes Scheduler是负责Pod调度的重要功能模块，Kubernetes Scheduler在整个系统中承担了“承上启下”的重要功能，“承上”是指它负责接收Controller Manager创建的新Pod，为其调度至目标Node；“启下”是指调度完成后，目标Node上的kubelet服务进程接管后继工作，负责Pod接下来生命周期。\nKubernetes Scheduler的作用是将待调度的Pod（API新创建的Pod、Controller Manager为补足副本而创建的Pod等）按照特定的调度算法和调度策略绑定（Binding）到集群中某个合适的Node上，并将绑定信息写入etcd中。\n在整个调度过程中涉及三个对象，分别是待调度Pod列表、可用Node列表，以及调度算法和策略。\nKubernetes Scheduler通过调度算法调度为待调度Pod列表中的每个Pod从Node列表中选择一个最适合的Node来实现Pod的调度。随后，目标节点上的kubelet通过API Server监听到Kubernetes Scheduler产生的Pod绑定事件，然后获取对应的Pod清单，下载Image镜像并启动容器。\n简述Kubernetes Scheduler使用哪两种算法将Pod绑定到worker节点？ # Kubernetes Scheduler根据如下两种调度算法将 Pod 绑定到最合适的工作节点：\n预选（Predicates）：输入是所有节点，输出是满足预选条件的节点。kube-scheduler根据预选策略过滤掉不满足策略的Nodes。如果某节点的资源不足或者不满足预选策略的条件则无法通过预选。如“Node的label必须与Pod的Selector一致”。\n优选（Priorities）：输入是预选阶段筛选出的节点，优选会根据优先策略为通过预选的Nodes进行打分排名，选择得分最高的Node。例如，资源越富裕、负载越小的Node可能具有越高的排名。\n简述Kubernetes kubelet的作用？ # 在Kubernetes集群中，在每个Node（又称Worker）上都会启动一个kubelet服务进程。该进程用于处理Master下发到本节点的任务，管理Pod及Pod中的容器。每个kubelet进程都会在API Server上注册节点自身的信息，定期向Master汇报节点资源的使用情况，并通过cAdvisor监控容器和节点资源。\n简述Kubernetes kubelet监控Worker节点资源是使用什么组件来实现的？ # kubelet使用cAdvisor对worker节点资源进行监控。在 Kubernetes 系统中，cAdvisor 已被默认集成到 kubelet 组件内，当 kubelet 服务启动时，它会自动启动 cAdvisor 服务，然后 cAdvisor 会实时采集所在节点的性能指标及在节点上运行的容器的性能指标。\n简述Kubernetes如何保证集群的安全性？ # Kubernetes通过一系列机制来实现集群的安全控制，主要有如下不同的维度：\n基础设施方面：保证容器与其所在宿主机的隔离；\n权限方面：\n最小权限原则：合理限制所有组件的权限，确保组件只执行它被授权的行为，通过限制单个组件的能力来限制它的权限范围。 用户权限：划分普通用户和管理员的角色。 集群方面：\nAPI Server的认证授权：Kubernetes集群中所有资源的访问和变更都是通过Kubernetes API Server来实现的，因此需要建议采用更安全的HTTPS或Token来识别和认证客户端身份（Authentication），以及随后访问权限的授权（Authorization）环节。 API Server的授权管理：通过授权策略来决定一个API调用是否合法。对合法用户进行授权并且随后在用户访问时进行鉴权，建议采用更安全的RBAC方式来提升集群安全授权。 敏感数据引入Secret机制：对于集群敏感数据建议使用Secret方式进行保护。 AdmissionControl（准入机制）：对kubernetes api的请求过程中，顺序为：先经过认证 \u0026amp; 授权，然后执行准入操作，最后对目标对象进行操作。 简述Kubernetes准入机制？ # 在对集群进行请求时，每个准入控制代码都按照一定顺序执行。如果有一个准入控制拒绝了此次请求，那么整个请求的结果将会立即返回，并提示用户相应的error信息。\n准入控制（AdmissionControl）准入控制本质上为一段准入代码，在对kubernetes api的请求过程中，顺序为：先经过认证 \u0026amp; 授权，然后执行准入操作，最后对目标对象进行操作。常用组件（控制代码）如下：\nAlwaysAdmit：允许所有请求 AlwaysDeny：禁止所有请求，多用于测试环境。 ServiceAccount：它将serviceAccounts实现了自动化，它会辅助serviceAccount做一些事情，比如如果pod没有serviceAccount属性，它会自动添加一个default，并确保pod的serviceAccount始终存在。 LimitRanger：观察所有的请求，确保没有违反已经定义好的约束条件，这些条件定义在namespace中LimitRange对象中。 NamespaceExists：观察所有的请求，如果请求尝试创建一个不存在的namespace，则这个请求被拒绝。 简述Kubernetes RBAC及其特点（优势）？ # RBAC是基于角色的访问控制，是一种基于个人用户的角色来管理对计算机或网络资源的访问的方法。\n相对于其他授权模式，RBAC具有如下优势：\n对集群中的资源和非资源权限均有完整的覆盖。 整个RBAC完全由几个API对象完成， 同其他API对象一样， 可以用kubectl或API进行操作。 可以在运行时进行调整，无须重新启动API Server。 简述Kubernetes Secret作用？ # Secret对象，主要作用是保管私密数据，比如密码、OAuth Tokens、SSH Keys等信息。将这些私密信息放在Secret对象中比直接放在Pod或Docker Image中更安全，也更便于使用和分发。\n简述Kubernetes Secret有哪些使用方式？ # 创建完secret之后，可通过如下三种方式使用：\n在创建Pod时，通过为Pod指定Service Account来自动使用该Secret。 通过挂载该Secret到Pod来使用它。 在Docker镜像下载时使用，通过指定Pod的spc.ImagePullSecrets来引用它。 简述Kubernetes PodSecurityPolicy机制？ # Kubernetes PodSecurityPolicy是为了更精细地控制Pod对资源的使用方式以及提升安全策略。在开启PodSecurityPolicy准入控制器后，Kubernetes默认不允许创建任何Pod，需要创建PodSecurityPolicy策略和相应的RBAC授权策略（Authorizing Policies），Pod才能创建成功。\n简述Kubernetes PodSecurityPolicy机制能实现哪些安全策略？ # 在PodSecurityPolicy对象中可以设置不同字段来控制Pod运行时的各种安全策略，常见的有：\n特权模式：privileged是否允许Pod以特权模式运行。 宿主机资源：控制Pod对宿主机资源的控制，如hostPID：是否允许Pod共享宿主机的进程空间。 用户和组：设置运行容器的用户ID（范围）或组（范围）。 提升权限：AllowPrivilegeEscalation：设置容器内的子进程是否可以提升权限，通常在设置非root用户（MustRunAsNonRoot）时进行设置。 SELinux：进行SELinux的相关配置。 简述Kubernetes网络模型？ # Kubernetes网络模型中每个Pod都拥有一个独立的IP地址，并假定所有Pod都在一个可以直接连通的、扁平的网络空间中。所以不管它们是否运行在同一个Node（宿主机）中，都要求它们可以直接通过对方的IP进行访问。设计这个原则的原因是，用户不需要额外考虑如何建立Pod之间的连接，也不需要考虑如何将容器端口映射到主机端口等问题。\n同时为每个Pod都设置一个IP地址的模型使得同一个Pod内的不同容器会共享同一个网络命名空间，也就是同一个Linux网络协议栈。这就意味着同一个Pod内的容器可以通过localhost来连接对方的端口。\n在Kubernetes的集群里，IP是以Pod为单位进行分配的。一个Pod内部的所有容器共享一个网络堆栈（相当于一个网络命名空间，它们的IP地址、网络设备、配置等都是共享的）。\n简述Kubernetes CNI模型？ # CNI提供了一种应用容器的插件化网络解决方案，定义对容器网络进行操作和配置的规范，通过插件的形式对CNI接口进行实现。CNI仅关注在创建容器时分配网络资源，和在销毁容器时删除网络资源。在CNI模型中只涉及两个概念：容器和网络。\n容器（Container）：是拥有独立Linux网络命名空间的环境，例如使用Docker或rkt创建的容器。容器需要拥有自己的Linux网络命名空间，这是加入网络的必要条件。\n网络（Network）：表示可以互连的一组实体，这些实体拥有各自独立、唯一的IP地址，可以是容器、物理机或者其他网络设备（比如路由器）等。\n对容器网络的设置和操作都通过插件（Plugin）进行具体实现，CNI插件包括两种类型：CNI Plugin和IPAM（IP Address Management）Plugin。CNI Plugin负责为容器配置网络资源，IPAM Plugin负责对容器的IP地址进行分配和管理。IPAM Plugin作为CNI Plugin的一部分，与CNI Plugin协同工作。\n简述Kubernetes网络策略？ # 为实现细粒度的容器间网络访问隔离策略，Kubernetes引入Network Policy。\nNetwork Policy的主要功能是对Pod间的网络通信进行限制和准入控制，设置允许访问或禁止访问的客户端Pod列表。Network Policy定义网络策略，配合策略控制器（Policy Controller）进行策略的实现。\n简述Kubernetes网络策略原理？ # Network Policy的工作原理主要为：policy controller需要实现一个API Listener，监听用户设置的Network Policy定义，并将网络访问规则通过各Node的Agent进行实际设置（Agent则需要通过CNI网络插件实现）。\n简述Kubernetes中flannel的作用？ # Flannel可以用于Kubernetes底层网络的实现，主要作用有：\n它能协助Kubernetes，给每一个Node上的Docker容器都分配互相不冲突的IP地址。 它能在这些IP地址之间建立一个覆盖网络（Overlay Network），通过这个覆盖网络，将数据包原封不动地传递到目标容器内。 简述Kubernetes Calico网络组件实现原理？ # Calico是一个基于BGP的纯三层的网络方案，与OpenStack、Kubernetes、AWS、GCE等云平台都能够良好地集成。\nCalico在每个计算节点都利用Linux Kernel实现了一个高效的vRouter来负责数据转发。每个vRouter都通过BGP协议把在本节点上运行的容器的路由信息向整个Calico网络广播，并自动设置到达其他节点的路由转发规则。\nCalico保证所有容器之间的数据流量都是通过IP路由的方式完成互联互通的。Calico节点组网时可以直接利用数据中心的网络结构（L2或者L3），不需要额外的NAT、隧道或者Overlay Network，没有额外的封包解包，能够节约CPU运算，提高网络效率。\n简述Kubernetes共享存储的作用？ # Kubernetes对于有状态的容器应用或者对数据需要持久化的应用，因此需要更加可靠的存储来保存应用产生的重要数据，以便容器应用在重建之后仍然可以使用之前的数据。因此需要使用共享存储。\n简述Kubernetes数据持久化的方式有哪些？ # Kubernetes 通过数据持久化来持久化保存重要数据，常见的方式有：\nEmptyDir（空目录）：没有指定要挂载宿主机上的某个目录，直接由Pod内保部映射到宿主机上。类似于docker中的manager volume。\n场景：\n只需要临时将数据保存在磁盘上，比如在合并/排序算法中； 作为两个容器的共享存储。 特性：\n同个pod里面的不同容器，共享同一个持久化目录，当pod节点删除时，volume的数据也会被删除。 emptyDir的数据持久化的生命周期和使用的pod一致，一般是作为临时存储使用。 Hostpath：将宿主机上已存在的目录或文件挂载到容器内部。类似于docker中的bind mount挂载方式。\n特性：增加了pod与节点之间的耦合。 PersistentVolume（简称PV）：如基于NFS服务的PV，也可以基于GFS的PV。它的作用是统一数据持久化目录，方便管理。\n简述Kubernetes PV和PVC？ # PV是对底层网络共享存储的抽象，将共享存储定义为一种“资源”。\nPVC则是用户对存储资源的一个“申请”。\n简述Kubernetes PV生命周期内的阶段？ # 某个PV在生命周期中可能处于以下4个阶段（Phaes）之一。\nAvailable：可用状态，还未与某个PVC绑定。 Bound：已与某个PVC绑定。 Released：绑定的PVC已经删除，资源已释放，但没有被集群回收。 Failed：自动资源回收失败。 简述Kubernetes所支持的存储供应模式？ # Kubernetes支持两种资源的存储供应模式：静态模式（Static）和动态模式（Dynamic）。\n静态模式：集群管理员手工创建许多PV，在定义PV时需要将后端存储的特性进行设置。\n动态模式：集群管理员无须手工创建PV，而是通过StorageClass的设置对后端存储进行描述，标记为某种类型。此时要求PVC对存储的类型进行声明，系统将自动完成PV的创建及与PVC的绑定。\n简述Kubernetes CSI模型？ # Kubernetes CSI是Kubernetes推出与容器对接的存储接口标准，存储提供方只需要基于标准接口进行存储插件的实现，就能使用Kubernetes的原生存储机制为容器提供存储服务。CSI使得存储提供方的代码能和Kubernetes代码彻底解耦，部署也与Kubernetes核心组件分离，显然，存储插件的开发由提供方自行维护，就能为Kubernetes用户提供更多的存储功能，也更加安全可靠。\nCSI包括CSI Controller和CSI Node：\nCSI Controller的主要功能是提供存储服务视角对存储资源和存储卷进行管理和操作。 CSI Node的主要功能是对主机（Node）上的Volume进行管理和操作。 简述Kubernetes Worker节点加入集群的过程？ # 通常需要对Worker节点进行扩容，从而将应用系统进行水平扩展。主要过程如下：\n1、在该Node上安装Docker、kubelet和kube-proxy服务； 2、然后配置kubelet和kubeproxy的启动参数，将Master URL指定为当前Kubernetes集群Master的地址，最后启动这些服务； 3、通过kubelet默认的自动注册机制，新的Worker将会自动加入现有的Kubernetes集群中； 4、Kubernetes Master在接受了新Worker的注册之后，会自动将其纳入当前集群的调度范围。 简述Kubernetes Pod如何实现对节点的资源控制？ # Kubernetes集群里的节点提供的资源主要是计算资源，计算资源是可计量的能被申请、分配和使用的基础资源。当前Kubernetes集群中的计算资源主要包括CPU、GPU及Memory。CPU与Memory是被Pod使用的，因此在配置Pod时可以通过参数CPU Request及Memory Request为其中的每个容器指定所需使用的CPU与Memory量，Kubernetes会根据Request的值去查找有足够资源的Node来调度此Pod。\n通常，一个程序所使用的CPU与Memory是一个动态的量，确切地说，是一个范围，跟它的负载密切相关：负载增加时，CPU和Memory的使用量也会增加。\n简述Kubernetes Requests和Limits如何影响Pod的调度？ # 当一个Pod创建成功时，Kubernetes调度器（Scheduler）会为该Pod选择一个节点来执行。对于每种计算资源（CPU和Memory）而言，每个节点都有一个能用于运行Pod的最大容量值。调度器在调度时，首先要确保调度后该节点上所有Pod的CPU和内存的Requests总和，不超过该节点能提供给Pod使用的CPU和Memory的最大容量值。\n简述Kubernetes Metric Service？ # 在Kubernetes从1.10版本后采用Metrics Server作为默认的性能数据采集和监控，主要用于提供核心指标（Core Metrics），包括Node、Pod的CPU和内存使用指标。\n对其他自定义指标（Custom Metrics）的监控则由Prometheus等组件来完成。\n简述Kubernetes中，如何使用EFK实现日志的统一管理？ # 在Kubernetes集群环境中，通常一个完整的应用或服务涉及组件过多，建议对日志系统进行集中化管理，通常采用EFK实现。\nEFK是 Elasticsearch、Fluentd 和 Kibana 的组合，其各组件功能如下：\nElasticsearch：是一个搜索引擎，负责存储日志并提供查询接口； Fluentd：负责从 Kubernetes 搜集日志，每个node节点上面的fluentd监控并收集该节点上面的系统日志，并将处理过后的日志信息发送给Elasticsearch； Kibana：提供了一个 Web GUI，用户可以浏览和搜索存储在 Elasticsearch 中的日志。 通过在每台node上部署一个以DaemonSet方式运行的fluentd来收集每台node上的日志。Fluentd将docker日志目录/var/lib/docker/containers和/var/log目录挂载到Pod中，然后Pod会在node节点的/var/log/pods目录中创建新的目录，可以区别不同的容器日志输出，该目录下有一个日志文件链接到/var/lib/docker/contianers目录下的容器日志输出。\n简述Kubernetes如何进行优雅的节点关机维护？ # 由于Kubernetes节点运行大量Pod，因此在进行关机维护之前，建议先使用kubectl drain将该节点的Pod进行驱逐，然后进行关机维护。\n简述Kubernetes集群联邦？ # Kubernetes集群联邦可以将多个Kubernetes集群作为一个集群进行管理。因此，可以在一个数据中心/云中创建多个Kubernetes集群，并使用集群联邦在一个地方控制/管理所有集群。\n简述Helm及其优势？ # Helm 是 Kubernetes 的软件包管理工具。类似 Ubuntu 中使用的apt、Centos中使用的yum 或者Python中的 pip 一样。\nHelm能够将一组K8S资源打包统一管理, 是查找、共享和使用为Kubernetes构建的软件的最佳方式。\nHelm中通常每个包称为一个Chart，一个Chart是一个目录（一般情况下会将目录进行打包压缩，形成name-version.tgz格式的单一文件，方便传输和存储）。\nHelm优势 # 在 Kubernetes中部署一个可以使用的应用，需要涉及到很多的 Kubernetes 资源的共同协作。使用helm则具有如下优势：\n统一管理、配置和更新这些分散的 k8s 的应用资源文件； 分发和复用一套应用模板； 将应用的一系列资源当做一个软件包管理。 对于应用发布者而言，可以通过 Helm 打包应用、管理应用依赖关系、管理应用版本并发布应用到软件仓库。 对于使用者而言，使用 Helm 后不用需要编写复杂的应用部署文件，可以以简单的方式在 Kubernetes 上查找、安装、升级、回滚、卸载应用程序。 "},{"id":122,"href":"/docs/shortcodes/katex/","title":"KaTeX","section":"Shortcodes","content":" KaTeX # KaTeX shortcode let you render math typesetting in markdown document. See KaTeX\nExample # {{\u0026lt; katex display=true \u0026gt;}} f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)\\,e^{2 \\pi i \\xi x}\\,d\\xi {{\u0026lt; /katex \u0026gt;}} Display Mode Example # Here is some inline example: \\(\\pi(x)\\) , rendered in the same line. And below is display example, having display: block \\[ f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)\\,e^{2 \\pi i \\xi x}\\,d\\xi \\] Text continues here.\n"},{"id":123,"href":"/docs/kruise%E5%8E%9F%E5%9C%B0%E5%8D%87%E7%BA%A7%E8%A7%A3%E6%9E%90-kruise-yuan-de-sheng-ji-jie-xi/","title":"kruise原地升级解析 2024-08-02 17:59:31.689","section":"Docs","content":"从源码解析Kruise原地升级原理 本文从源码的角度分析 Kruise 原地升级相关功能的实现。\n本篇Kruise版本为v1.5.2。\nKruise项目地址: https://github.com/openkruise/kruise\n更多云原生、K8S相关文章请点击【专栏】查看！\n原地升级的概念 当我们使用deployment等Workload， 我们更改镜像版本时，k8s会删除原有pod进行重建，重建后pod的相关属性都有可能会变化， 比如uid、node、ipd等。\n原地升级的目的就是保持pod的相关属性不变，只更改镜像版本。\n下面的测试可以帮助理解kubelet的原地升级功能。\n测试一： 修改deployment镜像版本 比如当前deployment使用nginx作为镜像, 且有一个pod实例：\n~|⇒ kubectl get deployment test -o jsonpath=\u0026#34;{.spec.template.spec.containers[0]}\u0026#34; {\u0026#34;image\u0026#34;:\u0026#34;nginx\u0026#34;,\u0026#34;imagePullPolicy\u0026#34;:\u0026#34;Always\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;nginx\u0026#34;,\u0026#34;resources\u0026#34;:{},\u0026#34;terminationMessagePath\u0026#34;:\u0026#34;/dev/termination-log\u0026#34;,\u0026#34;terminationMessagePolicy\u0026#34;:\u0026#34;File\u0026#34;} ~|⇒ kubectl get pod NAME READY STATUS RESTARTS AGE test-5746d4c59f-nwc6q 1/1 Running 0 10m web-0 1/1 Running 1 (71m ago) 18d 修改镜像版本后， pod会被重建：\n~|⇒ kubectl edit deployment test deployment.apps/test edited ~|⇒ kubectl get pod NAME READY STATUS RESTARTS AGE test-5746d4c59f-nwc6q 1/1 Running 0 11m test-674d57777c-8qc7c 0/1 ContainerCreating 0 2s web-0 1/1 Running 1 (72m ago) 18d ~|⇒ kubectl get pod NAME READY STATUS RESTARTS AGE test-674d57777c-8qc7c 1/1 Running 0 42s 可以看到，pod被重建后，pod的名称（以及其他属性）发生了变化。\n测试二： 修改pod的镜像版本 # 比如当前deployment使用nginx:1.25作为镜像, 且有一个pod实例：\n~|⇒ kubectl get deployment test -o jsonpath=\u0026#34;{.spec.template.spec.containers[0]}\u0026#34; {\u0026#34;image\u0026#34;:\u0026#34;nginx:1.25\u0026#34;,\u0026#34;imagePullPolicy\u0026#34;:\u0026#34;Always\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;nginx\u0026#34;,\u0026#34;resources\u0026#34;:{},\u0026#34;terminationMessagePath\u0026#34;:\u0026#34;/dev/termination-log\u0026#34;,\u0026#34;terminationMessagePolicy\u0026#34;:\u0026#34;File\u0026#34;}% ~|⇒ kubectl get pod NAME READY STATUS RESTARTS AGE test-76f8989b6c-8s9s2 1/1 Running 0 3m17s 直接修改pod的镜像版本后， pod不会被重建(但是会增加一次restart)：\n~|⇒ kubectl edit pod test-76f8989b6c-8s9s2 pod/test-76f8989b6c-8s9s2 edited ~|⇒ kubectl get pod NAME READY STATUS RESTARTS AGE test-76f8989b6c-8s9s2 1/1 Running 1 (4s ago) 5m38s pod的镜像版本变动后，并不会逆向同步到deployment。\n~|⇒ kubectl get deployment test -o jsonpath=\u0026#34;{.spec.template.spec.containers[0]}\u0026#34; {\u0026#34;image\u0026#34;:\u0026#34;nginx:1.25\u0026#34;,\u0026#34;imagePullPolicy\u0026#34;:\u0026#34;Always\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;nginx\u0026#34;,\u0026#34;resources\u0026#34;:{},\u0026#34;terminationMessagePath\u0026#34;:\u0026#34;/dev/termination-log\u0026#34;,\u0026#34;terminationMessagePolicy\u0026#34;:\u0026#34;File\u0026#34;}% 但是pod的镜像版本变化了， uid、名称的属性都没有变化。\n-- old apiVersion: v1 kind: Pod metadata: creationTimestamp: \u0026#34;2024-02-20T03:53:34Z\u0026#34; generateName: test-76f8989b6c- labels: app: test pod-template-hash: 76f8989b6c name: test-76f8989b6c-8s9s2 namespace: default ownerReferences: - apiVersion: apps/v1 blockOwnerDeletion: true controller: true kind: ReplicaSet name: test-76f8989b6c uid: 68434490-0948-4c88-bf59-e1f63887e02f resourceVersion: \u0026#34;2160531\u0026#34; uid: 9f5fb37b-01ae-45a6-b50f-fc2385b6e317 spec: containers: - image: nginx:1.25 imagePullPolicy: Always name: nginx --- new apiVersion: v1 kind: Pod metadata: creationTimestamp: \u0026#34;2024-02-20T03:53:34Z\u0026#34; generateName: test-76f8989b6c- labels: app: test pod-template-hash: 76f8989b6c name: test-76f8989b6c-8s9s2 namespace: default ownerReferences: - apiVersion: apps/v1 blockOwnerDeletion: true controller: true kind: ReplicaSet name: test-76f8989b6c uid: 68434490-0948-4c88-bf59-e1f63887e02f resourceVersion: \u0026#34;2161008\u0026#34; uid: 9f5fb37b-01ae-45a6-b50f-fc2385b6e317 spec: containers: - image: nginx:1.25.4 imagePullPolicy: Always name: nginx 测试三： 停止pod内容器 # 依旧是“测试二”中的pod：\n~|⇒ kubectl get pod NAME READY STATUS RESTARTS AGE test-76f8989b6c-8s9s2 1/1 Running 1 (112m ago) 118m 找到其对应的容器， 对其进行停止操作：\n# 正在运行无法直接删除， 可以强制删除或者先停止 $ docker rm 518f1b0accada9c9587cd5d7655cbda0bc7a33bebaf11f0ec99877b6a9c92222 Error response from daemon: You cannot remove a running container 518f1b0accada9c9587cd5d7655cbda0bc7a33bebaf11f0ec99877b6a9c92222. Stop the container before attempting removal or force remove $ docker stop 518f1b0accada9c9587cd5d7655cbda0bc7a33bebaf11f0ec99877b6a9c92222 518f1b0accada9c9587cd5d7655cbda0bc7a33bebaf11f0ec99877b6a9c92222 # 已经停止 $ docker ps | grep 518f1b0accada9c9587cd5d7655cbda0bc7a33bebaf11f0ec99877b6a9c92222 # 拉起了新的容器 $ docker ps | grep nginx ebb42aafa572 nginx \u0026#34;/docker-entrypoint.…\u0026#34; 3 minutes ago Up 3 minutes k8s_nginx_test-76f8989b6c-8s9s2_default_9f5fb37b-01ae-45a6-b50f-fc2385b6e317_2 容器停止后， 会被kubelet中的Runonce方法拉起， pod的属性不会变化, 状态中的containerID会更新。\n结论 pod本身其实具备原地升级的能力，所以简单来说(一个pod多个容器仅其中一个升级的状况会更复杂)， 对deployment实现原地升级只需要几步就可以做到：\n修改workload镜像版本，但是需要拦截pod重建动作 提前拉取新版镜像， 加快过程 更新pod镜像版本，重新启动容器 kruise原地升级原理 Container Restart\nContainerRecreateRequest是一个CRD,可以帮助用户重启/重建存量 Pod 中一个或多个容器。下文称之为CRR\n和 Kruise 提供的原地升级类似，当一个容器重建的时候，Pod 中的其他容器还保持正常运行。重建完成后，Pod 中除了该容器的 restartCount 增加以外不会有什么其他变化。 注意，之前临时写到旧容器 rootfs 中的文件会丢失，但是 volume mount 挂载卷中的数据都还存在。\nCRR的具体管理者是kruise-daemon进程。\nkruise-daemon 除此之外还会管理NnodeImageCRD\nCRR`资源管理的实现在`pkg/daemon/containerrecreate 资源的处理最终会由Controller.sync方法执行\nfunc (c *Controller) sync(key string) (retErr error) { namespace, podName, err := cache.SplitMetaNamespaceKey(key) objectList, err := c.crrInformer.GetIndexer().ByIndex(CRRPodNameIndex, podName) crrList := make([]*appsv1alpha1.ContainerRecreateRequest, 0, len(objectList)) // 弹出一个CRR进行处理 crr, err := c.pickRecreateRequest(crrList) if err != nil || crr == nil { return err } // ... // 省略一些状态判断 return c.manage(crr) } func (c *Controller) manage(crr *appsv1alpha1.ContainerRecreateRequest) error { runtimeManager, err := c.newRuntimeManager(c.runtimeFactory, crr) pod := convertCRRToPod(crr) podStatus, err := runtimeManager.GetPodStatus(pod.UID, pod.Name, pod.Namespace) newCRRContainerRecreateStates := getCurrentCRRContainersRecreateStates(crr, podStatus) if !reflect.DeepEqual(crr.Status.ContainerRecreateStates, newCRRContainerRecreateStates) { return c.patchCRRContainerRecreateStates(crr, newCRRContainerRecreateStates) } var completedCount int for i := range newCRRContainerRecreateStates { state := \u0026amp;newCRRContainerRecreateStates[i] // ... // 省略一些状态判断 // 从pod状态中获取容器id，调用cri停止对应容器 err := runtimeManager.KillContainer(pod, kubeContainerStatus.ID, state.Name, msg, nil) if err != nil { if crr.Spec.Strategy.FailurePolicy == appsv1alpha1.ContainerRecreateRequestFailurePolicyIgnore { continue } return c.patchCRRContainerRecreateStates(crr, newCRRContainerRecreateStates) } state.IsKilled = true state.Phase = appsv1alpha1.ContainerRecreateRequestRecreating break } // 更新CCR状态 if !reflect.DeepEqual(crr.Status.ContainerRecreateStates, newCRRContainerRecreateStates) { return c.patchCRRContainerRecreateStates(crr, newCRRContainerRecreateStates) } if completedCount == len(newCRRContainerRecreateStates) { return c.completeCRRStatus(crr, \u0026#34;\u0026#34;) } if crr.Spec.Strategy != nil \u0026amp;\u0026amp; crr.Spec.Strategy.MinStartedSeconds \u0026gt; 0 { c.queue.AddAfter(objectKey(crr), time.Duration(crr.Spec.Strategy.MinStartedSeconds)*time.Second) } return nil } 可以看到整体逻辑比较简单， 主要是越过上层workload资源，直接停止对应的容器，利用k8s kubelet本身的container状态监控机制再次拉起， 完成原地重启。\n总的来说所，他与我们手动去删除容器的操作大体相同， 不过帮我们省略其中查找容器、登陆node的重复操作， 并提供了一些状态控制机制。\napiVersion: apps.kruise.io/v1alpha1 kind: ContainerRecreateRequest metadata: namespace: pod-namespace name: xxx spec: podName: pod-name containers: # 要重建的容器名字列表，至少要有 1 个 - name: app - name: sidecar strategy: failurePolicy: Fail # \u0026#39;Fail\u0026#39; 或 \u0026#39;Ignore\u0026#39;，表示一旦有某个容器停止或重建失败， CRR 立即结束 orderedRecreate: false # \u0026#39;true\u0026#39; 表示要等前一个容器重建完成了，再开始重建下一个 terminationGracePeriodSeconds: 30 # 等待容器优雅退出的时间，不填默认用 Pod 中定义的 unreadyGracePeriodSeconds: 3 # 在重建之前先把 Pod 设为 not ready，并等待这段时间后再开始执行重建 minStartedSeconds: 10 # 重建后新容器至少保持运行这段时间，才认为该容器重建成功 activeDeadlineSeconds: 300 # 如果 CRR 执行超过这个时间，则直接标记为结束（未结束的容器标记为失败） ttlSecondsAfterFinished: 1800 # CRR 结束后，过了这段时间自动被删除掉 cloneSet原地升级 # 原地升级与上面的CRR的原理基本相同， 不过多了一步修改信息的操作（如image、annotation）.\nkruise中支持原地升级的workload类型， 基本上用的是同一套代码逻辑， 我们以cloneSet为例进行分析。\n代码路径: pkg/controller/cloneset\n本文中不会对代码实现全部展开分析， 会更加偏向于整体流程的理解。\ncontroller # kruise controller中通过Reconciler来实现workload状态同步，interface定义如下：\ntype Reconciler interface { Reconcile(context.Context, Request) (Result, error) } workload会实现这个interface，并在其中实现状态同步的逻辑。这里面就包含原地升级。\n我们忽略cloneSet控制器中其他的逻辑， 只关注原地升级， 最终定位到sync/cloneset_update.go/realControl.updatePod这个方法。\nfunc (c *realControl) updatePod(cs *appsv1alpha1.CloneSet, coreControl clonesetcore.Control, updateRevision *apps.ControllerRevision, revisions []*apps.ControllerRevision, pod *v1.Pod, pvcs []*v1.PersistentVolumeClaim, ) (time.Duration, error) { if cs.Spec.UpdateStrategy.Type == appsv1alpha1.InPlaceIfPossibleCloneSetUpdateStrategyType || // ... // 省略一些状态判断 // 判断是否可以原地升级 if c.inplaceControl.CanUpdateInPlace(oldRevision, updateRevision, coreControl.GetUpdateOptions()) { // ... // 省略一些状态判断 // 原地升级 opts := coreControl.GetUpdateOptions() opts.AdditionalFuncs = append(opts.AdditionalFuncs, lifecycle.SetPodLifecycle(appspub.LifecycleStateUpdating)) // 执行升级动作 res := c.inplaceControl.Update(pod, oldRevision, updateRevision, opts) if res.InPlaceUpdate { if res.UpdateErr == nil { clonesetutils.ResourceVersionExpectations.Expect(\u0026amp;metav1.ObjectMeta{UID: pod.UID, ResourceVersion: res.NewResourceVersion}) return res.DelayDuration, nil } return res.DelayDuration, res.UpdateErr } } if cs.Spec.UpdateStrategy.Type == appsv1alpha1.InPlaceOnlyCloneSetUpdateStrategyType { return 0, fmt.Errorf(\u0026#34;find Pod %s update strategy is InPlaceOnly but can not update in-place\u0026#34;, pod.Name) } } // 省略状态更新 // ... return 0, nil } 可以看到， 关键的处理逻辑在c.inplaceControl这个对象中。这个对象是inplaceupdate.Interface类型。\ninplaceupdate # 查看文件pkg/util/inplaceupdate/inplace_update.go\ntype Interface interface { // 判断是否可以原地升级 CanUpdateInPlace(oldRevision, newRevision *apps.ControllerRevision, opts *UpdateOptions) bool // 执行原地升级 Update(pod *v1.Pod, oldRevision, newRevision *apps.ControllerRevision, opts *UpdateOptions) UpdateResult // 刷新一些状态信息 Refresh(pod *v1.Pod, opts *UpdateOptions) RefreshResult } UpdateOptions包含了一些重要的函数， 比如需要计算更新的字段、更新字段等。\ntype UpdateOptions struct { GracePeriodSeconds int32 AdditionalFuncs []func(*v1.Pod) // 计算更新的字段, 也用于判断是否可以原地升级 CalculateSpec func(oldRevision, newRevision *apps.ControllerRevision, opts *UpdateOptions) *UpdateSpec // 更新字段 PatchSpecToPod func(pod *v1.Pod, spec *UpdateSpec, state *appspub.InPlaceUpdateState) (*v1.Pod, error) // 检查更新状态 CheckPodUpdateCompleted func(pod *v1.Pod) error // 检查容器更新状态 CheckContainersUpdateCompleted func(pod *v1.Pod, state *appspub.InPlaceUpdateState) error GetRevision func(rev *apps.ControllerRevision) string } // 默认CalculateSpec函数, 这里体现出只支持label、annotation、镜像的更新的原地升级 func defaultCalculateInPlaceUpdateSpec(oldRevision, newRevision *apps.ControllerRevision, opts *UpdateOptions) *UpdateSpec { // ... for _, op := range patches { // 计算更新镜像 op.Path = strings.Replace(op.Path, \u0026#34;/spec/template\u0026#34;, \u0026#34;\u0026#34;, 1) if !strings.HasPrefix(op.Path, \u0026#34;/spec/\u0026#34;) { if strings.HasPrefix(op.Path, \u0026#34;/metadata/\u0026#34;) { metadataPatches = append(metadataPatches, op) continue } return nil } if op.Operation != \u0026#34;replace\u0026#34; || !containerImagePatchRexp.MatchString(op.Path) { return nil } // for example: /spec/containers/0/image words := strings.Split(op.Path, \u0026#34;/\u0026#34;) idx, _ := strconv.Atoi(words[3]) if len(oldTemp.Spec.Containers) \u0026lt;= idx { return nil } updateSpec.ContainerImages[oldTemp.Spec.Containers[idx].Name] = op.Value.(string) } if len(metadataPatches) \u0026gt; 0 { // 计算lbels、annotations的更新 if utilfeature.DefaultFeatureGate.Enabled(features.InPlaceUpdateEnvFromMetadata) { for _, op := range metadataPatches { //... for i := range newTemp.Spec.Containers { c := \u0026amp;newTemp.Spec.Containers[i] objMeta := updateSpec.ContainerRefMetadata[c.Name] switch words[2] { case \u0026#34;labels\u0026#34;: // ... case \u0026#34;annotations\u0026#34;: // ... } updateSpec.ContainerRefMetadata[c.Name] = objMeta updateSpec.UpdateEnvFromMetadata = true } } } // ... updateSpec.MetaDataPatch = patchBytes } return updateSpec } // 默认CheckContainersUpdateCompleted函数， 实际CheckPodUpdateCompleted也是调用的这个 func defaultCheckContainersInPlaceUpdateCompleted(pod *v1.Pod, inPlaceUpdateState *appspub.InPlaceUpdateState) error { // ... containerImages := make(map[string]string, len(pod.Spec.Containers)) for i := range pod.Spec.Containers { c := \u0026amp;pod.Spec.Containers[i] containerImages[c.Name] = c.Image if len(strings.Split(c.Image, \u0026#34;:\u0026#34;)) \u0026lt;= 1 { containerImages[c.Name] = fmt.Sprintf(\u0026#34;%s:latest\u0026#34;, c.Image) } } for _, cs := range pod.Status.ContainerStatuses { if oldStatus, ok := inPlaceUpdateState.LastContainerStatuses[cs.Name]; ok { // 通过判断镜像id是否变化来判断是否更新 if oldStatus.ImageID == cs.ImageID { if containerImages[cs.Name] != cs.Image { return fmt.Errorf(\u0026#34;container %s imageID not changed\u0026#34;, cs.Name) } } delete(inPlaceUpdateState.LastContainerStatuses, cs.Name) } } // ... return nil } realControl实现了inplaceupdate.Interface。\nfunc (c *realControl) CanUpdateInPlace(oldRevision, newRevision *apps.ControllerRevision, opts *UpdateOptions) bool { opts = SetOptionsDefaults(opts) // 判断是否可以原地升级, 通过计算更新的字段来判断 return opts.CalculateSpec(oldRevision, newRevision, opts) != nil } func (c *realControl) Update(pod *v1.Pod, oldRevision, newRevision *apps.ControllerRevision, opts *UpdateOptions) UpdateResult { opts = SetOptionsDefaults(opts) // 1. 计算更新字段 spec := opts.CalculateSpec(oldRevision, newRevision, opts) // 2. 更新状态 if containsReadinessGate(pod) { newCondition := v1.PodCondition{ Type: appspub.InPlaceUpdateReady, LastTransitionTime: metav1.NewTime(Clock.Now()), Status: v1.ConditionFalse, Reason: \u0026#34;StartInPlaceUpdate\u0026#34;, } if err := c.updateCondition(pod, newCondition); err != nil { return UpdateResult{InPlaceUpdate: true, UpdateErr: err} } } // 3.更新镜像信息 newResourceVersion, err := c.updatePodInPlace(pod, spec, opts) // ... return UpdateResult{InPlaceUpdate: true, DelayDuration: delayDuration, NewResourceVersion: newResourceVersion} } // 3.更新镜像信息 // newResourceVersion, err := c.updatePodInPlace(pod, spec, opts) func (c *realControl) updatePodInPlace(pod *v1.Pod, spec *UpdateSpec, opts *UpdateOptions) (string, error) { var newResourceVersion string retryErr := retry.RetryOnConflict(retry.DefaultBackoff, func() error { // 1. 准备：获取pod clone, err := c.podAdapter.GetPod(pod.Namespace, pod.Name) // 2. 准备：设置Annotations， 记录相关信息 inPlaceUpdateState := appspub.InPlaceUpdateState{ Revision: spec.Revision, UpdateTimestamp: metav1.NewTime(Clock.Now()), UpdateEnvFromMetadata: spec.UpdateEnvFromMetadata, } inPlaceUpdateStateJSON, _ := json.Marshal(inPlaceUpdateState) clone.Annotations[appspub.InPlaceUpdateStateKey] = string(inPlaceUpdateStateJSON) delete(clone.Annotations, appspub.InPlaceUpdateStateKeyOld) // 3. 更新pod if spec.GraceSeconds \u0026lt;= 0 { // GraceSeconds \u0026lt;= 0时会立即更新pod状态为notready if clone, err = opts.PatchSpecToPod(clone, spec, \u0026amp;inPlaceUpdateState); err != nil { return err } appspub.RemoveInPlaceUpdateGrace(clone) } else { inPlaceUpdateSpecJSON, _ := json.Marshal(spec) clone.Annotations[appspub.InPlaceUpdateGraceKey] = string(inPlaceUpdateSpecJSON) } // 执行更新，这时会调用k8s API将数据更新到server， 后续的容器重建工作由kubelet完成 newPod, updateErr := c.podAdapter.UpdatePod(clone) if updateErr == nil { newResourceVersion = newPod.ResourceVersion } return updateErr }) return newResourceVersion, retryErr } 总结 原地升级的原理比较简单， 主要还是利用了pod自身的特性和kubelet的拉起功能。\nkruise中仅对自己的CRD Workload支持原地升级， 其实也可以扩展到对原生资源的支持（如一开始的测试），但会存在一些问题和限制（如测试二中deployment的镜像版本不会发生改变）。\n"},{"id":124,"href":"/docs/kube-router-%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6-kube-router-wang-luo-zu-jian/","title":"kube-router 网络组件 2023-09-28 15:33:20.658","section":"Docs","content":" kube-router 网络组件 # (以下文档暂未更新，以插件官网文档为准) # kube-router是一个简单、高效的网络插件，它提供一揽子解决方案： # 基于GoBGP 提供Pod 网络互联（Routing） # 使用ipsets优化的iptables 提供网络策略支持（Firewall/NetworkPolicy） # 基于IPVS/LVS 提供高性能服务代理（Service Proxy）(注：由于 k8s 新版本中 ipvs 已可用，因此这里不选择启用kube-router基于ipvs的service proxy) # 更多介绍请前往https://github.com/cloudnativelabs/kube-router # 配置 # 本项目提供多种网络插件可选，如果需要安装kube-router，请在/etc/kubeasz/hosts文件中设置变量 CLUSTER_NETWORK=\u0026quot;kube-router\u0026quot;，更多设置请查看roles/kube-router/defaults/main.yml # kube-router需要在所有master节点和node节点安装 # 安装 # 单步安装已经集成：ansible-playbook 90.setup.yml # 分步安装请执行：ansible-playbook 06.network.yml # 验证 # 1.pod间网络联通性：略\n2.host路由表\n# master上路由 root@master1:~$ ip route ... 172.20.1.0/24 via 192.168.1.2 dev ens3 proto 17 172.20.2.0/24 via 192.168.1.3 dev ens3 proto 17 ... # node3上路由 root@node3:~$ ip route ... 172.20.0.0/24 via 192.168.1.1 dev ens3 proto 17 172.20.1.0/24 via 192.168.1.2 dev ens3 proto 17 172.20.2.0/24 dev kube-bridge proto kernel scope link src 172.20.2.1 ... 3.bgp连接状态 # master上 root@master1:~$ netstat -antlp|grep router|grep LISH|grep 179 tcp 0 0 192.168.1.1:179 192.168.1.3:58366 ESTABLISHED 26062/kube-router tcp 0 0 192.168.1.1:42537 192.168.1.2:179 ESTABLISHED 26062/kube-router # node3上 root@node3:~$ netstat -antlp|grep router|grep LISH|grep 179 tcp 0 0 192.168.1.3:58366 192.168.1.1:179 ESTABLISHED 18897/kube-router tcp 0 0 192.168.1.3:179 192.168.1.2:43928 ESTABLISHED 18897/kube-router 4.NetworkPolicy有效性，验证参照这里\n5.ipset列表查看\n$ ipset list ... Name: kube-router-pod-subnets Type: hash:net Revision: 6 Header: family inet hashsize 1024 maxelem 65536 timeout 0 Size in memory: 672 References: 2 Members: 172.20.1.0/24 timeout 0 172.20.2.0/24 timeout 0 172.20.0.0/24 timeout 0 Name: kube-router-node-ips Type: hash:ip Revision: 4 Header: family inet hashsize 1024 maxelem 65536 timeout 0 Size in memory: 416 References: 1 Members: 192.168.1.1 timeout 0 192.168.1.2 timeout 0 192.168.1.3 timeout 0 ... "},{"id":125,"href":"/docs/2024-12-05-kubeasz%E9%83%A8%E7%BD%B2k8s/","title":"kubeasz部署k8s","section":"Docs","content":" 00-集群规划和基础参数设定 # HA architecture # 注意1：确保各节点时区设置一致、时间同步。 如果你的环境没有提供NTP 时间同步，推荐集成安装chrony 注意2：确保在干净的系统上开始安装，不要使用曾经装过kubeadm或其他k8s发行版的环境 注意3：建议操作系统升级到新的稳定内核，请结合阅读内核升级文档 注意4：在公有云上创建多主集群，请结合阅读在公有云上部署 kubeasz 高可用集群所需节点配置如下 # 角色 数量 描述 部署节点 1 运行ansible/ezctl命令，一般复用第一个master节点 etcd节点 3 注意etcd集群需要1,3,5,\u0026hellip;奇数个节点，一般复用master节点 master节点 2 高可用集群至少2个master节点 node节点 n 运行应用负载的节点，可根据需要提升机器配置/增加节点数 机器配置：\nmaster节点：4c/8g内存/50g硬盘 worker节点：建议8c/32g内存/200g硬盘以上 注意：默认配置下容器运行时和kubelet会占用/var的磁盘空间，如果磁盘分区特殊，可以设置config.yml中的容器运行时和kubelet数据目录：CONTAINERD_STORAGE_DIR DOCKER_STORAGE_DIR KUBELET_ROOT_DIR\n在 kubeasz 2x 版本，多节点高可用集群安装可以使用2种方式\n1.按照本文步骤先规划准备，预先配置节点信息后，直接安装多节点高可用集群 2.先部署单节点集群 AllinOne部署，然后通过 节点添加 扩容成高可用集群 部署步骤 # 以下示例创建一个4节点的多主高可用集群，文档中命令默认都需要root权限运行。\n1.基础系统配置 # 2c/4g内存/40g硬盘（该配置仅测试用） 最小化安装Ubuntu 16.04 server或者CentOS 7 Minimal 配置基础网络、更新源、SSH登录等 2.在每个节点安装依赖工具 # 推荐使用ansible in docker 容器化方式运行，无需安装额外依赖。\n3.准备ssh免密登陆 # 配置从部署节点能够ssh免密登陆所有节点，并且设置python软连接\n本地客户端生成公私钥：（一路回车默认即可） ssh-keygen ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.0.51 ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.0.52 ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.0.53 #$IP为所有节点地址包括自身，按照提示输入yes 和root密码 ssh-copy-id $IP ssh-copy-id 192.168.0.51 ssh-copy-id 192.168.0.52 ssh-copy-id 192.168.0.53 4.在部署节点编排k8s安装 # 4.1 下载项目源码、二进制及离线镜像 下载工具脚本ezdown，举例使用kubeasz版本3.5.0\nexport release=3.5.0 wget https://github.com/easzlab/kubeasz/releases/download/${release}/ezdown chmod +x ./ezdown export release=3.6.1 wget https://github.com/easzlab/kubeasz/releases/download/${release}/ezdown chmod +x ./ezdown 下载kubeasz代码、二进制、默认容器镜像（更多关于ezdown的参数，运行./ezdown 查看）\n# 国内环境 ./ezdown -D # 海外环境 #./ezdown -D -m standard 【可选】下载额外容器镜像（cilium,flannel,prometheus等）\n# 按需下载 ./ezdown -X flannel ./ezdown -X prometheus ... 【可选】下载离线系统包 (适用于无法使用yum/apt仓库情形)\n./ezdown -P 上述脚本运行成功后，所有文件（kubeasz代码、二进制、离线镜像）均已整理好放入目录/etc/kubeasz\n4.2 创建集群配置实例\ncentos7-2009-1 centos7-2009-2 centos7-2009-3 # 容器化运行kubeasz ./ezdown -S # 创建新集群 k8s-01 docker exec -it kubeasz ezctl new k8s-01 2021-01-19 10:48:23 DEBUG generate custom cluster files in /etc/kubeasz/clusters/k8s-01 2021-01-19 10:48:23 DEBUG set version of common plugins 2021-01-19 10:48:23 DEBUG cluster k8s-01: files successfully created. 2021-01-19 10:48:23 INFO next steps 1: to config \u0026#39;/etc/kubeasz/clusters/k8s-01/hosts\u0026#39; 2021-01-19 10:48:23 INFO next steps 2: to config \u0026#39;/etc/kubeasz/clusters/k8s-01/config.yml\u0026#39; 然后根据提示配置\u0026rsquo;/etc/kubeasz/clusters/k8s-01/hosts\u0026rsquo; 和 \u0026lsquo;/etc/kubeasz/clusters/k8s-01/config.yml\u0026rsquo;：根据前面节点规划修改hosts 文件和其他集群层面的主要配置选项；其他集群组件等配置项可以在config.yml 文件中修改。\n4.3 开始安装 如果你对集群安装流程不熟悉，请阅读项目首页 安装步骤 讲解后分步安装，并对 每步都进行验证 #建议使用alias命令，查看~/.bashrc 文件应该包含：alias dk=\u0026#39;docker exec -it kubeasz\u0026#39; source ~/.bashrc # 一键安装，等价于执行docker exec -it kubeasz ezctl setup k8s-01 all dk ezctl setup k8s-01 all # 或者分步安装，具体使用 dk ezctl help setup 查看分步安装帮助信息 # dk ezctl setup k8s-01 01 # dk ezctl setup k8s-01 02 # dk ezctl setup k8s-01 03 # dk ezctl setup k8s-01 04 ... dk ezctl setup k8s-01 07 [root@centos7-2009-1 ~]# systemctl stop etcd [root@centos7-2009-1 ~]# systemctl stop kube-apiserver [root@centos7-2009-1 ~]# systemctl stop kube-scheduler [root@centos7-2009-1 ~]# systemctl stop kube-controller-manager [root@centos7-2009-1 ~]# systemctl stop kubelet fatal: [192.168.0.52]: FAILED! =\u0026gt; {\u0026#34;changed\u0026#34;: true, \u0026#34;cmd\u0026#34;: \u0026#34;/etc/kubeasz/bin/kubectl config set-credentials system:node:worker-02 --client-certificate=/etc/kubeasz/clusters/k8s-01/ssl/worker-02-kubelet.pem --embed-certs=true --client-key=/etc/kubeasz/clusters/k8s-01/ssl/worker-02-kubelet-key.pem --kubeconfig=/etc/kubeasz/clusters/k8s-01/worker-02-kubelet.kubeconfig\u0026#34;, \u0026#34;delta\u0026#34;: \u0026#34;0:00:00.046326\u0026#34;, \u0026#34;end\u0026#34;: \u0026#34;2023-08-09 14:37:27.535734\u0026#34;, \u0026#34;msg\u0026#34;: \u0026#34;non-zero return code\u0026#34;, \u0026#34;rc\u0026#34;: 1, \u0026#34;start\u0026#34;: \u0026#34;2023-08-09 14:37:27.489408\u0026#34;, \u0026#34;stderr\u0026#34;: \u0026#34;error: open /etc/kubeasz/clusters/k8s-01/worker-02-kubelet.kubeconfig.lock: file exists\u0026#34;, \u0026#34;stderr_lines\u0026#34;: [\u0026#34;error: open /etc/kubeasz/clusters/k8s-01/worker-02-kubelet.kubeconfig.lock: file exists\u0026#34;], \u0026#34;stdout\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;stdout_lines\u0026#34;: []} 更多ezctl使用帮助，请参考这里\n后一篇\n"},{"id":126,"href":"/docs/kubekey%E6%B7%BB%E5%8A%A0%E6%96%B0%E8%8A%82%E7%82%B9-kubekey-tian-jia-xin-jie-dian/","title":"kubekey添加新节点 2024-04-03 15:36:47.646","section":"Docs","content":" 添加新节点 # KubeSphere 使用一段时间之后，由于工作负载不断增加，您可能需要水平扩展集群。自 KubeSphere v3.0.0 起，您可以使用全新的安装程序 KubeKey 将新节点添加到集群。从根本上说，该操作是基于 Kubelet 的注册机制。换言之，新节点将自动加入现有的 Kubernetes 集群。KubeSphere 支持混合环境，这意味着新添加的主机操作系统可以是 CentOS 或者 Ubuntu。\n本教程演示了如何将新节点添加到单节点集群。若要水平扩展多节点集群，操作步骤基本相同。\n准备工作 # 您需要一个单节点集群。有关更多信息，请参见在 Linux 上以 All-in-One 模式安装 KubeSphere。 您需要已经下载了 KubeKey。 先执行以下命令以确保您从正确的区域下载 KubeKey。\nexport KKZONE=cn 执行以下命令下载 KubeKey：\ncurl -sfL https://get-kk.kubesphere.io | VERSION=v1.2.1 sh - 备注\n下载 KubeKey 后，如果您将其传输至访问 Googleapis 同样受限的新机器，请您在执行以下步骤之前务必再次执行 export KKZONE=cn 命令。\n备注\n执行以上命令会下载最新版 KubeKey (v1.2.1)，您可以修改命令中的版本号下载指定版本。\n添加工作节点 # Ssh分发秘钥\nssh-copy-id -i ~/.ssh/id_rsa -p 22 root@192.168.1.108 使用 KubeKey 检索集群信息。以下命令会创建配置文件 (sample.yaml)。\n./kk create config3 --from-cluster 备注\n如果您的机器上已有配置文件，就可以跳过此步骤。例如，若要将节点添加到由 KubeKey 设置的多节点集群，如果您没有删除该集群，则可能仍拥有该配置文件。\n在配置文件中，将新节点的信息放在 hosts 和 roleGroups 之下。该示例添加了两个新节点（即 node1 和 node2）。这里的 master1 是现有节点。\n··· spec: hosts: - {name: master1, address: 192.168.0.3, internalAddress: 192.168.0.3, user: root, password: Qcloud@123} - {name: node1, address: 192.168.0.4, internalAddress: 192.168.0.4, user: root, password: Qcloud@123} - {name: node2, address: 192.168.0.5, internalAddress: 192.168.0.5, user: root, password: Qcloud@123} roleGroups: etcd: - master1 master: - master1 worker: - node1 - node2 ··· 备注\n有关更多配置文件的信息，请参见编辑配置文件。 添加新节点时，请勿修改现有节点的主机名。 用自己的主机名替换示例中的主机名。 执行以下命令：\n./kk add nodes -f config-sample.yaml 安装完成后，您将能够在 KubeSphere 的控制台上查看新节点及其信息。在集群管理页面，选择左侧菜单节点下的集群节点，或者执行命令 kubectl get node 以检查更改。\n$ kubectl get node NAME STATUS ROLES AGE VERSION master1 Ready master,worker 20d v1.17.9 node1 Ready worker 31h v1.17.9 node2 Ready worker 31h v1.17.9 添加主节点以实现高可用 # 添加主节点的步骤与添加工作节点的步骤大体一致，不过您需要为集群配置负载均衡器。您可以使用任何云负载均衡器或者硬件负载均衡器（例如 F5）。另外，Keepalived 和 HAproxy、或者 Nginx 也是创建高可用集群的替代方案。\n使用 KubeKey 创建配置文件。\n./kk create config --from-cluster 打开文件，可以看到一些字段预先填充了值。将新节点和负载均衡器的信息添加到文件中。以下示例供您参考：\napiVersion: kubekey.kubesphere.io/v1alpha1 kind: Cluster metadata: name: sample spec: hosts: # You should complete the ssh information of the hosts - {name: master1, address: 172.16.0.2, internalAddress: 172.16.0.2, user: root, password: Testing123} - {name: master2, address: 172.16.0.5, internalAddress: 172.16.0.5, user: root, password: Testing123} - {name: master3, address: 172.16.0.6, internalAddress: 172.16.0.6, user: root, password: Testing123} - {name: worker1, address: 172.16.0.3, internalAddress: 172.16.0.3, user: root, password: Testing123} - {name: worker2, address: 172.16.0.4, internalAddress: 172.16.0.4, user: root, password: Testing123} - {name: worker3, address: 172.16.0.7, internalAddress: 172.16.0.7, user: root, password: Testing123} roleGroups: etcd: - master1 - master2 - master3 master: - master1 - master2 - master3 worker: - worker1 - worker2 - worker3 controlPlaneEndpoint: # If loadbalancer is used, \u0026#39;address\u0026#39; should be set to loadbalancer\u0026#39;s ip. domain: lb.kubesphere.local address: 172.16.0.253 port: 6443 kubernetes: version: v1.21.5 imageRepo: kubesphere clusterName: cluster.local proxyMode: ipvs masqueradeAll: false maxPods: 110 nodeCidrMaskSize: 24 network: plugin: calico kubePodsCIDR: 10.233.64.0/18 kubeServiceCIDR: 10.233.0.0/18 registry: privateRegistry: \u0026#34;\u0026#34; 请注意 controlPlaneEndpoint 字段。\ncontrolPlaneEndpoint: # If you use a load balancer, the address should be set to the load balancer\u0026#39;s ip. domain: lb.kubesphere.local address: 172.16.0.253 port: 6443 负载均衡器的域名默认为 lb.kubesphere.local，用于内部访问。您可以按需进行更改。 大多数情况下，您需要为 address 字段提供负载均衡器的私有 IP 地址。然而，不同的云厂商可能为负载均衡器进行不同的配置。例如，如果您在阿里云上配置服务器负载均衡 (SLB)，该平台会为 SLB 分配一个公共 IP 地址，这意味着您需要为 address 字段指定公共 IP 地址。 port 字段指代 api-server 的端口。 保存文件并执行以下命令以应用配置。\n./kk add nodes -f sample.yaml 在Kubernetes（K8s）中，可以通过标签（label）来标识节点（node）的角色。节点可以被标记为master或worker，以便于集群的调度和管理工作。 要给节点打上master或worker标签，可以使用kubectl命令行工具。具体操作如下： 给节点打上master标签： bash kubectl label node \u0026lt;node-name\u0026gt; node-role.kubernetes.io/master=true 其中，\u0026lt;node-name\u0026gt;是要标记的节点的名称。 给节点打上worker标签： bash kubectl label node \u0026lt;node-name\u0026gt; node-role.kubernetes.io/worker=true 同样，\u0026lt;node-name\u0026gt;是要标记的节点的名称。 请注意，在执行这些命令之前，需要确保已经正确安装和配置了kubectl工具，并且具有足够的权限来执行这些操作。此外，还需要确保节点已经加入到Kubernetes集群中。 "},{"id":127,"href":"/docs/kubernetes-api-kubernetesapi/","title":"Kubernetes API 2024-04-03 14:47:30.212","section":"Docs","content":" 简介 # kube-apiserver 组件是 k8s 中非常重要的组件，每个组件都只能和 kube-apiserver 进行通信，kube-apiserver 提供所有的 API。 # 资源与 API # 在 k8s 中，一般都说某某资源，并不说接口。比如 Deployment，Service 等资源，这些资源就是 k8s api 操作的实体，最终这些资源都会存储到 etcd 中，其实最终就是对 etcd 中的这些资源做 CRUD。 # 例子 # 当我们使用 kubectl get deployment查看集群 default 命令空间的 deployment时，其实 kubectl 最终是将命令语言转化为 API 发送给 kube-apiserver，然后将 kube-apiserver 返回的数据再转成特定格式打印出来。 # $ kubectl get deployment --\u0026gt; https://apiserver.cluster.local:6443/apis/apps/v1/namespaces/default/deployments # apiserver.cluster.local:6443 这个是 kube-apiserver 的访问 url # apis: 表示下面有多组 api # apps: 表示 api 组 # v1: 表示 api version # default: 表示命名空间 # deployments: 表示操作的具体资源类型 下面详细看看一个 url 的设计 # API 组 # k8s 将每一个 api 都设置组和版本即 groupVersion。 # 例如：/apis/apps/v1/deployment，apis 表示有多组 api，apps 是 group，v1 是 version # 但是 /api/v1 这一组 API 例外，因为在 k8s 刚开发时，并没有预料到后面会发展这么多的 API，当时并没有设置 group，现在都认为 /api/v1 都是核心组，可以这么理解 /api/core/v1，这样就和目前所有 API 结构对应上了。 # 例如：/api/v1/service，api 表示只有一组 api 及核心组，v1 是 version # API 版本 # 每一个 API 除了有 group，还需要拥有 version 属性，因为每一个 API 都需要经历多次打磨才能稳定，k8s 是这样定义 API version 的。 # • Alpha 级别，例如 v1alpha1 默认情况下是被禁用的，可以随时删除对功能的支持，所以要慎用 # • Beta 级别，例如 v2beta1 默认情况下是启用的，表示代码已经经过了很好的测试，但是对象的语义可能会在随后的版本中以不兼容的方式更改 # • 稳定级别，比如 v1 表示已经是稳定版本了，也会出现在后续的很多版本中。 # 举例 # 可以用 kubectl get - - raw / 命令查看 k8s 集群中有哪些 API # $ kubectl get --raw / { \u0026#34;paths\u0026#34;: [ \u0026#34;/.well-known/openid-configuration\u0026#34;, \u0026#34;/api\u0026#34;, \u0026#34;/api/v1\u0026#34;, \u0026#34;/apis\u0026#34;, \u0026#34;/apis/\u0026#34;, \u0026#34;/apis/admissionregistration.k8s.io\u0026#34;, \u0026#34;/apis/admissionregistration.k8s.io/v1\u0026#34;, \u0026#34;/apis/apiextensions.k8s.io\u0026#34;, \u0026#34;/apis/apiextensions.k8s.io/v1\u0026#34;, \u0026#34;/apis/apiregistration.k8s.io\u0026#34;, \u0026#34;/apis/apiregistration.k8s.io/v1\u0026#34;, \u0026#34;/apis/apps\u0026#34;, \u0026#34;/apis/apps/v1\u0026#34;, \u0026#34;/apis/authentication.k8s.io\u0026#34;, \u0026#34;/apis/authentication.k8s.io/v1\u0026#34;, \u0026#34;/apis/authorization.k8s.io\u0026#34;, \u0026#34;/apis/authorization.k8s.io/v1\u0026#34;, \u0026#34;/apis/autoscaling\u0026#34;, \u0026#34;/apis/autoscaling/v1\u0026#34;, \u0026#34;/apis/autoscaling/v2beta1\u0026#34;, \u0026#34;/apis/autoscaling/v2beta2\u0026#34;, \u0026#34;/apis/batch\u0026#34;, \u0026#34;/apis/batch/v1\u0026#34;, \u0026#34;/apis/batch/v1beta1\u0026#34;, \u0026#34;/apis/builder.moss.iflytek.com\u0026#34;, \u0026#34;/apis/builder.moss.iflytek.com/v1\u0026#34;, \u0026#34;/apis/certificates.k8s.io\u0026#34;, \u0026#34;/apis/certificates.k8s.io/v1\u0026#34;, \u0026#34;/apis/cluster.moss.iflytek.com\u0026#34;, \u0026#34;/apis/cluster.moss.iflytek.com/v1\u0026#34;, \u0026#34;/apis/coordination.k8s.io\u0026#34;, \u0026#34;/apis/coordination.k8s.io/v1\u0026#34;, \u0026#34;/apis/crd.projectcalico.org\u0026#34;, \u0026#34;/apis/crd.projectcalico.org/v1\u0026#34;, \u0026#34;/apis/discovery.k8s.io\u0026#34;, \u0026#34;/apis/discovery.k8s.io/v1\u0026#34;, \u0026#34;/apis/discovery.k8s.io/v1beta1\u0026#34;, \u0026#34;/apis/events.k8s.io\u0026#34;, \u0026#34;/apis/events.k8s.io/v1\u0026#34;, \u0026#34;/apis/events.k8s.io/v1beta1\u0026#34;, \u0026#34;/apis/flowcontrol.apiserver.k8s.io\u0026#34;, \u0026#34;/apis/flowcontrol.apiserver.k8s.io/v1beta1\u0026#34;, \u0026#34;/apis/metrics.k8s.io\u0026#34;, \u0026#34;/apis/metrics.k8s.io/v1beta1\u0026#34;, \u0026#34;/apis/networking.k8s.io\u0026#34;, \u0026#34;/apis/networking.k8s.io/v1\u0026#34;, \u0026#34;/apis/node.k8s.io\u0026#34;, \u0026#34;/apis/node.k8s.io/v1\u0026#34;, \u0026#34;/apis/node.k8s.io/v1beta1\u0026#34;, \u0026#34;/apis/openebs.io\u0026#34;, \u0026#34;/apis/openebs.io/v1alpha1\u0026#34;, \u0026#34;/apis/operator.tigera.io\u0026#34;, \u0026#34;/apis/operator.tigera.io/v1\u0026#34;, \u0026#34;/apis/policy\u0026#34;, \u0026#34;/apis/policy/v1\u0026#34;, \u0026#34;/apis/policy/v1beta1\u0026#34;, \u0026#34;/apis/projectcalico.org\u0026#34;, \u0026#34;/apis/projectcalico.org/v3\u0026#34;, \u0026#34;/apis/rbac.authorization.k8s.io\u0026#34;, \u0026#34;/apis/rbac.authorization.k8s.io/v1\u0026#34;, \u0026#34;/apis/scheduling.k8s.io\u0026#34;, \u0026#34;/apis/scheduling.k8s.io/v1\u0026#34;, \u0026#34;/apis/storage.k8s.io\u0026#34;, \u0026#34;/apis/storage.k8s.io/v1\u0026#34;, \u0026#34;/apis/storage.k8s.io/v1beta1\u0026#34;, \u0026#34;/healthz\u0026#34;, \u0026#34;/healthz/autoregister-completion\u0026#34;, \u0026#34;/healthz/etcd\u0026#34;, \u0026#34;/healthz/log\u0026#34;, \u0026#34;/healthz/ping\u0026#34;, \u0026#34;/healthz/poststarthook/aggregator-reload-proxy-client-cert\u0026#34;, \u0026#34;/healthz/poststarthook/apiservice-openapi-controller\u0026#34;, \u0026#34;/healthz/poststarthook/apiservice-registration-controller\u0026#34;, \u0026#34;/healthz/poststarthook/apiservice-status-available-controller\u0026#34;, \u0026#34;/healthz/poststarthook/bootstrap-controller\u0026#34;, \u0026#34;/healthz/poststarthook/crd-informer-synced\u0026#34;, \u0026#34;/healthz/poststarthook/generic-apiserver-start-informers\u0026#34;, \u0026#34;/healthz/poststarthook/kube-apiserver-autoregistration\u0026#34;, \u0026#34;/healthz/poststarthook/priority-and-fairness-config-consumer\u0026#34;, \u0026#34;/healthz/poststarthook/priority-and-fairness-config-producer\u0026#34;, \u0026#34;/healthz/poststarthook/priority-and-fairness-filter\u0026#34;, \u0026#34;/healthz/poststarthook/rbac/bootstrap-roles\u0026#34;, \u0026#34;/healthz/poststarthook/scheduling/bootstrap-system-priority-classes\u0026#34;, \u0026#34;/healthz/poststarthook/start-apiextensions-controllers\u0026#34;, \u0026#34;/healthz/poststarthook/start-apiextensions-informers\u0026#34;, \u0026#34;/healthz/poststarthook/start-cluster-authentication-info-controller\u0026#34;, \u0026#34;/healthz/poststarthook/start-kube-aggregator-informers\u0026#34;, \u0026#34;/healthz/poststarthook/start-kube-apiserver-admission-initializer\u0026#34;, \u0026#34;/livez\u0026#34;, \u0026#34;/livez/autoregister-completion\u0026#34;, \u0026#34;/livez/etcd\u0026#34;, \u0026#34;/livez/log\u0026#34;, \u0026#34;/livez/ping\u0026#34;, \u0026#34;/livez/poststarthook/aggregator-reload-proxy-client-cert\u0026#34;, \u0026#34;/livez/poststarthook/apiservice-openapi-controller\u0026#34;, \u0026#34;/livez/poststarthook/apiservice-registration-controller\u0026#34;, \u0026#34;/livez/poststarthook/apiservice-status-available-controller\u0026#34;, \u0026#34;/livez/poststarthook/bootstrap-controller\u0026#34;, \u0026#34;/livez/poststarthook/crd-informer-synced\u0026#34;, \u0026#34;/livez/poststarthook/generic-apiserver-start-informers\u0026#34;, \u0026#34;/livez/poststarthook/kube-apiserver-autoregistration\u0026#34;, \u0026#34;/livez/poststarthook/priority-and-fairness-config-consumer\u0026#34;, \u0026#34;/livez/poststarthook/priority-and-fairness-config-producer\u0026#34;, \u0026#34;/livez/poststarthook/priority-and-fairness-filter\u0026#34;, \u0026#34;/livez/poststarthook/rbac/bootstrap-roles\u0026#34;, \u0026#34;/livez/poststarthook/scheduling/bootstrap-system-priority-classes\u0026#34;, \u0026#34;/livez/poststarthook/start-apiextensions-controllers\u0026#34;, \u0026#34;/livez/poststarthook/start-apiextensions-informers\u0026#34;, \u0026#34;/livez/poststarthook/start-cluster-authentication-info-controller\u0026#34;, \u0026#34;/livez/poststarthook/start-kube-aggregator-informers\u0026#34;, \u0026#34;/livez/poststarthook/start-kube-apiserver-admission-initializer\u0026#34;, \u0026#34;/logs\u0026#34;, \u0026#34;/metrics\u0026#34;, \u0026#34;/openapi/v2\u0026#34;, \u0026#34;/openid/v1/jwks\u0026#34;, \u0026#34;/readyz\u0026#34;, \u0026#34;/readyz/autoregister-completion\u0026#34;, \u0026#34;/readyz/etcd\u0026#34;, \u0026#34;/readyz/informer-sync\u0026#34;, \u0026#34;/readyz/log\u0026#34;, \u0026#34;/readyz/ping\u0026#34;, \u0026#34;/readyz/poststarthook/aggregator-reload-proxy-client-cert\u0026#34;, \u0026#34;/readyz/poststarthook/apiservice-openapi-controller\u0026#34;, \u0026#34;/readyz/poststarthook/apiservice-registration-controller\u0026#34;, \u0026#34;/readyz/poststarthook/apiservice-status-available-controller\u0026#34;, \u0026#34;/readyz/poststarthook/bootstrap-controller\u0026#34;, \u0026#34;/readyz/poststarthook/crd-informer-synced\u0026#34;, \u0026#34;/readyz/poststarthook/generic-apiserver-start-informers\u0026#34;, \u0026#34;/readyz/poststarthook/kube-apiserver-autoregistration\u0026#34;, \u0026#34;/readyz/poststarthook/priority-and-fairness-config-consumer\u0026#34;, \u0026#34;/readyz/poststarthook/priority-and-fairness-config-producer\u0026#34;, \u0026#34;/readyz/poststarthook/priority-and-fairness-filter\u0026#34;, \u0026#34;/readyz/poststarthook/rbac/bootstrap-roles\u0026#34;, \u0026#34;/readyz/poststarthook/scheduling/bootstrap-system-priority-classes\u0026#34;, \u0026#34;/readyz/poststarthook/start-apiextensions-controllers\u0026#34;, \u0026#34;/readyz/poststarthook/start-apiextensions-informers\u0026#34;, \u0026#34;/readyz/poststarthook/start-cluster-authentication-info-controller\u0026#34;, \u0026#34;/readyz/poststarthook/start-kube-aggregator-informers\u0026#34;, \u0026#34;/readyz/poststarthook/start-kube-apiserver-admission-initializer\u0026#34;, \u0026#34;/readyz/shutdown\u0026#34;, \u0026#34;/version\u0026#34; ] } 从上图中我们也可以看出 Kubernetes 的 API 对象的组织方式，在顶层，我们可以看到有一个核心组（由于历史原因，开发过程不可能完全预示以后的api这么丰富，当时把所有的资源对象 api 全部放在 /api/v1 下面。是 /api/v1 下的所有内容而不是在 /apis/core/v1 下面）和命名组（路径 /apis/$NAME/$VERSION）和系统范围内的实体，比如 /metrics。所以例如，pod, serivce 等资源的 api 不存在 group # API 示例 # namespaced resources\n所谓的 namespaced resources , 就是这个 resource 是从属于某个 namespace 的, 也就是说它不是 cluster-scoped 的资源. 比如 pod, deployment, service 都属于 namespaced resource. 那么我们看一下如何请求一个 namespaced resources. # http://localhost:8080/api/v1/namespaces/default/pods/test-pod 可以看出, 该 restful api 的组织形式是: # 这里 api version 如果是 v1 的话,表示这是一个很稳定的版本了, 以后不会有大的修改,并且当前版本所支持的所有特性以后都会兼容. 而如果版本号是 v1alpha1, v1beta1 之类的,则不保证其稳定性. # non-namespaced resources\nhttp://localhost:8080/apis/rbac.authorization.k8s.io/v1/clusterroles/test-clusterrole 这里可以观察到它 clusterrole 与 pod 不同, apis 表示这是一个非核心 api，rbac.authorization.k8s.io 指代的是 api-group, 另外它没有 namespaces 字段, 其他与 namespaced resources 类似.不再赘述. # non-resource url\n这类资源和 pod, clusterrole 都不同. 例如 # http://localhost:8080/healthz/etcd 这就是用来确认 etcd 服务是不是健康的.它不属于任何 namespace ,也不属于任何 api 版本. # custom api\n当开发 operator 或者聚合 api 时，都会自定义 API，例如： # http://localhost:8080/apis/custom.io/v1/test custom.io[1] 表示自定义 group # test 为自定义资源 # k8s 的 REST API 的设计结构为: # api/apis / [api-group] / api-version / namespaces / namespace-name / resource-kind / resource-name 示例：\napis / rbac.authorization.k8s.io / v1 / namespaces / default / roles / test-role 总结 # 弄清楚 K8s 的 API 结构，对看源码以及后面开发自定义 API 都很有帮助。 # "},{"id":128,"href":"/docs/kubernetes-%E6%BA%90%E7%A0%81%E7%BB%93%E6%9E%84-kubernetes-yuan-ma-jie-gou/","title":"Kubernetes 源码结构 2024-04-03 14:48:05.954","section":"Docs","content":" 简介 # Kubernetes 源码非常庞大，其中包括自身核心代码，以及对外提供给开发者的 sdk 等。\nKubernetes 的源码结构也是按照上述五个组成部分来组织的，这里以 Kubernetes:v1.22.17 讲解，其主要目录结构如下：\nkubernetes/ ├── api/ ├── build/ ├── CHANGELOG/ ├── cmd/ ├── docs/ ├── hack/ ├── pkg/ ├── plugin/ ├── staging/ ├── test/ └── vendor/ 目录作用 # api # 包含 Kubernetes 的 API 定义文件，如 Pod、Service、ReplicationController 等。但是现在 K8s 的 api 基本都移到 k8s.io/api，和 k8s.io/apis 项目下。 # build # 包含 Kubernetes 内部组件编译的脚本以及制作 Dcoker 镜像的 Dockerfile 等。 # CHANGELOG # 本次版本更新的 Future 以及修复的 Bug 记录 # cmd # 包含 Kubernetes 组件启动命令，如 kube-apiserver，kube-controller-manager 等 # docs # 包含 Kubernetes 的文档，如开发者指南、API 文档等。这些文档是用 MkDocs 工具编写的，可以生成静态网站供用户参考。Kubernetes 的文档非常丰富，包括了从安装到使用到开发的所有内容。对于初学者来说，阅读 Kubernetes 的官方文档是非常必要的。 # hack # 包含 Kubernetes 的构建和测试脚本。这些脚本用于自动化构建、测试和发布 Kubernetes。在这些脚本中，包含了大量的构建细节和测试用例。这些脚本可以大大提高我们的工作效率，同时也可以确保 Kubernetes 的代码质量和稳定性。 # pkg # 包含 Kubernetes 的核心代码，如 API Server、Controller Manager、Scheduler 等。 # plugin # 包含 Kubernetes 的插件，例如存储插件、认证插件等，它们都可以让 Kubernetes 更加灵活和强大。 # test # 包含 Kubernetes 的测试用例。这些测试用例用于测试 Kubernetes 的功能是否正常。在 Kubernetes 的开发过程中，测试是非常重要的环节。通过测试，我们可以发现和解决各种问题，确保 Kubernetes 的功能正确性和稳定性。 # vendor # 用于存放 Kubernetes 所有依赖的第三方库的代码。在编译 Kubernetes 源码时，需要使用大量的第三方库，例如 etcd、docker、glog 等。这些库的源码会被存放在 vendor 目录下，它们会被自动下载和编译，最终被打包到 Kubernetes 的二进制文件中。 # staging # 这个目录比较特殊，单独拿出来说。 # Staging 目录 # 在 kubernetes 源码中，对 kubernetes 项目代码的引用使用的都是 k8s.io[1]： # package proxy import ( \u0026#34;bytes\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;github.com/pkg/errors\u0026#34; apps \u0026#34;k8s.io/api/apps/v1\u0026#34; \u0026#34;k8s.io/api/core/v1\u0026#34; rbac \u0026#34;k8s.io/api/rbac/v1\u0026#34; metav1 \u0026#34;k8s.io/apimachinery/pkg/apis/meta/v1\u0026#34; kuberuntime \u0026#34;k8s.io/apimachinery/pkg/runtime\u0026#34; clientset \u0026#34;k8s.io/client-go/kubernetes\u0026#34; clientsetscheme \u0026#34;k8s.io/client-go/kubernetes/scheme\u0026#34; kubeadmapi \u0026#34;k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm\u0026#34; \u0026#34;k8s.io/kubernetes/cmd/kubeadm/app/componentconfigs\u0026#34; \u0026#34;k8s.io/kubernetes/cmd/kubeadm/app/constants\u0026#34; \u0026#34;k8s.io/kubernetes/cmd/kubeadm/app/images\u0026#34; kubeadmutil \u0026#34;k8s.io/kubernetes/cmd/kubeadm/app/util\u0026#34; \u0026#34;k8s.io/kubernetes/cmd/kubeadm/app/util/apiclient\u0026#34; ) 主项目代码 # 第一种情况是对主项目代码的引用。k8s.io/kubernetes 就是主项目代码的 package name，在 go-modules[2] 使用的 go.mod 文件中定义的： # module k8s.io/kubernetes go 1.16 因此引用主项目代码，需要使用 k8s.io/kubernetes，而不是其他项目中经常看到 github.com/xxxx。 # 单独发布的代码 # 第二种情况是对位于主项目中但是独立发布的代码的引用。 # kubernetes 的一些代码以独立项目的方式发布的，譬如：kubernetes/api[3]、kubernetes/client-go[4] 等，主要是考虑这些包开发者会经常用到，所以将这些包单独拿到 k8s 源码项目之外，方便开发者引用。这些项目的 package name 也用同样的方式在 go.mod 中定义： # module k8s.io/api 或者 module k8s.io/client-go 要注意的是，这些代码虽然以独立项目发布，但是都在 kubernetes 主项目中维护，位于目录 kubernetes/staging/ ，这里面的代码代码被定期同步到各个独立项目中。 # kubernetes/staging/[5] 列出了独立发布的代码： # k8s.io/api k8s.io/apiextensions-apiserver k8s.io/apimachinery k8s.io/apiserver k8s.io/cli-runtime k8s.io/client-go k8s.io/cloud-provider k8s.io/cluster-bootstrap k8s.io/code-generator k8s.io/component-base k8s.io/cri-api k8s.io/csi-api k8s.io/csi-translation-lib k8s.io/kube-aggregator k8s.io/kube-controller-manager k8s.io/kube-proxy k8s.io/kube-scheduler k8s.io/kubectl k8s.io/kubelet k8s.io/legacy-cloud-providers k8s.io/metrics k8s.io/node-api k8s.io/sample-apiserver k8s.io/sample-cli-plugin k8s.io/sample-controller 更需要注意的是，kubernetes 主项目引用这些独立发布的代码时，引用是位于主项目 staging 目录中的代码，而不是独立 repo 中的代码。这是因为主项目的 vendor 目录中设置了软链接。 # 只要单独项目发生了更新，例如：k8s.io/apimachinery ，就会被自动同步到 Kubernetes 源码的 staging/src/k8s.io/apimachinery 之下。 # API 代码 # api 定义代码位于 kubernetes/staging/src/k8s.io/api。 # 需要注意 kubernetes 主项目中还有kubernetes/api，kubernetes/pkg/api，kubernetes/pkg/apis 三个目录。 # kubernetes/api: api 规范定义 kuberntes/pkg/api: 简单的操作函数, api util 方法 kuberntes/pkg/apis: 与 kubernetes/staging/src/k8s.io/api 内容类似，也定义了内置 api，但是这个项目只建议被 Kubernetes 内部引用，如果外部项目引用建议使用 kubernetes/staging/src/k8s.io/api。而且 Kubernetes 内置代码也有很多引用了 kubernetes/staging/src/k8s.io/api 下面的 api，所以后面可能都会迁移至 kubernetes/staging/src/k8s.io/api 项目下。 总结 # 总之，了解 Kubernetes 的源码结构可以帮助我们更好地理解 Kubernetes 的实现原理，从而更好地使用和开发 Kubernetes。阅读 kubernetes 源码，先梳理好源码结构非常重要。 # 引用链接 # [1] k8s.io: http://k8s.io/ [2] go-modules: https://www.lijiaocn.com/prog/go/chapter04/01-dependency.html#go-modules [3] kubernetes/api: https://github.com/kubernetes/api [4] kubernetes/client-go: https://github.com/kubernetes/client-go [5] kubernetes/staging/: https://github.com/kubernetes/kubernetes/tree/v1.16.3/staging\n"},{"id":129,"href":"/docs/kubernetes-%E8%AF%81%E4%B9%A6%E8%AF%A6%E8%A7%A3%E8%AE%A4%E8%AF%81-kubernetes-zheng-shu-xiang-jie--ren-zheng-/","title":"Kubernetes 证书详解(认证) 2024-04-03 14:52:56.54","section":"Docs","content":" K8S 证书介绍 # 在 Kube-apiserver 中提供了很多认证方式，其中最常用的就是 TLS 认证，当然也有 BootstrapToken，BasicAuth 认证等，只要有一个认证通过，那么 Kube-apiserver 即认为认证通过。下面就主要讲解 TLS 认证。 # 如果你是使用 kubeadm[1] 安装的 Kubernetes， 则会自动生成集群所需的证书。但是如果是通过二进制搭建，所有的证书是需要自己生成的，这里我们说说集群必需的证书。 # 在了解 Kubernetes 证书之前，需要先了解什么是 “单向 TLS 认证” 和 “双向 TLS 认证” # • 服务器单向认证：只需要服务器端提供证书，客户端通过服务器端证书验证服务的身份，但服务器并不验证客户端的身份。这种情况一般适用于对 Internet 开放的服务，例如搜索引擎网站，任何客户端都可以连接到服务器上进行访问，但客户端需要验证服务器的身份，以避免连接到伪造的恶意服务器。 # • 双向 TLS 认证：除了客户端需要验证服务器的证书，服务器也要通过客户端证书验证客户端的身份。这种情况下服务器提供的是敏感信息，只允许特定身份的客户端访问。开启服务端验证客户端默认是关闭的，需要在服务端开启认证配置。 # Kubernetes 为了安全性，都是采用双向认证。通常我们使用 Kubeadm 在部署 Kubernetes 时候，Kubeadm 会自动生成集群所需要的证书，下面我们就这些证书一一给大家进行讲解。 # 这是我们用 Kubeadm 搭建完一个集群后在 /etc/kubernetes 目录下所生成的文件 # $ tree kubernetes/ kubernetes/ |-- admin.conf |-- controller-manager.conf |-- kubelet.conf |-- scheduler.conf |-- manifests | |-- etcd.yaml | |-- kube-apiserver.yaml | |-- kube-controller-manager.yaml | `-- kube-scheduler.yaml |-- pki | |-- apiserver.crt | |-- apiserver-etcd-client.crt | |-- apiserver-etcd-client.key | |-- apiserver.key | |-- apiserver-kubelet-client.crt | |-- apiserver-kubelet-client.key | |-- ca.crt | |-- ca.key | |-- etcd | | |-- ca.crt | | |-- ca.key | | |-- healthcheck-client.crt | | |-- healthcheck-client.key | | |-- peer.crt | | |-- peer.key | | |-- server.crt | | `-- server.key | |-- front-proxy-ca.crt | |-- front-proxy-ca.key | |-- front-proxy-client.crt | |-- front-proxy-client.key | |-- sa.key | `-- sa.pub 下面我们根据这个 Kubernetes 的组件之间通讯图来一一讲解每个证书的作用。本文基于 Kubernetes:v1.22.17[2] # k8s-crt-arch.png\nCA证书 # Kubeadm 安装的集群中我们都是用 3 套 CA 证书来管理和签发其他证书，一套 CA 给 ETCD 使用，一套是给 Kuberntes 内部组件使用，还有一套是给配置聚合层使用的，当然如果你觉得管理 3 套 CA 比较麻烦，您也可以用一套来管理。 # Etcd 证书 # ca.crt ca.key healthcheck-client.crt healthcheck-client.key peer.crt peer.key server.crt server.key etcd 证书位于 /etc/kubernetes/pki/etcd 目录下，我们根据 etcd 的 static-pod yaml 配置解释下证书的作用 # spec: containers: - command: - etcd - --advertise-client-urls=https://10.0.4.3:2379 - --cert-file=/etc/kubernetes/pki/etcd/server.crt - --client-cert-auth=true - --data-dir=/var/lib/etcd - --initial-advertise-peer-urls=https://10.0.4.3:2380 - --initial-cluster=vm-4-3-centos=https://10.0.4.3:2380 - --key-file=/etc/kubernetes/pki/etcd/server.key - --listen-client-urls=https://127.0.0.1:2379,https://10.0.4.3:2379 - --listen-peer-urls=https://10.0.4.3:2380 - --name=vm-4-3-centos - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt - --peer-client-cert-auth=true - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt - --snapshot-count=10000 - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt image: k8s.gcr.io/etcd:3.3.10 imagePullPolicy: IfNotPresent livenessProbe: exec: command: - /bin/sh - -ec - ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key get foo failureThreshold: 8 Etcd 根证书 # Etcd 根证书用于签发其余证书，比如服务端证书，客户端证书等 # ca.crt ca.key Etcd 服务端证书 # Etcd 对外提供服务的服务器证书及私钥，比如 etcd-ctl 访问 Etcd 的时候就会用 ca.crt 去验证 server.crt # Etcd 启动时通过 - --cert-file=/etc/kubernetes/pki/etcd/server.crt，- --key-file=/etc/kubernetes/pki/etcd/server.key 来配置服务端证书可私钥 # server.crt server.key Etcd node 间证书 # Etcd 节点之间相互进行认证的 peer 证书、私钥，结点之间心跳检测，数据同步等通信都会使用 peer.crt 来验证 # 通过 - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt，- --peer-key-file=/etc/kubernetes/pki/etcd/peer.key 来配置 # peer.crt peer.key Etcd 健康检查客户端证书 # 探测 Etcd 服务健康检查接口，客户端会下载服务端证书进行验证，服务端也会下载客户端证书验证，即下面的客户端证书，这个需要客户端来配置 # healthcheck-client.crt healthcheck-client.key Kube-apiserver # Kube-apiserver 证书位于 /etc/kubernetes/pki ，同样我们通过 Kube-apiserver 的 static-pod yaml 文件来一一解释下每个证书的作用。 # name: kube-apiserver namespace: kube-system spec: containers: - command: - kube-apiserver - --advertise-address=10.0.4.3 - --allow-privileged=true - --authorization-mode=Node,RBAC - --client-ca-file=/etc/kubernetes/pki/ca.crt - --enable-admission-plugins=NodeRestriction - --enable-bootstrap-token-auth=true - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key - --etcd-servers=https://127.0.0.1:2379 - --insecure-port=0 - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key - --requestheader-allowed-names=front-proxy-client - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt - --requestheader-extra-headers-prefix=X-Remote-Extra- - --requestheader-group-headers=X-Remote-Group - --requestheader-username-headers=X-Remote-User - --secure-port=6443 - --service-account-key-file=/etc/kubernetes/pki/sa.pub - --service-cluster-ip-range=10.96.0.0/12 - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key image: k8s.gcr.io/kube-apiserver:v1.15.2 Kube-apiserver 根证书 # 用来签发 Kubernetes 中其他证书的 CA 证书及私钥，Kube-apiserver 会配置自己的根证书，也会配置 etcd 的根证书，是因为 Kube-apiserver 会作为客户端去访问 Kubelet，需要 ca.crt 来验证 Kubelet 的服务端证书，而且 Kube-apiserver 也会作为客户端去访问 Etcd，因为 Etcd 与 Kubernetes 不同属一个根证书，所以配置两个不同 CA。 # 通过 - --client-ca-file=/etc/kubernetes/pki/ca.crt，- --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt 分别配置 # ca.crt ca.key Kube-apiserver 服务端证书 # Kube-apiserver 对外提供服务的证书及私钥，通过 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt，--tls-private-key-file=/etc/kubernetes/pki/apiserver.key 配置 # apiserver.crt apiserver.key 假如 Kube-apiserver 自定义对外访问时，要在服务端证书的 SANs（Subject Alternative Name) 字段中添加对应的 DNS名称 或 IP地址，否则客户端会因访问地址与证书不匹配而报错。kubeadm 会帮我们设置一些默认的 SANs，包括 master 结点 IP，Kube-apiserver SVC IP 等。可以通过 openssl 命令查看证书的 SANs # $ openssl x509 -noout -text -in /etc/kubernetes/pki/apiserver.crt Certificate: Data: Version: 3 (0x2) Serial Number: 1302536908518083956 (0x12138a6acb0e4d74) Signature Algorithm: sha256WithRSAEncryption Issuer: CN=kubernetes Validity Not Before: May 10 06:48:30 2023 GMT Not After : Apr 16 06:48:32 2123 GMT Subject: CN=kube-apiserver Subject Public Key Info: Public Key Algorithm: rsaEncryption Public-Key: (2048 bit) Modulus: 00:d8:c6:9c:82:2c:92:53:5b:68:34:ac:09:4a:2c: 3c:1f:8b:e9:bd:be:bb:61:cf:96:f6:e8:5d:60:da: 4f:ea:38:c4:81:6a:bf:33:6f:d7:42:1f:9e:02:09: 51:f6:bc:9d:8f:56:9a:aa:fd:d7:b1:41:20:1e:cd: 69:6c:1e:04:d3:5f:6a:cd:3a:84:9b:51:a5:c5:79: 9c:8b:d8:b0:a0:fb:7e:3c:b6:b0:47:a7:56:d9:bf: cd:76:40:e5:5f:08:a0:e1:50:dd:89:8a:76:2b:fc: 46:8b:53:fb:92:a1:ab:35:01:fe:11:8b:5b:d2:9a: c8:41:4a:1f:6f:09:04:24:a1:44:bd:d2:73:97:75: d7:25:9a:18:cf:a0:42:8b:22:9b:0e:c4:98:09:c2: 95:11:30:56:30:4e:7c:cb:47:18:9b:4e:f4:3d:5f: cd:c2:f1:ca:f5:f2:02:78:9a:26:c8:cd:97:d4:30: da:07:97:33:9e:63:54:5f:a4:3a:e9:82:00:f2:53: 2a:bc:98:b6:bc:ba:22:9a:c9:22:34:2e:86:cd:4f: 9a:e7:7a:1d:e4:5f:d8:8a:2e:28:12:01:d3:40:5e: 63:37:ba:46:c4:e2:1d:be:20:52:fd:69:37:75:79: 1b:69:e6:20:d7:c8:43:bf:09:3f:27:0d:f8:5e:95: fd:db Exponent: 65537 (0x10001) X509v3 extensions: X509v3 Key Usage: critical Digital Signature, Key Encipherment X509v3 Extended Key Usage: TLS Web Server Authentication X509v3 Authority Key Identifier: keyid:8B:81:2B:52:44:15:2A:D0:CF:96:FE:FD:40:14:E8:C0:56:8E:83:9E **# 这里** X509v3 Subject Alternative Name: DNS:kubernetes.default, DNS:kubernetes.default.svc, DNS:apiserver.cluster.local, DNS:kubernetes.default.svc.cluster.local, DNS:master-172-31-97-104, DNS:localhost, DNS:kubernetes, IP Address:127.0.0.1, IP Address:10.233.0.1, IP Address:10.103.97.2, IP Address:172.31.97.104 Signature Algorithm: sha256WithRSAEncryption c2:c4:a4:48:1c:78:32:3c:04:37:79:f0:87:7e:92:ac:14:64: ef:84:28:d2:f7:c0:62:75:c3:bf:cf:ec:f7:c2:8f:2f:91:3e: b0:99:f1:6c:7f:98:62:4b:82:a5:d6:e5:d0:4a:cb:16:b2:8d: d6:95:89:ff:50:15:01:0f:29:13:49:c7:8c:69:c4:50:9a:5d: 7c:fc:b1:8a:30:02:10:2e:c1:cf:b5:37:65:a3:5c:e6:50:ee: b0:60:a6:77:6e:3b:98:b7:2d:c2:4c:e3:2d:8f:9e:9f:25:b1: 32:97:e7:08:d9:cd:cb:69:29:5f:30:08:b3:37:23:25:1d:6a: b7:41:20:10:30:44:df:e3:7a:0f:f9:6f:a0:e8:7f:0d:6a:d0: 89:80:cb:99:a1:32:b9:ca:84:a5:1d:95:91:c5:a6:17:c8:87: 88:3e:44:b6:5b:d9:21:09:7d:13:68:42:43:2e:33:4d:49:d4: c7:0c:38:55:b7:96:d5:27:3d:71:dd:f5:73:de:d9:bd:f9:6b: 5c:9b:42:c9:18:2c:f9:29:37:87:cc:58:12:24:66:b8:58:31: d3:5b:1a:08:a0:f6:b7:ea:f9:49:31:12:a2:aa:8e:6c:3a:56: 5c:c4:2c:d9:91:32:d3:3a:7d:5e:8c:d5:85:4c:d7:49:71:8b: 53:26:1b:71 可以看到在 Subject Alternative Name 字段中，已经包含了一些默认的 Kube-apiserver 访问 DNS 或者 IP。 # Tips： 当我们使用 kubeadm 创建集群时候，可以在init时使用 --apiserver-cert-extra-sans 参数指定 SANs，kubeadm 会在生成证书时在默认的基础上增加设置的 SANs。\nKube-apiserver 访问 Etcd 的客户端证书 # 前面说过 Kubernetest 组件间访问都是采用双向 TLS 认证，所以 Kube-apiserver 访问 Etcd 的时候，Kube-apiserver 回去校验 Etcd 服务端证书，同时 Etcd 也会校验 Kube-apiserver 的客户端证书，达到双向认证。因为 Etcd 服务端证书是有 Etcd 的根证书签发，所以 Kube-apiserver 需要配置该 CA，通过 --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt 配置。Etcd 校验 Kube-apiserver 的客户端证书时，Kube-apiserver 会把该证书发送给 Etcd，通过 --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt，--etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key 配置。 # apiserver-etcd-client.key apiserver-etcd-client.crt Kube-apiserver 访问 Kubelet 的客户端证书 # Kube-apiserver 也会去访问 Kubelet，例如 kubectl 查看 pod 日志，或者进入 pod 内部。和访问 Etcd 一样，Kube-apiserver 访问 Kubelet 也是双向 TLS 认证，Kube-apiserver 校验 Kubelet 的服务端证书，通过 --client-ca-file=/etc/kubernetes/pki/ca.crt，Kubelet 校验 Kube-apiserver 通过 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt，--kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key。 # apiserver-kubelet-client.crt apiserver-kubelet-client.key 聚合层/Webhook 证书 # 要扩展 Kube-apiserver 的 API 时，可以采用 Kube-apiserver 聚合功能，具体 Kube-apiserver 聚合原理参考 https://kubernetes.io/zh-cn/docs/tasks/extend-kubernetes/configure-aggregation-layer/。 # 或者自行开发的 Webhook，这两种开发都需要 Kube-apiserver 来调用，所以都会涉及 TLS 认证，Webhook 原理见 https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/extensible-admission-controllers/ # 这里以扩展 apiserver 简单说明下请求链路，当使用 kubectl 对扩展 API 发起请求时，首先 Kube-apiserver 收到请求对 kubectl 使用的 Kube-config 进行认证、鉴权，通过后将请求转发给 APIAggregator，APIAggregator 可以理解为一层代理，然后 APIAggregator 根据 API 的 GroupVersion 来将请求转发给扩展 apiserver，所以 APIAggregator 与开发者开发的扩展 apiserver 就需要进行 TLS 认证。 # 理想情况扩展 apiserver 需要自己签发 CA，然后使用该 CA 签发服务端证书，服务端证书由扩展 apiserver 程序使用，CA 通过 APIService 资源来发布告知 Kube-apiserver APIAggregator ，然后 APIAggregator 访问时获取 APIService 的 caBundle 字段来认证扩展 apiserver。 # --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt 来校验，同时扩展 apiserver 也会校验 APIAggregator 的客户端证书，APIAggregator 的客户端证书通过 --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt，--proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key 配置，扩展 apiserver 会通过 kube-system 命名空间下的 extension-apiserver-authentication configmap 获取签发 *front-proxy-client.crt 的 CA，即 --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt ，*Kube-apiserver 启动时会将该 CA 信息写入 extension-apiserver-authentication configmap 中。这样就达到双向 TLS 认证的效果。但是扩展 apiserver 也可以关闭服务端校验，通过 APIservice 的配置 *insecureSkipTLSVerify: true ，*这样就只会扩展 apiserver 校验 APIAggregator 了。 # front-proxy-ca.crt front-proxy-client.crt front-proxy-ca.key front-proxy-client.key 包括代理转发到用户 api-server 的请求和调用 Webhook 准入控制插件的请求，Kube-apiserver 都是用 --proxy-client-cert-file 来认证的 # 上面所说的证书都在 /etc/kubernetes/pki 目录下，除了 sa.pub[3] 和 sa.key，这个下文讲解。在 Kubernetes 集群中，Kube-controller-manager 和 Kube-scheduler，Kubelet，Kubectl 都是通过 KubeConfig 来访问 Kube-apiserver，原理上都是证书，下面详细讲解下。 # Kube-controller-mananger # 还是和之前一样，我们通过 kube-controller-mananger 的 yaml 文件配置来看看是如何访问 Kube-apiserver。 # spec: containers: - command: - kube-controller-manager - --allocate-node-cidrs=true - --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf - --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf - --bind-address=127.0.0.1 - --client-ca-file=/etc/kubernetes/pki/ca.crt - --cluster-cidr=10.244.0.0/16 - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt - --cluster-signing-key-file=/etc/kubernetes/pki/ca.key - --controllers=*,bootstrapsigner,tokencleaner - --kubeconfig=/etc/kubernetes/controller-manager.conf - --leader-elect=true - --node-cidr-mask-size=24 - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt - --root-ca-file=/etc/kubernetes/pki/ca.crt - --service-account-private-key-file=/etc/kubernetes/pki/sa.key - --use-service-account-credentials=true image: k8s.gcr.io/kube-controller-manager:v1.15.2 你会发现在 yaml 中配置了 /etc/kubernetes/controller-manager.conf 这个配置文件，而不是配置 controller-manager 的客户端证书之类的。Kubernetes 这里的设计是这样的，kube-controller-mananger、kube-scheduler、kubelet 等组件，采用一个kubeconfig 文件中配置的信息来访问 Kube-apiserver。该文件中包含了 Kube-apiserver 的地址，验证 Kube-apiserver 服务器证书的 CA 证书，自己的客户端证书和私钥等访问信息，这样组件只需要配置这个 Kubeconfig 就行。 # 下面我们看看 controller-manager.conf 这个文件配置的证书和秘钥是什么。 # $ cat controller-manager.conf apiVersion: v1 clusters: - cluster: certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJd01EZ3lNREF5TXpBd05Wb1hEVE13TURneE9EQXlNekF3TlZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBSndoCmw4ZVd5SlBsSWpwajlTN09VSWRSTWVxV0Mwb2crN3hQemJQZDhzS2NTemZqWjdHc0ttUXlvQjhoQnNlaVVDdUwKai9teVl5Tk02MkxIa0ZKbDI3MXNFWVdmOEtiWS81Y210UmFjRnlMOEpyaTNLQi91eHZnZlEvMXhMK2c3UmRBcQpGQllWRzNtaSs1T1orTExyZlVMUU5qemtoTVllaEhDdHNDRmZJMGF5amJpYk1UUGJLT3lobjV3cHVMZzgvOVdlClNTSnI1TmtnK2R0WHJSZ05YelNpc1JMQVF5MmdEczdOaTN0SklaNjRuRGdIakpyS21HR2dqbEljN1RFdGFUdWcKcnltKy92akVZZ2NxTlhHakY2ekJlT1FXNW5NdUh0K1plYXphZ1QyQTNkUDhGY3lEWVZrSFJVd0RESDBZOVZlcwpOUFAyZnhURzVVZlhWOUV0WVJNQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFEajZLYXVQR2dvVnlGQmdNUzFZYlVFRXFHQmoKN3IwaG5vclNuOVp4dlxZkM1UkZ0UEd0OEI0YU40T3RMa1REUno5ZmdFc1ZidFdoMXRXWURIWUF6N2FDYkVZawpMRTArRzZQMkpxR043SHlrd05BZFp1QS96emhOdVFKZnhjZG5qVHlIRWZXZyt5OEd1S2JqSU1QdFJVOU45bFpoCkZTeUxsYjNvektYbURDK2RuSHhHMXhNbnpCM05TQStYeGk3ZDVHakExemUzYXFxZXM2bWVONTNYWnFkeDE2N0gKLzNBNld6NjZ4UE9nOHlsUFNVa3R5bU1HNTFkOTFsdTFiZWJYUExtdmc0K3BBeFdhZGJGZ21MR0Z0UE1URXcrWgpIRHZzK3E2NDBIOWJpeitPV2Rld0hjUXE0TW9oQ1dubDhhVzVJYWVSYW1mWS9zZy8xd1NXMkZteGViQT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo= server: https://10.0.4.3:6443 name: kubernetes contexts: - context: cluster: kubernetes user: system:kube-controller-manager name: system:kube-controller-manager@kubernetes current-context: system:kube-controller-manager@kubernetes kind: Config preferences: {} users: - name: system:kube-controller-manager user: client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM1ekNDQWMrZ0F3SUJBZ0lJSWN4Ynk4VWEvV1F3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TURBNE1qQXdNak13TURWYUZ3MHlNVEE0TWpBd01qTXdNRGRhTUNreApKekFsQmdOVkJBTVRIbk41YzNSbGJUcHJkV0psTFdOdmJuUnliMnhzWlhJdGJXRnVZV2RsY2pDQ0FTSXdEUVlKCktvWklodmNOQVFFQkJRQURnZ0VQQURDQ0FRb0NnZ0VCQU5YK0Vqa3c0NDNTNzh5d05LL0dIQTV2eFZCS0Rhbi8KZ21yaUlFTW1hYWhlbDllREJXR0s0dVVtY1VXMXU1TCszeUR0amJlKy83MHZ2M3hvSWY1VkNZQXZqYUorN2twUQpyYW5RUE93cFJnbUlqNTEzV1FsZzMxWDlqREpuNlAybVpYTmZ6YWVOalBwOXdrZGkzZGVqSUZaSm1zYjQ0R3VwCkNrdlpodE5iYUlrVVU1U3dCT3h1dE92Um1uemdHQ3BQa0c4ME9pNWdYcDVzTHJ2dmVYSWxpem5wbHNsa3pxbjQKdWNJMHZMekhQY0JsSWhncEVJOXdCVTFOK3VWLzIxTmRaT3p1UlpFVFRMQ0xmNjhVR0FlM0ZCVXJHblJCUTJJZgpKLzhpNnJVQ2l1T25PQWUvOFNLbzlVM0ExOHN3RDJYandTZVo1NzRRclRGdkFjUjBYQ1BibW4wQ0F3RUFBYU1uCk1DVXdEZ1lEVlIwUEFRSC9CQVFEQWdXZ01CTUdBMVVkSlFRTU1Bb0dDQ3NHQVFVRkJ3TUNNQTBHQ1NxR1NJYjMKRFFFQkN3VUFBNElCQVFBcTY0cVBnVllzRzFGb05QQTRTNlJ0bGwrbUdTVUE2QlVNakQrWkt0eVM1NExCVFZnWQp5K1IrL0Zpd3o2RW1xWUpnZ0EyNWZGdkszSWlGNCt5d3JxeDZETlVZa3BBQkZFWXQ5VjU4a2gxV0pha3BvMEZQCnRZRkFaNmlEMlg4UlBZeUUwSXBMYlFqTGRncS9LYTRiSlhZRFhsS3RTV2UwbmJoY2FUWjRpRm5BcldndmpRQ0sKU05kV0tmSUpGNjJiWGE5a1BGc3ROYWVrWjdoQVZEZzhBbEd1c0tlYVFLdFNLZ2dMREFreElRWjlnNTZSVUprYwp6UUhRVHlibmVTcXJEN3cxT0xIR2RpYmZEYXhzMWdtbi9oL20xNk5ib3NMUlgxNkkxK3VKOWV1d29TWlp3Z29zCmpVRExuWVg1Zm1ZcEdhK2ZDbjdiMTJ4Mzg3SFpmbkE4eTFDTQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg== client-key-data: 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 我们解析下 certificate-authority-data 这个内容看看是不是 Kubernetes 的 CA 的证书\n$ echo \u0026#34;LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJd01EZ3lNREF5TXpBd05Wb1hEVE13TURneE9EQXlNekF3TlZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBSndoCmw4ZVd5SlBsSWpwajlTN09VSWRSTWVxV0Mwb2crN3hQemJQZDhzS2NTemZqWjdHc0ttUXlvQjhoQnNlaVVDdUwKai9teVl5Tk02MkxIa0ZKbDI3MXNFWVdmOEtiWS81Y210UmFjRnlMOEpyaTNLQi91eHZnZlEvMXhMK2c3UmRBcQpGQllWRzNtaSs1T1orTExyZlVMUU5qemtoTVllaEhDdHNDRmZJMGF5amJpYk1UUGJLT3lobjV3cHVMZzgvOVdlClNTSnI1TmtnK2R0WHJSZ05YelNpc1JMQVF5MmdEczdOaTN0SklaNjRuRGdIakpyS21HR2dqbEljN1RFdGFUdWcKcnltKy92akVZZ2NxTlhHakY2ekJlT1FXNW5NdUh0K1plYXphZ1QyQTNkUDhGY3lEWVZrSFJVd0RESDBZOVZlcwpOUFAyZnhURzVVZlhWOUV0WVJNQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFEajZLYXVQR2dvVnlGQmdNUzFZYlVFRXFHQmoKN3IwaG5vclNuOVp4dlUxZkM1UkZ0UEd0OEI0YU40T3RMa1REUno5ZmdFc1ZidFdoMXRXWURIWUF6N2FDYkVZawpMRTArRzZQMkpxR043SHlrd05BZFp1QS96emhOdVFKZnhjZG5qVHlIRWZXZyt5OEd1S2JqSU1QdFJVOU45bFpoCkZTeUxsYjNvektYbURDK2RuSHhHMXhNbnpCM05TQStYeGk3ZDVHakExemUzYXFxZXM2bWVONTNYWnFkeDE2N0gKLzNBNld6NjZ4UE9nOHlsUFNVa3R5bU1HNTFkOTFsdTFiZWJYUExtdmc0K3BBeFdhZGJGZ21MR0Z0UE1URXcrWgpIRHZzK3E2NDBIOWJpeitPV2Rld0hjUXE0TW9oQ1dubDhhVzVJYWVSYW1mWS9zZy8xd1NXMkZteGViQT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=\u0026#34;|base64 -d -----BEGIN CERTIFICATE----- MIICyDCCAbCgAwIBAgIBADANBgkqhkiG9w0BAQsFADAVMRMwEQYDVQQDEwprdWJl cm5ldGVzMB4XDTIwMDgyMDAyMzAwNVoXDTMwMDgxODAyMzAwNVowFTETMBEGA1UE AxMKa3ViZXJuZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAJwh l8eWyJPlIjpj9S7OUIdRMeqWC0og+7xPzbPd8sKcSzfjZ7GsKmQyoB8hBseiUCuL j/myYyNM62LHkFJl271sEYWf8KbY/5cmtRacFyL8Jri3KB/uxvgfQ/1xL+g7RdAq FBYVG3mi+5OZ+LLrfULQNjzkhMYehHCtsCFfI0ayjbibMTPbKOyhn5wpuLg8/9We SSJr5Nkg+dtXrRgNXzSisRLAQy2gDs7Ni3tJIZ64nDgHjJrKmGGgjlIc7TEtaTug rym+/vjEYgcqNXGjF6zBeOQW5nMuHt+ZeazagT2A3dP8FcyDYVkHRUwDDH0Y9Ves NPP2fxTG5UfXV9EtYRMCAwEAAaMjMCEwDgYDVR0PAQH/BAQDAgKkMA8GA1UdEwEB /wQFMAMBAf8wDQYJKoZIhvcNAQELBQADggEBADj6KauPGgoVyFBgMS1YbUEEqGBj 7r0hnorSn9ZxvU1fC5RFtPGt8B4aN4OtLkTDRz9fgEsVbtWh1tWYDHYAz7aCbEYk LE0+G6P2JqGN7HykwNAdZuA/zzhNuQJfxcdnjTyHEfWg+y8GuKbjIMPtRU9N9lZh FSyLlb3ozKXmDC+dnHxG1xMnzB3NSA+Xxi7d5GjA1ze3aqqes6meN53XZqdx167H /3A6Wz66xPOg8ylPSUktymMG51d91lu1bebXPLmvg4+pAxWadbFgmLGFtPMTEw+Z HDvs+q640H9biz+OWdewHcQq4MohCWnl8aW5IaeRamfY/sg/1wSW2FmxebA= -----END CERTIFICATE----- // 查看集群 ca $ cat pki/ca.crt -----BEGIN CERTIFICATE----- MIICyDCCAbCgAwIBAgIBADANBgkqhkiG9w0BAQsFADAVMRMwEQYDVQQDEwprdWJl cm5ldGVzMB4XDTIwMDgyMDAyMzAwNVoXDTMwMDgxODAyMzAwNVowFTETMBEGA1UE AxMKa3ViZXJuZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAJwh l8eWyJPlIjpj9S7OUIdRMeqWC0og+7xPzbPd8sKcSzfjZ7GsKmQyoB8hBseiUCuL j/myYyNM62LHkFJl271sEYWf8KbY/5cmtRacFyL8Jri3KB/uxvgfQ/1xL+g7RdAq FBYVG3mi+5OZ+LLrfULQNjzkhMYehHCtsCFfI0ayjbibMTPbKOyhn5wpuLg8/9We SSJr5Nkg+dtXrRgNXzSisRLAQy2gDs7Ni3tJIZ64nDgHjJrKmGGgjlIc7TEtaTug rym+/vjEYgcqNXGjF6zBeOQW5nMuHt+ZeazagT2A3dP8FcyDYVkHRUwDDH0Y9Ves NPP2fxTG5UfXV9EtYRMCAwEAAaMjMCEwDgYDVR0PAQH/BAQDAgKkMA8GA1UdEwEB /wQFMAMBAf8wDQYJKoZIhvcNAQELBQADggEBADj6KauPGgoVyFBgMS1YbUEEqGBj 7r0hnorSn9ZxvU1fC5RFtPGt8B4aN4OtLkTDRz9fgEsVbtWh1tWYDHYAz7aCbEYk LE0+G6P2JqGN7HykwNAdZuA/zzhNuQJfxcdnjTyHEfWg+y8GuKbjIMPtRU9N9lZh FSyLlb3ozKXmDC+dnHxG1xMnzB3NSA+Xxi7d5GjA1ze3aqqes6meN53XZqdx167H /3A6Wz66xPOg8ylPSUktymMG51d91lu1bebXPLmvg4+pAxWadbFgmLGFtPMTEw+Z HDvs+q640H9biz+OWdewHcQq4MohCWnl8aW5IaeRamfY/sg/1wSW2FmxebA= -----END CERTIFICATE----- 从解码可以发现，Kubeconfig 配置的就是 Kubernetes 的 CA 证书，client-certificate-data 和 client-key-data 就是 Kube-controller-manager 用来访问 Kube-apiserver 的客户端证书和秘钥，只不过 Kubeconfig 对内容进行了 base64 编码。这个就是整个 Kube-controller-manager和 Kube-apiserver 证书认证的方式。 # Kube-scheduler # Kube-scheduler 也是同样的原理，也是在 yaml 中配置一个 Kubeconfig 来进行访问 Kube-apiserver # $ cat scheduler.conf apiVersion: v1 clusters: - cluster: certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJd01EZ3lNREF5TXpBd05Wb1hEVE13TURneE9EQXlNekF3TlZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBSndoCmw4ZVd5SlBsSWpwajlTN09VSWRSTWVxV0Mwb2crN3hQemJQZDhzS2NTemZqWjdHc0ttUXlvQjhoQnNlaVVDdUwKai9teVl5Tk02MkxIa0ZKbDI3MXNFWVdmOEtiWS81Y210UmFjRnlMOEpyaTNLQi91eHZnZlEvMXhMK2c3UmRBcQpGQllWRzNtaSs1T1orTExyZlVMUU5qemtoTVllaEhDdHNDRmZJMGF5amJpYk1UUGJLT3lobjV3cHVMZzgvOVdlClNTSnI1TmtnK2R0WHJSZ05YelNpc1JMQVF5MmdEczdOaTN0SklaNjRuRGdIakpyS21HR2dqbEljN1RFdGFUdWcKcnltKy92akVZZ2NxTlhHakY2ekJlT1FXNW5NdUh0K1plYXphZ1QyQTNkUDhGY3lEWVZrSFJVd0RESDBZOVZlcwpOUFAyZnhURzVVZlhWOUV0WVJNQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFEajZLYXVQR2dvVnlGQmdNUzFZYlVFRXFHQmoKN3IwaG5vclNuOVp4dlUxZkM1UkZ0UEd0OEI0YU40T3RMa1REUno5ZmdFc1ZidFdoMXRXWURIWUF6N2FDYkVZawpMRTArRzZQMkpxR043SHlrd05BZFp1QS96emhOdVFKZnhjZG5qVHlIRWZXZyt5OEd1S2JqSU1QdFJVOU45bFpoCkZTeUxsYjNvektYbURDK2RuSHhHMXhNbnpCM05TQStYeGk3ZDVHakExemUzYXFxZXM2bWVONTNYWnFkeDE2N0gKLzNBNld6NjZ4UE9nOHlsUFNVa3R5bU1HNTFkOTFsdTFiZWJYUExtdmc0K3BBeFdhZGJGZ21MR0Z0UE1URXcrWgpIRHZzK3E2NDBIOWJpeitPV2Rld0hjUXE0TW9oQ1dubDhhVzVJYWVSYW1mWS9zZy8xd1NXMkZteGViQT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo= server: https://10.0.4.3:6443 name: kubernetes contexts: - context: cluster: kubernetes user: system:kube-scheduler name: system:kube-scheduler@kubernetes current-context: system:kube-scheduler@kubernetes kind: Config preferences: {} users: - name: system:kube-scheduler user: client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUMzakNDQWNhZ0F3SUJBZ0lJVlUybER1V2Y1OHd3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TURBNE1qQXdNak13TURWYUZ3MHlNVEE0TWpBd01qTXdNRGhhTUNBeApIakFjQmdOVkJBTVRGWE41YzNSbGJUcHJkV0psTFhOamFHVmtkV3hsY2pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCCkJRQURnZ0VQQURDQ0FRb0NnZ0VCQU14SzJrNmZnWG50cHVNM2JPZ2ZUS0V4aVhsdzdMQVc2VHpUK2thcndVS2UKK2hKSExWSjF4OUphazlDajZ2VWRZdEdkRzMyd0V1R0VFa3ltN0dFZXlyeHJneGRsU3NyVmRqQkFTYnhwNndpZApvZ3dmL2xVa2kza2FPVUozVXd6bmFnWCt6ZUh1d2hVN0R3NkNuaUpkMy9SZW9hU0FjZitvbDl0TTRiazRldVRrCnRXaUE5SDk0VnlQam42SUpkUDdNb1h4TWpZN1c1UysxNy9aczBwbXJabHhuWFdqZjZESXdyNnplbStSNlF1YnAKeE5adEk1WWdsNDk2a09BaTZMVW5xemhCNHIzaDdDOUd0SjFnVDk1YmxiQ0VZNzRtNmVLREZpNXFwZ3JRZnA0YwoxMlhRYzNtcGQzY2IrZXlGUFNsYUVDUmRwS1BKazRpZXgxNnN4TmwzRmk4Q0F3RUFBYU1uTUNVd0RnWURWUjBQCkFRSC9CQVFEQWdXZ01CTUdBMVVkSlFRTU1Bb0dDQ3NHQVFVRkJ3TUNNQTBHQ1NxR1NJYjNEUUVCQ3dVQUE0SUIKQVFBWFovVTcxSnRqQXQ3MjJLeVl6Q1RDZlF1bHdMM2EySGN6NGw5NXVaMFNWVG5ncTNhWUJxeVdwQ2puM3VNaApTaGN5OUZ4ZC92am52YXVTWUdXY05abm84dEVNUFhTaitNNzI5bW1vTUNUa0xCUGJSVGZwRGt3aDNnRS9IRWtuCnN0emRoZTZ3dWp4OWduMXl5WTJSOTFTZ3U3cjdwZjlLM1hOeFh2SFo3Z0tDQnJIVisyMVlQTkNCaC8rYlVuZkcKY2pvNlNNZHphT0Y5SlJod2pUS0l5VTlkeXJkbFBLUlR0Q3NGVEttdy9HM1d4Z1gvbGRCZnNsZmNaVXR4TlpsYQpablBVNlYvK3gwelBTVG56RzRmYTQ3UkhlZmc3YzczQkZjL0ZiYW9obmhrZHNPMVBNWGdhSjQ1bGo1NVNPL1phCmlIbUphZUF3bnh5d0hMazFtclE1b0ttVAotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg== client-key-data: LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFcEFJQkFBS0NBUUVBekVyYVRwK0JlZTJtNHpkczZCOU1vVEdKZVhEc3NCYnBQTlA2UnF2QlFwNzZFa2N0ClVuWEgwbHFUMEtQcTlSMWkwWjBiZmJBUzRZUVNUS2JzWVI3S3ZHdURGMlZLeXRWMk1FQkp2R25yQ0oyaURCLysKVlNTTGVSbzVRbmRURE9kcUJmN040ZTdDRlRzUERvS2VJbDNmOUY2aHBJQngvNmlYMjB6aHVUaDY1T1MxYUlEMApmM2hYSStPZm9nbDAvc3loZkV5Tmp0YmxMN1h2OW16U21hdG1YR2RkYU4vb01qQ3ZyTjZiNUhwQzV1bkUxbTBqCmxpQ1hqM3FRNENMb3RTZXJPRUhpdmVIc0wwYTBuV0JQM2x1VnNJUmp2aWJwNG9NV0xtcW1DdEIrbmh6WFpkQnoKZWFsM2R4djU3SVU5S1ZvUUpGMmtvOG1UaUo3SFhxekUyWGNXTHdJREFRQUJBb0lCQVFDbmJhVlRFSGlkdy82bApjMVJIUFBlaG1DYXlKN0ZqYzdOOWpjRXRVREJZZUZBODBLYTlVUmdPTnZ1ejM5TjlSYk1xVlpjbFFEdUpKYU9WCnZLdzN3SE9wVG5lbW9mWlZHL0w4QW9RcjdhYVpiZzlUM3BpamtRclptbnRaRk5BMDRDZk5lQkdsMi9hbVRidSsKU2FCdVMvOXltR2ZqbVAxVTZRaGp5N09uQ0RuNEFsMmw1SXJpUCtlSTgxS3ZjVjRWUWNhcmtQL0F5SityQm56SgpSZjNRSFBRL2pFaUVRMm9kKzQ4N1FzSWNZVlcwZ0FmQ3pKN05NMUJMeTE0Yy8zUFJRS3JLZTdXT3AwM2Z3QmkxCk5TRlc1dmxSUkpnTDduSHQ4TkdsSUhHRE5VT29KR0UvVnppekJUVFpQRS9nOEkxZ1FLYko5b3ZSdXJQa0J6VGsKRHNGQ281bEJBb0dCQU5qalNTVXIybFo0Z1B6MEZ6bnBMTEFBaVhBSDkwZFlBb1BQSUhoWDR2WmtRUjE4Ykx2NgoraXVSR2dMTkQyckV4dHk3dE1tcTQrZkt2S1VXRWorOVR0KzNqWTVmTFdCeWhNTk1uaXN2eDFsdkxlZnFybkRvCnlkODdPb0p3TnZZdit2YitQR3NGaU51SHdXUTR4Wit3WTFaYitCUnB5UVJNUEs1TnVEbDRFMXZQQW9HQkFQRWkKRitwS1VJSmE0NGZuWDk3L1lHalh6Z1lWTEQ4RkRRMTh4dmY3TG43UEhUNzJoK1VCaXJFV29uK2RmcDFBZS95UwpTMER6Q2ZLUDdiM0R3YkxPbmRKcHdLWnUrUjdBaEs5RGFlVEJ3Y2FkRDFpTzNoME5RSVFoVFJPaVJRclN4RWpmCllXVnRmUXFuSUJhS3pMSnRCakVtakRDcXJ2QjJ3QTRza1Z5d09WZWhBb0dBTWZWb3k5OG1FL1QrQVVaWWMwWjYKdksvaStLTmRHbG56ZWxranFaVFUrdHh0QTFXOTFpOGhvUmR6WG1ITncxSkFYR2dBWk5Pd1c1d2ZpQWRsZkxrbQppZkhGOFoySzNrU0N3Rm5OdFRUMFBtMlZyVzRwY0dpdTEzVFZMV2Fid21tYTdYbnlnTlJ0aWVQamNDcURteDBPClJMNDZqcmt2VElZakZDTmk1Qm44bTVFQ2dZQmNHdUs1cW1Nd041bGJpd1J5d0dkS0JNeDhSRkFmVGtXYkZrTkYKNjVycDh5Qy9zUmxkWHdaaitEcGZ0bi9yZnZzZEVhQlBFY2FGOFhZbEd3WDh6N0UyOHhBVVFxVkRtdFBUd2xOTApmcnNPcTJWMk5UUWdNclNuQTdWV1A1QlJ2d29jcjc2YktJUXZzb0N1TzV4T3R4ZzdZL2IraStQQWxBdHVIcFh6CnFwaHNvUUtCZ1FERkxITzFwTTNPNlRWN3cybThKWVI0WGxBUWtLZkRPMlFGaDB0bGM1bk1rZUdZbHZFUUlZdVMKS2liV3NJNHVwMHFRcFZjdHF2VU9wc2V1Rk5ZdGVRQzF6YncxNWp4a0xEMm9Gb2c1Yk9WRXk3ekZERU1kVmdpRwpEbjhkbHN3SWp0bUF1SDFGOWdBbGR1V1M0cXkyV0I0SlRPZjBlTDVOM1dTWkRzcm91anA5NlE9PQotLS0tLUVORCBSU0EgUFJJVkFURSBLRVktLS0tLQo= -24 同理，解析 certificate-authority-data 也是 Kubernates 的 CA 证书，client-certificate-data 和 client-key-data 就是 Kube-scheduler 用来访问Kube-apiserver 的客户端证书和秘钥 # Kubectl # Kuectl 访问 Kube-apiserver 与 Kube-controller-manager、Kube-scheduler 原理类似，这里不再赘述~ # Kubelet # Kubelet 与 Kube-apiserver 一样，即可以作为服务端，又可以作为客户端，所以分类讲解 # Kubelet 客户端证书 # Kubelet 和其他组件类似，用的 Kubeconfig 与 Kube-apiserver 进行认证、鉴权的，都是用 Kubernates 的 CA 签发。 # 这边我们会给每个节点生成一份客户端的证书和私钥，直接指向一个 kubelet-client-current.pem 文件，这里包含了证书和私钥，每一个节点都不一样。因此每个节点都会有一个自己的客户端证书和私钥。 # $ cat /etc/kubernetes/kubelet.conf apiVersion: v1 clusters: - cluster: certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJd01EZ3lNREF5TXpBd05Wb1hEVE13TURneE9EQXlNekF3TlZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBSndoCmw4ZVd5SlBsSWpwajlTN09VSWRSTWVxV0Mwb2crN3hQemJQZDhzS2NTemZqWjdHc0ttUXlvQjhoQnNlaVVDdUwKai9teVl5Tk02MkxIa0ZKbDI3MXNFWVdmOEtiWS81Y210UmFjRnlMOEpyaTNLQi91eHZnZlEvMXhMK2c3UmRBcQpGQllWRzNtaSs1T1orTExyZlVMUU5qemtoTVllaEhDdHNDRmZJMGF5amJpYk1UUGJLT3lobjV3cHVMZzgvOVdlClNTSnI1TmtnK2R0WHJSZ05YelNpc1JMQVF5MmdEczdOaTN0SklaNjRuRGdIakpyS21HR2dqbEljN1RFdGFUdWcKcnltKy92akVZZ2NxTlhHakY2ekJlT1FXNW5NdUh0K1plYXphZ1QyQTNkUDhGY3lEWVZrSFJVd0RESDBZOVZlcwpOUFAyZnhURzVVZlhWOUV0WVJNQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFEajZLYXVQR2dvVnlGQmdNUzFZYlVFRXFHQmoKN3IwaG5vclNuOVp4dlUxZkM1UkZ0UEd0OEI0YU40T3RMa1REUno5ZmdFc1ZidFdoMXRXWURIWUF6N2FDYkVZawpMRTArRzZQMkpxR043SHlrd05BZFp1QS96emhOdVFKZnhjZG5qVHlIRWZXZyt5OEd1S2JqSU1QdFJVOU45bFpoCkZTeUxsYjNvektYbURDK2RuSHhHMXhNbnpCM05TQStYeGk3ZDVHakExemUzYXFxZXM2bWVONTNYWnFkeDE2N0gKLzNBNld6NjZ4UE9nOHlsUFNVa3R5bU1HNTFkOTFsdTFiZWJYUExtdmc0K3BBeFdhZGJGZ21MR0Z0UE1URXcrWgpIRHZzK3E2NDBIOWJpeitPV2Rld0hjUXE0TW9oQ1dubDhhVzVJYWVSYW1mWS9zZy8xd1NXMkZteGViQT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo= server: https://10.0.4.3:6443 name: default-cluster contexts: - context: cluster: default-cluster namespace: default user: default-auth name: default-context current-context: default-context kind: Config preferences: {} users: - name: default-auth user: client-certificate: /var/lib/kubelet/pki/kubelet-client-current.pem client-key: /var/lib/kubelet/pki/kubelet-client-current.pem $ cat /var/lib/kubelet/pki/kubelet-client-current.pem -----BEGIN CERTIFICATE----- MIICZzCCAU+gAwIBAgIUPrHB6WlowbhzImI5+NnT0Y4ZzlAwDQYJKoZIhvcNAQEL BQAwFTETMBEGA1UEAxMKa3ViZXJuZXRlczAeFw0yMDA4MjAwMjI4MDBaFw0yMTA4 MjAwMjI4MDBaMDsxFTATBgNVBAoTDHN5c3RlbTpub2RlczEiMCAGA1UEAxMZc3lz dGVtOm5vZGU6dm0tNC05LWNlbnRvczBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IA BJ1Qb3DwFRUjIYaaBxNGTieXObGKdGLG8/HVdwXNkVSIWLGBkz9QsFaOh1IsiQ6g 5FfxRBneWhyQTOgMmD0yvymjVDBSMA4GA1UdDwEB/wQEAwIFoDATBgNVHSUEDDAK BggrBgEFBQcDAjAMBgNVHRMBAf8EAjAAMB0GA1UdDgQWBBR2QsIZ/qWdhOExDObO wiBjcpbUMTANBgkqhkiG9w0BAQsFAAOCAQEATF/xpZD9kcCMFqFDlbo1Zn4DwXh6 X3s5T6r3QNtZQ1SeUHUhnL2Q1DrpICAEFxoqMdB75hxlYCs5UOP6YwBUX77qAVs9 QAXW7/sEhS5firGGP8pEQXgaUWwv6tu2V574JL7M9p+koHL/Fbev9fad8I71XIDQ qkmnf892VCYnkvw1s7wNJENlxNQUQ1rw0wEccyKlYpxbqXCYStSloSaz6JCFnT06 +EXV5cr/G8UZnYRoMNu6jiajIxhFmYQqBNCqOlJo24TVjeLlNTL5AD8aSXcQ+O16 PWhBYNdEOulokdjg84gAg6jSqN2g+hi4+gHMG1Rw2h+9iu5E5txFjKGiMQ== -----END CERTIFICATE----- -----BEGIN EC PRIVATE KEY----- MHcCAQEEIN75eP2QG76VLv/nRWiLzW9Fg9hCzeb33BrZ5n9PhwhToAoGCCqGSM49 AwEHoUQDQgAEnVBvcPAVFSMhhpoHE0ZOJ5c5sYp0Ysbz8dV3Bc2RVIhYsYGTP1Cw Vo6HUiyJDqDkV/FEGd5aHJBM6AyYPTK/KQ== -----END EC PRIVATE KEY----- Kubelet 客户端证书可以被自动续签，上面的证书期限都是固定的，下面具体续签的原理 # CSR(CertificateSigningRequest) # Kubernetes 提供了一套证书相关的 API(Certificate API），用于实现证书的申请与自动签发。证书的申请者，如 Kubelet，可以通过创建一个 CSR资源来向指定的证书签名者（由 CSR 的 singerName 字段指定）申请证书签名，CSR 请求可能被批准，也可能被拒绝。当 CSR 请求被批准后，对应的证书签名者才会对证书签名，并将签名后的证书保存在 CSR 的 status.Certificat 字段中，至此整个签发流程就完成了。证书的申请者可以从 status.Certificate 中获取已经签名的证书。Kube-controller-manager 内置了一些签名者，分别处理对应 singerName 的 CSR 请求： # • kubernetes.io/kube-apiserver-client，申请用于访问 Kube-apiserver的证书，不会自动批准。 • kubernetes.io/kube-apiserver-client-kubelet，Kubelet 申请用于访问 Kube-apiserver的客户端证书，可能会被自动批准。 • kubernetes.io/kubelet-serving， Kubelet 的服务端证书，不会自动批准。 • kubernetes.io/legacy-unknown，第三方应用的证书申请，不会自动批准。 Kubelet 进行客户端证书轮换时，创建的 CSR 中的 singerName 就是 kubernetes.io/kube-apiserver-client-kubelet，正常情况下，会被 Kube-controller-manager 自动批准，然后签发证书。 # 当 CSR 提交后，需要由审批者（可以是人，也可以是程序）批准后才能进行后续的证书签发操作。Kube-controller-manager 内置了一个审批控制器，可以自动批准某些 CSR 请求。但为了防止与其他的审批者发生冲突，Kube-controller-manager 不会显式的拒绝任何 CSR。对于不会被Kube-controller-manager 处理的 CSR，我们可以使用 API 的方式处理，如实现一个专门的控制器来来自动批准或拒绝，或者使用 Kubectl 命令行： # # 批准一个CSR $ kubectl certificate approve \u0026lt;certificate-signing-request-name\u0026gt; # 拒绝一个CSR $ kubectl certificate deny \u0026lt;certificate-signing-request-name\u0026gt; Kubelet 客户端证书自动续签 # 对于 kubernetes.io/kube-apiserver-client-kubelet 类型的 CSR，Kube-controller-manager 根据申请者是否具备对应的 RBAC 权限，来决定是否批准该 CSR。Kubelet 在两种情况下都会创建 CSR 请求： # 1、在首次加入集群时，还没有生成客户端证书，Kubelet 需要创建 CSR 资源来申请，这个阶段也就是 TLS 引导阶段。 # 2、当客户端证书快过期时，Kubelet 会发起证书轮换，创建 CSR 请求申请新的证书。 # 对于这两种场景，Kubernetes 提供了两个默认权限（ClusterRole）： # 1、nodeclient：当节点首次创建证书时，Kubelet 还没有正式的客户端证书，处于 TLS 引导阶段。此时Kubelet 会使用 bootstrap token 认证方式来请求 Kube-apiserver。kubeadm init创建的 bootstrap token 所属用户组为 system:bootstrappers:kubeadm:default-node-token，kubeadm 会负责将 nodeclient 权限赋予该用户组。 # 2、selfnodeclient：当节点请求证书轮换时，Kubelet 已经有一个正式的客户端证书。Kubelet 的证书属于 system:nodes 用户组，kubeadm 会负责将 selfnodeclient 权限赋予该用户组。 # # 两个默认权限 $ kubectl get clusterrole -l kubernetes.io/bootstrapping=rbac-defaults | grep nodeclient system:certificates.k8s.io:certificatesigningrequests:nodeclient 2021-09-08T14:59:17Z system:certificates.k8s.io:certificatesigningrequests:selfnodeclient 2021-09-08T14:59:17Z # kubeadm会负责将这两个权限绑定到对应的用户组 $ kubectl get clusterrolebinding kubeadm:node-autoapprove-bootstrap kubeadm:node-autoapprove-certificate-rotation -owide NAME ROLE AGE USERS GROUPS SERVICEACCOUNTS kubeadm:node-autoapprove-bootstrap ClusterRole/system:certificates.k8s.io:certificatesigningrequests:nodeclient 27d system:bootstrappers:kubeadm:default-node-token kubeadm:node-autoapprove-certificate-rotation ClusterRole/system:certificates.k8s.io:certificatesigningrequests:selfnodeclient 27d system:nodes 当 CSR 请求被批准后，签发者才可以签发证书。Kube-controller-manager 同样也内置了签发控制器。通过为 Kube-controller-manager 设置 --cluster-signing-cert-file 和 --cluster-signing-key-file 启动参数以开启内置的签发控制器，这两个参数分别表示用于签名的证书和私钥，也就是集群的 CA 证书。 # 在1.18之前，kube-controller-manager会为所有已经批准的CSR签发证书。1.18之后，kube-controller-manager限制了CSR的singerName，只会为上述的四种指定singerName的CSR请求签发证书。类似的，对于不会自动签发证书的CSR请求，我们同样可以通过kubectl来手动签发，亦或者通过实现一个专门的控制器来自动签发。 # Kube-controller-manager 通过配置 Kubelet 客户端证书续签周期 --experimental-cluster-signing-duration=87600h0m0s，来开启自动续签 Kubelet 客户端证书\nKubelet 的服务端证书 # Kubelet 同样对外暴露了 HTTPS 服务，其客户端主要是 Kube-apiserver 和一些监控组件，如 metric-server。Kube-apiserver 需要访问 Kubelet 来获取容器的日志和执行命令（kubectl logs/exec)， 监控组件需要访问 Kubelet 暴露的 cadvisor 接口来获取监控信息。理想情况下，我们需要将 Kubelet 的 CA 证书配置到 Kube-apiserver 和 metric-server 中，以便于校验 Kubelet 的服务端证书，保证安全性。但使用默认的集群设置方法是无法做到这点的，需要做一些额外的工作。 # Kubernetes 中除了 Kubelet 的服务端证书以外，其他证书都要由集群根 CA（或是基于根CA的中间CA）签发。 Kubelet 的证书则没有这个要求。实际上，在 Kubelet 在启动时，如果没有指定服务端证书路径，会创建一个自签的 CA 证书，并使用该 CA 为自己签发服务端证书。 # Kubelet 服务端证书和客户端证书生成逻辑不一样，有以下三种情况，可自选： # • 使用通过 --tls-private-key-file 和 --tls-cert-file 所设置的密钥和证书，这样每个节点的根证书有可能就不一样 # • 如果没有提供密钥和证书，则创建自签名的密钥和证书，也会导致每个节点的根证书不一样(如果 kubeadm init/join 没有其他配置，默认都是这种情况)，Kubelet 每次重启都会创建证书和私钥 # • 通过 CSR API 从集群服务器请求服务证书 # 前面两种情况就会导致每个节点的 Kubelet 的根 CA 可能都不一样，这就导致客户端组件，如 metric-server ，Kube-apiserver 都没办法校验 Kubelet 的服务端证书。为了应对这种情况，metric-server 需要添加 --kubelet-insecure-tls 来跳过服务端证书的校验，而 Kube-apiserver 默认不校验 Kubelet 服务端证书。 # 第三种情况是 CSR 签发者统一用集群的根 CA 为各 Kubelet 签发服务端证书，Kube-apiserver 和其他组件就可以通过配置集群根 CA 来实现 HTTPS 的服务端证书校验了。我们可以在 Kubelet 配置文件配置 serverTLSBootstrap = true 就可以启用这项特性，使用 CSR 来申请服务端证书。这项配置同样也会开启服务端证书的自动轮换功能。不过这个过程并不是全自动的，在 CSR(CertificateSigningRequest) 章节中提到，Kubelet 的服务端证书 CSR 请求，即 singerName 为 kubernetes.io/kubelet-serving 的 CSR 请求，不会被 Kube-controller-manager 自动批准，也就是说我们需要手动批准这些 CSR，或者使用第三方控制器。 # 为什么 Kubernetes 不自动批准 Kubelet 的服务端证书呢？这样不是很方便吗？原因是出于安全考量—— Kubernetes 没有足够的能力来辨别该 CSR 是否应该被批准。 # HTTPS 服务端证书的重要作用就是向客户端证明“我是我”，防止有人冒充“我”跟客户端通信，也就是防止中间人攻击。在向权威 CA 机构申请证书时，我们要提供一系列证明材料，证明这个站点是我的，包括要证明我是该站点域名的所有者，CA 审核通过后才会签发证书。但 K8S 集群本身是没有足够的能力来辨别 Kubelet 身份的，因为节点 IP，DNS 名称可能发生变化，K8S 自身没有足够的能力判断哪些 IP，哪些 DNS 是合法的，这属于基础设施管理者的职责范围。如果你的集群是云厂商提供，那么你的云厂商可以提供对应的控制器来判断 CSR 请求的合法性，批准合法的 CSR 请求。如果是自建集群，那么只有集群管理员才能判断 CSR 请求中包含的节点 IP，DNS 名称是不是真实有效的。如果 Kube-controller-manager 自动签发这些证书，则会产生中间人攻击的风险。 # 假设节点 A 上的服务 bar 使用 HTTPS 暴露服务，并且服务端证书是通过 CSR 请求申请的，由集群根 CA 签发。假设有入侵者获取了节点 A 的权限，那他可以很方便的利用 Kubelet 的客户端证书的权限，创建一个 CSR 请求来申请一份 IP 为 bar service IP，DNS 名称为 bar service DNS 的服务端证书。如果 Kube-controller-manager 自动通过并签发这个证书，那入侵者就可以使用这个证书，配合节点上的 Kube-proxy，劫持所有经过 bar 服务的流量。 # Service Account 认证 # 在 Kubernetes 集群内部访问 Kube-apiserver 使用的是 Service Account ，如 pod 访问 Kube-apiserver # 在 /etc/kubernetes/pki 目录下，还有 sa.pub，sa.key 这两个文件没有讲解。这两个就是用于 ServiceAccount 认证的，这两个文件是一对密钥对，sa.pub 代表公钥，sa.key 代表私钥。 # Kubernetes 集群中还有个重要的系统组件 Kube-proxy，它也需要访问 Kube-apiserver，但是它和 Kube-controller-manager，Kube-scheduler 不一样，它就是使用 ServiceAccount 来与 Kube-apiserver 进行认证，下面详细看看。 # 当 Kube-proxy pod 在集群中创建时，如果 Pod 没有指定 ServiceAccount，kube-controller-manager 会默认创建一个没有任何权限的 ServiceAccount，同时 Kube-controller-manager 为该 ServiceAccount 生成一个 JWT token，并使用 secret 将该 token 挂载到 Pod 内部。 # $ kubectl get pod kube-proxy-6bf2t -n kube-system -o yaml ..... containers: - command: - /usr/local/bin/kube-proxy - --config=/var/lib/kube-proxy/config.conf - --hostname-override=$(NODE_NAME) ... volumeMounts: ... // token 文件在 pod 的路径 - mountPath: /var/run/secrets/kubernetes.io/serviceaccount name: kube-proxy-token-rd92l readOnly: true dnsPolicy: ClusterFirst ..... volumes: ... - name: kube-proxy-token-rd92l secret: defaultMode: 420 secretName: kube-proxy-token-rd92l 下面看看 secret 的内容 # $ kubectl get secret -n kube-system kube-proxy-token-rd92l -o yaml apiVersion: v1 data: // 该 ca 就是 Kubernetes 集群中的 CA, 用于 pod 校验 Kube-apiserver 的服务端证书 ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJd01EZ3lNREF5TXpBd05Wb1hEVE13TURneE9EQXlNekF3TlZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBSndoCmw4ZVd5SlBsSWpwajlTN09VSWRSTWVxV0Mwb2crN3hQemJQZDhzS2NTemZqWjdHc0ttUXlvQjhoQnNlaVVDdUwKai9teVl5Tk02MkxIa0ZKbDI3MXNFWVdmOEtiWS81Y210UmFjRnlMOEpyaTNLQi91eHZnZlEvMXhMK2c3UmRBcQpGQllWRzNtaSs1T1orTExyZlVMUU5qemtoTVllaEhDdHNDRmZJMGF5amJpYk1UUGJLT3lobjV3cHVMZzgvOVdlClNTSnI1TmtnK2R0WHJSZ05YelNpc1JMQVF5MmdEczdOaTN0SklaNjRuRGdIakpyS21HR2dqbEljN1RFdGFUdWcKcnltKy92akVZZ2NxTlhHakY2ekJlT1FXNW5NdUh0K1plYXphZ1QyQTNkUDhGY3lEWVZrSFJVd0RESDBZOVZlcwpOUFAyZnhURzVVZlhWOUV0WVJNQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFEajZLYXVQR2dvVnlGQmdNUzFZYlVFRXFHQmoKN3IwaG5vclNuOVp4dlUxZkM1UkZ0UEd0OEI0YU40T3RMa1REUno5ZmdFc1ZidFdoMXRXWURIWUF6N2FDYkVZawpMRTArRzZQMkpxR043SHlrd05BZFp1QS96emhOdVFKZnhjZG5qVHlIRWZXZyt5OEd1S2JqSU1QdFJVOU45bFpoCkZTeUxsYjNvektYbURDK2RuSHhHMXhNbnpCM05TQStYeGk3ZDVHakExemUzYXFxZXM2bWVONTNYWnFkeDE2N0gKLzNBNld6NjZ4UE9nOHlsUFNVa3R5bU1HNTFkOTFsdTFiZWJYUExtdmc0K3BBeFdhZGJGZ21MR0Z0UE1URXcrWgpIRHZzK3E2NDBIOWJpeitPV2Rld0hjUXE0TW9oQ1dubDhhVzVJYWVSYW1mWS9zZy8xd1NXMkZteGViQT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo= namespace: a3ViZS1zeXN0ZW0= // kube-contoller-manager 生成的 token token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJbXRwWkNJNklpSjkuZXlKcGMzTWlPaUpyZFdKbGNtNWxkR1Z6TDNObGNuWnBZMlZoWTJOdmRXNTBJaXdpYTNWaVpYSnVaWFJsY3k1cGJ5OXpaWEoyYVdObFlXTmpiM1Z1ZEM5dVlXMWxjM0JoWTJVaU9pSnJkV0psTFhONWMzUmxiU0lzSW10MVltVnlibVYwWlhNdWFXOHZjMlZ5ZG1salpXRmpZMjkxYm5RdmMyVmpjbVYwTG01aGJXVWlPaUpyZFdKbExYQnliM2g1TFhSdmEyVnVMWEprT1RKc0lpd2lhM1ZpWlhKdVpYUmxjeTVwYnk5elpYSjJhV05sWVdOamIzVnVkQzl6WlhKMmFXTmxMV0ZqWTI5MWJuUXVibUZ0WlNJNkltdDFZbVV0Y0hKdmVIa2lMQ0pyZFdKbGNtNWxkR1Z6TG1sdkwzTmxjblpwWTJWaFkyTnZkVzUwTDNObGNuWnBZMlV0WVdOamIzVnVkQzUxYVdRaU9pSmhOemRrTjJKaE1TMW1Zek5pTFRRM1lUTXRZV00wWkMweVpXRmtPVEV6WkRVd09ESWlMQ0p6ZFdJaU9pSnplWE4wWlcwNmMyVnlkbWxqWldGalkyOTFiblE2YTNWaVpTMXplWE4wWlcwNmEzVmlaUzF3Y205NGVTSjkuSTRuR0UxOVhJakFPU0lKcWZyb3A2azhHcXBickxBeVFzQ3NoeFhxMEc3RklTZmJudS1TTW9xV1pHUjU0S2hwREdlaGd6WkQwMGVGZG14bEM1ZzBIc2ZzZE40V0tmVFI1ZjY1b3kzTnVvWUxxcUIzUzgySUxLelJHREVBNHpwWmFXeG1lRmtzdU1mdl9UWDRjSGdtYUI3V0ZQZzJ5RWtxV0VPa3kwT0hOWnIxNmd4Mzl3S1owWDRhQ29FOVd0cGlZU1BKYU5SdmtVbENfNTlPZHJTYnBCYnlkd2JOaWVaRjdhcWRBbFdWQ3JXQkRfWmlCaHNnZklVYUpEcVg5TWtRbUpjVS1Yb2pzWUpXNFpNejZ3OEZFTHY4THpCazRLTUc5V185aG5Jc3FfVlFUM2xDek5iSHlNSktWeXZ1VlVrblo5X3AwaTJGQlpDeGVVdlpVazdrd01R kind: Secret metadata: annotations: kubernetes.io/service-account.name: kube-proxy kubernetes.io/service-account.uid: a77d7ba1-fc3b-47a3-ac4d-2ead913d5082 creationTimestamp: \u0026#34;2020-08-20T02:30:48Z\u0026#34; name: kube-proxy-token-rd92l namespace: kube-system resourceVersion: \u0026#34;196\u0026#34; selfLink: /api/v1/namespaces/kube-system/secrets/kube-proxy-token-rd92l uid: c9ff07a0-4176-4053-a93c-11c7d0aff285 type: kubernetes.io/service-account-token Kube-controller-manager 用 sa.key 即私钥对该 token 进行签名。当 Pod 需要访问 Kube-apiserver 的时候，认证逻辑如下： # • Pod 使用 secret 的 ca.crt 来校验 Kube-apiserver 的服务端证书 # • Kube-apiserver 使用 sa.pub 即公钥对 Pod 的 token 进行验证，如果验证成功，则认证通过 # 这样就达到了 Pod 与 Kube-apiserver 双向认证(这里不是双向 TLS 认证)，所以 ServiceAccount 这种认证方式属于 Kube-apiserver 的 Token 认证。 # 下面是 ServiceAccout 认证流程图： # serviceAccount.png\nsa.pub 和 sa.key 分别被配置到了 Kube-apiserver 和 Kube-controller-manager 的命令行参数中，如下所示：\n/usr/local/bin/kube-apiserver \\\\ --service-account-key-file=/etc/kubernetes/pki/sa.pub \\\\ # 用于验证 service account token 的公钥 ... /usr/local/bin/kube-controller-manager \\\\ --service-account-private-key-file=/etc/kubernetes/pki/sa.key # 用于对 service account token 进行签名的私钥 ... 总结 # Kubernetes 证书系统还是比较复杂的，主要是涉及到双向 TLS 认证，但是只要能够弄清楚组件之间相互调用的关系以及双向 TLS 认证原理，就比较容易弄明白 Kubernetes 证书了。 # 以上主要是分析了 Kubernetes 集群中所有的证书和组件如何使用证书的，对于 Kube-apiserver 来说，我们只分析了 Kube-apiserver 如何根据证书进行认证，后续如何根据证书进行鉴权还没说。由于本篇篇幅较大，证书鉴权内容留到下一篇~ # [1] kubeadm: https://kubernetes.io/zh/docs/reference/setup-tools/kubeadm/kubeadm/ [2] Kubernetes:v1.22.17: https://github.com/kubernetes/kubernetes/tree/release-1.22 [3] sa.pub: http://sa.pub "},{"id":130,"href":"/docs/kubernetes-%E8%AF%81%E4%B9%A6%E8%AF%A6%E8%A7%A3%E9%89%B4%E6%9D%83-kubernetes-zheng-shu-xiang-jie--jian-quan-/","title":"Kubernetes 证书详解(鉴权) 2024-04-03 14:49:04.385","section":"Docs","content":" 简介 # 上一篇 Kubernetes 证书详解(认证) 系统分析了 Kubernetes 集群中每个证书的作用和证书认证的原理。对于 Kube-apiserver，Kubelet 来说，它们都能提供 HTTPS 服务，Kube-apiserver、Kubelet 对于一个请求，既要认证也要鉴权。在 Kube-apiserver 中，鉴权也有多种方式： # • Node # • ABAC # • RBAC # • Webhook # 在 TLS + RBAC 模式下，访问 Kube-apiserver 有三种方式： # • 证书 + RBAC(就是上一篇说到的那些证书) # • Node + RBAC( Kubelet 访问 Kube-apiserver 时) # • ServiceAccount + RBAC( Kubernetes 集群内 Pod 访问 Kube-apiserver ） # 关于 RBAC 的内容不熟悉的可以参考官网[1] # K8S 证书的 CN、O # RBAC 鉴权需要对 User 或者 Group 来绑定相应权限达到效果。Kubernetes 证书中的 CN 表示 User，O 表示 Group，看一个例子： # 用 openssl 命令解析 kubelet 的客户端证书，kubelet 访问 Kube-apiserver 的时候就会用这个证书来认证，鉴权。 # $ openssl x509 -noout -text -in kubelet-client-current.pem Certificate: Data: Version: 3 (0x2) Serial Number: 271895513527774644 (0x3c5f7876bd86db4) Signature Algorithm: sha256WithRSAEncryption Issuer: CN=kubernetes Validity Not Before: May 10 06:48:30 2023 GMT Not After : Apr 16 06:48:37 2123 GMT # 看这里 Subject: O=system:nodes, CN=system:node:master-172-31-97-104 Subject Public Key Info: ....... 可以发现 Kubelet 的客户端证书的 O 是 system.nodes，CN 是 system:node:master-172-31-97-104 ，所以在 Kubernetes 中，每个结点的 Kubelet 都被赋予 system:node:\u0026quot;结点名称“ 的 User，且附属于 system.nodes 的 Group。 # Kubernetes RBAC 鉴权机制就是利用将权限绑定到 User 或者 Group，使得 User、Group 拥有对应权限，下面就看看 Kubernetes 如何 根据证书、ServiceAccount 鉴权的。 # Kubectl # Kubectl 使用 KubeConfig 与 Kube-apiserver 进行认证、鉴权。认证上一篇说过了，就是通过 TLS 认证。这里说鉴权，先看看 KubeConfig 的客户端证书 O、CN # 使用 openssl 命令解析 KubeConfig 中 client-certificate-data 字段，查看 KubeConfig 客户端证书的 O、CN # $ cat /root/.kube/config | grep client-certificate-data: | sed \u0026#39;s/ client-certificate-data: //g\u0026#39; | base64 -d | openssl x509 -noout -subject # 结果 subject= /O=system:masters/CN=kubernetes-admin 可以发现 KubeConfig 客户端证书为 kubernetes-admin User 且属于 system:masters Group。 # system:masters 是 Kubernetes 内置的用户组，且 Kubernetes 集群中也包含许多默认 ClusterRole、ClusterRolebinding，其中 cluster-admin 的 ClusterRolebinding 就将 cluster-admin 的 ClusterRole 绑定到 system:masters Group，这样 KubeConfig 就拥有权限来操作 Kube-apiserver 了。 # # cluster-admin ClusterRole 拥有所有资源的所有权限 $ kubectl get clusterrole cluster-admin -o yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \u0026#34;true\u0026#34; creationTimestamp: \u0026#34;2023-05-10T06:49:27Z\u0026#34; labels: kubernetes.io/bootstrapping: rbac-defaults name: cluster-admin resourceVersion: \u0026#34;85\u0026#34; uid: 006af228-e6ef-43fa-a73f-ca0c109b13f0 rules: - apiGroups: - \u0026#39;*\u0026#39; resources: - \u0026#39;*\u0026#39; verbs: - \u0026#39;*\u0026#39; - nonResourceURLs: - \u0026#39;*\u0026#39; verbs: - \u0026#39;*\u0026#39; ------------------------------------------------------------------- # cluster-admin ClusterRoleBinding 将 cluster-admin ClusterRole 绑定到 system:masters Group $ kubectl get clusterrolebinding cluster-admin -o yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \u0026#34;true\u0026#34; creationTimestamp: \u0026#34;2023-05-10T06:49:27Z\u0026#34; labels: kubernetes.io/bootstrapping: rbac-defaults name: cluster-admin resourceVersion: \u0026#34;147\u0026#34; uid: 980fbdff-6750-4957-b5fa-954a5013b192 roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole # clusterRole name name: cluster-admin subjects: - apiGroup: rbac.authorization.k8s.io kind: Group # Group name name: system:masters 经过上面的操作，Kubectl 就可以使用 KubeConfig 来操作 Kube-apiserver 了。 # Kube-scheduler、Kube-controller-manager # 同样 Kube-scheduler、Kube-controller-manager 也是使用各自的 KubeConfig 来认证、鉴权。但是他们和 Kubectl 的 KubeConfig 不属于同一个用户、组。 # 使用 openssl 命令解析 Kube-scheduler 的 KubeConfig 中 client-certificate-data 字段，查看 KubeConfig 客户端证书的 O、CN # $ cat /etc/kubernetes/scheduler.conf | grep client-certificate-data: | sed \u0026#39;s/ client-certificate-data: //g\u0026#39; | base64 -d | openssl x509 -noout -subject # 结果 subject= /CN=system:kube-scheduler 可以发现 KubeConfig 客户端证书为 kubernetes-admin User 但是不属于某个 Group。 # system:kube-scheduler 也是 Kubernetes 集群内部的设置的用户，Kubernetes 集群中也存在对应默认的 ClusterRole system:kube-scheduler 和 ClusterRoleBinding system:kube-scheduler 。 # # system:kube-scheduler ClusterRole 拥有细分的权限 $ kubectl get clusterrole system:kube-scheduler -o yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \u0026#34;true\u0026#34; creationTimestamp: \u0026#34;2023-05-10T06:49:27Z\u0026#34; labels: kubernetes.io/bootstrapping: rbac-defaults name: system:kube-scheduler resourceVersion: \u0026#34;115\u0026#34; uid: b1ddf98c-bdb9-4d4d-9af3-9db1c97b038a rules: - apiGroups: - \u0026#34;\u0026#34; - events.k8s.io resources: - events verbs: - create - patch - update - apiGroups: - coordination.k8s.io resources: - leases verbs: - create - apiGroups: - coordination.k8s.io resourceNames: - kube-scheduler resources: - leases verbs: - get - update - apiGroups: - \u0026#34;\u0026#34; resources: - endpoints verbs: - create ....... # 一些更细致的权限 -------------------------------------------------------------------------- # system:kube-scheduler ClusterRoleBinding 将 system:kube-scheduler ClusterRole 绑定到 system:kube-scheduler User $ kubectl get clusterrolebinding system:kube-scheduler -o yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \u0026#34;true\u0026#34; creationTimestamp: \u0026#34;2023-05-10T06:49:27Z\u0026#34; labels: kubernetes.io/bootstrapping: rbac-defaults name: system:kube-scheduler resourceVersion: \u0026#34;155\u0026#34; uid: a9b8a85e-bb5c-483c-a08e-51822ce84d7f roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole # ClusterRole name name: system:kube-scheduler subjects: - apiGroup: rbac.authorization.k8s.io kind: User # User name name: system:kube-scheduler 可以发现 Kube-scheduler 权限并不是向 Kubectl 那样拥有所有资源的所有操作权限，即使是内部组件，基于最小权限原则，Kubernetes 依然会为这两个用户只绑定必要的权限。 # Kube-controller-manager 和 Kube-scheduler 一样，只不过 Kube-controller-manager 的 User 是 system:kube-controller-manager ，其默认的 ClusterRole、ClusterRoleBinding 名称不同而已，可以根据 Kube-scheduler 自行验证~ # system: 前缀是 Kubernetes 保留的关键字，用于描述内置的系统用户和系统用户组，在 Kube-apiserver 启动时，会默认为这些用户绑定好对应的权限，具体参考官网[2]\nKubelet # Kubelet 同样也会访问 Kube-apiserver，Kubelet 使用其客户端证书与 Kube-apiserver 认证、鉴权。 # 使用 openssl 命令解析 Kubelet 客户端证书，查看证书的 O、CN # $ openssl x509 -noout -subject -in /var/lib/kubelet/pki/kubelet-client-current.pem # 结果 subject= /O=system:nodes/CN=system:node:master-172-31-97-104 可以发现 Kubelet 客户端证书为 system:node:master-172-31-97-104 User 且属于 system:nodes Group。 # 还是和上面一样, system:nodes 也是 Kubernetes 内置用户组，通过查看其默认的 ClusterRole、ClusterRoleBinding # $ kubectl get clusterrolebinding system:node -o yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \u0026#34;true\u0026#34; creationTimestamp: \u0026#34;2023-05-10T06:49:27Z\u0026#34; labels: kubernetes.io/bootstrapping: rbac-defaults name: system:node resourceVersion: \u0026#34;157\u0026#34; uid: 8ba46211-bdae-493f-8f5f-3386fe63ba29 roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:node 发现 system:node ClusterRoleBinding 并没有绑定 system:node:master-172-31-97-104 User 或者 system:nodes Group。官方文档也有介绍：https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/rbac/#core-component-roles # 实际上 Kubernetes 现在基于 Node Authorizer 来限制kubelet只能读取和修改本节点上的资源，并不是使用 RBAC 来鉴权。为了获得节点鉴权器的授权，Kubelet 必须使用一个凭证以表示它在 system:nodes 组中，用户名为 system:node:\u0026lt;nodeName\u0026gt;。这个凭证就是从 Kubelet 客户端证书获取，也就是上面的 O、CN # Node Authorizer 参考：https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/node/\nKube-apiserver # 前面都是在说其他组件访问 Kube-apiserver，然后 Kube-apiserver 对访问者进行授权，然而 Kube-apiserver 也会作为客户端去访问 Kubelet，例如：kubectl logs/exec，Kube-apiserver 都会去访问 Kubelet。Kubelet 作为服务端内部也是通过 TLS + RBAC 这种模式去认证、鉴权。 # 使用 openssl 命令解析 Kube-apiserver 客户端证书，查看证书的 O、CN # $ openssl x509 -noout -subject -in /etc/kubernetes/pki/apiserver-kubelet-client.crt # 结果 subject= /O=system:masters/CN=kube-apiserver-kubelet-client 可以发现 Kube-apiserver 访问 Kubelet 客户端证书为 kube-apiserver-kubelet-client User 且属于 system:masters Group。 # 在 Kubectl 章节介绍了 system:masters 属于内置用户组，且默认拥有超级权限，所以 Kube-apiserver 可以去访问 Kubelet 操作资源。 # ServiceAccount # 上面说的几种情况，都是根据 User 或者 Group 鉴别其是否拥有权限，ServiceAccount 和 User、Group 属于同一性质。 # 在 Kubernetes 集群内部，比如 Pod 需要访问 Kube-apiserver，就会使用其配置的 Service Account（没有配置，则使用默认) 与 Kube-apiserver 认证、鉴权。 # 通过一个例子来说明： # mysql 这个 Pod 配置 mysql-sa 这个 ServiceAccount # # pod 部分 yaml nodeName: master-172-31-97-104 preemptionPolicy: PreemptLowerPriority priority: 0 restartPolicy: Always schedulerName: default-scheduler securityContext: {} # serviceAccount 配置 serviceAccount: mysql-sa serviceAccountName: mysql-sa terminationGracePeriodSeconds: 10 tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master operator: Exists - effect: NoExecute key: node.kubernetes.io/not-ready operator: Exists tolerationSeconds: 300 - effect: NoExecute key: node.kubernetes.io/unreachable operator: Exists tolerationSeconds: 300 然后查看对应 Role，查看拥有的权限 # $ kubectl get role mysql-role -n test -o yaml apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: mysql-role namespace: test rules: - apiGroups: - \u0026#34;\u0026#34; resources: - configmaps verbs: - get - list - watch - create - update - patch - delete - apiGroups: - \u0026#34;\u0026#34; resources: - events verbs: - create - patch 查看 RoleBinding，然后会发现将上面的 Role 与 Pod 的 ServiceAccount 绑定。 # $ kubectl get rolebinding mysql-role -n test -o yaml apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: mysql-rolebinding namespace: test roleRef: apiGroup: rbac.authorization.k8s.io kind: Role # role name name: mysql-role subjects: - kind: ServiceAccount # serviceAccount name name: mysql-sa namespace: test 这样 mysql-sa ServiceAccount 就拥有了 Configmmap，Event 这两个资源的所有权限。 # 所以当 Pod 内部去访问 Kube-apiserver，实际上 Kube-apiserver 是根据其 ServiceAccount 来鉴定其拥有的权限，而不是 User 或者 Group。 # 总结 # 上面的鉴权原理，可以下面这张图做个总结 # ​ k8s-crt-auth.png\n每个组件都代表着不同的 K8S 角色与 Kube-apiserver 鉴权。 # 到这里整个 Kubernetes 证书都讲解完了，包括认证、鉴权。掌握了 Kubernetes 组件之间的调用关系，以及双向 TLS 认证 就可以理清证书的作用和关系，同时还需掌握证书内容的 O、CN、SANS 等字段作用，才能明白 Kubernetes 是如何根据证书 + RBAC 进行访问授权的。 # 最后有兴趣可以使用二进制部署一个 Kubernetes 集群，通过手动签发证书，来加深理解~ # 引用链接 # [1] 官网: https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/rbac/ [2] 官网: https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/rbac/#core-component-roles [3] Kubernetes 官方文档 Node Authorizer: https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/node/ [4] Kubernetes 官方文档 RBAC: https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/rbac/\n"},{"id":131,"href":"/docs/kubewharf-kubewharf/","title":"kubewharf 2024-08-02 17:41:32.72","section":"Docs","content":"深入云原生—基于 KubeWharf 深度剖析场景与解读。我们需要先了解一下 KubeWharf，可能很多人都感觉到有点陌生吧，下面我们来一起学习！\n🌰一.KubeWharf 详解 # KubeWharf 是字节跳动基础架构团队在对 Kubernetes 进行了大规模应用和不断优化增强之后的技术结晶。这是一套以 Kubernetes 为基础构建的分布式操作系统，由一组云原生组件构成，专注于提高系统的可扩展性、功能性、稳定性、可观测性、安全性等，以支持大规模多租集群、在离线混部、存储和机器学习云原生化等场景。\nKubeWharf 由以下项目组成：\n● KubeBrain 是一个高性能的 Kubernetes 元数据系统，用于存储和管理 Kubernetes 集群的元数据。\nKubeZoo 是一个轻量级的 Kubernetes 多租户网关，用于在 Kubernetes 集群之间进行安全隔离。\n● KubeGateway 是一个 Kubernetes 网关，用于在本地和云端 Kubernetes 集群之间进行通信。\n● Godel Scheduler 是一个高性能的 Kubernetes 调度器，用于在 Kubernetes 集群中智能地调度容器。\nKubeWharf 是一个分布式操作系统，由字节跳动基础架构团队在应用和优化增强 Kubernetes 之后创建。这个系统是一套以 Kubernetes 为基础构建的分布式操作系统，由一组云原生组件构成，专注于提高系统的可扩展性、功能性、稳定性、可观测性、安全性等，以支持大规模多租集群、在离线混部、存储和机器学习云原生化等场景。\n二.一个真实的 KubeWharf 应用场景 # 这是一个真实的应用场景哈，因为涉及业务，这里只能脱敏化的描述了。\n一家大型互联网公司某某 X 需要构建一个可扩展、稳定、安全的 Kubernetes 集群，用于支持其在线业务和离线业务。\n需求：\n● 需要支持数万个节点。\n● 需要提供丰富的功能，满足各种业务需求。\n● 需要经过大规模生产环境的验证。\n● 需要提供强大的可观测性功能。\n● 需要提供多种安全功能。\n最后的解决方案是：\n某某 X 使用了 KubeWharf 来构建其 Kubernetes 集群。\n操作步骤：\n● 部署 KubeBrain、KubeZoo、KubeGateway 和 Godel Scheduler。\n● 创建 Kubernetes 集群。\n● 部署业务应用。\n该公司使用 KubeWharf 成功构建了一个可扩展、稳定、安全的 Kubernetes 集群。该集群支持了该公司的在线业务和离线业务，满足了该公司的所有需求。\n我总结了一下 KubeWharf 在这个业务场景的优势：满足了该公司的所有需求。\n\\1. KubeWharf 可以扩展到数万个节点。\n\\2. KubeWharf 提供了丰富的功能，包括多租户、资源隔离、安全控制等。\n\\3. KubeWharf 经过了字节跳动大规模生产环境的验证。\n\\4. KubeWharf 提供了强大的可观测性功能，包括日志、监控、告警等。\n\\5. KubeWharf 提供了多种安全功能，包括访问控制、数据加密等。\n完全的满足了公司的业务需求！\n公司业务，我着重对这部分（逻辑时钟）有很深的影响，来具体的写一下：\n这块业务比较熟悉，对（逻辑时钟）这块记忆犹新。\n2.1KubeBrain 编译与启动学习记录 # KubeBrain 编译与启动这块我当时还特意的做了笔记，有所记录（比官网的介绍更加贴合我们开发者学习的角度上记录的)。\n编译启动命令：\nmake tikv./bin/kube-brain --key-prefix \u0026#34;/\u0026#34; --pd-addrs=127.0.0.1:2379 --port=3379 --peer-port=3380 --compatible-with-etcd=true 多个 KubeBrain 共用一个 TiKV 集群时，需要注意以下两个配置：\n● key-prefix 参数和 APIServer 对应的 APIServer 的 etcd-prefix 参数必须保持一致。 这是为了保证多个 KubeBrain 能够正确访问 TiKV 集群中的元数据。\n● compatible-with-etcd 参数必须设置为 true。 这是为了开启从节点的支持 txn 和 watch，从而实现对 etcd 功能的完全兼容。\n具体配置如下：\n## key-prefix 参数 key-prefix = \u0026#34;/kubebrain\u0026#34; ## compatible-with-etcd 参数 compatible-with-etcd = true KubeBrain 编译与启动只是整个使用流程中的第一步，接下来还需要进行以下操作：\n● 配置 KubeBrain：需要配置 KubeBrain 的各种参数，例如元数据存储、事件生成、事件流等。\n● 创建 Kubernetes 集群：需要创建 Kubernetes 集群，并将其加入 KubeBrain 的管理范围。\n● 部署 Kubernetes 应用：可以使用 KubeBrain 来部署 Kubernetes 应用，包括创建 Deployment、Service、Pod 等。\n● 管理 Kubernetes 集群：可以使用 KubeBrain 来管理 Kubernetes 集群，包括查看集群状态、修改集群配置等。\n比如说：\n配置 KubeBrain\n配置 KubeBrain 需要修改 KubeBrain 的配置文件，该文件位于 /etc/kubebrain/config.yaml。配置文件中包含了 KubeBrain 的各种参数，例如：\n● key-prefix：元数据存储的键前缀。\n● etcd-prefix：APIServer 对应的 APIServer 的 etcd-prefix 参数。\n● compatible-with-etcd：是否开启从节点的支持 txn 和 watch。\n创建 Kubernetes 集群\n可以使用 Kubernetes 官方提供的工具来创建 Kubernetes 集群，例如 kubeadm、minikube 等。\n部署 Kubernetes 应用\n可以使用 KubeBrain 提供的 API 或 CLI 来部署 Kubernetes 应用。\n管理 Kubernetes 集群\n可以使用 KubeBrain 提供的 UI 或 API 来管理 Kubernetes 集群。\n我们需要学习的：\n配置 KubeBrain\nYAML\nkey-prefix: \u0026#34;/kubebrain\u0026#34; etcd-prefix: \u0026#34;/kubebrain\u0026#34; compatible-with-etcd: true 创建 Kubernetes 集群\nkubeadm init 部署 Kubernetes 应用\nkubectl create deployment nginx --image=nginx 管理 Kubernetes 集群\nkubectl get nodes 三.KubeWharf 应用场景探索 # 我觉得 KubeWharf 应用场景是非常多，因为它的优势非常明显。\nKubeWharf 是一个开源分布式操作系统，基于 Kubernetes。它可以用于构建和管理各种 Kubernetes 集群，包括：\n● 大规模多租集群：KubeWharf 可以扩展到数万个节点，满足大规模多租集群的需求。\n● 在离线混部：KubeWharf 可以用于在本地和云端混合部署 Kubernetes 集群，满足在离线混部的需求。\n● 存储和机器学习云原生化：KubeWharf 可以用于云原生化存储和机器学习应用，满足存储和机器学习云原生化的需求。\n经过我的不断学习，KubeWharf 可以应用在以下几个场景：\n● 在线业务：KubeWharf 可以用于部署各种在线业务应用，例如 Web 应用、后端服务等。\n● 离线业务：KubeWharf 可以用于部署各种离线业务应用，例如数据分析、机器学习等。\n● 云原生应用：KubeWharf 可以用于部署各种云原生应用，例如容器应用、微服务应用等。\n● 混合云：KubeWharf 可以用于在本地和云端混合部署 Kubernetes 集群。\n● 边缘计算：KubeWharf 可以用于部署边缘计算 Kubernetes 集群。\nKubeWharf 是一个强大的工具，可用于构建和管理各种 Kubernetes 集群。它可以满足各种场景的需求，是构建可扩展、功能强大、稳定、可观测和安全的 Kubernetes 集群的理想选择。\n🌰四.心得与总结 # 🐹4.1 心得 # 我深入探讨了云原生领域中的一个重要工具——KubeWharf，并通过详细解读其组成部分和一个真实的应用场景，展示了它在构建可扩展、功能强大、稳定、可观测和安全的 Kubernetes 集群方面的优势。以下是我的心得：\n● KubeWharf 基于 Kubernetes，专注于提高系统的各项性能，包括可扩展性、功能性、稳定性、可观测性和安全性。\n● 它由多个项目组成，如 KubeBrain、KubeZoo、KubeGateway 和 Godel Scheduler，每个项目都有特定的功能，共同构建了一个强大的分布式操作系统。\n我通过一个真实的应用场景，文章展示了 KubeWharf 在构建大型、可扩展、稳定、安全的 Kubernetes 集群方面的成功经验。\n强调了 KubeWharf 的优势，包括可扩展性、丰富的功能、经过大规模验证、强大的可观测性和多种安全功能。这个案例为读者提供了一个具体的实例，说明 KubeWharf 在实际业务中的应用效果。\n我对 KubeWharf 的一个关键应用领域——元数据存储系统进行了深入剖析。特别是对逻辑时钟的影响，我详细描述了元数据存储系统如何监听逻辑时钟，以保证系统的数据最终一致性。这一部分的具体实现细节使读者更深入地理解了 KubeWharf 的内部机制。\n我通过对 KubeBrain 的编译与启动以及 KubeWharf 的应用场景进行探索，为大家提供了进一步学习的方向。通过展示编译与启动命令以及相关配置，大家可以更好地理解如何在实际中使用 KubeWharf。同时，对 KubeWharf 的应用场景进行了分类，涵盖了在线业务、离线业务、云原生应用、混合云和边缘计算等多个领域，强调了 KubeWharf 的通用性和灵活性。\n🐹4.1 总结 # 这篇文章深入剖析了云原生领域中基于 KubeWharf 的分布式操作系统。通过对 KubeWharf 的构建背景、组成部分和真实应用场景的详细介绍，读者能够深刻理解这一工具在构建强大、可扩展、稳定的 Kubernetes 集群方面的重要性。我通过清晰的步骤和实例演示，使得即使是对 KubeWharf 陌生的读者也能够快速入门🐹。\n在实际应用场景中，KubeWharf 在大规模多租户、离线混部、存储和机器学习云原生化等领域的优越表现。通过一家大型互联网公司的案例，读者不仅了解了 KubeWharf 的具体应用流程，还明白了它在多个方面的优势，包括扩展性、功能丰富性、稳定性、可观测性和安全性。这为读者提供了一个清晰的实际应用场景，帮助他们更好地理解 KubeWharf 的实际作用。\n对于 KubeWharf 中一个关键领域——元数据存储系统的深入剖析，更是为读者提供了深入了解 KubeWharf 内部工作机制的机会。透过对逻辑时钟的具体影响和实现细节的描述，读者能够更好地理解元数据存储系统的工作原理，从而加深对 KubeWharf 整体架构的理解。\n通过对 KubeBrain 编译与启动的学习记录以及对 KubeWharf 应用场景的探索，文章为读者提供了进一步学习和应用的指导。这些内容不仅仅是理论性的介绍，更是实际操作的具体步骤和场景应用的分类，使得读者能够更好地掌握 KubeWharf 的使用和适用范围。\n作为一个研究云原生多年的我来说，我是非常的看好 KubeWharf，我相信 KubeWharf 随着版本的优化迭代，会变的越来越好的！！！\n"},{"id":132,"href":"/docs/linux-awk%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E5%99%A8-8%E4%B8%AA%E6%A1%88%E4%BE%8B-linuxawk-wen-ben-chu-li-qi-8-ge-an-li/","title":"linux awk文本处理器 8个案例 2024-08-02 17:46:53.603","section":"Docs","content":"AWK是Linux下一个非常强大的文本处理工具，能够分析各种复杂文件。\n以下是8个常见的AWK文本处理案例，可助你提升AWK技能！\n案例一：分析Nginx访问日志\n统计访问IP次数： awk \u0026#39;{a[$1]++}END{for(v in a)print v,a[v]}\u0026#39; access.log 统计访问IP次数： awk \u0026#39;{a[$1]++}END{for(v in a)print v,a[v]}\u0026#39; access.log 统计访问访问大于100次的IP： awk \u0026#39;{a[$1]++}END{for(v in a){if(a[v]\u0026gt;100)print v,a[v]}}\u0026#39; access.log 统计访问IP次数并排序取前10： awk \u0026#39;{a[$1]++}END{for(v in a)print v,a[v]|\u0026#34;sort -k2 -nr |head -10\u0026#34;}\u0026#39; access.log 统计时间段访问最多的IP： awk \u0026#39;$4\u0026gt;=\u0026#34;[02/Jan/2017:00:02:00\u0026#34; \u0026amp;\u0026amp; $4\u0026lt;=\u0026#34;[02/Jan/2017:00:03:00\u0026#34;{a[$1]++}END{for(v in a)print v,a[v]}\u0026#39; access.log 统计上一分钟访问量： date=$(date -d \u0026#39;-1 minute\u0026#39; +%d/%d/%Y:%H:%M) awk -vdate=$date \u0026#39;$4~date{c++}END{print c}\u0026#39; access.log 统计访问最多的10个页面： awk \u0026#39;{a[$7]++}END{for(v in a)print v,a[v]|\u0026#34;sort -k1 -nr|head -n10\u0026#34;}\u0026#39; access.log 统计每个URL数量和返回内容总大小: awk \u0026#39;{a[$7]++;size[$7]+=$10}END{for(v in a)print a[v],v,size[v]}\u0026#39; access.log 统计每个IP访问状态码数量： awk \u0026#39;{a[$1\u0026#34; \u0026#34;$9]++}END{for(v in a)print v,a[v]}\u0026#39; access.log 统计访问IP是404状态次数： awk \u0026#39;{if($9~/404/)a[$1\u0026#34; \u0026#34;$9]++}END{for(i in a)print v,a[v]}\u0026#39; access.log 案例二：两个文件差异对比\n生成a和b两个文件，用于测试：\n$ seq 1 5 \u0026gt; a $ seq 3 7 \u0026gt; b 找出b文件在a文件相同记录：\n方法1： awk \u0026#39;FNR==NR{a[$0];next}{if($0 in a)print $0}\u0026#39; a b 3 4 5 awk \u0026#39;FNR==NR{a[$0];next}{if($0 in a)print FILENAME,$0}\u0026#39; a b b 3 b 4 b 5 awk \u0026#39;FNR==NR{a[$0]}NR\u0026gt;FNR{if($0 ina)print $0}\u0026#39; a b 3 4 5 awk \u0026#39;FNR==NR{a[$0]=1;next}(a[$0]==1)\u0026#39; a b # a[$0]是通过b文件每行获取值，如果是1说明有 awk \u0026#39;FNR==NR{a[$0]=1;next}{if(a[$0]==1)print}\u0026#39; a b 3 4 5 方法2： awk \u0026#39;FILENAME==\u0026#34;a\u0026#34;{a[$0]}FILENAME==\u0026#34;b\u0026#34;{if($0 in a)print $0}\u0026#39; a b 3 4 5 4 5 方法3： awk \u0026#39;ARGIND==1{a[$0]=1}ARGIND==2 \u0026amp;\u0026amp; a[$0]==1\u0026#39; a b 3 4 5 找出b文件在a文件不同记录：\n方法1： awk \u0026#39;FNR==NR{a[$0];next}!($0 in a)\u0026#39; a b 6 7 awk \u0026#39;FNR==NR{a[$0]=1;next}(a[$0]!=1)\u0026#39; a b 或 awk \u0026#39;FNR==NR{a[$0]=1;next}{if(a[$0]!=1)print}\u0026#39; a b 6 7 方法2： awk \u0026#39;FILENAME==\u0026#34;a\u0026#34;{a[$0]=1}FILENAME==\u0026#34;b\u0026#34; \u0026amp;\u0026amp; a[$0]!=1\u0026#39; a b 方法3： awk \u0026#39;ARGIND==1{a[$0]=1}ARGIND==2 \u0026amp;\u0026amp; a[$0]!=1\u0026#39; a b 案例三：合并两个文件 # 生成a和b两个文件，用于测试：\n$ cat a zhangsan 20 lisi 23 wangwu 29 $ cat b zhangsan man lisi woman wangwu ma 将a文件合并到b文件：\n方法1： awk \u0026#39;FNR==NR{a[$1]=$0;next}{print a[$1],$2}\u0026#39; a b zhangsan 20 man lisi 23 woman wangwu 29 man 方法2： awk \u0026#39;FNR==NR{a[$1]=$0}NR\u0026gt;FNR{print a[$1],$2}\u0026#39; a b zhangsan 20 man lisi 23 woman wangwu 29 man 将a文件相同IP的服务名合并：\n$ cat a 192.168.1.1: httpd 192.168.1.1: tomcat 192.168.1.2: httpd 192.168.1.2: postfix 192.168.1.3: mysqld 192.168.1.4: httpd awk \u0026#39;BEGIN{FS=\u0026#34;:\u0026#34;;OFS=\u0026#34;:\u0026#34;}{a[$1]=a[$1] $2}END{for(v in a)print v,a[v]}\u0026#39; a 192.168.1.4: httpd 192.168.1.1: httpd tomcat 192.168.1.2: httpd postfix 192.168.1.3: mysql 解读：\n数组a存储是$1=a[$1] $2，第一个a[$1]是以第一个字段为下标，值是a[$1] $2，也就是$1=a[$1] $2，值的a[$1]是用第一个字段为下标获取对应的值，但第一次数组a还没有元素，那么a[$1]是空值，此时数组存储是192.168.1.1=httpd，再遇到192.168.1.1时，a[$1]通过第一字段下标获得上次数组的httpd，把当前处理的行第二个字段放到上一次同下标的值后面，作为下标192.168.1.1的新值。此时数组存储是192.168.1.1=httpd tomcat。每次遇到相同的下标（第一个字段）就会获取上次这个下标对应的值与当前字段并作为此下标的新值。\n案例四：将第一列合并到一行\n$ cat file 1 2 3 4 5 6 7 8 9 awk \u0026#39;{for(i=1;i\u0026lt;=NF;i++)a[i]=a[i]$i\u0026#34; \u0026#34;}END{for(vin a)print a[v]}\u0026#39; file 1 4 7 2 5 8 3 6 解读：\nfor循环是遍历每行的字段，NF等于3，循环3次。\n读取第一行时：\n第一个字段：a[1]=a[1]1\u0026quot; \u0026quot; 值a[1]还未定义数组，下标也获取不到对应的值，所以为空，因此a[1]=1 。\n第二个字段：a[2]=a[2]2\u0026quot; \u0026quot; 值a[2]数组a已经定义，但没有2这个下标，也获取不到对应的值，为空，因此a[2]=2 。\n第三个字段：a[3]=a[3]3\u0026quot; \u0026quot; 值a[2]与上面一样，为空,a[3]=3 。\n读取第二行时：\n第一个字段：a[1]=a[1]4\u0026quot; \u0026quot; 值a[2]获取数组a的2为下标对应的值，上面已经有这个下标了，对应的值是1，因此a[1]=1 4\n第二个字段：a[2]=a[2]5\u0026quot; \u0026quot; 同上，a[2]=2 5\n第三个字段：a[3]=a[3]6\u0026quot; \u0026quot; 同上，a[2]=3 6\n读取第三行时处理方式同上，数组最后还是三个下标，分别是1=1 4 7，2=2 5 8，3=36 9。最后for循环输出所有下标值。\n案例五：字符串拆分\n方法1： echo \u0026#34;hello\u0026#34; |awk -F \u0026#39;\u0026#39;\u0026#39;{for(i=1;i\u0026lt;=NF;i++)print $i}\u0026#39; h e l l o 方法2： echo \u0026#34;hello\u0026#34; |awk \u0026#39;{split($0,a,\u0026#34;\u0026#39;\u0026#39;\u0026#34;);for(v in a)print a[v]}\u0026#39; l o h e l 案例六：统计出现的次数 # 统计字符串中每个字母出现的次数：\necho \u0026#34;a.b.c,c.d.e\u0026#34; |awk -F\u0026#39;[.,]\u0026#39; \u0026#39;{for(i=1;i\u0026lt;=NF;i++)a[$i]++}END{for(v in a)print v,a[v]}\u0026#39; a 1 b 1 c 2 d 1 e 1 案例七：获取某列数字最大数 # $ cat a a b 1 c d 2 e f 3 g h 3 i j 2 获取第三字段最大值： awk \u0026#39;BEGIN{max=0}{if($3\u0026gt;max)max=$3}END{print max}\u0026#39; a 3 打印第三字段最大行： awk \u0026#39;BEGIN{max=0}{a[$0]=$3;if($3\u0026gt;max)max=$3}END{for(v in a)if(a[v]==max)print v}\u0026#39;a g h 3 e f 3 案例八：去除文本第一行和最后一行\nseq 5 |awk \u0026#39;NR\u0026gt;2{print s}{s=$0}\u0026#39; 2 3 4 解读：\n读取第一行，NR=1，不执行print s，s=1\n读取第二行，NR=2，不执行print s，s=2 （大于为真）\n读取第三行，NR=3，执行print s，此时s是上一次p赋值内容2，s=3\n最后一行，执行print s，打印倒数第二行，s=最后一行。\n掌握这些 AWK 案例，你将在 Linux 下处理文本无敌了！\n"},{"id":133,"href":"/docs/linux-%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E5%A4%A7%E5%85%A8-linux-xing-neng-you-hua-da-quan/","title":"Linux 性能优化大全 2024-04-03 15:09:06.854","section":"Docs","content":"性能优化\n性能指标 # 高并发和响应快对应着性能优化的两个核心指标：吞吐和延时 # 应用负载角度：直接影响了产品终端的用户体验 # 系统资源角度：资源使用率、饱和度等 # 性能问题的本质就是系统资源已经到达瓶颈，但请求的处理还不够快，无法支撑更多的请求。性能分析实际上就是找出应用或系统的瓶颈，设法去避免或缓解它们。 # 选择指标评估应用程序和系统性能 # 为应用程序和系统设置性能目标 # 进行性能基准测试 # 性能分析定位瓶颈 # 性能监控和告警 # 对于不同的性能问题要选取不同的性能分析工具。下面是常用的Linux Performance Tools以及对应分析的性能问题类型。 # 到底应该怎么理解”平均负载” # 平均负载：单位时间内，系统处于可运行状态和不可中断状态的平均进程数，也就是平均活跃进程数。它和我们传统意义上理解的CPU使用率并没有直接关系。 # 其中不可中断进程是正处于内核态关键流程中的进程（如常见的等待设备的I/O响应）。不可中断状态实际上是系统对进程和硬件设备的一种保护机制。 # 平均负载多少时合理 # 实际生产环境中将系统的平均负载监控起来，根据历史数据判断负载的变化趋势。当负载存在明显升高趋势时，及时进行分析和调查。当然也可以当设置阈值（如当平均负载高于CPU数量的70%时） # 现实工作中我们会经常混淆平均负载和CPU使用率的概念，其实两者并不完全对等： # CPU 密集型进程，大量 CPU 使用会导致平均负载升高，此时两者一致 # I/O 密集型进程，等待 I/O 也会导致平均负载升高，此时 CPU 使用率并不一定高 # 大量等待 CPU 的进程调度会导致平均负载升高，此时 CPU 使用率也会比较高 # 平均负载高时可能是 CPU 密集型进程导致，也可能是 I/O 繁忙导致。具体分析时可以结合 mpstat/pidstat 工具辅助分析负载来源。 # CPU # CPU上下文切换(上) # CPU 上下文切换，就是把前一个任务的 CPU 上下文（CPU 寄存器和 PC）保存起来，然后加载新任务的上下文到这些寄存器和程序计数器，最后再跳转到程序计数器所指的位置，运行新任务。其中，保存下来的上下文会存储在系统内核中，待任务重新调度执行时再加载，保证原来的任务状态不受影响。 # 按照任务类型，CPU 上下文切换分为： # 进程上下文切换 # 线程上下文切换 # 中断上下文切换 # 进程上下文切换 # Linux 进程按照等级权限将进程的运行空间分为内核空间和用户空间。从用户态向内核态转变时需要通过系统调用来完成。 # 一次系统调用过程其实进行了两次 CPU 上下文切换： # CPU 寄存器中用户态的指令位置先保存起来，CPU 寄存器更新为内核态指令的位置，跳转到内核态运行内核任务； # 系统调用结束后，CPU 寄存器恢复原来保存的用户态数据，再切换到用户空间继续运行。 # 系统调用过程中并不会涉及虚拟内存等进程用户态资源，也不会切换进程。和传统意义上的进程上下文切换不同。因此系统调用通常称为特权模式切换。 # 进程是由内核管理和调度的，进程上下文切换只能发生在内核态。因此相比系统调用来说，在保存当前进程的内核状态和CPU寄存器之前，需要先把该进程的虚拟内存，栈保存下来。再加载新进程的内核态后，还要刷新进程的虚拟内存和用户栈。 # 进程只有在调度到CPU上运行时才需要切换上下文，有以下几种场景：CPU时间片轮流分配，系统资源不足导致进程挂起，进程通过sleep函数主动挂起，高优先级进程抢占时间片，硬件中断时CPU上的进程被挂起转而执行内核中的中断服务。 # 线程上下文切换 # 线程上下文切换分为两种： # 前后线程同属于一个进程，切换时虚拟内存资源不变，只需要切换线程的私有数据，寄存器等； # 前后线程属于不同进程，与进程上下文切换相同。 # 同进程的线程切换消耗资源较少，这也是多线程的优势。 # 中断上下文切换 # 中断上下文切换并不涉及到进程的用户态，因此中断上下文只包括内核态中断服务程序执行所必须的状态（CPU寄存器，内核堆栈，硬件中断参数等）。 # 中断处理优先级比进程高，所以中断上下文切换和进程上下文切换不会同时发生 # CPU上下文切换(下) # 通过 vmstat 可以查看系统总体的上下文切换情况\nvmstat 5 #每隔5s输出一组数据procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 1 0 0 103388 145412 511056 0 0 18 60 1 1 2 1 96 0 0 0 0 0 103388 145412 511076 0 0 0 2 450 1176 1 1 99 0 0 0 0 0 103388 145412 511076 0 0 0 8 429 1135 1 1 98 0 0 0 0 0 103388 145412 511076 0 0 0 0 431 1132 1 1 98 0 0 0 0 0 103388 145412 511076 0 0 0 10 467 1195 1 1 98 0 0 1 0 0 103388 145412 511076 0 0 0 2 426 1139 1 0 99 0 0 4 0 0 95184 145412 511108 0 0 0 74 500 1228 4 1 94 0 0 0 0 0 103512 145416 511076 0 0 0 455 723 1573 12 3 83 2 0 cs （context switch） 每秒上下文切换次数 # in （interrupt） 每秒中断次数 # r （runnning or runnable）就绪队列的长度，正在运行和等待CPU的进程数 # b （Blocked） 处于不可中断睡眠状态的进程数 # 要查看每个进程的详细情况，需要使用pidstat来查看每个进程上下文切换情况 # pidstat -w 514时51分16秒 UID PID cswch/s nvcswch/s Command14时51分21秒 0 1 0.80 0.00 systemd14时51分21秒 0 6 1.40 0.00 ksoftirqd/014时51分21秒 0 9 32.67 0.00 rcu_sched14时51分21秒 0 11 0.40 0.00 watchdog/014时51分21秒 0 32 0.20 0.00 khugepaged14时51分21秒 0 271 0.20 0.00 jbd2/vda1-814时51分21秒 0 1332 0.20 0.00 argusagent14时51分21秒 0 5265 10.02 0.00 AliSecGuard14时51分21秒 0 7439 7.82 0.00 kworker/0:214时51分21秒 0 7906 0.20 0.00 pidstat14时51分21秒 0 8346 0.20 0.00 sshd14时51分21秒 0 20654 9.82 0.00 AliYunDun14时51分21秒 0 25766 0.20 0.00 kworker/u2:114时51分21秒 0 28603 1.00 0.00 python3 cswch 每秒自愿上下文切换次数（进程无法获取所需资源导致的上下文切换） # nvcswch 每秒非自愿上下文切换次数（时间片轮流等系统强制调度） # vmstat 1 1 #新终端观察上下文切换情况此时发现cs数据明显升高，同时观察其他指标：r列：远超系统CPU个数，说明存在大量CPU竞争us和sy列：sy列占比80%，说明CPU主要被内核占用in列：中断次数明显上升，说明中断处理也是潜在问题 说明运行/等待CPU的进程过多，导致大量的上下文切换，上下文切换导致系统的CPU占用率高 # pidstat -w -u 1 #查看到底哪个进程导致的问题 从结果中看出是 sysbench 导致 CPU 使用率过高，但是 pidstat 输出的上下文次数加起来也并不多。分析 sysbench 模拟的是线程的切换，因此需要在 pidstat 后加 -t 参数查看线程指标。 # 另外对于中断次数过多，我们可以通过 /proc/interrupts 文件读取 # watch -d cat /proc/interrupts 发现次数变化速度最快的是重调度中断（RES），该中断用来唤醒空闲状态的CPU来调度新的任务运行。分析还是因为过多任务的调度问题，和上下文切换分析一致。 # 某个应用的CPU使用率达到100%，怎么办？\nLinux作为多任务操作系统，将CPU时间划分为很短的时间片，通过调度器轮流分配给各个任务使用。为了维护CPU时间，Linux通过事先定义的节拍率，触发时间中断，并使用全局变了jiffies记录开机以来的节拍数。时间中断发生一次该值+1. # CPU使用率，除了空闲时间以外的其他时间占总CPU时间的百分比。可以通过/proc/stat中的数据来计算出CPU使用率。因为/proc/stat时开机以来的节拍数累加值，计算出来的是开机以来的平均CPU使用率，一般意义不大。可以间隔取一段时间的两次值作差来计算该段时间内的平均CPU使用率。性能分析工具给出的都是间隔一段时间的平均CPU使用率，要注意间隔时间的设置。 # CPU使用率可以通过top 或 ps来查看。分析进程的CPU问题可以通过perf，它以性能事件采样为基础，不仅可以分析系统的各种事件和内核性能，还可以用来分析指定应用程序的性能问题。 # perf top / perf record / perf report （-g 开启调用关系的采样） # sudo docker run --name nginx -p 10000:80 -itd feisky/nginxsudo docker run --name phpfpm -itd --network container:nginx feisky/php-fpm ab -c 10 -n 100 http://XXX.XXX.XXX.XXX:10000/ #测试Nginx服务性能 发现此时每秒可承受请求给长少，此时将测试的请求数从100增加到10000。在另外一个终端运行top查看每个CPU的使用率。发现系统中几个php-fpm进程导致CPU使用率骤升。 # 接着用perf来分析具体是php-fpm中哪个函数导致该问题。 # perf top -g -p XXXX #对某一个php-fpm进程进行分析 发现其中 sqrt 和 add_function 占用 CPU 过多， 此时查看源码找到原来是sqrt中在发布前没有删除测试代码段，存在一个百万次的循环导致。将该无用代码删除后发现nginx负载能力明显提升 # 系统的CPU使用率很高，为什么找不到高CPU的应用？\nsudo docker run --name nginx -p 10000:80 -itd feisky/nginx:spsudo docker run --name phpfpm -itd --network container:nginx feisky/php-fpm:spab -c 100 -n 1000 http://XXX.XXX.XXX.XXX:10000/ #并发100个请求测试 实验结果中每秒请求数依旧不高，我们将并发请求数降为5后，nginx负载能力依旧很低。 # 此时用top和pidstat发现系统CPU使用率过高，但是并没有发现CPU使用率高的进程。 # 出现这种情况一般时我们分析时遗漏的什么信息，重新运行top命令并观察一会。发现就绪队列中处于Running状态的进行过多，超过了我们的并发请求次数5. 再仔细查看进程运行数据，发现nginx和php-fpm都处于sleep状态，真正处于运行的却是几个stress进程。 # 下一步就利用pidstat分析这几个stress进程，发现没有任何输出。用ps aux交叉验证发现依旧不存在该进程。说明不是工具的问题。再top查看发现stress进程的进程号变化了，此时有可能时以下两种原因导致： # 进程不停的崩溃重启（如段错误/配置错误等），此时进程退出后可能又被监控系统重启； # 短时进程导致，即其他应用内部通过 exec 调用的外面命令，这些命令一般只运行很短时间就结束，很难用top这种间隔较长的工具来发现 # 可以通过pstree来查找 stress 的父进程，找出调用关系。 # pstree | grep stress 发现是php-fpm调用的该子进程，此时去查看源码可以看出每个请求都会调用一个stress命令来模拟I/O压力。之前top显示的结果是CPU使用率升高，是否真的是由该stress命令导致的，还需要继续分析。代码中给每个请求加了verbose=1的参数后可以查看stress命令的输出，在中断测试该命令结果显示stress命令运行时存在因权限问题导致的文件创建失败的bug。 # 此时依旧只是猜测，下一步继续通过perf工具来分析。性能报告显示确实时stress占用了大量的CPU，通过修复权限问题来优化解决即可。 # 系统中出现大量不可中断进程和僵尸进程怎么办？\n进程状态\nR Running/Runnable，表示进程在CPU的就绪队列中，正在运行或者等待运行； D Disk Sleep，不可中断状态睡眠，一般表示进程正在跟硬件交互，并且交互过程中不允许被其他进程中断； Z Zombie，僵尸进程，表示进程实际上已经结束，但是父进程还没有回收它的资源； S Interruptible Sleep，可中断睡眠状态，表示进程因为等待某个事件而被系统挂起，当等待事件发生则会被唤醒并进入R状态； I Idle，空闲状态，用在不可中断睡眠的内核线程上。该状态不会导致平均负载升高； T Stop/Traced，表示进程处于暂停或跟踪状态（SIGSTOP/SIGCONT， GDB调试）； X Dead，进程已经消亡，不会在top/ps中看到。\n对于不可中断状态，一般都是在很短时间内结束，可忽略。但是如果系统或硬件发生故障，进程可能会保持不可中断状态很久，甚至系统中出现大量不可中断状态，此时需注意是否出现了I/O性能问题。 # 僵尸进程一般多进程应用容易遇到，父进程来不及处理子进程状态时子进程就提前退出，此时子进程就变成了僵尸进程。大量的僵尸进程会用尽PID进程号，导致新进程无法建立。 # 磁盘O_DIRECT问题\nsudo docker run --privileged --name=app -itd feisky/app:iowaitps aux | grep \u0026#39;/app\u0026#39; 可以看到此时有多个app进程运行，状态分别时Ss+和D+。其中后面s表示进程是一个会话的领导进程，+号表示前台进程组。 # 其中进程组表示一组相互关联的进程，子进程是父进程所在组的组员。会话指共享同一个控制终端的一个或多个进程组。 # 用top查看系统资源发现：1）平均负载在逐渐增加，且1分钟内平均负载达到了CPU个数，说明系统可能已经有了性能瓶颈；2）僵尸进程比较多且在不停增加；3）us和sys CPU使用率都不高，iowait却比较高；4）每个进程CPU使用率也不高，但有两个进程处于D状态，可能在等待IO。 # 分析目前数据可知：iowait过高导致系统平均负载升高，僵尸进程不断增长说明有程序没能正确清理子进程资源。 # 用dstat来分析，因为它可以同时查看CPU和I/O两种资源的使用情况，便于对比分析。 # dstat 1 10 #间隔1秒输出10组数据 可以看到当wai（iowait）升高时磁盘请求read都会很大，说明iowait的升高和磁盘的读请求有关。接下来分析到底时哪个进程在读磁盘。 # 之前 Top 查看的处于 D 状态的进程号，用 pidstat -d -p XXX 展示进程的 I/O 统计数据。发现处于 D 状态的进程都没有任何读写操作。在用 pidstat -d 查看所有进程的 I/O统计数据，看到 app 进程在进行磁盘读操作，每秒读取 32MB 的数据。进程访问磁盘必须使用系统调用处于内核态，接下来重点就是找到app进程的系统调用。 # sudo strace -p XXX #对app进程调用进行跟踪 报错没有权限，因为已经时 root 权限了。所以遇到这种情况，首先要检查进程状态是否正常。ps 命令查找该进程已经处于Z状态，即僵尸进程。 # 这种情况下top pidstat之类的工具无法给出更多的信息，此时像第5篇一样，用 perf record -d和 perf report 进行分析，查看app进程调用栈。 # 看到 app 确实在通过系统调用 sys_read() 读取数据，并且从 new_sync_read和 blkdev_direct_IO看出进程时进行直接读操作，请求直接从磁盘读，没有通过缓存导致iowait升高。 # 通过层层分析后，root cause 是 app 内部进行了磁盘的直接I/O。然后定位到具体代码位置进行优化即可。 # 僵尸进程\n上述优化后 iowait 显著下降，但是僵尸进程数量仍旧在增加。首先要定位僵尸进程的父进程，通过pstree -aps XXX，打印出该僵尸进程的调用树，发现父进程就是app进程。 # 查看app代码，看看子进程结束的处理是否正确（是否调用wait()/waitpid()，有没有注册SIGCHILD信号的处理函数等）。 # 碰到iowait升高时，先用dstat pidstat等工具确认是否存在磁盘I/O问题，再找是哪些进程导致I/O，不能用strace直接分析进程调用时可以通过perf工具分析。 # 对于僵尸问题，用pstree找到父进程，然后看源码检查子进程结束的处理逻辑即可。 # CPU性能指标 # CPU使用率\n用户CPU使用率, 包括用户态(user)和低优先级用户态(nice). 该指标过高说明应用程序比较繁忙. # 系统CPU使用率, CPU在内核态运行的时间百分比(不含中断). 该指标高说明内核比较繁忙. # 等待I/O的CPU使用率, iowait, 该指标高说明系统与硬件设备I/O交互时间比较长. # 软/硬中断CPU使用率, 该指标高说明系统中发生大量中断. # steal CPU / guest CPU, 表示虚拟机占用的CPU百分比. # 平均负载\n理想情况下平均负载等于逻辑CPU个数,表示每个CPU都被充分利用. 若大于则说明系统负载较重. # 进程上下文切换\n包括无法获取资源的自愿切换和系统强制调度时的非自愿切换. 上下文切换本身是保证Linux正常运行的一项核心功能. 过多的切换则会将原本运行进程的CPU时间消耗在寄存器,内核占及虚拟内存等数据保存和恢复上 # CPU缓存命中率\nCPU缓存的复用情况,命中率越高性能越好. 其中L1/L2常用在单核,L3则用在多核中 # 性能工具 # 平均负载案例\n先用uptime查看系统平均负载 # 判断负载在升高后再用mpstat和pidstat分别查看每个CPU和每个进程CPU使用情况.找出导致平均负载较高的进程. # 上下文切换案例\n先用vmstat查看系统上下文切换和中断次数 # 再用pidstat观察进程的自愿和非自愿上下文切换情况 # 最后通过pidstat观察线程的上下文切换情况 # 进程CPU使用率高案例\n先用top查看系统和进程的CPU使用情况,定位到进程 # 再用perf top观察进程调用链,定位到具体函数 # 系统CPU使用率高案例\n先用top查看系统和进程的CPU使用情况,top/pidstat都无法找到CPU使用率高的进程 # 重新审视top输出 # 从CPU使用率不高,但是处于Running状态的进程入手 # perf record/report发现短时进程导致 (execsnoop工具) # 不可中断和僵尸进程案例\n先用top观察iowait升高,发现大量不可中断和僵尸进程 # strace无法跟踪进程系统调用 # perf分析调用链发现根源来自磁盘直接I/O # 软中断案例\ntop观察系统软中断CPU使用率高 # 查看/proc/softirqs找到变化速率较快的几种软中断 # sar命令发现是网络小包问题 # tcpdump找出网络帧的类型和来源，确定SYN FLOOD攻击导致 # 根据不同的性能指标来找合适的工具： # 先运行几个支持指标较多的工具，如 top/vmstat/pidstat，根据它们的输出可以得出是哪种类型的性能问题。定位到进程后再用 strace/perf 分析调用情况进一步分析。如果是软中断导致用 /proc/softirqs\nCPU优化 # 应用程序优化\n编译器优化：编译阶段开启优化选项，如gcc -O2 # 算法优化 # 异步处理：避免程序因为等待某个资源而一直阻塞，提升程序的并发处理能力。(将轮询替换为事件通知) # 多线程代替多进程：减少上下文切换成本 # 善用缓存：加快程序处理速度 # 系统优化\nCPU绑定：将进程绑定要1个/多个CPU上，提高CPU缓存命中率，减少CPU调度带来的上下文切换 # CPU独占：CPU亲和性机制来分配进程 # 优先级调整：使用nice适当降低非核心应用的优先级 # 为进程设置资源显示: cgroups设置使用上限，防止由某个应用自身问题耗尽系统资源 # NUMA优化: CPU尽可能访问本地内存 # 中断负载均衡: irpbalance，将中断处理过程自动负载均衡到各个CPU上 # TPS、QPS、系统吞吐量的区别和理解\nQPS（TPS） # 并发数 # 响应时间 # QPS（TPS）=并发数/平均相应时间 # 用户请求服务器 # 服务器内部处理 # 服务器返回给客户 # QPS 类似 TPS，但是对于一个页面的访问形成一个 TPS，但是一次页面请求可能包含多次对服务器的请求，可能计入多次 QPS # QPS（Queries Per Second）每秒查询率，一台服务器每秒能够响应的查询次数. # TPS（Transactions Per Second）每秒事务数，软件测试的结果. # 系统吞吐量，包括几个重要参数： # QPS(TPS) # 并发数 # 响应时间 # QPS(TPS)=并发数/平均相应时间 # 内存 # Linux内存是怎么工作的 # 内存映射 # 大多数计算机用的主存都是动态随机访问内存(DRAM)，只有内核才可以直接访问物理内存。Linux内核给每个进程提供了一个独立的虚拟地址空间，并且这个地址空间是连续的。这样进程就可以很方便的访问内存(虚拟内存)。 # 虚拟地址空间的内部分为内核空间和用户空间两部分，不同字长的处理器地址空间的范围不同。32位系统内核空间占用1G，用户空间占3G。64位系统内核空间和用户空间都是128T，分别占内存空间的最高和最低处，中间部分为未定义。 # 并不是所有的虚拟内存都会分配物理内存，只有实际使用的才会。分配后的物理内存通过内存映射管理。为了完成内存映射，内核为每个进程都维护了一个页表，记录虚拟地址和物理地址的映射关系。页表实际存储在CPU的内存管理单元MMU中，处理器可以直接通过硬件找出要访问的内存。 # 当进程访问的虚拟地址在页表中查不到时，系统会产生一个缺页异常，进入内核空间分配物理内存，更新进程页表，再返回用户空间恢复进程的运行。 # MMU以页为单位管理内存，页大小4KB。为了解决页表项过多问题Linux提供了多级页表和HugePage的机制。 # 虚拟内存空间分布 # 用户空间内存从低到高是五种不同的内存段： # 只读段 代码和常量等 # 数据段 全局变量等 # 堆 动态分配的内存，从低地址开始向上增长 # 文件映射 动态库、共享内存等，从高地址开始向下增长 # 栈 包括局部变量和函数调用的上下文等，栈的大小是固定的。一般8MB # 内存分配与回收 # 分配\nmalloc 对应到系统调用上有两种实现方式： # brk() 针对小块内存(\u0026lt;128K)，通过移动堆顶位置来分配。 # 内存释放后不立即归还内存，而是被缓存起来。 # mmap()针对大块内存(\u0026gt;128K)，直接用内存映射来分配，即在文件映射段找一块空闲内存分配。 # 前者的缓存可以减少缺页异常的发生，提高内存访问效率。但是由于内存没有归还系统，在内存工作繁忙时，频繁的内存分配/释放会造成内存碎片。 # 后者在释放时直接归还系统，所以每次mmap都会发生缺页异常。 # 在内存工作繁忙时，频繁内存分配会导致大量缺页异常，使内核管理负担增加。 # 上述两种调用并没有真正分配内存，这些内存只有在首次访问时，才通过缺页异常进入内核中，由内核来分配。 # 回收\n内存紧张时，系统通过以下方式来回收内存： # 回收缓存：LRU算法回收最近最少使用的内存页面； # 回收不常访问内存：把不常用的内存通过交换分区写入磁盘 # 杀死进程：OOM内核保护机制（进程消耗内存越大 oom_score 越大，占用 CPU 越多 oom_score 越小，可以通过 /proc 手动调整 oom_adj） # echo -16 \u0026gt; /proc/$(pidof XXX)/oom_adj 如何查看内存使用情况\nfree来查看整个系统的内存使用情况 # top/ps来查看某个进程的内存使用情况 # VIRT 进程的虚拟内存大小 # RES 常驻内存的大小，即进程实际使用的物理内存大小，不包括swap和共享内存 # SHR 共享内存大小，与其他进程共享的内存，加载的动态链接库以及程序代码段 # %MEM 进程使用物理内存占系统总内存的百分比 # 怎样理解内存中的Buffer和Cache？\nbuffer是对磁盘数据的缓存，cache是对文件数据的缓存，它们既会用在读请求也会用在写请求中 # 如何利用系统缓存优化程序的运行效率\n缓存命中率\n缓存命中率是指直接通过缓存获取数据的请求次数，占所有请求次数的百分比。命中率越高说明缓存带来的收益越高，应用程序的性能也就越好。 # 安装bcc包后可以通过cachestat和cachetop来监测缓存的读写命中情况。 # 安装pcstat后可以查看文件在内存中的缓存大小以及缓存比例 # #首先安装Goexport GOPATH=~/goexport PATH=~/go/bin:$PATHgo get golang.org/x/sys/unixgo ge github.com/tobert/pcstat/pcstat dd缓存加速\ndd if=/dev/sda1 of=file bs=1M count=512 #生产一个512MB的临时文件echo 3 \u0026gt; /proc/sys/vm/drop_caches #清理缓存pcstat file #确定刚才生成文件不在系统缓存中，此时cached和percent都是0cachetop 5dd if=file of=/dev/null bs=1M #测试文件读取速度#此时文件读取性能为30+MB/s，查看cachetop结果发现并不是所有的读都落在磁盘上，读缓存命中率只有50%。dd if=file of=/dev/null bs=1M #重复上述读文件测试#此时文件读取性能为4+GB/s，读缓存命中率为100%pcstat file #查看文件file的缓存情况，100%全部缓存 O_DIRECT选项绕过系统缓存\ncachetop 5sudo docker run --privileged --name=app -itd feisky/app:io-directsudo docker logs app #确认案例启动成功#实验结果表明每读32MB数据都要花0.9s，且cachetop输出中显示1024次缓存全部命中 但是凭感觉可知如果缓存命中读速度不应如此慢，读次数时1024，页大小为4K，五秒的时间内读取了1024*4KB数据，即每秒0.8MB，和结果中32MB相差较大。说明该案例没有充分利用缓存，怀疑系统调用设置了直接I/O标志绕过系统缓存。因此接下来观察系统调用。 # strace -p $(pgrep app)#strace 结果可以看到openat打开磁盘分区/dev/sdb1，传入参数为O_RDONLY|O_DIRECT 这就解释了为什么读32MB数据那么慢，直接从磁盘读写肯定远远慢于缓存。找出问题后我们再看案例的源代码发现flags中指定了直接IO标志。删除该选项后重跑，验证性能变化。 # 内存泄漏，如何定位和处理？ # 对应用程序来说，动态内存的分配和回收是核心又复杂的一个逻辑功能模块。管理内存的过程中会发生各种各样的“事故”： # 没正确回收分配的内存，导致了泄漏 # 访问的是已分配内存边界外的地址，导致程序异常退出 # 内存的分配与回收\n虚拟内存分布从低到高分别是只读段，数据段，堆，内存映射段，栈五部分。其中会导致内存泄漏的是： # 堆：由应用程序自己来分配和管理，除非程序退出这些堆内存不会被系统自动释放。 # 内存映射段：包括动态链接库和共享内存，其中共享内存由程序自动分配和管理 # 内存泄漏的危害比较大，这些忘记释放的内存，不仅应用程序自己不能访问，系统也不能把它们再次分配给其他应用。内存泄漏不断累积甚至会耗尽系统内存。 # 如何检测内存泄漏\n预先安装systat，docker，bcc # sudo docker run --name=app -itd feisky/app:mem-leaksudo docker logs appvmstat 3 可以看到free在不断下降，buffer和cache基本保持不变。说明系统的内存一致在升高。但并不能说明存在内存泄漏。此时可以通过memleak工具来跟踪系统或进程的内存分配/释放请求 # /usr/share/bcc/tools/memleak -a -p $(pidof app) 从 memleak 输出可以看到，应用在不停地分配内存，并且这些分配的地址并没有被回收。通过调用栈看到是 fibonacci 函数分配的内存没有释放。定位到源码后查看源码来修复增加内存释放函数即可。 # 为什么系统的 Swap 变高 # 系统内存资源紧张时通过内存回收和OOM杀死进程来解决。其中可回收内存包括： # 缓存/缓冲区，属于可回收资源，在文件管理中通常叫做文件页 # 在应用程序中通过fsync将脏页同步到磁盘 # 交给系统，内核线程pdflush负责这些脏页的刷新 # 被应用程序修改过暂时没写入磁盘的数据(脏页)，要先写入磁盘然后才能内存释放 # 内存映射获取的文件映射页，也可以被释放掉，下次访问时从文件重新读取 # 对于程序自动分配的堆内存，也就是我们在内存管理中的匿名页，虽然这些内存不能直接释放，但是 Linux 提供了 Swap 机制将不常访问的内存写入到磁盘来释放内存，再次访问时从磁盘读取到内存即可。 # Swap原理 # Swap本质就是把一块磁盘空间或者一个本地文件当作内存来使用，包括换入和换出两个过程： # 换出：将进程暂时不用的内存数据存储到磁盘中，并释放这些内存 # 换入：进程再次访问内存时，将它们从磁盘读到内存中 # Linux如何衡量内存资源是否紧张？\n直接内存回收新的大块内存分配请求，但剩余内存不足。 # 此时系统会回收一部分内存； # kswapd0 内核线程定期回收内存。 # 为了衡量内存使用情况，定义了pages_min，pages_low，pages_high 三个阈值，并根据其来进行内存的回收操作。 # 剩余内存 \u0026lt; pages_min，进程可用内存耗尽了，只有内核才可以分配内存 # pages_min \u0026lt; 剩余内存 \u0026lt; pages_low,内存压力较大，kswapd0执行内存回收，直到剩余内存 \u0026gt; pages_high # pages_low \u0026lt; 剩余内存 \u0026lt; pages_high，内存有一定压力，但可以满足新内存请求 # 剩余内存 \u0026gt; pages_high，说明剩余内存较多，无内存压力 # pages_low = pages_min 5 / 4 pages_high = pages_min 3 / 2 # NUMA 与 SWAP # 很多情况下系统剩余内存较多，但 SWAP 依旧升高，这是由于处理器的 NUMA 架构。 # 在NUMA架构下多个处理器划分到不同的 Node，每个Node都拥有自己的本地内存空间。在分析内存的使用时应该针对每个Node单独分析 # numactl --hardware #查看处理器在Node的分布情况，以及每个Node的内存使用情况 内存三个阈值可以通过 /proc/zoneinfo 来查看，该文件中还包括活跃和非活跃的匿名页/文件页数。 # 当某个Node内存不足时，系统可以从其他Node寻找空闲资源，也可以从本地内存中回收内存。通过/proc/sys/vm/zone_raclaim_mode来调整。 # 0表示既可以从其他Node寻找空闲资源，也可以从本地回收内存 # 1，2，4 表示只回收本地内存，2表示可以会回脏数据回收内存，4表示可以用Swap方式回收内存。 # swappiness # 在实际回收过程中Linux根据 /proc/sys/vm/swapiness 选项来调整使用Swap的积极程度，从 0-100，数值越大越积极使用 Swap，即更倾向于回收匿名页；数值越小越消极使用 Swap，即更倾向于回收文件页。 # 注意：这只是调整 Swap 积极程度的权重，即使设置为0，当剩余内存+文件页小于页高阈值时，还是会发生 Swap。 # Swap升高时如何定位分析\nfree #首先通过free查看swap使用情况，若swap=0表示未配置Swap#先创建并开启swapfallocate -l 8G /mnt/swapfilechmod 600 /mnt/swapfilemkswap /mnt/swapfileswapon /mnt/swapfile free #再次执行free确保Swap配置成功 dd if=/dev/sda1 of=/dev/null bs=1G count=2048 #模拟大文件读取sar -r -S 1 #查看内存各个指标变化 -r内存 -S swap#根据结果可以看出，%memused在不断增长，剩余内存kbmemfress不断减少，缓冲区kbbuffers不断增大，由此可知剩余内存不断分配给了缓冲区#一段时间之后，剩余内存很小，而缓冲区占用了大部分内存。此时Swap使用之间增大，缓冲区和剩余内存只在小范围波动 停下sar命令cachetop5 #观察缓存#可以看到dd进程读写只有50%的命中率，未命中数为4w+页，说明正式dd进程导致缓冲区使用升高watch -d grep -A 15 ‘Normal’ /proc/zoneinfo #观察内存指标变化#发现升级内存在一个小范围不停的波动，低于页低阈值时会突然增大到一个大于页高阈值的值 说明剩余内存和缓冲区的波动变化正是由于内存回收和缓存再次分配的循环往复。有时候 Swap 用的多，有时候缓冲区波动更多。此时查看 swappiness 值为60，是一个相对中和的配置，系统会根据实际运行情况来选去合适的回收类型。 # 如何“快准狠”找到系统内存存在的问题\n内存性能指标\n系统内存指标 # 已用内存/剩余内存 # 共享内存 （tmpfs实现） # 可用内存：包括剩余内存和可回收内存 # 缓存：磁盘读取文件的页缓存，slab分配器中的可回收部分 # 缓冲区：原始磁盘块的临时存储，缓存将要写入磁盘的数据 # 进程内存指标\n虚拟内存：5大部分 # 常驻内存：进程实际使用的物理内存，不包括Swap和共享内存 # 共享内存：与其他进程共享的内存，以及动态链接库和程序的代码段 # Swap 内存：通过Swap换出到磁盘的内存 # 缺页异常\n可以直接从物理内存中分配，次缺页异常 需要磁盘 IO 介入（如 Swap），主缺页异常。此时内存访问会慢很多 内存性能工具 # 根据不同的性能指标来找合适的工具：\n内存分析工具包含的性能指标：\n如何迅速分析内存的性能瓶颈 # 通常先运行几个覆盖面比较大的性能工具，如 free，top，vmstat，pidstat 等\n先用 free 和 top 查看系统整体内存使用情况 再用 vmstat 和 pidstat，查看一段时间的趋势，从而判断内存问题的类型 最后进行详细分析，比如内存分配分析，缓存/缓冲区分析，具体进程的内存使用分析等 常见的优化思路：\n最好禁止 Swap，若必须开启则尽量降低 swappiness 的值 减少内存的动态分配，如可以用内存池，HugePage 等 尽量使用缓存和缓冲区来访问数据。如用堆栈明确声明内存空间来存储需要缓存的数据，或者用 Redis 外部缓存组件来优化数据的访问 cgroups 等方式来限制进程的内存使用情况，确保系统内存不被异常进程耗尽 /proc/pid/oom_adj 调整核心应用的 oom_score，保证即使内存紧张核心应用也不会被OOM杀死 vmstat 使用详解\nvmstat 命令是最常见的 Linux/Unix 监控工具，可以展现给定时间间隔的服务器的状态值，包括服务器的 CPU 使用率，内存使用，虚拟内存交换情况，IO读写情况。可以看到整个机器的 CPU，内存，IO 的使用情况，而不是单单看到各个进程的 CPU 使用率和内存使用率（使用场景不一样）。\nvmstat 2 procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 1 0 0 1379064 282244 11537528 0 0 3 104 0 0 3 0 97 0 0 0 0 0 1372716 282244 11537544 0 0 0 24 4893 8947 1 0 98 0 0 0 0 0 1373404 282248 11537544 0 0 0 96 5105 9278 2 0 98 0 0 0 0 0 1374168 282248 11537556 0 0 0 0 5001 9208 1 0 99 0 0 0 0 0 1376948 282248 11537564 0 0 0 80 5176 9388 2 0 98 0 0 0 0 0 1379356 282256 11537580 0 0 0 202 5474 9519 2 0 98 0 0 1 0 0 1368376 282256 11543696 0 0 0 0 5894 8940 12 0 88 0 0 1 0 0 1371936 282256 11539240 0 0 0 10554 6176 9481 14 1 85 1 0 1 0 0 1366184 282260 11542292 0 0 0 7456 6102 9983 7 1 91 0 0 1 0 0 1353040 282260 11556176 0 0 0 16924 7233 9578 18 1 80 1 0 0 0 0 1359432 282260 11549124 0 0 0 12576 5495 9271 7 0 92 1 0 0 0 0 1361744 282264 11549132 0 0 0 58 8606 15079 4 2 95 0 0 1 0 0 1367120 282264 11549140 0 0 0 2 5716 9205 8 0 92 0 0 0 0 0 1346580 282264 11562644 0 0 0 70 6416 9944 12 0 88 0 0 0 0 0 1359164 282264 11550108 0 0 0 2922 4941 8969 3 0 97 0 0 1 0 0 1353992 282264 11557044 0 0 0 0 6023 8917 15 0 84 0 0 # 结果说明 - r 表示运行队列(就是说多少个进程真的分配到CPU)，我测试的服务器目前CPU比较空闲，没什么程序在跑，当这个值超过了CPU数目，就会出现CPU瓶颈了。这个也和top的负载有关系，一般负载超过了3就比较高，超过了5就高，超过了10就不正常了，服务器的状态很危险。top的负载类似每秒的运行队列。如果运行队列过大，表示你的CPU很繁忙，一般会造成CPU使用率很高。 - b 表示阻塞的进程,这个不多说，进程阻塞，大家懂的。 - swpd 虚拟内存已使用的大小，如果大于0，表示你的机器物理内存不足了，如果不是程序内存泄露的原因，那么你该升级内存了或者把耗内存的任务迁移到其他机器。 - free 空闲的物理内存的大小，我的机器内存总共8G，剩余3415M。 - buff Linux/Unix系统是用来存储，目录里面有什么内容，权限等的缓存，我本机大概占用300多M - cache cache直接用来记忆我们打开的文件,给文件做缓冲，我本机大概占用300多M(这里是Linux/Unix的聪明之处，把空闲的物理内存的一部分拿来做文件和目录的缓存，是为了提高 程序执行的性能，当程序使用内存时，buffer/cached会很快地被使用。) - si 每秒从磁盘读入虚拟内存的大小，如果这个值大于0，表示物理内存不够用或者内存泄露了，要查找耗内存进程解决掉。我的机器内存充裕，一切正常。 - so 每秒虚拟内存写入磁盘的大小，如果这个值大于0，同上。 - bi 块设备每秒接收的块数量，这里的块设备是指系统上所有的磁盘和其他块设备，默认块大小是1024byte，我本机上没什么IO操作，所以一直是0，但是我曾在处理拷贝大量数据(2-3T)的机器上看过可以达到140000/s，磁盘写入速度差不多140M每秒 - bo 块设备每秒发送的块数量，例如我们读取文件，bo就要大于0。bi和bo一般都要接近0，不然就是IO过于频繁，需要调整。 - in 每秒CPU的中断次数，包括时间中断 - cs 每秒上下文切换次数，例如我们调用系统函数，就要进行上下文切换，线程的切换，也要进程上下文切换，这个值要越小越好，太大了，要考虑调低线程或者进程的数目,例如在apache和nginx这种web服务器中，我们一般做性能测试时会进行几千并发甚至几万并发的测试，选择web服务器的进程可以由进程或者线程的峰值一直下调，压测，直到cs到一个比较小的值，这个进程和线程数就是比较合适的值了。系统调用也是，每次调用系统函数，我们的代码就会进入内核空间，导致上下文切换，这个是很耗资源，也要尽量避免频繁调用系统函数。上下文切换次数过多表示你的CPU大部分浪费在上下文切换，导致CPU干正经事的时间少了，CPU没有充分利用，是不可取的。 - us 用户CPU时间，我曾经在一个做加密解密很频繁的服务器上，可以看到us接近100,r运行队列达到80(机器在做压力测试，性能表现不佳)。 - sy 系统CPU时间，如果太高，表示系统调用时间长，例如是IO操作频繁。 - id 空闲CPU时间，一般来说，id + us + sy = 100,一般我认为id是空闲CPU使用率，us是用户CPU使用率，sy是系统CPU使用率。 - wt 等待IO CPU时间 pidstat 使用详解\npidstat 主要用于监控全部或指定进程占用系统资源的情况，如CPU，内存、设备IO、任务切换、线程等。 # 使用方法：\npidstat –d interval times 统计各个进程的IO使用情况 # pidstat –u interval times 统计各个进程的CPU统计信息 # pidstat –r interval times 统计各个进程的内存使用信息 # pidstat -w interval times 统计各个进程的上下文切换 # p PID 指定PID # 1、统计 IO 使用情况\npidstat -d 1 10 03:02:02 PM UID PID kB_rd/s kB_wr/s kB_ccwr/s Command03:02:03 PM 0 816 0.00 918.81 0.00 jbd2/vda1-803:02:03 PM 0 1007 0.00 3.96 0.00 AliYunDun03:02:03 PM 997 7326 0.00 1904.95 918.81 java03:02:03 PM 997 8539 0.00 3.96 0.00 java03:02:03 PM 0 16066 0.00 35.64 0.00 cmagent 03:02:03 PM UID PID kB_rd/s kB_wr/s kB_ccwr/s Command03:02:04 PM 0 816 0.00 1924.00 0.00 jbd2/vda1-803:02:04 PM 997 7326 0.00 11156.00 1888.00 java03:02:04 PM 997 8539 0.00 4.00 0.00 java UID PID kB_rd/s：每秒进程从磁盘读取的数据量 KB 单位 read from disk each second KB kB_wr/s：每秒进程向磁盘写的数据量 KB 单位 write to disk each second KB kB_ccwr/s：每秒进程向磁盘写入，但是被取消的数据量，This may occur when the task truncates some dirty pagecache. iodelay：Block I/O delay，measured in clock ticks Command：进程名 task name 2、统计 CPU 使用情况\n# 统计CPUpidstat -u 1 1003:03:33 PM UID PID %usr %system %guest %CPU CPU Command03:03:34 PM 0 2321 3.96 0.00 0.00 3.96 0 ansible03:03:34 PM 0 7110 0.00 0.99 0.00 0.99 4 pidstat03:03:34 PM 997 8539 0.99 0.00 0.00 0.99 5 java03:03:34 PM 984 15517 0.99 0.00 0.00 0.99 5 java03:03:34 PM 0 24406 0.99 0.00 0.00 0.99 5 java03:03:34 PM 0 32158 3.96 0.00 0.00 3.96 2 ansible UID PID %usr: 进程在用户空间占用 cpu 的百分比 %system: 进程在内核空间占用 CPU 百分比 %guest: 进程在虚拟机占用 CPU 百分比 %wait: 进程等待运行的百分比 %CPU: 进程占用 CPU 百分比 CPU: 处理进程的 CPU 编号 Command: 进程名 3、统计内存使用情况\n# 统计内存 pidstat -r 1 10 Average: UID PID minflt/s majflt/s VSZ RSS %MEM Command Average: 0 1 0.20 0.00 191256 3064 0.01 systemd Average: 0 1007 1.30 0.00 143256 22720 0.07 AliYunDun Average: 0 6642 0.10 0.00 6301904 107680 0.33 java Average: 997 7326 10.89 0.00 13468904 8395848 26.04 java Average: 0 7795 348.15 0.00 108376 1233 0.00 pidstat Average: 997 8539 0.50 0.00 8242256 2062228 6.40 java Average: 987 9518 0.20 0.00 6300944 1242924 3.85 java Average: 0 10280 3.70 0.00 807372 8344 0.03 aliyun-service Average: 984 15517 0.40 0.00 6386464 1464572 4.54 java Average: 0 16066 236.46 0.00 2678332 71020 0.22 cmagent Average: 995 20955 0.30 0.00 6312520 1408040 4.37 java Average: 995 20956 0.20 0.00 6093764 1505028 4.67 java Average: 0 23936 0.10 0.00 5302416 110804 0.34 java Average: 0 24406 0.70 0.00 10211672 2361304 7.32 java Average: 0 26870 1.40 0.00 1470212 36084 0.11 promtail UID PID Minflt/s : 每秒次缺页错误次数 （minor page faults），虚拟内存地址映射成物理内存地址产生的 page fault 次数 Majflt/s : 每秒主缺页错误次数 (major page faults), 虚拟内存地址映射成物理内存地址时，相应 page 在 swap 中 VSZ virtual memory usage : 该进程使用的虚拟内存 KB 单位 RSS : 该进程使用的物理内存 KB 单位 %MEM : 内存使用率 Command : 该进程的命令 task name 4、查看具体进程使用情况\npidstat -T ALL -r -p 20955 1 10 03:12:16 PM UID PID minflt/s majflt/s VSZ RSS %MEM Command 03:12:17 PM 995 20955 0.00 0.00 6312520 1408040 4.37 java 03:12:16 PM UID PID minflt-nr majflt-nr Command 03:12:17 PM 995 20955 0 0 java 本内容由社区会员上传分享，仅供读者交流学习，版权归原作者所有，若所引用的图片、数据、文字等来源标注有误或涉及侵权或内容不完整，请及时联系处理。\n原文地址：https://www.ctq6.cn/linux%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/\n觉得本文有用，请转发或点击“在看”，让更多同行看到\n"},{"id":134,"href":"/docs/linux%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-%E4%B8%83%E4%B8%AA%E5%AE%9E%E6%88%98%E7%BB%8F%E9%AA%8C-linux-xi-tong-xing-neng-you-hua-qi-ge-shi-zhan-jing-yan/","title":"linux系统性能优化 七个实战经验 2024-08-02 17:47:26.212","section":"Docs","content":" Linux系统性能优化：七个实战经验 # Linux系统的性能是指操作系统完成任务的有效性、稳定性和响应速度。Linux系统管理员可能经常会遇到系统不稳定、响应速度慢等问题，例如在Linux上搭建了一个web服务，经常出现网页无法打开、打开速度慢等现象，而遇到这些问题，就有人会抱怨Linux系统不好，其实这些都是表面现象。\n操作系统完成一个任务时，与系统自身设置、网络拓朴结构、路由设备、路由策略、接入设备、物理线路等多个方面都密切相关，任何一个环节出现问题，都会影响整个系统的性能。因此当Linux应用出现问题时，应当从应用程序、操作系统、服务器硬件、网络环境等方面综合排查，定位问题出现在哪个部分，然后集中解决。\n随着容器时代的普及和AI技术的颠覆，面对越来越复杂的业务和架构，再加上企业的降本增效已提上了日程，因此对Linux的高性能、可靠性提出了更高的要求，Linux性能优化成为运维人员的必备的核心技能。\n例如，主机CPU使用率过高报警，登录Linux上去top完之后，却不知道怎么进一步定位，到底是系统CPU资源太少，还是应用程序导致的问题？这些Linux性能问题一直困扰着我们，哪怕工作多年的资深工程师也不例外。\n本文根据社区探讨，整理出企业Linux系统性能优化的7个实战经验，来自社区专家和会员分享，希望对大家有所帮助。 # # 1、影响Linux系统性能的因素一般有哪些？ # @zhaoxiaoyong081 平安科技 资深工程师： # Linux系统的性能受多个因素的影响。以下是一些常见的影响Linux系统性能的因素：\nCPU负载：CPU的利用率和负载水平对系统性能有直接影响。高CPU负载可能导致进程响应变慢、延迟增加和系统变得不稳定。 内存使用：内存是系统运行的关键资源。当系统内存不足时，可能会导致进程被终止、交换分区使用过多以及系统性能下降。 磁盘I/O：磁盘I/O性能是影响系统响应时间和吞吐量的重要因素。高磁盘I/O负载可能导致延迟增加、响应变慢和系统性能下降。 网络负载：网络流量的增加和网络延迟会对系统性能产生影响。高网络负载可能导致网络延迟增加、响应变慢和系统资源竞争。 进程调度：Linux系统使用进程调度器来管理和分配CPU资源。调度算法的选择和配置会影响进程的优先级和执行顺序，从而影响系统的响应能力和负载均衡。 文件系统性能：文件系统的选择和配置对磁盘I/O性能有影响。不同的文件系统可能在性能方面有所差异，适当的文件系统选项和调整可以改善系统性能。 内核参数：Linux内核有许多可调整的参数，可以影响系统的性能和行为。例如，TCP/IP参数、内存管理参数、文件系统缓存等。适当的内核参数调整可以改善系统的性能和资源利用率。 资源限制和配额：在多用户环境中，资源限制和配额的设置可以控制每个用户或进程可使用的资源量。适当的资源管理可以避免某些进程耗尽系统资源而导致性能问题。 这些因素之间相互关联，对系统性能产生综合影响。为了优化Linux系统性能，需要综合考虑并适当调整这些因素，以满足特定的需求和使用情况。\n2、工作中有没有快速排除故障的办法？ # # @zhaoxiaoyong081 平安科技 资深工程师： # 1.CPU 性能分析\n利用 top、vmstat、pidstat、strace 以及 perf 等几个最常见的工具，获取 CPU 性能指标后，再结合进程与 CPU 的工作原理，就可以迅速定位出 CPU 性能瓶颈的来源。\n比如说，当你收到系统的用户 CPU 使用率过高告警时，从监控系统中直接查询到，导致 CPU 使用率过高的进程；然后再登录到进程所在的 Linux 服务器中，分析该进程的行为。你可以使用 strace，查看进程的系统调用汇总；也可以使用 perf 等工具，找出进程的热点函数；甚至还可以使用动态追踪的方法，来观察进程的当前执行过程，直到确定瓶颈的根源。\n2.内存性能分析\n可以通过 free 和 vmstat 输出的性能指标，确认内存瓶颈；然后，再根据内存问题的类型，进一步分析内存的使用、分配、泄漏以及缓存等，最后找出问题的来源。\n比如说，当你收到内存不足的告警时，首先可以从监控系统中。找出占用内存最多的几个进程。然后，再根据这些进程的内存占用历史，观察是否存在内存泄漏问题。确定出最可疑的进程后，再登录到进程所在的 Linux 服务器中，分析该进程的内存空间或者内存分配，最后弄清楚进程为什么会占用大量内存。\n3.磁盘和文件系统 I/O 性能分析\n当你使用 iostat ，发现磁盘 I/O 存在性能瓶颈（比如 I/O 使用率过高、响应时间过长或者等待队列长度突然增大等）后，可以再通过 pidstat、 vmstat 等，确认 I/O 的来源。接着，再根据来源的不同，进一步分析文件系统和磁盘的使用率、缓存以及进程的 I/O 等，从而揪出 I/O 问题的真凶。\n比如说，当你发现某块磁盘的 I/O 使用率为 100% 时，首先可以从监控系统中，找出 I/O 最多的进程。然后，再登录到进程所在的 Linux 服务器中，借助 strace、lsof、perf 等工具，分析该进程的 I/O 行为。最后，再结合应用程序的原理，找出大量 I/O 的原因。\n4.网络性能分析\n而要分析网络的性能，要从这几个协议层入手，通过使用率、饱和度以及错误数这几类性能指标，观察是否存在性能问题。比如 ：\n在链路层，可以从网络接口的吞吐量、丢包、错误以及软中断和网络功能卸载等角度分析；\n在网络层，可以从路由、分片、叠加网络等角度进行分析；\n在传输层，可以从 TCP、UDP 的协议原理出发，从连接数、吞吐量、延迟、重传等角度进行分析；\n比如，当你收到网络不通的告警时，就可以从监控系统中，查找各个协议层的丢包指标，确认丢包所在的协议层。然后，从监控系统的数据中，确认网络带宽、缓冲区、连接跟踪数等软硬件，是否存在性能瓶颈。最后，再登录到发生问题的 Linux 服务器中，借助 netstat、tcpdump、bcc 等工具，分析网络的收发数据，并且结合内核中的网络选项以及 TCP 等网络协议的原理，找出问题的来源。\n3、Linux环境下，怎么排查os中系统负载过高的原因瓶颈？ # # @zhaoxiaoyong081 平安科技 资深工程师： # 在Linux环境下排查系统负载过高的原因和瓶颈，可以采取以下步骤：\n使用top或htop命令观察系统整体负载情况。查看load average的值，分别表示系统在1分钟、5分钟和15分钟内的平均负载。如果负载值超过CPU核心数量的70-80%，表示系统负载过高。 使用top或htop命令查看CPU占用率。观察哪些进程占用了大量的CPU资源。如果有某个进程持续高CPU占用，可能是引起负载过高的原因之一。 使用free命令查看系统内存使用情况。观察内存的使用量和剩余量。如果内存使用量接近或超过物理内存容量，可能导致系统开始使用交换空间（swap），进而影响系统性能。 使用iotop命令查看磁盘I/O使用情况。观察磁盘读写速率和占用率。如果磁盘I/O负载过高，可能导致系统响应变慢。 使用netstat命令或类似工具查看网络连接情况。观察是否存在大量的网络连接或网络流量。如果网络连接过多或网络流量过大，可能影响系统的性能。 检查日志文件。查看系统日志文件（如/var/log/messages、/var/log/syslog）以及应用程序日志，寻找任何异常或错误信息，可能有助于确定导致负载过高的问题。 使用perf或strace等工具进行进程级别的性能分析。这些工具可以帮助你跟踪进程的系统调用、函数调用和性能瓶颈，进一步确定导致负载过高的具体原因。 检查系统的配置和参数设置。审查相关的配置文件（如/etc/sysctl.conf、/etc/security/limits.conf）和参数设置，确保系统的设置与实际需求相匹配，并进行适当的调整。 综合上述步骤，可以帮助你定位系统负载过高的原因和瓶颈，并进一步采取相应的措施来优化系统性能。\n4、Linux怎么找出占用负载top5的进程及主要瓶颈在哪个资源（CPU or 内容 or 磁盘 IO）？ # # @zhaoxiaoyong081 平安科技 资深工程师： # # CPU 使用排名\nps aux \u0026ndash;sort=-%cpu | head -n 5\n内存 使用排名\nps aux \u0026ndash;sort=-%mem | head -n 6\nIO 使用排名\niotop -oP\n@zwz99999 dcits 系统工程师：\n查看最占用 CPU 的 10 个进程\n#ps aux|grep -v USER|sort +2|tail -n 10\n查看最占用内存的 10 个进程\n#ps aux|grep -v USER|sort +3|tail -n 10\nio\niostat 1 10看哪个磁盘busy高\n5、Linux的内存计算不准如何解决？ # # @Acdante HZTYYJ 技术总监： # free是执行时间的瞬时计数，/proc/memory内存是实时变化的。\n而且free会把缓存和缓冲区内存都计入使用内存，所以会导致看到的可用内存少很多。\n准确值的话，建议结合多种监控指标和命令手段去持续观测内存情况。\n如：htop 、 nmon 、 syssta、top等，可以结合运维软件和平台，而非站在某个时间点，最好是有一定时间的性能数据积累，从整体趋势和具体问题点位去分析。 内存只是一个资源指标，使用内存的调用才是问题根源。\n@zhaoxiaoyong081 平安科技 资深工程师： # 在一些情况下，通过ps或top命令查看的内存使用累计值与free命令或/proc/meminfo文件中的内存统计值之间可能存在较大差异。这可以由以下原因导致：\n缓存和缓冲区：Linux系统使用缓存和缓冲区来提高文件系统性能。这些缓存和缓冲区占用的内存会被标记为\u0026quot;cached\u0026quot;（缓存）和\u0026quot;buffers\u0026quot;（缓冲区）类型。然而，这些内存并不一定是实际被进程使用的内存，而是被内核保留用于提高IO性能。因此，ps或top命令显示的内存使用累计值可能包括了这些缓存和缓冲区，而free命令或/proc/meminfo中的统计值通常不包括它们。 共享内存：共享内存是一种特殊的内存区域，多个进程可以访问和共享它。ps或top命令显示的内存使用累计值可能会包括共享内存的大小，而free命令或/proc/meminfo中的统计值通常不会将其计算在内。 内存回收：Linux系统具有内存回收机制，可以在需要时回收未使用的内存。这意味着一些进程释放的内存可能不会立即反映在ps或top命令显示的内存使用累计值中。相比之下，free命令或/proc/meminfo中的统计值通常更及时地反映实际的内存使用情况。 综上所述，ps或top命令显示的内存使用累计值和free命令或/proc/meminfo中的内存统计值之间的差异通常是由于缓存和缓冲区、共享内存以及内存回收等因素造成的。如果你需要更准确地了解进程实际使用的内存，建议参考free命令或/proc/meminfo中的统计值，并结合其他工具和方法进行综合分析\n@wenwen123 项目经理：\n在Linux中，可能会出现内存计算不准确的情况，导致ps、top命令中的内存使用累计值与free命令或/proc/meminfo中的内存统计值之间存在较大差异。这种差异可能由以下原因导致：\n共享内存：共享内存是多个进程之间共享的一块内存区域，用于进程间通信。共享内存不会被ps、top等工具计算在内存使用量中，因为它们只统计进程的私有内存使用量。因此，如果进程使用了大量的共享内存，它的内存使用量在工具中显示的数值可能较低。 缓存和缓冲区：Linux系统会将一部分内存用于缓存和缓冲区，以提高文件系统和IO操作的性能。这些缓存和缓冲区的内存在ps、top等工具中被视为可回收的，因此它们通常不计入进程的内存使用量中。但是，在free命令或/proc/meminfo中，这些缓存和缓冲区的内存会被纳入统计。 内存回收机制：Linux内核具有内存回收机制，根据需要自动回收和分配内存。这可能导致在ps、top等工具显示的内存使用量和free命令或/proc/meminfo中的统计值之间存在差异。这种差异通常是正常的，并且Linux会动态管理内存以满足系统的需求。 针对内存计算不准确的问题，关注共享内存是合理的。共享内存的使用可能对进程的内存使用量造成影响，但不会被ps、top等工具计算在内存使用量中。如果需要更准确地了解进程的内存使用情况，可以使用专门的工具，如pmap、smem等，这些工具可以提供更详细和准确的内存统计信息。\n需要注意的是，Linux内存计算的准确性也取决于内核版本、系统配置和使用的工具等因素。在排查内存计算不准确的问题时，建议使用多个工具进行对比，并结合具体场景和需求进行分析和判断。\n***6、***Swap现在的应用场景还有哪些？ # # @zhaoxiaoyong081 平安科技 资深工程师： # # 虽然现代计算机的内存容量越来越大，但交换分区（swap）仍然在某些场景下具有重要的应用。以下是一些使用交换分区的常见场景：\n内存不足：交换分区作为内存不足时的后备机制，用于将不经常使用或暂时不需要的内存页面转移到磁盘上。当物理内存（RAM）不足以容纳所有活动进程和数据时，交换分区可以提供额外的虚拟内存空间，以避免系统发生内存耗尽错误（Out of Memory）。 休眠/睡眠模式：交换分区在某些操作系统中用于支持休眠（hibernation）或睡眠（suspend）模式。当计算机进入休眠或睡眠状态时，系统的内存状态会被保存到交换分区中，以便在唤醒时恢复到先前的状态。 虚拟化环境：在虚拟化环境中，交换分区可以用于虚拟机的内存管理。当宿主机的物理内存不足时，虚拟机的内存页面可以被交换到宿主机的交换分区，以提供额外的内存空间。 内存回收和页面置换：交换分区可以用于内存回收和页面置换算法。当操作系统需要释放物理内存以满足更紧急的需求时，它可以将不活动的内存页面置换到交换分区中，以便将物理内存分配给更重要的任务或进程。 尽管交换分区在上述场景中发挥作用，但需要注意的是，过度依赖交换分区可能会导致性能下降。频繁的交换操作可能会增加I/O负载，并导致响应时间延迟。因此，在现代系统中，通常建议合理配置物理内存，以尽量减少对交换分区的依赖，并保持足够的内存可用性。\n7、在Linux tcp方面有什么调优经验或案例？ # # @zhanxuechao 数字研究院 咨询专家： # centos7-os-init.sh\n@zhaoxiaoyong081 平安科技 资深工程师： # TCP 优化，分三类情况详细说明：\n第一类，在请求数比较大的场景下，你可能会看到大量处于 TIME_WAIT 状态的连接，它们会占用大量内存和端口资源。这时，我们可以优化与 TIME_WAIT 状态相关的内核选项，比如采取下面几种措施。\n增大处于 TIME_WAIT 状态的连接数量 net.ipv4.tcp_max_tw_buckets ，并增大连接跟踪表的大小 net.netfilter.nf_conntrack_max。\n减小 net.ipv4.tcp_fin_timeout 和 net.netfilter.nf_conntrack_tcp_timeout_time_wait ，让系统尽快释放它们所占用的资源。\n开启端口复用 net.ipv4.tcp_tw_reuse。这样，被 TIME_WAIT 状态占用的端口，还能用到新建的连接中。\n增大本地端口的范围 net.ipv4.ip_local_port_range 。这样就可以支持更多连接，提高整体的并发能力。\n增加最大文件描述符的数量。你可以使用 fs.nr_open 和 fs.file-max ，分别增大进程和系统的最大文件描述符数；或在应用程序的 systemd 配置文件中，配置 LimitNOFILE ，设置应用程序的最大文件描述符数。\n第二类，为了缓解 SYN FLOOD 等，利用 TCP 协议特点进行攻击而引发的性能问题，你可以考虑优化与 SYN 状态相关的内核选项，比如采取下面几种措施。\n增大 TCP 半连接的最大数量 net.ipv4.tcp_max_syn_backlog ，或者开启 TCP SYN Cookies net.ipv4.tcp_syncookies ，来绕开半连接数量限制的问题（注意，这两个选项不可同时使用）。\n减少 SYN_RECV 状态的连接重传 SYN+ACK 包的次数 net.ipv4.tcp_synack_retries。\n第三类，在长连接的场景中，通常使用 Keepalive 来检测 TCP 连接的状态，以便对端连接断开后，可以自动回收。但是，系统默认的 Keepalive 探测间隔和重试次数，一般都无法满足应用程序的性能要求。所以，这时候你需要优化与 Keepalive 相关的内核选项，比如：\n缩短最后一次数据包到 Keepalive 探测包的间隔时间 net.ipv4.tcp_keepalive_time；\n缩短发送 Keepalive 探测包的间隔时间 net.ipv4.tcp_keepalive_intvl；\n减少 Keepalive 探测失败后，一直到通知应用程序前的重试次数 net.ipv4.tcp_keepalive_probes。\n"},{"id":135,"href":"/docs/linux%E8%BF%90%E7%BB%B4%E5%B7%A5%E7%A8%8B%E5%B8%8850%E4%B8%AA%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98-linux-yun-wei-gong-cheng-shi-50-ge-chang-jian-mian-shi-ti/","title":"linux运维工程师50个常见面试题 2024-08-02 17:42:12.968","section":"Docs","content":"1、请简述OSI七层网络模型有哪些层及各自的含义?\n物理层：底层数据传输，比如网线、网卡标准 数据链路层：定义数据的基本格式，如何传输，如何标识。比如网卡MAC地址 网络层：定义IP编码，定义路由功能，比如不同设备的数据转发 传输层：端到端传输数据的基本功能，比如TCP、UDP 会话层：控制应用程序之间会话能力，比如不同软件数据分发给不停软件 表示层：数据格式标识，基本压缩加密功能。 应用层：各种应用软件，包括 Web 应用。 2、在Linux的LVM分区格式下，请简述给根分区磁盘扩容的步骤? # 这个分3种\n第一种方法:\ngrowpart /dev/vda 1 resize2fs /dev/vda1 第二种方法:\npartpeobe /dev/sda resize2fs /dev/vda1 第三种方法:\nfdisk /dev/sdb # n p 1 1 回车 回车 t 8e w pvcreate /dev/sdb1 vgextend datavg /dev/sdb1 lvextend -r -L +100%free /dev/mapper/datavg-lv01 3、讲述一下Tomcat8005、8009、8080三个端口的含义？ # 8005 关闭时使用 8009为AJP端口，即容器使用，如Apache能通过AJP协议访问Tomcat的8009端口来实现功能 8080 一般应用使用 4、简述DNS进行域名解析的过程？ # 迭代查询（返回最优结果）、递归查询（本地找DNS）用户要访问 www.baidu.com，会先找本机的host文件，再找本地设置的DNS服务器，如果也没有找到，就去网络中找根服务器，根服务器反馈结果，说只能提供一级域名服务器.cn，就去找一级域名服务器，一级域名服务器说只能提供二级域名服务器.com.cn,就去找二级域名服务器，二级域服务器只能提供三级域名服务器.baidu.com.cn，就去找三级域名服务器，三级域名服务器正好有这个网站www.baidu.com，然后发给请求的服务器，保存一份之后，再发给客户端。\n5、讲一下Keepalived的工作原理？ # 在一个虚拟路由器中，只有作为MASTER的VRRP(虚拟路由冗余协议)路由器会一直发送VRRP通告信息，BACKUP不会抢占MASTER，除非它的优先级更高。当MASTER不可用时(BACKUP收不到通告信息)多台BACKUP中优先级最高的这台会被抢占为MASTER。这种抢占是非常快速的(\u0026lt;1s)，以保证服务的连续性由于安全性考虑，VRRP包使用了加密协议进行加密。BACKUP不会发送通告信息，只会接收通告信息。\n6、LVS、Nginx、HAproxy有什么区别？工作中你怎么选择？ # LVS：\n抗负载能力强、工作在第4层仅作分发之用，没有流量的产生，这个特点也决定了它在负载均衡软件里的性能最强的；无流量，同时保证了均衡器IO的性能不会受到大流量的影响； 工作稳定，自身有完整的双机热备方案，如LVS+Keepalived和LVS+Heartbeat； 应用范围比较广，可以对所有应用做负载均衡； 配置简单，因为没有可太多配置的东西，所以并不需要太多接触，大大减少了人为出错的几率； LVS的缺点：\n软件本身不支持正则处理，不能做动静分离，这就凸显了Nginx/HAProxy+Keepalived的优势。 如果网站应用比较庞大，LVS/DR+Keepalived就比较复杂了，特别是后面有Windows Server应用的机器，实施及配置还有维护过程就比较麻烦，相对而言，Nginx/HAProxy+Keepalived就简单多了。 Nginx：\n工作在第7层，应用层，可以针对http应用做一些分流的策略。比如针对域名、目录结构。它的正则比HAProxy更为强大和灵活； Nginx对网络的依赖非常小，理论上能ping通就就能进行负载功能 Nginx安装和配置简单 可以承担高的负载压力且稳定，一般能支撑超过几万次的并发量； Nginx不仅仅是一款优秀的负载均衡器/反向代理软件，它同时也是功能强大的Web应用服务器。Nginx在处理静态页面、特别是抗高并发方面相对apache有优势； Nginx作为Web反向代理加速缓存越来越成熟，速度比传统的Squid服务器更快 Nginx的缺点：\nNginx不支持url来检测。 Nginx仅能支持http、https和Email协议 Nginx的Session的保持，Cookie的引导能力相对欠缺。 HAProxy：\nHAProxy是支持虚拟主机的，可以工作在4、7层(支持多网段)； 能够补充Nginx的一些缺点比如Session的保持，Cookie的引导等工作； 支持url检测后端的服务器； 它跟LVS一样，本身仅仅就只是一款负载均衡软件；单纯从效率上来讲HAProxy更会比Nginx有更出色的负载均衡速度，在并发处理上也是优于Nginx的； HAProxy可以对Mysql读进行负载均衡，对后端的MySQL节点进行检测和负载均衡，不过在后端的MySQL slaves数量超过10台时性能不如LVS； HAProxy的算法较多，达到8种； 工作选择：\nHAproxy和Nginx由于可以做七层的转发，所以URL和目录的转发都可以做在很大并发量的时候我们就要选择LVS，像中小型公司的话并发量没那么大选择HAproxy或者Nginx足已，由于HAproxy由是专业的代理服务器配置简单，所以中小型企业推荐使用HAproxy。\n7、docker的工作原理是什么，讲一下 # docker是一个Client-Server结构的系统，docker守护进程运行在宿主机上，守护进程从客户端接受命令并管理运行在主机上的容器，容器是一个运行时环境，这就是我们说的集装箱。\n8、docker的组成包含哪几大部分 # 一个完整的docker有以下几个部分组成：\ndocker client，客户端，为用户提供一系列可执行命令，用户用这些命令实现跟 docker daemon 交互； docker daemon，守护进程，一般在宿主主机后台运行，等待接收来自客户端的请求消息； docker image，镜像，镜像run之后就生成为docker容器； docker container，容器，一个系统级别的服务，拥有自己的ip和系统目录结构；运行容器前需要本地存在对应的镜像，如果本地不存在该镜像则就去镜像仓库下载。 docker 使用客户端-服务器 (C/S) 架构模式，使用远程api来管理和创建docker容器。docker 容器通过 docker 镜像来创建。容器与镜像的关系类似于面向对象编程中的对象与类。\n9、docker与传统虚拟机的区别什么？ # 传统虚拟机是需要安装整个操作系统的，然后再在上面安装业务应用，启动应用，通常需要几分钟去启动应用，而docker是直接使用镜像来运行业务容器的，其容器启动属于秒级别； Docker需要的资源更少，Docker在操作系统级别进行虚拟化，Docker容器和内核交互，几乎没有性能损耗，而虚拟机运行着整个操作系统，占用物理机的资源就比较多; Docker更轻量，Docker的架构可以共用一个内核与共享应用程序库，所占内存极小;同样的硬件环境，Docker运行的镜像数远多于虚拟机数量，对系统的利用率非常高; 与虚拟机相比，Docker隔离性更弱，Docker属于进程之间的隔离，虚拟机可实现系统级别隔离; Docker的安全性也更弱，Docker的租户root和宿主机root相同，一旦容器内的用户从普通用户权限提升为root权限，它就直接具备了宿主机的root权限，进而可进行无限制的操作。虚拟机租户root权限和宿主机的root虚拟机权限是分离的，并且虚拟机利用如Intel的VT-d和VT-x的ring-1硬件隔离技术，这种技术可以防止虚拟机突破和彼此交互，而容器至今还没有任何形式的硬件隔离; Docker的集中化管理工具还不算成熟，各种虚拟化技术都有成熟的管理工具，比如：VMware vCenter提供完备的虚拟机管理能力; Docker对业务的高可用支持是通过快速重新部署实现的，虚拟化具备负载均衡，高可用、容错、迁移和数据保护等经过生产实践检验的成熟保障机制，Vmware可承诺虚拟机99.999%高可用，保证业务连续性; 虚拟化创建是分钟级别的，Docker容器创建是秒级别的，Docker的快速迭代性，决定了无论是开发、测试、部署都可以节省大量时间; 虚拟机可以通过镜像实现环境交付的一致性，但镜像分发无法体系化，Docker在Dockerfile中记录了容器构建过程，可在集群中实现快速分发和快速部署。from wljslmz 10、docker技术的三大核心概念是什么？ # 镜像：镜像是一种轻量级、可执行的独立软件包，它包含运行某个软件所需的所有内容，我们把应用程序和配置依赖打包好形成一个可交付的运行环境(包括代码、运行时需要的库、环境变量和配置文件等)，这个打包好的运行环境就是image镜像文件。 容器：容器是基于镜像创建的，是镜像运行起来之后的一个实例，容器才是真正运行业务程序的地方。如果把镜像比作程序里面的类，那么容器就是对象。 镜像仓库：存放镜像的地方，研发工程师打包好镜像之后需要把镜像上传到镜像仓库中去，然后就可以运行有仓库权限的人拉取镜像来运行容器了。 11、centos镜像几个G，但是docker centos镜像才几百兆，这是为什么？ # 一个完整的Linux操作系统包含Linux内核和rootfs根文件系统，即我们熟悉的/dev、/proc/、/bin等目录。我们平时看到的centOS除了rootfs，还会选装很多软件，服务，图形桌面等，所以centOS镜像有好几个G也不足为奇。\n而对于容器镜像而言，所有容器都是共享宿主机的Linux 内核的，而对于docker镜像而言，docker镜像只需要提供一个很小的rootfs即可，只需要包含最基本的命令，工具，程序库即可，所有docker镜像才会这么小。\n12、讲一下镜像的分层结构以及为什么要使用镜像的分层结构？ # 一个新的镜像其实是从 base 镜像一层一层叠加生成的。每安装一个软件，dockerfile中使用RUM命令，就会在现有镜像的基础上增加一层，这样一层一层的叠加最后构成整个镜像。所以我们docker pull拉取一个镜像的时候会看到docker是一层层拉去的。\n分层机构最大的一个好处就是 ：共享资源。比如：有多个镜像都从相同的 base 镜像构建而来，那么 Docker Host 只需在磁盘上保存一份 base 镜像；同时内存中也只需加载一份 base 镜像，就可以为所有容器服务了。而且镜像的每一层都可以被共享。\n13、讲一下容器的copy-on-write特性，修改容器里面的内容会修改镜像吗？ # 我们知道，镜像是分层的，镜像的每一层都可以被共享，同时，镜像是只读的。当一个容器启动时，一个新的可写层被加载到镜像的顶部，这一层通常被称作“容器层”，“容器层”之下的都叫“镜像层”。\n所有对容器的改动 - 无论添加、删除、还是修改文件，都只会发生在容器层中，因为只有容器层是可写的，容器层下面的所有镜像层都是只读的。镜像层数量可能会很多，所有镜像层会联合在一起组成一个统一的文件系统。如果不同层中有一个相同路径的文件，比如 /a，上层的 /a 会覆盖下层的 /a，也就是说用户只能访问到上层中的文件 /a。在容器层中，用户看到的是一个叠加之后的文件系统。\n添加文件：在容器中创建文件时，新文件被添加到容器层中。 读取文件：在容器中读取某个文件时，Docker 会从上往下依次在各镜像层中查找此文件。一旦找到，立即将其复制到容器层，然后打开并读入内存。 修改文件：在容器中修改已存在的文件时，Docker 会从上往下依次在各镜像层中查找此文件。一旦找到，立即将其复制到容器层，然后修改之。 删除文件：在容器中删除文件时，Docker 也是从上往下依次在镜像层中查找此文件。找到后，会在容器层中记录下此删除操作。 只有当需要修改时才复制一份数据，这种特性被称作 Copy-on-Write。可见，容器层保存的是镜像变化的部分，不会对镜像本身进行任何修改。\n14、简单描述一下Dockerfile的整个构建镜像过程 # 首先，创建一个目录用于存放应用程序以及构建过程中使用到的各个文件等； 然后，在这个目录下创建一个Dockerfile文件，一般建议Dockerfile的文件名就是Dockerfile； 编写Dockerfile文件，编写指令，如，使用FORM指令指定基础镜像，COPY指令复制文件，RUN指令指定要运行的命令，ENV设置环境变量，EXPOSE指定容器要暴露的端口，WORKDIR设置当前工作目录，CMD容器启动时运行命令，等等指令构建镜像； Dockerfile编写完成就可以构建镜像了，使用docker build -t 镜像名:tag . 命令来构建镜像，最后一个点是表示当前目录，docker会默认寻找当前目录下的Dockerfile文件来构建镜像，如果不使用默认，可以使用-f参数来指定dockerfile文件，如：docker build -t 镜像名:tag -f /xx/xxx/Dockerfile ； 使用docker build命令构建之后，docker就会将当前目录下所有的文件发送给docker daemon，顺序执行Dockerfile文件里的指令，在这过程中会生成临时容器，在临时容器里面安装RUN指定的命令，安装成功后，docker底层会使用类似于docker commit命令来将容器保存为镜像，然后删除临时容器，以此类推，一层层的构建镜像，运行临时容器安装软件，直到最后的镜像构建成功。 15、Dockerfile构建镜像出现异常，如何排查？ # 首先，Dockerfile是一层一层的构建镜像，期间会产生一个或多个临时容器，构建过程中其实就是在临时容器里面安装应用，如果因为临时容器安装应用出现异常导致镜像构建失败，这时容器虽然被清理掉了，但是期间构建的中间镜像还在，那么我们可以根据异常时上一层已经构建好的临时镜像，将临时镜像运行为容器，然后在容器里面运行安装命令来定位具体的异常。\n16、Dockerfile的基本指令有哪些？ # FROM 指定基础镜像（必须为第一个指令，因为需要指定使用哪个基础镜像来构建镜像）； MAINTAINER 设置镜像作者相关信息，如作者名字，日期，邮件，联系方式等； COPY 复制文件到镜像； ADD 复制文件到镜像（ADD与COPY的区别在于，ADD会自动解压tar、zip、tgz、xz等归档文件，而COPY不会，同时ADD指令还可以接一个url下载文件地址，一般建议使用COPY复制文件即可，文件在宿主机上是什么样子复制到镜像里面就是什么样子这样比较好）； ENV 设置环境变量； EXPOSE 暴露容器进程的端口，仅仅是提示别人容器使用的哪个端口，没有过多作用； VOLUME 数据卷持久化，挂载一个目录； WORKDIR 设置工作目录，如果目录不在，则会自动创建目录； RUN 在容器中运行命令，RUN指令会创建新的镜像层，RUN指令经常被用于安装软件包； CMD 指定容器启动时默认运行哪些命令，如果有多个CMD，则只有最后一个生效，另外，CMD指令可以被docker run之后的参数替换； ENTRYOINT 指定容器启动时运行哪些命令，如果有多个ENTRYOINT，则只有最后一个生效，另外，如果Dockerfile中同时存在CMD和ENTRYOINT，那么CMD或docker run之后的参数将被当做参数传递给ENTRYOINT； 17、如何进入容器？使用哪个命令 # 进入容器有两种方法：docker attach、docker exec。\n18、什么是k8s？说出你的理解 # K8s是kubernetes的简称，其本质是一个开源的容器编排系统，主要用于管理容器化的应用，其目标是让部署容器化的应用简单并且高效（powerful）,Kubernetes提供了应用部署，规划，更新，维护的一种机制。\n说简单点：k8s就是一个编排容器的系统，一个可以管理容器应用全生命周期的工具，从创建应用，应用的部署，应用提供服务，扩容缩容应用，应用更新，都非常的方便，而且还可以做到故障自愈，所以，k8s是一个非常强大的容器编排系统。\n19、k8s的组件有哪些，作用分别是什么？ # k8s主要由master节点和node节点构成。master节点负责管理集群，node节点是容器应用真正运行的地方。\nmaster节点包含的组件有：kube-api-server、kube-controller-manager、kube-scheduler、etcd。 node节点包含的组件有：kubelet、kube-proxy、container-runtime。 kube-api-server：以下简称api-server，api-server是k8s最重要的核心组件之一，它是k8s集群管理的统一访问入口，提供了RESTful API接口, 实现了认证、授权和准入控制等安全功能；api-server还是其他组件之间的数据交互和通信的枢纽，其他组件彼此之间并不会直接通信，其他组件对资源对象的增、删、改、查和监听操作都是交由api-server处理后，api-server再提交给etcd数据库做持久化存储，只有api-server才能直接操作etcd数据库，其他组件都不能直接操作etcd数据库，其他组件都是通过api-server间接的读取，写入数据到etcd。\nkube-controller-manager：以下简称controller-manager，controller-manager是k8s中各种控制器的的管理者，是k8s集群内部的管理控制中心，也是k8s自动化功能的核心；controller-manager内部包含replication controller、node controller、deployment controller、endpoint controller等各种资源对象的控制器，每种控制器都负责一种特定资源的控制流程，而controller-manager正是这些controller的核心管理者。\nkube-scheduler：以下简称scheduler，scheduler负责集群资源调度，其作用是将待调度的pod通过一系列复杂的调度算法计算出最合适的node节点，然后将pod绑定到目标节点上。shceduler会根据pod的信息（关注微信公众号：网络技术联盟站），全部节点信息列表，过滤掉不符合要求的节点，过滤出一批候选节点，然后给候选节点打分，选分最高的就是最佳节点，scheduler就会把目标pod安置到该节点。\nEtcd：etcd是一个分布式的键值对存储数据库，主要是用于保存k8s集群状态数据，比如，pod，service等资源对象的信息；etcd可以是单个也可以有多个，多个就是etcd数据库集群，etcd通常部署奇数个实例，在大规模集群中，etcd有5个或7个节点就足够了；另外说明一点，etcd本质上可以不与master节点部署在一起，只要master节点能通过网络连接etcd数据库即可。\nkubelet：每个node节点上都有一个kubelet服务进程，kubelet作为连接master和各node之间的桥梁，负责维护pod和容器的生命周期，当监听到master下发到本节点的任务时，比如创建、更新、终止pod等任务，kubelet 即通过控制docker来创建、更新、销毁容器；每个kubelet进程都会在api-server上注册本节点自身的信息，用于定期向master汇报本节点资源的使用情况。\nkube-proxy：kube-proxy运行在node节点上，在Node节点上实现Pod网络代理，维护网络规则和四层负载均衡工作，kube-proxy会监听api-server中从而获取service和endpoint的变化情况，创建并维护路由规则以提供服务IP和负载均衡功能。简单理解此进程是Service的透明代理兼负载均衡器，其核心功能是将到某个Service的访问请求转发到后端的多个Pod实例上。\ncontainer-runtime：容器运行时环境，即运行容器所需要的一系列程序，目前k8s支持的容器运行时有很多，如docker、rkt或其他，比较受欢迎的是docker，但是新版的k8s已经宣布弃用docker。\n20、kubelet的功能、作用是什么？（重点，经常会问） # kubelet部署在每个node节点上的，它主要有4个功能：\n节点管理。kubelet启动时会向api-server进行注册，然后会定时的向api-server汇报本节点信息状态，资源使用状态等，这样master就能够知道node节点的资源剩余，节点是否失联等等相关的信息了。master知道了整个集群所有节点的资源情况，这对于 pod 的调度和正常运行至关重要。 pod管理。kubelet负责维护node节点上pod的生命周期，当kubelet监听到master的下发到自己节点的任务时，比如要创建、更新、删除一个pod，kubelet 就会通过CRI（容器运行时接口）插件来调用不同的容器运行时来创建、更新、删除容器；常见的容器运行时有docker、containerd、rkt等等这些容器运行时，我们最熟悉的就是docker了，但在新版本的k8s已经弃用docker了，k8s1.24版本中已经使用containerd作为容器运行时了。 容器健康检查。pod中可以定义启动探针、存活探针、就绪探针等3种，我们最常用的就是存活探针、就绪探针，kubelet 会定期调用容器中的探针来检测容器是否存活，是否就绪，如果是存活探针，则会根据探测结果对检查失败的容器进行相应的重启策略； Metrics Server资源监控。在node节点上部署Metrics Server用于监控node节点、pod的CPU、内存、文件系统、网络使用等资源使用情况，而kubelet则通过Metrics Server获取所在节点及容器的上的数据。 21、kube-api-server的端口是多少？各个pod是如何访问kube-api-server的？ # kube-api-server的端口是8080和6443，前者是http的端口，后者是https的端口，以我本机使用kubeadm安装的k8s为例：\n在命名空间的kube-system命名空间里，有一个名称为kube-api-master的pod，这个pod就是运行着kube-api-server进程，它绑定了master主机的ip地址和6443端口，但是在default命名空间下，存在一个叫kubernetes的服务，该服务对外暴露端口为443，目标端口6443，这个服务的ip地址是clusterip地址池里面的第一个地址，同时这个服务的yaml定义里面并没有指定标签选择器，也就是说这个kubernetes服务所对应的endpoint是手动创建的，该endpoint也是名称叫做kubernetes，该endpoint的yaml定义里面代理到master节点的6443端口，也就是kube-api-server的IP和端口。这样一来，其他pod访问kube-api-server的整个流程就是：pod创建后嵌入了环境变量，pod获取到了kubernetes这个服务的ip和443端口，请求到kubernetes这个服务其实就是转发到了master节点上的6443端口的kube-api-server这个pod里面。\n22、k8s中命名空间的作用是什么？ # amespace是kubernetes系统中的一种非常重要的资源，namespace的主要作用是用来实现多套环境的资源隔离，或者说是多租户的资源隔离。\nk8s通过将集群内部的资源分配到不同的namespace中，可以形成逻辑上的隔离，以方便不同的资源进行隔离使用和管理。不同的命名空间可以存在同名的资源，命名空间为资源提供了一个作用域。\n可以通过k8s的授权机制，将不同的namespace交给不同的租户进行管理，这样就实现了多租户的资源隔离，还可以结合k8s的资源配额机制，限定不同的租户能占用的资源，例如CPU使用量、内存使用量等等来实现租户可用资源的管理。\n23、pod资源控制器类型有哪些? # Deployments：Deployment为Pod和ReplicaSet提供声明式的更新能力。 ReplicaSet：ReplicaSet的目的是维护一组在任何时候都处于运行状态的Pod副本的稳定集合。因此，它通常用来保证给定数量的、完全相同的Pod的可用性。 StatefulSets：和Deployment类似，StatefulSet管理基于相同容器规约的一组Pod。但和Deployment不同的是，StatefulSet为它们的每个Pod维护了一个有粘性的ID。这些Pod是基于相同的规约来创建的，但是不能相互替换：无论怎么调度，每个Pod都有一个永久不变的ID。 DaemonSet：DaemonSet确保全部（或者某些）节点上运行一个Pod的副本。当有节点加入集群时，也会为他们新增一个Pod。当有节点从集群移除时，这些Pod也会被回收。删除DaemonSet将会删除它创建的所有Pod。 Jobs：Job会创建一个或者多个Pod，并将继续重试Pod的执行，直到指定数量的Pod成功终止。随着Pod成功结束，Job跟踪记录成功完成的Pod个数。当数量达到指定的成功个数阈值时，任务（即Job）结束。删除Job的操作会清除所创建的全部Pod。挂起Job的操作会删除Job的所有活跃Pod，直到Job被再次恢复执行。 Automatic Clean-up for Finished Jobs：TTL-after-finished控制器提供了一种TTL机制来限制已完成执行的资源对象的生命周期。TTL控制器目前只处理Job。 CronJob：一个CronJob对象就像crontab(crontable)文件中的一行。它用Cron格式进行编写，并周期性地在给定的调度时间执行Job。 ReplicationController：ReplicationController确保在任何时候都有特定数量的Pod副本处于运行状态。换句话说，ReplicationController确保一个Pod或一组同类的Pod总是可用的。 24、nginx算法策略 # 轮询（默认）\n加权轮询（轮询+weight）\nip_hash\n每一个请求的访问IP，都会映射成一个hash，再通过hash算法（hash值%node_count），分配到不同的后端服务器，访问ip相同的请求会固定访问同一个后端服务器，这样可以做到会话保持，解决session同步问题。\nleast_conn（最少连接）\n使用最少连接的负载平衡，nginx将尝试不会使繁忙的应用程序服务器超载请求过多，而是将新请求分发给不太繁忙的服务器。\n25、nignx常用模块 # upstream rewrite location proxy_pass 26、如何查看并且杀死僵尸进程？ # top —\u0026gt; task (line)—\u0026gt; zombie.\n把父进程杀掉，父进程死后，过继给1号进程init，init 始终负责清理僵尸进程，它产生的所有僵尸进程跟着消失；如果你使用kill ，一般都不能杀掉 defunct进程.。用了kill -15,kill -9以后 之后反而会多出更多的僵尸进程。\n27、搜索某个用户运行的进程 # pgrep -au neteagle 28、查看某个端口正在被哪个进程使用 # lsof -i :[port] 29、端口转发 # iptables -t nat -A PREROUTING -d 10.0.0.8 -p tcp --dport 80 -j REDIRECT --to-ports 8080 30、查看http的并发请求数与其TCP连接状态 # etstat-n|awk\u0026#39;/^tcp/{++b[$NF]}END{for(ainb)printa,b[a]}\u0026#39; 31、查看/var/log目录下文件数 # ls/var/log/-lR|grep\u0026#34;^-\u0026#34;|wc-l 32、linux系统启动流程 # 第一步：开机自检，加载BIOS 第二步：读取ＭＢＲ 第三步：Boot Loader　grub引导菜单 第四步：加载kernel内核 第五步：init进程依据inittab文件夹来设定运行级别 第六步：init进程执行rc.sysinit 第七步：启动内核模块 第八步：执行不同运行级别的脚本程序 第九步：执行/etc/rc.d/rc.lo 33、Linux文件类型 # -：常规文件，即file d：目录文件 b：block device 即块设备文件，如硬盘;支持以block为单位进行随机访问 c：character device 即字符设备文件，如键盘支持以character为单位进行线性访问 l：symbolic link 即符号链接文件（关注微信公众号：网络技术联盟站），又称软链接文件 p：pipe 即命名管道文件 s：socket 即套接字文件，用于实现两个进程进行通信 34、简述lvm，如何给使用lvm的/分区扩容？ # 功能：可以对磁盘进行动态管理。动态按需调整大小\n概念：\nPV 物理卷：物理卷在逻辑卷管理中处于最底层，它可以是实际物理硬盘上的分区，也可以是整个物理硬盘，也可以是raid设备。 VG 卷组：卷组建立在物理卷之上，一个卷组中至少要包括一个物理卷，在卷组建立之后可动态添加物理卷到卷组中。一个逻辑卷管理系统工程中可以只有一个卷组，也可以拥有多个卷组。 LV 逻辑卷：逻辑卷建立在卷组之上，卷组中的未分配空间可以用于建立新的逻辑卷，逻辑卷建立后可以动态地扩展和缩小空间。系统中的多个逻辑卷可以属于同一个卷组，也可以属于不同的多个卷组。 给/分区扩容步骤：\n添加磁盘 使用fdisk命令对新增加的磁盘进行分区 分区完成后修改分区类型为lvm 使用pvcreate创建物理卷 使用vgextend命令将新增加的分区加入到根目录分区中 使用lvextend命令进行扩容 使用xfs_growfs调整卷分区大小 35、如何在文本里面进行复制、粘贴，删除行，删除全部，按行查找和按字母查找。 # 以下操作全部在vi/vim命令行状态操作，不要在编辑状态操作：\n在文本里 移动到想要复制的行按yy想复制到哪就移动到哪，然后按P就黏贴了 删除行 移动到改行 按dd 删除全部dG这里注意G一定要大写 按行查找 :90 这样就是找到第90行 按字母查找 /path 这样就是找到path这个单词所在的位置，文本里可能存在多个，多次查找会显示在不同的位置。 36、符号链接与硬链接的区别 # 我们可以把符号链接，也就是软连接 当做是 windows系统里的 快捷方式。 硬链接 就好像是 又复制了一份. ln 3.txt 4.txt 这是硬链接，相当于复制，不可以跨分区，但修改3,4会跟着变，若删除3,4不受任何影响。 ln -s 3.txt 4.txt 这是软连接，相当于快捷方式。修改4,3也会跟着变，若删除3,4就坏掉了。不可以用了。 37、什么是正向代理？ # 一个位于客户端和原始服务器(origin server)之间的服务器，为了从原始服务器取得内容，客户端向代理发送一个请求并指定目标(原始服务器)，然后代理向原始服务器转交请求并将获得的内容返回给客户端。\n客户端才能使用正向代理。正向代理总结就一句话：代理端代理的是客户端。例如说：我们使用的OpenVPN 等等。\n38、什么是反向代理？ # 反向代理（Reverse Proxy）方式，是指以代理服务器来接受 Internet上的连接请求，然后将请求，发给内部网络上的服务器并将从服务器上得到的结果返回给 Internet 上请求连接的客户端，此时代理服务器对外就表现为一个反向代理服务器。\n反向代理总结就一句话：代理端代理的是服务端。\n39、什么是动态资源、静态资源分离？ # 动态资源、静态资源分离，是让动态网站里的动态网页根据一定规则把不变的资源和经常变的资源区分开来，动静资源做好了拆分以后我们就可以根据静态资源的特点将其做缓存操作，这就是网站静态化处理的核心思路。\n动态资源、静态资源分离简单的概括是：动态文件与静态文件的分离。\n在我们的软件开发中，有些请求是需要后台处理的（如：.jsp,.do 等等），有些请求是不需要经过后台处理的（如：css、html、jpg、js 等等文件），这些不需要经过后台处理的文件称为静态文件，否则动态文件。\n因此我们后台处理忽略静态文件。这会有人又说那我后台忽略静态文件不就完了吗？当然这是可以的，但是这样后台的请求次数就明显增多了。在我们对资源的响应速度有要求的时候，我们应该使用这种动静分离的策略去解决动、静分离将网站静态资源（HTML，JavaScript，CSS，img等文件）与后台应用分开部署，提高用户访问静态代码的速度，降低对后台应用访问\n这里我们将静态资源放到 Nginx 中，动态资源转发到 Tomcat 服务器中去。\n当然，因为现在七牛、阿里云等 CDN 服务已经很成熟，主流的做法，是把静态资源缓存到 CDN 服务中，从而提升访问速度。\n相比本地的 Nginx 来说，CDN 服务器由于在国内有更多的节点，可以实现用户的就近访问。并且，CDN 服务可以提供更大的带宽，不像我们自己的应用服务，提供的带宽是有限的。\n40、网站登陆缓慢是什么原因? # 网络带宽，这是一个很常见的瓶颈。 cpu、硬盘、内存配置过低，服务器负载不起来。 网站的开发代码不够完善，例如mysql语句没有进行优化，导致数据库的读写相当耗费时间。 数据库的瓶颈。当我们的数据库的数据变得越来越多的时候，那么对于数据库的读写压力肯定会变大。 41、a与b服务器不在同一网段怎么设置?设置完还ping不通怎么排查? # AB服务器不在同一个网段 首先把不同IP段的服务器划分给不同的vlan 在通过通过三层交换机添加虚拟IP路由实在不同网段的vlan的连接 42、在AB两台服务器之间通过一个服务器c做软路由使用给路由器c配置两块网卡并开启自身的路由功能 # vi /etc/sysconfig/network-scripts/ifcfg-eth0 查看网卡状况IP -a -s 网卡的名字\nA服务器设置相关网卡信息\n子网掩码：255.255.255.0 IP=10.0.0.1 网关=10.0.0.254 重启网卡生效 查看路由信息 route -n 添加对应路由 route add -net 10.0.1.0/24 gw 10.0.0.11 B服务器的设置相关信息\nIP=10.0.1.10 网关10.0.1.254 重启网卡生效 route -n 添加对应的路由 route add -net 10.0.0.0/24 gw 10.0.1.11 C服务器的两块网卡\n网卡1 IP=10.0.0.11 网关=10.0.0.254 网卡2 IP=10.0.1.11 网关=10.0.1.254 重启网卡生效 route -n vi /etc/sysctl.conf net.ipv4.ip_forword = 1 43、如果PING不通怎么排查 # 首先先看看是不是网路接口故障水晶头或是网卡接口接触不良造成，其次检查交换机和路由等网络设备是有故障 是否关闭了防火墙和selinux机制 然后查看网卡和路由和网关是否配置正确 44、docker容器ping不通是什么原因? # ifconfig 查看一下docker0网桥，ping一下网桥看看是否通，有可能是网桥配置问题\nweave路由器端口6783\n安装docker容器的服务器没有关闭防火墙(访问一下安装docker物理机的，是否能访问，如果不能访问就变不能访问docker) docker在创建镜像的时候没有做端口映射(出现这种情况能访问物理机不能访问docker)使用dockers ps 查看镜像的端口映射情况 端口映射不正确 查看网络配置ping网桥看是否能ping通，有可能是网桥的原因 45、如果一台办公室内主机无法上网(打不开网站)，请给出你的排查步骤? # 首先确定物理链路是否联通正常。 查看本机IP，路由，DNS的设置情况是否达标。 telnet检查服务器的WEB有没有开启以及防火墙是否阻拦。 ping一下网关，进行最基础的检查，通了，表示能够到达服务器。 测试到网关或路由器的通常情况，先测网关，然后再测路由器一级一级的测试。 测试ping公网ip的通常情况(记住几个外部IP)， 测试DNS的通畅。ping出对应IP。 通过以上检查后，还在网管的路由器上进行检查。 46、如果我们的网站打开速度慢请说下您的排查思路? # 判断原因\n首先我要以用户的身份登录我们的网站，判断问题出现在我们自身原因，还是用户那边的原因。\n如果是用户问题有以下几个原因：\n用户那边的带宽 用户的浏览器器版本低，安装插件太多 中毒和电脑里的垃圾文件过多 用户主机的主机的性能和操作系统 如果是我们的网站自身问题有一下几个原因\n网络带宽 服务器的cpu、硬盘、内存过低服务器负载不起来也就是说服务器自身的性能方面 网站代码不够完善。如mysql语句没有进行优化导致数据库读写耗时 服务器未开启图片压缩 网页台下 死连接过多插件使用及js文件调用频繁网站服务器的速度或是租用空间所在的服务器速度 解决思路\n1、检测服务器速度的快慢\nping命令查看连接到服务器的时间和丢包情况(ping 测试网址的) 查看丢包率(1000个包没有丢一个是最理想的、一般一个速度好的机房丢包率不超过1%) ping值要小同城电信adsl ping平均值绝对不能超过20，一般都在10，跨省的平均值20-40属于正常 ping值要均匀最小值和最大值相差太大说明路由不稳定的表现 2、查看服务器自身性能\n查看cpu的使用率uptime\n查看内存情况 free -m\n查看I/O读写iostat 磁盘I/O读写等看看是那个进程大量占用系统资源导致我的服务器变慢\n3、看看访问最多的URL和IP有什么特征，如果是恶意URL和IP就把他屏蔽掉如果是善意的就限流有可能是CDN回源量大造成网站无法访问\n4、查看同台服务器上其他网站的打开速度，可以通过查询工具查看和自己在同一台服务器上的网站个数和网址可以看他们打开快慢\n5、电信和联通互访的问题\n如果是空间打开时快时慢，有时打不开那就是空间不稳定找空间商解决或是换空间伤，如果是有的地方快有的地方慢应该是网络线路问题，比如电信用户访问放在联通服务器上的网站，联通用户访问放在电信服务器上的网站，解决办吧是：使用双线空间或是多线空间\n6、从网站自身的原因\n网站的程序设计结构是否合理是否由于幻灯片代码影响网站打开速度(找程序设计相关人士解决) 网页的设计结构和代码错误(请专业人士进行修改) 网页的内容如：大尺寸图片、大尺寸flash、过多的引用其他网站内容，如果被引用内容的网站速度慢，也影响自身网站把。譬如友情连接可以把对方 的图片放到自己网站上 解决办法\n优化图片，限制图片大小尺寸，降低图片质量，减少图片数量 限定图片的格式：jpg，png，gif 减少http的请求数(当打开网页时浏览器会发出很多对象请求，每个对象的加载都会有所延时，如果网页上的对象很多就会花费大量的时间，去除不必要的对象，将临近的图片合成一张，合并css文件) f r o m ：w l j s l m z 46、如何查看二进制文件的内容 # 我们一般通过 hexdump 命令 来查看二进制文件的内容。\nhexdump -C XXX(文件名) -C 是参数 不同的参数有不同的意义\n-C 是比较规范的 十六进制和 ASCII 码显示\n-c 是单字节字符显示\n-b 单字节八进制显示\n-o 是双字节八进制显示\n-d 是双字节十进制显示\n-x 是双字节十六进制显示\n等等等等\n47、你是怎么备份数据的，包括数据库备份？ # 在生产环境下，不管是应用数据、还是数据库数据首先在部署的时候就会有主从架构、或者集群，这本身就是属于数据的热备份；其实考虑冷备份，用专门一台服务器做为备份服务器，比如可以用rsync+inotify配合计划任务来实现数据的冷备份，如果是发版的包备份，正常情况下有台发布服务器，每次发版都会保存好发版的包。\n48、zabbix常用术语你知道几个？ # 主机（host）：要监控的网络设备，可由IP或DNS名称指定； 主机组（hostgroup）：主机的逻辑容器，可以包含主机和模板，但同一个组织内的主机和模板不能互相链接；主机组通常在给用户或用户组指派监控权限时使用； 监控项（item）：一个特定监控指标的相关的数据；这些数据来自于被监控对象；item是zabbix进行数据收集的核心，相对某个监控对象，每个item都由\u0026quot;key\u0026quot;标识； 触发器（trigger）：一个表达式，用于评估某监控对象的特定item内接收到的数据是否在合理范围内，也就是阈值；接收的数据量大于阈值时，触发器状态将从\u0026quot;OK\u0026quot;转变为\u0026quot;Problem\u0026quot;，当数据再次恢复到合理范围，又转变为\u0026quot;OK\u0026quot;； 事件（event）：触发一个值得关注的事情，比如触发器状态转变，新的agent或重新上线的agent的自动注册等； 动作（action）：指对于特定事件事先定义的处理方法，如发送通知，何时执行操作； 报警升级（escalation）：发送警报或者执行远程命令的自定义方案，如每隔5分钟发送一次警报，共发送5次等； 媒介（media）：发送通知的手段或者通道，如Email、Jabber或者SMS等； 通知（notification）：通过选定的媒介向用户发送的有关某事件的信息；远程命令（remote command）：预定义的命令，可在被监控主机处于某特定条件下时自动执行； 模板（template）：用于快速定义被监控主机的预设条目集合，通常包含了item、trigger、graph、screen、application以及low-level discovery rule；模板可以直接链接至某个主机；应用（application）：一组item的集合； web场景（webscennario）：用于检测web站点可用性的一个活多个HTTP请求；前端（frontend）：Zabbix的web接口； 49、虚拟化技术有哪些表现形式 # 完全拟化技术：通过软件实现对操作系统的资源再分配，比较成熟，完全虚拟化代表技术：KVM、ESXI、Hyper-V。 半虚拟化技术：通过代码修改已有的系统，形成一种新的可虚拟化的系统，调用硬件资源去安装多个系统，整体速度上相对高一点，半虚拟化代表技术：Xen。 轻量级虚拟化：介于完全虚拟化、半虚拟化之间，轻量级虚拟化代表技术：Docker。 50、修改线上业务配置文件流程 # 先告知运维经理和业务相关开发人员 在测试环境测试，并备份之前的配置文件 测试无误后修改生产环境配置 观察生产环境是否正常，是否有报警 完成配置文件更改 "},{"id":136,"href":"/docs/metallb-l2-%E5%8E%9F%E7%90%86-metallbl2-yuan-li/","title":"MetalLB L2 原理 2024-04-03 14:52:24.662","section":"Docs","content":" 穷人版 LB \u0026ndash; MetalLB L2 原理 # LB 的作用 # 在 Kubernetes 集群中，要想把服务暴露给别人访问，无外乎是使用 Service NodePort 或者 hostNetwork 网络模式。这种方式会使得主机端口随着服务的数量增加而增加，显然不建议使用。 # 那么可以使用网关来转发到后端服务，这样只需要开启网关的 Service NodePort 即可，显然网关承载了入口流量，网关就需要实现高可用。一般会在网关前面部署一个 LB ，流量经过 LB 负载到多个网关示例。那么可能又有人会，这个 LB 又怎么高可用？ # 公有云厂商直接提供 LB，对于自行搭建的 Kubetnetes 集群，就需要一个乞丐版的 LB 来实现，开源的 LB 也有很多，这里介绍 Metallb[1]，MetalLB 实现了所说的负载均衡功能，同时自身也是高可用的。 # 使用 Metallb # Metallb 提供了自行搭建 Kubernetes 集群负载均衡的功能，类似于公有云一样来体验负载均衡。 # Metallb 提供两种模式：BGP、L2。两种模式各有优缺点，本文主要讲解 L2 模式。 # 依赖 # 在 Kubernetes 集群中使用 Metallb 需要满足以下需求： # • Kubernetes 版本 ≥ v1.13 # • 提供 IP 地址段或者多个 IP 给 Metallb 分配 # • 如果使用 L2 模式，主机上需要放行 7946 端口 # 部署 # 修改 Kube-proxy 配置 # $ kubectl edit configmap -n kube-system kube-proxy apiVersion: kubeproxy.config.k8s.io/v1alpha1 kind: KubeProxyConfiguration mode: \u0026#34;ipvs\u0026#34; ipvs: strictARP: true 通过 manifests 安装，当然也可以通过 helm 或者 operator 方式安装，可参考官网[2] # $ kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.13.12/config/manifests/metallb-native.yaml 部署成功后，集群中在 metallb-system namespace 下会存在名为 Controller 的 deployment 和 Speaker 的 daemonSet # • metallb-system/controller deployment，该 Controller 用于 watch 集群中使用 LoadBalancer 类型的 service 并为其分配 EXTERNAL-IP # • metallb-system/speaker daemonset，该组件在每个节点都会运行，使用 hostNetwork 网络模式，可以保证上面分配的 IP 可访问。 # $ kubectl get pods -n metallb-system -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES controller-679f6b6cb5-l5k2z 1/1 Running 0 46h 100.80.192.27 node-172-30-91-215 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; speaker-6qxxj 1/1 Running 0 46h 172.30.91.17 node-172-30-91-17 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; speaker-chb8g 1/1 Running 0 46h 172.30.91.215 node-172-30-91-215 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; speaker-kw9vd 1/1 Running 0 46h 172.30.91.166 master-172-30-91-166 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 配置 # 上面只是部署了 Metallb，要想使用 Metallb L2 模式还需要一些配置，才能正常使用。 # 上面说到 Metallb 会自动分配 IP 给集群中的 LoadBalancer service，所以就需要提供地址池给 Metallb，通过创建 IPAddressPool CR # 地址池可以支持 IP 网段，也支持 IP 组 # # 地址池配置多个 IP apiVersion: metallb.io/v1beta1 kind: IPAddressPool metadata: name: subnet-91 namespace: metallb-system spec: addresses: - 172.30.91.2/32 - 172.30.91.3/32 - 172.30.91.4/32 # 地址池配置 IP 网络段 apiVersion: metallb.io/v1beta1 kind: IPAddressPool metadata: name: subnet-91 namespace: metallb-system spec: addresses: - 172.30.91.0/24 上面配置创建成功后，可通过命令查看 # $ kubectl get IPAddressPool -n metallb-system NAME AUTO ASSIGN AVOID BUGGY IPS ADDRESSES subnet-91 true false [\u0026#34;172.30.91.2/32\u0026#34;,\u0026#34;172.30.91.3/32\u0026#34;,\u0026#34;172.30.91.4/32\u0026#34;] 1、以上配置的 IP 需要路由可达或者和 K8S 集群节点同属一个网段\n2、在云平台上可能还需要将 IP 地址池绑定到 K8S 集群节点上\n下面创建 L2 模式宣告配置 # apiVersion: metallb.io/v1beta1 kind: L2Advertisement metadata: name: subnet-91 namespace: metallb-system spec: ipAddressPools: # 上面的 ippooladdress name - subnet-91 同样可通过命令查看 # $ kubectl get L2Advertisement -n metallb-system NAME IPADDRESSPOOLS IPADDRESSPOOL SELECTORS INTERFACES subnet-91 [\u0026#34;subnet-91\u0026#34;] 示例 # 下面通过创建 nginx 服务并使用 Metallb 来访问 # 通过下面 yaml 来创建 nginx 服务 # apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:latest ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx-service spec: selector: app: nginx ports: - protocol: TCP port: 80 targetPort: 80 type: LoadBalancer 创建成功后，会发现该 nginx service 已经自动分配了 EXTERNAL-IP为了方便后面描述，后面篇幅看到 VIP 可以理解为 EXTERNAL-IP # $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx-service LoadBalancer 10.233.21.231 172.30.91.2 80:32105/TCP 9s 通过 http://172.30.91.2 和 http://172.30.91.166:32105 访问 # $ curl http://172.30.91.2 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026#34;http://nginx.org/\u0026#34;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026#34;http://nginx.com/\u0026#34;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; $ curl http://172.30.91.166:32105 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026#34;http://nginx.org/\u0026#34;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026#34;http://nginx.com/\u0026#34;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 可以发现使用 VIP 和 NodePort 最终都是负载到多个 Nginx 实例上，那岂不是 NodePort 更方便？ # 假如某个 Node 运行异常，节点 IP 无法使用，那么客户端就需要切换 Node IP 访问，显然没有高可用；对于使用 VIP 访问，MetalLB 自带高可用属性，无需担心节点异常等异常场景，下文会讲解高可用是如何实现的。 # Metallb L2 原理 # 通过上面的示例，大家是否会有这两个疑问： # • VIP 没有绑定到服务器的任何一张网卡上，为什么这个 ip 可以访问？ # • VIP 是如何将流量负载到对应后端服务的？ # • MetalLB 自身是如何实现高可用？ # arp 请求 # 先看第一个问题，两张网卡通过一根网线连接，这是一个最简易的二层组网。二层网络是不需要路由转发的，只需拿到对端的 MAC 地址即可封装报文。下面通过实验看看数据包是如何传输的？ # 可以利用 Linux 虚拟网络技术来模拟上面组网环境，Linux 虚拟网络技术可参考前文。将两个 Namespace 通过 veth pair 连接起来，并验证连通性。 # 创建两个 namespace，ns1、ns2\n$ ip netns add ns1 $ ip netns add ns2 创建一个 veth pair # $ ip link add veth-ns1 type veth peer name veth-ns2 将 veth pair 一端接入放入 ns1，另一端接入 ns2，这样就相当于采用网线将两个 Network Namespace 连接起来了。 # $ ip link set veth-ns1 netns ns1 $ ip link set veth-ns2 netns ns2 为两个网卡分别设置 IP 地址，这两个网卡的地址位于同一个子网 192.168.1.0/24 中。 # $ ip -n ns1 addr add 192.168.1.1/24 dev veth-ns1 $ ip -n ns2 addr add 192.168.1.2/24 dev veth-ns2 使用 ip link 命令设置两张虚拟网卡状态为 up。 # $ ip -n ns1 link set veth-ns1 up $ ip -n ns2 link set veth-ns2 up 从 ns1 ping ns2 的 IP，测试连通性，并在 ns1 里抓包。 # $ ping 192.168.1.2 -w 3 PING 192.168.1.2 (192.168.1.2) 56(84) bytes of data. 64 bytes from 192.168.1.2: icmp_seq=1 ttl=64 time=0.074 ms 64 bytes from 192.168.1.2: icmp_seq=2 ttl=64 time=0.049 ms 64 bytes from 192.168.1.2: icmp_seq=3 ttl=64 time=0.040 ms # 同时在 ns1 抓 arp 包 $ tcpdump -nn -i veth-ns1 arp tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on veth-ns1, link-type EN10MB (Ethernet), capture size 262144 bytes 10:47:45.568080 ARP, Request who-has 192.168.1.2 tell 192.168.1.1, length 28 10:47:45.568104 ARP, Reply 192.168.1.2 is-at 3a:c2:65:ac:c2:fb, length 28 # 查看 ns1 下的 arp 记录，发现存在一条 192.168.1.2 的 arp 记录 $ arp Address HWtype HWaddress Flags Mask Iface 192.168.1.2 ether 3a:c2:65:ac:c2:fb C veth-ns1 可以发现在 ns1 访问 ns2，会在这个广播域内发送一个 ARP 请求，这个请求报文就是为了拿到对端的 MAC 地址，只有有该 MAC 地址的设备才会回复该请求。拿到对端的 MAC 地址后就可以到达物理层进而到对端完成解包。 # 上面的示例，是将 IP 绑定在一个真实的网卡上，每张网卡都会有一个独一无二的 MAC 地址，这样该网卡就会自动回复 ARP 请求。那么对于一个虚拟 IP，它没有绑定任何设备，又该如何回复该虚拟 IP 地址的 ARP 请求呢？ # 回到 MetalLB 中，MetalLB L2 模式就是通过充当 VIP 的物理设备来实现的，Metallb 的 Leader Speaker 组件会不断监听该 VIP 的 ARP 请求，然后把当前 Spaeker 所在的 Node 主机网卡 MAC 地址回复给对端，这就是为什么 Speaker 为什么使用 hostNetwork 网络模式。这样就类似将 VIP 绑定在某个 K8S 节点的物理网卡上，只不过回复该 VIP 的 ARP 请求是程序并不是 Linux 内核。 # 下面还是通过 Nginx 示例演示： # 环境有三个 K8S 节点，且部署一个 nginx 服务，使用 MetalLB 分配 LB # $ kubectl get node NAME STATUS ROLES AGE VERSION master-172-30-91-166 Ready control-plane,master 32d v1.22.17 node-172-30-91-17 Ready \u0026lt;none\u0026gt; 32d v1.22.17 node-172-30-91-215 Ready \u0026lt;none\u0026gt; 32d v1.22.17 $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx-service LoadBalancer 10.233.69.89 172.30.91.43 80:31729/TCP 18m 在 master-172-30-91-166上向 VIP 发 ARP 请求，发现可以正常回复 MAC 地址，那么这个 MAC 地址是哪个 Speaker 响应的？ # $ arping -c 1 -I eth0 172.30.91.43 ARPING 172.30.91.43 from 172.30.91.166 eth0 Unicast reply from 172.30.91.43 [FA:16:3E:7E:79:C9] 1.062ms Unicast reply from 172.30.91.43 [FA:16:3E:7E:79:C9] 1.218ms Unicast reply from 172.30.91.43 [FA:16:3E:74:CC:83] 3.298ms Sent 1 probes (1 broadcast(s)) Received 3 response(s) MetalLB Speaker 在选举完成后，会向对应 Service 发送一个 Event，表明被哪个节点宣告了。通过以下命令可以知道该 VIP 会被 node-172-30-91-215 上的 Speaker 响应。 # $ kubectl describe svc nginx-service Name: nginx-service Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: metallb.universe.tf/ip-allocated-from-pool: subnet-91 Selector: app=nginx Type: LoadBalancer IP Family Policy: SingleStack IP Families: IPv4 IP: 10.233.69.89 IPs: 10.233.69.89 LoadBalancer Ingress: 172.30.91.43 Port: \u0026lt;unset\u0026gt; 80/TCP TargetPort: 80/TCP NodePort: \u0026lt;unset\u0026gt; 31729/TCP Endpoints: 100.76.222.87:80 Session Affinity: None External Traffic Policy: Cluster Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal IPAllocated 21m metallb-controller Assigned IP [\u0026#34;172.30.91.43\u0026#34;] Normal nodeAssigned 21m metallb-speaker announcing from node \u0026#34;node-172-30-91-215\u0026#34; with protocol \u0026#34;layer2\u0026#34; 查看对应 Speaker pod 日志，可以发现日志详细打印了 ARP 请求的源 IP，源 MAC，以及 VIP 和 VIP MAC 地址。该 MAC 地址又是属于哪个网卡设备呢？ # $ kubectl get pods -n metallb-system -o wide | grep node-172-30-91-215 speaker-r55rn 1/1 Running 0 6d2h 172.30.91.215 node-172-30-91-215 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; $ kubectl logs -f pods/speaker-r55rn -n metallb-system|grep arp {\u0026#34;caller\u0026#34;:\u0026#34;arp.go:110\u0026#34;,\u0026#34;interface\u0026#34;:\u0026#34;eth0\u0026#34;,\u0026#34;ip\u0026#34;:\u0026#34;172.30.91.43\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;debug\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;got ARP request for service IP, sending response\u0026#34;,\u0026#34;responseMAC\u0026#34;:\u0026#34;fa:16:3e:7e:79:c9\u0026#34;, \u0026#34;senderIP\u0026#34;:\u0026#34;172.30.91.166\u0026#34;,\u0026#34;senderMAC\u0026#34;:\u0026#34;fa:16:3e:43:bf:71\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2024-01-08T09:36:31Z\u0026#34;} 根据 Speaker 原理，Speaker 会回复其节点上的可通信的物理网卡的 MAC 地址 # $ ifconfig eth0 eth0: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet 172.30.91.215 netmask 255.255.255.0 broadcast 172.30.91.255 inet6 fe80::f816:3eff:fe7e:79c9 prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; ether fa:16:3e:7e:79:c9 txqueuelen 1000 (Ethernet) RX packets 317533468 bytes 165612717123 (154.2 GiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 291439852 bytes 95753527594 (89.1 GiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 发现 Speaker 回复的就是当前节点的 eth0 MAC 地址。 # 通过一系列的实验和原理说明，网络中一个 IP 没有绑定到某个设备，只要该 IP 在网络层是可到达，那么只要能够回复该 IP 的 ARP 请求就能实现通信。MetalLB L2 模式也是如此，到这基本回答了提出的第一个问题。 # 负载均衡 # 第二个问题实际上是每个 LB 都需要解决的，因为这就是 LB 需要实现的功能，将请求的流量负载到所有的后端实例上。 # MetalLB 的负载均衡实际上是 Kube-proxy 实现的，MetalLB 将 IP 更新至 LoadBalancer Service 后，Kube-proxy 会 Watch 到 Service 的更新操作，直接在集群每个节点上创建一条 IPVS 规则。该条规则就是将上述 MetalLB 分配的 IP 负载到该 Service 的 Endpoint，进而实现了负载均衡的功能。 # $ ipvsadm -Ln |grep 172.30.91.43 -A3 TCP 172.30.91.43:80 rr -\u0026gt; 100.76.222.87:80 Masq 1 0 0 这里就回答了第二个问题。 # 工作过程 # MetalLB 分为两部分组成，分别是 Controller 和 Speaker。两者的分工如下所示： # • Controller 负责监听 Service 变化，依据对应的 IP 池进行 IP 分配。 # • Speaker 负责监听 Service 的变化，并依据具体的协议发起相应的广播或应答、节点的 Leader 选举。 # 所以说 MetalLB 的 Controller 和 Speaker 并没有直接通讯，而是依赖于集群中的 LoadBalancer Service 间接协作 # 下面针对集群中新增一个 LoadBalancer Service 阐述 MetalLB 的工作过程： # • Controller Watch 到集群中新增了一个 LoadBalancer Service，从 IpPoolAddress 中获取一个 没有被使用的 IP，并至该 Service 的 status.loadBalancer # • Kube-proxy Watch 到 Service 的更新操作，在集群每个节点上创建一条 IPVS 规则，该规则就是将上述 MetalLB 分配的 IP 负载到该 Service 的 Endpoint # • Speaker 是 DaemonSet 类型的工作负载，所以每个 K8S 节点都有一个实例。上述 Controller 和 Kube-proxy 工作的同时，Speaker 也同时 Watch Service 的新增。首先根据 memberlist[3] 选举出一个 Leader (这里就回答了第三个问题)，会不断监听该 VIP 的 ARP 请求，然后把当前 Spaeker 所在的 Node 主机网卡 MAC 地址回复给对端。 # • Speaker 会向 Service 发送一条 event 记录，表明该 Service 的 LoadBalancer 被该节点承载流量。 # 根据上面的描述，就是对于一个 LoadBalancer Service 最终只有一个节点承载流量，当前节点出现故障会立即进行选主，新的 Leader Speaker 会承载 ARP 请求，显然这种高可用的方式不是很纯粹。\n这里说的并不是所有的 Service 的流量都被某一个节点承载，只是当前 Service 的生命周期的流量都在一个节点上。因为每个 Service 都会通过 memberlist[4] 选举一个 Leader。 # 这时候通过 172.30.91.43 去访问 nginx 服务时，整个数据流是什么样的？ # • 向 172.30.91.43 发起请求时，数据包到数据链路层发送 ARP 广播请求，询问该广播域谁有 172.30.91.43 的 MAC 地址 # • 这时候 Speaker Leader 会回复该 ARP 请求，将当前节点的网卡 MAC 地址回复给对端 # • 客户端拿到了 MAC 地址后数据包继续封装发送成功到达对端，也就是 Speaker Leader 节点 # • 根据上面介绍 Kube-proxy 会在 MetalLB 分配 IP 后在每个节点创建一条 IPVS 转发规则，将请求流量负载到后端每个实例。所以流量到达 Speaker Leader 节点后会被负载到后端多个 nginx 实例上 # 总结 # 本篇文章主要讲解了 MetalLB L2 模式的使用和原理，其使用非常简洁。L2 模式的原理也很简单，就是 Seeaker 充当 VIP 的物理设备回复广播域中的 ARP 请求。 # 但是 L2 模式只能将流量引流到集群中的某台主机上，从而导致集群对外暴露的流量受限于单台主机的流量限制。BGP模式是一种较为理想的实现。BGP需要支持BGP路由协议的软路由或者硬件路由器设备，而无需定制的负载均衡器。对于一些高性能转发的场景，建议使用 BGP 模式。 # "},{"id":137,"href":"/docs/netctl%E6%A3%80%E6%B5%8B%E9%9B%86%E7%BE%A4pod%E9%97%B4%E8%BF%9E%E9%80%9A%E6%80%A7-netctl-jian-ce-ji-qun-pod-jian-lian-tong-xing/","title":"netctl检测集群pod间连通性 2024-08-02 18:00:36.753","section":"Docs","content":" NetDoctor # English | 中文\n介绍 # Kubernetes集群投入使用后集群网络可能会存在种种的连通性问题，因此我们希望可以有一个验收工具，在完成部署后检查集群的网络连通性是否正常。\n另一方面，Kosmos是一个跨集群的解决方案，在Kosmos管理多个集群之前，需要先检查各个集群的容器网络自身是否存在问题，部署完成后也需要验证跨集群的网络是否已经被Kosmos连通。\n出于以上两个方面，我们设计了NetDoctor工具用于解决Kubernetes集群遇到的网络问题。\n架构 # 先决条件 # go 版本 v1.15+ kubernetes 版本 v1.16+ 快速开始 # netctl工具 # NetDoctor提供配套工具netctl，您可以方便的通过命令行去进行Kubernetes集群的网络连通性检查。 从制品库获取 # wget https://github.com/kosmos-io/netdoctor/releases/download/v0.0.1/netctl-linux-amd64 mv netctl-linux-amd64 netctl 从源码编译 # # 下载项目源码 $ git clone https://github.com/kosmos-io/netdoctor.git # 执行后netctl会输出至./netdoctor/_output/bin/linux/amd64目录 $ make netctl netctl命令 # netctl init命令用于在当前目录生成网络检查需要的配置文件config.json，示例如下： $ netctl init I0205 16:27:26.258964 2765415 init.go:69] write opts success $ cat config.json { \u0026#34;namespace\u0026#34;: \u0026#34;kosmos-system\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;v0.2.0\u0026#34;, \u0026#34;protocol\u0026#34;: \u0026#34;tcp\u0026#34;, \u0026#34;podWaitTime\u0026#34;: 30, \u0026#34;port\u0026#34;: \u0026#34;8889\u0026#34;, \u0026#34;maxNum\u0026#34;: 3, \u0026#34;cmdTimeout\u0026#34;: 10, \u0026#34;srcKubeConfig\u0026#34;: \u0026#34;~/.kube/config\u0026#34;, \u0026#34;srcImageRepository\u0026#34;: \u0026#34;ghcr.io/kosmos-io\u0026#34; } netctl check命令会读取config.json，然后创建一个名为Floater的DaemonSet以及相关联的一些资源，之后会获取所有的Floater的IP信息，然后依次进入到Pod中执行Ping或者Curl命令。需要注意的是，这个操作是并发执行的，并发度根据config.json中的maxNum参数动态变化。 $ netctl check I0205 16:34:06.147671 2769373 check.go:61] use config from file!!!!!! I0205 16:34:06.148619 2769373 floater.go:73] create Clusterlink floater, namespace: kosmos-system I0205 16:34:06.157582 2769373 floater.go:83] create Clusterlink floater, apply RBAC I0205 16:34:06.167799 2769373 floater.go:94] create Clusterlink floater, version: v0.2.0 I0205 16:34:09.178566 2769373 verify.go:79] pod: clusterlink-floater-9dzsg is ready. status: Running I0205 16:34:09.179593 2769373 verify.go:79] pod: clusterlink-floater-cscdh is ready. status: Running Do check... 100% [================================================================================] [0s] +-----+----------------+----------------+-----------+-----------+ | S/N | SRC NODE NAME | DST NODE NAME | TARGET IP | RESULT | +-----+----------------+----------------+-----------+-----------+ | 1 | ecs-net-dr-001 | ecs-net-dr-001 | 10.0.1.86 | SUCCESSED | | 2 | ecs-net-dr-002 | ecs-net-dr-002 | 10.0.2.29 | SUCCESSED | +-----+----------------+----------------+-----------+-----------+ +-----+----------------+----------------+-----------+-----------+-------------------------------+ | S/N | SRC NODE NAME | DST NODE NAME | TARGET IP | RESULT | LOG | +-----+----------------+----------------+-----------+-----------+-------------------------------+ | 1 | ecs-net-dr-002 | ecs-net-dr-001 | 10.0.1.86 | EXCEPTION |exec error: unable to upgrade | | 2 | ecs-net-dr-001 | ecs-net-dr-002 | 10.0.2.29 | EXCEPTION |connection: container not......| +-----+----------------+----------------+-----------+-----------+-------------------------------+ I0205 16:34:09.280220 2769373 do.go:93] write opts success 在check命令执行的过程中，会有进度条显示校验进度。命令执行完成后，会打印检查结果，并将结果保存在文件resume.json中。 [ { \u0026#34;Status\u0026#34;: 0, \u0026#34;ResultStr\u0026#34;: \u0026#34;exec error: unable to upgrade connection: container not found (\\\u0026#34;floater\\\u0026#34;), stderr: \u0026#34;, \u0026#34;srcNodeName\u0026#34;: \u0026#34;ecs-sealos-001\u0026#34;, \u0026#34;dstNodeName\u0026#34;: \u0026#34;ecs-sealos-002\u0026#34;, \u0026#34;targetIP\u0026#34;: \u0026#34;10.0.2.29\u0026#34; }, { \u0026#34;Status\u0026#34;: 0, \u0026#34;ResultStr\u0026#34;: \u0026#34;exec error: command terminated with exit code 7, stderr % Total % Received % Xferd Average Speed Time Time Time Current\\n Dload Upload Total Spent Left Speed\\n\\r 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0\\r 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0\\ncurl: (7) Failed to connect to 10.0.0.36 port 8889 after 0 ms: Couldn\u0026#39;t connect to server\\n\u0026#34;, \u0026#34;srcNodeName\u0026#34;: \u0026#34;ecs-sealos-002\u0026#34;, \u0026#34;dstNodeName\u0026#34;: \u0026#34;ecs-sealos-001\u0026#34;, \u0026#34;targetIP\u0026#34;: \u0026#34;10.0.0.36\u0026#34; } ] 如果需要检查Kosmos集群联邦中任意两个集群之间的网络连通性，则可以在配置文件config.json增加参数dstKubeConfig和dstImageRepository，这样就可以检查两个集群之间网络连通性了。 $ vim config.json { \u0026#34;namespace\u0026#34;: \u0026#34;kosmos-system\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;v0.2.0\u0026#34;, \u0026#34;protocol\u0026#34;: \u0026#34;tcp\u0026#34;, \u0026#34;podWaitTime\u0026#34;: 30, \u0026#34;port\u0026#34;: \u0026#34;8889\u0026#34;, \u0026#34;maxNum\u0026#34;: 3, \u0026#34;cmdTimeout\u0026#34;: 10, \u0026#34;srcKubeConfig\u0026#34;: \u0026#34;~/.kube/src-config\u0026#34;, \u0026#34;srcImageRepository\u0026#34;: \u0026#34;ghcr.io/kosmos-io\u0026#34; \u0026#34;dstKubeConfig\u0026#34;: \u0026#34;~/.kube/dst-config\u0026#34;, \u0026#34;dstImageRepository\u0026#34;: \u0026#34;ghcr.io/kosmos-io\u0026#34; } netctl resume命令用于复测时只检验第一次检查时有问题的集群节点。因为线上环境节点数量很多，单次检查可能会需要比较长的时间才能生成结果，所以我们希望仅对前一次检查异常的节点进行复测。resume命令因此被开发，该命令会读取resume.json文件，并对前一次异常的节点进行再次检查，我们可以重复执行此命令至没有异常的结果后再执行全量检查。 $ netctl resume I0205 16:34:06.147671 2769373 check.go:61] use config from file!!!!!! I0205 16:34:06.148619 2769373 floater.go:73] create Clusterlink floater, namespace: kosmos-system I0205 16:34:06.157582 2769373 floater.go:83] create Clusterlink floater, apply RBAC I0205 16:34:06.167799 2769373 floater.go:94] create Clusterlink floater, version: v0.2.0 I0205 16:34:09.178566 2769373 verify.go:79] pod: clusterlink-floater-9dzsg is ready. status: Running I0205 16:34:09.179593 2769373 verify.go:79] pod: clusterlink-floater-cscdh is ready. status: Running Do check... 100% [================================================================================] [0s] +-----+----------------+----------------+-----------+-----------+ | S/N | SRC NODE NAME | DST NODE NAME | TARGET IP | RESULT | +-----+----------------+----------------+-----------+-----------+ | 1 | ecs-net-dr-002 | ecs-net-dr-001 | 10.0.1.86 | SUCCESSED | | 2 | ecs-net-dr-001 | ecs-net-dr-002 | 10.0.2.29 | SUCCESSED | +-----+----------------+----------------+-----------+-----------+ netctl clean命令用于清理NetDoctor创建的所有资源。 贡献代码 # 我们欢迎任何形式的帮助，包括但不限定于完善文档、提出问题、修复 Bug 和增加特性。 联系我们 # 如果您在使用过程中遇到了任何问题，欢迎提交 Issue 进行反馈。 您也可以扫描WeChat加入技术交流群。 "},{"id":138,"href":"/docs/network-check-network-check/","title":"network-check 2023-09-28 15:36:33.04","section":"Docs","content":" network-check # 网络测试组件，根据cilium connectivity-check 脚本修改而来；利用cronjob 定期检测集群各节点、容器、serviceip、nodeport等之间的网络联通性；可以方便的判断当前集群网络是否正常。\n目前检测如下：\nkubectl get cronjobs.batch -n network-test NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE test01-pod-to-container */5 * * * * False 0 3m19s 6d3h test02-pod-to-node-nodeport */5 * * * * False 0 3m19s 6d3h test03-pod-to-multi-node-clusterip */5 * * * * False 1 6d3h 6d3h test04-pod-to-multi-node-headless */5 * * * * False 1 6d3h 6d3h test05-pod-to-multi-node-nodeport */5 * * * * False 1 6d3h 6d3h test06-pod-to-external-1111 */5 * * * * False 0 3m19s 6d3h test07-pod-to-external-fqdn-baidu */5 * * * * False 0 3m19s 6d3h test08-host-to-multi-node-clusterip */5 * * * * False 1 6d3h 6d3h test09-host-to-multi-node-headless */5 * * * * False 1 6d3h 6d3h 带multi-node的测试需要多节点集群才能运行，如果单节点集群，测试pod会处于Pending状态 带external的测试需要节点能够访问互联网，否则测试会失败 启用网络检测 # 下载额外容器镜像 ./ezdown -X network-check\n配置集群，在配置文件/etc/kubeasz/clusters/xxx/config.yml (xxx为集群名) 修改如下选项\n# network-check 自动安装 network_check_enabled: true network_check_schedule: \u0026#34;*/5 * * * *\u0026#34; # 检测频率，默认5分钟执行一次 安装网络检测插件 docker exec -it kubeasz ezctl setup xxx 07 检查测试结果 # 大约等待5分钟左右，查看运行结果，如果pod 状态为Completed 表示检测正常通过。\nkubectl get pod -n network-test NAME READY STATUS RESTARTS AGE echo-server-58d7bb7f6-77ps6 1/1 Running 0 6d4h echo-server-host-cc87c966d-bk57t 1/1 Running 0 6d4h test01-pod-to-container-27606775-q6xlb 0/1 Completed 0 3m10s test02-pod-to-node-nodeport-27606775-x2v5d 0/1 Completed 0 3m10s test03-pod-to-multi-node-clusterip-27597895-cbq8d 0/1 Pending 0 6d4h test04-pod-to-multi-node-headless-27597895-qzsgz 0/1 Pending 0 6d4h test05-pod-to-multi-node-nodeport-27597895-kb5r7 0/1 Pending 0 6d4h test06-pod-to-external-1111-27606775-p6v8s 0/1 Completed 0 3m10s test07-pod-to-external-fqdn-baidu-27606775-qdfwd 0/1 Completed 0 3m10s test08-host-to-multi-node-clusterip-27597895-qsgn9 0/1 Pending 0 6d4h test09-host-to-multi-node-headless-27597895-hpkt5 0/1 Pending 0 6d4h pod 状态为Completed 表示检测正常通过 pod 状态为Pending 表示该检测需要多节点的k8s集群才会运行 禁用网络检测 # 如果集群已经开启网络检测，检测结果符合预期，并且不想继续循环检测时，只要删除对应namespace即可\nkubectl delete ns network-test "},{"id":139,"href":"/docs/nginx%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E6%83%8A%E7%BE%A4%E6%95%88%E5%BA%94-nginx-ru-he-jie-jue-jing-qun-xiao-ying/","title":"nginx如何解决惊群效应 2024-08-02 17:45:54.531","section":"Docs","content":" 前置知识 # linux 网络处理的基本方法：bind listen accept epoll 的基本方法：epoll_create epoll_ctl epoll_wait 什么是惊群效应？ # 第一次听到的这个名词的时候觉得很是有趣，不知道是个什么意思，总觉得又是奇怪的中文翻译导致的。\n复杂的说（来源于网络）TLDR;\n惊群效应（thundering herd）是指多进程（多线程）在同时阻塞等待同一个事件的时候（休眠状态），如果等待的这个事件发生，那么他就会唤醒等待的所有进程（或者线程），但是最终却只能有一个进程（线程）获得这个时间的“控制权”，对该事件进行处理，而其他进程（线程）获取“控制权”失败，只能重新进入休眠状态，这种现象和性能浪费就叫做惊群效应。\n简单的讲（我的大白话）\n有一道雷打下来，把很多人都吵醒了，但只有其中一个人去收衣服了。也就是：有一个请求过来了，把很多进程都唤醒了，但只有其中一个能最终处理。\n原因\u0026amp;问题 # 说起来其实也简单，多数时候为了提高应用的请求处理能力，会使用多进程（多线程）去监听请求，当请求来时，因为都有能力处理，所以就都被唤醒了。\n而问题就是，最终还是只能有一个进程能来处理。当请求多了，不停地唤醒、休眠、唤醒、休眠，做了很多的无用功，上下文切换又累，对吧。那怎么解决这个问题呢？下面就是今天要看的重点，我们看看 nginx 是如何解决这个问题的。\nNginx 架构 # 第一点我们需要了解 nginx 大致的架构是怎么样的。nginx 将进程分为 master 和 worker 两类，非常常见的一种 M-S 策略，也就是 master 负责统筹管理 worker，当然它也负责如：启动、读取配置文件，监听处理各种信号等工作。\n但是，第一个要注意的问题就出现了，master 的工作有且只有这些，对于请求来说它是不管的，就如同图中所示，请求是直接被 worker 处理的。如此一来，请求应该被哪个 worker 处理呢？worker 内部又是如何处理请求的呢？\nNginx 使用 epoll # 接下来我们就要知道 nginx 是如何使用 epoll 来处理请求的。下面可能会涉及到一些源码的内容，但不用担心，你不需要全部理解，只需要知道它们的作用就可以了。顺便我会简单描述一下我是如何去找到这些源码的位置的。\nMaster 的工作 # 其实 Master 并不是毫无作为，至少端口是它来占的。https://github.com/nginx/nginx/blob/b489ba83e9be446923facfe1a2fe392be3095d1f/src/core/ngx_connection.c#L407C13-L407C13\nngx_open_listening_sockets(ngx_cycle_t *cycle) { ..... for (i = 0; i \u0026lt; cycle-\u0026gt;listening.nelts; i++) { ..... if (bind(s, ls[i].sockaddr, ls[i].socklen) == -1) { if (listen(s, ls[i].backlog) == -1) { } 那么，根据我们 nginx.conf 的配置文件，看需要监听哪个端口，于是就去 bind 的了，这里没问题。\n【发现源码】这里我是直接在代码里面搜 bind 方法去找的，因为我知道，不管你怎么样，你总是要绑定端口的\n然后是创建 worker 的，虽不起眼，但很关键。https://github.com/nginx/nginx/blob/b489ba83e9be446923facfe1a2fe392be3095d1f/src/os/unix/ngx_process.c#L186\nngx_spawn_process(ngx_cycle_t *cycle, ngx_spawn_proc_pt proc, void *data, char *name, ngx_int_t respawn) { .... pid = fork(); 【发现源码】这里我直接搜 fork，整个项目里面需要 fork 的情况只有两个地方，很快就找到了 worker\n由于是 fork 创建的，也就是复制了一份 task_struct 结构。所以 master 的几乎全部它都有。\nWorker 的工作 # Nginx 有一个分模块的思想，它将不同功能分成了不同的模块，而 epoll 自然就是在 ngx_epoll_module.c 中了\nhttps://github.com/nginx/nginx/blob/b489ba83e9be446923facfe1a2fe392be3095d1f/src/event/modules/ngx_epoll_module.c#L330C23-L330C23\nngx_epoll_init(ngx_cycle_t *cycle, ngx_msec_t timer) { ngx_epoll_conf_t *epcf; epcf = ngx_event_get_conf(cycle-\u0026gt;conf_ctx, ngx_epoll_module); if (ep == -1) { ep = epoll_create(cycle-\u0026gt;connection_n / 2); 其他不重要，就连 epoll_ctl 和 epoll_wait 也不重要了，这里你需要知道的就是，从调用链路来看，是 worker 创建的 epoll 对象，也就是每个 worker 都有自己的 epoll 对象，而监听的sokcet 是一样的！\n【发现源码】这里更加直接，搜索 epoll_create 肯定就能找到\n问题的关键 # 此时问题的关键基本就能了解了，每个 worker 都有处理能力，请求来了此时应该唤醒谁呢？讲道理那不是所有 epoll 都会有事件，所有 worker 都 accept 请求？显然这样是不行的。那么 nginx 是如何解决的呢？\n如何解决 # 解决方式一共有三种，下面我们一个个来看：\naccept_mutex（应用层的解决方案） EPOLLEXCLUSIVE（内核层的解决方案） SO_REUSEPORT（内核层的解决方案） accept_mutex # 看到 mutex 可能你就知道了，锁嘛！这也是对于高并发处理的 ”基操“ 遇事不决加锁，没错，加锁肯定能解决问题。https://github.com/nginx/nginx/blob/b489ba83e9be446923facfe1a2fe392be3095d1f/src/event/ngx_event_accept.c#L328\n具体代码就不展示了，其中细节很多，但本质很容易理解，就是当请求来了，谁拿到了这个锁，谁就去处理。没拿到的就不管了。锁的问题很直接，除了慢没啥不好的，但至少很公平。\nEPOLLEXCLUSIVE # EPOLLEXCLUSIVE 是 2016 年 4.5+ 内核新添加的一个 epoll 的标识。它降低了多个进程/线程通过 epoll_ctl 添加共享 fd 引发的惊群概率，使得一个事件发生时，只唤醒一个正在 epoll_wait 阻塞等待唤醒的进程（而不是全部唤醒）。\n关键是：每次内核只唤醒一个睡眠的进程处理资源\n但，这个方案不是完美的解决了，它仅是降低了概率哦。为什么这样说呢？相比于原来全部唤醒，那肯定是好了不少，降低了冲突。但由于本质来说 socket 是共享的，当前进程处理完成的时间不确定，在后面被唤醒的进程可能会发现当前的 socket 已经被之前唤醒的进程处理掉了。\nSO_REUSEPORT # Nginx 在 1.9.1 版本加入了这个功能 https://www.nginx.com/blog/socket-sharding-nginx-release-1-9-1/\n其本质是利用了 Linux 的 reuseport 的特性，使用 reuseport 内核允许多个进程 listening socket 到同一个端口上，而从内核层面做了负载均衡，每次唤醒其中一个进程。\n反应到 Nginx 上就是，每个 Worker 进程都创建独立的 listening socket，监听相同的端口，accept 时只有一个进程会获得连接。效果就和下图所示一样。\n而使用方式则是：\nhttp { server { listen 80 reuseport; server_name localhost; # ... } } 从官方的测试情况来看确实是厉害\n当然，正所谓：完事无绝对，技术无银弹。这个方案的问题在于内核是不知道你忙还是不忙的。只会无脑的丢给你。与之前的抢锁对比，抢锁的进程一定是不忙的，现在手上的工作都已经忙不过来了，没机会去抢锁了；而这个方案可能导致，如果当前进程忙不过来了，还是会只要根据 reuseport 的负载规则轮到你了就会发送给你，所以会导致有的请求被前面慢的请求卡住了。\n总结 # 本文，从了解什么 ”惊群效应“ 到 nginx 架构和 epoll 处理的原理，最终分析三种不同的处理 “惊群效应” 的方案。分析到这里，我想你应该明白其实 nginx 这个多队列服务模型是所存在的一些问题，只不过绝大多数场景已经完完全全够用了。\n"},{"id":140,"href":"/docs/openkruise%E8%AF%A6%E7%BB%86%E8%A7%A3%E9%87%8A%E4%BB%A5%E5%8F%8A%E5%8E%9F%E5%9C%B0%E5%8D%87%E7%BA%A7%E5%8F%8A%E5%85%A8%E9%93%BE%E8%B7%AF%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83%E6%96%B9%E6%A1%88-openkruise-xiang-xi-jie-shi-yi-ji-yuan-de-sheng-ji-ji-quan-lian-lu-hui-du-fa-bu-fang-an/","title":"OpenKruise详细解释以及原地升级及全链路灰度发布方案 2024-07-22 11:44:46.892","section":"Docs","content":"OpenKruise简介 OpenKruise来源 它是由阿里巴巴集团的阿里云团队维护和开发的，并且在2018年将其贡献给了云原生计算基金会（CNCF），成为了CNCF的孵化项目。\nOpenKruise是什么？ OpenKruise 是一个基于 Kubernetes 的扩展项目，旨在增强和扩展 Kubernetes 的原生能力，以更好地支持大规模应用的管理和运维。它通过提供一系列自定义控制器和自定义资源（CRD），帮助用户在 Kubernetes 集群中更加灵活、高效地管理容器化应用。\n核心组件有什么？ 1.CloneSet: 功能: 用于管理一组具有相同模板的 Pod。类似于 Kubernetes 的 Deployment，但提供了更多高级特性，如灰度发布、并行和顺序更新策略、最大不可用副本数等。 用途: 适用于需要复杂更新策略和高可用性的应用场景。\n2.SidecarSet: 功能: 用于管理 Sidecar 容器。可以动态地将 Sidecar 容器注入到指定的 Pod 中，而不需要修改 Pod 的模板。 用途: 适用于需要在多个应用 Pod 中添加统一的辅助容器，如日志收集、监控代理等。\n3.StatefulSet: 功能: 扩展了 Kubernetes 的 StatefulSet 功能。支持有状态应用的管理，提供了更灵活的更新和扩展策略。 用途: 适用于有状态应用，如数据库、缓存服务等。\n4.Advanced DaemonSet: 功能: 提供了比原生 Kubernetes DaemonSet 更加灵活的功能，如灰度发布、并行和顺序更新策略等。 用途: 适用于需要在每个节点上运行一个副本的应用，如监控代理、日志收集代理等。\n5.BroadcastJob: 功能: 类似于 Kubernetes 的 Job，但用于在集群中所有或部分节点上运行一次性任务。 用途: 适用于需要在每个节点上执行一次性任务的场景，如节点初始化、数据分发等。\n6.ImagePullJob: 功能: 用于在集群的所有或部分节点上预拉取镜像，以减少 Pod 启动时的延迟。 用途: 适用于需要快速启动大量 Pod 的场景，如批量部署、大规模弹性扩容等。\n有什么特性和优势？ 高级发布策略: 支持蓝绿发布、金丝雀发布等高级发布策略，提供更灵活的应用更新方式。 高可用性和容错性: 通过高级的副本管理和更新策略，确保应用的高可用性和容错性。 易于集成: 与 Kubernetes 原生功能无缝集成，用户可以在现有 Kubernetes 集群上直接使用 OpenKruise 提供的功能。 社区支持: 由阿里巴巴开源，并得到社区广泛支持和贡献，持续更新和优化。\n适用于什么场景？ 互联网企业: 需要频繁更新和部署的在线服务，如电商网站、社交平台等。 金融行业: 高可用性和高安全性要求的金融应用，如银行系统、交易平台等。 大数据和 AI: 需要大规模计算和数据处理的应用，如数据分析、机器学习等。 企业 IT: 企业内部的各类业务系统和应用，如 CRM、ERP 系统等\n什么是OpenKruise的原地升级 因为后面会使用，这里详细讲一下\nOpenKruise 的原地升级（In-Place Update）是一种更新策略，允许在不销毁和重建 Pod 的情况下直接更新 Pod 内的容器镜像或资源配置。这种方法与传统的替换策略不同，可以显著减少更新过程中应用的中断时间和资源开销。\n原地升级的关键特性 无需重建 Pod: 优势: 避免了 Pod 重建过程中的网络重连、存储重新挂载等操作，减少了因重建导致的潜在服务中断和性能抖动。 用途: 特别适用于那些对重建敏感的应用，如状态有管理复杂的有状态应用。\n减少资源开销: 优势: 在更新过程中，不需要重新调度和启动新的 Pod，从而减少了对集群资源的占用。 用途: 适用于资源紧张的集群环境，可以有效节约计算和存储资源。\n提升更新速度: 优势: 直接更新现有 Pod 的容器镜像或配置，省去了 Pod 创建和销毁的时间，显著提升了更新操作的效率。 用途: 适用于需要快速更新的场景，如紧急补丁、快速迭代开发等。\n使用原地升级的组件 CloneSet: 原地升级功能: 通过配置 CloneSet 的 inPlaceUpdateStrategy，可以实现对应用的原地升级。 适用场景: 需要频繁更新且对更新速度和资源开销有较高要求的无状态应用。\nAdvanced StatefulSet: 原地升级功能: 扩展了原生 StatefulSet 的能力，支持对有状态应用进行原地升级。 适用场景: 需要高可用性和快速更新的有状态应用，如数据库、缓存服务等。\n原地升级的工作原理 镜像更新: 操作: 修改 Pod 的容器镜像标签或 ID。 过程: 控制器监控到更新请求后，直接在原有 Pod 上更新容器镜像，触发容器重启但不销毁 Pod。\n资源更新: 操作: 修改 Pod 的资源请求和限制（如 CPU 和内存）。 过程: 控制器监控到更新请求后，直接在原有 Pod 上更新资源配置，可能触发资源重调度但不销毁 Pod。\n应用环境\n虚拟机\nIp 主机名 cpu 内存 硬盘 192.168.10.11 master01 2cpu双核 4G 100G 192.168.10.12 worker01 2cpu双核 4G 100G 192.168.10.13 worker02 2cpu双核 4G 100G 版本 centos7.9 已部署k8s-1.27\n一、OpenKruise部署 # 本案例使用helm方式安装部署\nHelm用于实现kubernetes中相互关联的多个yaml文件的安装部署，相当于linux系统中的yum工具\n1.安装helm客户端工具 # wget https://get.helm.sh/helm-v3.13.2-linux-amd64.tar.gz tar xf helm-v3.13.2-linux-amd64.tar.gz mv linux-amd64/helm /usr/bin/ helm version helm repo add stable http://mirror.azure.cn/kubernetes/charts/ helm repo add bitnami https://charts.bitnami.com/bitnami helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm repo list 详情请看 链接: k8s学习–helm的详细解释及安装和常用命令\n通过 helm 安装\nhelm repo add openkruise https://openkruise.github.io/charts/ helm repo update helm search repo openkruise k create ns kruise-system # 添加标签 kubectl label namespace kruise-system app.kubernetes.io/managed-by=Helm # 添加注解 kubectl annotate namespace kruise-system meta.helm.sh/release-name=openkruise kubectl annotate namespace kruise-system meta.helm.sh/release-namespace=kruise-system helm -n newland install redis -f my-values.yaml ./redis kruise-system featureGates: \u0026#34;ImagePullJobGate=true\u0026#34; helm -n kruise-system install openkruise -f values.yaml ./ helm -n kruise-system install openkruise -f my-values.yaml ./ --set manager.image.repository=192.168.0.140:881/openkruise/kruise-manager helm install openkruise -f my-values.yaml ./ helm -n kruise-system install openkruise -f my-values.yaml ./ --set manager.image.repository=openkruise-registry.cn-hangzhou.cr.aliyuncs.com/openkruise/kruise-manager openkruise-registry.cn-hangzhou.cr.aliyuncs.com/openkruise/kruise-manager:v1.7.0-alpha1.1 openkruise-registry.cn-hangzhou.cr.aliyuncs.com/openkruise/kruise-manager:v1.7.0-alpha1.1 $ helm install kruise https://... --set manager.image.repository=openkruise-registry.cn-shanghai.cr.aliyuncs.com/openkruise/kruise-manager # Firstly add openkruise charts repository if you haven\u0026#39;t do this. $ helm repo add openkruise https://openkruise.github.io/charts/ # [Optional] $ helm repo update # Install the latest version. $ helm install kruise openkruise/kruise --version 1.6.3 helm install kruise openkruise/kruise --version 1.6.3 --set manager.image.repository=192.168.0.140:881/openkruise/kruise-manager [root@longxi-master201 ~]# cat /usr/lib/systemd/system/docker.service [Unit] Description=Docker Application Container Engine Documentation=https://docs.docker.com After=network-online.target firewalld.service containerd.service Wants=network-online.target Requires=docker.socket containerd.service [Service] Type=notify ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock ExecReload=/bin/kill -s HUP $MAINPID TimeoutSec=0 RestartSec=2 Restart=always StartLimitBurst=3 StartLimitInterval=60s LimitNOFILE=infinity LimitNPROC=infinity LimitCORE=infinity TasksMax=infinity Delegate=yes KillMode=process OOMScoreAdjust=-500 [Install] WantedBy=multi-user.target [root@longxi-master201 ~]# 搜索OpenKruise仓库中的Charts\n可以看到已经存在\n由于本次部署在K8S 1.27版本集群，并使用cri-dockerd，所以手动指定CRI。 注意：如果是1.24以下，则不需要，因为默认就是docker\nhelm install kruise openkruise/kruise --version 1.6.3 --set daemon.socketLocation=/var/run --set daemon.socketFile=cri-dockerd.sock helm install kruise openkruise/kruise --version 1.6.3 --set daemon.socketLocation=/var/run --set daemon.socketFile=cri-dockerd.sock 查看一下\nhelm list kubectl -n kruise-system get all 可以看到都已经运行起来了\n二、OpenKruise使用案例 # 1. 部署应用 # mkdir yaml \u0026amp;\u0026amp; cd yaml vim 01-ok.yaml apiVersion: apps.kruise.io/v1alpha1 kind: CloneSet metadata: name: nginxweb1 namespace: default spec: replicas: 3 selector: matchLabels: app: nginxweb1 template: metadata: labels: app: nginxweb1 spec: containers: - name: nginx image: nginx:1.20 imagePullPolicy: IfNotPresent ports: - containerPort: 80 应用yaml文件并查看\nkubectl apply -f 01-ok.yaml kubectl get clonesets kubectl get all 创建成功之后通过 kubectl get all命令查看对应的信息，可以发现cloneset- controller 是直接创建的 Pod，而原生的Deployment 是通过 ReplicaSet 去创建的 Pod\n2. 应用扩容 # vim 02-ok.yaml apiVersion: apps.kruise.io/v1alpha1 kind: CloneSet metadata: name: nginxweb1 namespace: default spec: minReadySeconds: 30 scaleStrategy: maxUnavailable: 1 replicas: 5 selector: matchLabels: app: nginxweb1 template: metadata: labels: app: nginxweb1 spec: containers: - name: nginx image: nginx:1.20 imagePullPolicy: IfNotPresent ports: - containerPort: 80 minReadySeconds: 30 创建了一个pod之后30s才会创建第二个 应用yaml文件并持续查看 注意： watch是持续查看 ctrl+c退出\nkubectl apply -f 02-ok.yaml watch kubectl get pods 可以看到30s之后才创建出来的第二个\nkubectl get cloneset kubectl get pods 3. 应用缩容 # vim 03-ok.yaml apiVersion: apps.kruise.io/v1alpha1 kind: CloneSet metadata: name: nginxweb1 namespace: default spec: minReadySeconds: 30 scaleStrategy: maxUnavailable: 1 podsToDelete: - nginxweb1-frcl2 # 可指定多个 replicas: 4 selector: matchLabels: app: nginxweb1 template: metadata: labels: app: nginxweb1 spec: containers: - name: nginx image: nginx:1.20 imagePullPolicy: IfNotPresent ports: - containerPort: 80 缩容时， CloneSet可以指定一些pod删除，而 StatefulSet 或者 Deployment 做不到: StatefulSet 是根据序号来删除 Pod，而 Deployment/ReplicaSet 目前只能根据控制器里定义的排序来删除。 而 CloneSet 允许用户在缩小 replicas 数量的同时，指定想要删除的 Pod 名字。 如果只是把name加入podsToDelete，而没有修改replicas的话，删完之后会再扩一个pod\nkubectl apply -f 03-ok.yaml kubectl get pods 可以看到指定的pod已经被删除\n4. 原地升级 # 先查看一下pod内nginx的版本\nkubectl exec -it nginxweb1-g44p9 -- nginx -v vim 04-ok.yaml apiVersion: apps.kruise.io/v1alpha1 kind: CloneSet metadata: name: nginxweb1 namespace: default spec: minReadySeconds: 30 updateStrategy: #添加更新策略 type: InPlaceIfPossible scaleStrategy: maxUnavailable: 1 replicas: 4 selector: matchLabels: app: nginxweb1 template: metadata: labels: app: nginxweb1 spec: containers: - name: nginx image: nginx:latest #更换镜像版本 imagePullPolicy: IfNotPresent ports: - containerPort: 80 kubectl apply -f 04-ok.yaml 更新过程\nwatch kubectl get pods kubectl exec -it nginxweb1-g44p9 -- nginx -v 可以看到nginx版本已经更新成功\n5. 灰度更新 # 通过灰度更新可以更新部分pod\nvim 05-ok.yaml apiVersion: apps.kruise.io/v1alpha1 kind: CloneSet metadata: name: nginxweb1 namespace: default spec: minReadySeconds: 30 updateStrategy: # 添加更新策略 type: InPlaceIfPossible partition: 2 # 保留旧版本pod数量 scaleStrategy: maxUnavailable: 1 replicas: 4 selector: matchLabels: app: nginxweb1 template: metadata: labels: app: nginxweb1 spec: containers: - name: nginx image: nginx:1.20 imagePullPolicy: IfNotPresent ports: - containerPort: 80 kubectl get pods kubectl apply -f 05-ok.yaml watch kubectl get pods 查看版本\nkubectl exec -it nginxweb1-g44p9 -- nginx -v kubectl exec -it nginxweb1-jjsjp -- nginx -v kubectl exec -it nginxweb1-wr8h7 -- nginx -v kubectl exec -it nginxweb1-zhqxj -- nginx -v nginxweb1-89nkb 1/1 Running 1 (57s ago) 6m18s nginxweb1-kmp4g 1/1 Running 1 (2m12s ago) 4m45s nginxweb1-pb9qt 1/1 Running 1 (24s ago) 6m18s nginxweb1-x9vbh 1/1 Running 1 (94s ago) 5m17s kubectl exec -it nginxweb1-89nkb -- nginx -v kubectl exec -it nginxweb1-kmp4g -- nginx -v kubectl exec -it nginxweb1-pb9qt -- nginx -v kubectl exec -it nginxweb1-x9vbh -- nginx -v 我们会发现只更新了2个pod，还有2个pod没有更新。\n完成\n总结 # OpenKruise 通过增强 Kubernetes 的能力，提供了更灵活、高效的应用管理方式。它特别适用于需要高级更新策略、高可用性和大规模部署的场景，是对 Kubernetes 原生功能的强有力补充。\n# Kruise v1.7.0-alpha.1 ## Configuration The following table lists the configurable parameters of the kruise chart and their default values. ### manager parameters | Parameter | Description | Default | | ----------------------------------------- | ------------------------------------------------------------ | ----------------------------- | | `featureGates` | Feature gates for Kruise, empty string means all enabled | ` ` | | `installation.namespace` | namespace for kruise installation | `kruise-system` | | `installation.createNamespace` | Whether to create the installation.namespace | `true` | | `manager.log.level` | Log level that kruise-manager printed | `4` | | `manager.replicas` | Replicas of kruise-controller-manager deployment | `2` | | `manager.image.repository` | Repository for kruise-manager image | `openkruise/kruise-manager` | | `manager.image.tag` | Tag for kruise-manager image | `v1.7.0-alpha.1` | | `manager.resources.limits.cpu` | CPU resource limit of kruise-manager container | `200m` | | `manager.resources.limits.memory` | Memory resource limit of kruise-manager container | `512Mi` | | `manager.resources.requests.cpu` | CPU resource request of kruise-manager container | `100m` | | `manager.resources.requests.memory` | Memory resource request of kruise-manager container | `256Mi` | | `manager.metrics.port` | Port of metrics served | `8080` | | `manager.webhook.port` | Port of webhook served | `9443` | | `manager.pprofAddr` | Address of pprof served | `localhost:8090` | | `manager.nodeAffinity` | Node affinity policy for kruise-manager pod | `{}` | | `manager.nodeSelector` | Node labels for kruise-manager pod | `{}` | | `manager.tolerations` | Tolerations for kruise-manager pod | `[]` | | `webhookConfiguration.timeoutSeconds` | The timeoutSeconds for all webhook configuration | `30` | | `crds.managed` | Kruise will not install CRDs with chart if this is false | `true` | | `manager.resyncPeriod` | Resync period of informer kruise-manager, defaults no resync | `0` | | `manager.hostNetwork` | Whether kruise-manager pod should run with hostnetwork | `false` | | `imagePullSecrets` | The list of image pull secrets for kruise image | `false` | | `enableKubeCacheMutationDetector` | Whether to enable KUBE_CACHE_MUTATION_DETECTOR | `false` | ### daemon parameters | Parameter | Description | Default | | ----------------------------------------- | ------------------------------------------------------------ | ----------------------------- | | `daemon.extraEnvs` | Extra environment variables that will be pass onto pods | `[]` | | `daemon.log.level` | Log level that kruise-daemon printed | `4` | | `daemon.port` | Port of metrics and healthz that kruise-daemon served | `10221` | | `daemon.pprofAddr` | Address of pprof served | `localhost:10222` | | `daemon.resources.limits.cpu` | CPU resource limit of kruise-daemon container | `50m` | | `daemon.resources.limits.memory` | Memory resource limit of kruise-daemon container | `128Mi` | | `daemon.resources.requests.cpu` | CPU resource request of kruise-daemon container | `0` | | `daemon.resources.requests.memory` | Memory resource request of kruise-daemon container | `0` | | `daemon.affinity` | Affinity policy for kruise-daemon pod | `{}` | | `daemon.socketLocation` | Location of the container manager control socket | `/var/run` | | `daemon.socketFile` | Specify the socket file name in `socketLocation` (if you are not using containerd/docker/pouch/cri-o) | ` ` | | `daemon.credentialProvider.enable` | Whether to enable credential provider for image pull job | `false` | | `daemon.credentialProvider.hostPath` | credential provider plugin node dir, will volume mount into kruise-daemon | `credential-provider-plugin` | | `daemon.credentialProvider.configmap` | credential provider yaml configmap name in kruise-system ns | `credential-provider-config` | Specify each parameter using the `--set key=value[,key=value]` argument to `helm install`. For example, ### Optional: feature-gate Feature-gate controls some influential features in Kruise: | Name | Description | Default | Effect (if closed) | |---------------------------------------------|-----------------------------------------------------------------------------------------------------------------------| ------- |-------------------------------------------------------------------------------------------------------------------| | `PodWebhook` | Whether to open a webhook for Pod **create** | `true` | SidecarSet/KruisePodReadinessGate disabled | | `KruiseDaemon` | Whether to deploy `kruise-daemon` DaemonSet | `true` | ImagePulling/ContainerRecreateRequest disabled | | `DaemonWatchingPod` | Should each `kruise-daemon` watch pods on the same node | `true` | For in-place update with same imageID or env from labels/annotations | | `CloneSetShortHash` | Enables CloneSet controller only set revision hash name to pod label | `false` | CloneSet name can not be longer than 54 characters | | `KruisePodReadinessGate` | Enables Kruise webhook to inject \u0026#39;KruisePodReady\u0026#39; readiness-gate to all Pods during creation | `false` | The readiness-gate will only be injected to Pods created by Kruise workloads | | `PreDownloadImageForInPlaceUpdate` | Enables CloneSet controller to create ImagePullJobs to pre-download images for in-place update | `true` | No image pre-download for in-place update | | `CloneSetPartitionRollback` | Enables CloneSet controller to rollback Pods to currentRevision when number of updateRevision pods is bigger than (replicas - partition) | `false` | CloneSet will only update Pods to updateRevision | | `ResourcesDeletionProtection` | Enables protection for resources deletion | `true` | No protection for resources deletion | | `TemplateNoDefaults` | Whether to disable defaults injection for pod/pvc template in workloads | `false` | Should not close this feature if it has open | | `PodUnavailableBudgetDeleteGate` | Enables PodUnavailableBudget for pod deletion, eviction | `true` | No protection for pod deletion, eviction | | `PodUnavailableBudgetUpdateGate` | Enables PodUnavailableBudget for pod.Spec update | `false` | No protection for in-place update | | `WorkloadSpread` | Enables WorkloadSpread to manage multi-domain and elastic deploy | `true` | WorkloadSpread disabled | | `InPlaceUpdateEnvFromMetadata` | Enables Kruise to in-place update a container in Pod when its env from labels/annotations changed and pod is in-place updating | `true` | Only container image can be in-place update | | `StatefulSetAutoDeletePVC` | Enables policies controlling deletion of PVCs created by a StatefulSet | `true` | No deletion of PVCs by StatefulSet | | `PreDownloadImageForDaemonSetUpdate` | Enables DaemonSet controller to create ImagePullJobs to pre-download images for in-place update | `false` | No image pre-download for in-place update | | `PodProbeMarkerGate` | Whether to turn on PodProbeMarker ability | `true` | PodProbeMarker disabled | | `SidecarSetPatchPodMetadataDefaultsAllowed` | Allow SidecarSet patch any annotations to Pod Object | `false` | Annotations are not allowed to patch randomly and need to be configured via SidecarSet_PatchPodMetadata_WhiteList | | `SidecarTerminator` | SidecarTerminator enables SidecarTerminator to stop sidecar containers when all main containers exited | `false` | SidecarTerminator disabled | | `CloneSetEventHandlerOptimization` | CloneSetEventHandlerOptimization enable optimization for cloneset-controller to reduce the queuing frequency cased by pod update | `false` | optimization for cloneset-controller to reduce the queuing frequency cased by pod update disabled | | `PreparingUpdateAsUpdate` | PreparingUpdateAsUpdate enable CloneSet/Advanced StatefulSet controller to regard preparing-update Pod as updated when calculating update/current revision during scaling. | `false` | Pods at preparing update state will be regarded as current revision instead of update revision | | `ImagePullJobGate` | ImagePullJobGate enable imagepulljob-controller execute ImagePullJob | `false` | ImagePullJob and PreDownloadImageForInPlaceUpdate are disabled | | `ResourceDistributionGate` | ResourceDistributionGate enable resourcedistribution-controller execute ResourceDistribution. | `false` | ResourceDistribution disabled | | `DeletionProtectionForCRDCascadingGate` | DeletionProtectionForCRDCascadingGate enable deletionProtection for crd Cascading | `false` | CustomResourceDefinition deletion protection disabled | If you want to configure the feature-gate, just set the parameter when install or upgrade. Such as: ```bash $ helm install kruise https://... --set featureGates=\u0026#34;ResourcesDeletionProtection=true\\,PreDownloadImageForInPlaceUpdate=true\u0026#34; ... ``` If you want to enable all feature-gates, set the parameter as `featureGates=AllAlpha=true`. ### Optional: the local image for China If you are in China and have problem to pull image from official DockerHub, you can use the registry hosted on Alibaba Cloud: ```bash $ helm install kruise https://... --set manager.image.repository=openkruise-registry.cn-hangzhou.cr.aliyuncs.com/openkruise/kruise-manager ... ``` "},{"id":141,"href":"/docs/pixie-pixie/","title":"pixie 2024-08-02 17:52:17.843","section":"Docs","content":"Pixie是基于Ebpf技术构建的一套可观测性平台，Pixie默认已经集成了许多可观测性工具，例如：我们可以清晰的通过Pixie观测到K8S内部的流量情况、DNS解析时延、TCP丢包、掉包等，同时我们还可以通过Pixie多K8S内Namespace、Pod等进行监控，当然大名鼎鼎的Cilium（一款K8S的网络插件）也可以做到，但是如果您的K8S集群不是使用到Cilium的话，使用Pixie作为K8S可观测性平台是非常不错的选择。\nPixie官方主要给出了两种搭建模式，第一种是Community Cloud for Pixie,第二种是通过Self-Hosted模式进行搭建，如果您的K8S集群在公有云，那么选择Community Cloud的模式是一种快捷、简单的方式，在这种模式下用户只需要部署Pixie采集端，而无需单独部署Pixie Cloud(用于管理部署在各个K8S集群的Pixie采集端)，Pixie Cloud由社区来提供，但是这种方式对于我们国内很多系统来说是不适用的，它要求K8S集群能够直连到Pixie社区提供的Cloud上去，这意味着K8S需要与公网打通，这往往不符合安全要求，于使只能采用第二种模式来进行部署，将Pixie Cloud和Pixie采集端部署到私有的集群上面。以下是笔者在实验环境搭建的过程。 首先我的实验环境主要由三台虚拟机组成，基于Kubernetes 1.23.2版本搭建了一套集群，集群信息如下，这里要注意Pixie要求K8S的版本需要高于1.16，同时集群中主机需要使用Cgroup v1版本，笔者尝试过Cgroup v2，但是Pixie无法正常运行。 接下来我们可以按照官网的步骤开始搭建，这里贴出官网地址，大家可以参考：\nhttps://docs.px.dev/installing-pixie/install-guides/self-hosted-pixie/ 1.第一步：按照官网要求，我们需要首先使用Git将Pixie源码切到本地，这里没有特殊的说明\ngit clone https://github.com/pixie-io/pixie.git cd pixie 2.第二步：安装mkcert，具体如何安装这里不详细写出来，各位可以参考官方文档\nhttps://github.com/FiloSottile/mkcert#installation 这个mkcert是作用是生成本地的CA证书，之后Pixie Cloud和客户端之间需要通过SSL来进行通信，这里没有什么坑，安装即可。\n3.第三步：生成CA证书，这里也没什么好说的，往下走\nmkcert -install 4.第四步：创建名为plc的命名空间，这个命名空间是用于Pixie Cloud的服务\nkubectl create namespace plc 5.第五步：创建Pixie Cloud中所需要的secrets资源，包括一些密钥之类的，这一步一定确保已经在命名空间中创建，如果这里缺失了，后面Pixie Cloud的部分服务就会无法启动，创建以后可以通过Kubectl 进行查看下。\n./scripts/create_cloud_secrets.sh 6.第六步：安装kustomize，这个工具是用于生成安装Pixie Cloud的YAML文件的，这步没有坑，安装方法如下:\nhttps://kubectl.docs.kubernetes.io/installation/kustomize/ 7.第七步：安装Pixie Cloud的依赖，这一步会安装Pixie Cloud服务依赖的一些中间件之类的，比如es,postgres,nat,stan等。注意这些服务都需要挂在数据卷，在部署之前需要提前把需要的数据卷给创建好，因为笔者这里使用的是本地环境，于使创建的是宿主机数据卷，服务在启动的时候会自动去Bound数据卷，数据卷的大小需要查看依赖服务的PVC的要求来创建。\nkustomize build k8s/cloud_deps/public/ | kubectl apply -f - --namespace=plc kustomize build k8s/cloud_deps/public/ | kubectl delete -f - --namespace=plc 8.第八步：安装Pixie Cloud，这里就是等待Pull镜像和安装成功，可以通过查看plc命名空间下容器状态来观察：\nkustomize build k8s/cloud/public/ | kubectl apply -f - --namespace=plc kustomize build k8s/cloud/public/ | kubectl delete -f - --namespace=plc 9.第九步：为Pixie Cloud的两个Service配置External 我们需要为cloud-proxy-service ，vzconn-service配置External IP，用于集群外部访问，注意这里按照官方步骤执行后，这两个服务的External IP是pending状态，需要手工修改Service的yaml配置来绑定IP，这里因为是实验环境，笔者把K8S Master的宿主机IP与其绑定。\n10.第十步 ：设置DNS，这一步的目的使集群内部的DNS能够解析到Pixie Cloud的域名，首先需要在本地环境中安装Go，编译dev_dns_updater.go，然后执行，这里的目的就是把dev.withpixie.dev这个域名加入到集群DNS解析中，当然本地环境通过配置HOST也是可以的。\ngo build src/utils/dev_dns_updater/dev_dns_updater.go ./dev_dns_updater --domain-name=\u0026#34;dev.withpixie.dev\u0026#34; --kubeconfig=$HOME/.kube/config --n=plc 11.第十一步：配置Pixie Cloud页面的登陆账号和密码，这里它需要去查询一个create-admin-job的日志，日志中指示需要打开又给URL地址即可，注意这里大家在打开浏览器的时候会遇到证书不安全的问题，同时浏览器开启了HSTS策略导致浏览器打不开的问题，笔者在Chrome上解决了这个问题，需要在浏览器界面上输入thisissafe即可，火狐浏览器暂未找到解决方法\nkubectl logs create-admin-job-\u0026lt;pod_string\u0026gt; -n plc 12.第十二步：解析来就是安装Pixie Cli和Pixie采集端了，这里只需要执行以下步骤，当然这里有个问题，Pixie每次部署会重新去拉最新版本的镜像，这样会导致拉去时间过长和需要联网，这部分问题需要把部署YAML文件导出后通过修改YAML文件并用YAML文件安装来解决。注意这还有个可能遇到的问题就是vizier-pem服务会反复重启报错，笔者的解决办法是调大容器的内存以及修改vizier-cloud-connector和vizier-metadata服务的参数，参数名称为PL_RENEW_PERIOD，值调为10000。\nexport PL_CLOUD_ADDR=dev.withpixie.dev bash -c \u0026#34;$(curl -fsSL https://withpixie.ai/install.sh)\u0026#34; px deploy --dev_cloud_namespace plc 安装完成后就可以使用了\ngcr.io/pixie-oss/pixie-prod/cloud-api_server_image:latest gcr.io/pixie-oss/pixie-prod/cloud-artifact_tracker_server_image:latest gcr.io/pixie-oss/pixie-prod/cloud-auth_server_image:latest gcr.io/pixie-oss/pixie-prod/cloud-proxy_server_image:latest gcr.io/pixie-oss/pixie-prod/cloud-config_manager_server_image:latest gcr.io/pixie-oss/pixie-prod/cloud-cron_script_server_image:latest oryd/hydra:v1.9.2-sqlite@sha256:61771c706934e1ffd66f86700a28a294ce4ed150fbf30cc131710924271a5871 oryd/hydra:v1.9.2-alpine@sha256:faa6ca02e77e0a08f66bfa7470a5e06d80e6e68c9c35410c65a4ea7b501aea61 oryd/hydra:v1.9.2-sqlite@sha256:61771c706934e1ffd66f86700a28a294ce4ed150fbf30cc131710924271a5871 gcr.io/pixie-oss/pixie-prod/cloud-indexer_server_image:latest oryd/kratos:v0.10.1@sha256:fdcfac3da3b64e619af553451607e1ab00160e59860bb19ec145cdc6f6f9c41d ghcr.io/pixie-io/pixie-oss-pixie-dev-public-curl:multiarch-7.87.0@sha256:f7f265d5c64eb4463a43a99b6bf773f9e61a50aaa7cefaf564f43e42549a01dd oryd/kratos:v0.10.1@sha256:fdcfac3da3b64e619af553451607e1ab00160e59860bb19ec145cdc6f6f9c41d gcr.io/pixie-oss/pixie-prod/cloud-metrics_server_image:latest gcr.io/pixie-oss/pixie-prod/vizier-deps/nats:2.9.19-scratch@sha256:5de59286eb54ead4d4a9279846098d4097b9c17a3c0588182398a7250cde1af9 gcr.io/pixie-oss/pixie-prod/vizier-deps/nats:2.9.19-scratch@sha256:5de59286eb54ead4d4a9279846098d4097b9c17a3c0588182398a7250cde1af9 postgres:14-alpine@sha256:446abaf8831c54f57212c0ae52f5df84e69eeb2767e2376d07bed9c9742b1243 gcr.io/pixie-oss/pixie-prod/cloud-plugin_server_image:latest postgres:14-alpine@sha256:446abaf8831c54f57212c0ae52f5df84e69eeb2767e2376d07bed9c9742b1243 gcr.io/pixie-oss/pixie-prod/cloud-profile_server_image:latest gcr.io/pixie-oss/pixie-prod/cloud-project_manager_server_image:latest gcr.io/pixie-oss/pixie-prod/cloud-scriptmgr_server_image:latest gcr.io/pixie-oss/pixie-prod/cloud-vzconn_server_image:latest gcr.io/pixie-oss/pixie-prod/cloud-vzmgr_server_image:latest "},{"id":142,"href":"/docs/2025-2-28-prometheus/","title":"prometheus 面试题","section":"Docs","content":" Prometheus # Prometheus的工作流程 # Prometheus 的工作流程可以概括为以下几个主要步骤：\n数据抓取（Scraping）： Prometheus 会定期从配置好的目标（如应用程序、服务、节点等）抓取数据。这些目标通常通过 HTTP 接口暴露 Prometheus 格式的监控数据（通常是 /metrics 路径）。 这些数据包括各类指标，如 CPU 使用率、内存使用情况、请求数量等。 存储（Storage）： 抓取到的数据会被存储在 Prometheus 的本地时间序列数据库（TSDB）中。每个时间序列由一个指标名称和一组标签（如 instance, job, region 等）组成。 查询（Querying）： Prometheus 提供了强大的查询语言——PromQL（Prometheus Query Language），可以通过 PromQL 查询已存储的数据。 Prometheus 可以通过其 Web 界面、API 或 Grafana 进行查询，显示时间序列数据的图表，或者用于告警规则的计算。 告警（Alerting）： Prometheus 可以基于定义的告警规则（通过 PromQL 查询语句）进行告警。例如，当 CPU 使用率超过一定阈值时，触发告警。 告警规则可以定义在 Prometheus 配置文件中，告警信息可以通过 Alertmanager 发送到各类通知系统（如邮件、Slack、钉钉等）。 数据可视化（Visualization）： Prometheus 本身也提供了简单的图表功能，但通常会与第三方工具（如 Grafana）结合使用，以便提供更丰富的可视化效果。 Grafana 可以从 Prometheus 查询数据，生成漂亮的仪表板，帮助团队实时监控系统健康状况。 服务发现与目标自动化（Service Discovery）： Prometheus 支持多种服务发现机制，可以自动发现需要抓取数据的目标。常见的有 Kubernetes、Consul 等服务发现工具，也可以使用静态配置。 整体而言，Prometheus 的核心是周期性地抓取目标的监控数据，存储成时间序列，并通过查询、告警和可视化等功能帮助用户监控和维护系统的健康。\n如果你有某个具体环节的疑问，欢迎随时问！\nMetric的几种类型？分别是什么？☆ # Prometheus 中有四种主要的 Metric（度量）类型，分别是：\nCounter（计数器）：\n定义：计数器是一种只增不减的度量类型。它表示一个累积值，比如请求的总数、处理的任务数等。 特点：它的值始终是递增的，并且可以在某些情况下被重置（如重启应用）。 例子：HTTP 请求的总数、错误发生次数等。 常见用途：表示某种事件或状态发生的总次数。 http_requests_total{method=\u0026#34;GET\u0026#34;, status=\u0026#34;200\u0026#34;} Gauge（仪表）：\n定义：仪表度量类型表示一个可以任意增减的值。它适用于那些可能会增加或减少的度量，比如当前内存使用量、CPU 使用率等。 特点：它的值可以上升或下降，适合表示某些瞬时的、可变的数值。 例子：当前温度、内存使用、磁盘空间等。 常见用途：表示某些动态的、瞬时的数值。 memory_usage_bytes Histogram（直方图）：\n定义：直方图用于记录一系列的分布数据，按指定的桶（bucket）来聚合样本数据。它能够提供数据的分布情况（如请求延迟的分布），并可以通过统计数据计算出一些聚合指标（如平均值、百分位数等）。 特点：它会提供桶内的计数值，可以计算出值的分布（如请求延迟的 95th 百分位）。 例子：HTTP 请求的响应时间，数据库查询时间等。 常见用途：分布数据的度量，例如请求延迟、处理时间等。 http_request_duration_seconds_bucket{le=\u0026#34;0.1\u0026#34;} Summary（摘要）：\n定义：摘要与直方图类似，都是用来统计分布数据的。不同之处在于，摘要记录的是在某一时间段内发生的样本的摘要统计信息，如累计总数、累计和、最大值等，而直方图则通过分桶统计数据分布。 特点：摘要度量提供了直接的统计值，如百分位数（p50, p90, p99等），不需要根据桶计算。 例子：请求的延迟百分位数，API 响应时间等。 常见用途：需要精确计算百分位数和其他统计量时，如请求响应时间的分位数。 http_request_duration_seconds{quantile=\u0026#34;0.95\u0026#34;} 总结： # Counter：只增不减，表示累计总数。 Gauge：可以增减，表示当前状态的瞬时值。 Histogram：用于表示数据的分布，按桶聚合数据。 Summary：类似于直方图，但直接提供百分位数等统计信息。 这四种类型是 Prometheus 度量的核心构成，它们可以帮助你监控系统的不同方面，并从多个角度分析数据。如果有某个度量类型的具体应用场景或者细节需要进一步了解，随时告诉我！\nPrometheus有哪几种服务发现☆ # Prometheus 提供了多种服务发现（Service Discovery）机制，使其能够自动发现并抓取目标（如应用、服务器等）的监控数据。服务发现可以大大简化配置管理，尤其是在动态环境中（如 Kubernetes 或云平台）非常有用。Prometheus 支持以下几种服务发现方式：\n1. Kubernetes 服务发现： # 描述：在 Kubernetes 环境中，Prometheus 可以通过 Kubernetes API 自动发现集群中的服务、Pod 和节点。它能够发现 K8s 中所有符合条件的目标（如通过标签选择器、命名空间等进行筛选）。 配置：Prometheus 配置文件中使用 kubernetes_sd_configs 配置项来实现。 应用场景：适用于容器化的环境，尤其是 Kubernetes 集群中的服务发现。 scrape_configs: - job_name: \u0026#39;kubernetes-pods\u0026#39; kubernetes_sd_configs: - api_server: \u0026#39;https://k8s-api-server\u0026#39; role: pod 2. Consul 服务发现： # 描述：Prometheus 支持通过 Consul 进行服务发现。Consul 是一个服务网格工具，它可以管理服务注册和发现。Prometheus 会从 Consul 获取服务实例列表并进行抓取。 配置：通过 consul_sd_configs 配置项。 应用场景：适用于使用 Consul 作为服务注册和发现的环境。 scrape_configs: - job_name: \u0026#39;consul\u0026#39; consul_sd_configs: - server: \u0026#39;localhost:8500\u0026#39; services: [\u0026#39;my-service\u0026#39;] 3. EC2 服务发现： # 描述：在 AWS 环境中，Prometheus 可以通过 AWS EC2 服务发现，自动抓取 EC2 实例的指标。它通过 AWS API 获取当前运行的 EC2 实例信息，基于标签、实例状态等进行筛选。 配置：通过 ec2_sd_configs 配置项来实现。 应用场景：适用于在 AWS EC2 上运行的服务或实例。 scrape_configs: - job_name: \u0026#39;ec2\u0026#39; ec2_sd_configs: - region: \u0026#39;us-west-2\u0026#39; access_key: \u0026#39;AWS_ACCESS_KEY\u0026#39; secret_key: \u0026#39;AWS_SECRET_KEY\u0026#39; 4. GCE（Google Compute Engine）服务发现： # 描述：Prometheus 可以在 Google Cloud 环境中使用 GCE 服务发现来自动发现 Google Compute Engine 实例。 配置：通过 gce_sd_configs 配置项来实现。 应用场景：适用于 Google Cloud 平台的虚拟机实例。 scrape_configs: - job_name: \u0026#39;gce\u0026#39; gce_sd_configs: - project: \u0026#39;my-gcp-project\u0026#39; zone: \u0026#39;us-central1-a\u0026#39; 5. Azure 服务发现： # 描述：Prometheus 支持在 Azure 环境中自动发现虚拟机、虚拟机规模集等服务实例。通过 Azure API 获取实例的 IP 地址、标签等信息。 配置：通过 azure_sd_configs 配置项来实现。 应用场景：适用于在 Azure 云平台上部署的服务。 scrape_configs: - job_name: \u0026#39;azure\u0026#39; azure_sd_configs: - subscription_id: \u0026#39;your-subscription-id\u0026#39; tenant_id: \u0026#39;your-tenant-id\u0026#39; client_id: \u0026#39;your-client-id\u0026#39; client_secret: \u0026#39;your-client-secret\u0026#39; 6. DNS 服务发现： # 描述：Prometheus 可以使用 DNS 查询来发现目标。通过配置 DNS 名称解析获取需要监控的目标。 配置：通过 dns_sd_configs 配置项。 应用场景：适用于使用 DNS 进行动态服务发现的环境。 scrape_configs: - job_name: \u0026#39;dns\u0026#39; dns_sd_configs: - names: - \u0026#39;myservice.local\u0026#39; type: \u0026#39;A\u0026#39; port: 8080 7. File 服务发现： # 描述：Prometheus 可以通过读取外部文件（如 JSON 或 YAML 格式）来进行静态服务发现。文件中的目标可以动态更新，Prometheus 会定期重新加载这些文件。 配置：通过 file_sd_configs 配置项。 应用场景：适用于较为静态的环境，或者当目标实例列表存储在外部文件中的情况。 scrape_configs: - job_name: \u0026#39;file\u0026#39; file_sd_configs: - files: - \u0026#39;/path/to/targets.json\u0026#39; 8. Static 服务发现： # 描述：Prometheus 也支持通过静态配置来发现目标。这种方式适用于目标列表不频繁变化的场景。 配置：通过 static_configs 配置项，可以手动列出目标。 应用场景：适用于目标数量较少且较为固定的环境。 scrape_configs: - job_name: \u0026#39;static\u0026#39; static_configs: - targets: [\u0026#39;localhost:9090\u0026#39;, \u0026#39;localhost:8080\u0026#39;] 9. OpenStack 服务发现： # 描述：Prometheus 支持 OpenStack 环境中的服务发现。通过与 OpenStack API 交互，自动发现相关的服务实例和节点。 配置：通过 openstack_sd_configs 配置项来实现。 应用场景：适用于 OpenStack 环境的服务发现。 scrape_configs: - job_name: \u0026#39;openstack\u0026#39; openstack_sd_configs: - auth_url: \u0026#39;https://openstack.example.com\u0026#39; username: \u0026#39;username\u0026#39; password: \u0026#39;password\u0026#39; project_name: \u0026#39;project\u0026#39; region: \u0026#39;region\u0026#39; 总结： # Prometheus 支持多种服务发现机制，涵盖了云平台、容器平台、传统服务器和静态配置等多种场景。你可以根据自己的环境选择合适的服务发现方式，帮助 Prometheus 自动发现和抓取目标。\n如果你对某种服务发现方式有更多问题或需要具体示例，欢迎继续提问！\nPrometheus常用函数 # Prometheus 提供了丰富的函数和操作符，帮助用户在 PromQL 查询语言中处理和分析时间序列数据。以下是一些常用的 Prometheus 函数：\n1. 聚合函数（Aggregation Functions） # 聚合函数用于按某些维度聚合数据，通常用于将多个时间序列合并为一个更具概括性的度量。\navg()\n：计算平均值\navg(http_requests_total) sum()\n：计算总和\nsum(http_requests_total) min()\n：计算最小值\nmin(http_requests_total) max()\n：计算最大值\nmax(http_requests_total) count()\n：计算数量\ncount(http_requests_total) count_values()\n：计算不同标签值的数量\ncount_values(\u0026#34;method\u0026#34;, http_requests_total) topk(k, expr)\n：返回值排名前\nk 的时间序列\ntopk(3, http_requests_total) bottomk(k, expr)\n：返回值排名后\nk 的时间序列\nbottomk(3, http_requests_total) 2. 统计函数（Statistical Functions） # rate()\n：计算时间序列在某个时间段内的速率（适用于计数器类型）\nrate(http_requests_total[5m]) irate()\n：计算瞬时速率，适用于计数器类型\nirate(http_requests_total[1m]) increase()\n：计算计数器类型的增量值\nincrease(http_requests_total[1h]) delta()\n：计算某时间段内的值的变化量\ndelta(http_requests_total[5m]) avg_over_time()\n：计算某个时间区间内的平均值\navg_over_time(http_requests_total[1h]) min_over_time()\n：计算某个时间区间内的最小值\nmin_over_time(http_requests_total[1h]) max_over_time()\n：计算某个时间区间内的最大值\nmax_over_time(http_requests_total[1h]) 3. 时间函数（Time Functions） # 时间函数用于操作时间戳和时间序列。\ntime()\n：返回当前的时间戳（以秒为单位）\ntime() timestamp()\n：返回时间序列的时间戳\ntimestamp(http_requests_total) 4. 数值处理函数（Mathematical Functions） # 这些函数用于进行常见的数学运算。\nabs()\n：返回绝对值\nabs(http_requests_total) ceil()\n：返回大于或等于该值的最小整数\nceil(http_requests_total) floor()\n：返回小于或等于该值的最大整数\nfloor(http_requests_total) round()\n：返回四舍五入的值\nround(http_requests_total) sqrt()\n：计算平方根\nsqrt(http_requests_total) 5. 字符串处理函数（String Functions） # 字符串函数主要用于对标签值或字符串型数据进行处理。\nlabel_replace()\n：用于替换标签中的部分内容\nlabel_replace(http_requests_total, \u0026#34;method\u0026#34;, \u0026#34;$1\u0026#34;, \u0026#34;status\u0026#34;, \u0026#34;(.*)\u0026#34;) label_join()\n：将多个标签值连接为一个新标签\nlabel_join(http_requests_total, \u0026#34;method\u0026#34;, \u0026#34;-\u0026#34;, \u0026#34;status\u0026#34;, \u0026#34;code\u0026#34;) 6. 查询时间函数（Time Interval Functions） # 这些函数用于处理时间区间。\navg_over_time()\n：计算某个时间范围内的平均值\navg_over_time(http_requests_total[1h]) rate()\n：计算在某段时间内的速率\nrate(http_requests_total[5m]) 7. 率函数（Rate Functions） # 用于计算单位时间内的数据变化量。\nrate()\n：适用于计数器类型，计算单位时间内的变化速率\nrate(http_requests_total[5m]) irate()\n：即时速率，适用于计数器类型，计算最近两个数据点的速率\nirate(http_requests_total[1m]) 8. 逻辑函数（Logical Functions） # 这些函数帮助进行逻辑判断和操作。\nand\n：用于两个表达式的与操作\nhttp_requests_total{status=\u0026#34;200\u0026#34;} and http_requests_total{status=\u0026#34;500\u0026#34;} or\n：用于两个表达式的或操作\nhttp_requests_total{status=\u0026#34;200\u0026#34;} or http_requests_total{status=\u0026#34;404\u0026#34;} unless\n：排除某些条件\nhttp_requests_total unless http_requests_total{status=\u0026#34;500\u0026#34;} 9. 过滤与条件（Filters \u0026amp; Conditionals） # on()\n：用于条件匹配，可以指定某些标签参与匹配\nhttp_requests_total{status=\u0026#34;200\u0026#34;} on(method) http_requests_total{status=\u0026#34;500\u0026#34;} ignoring()\n：忽略特定的标签进行条件匹配\nhttp_requests_total{status=\u0026#34;200\u0026#34;} ignoring(method) http_requests_total{status=\u0026#34;500\u0026#34;} 10. 百分比计算（Percentile Calculation） # histogram_quantile()\n：用于计算直方图的指定分位数（如 95th 百分位）\nhistogram_quantile(0.95, http_request_duration_seconds_bucket) 总结： # Prometheus 提供了多种函数来帮助你处理和分析监控数据。你可以通过这些函数来进行聚合、时间序列计算、统计分析以及进行复杂的查询和可视化。\n如果你对某个函数或用法有疑问，随时可以问我！\nthanos架构☆ # Thanos 是一个用于扩展 Prometheus 的高可用性、长期存储和全球查询的解决方案。它通过无缝地集成 Prometheus 和其他组件，提供了分布式、可扩展的监控架构，特别适用于大规模部署和多集群环境。\nThanos 架构概述 # Thanos 的架构设计基于以下几个核心组件：\nPrometheus：\n作用：Prometheus 仍然是数据的抓取和存储核心，负责从各类目标（如服务、应用、节点等）抓取时间序列数据。 特点：Thanos 并不替代 Prometheus，而是增强其功能，特别是在高可用、长期存储和多集群管理方面。 Thanos Sidecar：\n作用\n：Thanos Sidecar 是运行在 Prometheus 实例旁边的一个代理，它负责：\n将 Prometheus 中的时间序列数据上传到远程存储（如 S3、GCS、HDFS 等）。 将 Prometheus 数据暴露给 Thanos Query 和其他组件。 特点：Sidecar 是 Thanos 架构中的关键组件之一，它帮助 Prometheus 实现持久化存储，并且为 Thanos Query 提供数据源。\nSidecar 配置：\n- --tsdb.path=/prometheus - --objstore.config=\u0026#34;type: S3\\nconfig:\\n bucket: my-bucket\u0026#34; Thanos Store：\n作用：Thanos Store 是一个存储组件，负责从远程对象存储（如 S3、GCS 等）读取历史数据。它充当了 Prometheus 数据的长期存储。 特点：Store 主要用于查询过去的历史数据，并提供了统一的接口来访问不同时间窗口内的数据。 Thanos Query：\n作用：Thanos Query 是一个用于聚合查询的组件，它允许跨多个 Prometheus 和 Thanos 存储实例执行全局查询。它从多个 Prometheus 实例和 Thanos Store 获取数据，并提供统一的查询结果。 特点：Thanos Query 实现了高效的分布式查询，支持从不同 Prometheus 实例和 Store 中读取数据，可以跨多个集群或多数据源执行联合查询。 Thanos Compact：\n作用：Thanos Compact 负责合并、压缩和优化存储在对象存储中的时间序列数据。它将多个时间块（Block）合并为更大的存储块，以减少存储碎片并优化查询性能。 特点：它定期运行，以确保数据在存储层面的优化和压缩。 Thanos Ruler：\n作用：Thanos Ruler 是基于 Prometheus 的规则引擎，它允许你在 Thanos 集群中运行 Prometheus 规则、告警规则和录制规则。 特点：它提供了一种在全局范围内执行告警和规则评估的机制。Thanos Ruler 支持在多个 Prometheus 实例之间共享规则和告警信息。 Thanos 架构图 # +----------------------+ +----------------------+ +----------------------+ | Prometheus |\u0026lt;-----\u0026gt;| Thanos Sidecar |\u0026lt;-----\u0026gt;| Remote Object Store | +----------------------+ +----------------------+ +----------------------+ | | | | v v | +--------------------+ +------------------+ | | Thanos Query |\u0026lt;-----\u0026gt;| Thanos Store | | +--------------------+ +------------------+ | | | v | +------------------+ | | Thanos Ruler | | +------------------+ | v +------------------+ | Thanos Compact | +------------------+ Thanos 核心功能 # 高可用性和水平扩展： Thanos 提供跨多个 Prometheus 实例和多个集群的查询能力，帮助实现 Prometheus 集群的高可用性。 多个 Prometheus 实例可以分别在不同的区域或集群中部署，Thanos Query 可以将其聚合成一个全局视图。 长期存储： 通过将数据持久化到远程对象存储（如 S3、GCS、Azure Blob 等），Thanos 提供了可靠且成本效益高的长期存储解决方案。 对历史数据的访问可以通过 Thanos Store 组件来查询。 全球查询： Thanos Query 提供全局视图，支持跨多个集群或 Prometheus 实例执行统一查询。 即使数据分布在多个 Prometheus 实例上，Thanos Query 也能通过聚合各个数据源的结果，提供统一的查询接口。 压缩与存储优化： Thanos Compact 功能可以将存储中的多个数据块合并和压缩，降低存储成本，同时提升查询效率。 规则和告警： Thanos Ruler 允许在 Thanos 集群中执行 Prometheus 规则和告警，并支持跨 Prometheus 实例共享规则和告警配置。 数据流控制： 使用 Thanos Sidecar，用户可以将 Prometheus 数据推送到远程存储，也可以实现多个 Prometheus 实例之间的数据复制和备份。 Thanos 的使用场景 # 跨地域和跨集群监控： Thanos 非常适合在多集群、多区域部署 Prometheus 实例的环境中使用，它能够聚合来自不同集群的数据，进行跨区域查询。 长期存储解决方案： Thanos 可以将 Prometheus 的数据持久化到对象存储（如 AWS S3、Google Cloud Storage 等），实现高效且成本低的长期存储解决方案。 高可用和高容错： 通过在多个节点或集群中运行 Prometheus 和 Thanos，实现高可用性和容错能力，避免单点故障。 跨多个 Prometheus 实例的全局查询： 对于需要跨多个 Prometheus 实例进行全局查询的场景，Thanos 提供了无缝的查询能力。 总结 # Thanos 通过与 Prometheus 紧密集成，提供了对大规模、跨区域、多集群部署环境的支持。它解决了 Prometheus 在高可用、长期存储、分布式查询等方面的瓶颈，是一个强大的扩展解决方案，特别适用于需要全球查询和大规模监控环境的使用场景。\n如果你有任何关于 Thanos 的具体问题，或者需要进一步了解配置和实践，随时问我！\nthanos与VictoriaMetrics对比 # thanos sidecar和receive区别☆ # thanos rule组件和prometheus区别 # Prometheus告警从触发到收到通知延迟在哪 # 告警抑制怎么做☆ # 告警架构高可用怎么做☆ # Pod指标WSS和RSS区别☆ # 监控四个黄金指标 # 在大规模环境下，如何优化Prometheus性能 # 如何实现告警的自动化响应☆ # Prometheus数据压缩和持久化实现原理 # kubectl top输出与Linux free命令不一致原因☆ # 用到了哪些exporter，功能是什么 # 是否自己开发过exporter☆ # target down的情况如何进行故障排除？ # Exporter 停止工作，如何监控？ # Prometheus的拉取模式与zabbix推送模式有何区别？各有什么优缺点？ # Prometheus operator怎么添加targets和告警规则 # k8s集群外exporter怎么使用Prometheus监控 # "},{"id":143,"href":"/docs/prometheus-stack-prometheus-stack/","title":"prometheus-stack 2024-08-02 17:55:22.326","section":"Docs","content":"image: registry: quay.io repository: thanos/thanos tag: v0.35.1 sha: \u0026#34;\u0026#34; quay.io/thanos/thanos:v0.35.1 #原镜像 quay.io/thanos/thanos:v0.35.1 #转换后镜像 anjia0532/quay.thanos.thanos:v0.35.1 #下载并重命名镜像 docker pull anjia0532/quay.thanos.thanos:v0.35.1 docker tag anjia0532/quay.thanos.thanos:v0.35.1 quay.io/thanos/thanos:v0.35.1 docker images | grep $(echo quay.io/thanos/thanos:v0.35.1 |awk -F\u0026#39;:\u0026#39; \u0026#39;{print $1}\u0026#39;) docker pull cloudsx.top/anjia0532/quay.thanos.thanos:v0.35.1 docker tag cloudsx.top/anjia0532/quay.thanos.thanos:v0.35.1 192.168.0.140:881/thanos/thanos:v0.35.1 image: registry: quay.io repository: prometheus/alertmanager tag: v0.27.0 sha: \u0026#34;\u0026#34; quay.io/prometheus/alertmanager:v0.27.0 #原镜像 quay.io/prometheus/alertmanager:v0.27.0 #转换后镜像 anjia0532/quay.prometheus.alertmanager:v0.27.0 #下载并重命名镜像 docker pull anjia0532/quay.prometheus.alertmanager:v0.27.0 docker tag anjia0532/quay.prometheus.alertmanager:v0.27.0 quay.io/prometheus/alertmanager:v0.27.0 docker images | grep $(echo quay.io/prometheus/alertmanager:v0.27.0 |awk -F\u0026#39;:\u0026#39; \u0026#39;{print $1}\u0026#39;) docker pull cloudsx.top/anjia0532/quay.prometheus.alertmanager:v0.27.0 docker tag cloudsx.top/anjia0532/quay.prometheus.alertmanager:v0.27.0 192.168.0.140:881/prometheus/alertmanager:v0.27.0 docker push 192.168.0.140:881/prometheus/alertmanager:v0.27.0 image: registry: quay.io repository: prometheus-operator/admission-webhook # if not set appVersion field from Chart.yaml is used tag: \u0026#34;\u0026#34; sha: \u0026#34;\u0026#34; pullPolicy: IfNotPresent quay.io/prometheus-operator/admission-webhook:v0.75.1 #原镜像 quay.io/prometheus-operator/admission-webhook:v0.75.1 #转换后镜像 anjia0532/quay.prometheus-operator.admission-webhook:v0.75.1 #下载并重命名镜像 docker pull anjia0532/quay.prometheus-operator.admission-webhook:v0.75.1 docker tag anjia0532/quay.prometheus-operator.admission-webhook:v0.75.1 quay.io/prometheus-operator/admission-webhook:v0.75.1 docker images | grep $(echo quay.io/prometheus-operator/admission-webhook:v0.75.1 |awk -F\u0026#39;:\u0026#39; \u0026#39;{print $1}\u0026#39;) docker pull cloudsx.top/anjia0532/quay.prometheus-operator.admission-webhook:v0.75.1 docker tag cloudsx.top/anjia0532/quay.prometheus-operator.admission-webhook:v0.75.1 192.168.0.140:881/prometheus-operator/admission-webhook:v0.75.1 docker push 192.168.0.140:881/prometheus-operator/admission-webhook:v0.75.1 image: registry: registry.k8s.io repository: ingress-nginx/kube-webhook-certgen tag: v20221220-controller-v1.5.1-58-g787ea74b6 sha: \u0026#34;\u0026#34; registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20221220-controller-v1.5.1-58-g787ea74b6 #原镜像 registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20221220-controller-v1.5.1-58-g787ea74b6 #转换后镜像 anjia0532/google-containers.ingress-nginx.kube-webhook-certgen:v20221220-controller-v1.5.1-58-g787ea74b6 #下载并重命名镜像 docker pull anjia0532/google-containers.ingress-nginx.kube-webhook-certgen:v20221220-controller-v1.5.1-58-g787ea74b6 docker tag anjia0532/google-containers.ingress-nginx.kube-webhook-certgen:v20221220-controller-v1.5.1-58-g787ea74b6 registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20221220-controller-v1.5.1-58-g787ea74b6 docker images | grep $(echo registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20221220-controller-v1.5.1-58-g787ea74b6 |awk -F\u0026#39;:\u0026#39; \u0026#39;{print $1}\u0026#39;) docker pull cloudsx.top/anjia0532/google-containers.ingress-nginx.kube-webhook-certgen:v20221220-controller-v1.5.1-58-g787ea74b6 docker tag cloudsx.top/anjia0532/google-containers.ingress-nginx.kube-webhook-certgen:v20221220-controller-v1.5.1-58-g787ea74b6 192.168.0.140:881/ingress-nginx/kube-webhook-certgen:v20221220-controller-v1.5.1-58-g787ea74b6 docker push 192.168.0.140:881/ingress-nginx/kube-webhook-certgen:v20221220-controller-v1.5.1-58-g787ea74b6 image: registry: quay.io repository: prometheus-operator/prometheus-operator # if not set appVersion field from Chart.yaml is used tag: \u0026#34;\u0026#34; sha: \u0026#34;\u0026#34; pullPolicy: IfNotPresent quay.io/prometheus-operator/prometheus-operator:v0.75.1 #原镜像 quay.io/prometheus-operator/prometheus-operator:v0.75.1 #转换后镜像 anjia0532/quay.prometheus-operator.prometheus-operator:v0.75.1 #下载并重命名镜像 docker pull anjia0532/quay.prometheus-operator.prometheus-operator:v0.75.1 docker tag anjia0532/quay.prometheus-operator.prometheus-operator:v0.75.1 quay.io/prometheus-operator/prometheus-operator:v0.75.1 docker images | grep $(echo quay.io/prometheus-operator/prometheus-operator:v0.75.1 |awk -F\u0026#39;:\u0026#39; \u0026#39;{print $1}\u0026#39;) docker pull cloudsx.top/anjia0532/quay.prometheus-operator.prometheus-operator:v0.75.1 docker tag cloudsx.top/anjia0532/quay.prometheus-operator.prometheus-operator:v0.75.1 192.168.0.140:881/prometheus-operator/prometheus-operator:v0.75.1 docker push 192.168.0.140:881/prometheus-operator/prometheus-operator:v0.75.1 image: registry: quay.io repository: prometheus-operator/prometheus-config-reloader # if not set appVersion field from Chart.yaml is used tag: \u0026#34;\u0026#34; sha: \u0026#34;\u0026#34; quay.io/prometheus-operator/prometheus-config-reloader:v0.75.1 #原镜像 quay.io/prometheus-operator/prometheus-config-reloader:v0.75.1 #转换后镜像 anjia0532/quay.prometheus-operator.prometheus-config-reloader:v0.75.1 #下载并重命名镜像 docker pull anjia0532/quay.prometheus-operator.prometheus-config-reloader:v0.75.1 docker tag anjia0532/quay.prometheus-operator.prometheus-config-reloader:v0.75.1 quay.io/prometheus-operator/prometheus-config-reloader:v0.75.1 docker images | grep $(echo quay.io/prometheus-operator/prometheus-config-reloader:v0.75.1 |awk -F\u0026#39;:\u0026#39; \u0026#39;{print $1}\u0026#39;) docker pull cloudsx.top/anjia0532/quay.prometheus-operator.prometheus-config-reloader:v0.75.1 docker tag cloudsx.top/anjia0532/quay.prometheus-operator.prometheus-config-reloader:v0.75.1 192.168.0.140:881/prometheus-operator/prometheus-config-reloader:v0.75.1 docker push 192.168.0.140:881/prometheus-operator/prometheus-config-reloader:v0.75.1 image: registry: quay.io repository: prometheus/prometheus tag: v2.53.1 sha: \u0026#34;\u0026#34; quay.io/prometheus/prometheus:v2.53.1 #原镜像 quay.io/prometheus/prometheus:v2.53.1 #转换后镜像 anjia0532/quay.prometheus.prometheus:v2.53.1 #下载并重命名镜像 docker pull anjia0532/quay.prometheus.prometheus:v2.53.1 docker tag anjia0532/quay.prometheus.prometheus:v2.53.1 quay.io/prometheus/prometheus:v2.53.1 docker images | grep $(echo quay.io/prometheus/prometheus:v2.53.1 |awk -F\u0026#39;:\u0026#39; \u0026#39;{print $1}\u0026#39;) docker pull cloudsx.top/anjia0532/quay.prometheus.prometheus:v2.53.1 docker tag cloudsx.top/anjia0532/quay.prometheus.prometheus:v2.53.1 192.168.0.140:881/prometheus/prometheus:v2.53.1 grafana adminPassword: prom-operator 安装prometheus # helm install promethues -f my-value.yaml ./ helm install grafana ./ #原镜像 quay.io/prometheus/node-exporter:v1.8.1 #转换后镜像 anjia0532/quay.prometheus.node-exporter:v1.8.1 #下载并重命名镜像 docker pull anjia0532/quay.prometheus.node-exporter:v1.8.1 docker tag anjia0532/quay.prometheus.node-exporter:v1.8.1 quay.io/prometheus/node-exporter:v1.8.1 docker images | grep $(echo quay.io/prometheus/node-exporter:v1.8.1 |awk -F\u0026#39;:\u0026#39; \u0026#39;{print $1}\u0026#39;) docker pull cloudsx.top/anjia0532/quay.prometheus.node-exporter:v1.8.1 docker tag cloudsx.top/anjia0532/quay.prometheus.node-exporter:v1.8.1 192.168.0.140:881/prometheus/node-exporter:v1.8.1 docker push 192.168.0.140:881/prometheus/node-exporter:v1.8.1 #原镜像 quay.io/kiwigrid/k8s-sidecar:1.26.1 #转换后镜像 anjia0532/quay.kiwigrid.k8s-sidecar:1.26.1 #下载并重命名镜像 docker pull anjia0532/quay.kiwigrid.k8s-sidecar:1.26.1 docker tag anjia0532/quay.kiwigrid.k8s-sidecar:1.26.1 quay.io/kiwigrid/k8s-sidecar:1.26.1 docker images | grep $(echo quay.io/kiwigrid/k8s-sidecar:1.26.1 |awk -F\u0026#39;:\u0026#39; \u0026#39;{print $1}\u0026#39;) docker pull cloudsx.top/anjia0532/quay.kiwigrid.k8s-sidecar:1.26.1 docker tag cloudsx.top/anjia0532/quay.kiwigrid.k8s-sidecar:1.26.1 192.168.0.140:881/kiwigrid/k8s-sidecar:1.26.1 docker push 192.168.0.140:881/kiwigrid/k8s-sidecar:1.26.1 #原镜像 registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.12.0 #转换后镜像 anjia0532/google-containers.kube-state-metrics.kube-state-metrics:v2.12.0 #下载并重命名镜像 docker pull anjia0532/google-containers.kube-state-metrics.kube-state-metrics:v2.12.0 docker tag anjia0532/google-containers.kube-state-metrics.kube-state-metrics:v2.12.0 registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.12.0 docker images | grep $(echo registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.12.0 |awk -F\u0026#39;:\u0026#39; \u0026#39;{print $1}\u0026#39;) docker pull cloudsx.top/anjia0532/google-containers.kube-state-metrics.kube-state-metrics:v2.12.0 docker tag cloudsx.top/anjia0532/google-containers.kube-state-metrics.kube-state-metrics:v2.12.0 192.168.0.140:881/kube-state-metrics/kube-state-metrics:v2.12.0 docker push 192.168.0.140:881/kube-state-metrics/kube-state-metrics:v2.12.0 image: # -- The Docker registry registry: docker.io repository: library/busybox tag: \u0026#34;1.31.1\u0026#34; sha: \u0026#34;\u0026#34; pullPolicy: IfNotPresent docker.io/library/busybox:1.31.1 sidecar: image: # -- The Docker registry registry: quay.io repository: kiwigrid/k8s-sidecar tag: 1.26.1 sha: \u0026#34;\u0026#34; imagePullPolicy: IfNotPresent resources: {} quay.io/kiwigrid/k8s-sidecar:1.26.1 #原镜像 quay.io/kiwigrid/k8s-sidecar:1.26.1 #转换后镜像 anjia0532/quay.kiwigrid.k8s-sidecar:1.26.1 #下载并重命名镜像 docker pull anjia0532/quay.kiwigrid.k8s-sidecar:1.26.1 docker tag anjia0532/quay.kiwigrid.k8s-sidecar:1.26.1 quay.io/kiwigrid/k8s-sidecar:1.26.1 docker images | grep $(echo quay.io/kiwigrid/k8s-sidecar:1.26.1 |awk -F\u0026#39;:\u0026#39; \u0026#39;{print $1}\u0026#39;) image: # -- The Docker registry registry: docker.io # image-renderer Image repository repository: grafana/grafana-image-renderer # image-renderer Image tag tag: latest # image-renderer Image sha (optional) sha: \u0026#34;\u0026#34; # image-renderer ImagePullPolicy pullPolicy: Always # extra environment variables docker.io/grafana/grafana-image-renderer:latest image: # -- The Docker registry registry: docker.io # -- Docker image repository repository: grafana/grafana # Overrides the Grafana image tag whose default is the chart appVersion tag: \u0026#34;\u0026#34; sha: \u0026#34;\u0026#34; pullPolicy: IfNotPresent docker.io/grafana/grafana:11.1.0 registry: docker.io repository: curlimages/curl tag: 7.85.0 sha: \u0026#34;\u0026#34; pullPolicy: IfNotPresent docker.io/curlimages/curl:7.85.0 image: # -- The Docker registry registry: docker.io repository: bats/bats tag: \u0026#34;v1.4.1\u0026#34; imagePullPolicy: IfNotPresent docker.io/bats/bats:v1.4.1 image: # -- The Docker registry registry: docker.io # image-renderer Image repository repository: grafana/grafana-image-renderer # image-renderer Image tag tag: latest # image-renderer Image sha (optional) sha: \u0026#34;\u0026#34; # image-renderer ImagePullPolicy pullPolicy: Always docker.io/grafana/grafana-image-renderer:latest docker.io/grafana/grafana:11.1.0 docker.io/library/busybox:1.31.1 docker pull cloudsx.top/grafana/grafana:11.1.0 docker pull cloudsx.top/library/busybox:1.31.1 docker tag cloudsx.top/grafana/grafana:11.1.0 192.168.0.140:881/grafana/grafana:11.1.0 docker tag cloudsx.top/library/busybox:1.31.1 192.168.0.140:881/library/busybox:1.31.1 "},{"id":144,"href":"/docs/prometheus%E4%BC%81%E4%B8%9A%E7%BA%A7%E7%9B%91%E6%8E%A7%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93-prometheus-qi-ye-ji-jian-kong-shi-yong-zong-jie/","title":"prometheus企业级监控使用总结 2024-04-03 15:13:11.185","section":"Docs","content":" 一、监控概念\u0026amp;误区 # 监控是管理基础设施和业务的核心工具，监控应该和应用程序一起构建和部署，没有监控，将无法了解你的系统运行环境，进行故障诊断，也无法阻止提供系统性的性能、成本和状态等信息。\n误区：要尽量避免进行机械式的监控、不够准确的监控、静态和监控、不频繁的监控、缺少自动化或自服务。\n二、黑盒监控\u0026amp;白盒监控 # 1、黑盒监控\n应用程序或主机是从外部观察的，因此，这种方法可能相当有限。检查是为了评估被观察的系统是否以已知的方式响应探测。 例子： 1）主机是否相应PING的请求\n2）特定的TCP端口是否打开\n3）应用程序在接受到特定的HTTP请求时，是否使用正确的数据和状态代码进行响应\n4）特定应用程序的进程是否在其主机中运行\n2、白盒监控 # 系统在被测对象表面显示其内部状态和临界段的性能数据。这种类型的自省可能非常强大，因为它暴露了内部操作，显示不同内部组件的健康状况，否则很难甚至不可能确定。这种数据处理通常以胰腺癌方式进行处理：\n1）通过日志导出：到目前为止。这是也是在广泛引入库之前，应用程序是如何暴露其内部工作的最常见的情况，例如：可以处理 HTTP 服务器的访问日志来监视请求率、延迟和错误百分比；\n2）以结构化的事件输出：这种方法类似于日志记录，但不是将数据写入磁盘，而是直接将数据发送到处理系统进行分析和聚合。\n3）以聚合的方式保存在内存中：这种格式的数据可以驻留在端点中，也可以直接从命令行工具中读取。这种方法的例子有/metrics with Prometheus metrics、HAProxy 的 stats 页面或 varnishstats 命令行工具。\n三、度量指标 # 度量指标有监控系统执行的过程通常可以分为两种方式：push（监控系统去服务进行拉取）、pull（被监控的服务自动往监控系统进行推送）【站在客户的角度】\nPush VS Pull 测量什么： 谷歌提出应该监控的四个指标：\n延迟：服务请求所需的时间 流量：正在发出的请求的数量 错误：求失败的比率 饱和：未处理的工作量，通常在队列中 Brendan 的方法更关注于及其他声明对于每个资源（CPU、磁盘、网络接口等等），应该监视以下指标：\n利用率：以资源繁忙的百分比来衡量 饱和：资源无法处理的工作量，通常会排队 错误：发生的错误数量 汤姆威尔基的红色方法：更侧重于服务级别方法，而不是底层系统本身。显然，这种才领略对于见识服务很有用，对于预测外部客户的体验也很有价值。如果服务的错误率增加，那么就可以合理地假设这些错误将直接或间接地影响客户的体验。\n速率：转换成每秒请求数\n错误：每秒失败请求的数量\n持久性：这些请求所花费的时间\n四、Prometheus # ** **\n1、介绍\u0026amp;架构 # Prometheus 是一个开源系统监控和警报工具包，将其监控的指标进行收集并存储为时间序列数据，即指标信息与记录时的时间戳以及称为标签的可选键值对一起存储。很多公司用来监控 K8s集群。\n2. 合适\u0026amp;不合适场景 # 合适场景：Prometheus 可以很好地记录任何数字时间序列，它既适合以机器为中心的监控，也适合监控高度动态的面向服务的架构。在微服务的世界中，他对多维数据收集的查询的支持是一个特殊的优势。专为可靠性而设计，是在中断期间可以使用的系统，可让你快速诊断问题。每个Prometheus服务器都是独立的，不依赖于网络存储或其他远程服务。当你的基础设施的其他部分损坏时，你可以依赖他，并且你无需设置大量基础设施即可使用 不合适场景：你需要100%准确性，例如按请求计费。这时候Prometheus就不太适合，你最好使用其他系统来收集和分析数据以进行计费。 3. 数据模型 # 因为监控数量极大，所以使用了时间序列数据存储（就是带时间戳和值的）\nPrometheus本地存储： Prometheus的本地存储被称为 Prometheus TSDB。TSDB的设计核心有两个：block和WAL，而block又包含chunk、index、meta.json、tombstones。\nTSDB将存储的监控数据按照时间分隔成block，block大小并不固定，按照设定的步长倍数递增。随着数据量的不断增长，TSDB会将小的block合并成大的block，这样不仅可以减少数据存储，还可以减少内存中的block个数，便于对数据进行索引。\n每个block都有全局唯一的名称，通过ULID（Universally Unique Lexicograpphically Sortable Indetifier，全局字典可排序ID）原理生成，可以通过block的文件名确定这个block的创建时间，从而很方便的按照时间对block排序。对时序数据库的查询通常会涉及到连续的很多块，这种通过命名便可以排序的设计非常简便。\nWAL（Write-Ahead Logging，预写日志）是关系型数据库中利用日志来实现事务性和持久性的一种技术，即在进行某个操作之前先将这件事情记录下来，以便之后数据进行回滚、重试等操作并保证数据的可靠性。Prometheus为了防止丢失暂存在内存中还未被写入磁盘的监控数据，引入了WAL机制。\n按照每种对象设定的采集周期，Prometheus会将周期性采集的监控数据通过Add接口添加到head block中，但这些数据没有被持久化，TSDB通过WAL将提交的数据先保存到磁盘中，在TSDB宕机重启后，会首先启动多协程读取WAL，从而恢复之前的状态。\nPrometheus 数据模型： Prometheus 将数据存储为时间序列，其中包括称为标签的键值对、时间戳和最后的值：\n表示法：\n\u0026lt;metric_name\u0026gt;[{\u0026lt;label_1=“value_1”\u0026gt;,\u0026lt;label_N=“value_N”\u0026gt;}]\u0026lt;datapoint_numercial_value\u0026gt; 4. 指标 # Counter：Prometheus实例接收的数据包总数**（一直增）** Gauge：测量是一种度量，他在收集时对给定的测量进行快照，可以增加或减少（例如温度、磁盘空间、内存使用量） Histogram：常常用于观察，一个Histogram包含下列值的合并：【某时间段内的百分比或者请求数量有多少】 5. 指标的摘要和聚合 # 指标摘要：通常来说。单个指标对我们来说价值很小，往往需要联合并可视化多个指标，这其中需要一些数学变换，例如我们可能会统计函数应用于指标或指标组，常见函数有：计数、求和、平均值、中间数、百分位数、标准差、变化率等等\n指标聚合：就是能看到来自多个源的指标的聚合视图 6. NodeExporter部署 # Prometheus使用exporter工具来暴露主机和应用程序上的指标。有很多种类型的exporter。\n7. cAdvisor监控Docker容器 # cAdvisor（Constainer Advisor）是由谷歌开发的一个项目，让从正在运行的容器手机、聚合、分析和导出数据。可用的数据涵盖了几乎所有你可能需要的东西，从内存限制到GPU指标\ncAdvisor 并不绑定到 Docker 容器，但它通常作为一个容器部署，从容器守护进程和 Linux cgroups 收集数据，是容器的发现透明且完全自动化。 除了以 Prometheus 格式公开指标之外，cAdvisor 还提供了一个有用的 web界面，允许即使可视化主机及其容器的状态 8. 捕获目标生命周期 # 服务发现-\u0026gt;配置-\u0026gt;重新标记（relable_configs）-\u0026gt; 抓取 -\u0026gt; metrics_relable_configs\n9. PromQL查询语言 # 选择器及标签匹配器：\n（1）选择器\nPrometheus被设计用来处理成千上万的时间序列、根据标签的组合，咩哥指标名称可以有几个不同的时间序列；当来自不同的工作的类型名称的指标混合在一起时，查询正确的数据可能看起来比较困难。所以在Prometheus中，选择器指的是一组标签匹配器、度量名称也包含在这个定义中，因为从技术上讲，他的内容表示也是一个标签，尽管是一个特殊的标签：name。\n选择器中的每个标签名称/值对称为标签匹配器，多个匹配器可用于进一步筛选选择器匹配的时间序列。标签匹配器用花括号括起来。如果不需要匹配器，可以省略花括号。选择器可以返回及时或范围向量\n//例如：$ prometheus_build_info{versinotallow=\u0026#34;2.17.0\u0026#34;} （2）标签匹配器\n标签匹配器用于将查询搜索限制为特定的一组标签值。下面将使用node_cpu_secends_total metric来阐述标签匹配的操作，匹配的操作符有=、!=、=和! 如果没有任何匹配的规范。仅此度量就会返回一个包含度量名称的所有可用时间序列的及时向量。以及所有的CPU核心数（cpu=“0”，cpu=“1”）和CPU的型号（mode=“idle”，mode=“iowait”，mode=“irq”，mode=“nice”，mode=“softirq”，mode=“steal”，mode=“user”，mode=“system”）\n（3）范围、偏移、子查询\n范围向量：如果要定义一个范围向量选择查询，你必须设置一个及时向量选择器和使用[]追加一个范围。\n偏移量的修饰符：offset的修饰符查询过去的数据，也就是说可双选择相对于当前时间的多长时间以前\n子查询【道理类似于 MySQL中】\n（4）PromQL操作符\n向量匹配：有one-to-one、many-to-one、one-to-many【其实就类似于mysql的左右外连接】\n（5）PromQL函数\nlable_join()和label_replace()这些函数用于操作标签——他们允许您将标签连接到其他标签，提取标签值的一部分，甚至删除标签（尽管使用标准的聚合操作更容易、更符合人体工程学）。在这两个函数中，如果定义的目标标签是一个新的，它将被添加到标签集；如果他是一个现有的标签，它将被取代。【也就是说，如果该语句满足什么条件的话，机会产生相对应的结果】\npredict_linear（）函数可以预测时间序列v在t秒后的值，它基于简单线性回归的方式，对时间窗口内的样本数据进行统计，从而可以对时间序列的变化趋势作出预测。该函数的返回结果不带有度量指标，只有标签列表。\nrate()和irate()函数：\nsort()和sort_desc()\n10. 计算CPU的使用率 # //例子：avg(irate(node_cpu_seconds_total{job=\u0026#34;node\u0026#34;}[5m] by (instance) * 100)) 11. 计算CPU负载（饱和度） # 在主机上获得CPU饱和的一种方法是跟踪平均负载，实际上它是将主机上的CPU数量考虑在内的一段时间内的平均运行队列长度。平均负载少于CPU的数量通常是正常的，长时间内超过该数字的平均值则表示CPU已经饱和。\n要查看主机的平均负载，可以使用node_load*指标，他们显示1分钟、5分钟和15分钟的平均负载。比如使用1分钟的平均负载：node_load1\n//计算主机上的CPU数量，可以使用count聚合实现count by (instance)(node_cpu_seconds_total{mode=\u0026#34;idle\u0026#34;})//接下来将此计算与node_load指标结合起来node_load1 \u0026gt; on (instance) 2 * count by (instance)(node_cpu_seconds_total{mode=\u0026#34;idle\u0026#34;})//这里我们查询的是1分钟的负载超过主机CPU数量的两倍的结果 12. 计算内存使用率 # Node Exporter的内存指标按内存的类型和使用率进行细分。可以在node_memory为前缀的指标列表找到他们。\n//查看主机上的总内存node_memory_MemTotal_bytes//主机上的可用内存node_memory_MemFree_bytes//缓冲缓存中的内存node_memory_Buffers_bytes//页面缓存中的内存node_memory_Cached_bytes//通过以上的就可以计算出内存使用率（总内存-可用内存-缓冲缓存中的内存-页面缓冲中的内存）/总内存 * 100 13. 计算内存饱和度 # 还可以通过检查内存和磁盘的读写来监控内存饱和度，可以使用从/proc/vmstat收集的两个Node Exporter指标\nnode_vmstat_pswpin：系统每秒从磁盘读到内存的字节数\nnode_vmstat_pswpout：系统每秒从内存写到磁盘的字节数\n两者都是自上次启动以来的字节数，以KB为单位\n为了获得饱和度指标，对每个指标计算每一分钟的速率，将两个速率相加，然后乘以1024获得字节数\n1024 * sum by (instance) ((rate(node_vmstat_pgpgin[1m]) + rate(node_vmstat_pgpgout[1m]))) 然后，可以对此设置图形化展示或者警报，以识别行为不当的应用程序主机。\n14. 磁盘使用率 # 对于磁盘，只测量磁盘使用情况而不是使用率、饱和或错误。这是因为在大多数情况下，它是对可视化和警报最有用的数据。\n//node_filesystem_size_bytes指标显示了被监控的每个文件系统挂载的大小。node_filesystem_size_bytes 可以使用与内存指标类似的查询来生成在主机上使用的磁盘空间百分比。\n(node_filesystem_size_bytes{mountpoint=\u0026#34;/\u0026#34;} - node_filesystem_free_bytes{mountpoint=\u0026#34;/\u0026#34;}) / node_filesystem_size_bytes{mountpoint=\u0026#34;/\u0026#34;} * 100 与内存指标不同，在每个主机上的每个挂载点都有文件系统指标。所以添加了mountpoint标签，特别是跟文件系统”/“挂载。这将在每台主机上返回该文件系统磁盘使用指标。\n如果想要或需要监控特定挂载点，那么我们可以为其添加查询。比如要监控/data挂载点，可以使用。\n(node_filesystem_size_bytes{mountpoint=\u0026#34;/data\u0026#34;} - node_filesystem_free_bytes{mountpoint=\u0026#34;/data\u0026#34;}) / node_filesystem_size_bytes{mountpoint=\u0026#34;/data\u0026#34;} * 100 或者可以使用正则表达式匹配多个挂载点\n(node_filesystem_size_bytes{mountpoint=\u0026#34;/|/run\u0026#34;} - node_filesystem_free_bytes{mountpoint=\u0026#34;/|/run\u0026#34;}) / node_filesystem_size_bytes{mountpoint=\u0026#34;/|/run\u0026#34;} * 100 可以使用 predict_linear 函数来构建在未来什么时候会耗尽磁盘空间\n//预测四小时之后磁盘空间会不会爆满predict_linear(node_filesystem_free_bytes{mountpoint=\u0026#34;/\u0026#34;}[1h], 4* 3600) \u0026lt; 0 上面是指定跟文件系统，还可以通过制定作业名称或使用正则表达式来选择所有文件系统\npredict_linear(node_filesystem_free_bytes{job=\u0026#34;node\u0026#34;}[1h], 4* 3600) \u0026lt; 0 "},{"id":145,"href":"/docs/ssl%E8%AF%81%E4%B9%A6%E8%87%AA%E7%AD%BE%E5%8F%91-ssl-zheng-shu-zi-qian-fa/","title":"ssl证书自签发 2024-04-03","section":"Docs","content":"一.相关介绍 1.生成步骤\n（1）生成私钥（Private Key）：使用 OpenSSL 工具生成一个私钥文件，用于加密和解密传输的数据。 # （2）生成证书签名请求（Certificate Signing Request，CSR）：使用 OpenSSL 工具生成一个 CSR 文件，其中包含你的服务器公钥和相关的信息，以便用于生成证书。 # （3）自签名证书生成：使用 OpenSSL 工具根据 CSR 文件和私钥生成自签名的 SSL 证书文件。 # （4）Nginx 配置修改：在 Nginx 配置文件中进行相应的修改，包括指定 SSL 证书文件路径、私钥文件路径以及其他相关的 SSL 配置项。 # 总：Nginx 就可以使用自签名 SSL 证书来启用 HTTPS，实现加密和安全的通信。需要注意的是，自签名 SSL 证书不会受到公信任的证书颁发机构（Certificate Authority）认可，因此浏览器会显示安全警告。在生产环境中，建议使用由受信任的证书颁发机构签发的证书来获得更高的安全性和可信度。 # 1 2.相关名词介绍\n（1）私钥（Key）生成：使用 OpenSSL 工具的 “genrsa” 命令生成私钥文件，其中私钥是用于加密和解密数据的关键。 # （2）公钥（CSR）生成：使用私钥文件生成证书签名请求（Certificate Signing Request，CSR），其中包含公钥和其他相关信息。这个公钥将被用于生成证书，并在浏览器连接时进行身份验证。 # （3）证书（CRT）生成：证书由公钥（CSR）和签名组成。签名可以是自签名的，也可以是由受信任的证书颁发机构（CA）签名的。通过使用私钥（Key）与公钥（CSR）进行签名，最终生成证书（CRT）文件。 # （4）服务器证书（server.crt）：生成的证书文件就是服务器证书，通常命名为 “server.crt”。 # （5）签名过程：签名是使用私钥（Key）与公钥（CSR）进行证书生成的过程。私钥用于对公钥进行签名，以确保证书的完整性和身份验证。 # 二.Nginx中实现自签名SSL证书生成与配置 # 1.私钥生成 # #关闭防火墙及安全机制 systemctl stop firewalld.service setenforce 0 #在root用户的家目录下执行 # cd ~ #使用ssl生成私钥名为 server.key openssl genrsa -des3 -out server.key 1024 openssl genrsa -des3 -out server.key 2048 #回车，输入自定义的密码文本，此处设置为12345 #输入两次 #查看生成的私钥 cat server.key 2.公钥生成 # #基于创建的server.key私钥创建server.csr公钥 openssl req -new -key server.key -out server.csr #查看私钥加密的内容 openssl req -text -in server.csr -noout 3.生成解密的私钥key # #基于server.key私钥生成server.key.unsecure的解密私钥 openssl rsa -in server.key -out server.key.unsecure 4.签名生成证书 # 方法1： #方法1需要输入密码，私钥密码为12345 openssl x509 -req -days 1000 -in server.csr -signkey server.key -out server.crt openssl x509 -req -days 36500 -in server.csr -signkey server.key -out server.crt #使用私钥和公钥生成server.crt签名证书，-days为1000天 -in指定公钥，-signkey指定私钥，生成的前面证书为server.crt 方法2： openssl x509 -req -days 1000 -in server.csr -signkey server.key.unsecure -out server1.crt openssl x509 -req -days 3650 -in server.csr -signkey server.key.unsecure -out server1.crt #使用解密私钥和公钥生成server.crt签名证书，-days为1000天 -in指定公钥，-signkey指定解密后的私钥，生成的前面证书为server.crt #查看证书的内容，server.crt内容 openssl x509 -text -in server.crt -noout openssl x509 -text -in server1.crt -noout 5.配置证书并验证\n#安装额外源 并安装启动nginx yum install epel-release -y yum install nginx -y systemctl start nginx vim /etc/nginx/nginx.conf #编辑nginx主配置文件文件末尾添加内容如下 server { listen 443 ssl ; server_name localhost ; ssl_certificate \u0026#34;/root/server.key\u0026#34;; ssl_certificate_key \u0026#34;/root/server.key.unsecure\u0026#34;; } #创建一个新的server模块，注意要在http模块里面，listen表示监听端口，server_name写主机地址或localhost都可以，ssl_certificate是签名证书的路径，ssl_certificate_key是私钥的路径，本文私钥路径写了解密后的私钥，写加密时的私钥有报错 #重启nginx到浏览器上访问验证 systemctl start nginx 报错信息： [root@test5 ~]# systemctl restart nginx Job for nginx.service failed because the control process exited with error code. See “systemctl status nginx.service” and “journalctl -xe” for details. [root@test5 ~]# systemctl status nginx.service ● nginx.service - The nginx HTTP and reverse proxy server Loaded: loaded (/usr/lib/systemd/system/nginx.service; disabled; vendor preset: disabled) Active: failed (Result: exit-code) since 四 2023-09-07 17:47:46 CST; 15s ago Process: 54283 ExecStart=/usr/sbin/nginx (code=exited, status=0/SUCCESS) Process: 55399 ExecStartPre=/usr/sbin/nginx -t (code=exited, status=1/FAILURE) Process: 55397 ExecStartPre=/usr/bin/rm -f /run/nginx.pid (code=exited, status=0/SUCCESS) Main PID: 54285 (code=exited, status=0/SUCCESS) 9月 07 17:47:46 test5 systemd[1]: Starting The nginx HTTP and reverse proxy se… 9月 07 17:47:46 test5 nginx[55399]: nginx: [emerg] cannot load certificate key…e) 9月 07 17:47:46 test5 nginx[55399]: nginx: configuration file /etc/nginx/nginx…ed 9月 07 17:47:46 test5 systemd[1]: nginx.service: control process exited, code=…=1 9月 07 17:47:46 test5 systemd[1]: Failed to start The nginx HTTP and reverse p…r. 9月 07 17:47:46 test5 systemd[1]: Unit nginx.service entered failed state. 9月 07 17:47:46 test5 systemd[1]: nginx.service failed. Hint: Some lines were ellipsized, use -l to show in full. 解决方案：里面使用解密后的私钥文件路径 vim /etc/nginx/nginx.conf ssl_certificate “/root/server.crt”; systemctl start nginx 6.登录 # https://192.168.198.15/ "},{"id":146,"href":"/docs/teg%E4%B8%8Eistio%E9%9B%86%E6%88%90-teg-yu-istio-ji-cheng/","title":"TEG与istio集成 2024-08-02 17:51:20.389","section":"Docs","content":"介绍 Tetrate Enterprise Gateway 及与 Istio 集成：云原生应用的全面网关解决方案\n深入了解 Tetrate Enterprise Gateway (TEG) 及其如何与 Istio 服务网格集成 —— 一种基于 Envoy 的企业级网关解决方案，包括它的架构、基本功能以及如何在 Kubernetes 中使用 TEG 来暴露和管理应用。\nTEG 简介 # Tetrate Enterprise Gateway（TEG）是基于 Envoy Gateway (EG) 的企业级解决方案，专门针对 Envoy Proxy 设计，通过 Kubernetes Gateway API 提供更易于消费的 Envoy 代理配置和管理包。TEG 结合了 Kubernetes Gateway API 的特性，支持在 Kubernetes 中轻松暴露服务和应用程序。\nTEG 相对于 Envoy Gateway 的主要新增特性包括：\n全局速率限制（Rate Limiting）：TEG 支持基于 IP 5-tuple、请求头等进行流量控制，需要通过 Redis 实例管理。 WAF 功能（Web Application Firewall）：TEG 提供了与 mod_security 兼容的 WAF 功能，增强了安全防护能力。 OIDC/OAuth2认证：支持在网关级别进行 OIDC/OAuth2 认证，应用程序可以按路由配置认证方式。 使用 Kubernetes Gateway API：相较于其他 API，Kubernetes Gateway API 的设计更加现代，结合了众多 Ingress 实现的经验，将网关的配置与流量的路由分离，使平台所有者可以管理网关，而应用团队可以掌控流量路由。 TEG 将 Envoy 的高级网络流量处理能力带入 Kubernetes 环境，提供了一种简化的方法来部署和管理负载平衡、API 网关功能、安全控制等，同时支持现代的、开放的应用程序暴露 API，如 Kubernetes Gateway API。这些特性使 TEG 成为一个功能丰富、易于管理的企业级网关解决方案。\nTEG 的能力 # Tetrate Enterprise Gateway for Envoy (TEG) 构建于 Envoy Gateway 项目之上，提供了一种易于使用和操作的入口，具有先进的按请求流量控制功能、与现有环境的轻松集成，以及一流的可观测性，以理解应用流量和入口健康状况。\n易于安装、操作和升级 # TEG 从头到尾注重易用性：从首次安装到启用应用团队，从故障排查到执行升级。TEG 的初始安装只需几分钟，你就可以开始使用高级功能，如速率限制、单点登录和金丝雀流量路由。TEG 还简化了运维流程，与你现有的指标、跟踪和日志记录管道相适应，我们还提供了一个完整的、预配置的可观测性堆栈，以评估 EG 产生的数据，并帮助你计划如何将 TEG 集成到你的现有指标堆栈中。\n操作性：一流的功能 # TEG 由在生产环境中运行大型、关键系统的经验丰富团队构建。TEG 简化了漏洞检查和持续升级过程，与你现有的指标和跟踪提供商轻松集成，并为你现有的 Grafana 部署提供了一套强大的入口可观测性仪表板。\n与现有的环境集成 # TEG 不仅适用于绿地部署的启动，还可以直接与传统环境以及现代云原生环境集成。它可以帮助你在现有的应用生态系统和你正在构建的云原生目标之间架起桥梁。\n引入现有的可观测性堆栈 # 你的组织可能已经有一个可观测性系统，你的应用和运营团队已经训练有素地使用它。TEG 可以轻松地嵌入到现有的基础设施中，并在你的组织中运行。TEG 将使 Envoy 的丰富指标集导出，让你的应用团队对其应用流量的行为有最佳的洞察，并看到他们所做配置更改的效果。TEG 还为运行它的平台团队提供了仪表板和警报功能，使你能够自信地操作并快速解决发现的问题。\n简单的负载平衡 # Envoy 非常强大，但要使其启动并运行简单用例可能很难——像 Istio 这样的系统提供 Envoy 入口管理作为更广泛功能套件的一部分，也附带了许多与简单、流畅的操作体验相冲突的额外功能。这就是 Envoy Gateway 存在的原因：使 Envoy 的强大功能易于用于入口用例。\n简单的 API 网关 # 组织中绝大多数 API 网关的使用归结为三件事：认证发起请求的用户；限制用户对服务的访问；在此 API 端点的服务实例之间进行负载平衡。TEG 简化了在传统和云原生环境中完成这三项任务的过程。\nTEG 的架构 # 下图展示的是 TEG 的架构图。\n从架构图中可以看出，Tetrate Enterprise Gateway for Envoy (TEG) 的架构设计包括以下主要组件和流程：\n主要组件 # Kubernetes Cluster Envoy Gateway：作为控制平面，配置和管理 Envoy 代理，消费 Kubernetes Gateway API 的配置。 Metrics Collection：使用 Prometheus 或 OpenTelemetry (OTEL) 作为指标收集点，用于监控 Envoy Proxy 的性能和健康状态。 Envoy Proxy 作为数据平面，直接处理所有进入的流量，支持基于 Kubernetes Gateway API 的配置。 Coraza WAF 作为 TEG 的一部分部署，执行 WAF 规则以保护应用免受恶意请求攻击。 Redis Rate Limit Store 作为全局速率限制的存储解决方案，用于跨所有 Envoy 实例维护统一的速率限制计数。 Your OIDC Server 处理 OAuth2.0 和 OIDC 认证流程，确保只有经过认证的用户可以访问特定的路由和服务。 工作流程 # 流量入口 所有外部流量首先通过上游的负载均衡器，然后被路由到 Envoy Proxy。 Envoy Proxy 处理 Envoy Proxy 根据 Kubernetes Cluster 中的 Envoy Gateway 的配置处理流量。 配置信息包括路由规则、安全策略（如 WAF 和速率限制）等。 安全和认证 Coraza WAF：在流量到达应用前，根据配置的 WAF 规则进行检查和过滤，提高安全性。 OIDC 认证：OIDC Server 处理认证，Envoy Proxy 根据 OIDC Server 的验证结果决定是否允许访问。 速率限制 使用 Redis 存储进行速率限制，Envoy Proxy 将根据从 Redis 获取的数据执行速率限制策略。 性能监控 Envoy Proxy 的性能和健康状态通过集成的指标收集系统（Prometheus 或 OTEL）进行监控。 配置和管理 # 用户可以通过 Kubernetes Gateway API 定义和应用 Envoy Proxy 的配置。 这包括定义专用网关的具体配置，如安全规则、路由策略等。 这种架构设计利用了 Kubernetes 的灵活性和扩展性，并通过 Envoy 提供了强大的流量管理和安全功能。\n部署 TEG # 执行下面的命令部署 TEG V0.0.0：\nexport REGISTRY=\u0026#34;oci://docker.io/tetrate\u0026#34; export CHART_VERSION=\u0026#34;v0.0.0-latest\u0026#34; helm install teg ${REGISTRY}/teg-envoy-gateway-helm \\ --version ${CHART_VERSION} \\ -n envoy-gateway-system --create-namespace 检查部署：\nkubectl get pod -n envoy-gateway-system 你将看到下面的结果：\nNAMESPACE NAME READY STATUS RESTARTS AGE envoy-gateway-system envoy-gateway-596dfbcb88-tx7xb 1/1 Running 0 3m55s envoy-gateway-system envoy-ratelimit-674b8c955c-jhlfn 2/2 Running 2 (3m48s ago) 3m54s envoy-gateway-system teg-envoy-gateway-64fd8c8fbb-59b4l 1/1 Running 0 3m55s envoy-gateway-system teg-redis-86bb7d9b9d-27n44 1/1 Running 0 3m55s 部署示例应用：\nkubectl create namespace httpbin kubectl apply -n httpbin -f https://raw.githubusercontent.com/istio/istio/master/samples/httpbin/httpbin.yaml 部署 Envoy Proxy：\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: gateway.networking.k8s.io/v1 kind: Gateway metadata: name: dedicated-gateway namespace: httpbin spec: gatewayClassName: teg listeners: - name: http protocol: HTTP port: 80 EOF 然后你会在 envoy-gateway-system 命名空间下看到一个新的 Envoy 代理。\n部署 HTTPRoute，给网关配置路由：\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: gateway.networking.k8s.io/v1 kind: HTTPRoute metadata: name: httpbin namespace: httpbin spec: parentRefs: - group: gateway.networking.k8s.io kind: Gateway name: dedicated-gateway rules: - matches: - path: type: PathPrefix value: /httpbin/ filters: - type: URLRewrite urlRewrite: path: type: ReplacePrefixMatch replacePrefixMatch: / backendRefs: - group: \u0026#34;\u0026#34; kind: Service name: httpbin port: 8000 EOF 这个路由配置中有一个 URLRewrite filter，重写 URL 前缀，去掉了 /httpbin/ 部分。\n发送流量测试：\nexport DEDICATED_GATEWAY_IP=$(kubectl get gateway/dedicated-gateway -n httpbin -o jsonpath=\u0026#39;{.status.addresses[0].value}\u0026#39;) curl -i http://${DEDICATED_GATEWAY_IP}/httpbin/get 为什么使用 /httpbin/get?\n在通过 Tetrate Enterprise Gateway for Envoy (TEG) 暴露 httpbin 应用时，选择 /httpbin/get 作为访问路径的原因主要是为了在同一个 Envoy 网关下能够同时支持多个应用或服务，并能根据不同的路径将流量正确地路由到指定的服务。\n这种路径前缀的设置方法允许系统管理员或开发人员为每个服务配置独立的路径前缀，从而通过单一的入口点（即 Envoy 网关）来管理对多个后端服务的访问。这样的配置增加了路由的灵活性，使得在不更改现有服务配置的情况下，轻松地扩展或修改服务的暴露方式。\n作为 Istio 的入口网关 # Istio 提供了成熟且灵活的入口网关支持，基于与 Tetrate Enterprise Gateway（TEG）相同的 Envoy 代理。Istio 主要专注于处理集群内服务之间的通信。相较之下，TEG 设计用于向外界暴露应用，处理人类用户的请求，并支持如 OIDC 单点登录等高级功能。通过结合 Istio 网格和 TEG 的高级网关功能，两者可以共同使用，以提升整体应用的可访问性和安全性。\n以下图示展示了 Istio 网格中入口网关的流量路径。\n下图展示了在引入 TEG 之后，流量如何从 Istio 网格边缘进入到内部。\n将 TEG 集成到 Istio 网格中，通过在 TEG 上配置 sidecar 来颁发证书，同时避免 sidecar 拦截 TEG 中的流量。然后通过 Envoy Gateway 控制入口网关的流量路径。\n为 TEG 与 Istio 的互操作做准备 # 为了使 TEG 作为 Istio 的入口网关，应注意以下关键点：\n在安装 Istio 时，避免启用 Ingress Gateway。我们将手动安装并配置 TEG 作为 Istio 的入口网关。 由于 Istio 和 TEG 都使用 Envoy 作为代理，我们需要让 Istio 为 TEG 的网关 Pod 注入 Envoy sidecar，以便 TEG 可以安全地与 Istio 数据平面通信。 配置 Envoy Gateway 创建的 Envoy 代理的路由类型为 Service 而非 Endpoint，确保 Envoy 代理能正确找到路由。 为 TEG 的命名空间添加标签，以确保数据平面获得 Istio sidecar 的注入。\nkubectl label namespace envoy-gateway-system --overwrite=true istio-injection=enabled 我们还需要配置 TEG 的 sidecar，使其不处理进入网关的 Envoy 流量。注入 sidecar 的目的是使 Envoy Gateway 的组件及其创建的代理能够被纳入 Istio 网格，并挂载正确的证书进行安全通信。\ncontrol-plane-tls.yaml spec: ports: - port: 18000 appProtocol: tls kubectl patch service -n envoy-gateway-system envoy-gateway \\ --type strategic --patch-file control-plane-tls.yaml 配置 Envoy Gateway 中的 sidecar 不拦截流量：\nteg-sidecars-no-inbound.yaml --- apiVersion: gateway.envoyproxy.io/v1alpha1 kind: EnvoyProxy metadata: name: data-plane-sidecars namespace: envoy-gateway-system spec: provider: type: Kubernetes kubernetes: envoyDeployment: pod: annotations: traffic.sidecar.istio.io/includeInboundPorts: \u0026#34;\u0026#34; routingType: Service kubectl apply -f teg-sidecars-no-inbound.yaml 修改 GatewayClass 的配置，将上述 sidecar 配置应用到 Envoy Gateway 数据平面的所有 EnvoyProxy 上：\ngtwcls-use-envoyproxy.yaml --- spec: parametersRef: group: gateway.envoyproxy.io kind: EnvoyProxy namespace: envoy-gateway-system name: data-plane-sidecars kubectl patch gatewayclass teg --patch-file gtwcls-use-envoyproxy.yaml --type merge 安装 Istio # 使用 minimal profile 部署 Istio，从而不部署 Ingress Gateway：\nistioctl install --set profile=minimal -y 重启 TEG 控制平面 # 现在 Istio 的 sidecar 注入已准备就绪，我们将重启所有 TEG 控制平面 Pod，它们将带有 sidecar 重新启动。\nfor d in envoy-gateway envoy-ratelimit teg-envoy-gateway teg-redis; \\ do kubectl rollout restart deployment -n envoy-gateway-system $d; \\ done 部署测试应用 # 此步应在安装 Istio 之后进行，以确保它们也获得 sidecar 的注入。\nkubectl create namespace httpbin kubectl label namespace httpbin --overwrite=true istio-injection=enabled kubectl apply -n httpbin -f https://raw.githubusercontent.com/istio/istio/master/samples/httpbin/httpbin.yaml 配置 TEG # 现在我们配置 TEG 处理边缘流量。\napps-gateway.yaml --- apiVersion: gateway.networking.k8s.io/v1 kind: Gateway metadata: name: apps namespace: httpbin spec: gatewayClassName: teg listeners: - name: http protocol: HTTP port: 80 kubectl apply -f apps-gateway.yaml 部署应用网关，包含以下容器：\nistio-init：由 Istio 注入，负责修改 pod 中的 iptables envoy：由 TEG 控制，作为入口网关 istio-proxy：由 Istio 注入，负责与集群内部 pod 联系 shutdown-manager：由 TEG 控制，负责 Pod 启停 创建 HTTP 路由：\nhttpbin-route.yaml --- apiVersion: gateway.networking.k8s.io/v1 kind: HTTPRoute metadata: name: httpbin namespace: httpbin spec: parentRefs: - group: gateway.networking.k8s.io kind: Gateway name: apps hostnames: - \u0026#34;www.example.com\u0026#34; rules: - matches: - path: type: PathPrefix value: /httpbin/ filters: - type: URLRewrite urlRewrite: path: type: ReplacePrefixMatch replacePrefixMatch: / backendRefs: - kind: Service name: httpbin port: 8000 kubectl apply -f httpbin-route.yaml 发送测试请求 # 获取网关的负载均衡器 IP 地址，并发送测试请求：\nexport GATEWAY_URL=$(kubectl get svc -n envoy-gateway-system -l gateway.envoyproxy.io/owning-gateway-name=apps -o jsonpath=\u0026#39;{.items[0].status.loadBalancer.ingress[0].ip}\u0026#39;) curl -v -H Host:www.example.com http://$GATEWAY_URL/httpbin/get 你将看到来自 httpbin 服务的正确响应，如下所示：\n* Trying 34.41.0.90:80... * Connected to 34.41.0.90 (34.41.0.90) port 80 \u0026gt; GET /httpbin/get HTTP/1.1 \u0026gt; Host:www.example.com \u0026gt; User-Agent: curl/8.7.1 \u0026gt; Accept: */* \u0026gt; * Request completely sent off \u0026lt; HTTP/1.1 200 OK \u0026lt; server: envoy \u0026lt; date: Wed, 31 Jul 2024 08:21:58 GMT \u0026lt; content-type: application/json \u0026lt; content-length: 282 \u0026lt; access-control-allow-origin: * \u0026lt; access-control-allow-credentials: true \u0026lt; x-envoy-upstream-service-time: 11 \u0026lt; { \u0026#34;args\u0026#34;: {}, \u0026#34;headers\u0026#34;: { \u0026#34;Accept\u0026#34;: \u0026#34;*/*\u0026#34;, \u0026#34;Host\u0026#34;: \u0026#34;www.example.com\u0026#34;, \u0026#34;User-Agent\u0026#34;: \u0026#34;curl/8.7.1\u0026#34;, \u0026#34;X-Envoy-Attempt-Count\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;X-Envoy-External-Address\u0026#34;: \u0026#34;123.120.227.173\u0026#34; }, \u0026#34;origin\u0026#34;: \u0026#34;123.120.227.173\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;http://www.example.com/get\u0026#34; } * Connection #0 to host 34.41.0.90 left intact 启用严格的 mTLS # 运行下面的命令启用严格的 mTLS：\nstrict-mtls.yaml --- apiVersion: security.istio.io/v1beta1 kind: PeerAuthentication metadata: name: \u0026#34;default\u0026#34; namespace: \u0026#34;istio-system\u0026#34; spec: mtls: mode: STRICT kubectl apply -f strict-mtls.yaml 修改我们之前创建的 HTTPRoute：\nhttpbin-route-mtls.yaml --- apiVersion: gateway.networking.k8s.io/v1 kind: HTTPRoute metadata: name: httpbin namespace: httpbin spec: parentRefs: - group: gateway.networking.k8s.io kind: Gateway name: apps hostnames: - \u0026#34;httpbin.httpbin.svc.cluster.local\u0026#34; rules: - matches: - path: type: PathPrefix value: /httpbin/ filters: - type: URLRewrite urlRewrite: path: type: ReplacePrefixMatch replacePrefixMatch: / backendRefs: - kind: Service name: httpbin port: 8000 kubectl apply -f httpbin-route-mtls.yaml 注意：这次我们使用 Host 是 httpbin 在集群内部的主机名。如果使用 www.example.com 作为请求的主机名访问将返回 404 错误。\ncurl -v -H Host:httpbin.httpbin.svc.cluster.local http://$GATEWAY_URL/httpbin/get 现在 httpbin 服务能够正常访问了。\n为网关启用 TLS # 创建用于服务签名的根证书和私钥：\nmkdir example_certs openssl req -x509 -sha256 -nodes -days 365 -newkey rsa:2048 -subj \u0026#39;/O=example Inc./CN=example.com\u0026#39; -keyout example_certs/example.com.key -out example_certs/example.com.crt 为 www.example.com 创建证书和私钥：\nopenssl req -out example_certs/www.example.com.csr -newkey rsa:2048 -nodes -keyout example_certs/www.example.com.key -subj \u0026#34;/CN=www.example.com/O=www organization\u0026#34; openssl x509 -req -sha256 -days 365 -CA example_certs/example.com.crt -CAkey example_certs/example.com.key -set_serial 0 -in example_certs/www.example.com.csr -out example_certs/www.example.com.crt 为入口网关创建 secret：\nkubectl create -n httpbin secret tls httpbin-credential \\ --key=example_certs/www.example.com.key \\ --cert=example_certs/www.example.com.crt 配置入口网关：\ntls-apps-gateway.yaml --- apiVersion: gateway.networking.k8s.io/v1 kind: Gateway metadata: name: apps namespace: httpbin spec: gatewayClassName: teg listeners: - name: https protocol: HTTPS port: 443 tls: mode: Terminate certificateRefs: - name: httpbin-credential kubectl apply -f tls-apps-gateway.yaml 发送测试请求：\ncurl -v -H Host:httpbin.httpbin.svc.cluster.local --resolve \u0026#34;www.example.com:443:$GATEWAY_URL\u0026#34; \\ --cacert example_certs/example.com.crt \u0026#34;https://www.example.com:443/httpbin/get\u0026#34; 你将可以通过 HTTPS 访问网格内的 httpbin 服务。\n总结 # Tetrate Enterprise Gateway 为企业提供了一种强大的网关解决方案，能够在云原生环境中高效地暴露和管理应用服务。通过其基于 Envoy 的架构和对 Kubernetes Gateway API 的支持，TEG 不仅确保了高性能的流量管理，还大幅简化了网关的部署和维护。无论是面对复杂的安全需求还是高流量的业务场景，TEG 都能提供可靠的支持，帮助企业实现其业务连续性和技术创新。\n参考 # Using TEG in Conjunction with an Istio Service Mesh - docs.tetrate.io "},{"id":147,"href":"/docs/%E4%B8%A4%E5%BC%A0%E5%9B%BE%E5%85%A8%E9%9D%A2%E7%90%86%E8%A7%A3k8s%E5%8E%9F%E7%90%86-liang-zhang-tu-quan-mian-li-jie-k8s-yuan-li/","title":"两张图全面理解K8S原理 2024-04-03 14:51:27.431","section":"Docs","content":" Kubernetes 简介 # Kubernetes 源于希腊语，意为“舵手”。k8s 缩写是因为 k 和 s 之间有八个字符的原因。它是 Google 在 2015 开源的容器调度编排的平台。\nKubernetes 作为一款优秀的容器编排工具，拥有非常精妙的架构设计。\nKubernetes 架构 # Kubernetes 是一个 Master + Worker 的架构，Master 可理解为 Kubernetes 的控制面，Worker 理解为 Kubernetes 的数据面。 # Master 节点一般只运行 Kubernetes 控制组件，是整个集群的大脑，一般不运行业务容器。 # Worker 节点是运行业务容器的节点。 # Master # Kubernetes Master 节点需要运行以下组件： # • **Kube-apiserver：**Kube-apiserver 是 Kubernetes 的 API 入口，是集群流量入口； # • **Kube-controller-manager：**Kube-controller-manager 包含很多个 Controller，用于调谐集群中的资源； # • **Kube-scheduler：**Kube-scheduler 是集群中默认调度器，给 Pod 选择最优节点调度； # • **Etcd：**Kuebernetes 的后端存储，集群中所有可持久化数据都存储在 Etcd 中。 # Kubernetes 中采用声明式 API，即 API 不需要关心具体功能实现，只需关心最终状态。一个 API 对应一个 Controller，具体功能由 Controller 实现同时调谐至预期状态。 # Master 节点的高可用一般取决于 Etcd，Etcd 高可用推荐三节点或者五节点，所以 Master 节点通常为三个或者五个，如果接入外部 Etcd 集群，那么 Master 节点可以是偶数个。 # 上图 Kubernetes 使用内部 Etcd，即 Etcd 与 Master 节点个数一致，部署在 Kubernetes 集群中。 # Etcd 集群一般要求三个、五个类似奇数个实例，Etcd 集群选举机制要求集群中半数以上的实例投票选举，如果集群是两个实例，那么一个实例宕机，剩下一个实例没有办法选举。同样四个实例和三个实例实际上效果是一样的。 # Worker # Kubernetes Worker 节点作为容器运行节点，需要部署以下组件： # • **CRI：**容器运行时，管理容器生命周期； # • **Kubelet：**管理 Pod 生命周期，Pod 是 Kubernetes 中最小调度单元； # • **CNI：**容器网络接口，实现 Kubernetes 中 Pod 间网络联通； # • **CSI：**容器存储接口，屏蔽底层存储实现，方便用户使用第三方存储； # • **Kube-proxy：**该组件主要实现多组 Pod 的负载均衡； # 为什么 Kubernetes 需要在容器上之上抽象一个 Pod 资源呢？大部分情况是一个 Pod 对应一个容器，有的场景就需要一个 Pod 对应多个容器，例如日志收集场景，每个 Pod 都会包含一个业务容器和一个日志收集容器，将这两个容器放在一个 Pod 里可用共享日志 Volume。 # Worker 节点的 Kubelet 需要注册到集群中，就需要每个 Worker 节点的 Kubelet 能够连接 Master 节点的 Kube-apiserver。如果集群中 Master 采用高可用部署，就会存在多个 Master，那么 Worker 节点的 Kubelet 就需要同时连接所有的 Kube-apiserver 保证高可用。实现这种高可用的方式有很多种，例如 Haproxy + Keepalived 、Nginx、Envoy 等。上图就是 LB 组件就代表这些实现负载 Kube-apsierver 的组件。 # 创建一个 Pod 需要经历哪些流程 # 当用户创建一个 Deployment 的时候，Kubernetes 中各组件的工作流程是如何的？ # • 用户通过 kubectl 创建一个 Deployment，请求会发给 Kube-apiserver； # • Kube-apiserver 会将 Deployment 的描述信息写入 Etcd 中，Kube-apiserver 将请求结果返回给用户； # • Kube-controller-manager 的 Deployment Controller 从 Kube-apiserver Watch 到 Deployment 的创建事件，并创建一个 ReplicaSet；\n• Kube-apiserver 会将 ReplicaSet 的描述信息写入 Etcd 中； # • Kube-controller-manager 的 ReplicaSet Controller 从 Kube-apiserver Watch 到 ReplicaSet 的创建事件，并创建一个 Pod；\n• Kube-apiserver 会将 Pod 的描述信息写入 Etcd 中； # • Kube-scheduler 从 Kube-apiserver Watch 到 Pod 的创建事件，并根据调度算法从集群中选择一个最优的节点，并更新 Pod 的 nodeName 字段； # • Kube-apiserver 会将 Pod 的更新信息写入 Etcd 中； # • 上述绑定的节点 Kubelet 从 Kube-apiserver Watch 到 Pod 绑定节点是自身，直接调用 CRI 创建容器； # • 结果返回，并写入 Etcd。 # 可以发现，Kubernetes 中各组件基本都是与 Kube-apiserver 进行数据流发送，整体流程非常清晰。\n总结 # Kubernetes 中组件较多，且初学较难理解，需要结合实践才能更深刻地掌握。 # Kubernetes 不管是架构设计还是软件开发设计思想都值得我们去深度思考和学习，都可以应用到平常项目开发中，例如声明式 API 思想、Master + Worker 架构设计等。 # "},{"id":148,"href":"/docs/%E4%B8%AA%E6%80%A7%E5%8C%96%E9%9B%86%E7%BE%A4%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE-ge-xing-hua-ji-qun-can-shu-pei-zhi/","title":"个性化集群参数配置 2023-09-28 15:30:56.072","section":"Docs","content":" 个性化集群参数配置 # kubeasz创建集群主要在以下两个地方进行配置：(假设集群名xxxx)\nclusters/xxxx/hosts 文件（模板在example/hosts.multi-node）：集群主要节点定义和主要参数配置、全局变量 clusters/xxxx/config.yml（模板在examples/config.yml）：其他参数配置或者部分组件附加参数 clusters/xxxx/hosts (ansible hosts) # 如集群规划与安装概览中介绍，主要包括集群节点定义和集群范围的主要参数配置\n尽量保持配置简单灵活 尽量保持配置项稳定 常用设置项：\n修改容器运行时: CONTAINER_RUNTIME=\u0026ldquo;containerd\u0026rdquo; 修改集群网络插件：CLUSTER_NETWORK=\u0026ldquo;calico\u0026rdquo; 修改容器网络地址：CLUSTER_CIDR=\u0026ldquo;192.168.0.0/16\u0026rdquo; 修改NodePort范围：NODE_PORT_RANGE=\u0026ldquo;30000-32767\u0026rdquo; clusters/xxxx/config.yml # 主要包括集群某个具体组件的个性化配置，具体组件的配置项可能会不断增加；可以在不做任何配置更改情况下使用默认值创建集群\n根据实际需要配置 k8s 集群，常用举例\n配置使用离线安装系统包：INSTALL_SOURCE: \u0026ldquo;offline\u0026rdquo; （需要ezdown -P 下载离线系统软件） 配置CA证书以及其签发证书的有效期 配置 apiserver 支持公网域名：MASTER_CERT_HOSTS 配置 cluster-addon 组件安装 \u0026hellip; "},{"id":149,"href":"/docs/2025-2-24-%E4%B8%AD%E7%BA%A7%E8%BF%90%E7%BB%B4%E9%9D%A2%E8%AF%95%E9%A2%98_%E9%A2%98%E7%9B%AE/","title":"中级运维面试题","section":"Docs","content":" 一、linux # 1. linux系统启动流程 # 第一步：开机自检，加载BIOS 第二步：读取ＭＢＲ 第三步：Boot Loader　grub引导菜单 第四步：加载kernel内核 第五步：init进程依据inittab文件夹来设定运行级别 第六步：init进程执行rc.sysinit 第七步：启动内核模块 第八步：执行不同运行级别的脚本程序 第九步：执行/etc/rc.d/rc.lo Linux 系统启动流程是一个多阶段的过程，涉及多个重要的系统组件。以下是 Linux 系统从开机到完全启动的详细步骤：\n1. 加电自检 (POST) # 当电脑通电后，首先会执行硬件自检（POST，Power-On Self-Test），由 BIOS 或 UEFI（现代系统通常使用 UEFI）来完成。这一步检查硬件设备是否正常，比如内存、硬盘、显示器等。 2. 加载引导程序 (Bootloader) # BIOS 或 UEFI 会查找并加载存储设备（如硬盘或 SSD）上的引导加载程序（Bootloader）。常见的引导加载程序有 GRUB（GRand Unified Bootloader）和 LILO（Linux Loader）等。 引导加载程序的作用是加载并启动操作系统。它会显示可用的操作系统列表（如果有多个系统），并允许用户选择要启动的操作系统。 3. 加载内核 (Kernel) # 选择操作系统后，引导加载程序会加载操作系统的内核（通常是 vmlinuz 文件）。内核是操作系统的核心，负责管理硬件资源和提供系统服务。 内核首先会解压并加载到内存中，然后初始化硬件设备（如处理器、内存、硬盘等）。 4. 初始化硬件和挂载根文件系统 (Root Filesystem) # 内核在加载完毕后，会开始初始化硬件，包括磁盘驱动程序、输入设备、网络设备等。 内核通过 initramfs 或 initrd（初始 RAM 文件系统）来帮助加载根文件系统（通常是 /）到内存中。initramfs 是一个压缩的文件系统镜像，包含必要的驱动程序和工具，确保系统能够挂载根文件系统。 5. 启动 init 进程 # 内核初始化完成后，会启动系统的第一个用户空间进程，即 init 进程（PID 1）。这是 Linux 系统中最重要的进程，它负责启动和管理系统的其他进程。 在现代 Linux 系统中，init 进程通常由 systemd 替代，systemd 是一种系统和服务管理器，负责管理系统启动过程、服务和进程。 6. 运行 init 脚本或服务管理器 # systemd\n或其他 init 系统（如\nSysVinit 或\nUpstart ）会根据配置文件（如\n/etc/systemd/system/ 中的单元文件）启动各个系统服务和进程。\n对于 systemd，它会根据目标（如 multi-user.target 或 graphical.target）启动相关的服务。 这包括启动网络服务、硬件驱动、用户登录管理器、数据库服务、应用程序等。\n7. 启动用户空间进程 # 系统服务启动完毕后，开始启动用户空间的应用程序和守护进程。这些进程提供系统的功能，如网络连接、图形界面、文件共享等。 例如，login 或 getty 等进程会启动终端（TTY）或图形用户界面（GUI）。 8. 登录界面 # 如果启用了图形用户界面（GUI），例如使用 X Window 系统（如 X11）或 Wayland，系统会启动图形显示管理器（如 GDM，LightDM，SDDM）。 用户在图形界面中输入用户名和密码，或者在终端中使用 login 命令进行登录。 9. 用户登录 # 用户登录后，会进入到其个人的 shell 环境（如 bash 或 zsh），并可以开始使用系统。 简要总结 # 硬件自检 (POST): BIOS/UEFI 检查硬件。 加载引导程序 (Bootloader): 引导程序如 GRUB 加载内核。 加载内核 (Kernel): 内核初始化硬件并启动 initramfs。 启动 init 进程: 由 init（或 systemd）启动系统服务。 初始化服务和进程: 启动必要的服务（如网络、硬件驱动等）。 用户界面: 启动图形界面或终端，用户登录。 登录并使用系统: 用户可以使用系统进行操作。 这个过程是 Linux 系统启动的标准流程，虽然不同的发行版或配置可能有细微的差别，但大致步骤是一样的。\n2. linux文件类型 # 文件属性 文件类型 - 常规文件，即file d 目录文件 b block device 即块设备文件，如硬盘;支持以block为单位进行随机访问 c character device 即字符设备文件，如键盘支持以character为单位进行线性访问 l symbolic link 即符号链接文件，又称软链接文件 p pipe 即命名管道文件 s socket 即套接字文件，用于实现两个进程进行通信 在 Linux 中，文件是系统的基本单位，每个文件都有一个特定的类型。文件类型决定了文件的性质和如何操作该文件。Linux 文件类型可以分为以下几种：\n1. 普通文件（Regular File） # 描述：普通文件是最常见的文件类型，存储着用户数据，如文本文件、图片、音频、视频、程序二进制文件等。 常见扩展名：.txt（文本文件），.jpg（图片文件），.mp3（音频文件），.bin（二进制文件），.cpp（C++源代码文件）等。 查看方式：在 ls -l 命令的输出中，普通文件的文件类型表示为 -。 2. 目录文件（Directory File） # 描述：目录文件包含指向其他文件或目录的指针。目录文件本身并不存储实际数据，而是存储文件名与文件的 inode 之间的映射关系。 查看方式：在 ls -l 命令的输出中，目录文件的文件类型表示为 d。 3. 符号链接文件（Symbolic Link，或 Symlink） # 描述：符号链接文件是一种指向另一个文件的引用。符号链接本身是一个指向目标文件路径的特殊文件。当访问符号链接时，系统会自动将操作转发到目标文件。 查看方式：在 ls -l 命令的输出中，符号链接文件的文件类型表示为 l，并且会显示链接的目标路径。 4. 字符设备文件（Character Device File） # 描述：字符设备文件代表可以按字符流进行读写的设备。例如终端、打印机、键盘等。数据通过字符设备以字节为单位顺序处理。 查看方式：在 ls -l 命令的输出中，字符设备文件的文件类型表示为 c。 5. 块设备文件（Block Device File） # 描述：块设备文件代表可以按块进行读写的设备，如硬盘、光驱、U 盘等。数据是以块（通常是 512 字节或更大）的形式进行传输和处理的。 查看方式：在 ls -l 命令的输出中，块设备文件的文件类型表示为 b。 6. FIFO 文件（FIFO, 也叫命名管道） # 描述：FIFO 文件是用于进程间通信（IPC）的特殊文件。它提供了一种单向的、按顺序流动的数据通道。通过 FIFO，数据可以从一个进程流向另一个进程。通常用于进程间的同步或数据传输。 查看方式：在 ls -l 命令的输出中，FIFO 文件的文件类型表示为 p。 7. 套接字文件（Socket File） # 描述：套接字文件是一种特殊的文件类型，通常用于进程间通信（IPC），尤其是网络通信。套接字可以用于本地进程间通信，或者通过网络进行通信。 查看方式：在 ls -l 命令的输出中，套接字文件的文件类型表示为 s。 8. 空文件（Empty File） # 描述：空文件是一个文件，但是它不包含任何数据。通常可以创建一个空文件用于占位或临时存储。 查看方式：空文件没有特殊的标识符，仍然是普通文件的一种。 查看文件类型 # 可以使用 ls -l 命令查看文件的详细信息，文件类型会出现在第一列：\n-：普通文件 d：目录 l：符号链接 c：字符设备文件 b：块设备文件 p：FIFO 文件（命名管道） s：套接字文件 例如：\n$ ls -l -rw-r--r-- 1 user user 12345 Jan 1 12:34 example.txt # 普通文件 drwxr-xr-x 2 user user 4096 Jan 1 12:34 dir1 # 目录 lrwxrwxrwx 1 user user 10 Jan 1 12:34 symlink -\u0026gt; example.txt # 符号链接 crw-rw---- 1 root tty 5, 0 Jan 1 12:34 /dev/tty0 # 字符设备文件 brw-rw---- 1 root disk 8, 0 Jan 1 12:34 /dev/sda # 块设备文件 prw-r--r-- 1 user user 0 Jan 1 12:34 mypipe # FIFO 文件 srw-rw---- 1 user user 0 Jan 1 12:34 mysocket # 套接字文件 总结 # 普通文件（Regular File）: 存储实际数据。 目录文件（Directory File）: 包含文件名和 inode 的映射。 符号链接文件（Symbolic Link）: 指向其他文件的引用。 字符设备文件（Character Device）: 按字符流读写的设备。 块设备文件（Block Device）: 按块读写的设备。 FIFO 文件（FIFO，命名管道）: 用于进程间通信。 套接字文件（Socket）: 用于网络或进程间通信。 3. centos6和7怎么将源码安装的程序添加到开机自启动？ # 通用方法：编辑/etc/rc.d/rc.local文件，在文件末尾添加启动服务命令 centos6：①进入到/etc/rc.d/init.d目录下，②新建一个服务启动脚本，脚本中指定chkconfig参数，③添加执行权限，④执行chkconfig \u0026ndash;add 添加服务自启动 centos7：①进入到/usr/lib/systemd/system目录下，②新建自定义服务文件，文件中包含[Unit] [Service] [Install]相关配置，然后添加下执行权限，③执行systemctl enable 服务名称 在 CentOS 6 和 CentOS 7 中，尽管系统的初始化方式有所不同，但将源码安装的程序添加到开机自启动的方式基本相同。以下是针对这两个版本的操作方法。\nCentOS 6 和 CentOS 7 开机自启动的基本步骤 # 创建启动脚本： 无论是 CentOS 6 还是 CentOS 7，首先需要为你的程序编写一个启动脚本。假设你有一个程序 /usr/local/bin/myprogram，你可以创建一个启动脚本 myprogram.sh，内容如下：\n#!/bin/bash # 启动程序 /usr/local/bin/myprogram \u0026amp; 这里的 \u0026amp; 确保程序在后台运行。\n设置脚本权限： 确保你的脚本可执行：\nchmod +x /path/to/myprogram.sh 将启动脚本添加到开机自启动：\nCentOS 6（使用 init.d 和 chkconfig）： # CentOS 6 使用 SysVinit 初始化系统，启动脚本一般放在 /etc/init.d/ 目录下，并通过 chkconfig 命令将其设置为开机自启动。\n将脚本移动到 /etc/init.d/ 目录：\nmv /path/to/myprogram.sh /etc/init.d/myprogram 使用 chkconfig 命令将脚本设置为开机自启动：\nchkconfig --add myprogram chkconfig myprogram on 可以使用以下命令检查脚本是否已添加到启动项：\nchkconfig --list myprogram CentOS 7（使用 systemd）： # CentOS 7 引入了 systemd，因此你需要创建一个 systemd 服务单元文件来管理程序的启动。\n在 /etc/systemd/system/ 目录下创建一个 .service 文件。例如，创建一个名为 myprogram.service 的文件：\nsudo nano /etc/systemd/system/myprogram.service 文件内容如下：\n[Unit] Description=My Program After=network.target [Service] ExecStart=/usr/local/bin/myprogram Restart=always User=root [Install] WantedBy=multi-user.target After=network.target：表示该服务将在网络服务之后启动。 ExecStart：指定程序的路径和启动命令。 Restart=always：程序崩溃后会自动重启。 WantedBy=multi-user.target：将服务添加到 multi-user.target，表示程序将在多用户模式下启动。 保存并退出后，重新加载 systemd，使其识别新的服务单元文件：\nsudo systemctl daemon-reload 启动服务：\nsudo systemctl start myprogram 设置服务为开机自启动：\nsudo systemctl enable myprogram 可以使用以下命令检查服务状态：\nsudo systemctl status myprogram 重启测试： 在完成上述设置后，重启系统进行测试，确认程序是否在开机时自动启动。\n在 CentOS 6 中使用：\nsudo reboot 在 CentOS 7 中使用：\nsudo systemctl reboot 总结： # CentOS 6：使用 init.d 脚本并通过 chkconfig 设置开机自启动。 CentOS 7：使用 systemd 创建服务单元文件并通过 systemctl 设置开机自启动。 两种方式的共同点是都需要为程序创建一个启动脚本，然后将其添加到开机自启动项中。\n4. 简述lvm，如何给使用lvm的/分区扩容？ # 功能：可以对磁盘进行动态管理。动态按需调整大小 概念： ①PV - 物理卷：物理卷在逻辑卷管理中处于最底层，它可以是实际物理硬盘上的分区，也可以是整个物理硬盘，也可以是raid设备。 ②VG - 卷组：卷组建立在物理卷之上，一个卷组中至少要包括一个物理卷，在卷组建立之后可动态添加物理卷到卷组中。一个逻辑卷管理系统工程中可以只有一个卷组，也可以拥有多个卷组。 ③LV - 逻辑卷：逻辑卷建立在卷组之上，卷组中的未分配空间可以用于建立新的逻辑卷，逻辑卷建立后可以动态地扩展和缩小空间。系统中的多个逻辑卷可以属于同一个卷组，也可以属于不同的多个卷组。\n给/分区扩容步骤： ①添加磁盘 ②使用fdisk命令对新增加的磁盘进行分区 ③分区完成后修改分区类型为lvm ④使用pvcreate创建物理卷 ⑤使用vgextend命令将新增加的分区加入到根目录分区中 ⑥使用lvextend命令进行扩容 ⑦使用xfs_growfs调整卷分区大小\nLVM（Logical Volume Management）简介 # LVM（逻辑卷管理）是 Linux 中一种高级的磁盘管理方式，允许用户在逻辑层面上管理磁盘空间。LVM 将磁盘分为物理卷（Physical Volume，PV）、卷组（Volume Group，VG）和逻辑卷（Logical Volume，LV）。通过 LVM，用户可以更灵活地管理存储空间，进行动态扩展、缩减、迁移和快照等操作。\n物理卷（PV）：物理存储设备（如硬盘、分区或 LUN），可以通过 pvcreate 命令初始化。 卷组（VG）：由多个物理卷组成的存储池，用户可以在卷组中分配空间。 逻辑卷（LV）：从卷组中划分出的存储空间，类似于传统分区，但更加灵活。 如何给使用 LVM 的 / 分区扩容 # 假设你已经有一个使用 LVM 的 / 分区，并且需要扩容。扩容过程涉及以下几个步骤：\n1. 检查当前 LVM 结构 # 首先，使用以下命令查看当前的磁盘布局和 LVM 结构，确保你已经有足够的未分配空间，或者有一个可以扩展的物理卷（PV）。\n# 查看所有物理卷 sudo pvdisplay # 查看所有卷组 sudo vgdisplay # 查看所有逻辑卷 sudo lvdisplay 2. 添加新的物理卷（如果需要） # 如果当前卷组（VG）没有足够的空间，你需要添加一个新的物理磁盘或分区，并将其添加到卷组中。\n假设你有一个新的磁盘 /dev/sdb，可以通过以下命令初始化该磁盘并将其添加到卷组（如 centos 卷组）：\n# 创建物理卷 sudo pvcreate /dev/sdb # 将物理卷添加到卷组 sudo vgextend centos /dev/sdb 3. 扩展逻辑卷 # 一旦有了足够的空间，下一步是扩展 / 分区的逻辑卷。假设 / 分区的逻辑卷是 /dev/centos/root，可以使用以下命令扩展逻辑卷：\n# 扩展逻辑卷 sudo lvextend -l +100%FREE /dev/centos/root -l +100%FREE 表示使用卷组中所有剩余的未分配空间。如果你只想使用一部分空间，可以指定实际的扩展大小，如 -L +10G 扩展 10GB。\n4. 扩展文件系统 # 扩展逻辑卷后，你还需要扩展文件系统，以便操作系统可以使用新的空间。根据你使用的文件系统类型（如 ext4 或 xfs），扩展命令不同。\n对于 ext4 文件系统，使用以下命令：\nsudo resize2fs /dev/centos/root 对于 xfs 文件系统，使用以下命令（xfs 文件系统不需要指定大小，自动调整）：\nsudo xfs_growfs /dev/centos/root 5. 验证扩容 # 完成扩容后，使用以下命令验证分区是否已成功扩展：\n# 查看文件系统的使用情况 df -h # 查看逻辑卷信息 sudo lvdisplay /dev/centos/root 总结 # 检查现有 LVM 配置：使用 pvdisplay、vgdisplay 和 lvdisplay 命令。 添加新的物理卷（如果需要）：pvcreate 和 vgextend。 扩展逻辑卷：使用 lvextend 命令扩展逻辑卷。 扩展文件系统：根据文件系统类型，使用 resize2fs（ext4）或 xfs_growfs（xfs）进行扩展。 验证扩容结果：使用 df -h 和 lvdisplay 确认空间是否已扩展。 通过 LVM，你可以非常灵活地管理磁盘空间，进行动态扩展而不需要重新分区或重新格式化磁盘。\n5. 为何du和df统计结果不一致？ # 用户删除了大量的文件被删除后，在文件系统目录中已经不可见了，所以du就不会再统计它。 然而如果此时还有运行的进程持有这个已经被删除的文件句柄，那么这个文件就不会真正在磁盘中被删除，分区超级块中的信息也就不会更改，df仍会统计这个被删除的文件。 可通过 lsof命令查询处于deleted状态的文件，被删除的文件在系统中被标记为deleted。如果系统有大量deleted状态的文件，会导致du和df统计结果不一致。 du 和 df 都是用于查看磁盘使用情况的命令，但它们的统计方式和报告的内容不同，因此有时会导致它们显示的结果不一致。以下是导致 du 和 df 统计结果不一致的原因：\n1. du 命令的工作原理 # du（Disk Usage）命令通过递归地扫描文件系统中的文件和目录，计算出文件或目录所占的磁盘空间。它的计算基于 文件实际占用的磁盘块。 du 统计的是 文件系统中每个文件和目录的实际占用空间，包括文件内容和文件系统的元数据（如 inode 信息、目录项等）。 du 默认会以块为单位（通常是 1K 或 4K）显示磁盘使用情况，且其结果是基于 文件系统中的数据，包括但不限于实际数据、磁盘对齐和文件系统元数据等。 2. df 命令的工作原理 # df（Disk Free）命令报告文件系统的整体磁盘空间使用情况，显示文件系统的总空间、已用空间、可用空间和挂载点。 df 报告的是 整个文件系统的空间，并且它计算的是 块设备的容量，包括文件系统的元数据、日志和预留空间（例如：ext4 文件系统会保留一部分空间供超级用户使用）。 df 通常显示的是 文件系统层面的空间信息，它会考虑到预留空间、磁盘格式化信息等，而不仅仅是文件的实际占用空间。 3. 为什么 du 和 df 显示不一致？ # 保留空间：某些文件系统（如 ext4）会为 root 用户预留一部分空间，以防止非管理员用户填满整个文件系统。df 会显示包括这些保留空间在内的总空间，而 du 不会计算这些保留空间，因此 du 的结果通常会比 df 少。 文件系统元数据：df 统计的空间包含了文件系统的元数据（如 inode、目录项、日志等），而 du 只计算文件和目录的实际数据占用。因此，df 的总空间可能会显示为比 du 更多的空间。 挂载点：du 只会计算指定目录（及其子目录）下的文件占用空间，而 df 计算的是整个挂载点（包括所有子目录和挂载的其他文件系统）。如果有其他挂载点（如 /mnt 或 /home）在当前目录下，df 会包括这些挂载点的空间，而 du 则不会。 文件系统特性：某些文件系统（如 Btrfs 或 ZFS）具有快照功能，du 可能会统计到这些快照的空间，而 df 可能不显示这些快照占用的空间，或者显示的是快照所使用的空间。 软链接、挂载点和文件系统限制：如果文件系统有软链接或其他挂载点，du 会统计软链接指向的文件所占的空间，但不会考虑挂载点。如果某个文件系统的磁盘空间被多次挂载或链接，df 会报告总体空间，而 du 可能只计算一次。 4. 常见场景示例 # 保留空间：对于 ext4 文件系统，默认情况下，会保留 5% 的空间供超级用户使用，df 会显示这些保留空间，而 du 不会。\n示例：\n# df 显示包括保留空间 $ df -h / Filesystem Size Used Avail Use% Mounted on /dev/sda1 50G 20G 25G 43% / # du 只显示实际使用的空间 $ du -sh / 18G / 挂载点和软链接：如果你挂载了某个分区（例如 /mnt），df 会显示整个文件系统的空间，包括 /mnt，而 du 只会统计 /mnt 目录下的空间。\n5. 如何让 du 和 df 的结果更接近？ # 排除挂载点：可以使用 du 命令的 --one-file-system 选项，避免计算挂载在当前文件系统之外的其他文件系统。\ndu -sh --one-file-system / 排除保留空间：使用 df 时可以使用 --total 选项查看所有文件系统的总使用情况，这样可以忽略一些特殊文件系统的影响。\n总结： # du 和 df 显示不一致主要是因为它们统计的内容不同：du 统计的是文件实际占用的空间，而 df 统计的是文件系统级别的空间，包括保留空间、元数据和日志等。 要更准确地理解磁盘空间使用情况，建议结合 du 和 df 的输出，并注意它们各自的局限性。 6. 如何升级内核？ # 方法一 # 添加第三方yum源进行下载安装。 Centos 6 YUM源：http://www.elrepo.org/elrepo-release-6-6.el6.elrepo.noarch.rpm Centos 7 YUM源：http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm # 先导入elrepo的key，然后安装elrepo的yum源： rpm -import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm # 查看可用的内核相关包 yum --disablerepo=\u0026#34;*\u0026#34; --enablerepo=\u0026#34;elrepo-kernel\u0026#34; list available yum -y --enablerepo=elrepo-kernel install 方法二 # 通过下载kernel image的rpm包进行安装。 官方 Centos 6: http://elrepo.org/linux/kernel/el6/x86_64/RPMS/ 官方 Centos 7: http://elrepo.org/linux/kernel/el7/x86_64/RPMS/ # 获取下载链接进行下载安装即可 wget https://elrepo.org/linux/kernel/el7/x86_64/RPMS/kernel-lt-4.4.185-1.el7.elrepo.x86_64.rpm rpm -ivh kernel-lt-4.4.185-1.el7.elrepo.x86_64.rp # 查看默认启动顺序 [root@localhost ~]# awk -F\\\u0026#39; \u0026#39;$1==\u0026#34;menuentry \u0026#34; {print $2}\u0026#39; /etc/grub2.cfg CentOS Linux (5.2.2-1.el7.elrepo.x86_64) 7 (Core) CentOS Linux (4.4.182-1.el7.elrepo.x86_64) 7 (Core) CentOS Linux (3.10.0-957.21.3.el7.x86_64) 7 (Core) CentOS Linux (3.10.0-957.10.1.el7.x86_64) 7 (Core) CentOS Linux (3.10.0-327.el7.x86_64) 7 (Core) CentOS Linux (0-rescue-e34fb4f1527b4f2d9fc75b77c016b6e7) 7 (Core) 由上面可以看出新内核(4.12.4)目前位置在0，原来的内核(3.10.0)目前位置在1 # 设置默认启动 [root@localhost ~]# grub2-set-default 0　// 0代表当前第一行，也就是4.12.4版本 # 重启验证 升级 Linux 内核是一个相对常见的操作，尤其是当你需要使用某些新功能或解决安全问题时。下面是如何在 CentOS 6 和 CentOS 7 上升级内核的详细步骤。请根据你的系统版本和需求选择适合的方式。\n1. 通过官方仓库更新内核（CentOS 7） # 在 CentOS 7 上，默认情况下会从官方仓库安装和更新内核。通过 yum 命令可以轻松进行内核更新，但有时这些仓库可能不会提供最新版本的内核。尽管如此，这仍然是升级内核的最简单方法。\n步骤： # 更新当前系统：\nsudo yum update 安装最新内核版本：\n默认情况下，CentOS 7 会自动将内核更新到最新版本。如果不确定，执行以下命令来确保已安装最新版本的内核：\nsudo yum install kernel 该命令会确保安装官方仓库中最新的稳定内核。\n查看当前内核版本：\n你可以通过以下命令检查当前的内核版本：\nuname -r 重启系统：\n更新内核后，重启系统以使新内核生效：\nsudo reboot 确认新内核是否生效：\n重启后，再次执行 uname -r 命令检查内核版本，确认系统已经使用了新的内核。\nuname -r 2. 通过 ELRepo 安装最新的内核（CentOS 7） # CentOS 官方仓库通常不会提供最新的内核版本，因此你可以通过第三方仓库 ELRepo 安装最新的内核。\n步骤： # 安装 ELRepo 仓库：\n首先，安装 ELRepo 仓库：\nsudo yum install -y https://www.elrepo.org/elrepo-release-7.el7.elrepo.rpm 安装最新的稳定内核：\nELRepo 提供了最新的内核版本。通过以下命令安装最新的内核（一般是 kernel-ml，即 Mainline Kernel）：\nsudo yum --enablerepo=elrepo-kernel install kernel-ml 如果你希望安装长期支持版本（LTS），可以安装 kernel-lt：\nsudo yum --enablerepo=elrepo-kernel install kernel-lt 检查内核版本：\n安装后，执行以下命令检查内核版本：\nuname -r 重启系统：\n重启系统以加载新内核：\nsudo reboot 确认新内核：\n再次使用 uname -r 命令检查当前内核版本。\n3. 通过源代码手动编译安装内核（适用于所有版本） # 手动编译内核的步骤适用于那些需要非常具体内核配置或想要安装最新稳定版内核的用户。\n步骤： # 下载内核源代码：\n你可以从内核官方网站下载最新的内核源代码压缩包，网址：https://www.kernel.org。\n例如，使用 wget 下载最新内核源代码：\nwget https://cdn.kernel.org/pub/linux/kernel/v5.x/linux-5.x.x.tar.xz 解压源代码并进入目录：\n解压下载的内核源代码：\ntar -xvf linux-5.x.x.tar.xz cd linux-5.x.x 配置内核：\n配置内核参数。你可以使用以下命令来加载当前系统的配置，或者手动进行选择：\nmake menuconfig 如果你想使用当前内核的配置，可以执行：\ncp /boot/config-$(uname -r) .config 编译内核：\n使用 make 命令编译内核，通常会花费一些时间：\nmake 安装内核模块：\n编译完成后，安装内核模块：\nsudo make modules_install 安装内核：\n然后安装内核：\nsudo make install 更新引导加载器（GRUB）：\n安装完成后，更新 GRUB 配置：\nsudo grub2-mkconfig -o /boot/grub2/grub.cfg 对于 UEFI 系统，使用：\nsudo grub2-mkconfig -o /boot/efi/EFI/centos/grub.cfg 重启并选择新内核：\n重启系统并选择新安装的内核。如果你使用的是 UEFI 系统，确保在 BIOS 或引导菜单中选择正确的启动项。\n验证内核版本：\n重启后使用以下命令确认新内核已生效：\nuname -r 4. 常见问题和注意事项 # 保留旧内核：安装新内核时，旧内核通常不会被删除。你可以通过 yum remove kernel 删除不需要的旧内核，或者手动删除 /boot 中的旧内核文件。务必保留至少一个旧内核，以防新内核出现问题时能回滚。\nsudo yum remove kernel-\u0026lt;version\u0026gt; 使用 GRUB 选择内核：在系统启动时，可以通过 GRUB 菜单选择启动哪个内核。在启动时按下 Esc 键或 Shift 键（取决于你的系统配置），选择你需要的内核。\n引导加载器问题：如果在系统启动时遇到问题，可以进入 GRUB 进行内核选择，或者从 Live CD 启动修复。\n总结 # CentOS 7 用户可以使用 yum 命令更新内核，或者通过安装 ELRepo 提供的最新内核。 手动编译内核 是一种高级选项，适用于需要自定义内核配置的用户。 在更新内核后，记得重启系统并验证内核版本。 通过这些步骤，你可以根据自己的需求顺利升级 Linux 内核。\n7. nginx日志访问量前十的ip怎么统计？ # awk \u0026#39;{array[$1]++}END{for (ip in array)print ip,array[ip]}\u0026#39; access.log |sort -k2 -rn|head 要统计 Nginx 日志中访问量前十的 IP 地址，可以通过 awk、sort 和 uniq 等命令来处理 Nginx 的访问日志。以下是一些常见的步骤和命令来实现这个目标。\n1. 基本的 Nginx 访问日志格式 # 假设 Nginx 的访问日志格式为以下标准格式：\n127.0.0.1 - - [12/Oct/2023:14:28:14 +0000] \u0026#34;GET /index.html HTTP/1.1\u0026#34; 200 1024 \u0026#34;-\u0026#34; \u0026#34;Mozilla/5.0\u0026#34; 其中，第一个字段（127.0.0.1）是访问的客户端 IP 地址。\n2. 通过命令统计访问量前十的 IP # 你可以使用 awk 提取 IP 地址，然后使用 sort 和 uniq 统计每个 IP 的访问量，并显示访问量前十的 IP。\n统计步骤： # 提取 IP 地址并统计次数：\n假设你的 Nginx 访问日志文件是 /var/log/nginx/access.log，你可以使用以下命令：\ncat /var/log/nginx/access.log | awk \u0026#39;{print $1}\u0026#39; | sort | uniq -c | sort -nr | head -n 10 解释：\ncat /var/log/nginx/access.log：输出 Nginx 访问日志。 awk '{print $1}'：提取日志中的 IP 地址（假设 IP 是日志的第一个字段）。 sort：对 IP 地址进行排序，sort 是为 uniq 做准备。 uniq -c：统计每个 IP 出现的次数。 sort -nr：按照访问次数进行降序排序。 head -n 10：输出访问量前十的 IP 地址。 该命令会输出类似如下的结果：\n320 192.168.1.1 210 10.0.0.2 180 192.168.0.3 150 10.0.0.4 ... 其中数字表示该 IP 地址的访问次数。\n3. 详细的字段输出 # 如果你希望输出 IP 地址和访问次数，并且显示更详细的信息（比如每个 IP 的访问量），可以使用以下命令：\ncat /var/log/nginx/access.log | awk \u0026#39;{print $1}\u0026#39; | sort | uniq -c | sort -nr | head -n 10 | awk \u0026#39;{print \u0026#34;IP:\u0026#34;, $2, \u0026#34;访问量:\u0026#34;, $1}\u0026#39; 输出示例：\nIP: 192.168.1.1 访问量: 320 IP: 10.0.0.2 访问量: 210 IP: 192.168.0.3 访问量: 180 IP: 10.0.0.4 访问量: 150 ... 4. 按日期/时间过滤日志 # 如果你只关心某个时间段内的访问量，可以通过 grep 结合时间过滤日志。例如，查看 2023 年 10 月 12 日的日志：\ngrep \u0026#39;12/Oct/2023\u0026#39; /var/log/nginx/access.log | awk \u0026#39;{print $1}\u0026#39; | sort | uniq -c | sort -nr | head -n 10 5. 按请求类型（如 GET, POST）进行过滤 # 如果你只关心某种请求类型的访问量（如 GET 请求），可以通过 grep 过滤：\ngrep \u0026#39;GET\u0026#39; /var/log/nginx/access.log | awk \u0026#39;{print $1}\u0026#39; | sort | uniq -c | sort -nr | head -n 10 这样，你可以针对特定类型的请求来统计访问量前十的 IP。\n6. 自动化统计并输出到文件 # 如果你想将统计结果保存到文件中，可以将命令输出重定向到一个文件：\ncat /var/log/nginx/access.log | awk \u0026#39;{print $1}\u0026#39; | sort | uniq -c | sort -nr | head -n 10 \u0026gt; top_ip.txt 然后可以查看 top_ip.txt 文件获取统计信息。\n总结： # 使用 awk 提取 IP 地址，sort 排序，uniq -c 统计访问次数，sort -nr 按次数降序排列。 可选择将日志按日期或请求类型进行过滤。 可以将统计结果保存到文件中以便后续查看。 通过这种方式，你可以轻松统计 Nginx 日志中的访问量前十的 IP 地址。\n8. 如何删除/var/log/下.log结尾的30天前的日志？ # find /var/log/ -type f -name .*.log -mtime 30|xargs rm -f 要删除 /var/log/ 目录下所有 .log 结尾且超过 30 天的日志文件，可以使用 find 命令来查找这些文件并执行删除操作。以下是具体的步骤：\n1. 查看 .log 文件中超过 30 天的文件 # 首先，你可以使用 find 命令查看 /var/log/ 目录下所有 .log 文件，且这些文件的最后修改时间超过 30 天：\nfind /var/log/ -type f -name \u0026#34;*.log\u0026#34; -mtime +30 解释：\n/var/log/：指定要搜索的目录。 -type f：查找普通文件。 -name \u0026quot;*.log\u0026quot;：查找以 .log 结尾的文件。 -mtime +30：查找修改时间超过 30 天的文件（+30 表示大于 30 天）。 如果执行该命令后，你可以看到符合条件的日志文件列表。\n2. 删除超过 30 天的 .log 文件 # 如果确认要删除这些文件，可以将 find 命令与 -exec 选项结合，执行删除操作：\nfind /var/log/ -type f -name \u0026#34;*.log\u0026#34; -mtime +30 -exec rm -f {} \\; 解释：\n-exec rm -f {} \\;：对于找到的每个文件，执行 rm -f 删除操作。{} 是一个占位符，表示当前找到的文件名，\\; 表示命令的结束。 3. 在删除前进行确认 # 为了避免误删，可以先运行一个测试命令，确认哪些文件将被删除：\nfind /var/log/ -type f -name \u0026#34;*.log\u0026#34; -mtime +30 -exec ls -l {} \\; 这个命令将列出所有符合条件的文件及其详细信息（包括文件大小、修改时间等），你可以检查确认是否正确。\n4. 使用 logrotate 自动管理日志 # 如果你需要定期删除过期日志文件，建议使用 logrotate 工具来管理日志文件的轮换和删除。logrotate 是 Linux 系统上用于自动化日志文件管理的工具，通常它会配置为定期归档和删除旧日志文件。\n配置示例： # /etc/logrotate.d/ 目录下存放着每个服务的日志轮换配置文件。你可以在该目录中为 /var/log/ 下的日志创建或修改一个配置文件来自动管理日志。\n一个基本的 logrotate 配置示例：\n/var/log/*.log { daily # 每天轮换 rotate 7 # 保留7个历史日志 compress # 压缩归档的日志 missingok # 如果日志文件缺失，则不报错 notifempty # 如果日志文件为空，不进行轮换 create 0644 root root # 新日志文件创建时的权限 dateext # 使用日期作为归档文件名的一部分 maxage 30 # 删除超过30天的日志文件 } 总结： # 使用 find 命令查找并删除 /var/log/ 目录下修改时间超过 30 天的 .log 文件。 可以先使用 ls -l 查看将要删除的文件，确保删除操作不会误删重要文件。 建议使用 logrotate 工具自动管理和清理日志文件，避免手动删除。 9. ansible有哪些模块？功能是什么？ # 模块 功能 copy 拷贝文件到被控端 cron 定时任务 fetch 拷贝被控端文件到本地 file 文件模块 group 用户组模块 user 用户模块 hostname 主机名模块 script 脚本模块 service 服务启动模块 command 远程执行命令模块 shell 远程执行命令模块，command高级用法 yum 安装包组模块 setup 查看主机系统信息 Ansible 提供了大量的模块来实现各种自动化任务。这些模块是 Ansible 执行任务的基本单元，它们负责执行特定的操作，例如文件管理、系统管理、应用部署等。模块可以分为不同的类别，每个模块实现不同的功能。以下是 Ansible 常见模块的分类和功能概述。\n1. 文件和目录管理模块 # 这些模块用于文件和目录的创建、删除、修改、权限管理等操作。\nfile：管理文件或目录的属性，如权限、所有者和群组。 示例：ansible -m file -a \u0026quot;path=/tmp/testfile state=touch mode=0644\u0026quot; copy：将本地文件或目录复制到远程主机。 示例：ansible -m copy -a \u0026quot;src=/path/to/local/file dest=/path/to/remote/file\u0026quot; template：将模板文件（通常是 Jinja2 格式的）从本地复制到远程主机，并渲染模板。 示例：ansible -m template -a \u0026quot;src=/path/to/template.j2 dest=/path/to/remote/file\u0026quot; fetch：从远程主机下载文件到本地。 示例：ansible -m fetch -a \u0026quot;src=/path/to/remote/file dest=/path/to/local/dir flat=yes\u0026quot; lineinfile：修改文件中的某一行。 示例：`ansible -m lineinfile -a \u0026ldquo;path=/etc/hosts line=\u0026lsquo;192.168.1.1 myhost\u0026rsquo;\u0026rdquo; file：改变文件的属性，移动文件，或删除文件。 示例：ansible -m file -a \u0026quot;path=/path/to/file state=absent\u0026quot; 2. 系统管理模块 # 这些模块用于管理操作系统中的各种资源。\nuser：管理用户账户。 示例：ansible -m user -a \u0026quot;name=test_user state=present\u0026quot; group：管理用户组。 示例：ansible -m group -a \u0026quot;name=test_group state=present\u0026quot; service：管理服务的状态（启动、停止、重启等）。 示例：ansible -m service -a \u0026quot;name=httpd state=started\u0026quot; command：在远程主机上执行命令。 示例：ansible -m command -a \u0026quot;df -h\u0026quot; shell：在远程主机上执行 shell 命令。 示例：ansible -m shell -a \u0026quot;echo 'Hello' \u0026gt; /tmp/hello.txt\u0026quot; cron：管理定时任务。 示例：ansible -m cron -a \u0026quot;name='run_backup' minute='0' hour='2' job='/usr/bin/backup.sh' state=present\u0026quot; reboot：重启远程主机。 示例：ansible -m reboot -a \u0026quot;connect_timeout=5\u0026quot; hostname：设置主机名。 示例：ansible -m hostname -a \u0026quot;name=myhost\u0026quot; 3. 网络管理模块 # 这些模块用于网络设备的管理和配置。\nuri：与 REST API 进行交互，发送 HTTP 请求。 示例：ansible -m uri -a \u0026quot;url=http://example.com method=GET\u0026quot; ping：测试与远程主机的连接。 示例：ansible -m ping all firewalld：管理 firewalld 防火墙规则。 示例：ansible -m firewalld -a \u0026quot;service=http permanent=true state=enabled\u0026quot; docker：管理 Docker 容器和镜像。 示例：ansible -m docker_container -a \u0026quot;name=test_container state=started image=ubuntu\u0026quot; 4. 包管理模块 # 这些模块用于管理操作系统上的软件包（安装、更新、删除）。\nyum：用于管理基于 RPM 的 Linux 发行版（如 CentOS、RHEL）的软件包。 示例：ansible -m yum -a \u0026quot;name=httpd state=present\u0026quot; apt：用于管理基于 Debian 的 Linux 发行版（如 Ubuntu）的软件包。 示例：ansible -m apt -a \u0026quot;name=nginx state=present\u0026quot; pip：用于安装和管理 Python 包。 示例：ansible -m pip -a \u0026quot;name=flask state=present\u0026quot; npm：用于管理 Node.js 包。 示例：ansible -m npm -a \u0026quot;name=express state=present\u0026quot; 5. 数据库管理模块 # 这些模块用于管理数据库实例和执行相关任务。\nmysql_db：管理 MySQL 数据库的创建、删除、备份等操作。 示例：ansible -m mysql_db -a \u0026quot;name=testdb state=present\u0026quot; postgresql_db：管理 PostgreSQL 数据库。 示例：ansible -m postgresql_db -a \u0026quot;name=testdb state=present\u0026quot; mongodb：管理 MongoDB 数据库。 示例：ansible -m mongodb -a \u0026quot;name=testdb state=present\u0026quot; 6. 云服务模块 # 这些模块用于云平台的资源管理，如 AWS、Azure、Google Cloud 等。\nec2：在 AWS 上创建、管理 EC2 实例。 示例：ansible -m ec2 -a \u0026quot;image=ami-xxxxxxxx region=us-west-2 instance_type=t2.micro count=1\u0026quot; gce_instance：管理 Google Cloud 上的实例。 示例：ansible -m gce_instance -a \u0026quot;name=my-instance zone=us-central1-a image_family=debian-9 image_project=debian-cloud\u0026quot; azure_rm_virtualmachine：在 Azure 上创建、管理虚拟机。 示例：ansible -m azure_rm_virtualmachine -a \u0026quot;resource_group=myResourceGroup name=myVM image=UbuntuLTS size=Standard_B1s\u0026quot; 7. 监控与报警模块 # 这些模块用于监控和报警管理。\nprometheus：与 Prometheus 进行集成，管理和查询 Prometheus 指标。 示例：ansible -m prometheus -a \u0026quot;job=myjob target=localhost\u0026quot; grafana_dashboard：用于管理 Grafana 仪表盘。 示例：ansible -m grafana_dashboard -a \u0026quot;name=MyDashboard state=present json=/path/to/dashboard.json\u0026quot; 8. 用户输入和通知模块 # 这些模块用于处理与用户的交互或向外部系统发送通知。\npause：暂停执行一段时间，或等待用户输入。 示例：ansible -m pause -a \u0026quot;minutes=5\u0026quot; debug：输出调试信息。 示例：ansible -m debug -a \u0026quot;msg='This is a debug message'\u0026quot; slack：向 Slack 通道发送消息。 示例：ansible -m slack -a \u0026quot;token=YOUR_SLACK_TOKEN channel=#general msg='Deployment Complete'\u0026quot; 9. 容器和虚拟化管理模块 # 这些模块用于容器和虚拟机的管理。\ndocker_image：管理 Docker 镜像的拉取和删除。 示例：ansible -m docker_image -a \u0026quot;name=nginx state=present\u0026quot; docker_container：管理 Docker 容器的启动、停止、重启等操作。 示例：ansible -m docker_container -a \u0026quot;name=my-container state=started image=nginx\u0026quot; virt：管理虚拟机。 示例：ansible -m virt -a \u0026quot;name=my-vm state=running\u0026quot; 总结： # Ansible 提供了丰富的模块，涵盖了系统管理、文件管理、数据库管理、云服务管理、网络管理等多个领域。你可以根据任务需求，选择相应的模块来实现自动化操作。Ansible 的模块功能非常灵活，支持不同操作系统和云平台，极大地方便了系统管理和应用部署。\n10. nginx为什么比apache快？ # nginx采用epoll模型 apache采用select模型 Nginx 通常比 Apache 快，主要有以下几个原因：\n1. 事件驱动架构 # Nginx 采用的是事件驱动（Event-Driven）架构，而 Apache 默认使用的是基于进程（或线程）模型。Nginx 的事件驱动模型使得它能够处理大量的并发连接，而不需要为每个连接创建新的进程或线程。\nNginx：使用单个或少数线程来处理请求，所有请求都由事件循环（event loop）管理，且每个连接在处理时都不阻塞其他连接。这使得 Nginx 在处理高并发请求时非常高效。 Apache：默认使用进程/线程池（比如 prefork 模块），每个请求都会启动一个独立的进程或线程来处理。这样虽然能处理并发请求，但每个请求都需要额外的内存和上下文切换，导致效率较低。 2. 内存消耗 # 由于 Nginx 是基于异步非阻塞的事件驱动模式，它能在一个工作进程中处理成千上万的并发连接。每个连接消耗的资源非常少，通常只有几十个字节的内存。\n相比之下，Apache 的线程/进程模式需要为每个请求分配独立的内存和操作系统资源。即使请求量不高，资源消耗也相对较大，因此在高并发场景下性能不如 Nginx。\n3. 高效的静态内容处理 # Nginx 被设计为一个高效的反向代理服务器，特别擅长处理静态文件请求（如图片、CSS、JavaScript、HTML 等）。它能够高效地使用操作系统的内存和硬件缓存机制，快速响应静态内容的请求。\nApache 虽然也支持静态内容的处理，但在处理静态文件时，Apache 的性能通常低于 Nginx，因为 Apache 需要通过多个模块进行处理，并且涉及更多的进程/线程调度。\n4. 非阻塞 I/O # Nginx 使用非阻塞 I/O（Non-blocking I/O）模型，即使在处理大量请求时，进程不会被阻塞等待 I/O 操作完成。通过异步事件通知机制，Nginx 可以高效地在多个连接之间切换，从而避免了传统的线程/进程阻塞。\nApache 默认使用阻塞 I/O，这意味着当一个请求在处理时，如果需要等待 I/O 操作（比如访问数据库或文件系统），它会阻塞当前线程，直到操作完成。这样在高并发情况下，Apache 的响应时间会变慢。\n5. 更低的 CPU 和内存开销 # 由于 Nginx 的事件驱动模式和非阻塞 I/O，它通常需要更少的 CPU 和内存资源来处理大量的请求。它的工作进程不需要频繁的上下文切换，也不会为每个请求分配大量的内存。\nApache 在高并发情况下，由于每个请求都创建一个新的进程或线程，导致 CPU 和内存消耗较高。因此，Nginx 在处理大量并发请求时比 Apache 更加高效。\n6. 负载均衡和反向代理能力 # Nginx 作为反向代理服务器和负载均衡器，提供了非常高效的请求分发和负载均衡机制。它能够通过多个算法（如轮询、IP 哈希、最少连接等）将请求分发到后端服务器，支持高效的分布式架构，极大提高了性能和扩展性。\n虽然 Apache 也支持反向代理和负载均衡，但 Nginx 通常比 Apache 更高效，并且对请求的分发和处理更加精细。\n7. 配置简洁与优化 # Nginx 的配置文件相对简洁且高效，能够快速加载和应用配置。而 Apache 的配置文件可能更加复杂，尤其在启用多个模块时，可能会引入不必要的复杂性和性能开销。\n总结： # Nginx 比 Apache 快的主要原因是其采用了事件驱动的非阻塞 I/O 模型，能够高效处理大量并发连接，而不需要为每个请求创建新的进程或线程。此外，Nginx 对静态内容的处理非常高效，内存消耗较少，并且通过负载均衡和反向代理功能提高了性能。而 Apache 的多进程/多线程模式虽然灵活，但在高并发和资源消耗方面通常不如 Nginx 高效。\n11. 四层负载和七层负载区别是什么？ # 四层基于IP+端口进行转发 七层就是基于URL等应用层信息的负载均衡 四层负载均衡和七层负载均衡的区别主要体现在它们处理请求的层次不同，以及基于不同协议和信息进行流量分配。具体区别如下：\n1. OSI模型层次 # 四层负载均衡（Layer 4 Load Balancing）：操作在 OSI 模型的第四层，也就是 传输层。它处理的是 TCP 或 UDP 协议的数据包，主要根据 IP 地址和端口号来进行流量分配。 七层负载均衡（Layer 7 Load Balancing）：操作在 OSI 模型的第七层，也就是 应用层。它根据应用层协议（如 HTTP、HTTPS、FTP 等）中的内容来做决策，可以基于请求的 URL、头部、Cookie、HTTP 方法等信息来进行流量调度。 2. 负载均衡决策依据 # 四层负载均衡\n：\n基于 IP 地址和端口：四层负载均衡在流量到达服务器之前，主要基于客户端 IP 地址、目标 IP 地址和端口号来决定如何将流量转发到后端服务器。 它不理解上层协议的内容，仅仅是在传输层（TCP、UDP）进行决策，因此它的处理速度通常较快，适用于简单的流量分发。 七层负载均衡\n：\n基于应用层信息：七层负载均衡可以理解并使用更复杂的应用层协议信息来进行决策。例如，它可以根据 HTTP 请求的 URL、请求头、Cookie 等信息来决定请求应该转发到哪个后端服务器。 七层负载均衡提供了更多的灵活性，能够进行内容交换、SSL 终止、基于会话的负载分配等。 3. 处理复杂度和性能 # 四层负载均衡\n：\n性能更高，处理更简单：由于四层负载均衡只关注网络层和传输层的基本信息，处理速度较快，适合高并发的网络流量。它通常用于较为简单的负载均衡需求，如 TCP、UDP 服务的流量分发。 七层负载均衡\n：\n处理较复杂，性能较低：七层负载均衡需要解析应用层的内容，计算量较大，可能影响性能，但它能提供更多的功能和灵活性。例如，应用层负载均衡可以对不同的请求类型执行不同的策略，如按请求 URL 或 HTTP 方法进行流量分配。 4. 应用场景 # 四层负载均衡： 适合简单的 TCP 或 UDP 服务，通常用于数据库集群、VPN、邮件服务器、实时通信等应用场景。 常见的四层负载均衡设备或软件如：硬件负载均衡器（F5 Big-IP）、Nginx、IPVS（Linux Virtual Server）等。 七层负载均衡： 适用于 Web 应用和更复杂的应用场景，特别是需要基于请求内容（如 HTTP 头部、URL）进行智能流量分配的情况。七层负载均衡器常用于高效的 Web 应用服务、微服务架构和 API 网关等场景。 常见的七层负载均衡器如：Nginx、HAProxy、Traefik、Apache HTTP Server等。 5. 功能特性 # 四层负载均衡\n：\n支持基于源 IP 或端口号的流量分配。 支持透明传输和高效的负载分配，但不处理任何应用层协议的内容。 支持高并发连接的负载均衡，通常用于低延迟的服务。 七层负载均衡\n：\n支持更细粒度的流量控制，如基于 URL 路径、HTTP 方法（GET、POST）、头部字段、Cookie、查询字符串等进行流量分发。 可以实现内容交换（如缓存、压缩、SSL 终止等），对请求进行智能化的转发。 支持 A/B 测试、蓝绿部署、会话保持等高级功能。 6. SSL/TLS 终止 # 四层负载均衡：不处理 SSL/TLS 加密和解密操作，流量会以加密的形式传递到后端服务器。 七层负载均衡：通常可以处理 SSL/TLS 终止，即解密客户端的加密请求后，再转发给后端服务器。这样可以减轻后端服务器的负担，并提供对 HTTPS 流量的更多控制。 总结： # 四层负载均衡关注的是 传输层（IP 地址、端口），适用于简单的 TCP/UDP 流量分发，性能高，适合高并发和低延迟需求。 七层负载均衡则可以解析 应用层 协议（如 HTTP、HTTPS），能够做基于内容的精细化流量调度，功能更强大，但处理复杂度较高，适合 Web 应用和其他需要基于内容进行智能流量分配的场景。 12. lvs有哪些工作模式？哪个性能高？ # dr：直接路由模式，请求由 LVS 接受，由真实提供服务的服务器直接返回给用户，返回的时候不经过 LVS。（性能最高） tun：隧道模式，客户端将访问vip报文发送给LVS服务器。LVS服务器将请求报文重新封装，发送给后端真实服务器。后端真实服务器将请求报文解封，在确认自身有vip之后进行请求处理。后端真实服务器在处理完数据请求后，直接响应客户端。 nat：网络报的进出都要经过 LVS 的处理。LVS 需要作为 RS 的网关。当包到达 LVS 时，LVS 做目标地址转换（DNAT），将目标 IP 改为 RS 的 IP。RS 接收到包以后，仿佛是客户端直接发给它的一样。RS 处理完，返回响应时，源 IP 是 RS IP，目标 IP 是客户端的 IP。这时 RS 的包通过网关（LVS）中转，LVS 会做源地址转换（SNAT），将包的源地址改为 VIP，这样，这个包对客户端看起来就仿佛是 LVS 直接返回给它的。客户端无法感知到后端 RS 的存在。 fullnat模式：fullnat模式和nat模式相似，但是与nat不同的是nat模式只做了两次地址转换，fullnat模式却做了四次。 LVS（Linux Virtual Server）是一个高性能的负载均衡解决方案，它提供了几种不同的工作模式，用于处理不同类型的负载均衡需求。LVS 的工作模式包括：\n1. NAT (Network Address Translation) 模式 # 在 NAT 模式下，LVS 会修改客户端请求的目标 IP 地址（即将目标 IP 地址更改为后端服务器的 IP 地址），然后将请求转发到合适的后端服务器。后端服务器的响应也会通过 LVS 转发回客户端。\n特点\n：\nLVS 充当了客户端和后端服务器之间的代理，客户端认为自己在与 LVS 交互，而 LVS 再将请求转发到实际的后端服务器。 每个后端服务器的 IP 地址是私有地址，LVS 充当路由器，NAT 会更改请求的目标地址。 LVS 需要修改网络数据包，因此需要较多的计算资源。 适用场景\n：\n当后端服务器的 IP 地址是私有的，且不直接暴露在公网时，NAT 模式是最常用的。 2. DR (Direct Routing) 模式 # 在 DR 模式下，LVS 直接将请求转发到后端服务器，而不需要修改目标地址。LVS 只是根据请求选择合适的后端服务器，然后将请求转发给该服务器，后端服务器直接将响应返回给客户端，LVS 不参与响应的转发。\n特点： 在 DR 模式下，客户端请求的目标 IP 地址不需要改变，后端服务器的 IP 地址必须是公有 IP 地址，且 LVS 的虚拟 IP 地址（VIP）必须在后端服务器的网络接口上。 后端服务器直接发送响应，LVS 不会再次转发响应。 性能高，因为没有修改数据包，也没有代理响应数据，减少了系统负载。 适用场景： 后端服务器有公有 IP 地址，并且 LVS 和后端服务器位于同一网络。 3. TUN (IP Tunneling) 模式 # 在 TUN 模式下，LVS 使用 IP 隧道技术将客户端请求转发到后端服务器。LVS 将客户端请求封装在一个新的数据包中，传输给后端服务器，后端服务器解封装并处理请求。与 NAT 模式不同，TUN 模式没有直接修改目标 IP 地址，而是通过 IP 隧道将数据包传递给后端服务器。\n特点\n：\n适用于后端服务器与 LVS 在不同网络之间进行通信的情况。 LVS 通过 IP 隧道将请求转发给后端服务器，这样可以跨越网络边界，但需要更多的网络带宽和计算资源。 适用场景\n：\n后端服务器和 LVS 位于不同的子网，且需要通过隧道进行通信。 性能对比： # NAT 模式：性能相对较低，因为每个数据包都需要被 LVS 修改，增加了处理负担。特别是在高负载时，NAT 模式的性能可能会受到影响。 DR 模式：性能最高。因为 DR 模式不需要修改数据包，也不需要代理响应，所以它的效率非常高，几乎不会给系统带来额外的负载。适用于高并发场景。 TUN 模式：性能次之。虽然数据包不需要修改，但 IP 隧道的开销较大，尤其在跨网络传输时需要额外的计算资源。因此，TUN 模式的性能通常低于 DR 模式，但比 NAT 模式要好。 总结： # DR 模式 性能最好，适用于 LVS 和后端服务器在同一网络中的情况。 TUN 模式 性能较高，但会受到隧道开销的影响，适用于跨网络传输的场景。 NAT 模式 性能相对较低，适用于需要将后端服务器的私有 IP 地址隐藏的场景。 因此，DR 模式 被认为是性能最优的工作模式，尤其是在 LVS 和后端服务器在同一网络中时。\n13. lvs nginx haproxy keeplived区别，优缺点？ # 参考链接：https://blog.51cto.com/816885/2529993?source=dra\nLVS、Nginx、HAProxy 和 Keepalived 都是常见的负载均衡解决方案，它们各有优缺点，适用于不同的场景。以下是这四者的区别、优缺点以及适用场景的详细对比。\n1. LVS (Linux Virtual Server) # LVS 是一种基于 Linux 的高性能负载均衡解决方案，主要通过不同的工作模式（NAT、DR、TUN）来进行负载均衡。\n优点： # 高性能：LVS 是内核级负载均衡器，直接在 Linux 内核中实现，处理速度非常快。 支持高并发：能够处理大量的并发连接，适用于大规模应用。 灵活性：支持多种负载均衡模式（NAT、DR、TUN），可以根据实际需求选择合适的模式。 故障切换：支持高可用配置，可以和 Keepalived 配合使用，提供故障切换和虚拟 IP 高可用性。 缺点： # 配置复杂：相对于 Nginx 和 HAProxy，LVS 的配置和管理较为复杂，调试和排错也较为困难。 缺少应用层负载均衡功能：LVS 主要操作在传输层（L4），不支持深入的应用层（L7）负载均衡，如基于 URL、HTTP 方法等。 适用场景： # 高流量网站和高并发应用，尤其适用于 Web 服务器集群、数据库集群等。 2. Nginx # Nginx 是一个高性能的 Web 服务器和反向代理服务器，也可以作为负载均衡器使用。它支持应用层负载均衡，能够处理 HTTP、HTTPS、TCP、UDP 流量。\n优点： # 高性能：作为反向代理和负载均衡器，Nginx 性能非常好，能够处理大量并发请求。 灵活性：支持 L4 和 L7 负载均衡，可以根据 URL、请求头、Cookie 等应用层信息进行流量分发。 易于配置和管理：配置文件简洁，管理方便，支持热部署。 功能丰富：支持 SSL 终止、缓存、会话保持等功能，适用于 Web 应用。 缺点： # 仅限于 HTTP/S 和 TCP/UDP 负载均衡：虽然支持多种协议，但在一些特殊应用场景（如数据库负载均衡）可能不如 LVS 高效。 在大规模集群环境下的性能不如 LVS：对于超高并发的负载均衡，Nginx 的性能可能不如 LVS。 适用场景： # Web 服务、API 网关、微服务架构，适用于处理 HTTP、HTTPS 等应用层流量的负载均衡。 3. HAProxy # HAProxy 是一个高性能的负载均衡器，广泛用于 Web 应用的负载均衡，尤其擅长高可用性和高并发环境。\n优点： # 高性能：HAProxy 在 L4 和 L7 负载均衡方面表现非常好，特别适用于高并发、高可用性要求的环境。 灵活性：支持多种负载均衡算法（轮询、最少连接等），并且可以基于 HTTP 请求头、URL、IP 地址等进行智能流量分配。 高可用性：支持健康检查、会话保持、SSL 终止等功能，并且和 Keepalived 配合使用时可以提供高可用性。 监控和统计：提供详细的日志和监控功能，便于性能分析和故障排查。 缺点： # 配置复杂：HAProxy 的配置文件相对较为复杂，尤其是在需要自定义负载均衡规则时。 不支持静态内容缓存：HAProxy 不能像 Nginx 一样处理静态文件，通常需要与其他 Web 服务器结合使用。 适用场景： # 高并发 Web 服务、微服务架构、API 网关，适用于需要高可用性和高性能负载均衡的场景。 4. Keepalived # Keepalived 主要用于提供高可用性，它与 LVS、Nginx 或 HAProxy 配合使用，管理虚拟 IP 地址，提供故障切换和负载均衡功能。\n优点： # 高可用性：Keepalived 通过 VRRP 协议提供虚拟 IP 地址，能够在主节点故障时自动切换到备节点，保证系统的高可用性。 简单配置：Keepalived 配置相对简单，能够与 LVS、Nginx 或 HAProxy 等负载均衡器无缝集成。 支持健康检查：Keepalived 可以监控负载均衡器的状态，自动进行故障转移。 缺点： # 单独使用不提供负载均衡功能：Keepalived 本身并不提供负载均衡功能，必须与 LVS、Nginx 或 HAProxy 配合使用。 主要针对高可用性设计：对于负载均衡功能，Keepalived 本身并不提供复杂的负载均衡策略，更多侧重于故障转移。 适用场景： # 高可用性场景，特别是与 LVS、Nginx 或 HAProxy 配合使用，确保虚拟 IP 地址的故障转移和负载均衡器的高可用性。 总结与适用场景： # LVS：性能最强，适用于大规模高并发的流量负载均衡，尤其适用于需要高效 TCP/UDP 负载均衡的场景。配置较复杂。 Nginx：适用于 Web 应用和 API 的负载均衡，支持 L4 和 L7，能够基于应用层信息做智能流量分发，配置简单，性能优秀，适合中小规模应用。 HAProxy：适用于高可用性和高并发的 Web 应用负载均衡，支持 L4 和 L7，配置较复杂，但在高性能和高可用性场景下非常出色。 Keepalived：提供高可用性和故障切换功能，通常与 LVS、Nginx 或 HAProxy 配合使用，确保系统的容错和高可用性。 通常，LVS 和 Keepalived 配合使用适用于需要极高性能的负载均衡环境，而 Nginx 和 HAProxy 更适合 Web 应用和高可用负载均衡。\n14. 如下url地址，各个部分的含义 # https://www.baidu.com/s?word=123\u0026ie=utf-8\nhttps: 使用https加密协议访问 www.baidu.com/s: 请求地址 ?word\u0026amp;ie=utf-8: get请求的参数，多个参数\u0026amp;连接 这个 URL 地址是一个典型的 HTTP 请求 URL，通常用于发起一个搜索请求。下面是对各个部分的解析：\nhttps://www.baidu.com/s?word=123\u0026amp;ie=utf-8 1. 协议部分：https:// # https 表示使用的是安全的超文本传输协议（HTTP Secure），即对数据进行加密传输。 :// 是协议与后续内容的分隔符。 2. 主机名部分：www.baidu.com # www.baidu.com 是服务器的域名，指向百度的官方网站。 这个部分用于定位服务器的地址，也就是目标主机。 3. 路径部分：/s # /s 是一个路径，表示对百度网站上的某个页面进行请求。在百度中，通常 /s 表示进行搜索的页面。 4. 查询参数部分：?word=123\u0026amp;ie=utf-8 # ? 表示后面跟随的是查询字符串。\n查询字符串由多个参数组成，每个参数之间通过 \u0026amp; 符号分隔。\nword=123\n：\nword 是一个查询参数的名称，表示搜索的关键字。 123 是该参数的值，表示搜索内容是 123。 ie=utf-8\n：\nie 是一个查询参数的名称，表示字符编码格式。 utf-8 是该参数的值，表示使用 UTF-8 编码格式来传输数据。 总结： # https://www.baidu.com/s 是访问百度搜索页面的路径。 word=123 表示搜索关键字是 123。 ie=utf-8 表示使用 UTF-8 编码格式。 所以这个 URL 的含义是：通过 HTTPS 协议访问百度的搜索页面，搜索内容是 123，并使用 UTF-8 编码格式来处理请求和响应。\n15. tomcat各个目录含义，如何修改端口，如何修改内存数？ # bin 存放tomcat命令 conf 存放tomcat配置文件 lib 存放tomcat运行需要加载的jar包 log 存在Tomcat运行产生的日志 temp 运行过程中产生的临时文件 webapps 站点目录 work 存放tomcat运行时的编译后的文件 conf/server.xml 修改端口号 bin/catalina.sh 修改jvm内存 Tomcat 目录结构含义 # Tomcat 的目录结构主要包含以下几个重要的文件夹和文件，每个文件夹的含义如下：\nbin/： 包含 Tomcat 的启动脚本和控制脚本。 startup.sh：启动 Tomcat 的脚本（Linux/Unix 系统）。 startup.bat：启动 Tomcat 的脚本（Windows 系统）。 shutdown.sh：关闭 Tomcat 的脚本（Linux/Unix 系统）。 shutdown.bat：关闭 Tomcat 的脚本（Windows 系统）。 catalina.sh：Tomcat 启动的核心脚本。 catalina.bat：Tomcat 启动的核心脚本（Windows 系统）。 conf/： 包含 Tomcat 的配置文件。 server.xml：Tomcat 的主要配置文件，定义了 Tomcat 的各项配置，包括端口、连接器、虚拟主机等。 web.xml：Web 应用的默认配置文件，定义了 Tomcat 如何处理特定请求，配置 servlet、过滤器等。 context.xml：为每个 Web 应用提供单独的配置，可以在其中定义数据库连接、路径等。 tomcat-users.xml：定义 Tomcat 的用户及权限，常用于设置管理员权限。 lib/： 包含 Tomcat 所需的库文件（.jar 文件）。 这些是 Tomcat 运行时所依赖的 Java 类库。 logs/： 存放 Tomcat 运行过程中的日志文件。 catalina.out：Tomcat 启动和运行时的标准输出日志。 localhost.log、manager.log 等：具体的日志文件，用于记录不同方面的日志。 webapps/： 存放 Web 应用的目录。 默认情况下，Tomcat 会将其 Web 应用放在这个目录下，每个 Web 应用对应一个文件夹。 你可以将自己的 Web 应用程序（如 .war 文件）放在这里，Tomcat 会自动解压并部署。 work/： 存放 Tomcat 编译过程中生成的工作文件。 包含 JSP 编译后的文件、Servlet 生成的临时文件等。 temp/： 存放 Tomcat 在运行时使用的临时文件。 ROOT/： 这是默认的 Web 应用程序目录，也就是 Tomcat 部署时会映射到 / 路径。 如何修改 Tomcat 的端口？ # Tomcat 默认的 HTTP 端口是 8080，要修改端口，可以编辑 server.xml 文件：\n打开 conf/server.xml 文件。 找到以下内容，修改端口号： \u0026lt;Connector port=\u0026#34;8080\u0026#34; protocol=\u0026#34;HTTP/1.1\u0026#34; connectionTimeout=\u0026#34;20000\u0026#34; redirectPort=\u0026#34;8443\u0026#34; /\u0026gt; 将 port=\u0026quot;8080\u0026quot; 修改为你想要的端口（如：port=\u0026quot;9090\u0026quot;）。 保存并关闭文件。 重启 Tomcat。 如何修改 Tomcat 的内存设置？ # Tomcat 的内存配置通常通过 Java 虚拟机（JVM）的启动参数进行设置，这些设置可以在 bin/catalina.sh（Linux/Unix）或 bin/catalina.bat（Windows）中找到。\n1. 修改 catalina.sh（Linux/Unix）： # 打开 bin/catalina.sh 文件。 找到 JAVA_OPTS 变量，并设置你想要的内存大小。 例如，设置最大堆内存为 2GB，初始堆内存为 1GB：\nJAVA_OPTS=\u0026#34;-Xms1g -Xmx2g\u0026#34; 其中：\n-Xms1g：指定初始内存为 1GB。 -Xmx2g：指定最大内存为 2GB。 保存并关闭文件。 2. 修改 catalina.bat（Windows）： # 打开 bin/catalina.bat 文件。 找到类似的行并设置内存大小： set JAVA_OPTS=-Xms1g -Xmx2g 保存并关闭文件。 3. 其他内存设置（可选）： # PermGen/Metaspace（JVM）\n： 如果使用的是较旧的 JDK（例如 JDK 7），可以配置\nPermGen\n内存：\nJAVA_OPTS=\u0026#34;-XX:PermSize=128m -XX:MaxPermSize=256m\u0026#34; 对于 JDK 8 及以后版本，\nPermGen 被替换为\nMetaspace ，可以配置：\nJAVA_OPTS=\u0026#34;-XX:MetaspaceSize=128m -XX:MaxMetaspaceSize=256m\u0026#34; 总结： # 修改端口：编辑 conf/server.xml 文件，修改 \u0026lt;Connector port=\u0026quot;8080\u0026quot; /\u0026gt; 部分的端口。 修改内存：在 bin/catalina.sh 或 bin/catalina.bat 文件中，通过 JAVA_OPTS 设置内存参数（例如：-Xms1g -Xmx2g）。 16. nginx反向代理时，如何使后端获取真正的访问来源ip？ # 在location配置段添加以下内容： proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme;\n在使用 Nginx 作为反向代理时，后端服务器通常只能看到来自 Nginx 的 IP 地址，而无法直接获取客户端的真实 IP 地址。为了使后端服务器能够获取到客户端的真实 IP 地址，你需要在 Nginx 中设置一些特殊的头部信息（X-Forwarded-For）来传递客户端的 IP 地址。\n步骤： # 修改 Nginx 配置文件：\n打开 Nginx 的配置文件（通常是 /etc/nginx/nginx.conf 或者是 /etc/nginx/sites-available/default 等文件，具体路径根据安装方式而不同）。\n添加或修改 proxy_set_header 指令：\n在反向代理相关的 location 块中，添加 X-Forwarded-For 头部。通常，你需要确保传递正确的客户端 IP 地址。\n修改或添加如下内容：\nlocation / { proxy_pass http://backend_server; proxy_set_header X-Real-IP $remote_addr; # 将客户端真实IP传递给后端 proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # 保留原有的X-Forwarded-For头 proxy_set_header X-Forwarded-Proto $scheme; # 如果需要，还可以传递协议类型（http 或 https） } 解释：\nproxy_set_header X-Real-IP $remote_addr;：将客户端的真实 IP 地址（$remote_addr）传递给后端服务器。 proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;：将客户端的 IP 地址添加到 X-Forwarded-For 头部。$proxy_add_x_forwarded_for 会将客户端的 IP 地址追加到现有的 X-Forwarded-For 头部（如果该头部已经存在的话）。 proxy_set_header X-Forwarded-Proto $scheme;：如果你的服务器使用了 HTTPS 或者 HTTP，可以通过此头部传递协议类型，后端可以根据此值判断请求是使用的 HTTPS 还是 HTTP。 确保后端服务器正确解析 X-Forwarded-For：\n后端应用程序（如 Web 服务器）需要从 X-Forwarded-For 头部获取客户端的真实 IP 地址，而不是直接使用 REMOTE_ADDR。通常，大多数 Web 框架（如 PHP、Django、Node.js）会自动处理这一点，但如果需要，可能需要手动配置代码来读取该头部。 在一些常见的 Web 服务器配置中，可以参考以下方法： Apache: 可以使用 mod_remoteip 模块来读取 X-Forwarded-For 头部。 PHP: 使用 $_SERVER['HTTP_X_FORWARDED_FOR'] 来获取客户端 IP 地址。 Node.js: 在请求对象中查看 req.headers['x-forwarded-for']。 重启 Nginx：\n配置修改完成后，记得重启 Nginx 使更改生效：\nsudo nginx -t # 测试配置是否正确 sudo systemctl restart nginx # 重启 Nginx 重要提示： # 代理链中的多个代理：如果请求经过多个代理服务器（比如 Nginx 后面还有其他的负载均衡器），X-Forwarded-For 头部将包含多个 IP 地址。第一个 IP 地址通常是客户端的真实 IP，后面的 IP 地址是中间代理服务器的 IP 地址。你可以根据需要处理这些地址。\n例如，如果后端希望获取真实的客户端 IP 地址（即第一个 IP），需要从 X-Forwarded-For 头部获取第一个 IP 地址。\n# 示例：从 X-Forwarded-For 获取第一个 IP 地址 client_ip = request.headers.get(\u0026#39;X-Forwarded-For\u0026#39;, \u0026#39;\u0026#39;).split(\u0026#39;,\u0026#39;)[0] 安全性：由于 X-Forwarded-For 头部可以被客户端伪造，因此在没有可信代理的情况下，最好只信任来自 Nginx 服务器的 X-Forwarded-For 头部。为了避免头部被伪造，可以只允许特定的代理服务器添加此头部，而不信任来自客户端的请求。\n总结： # 通过在 Nginx 中设置 proxy_set_header X-Real-IP 和 proxy_set_header X-Forwarded-For，你可以将客户端的真实 IP 地址传递到后端服务器。后端可以从这些头部获取客户端的真实 IP 地址，而不是 Nginx 的 IP 地址。\n17. nginx负载均衡算法有哪些？ # rr 轮训 weight 加权轮训 ip_hash 静态调度算法 fair 动态调度算法 url_hash url哈希 leat_conn 最小连接数 Nginx 提供了多种负载均衡算法，用于将客户端请求分发到多个后端服务器。根据不同的需求和场景，你可以选择不同的算法。下面是 Nginx 支持的常见负载均衡算法：\n1. 轮询（Round Robin） # 默认算法：Nginx 默认使用的是轮询（Round Robin）算法。\n描述：将请求按顺序轮流分发给每个后端服务器，适合后端服务器性能相似的场景。\n特点\n：\n简单有效，负载分配均匀。 没有考虑每个服务器的负载情况。 对于大部分场景是有效的，但在后端服务器性能不均衡时，可能导致一些服务器负载过高，另一些服务器几乎没有负载。 upstream backend { server backend1.example.com; server backend2.example.com; } 2. 最少连接（Least Connections） # 描述：将请求转发给当前连接数最少的后端服务器。这个算法适合请求处理时间不均匀的应用场景。\n特点\n：\n更加智能，根据每个后端服务器的负载情况分配请求。 可以避免某些服务器被过载（当请求处理时间不均匀时）。 对于高并发和长连接场景（如 HTTP 长连接）效果显著。 upstream backend { least_conn; server backend1.example.com; server backend2.example.com; } 3. IP 哈希（IP Hash） # 描述：根据客户端的 IP 地址计算哈希值，然后将请求分发到与该哈希值对应的服务器。这样，同一个客户端的请求会始终发送到同一台后端服务器（持久连接）。\n特点\n：\n保证同一个客户端的请求总是分发到相同的后端服务器。 适用于需要会话保持的场景（例如购物车等状态依赖于会话的应用）。 不一定能平衡负载，特别是在客户端 IP 分布不均匀时，某些服务器可能会收到更多请求。 upstream backend { ip_hash; server backend1.example.com; server backend2.example.com; } 4. 加权轮询（Weighted Round Robin） # 描述：在轮询的基础上为每个服务器设置一个权重值，根据权重值来调整请求的分配比例。权重高的服务器将获得更多的请求。\n特点\n：\n适用于后端服务器性能不均衡的场景。 权重可以设置为任意正整数，表示每台服务器的负载能力。 更灵活，可以根据后端服务器的性能调整请求的分配策略。 upstream backend { server backend1.example.com weight=3; server backend2.example.com weight=1; } 5. 加权最少连接（Weighted Least Connections） # 描述：结合最少连接和权重的概念，根据每个后端服务器的当前连接数和设置的权重来分配请求。服务器的权重较高且连接较少时，将获得更多的请求。\n特点\n：\n动态分配请求，根据每个服务器的连接数和权重调整负载。 适用于负载情况较为复杂的场景。 upstream backend { least_conn; server backend1.example.com weight=3; server backend2.example.com weight=1; } 6. 随机（Random） # 描述：将请求随机地分发到后端服务器。这个算法简单，适用于负载非常均衡的情况。\n特点\n：\n非常简单且随机，适用于请求不太频繁的负载均衡场景。 可能会导致请求不均匀分布。 upstream backend { random; server backend1.example.com; server backend2.example.com; } 7. 最少响应时间（Least Time） (Nginx Plus) # 描述：这个算法基于服务器的响应时间来分配请求，选择响应时间最短的服务器。Nginx Plus 提供了这个功能，它会选择响应时间最短的服务器来处理请求。\n特点\n：\n会动态根据服务器的响应时间来分配请求。 可以避免服务器因响应过慢导致其他请求堆积。 upstream backend { least_time; server backend1.example.com; server backend2.example.com; } 总结： # Round Robin：简单、均衡，适合负载相对均衡的场景。 Least Connections：根据连接数选择后端，适合负载不均的情况。 IP Hash：基于客户端 IP 保证同一个客户端始终访问同一服务器，适合需要会话保持的应用。 Weighted Round Robin：为每台服务器指定权重，适合后端服务器性能不均的情况。 Weighted Least Connections：结合了最少连接和权重，适用于更复杂的负载均衡场景。 Random：简单、随机，适用于负载非常均衡的情况。 Least Time：基于响应时间选择后端，适合需要低延迟的场景（仅限 Nginx Plus）。 选择负载均衡算法时，需要根据后端服务器的性能、应用场景、请求处理时长等因素来决定最合适的算法。\n18. 如何进行压力测试？ # 例如：模拟10个用户，对百度首页发起总共100次请求。 测试命令： ab -n 100 -c 10 https://www.baidu.com/index.htm\n进行压力测试是评估系统、应用程序或网络在高负载情况下表现的一个重要步骤。通过压力测试，可以发现系统的瓶颈、性能瓶颈和潜在的可扩展性问题。常见的压力测试包括并发用户负载测试、请求吞吐量测试、延迟测试等。\n以下是进行压力测试的常见方法和工具：\n1. 使用 Apache JMeter 进行压力测试 # JMeter 是一个流行的开源性能测试工具，广泛用于 Web 应用程序、数据库和其他服务器的压力测试。\n步骤： # 下载并安装 JMeter： 从 JMeter 官网 下载并解压。 创建测试计划： 打开 JMeter，创建一个新的测试计划（Test Plan）。 在测试计划中添加线程组（Thread Group），线程组指定了模拟的虚拟用户数和循环次数。 配置线程组： Thread Group：设置用户数（Number of Threads）、循环次数（Loop Count）以及启动延迟（Ramp-Up Period）。 每个线程代表一个模拟的并发用户。 添加 HTTP 请求： 在线程组下，添加 HTTP 请求（HTTP Request），配置目标 Web 服务器的地址、端口和请求参数。 添加监听器： 在测试计划中添加不同的监听器（Listener），如聚合报告、图形结果、查看结果树等，用于记录测试结果并生成报告。 运行测试并分析结果： 点击运行按钮开始测试，查看实时结果和图表，分析系统的响应时间、吞吐量、错误率等。 典型场景： # 测试 Web 应用、API 的并发性能。 负载、压力、容量和稳定性测试。 2. 使用 Locust 进行压力测试 # Locust 是一个基于 Python 的开源负载测试工具，支持分布式压力测试。\n步骤： # 安装 Locust：\npip install locust 编写测试脚本：\n创建一个 Python 脚本，定义用户行为（任务），例如：\nfrom locust import HttpUser, task, between class WebsiteUser(HttpUser): wait_time = between(1, 5) @task def load_main_page(self): self.client.get(\u0026#34;/\u0026#34;) @task def load_about_page(self): self.client.get(\u0026#34;/about\u0026#34;) 运行 Locust：\nlocust -f your_locust_file.py 访问 Web 界面：\n默认情况下，Locust 会在 http://localhost:8089 启动一个 Web 界面，您可以在界面中配置并发用户数、生成报告、查看测试结果等。 典型场景： # API、Web 应用的性能测试。 分布式负载测试，多个机器可以一起参与。 3. 使用 ab (Apache Bench) 进行压力测试 # Apache Bench 是一个轻量级的命令行工具，通常用于简单的 HTTP 负载测试。\n步骤： # 运行 ab 命令：\nab -n 1000 -c 10 http://your-server.com/ -n 1000：指定总请求数为 1000。 -c 10：设置并发请求数为 10。 分析结果：\nab 会输出请求的响应时间、吞吐量、请求成功率等性能指标。 典型场景： # 简单的压力测试，适用于快速验证 Web 服务器的性能。 测试 API 或静态资源的吞吐量。 4. 使用 siege 进行压力测试 # Siege 是一个命令行压力测试工具，支持对 Web 应用进行负载测试。\n步骤： # 安装 Siege：\n在 Linux 系统上，使用以下命令安装：\nsudo apt install siege 运行 Siege 测试：\nsiege -c 50 -t 30S http://your-server.com/ -c 50：指定并发用户数为 50。 -t 30S：指定测试持续时间为 30 秒。 分析结果：\nSiege 会显示测试的响应时间、成功请求数、错误率等指标。 典型场景： # 测试 Web 服务器在不同负载下的性能。 持续压力测试，分析系统在长时间运行下的表现。 5. 使用 Gatling 进行压力测试 # Gatling 是一个高性能的负载测试工具，适用于 HTTP、WebSocket 和其他协议的压力测试。\n步骤： # 下载并安装 Gatling：\n从 Gatling 官网 下载并安装。 编写测试脚本：\n使用 Scala 编写测试脚本，模拟用户行为和负载，例如：\nclass BasicSimulation extends Simulation { val httpProtocol = http .baseUrl(\u0026#34;http://your-server.com\u0026#34;) .acceptHeader(\u0026#34;application/json\u0026#34;) val scn = scenario(\u0026#34;Basic Load Test\u0026#34;) .exec(http(\u0026#34;request_1\u0026#34;).get(\u0026#34;/\u0026#34;)) setUp( scn.inject(atOnceUsers(100)) ).protocols(httpProtocol) } 运行测试：\n./bin/gatling.sh -s BasicSimulation 分析结果：\nGatling 会生成 HTML 格式的报告，展示响应时间、吞吐量、错误率等。 典型场景： # 高并发用户模拟。 跨多协议的压力测试。 6. 使用 Artillery 进行压力测试 # Artillery 是一个轻量级、强大的负载测试工具，支持 HTTP、WebSocket 等协议。\n步骤： # 安装 Artillery：\nnpm install -g artillery 编写测试脚本：\n使用 YAML 配置文件定义测试场景：\nconfig: target: \u0026#39;http://your-server.com\u0026#39; phases: - duration: 60 arrivalRate: 10 scenarios: - flow: - get: url: \u0026#34;/\u0026#34; 运行测试：\nartillery run your_test_config.yml 分析结果：\nArtillery 会输出实时测试结果，并生成详细报告。 典型场景： # Web 应用和 API 性能测试。 简单的压力和负载测试，支持高并发。 总结： # JMeter：功能强大，适用于复杂的负载测试，支持分布式测试。 Locust：基于 Python，适合高并发分布式测试。 ab：简单易用，适合基础的压力测试。 Siege：命令行工具，适用于快速的压力测试。 Gatling：高性能，适用于高并发的应用程序测试。 Artillery：现代的负载测试工具，适用于简单和高效的负载测试。 选择工具时，根据你的具体需求（如并发数、分布式支持、脚本编写难易度等）来选择最合适的压力测试工具。\n19. curl命令如何发送https请求？如何查看response头信息？如何发送get和post表单信息？ # 发送https请求：curl \u0026ndash;tlsv1 ‘https://www.bitstamp.net/api/v2/transactions/btcusd/’ response头信息 ：curl -I get：curl 请求地址?key1=value1\u0026amp;key2=value2\u0026amp;key3=value3 post： curl -d “key1=value1\u0026amp;key2=value2\u0026amp;key3=value3” curl 是一个非常常用的命令行工具，支持多种协议，包括 HTTP 和 HTTPS。你可以使用 curl 发送 HTTPS 请求、查看响应头信息，并发送 GET 或 POST 请求。下面是一些常见用法：\n1. 发送 HTTPS 请求 # 要发送一个简单的 HTTPS 请求，只需要指定 URL，并且 curl 会自动使用 HTTPS 协议。\ncurl https://www.example.com 2. 查看响应头信息 # 如果你只想查看响应头，而不关心响应体，可以使用 -I（大写字母 i）选项。这样 curl 会发送一个 HEAD 请求，只返回响应头。\ncurl -I https://www.example.com 如果你想同时查看响应头和响应体，可以使用 -i（小写字母 i）选项：\ncurl -i https://www.example.com 3. 发送 GET 请求 # curl 默认使用 GET 方法，如果你只需要发送一个 GET 请求，可以直接写 URL：\ncurl https://www.example.com?key=value\u0026amp;anotherkey=anothervalue 如果你希望更加明确地指定使用 GET 请求，可以使用 -X 选项（尽管 curl 默认是 GET）：\ncurl -X GET https://www.example.com?key=value 4. 发送 POST 请求 # 发送 POST 请求时，你需要使用 -X POST 来指定请求方法为 POST，并且用 -d 来传递请求的表单数据。\n发送表单数据（x-www-form-urlencoded） # curl -X POST https://www.example.com -d \u0026#34;key1=value1\u0026amp;key2=value2\u0026#34; 或者使用 --data（等同于 -d）：\ncurl --data \u0026#34;key1=value1\u0026amp;key2=value2\u0026#34; https://www.example.com 发送 POST 请求并传递 JSON 数据 # 如果你需要发送 JSON 格式的数据，可以设置 Content-Type 头部为 application/json，并传递 JSON 数据：\ncurl -X POST https://www.example.com \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;key1\u0026#34;: \u0026#34;value1\u0026#34;, \u0026#34;key2\u0026#34;: \u0026#34;value2\u0026#34;}\u0026#39; 发送带文件的 POST 请求 # 如果你需要发送文件，可以使用 -F 选项来模拟表单文件上传：\ncurl -X POST https://www.example.com/upload -F \u0026#34;file=@/path/to/file\u0026#34; 发送带 Cookie 的 POST 请求 # 如果你需要在 POST 请求中携带 Cookie，可以使用 -b（或者 --cookie）选项：\ncurl -X POST https://www.example.com \\ -b \u0026#34;name=value\u0026#34; \\ -d \u0026#34;key1=value1\u0026amp;key2=value2\u0026#34; 5. 发送带自定义头的请求 # 你可以使用 -H 选项来设置自定义的请求头。例如，设置 User-Agent 和 Authorization 头：\ncurl -X POST https://www.example.com \\ -H \u0026#34;User-Agent: MyCustomAgent\u0026#34; \\ -H \u0026#34;Authorization: Bearer your_token_here\u0026#34; \\ -d \u0026#34;key1=value1\u0026amp;key2=value2\u0026#34; 总结 # 发送 HTTPS 请求：curl https://example.com 查看响应头：curl -I https://example.com 查看响应头和响应体：curl -i https://example.com 发送 GET 请求：curl https://example.com?key=value 发送 POST 请求（表单数据）：curl -X POST https://example.com -d \u0026quot;key=value\u0026quot; 发送 JSON 数据的 POST 请求：curl -X POST https://example.com -H \u0026quot;Content-Type: application/json\u0026quot; -d '{\u0026quot;key\u0026quot;: \u0026quot;value\u0026quot;}' 发送文件的 POST 请求：curl -X POST https://example.com -F \u0026quot;file=@/path/to/file\u0026quot; 携带 Cookie 的 POST 请求：curl -X POST https://example.com -b \u0026quot;name=value\u0026quot; -d \u0026quot;key=value\u0026quot; 通过这些基本命令，你可以使用 curl 进行各种类型的 HTTP/HTTPS 请求，并控制请求和响应的各个方面。\n二、mysql # 1. 索引的为什么使查询加快？有啥缺点？ # 默认的方式是根据搜索条件进行全表扫描，遇到匹配条件的就加入搜索结果集合。如果我们对某一字段增加索引，查询时就会先去索引列表中一次定位到特定值的行数，大大减少遍历匹配的行数，所以能明显增加查询的速度 缺点：\n创建索引和维护索引要耗费时间，这种时间随着数据量的增加而增加 索引需要占物理空间，除了数据表占用数据空间之外，每一个索引还要占用一定的物理空间，如果需要建立聚簇索引，那么需要占用的空间会更大 以表中的数据进行增、删、改的时候，索引也要动态的维护，这就降低了整数的维护速度 索引 是数据库管理系统（DBMS）中的一种数据结构，用于加速数据的检索过程。索引通过在表中创建一个排序的数据结构，使得数据库可以更高效地查找数据。简单来说，索引就像书籍的目录，它帮助我们快速找到目标信息，而不需要从头到尾逐一查找。\n为什么索引能加快查询？ # 减少全表扫描： 如果没有索引，数据库查询通常需要对整个表进行全表扫描（扫描每一行）。对于大表，这种方式非常慢。 索引通过提供一个有序的数据结构（如 B+ 树），使得数据库可以通过二分查找或者树形查找等高效算法快速定位到目标数据，而不需要扫描整个表。 提高检索效率： 在没有索引时，查找某个值的时间复杂度是 O(n)，即需要逐行扫描。 有了索引后，查找时间复杂度降低为 O(log n)（例如 B+ 树），可以大大提升查询速度。 减少磁盘 I/O： 索引通常存储在磁盘上，但它比整个数据表小，因此查找一个值时，只需要读取少量的磁盘数据。 在磁盘和内存之间的 I/O 操作是数据库操作中的瓶颈，使用索引可以显著减少这种 I/O 操作，提升性能。 加速排序和聚合操作： 索引不仅能加速查询，还能加速排序（ORDER BY）和聚合（GROUP BY）操作。 如果查询涉及到排序和分组，索引可以直接提供有序的数据，避免了额外的排序操作。 索引的缺点和限制 # 虽然索引在提高查询性能方面有显著的优势，但它也存在一些缺点和限制：\n占用存储空间： 每个索引都会消耗一定的存储空间，特别是在有大量索引的情况下。对于大表，多个索引的存储开销可能非常大，尤其是当表的数据量不断增加时。 索引本身也是需要存储在磁盘上的，如果不合理使用索引，可能会导致磁盘空间的浪费。 降低写入性能： 每当向表中插入、更新或删除数据时，相关的索引也需要更新。因此，写操作（如 INSERT、UPDATE、DELETE）的性能会受到影响，尤其是在有多个索引的情况下。 例如，插入新记录时，不仅要将数据插入到表中，还要将该数据的索引条目添加到相应的索引中。更新和删除操作也同样会导致索引的重新调整。 索引的选择和维护成本： 在大型数据库系统中，选择哪些列建立索引、使用哪种类型的索引（例如 B+ 树、哈希索引等）需要根据具体查询的情况进行评估。这需要一定的经验和知识。 随着数据量的增长，索引的维护也变得更加复杂。例如，某些索引可能变得过时或不再有效，可能需要重新设计或调整。 影响查询优化器的选择： 在某些情况下，过多的索引可能会影响查询优化器的决策。查询优化器可能选择一个较差的索引路径，导致查询性能下降。 在有多个索引的情况下，数据库可能需要评估哪些索引最优，但优化器的选择并不总是完美的，可能会选择不适合当前查询的索引。 索引的更新和重建： 当表的数据发生变化时，索引会被更新，随着时间的推移，索引可能会变得不平衡或碎片化，影响查询性能。 在某些情况下，可能需要定期重建索引或重新组织索引，以保持查询性能。 总结： # 优点： 加速查询：通过减少全表扫描，使得数据检索更加高效。 提高排序和聚合操作：索引可以加速排序（ORDER BY）和聚合（GROUP BY）操作。 减少磁盘 I/O：减少查询过程中的磁盘读取，提高查询效率。 缺点： 占用存储空间：索引占用额外的磁盘空间。 降低写入性能：每次写操作都需要更新相关的索引。 索引维护成本：需要定期更新和优化索引，维护复杂。 不当索引选择：可能导致查询优化器选择错误的索引，影响查询性能。 合理地使用索引，可以显著提升查询性能。但也要平衡索引带来的开销，根据具体业务需求和查询特点，选择合适的索引类型和数量。\n2. sql语句左外连接 右外连接 内连接 全连接区别 # 在 SQL 中，JOIN 用于从多个表中查询数据，LEFT JOIN、RIGHT JOIN、INNER JOIN 和 FULL JOIN 是常见的几种连接类型，它们用于指定表之间数据的连接方式。每种连接类型的行为有所不同，以下是它们的详细区别：\n1. 内连接 (INNER JOIN) # 定义：INNER JOIN 返回的是两个表中满足连接条件的交集，即只返回两个表中都有的匹配数据。\n行为：如果两个表中有相同的匹配记录，则返回这些记录。对于没有匹配的记录，INNER JOIN 不会返回。 示例： # SELECT * FROM employees e INNER JOIN departments d ON e.department_id = d.department_id; 这个查询将返回所有有匹配的 employees 和 departments 的记录。\n特点： # 只返回两个表中匹配的记录。 如果一个表中没有匹配，数据行就不会出现在结果中。 2. 左外连接 (LEFT JOIN 或 LEFT OUTER JOIN) # 定义：LEFT JOIN 返回的是左表（LEFT）中的所有记录以及右表（RIGHT）中与左表匹配的记录。如果右表没有匹配的记录，右表的列将包含 NULL。\n行为：返回左表中的所有行，即使右表没有与左表匹配的行。对于没有匹配的右表行，将会显示 NULL。 示例： # SELECT * FROM employees e LEFT JOIN departments d ON e.department_id = d.department_id; 这个查询将返回 employees 表中的所有记录，即使某些员工没有对应的 department_id（右表为空时会返回 NULL）。\n特点： # 返回左表（LEFT）中的所有记录。 如果右表没有匹配，右表的值将为 NULL。 3. 右外连接 (RIGHT JOIN 或 RIGHT OUTER JOIN) # 定义：RIGHT JOIN 返回的是右表（RIGHT）中的所有记录以及左表（LEFT）中与右表匹配的记录。如果左表没有匹配的记录，左表的列将包含 NULL。\n行为：返回右表中的所有行，即使左表没有与右表匹配的行。对于没有匹配的左表行，将会显示 NULL。 示例： # SELECT * FROM employees e RIGHT JOIN departments d ON e.department_id = d.department_id; 这个查询将返回 departments 表中的所有记录，即使某些部门没有员工（左表为空时会返回 NULL）。\n特点： # 返回右表（RIGHT）中的所有记录。 如果左表没有匹配，左表的值将为 NULL。 4. 全外连接 (FULL JOIN 或 FULL OUTER JOIN) # 定义：FULL JOIN 返回的是两个表中的所有记录。如果某个表中没有与另一个表匹配的行，缺少的部分将用 NULL 填充。\n行为：返回两个表中的所有行。如果某个表中没有匹配，另一个表的列会显示 NULL。 示例： # SELECT * FROM employees e FULL JOIN departments d ON e.department_id = d.department_id; 这个查询将返回 employees 和 departments 表中的所有记录。如果一个表中的某些记录在另一个表中没有匹配，则会填充 NULL。\n特点： # 返回两个表中的所有记录。 如果某个表没有匹配，结果中的缺失部分用 NULL 填充。 总结： # 连接类型 返回记录 含义 INNER JOIN 只返回两个表中匹配的记录。 只返回两个表中满足连接条件的记录。 LEFT JOIN 返回左表的所有记录，以及右表中匹配的记录。右表没有匹配时返回 NULL。 返回左表的所有记录，并且如果右表没有匹配，右表的值为 NULL。 RIGHT JOIN 返回右表的所有记录，以及左表中匹配的记录。左表没有匹配时返回 NULL。 返回右表的所有记录，并且如果左表没有匹配，左表的值为 NULL。 FULL JOIN 返回两个表的所有记录。若某个表没有匹配，缺失部分为 NULL。 返回两个表中的所有记录，若某个表没有匹配，用 NULL 填充。 举例说明： # 假设有两张表：employees 和 departments。\nemployees 表：\nemployee_id name department_id 1 Alice 101 2 Bob 102 3 Charlie NULL departments 表：\ndepartment_id department_name 101 HR 102 IT 103 Finance INNER JOIN 结果： 只会返回 department_id 为 101 和 102 的员工，即 Alice 和 Bob。 LEFT JOIN 结果： 返回所有员工，Bob 和 Alice 会显示部门名称，Charlie 会显示 NULL 部门。 RIGHT JOIN 结果： 返回所有部门，HR 和 IT 部门对应的员工会显示，Finance 部门没有员工，所以员工列为 NULL。 FULL JOIN 结果： 返回所有员工和所有部门，无论是否匹配。没有匹配的部门或员工都会显示 NULL。 通过选择适合的连接类型，可以根据实际需求灵活地获取所需的数据。\n3. mysql数据备份方式，如何恢复？你们的备份策略是什么？ # 物理完全备份 备份所有数据库文件：/var/lib/mysql/* 备份所有binlog文件: /var/lib/mysql/mysql-bin.* 备份选项文件: /etc/my.cnf\nmysqldump逻辑备份 mysqldump -uroot -p \u0026ndash;all-databases \u0026gt; /backup/mysqldump/all.db\n物理备份恢复 mv /var/lib/mysql /var/lib/mysql.old #先把原来的数据目录改名 cp -a /backups/mysql /var/lib\n逻辑备份数据恢复 mysql \u0026gt; use db_name mysql \u0026gt; source /backup/mysqldump/db_name.db\n在 MySQL 中，数据备份是保障数据安全和防止数据丢失的重要措施。备份方式有多种，每种备份方式在不同场景下有不同的优劣。下面是常见的 MySQL 数据备份方式和恢复方法，并提供一份备份策略的参考。\n常见的 MySQL 数据备份方式 # 逻辑备份（Logical Backup）\n方式：通过 SQL 脚本导出数据库的表结构和数据。\n工具：mysqldump\n优点\n：\n备份文件是纯文本，可以跨平台恢复。 适用于小型数据库、迁移数据库到不同平台。 缺点\n：\n备份速度较慢，尤其对于大规模数据表，备份文件会很大。 需要更多的存储空间来存放备份文件。 如何备份 # mysqldump -u username -p database_name \u0026gt; backup.sql 如何恢复 # mysql -u username -p database_name \u0026lt; backup.sql 物理备份（Physical Backup）\n方式：直接复制数据库数据文件，通常通过 xtrabackup 或文件系统的备份。\n工具：Percona XtraBackup, cp 或 rsync（在停机或只读模式下）\n优点\n：\n备份速度快，适合大规模数据备份。 恢复速度快，直接复制数据文件即可恢复。 缺点\n：\n需要确保备份期间没有修改数据，通常会要求停机或者锁定表。 备份文件是二进制文件，跨平台恢复较麻烦。 如何备份 # 使用 Percona XtraBackup：\ninnobackupex --user=username --password=password /path/to/backup 如何恢复 # innobackupex --apply-log /path/to/backup innobackupex --copy-back /path/to/backup 增量备份（Incremental Backup）\n方式：只备份自上次备份以来发生变化的数据。\n工具：Percona XtraBackup（支持增量备份）。\n优点\n：\n备份速度比全量备份快，节省存储空间。 适用于需要频繁备份和恢复的场景。 缺点\n：\n恢复时需要先恢复最后的全量备份，然后再恢复所有增量备份。 增量备份需要管理更多的备份文件，增加了恢复复杂度。 如何备份 # 使用 Percona XtraBackup：\ninnobackupex --user=username --password=password --incremental /path/to/incremental_backup --incremental-basedir /path/to/previous_backup 如何恢复 # 首先恢复全量备份，然后按顺序恢复增量备份：\ninnobackupex --apply-log /path/to/full_backup innobackupex --apply-log --incremental-dir /path/to/incremental_backup /path/to/full_backup 二进制日志备份（Binary Log Backup）\n方式：备份 MySQL 的二进制日志，以便在全量备份后，捕获和恢复增量变更。\n工具：mysqlbinlog\n优点\n：\n可以记录所有的数据修改操作，用于恢复和点时间恢复（PITR）。 支持备份期间的数据变化捕获。 缺点\n：\n需要定期备份和归档二进制日志。 恢复过程相对复杂。 如何备份 # 在 MySQL 配置文件中启用二进制日志：\n[mysqld] log-bin=mysql-bin 获取二进制日志：\nmysqlbinlog /var/lib/mysql/mysql-bin.000001 \u0026gt; binary_log.sql 如何恢复 # 将二进制日志应用到备份数据库中：\nmysqlbinlog /path/to/binary_log | mysql -u username -p database_name 恢复的常见方式 # 恢复的方式与备份方式相关。常见的恢复方式有以下几种：\n从逻辑备份恢复：直接执行 .sql 文件恢复。 从物理备份恢复：将备份文件复制回数据库的相关目录，并确保权限正确。 从增量备份恢复：先恢复全量备份，再应用所有增量备份。 从二进制日志恢复：将二进制日志文件中的操作应用到备份数据中，恢复到特定时间点。 MySQL 备份策略 # 一个良好的备份策略应该根据业务的需求、数据量和恢复要求来制定，以下是一个常见的备份策略：\n全量备份： 每周进行一次全量备份（比如每周日进行）。这确保了我们可以快速恢复到一个较为完整的状态。 增量备份： 每天进行增量备份，备份自上次全量或增量备份以来变化的数据。增量备份可以减少备份数据的大小和备份时间。 二进制日志备份： 开启二进制日志，并每小时或每次应用更新时备份日志。这样可以保证在全量和增量备份之间的变更也能被备份，并在需要时进行点时间恢复（PITR）。 备份存储： 将备份存储在不同的物理位置（如备份到云端，或使用本地和远程存储）。 保持一定数量的备份副本（如保留过去 7 天的备份）。 使用加密来保护备份数据的安全性。 定期验证备份： 定期执行恢复演练，确保备份文件有效且恢复过程可行。 备份保留策略： 保留一定时间内的备份（如每月的全量备份保留一个月，日常增量备份保留一周）。 自动化备份： 使用脚本或备份工具（如 cron、Percona XtraBackup 等）自动化备份过程，减少人为干预。 总结： # 备份方式： 逻辑备份（mysqldump）适用于小型数据库或跨平台迁移。 物理备份（Percona XtraBackup）适用于大规模数据库，恢复速度快。 增量备份适用于大数据量并且需要频繁备份的场景。 二进制日志备份用于捕获和恢复数据库的变更操作。 恢复方式： 根据备份方式，恢复流程也会有所不同。通常包括从备份文件恢复或应用增量备份、二进制日志恢复等。 备份策略： 定期全量备份+增量备份+二进制日志备份，并确保备份存储的安全性、可恢复性和备份文件的有效性。 4. 如何配置数据库主从同步，实际工作中是否遇到数据不一致问题？如何解决？ # 为每个服务器配置唯一值的server-id\n主库 开启binlog日志 创建主从复制用户 查看master的状态\n从库 change master to设置主库信息 start slave开始复制\n数据库主从同步配置 # 在 MySQL 中，主从同步（Master-Slave Replication）是一种常见的数据库复制方式，用于实现数据的高可用性和负载均衡。配置 MySQL 主从同步通常涉及以下几个步骤：\n1. 配置主服务器（Master） # 首先，确保主服务器配置为允许复制操作，并创建一个专门的复制用户。\n步骤 1: 配置主服务器的 my.cnf 配置文件 # 编辑主服务器上的 my.cnf（或 my.ini，视操作系统而定）配置文件，启用二进制日志（binlog）并配置唯一的服务器 ID。\n[mysqld] server-id = 1 log-bin = mysql-bin binlog-do-db = your_database_name # 可选，只复制指定的数据库 server-id：每个 MySQL 实例必须有唯一的 server-id，且主服务器和从服务器的 server-id 必须不同。 log-bin：启用二进制日志，这样从服务器可以获取主服务器上的所有变更事件。 binlog-do-db：如果只希望同步某个特定数据库的数据，可以在这里配置。 步骤 2: 创建复制用户 # 登录到主服务器的 MySQL，创建一个用于复制的专用用户，并授权。\nCREATE USER \u0026#39;replica_user\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;password\u0026#39;; GRANT REPLICATION SLAVE ON *.* TO \u0026#39;replica_user\u0026#39;@\u0026#39;%\u0026#39;; FLUSH PRIVILEGES; replica_user：复制用户，% 表示允许所有 IP 连接。 REPLICATION SLAVE 权限：允许从服务器读取二进制日志。 步骤 3: 获取主服务器的二进制日志位置 # 在配置主从同步之前，需要获取主服务器的当前二进制日志文件名和位置。执行以下命令：\nSHOW MASTER STATUS; 输出类似于：\n+------------------+----------+--------------+------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | +------------------+----------+--------------+------------------+ | mysql-bin.000001 | 1234 | your_database_name | | +------------------+----------+--------------+------------------+ 记录下 File 和 Position，稍后需要在从服务器配置时使用。\n2. 配置从服务器（Slave） # 接下来，配置从服务器使其能够接收并执行主服务器的更新。\n步骤 1: 配置从服务器的 my.cnf 配置文件 # 编辑从服务器的 my.cnf 配置文件，设置唯一的 server-id。\n[mysqld] server-id = 2 # 从服务器的 ID 必须与主服务器不同 步骤 2: 启动从服务器复制 # 登录到从服务器的 MySQL，执行以下命令配置复制信息：\nCHANGE MASTER TO MASTER_HOST = \u0026#39;master_ip\u0026#39;, -- 主服务器的 IP 地址 MASTER_USER = \u0026#39;replica_user\u0026#39;, -- 复制用户 MASTER_PASSWORD = \u0026#39;password\u0026#39;, -- 复制用户密码 MASTER_LOG_FILE = \u0026#39;mysql-bin.000001\u0026#39;, -- 主服务器的二进制日志文件名 MASTER_LOG_POS = 1234; -- 主服务器的二进制日志位置 START SLAVE; MASTER_HOST：主服务器的 IP 地址。 MASTER_USER 和 MASTER_PASSWORD：复制用户和密码。 MASTER_LOG_FILE 和 MASTER_LOG_POS：主服务器的二进制日志文件名和位置（通过 SHOW MASTER STATUS 获取）。 步骤 3: 检查复制状态 # 使用以下命令查看从服务器的复制状态，确保没有错误：\nSHOW SLAVE STATUS\\G 你应该看到如下信息：\nSlave_IO_Running: Yes Slave_SQL_Running: Yes Slave_IO_Running：表示从服务器是否能从主服务器读取二进制日志。 Slave_SQL_Running：表示从服务器是否能执行读取到的 SQL 语句。 如果都为 Yes，说明主从同步已经正常工作。\n3. 验证同步是否正常 # 插入测试数据：在主服务器上插入数据，检查从服务器是否同步。\nINSERT INTO your_table (column1, column2) VALUES (\u0026#39;value1\u0026#39;, \u0026#39;value2\u0026#39;); 在从服务器查询：在从服务器上检查该数据是否已同步。\nSELECT * FROM your_table; 如果数据同步正常，那么配置成功。\n实际工作中遇到的数据不一致问题 # 在实际工作中，主从同步通常能工作良好，但也可能会遇到以下常见的 数据不一致问题：\n1. 延迟（Replication Lag） # 问题：从服务器延迟同步主服务器上的数据，导致从服务器的数据滞后。\n原因\n：\n主服务器的写入频繁，二进制日志增长过快。 从服务器的性能较差，无法及时执行来自主服务器的更新。 网络延迟或主从服务器的硬件瓶颈。 解决方案\n：\n优化从服务器性能，增加硬件资源（CPU、内存、磁盘 IO）。 调整 MySQL 配置，增加 slave_parallel_workers 参数并启用并行复制（适用于 MySQL 5.7 及以上版本）。 使用 pt-heartbeat 等工具来监控延迟并优化。 监控复制延迟并根据需要进行警告。 2. 数据丢失（Data Loss） # 问题：主服务器发生故障，导致主服务器的数据丢失，但从服务器数据尚未同步。\n原因\n：\n主服务器停机或崩溃时，未及时备份或持久化数据。 从服务器的复制延迟过大，主服务器的数据还未同步到从服务器。 解决方案\n：\n启用 MySQL 的 GTID（全局事务标识符）或 binlog，以确保事务的完整性。 定期进行备份，确保有最新的数据库备份。 使用双主或多主复制架构来减少单点故障的风险。 3. 复制失败（Replication Errors） # 问题：复制过程中出现错误，导致从服务器无法同步主服务器的数据。\n原因\n：\n主服务器上执行了从服务器无法理解或执行的 SQL 语句（例如，删除字段、数据类型变化）。 网络或连接问题导致复制中断。 解决方案\n：\n使用 SHOW SLAVE STATUS 查看错误信息，识别并解决复制失败的原因。 可以通过执行 STOP SLAVE;，然后使用 START SLAVE UNTIL SQL_AFTER_GTIDS='gtid_set' 指令来跳过错误的事务，确保复制继续。 使用 pt-table-sync 等工具进行数据对齐，确保主从数据一致。 4. 不一致的数据库结构（Schema Divergence） # 问题：主从服务器的数据库结构（表结构、索引等）不一致，导致从服务器复制失败。\n原因\n：\n主服务器上的数据库结构发生了变化（例如添加了新表、修改了字段），但未同步到从服务器。 解决方案\n：\n确保主从数据库的结构一致，可以使用工具（如 pt-table-checksum）来校验主从数据库结构是否一致。 在执行结构变更时，确保主从服务器一起变更。 总结 # 主从同步配置：配置主从同步需要在主服务器和从服务器上分别做配置，确保二进制日志启用并且设置复制用户。通过 CHANGE MASTER TO 和 START SLAVE 命令在从服务器配置主服务器的信息。 数据不一致问题：常见问题包括复制延迟、数据丢失、复制失败和数据库结构不一致等。解决这些问题的办法包括优化硬件、调整配置、使用工具进行同步检查，并且确保主从数据库结构一致。 解决方案：监控复制状态，定期备份，优化性能，使用并行复制和工具来确保主从同步的稳定性。 通过正确配置和监控，能够保证 MySQL 主从同步的可靠性和高效性。\n5. mysql约束有哪些？ # 非空约束 唯一约束 主键约束 外键约束 MySQL 中的 约束（Constraints） 是用来确保数据库中的数据符合特定规则的机制。约束可以用于保证数据的完整性、准确性和一致性。MySQL 提供了以下几种常见的约束：\n1. 主键约束（PRIMARY KEY） # 定义：主键约束用于唯一标识表中的每一行记录。一个表只能有一个主键。\n特点\n：\n主键列中的值必须是唯一的。 主键列不能包含 NULL 值。 MySQL 会自动为主键列创建索引。 示例： # CREATE TABLE users ( id INT PRIMARY KEY, username VARCHAR(50) ); 2. 唯一约束（UNIQUE） # 定义：唯一约束确保列中的所有值都是唯一的，但与主键不同，唯一约束允许有 NULL 值，且 NULL 值可以重复。\n特点\n：\n一列或多列组合起来定义唯一性。 允许多个 NULL 值。 示例： # CREATE TABLE users ( id INT PRIMARY KEY, email VARCHAR(100) UNIQUE ); 3. 外键约束（FOREIGN KEY） # 定义：外键约束用于确保一个表中的列值必须存在于另一个表中的列中。外键约束用于实现表之间的关系（如一对多、多对多关系）。\n特点\n：\n外键列的值必须在参照表的主键或唯一键列中。 外键约束可以设置删除和更新时的操作（如 CASCADE、SET NULL、NO ACTION 等）。 示例： # CREATE TABLE orders ( order_id INT PRIMARY KEY, user_id INT, FOREIGN KEY (user_id) REFERENCES users(id) ); 4. 检查约束（CHECK） # 定义：检查约束用于确保列中的数据满足特定的条件或规则。\n特点\n：\n在插入或更新数据时，MySQL 会检查是否满足 CHECK 条件。 该约束通常用于保证数据的合理性，如限制某个字段的取值范围。 示例： # CREATE TABLE products ( product_id INT PRIMARY KEY, price DECIMAL(10, 2) CHECK (price \u0026gt;= 0) ); 注意：CHECK 约束从 MySQL 8.0.16 版本开始才被支持。在此之前，MySQL 并不强制执行 CHECK 约束。\n5. 非空约束（NOT NULL） # 定义：非空约束用于确保列中的值不能为 NULL。对于需要保证某一列有数据的字段，通常会使用该约束。\n特点\n：\n在插入或更新数据时，列值不能为 NULL。 示例： # CREATE TABLE users ( id INT PRIMARY KEY, username VARCHAR(50) NOT NULL ); 6. 默认值约束（DEFAULT） # 定义：默认值约束用于在插入数据时，如果没有指定某个列的值，则该列会自动使用预设的默认值。\n特点\n：\n可以为列指定一个默认值。 如果插入数据时没有显式给某个列赋值，那么该列会使用默认值。 示例： # CREATE TABLE employees ( employee_id INT PRIMARY KEY, hire_date DATE DEFAULT CURRENT_DATE ); 7. 自动递增约束（AUTO_INCREMENT） # 定义：自动递增约束通常用于主键列，MySQL 会自动为每次插入的新记录生成一个唯一的数字值。\n特点\n：\n用于整型字段，MySQL 会自动为每次插入的记录生成一个唯一的增量值。 每次插入记录时，该字段的值会自动增加，无需手动赋值。 示例： # CREATE TABLE employees ( employee_id INT AUTO_INCREMENT PRIMARY KEY, name VARCHAR(100) ); 8. 索引约束（INDEX） # 定义：索引约束用于加速数据查询的速度。它通常用于表中经常查询的列上。\n特点\n：\n索引会提高数据检索速度，但会稍微降低插入和更新操作的性能。 你可以显式为某列创建索引，也可以在主键、唯一键等约束上自动创建索引。 示例： # CREATE TABLE users ( id INT PRIMARY KEY, username VARCHAR(50), INDEX idx_username (username) ); 9. 联合唯一约束（UNIQUE CONSTRAINT） # 定义：通过组合多列来创建唯一约束，以确保这些列的组合在表中是唯一的。\n特点\n：\n允许列中有 NULL 值，但 NULL 值也可以是唯一的（这取决于数据库引擎）。 可以确保某些列组合的唯一性。 示例： # CREATE TABLE employees ( first_name VARCHAR(50), last_name VARCHAR(50), email VARCHAR(100), UNIQUE (first_name, last_name) ); 总结 # 主键约束（PRIMARY KEY）：确保列的唯一性且不允许 NULL 值。 唯一约束（UNIQUE）：确保列的唯一性，允许 NULL 值。 外键约束（FOREIGN KEY）：确保数据表间的关系完整性，引用另一个表的主键。 检查约束（CHECK）：限制列的值符合特定条件。 非空约束（NOT NULL）：确保列的值不能为 NULL。 默认值约束（DEFAULT）：为列提供默认值。 自动递增约束（AUTO_INCREMENT）：为每条记录自动生成唯一的整数值。 索引约束（INDEX）：加速查询操作，提高查询性能。 联合唯一约束（UNIQUE CONSTRAINT）：确保多列的组合值唯一。 这些约束确保了数据库中数据的完整性、准确性和一致性，帮助实现更高效、可维护的数据库管理。\n6. 二进制日志（binlog）用途？ # BINLOG记录数据库的变更过程。例如创建数据库、建表、修改表等DDL操作、以及数据表的相关DML操作，这些操作会导致数据库产生变化，开启binlog以后导致数据库产生变化的操作会按照时间顺序以“事件”的形式记录到binlog二进制文件中\n二进制日志（binlog） 是 MySQL 中用于记录所有更改数据库状态的操作日志（包括数据表结构变化和数据修改操作）。它是 MySQL 数据库的核心日志之一，具有重要的用途。\n主要用途： # 数据复制（Replication） 主从复制：二进制日志是 MySQL 主从复制的基础。主服务器将所有更改数据的操作记录到 binlog 中，从服务器读取这些日志并执行相同的操作，保持与主服务器数据的一致性。 复制通常使用异步方式，binlog 记录了所有修改数据的操作，通过复制进程将这些操作传输到从服务器。 数据恢复（Point-in-time Recovery） 时间点恢复：通过使用 binlog，您可以将数据库恢复到某个特定的时间点。例如，数据库在某个时刻被误操作，您可以利用 binlog 恢复到错误发生之前的状态。 备份时，通常先做全量备份，然后利用 binlog 来恢复自备份以来的所有变动数据。 审计与监控（Auditing and Monitoring） 操作追踪：通过分析 binlog，可以追踪到数据库执行的所有写操作，如 INSERT、UPDATE、DELETE 等。它是进行审计的一个重要手段。 通过定期查看 binlog，您可以监控哪些操作被执行，哪些数据被修改。 数据库优化（Optimization） 查询优化：有时候，分析 binlog 可以帮助数据库管理员识别数据库中可能存在的性能瓶颈，例如频繁的写操作或重复的事务。 binlog 格式： # MySQL 提供了几种不同的 binlog 格式，分别是：\nSTATEMENT（语句模式） 记录的是 SQL 语句本身，例如 UPDATE table SET col = val。 优点：占用空间小，因为只记录 SQL 语句。 缺点：某些操作可能无法完全复制，特别是涉及到非确定性函数的情况（如 NOW()、RAND() 等）。 ROW（行模式） 记录的是每一行数据的变化。例如，更新某一行的数据时，会记录该行的旧值和新值。 优点：能够确保复制的准确性，尤其在有非确定性查询的情况下。 缺点：日志量较大，因为每个修改操作都需要记录每一行的变化。 MIXED（混合模式） 结合了 STATEMENT 和 ROW 模式，MySQL 会自动选择适合的模式来记录操作。 优点：在保证准确性的同时，尽量减少日志的空间消耗。 启用与配置 binlog： # 为了使用 binlog，需要在 MySQL 配置文件（通常是 my.cnf 或 my.ini）中进行配置：\n[mysqld] log-bin=mysql-bin # 启用 binlog binlog-format=ROW # 设置 binlog 格式（可选） 总结： # 数据复制：binlog 用于主从复制，确保主服务器和从服务器数据同步。 数据恢复：提供基于时间点的恢复能力。 审计与监控：帮助监控数据变更，进行操作审计。 数据库优化：通过分析 binlog，发现潜在的性能瓶颈。 通过合理利用 binlog，能够提高 MySQL 数据库的可靠性、可维护性以及性能优化。\n7. mysql数据引擎有哪些？ # 常用的 myisam、innodb 区别： InnoDB 支持事务，MyISAM 不支持，这一点是非常之重要。事务是一种高级的处理方式，如在一些列增删改中只要哪个出错还可以回滚还原，而 MyISAM就不可以了； MyISAM 适合查询以及插入为主的应用，InnoDB 适合频繁修改以及涉及到安全性较高的应用； InnoDB 支持外键，MyISAM 不支持； MyISAM 是默认引擎，InnoDB 需要指定； InnoDB 不支持 FULLTEXT 类型的索引； InnoDB 中不保存表的行数，如 select count() from table 时，InnoDB；需要扫描一遍整个表来计算有多少行，但是 MyISAM 只要简单的读出保存好的行数即可。注意的是，当 count()语句包含 where 条件时 MyISAM 也需要扫描整个表； 对于自增长的字段，InnoDB 中必须包含只有该字段的索引，但是在 MyISAM表中可以和其他字段一起建立联合索引； 清空整个表时，InnoDB 是一行一行的删除，效率非常慢。MyISAM 则会重建表； InnoDB 支持行锁（某些情况下还是锁整表，如 update table set a=1 where user like ‘%lee%’\nMySQL 支持多种存储引擎（Storage Engines），每种存储引擎具有不同的特性，适用于不同的场景。以下是 MySQL 常见的存储引擎及其简要介绍：\n1. InnoDB # 特点\n：\n事务支持：InnoDB 支持 ACID 事务，能够确保数据的原子性、一致性、隔离性和持久性。 外键支持：InnoDB 支持外键约束，用于维护表与表之间的关系。 行级锁：InnoDB 支持行级锁，以提高并发性能，适合高并发的读写操作。 崩溃恢复：通过日志文件进行崩溃恢复，保证数据的一致性。 自动增长主键：支持自动递增字段（如 AUTO_INCREMENT）。 事务日志：提供事务日志来记录对数据的更改，可以用于恢复数据。 适用场景：适用于需要支持事务、外键以及高并发的应用场景，如金融、电商、CRM 等。\n示例： # CREATE TABLE users ( id INT AUTO_INCREMENT PRIMARY KEY, name VARCHAR(50), age INT ) ENGINE=InnoDB; 2. MyISAM # 特点\n：\n表级锁：MyISAM 采用表级锁，不支持行级锁，适合读操作较多的场景，但在高并发写入时性能较差。 不支持事务：MyISAM 不支持事务管理，因此无法保证 ACID 特性。 快速读操作：MyISAM 对于读密集型应用表现较好，特别是对于复杂查询的优化。 存储方式：每个表由三个文件组成：数据文件、索引文件和表定义文件。 适用场景：适用于读操作频繁、对事务支持要求较低的场景，如日志记录、数据分析等。\n示例： # CREATE TABLE logs ( id INT AUTO_INCREMENT PRIMARY KEY, log_message TEXT ) ENGINE=MyISAM; 3. MEMORY（HEAP） # 特点\n：\n内存存储：所有数据都存储在内存中，速度非常快，适用于需要快速读写的场景。 表级锁：支持表级锁。 不支持事务和外键：不支持事务和外键约束。 数据丢失：由于数据存储在内存中，服务器重启时数据会丢失。 适用场景：适用于需要快速访问且对持久化要求不高的临时表、缓存等场景。\n示例： # CREATE TABLE temp_table ( id INT PRIMARY KEY, name VARCHAR(50) ) ENGINE=MEMORY; 4. CSV # 特点\n：\n文本格式存储：数据以 CSV（逗号分隔值）格式存储为文件。 不支持索引：CSV 引擎不支持索引，查询性能较差。 易于导入导出：数据以文本格式存储，适合用于与其他系统的导入导出。 适用场景：适用于需要将数据导出为 CSV 格式的场景，如数据交换、备份等。\n示例： # CREATE TABLE csv_table ( id INT PRIMARY KEY, name VARCHAR(50) ) ENGINE=CSV; 5. ARCHIVE # 特点\n：\n适合归档数据：ARCHIVE 引擎专为存储归档数据设计，适合需要存储大量历史数据的场景。 只支持 INSERT 和 SELECT：仅支持插入和查询操作，不支持更新、删除操作。 压缩存储：自动压缩数据以节省存储空间。 不支持索引：ARCHIVE 表没有索引，查询性能较差。 适用场景：适用于存储长期归档数据，且该数据大多不需要更新的场景。\n示例： # CREATE TABLE archive_table ( id INT PRIMARY KEY, data TEXT ) ENGINE=ARCHIVE; 6. NDB (Cluster) # 特点\n：\n分布式存储：NDB 存储引擎用于 MySQL Cluster，提供分布式数据库解决方案，能够支持高可用和高并发。 数据分片：数据可以在多台服务器上分布存储，提供水平扩展能力。 支持事务：NDB 支持事务，但性能开销较大。 高可用性：NDB 引擎的数据库集群通常是高可用的，能够容忍节点故障。 适用场景：适用于需要高可用、分布式存储、高并发访问的场景，如大规模在线应用、金融等。\n示例： # CREATE TABLE ndb_table ( id INT PRIMARY KEY, name VARCHAR(50) ) ENGINE=NDB; 7. BLACKHOLE # 特点\n：\n数据“黑洞”：数据写入后直接丢弃，不存储任何数据，适用于特定的复制场景。 不存储数据：所有插入操作都会被丢弃，但操作日志会被记录。 适用场景：用于主从复制中，作为从服务器接收主服务器数据，但不需要存储数据的场景。\n示例： # CREATE TABLE blackhole_table ( id INT PRIMARY KEY, name VARCHAR(50) ) ENGINE=BLACKHOLE; 8. TokuDB # 特点\n：\n面向大数据存储：TokuDB 是一个高效的存储引擎，适用于需要处理大规模数据的场景。 支持压缩和索引：TokuDB 提供高效的数据压缩，并支持高效的索引机制。 ACID 支持：支持事务，并具备较好的事务性能。 适用场景：适用于大数据量的存储和复杂查询的场景，如大规模分析和数据仓库。\n示例： # CREATE TABLE toku_table ( id INT PRIMARY KEY, name VARCHAR(50) ) ENGINE=TokuDB; 总结 # MySQL 提供了多种存储引擎，每种引擎在性能、支持的功能和适用场景上有所不同。常用的存储引擎包括：\nInnoDB：支持事务、外键、行级锁，适合高并发、高一致性的应用。 MyISAM：无事务支持，适合读密集型应用。 MEMORY：数据存储在内存中，适合临时表和快速存取。 ARCHIVE：适用于归档数据，数据压缩存储。 NDB：用于 MySQL Cluster，适合分布式、高可用场景。 BLACKHOLE：数据写入黑洞，通常用于复制。 TokuDB：适合大数据量存储，提供高效的压缩和索引。 选择合适的存储引擎要根据具体的业务需求、性能要求以及数据的特性来决定。\n8. 如何查询mysql数据库存放路径？ # mysql\u0026gt; show variables like \u0026#39;datadir%\u0026#39;; +---------------+------------------------+ | Variable_name | Value | +---------------+------------------------+ | datadir | /usr/local/mysql/data/ | +---------------+------------------------+ 1 row in set (0.00 sec) 要查询 MySQL 数据库的存放路径，可以通过以下几种方式：\n1. 查询 MySQL 数据目录配置 # MySQL 的数据文件存储路径可以通过查询 MySQL 的 datadir 配置项来获取。你可以使用以下 SQL 语句来查询：\nSHOW VARIABLES LIKE \u0026#39;datadir\u0026#39;; 这个命令会返回 datadir 配置项的值，表示 MySQL 数据库的存储目录路径。\n示例： # mysql\u0026gt; SHOW VARIABLES LIKE \u0026#39;datadir\u0026#39;; +---------------+-----------------------------------+ | Variable_name | Value | +---------------+-----------------------------------+ | datadir | /var/lib/mysql/ | +---------------+-----------------------------------+ 1 row in set (0.00 sec) 在这个例子中，MySQL 数据库的存储路径是 /var/lib/mysql/。\n2. 检查 MySQL 配置文件 # 你还可以通过查看 MySQL 配置文件来查找数据库存储路径。MySQL 配置文件通常位于 /etc/my.cnf 或 /etc/mysql/my.cnf，具体路径取决于你的操作系统和安装方式。\n打开配置文件并查找 datadir 配置项，类似这样：\n[mysqld] datadir = /var/lib/mysql 3. 查看操作系统文件系统 # 如果你不确定 MySQL 配置文件的位置，也可以检查操作系统中 MySQL 数据目录的位置，尤其是通过文件系统的查找：\n在 Linux 系统中，可以通过查找 mysql 进程的工作目录来找到数据存储路径：\nps aux | grep mysql 这条命令会显示 MySQL 服务的进程信息，在信息中可能会包含数据目录的路径。\n4. 通过 MySQL 启动命令检查 # 如果你知道 MySQL 是如何启动的，也可以通过启动命令查看 MySQL 的数据目录。MySQL 启动时会使用 --datadir 参数指定数据目录。\n例如，可以使用 ps 命令查找 MySQL 进程，并检查是否有 --datadir 参数：\nps aux | grep mysqld 如果 MySQL 启动时指定了 --datadir，它会显示数据目录的路径。\n总结 # 通过 SQL 查询：使用 SHOW VARIABLES LIKE 'datadir'; 获取 MySQL 数据目录。 查看 MySQL 配置文件：检查 /etc/my.cnf 或 /etc/mysql/my.cnf 中的 datadir 配置项。 操作系统级别查找：通过 ps 或其他系统命令查找 MySQL 进程的启动信息。 通常，datadir 默认是 /var/lib/mysql，但如果在安装时做了修改，数据目录路径也可能会有所不同。\n9. mysql数据库文件后缀名有哪些？用途什么？ # myisam .frm文件：保护表的定义 .myd：保存表的数据 .myi：表的索引文件\ninnodb .frm：保存表的定义 .ibd：表空间\nMySQL 数据库文件的后缀名通常与存储引擎相关，具体来说，不同的存储引擎会使用不同的文件格式来存储数据。下面是常见的 MySQL 数据库文件后缀名及其用途：\n1. .frm # 用途：frm 文件用于存储表的定义（即表的结构）。每个表都有一个对应的 .frm 文件，记录了表的列、索引、数据类型等元数据。 存储引擎：不论是 InnoDB 还是 MyISAM 存储引擎，都会为每个表创建一个 .frm 文件。 示例： # /var/lib/mysql/database_name/my_table.frm 2. .ibd # 用途：ibd 文件是 InnoDB 存储引擎使用的表空间文件，专门用来存储表的数据和索引。每个使用 InnoDB 存储引擎的表（如果启用了独立表空间模式）都会有一个 .ibd 文件，存储实际的数据和索引。 存储引擎：仅 InnoDB 存储引擎使用。 位置：在启用了独立表空间（innodb_file_per_table=1）的情况下，每个表会有一个独立的 .ibd 文件。 示例： # /var/lib/mysql/database_name/my_table.ibd 3. .myd # 用途：myd 文件是 MyISAM 存储引擎的数据文件，用来存储表中的数据。 存储引擎：仅 MyISAM 存储引擎使用。 位置：每个 MyISAM 表都会有一个 .myd 文件，用来存储实际的行数据。 示例： # /var/lib/mysql/database_name/my_table.myd 4. .myi # 用途：myi 文件是 MyISAM 存储引擎的索引文件，用来存储表的索引数据。 存储引擎：仅 MyISAM 存储引擎使用。 位置：每个 MyISAM 表都有一个 .myi 文件，用来存储该表的索引。 示例： # /var/lib/mysql/database_name/my_table.myi 5. .log（日志文件） # 用途\n：\nlog 文件通常是 MySQL 的日志文件，主要用于记录数据库的操作过程（如查询、事务等）。\n错误日志（error.log）：记录启动、运行和关闭时的错误信息。 查询日志（query.log）：记录所有 SQL 查询（如果启用）。 慢查询日志（slow_query.log）：记录执行时间较长的查询。 二进制日志（binlog）：记录数据库的更改操作，用于数据复制和数据恢复。 存储引擎：日志文件与存储引擎无关，而是 MySQL 的全局配置项。\n示例： # /var/log/mysql/mysql-bin.000001 # 二进制日志 /var/log/mysql/error.log # 错误日志 6. .ib_logfile # 用途\n：\nib_logfile 文件是\nInnoDB\n存储引擎的重做日志文件，用于事务日志的存储。InnoDB 采用两种日志：重做日志和撤销日志，这些日志文件用于崩溃恢复。\nib_logfile0 和 ib_logfile1：是 InnoDB 存储引擎的日志文件，记录事务的重做操作（Redo log）。 存储引擎：仅 InnoDB 存储引擎使用。\n示例： # /var/lib/mysql/ib_logfile0 /var/lib/mysql/ib_logfile1 7. .cnd # 用途：cnd 文件是 InnoDB 存储引擎的表空间的变化日志文件，记录数据库表空间的增删改操作。 存储引擎：仅 InnoDB 存储引擎使用。 示例： # /var/lib/mysql/database_name/your_table.cnd 8. .tmp # 用途：tmp 文件用于存储临时数据。在 MySQL 执行查询时，如果需要临时存储数据（例如在复杂的查询中使用临时表），MySQL 会使用 .tmp 文件。 存储引擎：与存储引擎无关，MySQL 在执行查询时可能会使用临时文件来存储中间数据。 示例： # /tmp/mysql_temp_file.tmp 9. .ibdata # 用途：ibdata 文件是 InnoDB 存储引擎的系统表空间文件，用于存储所有的表数据、索引和元数据（例如表的结构信息），如果没有启用独立表空间模式（innodb_file_per_table=0），则所有 InnoDB 表的数据都存储在这个文件中。 存储引擎：仅 InnoDB 存储引擎使用。 示例： # /var/lib/mysql/ibdata1 总结 # MySQL 中常见的数据库文件后缀名和用途如下：\n.frm：存储表的结构信息，适用于所有存储引擎。 .ibd：用于 InnoDB 存储引擎的表数据和索引文件。 .myd：用于 MyISAM 存储引擎的表数据文件。 .myi：用于 MyISAM 存储引擎的表索引文件。 .log：日志文件，如错误日志、查询日志、二进制日志等。 .ib_logfile：InnoDB 的重做日志文件。 .cnd：InnoDB 的表空间变化日志文件。 .tmp：用于临时存储的数据文件。 .ibdata：InnoDB 的系统表空间文件。 这些文件在 MySQL 中具有不同的作用，帮助管理和存储数据库的结构、数据、索引以及日志信息。\n10. 如何修改数据库用户的密码？ # mysql8之前 set password for 用户名@localhost = password(\u0026#39;新密码\u0026#39;); mysqladmin -u用户名 -p旧密码 password 新密码 update user set password=password(\u0026#39;123\u0026#39;) where user=\u0026#39;root\u0026#39; and host=\u0026#39;localhost\u0026#39;; mysql8之后 # mysql8初始对密码要求高，简单的字符串不让改。先改成:MyNewPass@123; alter user \u0026#39;root\u0026#39;@\u0026#39;localhost\u0026#39; identified by \u0026#39;MyNewPass@123\u0026#39;; # 降低密码难度 set global validate_password.policy=0; set global validate_password.length=4; # 修改成简易密码 alter user \u0026#39;root\u0026#39;@\u0026#39;localhost\u0026#39;IDENTIFIED BY \u0026#39;1111\u0026#39;; 在 MySQL 中修改数据库用户的密码，可以通过以下几种方式：\n1. 使用 ALTER USER 语句（推荐） # 在 MySQL 5.7.6 及以上版本中，可以使用 ALTER USER 语句来修改用户密码：\nALTER USER \u0026#39;username\u0026#39;@\u0026#39;host\u0026#39; IDENTIFIED BY \u0026#39;new_password\u0026#39;; 'username' 是数据库用户名。 'host' 是该用户的主机名（一般是 %，表示任何主机，或者 localhost，表示本机）。 'new_password' 是新的密码。 示例： # ALTER USER \u0026#39;myuser\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;newpassword123\u0026#39;; 2. 使用 SET PASSWORD 语句 # 对于 MySQL 5.7.5 及以下版本，或者如果 ALTER USER 不可用时，可以使用 SET PASSWORD：\nSET PASSWORD FOR \u0026#39;username\u0026#39;@\u0026#39;host\u0026#39; = PASSWORD(\u0026#39;new_password\u0026#39;); 示例： # SET PASSWORD FOR \u0026#39;myuser\u0026#39;@\u0026#39;localhost\u0026#39; = PASSWORD(\u0026#39;newpassword123\u0026#39;); 3. 使用 UPDATE 语句修改 mysql.user 表 # 在一些特殊情况下（例如：忘记了管理员密码），也可以直接通过更新 mysql.user 表来修改密码。不过这种方法在高版本 MySQL 中较少使用，推荐使用上面的方法。\nUPDATE mysql.user SET authentication_string = PASSWORD(\u0026#39;new_password\u0026#39;) WHERE User = \u0026#39;username\u0026#39; AND Host = \u0026#39;host\u0026#39;; FLUSH PRIVILEGES; authentication_string 字段保存密码（旧版本是 password 字段）。 FLUSH PRIVILEGES 用于刷新权限，使改动生效。 示例： # UPDATE mysql.user SET authentication_string = PASSWORD(\u0026#39;newpassword123\u0026#39;) WHERE User = \u0026#39;myuser\u0026#39; AND Host = \u0026#39;localhost\u0026#39;; FLUSH PRIVILEGES; 4. 重置 root 密码 # 如果需要重置 root 密码（例如忘记了密码），可以进入 MySQL 的安全模式进行修改：\n停止 MySQL 服务：\nsudo systemctl stop mysql 启动 MySQL 无密码模式：\nsudo mysqld_safe --skip-grant-tables \u0026amp; 进入 MySQL：\nmysql -u root 修改密码：\nUPDATE mysql.user SET authentication_string = PASSWORD(\u0026#39;newpassword123\u0026#39;) WHERE User = \u0026#39;root\u0026#39;; FLUSH PRIVILEGES; 退出 MySQL 并重启 MySQL 服务：\nsudo systemctl restart mysql 总结： # 推荐使用 ALTER USER 来修改密码。 SET PASSWORD 适用于较旧的版本。 在特殊情况下，可以直接修改 mysql.user 表并执行 FLUSH PRIVILEGES。 使用这些方法后，用户的密码就会被成功修改。\n在 MySQL 8.0 中，修改数据库用户密码的方法与 MySQL 5.7 版本有所不同，主要是通过 ALTER USER 语句来实现。下面是针对 MySQL 8.0 的几种常用修改密码的方式：\n1. 使用 ALTER USER 语句（推荐） # MySQL 8.0 版本推荐使用 ALTER USER 语句来修改用户的密码：\nALTER USER \u0026#39;username\u0026#39;@\u0026#39;host\u0026#39; IDENTIFIED BY \u0026#39;new_password\u0026#39;; 'username' 是要修改密码的用户名。 'host' 是用户的主机（如 % 或 localhost）。 'new_password' 是新的密码。 示例： # ALTER USER \u0026#39;myuser\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;newpassword123\u0026#39;; 2. 使用 SET PASSWORD 语句 # 在 MySQL 8.0 中，虽然 ALTER USER 是推荐方法，但 SET PASSWORD 仍然有效，可以通过该方法修改密码：\nSET PASSWORD FOR \u0026#39;username\u0026#39;@\u0026#39;host\u0026#39; = \u0026#39;new_password\u0026#39;; 示例： # SET PASSWORD FOR \u0026#39;myuser\u0026#39;@\u0026#39;localhost\u0026#39; = \u0026#39;newpassword123\u0026#39;; 3. 直接修改 mysql.user 表（不推荐） # 虽然可以直接更新 mysql.user 表来修改密码，但这种做法在 MySQL 8.0 中并不推荐，因为它可能导致安全问题，且需要手动刷新权限。\nUPDATE mysql.user SET authentication_string = PASSWORD(\u0026#39;new_password\u0026#39;) WHERE User = \u0026#39;username\u0026#39; AND Host = \u0026#39;host\u0026#39;; FLUSH PRIVILEGES; authentication_string 存储的是用户的加密密码。 示例： # UPDATE mysql.user SET authentication_string = PASSWORD(\u0026#39;newpassword123\u0026#39;) WHERE User = \u0026#39;myuser\u0026#39; AND Host = \u0026#39;localhost\u0026#39;; FLUSH PRIVILEGES; 4. 重置 root 密码（如果忘记密码） # 如果忘记了 root 密码，可以通过以下步骤重置密码：\n停止 MySQL 服务：\nsudo systemctl stop mysql 启动 MySQL 安全模式（跳过授权表）：\nsudo mysqld_safe --skip-grant-tables \u0026amp; 进入 MySQL 命令行：\nmysql -u root 修改 root 密码：\nALTER USER \u0026#39;root\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;newpassword123\u0026#39;; FLUSH PRIVILEGES; 退出 MySQL 并重启 MySQL 服务：\nsudo systemctl restart mysql 5. 查看密码加密方式 # 在 MySQL 8.0 中，密码加密方式有所变化。如果你需要查看当前用户的加密方式，可以执行以下命令：\nSELECT user, host, authentication_string FROM mysql.user WHERE user = \u0026#39;username\u0026#39;; 总结 # 推荐方法：使用 ALTER USER 语句修改用户密码。 次推荐方法：使用 SET PASSWORD 来修改密码。 不推荐的做法：直接修改 mysql.user 表。 如果忘记 root 密码，可以通过跳过授权表的方式重置密码。 MySQL 8.0 加强了对安全性的保护，推荐使用 ALTER USER 和 SET PASSWORD 来修改密码。\n11. 如何修改用户权限？如何查看？ # 授权： grant all on *.* to user@\u0026#39;%\u0026#39; identified by \u0026#39;passwd\u0026#39; 查看权限 show grants for user@\u0026#39;%\u0026#39;; 在 MySQL 中，修改用户权限和查看用户权限是常见的操作。这里提供如何修改和查看权限的详细步骤。\n1. 查看用户权限 # 要查看 MySQL 中某个用户的权限，可以使用以下查询：\nSHOW GRANTS FOR \u0026#39;username\u0026#39;@\u0026#39;host\u0026#39;; 'username' 是用户名。 'host' 是用户连接数据库的主机（通常是 localhost 或 %，表示任意主机）。 示例： # SHOW GRANTS FOR \u0026#39;myuser\u0026#39;@\u0026#39;localhost\u0026#39;; 这条命令会返回该用户在 MySQL 中的所有授权信息，输出类似如下：\nGRANT USAGE ON *.* TO \u0026#39;myuser\u0026#39;@\u0026#39;localhost\u0026#39; GRANT SELECT, INSERT, UPDATE, DELETE ON `mydb`.* TO \u0026#39;myuser\u0026#39;@\u0026#39;localhost\u0026#39; 2. 修改用户权限 # 修改用户权限的操作一般通过 GRANT 或 REVOKE 命令来实现。\n(1) 授予权限 # 通过 GRANT 语句，可以授予用户权限。常见的权限有 SELECT、INSERT、UPDATE、DELETE，以及数据库级、表级、列级权限等。\n语法如下：\nGRANT \u0026lt;privileges\u0026gt; ON \u0026lt;database\u0026gt;.\u0026lt;table\u0026gt; TO \u0026#39;username\u0026#39;@\u0026#39;host\u0026#39;; \u0026lt;privileges\u0026gt; 是要授予的权限，如 SELECT、INSERT 等，多个权限用逗号分隔，或使用 ALL PRIVILEGES 授予所有权限。 \u0026lt;database\u0026gt; 是要授予权限的数据库，* 表示所有数据库。 \u0026lt;table\u0026gt; 是要授予权限的表，* 表示所有表。 'username' 是要授予权限的用户。 'host' 是用户可以连接的主机，% 表示任意主机。 示例 1：授予 myuser 用户对 mydb 数据库的 SELECT 和 INSERT 权限 # GRANT SELECT, INSERT ON mydb.* TO \u0026#39;myuser\u0026#39;@\u0026#39;localhost\u0026#39;; 示例 2：授予 myuser 用户对所有数据库的所有权限 # GRANT ALL PRIVILEGES ON *.* TO \u0026#39;myuser\u0026#39;@\u0026#39;localhost\u0026#39;; 示例 3：授予 myuser 用户对特定表的 SELECT 权限 # GRANT SELECT ON mydb.mytable TO \u0026#39;myuser\u0026#39;@\u0026#39;localhost\u0026#39;; (2) 撤销权限 # 使用 REVOKE 语句可以撤销用户的某些权限。\n语法如下：\nREVOKE \u0026lt;privileges\u0026gt; ON \u0026lt;database\u0026gt;.\u0026lt;table\u0026gt; FROM \u0026#39;username\u0026#39;@\u0026#39;host\u0026#39;; 示例 1：撤销 myuser 用户对 mydb 数据库的 SELECT 权限 # REVOKE SELECT ON mydb.* FROM \u0026#39;myuser\u0026#39;@\u0026#39;localhost\u0026#39;; 示例 2：撤销 myuser 用户对所有数据库的所有权限 # REVOKE ALL PRIVILEGES ON *.* FROM \u0026#39;myuser\u0026#39;@\u0026#39;localhost\u0026#39;; 3. 刷新权限 # 当授予或撤销权限后，需要刷新权限表以使更改生效，使用以下命令：\nFLUSH PRIVILEGES; 4. 删除用户权限 # 如果你想要删除一个用户，删除用户时会自动撤销该用户的所有权限：\nDROP USER \u0026#39;username\u0026#39;@\u0026#39;host\u0026#39;; 示例： # DROP USER \u0026#39;myuser\u0026#39;@\u0026#39;localhost\u0026#39;; 总结 # 查看用户权限：使用 SHOW GRANTS FOR 'username'@'host';。 授予权限：使用 GRANT 命令。 撤销权限：使用 REVOKE 命令。 刷新权限：修改权限后使用 FLUSH PRIVILEGES; 来使改动生效。 删除用户：使用 DROP USER 删除用户及其所有权限。 通过这些操作，你可以灵活地管理 MySQL 用户的权限。\n三、nosql # 1. redis数据持久化有哪些方式？ # rdb aof Redis 提供了两种主要的数据持久化方式，以确保数据在重启或故障后不会丢失。它们分别是 RDB（Redis 数据库快照） 和 AOF（Append Only File）。除了这两种主要的持久化方式，Redis 还支持 混合持久化（RDB + AOF），具体如下：\n1. RDB（Redis 数据库快照） # RDB 是 Redis 默认的持久化方式，它通过在指定的时间间隔内创建数据库的快照来保存数据。\n工作原理： # Redis 会定期将数据库的状态（即数据）保存到硬盘上的一个 .rdb 文件中。 RDB 文件是压缩的二进制文件，可以在 Redis 启动时快速加载。 快照的创建是异步的，不会影响 Redis 的主操作。 配置： # 在 redis.conf 文件中，通过 save 指令配置保存快照的条件。例如：\nsave 900 1 # 如果在 900 秒（15分钟）内有至少 1 个 key 被修改，则进行快照 save 300 10 # 如果在 300 秒内有至少 10 个 key 被修改，则进行快照 save 60 10000 # 如果在 60 秒内有至少 10000 个 key 被修改，则进行快照 快照文件的路径和文件名可以通过 dir 和 dbfilename 指定：\ndir /var/lib/redis dbfilename dump.rdb 优缺点： # 优点：RDB 持久化方式对 Redis 性能影响较小，特别是在写入频繁的情况下，因为它是在后台异步操作的。 缺点：由于是周期性的保存快照，如果 Redis 崩溃，最后一次保存的数据和崩溃时的数据之间的更改会丢失，数据持久化的精度较低。 2. AOF（Append Only File） # AOF 通过记录所有写操作的日志来持久化数据，每次对 Redis 的写操作都会被追加到 AOF 文件中，Redis 启动时可以通过重放 AOF 文件来恢复数据。\n工作原理： # 每当发生写操作时，Redis 会将操作写入 AOF 文件。 AOF 文件是以追加的方式记录的，保留了 Redis 中所有的写操作，可以精确恢复数据。 Redis 提供了三种不同的同步策略： always：每次操作都同步写入磁盘，性能最差，持久化最可靠。 everysec：每秒钟同步写入一次，这是性能和可靠性的折衷方案，也是默认选项。 no：从不同步写入磁盘，性能最好，但一旦发生故障，数据会丢失。 配置： # 在 redis.conf 文件中通过 appendonly 和 appendfsync 配置 AOF 持久化：\nappendonly yes # 启用 AOF 持久化 appendfsync everysec # 每秒钟同步写入一次磁盘 AOF 文件的路径可以通过 dir 和 appendfilename 配置：\ndir /var/lib/redis appendfilename \u0026#34;appendonly.aof\u0026#34; 优缺点： # 优点：AOF 能够提供更高的数据可靠性，因为它记录了所有的写操作，可以做到“最小丢失”。 缺点：AOF 文件会不断增长，尤其是对大量数据写入的场景，AOF 文件可能会变得很大。为了优化 AOF 文件的大小，可以定期进行 AOF 重写（BGREWRITEAOF），将 AOF 文件压缩到更小的体积。 3. 混合持久化（RDB + AOF） # Redis 4.0 引入了混合持久化机制，结合了 RDB 和 AOF 两种持久化方式的优点。混合持久化允许 Redis 同时使用 RDB 快照和 AOF 日志文件，在 Redis 启动时能够尽量减少数据恢复的时间，同时保证数据的持久性。\n工作原理： # 在进行 AOF 重写时，Redis 会将数据存储在内存中的一个混合格式中（即 RDB 快照 + AOF 追加日志），这可以减少 AOF 重写时的性能开销。 启动时，Redis 会首先加载 RDB 快照文件，然后读取 AOF 文件中的操作来恢复数据。 配置： # 通过 redis.conf 文件中的 aof-use-rdb-preamble 配置来启用混合持久化：\naof-use-rdb-preamble yes 优缺点： # 优点：结合了 RDB 的启动速度和 AOF 的数据可靠性。 缺点：增加了配置复杂度，需要合理选择使用场景。 4. 持久化相关配置的选择 # 根据你的使用场景，选择合适的持久化方式至关重要：\n如果 对数据丢失的容忍度较低，且希望保证数据的完整性，建议使用 AOF（最好是 everysec 配置）。 如果希望 性能优先，并且可以接受 部分数据丢失，可以使用 RDB。 如果希望兼顾 性能和可靠性，可以考虑使用 RDB + AOF 混合持久化。 5. 如何优化 AOF 文件 # AOF 文件会随着时间的推移不断增长，因此 Redis 提供了 AOF 重写机制，可以压缩 AOF 文件：\nBGREWRITEAOF 通过 BGREWRITEAOF 命令，Redis 会生成一个新的 AOF 文件，合并其中的命令，从而减小 AOF 文件的大小。\n总结 # RDB：通过定期快照方式持久化，性能较高，但数据丢失可能性较大。 AOF：记录所有写操作，数据可靠性高，但文件会较大，可能影响性能。 混合持久化：结合了 RDB 快照和 AOF 日志，兼顾启动速度和数据可靠性。 根据实际的需求和对数据持久化的要求，可以选择最合适的持久化方式。\n2. redis集群方案有哪些？ # 官方cluster方案 twemproxy代理方案 哨兵模式 codis 客户端分片 Redis 集群是为了实现分布式数据存储而设计的一种方案，它将数据分布在多个 Redis 实例（节点）上，提供高可用、分布式存储、负载均衡和故障恢复功能。Redis 集群支持水平扩展（sharding），使得 Redis 可以在多个机器上分布式地存储大量数据。\nRedis 集群方案主要有以下几种： # 1. 原生 Redis 集群（Redis Cluster） # Redis Cluster 是 Redis 官方提供的分布式集群方案，旨在提供高可用和可扩展的 Redis 服务。\n特点： # 数据分片：数据被分散存储在多个 Redis 节点上，每个节点存储数据的一个子集。 自动分片：Redis 集群支持将数据分成多个 \u0026ldquo;槽\u0026rdquo;（hash slots），并自动将这些槽分配到不同的 Redis 节点。 高可用性：通过 Redis 集群的复制机制，每个主节点都有一个或多个副本节点，保证主节点故障时可以自动切换。 无中心化管理：Redis 集群没有一个中央的控制节点，每个节点都包含自己的配置和集群状态，所有节点通过协议进行通信。 工作原理： # Redis 集群将数据按一定的规则（哈希槽）分散到集群中的不同节点上。 Redis 集群默认将数据划分为 16384 个槽，每个键值会通过一致性哈希算法映射到一个槽，槽再由各个节点存储。 节点之间通过 Gossip 协议 和 TCP 连接 进行状态同步，确保集群中所有节点的状态一致。 优缺点： # 优点\n：\n提供水平扩展能力，可以通过增加节点来扩展集群容量。 内建故障恢复，主节点宕机时会自动将从节点提升为主节点。 对外暴露单一的 Redis 端口，客户端自动路由。 缺点\n：\n需要集群中的每个节点都有足够的内存来存储一部分数据。 配置较为复杂，需要至少 3 个主节点和 3 个副本来确保高可用性。 集群中发生故障时，可能会暂时导致部分槽的不可用。 配置： # Redis 集群需要至少 3 个主节点和 3 个从节点来实现高可用，并且节点数量是 2 的倍数。 具体配置可以参考 Redis 官方文档中的集群部署指南。\n2. Redis Sentinel # Redis Sentinel 是 Redis 官方提供的高可用性解决方案。它可以监控 Redis 主从架构的节点，自动进行故障转移。\n特点： # 高可用性：通过对 Redis 实例的监控，Sentinel 可以自动进行故障转移，主节点发生故障时，自动选举新的主节点。 自动恢复：当主节点发生故障，Sentinel 会将一个从节点提升为主节点，并更新集群中的配置。 集群监控：Redis Sentinel 提供对 Redis 实例的监控服务，并支持对集群进行通知和警告。 工作原理： # Sentinel 通过定期 ping 主节点和从节点来检查它们的状态。 如果检测到主节点不可用，Sentinel 会启动自动故障转移，将一个从节点提升为主节点，并通知其他 Sentinel 进行配置更新。 Sentinel 实例之间通过心跳机制和投票机制来选举新的主节点。 优缺点： # 优点\n：\n容错能力强，能够自动恢复主节点故障。 配置简单，适合大多数 Redis 部署。 可以与 Redis 单机模式结合使用，增加高可用性。 缺点\n：\nSentinel 本身并不提供数据分片能力，只适用于主从复制架构。 故障转移过程可能会有短暂的服务中断。 配置： # 至少需要 3 个 Sentinel 实例来保证故障转移的正常工作。 需要至少 1 个主节点和 1 个从节点来构建 Redis 集群。 3. Redis 代理方案（如 Twemproxy） # Twemproxy（又称 Nutcracker）是一个开源的代理工具，可以作为 Redis 集群的代理层。它允许在客户端与多个 Redis 实例之间进行负载均衡和路由。Twemproxy 适用于 Redis 主从架构或 Redis 集群架构。\n特点： # 负载均衡：通过代理进行请求路由，可以在多个 Redis 实例之间均衡负载。 透明代理：客户端与 Twemproxy 通信，Twemproxy 会根据请求路由到相应的 Redis 节点。 高性能：Twemproxy 是一个非常高效的代理，适用于高并发场景。 优缺点： # 优点\n：\n可以作为 Redis 集群的代理层，简化客户端操作。 支持连接池和负载均衡，提供性能优化。 缺点\n：\n不支持 Redis 集群的所有功能（如自动故障转移、复制等）。 需要额外的代理层，可能会增加系统复杂度。 配置： # 配置 Twemproxy 需要指定多个 Redis 实例作为后端，并在前端处理客户端请求。 4. Redis Sharding（手动分片） # 手动分片 是通过在多个 Redis 实例之间人工分配数据来实现的。客户端通过特定的规则来决定数据应该存储在哪个 Redis 实例上。\n特点： # 简单：不需要复杂的集群配置，适合小规模的 Redis 部署。 灵活：可以根据具体业务需求来设计数据分片策略。 优缺点： # 优点\n：\n简单易用，适合小规模应用。 可以灵活地进行分片和扩展。 缺点\n：\n数据分片的管理较为复杂，需要手动配置分片规则。 不提供自动故障转移和数据恢复机制。 总结 # Redis 集群（Redis Cluster） 是 Redis 原生的分布式集群方案，支持数据分片、自动故障转移和高可用性。 Redis Sentinel 主要提供高可用性和自动故障转移，但不提供数据分片功能，适用于主从架构。 Twemproxy 提供负载均衡和代理服务，适用于 Redis 集群和主从架构，但没有 Redis 集群的完整功能。 Redis Sharding 是手动分片方式，适合小规模部署，灵活但管理复杂。 选择哪种方案，取决于你的需求：\n如果需要 数据分片和高可用性，建议使用 Redis Cluster。 如果只需要 高可用性 而不需要分片，可以选择 Redis Sentinel。 如果需要 代理和负载均衡，可以使用 Twemproxy。 如果是 小规模部署，可以使用 手动分片。 这些方案都可以根据不同的应用场景和架构要求进行调整和优化。\n3. redis如何进行数据备份与恢复？ # 备份 redis 127.0.0.1:6379\u0026gt; SAVE 创建 redis 备份文件也可以使用命令 BGSAVE，该命令在后台执行。\n还原 只需将备份文件 (dump.rdb) 移动到 redis 安装目录并启动服务即可 redis 127.0.0.1:6379\u0026gt; CONFIG GET dir\n“dir” “/usr/local/redis/bin” Redis 提供了多种数据备份和恢复的方式，主要通过 持久化机制（RDB 和 AOF）以及 手动备份 来实现数据备份与恢复。以下是详细的备份与恢复方法：\n1. RDB 持久化备份与恢复 # RDB（Redis DataBase）是 Redis 的默认持久化方式，它将内存中的数据快照保存在硬盘上，以便在重启或故障时恢复数据。\nRDB 备份 # 自动备份：Redis 默认会根据配置（redis.conf）定期进行数据持久化。\n在\nredis.conf 文件中，\nsave 指令配置了定期备份的规则。例如：\nsave 900 1 # 900秒（15分钟）内有至少1次写操作时，自动生成快照 save 300 10 # 300秒（5分钟）内有至少10次写操作时，自动生成快照 save 60 10000 # 60秒内有至少10000次写操作时，自动生成快照 在这种配置下，Redis 会在满足这些条件时自动生成 RDB 文件。\n手动备份：可以使用 BGSAVE 命令进行手动备份，该命令会在后台生成数据快照，不会阻塞 Redis 服务。\n命令：\nBGSAVE 这会在后台异步生成\ndump.rdb 文件。\nRDB 文件位置：默认情况下，生成的 RDB 文件 dump.rdb 会存放在 Redis 数据目录下，通常在 /var/lib/redis/ 或 ./（Redis启动目录）。\nRDB 恢复 # 停止 Redis 服务。 将备份的 dump.rdb 文件放回 Redis 数据目录。 启动 Redis 服务，Redis 会在启动时自动加载 dump.rdb 文件中的数据。 如果你手动备份了 dump.rdb 文件，只需要将其恢复到 Redis 的数据目录并启动 Redis 服务即可。\n2. AOF 持久化备份与恢复 # AOF（Append-Only File）持久化将所有写操作记录到日志文件中，可以确保数据更持久和可靠。每个写命令都会记录在 AOF 文件中，因此 AOF 提供了比 RDB 更高的持久性。\nAOF 备份 # 自动备份：当 Redis 启用 AOF 持久化时，会将每一个写操作追加到 AOF 文件中。\n在\nredis.conf 中配置 AOF 相关参数：\nappendonly yes # 启用 AOF 持久化 appendfilename \u0026#34;appendonly.aof\u0026#34; # AOF 文件名 appendfsync everysec # 每秒同步一次 AOF 文件 appendfsync\n参数可以设置为：\nalways：每次写操作都同步到磁盘，性能较差。 everysec：每秒同步一次到磁盘，通常是推荐的设置。 no：不进行同步，由操作系统进行同步（性能最好，但可靠性最差）。 手动备份：使用 BGREWRITEAOF 命令来重写 AOF 文件，这会生成一个新的 AOF 文件并删除旧的 AOF 文件，从而节省空间。\n命令：\nBGREWRITEAOF 这会在后台进行 AOF 重写操作，生成一个新的压缩后的 AOF 文件。\nAOF 恢复 # 停止 Redis 服务。 将备份的 appendonly.aof 文件复制到 Redis 的数据目录。 启动 Redis 服务，Redis 会在启动时根据 AOF 文件恢复数据。 如果你启用了 AOF 持久化，只需要将备份的 appendonly.aof 文件放回 Redis 数据目录并重启 Redis 即可。\n3. 混合持久化（RDB + AOF） # Redis 4.0 及以后的版本支持同时启用 RDB 和 AOF 持久化（混合持久化）。这种方式能够结合 RDB 的快速启动和 AOF 的高可靠性，在启动时首先使用 RDB 快照，如果没有 RDB 快照，才使用 AOF 文件。\n启用混合持久化： # 在 redis.conf 中配置：\nappendonly yes # 启用 AOF appendfsync everysec # 每秒同步到 AOF save 900 1 # 900秒有一次写操作时进行 RDB 快照 使用混合持久化时，Redis 会在保存 RDB 快照时将 AOF 也作为增量记录保留。\n4. 手动备份（通过文件复制） # 如果你只是希望进行简单的备份，可以直接复制 Redis 的数据文件（dump.rdb 和 appendonly.aof 文件）。这种方式适用于备份频率较低的场景。\n停止 Redis 服务。 复制 dump.rdb 和 appendonly.aof 文件到备份存储。 启动 Redis 服务。 5. 增量备份 # Redis 本身不直接支持增量备份，但可以通过以下方式实现：\nRDB + AOF：结合 RDB 和 AOF 可以达到类似增量备份的效果。定期备份 RDB 快照，然后通过 AOF 文件记录增量数据。 外部工具：可以使用一些备份工具（如 redis-dump、redis-py 等）定期获取 Redis 数据并实现增量备份。 6. 备份策略与最佳实践 # 定期备份：定期进行 RDB 快照备份，建议根据业务量的大小、数据的重要性、磁盘容量等来调整备份频率。 多地备份：将备份文件存储在不同的物理位置，以防止硬盘损坏等不可预见的事件。 自动化备份：设置定时任务（如 cron job）来自动备份 Redis 数据。 监控与报警：结合监控工具，设置 Redis 数据备份的成功或失败的报警机制。 总结 # RDB：适合做定期的快照备份，恢复速度较快，数据丢失较少，但持久化操作可能较慢，适合对数据丢失容忍度较高的场景。 AOF：适合需要高可靠性、保证不丢失数据的场景，写入性能相对较低，恢复速度较慢。 混合持久化：结合了 RDB 的快速恢复和 AOF 的高可靠性，是目前较为推荐的方式。 选择备份方式时，可以根据实际需求决定是采用 RDB、AOF，还是两者结合使用。\n4. MongoDB如何进行数据备份？ # mongoexport / mongoimport mongodump / mongorestore\nMongoDB 提供了几种数据备份方式，主要有以下几种：\n1. 使用 mongodump 进行备份 # mongodump 是 MongoDB 提供的命令行工具，用于将数据库的内容导出为二进制格式的备份文件。\n备份操作 # 备份整个数据库：\nmongodump --host \u0026lt;host\u0026gt; --port \u0026lt;port\u0026gt; --out /path/to/backup/directory 这将备份所有数据库到指定目录。\n备份指定数据库：\nmongodump --host \u0026lt;host\u0026gt; --port \u0026lt;port\u0026gt; --db \u0026lt;database_name\u0026gt; --out /path/to/backup/directory 只备份指定的数据库。\n备份指定集合：\nmongodump --host \u0026lt;host\u0026gt; --port \u0026lt;port\u0026gt; --db \u0026lt;database_name\u0026gt; --collection \u0026lt;collection_name\u0026gt; --out /path/to/backup/directory 只备份指定数据库中的某个集合。\n指定认证： 如果 MongoDB 实例启用了认证，需要提供 --username 和 --password：\nmongodump --host \u0026lt;host\u0026gt; --port \u0026lt;port\u0026gt; --db \u0026lt;database_name\u0026gt; --username \u0026lt;user\u0026gt; --password \u0026lt;password\u0026gt; --out /path/to/backup/directory 备份压缩数据： 可以使用 --gzip 参数压缩备份文件：\nmongodump --host \u0026lt;host\u0026gt; --port \u0026lt;port\u0026gt; --out /path/to/backup/directory --gzip 备份数据的存储路径 # 备份的数据默认存储在 --out 指定的目录下，通常备份的数据会生成 *.bson 格式的文件以及一些元数据文件。 注意事项 # mongodump 是基于查询的方式进行备份的，因此会导致备份的时间较长，并且可能会对 MongoDB 的性能产生影响。 使用 mongodump 时会锁定数据库，所以在进行备份时，数据库的写操作可能会受到一定影响。 2. 使用 mongorestore 进行恢复 # mongorestore 是 MongoDB 提供的工具，用于将 mongodump 生成的备份文件恢复到数据库中。\n恢复整个备份：\nmongorestore --host \u0026lt;host\u0026gt; --port \u0026lt;port\u0026gt; /path/to/backup/directory 该命令将从备份目录恢复所有数据库和集合。\n恢复指定数据库：\nmongorestore --host \u0026lt;host\u0026gt; --port \u0026lt;port\u0026gt; --db \u0026lt;database_name\u0026gt; /path/to/backup/directory/\u0026lt;database_name\u0026gt; 恢复指定集合：\nmongorestore --host \u0026lt;host\u0026gt; --port \u0026lt;port\u0026gt; --db \u0026lt;database_name\u0026gt; --collection \u0026lt;collection_name\u0026gt; /path/to/backup/directory/\u0026lt;database_name\u0026gt;/\u0026lt;collection_name\u0026gt;.bson 恢复压缩的备份：\nmongorestore --host \u0026lt;host\u0026gt; --port \u0026lt;port\u0026gt; --gzip /path/to/backup/directory 注意事项 # 恢复时会根据备份文件覆盖数据库中的现有数据，如果你需要保留现有数据，请使用\n--drop 参数来删除现有集合再恢复数据：\nmongorestore --host \u0026lt;host\u0026gt; --port \u0026lt;port\u0026gt; --drop /path/to/backup/directory 3. 使用文件系统级备份 # 如果 MongoDB 以复制集的方式运行，可以通过文件系统级别的备份来实现备份。这种方法适合大规模的数据备份，通常用于生产环境中的定期备份。\n步骤： # 停用写操作（可选）： 你可以使用 fsync 命令让 MongoDB 进行一次同步操作，这有助于确保备份的一致性。\ndb.fsyncLock() 复制 MongoDB 数据目录： 备份 MongoDB 数据目录（默认情况下，数据目录为 /data/db）。 你可以使用操作系统的 cp 或 rsync 命令将数据库文件复制到备份存储。\nrsync -avz /data/db /path/to/backup/directory 解锁同步（可选）： 完成备份后，使用以下命令解锁写操作：\ndb.fsyncUnlock() 注意事项 # 使用文件系统备份时需要确保数据库的 journal 文件夹处于一致的状态，因此最好在 MongoDB 副本集的情况下进行备份。 此方法适用于大规模备份和恢复，但需要停机或锁定写操作，因此不适合高可用性要求非常高的场景。 4. 使用 MongoDB Atlas 进行备份（云托管服务） # 如果使用的是 MongoDB Atlas（MongoDB 的托管云服务），则可以利用其内建的自动备份功能。\n自动备份：MongoDB Atlas 提供了自动备份服务，可以按日、周、月等频率自动进行备份。 恢复备份：可以通过 Atlas 控制面板恢复备份，或者使用 MongoDB Atlas 的 API 恢复指定时间点的备份。 5. 增量备份 # MongoDB 本身并不直接支持增量备份，但可以通过以下方式实现类似的功能：\n复制集备份：通过 oplog （操作日志）实现增量备份。可以监控 MongoDB 的操作日志并提取增量数据，通常结合 mongodump 进行备份。 6. 定期备份和自动化 # 建议使用定时任务（如 cron job）来自动执行 mongodump 命令，以便定期备份数据库。这样可以确保数据定期得到保护，并且在数据库崩溃时能恢复。\n总结 # mongodump 是最常见的备份方式，适合大多数场景，使用时需要注意备份过程对性能的影响。 mongorestore 用于从 mongodump 备份恢复数据。 文件系统级备份 对于大规模数据库非常有效，但需要额外的同步和锁定操作。 MongoDB Atlas 提供了便捷的云备份和恢复服务，适合云环境。 根据具体业务需求选择合适的备份方式。\n5. kafka为何比redis rabbitmq快？ # https://www.zhihu.com/question/22480085\nKafka 通常比 Redis 和 RabbitMQ 在消息传递性能上要快，主要是由于它的架构设计和使用的技术不同。以下是一些关键原因：\n1. 顺序写入与高效磁盘存储 # Kafka 通过顺序写入磁盘来优化性能。磁盘的顺序写入性能通常比随机写入要好得多，这也是 Kafka 高效的关键。消息在 Kafka 中被追加到日志文件的末尾，这使得磁盘 I/O 性能得到极大提升。而 Redis 和 RabbitMQ 则依赖于不同的存储和数据结构，通常会发生更多的随机访问操作，从而影响性能。\n2. 持久化与内存管理 # Kafka 的消息持久化是基于日志文件的，它通过将消息追加到文件系统的顺序日志中，从而使得消息可以高效地写入磁盘，并通过日志分段来管理大规模的数据。Kafka 将存储和传输解耦，支持高吞吐量的写入和读取。 Redis 在默认情况下将所有数据存储在内存中（即使可以选择持久化数据到磁盘），这就意味着当处理大规模数据时，内存成为瓶颈。虽然 Redis 也支持磁盘持久化，但其持久化机制（RDB 或 AOF）可能会导致性能下降。 RabbitMQ 使用 Erlang 构建，并将消息持久化到磁盘时，会有更高的延迟，尤其是在消息需要强一致性保障时。其高吞吐量的能力不如 Kafka，且在高并发时可能会遇到性能瓶颈。 3. 分布式架构与水平扩展 # Kafka 从设计之初就支持分布式架构和水平扩展。它使用 分区 来水平分割数据，并可以通过增加分区和副本的数量来提高吞吐量。每个分区内的消息是按顺序存储的，并且可以并行处理，这使得 Kafka 能够支持大规模的并发消息传输。\nRedis 是单线程的，虽然它支持分片（Cluster），但是在高并发的场景下，它的性能和吞吐量会受到单个节点的 CPU 和内存限制。 RabbitMQ 可以通过集群和镜像队列来实现分布式，但是它的消息传递和存储机制相对复杂，且吞吐量通常比 Kafka 要低。 4. 消息顺序和消费模型 # Kafka 提供了更高效的消费者模型。Kafka 中的消费者是基于偏移量（offset）来消费消息的，可以非常灵活地在消息流中前进或回溯。并且，由于消息在分区内是按顺序写入的，因此 Kafka 能够提供更高的消息吞吐量和低延迟。\nRedis 通常依赖于发布/订阅模型，在高频率的数据推送场景下，消息的处理和消费可能会受到阻塞，尤其是在消息量非常大的情况下。 RabbitMQ 使用队列来存储和分发消息，虽然支持多个消费者并行消费，但其消费效率通常低于 Kafka，尤其是在大规模并发场景下。 5. 消息存储机制 # Kafka 是基于 append-only logs（仅追加日志）的方式进行存储的。这意味着一旦写入 Kafka，消息将被持久化并按时间顺序存储。这种方式大大减少了锁的竞争，写入性能非常高。\nRedis 是内存存储系统，数据放在内存中时速度快，但当数据量过大时，内存成为瓶颈，存储限制更为严格。 RabbitMQ 中的队列消息在传输和存储时需要在内存和磁盘之间频繁切换，尤其是当消息队列积压时，会引起更多的 I/O 操作，从而影响性能。 6. 延迟与吞吐量 # Kafka 设计时注重高吞吐量（high throughput）和低延迟，适合大规模的数据流传输，并且支持高并发的生产者和消费者。它能够处理数百万的消息和高频率的数据传输，而 RabbitMQ 和 Redis 虽然也能处理高吞吐量的场景，但相比之下，Kafka 更加专注于大规模、高并发的消息传递和持久化。\n7. 容错性和数据持久化 # Kafka 的设计有 日志复制 和 数据持久化 机制，确保了在节点发生故障时数据不丢失。同时，Kafka 的日志压缩功能使得消息可以存储较长时间，并且可以进行高效的回放和数据重放。\n总结 # Kafka 相对于 Redis 和 RabbitMQ 的优势主要在于：\n顺序写入和高效的磁盘存储。 高吞吐量和低延迟的分布式消息传递。 大规模扩展能力和高并发支持。 如果是处理需要高吞吐量、高并发、大规模数据流的场景，Kafka 无疑是更好的选择；而 Redis 和 RabbitMQ 更适合小规模、高频次的消息传递场景。\n四、docker # 1. dockerfile有哪些关键字？用途是什么？ # Dockerfile 是一个用于自动化创建 Docker 镜像的脚本文件，其中包含了构建镜像所需的所有命令和参数。Dockerfile 中的关键字定义了如何构建镜像。常用的关键字及其用途如下：\n1. FROM # 用途：指定基础镜像，是 Dockerfile 中的第一个命令。所有的 Dockerfile 都必须以 FROM 开始，表示从哪个镜像基础上构建。\n示例\n：\nFROM ubuntu:20.04 2. RUN # 用途：在镜像构建过程中执行命令。通常用于安装依赖、更新系统等操作。\n示例\n：\nRUN apt-get update \u0026amp;\u0026amp; apt-get install -y curl 3. CMD # 用途：为容器设置默认执行的命令。可以覆盖容器启动时的命令。CMD 可以接受命令和参数，通常用于运行容器时的入口点。\n示例\n：\nCMD [\u0026#34;python\u0026#34;, \u0026#34;app.py\u0026#34;] 4. ENTRYPOINT # 用途：定义容器启动时的默认命令。与 CMD 类似，但 ENTRYPOINT 是不可覆盖的，CMD 是可选的。如果 CMD 和 ENTRYPOINT 同时存在，CMD 提供默认参数。\n示例\n：\nENTRYPOINT [\u0026#34;python\u0026#34;, \u0026#34;app.py\u0026#34;] 5. COPY # 用途：将文件或目录从主机复制到镜像内。\n示例\n：\nCOPY ./app /usr/src/app 6. ADD # 用途：类似于 COPY，但还支持从 URL 下载文件以及自动解压 tar 包。\n示例\n：\nADD https://example.com/app.tar.gz /usr/src/app 7. EXPOSE # 用途：声明容器运行时监听的端口。它不会主动发布端口，而是作为文档的说明。\n示例\n：\nEXPOSE 80 8. ENV # 用途：设置环境变量。在容器内部可以使用这些环境变量。\n示例\n：\nENV APP_HOME /usr/src/app 9. ARG # 用途：定义构建时使用的变量。可以在 docker build 时通过 --build-arg 提供参数。\n示例\n：\nARG VERSION=1.0 10. VOLUME # 用途：创建挂载点，允许容器与主机或其他容器之间共享数据。\n示例\n：\nVOLUME [\u0026#34;/data\u0026#34;] 11. WORKDIR # 用途：设置工作目录。后续的 RUN、CMD、ENTRYPOINT 等指令将在此目录下执行。\n示例\n：\nWORKDIR /usr/src/app 12. USER # 用途：指定镜像中的用户。容器在启动时会以指定的用户身份运行。\n示例\n：\nUSER appuser 13. LABEL # 用途：给镜像添加元数据（如版本号、作者等）。\n示例\n：\nLABEL version=\u0026#34;1.0\u0026#34; maintainer=\u0026#34;yourname@example.com\u0026#34; 14. SHELL # 用途：设置 RUN 命令使用的 shell，默认为 /bin/sh -c，可以通过 SHELL 指令改变。\n示例\n：\nSHELL [\u0026#34;/bin/bash\u0026#34;, \u0026#34;-c\u0026#34;] 15. STOPSIGNAL # 用途：指定容器停止时要发送的信号，默认是 SIGTERM。\n示例\n：\nSTOPSIGNAL SIGINT 总结 # 这些关键字在 Dockerfile 中分别用于定义镜像的基础、运行时的命令、容器的配置等，帮助用户灵活配置和构建自定义镜像。通过合理的使用这些关键字，可以大大提高镜像构建的效率和可维护性。\n2. 如何减小dockerfile生成镜像体积？ # 尽量选取满足需求但较小的基础系统镜像，例如大部分时候可以选择debian:wheezy或debian:jessie镜像，仅有不足百兆大小； 清理编译生成文件、安装包的缓存等临时文件； 安装各个软件时候要指定准确的版本号，并避免引入不需要的依赖； 从安全角度考虑，应用要尽量使用系统的库和依赖； 如果安装应用时候需要配置一些特殊的环境变量，在安装后要还原不需要保持的变量值； 减小 Dockerfile 生成镜像体积是优化 Docker 镜像大小的一个重要步骤。较小的镜像不仅节省存储空间，还能提高镜像的拉取速度和启动速度。以下是一些减小 Docker 镜像体积的最佳实践：\n1. 选择合适的基础镜像 # 使用较小的基础镜像，例如 alpine，而不是较大的镜像如 ubuntu 或 centos。Alpine 是一个非常轻量的 Linux 发行版，通常它的体积只有 5 MB 左右。\n示例\n：\nFROM alpine:3.12 2. 尽量减少 RUN 指令的层数 # Dockerfile 中的每个命令都会创建一个新的镜像层。尽量将多个命令合并到一个 RUN 指令中，从而减少镜像层数。\n示例\n：\n# 不推荐 RUN apt-get update RUN apt-get install -y curl RUN apt-get install -y vim # 推荐 RUN apt-get update \u0026amp;\u0026amp; apt-get install -y curl vim 3. 删除临时文件和缓存 # 在安装软件时，通常会产生缓存文件和临时文件，这些文件在最终镜像中是没用的。在安装完成后，使用 \u0026amp;\u0026amp; 将删除缓存的操作合并到同一个 RUN 命令中。\n示例\n：\nRUN apt-get update \u0026amp;\u0026amp; apt-get install -y curl \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* 4. 使用 .dockerignore 文件 # 类似于 .gitignore 文件，.dockerignore 文件可以防止不需要的文件被复制到镜像中，减少镜像体积。通常应该忽略不需要的源代码文件、临时文件、日志文件等。\n示例\n：\n.git *.log *.md 5. 使用多阶段构建（Multi-stage Build） # 使用多阶段构建可以将构建过程和最终的运行镜像分开，在构建阶段安装依赖并进行编译，然后只将最终生成的二进制文件复制到一个更小的运行镜像中。这种方式可以有效减小最终镜像的大小。\n示例\n：\n# 构建阶段 FROM golang:1.15 as builder WORKDIR /src COPY . . RUN go build -o /bin/myapp # 运行阶段 FROM alpine:3.12 COPY --from=builder /bin/myapp /bin/myapp CMD [\u0026#34;/bin/myapp\u0026#34;] 6. 使用压缩格式的文件 # 如果你需要将文件添加到镜像中，可以考虑使用压缩文件。然后在镜像构建时解压这些文件，这样可以减小源文件的大小。\n示例\n：\nCOPY app.tar.gz /app/ RUN tar -zxvf /app/app.tar.gz -C /app \u0026amp;\u0026amp; rm /app/app.tar.gz 7. 避免使用 ADD 除非需要 # ADD 可以处理压缩包并自动解压缩文件，但它也会带来不必要的文件操作。除非需要解压缩或从 URL 下载文件，否则应使用 COPY 代替。\n示例\n：\n# 不推荐 ADD app.tar.gz /app/ # 推荐 COPY app.tar.gz /app/ 8. 精简镜像中的文件和依赖 # 避免安装不必要的库和工具。安装后删除不需要的文件，例如开发依赖和文档文件。\n示例\n：\nRUN apt-get update \u0026amp;\u0026amp; apt-get install -y --no-install-recommends \\ curl vim \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* 9. 使用合适的压缩格式 # Docker 镜像支持多种压缩格式，使用 gzip 格式可能会产生较大的镜像，而使用 xz 或 lzma 格式会使得镜像体积更小。通过 Docker 工具 docker export 和 docker import 还可以重新压缩镜像。 10. 减少日志文件和调试信息 # 在容器镜像中避免包含不必要的日志文件或调试信息，这些文件在生产环境中并不需要。 总结： # 通过合理的优化 Dockerfile，可以显著减小生成镜像的体积，提高部署速度、减少网络带宽消耗。最重要的优化措施是选择合适的基础镜像、合并 RUN 命令、清理临时文件以及使用多阶段构建。\n3. dockerfile中CMD与ENTRYPOINT区别是什么？ # CMD 和 ENTRYPOINT 指令都是用来指定容器启动时运行的命令。 指定 ENTRYPOINT 指令为 exec 模式时，CMD指定的参数会作为参数添加到 ENTRYPOINT 指定命令的参数列表中。 在 Dockerfile 中，CMD 和 ENTRYPOINT 都是用来指定容器启动时执行的命令，它们在功能上有些相似，但也有一些关键的区别：\n1. CMD（命令） # 用途：设置容器启动时的默认命令及其参数。可以通过 docker run 时提供的命令来覆盖。\n语法：\nCMD [\u0026quot;executable\u0026quot;, \u0026quot;param1\u0026quot;, \u0026quot;param2\u0026quot;]（JSON 数组格式） CMD [\u0026quot;param1\u0026quot;, \u0026quot;param2\u0026quot;]（Shell 格式，假设默认使用某个命令，比如 /bin/sh -c） CMD command param1 param2（Shell 格式） 特点：\n如果 Dockerfile 中既有 CMD，也有 ENTRYPOINT，则 CMD 提供默认的参数，ENTRYPOINT 定义默认的执行程序。 如果通过 docker run 命令指定了其它命令，CMD 中的命令会被覆盖。 示例：\nCMD [\u0026#34;echo\u0026#34;, \u0026#34;Hello, World!\u0026#34;] 运行时：docker run \u0026lt;image\u0026gt; 会输出 Hello, World!。\n通过 docker run \u0026lt;image\u0026gt; \u0026lt;new command\u0026gt; 可以覆盖 CMD 设置的命令。\n2. ENTRYPOINT（入口点） # 用途：设置容器启动时的入口命令，通常是容器启动时的主命令。与 CMD 不同，ENTRYPOINT 定义的是容器的执行方式，CMD 可以为 ENTRYPOINT 提供默认参数。\n语法：\nENTRYPOINT [\u0026quot;executable\u0026quot;, \u0026quot;param1\u0026quot;, \u0026quot;param2\u0026quot;]（JSON 数组格式） ENTRYPOINT command param1 param2（Shell 格式） 特点：\nENTRYPOINT 不会被 docker run 命令覆盖。如果需要动态地给 ENTRYPOINT 提供参数，通常通过 CMD 提供默认参数。 ENTRYPOINT 使容器执行的命令更固定，通常用于像服务程序等容器主程序。 示例：\nENTRYPOINT [\u0026#34;echo\u0026#34;] CMD [\u0026#34;Hello, World!\u0026#34;] 运行时：docker run \u0026lt;image\u0026gt; 会输出 Hello, World!，这是因为 CMD 提供了默认参数 Hello, World! 给 ENTRYPOINT。\n通过 docker run \u0026lt;image\u0026gt; \u0026lt;new param\u0026gt; 可以覆盖 CMD 的参数，但 ENTRYPOINT 不会被覆盖，echo 命令始终会执行。\n区别总结 # 特性 CMD ENTRYPOINT 作用 定义默认命令及其参数，容器启动时执行。 定义容器的主命令，通常不被覆盖。 能否被覆盖 可以通过 docker run 时指定命令覆盖。 不会被 docker run 指定的命令覆盖。 与 ENTRYPOINT 结合使用 可以为 ENTRYPOINT 提供默认参数。 一般作为主命令，CMD 提供默认参数。 用法 通常用于提供默认的参数或执行的命令。 用于定义容器的主进程命令，确保容器启动时执行。 常见的组合方式 # 只使用 CMD：适合容器启动时可以有灵活的命令行参数。\nCMD [\u0026#34;echo\u0026#34;, \u0026#34;Hello, World!\u0026#34;] 只使用 ENTRYPOINT：适合容器启动时执行固定的命令。\nENTRYPOINT [\u0026#34;echo\u0026#34;] CMD [\u0026#34;Hello, World!\u0026#34;] 这样可以通过 docker run 时指定不同的参数来改变输出内容：\ndocker run \u0026lt;image\u0026gt; \u0026#34;New Message\u0026#34; 使用 ENTRYPOINT + CMD：这种方式组合可以确保容器始终执行主命令，而 CMD 提供默认参数，且可以通过 docker run 覆盖。\nENTRYPOINT [\u0026#34;python\u0026#34;] CMD [\u0026#34;app.py\u0026#34;] 通过 docker run 可以指定不同的脚本：\ndocker run \u0026lt;image\u0026gt; app2.py 结论 # 使用 ENTRYPOINT 来定义固定的主命令，不希望用户覆盖。 使用 CMD 来为命令提供默认参数，或者提供默认命令，可以被用户覆盖。 4. dockerfile中COPY和ADD区别是什么？ # COPY指令和ADD指令都可以将主机上的资源复制或加入到容器镜像中 区别是ADD可以从 远程URL中的资源不会被解压缩。 如果是本地的压缩包ADD进去会被解压缩 在 Dockerfile 中，COPY 和 ADD 都是用来将文件或目录从宿主机复制到 Docker 镜像中，但它们有一些不同的功能和用途。下面是它们的主要区别：\n1. COPY # 用途：COPY 是最简单的文件复制指令，用于将宿主机的文件或目录复制到镜像中。它只做简单的复制操作，不会进行任何解压或额外的操作。\n语法：\nCOPY \u0026lt;src\u0026gt; \u0026lt;dest\u0026gt; \u0026lt;src\u0026gt;：源路径，可以是宿主机中的文件或目录。 \u0026lt;dest\u0026gt;：目标路径，将文件或目录复制到镜像中。 特点：\n只做文件复制，不支持解压压缩文件。 不会自动从 URL 下载文件。 适用于普通的文件复制需求。 示例：\nCOPY ./myapp /usr/src/app/ 2. ADD # 用途：ADD 是一个功能更强大的指令，不仅可以将文件从宿主机复制到镜像中，还支持从 URL 下载文件并解压 tar 文件到镜像中。\n语法：\nADD \u0026lt;src\u0026gt; \u0026lt;dest\u0026gt; \u0026lt;src\u0026gt;：源路径，可以是宿主机中的文件、目录或 URL。 \u0026lt;dest\u0026gt;：目标路径，将文件或目录复制到镜像中。 特点：\n如果源是一个 .tar 文件，ADD 会自动解压到目标路径。 支持从 URL 下载文件并将其复制到镜像中。 由于功能更多，因此 ADD 可能会引入一些不必要的复杂性，特别是在没有解压需求的情况下。 示例：\nADD ./myapp.tar /usr/src/app/ # 自动解压 myapp.tar ADD https://example.com/file.tar.gz /usr/src/app/ # 从 URL 下载并解压 区别总结 # 特性 COPY ADD 功能 仅仅复制文件或目录 复制文件或目录，同时支持解压 .tar 文件和从 URL 下载文件 解压支持 不支持自动解压文件 支持自动解压 .tar 文件 从 URL 下载文件 不支持 支持从 URL 下载文件并复制到镜像中 推荐场景 简单的文件复制操作，无需解压或下载功能 当需要解压 .tar 文件或从 URL 下载文件时 安全性和可控性 更加简洁和明确，推荐用于复制文件 由于额外功能（自动解压和下载），使用时需要小心，避免不必要的复杂性 使用建议 # 推荐使用 COPY：对于普通的文件复制操作，COPY 更加简单明了，避免了额外的复杂性。如果不需要解压或从 URL 下载文件，使用 COPY 更加合适。 使用 ADD 时：仅当你确实需要从 URL 下载文件或需要自动解压 .tar 文件时，才应使用 ADD，因为 ADD 的额外功能可能会增加不必要的复杂性。 示例 # 简单复制文件：\nCOPY ./myapp /usr/src/app/ 使用 COPY 来将文件从宿主机复制到镜像。 下载并解压文件：\nADD https://example.com/file.tar.gz /usr/src/app/ 使用 ADD 从 URL 下载文件并解压到镜像中。 结论 # 如果只是复制文件，优先使用 COPY，它更简洁且不会做多余的操作。 如果需要从 URL 下载文件或解压文件，才使用 ADD。 5. docker的cs架构组件有哪些？ # Docker 的 CS（Client-Server）架构是 Docker 的核心架构模式，其中包含以下主要组件：\n1. Docker Client（客户端） # 作用：Docker 客户端是用户与 Docker 引擎交互的接口，负责发送命令并接收输出。它通过命令行界面（CLI）与 Docker 引擎通信。 常见命令：docker run、docker ps、docker build、docker images 等。 与 Docker Engine 通信：客户端通过 Docker API 与 Docker 引擎进行交互，通常是通过 HTTP 或 Unix socket 来与 Docker 守护进程通信。 2. Docker Daemon（守护进程） # 作用：Docker 守护进程是 Docker 的核心，它负责管理容器生命周期（创建、启动、停止和销毁容器）。它还负责管理镜像、网络、卷等其他 Docker 组件，并处理来自 Docker 客户端的请求。 常见操作：docker run、docker build、docker network、docker volume 等。 通信方式：守护进程通常运行在后台，监听来自 Docker 客户端的请求并返回相应的结果。 3. Docker Registry（镜像仓库） # 作用：Docker Registry 是一个用于存储和分发 Docker 镜像的服务。官方的 Docker 镜像仓库是 Docker Hub，用户可以将自己的镜像上传到此仓库，也可以从仓库拉取所需的镜像。 常见操作：docker push（上传镜像）、docker pull（拉取镜像）、docker search（搜索镜像）。 本地仓库：除了 Docker Hub，还可以配置私有仓库来存储自己的镜像。 4. Docker Image（镜像） # 作用：Docker 镜像是一个包含了应用程序及其所有依赖项、库、配置文件等的只读模板。镜像是容器的蓝图。 构建过程：可以通过 Dockerfile 文件来构建镜像。 从镜像启动容器：通过 docker run 命令启动一个容器实例。 5. Docker Container（容器） # 作用：Docker 容器是镜像的一个可执行实例。它是一个轻量级的、可移植的、封装了应用及其依赖环境的运行时环境。容器运行在宿主操作系统内，且与宿主系统隔离。 常见操作：docker start、docker stop、docker exec、docker logs 等。 隔离性：容器提供进程、网络、文件系统的隔离，但与宿主系统共享内核。 6. Docker Volume（数据卷） # 作用：Docker 卷是持久化和共享容器数据的机制，允许容器和主机共享数据或在多个容器之间共享数据。数据卷独立于容器的生命周期存在，即使容器被删除，数据卷中的数据仍然存在。 常见操作：docker volume create、docker volume inspect、docker volume rm。 7. Docker Network（网络） # 作用：Docker 网络功能使容器能够互相通信。Docker 提供了几种网络模式，包括桥接网络（bridge）、主机网络（host）、覆盖网络（overlay）等。 常见操作：docker network create、docker network inspect、docker network connect。 8. Docker Swarm（集群管理） # 作用：Docker Swarm 是 Docker 提供的集群管理和编排工具。它使得多个 Docker 主机可以联合起来，共同运行容器，形成一个集群，提供容器的负载均衡、服务发现、扩展性等功能。 常见操作：docker swarm init（初始化集群）、docker service create（创建服务）、docker node ls（列出集群节点）。 9. Docker Compose（编排工具） # 作用：Docker Compose 是一个工具，用于定义和运行多容器 Docker 应用。通过 docker-compose.yml 配置文件，用户可以定义多个容器服务的组成、配置及其之间的依赖关系，方便进行多容器环境的管理和部署。 常见操作：docker-compose up（启动应用）、docker-compose down（停止应用）、docker-compose build（构建服务）。 10. Docker CLI（命令行工具） # 作用：Docker CLI 是与 Docker 守护进程进行交互的命令行工具，允许用户通过命令行来操作 Docker，管理容器、镜像、网络、卷等。 常见命令：docker ps（列出容器）、docker run（运行容器）、docker build（构建镜像）等。 总结 # Docker 的 CS 架构组件包括：\nDocker Client：用户与 Docker 引擎交互的客户端。 Docker Daemon：Docker 引擎，负责处理命令并管理容器、镜像等。 Docker Registry：用于存储和分发镜像的仓库。 Docker Image：容器的静态模板。 Docker Container：镜像的运行实例，具有隔离性。 Docker Volume：持久化容器数据。 Docker Network：容器之间的网络通信管理。 Docker Swarm：容器集群管理和编排工具。 Docker Compose：用于管理和编排多个容器的工具。 Docker CLI：命令行界面，允许与 Docker 引擎进行交互。 这些组件共同作用，确保 Docker 提供高效、灵活、易于扩展的容器化平台。\n6. docker网络类型有哪些？ # host模式 container模式 none模式 bridge模式 Docker 提供了几种不同的网络类型，以满足不同应用场景下的容器通信需求。以下是 Docker 支持的主要网络类型及其特点：\n1. Bridge Network（桥接网络） # 作用：这是 Docker 的默认网络模式。它在宿主机上创建一个虚拟的网络桥（bridge），容器会连接到这个网络上，所有容器共享此网络接口。容器之间通过桥接网络进行通信。\n特点：\n每个容器会获得一个私有 IP 地址，并且能够通过宿主机的 IP 地址与外界通信。 容器之间可以互相通信，但只能通过网络桥接接口（如 docker0）实现。 通常适用于单机环境，容器相互之间需要隔离但仍可以共享宿主机的网络。 使用场景：\n默认网络模式，适合单机环境下的容器通信。 常见命令：\ndocker network create --driver bridge my_bridge_network 2. Host Network（主机网络） # 作用：在这种模式下，容器直接使用宿主机的网络接口。容器不会获得独立的 IP 地址，而是与宿主机共享网络栈。\n特点：\n容器与宿主机共享网络 IP 和端口（容器中的端口映射到宿主机的端口）。 适用于要求低延迟和高性能的网络应用，因为它消除了网络隔离。 由于容器与宿主机共享网络，因此容器暴露的端口不会经过虚拟网络桥。 使用场景：\n性能要求高的场景，特别是需要访问宿主机网络资源的应用（如数据库连接、低延迟通信等）。 常见命令：\ndocker run --network host my_container 3. Overlay Network（覆盖网络） # 作用：覆盖网络是 Docker Swarm 模式下使用的网络类型，允许跨多个宿主机的容器之间进行通信。Overlay 网络使得分布式应用中的容器能够通过一个虚拟的网络在不同宿主机之间进行通信。\n特点：\n容器无需知道实际所在的宿主机，所有容器都可以使用相同的虚拟 IP 地址。 通过 VXLAN 隧道技术实现跨宿主机的容器网络连接。 适用于多主机集群中容器之间的通信，特别是 Docker Swarm 或 Kubernetes 集群中的容器通信。 使用场景：\n在 Docker Swarm 或 Kubernetes 等集群环境中使用，适合分布式微服务架构的部署。 常见命令：\ndocker network create --driver overlay my_overlay_network 4. Macvlan Network（Macvlan 网络） # 作用：Macvlan 网络为容器提供一个独立的 MAC 地址和 IP 地址，使得容器像物理设备一样直接连接到物理网络中。这种模式适合需要与物理网络设备直接通信的场景。\n特点：\n每个容器都有独立的 MAC 地址和 IP 地址，因此容器可以被视为网络中的独立设备。 容器可以直接与物理网络中的其他设备进行通信，如访问外部网络中的其他机器。 常用于要求容器与物理网络设备直接交互的情况，如通过 DHCP 获取 IP 地址等。 使用场景：\n容器需要与宿主机外的网络设备直接通信，或需要与其他物理主机共享网络。 常见命令：\ndocker network create -d macvlan --subnet=192.168.1.0/24 --gateway=192.168.1.1 my_macvlan_network 5. None Network（无网络） # 作用：这种网络模式下，容器不会连接任何网络接口，也没有自己的 IP 地址。容器可以完全隔离与外界的网络通信。\n特点：\n容器没有网络接口，无法访问外部网络，适用于容器不需要网络连接的场景。 如果容器需要使用其他网络，可以通过自定义配置实现。 使用场景：\n对于某些需要完全隔离的容器，或者某些只处理计算任务而不需要网络连接的场景。 常见命令：\ndocker run --network none my_container 6. Container Network（容器网络） # 作用：容器网络是指通过 --link 选项将一个容器的网络连接到另一个容器。容器与容器之间通过共享网络栈来进行通信。\n特点：\n一个容器的网络栈被直接共享给另一个容器，使得两个容器之间能够直接通信。 容器共享 IP 地址和端口，适用于需要高度耦合的应用。 使用场景：\n当容器之间需要直接共享网络环境时，例如服务间需要高度耦合并直接访问对方网络资源。 常见命令：\ndocker run --network container:\u0026lt;container_name\u0026gt; my_container 总结 # Docker 支持的网络类型包括：\nBridge Network：默认网络模式，适用于单机容器间的通信。 Host Network：容器与宿主机共享网络，适用于高性能场景。 Overlay Network：适用于 Docker Swarm 集群环境，跨主机容器通信。 Macvlan Network：给容器分配独立的 MAC 地址和 IP 地址，适用于与物理网络直接通信。 None Network：容器不使用任何网络接口，完全隔离。 Container Network：容器间共享网络栈，适用于需要紧密耦合的应用。 每种网络类型适用于不同的场景，根据你的应用需求选择合适的网络模式来保证容器间通信的效率与安全性。\n7. 如何配置docker远程访问？ # vim /lib/systemd/system/docker.service 在ExecStart=后添加配置，注意，需要先空格后，再输入 -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock 要配置 Docker 远程访问，需要修改 Docker 的配置，使其监听在一个可供远程访问的 IP 地址和端口上。以下是配置 Docker 远程访问的步骤：\n1. 修改 Docker 配置文件 # 默认情况下，Docker 只监听本地的 Unix Socket（/var/run/docker.sock），不允许远程访问。要启用远程访问，需要修改 Docker 的配置，使其监听 TCP 端口。\n步骤： # 打开 Docker 服务的配置文件，通常是 /lib/systemd/system/docker.service 或 /etc/systemd/system/docker.service，取决于你的 Linux 发行版和 Docker 的安装方式。\n找到以 ExecStart 开头的行。它会指定 Docker 的启动命令。\nExecStart=/usr/bin/dockerd --host=unix:///var/run/docker.sock 修改为：\nExecStart=/usr/bin/dockerd --host=unix:///var/run/docker.sock --host=tcp://0.0.0.0:2375 这将使 Docker 同时监听 Unix Socket 和 0.0.0.0 的 TCP 端口 2375（所有 IP 地址的 2375 端口）。你可以根据需要修改监听的 IP 地址和端口。若只允许特定 IP 访问，可以将 0.0.0.0 改为服务器的实际 IP 地址，例如 tcp://192.168.1.100:2375。\n2. 配置防火墙 # 确保防火墙允许访问 Docker 的 2375 端口。如果使用 firewalld，可以运行以下命令：\nsudo firewall-cmd --zone=public --add-port=2375/tcp --permanent sudo firewall-cmd --reload 如果使用 ufw，可以执行：\nsudo ufw allow 2375/tcp 3. 重启 Docker 服务 # 修改配置后，需要重启 Docker 服务使配置生效：\nsudo systemctl daemon-reload sudo systemctl restart docker 4. 测试远程访问 # 在客户端机器上使用 Docker 命令连接远程 Docker 守护进程。\n例如，使用以下命令：\ndocker -H tcp://\u0026lt;docker_host_ip\u0026gt;:2375 info 如果配置成功，你应该能够看到 Docker 服务器的详细信息。\n5. 使用 TLS 安全连接（推荐） # 暴露 Docker 的远程访问接口时，默认是没有加密和认证的，任何可以访问该端口的人都能控制 Docker 服务。这非常不安全，尤其是在生产环境中。为确保安全，建议启用 TLS 加密和认证。\n生成 SSL 证书并配置 Docker 使用它们：\n你可以使用 openssl 生成证书，或者使用 Let’s Encrypt 等证书提供商。\n配置 Docker 使用证书和私钥：\n修改 Docker 服务启动命令来指定证书和私钥：\nExecStart=/usr/bin/dockerd --host=tcp://0.0.0.0:2376 --tlsverify --tlscacert=/etc/docker/ca.pem --tlscert=/etc/docker/server-cert.pem --tlskey=/etc/docker/server-key.pem 在这种情况下，Docker 远程访问需要客户端提供相应的证书。\n6. 在客户端连接时使用 TLS # 客户端连接 Docker 服务时，必须使用 TLS 认证。\ndocker --host=tcp://\u0026lt;docker_host_ip\u0026gt;:2376 --tlsverify --tlscacert=/path/to/ca.pem --tlscert=/path/to/cert.pem --tlskey=/path/to/key.pem info 总结 # 开放端口：修改 Docker 配置，监听 TCP 端口（如 2375）来允许远程访问。 防火墙配置：确保防火墙允许外部访问 Docker 的端口。 远程连接：通过 docker -H tcp://\u0026lt;docker_host_ip\u0026gt;:2375 来进行远程连接。 安全：建议使用 TLS 加密和认证来保护 Docker 远程接口，防止未授权的访问。 通过以上步骤，你就可以配置 Docker 进行远程访问并与远程主机进行交互。\n8. docker核心namespace CGroups 联合文件系统功能是什么？ # namespace：资源隔离 cgroup：资源控制 联合文件系统：支持对文件系统的修改作为一次提交来一层层的叠加，同时可以将不同目录挂载到同一个虚拟文件系统下 Docker 作为一个容器化平台，依赖于操作系统的几个核心技术来实现容器的轻量化和高效性。Docker 主要通过 Namespace、Cgroups 和 联合文件系统（Union File System） 来实现容器的隔离性、资源限制和文件系统共享。下面是这三种技术的功能和作用：\n1. Namespace（命名空间） # 命名空间（Namespace） 是 Linux 内核的一个特性，提供了对系统资源的隔离，使得每个容器都可以拥有独立的视图，而互相之间是隔离的。Docker 使用命名空间来确保容器之间的资源不会互相干扰。Docker 支持以下几种命名空间：\nPID Namespace（进程命名空间）\n：\n每个容器有独立的进程 ID 空间。容器内的进程 ID 从 1 开始，和宿主机的进程 ID 是完全隔离的。这使得容器内的进程与宿主机或其他容器的进程互不干扰。 NET Namespace（网络命名空间）\n：\n每个容器有独立的网络栈，包括 IP 地址、路由表、网络接口、端口等。这样可以确保容器之间的网络是隔离的，不会互相干扰。 MNT Namespace（挂载命名空间）\n：\n每个容器可以拥有自己的文件系统挂载点，容器的挂载点与宿主机的文件系统是隔离的。这样可以使容器内的文件系统和宿主机或其他容器的文件系统互不干扰。 UTS Namespace（UNIX时间命名空间）\n：\n每个容器有独立的主机名和域名。容器内的主机名和域名与宿主机或其他容器的主机名和域名是隔离的。 IPC Namespace（进程间通信命名空间）\n：\n每个容器有独立的进程间通信资源（如共享内存、信号量、消息队列等）。这意味着容器内的进程不能与宿主机或其他容器中的进程共享内存或进行通信。 USER Namespace（用户命名空间）\n：\n容器内的用户和组 ID 被映射到宿主机的用户和组 ID，这可以有效地增强容器的安全性。容器内的 root 用户可以映射为宿主机上的非特权用户，从而降低安全风险。 总结：命名空间的作用是 资源隔离，每个容器在自己的命名空间中拥有独立的资源视图，互不干扰。\n2. Cgroups（控制组） # Cgroups（Control Groups） 是 Linux 内核的一个特性，用于限制、控制和监控进程组的资源使用。Docker 利用 Cgroups 来限制每个容器使用的 CPU、内存、磁盘 I/O 和网络等资源，以确保资源的公平分配和容器的限制性使用。\nCPU 限制\n：\n通过 Cgroups，可以为容器分配一定的 CPU 配额和权重，控制容器的 CPU 使用量。 内存限制\n：\nCgroups 可以为容器指定最大内存使用量，一旦超过该限制，容器内的进程会被 OOM Killer 杀死，避免因内存耗尽导致系统崩溃。 磁盘 I/O 限制\n：\nCgroups 可以限制容器的磁盘 I/O 性能，如读取和写入速度等，避免某些容器过度占用磁盘资源影响其他容器。 网络带宽限制\n：\n通过 Cgroups，容器的网络带宽也可以被控制和限制。 总结：Cgroups 的作用是 资源限制和管理，它确保每个容器使用的资源不会超出预定的限制，防止某个容器占用过多资源导致其他容器和宿主机资源不足。\n3. Union File System（联合文件系统） # 联合文件系统（Union FS） 是一种文件系统，它允许将多个目录合并成一个虚拟的文件系统。Docker 使用联合文件系统来实现容器文件系统的共享和隔离，从而避免重复的数据存储。\n层叠存储： Docker 使用 Union File System（如 AUFS、OverlayFS、Btrfs、ZFS 等）来将多个文件系统层叠在一起，形成一个单一的文件系统。每个容器的文件系统由多个只读层和一个可写层组成。容器启动时，会从镜像中加载文件系统的只读层，而容器写入的数据会存储在容器的可写层中。 镜像共享： 镜像的每一层是不可修改的，可以被多个容器共享。这使得容器能够快速启动并节省磁盘空间，因为不同容器共享相同的镜像层，只为容器的个性化修改保留单独的可写层。 文件系统效率： 由于文件系统层是基于差异（如只读层与可写层的差异），容器不会浪费存储空间来重复存储相同的数据，从而提高了文件系统的效率和节省存储空间。 总结：联合文件系统的作用是 高效的存储和层叠文件系统，它通过文件系统的层叠存储，使得容器能够共享镜像层、节省存储空间并提高性能。\n综合总结： # Namespace 提供了容器之间的 资源隔离，每个容器都有独立的资源视图。 Cgroups 用于 资源限制，保证容器不会占用过多的 CPU、内存等资源。 Union File System 实现了容器文件系统的 共享和高效存储，通过层叠文件系统降低存储浪费，提高效率。 这三种技术共同作用，使得 Docker 能够在保证容器资源隔离、限制和高效存储的同时，提供轻量级和高性能的容器化解决方案。\n9. 命令相关：导入导出镜像，进入容器，设置重启容器策略，查看镜像环境变量，查看容器占用资源 # 导入镜像 docker load -i xx.tar 导出镜像docker save -o xx.tar image_name 进入容器docker exec -it 容器命令 /bin/bash 设置容器重启策略启动时 \u0026ndash;restart选项 查看容器环境变量 docker exec {containerID} env 查看容器资源占用docker stats test2 以下是 Docker 常用命令及其用途，帮助你完成常见操作：\n1. 导入和导出镜像 # 导入镜像（从 tar 包导入镜像）：\n将一个 tar 文件（例如备份的镜像）导入到 Docker 本地镜像中：\ndocker import /path/to/your/image.tar 你也可以指定标签（tag）：\ndocker import /path/to/your/image.tar my_image:tag 导出镜像（将容器文件系统导出为 tar 包）：\n从容器中导出文件系统为 tar 包：\ndocker export \u0026lt;container_id\u0026gt; \u0026gt; /path/to/your/container_backup.tar 这将导出容器的文件系统，但不会导出镜像层。\n2. 进入容器 # 使用 docker exec 命令进入正在运行的容器：\ndocker exec -it \u0026lt;container_id_or_name\u0026gt; /bin/bash -i：保持标准输入流（stdin）打开，允许你与容器交互。 -t：为容器分配一个伪终端。 如果容器内没有 /bin/bash，可以尝试 /bin/sh：\ndocker exec -it \u0026lt;container_id_or_name\u0026gt; /bin/sh 3. 设置重启策略 # 使用 --restart 标志设置容器的重启策略，确保容器在特定情况下自动重启。常见的重启策略包括：\nno：默认值，容器退出时不重启。 always：容器退出时始终重启。 unless-stopped：容器退出时重启，除非它被手动停止。 on-failure：只有在容器非零退出时才会重启。 例如，创建一个总是重启的容器：\ndocker run -d --restart always my_image 或者，修改现有容器的重启策略：\ndocker update --restart always \u0026lt;container_id_or_name\u0026gt; 4. 查看镜像环境变量 # 要查看镜像中的环境变量，可以使用以下命令：\ndocker inspect --format \u0026#39;{{.Config.Env}}\u0026#39; \u0026lt;image_id_or_name\u0026gt; 这将返回镜像中定义的环境变量列表。\n如果你想查看容器的环境变量，可以执行：\ndocker exec \u0026lt;container_id_or_name\u0026gt; printenv 这将显示容器内所有的环境变量。\n5. 查看容器占用资源 # 要查看容器的资源使用情况，可以使用以下命令：\ndocker stats \u0026lt;container_id_or_name\u0026gt; 这将显示容器的 CPU 使用率、内存使用、网络 I/O 等实时信息。\ndocker stats 命令会持续输出资源使用情况，直到你按 Ctrl + C 停止。\n要查看所有容器的资源使用情况，可以不指定容器 ID，直接运行 docker stats：\ndocker stats 这个命令显示的是容器实时的资源消耗情况，可以帮助你识别资源瓶颈。\n总结：\n导入镜像：docker import 导出容器文件系统：docker export 进入容器：docker exec -it \u0026lt;container_id\u0026gt; /bin/bash 设置重启策略：--restart 或 docker update --restart 查看镜像环境变量：docker inspect --format '{{.Config.Env}}' 查看容器占用资源：docker stats 10. 构建镜像有哪些方式？ # dockerfile 容器提交为镜像 构建 Docker 镜像的方式有多种，常见的方式包括使用 Dockerfile、通过 docker commit 命令以及从已有镜像创建自定义镜像。以下是详细介绍：\n1. 通过 Dockerfile 构建镜像 # 最常见的方式是使用 Dockerfile 构建镜像。Dockerfile 是一组指令，用于定义镜像的构建过程。每一条指令都会创建一个新的镜像层。\n步骤：\n编写 Dockerfile，并指定基础镜像、安装应用、配置环境等。 使用 docker build 命令来构建镜像。 示例 Dockerfile：\n# 使用官方的 Ubuntu 镜像作为基础镜像 FROM ubuntu:20.04 # 设置维护者信息 LABEL maintainer=\u0026#34;yourname@example.com\u0026#34; # 安装必要的软件包 RUN apt-get update \u0026amp;\u0026amp; apt-get install -y \\ curl \\ vim # 设置工作目录 WORKDIR /app # 复制文件到容器中 COPY . /app # 设置容器的启动命令 CMD [\u0026#34;python3\u0026#34;, \u0026#34;app.py\u0026#34;] 构建命令：\ndocker build -t my_image_name:tag . -t my_image_name:tag：指定镜像的名称和标签（例如 my_image_name:latest）。 .：指定 Dockerfile 的路径，. 表示当前目录。 2. 使用 docker commit 构建镜像 # 如果你已经运行了一个容器，并且对容器进行了某些修改（比如安装了软件包或改变了配置），你可以通过 docker commit 命令将容器的当前状态保存为一个新的镜像。\n步骤：\n启动一个容器并对其进行修改。 使用 docker commit 命令将容器的当前状态保存为一个新的镜像。 命令：\ndocker commit \u0026lt;container_id_or_name\u0026gt; \u0026lt;new_image_name\u0026gt;:\u0026lt;tag\u0026gt; container_id_or_name：容器的 ID 或名称。 new_image_name:tag：要创建的镜像的名称和标签。 示例：\n假设你已经运行了一个容器，并且在容器中安装了一个新的软件包：\ndocker commit my_container my_new_image:v1 这样，你就创建了一个新的镜像 my_new_image:v1，其中包含你在容器中所做的修改。\n3. 从现有镜像创建自定义镜像 # 你也可以基于现有的镜像，手动修改容器并保存为新的镜像。这通常是通过交互式地进入容器进行修改，然后使用 docker commit 命令保存这些修改。\n步骤：\n启动一个容器并进入容器。 在容器内部进行修改。 使用 docker commit 保存容器的状态为新镜像。 命令：\n启动容器：\ndocker run -it ubuntu:20.04 /bin/bash 在容器内做一些修改（例如安装软件）：\napt-get update \u0026amp;\u0026amp; apt-get install -y curl 退出容器后，使用 docker commit 创建一个新的镜像：\ndocker commit \u0026lt;container_id\u0026gt; my_custom_image:v1 4. 通过 docker buildx 构建多平台镜像 # docker buildx 是 Docker 提供的一个构建工具，支持构建跨平台的多架构镜像（如 ARM、x86）。\n步骤：\n使用 docker buildx 创建支持多平台的镜像。 需要安装 buildx 插件并启用多平台支持。 示例：\ndocker buildx create --use docker buildx build --platform linux/amd64,linux/arm64 -t my_multiarch_image . 5. 使用 BuildKit 构建镜像 # Docker BuildKit 是 Docker 18.09 引入的一个功能，提供了更快、更高效、更灵活的构建方式。\n启用 BuildKit：\n通过设置环境变量启用 BuildKit：\nexport DOCKER_BUILDKIT=1 然后使用 docker build 命令构建镜像：\ndocker build -t my_image_name:tag . 优势：\n支持缓存优化、并行构建、以及更精细的构建控制。 提供更强的支持来构建多阶段构建（multi-stage builds）。 6. 使用 Docker Compose 构建镜像 # 如果你使用 Docker Compose 来管理多个容器，可以通过 docker-compose.yml 文件来定义镜像的构建过程。\n步骤：\n在 docker-compose.yml 文件中定义服务。 使用 docker-compose build 构建镜像。 示例 docker-compose.yml 文件：\nversion: \u0026#39;3\u0026#39; services: web: build: . ports: - \u0026#34;80:80\u0026#34; build: . 表示使用当前目录下的 Dockerfile 构建镜像。 构建镜像：\ndocker-compose build 总结： # Dockerfile 构建镜像：标准的镜像构建方式，灵活、可复用，适用于复杂的镜像构建。 docker commit 构建镜像：通过修改容器并保存为镜像，适用于快速创建镜像，通常不推荐用于生产环境。 现有镜像创建自定义镜像：手动修改容器后，通过 docker commit 保存修改，适用于基于已有镜像做定制化开发。 docker buildx 构建多平台镜像：支持跨平台架构构建，适用于构建多种平台的镜像。 Docker BuildKit：提供更高效的构建，支持并行构建和缓存优化，适用于复杂的构建场景。 Docker Compose 构建镜像：适用于多容器应用的镜像构建和管理。 这些方法可以根据项目需求、工作流程和团队协作的需要来选择。\n11. docker和vmware虚拟化区别？ # Docker 和 VMware 都是虚拟化技术，但它们的工作原理和使用场景有很大的不同。以下是它们的主要区别：\n1. 虚拟化类型 # Docker（容器化）：Docker 是一种 操作系统级虚拟化 技术，基于 Linux 内核的 cgroups 和 namespace 实现。容器在共享宿主机操作系统内核的基础上运行，每个容器是一个独立的用户空间环境，但它们共享宿主操作系统的内核。 VMware（虚拟机）：VMware 是一种 硬件级虚拟化 技术，使用虚拟机管理程序（Hypervisor）直接在硬件上创建和管理多个虚拟机（VM）。每个虚拟机都包含一个完整的操作系统，包括内核和用户空间。 2. 资源开销 # Docker：容器是共享宿主操作系统内核的，因此启动速度快，资源占用低，不需要为每个容器分配完整的操作系统。Docker 容器的资源开销相对较小。 VMware：虚拟机包含完整的操作系统和虚拟化的硬件资源，因此每个虚拟机的启动速度较慢，资源开销也较大。每个虚拟机都需要分配一定的内存、CPU、存储等硬件资源。 3. 启动时间 # Docker：容器启动速度非常快，通常只需几秒钟。因为容器仅包含运行应用程序所需的文件系统和依赖，它们不会像虚拟机一样加载完整的操作系统。 VMware：虚拟机启动速度相对较慢，需要加载整个操作系统，并进行硬件初始化，通常需要几分钟。 4. 隔离级别 # Docker：容器共享宿主机操作系统的内核，容器之间的隔离度较低。虽然 Docker 使用了 cgroups 和 namespaces 来实现一定程度的资源隔离和安全性，但容器间仍然存在一定的安全风险。 VMware：虚拟机完全隔离，虚拟机之间的资源是完全独立的，每个虚拟机运行自己的操作系统和内核，因此隔离性更强，安全性较高。 5. 性能 # Docker：由于容器直接运行在宿主操作系统上，资源占用少，性能接近宿主机本身的性能。容器化应用在性能方面相对虚拟机来说更加高效。 VMware：虚拟机需要模拟硬件资源，运行完整的操作系统，因此性能相较于容器会有一定的损耗。尤其是当虚拟机数量增多时，性能下降会更加明显。 6. 操作系统支持 # Docker：容器通常需要依赖宿主操作系统的内核。例如，Docker 在 Linux 上运行的容器使用 Linux 内核，而在 Windows 上的容器则使用 Windows 内核。Docker 仅能在兼容的操作系统上运行。 VMware：虚拟机可以运行不同的操作系统（包括 Linux、Windows、macOS 等），每个虚拟机都带有完整的操作系统和内核，因此可以在同一硬件上运行不同的操作系统。 7. 应用场景 # Docker\n：\n微服务架构：由于容器启动快、资源消耗少，适合部署微服务应用。 开发和测试：开发人员可以使用 Docker 快速构建、共享和运行应用程序，方便测试和持续集成。 CI/CD：容器化环境使得持续集成和持续交付变得更加高效，能够快速迭代和部署应用。 VMware\n：\n虚拟化数据中心：VMware 是数据中心虚拟化的主流技术，适合企业级虚拟化部署。可以虚拟化整个服务器，提供资源的独立性和完整性。 多种操作系统支持：适合需要多种不同操作系统和软件栈的场景，如同时运行多个操作系统。 企业级高可用性：VMware 提供了包括高可用性、容错、灾难恢复等企业级功能。 8. 资源分配 # Docker：容器使用宿主机的操作系统内核，通常直接共享宿主机的硬件资源（CPU、内存、磁盘等）。Docker 支持资源限制（如 CPU 限制、内存限制等），但不会像虚拟机那样为每个容器分配完整的虚拟硬件资源。 VMware：每个虚拟机都有自己的虚拟硬件（包括 CPU、内存、硬盘等），需要为每个虚拟机分配特定的资源。虚拟机的资源管理通常更加独立。 9. 迁移与扩展性 # Docker：容器的迁移通常更简单，可以轻松将容器从一台机器迁移到另一台机器上。Docker 也支持在云环境中快速扩展和部署。 VMware：虚拟机迁移和扩展通常需要更多的配置和资源调配，尤其是在跨不同硬件和虚拟化平台时。VMware 提供了 vMotion 等工具来实现虚拟机迁移，但仍然比容器迁移要复杂。 总结： # Docker 更加轻量级，适合应用级别的虚拟化，资源开销小，启动速度快，适用于微服务、开发/测试环境和 CI/CD 等场景。 VMware 更适合进行硬件虚拟化，提供完全的隔离和更强的安全性，适合运行不同操作系统的虚拟机，常用于企业级数据中心和多操作系统支持的场景。 根据需求，选择适合的虚拟化技术可以更高效地利用资源。\n五、kubernetes # 1. k8s的集群组件有哪些？功能是什么？ # Kubernetes（K8s）是一个开源的容器编排平台，旨在自动化容器化应用程序的部署、扩展和管理。K8s 集群由多个组件组成，主要分为 控制平面（Control Plane）和 工作节点（Node）。以下是 Kubernetes 集群的核心组件及其功能：\n1. 控制平面组件 # 控制平面负责管理 Kubernetes 集群，处理所有的控制任务，确保集群的工作状态与期望状态一致。\n1.1 kube-apiserver # 功能：API 服务器是 Kubernetes 控制平面的入口点。它接收来自客户端（如 kubectl）、内部组件（如 kube-scheduler 和 kube-controller-manager）和外部系统的 REST 请求，并将其转发到适当的组件。API 服务器也提供了集群的配置和状态信息接口。 作用：作为集群的前端，所有的操作和请求都通过它，所有的对象都在 etcd 中存储，通过它进行 CRUD 操作。 1.2 etcd # 功能：etcd 是一个高可用的键值存储数据库，存储 Kubernetes 集群的所有配置数据、状态信息和元数据。 作用：所有的集群数据（如节点、Pod、服务等资源对象）都保存在 etcd 中。它是 Kubernetes 的 \u0026ldquo;单一数据源\u0026rdquo;，保证了集群数据的持久性。 1.3 kube-scheduler # 功能：kube-scheduler 是 Kubernetes 控制平面的一个组件，负责监控待调度的 Pod，并根据一系列调度策略将其分配到合适的工作节点上。 作用：它根据集群的资源情况、约束条件和调度策略将 Pod 分配到最合适的节点上，确保负载均衡和资源的合理利用。 1.4 kube-controller-manager # 功能：kube-controller-manager 是一个控制器的集合，管理 Kubernetes 集群中各类控制循环。每个控制器负责监视集群的某个资源，并根据需要采取行动。 作用：如：ReplicaSet 控制器确保有指定数量的 Pod 副本在运行，Deployment 控制器管理应用程序的版本升级等。 1.5 cloud-controller-manager # 功能：cloud-controller-manager 是 Kubernetes 控制平面的一个可选组件，旨在与云服务提供商的 API 进行集成。它使得 Kubernetes 可以与不同的云平台（如 AWS、GCP、Azure）进行交互。 作用：它通过与云平台的 API 交互，管理节点、负载均衡器和存储卷等云资源。 2. 工作节点组件 # 工作节点负责运行容器化应用和服务，承载实际的业务负载。\n2.1 kubelet # 功能：kubelet 是运行在每个工作节点上的代理，负责管理该节点上的容器和 Pod。它与 Kubernetes API 服务器进行通信，确保容器按照预期运行。 作用：它从 API 服务器获取 Pod 描述，并确保 Pod 中的容器处于运行状态。如果容器崩溃，kubelet 会重新启动它们。 2.2 kube-proxy # 功能：kube-proxy 是一个网络代理，运行在每个节点上，负责处理 Pod 的网络通信。它通过负载均衡将流量路由到合适的 Pod 中。 作用：提供服务抽象（如 Kubernetes 服务），并将流量从集群外部或集群内部路由到正确的 Pod。它可以使用 iptables 或 IPVS 来实现负载均衡。 2.3 Container Runtime # 功能：容器运行时是负责在节点上运行容器的工具，kubelet 会调用容器运行时来创建和管理容器。Kubernetes 支持多种容器运行时（如 Docker、containerd、CRI-O 等）。 作用：它是运行容器的基础，负责容器的生命周期管理（如拉取镜像、启动容器等）。 3. 其他组件 # 这些组件有助于集群的管理、监控和运行。\n3.1 Helm # 功能：Helm 是 Kubernetes 的包管理工具，类似于 Linux 的 apt 或 yum，它使得在 Kubernetes 中部署应用变得更加简单。 作用：Helm 使用 Charts（Kubernetes 应用程序的包）来管理和部署应用程序，支持版本控制和应用配置的共享。 3.2 Ingress Controller # 功能：Ingress Controller 负责处理进出 Kubernetes 集群的 HTTP 和 HTTPS 流量。它可以通过设置规则，将外部流量路由到集群内部的服务。 作用：它通过配置 Ingress 资源，定义外部访问 Kubernetes 服务的 URL 路由规则。 3.3 CNI 插件 # 功能：CNI（Container Network Interface）插件提供容器网络功能，它决定了 Pod 和服务之间如何进行通信。 作用：它是容器网络的重要组成部分，负责容器之间的网络连接和网络策略的执行。常见的 CNI 插件有 Flannel、Calico、Weave 等。 3.4 监控与日志管理 # 功能：监控工具（如 Prometheus 和 Grafana）和日志管理工具（如 ELK Stack、Fluentd）用于收集和展示集群及容器的监控指标、日志信息。 作用：通过监控和日志收集，可以及时发现系统问题、优化资源、提升可用性。 总结 # Kubernetes 集群的核心组件包括 控制平面组件（kube-apiserver、etcd、kube-scheduler、kube-controller-manager、cloud-controller-manager）和 工作节点组件（kubelet、kube-proxy、容器运行时）。这些组件协同工作，保证集群的高效运行与自动化管理，同时也有助于扩展、监控和维护 Kubernetes 集群。\n2. kubectl命令相关：如何修改副本数，如何滚动更新和回滚，如何查看pod的详细信息，如何进入pod交互？ # 修改副本数 kubectl scale deployment redis \u0026ndash;replicas=3 活动更新kubectl set image deployments myapp-deploy myapp=myapp:v2 回滚kubectl rollout undo deployments myapp-deploy 查看pod详细信息kubectl describe pods/ 进入pod交互kubectl exec -it -c bash 以下是 kubectl 命令相关操作，用于修改副本数、滚动更新、回滚、查看 Pod 的详细信息以及进入 Pod 进行交互。\n1. 修改副本数 # 要修改部署（Deployment）中 Pod 的副本数，可以使用 kubectl scale 命令：\nkubectl scale deployment \u0026lt;deployment-name\u0026gt; --replicas=\u0026lt;number-of-replicas\u0026gt; 例如，将名为 my-app 的部署副本数修改为 5：\nkubectl scale deployment my-app --replicas=5 2. 滚动更新 # 滚动更新是 Kubernetes 中一种无停机的更新方式，使用 kubectl apply 或 kubectl rollout 进行：\n使用 kubectl apply 更新 Deployment： # 修改 Deployment 的配置文件，然后使用 kubectl apply 更新。\nkubectl apply -f \u0026lt;deployment-file.yaml\u0026gt; 使用 kubectl rollout 进行滚动更新： # kubectl rollout restart deployment \u0026lt;deployment-name\u0026gt; 例如，重启 my-app 部署：\nkubectl rollout restart deployment my-app 3. 回滚 # 回滚到上一个版本的部署，可以使用 kubectl rollout undo 命令：\nkubectl rollout undo deployment \u0026lt;deployment-name\u0026gt; 例如，回滚 my-app 部署：\nkubectl rollout undo deployment my-app 如果你希望回滚到指定版本：\nkubectl rollout undo deployment \u0026lt;deployment-name\u0026gt; --to-revision=\u0026lt;revision-number\u0026gt; 4. 查看 Pod 的详细信息 # 要查看 Pod 的详细信息，可以使用 kubectl describe 命令：\nkubectl describe pod \u0026lt;pod-name\u0026gt; 例如，查看名为 my-app-pod 的 Pod 的详细信息：\nkubectl describe pod my-app-pod 这将显示 Pod 的详细信息，包括容器状态、事件、挂载的卷、环境变量等。\n5. 进入 Pod 进行交互 # 要进入一个运行中的 Pod 进行交互，可以使用 kubectl exec 命令。你可以进入 Pod 并执行命令（如进入 shell）：\n进入 Pod 并启动交互式终端： # kubectl exec -it \u0026lt;pod-name\u0026gt; -- /bin/bash 例如，进入 my-app-pod 的交互式终端：\nkubectl exec -it my-app-pod -- /bin/bash 如果容器使用的是 sh 而不是 bash，可以使用：\nkubectl exec -it my-app-pod -- /bin/sh 通过这些命令，你可以方便地修改副本数、执行滚动更新、查看 Pod 的详细信息以及进入 Pod 进行交互操作。\n3. etcd数据如何备份？ # etcdctl \u0026ndash;endpoints=“https://192.168.32.129:2379,https://192.168.32.130:2379,192.168.32.128:2379” \u0026ndash;cacert=/etc/kubernetes/cert/ca.pem \u0026ndash;key=/etc/etcd/cert/etcd-key.pem \u0026ndash;cert=/etc/etcd/cert/etcd.pem snapshot save snashot1.db Snapshot saved at snashot1.db etcd 数据备份是确保集群数据安全和恢复的关键步骤。etcd 提供了命令行工具来进行备份和恢复。以下是备份和恢复 etcd 数据的步骤：\n1. 备份 etcd 数据 # etcd 使用 etcdctl 工具来执行备份操作，备份是对 etcd 数据的快照。\n备份命令： # ETCDCTL_API=3 etcdctl snapshot save \u0026lt;backup-path\u0026gt; 例如，将备份保存到 /tmp/etcd-backup.db：\nETCDCTL_API=3 etcdctl snapshot save /tmp/etcd-backup.db 备份选项： # ETCDCTL_API=3：指定使用 etcd 3.x API。 \u0026lt;backup-path\u0026gt;：备份文件的路径。 确保运行该命令的机器可以访问 etcd 集群，或者你需要设置相关的认证参数（如证书和密钥）。\n2. 指定备份时使用的 etcd 集群信息 # 如果 etcd 集群有多个节点或启用了身份验证，可能需要添加额外的参数：\n--endpoints：指定 etcd 集群的节点地址。 --cert-file 和 --key-file：如果启用了 SSL 加密通信。 --cacert：指定 CA 证书的路径（用于身份验证）。 例如，备份时指定集群端点：\nETCDCTL_API=3 etcdctl snapshot save /tmp/etcd-backup.db \\ --endpoints=https://etcd1:2379,https://etcd2:2379,https://etcd3:2379 \\ --cert-file=/path/to/cert \\ --key-file=/path/to/key \\ --cacert=/path/to/ca-cert 3. 恢复 etcd 数据 # 如果需要恢复 etcd 数据，使用 etcdctl snapshot restore 命令。\n恢复命令： # ETCDCTL_API=3 etcdctl snapshot restore \u0026lt;backup-path\u0026gt; \\ --data-dir=\u0026lt;new-data-dir\u0026gt; \\ --name=\u0026lt;node-name\u0026gt; \\ --initial-cluster=\u0026lt;cluster-config\u0026gt; \\ --initial-cluster-token=\u0026lt;cluster-token\u0026gt; 例如，恢复备份到 /tmp/etcd-backup.db，并将数据存储到 /var/lib/etcd：\nETCDCTL_API=3 etcdctl snapshot restore /tmp/etcd-backup.db \\ --data-dir=/var/lib/etcd \\ --name=etcd-node-1 \\ --initial-cluster=etcd-node-1=https://etcd-node-1:2380,etcd-node-2=https://etcd-node-2:2380,etcd-node-3=https://etcd-node-3:2380 \\ --initial-cluster-token=etcd-cluster-1 --data-dir：指定恢复后的数据存储目录。 --name：指定节点的名称。 --initial-cluster：指定集群节点的地址。 --initial-cluster-token：集群的标识。 恢复操作后，需要重新启动 etcd 服务。\n4. 定期备份 # 建议定期备份 etcd 数据，以防止意外数据丢失。可以使用脚本或定时任务（如 cron）来定期执行备份操作。\n5. 检查备份有效性 # 备份后，建议检查备份文件的完整性，确保在需要时能够恢复。\n4. k8s控制器有哪些？ # 副本集（ReplicaSet） 部署（Deployment） 状态集（StatefulSet） Daemon集（DaemonSet） 一次任务（Job） 计划任务（CronJob） 有状态集（StatefulSet） Kubernetes 控制器是用于管理 Kubernetes 集群中资源和对象的控制循环（control loop）。这些控制器定期检查集群状态并根据当前状态与期望状态之间的差异进行调整。以下是 Kubernetes 中一些常见的控制器及其功能：\n1. ReplicationController # 功能：确保某个特定的 Pod 副本数始终运行。当 Pod 数量低于指定副本数时，ReplicationController 会创建新的 Pod；当 Pod 数量超过指定副本数时，它会删除多余的 Pod。 用途：确保应用程序的高可用性和负载均衡。 2. ReplicaSet # 功能：ReplicaSet 是 ReplicationController 的增强版，用于确保某个特定副本数的 Pod 始终在集群中运行。通常用于支持 Deployment 资源，确保部署的 Pod 数量正确。 用途：与 Deployment 一起工作，确保在滚动更新时的稳定性。 3. Deployment # 功能：Deployment 控制器负责管理应用程序的部署，支持零停机的滚动更新、回滚等操作。它使用 ReplicaSet 来管理 Pod 副本。 用途：简化应用程序的部署和版本管理。 4. StatefulSet # 功能：StatefulSet 是用于管理有状态应用的控制器。它确保 Pod 在更新时能够保持其稳定的标识（如 Pod 名称和存储卷），适用于有持久化存储要求的应用，如数据库。 用途：管理有状态服务，例如数据库、缓存等，提供稳定的网络身份和持久化存储。 5. DaemonSet # 功能：DaemonSet 控制器确保每个节点上运行一个特定的 Pod。通常用于需要在每个节点上运行的守护进程（如日志收集、监控代理等）。 用途：确保每个节点上都有 Pod（例如：fluentd、prometheus-node-exporter 等）。 6. Job # 功能：Job 控制器用于管理批处理任务的执行。它确保指定数量的 Pod 成功执行一次任务。Job 会创建 Pod 来执行一次性任务，完成后 Pod 会被删除。 用途：用于执行短期批处理任务，例如数据迁移、批量计算等。 7. CronJob # 功能：CronJob 是 Job 控制器的扩展，允许定期执行任务，类似于 Linux 系统中的 Cron 作业。它定期创建 Job 来执行计划任务。 用途：例如定期备份、定时任务等。 8. HorizontalPodAutoscaler (HPA) # 功能：HPA 控制器根据 Pod 的 CPU 或内存使用情况自动调整 Pod 的副本数，以应对负载的变化。 用途：自动水平扩展应用程序的 Pod，适应流量的波动。 9. NetworkPolicy # 功能：NetworkPolicy 控制器用于定义和管理 Pod 之间的网络访问策略。它允许你控制哪些 Pod 可以通信，哪些不能。 用途：增强集群内的安全性，通过限制 Pod 之间的网络流量来隔离不同服务。 10. IngressController # 功能：IngressController 控制器管理 Ingress 资源，负责将外部请求路由到集群内的服务。它通常使用反向代理的形式，如 Nginx 或 Traefik，来处理 HTTP/S 流量。 用途：暴露服务给外部网络，并提供负载均衡、SSL 终止等功能。 11. Custom Controller # 功能：Kubernetes 允许用户创建自定义控制器，以便根据特定的业务需求自动化资源管理。用户可以根据自己的需求编写自定义控制器，并将其作为 Kubernetes 的一部分运行。 用途：满足一些特定需求的自动化任务，例如定期清理日志、资源调整等。 12. CertManager # 功能：CertManager 是一个 Kubernetes 控制器，用于自动化管理和签发 SSL/TLS 证书。它通过与外部证书颁发机构（如 Let\u0026rsquo;s Encrypt）交互来自动更新证书。 用途：为 Kubernetes 集群中的应用提供自动化证书管理。 总结 # Kubernetes 控制器的核心作用是确保集群中的资源按照预期方式运行，并自动处理集群资源的管理与维护。不同的控制器满足了不同场景下的需求，包括无状态服务的管理（Deployment），有状态服务的管理（StatefulSet），批处理任务的管理（Job、CronJob），以及对网络和安全的管理（NetworkPolicy、IngressController）等。\n5. 哪些是集群级别的资源？ # Namespace Node Role ClusterRole RoleBinding ClusterRoleBinding 在 Kubernetes 中，集群级别的资源是指跨整个集群范围的资源，它们的作用通常涉及对集群的管理、调度和配置。以下是一些常见的集群级别资源：\n1. Node # 功能：节点（Node）是 Kubernetes 集群中的物理或虚拟机器，它为 Pod 提供计算资源。每个节点都包含运行 Pod 的 Kubelet、容器运行时（如 Docker）以及其他必要的组件。 用途：管理集群中的计算资源，决定 Pod 的调度位置。 2. Namespace # 功能：命名空间（Namespace）用于在 Kubernetes 中划分集群中的资源。通过使用不同的命名空间，可以将资源分隔开，以便于不同团队或项目的管理。 用途：用于逻辑上分隔和组织集群中的资源，例如服务、Pod 和其他对象，可以在同一集群中支持多租户环境。 3. PersistentVolume (PV) # 功能：持久化卷（PersistentVolume）是集群级别的存储资源，用于提供持久化的存储，以便容器在 Pod 之间可以持久化数据。PV 是由管理员创建的，管理员定义了存储的大小、访问模式等属性。 用途：为集群中的 Pod 提供存储卷。 4. PersistentVolumeClaim (PVC) # 功能：持久化卷声明（PVC）是用户请求存储资源的方式。它是 Pod 访问持久化存储的接口，用户可以通过 PVC 来申请持久化存储，PVC 会与 PV 绑定。 用途：在集群中申请并绑定到合适的 PV，确保 Pod 获得持久化存储。 5. StorageClass # 功能：存储类（StorageClass）是管理员定义的存储策略，用于动态地供应存储卷。它定义了存储的类型、访问模式和其他属性。 用途：为 PVC 提供具体的存储供应方式，控制持久化存储的分配和使用策略。 6. ClusterRole 和 ClusterRoleBinding # 功能：ClusterRole 和 ClusterRoleBinding 是与权限管理相关的集群级别资源。ClusterRole 定义了一组集群范围内的权限，而 ClusterRoleBinding 将这些权限授予用户、组或服务账户。 用途：管理集群级别的访问控制和权限。 7. ConfigMap # 功能：ConfigMap 用于存储集群级别的配置信息。它允许您将配置数据与应用程序容器分离，便于管理和更新。 用途：存储应用程序配置或集群配置，不包含敏感数据。 8. Secret # 功能：Secret 用于存储敏感数据（如密码、密钥等）。它提供了一种安全的方式来存储和访问敏感信息。 用途：存储和管理集群中应用程序的敏感信息。 9. Ingress # 功能：Ingress 是集群级别的资源，用于管理外部 HTTP/S 流量的路由。它提供了一种通过负载均衡器将外部请求路由到集群内部服务的方式。 用途：暴露服务到集群外部，并通过反向代理进行流量控制。 10. ResourceQuota # 功能：ResourceQuota 用于限制命名空间内可用的资源数量和种类（如 CPU、内存、存储等）。它是在集群级别对资源使用进行管控的机制。 用途：帮助管理员限制和控制集群中资源的使用，防止某些命名空间消耗过多资源。 11. LimitRange # 功能：LimitRange 是用于限制命名空间中容器资源（如 CPU、内存）的请求和限制值。它是集群级别的资源，用于设置资源的默认限制。 用途：为命名空间内的容器设置资源使用的范围。 12. HorizontalPodAutoscaler (HPA) # 功能：HPA 控制器在集群级别运行，自动调整 Pod 的副本数以响应负载变化，通常基于 CPU 或内存使用情况。 用途：动态调整 Pod 副本数，确保集群资源能够自动适应流量的变化。 13. ServiceAccount # 功能：ServiceAccount 是 Kubernetes 中用于为 Pod 提供身份的集群级别资源。它关联了 API 访问权限，允许 Pod 在集群中进行身份验证和访问控制。 用途：为运行在 Kubernetes 上的应用程序分配身份，允许它们安全地访问集群中的资源。 14. ClusterOperator # 功能：ClusterOperator 是一个由 Operator 管理的集群级别资源，用于对集群进行自动化管理。它负责维护应用程序或基础设施的生命周期管理。 用途：简化集群管理过程，自动化常见任务。 15. API Aggregation Layer (API Server) # 功能：API Aggregation Layer 使得不同版本和功能的 API 被聚合到 Kubernetes API Server 中，这样可以让用户或系统使用不同的 API 资源。 用途：为集群提供统一的 API 接入点。 总结 # 集群级别的资源一般是跨整个 Kubernetes 集群的管理资源，涉及集群的配置、权限管理、存储管理等。它们通常由管理员来配置和管理，以确保集群内资源的合理使用、访问控制以及集群的安全性与稳定性。\n6. pod状态有哪些？ # Pending 等待中 Running 运行中 Succeeded 正常终止 Failed 异常停止 Unkonwn 未知状态 在 Kubernetes 中，Pod 的状态反映了 Pod 的生命周期和运行状况。Pod 状态由 Kubelet 和 API Server 更新，并且每个 Pod 会经历不同的阶段。以下是 Pod 状态的常见类型：\n1. Pending # 描述：Pod 已经被 Kubernetes 调度到某个节点上，但某些容器还没有被创建或启动。通常是因为容器镜像正在拉取、资源请求尚未满足、或者是其他初始化过程尚未完成。\n可能的原因\n：\n节点资源不足 容器镜像还在下载 需要挂载的卷还未准备好 2. Running # 描述：Pod 中的容器正在运行，并且至少有一个容器处于运行状态。Pod 被调度到一个节点，并且容器已经启动并在运行中。\n可能的原因\n：\n容器已启动且健康 Pod 正在执行指定的任务 3. Succeeded # 描述：Pod 中的所有容器都已成功运行并退出，且退出状态码为 0。这个状态通常表示 Pod 中的任务（例如批处理任务）已经完成。\n可能的原因\n：\n容器执行任务并成功退出 没有错误退出 4. Failed # 描述：Pod 中的至少一个容器已经结束，但退出状态码非 0。该状态表示 Pod 中的容器执行过程中发生了错误或故障。\n可能的原因\n：\n容器执行错误并退出 程序崩溃或其他失败 5. CrashLoopBackOff # 描述：Pod 中的容器持续崩溃并重启。Kubernetes 会尝试重新启动容器，但容器未能成功运行。这通常是由应用程序崩溃或容器配置错误引起的。\n可能的原因\n：\n容器启动后失败并退出，Kubernetes 尝试重启容器 容器配置错误或应用程序 bug 6. Unknown # 描述：Kubernetes 无法确定 Pod 的状态。这个状态通常是由于 Kubelet 与 API Server 之间的通信中断或节点失联。\n可能的原因\n：\n节点不可用 Kubelet 或 API Server 出现问题 7. Terminating # 描述：Pod 正在终止过程中。这通常表示 Pod 正在被删除，所有容器和相关资源正在被清理。\n可能的原因\n：\n用户或控制器删除 Pod 正在执行正常的关闭过程 8. Initializing (仅适用于 StatefulSet) # 描述：Pod 正在初始化阶段。在 StatefulSet 中，当某个 Pod 启动时，容器可能会按照顺序启动，并且 Pod 的状态会经历“Initializing”阶段。\n可能的原因\n：\nStatefulSet 中的 Pod 初始化时需要等待其他 Pod 完成启动 总结 # Pod 的状态反映了其在 Kubernetes 集群中的生命周期。管理员可以通过这些状态来诊断和排查问题，比如容器崩溃、资源不足、网络问题等。理解 Pod 的各种状态有助于有效地管理和调度应用程序。\n7. pod创建过程是什么？ # 在 Kubernetes 中，Pod 的创建过程是 Kubernetes 调度和管理 Pod 的一系列步骤。Pod 是 Kubernetes 中最小的部署单位，由一个或多个容器组成。以下是 Pod 创建过程的详细步骤：\n1. 用户提交请求 # 用户通过 kubectl 或 Kubernetes API 提交一个 Pod 资源对象的定义，通常是一个 YAML 或 JSON 文件，描述了 Pod 的配置、容器、卷挂载等信息。 请求可以来自： kubectl 命令行工具 Kubernetes 控制器（如 Deployment、StatefulSet、DaemonSet 等） 其他系统（如 Helm 等） 2. API Server 接收请求 # 用户提交的 Pod 定义会被 Kubernetes API Server 接收。 API Server 将请求转化为集群中的资源对象，并将其存储在 etcd（Kubernetes 的分布式存储系统）中。 这个 Pod 对象进入 etcd 中的队列，等待调度。 3. 调度（Scheduler）选择节点 # Kubernetes Scheduler\n负责选择一个合适的节点来运行这个 Pod。调度过程基于 Pod 的资源请求、节点的资源状况、标签匹配等条件。\n资源请求（如 CPU 和内存） 节点亲和性（如节点的标签和 Pod 的调度要求） Taints 和 Tolerations（节点的污点和 Pod 的容忍度） 其他调度策略（如 Pod 的优先级和抢占） 调度器根据这些条件选择一个符合要求的节点，并将 Pod 分配到该节点。\n4. Node 接收 Pod 配置 # 一旦调度完成，API Server 将调度结果发送给选定的节点（Node）。 该节点的 Kubelet（Kubernetes 节点代理）接收到 Pod 的配置并开始处理。 5. Kubelet 创建容器 # Kubelet 在目标节点上创建 Pod。Kubelet 会根据 Pod 配置文件中的容器信息启动相应的容器。 容器的启动过程包括： 拉取镜像：Kubelet 会确保容器镜像已经被拉取到本地节点（如果镜像尚未存在）。 配置容器：Kubelet 会按照 Pod 配置设置容器的 CPU、内存、环境变量、挂载卷等配置。 启动容器：Kubelet 会通过容器运行时（如 Docker、containerd）启动容器。 6. Pod 启动和就绪检查 # 容器启动后，Kubelet 会执行 Pod 中定义的 就绪探针（Readiness Probe） 和 存活探针（Liveness Probe），以确保容器正常工作。 如果就绪探针返回成功，Pod 会被标记为 Ready 状态，表示它已经可以开始接受流量。 如果存活探针失败，Kubelet 会尝试重新启动容器。 7. 网络和服务关联 # 一旦容器启动并且健康检查通过，Kubelet 会为容器分配一个 IP 地址。 Kubernetes 内部的 CNI（容器网络接口）插件 会为 Pod 配置网络，使得 Pod 中的容器能够通过内部网络相互通信。 如果 Pod 被关联到某个 Service，Kubernetes 会更新 Service 的 Endpoints，使得流量能够通过 Service 路由到新创建的 Pod。 8. Pod 完成启动 # 一旦容器成功运行并且健康检查通过，Pod 被标记为 Running 状态。 如果是通过 Controller（如 Deployment 或 StatefulSet）创建的 Pod，控制器会继续监控该 Pod 的运行状态，以确保副本数保持一致。 9. 监控和维护 # Kubelet 会继续监控 Pod 的状态和运行情况。如果 Pod 中的容器崩溃或死锁，Kubelet 会尝试重启容器或重建 Pod。 如果 Pod 所在的节点不可用或失败，调度器会根据策略重新调度该 Pod 到其他可用节点。 总结：Pod 创建的整个流程 # 用户提交 Pod 配置，通过 API Server 接收。 调度器选择节点，确定在哪个节点上运行 Pod。 Kubelet 在节点上创建容器，并启动容器。 网络配置和服务关联，Pod 配置好网络并准备好接受流量。 Pod 运行并就绪，容器完成启动，Pod 状态转为 Running。 通过这一过程，Kubernetes 管理和调度 Pod，实现容器化应用的高效运行与自动化管理。\n8. pod重启策略有哪些？ # Pod的重启策略有3种，默认值为Always。\nAlways ： 容器失效时，kubelet 自动重启该容器； OnFailure ： 容器终止运行且退出码不为0时重启； Never ： 不论状态为何， kubelet 都不重启该容器 在 Kubernetes 中，Pod 的重启策略定义了容器如何在失败后重新启动。Kubernetes 提供了三种常见的重启策略，每种策略适用于不同的场景。Pod 的重启策略是在 Pod 的规格文件中定义的，特别是对于容器化应用，重启策略的选择非常重要。\n1. Always # 描述：无论容器退出的状态是什么，Kubernetes 都会重新启动容器。这是最常用的重启策略，适用于那些需要始终运行的服务。\n应用场景\n：\n持续运行的应用程序，例如 Web 服务、数据库、后台任务等。 默认值：如果你没有在 Pod 规范中显式设置重启策略，Kubernetes 会默认使用 Always。\n示例\n：\napiVersion: v1 kind: Pod metadata: name: example-pod spec: restartPolicy: Always containers: - name: example-container image: example-image 2. OnFailure # 描述：当容器正常退出时（即退出码为 0）不会重启容器，但如果容器因错误退出（即退出码非 0）时，Kubernetes 会尝试重新启动容器。适用于那些正常结束时无需重启的应用。\n应用场景\n：\n用于执行一次性任务的容器，如果任务失败则需要重新启动进行重试，但如果任务成功就不需要重启。 示例\n：\napiVersion: v1 kind: Pod metadata: name: example-pod spec: restartPolicy: OnFailure containers: - name: example-container image: example-image 3. Never # 描述：Kubernetes 在容器退出后不会自动重启容器。适用于那些不需要容器重新启动的场景，通常用于批处理任务或一次性作业。\n应用场景\n：\n执行完任务后容器终止，且不需要再次启动。 示例\n：\napiVersion: v1 kind: Pod metadata: name: example-pod spec: restartPolicy: Never containers: - name: example-container image: example-image 4. Pod 重启策略与控制器的关系 # 当 Pod 是由控制器（如 Deployment、StatefulSet、DaemonSet 等）管理时，Kubernetes 会自动使用 Always 重启策略，因为控制器的目标是始终保持 Pod 的期望副本数。 在这种情况下，重启策略通常不需要显式设置，因为控制器会自动处理 Pod 的调度和重启。 总结 # Always：适用于需要持续运行的应用，容器退出时会自动重启。 OnFailure：适用于有可能失败的任务，容器异常退出时重启。 Never：适用于一次性任务或不希望重启的容器，容器退出后不会重启。 选择正确的重启策略有助于 Kubernetes 高效管理容器生命周期，提高服务的可用性和容错能力。\n9. 资源探针有哪些？ # ExecAction：在容器中执行一个命令，并根据其返回的状态码进行诊断的操作称为Exec探测，状态码为0表示成功，否则即为不健康状态。 TCPSocketAction：通过与容器的某TCP端口尝试建立连接进行诊断，端口能够成功打开即为正常，否则为不健康状态。 HTTPGetAction：通过向容器IP地址的某指定端口的指定path发起HTTP GET请求进行诊断，响应码为2xx或3xx时即为成功，否则为失败。 在 Kubernetes 中，资源探针（Probes）是用来检测容器的健康状况和可用性的机制。探针帮助 Kubernetes 了解容器的状态，以便采取相应的措施，例如重新启动容器、停止请求流量等。Kubernetes 主要有三种类型的探针，分别是 存活探针（Liveness Probe）、就绪探针（Readiness Probe） 和 启动探针（Startup Probe）。\n1. 存活探针（Liveness Probe） # 目的：检查容器是否处于健康状态。如果容器不健康（即探测失败），Kubernetes 会重新启动容器。\n适用场景：适用于判断应用是否死锁、挂起或进入无法恢复的状态，及时重启容器恢复服务。\n常见配置\n：\nhttpGet：通过 HTTP 请求检查容器健康。 tcpSocket：通过 TCP 连接检查容器的健康状况。 exec：执行指定的命令检查容器是否健康。 示例\n：\nlivenessProbe: httpGet: path: /healthz port: 8080 initialDelaySeconds: 3 periodSeconds: 5 failureThreshold: 3 2. 就绪探针（Readiness Probe） # 目的：检查容器是否已准备好接受流量。如果探测失败，Kubernetes 会停止将流量路由到该容器。这个探针可以帮助 Kubernetes 确保容器处于就绪状态后才开始接收请求。\n适用场景：适用于判断容器是否准备好对外提供服务。例如，应用启动期间可能需要时间加载资源或者连接外部服务，准备好后再开始接收流量。\n常见配置\n：\nhttpGet：通过 HTTP 请求检查容器是否准备好接收流量。 tcpSocket：通过 TCP 连接检查容器是否准备好接收流量。 exec：执行指定的命令检查容器是否准备好接收流量。 示例\n：\nreadinessProbe: httpGet: path: /readiness port: 8080 initialDelaySeconds: 5 periodSeconds: 10 3. 启动探针（Startup Probe） # 目的：检查容器是否已成功启动，适用于启动时间较长的应用。如果启动探针失败，Kubernetes 会终止容器并根据重启策略重新启动容器。启动探针通常用于长时间启动的应用，如数据库或其他大型应用。\n适用场景：用于容器启动过程较长，且不希望在启动阶段因存活探针失败而误判容器不健康的情况。\n常见配置\n：\nhttpGet：通过 HTTP 请求检查容器是否成功启动。 tcpSocket：通过 TCP 连接检查容器是否成功启动。 exec：执行指定的命令检查容器是否成功启动。 示例\n：\nstartupProbe: httpGet: path: /startup port: 8080 failureThreshold: 30 periodSeconds: 5 initialDelaySeconds: 10 4. 探针配置的常见字段 # initialDelaySeconds：探针开始检查容器状态之前的延迟时间（秒）。通常在容器启动后需要等待一段时间才能开始检查。 periodSeconds：探针检查的间隔时间（秒）。即每隔多久执行一次探针。 timeoutSeconds：探针请求的超时时间（秒）。如果探针在这个时间内没有响应，则会被视为失败。 failureThreshold：探针失败的最大次数。如果探针连续失败超过该次数，容器会被认为不健康并进行重启（仅适用于存活探针）。 successThreshold：探针成功的最小次数。如果探针成功超过该次数，容器才会被认为是健康的（仅适用于就绪探针）。 5. 探针之间的区别 # 存活探针（Liveness Probe）：用于检查容器是否健康，探测失败时会触发重启。 就绪探针（Readiness Probe）：用于检查容器是否准备好接受流量，探测失败时会停止流量的路由。 启动探针（Startup Probe）：用于容器启动期间的检查，防止启动慢的应用因为存活探针误判失败，适用于启动较慢的应用。 总结 # 存活探针（Liveness Probe）：判断容器是否健康，若失败则重新启动容器。 就绪探针（Readiness Probe）：判断容器是否准备好接受流量，若失败则停止将流量路由到容器。 启动探针（Startup Probe）：判断容器是否启动完成，通常用于启动时间较长的容器。 配置探针可以帮助 Kubernetes 更加精确地管理容器的生命周期，确保容器在正常情况下提供服务，减少服务中断和故障恢复时间。\n10. requests和limits用途是什么？ # “requests”属性定义其请求的确保可用值，即容器运行可能用不到这些额度的资源，但用到时必须要确保有如此多的资源可用 ”limits”属性则用于限制资源可用的最大值，即硬限制 在 Kubernetes 中，requests 和 limits 是容器资源管理的重要配置项，用来控制容器在运行时的 CPU 和内存资源使用情况。它们可以在 Pod 的规格中为每个容器指定，帮助 Kubernetes 更好地管理资源分配和调度。以下是 requests 和 limits 的详细说明及用途：\n1. Requests # 用途：requests 是容器在运行时请求的资源量，Kubernetes 调度器基于这些请求来决定在哪个节点上运行容器。requests 表示容器正常运行时所需的最小资源。它是容器启动时必须分配的资源量，调度器将根据 requests 值来安排 Pod 运行的节点。\n功能\n：\n控制容器所需的最小资源。 用于调度容器到合适的节点上。 如果容器实际使用的资源量低于请求的资源量，系统不会强制回收资源，保证容器在该节点上运行时能有足够的资源。 例子\n：容器请求 100 毫核 CPU 和 256 MiB 内存。\nresources: requests: cpu: 100m # 100毫核 memory: 256Mi # 256 MiB 内存 2. Limits # 用途：limits 是容器允许使用的最大资源量。如果容器使用的资源超过了 limits 的限制，Kubernetes 会采取措施（例如，CPU 资源限制会导致容器被限制使用更多的 CPU，内存资源限制则可能导致容器被杀死并重启）。limits 用来防止容器在消耗大量资源时影响到其他容器和服务。\n功能\n：\n设置容器允许使用的最大资源量。 防止容器消耗过多资源，影响集群中的其他工作负载。 在内存限制（memory limit）超出时，容器会被 OOM（Out of Memory）杀死。 例子\n：容器最大使用 200 毫核 CPU 和 512 MiB 内存。\nresources: limits: cpu: 200m # 最大 200毫核 CPU memory: 512Mi # 最大 512 MiB 内存 3. Requests 和 Limits 的关系 # 请求资源 vs 限制资源\n：\nrequests 是容器实际启动时的资源需求，决定容器的调度。 limits 是容器可以消耗的最大资源量，一旦超出此限制，Kubernetes 会限制容器使用更多资源，或者在内存超过时会杀死容器。 在资源请求和限制上最好设定相同的值\n：\n如果 requests 和 limits 设置得差不多，可以确保容器能在稳定的资源使用下运行。 如果 requests 设置较低而 limits 设置较高，则容器可以在不超过 limits 的情况下，使用更多的资源，但调度时会考虑请求的最小资源。 4. CPU 和 内存的不同处理 # CPU\n：\nKubernetes 的 CPU 资源是以“毫核”计算的。requests 指定了容器启动时所需的 CPU 资源，limits 指定了容器可以使用的最大 CPU。 资源限制在 CPU 上是通过 CPU 限制 来实现的，超出时容器会被“限制”而不是终止。 内存\n：\n对于内存，requests 是容器启动时请求的内存量，而 limits 是容器能够使用的最大内存。 如果容器的内存使用超过 limits，Kubernetes 会 杀死容器，并尝试根据策略重新启动它（通常是根据 restartPolicy 来控制）。 5. 例子 # 假设你有一个 Pod，其中的容器请求 100m 的 CPU 和 200Mi 的内存，并且设置了资源限制，最多使用 200m 的 CPU 和 400Mi 的内存。\napiVersion: v1 kind: Pod metadata: name: example-pod spec: containers: - name: example-container image: example-image resources: requests: cpu: 100m # 请求 100 毫核 CPU memory: 200Mi # 请求 200 MiB 内存 limits: cpu: 200m # 限制最大 200 毫核 CPU memory: 400Mi # 限制最大 400 MiB 内存 6. 总结 # Requests：容器在启动时需要的最小资源量，决定了 Pod 调度时所需的资源，保证容器运行时有足够资源。 Limits：容器能够使用的最大资源量，防止容器过度消耗资源，影响其他容器。 设置合理的 requests 和 limits，可以提高资源的利用率，避免容器过度占用资源，也有助于 Kubernetes 集群资源的合理分配和调度。 11. kubeconfig文件包含什么内容，用途是什么？ # 包含集群参数（CA证书、API Server地址），客户端参数（上面生成的证书和私钥），集群context 信息（集群名称、用户名）。\nkubeconfig 文件是 Kubernetes 的配置文件，它包含了连接到 Kubernetes 集群所需要的信息。Kubernetes 的 kubeconfig 文件存储了与集群交互所需的认证、访问控制和连接信息。它通常用于配置 kubectl（Kubernetes 命令行工具）和其他 Kubernetes 客户端与集群的连接。\nkubeconfig 文件的内容结构 # kubeconfig 文件是一个 YAML 格式的文件，通常包含以下几部分内容：\napiVersion:\napiVersion 字段指定了 kubeconfig 文件的版本，通常是 v1。 clusters:\nclusters 字段定义了集群的配置。每个集群包括集群的名称和 Kubernetes API 服务器的地址及证书信息。通过这些信息，kubectl 可以连接到指定的 Kubernetes 集群。\n示例\n：\nclusters: - name: my-cluster cluster: server: https://my-cluster-api-server:6443 certificate-authority: /path/to/ca.crt contexts:\ncontexts 字段包含了多个上下文配置。每个上下文定义了与集群交互的方式，包括集群名称、用户、命名空间等信息。上下文是 Kubernetes 配置中的一个组合，它将集群、用户和命名空间关联起来。用户可以通过切换上下文来在不同的集群和用户环境中工作。\n示例\n：\ncontexts: - name: my-context context: cluster: my-cluster user: my-user namespace: default current-context:\ncurrent-context 字段指定当前使用的上下文名称。kubectl 会使用该上下文进行所有后续操作，直到切换到其他上下文。\n示例\n：\ncurrent-context: my-context users:\nusers 字段定义了用于访问 Kubernetes API 的用户信息。每个用户包含认证信息，例如用户名、密码、令牌或客户端证书。通过这些认证信息，kubectl 可以验证用户身份。\n示例\n：\nusers: - name: my-user user: client-certificate: /path/to/cert.crt client-key: /path/to/cert.key token: my-token kubeconfig 文件示例 # 以下是一个完整的 kubeconfig 文件示例：\napiVersion: v1 kind: Config clusters: - name: my-cluster cluster: server: https://my-cluster-api-server:6443 certificate-authority: /path/to/ca.crt contexts: - name: my-context context: cluster: my-cluster user: my-user namespace: default current-context: my-context users: - name: my-user user: client-certificate: /path/to/cert.crt client-key: /path/to/cert.key token: my-token kubeconfig 的用途 # 配置 kubectl 和其他客户端工具： kubeconfig 文件为 kubectl 等客户端工具提供了所需的集群信息和认证信息。当用户运行 kubectl 命令时，工具会自动从 kubeconfig 中获取集群的连接信息并使用正确的认证方式。 支持多个集群和上下文： 一个 kubeconfig 文件可以存储多个集群的配置信息，以及多个上下文。用户可以通过切换上下文（kubectl config use-context）来在不同的集群之间切换工作。 用户认证： kubeconfig 文件存储了与集群交互所需的认证信息，支持多种认证方式，如证书、令牌、用户名和密码等。这些认证信息保证了用户能够访问集群，并在集群中执行相应的操作。 命名空间管理： kubeconfig 文件还可以指定与集群交互时使用的命名空间。通过设置上下文中的 namespace 字段，用户可以在默认命名空间之外指定特定的命名空间。 kubeconfig 文件的管理 # 文件位置： 默认情况下，kubeconfig 文件位于用户的主目录下 ~/.kube/config。也可以通过 KUBECONFIG 环境变量指定自定义位置。 合并多个 kubeconfig 文件： 可以将多个 kubeconfig 文件合并为一个文件。kubectl 会合并并优先使用 KUBECONFIG 环境变量中指定的文件。通过配置文件中的 contexts 字段，可以同时管理多个集群的访问。 修改当前上下文： 使用 kubectl config use-context \u0026lt;context-name\u0026gt; 来切换当前上下文，这样可以在不同的集群或用户间切换操作。 总结 # kubeconfig 文件是 Kubernetes 集群管理的核心配置文件，包含了集群、用户认证和上下文等信息，用于指导客户端工具（如 kubectl）如何访问集群。通过合理配置 kubeconfig 文件，可以方便地管理多个集群和用户，同时确保操作的安全性和准确性。\n12. RBAC中role和clusterrole区别，rolebinding和 clusterrolebinding区别？ # Role 可以定义在一个 namespace 中，如果想要跨 namespace则可以创建ClusterRole，ClusterRole 具有与 Role相同的权限角色控制能力，不同的是 ClusterRole 是集群级别的 RoleBinding 适用于某个命名空间内授权，而 ClusterRoleBinding 适用于集群范围内的授权 在 Kubernetes 中，RBAC（基于角色的访问控制）用于控制用户或服务账户对集群资源的访问权限。Role 和 ClusterRole，以及 RoleBinding 和 ClusterRoleBinding 是 RBAC 中的关键对象，它们定义了权限和如何分配这些权限。它们之间的区别如下：\n1. Role 和 ClusterRole 的区别 # Role： Role 是一个集群范围内（namespace 内）的角色，定义了在特定命名空间中对资源的访问权限。它只限于该命名空间内的资源。 只能授予对特定命名空间内的资源进行访问的权限（例如 Pod、Service 等）。 典型场景：当你需要在某个命名空间内限制某个用户或服务账户的权限时使用。 ClusterRole： ClusterRole 是一个集群范围内的角色，定义了对集群中的资源进行访问的权限。它不受命名空间的限制，适用于整个集群。 可以授予集群级别的权限（例如访问集群中的节点、PersistentVolume 等资源），也可以授予跨命名空间的权限（例如访问所有命名空间中的 Pod、Deployment 等资源）。 典型场景：当你需要跨多个命名空间或集群级别的权限时使用。 2. RoleBinding 和 ClusterRoleBinding 的区别 # RoleBinding： RoleBinding 将 Role（命名空间范围内的角色）绑定到一个用户、组或服务账户，并赋予该角色定义的权限。 只能将 Role 绑定到特定命名空间内的资源上，作用范围仅限于该命名空间。 典型场景：当你想将一个特定的权限绑定到某个命名空间的用户或服务账户时使用。 ClusterRoleBinding： ClusterRoleBinding 将 ClusterRole（集群范围内的角色）绑定到一个用户、组或服务账户，并赋予该角色定义的权限。 适用于集群范围的权限，能够绑定跨多个命名空间的用户或服务账户，或授予集群级别的权限。 典型场景：当你需要为一个用户或服务账户分配跨命名空间或集群级别的权限时使用。 总结 # Role 和 ClusterRole 都定义了权限，但 Role 限制在单一命名空间内，而 ClusterRole 是集群级别的，可以跨命名空间或授予集群级别的权限。 RoleBinding 和 ClusterRoleBinding 都将角色绑定到用户或服务账户，但 RoleBinding 只在特定命名空间内有效，而 ClusterRoleBinding 可以绑定到整个集群，应用于多个命名空间或集群级别的资源。 示例 # Role（命名空间内权限）：\nkind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: namespace: default name: pod-reader rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;pods\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;] ClusterRole（集群范围权限）：\nkind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: # No namespace field needed for ClusterRole name: cluster-admin rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;nodes\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;create\u0026#34;] RoleBinding（将 Role 绑定到用户）：\nkind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: read-pods namespace: default subjects: - kind: User name: jane apiGroup: rbac.authorization.k8s.io roleRef: kind: Role name: pod-reader apiGroup: rbac.authorization.k8s.io ClusterRoleBinding（将 ClusterRole 绑定到用户）：\nkind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: admin-binding subjects: - kind: User name: jane apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.authorization.k8s.io 通过这些对象，Kubernetes 可以灵活地控制用户和服务账户在集群中的权限。\n13. ipvs为啥比iptables效率高？ # IPVS模式与iptables同样基于Netfilter，但是ipvs采用的hash表，iptables采用一条条的规则列表。iptables又是为了防火墙设计的，集群数量越多iptables规则就越多，而iptables规则是从上到下匹配，所以效率就越是低下。因此当service数量达到一定规模时，hash查表的速度优势就会显现出来，从而提高service的服务性能\nIPVS（IP Virtual Server）比 iptables 更高效的原因主要在于它们的工作原理、性能优化和用途上的差异。下面是两者效率差异的几个关键因素：\n1. 工作方式不同 # iptables： iptables 是基于包过滤的工具，主要用于流量的过滤、NAT（网络地址转换）和路由转发。它对每个传入的网络包都会进行处理，包括检查每个包的目标端口、源地址、目标地址等，来决定如何处理。 iptables 主要通过链（chains）来过滤网络包，这在复杂的网络流量处理中可能会引入较大的性能开销。 IPVS： IPVS 是一个专门为负载均衡设计的解决方案。它工作在网络层（L4），并且直接使用了内核的负载均衡机制，处理的是传入的流量，而不是每个独立的包。IPVS 通过定义虚拟服务器、负载均衡算法（如轮询、加权轮询、最少连接等），将流量定向到后端真实服务器。 IPVS 基于流（connection-based）进行处理，它一次性建立一个流的映射，而不是逐个包进行检查。这减少了不必要的处理，因此能在高并发环境中提供更高的效率。 2. 性能优化 # iptables： iptables 由于需要逐个包进行检查，其性能会受到规则复杂度、包量以及匹配条件的影响，尤其在流量大、规则多时，可能导致性能瓶颈。 IPVS： IPVS 在内核级别实现负载均衡，它在数据包到达时，会直接通过查找流表来决定如何转发流量，从而避免了对每个包的逐一处理。IPVS 通过利用内核提供的高效机制，在性能上优于 iptables。 它使用了特定的高效算法来管理流量映射，减少了计算的开销。 3. 用途差异 # iptables 主要用于包过滤、防火墙规则、NAT 和简单的路由控制，它并不专门针对负载均衡优化。 IPVS 则是专门为大规模负载均衡设计的系统，特别适用于处理大量的并发连接和高性能的流量转发。它能够处理非常高的吞吐量，尤其在服务端和客户端之间有大量的连接时，能够显著提高效率。 4. 流量转发方式 # iptables：每次收到新的连接包时，iptables 都需要进行规则检查，尤其在规则复杂或者流量大的情况下，效率会下降。 IPVS：采用的是基于连接的负载均衡，它将每个连接映射到一个后端服务器，而不是每次处理单个包。这样，在流量较大时，减少了每个包都需要处理的负担，从而提高了处理效率。 总结 # 效率高：由于 IPVS 是为负载均衡专门设计的，且基于流的转发机制，减少了对每个包的处理开销，因此在大规模流量的场景下，性能要优于 iptables。 适用场景：iptables 是通用的包过滤工具，而 IPVS 是为负载均衡设计的专用工具，在需要高性能、高吞吐量的负载均衡环境中，IPVS 更具优势。 因此，IPVS 在进行负载均衡时，能够更高效地处理大规模并发连接，而 iptables 更适合用于防火墙和包过滤任务。\n14. sc pv pvc用途，容器挂载存储整个流程是什么？ # PVC：Pod 想要使用的持久化存储的属性，比如存储的大小、读写权限等。 PV ：具体的 Volume 的属性，比如 Volume 的类型、挂载目录、远程存储服务器地址等。 StorageClass：充当 PV 的模板。并且，只有同属于一个 StorageClass 的 PV 和 PVC，才可以绑定在一起。当然，StorageClass 的另一个重要作用，是指定 PV 的 Provisioner（存储插件）。这时候，如果你的存储插件支持 Dynamic Provisioning 的话，Kubernetes 就可以自动为你创建 PV 了。 在 Kubernetes 中，SC（StorageClass）、PV（PersistentVolume） 和 PVC（PersistentVolumeClaim） 是管理和使用存储资源的关键组件。下面是它们的用途以及容器挂载存储的整个流程。\n1. StorageClass (SC) # 用途：\nStorageClass 是一种 Kubernetes 资源类型，它为存储提供了一个抽象层，用于定义不同类型的存储，指定如何动态地分配存储资源。 StorageClass 可以指定存储提供者、存储类型、存储的性能要求（如 IOPS，延迟等）、存储的生命周期等。 它允许用户通过声明 PVC 时指定存储类型（例如高性能存储、标准存储等），从而实现存储的动态管理。 2. PersistentVolume (PV) # 用途：\nPersistentVolume 是一个持久化存储资源，表示一个已分配给 Kubernetes 集群的存储资源。 它的生命周期独立于 Pod。也就是说，即使 Pod 被销毁，PV 仍然存在，数据不会丢失。 PV 可以是由管理员手动创建，也可以通过 StorageClass 动态创建。它通常与底层存储（如本地磁盘、NFS、云存储等）关联。 3. PersistentVolumeClaim (PVC) # 用途：\nPersistentVolumeClaim 是用户对存储资源的请求，它指定了存储的大小、访问模式等。 PVC 允许用户声明他们需要的存储资源，并与集群中的 PV 进行绑定。 PVC 是对存储资源的需求声明，Kubernetes 会根据 PVC 的请求找到一个合适的 PV 并进行绑定。 4. 容器挂载存储的整个流程 # 容器挂载存储的整个流程包括以下步骤：\n1. 定义 StorageClass（可选） # 用户定义一个 StorageClass，指定动态分配存储的细节，如存储类型、参数等。如果不指定，Kubernetes 默认使用标准的存储类。\n示例：\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: fast-storage provisioner: kubernetes.io/aws-ebs parameters: type: gp2 2. 定义 PVC # 用户创建一个 PersistentVolumeClaim，它声明了对特定大小的存储的需求，并可以指定存储的访问模式（如 ReadWriteOnce, ReadOnlyMany 等）。\nKubernetes 根据 PVC 请求来寻找合适的 PV 进行绑定。\n示例：\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-claim spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: fast-storage 3. 动态创建 PV（如果没有现成的 PV） # 如果没有符合 PVC 的 PV，Kubernetes 会根据 PVC 中的 StorageClass 自动创建一个 PV。 创建的 PV 会根据 StorageClass 中指定的存储类型和参数进行配置。 4. PVC 和 PV 绑定 # Kubernetes 会将符合条件的 PersistentVolumeClaim 与 PersistentVolume 进行绑定。PVC 和 PV 绑定后，存储资源就分配给用户使用。 5. 挂载到 Pod # 在 Pod 配置文件中，用户声明需要挂载的 PVC，Kubernetes 会将绑定的 PV 挂载到 Pod 中的容器。\n示例：\napiVersion: v1 kind: Pod metadata: name: mypod spec: containers: - name: mycontainer image: nginx volumeMounts: - mountPath: /usr/share/nginx/html name: my-pvc volumes: - name: my-pvc persistentVolumeClaim: claimName: my-claim 6. 容器访问存储 # 容器内的应用程序可以通过指定的挂载路径（如 /usr/share/nginx/html）来访问持久化存储中的数据。 存储会在 Pod 生命周期内持续存在，即使 Pod 被删除，数据仍然保存在 PV 中，除非指定 PV 被删除。 总结： # StorageClass：定义存储的类型和配置。 PersistentVolume (PV)：表示一个具体的存储资源，生命周期与 Pod 无关。 PersistentVolumeClaim (PVC)：用户请求存储资源，它将请求映射到 PV。 Pod挂载存储：Pod 配置中引用 PVC，Kubernetes 将 PV 挂载到容器中。 这个流程通过 Kubernetes 的存储管理实现了对存储资源的动态配置和管理，使得容器化应用可以方便地使用持久化存储。\n15. nginx ingress的原理本质是什么？ # ingress controller通过和kubernetes api交互，动态的去感知集群中ingress规则变化， 然后读取它，按照自定义的规则，规则就是写明了哪个域名对应哪个service，生成一段nginx配置， 再写到nginx-ingress-controller的pod里，这个Ingress controller的pod里运行着一个Nginx服务，控制器会把生成的nginx配置写入/etc/nginx.conf文件中， 然后reload一下使配置生效。以此达到域名分配置和动态更新的问题。 Ingress 是 Kubernetes 中用于管理外部访问集群内服务的资源类型。其本质是提供一种 HTTP 和 HTTPS 路由机制，允许从外部网络访问集群内的应用服务。Ingress 资源配置了一些路由规则，这些规则将外部请求根据 URL 路径或主机名转发到集群中的不同服务。\nIngress 的原理本质： # 反向代理： Ingress 实际上是一个 反向代理，它处理来自外部的请求，将这些请求根据配置的规则路由到集群中的相应服务。 例如，可以根据请求的路径（如 /api、/web）或主机名（如 api.example.com、www.example.com）来决定将请求转发到哪个服务。 基于规则的路由： Ingress 通过定义一组规则来指定如何路由流量。这些规则可以基于 HTTP 请求的 host、path，或者基于其他请求头进行路由。 这些规则会由 Ingress 控制器（如 NGINX Ingress Controller）解析，并根据规则将请求转发到相应的后端服务。 Ingress Controller： Ingress Controller 是一个处理实际请求的组件，它根据定义在 Ingress 中的规则将流量路由到相应的服务。 Ingress 本身并不处理流量，它只是一个资源类型。真正处理流量的逻辑由 Ingress Controller 实现。常见的 Ingress Controller 有 NGINX、Traefik、HAProxy、Istio 等。 Ingress Controller 实际上在 Kubernetes 集群中运行，是集群外部流量的入口点。 SSL/TLS 终结： Ingress 还可以配置 HTTPS 路由，允许在入口处终止 SSL/TLS 连接。这意味着 Ingress Controller 负责解密 HTTPS 请求，然后将明文的 HTTP 请求转发到集群内的服务。 负载均衡： Ingress 也支持负载均衡，可以将流量均匀地分发到多个后端服务实例。 在一些 Ingress Controller（如 NGINX）中，负载均衡的方式可以是轮询（Round Robin）、最少连接（Least Connections）等。 Ingress 路由流程： # 外部用户通过浏览器或其他 HTTP 客户端发送请求，指定的主机名和路径。 这些请求首先由外部的负载均衡器或 DNS 服务传递到集群中的 Ingress Controller。 Ingress Controller 根据 Ingress 资源中定义的规则对请求进行匹配。 Ingress Controller 根据匹配的规则将请求转发到正确的服务和相应的 Pod。 服务和 Pod 响应请求并返回数据，数据最终通过 Ingress Controller 返回给外部客户端。 总结： # Ingress 的本质是提供一种 基于 HTTP(S) 请求的路由机制，通过 Ingress Controller 实现将外部请求转发到集群内的服务，支持负载均衡、路径路由、主机名路由以及 SSL/TLS 终结等功能。\n16. 描述不同node上的Pod之间的通信流程 # 在 Kubernetes 中，不同节点（Node）上的 Pod 之间的通信是通过 Kubernetes 网络模型来实现的。根据 Kubernetes 的网络模型，集群内的每个 Pod 都有一个唯一的 IP 地址，并且任何 Pod 都可以与其他 Pod 进行通信，而无需考虑它们是否位于同一个节点上。\n以下是不同 Node 上的 Pod 之间通信的基本流程：\n1. Kubernetes 网络模型 # Kubernetes 的网络模型确保了以下几点：\n每个 Pod 在集群中都有一个唯一的 IP 地址。 每个 Pod 可以与其他 Pod 通信，不论它们位于同一节点还是不同节点。 网络插件（如 Calico、Flannel、Weave 等）负责提供集群内部的网络连接。 2. 通信流程 # 假设有两个 Pod，分别位于两个不同的 Node 上，Pod1 位于 Node1 上，Pod2 位于 Node2 上。Pod1 想要访问 Pod2：\n步骤 1：DNS 查询或直接 IP 访问 # 如果 Pod1 知道 Pod2 的 IP 地址，它可以直接通过该 IP 地址进行通信。 如果 Pod1 通过 DNS 进行通信（Kubernetes 中的服务会为 Pod 提供 DNS 名称解析），则 Pod1 会查询 Kubernetes DNS 服务来解析 Pod2 所在服务的 IP 地址。 步骤 2：请求到达节点的网络插件 # 当 Pod1 发送请求时，请求首先到达 Pod1 所在节点（Node1）的网络插件。网络插件负责将请求发送到 Kubernetes 集群中其他节点。 如果 Pod1 访问的目标 Pod2 在 Node2 上，Node1 的网络插件会将请求转发给 Node2。 步骤 3：跨节点通信（通过 Overlay 网络或路由） # 在跨节点通信中，网络插件（如 Flannel、Calico、Weave 等）会为 Pod 之间的流量提供\nOverlay 网络\n或直接使用\n路由\n。\nOverlay 网络：网络插件会创建一个虚拟网络，将每个节点的虚拟网络接口通过隧道（如 VXLAN 或 GRE）连接起来。这允许不同节点上的 Pod 通过虚拟网络进行通信，即使它们位于物理上不同的服务器上。 直接路由：某些网络插件可能会采用直接路由的方式，直接使用集群内的路由来实现跨节点的流量转发。 步骤 4：Node2 上的网络插件接收请求 # 请求到达 Node2 后，Node2 上的网络插件会处理该请求，并将其转发到 Pod2。 网络插件确保请求通过正确的路由或隧道送到 Pod2。 步骤 5：Pod2 响应请求 # Pod2 接收到请求后，处理该请求并返回响应。响应过程与请求过程类似，Pod2 通过其所在节点的网络插件将响应发送回 Pod1。 步骤 6：返回响应 # 网络插件会将 Pod2 的响应传递给 Pod1，最终请求和响应通过网络插件成功传递到 Pod1。 3. 使用 Service 实现 Pod 访问 # 在 Kubernetes 中，直接通过 Pod 的 IP 地址进行通信并不推荐，通常通过 Kubernetes 的 Service 机制来实现 Pod 之间的通信。Service 提供了负载均衡和 DNS 解析服务，可以跨多个 Pod 实现高可用。\nService 的作用： # DNS 解析：Kubernetes 会为 Service 分配一个 DNS 名称（如 my-service.default.svc.cluster.local），其他 Pod 可以通过这个 DNS 名称访问该 Service。 负载均衡：Service 会将请求均衡地转发到其背后所有 Pod，无论这些 Pod 是否位于同一个节点上。 透明通信：Service 对客户端透明，它会根据 Pod 的 IP 地址变化动态更新后端 Pod 的列表，确保负载均衡。 4. 安全性和网络策略 # 在跨节点通信时，Kubernetes 还可以通过 网络策略（Network Policies）来限制或允许不同 Pod 之间的通信。例如，可以限制某些 Pod 只能与同一命名空间下的 Pod 通信，或者允许特定的 Pod 之间的流量通过防火墙策略控制。\n总结： # Kubernetes 网络模型确保每个 Pod 都有一个唯一的 IP 地址，并支持跨节点通信。 跨节点通信通过网络插件实现，通常采用 Overlay 网络（如 VXLAN）或直接路由。 Service 提供了 Pod 间的高层通信抽象，支持 DNS 名称解析和负载均衡。 网络策略可以控制 Pod 间的访问权限，增强集群的安全性。 这个过程确保了在不同 Node 上的 Pod 之间能够高效、安全地进行通信。\n17. k8s集群节点需要关机维护，需要怎么操作 # 进行pod驱逐：kubelet drain \u0026lt;node_name\u0026gt; 检查node上是否无pod运行，切被驱逐的pod已经在其他节点运行正常 关机维护 开机启动相关服务（注意启动顺序） 解除node节点不可调度：kubectl uncordon node 创建测试pod，并使用节点标签测试节点可以被正常调度 在 Kubernetes 集群中，如果需要对某个节点进行关机维护，必须采取一些操作步骤来确保服务不中断，同时避免出现 Pod 的意外丢失或服务中断。以下是进行节点关机维护的推荐步骤：\n步骤 1：驱逐节点上的 Pod # 首先，需要将节点上的 Pod 移动到其他健康节点。Kubernetes 会在节点关闭时自动将 Pod 从该节点驱逐，但为了保证在维护时不会影响服务，可以提前手动驱逐 Pod。\n标记节点为不可调度 将该节点标记为 cordon，使 Kubernetes 不再在该节点上调度新的 Pod：\nkubectl cordon \u0026lt;node-name\u0026gt; 驱逐节点上的 Pod 使用 kubectl drain 命令来驱逐该节点上的所有 Pod（除了 DaemonSet 和 PodDisruptionBudget 控制的 Pod）：\nkubectl drain \u0026lt;node-name\u0026gt; --ignore-daemonsets --delete-local-data --ignore-daemonsets：保留由 DaemonSet 管理的 Pod。 --delete-local-data：删除节点上没有持久化存储的 Pod（如有本地存储）。 该命令会将该节点上的 Pod 逐个迁移到集群中的其他健康节点。\n等待 Pod 被迁移 等待 Kubernetes 将节点上的 Pod 调度到其他健康的节点，确保 Pod 状态恢复正常。\n步骤 2：检查集群状态 # 在节点被驱逐之后，需要检查集群的状态，确保所有的服务和 Pod 都已经成功调度到其他节点上，并且没有出现新的故障。\n查看节点状态：\nkubectl get nodes 节点状态应该是 SchedulingDisabled。\n查看 Pod 的状态，确保它们都已迁移并正常运行：\nkubectl get pods --all-namespaces 步骤 3：关闭维护节点 # 在确保节点上没有重要任务运行，且其他节点上的 Pod 已经调度完毕后，可以开始关闭该节点进行维护。\n如果是物理机或者虚拟机，直接在操作系统层面执行关机命令：\nsudo shutdown -h now 如果是云环境中的虚拟机，可以通过云平台的控制台关闭该虚拟机。\n步骤 4：维护完成后重新启动节点 # 节点维护完成后，需要重新启动节点。\n启动节点 启动节点并确保它能够恢复正常运行。\n标记节点为可调度 一旦节点重启并恢复正常运行，使用以下命令将节点标记为可调度状态，使其可以再次接收 Pod：\nkubectl uncordon \u0026lt;node-name\u0026gt; 检查节点状态 确保节点恢复为 Ready 状态，表示其正常加入集群：\nkubectl get nodes 验证 Pod 状态 确保节点上的 Pod 能够正常运行。如果该节点之前有被驱逐的 Pod，可以确认它们是否已成功重新调度并运行：\nkubectl get pods -o wide 步骤 5：确保服务正常 # 维护完成后，需要确保所有的服务、Pod 和应用程序正常运行，检查是否有 Pod 无法调度或服务不可用的情况。\n注意事项： # PodDisruptionBudget (PDB)：如果你有设置 PodDisruptionBudget，确保在执行 kubectl drain 时不会违反 PDB 规则。PDB 可以保证在维护节点时，最小副本数和应用程序可用性。 DaemonSets：DaemonSet 会自动在每个节点上创建 Pod，所以在节点关闭时，它们不会被删除。维护节点时，DaemonSet 管理的 Pod 不会被驱逐。如果需要停止 DaemonSet 的 Pod，可以先删除相应的 DaemonSet。 持久化存储：确保有持久化存储的 Pod 在驱逐时能够正常挂载到其他节点，否则会出现数据丢失或服务中断的问题。 集群资源健康：维护过程中，确保集群中的资源（如 CPU、内存、网络等）能够满足其他 Pod 的需求，以避免资源瓶颈影响服务。 通过这些步骤，可以确保在节点维护期间，Kubernetes 集群中的服务不会受到影响，维护工作完成后，节点可以安全地恢复并重新加入集群。\n18. Calico和flannel区别 # Flannel（简单、使用居多）：基于Vxlan技术（叠加网络+二层隧道），不支持网络策略 Calico（较复杂，使用率少于Flannel）：也可以支持隧道网络，但是是三层隧道（IPIP），支持网络策略 Calico项目既能够独立地为Kubernetes集群提供网络解决方案和网络策略，也能与flannel结合在一起，由flannel提供网络解决方案，而Calico此时仅用于提供网络策略。 Calico 和 Flannel 都是 Kubernetes 中常用的网络插件，用于为容器提供网络连接。它们都可以实现跨 Pod 的网络通信，但是在架构、功能和性能方面有所不同。下面是它们的主要区别：\n1. 网络模型和架构 # Flannel\n：\nFlannel 是一个简单的 Overlay 网络插件。它为 Kubernetes 集群中的 Pod 提供虚拟网络，将不同节点的 Pod 通过隧道连接起来。Flannel 使用 VXLAN 或其他隧道协议来创建跨节点的虚拟网络。 Flannel 的网络模型一般是 Layer 3 Overlay，它会为每个 Pod 分配一个 IP 地址并通过隧道进行通信。 Calico\n：\nCalico 是一个功能更强大的网络插件，支持多种模式，包括 Overlay 网络 和 路由模式。Calico 的核心功能是基于 BGP（边界网关协议）进行 IP 路由，因此它可以在 Kubernetes 集群中提供高性能的 L3 网络。 Calico 支持 无代理的网络模型，通过直接路由实现容器之间的通信，这使得其在某些场景下具有更高的性能。 2. 网络类型 # Flannel\n：\nOverlay 网络：Flannel 创建一个虚拟的 Overlay 网络，通过 VXLAN、GRE 或其他协议将不同节点的网络连接起来。Flannel 会在每个节点上创建一个虚拟网卡，利用隧道协议将 Pod 之间的流量转发。 Calico\n：\nOverlay 网络：Calico 也支持 Overlay 模式，但它同时支持 路由模式，即不需要额外的隧道协议。在路由模式下，Calico 会通过直接路由的方式连接集群中的不同节点。这使得它的性能在大多数情况下比 Flannel 要好。 无代理模式：Calico 不使用网络代理（如 Flannel 中的 VXLAN）来转发流量，而是直接使用主机路由表，从而减少了网络开销。 3. 性能 # Flannel： Flannel 的性能受限于其使用的隧道协议（如 VXLAN）。虽然 VXLAN 为跨节点通信提供了灵活性，但它引入了额外的开销，可能导致网络性能较差，尤其在大规模集群中。 隧道协议可能会导致额外的延迟和带宽消耗，尤其在集群规模增大时，性能可能会受到影响。 Calico： Calico 的性能通常较好，因为它通过路由模式（而非隧道）来实现节点间的通信。Calico 可以直接将流量路由到目标节点，无需通过额外的隧道协议。 在支持路由模式的情况下，Calico 提供了比 Flannel 更低的延迟和更高的吞吐量，特别是在大规模集群中。 4. 安全性 # Flannel： Flannel 本身不提供强大的安全性功能，它仅负责网络通信，通常与网络策略（Network Policies）配合使用。 安全性依赖于其他工具或服务（例如，Kubernetes 本身的网络策略）。 Calico： Calico 内置了强大的网络安全功能，包括 网络策略（Network Policies）。这些策略允许用户根据网络流量的来源和目的控制通信，从而提高集群的安全性。 Calico 支持更细粒度的流量控制，可以定义哪些 Pod 或服务可以进行通信，哪些不能，从而提升网络的安全性。 5. 网络策略 # Flannel\n：\nFlannel 本身不提供网络策略功能，但可以与 Kubernetes 的 NetworkPolicy 进行集成，来实现访问控制。 Calico\n：\nCalico 内置了强大的网络策略功能，支持复杂的访问控制和流量过滤。通过 Calico，你可以创建更细粒度的安全策略，控制哪些 Pod 或 IP 之间的流量可以通过。 6. 可扩展性 # Flannel\n：\nFlannel 设计较简单，适用于小型和中型集群，但在大规模集群中可能会受到性能瓶颈的影响。 Calico\n：\nCalico 适用于大规模集群，并且可以与 BGP（边界网关协议）等协议一起工作，实现跨数据中心的容器网络。Calico 的可扩展性比 Flannel 更强，适用于更复杂的网络需求。 7. 安装和配置 # Flannel： Flannel 安装和配置较为简单，易于上手。它通常用于简单的 Kubernetes 集群，适合没有复杂网络需求的场景。 Calico： Calico 配置更复杂一些，虽然它也支持简单的安装方式，但由于其强大的功能和灵活性，可能需要更精细的配置。特别是在使用 Calico 的网络策略时，管理员需要更多的管理和配置工作。 8. 支持的环境 # Flannel： Flannel 支持多种环境，包括本地部署的 Kubernetes 集群、云环境以及混合云环境。它也支持多种存储插件和网络插件。 Calico： Calico 除了支持 Kubernetes 外，还可以与 OpenShift、Docker EE、Mesos 等其他容器编排平台一起使用。它支持多种类型的云环境，并且可以跨数据中心实现连接。 总结： # 特性 Flannel Calico 网络模型 Overlay 网络（VXLAN, GRE） Overlay 网络或路由模式（BGP） 性能 较低（有隧道协议开销） 较高（路由模式，低延迟） 网络策略 依赖 Kubernetes 的 NetworkPolicy 内置强大的网络策略功能 安全性 基础安全性，依赖外部工具 内置强大的安全性控制，支持流量过滤 可扩展性 适合小中型集群 适合大规模集群，支持跨数据中心 安装复杂度 简单易安装 配置和安装较复杂 支持的环境 本地云环境、混合云环境 本地云环境、跨数据中心支持 选择建议： # 如果你的 Kubernetes 集群规模较小，且对性能没有严格要求，可以使用 Flannel，它安装简单，适合快速部署。 如果你需要高性能的网络、跨数据中心连接或者更复杂的网络安全控制，建议使用 Calico。它适用于更大规模的集群，并提供内置的网络策略和安全功能。 六、prometheus # 1. prometheus对比zabbix有哪些优势？ # https://blog.csdn.net/wangyiyungw/article/details/85774969**\nPrometheus 和 Zabbix 都是非常流行的监控工具，但它们的设计理念和实现方式有所不同。以下是 Prometheus 相对于 Zabbix 的一些优势：\n1. 监控模式：Pull vs Push # Prometheus：采用 Pull 模式，定期从目标（如应用、服务器、数据库、容器等）主动拉取监控数据。这使得 Prometheus 可以轻松适应动态变化的环境（如容器、微服务等），并且可以通过多种方式实现监控（例如使用 exporters 进行自定义指标采集）。 Zabbix：主要采用 Push 模式，通过代理或被监控设备主动推送数据，或者使用被动的 Pull 模式。虽然 Zabbix 支持 Push 模式，但它的监控主要还是依赖于代理的推送，因此对于大规模动态环境的适应性较差。 优势：Prometheus 的 Pull 模式使其更加适应云原生和容器化环境，尤其是 Kubernetes 等动态变化的集群环境。\n2. 时序数据存储和查询 # Prometheus：专门为时序数据设计，具有高效的时间序列数据存储引擎。Prometheus 存储的所有数据都有时间戳，允许对时间序列数据进行强大的查询和聚合操作，支持 PromQL 查询语言，可以轻松进行复杂的数据分析。 Zabbix：使用传统的关系型数据库（如 MySQL、PostgreSQL）存储数据。虽然 Zabbix 支持时间序列数据的存储，但它的存储引擎和查询语言没有 Prometheus 的时序数据库那么高效，特别是在大规模数据量下。 优势：Prometheus 的时序数据库设计和强大的查询能力使其在大数据量的监控和高效查询方面比 Zabbix 更具优势。\n3. 动态环境支持 # Prometheus：具有出色的支持云原生和容器化环境的能力。通过与 Kubernetes 的集成，可以自动发现和监控集群中的所有容器和服务。同时，Prometheus 对动态 IP、服务发现等场景有很好的适应能力。 Zabbix：虽然 Zabbix 支持动态环境，但相对于 Prometheus，它的配置和维护较为复杂，特别是在大规模动态环境（如 Kubernetes、Docker）中，Zabbix 的自动发现和动态监控能力相对较弱。 优势：Prometheus 在动态环境（如微服务、容器化）中的自动化、灵活性和可扩展性更强。\n4. 集成与生态系统 # Prometheus：具有丰富的生态系统，支持多种数据采集方式和监控工具集成。例如，通过 exporters 可以从各种第三方服务（如数据库、消息队列等）收集指标，并且可以与 Grafana 等可视化工具无缝集成，提供强大的监控面板和仪表盘。 Zabbix：虽然也有丰富的插件和集成工具，但与 Prometheus 相比，Zabbix 的集成方式和灵活性较为有限，尤其在容器化、微服务架构的集成上，Prometheus 更加友好和高效。 优势：Prometheus 的生态系统更加开放，能够轻松集成各种监控需求，尤其适用于微服务和容器化应用。\n5. 告警和通知 # Prometheus：通过 Alertmanager 提供强大的告警功能，支持灵活的告警规则、告警抑制、告警分组等功能。同时，Prometheus 的告警系统与数据查询紧密集成，能够根据 PromQL 查询结果动态触发告警。 Zabbix：提供内置的告警系统，支持自定义告警条件、自动化恢复和通知。Zabbix 的告警系统较为传统，灵活性和可扩展性略逊色于 Prometheus，尤其在大规模系统中的复杂告警配置可能需要更多的人工干预。 优势：Prometheus 的告警机制更加灵活、易于配置，并且能与监控数据和查询紧密结合，能够为云原生应用提供更好的告警体验。\n6. 安装和配置 # Prometheus：相对简单，尤其在容器化环境中，可以通过 Docker 或 Kubernetes 等快速部署 Prometheus 集群，具有很好的易用性和自动化配置能力。 Zabbix：安装和配置相对复杂，尤其在大规模分布式环境中，Zabbix 需要配置数据库、代理、服务器等多个组件，较为繁琐。 优势：Prometheus 的安装和配置更加简便，尤其在现代 DevOps 环境和云原生环境中更加高效。\n7. 资源消耗 # Prometheus：由于其设计为轻量级的时序数据存储，Prometheus 在资源消耗上相对较低，特别是在小型或中型集群中。 Zabbix：在资源消耗方面，Zabbix 可能会较为沉重，尤其是当需要通过代理收集大量数据时，资源消耗可能会增加。 优势：Prometheus 在资源消耗和扩展性方面比 Zabbix 更具优势，尤其是在大规模环境中。\n8. 水平扩展 # Prometheus：支持水平扩展，Prometheus 可以与 Thanos 或 Cortex 等工具配合，进行跨集群查询和高可用性部署。通过将多个 Prometheus 实例集成，可以支持大规模集群监控。 Zabbix：Zabbix 的扩展性较差，虽然支持分布式监控，但相比 Prometheus 的水平扩展，Zabbix 更依赖集中式架构，扩展性和灵活性较差。 优势：Prometheus 提供更好的水平扩展能力，适应大规模集群和跨区域监控。\n总结： # Prometheus 相对于 Zabbix 的优势在于：\n更适合容器化、云原生和微服务架构。 支持 Pull 模式，灵活应对动态变化的环境。 高效的时序数据库和强大的查询语言（PromQL）。 灵活的告警系统和自动化集成。 良好的生态系统与其他工具（如 Grafana）集成。 安装、配置简单，易于扩展和维护。 当然，Zabbix 也有其优点，特别是在传统 IT 环境中的监控和大规模基础设施的管理，但对于现代化的应用监控和云原生架构，Prometheus 无疑是更优的选择。\n2. prometheus组件有哪些，功能是什么？ # Prometheus 是一个开源的监控系统，专门用于监控多种微服务架构、容器化环境等场景。它由多个组件构成，每个组件都负责不同的功能。以下是 Prometheus 的主要组件及其功能：\n1. Prometheus Server # 功能：Prometheus 服务器是核心组件，负责数据的采集、存储、查询和管理。它通过 Pull 模式定期从配置的目标（如应用程序、服务或节点）拉取时间序列数据。\n主要功能\n：\n拉取数据：根据配置从 exporter 或其他 Prometheus 实例拉取指标数据。 存储：将拉取的数据存储在时序数据库中，支持高效查询。 查询：使用 PromQL（Prometheus 查询语言）提供灵活的数据查询能力。 警报：根据配置的规则，触发告警并将告警信息发送到 Alertmanager。 2. Alertmanager # 功能：Alertmanager 负责接收来自 Prometheus 的告警信息，处理告警并进行相应的通知（例如，通过邮件、Slack、PagerDuty 等）。\n主要功能\n：\n告警聚合：Alertmanager 可以聚合多个相同类型的告警，并合并重复的告警信息。 告警路由：根据告警的不同标签（如严重性、团队等）将告警路由到不同的接收器（如 Slack、邮件等）。 告警抑制：可以设置告警抑制规则，避免相同问题的重复告警。 告警分组：可以将多个告警按时间或内容分组，减少告警的噪声。 3. Prometheus Exporters # 功能：Exporters 是用于从不同的系统（如操作系统、数据库、硬件等）收集监控数据的应用程序或服务。它们将系统的度量指标暴露为 Prometheus 可以拉取的 HTTP 格式。\n常见的 Exporter\n：\nNode Exporter：用于暴露 Linux/Unix 系统级别的度量指标，如 CPU 使用率、内存使用量、磁盘 I/O 等。 Blackbox Exporter：用于测试外部服务的可用性，比如 HTTP 请求、TCP 连接等。 MySQL Exporter、Redis Exporter：用于暴露数据库或中间件的指标数据。 Kube-state-metrics：用于暴露 Kubernetes 资源（如 Pod、Node、Deployment 等）相关的度量指标。 4. Prometheus Query Language (PromQL) # 功能：PromQL 是 Prometheus 提供的查询语言，用于从 Prometheus 数据库中提取、聚合和过滤时间序列数据。\n主要功能\n：\n查询：用户可以通过 PromQL 编写复杂的查询，从 Prometheus 存储的时间序列数据中提取所需的信息。 聚合：PromQL 支持多种聚合操作（如 sum、avg、max、min 等）来对时间序列数据进行处理。 时间操作：PromQL 提供时间范围选择功能，可以指定查询的时间范围，支持相对时间（如过去 5 分钟）和绝对时间。 5. Prometheus UI # 功能：Prometheus 提供了一个 Web 界面，用户可以通过它查看查询结果、配置告警规则、查看存储的时序数据等。\n主要功能\n：\n查询功能：可以直接在界面上执行 PromQL 查询。 查看指标：查看所有可用的指标以及其时间序列数据。 配置告警：可以通过 UI 配置告警规则，进行告警通知设置。 6. Prometheus Pushgateway # 功能：Pushgateway 允许应用程序将度量指标主动推送到 Prometheus，而不是通过 Prometheus 的 Pull 方式来拉取数据。常用于短生命周期的批处理作业或定时任务（例如 ETL 作业），它们在运行时将数据推送到 Pushgateway。\n主要功能\n：\nPush：应用程序将指标推送到 Pushgateway。 Pull：Prometheus 从 Pushgateway 拉取数据。 用于短期存在的任务，避免 Prometheus 无法拉取数据的情况。 7. Thanos (可选) # 功能：Thanos 是一个用于扩展 Prometheus 的组件，提供高可用性、长期存储和全局查询的功能。Thanos 通过集成多个 Prometheus 实例，将多个 Prometheus 集群的数据汇聚到一个全局查询层。\n主要功能\n：\n高可用：Thanos 可以使 Prometheus 集群成为高可用，多个 Prometheus 实例可以同时工作，避免单点故障。 长期存储：Thanos 支持将 Prometheus 存储的数据迁移到对象存储（如 AWS S3），从而提供长期存储功能。 全局查询：Thanos 允许跨多个 Prometheus 实例查询数据，提供全局视图。 8. Cortex (可选) # 功能：Cortex 是另一种用于扩展 Prometheus 的解决方案，支持多租户、长期存储和水平扩展。它支持将 Prometheus 的时间序列数据存储在分布式对象存储（如 Amazon S3）中，并提供高可用性和扩展性。\n主要功能\n：\n多租户：Cortex 支持多租户环境，每个租户有独立的存储和查询空间。 长期存储：支持将 Prometheus 数据持久化到外部存储后端，如 S3。 水平扩展：Cortex 设计为高度可扩展，可以根据需求动态增加存储和查询能力。 总结 # Prometheus 由多个关键组件组成，各自负责不同的功能：\nPrometheus Server：负责数据的采集、存储和查询。 Alertmanager：处理告警信息并进行通知。 Exporters：从不同的服务和系统收集指标数据。 PromQL：查询 Prometheus 数据库的查询语言。 Pushgateway：允许应用主动推送数据到 Prometheus。 Thanos 或 Cortex：扩展 Prometheus 提供高可用性、长期存储和跨集群查询功能。 3. 指标类型有哪些？ # Counter（计数器） Guage（仪表盘） Histogram（直方图） Summary（摘要） Prometheus 支持多种类型的指标（metrics），每种指标类型都有不同的用途和特性。以下是 Prometheus 中常见的指标类型：\n1. Counter（计数器） # 定义：计数器是单调递增的值。它只能增加或重置为零。常用于记录事件的总次数，比如请求次数、错误次数等。\n用途：适用于记录某个事件发生的总数，如请求数、成功数、失败数等。\n特点\n：\n只能递增，无法减少。 在 Prometheus 查询中，经常与 rate() 或 increase() 函数一起使用来计算某个事件的增长速率。 示例\n：\nhttp_requests_total{status=\u0026#34;200\u0026#34;} # 记录 HTTP 请求总数（状态为 200） 2. Gauge（仪表盘） # 定义：仪表盘指标表示一个可以任意增加或减少的值。适用于表示当前状态或度量。\n用途：用于表示一些瞬时变化的值，如内存使用量、CPU 使用率、当前活跃的连接数等。\n特点\n：\n值可以递增，也可以递减。 可以表示“当前”某个状态的实时值。 示例\n：\nmemory_usage_bytes # 当前内存使用量（单位：字节） 3. Histogram（直方图） # 定义：直方图用于记录数据分布。它将数据分成多个“桶”，可以记录每个桶的计数值，并且可以计算数据的分布情况（例如请求时延的分布）。\n用途：用于统计某些值的分布，如请求时延、响应大小等。可以通过设置不同的桶（buckets）来统计不同范围内的数据。\n特点\n：\n提供多个桶，以便对数据进行分布统计。 可通过 rate() 函数计算事件的平均时间、请求时延等指标。 可以与 sum() 和 count() 一起使用计算平均值等。 示例\n：\nhttp_request_duration_seconds_bucket # HTTP 请求持续时间的桶 4. Summary（摘要） # 定义：摘要用于计算观测值的分位数（percentiles）。它是对直方图的扩展，除了提供直方图的桶计数，还提供如 0.5（50th percentile）等分位数的估算值。\n用途：用于获取值的分位数，比如计算 95% 请求的响应时延。Summary 能够直接给出某个百分位（如 90th、95th）的数据。\n特点\n：\n会提供统计数据的分位数，如 p50、p90 等。 比直方图更加精确地计算分位数，但它的存储开销和计算开销相对较高。 示例\n：\nhttp_request_duration_seconds_sum # HTTP 请求持续时间的总和 http_request_duration_seconds_count # HTTP 请求持续时间的请求计数 http_request_duration_seconds{quantile=\u0026#34;0.9\u0026#34;} # HTTP 请求持续时间的 90th 百分位 总结： # Counter：只增不减，用于表示累计的总数（例如，请求数量、错误次数）。 Gauge：可以增可以减，用于表示当前瞬时的数值（例如，CPU 使用率、内存占用）。 Histogram：用于记录数据分布，支持多个桶，适合用于计算数据的分布情况（如请求时延分布）。 Summary：用于计算分位数（百分位），能够获取某些统计值的分位数，适合用于监控响应时间等。 Prometheus 的指标类型可以根据不同的监控需求选择和使用，在云原生应用、微服务架构和容器化环境中，合理选择这些指标类型可以帮助精确地监控系统的状态和性能。\n4. 在应对上千节点监控时，如何保障性能 # 降低采集频率 缩小历史数据保存天数， 使用集群联邦和远程存储 在处理大规模 Kubernetes 集群或包含上千个节点的监控时，Prometheus 可能面临性能瓶颈，包括存储、查询延迟、数据收集速度等问题。为了保障 Prometheus 在这种大规模环境下的性能，可以采取以下几种策略：\n1. 水平扩展（Horizontal Scaling） # Prometheus 分区（Sharding）\n：\n将 Prometheus 集群水平扩展，通过多个 Prometheus 实例进行分片（shard）。每个实例负责监控集群的一部分节点，这样可以减轻单个实例的负载。 在分片时，需要在 Prometheus 配置中使用 federation 或外部聚合方式，将多个 Prometheus 实例的数据汇总到一个集中的查询接口。 多集群 Prometheus 集群\n：\n使用多个 Prometheus 实例，每个实例负责不同的数据中心或 Kubernetes 集群。然后使用 Prometheus 的 联邦（federation） 功能将多个集群中的指标汇聚到一个中央 Prometheus 进行统一查询和存储。 这种方式可以避免单个 Prometheus 实例承载全部负载，提高可扩展性和高可用性。 2. 使用 *Prometheus Operator* # Prometheus Operator 可以简化多实例 Prometheus 集群的管理。它支持自动化部署、配置管理和扩展，帮助简化 Prometheus 集群的维护。 配置 Prometheus Operator 时，可以使用 k8s CustomResourceDefinitions (CRD) 来管理 Prometheus 实例、Alertmanager 和其他组件，确保系统能够根据需求动态扩展。 3. 优化数据存储 # 长期存储（Long-Term Storage）： 对于大规模集群，单个 Prometheus 实例的存储很可能会成为瓶颈。可以将 Prometheus 数据存储与外部持久化存储（如 Thanos 或 Cortex）集成，以提供高可用和高扩展性的长期存储。 Thanos：Thanos 是一个针对 Prometheus 的聚合层，它将 Prometheus 存储与长期存储（如对象存储）结合起来，支持查询多个 Prometheus 实例的聚合数据。 Cortex：Cortex 是另一种支持 Prometheus 数据长期存储的方案，提供了分布式和可扩展的存储解决方案，适用于大规模环境。 数据压缩与降采样： 配置适当的数据采集频率，减少无用的数据采集，可以通过 scrape_interval 和 scrape_timeout 来调优。 使用 数据降采样（downsampling）技术，减少存储中的数据精度。例如，存储高精度数据只有一段时间，然后对其进行降采样。 4. 优化指标收集 # 控制 Scrape 频率： 对不同的指标设置合理的采集频率，例如，对于某些不频繁变化的指标（如主机负载、磁盘空间等），可以减少 scrape 频率，避免过多的数据采集。 分组采集指标： 将同一类型的指标聚合在一起，避免每个指标独立采集。通过调整采集配置，减少不必要的指标收集，降低负载。 使用 Pushgateway： 对于短期存在的批量作业（如批量任务或一次性任务），可以使用 Pushgateway 将数据推送到 Prometheus，而不是通过 scrape 进行轮询。Pushgateway 可以减少 Prometheus 本身的 scrape 负担。 5. 优化查询性能 # 查询优化： 为了避免 Prometheus 查询时的性能瓶颈，尽量避免查询过于复杂的多维度数据。尽量简化查询条件，减少聚合和 Join 操作。 适当调整查询缓存配置，减少不必要的实时计算。 PromQL 查询优化： 使用高效的 PromQL 查询语法，避免不必要的 rate、avg 等聚合操作。尽量将查询范围限制在必要的数据范围内，避免全量查询。 6. 引入 *Thanos* 或 *Cortex* 来实现高可用和长期存储 # Thanos 或 Cortex 是 Prometheus 的外部聚合层，提供高可用、持久化存储以及全局查询功能，可以帮助处理超大规模数据。 Thanos：将多个 Prometheus 实例的数据集中到一个全局查询层，支持对象存储后端，解决了数据冗余和高可用问题。 Cortex：与 Thanos 类似，Cortex 提供了高度可扩展的存储后端，适用于大规模 Prometheus 集群，支持多租户和跨集群查询。 7. 使用 Alertmanager 的集群模式 # 在大规模集群中，Alertmanager 的配置也需要优化。使用 Alertmanager 的集群模式，可以将告警请求分散到多个实例，从而提高告警的处理能力。 8. 使用外部高效的存储后端 # 考虑将数据存储引入更为高效的存储系统，如 分布式对象存储（例如 Amazon S3、Google Cloud Storage）以提高读写性能。 9. 优化网络 # 通过合适的网络带宽、延迟优化，确保 Prometheus 与所有节点的连接稳定，避免网络瓶颈导致数据丢失或延迟。 总结： # 在大规模 Kubernetes 集群或包含上千节点的环境中，Prometheus 的性能可以通过以下几种方式保障：\n水平扩展：通过分片和联邦部署多个 Prometheus 实例来减轻单实例负载。 长期存储解决方案：使用 Thanos 或 Cortex 进行数据的长期存储和高可用。 查询和存储优化：通过合理的查询语句、数据降采样和减少不必要的采集频率来降低系统负担。 数据收集优化：合理配置 Scrape 间隔、使用 Pushgateway 等方式减少不必要的数据流量。 监控网络性能：通过优化 Prometheus 集群的网络、存储及告警管理等各方面的配置，保障整体性能。 5. 简述从添加节点监控到grafana成图的整个流程 # 被监控节点安装exporter prometheus服务端添加监控项 查看prometheus web界面——status——targets grafana创建图表 在 Prometheus + Grafana 环境下，添加监控节点并在 Grafana 上生成图表的整个流程通常包含以下几个步骤：\n1. 安装 Prometheus 监控系统 # 安装 Prometheus：首先确保 Prometheus 已经安装并启动。Prometheus 作为监控系统，负责从各种节点（如应用服务器、容器、数据库等）收集指标。\n配置 Prometheus 配置文件（prometheus.yml）\n：配置 Prometheus 从哪些目标（节点）采集数据。\nscrape_configs\n：在\nprometheus.yml 中，使用\nscrape_configs 配置需要监控的目标节点及其端口。例如：\nscrape_configs: - job_name: \u0026#39;node_exporter\u0026#39; static_configs: - targets: [\u0026#39;\u0026lt;node-ip\u0026gt;:9100\u0026#39;] 2. 安装并配置 Node Exporter（在被监控节点上） # 安装 Node Exporter：Node Exporter 是 Prometheus 用来收集服务器硬件和操作系统指标（如 CPU 使用率、内存、磁盘等）的工具。\n启动 Node Exporter\n：在每个被监控的节点上安装并启动 Node Exporter。默认端口为\n9100 。\n# 在每个被监控的节点上启动 Node Exporter ./node_exporter 确保节点可以被 Prometheus 访问：Prometheus 需要能够访问节点的 9100 端口。\n3. Prometheus 收集数据 # Prometheus 会根据配置的 scrape_interval 定期从每个节点（通过 Node Exporter）拉取指标数据。例如，Prometheus 每 15 秒拉取一次数据。 通过 http://\u0026lt;node-ip\u0026gt;:9100/metrics 访问 Node Exporter 提供的监控数据（例如 CPU 使用率、内存、磁盘空间等）。 4. 安装 Grafana # 安装 Grafana\n：Grafana 是一个开源的数据可视化工具，它与 Prometheus 配合使用，提供图形化的监控面板。\n# 安装 Grafana sudo apt-get install grafana 5. 配置 Grafana 数据源 # 登录 Grafana\n：默认情况下，Grafana 在\nhttp://\u0026lt;grafana-server-ip\u0026gt;:3000 上提供 Web 界面。\n默认用户名和密码是 admin / admin。 添加 Prometheus 数据源\n：\n进入 Grafana 控制台后，点击左侧菜单的 “Configuration” \u0026gt; “Data Sources”。 选择 “Prometheus” 数据源，并填写 Prometheus 的 URL（例如：http://\u0026lt;prometheus-server-ip\u0026gt;:9090）。 保存并测试连接。 6. 创建仪表板和图表 # 创建仪表板\n：\n进入 Grafana 后，点击左侧菜单的 “+” \u0026gt; “Dashboard” 创建新的仪表板。 在新仪表板中，可以添加多个面板（Panel）来展示不同的监控指标。 选择指标\n：\n每个面板都可以选择一个 Prometheus 查询，Grafana 会显示该查询的实时结果。例如，查询 CPU 使用率：\navg(rate(node_cpu_seconds_total{mode=\u0026#34;user\u0026#34;}[5m])) by (instance) 选择不同的时间范围和图表类型（如时间序列图、条形图、饼图等）。\n7. 设置警报（可选） # 设置警报\n：可以为某些面板设置告警规则，当某个指标超出预定范围时，Grafana 会通过邮件、Slack、Webhook 等方式发送通知。\n在面板的设置中，进入 “Alert” 标签页，配置警报规则。 8. 查看监控数据 # 完成配置后，Grafana 会开始通过 Prometheus 数据源实时获取数据，并以图表的形式展示在仪表板上。您可以通过 Grafana 控制面板来监控不同节点的各种性能指标，如 CPU 使用率、内存、磁盘 IO、网络流量等。 9. 优化和调整 # 优化 Prometheus 配置：根据监控需求，可以调整 prometheus.yml 中的 scrape_interval、scrape_timeout 等参数，以获得更高的采集频率或更低的延迟。 调整 Grafana 面板：可以根据需要调整面板的布局、查询频率、时间范围等。 总结： # 安装 Prometheus 和配置目标节点，确保 Prometheus 能定期从 Node Exporter 等数据源拉取指标。 安装 Grafana，并在 Grafana 中配置 Prometheus 作为数据源。 创建 Grafana 仪表板，并设置相应的查询和图表。 查看和监控节点数据，使用 Grafana 可视化数据，并根据需要设置警报。 通过以上步骤，您就能实现从添加节点监控到 Grafana 上生成实时图表的完整流程。\n6. 在工作中用到了哪些exporter # node-exporter监控linux主机 cAdvisor监控容器 MySQLD Exporter监控mysql Blackbox Exporter网络探测 Pushgateway采集自定义指标监控 process exporter进程监控 在工作中，常用的 exporter 主要有以下几种：\nNode Exporter： 用于收集主机的硬件和操作系统指标，如 CPU、内存、磁盘使用情况、网络流量等。通常用于监控整个主机的性能。 Blackbox Exporter： 用于监控外部服务的可达性（如 HTTP、HTTPS、TCP、DNS 等）。可以用来监控网站、API、数据库等外部服务的健康状况。 MySQL Exporter： 用于从 MySQL 数据库收集各种性能指标，如连接数、查询执行情况、慢查询等。帮助监控数据库的性能和健康状况。 Redis Exporter： 用于收集 Redis 的性能指标，包括内存使用、连接数、命令执行情况等。适用于 Redis 的性能监控和优化。 JMX Exporter： 用于从 Java 应用程序（如 Tomcat、Kafka、Hadoop 等）收集指标。通过暴露 JMX（Java Management Extensions）指标，将 Java 应用的性能数据提供给 Prometheus。 Kubernetes Metrics Server： Kubernetes 集群中用于收集各个 Pod 和 Node 的资源使用数据（如 CPU、内存使用量）。通常与 Prometheus 配合使用，提供集群级别的监控数据。 NGINX Exporter： 用于收集 Nginx 服务器的运行指标，如请求数量、响应时间、连接数等。帮助监控 Nginx 的负载和健康状况。 Kafka Exporter： 用于监控 Kafka 集群的健康和性能指标，如主题（topic）大小、分区状态、消费者延迟等。 PostgreSQL Exporter： 用于从 PostgreSQL 数据库收集指标，包括连接数、查询性能、索引使用情况等，帮助分析数据库的运行状态。 Docker Exporter： 用于收集 Docker 容器的资源使用情况，如 CPU、内存、磁盘等，适用于容器化环境的监控。 Consul Exporter： 用于从 HashiCorp Consul 中收集监控数据，主要是关于服务健康检查的状态和元数据。 这些 exporters 使得 Prometheus 能够全面地监控基础设施、应用程序和数据库等关键组件，并结合 Grafana 提供实时的可视化数据。\n"},{"id":150,"href":"/docs/%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2k8s%E5%8A%A0%E8%8A%82%E7%82%B9%E6%93%8D%E4%BD%9C-er-jin-zhi-bu-shu-k8s-jia-jie-dian-cao-zuo/","title":"二进制部署K8S加节点操作 2024-04-03 15:08:26.726","section":"Docs","content":" 安装docker # #拷贝另一台work节点的二进制文件 到/usr/bin/下 - name: copy docker to /usr/bin copy: src={{ item }} dest=/usr/bin/{{ item }} mode=755 with_items: - docker - dockerd - docker-init - docker-proxy - containerd - containerd-shim - containerd-shim-runc-v2 - ctr - runc #创建docker目录 /etc/docker - name: make config dir \u0026#34;/etc/docker\u0026#34; file: dest=/etc/docker mode=755 state=directory #拷贝另一台work节点的daemon.json文件到/etc/docker/daemon.json - name: copy config file docker to /etc/docker template: src=daemon.json dest=/etc/docker/daemon.json #拷贝另一台work节点的docker.service文件到/usr/lib/systemd/system/docker.service - name: copy docker.service to /usr/lib/systemd/system copy: src=docker.service dest=/usr/lib/systemd/system/docker.service #拷贝另一台work节点的containerd.service文件到/usr/lib/systemd/system/containerd.service - name: copy containerd.service to /usr/lib/systemd/system copy: src=containerd.service dest=/usr/lib/systemd/system/containerd.service #拷贝另一台work节点的docker.socket文件到/usr/lib/systemd/system/docker.socket - name: copy docker.socket to /usr/lib/systemd/system copy: src=docker.socket dest=/usr/lib/systemd/system/docker.socket #启动并检查服务状态 systemctl daemon-reload - name: systemctl daemon-reload command: systemctl daemon-reload #重启docker systemctl restart docker - name: systemctl daemon-reload command: systemctl restart docker #检查docker状态 systemctl status docker - name: check docker service started ok command: systemctl status docker 安装kubelet和kube-proxy # --- #拷贝另一台work节点的kubelet二进制文件到/usr/bin/下并赋予755权限 - name: cp kubelet to /usr/bin copy: src=kubelet dest=/usr/bin/ mode=755 #拷贝另一台work节点的kube-proxy二进制文件到/usr/bin/下并赋予755权限 - name: copy kube-proxy to /usr/bin copy: src=kube-proxy dest=/usr/bin/ mode=755 #创建/etc/kubernetes/ssl文件夹并赋755权限 - name: make dir /etc/kubernetes/ssl file: dest=/etc/kubernetes/ssl mode=755 state=directory #创建/var/run/kubernetes文件夹并赋755权限 - name: make dir /var/run/kubernetes file: dest=/var/run/kubernetes mode=755 state=directory #创建/opt/log/kubernetes文件夹并赋755权限 - name: make log dir \u0026#34;/opt/log/kubernetes\u0026#34; file: dest={{kube_log_dir}} mode=755 state=directory #拷贝另一台work节点的kubelet配置文件到/etc/kubernetes/kubelet下，并修改kubelet文件中hostname-override为本机地址 - name: copy config file kubelet to /etc/kubernetes template: src=kubelet dest=/etc/kubernetes/kubelet #拷贝另一台work节点proxy配置文件到/etc/kubernetes/proxy下，并修改proxy文件中hostname-override为本机地址 - name: copy config file proxy to /etc/kubernetes template: src=proxy dest=/etc/kubernetes/proxy #拷贝另一台work节点kubelet.config文件到/etc/kubernetes/kubelet.config - name: copy config file kubelet.config to /etc/kubernetes template: src=kubelet.config dest=/etc/kubernetes/kubelet.config #拷贝另一台work节点kubelet.service文件到/usr/lib/systemd/system/kubelet.service - name: copy kubelet.service to /usr/lib/systemd/system copy: src=kubelet.service dest=/usr/lib/systemd/system/kubelet.service #拷贝另一台work节点kube-proxy.service文件到/usr/lib/systemd/system/kube-proxy.service - name: copy kube-proxy.service to /usr/lib/systemd/system copy: src=kube-proxy.service dest=/usr/lib/systemd/system/kube-proxy.service #创建/opt/kubernetes文件夹并赋755权限 - name: make dir /opt/kubernetes file: dest={{ kube_dir }} mode=755 state=directory #拷贝另一台work节点kubeconfig文件到/etc/kubernetes/ssl/下 - name: copy kubeconfig template: src=kubeconfig dest=/etc/kubernetes/ssl/ #拷贝另一台work节点ca.crt文件到/etc/kubernetes/ssl/下 - name: copy ca.crt copy: src={{ssl_tmp_dir}}/yunxing/ca.crt dest=/etc/kubernetes/ssl/ #拷贝另一台work节点client.key文件到/etc/kubernetes/ssl/下 - name: copy client.key copy: src={{ssl_tmp_dir}}/yunxing/client.key dest=/etc/kubernetes/ssl/ #拷贝另一台work节点client.crt 到/etc/kubernetes/ssl/下 - name: copy client.crt copy: src={{ssl_tmp_dir}}/yunxing/client.crt dest=/etc/kubernetes/ssl/ #重载系统配置systemctl daemon-reload - name: systemctl daemon-reload command: systemctl daemon-reload #重启kubelet systemctl restart kubelet - name: start kubelet service service: name=kubelet state=restarted enabled=yes #重启kube-proxy systemctl restart kube-proxy - name: start kube-proxy service service: name=kube-proxy state=restarted enabled=yes #查看kubelet状态 systemctl status kubelet - name: check kubelet service started ok command: systemctl status kubelet #查看kube-proxy状态 systemctl status kube-proxy - name: check kube-proxy service started ok command: systemctl status kube-proxy "},{"id":151,"href":"/docs/2024-10-20-%E5%88%9B%E5%BB%BA%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/","title":"使用 Keepalived 和 HAproxy 创建高可用 Kubernetes 集群","section":"Docs","content":" 使用 Keepalived 和 HAproxy 创建高可用 Kubernetes 集群 # 高可用 Kubernetes 集群能够确保应用程序在运行时不会出现服务中断，这也是生产的需求之一。为此，有很多方法可供选择以实现高可用。\n本教程演示了如何配置 Keepalived 和 HAproxy 使负载均衡、实现高可用。步骤如下：\n准备主机。 配置 Keepalived 和 HAproxy。 使用 KubeKey 创建 Kubernetes 集群，并安装 KubeSphere。 集群架构 # 示例集群有三个主节点，三个工作节点，两个用于负载均衡的节点，以及一个虚拟 IP 地址。本示例中的虚拟 IP 地址也可称为“浮动 IP 地址”。这意味着在节点故障的情况下，该 IP 地址可在节点之间漂移，从而实现高可用。\n请注意，在本示例中，Keepalived 和 HAproxy 没有安装在任何主节点上。但您也可以这样做，并同时实现高可用。然而，配置两个用于负载均衡的特定节点（您可以按需增加更多此类节点）会更加安全。这两个节点上只安装 Keepalived 和 HAproxy，以避免与任何 Kubernetes 组件和服务的潜在冲突。\n准备主机 # IP 地址 主机名 角色 172.16.0.2 lb1 Keepalived \u0026amp; HAproxy 172.16.0.3 lb2 Keepalived \u0026amp; HAproxy 172.16.0.4 master1 master, etcd 172.16.0.5 master2 master, etcd 172.16.0.6 master3 master, etcd 172.16.0.7 worker1 worker 172.16.0.8 worker2 worker 172.16.0.9 worker3 worker 172.16.0.10 虚拟 IP 地址 有关更多节点、网络、依赖项等要求的信息，请参见多节点安装。\n配置负载均衡 # Keepalived 提供 VRRP 实现，并允许您配置 Linux 机器使负载均衡，预防单点故障。HAProxy 提供可靠、高性能的负载均衡，能与 Keepalived 完美配合。\n由于 lb1 和 lb2 上安装了 Keepalived 和 HAproxy，如果其中一个节点故障，虚拟 IP 地址（即浮动 IP 地址）将自动与另一个节点关联，使集群仍然可以正常运行，从而实现高可用。若有需要，也可以此为目的，添加更多安装 Keepalived 和 HAproxy 的节点。\n先运行以下命令安装 Keepalived 和 HAproxy。\nyum install keepalived haproxy psmisc -y HAproxy # 在两台用于负载均衡的机器上运行以下命令以配置 Proxy（两台机器的 Proxy 配置相同）：\nvi /etc/haproxy/haproxy.cfg 以下是示例配置，供您参考（请注意 server 字段。请记住 6443 是 apiserver 端口）：\nglobal log /dev/log local0 warning chroot /var/lib/haproxy pidfile /var/run/haproxy.pid maxconn 4000 user haproxy group haproxy daemon stats socket /var/lib/haproxy/stats defaults log global option httplog option dontlognull timeout connect 5000 timeout client 50000 timeout server 50000 frontend kube-apiserver bind *:6443 mode tcp option tcplog default_backend kube-apiserver backend kube-apiserver mode tcp option tcplog option tcp-check balance roundrobin default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100 server kube-apiserver-1 172.16.0.4:6443 check # Replace the IP address with your own. server kube-apiserver-2 172.16.0.5:6443 check # Replace the IP address with your own. server kube-apiserver-3 172.16.0.6:6443 check # Replace the IP address with your own. 保存文件并运行以下命令以重启 HAproxy。\nsystemctl restart haproxy 使 HAproxy 在开机后自动运行：\nsystemctl enable haproxy 确保您在另一台机器 (lb2) 上也配置了 HAproxy。\nKeepalived # 两台机器上必须都安装 Keepalived，但在配置上略有不同。\n运行以下命令以配置 Keepalived。\nvi /etc/keepalived/keepalived.conf 以下是示例配置 (lb1)，供您参考：\nglobal_defs { notification_email { } router_id LVS_DEVEL vrrp_skip_check_adv_addr vrrp_garp_interval 0 vrrp_gna_interval 0 } vrrp_script chk_haproxy { script \u0026#34;killall -0 haproxy\u0026#34; interval 2 weight 2 } vrrp_instance haproxy-vip { state BACKUP priority 100 interface eth0 # Network card virtual_router_id 60 advert_int 1 authentication { auth_type PASS auth_pass 1111 } unicast_src_ip 172.16.0.2 # The IP address of this machine unicast_peer { 172.16.0.3 # The IP address of peer machines } virtual_ipaddress { 172.16.0.10/24 # The VIP address } track_script { chk_haproxy } } 备注\n对于 interface 字段，您必须提供自己的网卡信息。您可以在机器上运行 ifconfig 以获取该值。 为 unicast_src_ip 提供的 IP 地址是您当前机器的 IP 地址。对于也安装了 HAproxy 和 Keepalived 进行负载均衡的其他机器，必须在字段 unicast_peer 中输入其 IP 地址。 保存文件并运行以下命令以重启 Keepalived。\nsystemctl restart keepalived 使 Keepalived 在开机后自动运行：\nsystemctl enable keepalived 确保您在另一台机器 (lb2) 上也配置了 Keepalived。\n验证高可用 # 在开始创建 Kubernetes 集群之前，请确保已经测试了高可用。\n在机器 lb1 上，运行以下命令：\n[root@lb1 ~]# ip a s 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eth0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 52:54:9e:27:38:c8 brd ff:ff:ff:ff:ff:ff inet 172.16.0.2/24 brd 172.16.0.255 scope global noprefixroute dynamic eth0 valid_lft 73334sec preferred_lft 73334sec inet 172.16.0.10/24 scope global secondary eth0 # The VIP address valid_lft forever preferred_lft forever inet6 fe80::510e:f96:98b2:af40/64 scope link noprefixroute valid_lft forever preferred_lft forever 如上图所示，虚拟 IP 地址已经成功添加。模拟此节点上的故障：\nsystemctl stop haproxy 再次检查浮动 IP 地址，您可以看到该地址在 lb1 上消失了。\n[root@lb1 ~]# ip a s 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eth0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 52:54:9e:27:38:c8 brd ff:ff:ff:ff:ff:ff inet 172.16.0.2/24 brd 172.16.0.255 scope global noprefixroute dynamic eth0 valid_lft 72802sec preferred_lft 72802sec inet6 fe80::510e:f96:98b2:af40/64 scope link noprefixroute valid_lft forever preferred_lft forever 理论上讲，若配置成功，该虚拟 IP 会漂移到另一台机器 (lb2) 上。在 lb2 上运行以下命令，这是预期的输出：\n[root@lb2 ~]# ip a s 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eth0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 52:54:9e:3f:51:ba brd ff:ff:ff:ff:ff:ff inet 172.16.0.3/24 brd 172.16.0.255 scope global noprefixroute dynamic eth0 valid_lft 72690sec preferred_lft 72690sec inet 172.16.0.10/24 scope global secondary eth0 # The VIP address valid_lft forever preferred_lft forever inet6 fe80::f67c:bd4f:d6d5:1d9b/64 scope link noprefixroute valid_lft forever preferred_lft forever 如上所示，高可用已经配置成功。\n使用 KubeKey 创建 Kubernetes 集群 # KubeKey 是一款用来创建 Kubernetes 集群的工具，高效而便捷。请按照以下步骤下载 KubeKey。\n如果您能正常访问 GitHub/Googleapis 如果您访问 GitHub/Googleapis 受限 从 GitHub Release Page 下载 KubeKey 或者直接使用以下命令。\ncurl -sfL https://get-kk.kubesphere.io | VERSION=v3.0.13 sh - 如果您访问 GitHub/Googleapis 受限\n首先运行以下命令，以确保您从正确的区域下载 KubeKey。\nexport KKZONE=cn 运行以下命令来下载 KubeKey：\ncurl -sfL https://get-kk.kubesphere.io | VERSION=v3.0.13 sh - 备注\n下载 KubeKey 之后，如果您将其转移到访问 Googleapis 受限的新机器上，请务必再次运行 export KKZONE=cn，然后继续执行以下步骤。\n备注\n通过以上命令，可以下载 KubeKey 的最新版本。您可以更改命令中的版本号来下载特定的版本。\n使 kk 成为可执行文件：\nchmod +x kk 使用默认配置创建一个示例配置文件。此处以 Kubernetes v1.22.12 作为示例。\n./kk create config --with-kubesphere v3.4.1 --with-kubernetes v1.22.12 备注\n安装 KubeSphere 3.4 的建议 Kubernetes 版本：v1.20.x、v1.21.x、v1.22.x、v1.23.x、* v1.24.x、* v1.25.x 和 * v1.26.x。带星号的版本可能出现边缘节点部分功能不可用的情况。因此，如需使用边缘节点，推荐安装 v1.23.x。如果不指定 Kubernetes 版本，KubeKey 将默认安装 Kubernetes v1.23.10。有关受支持的 Kubernetes 版本的更多信息，请参见支持矩阵。 如果您没有在本步骤的命令中添加标志 --with-kubesphere，那么除非您使用配置文件中的 addons 字段进行安装，或者稍后使用 ./kk create cluster 时再添加该标志，否则 KubeSphere 将不会被部署。 如果您添加标志 --with-kubesphere 时未指定 KubeSphere 版本，则会安装最新版本的 KubeSphere。 部署 KubeSphere 和 Kubernetes # 运行上述命令后，将创建配置文件 config-sample.yaml。编辑文件以添加机器信息、配置负载均衡器等。\n备注\n如果自定义文件名，那么文件名可能会有所不同。\nconfig-sample.yaml 示例 # ... spec: hosts: - {name: master1, address: 172.16.0.4, internalAddress: 172.16.0.4, user: root, password: Testing123} - {name: master2, address: 172.16.0.5, internalAddress: 172.16.0.5, user: root, password: Testing123} - {name: master3, address: 172.16.0.6, internalAddress: 172.16.0.6, user: root, password: Testing123} - {name: worker1, address: 172.16.0.7, internalAddress: 172.16.0.7, user: root, password: Testing123} - {name: worker2, address: 172.16.0.8, internalAddress: 172.16.0.8, user: root, password: Testing123} - {name: worker3, address: 172.16.0.9, internalAddress: 172.16.0.9, user: root, password: Testing123} roleGroups: etcd: - master1 - master2 - master3 control-plane: - master1 - master2 - master3 worker: - worker1 - worker2 - worker3 controlPlaneEndpoint: domain: lb.kubesphere.local address: 172.16.0.10 # The VIP address port: 6443 ... 备注\n请使用您自己的 VIP 地址来替换 controlPlaneEndpoint.address 的值。 有关更多本配置文件中不同参数的信息，请参见多节点安装。 开始安装 # 完成配置之后，可以执行以下命令开始安装：\n./kk create cluster -f config-sample.yaml 验证安装 # 运行以下命令以检查安装日志。\nkubectl logs -n kubesphere-system $(kubectl get pod -n kubesphere-system -l \u0026#39;app in (ks-install, ks-installer)\u0026#39; -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) -f 看到以下信息时，表明高可用集群已成功创建。\n##################################################### ### Welcome to KubeSphere! ### ##################################################### Console: http://172.16.0.4:30880 Account: admin Password: P@88w0rd NOTES： 1. After you log into the console, please check the monitoring status of service components in the \u0026#34;Cluster Management\u0026#34;. If any service is not ready, please wait patiently until all components are up and running. 2. Please change the default password after login. ##################################################### https://kubesphere.io 2020-xx-xx xx:xx:xx ##################################################### 反馈\n这篇文章对您有帮助吗？\n"},{"id":152,"href":"/docs/%E4%BD%BF%E7%94%A8-keepalived-%E5%92%8C-haproxy-%E5%88%9B%E5%BB%BA%E9%AB%98%E5%8F%AF%E7%94%A8-kubernetes-%E9%9B%86%E7%BE%A4-shi-yong-keepalived-he-haproxy-chuang-jian-gao-ke-yong-kubernetes-ji-qun/","title":"使用 Keepalived 和 HAproxy 创建高可用 Kubernetes 集群 2023-04-12 04:04:09.458","section":"Docs","content":" 使用 Keepalived 和 HAproxy 创建高可用 Kubernetes 集群 # 高可用 Kubernetes 集群能够确保应用程序在运行时不会出现服务中断，这也是生产的需求之一。为此，有很多方法可供选择以实现高可用。\n本教程演示了如何配置 Keepalived 和 HAproxy 使负载均衡、实现高可用。步骤如下：\n准备主机。 配置 Keepalived 和 HAproxy。 使用 KubeKey 创建 Kubernetes 集群，并安装 KubeSphere。 集群架构 # 示例集群有三个主节点，三个工作节点，两个用于负载均衡的节点，以及一个虚拟 IP 地址。本示例中的虚拟 IP 地址也可称为“浮动 IP 地址”。这意味着在节点故障的情况下，该 IP 地址可在节点之间漂移，从而实现高可用。\n请注意，在本示例中，Keepalived 和 HAproxy 没有安装在任何主节点上。但您也可以这样做，并同时实现高可用。然而，配置两个用于负载均衡的特定节点（您可以按需增加更多此类节点）会更加安全。这两个节点上只安装 Keepalived 和 HAproxy，以避免与任何 Kubernetes 组件和服务的潜在冲突。\n准备主机 # IP 地址 主机名 角色 172.16.0.2 lb1 Keepalived \u0026amp; HAproxy 172.16.0.3 lb2 Keepalived \u0026amp; HAproxy 172.16.0.4 master1 master, etcd 172.16.0.5 master2 master, etcd 172.16.0.6 master3 master, etcd 172.16.0.7 worker1 worker 172.16.0.8 worker2 worker 172.16.0.9 worker3 worker 172.16.0.10 虚拟 IP 地址 有关更多节点、网络、依赖项等要求的信息，请参见多节点安装。\n配置负载均衡 # Keepalived 提供 VRRP 实现，并允许您配置 Linux 机器使负载均衡，预防单点故障。HAProxy 提供可靠、高性能的负载均衡，能与 Keepalived 完美配合。\n由于 lb1 和 lb2 上安装了 Keepalived 和 HAproxy，如果其中一个节点故障，虚拟 IP 地址（即浮动 IP 地址）将自动与另一个节点关联，使集群仍然可以正常运行，从而实现高可用。若有需要，也可以此为目的，添加更多安装 Keepalived 和 HAproxy 的节点。\n先运行以下命令安装 Keepalived 和 HAproxy。\nyum install keepalived haproxy psmisc -y HAproxy # 在两台用于负载均衡的机器上运行以下命令以配置 Proxy（两台机器的 Proxy 配置相同）：\nvi /etc/haproxy/haproxy.cfg 以下是示例配置，供您参考（请注意 server 字段。请记住 6443 是 apiserver 端口）：\nglobal log /dev/log local0 warning chroot /var/lib/haproxy pidfile /var/run/haproxy.pid maxconn 4000 user haproxy group haproxy daemon stats socket /var/lib/haproxy/stats defaults log global option httplog option dontlognull timeout connect 5000 timeout client 50000 timeout server 50000 frontend kube-apiserver bind *:6443 mode tcp option tcplog default_backend kube-apiserver backend kube-apiserver mode tcp option tcplog option tcp-check balance roundrobin default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100 server kube-apiserver-1 172.16.0.4:6443 check # Replace the IP address with your own. server kube-apiserver-2 172.16.0.5:6443 check # Replace the IP address with your own. server kube-apiserver-3 172.16.0.6:6443 check # Replace the IP address with your own. 保存文件并运行以下命令以重启 HAproxy。\nsystemctl restart haproxy 使 HAproxy 在开机后自动运行：\nsystemctl enable haproxy 确保您在另一台机器 (lb2) 上也配置了 HAproxy。\nKeepalived # 两台机器上必须都安装 Keepalived，但在配置上略有不同。\n运行以下命令以配置 Keepalived。\nvi /etc/keepalived/keepalived.conf 以下是示例配置 (lb1)，供您参考：\nglobal_defs { notification_email { } router_id LVS_DEVEL vrrp_skip_check_adv_addr vrrp_garp_interval 0 vrrp_gna_interval 0 } vrrp_script chk_haproxy { script \u0026#34;killall -0 haproxy\u0026#34; interval 2 weight 2 } vrrp_instance haproxy-vip { state BACKUP priority 100 interface eth0 # Network card virtual_router_id 60 advert_int 1 authentication { auth_type PASS auth_pass 1111 } unicast_src_ip 172.16.0.2 # The IP address of this machine unicast_peer { 172.16.0.3 # The IP address of peer machines } virtual_ipaddress { 172.16.0.10/24 # The VIP address } track_script { chk_haproxy } } 备注\n对于 interface 字段，您必须提供自己的网卡信息。您可以在机器上运行 ifconfig 以获取该值。 为 unicast_src_ip 提供的 IP 地址是您当前机器的 IP 地址。对于也安装了 HAproxy 和 Keepalived 进行负载均衡的其他机器，必须在字段 unicast_peer 中输入其 IP 地址。 保存文件并运行以下命令以重启 Keepalived。\nsystemctl restart keepalived 使 Keepalived 在开机后自动运行：\nsystemctl enable keepalived 确保您在另一台机器 (lb2) 上也配置了 Keepalived。\n验证高可用 # 在开始创建 Kubernetes 集群之前，请确保已经测试了高可用。\n在机器 lb1 上，运行以下命令：\n[root@lb1 ~]# ip a s 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eth0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 52:54:9e:27:38:c8 brd ff:ff:ff:ff:ff:ff inet 172.16.0.2/24 brd 172.16.0.255 scope global noprefixroute dynamic eth0 valid_lft 73334sec preferred_lft 73334sec inet 172.16.0.10/24 scope global secondary eth0 # The VIP address valid_lft forever preferred_lft forever inet6 fe80::510e:f96:98b2:af40/64 scope link noprefixroute valid_lft forever preferred_lft forever 如上图所示，虚拟 IP 地址已经成功添加。模拟此节点上的故障：\nsystemctl stop haproxy 再次检查浮动 IP 地址，您可以看到该地址在 lb1 上消失了。\n[root@lb1 ~]# ip a s 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eth0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 52:54:9e:27:38:c8 brd ff:ff:ff:ff:ff:ff inet 172.16.0.2/24 brd 172.16.0.255 scope global noprefixroute dynamic eth0 valid_lft 72802sec preferred_lft 72802sec inet6 fe80::510e:f96:98b2:af40/64 scope link noprefixroute valid_lft forever preferred_lft forever 理论上讲，若配置成功，该虚拟 IP 会漂移到另一台机器 (lb2) 上。在 lb2 上运行以下命令，这是预期的输出：\n[root@lb2 ~]# ip a s 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eth0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 52:54:9e:3f:51:ba brd ff:ff:ff:ff:ff:ff inet 172.16.0.3/24 brd 172.16.0.255 scope global noprefixroute dynamic eth0 valid_lft 72690sec preferred_lft 72690sec inet 172.16.0.10/24 scope global secondary eth0 # The VIP address valid_lft forever preferred_lft forever inet6 fe80::f67c:bd4f:d6d5:1d9b/64 scope link noprefixroute valid_lft forever preferred_lft forever 如上所示，高可用已经配置成功。\n使用 KubeKey 创建 Kubernetes 集群 # KubeKey 是一款用来创建 Kubernetes 集群的工具，高效而便捷。请按照以下步骤下载 KubeKey。\n如果您能正常访问 GitHub/Googleapis 如果您访问 GitHub/Googleapis 受限 从 GitHub Release Page 下载 KubeKey 或者直接使用以下命令。\ncurl -sfL https://get-kk.kubesphere.io | VERSION=v2.0.0 sh - 备注\n通过以上命令，可以下载 KubeKey 的最新版本 (v2.0.0)。您可以更改命令中的版本号来下载特定的版本。\n使 kk 成为可执行文件：\nchmod +x kk 使用默认配置创建一个示例配置文件。此处以 Kubernetes v1.21.5 作为示例。\n./kk create config --with-kubesphere v3.2.1 --with-kubernetes v1.21.5 备注\n安装 KubeSphere 3.2.1 的建议 Kubernetes 版本：v1.19.x、v1.20.x、v1.21.x 或 v1.22.x（实验性支持）。如果不指定 Kubernetes 版本，KubeKey 将默认安装 Kubernetes v1.21.5。有关受支持的 Kubernetes 版本的更多信息，请参见支持矩阵。 如果您没有在本步骤的命令中添加标志 --with-kubesphere，那么除非您使用配置文件中的 addons 字段进行安装，或者稍后使用 ./kk create cluster 时再添加该标志，否则 KubeSphere 将不会被部署。 如果您添加标志 --with-kubesphere 时未指定 KubeSphere 版本，则会安装最新版本的 KubeSphere。 部署 KubeSphere 和 Kubernetes # 运行上述命令后，将创建配置文件 config-sample.yaml。编辑文件以添加机器信息、配置负载均衡器等。\n备注\n如果自定义文件名，那么文件名可能会有所不同。\nconfig-sample.yaml 示例 # ... spec: hosts: - {name: master1, address: 172.16.0.4, internalAddress: 172.16.0.4, user: root, password: Testing123} - {name: master2, address: 172.16.0.5, internalAddress: 172.16.0.5, user: root, password: Testing123} - {name: master3, address: 172.16.0.6, internalAddress: 172.16.0.6, user: root, password: Testing123} - {name: worker1, address: 172.16.0.7, internalAddress: 172.16.0.7, user: root, password: Testing123} - {name: worker2, address: 172.16.0.8, internalAddress: 172.16.0.8, user: root, password: Testing123} - {name: worker3, address: 172.16.0.9, internalAddress: 172.16.0.9, user: root, password: Testing123} roleGroups: etcd: - master1 - master2 - master3 master: - master1 - master2 - master3 worker: - worker1 - worker2 - worker3 controlPlaneEndpoint: domain: lb.kubesphere.local address: 172.16.0.10 # The VIP address port: 6443 ... 备注\n请使用您自己的 VIP 地址来替换 controlPlaneEndpoint.address 的值。 有关更多本配置文件中不同参数的信息，请参见多节点安装。 开始安装 # 完成配置之后，可以执行以下命令开始安装：\n./kk create cluster -f config-sample.yaml 验证安装 # 运行以下命令以检查安装日志。\nkubectl logs -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-install -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) -f 看到以下信息时，表明高可用集群已成功创建。\n##################################################### ### Welcome to KubeSphere! ### ##################################################### Console: http://172.16.0.4:30880 Account: admin Password: P@88w0rd NOTES： 1. After you log into the console, please check the monitoring status of service components in the \u0026#34;Cluster Management\u0026#34;. If any service is not ready, please wait patiently until all components are up and running. 2. Please change the default password after login. ##################################################### https://kubesphere.io 2020-xx-xx xx:xx:xx ##################################################### Keepalive+Haproxy部署 # 实验环境：\nIP\t备注 172.16.3.225/21\tKeepalive-Master+Haproxy+Nginx 172.16.3.226/21\tKeepalive-Backup+Haproxy+Nginx 172.16.3.200/24\tVIP 注：这里因为自己的环境有限为了更好地测试，所以就用两台机器。\n实验效果\n最终的实验效果是通过一个虚拟IP反向代理两台Nginx，当我们Keepalive-Master宕机VIP会转到Keepalive-Backup并且还可以正常对外访问 实验步骤\n1、部署Keepalive\n1）关闭两台Firewalls、Selinux\n[root@bogon ~]# setenforce 0 \u0026amp;\u0026amp; sed -i \u0026#39;s/^SELINUX=.*/SELINUX=disabled/\u0026#39; /etc/selinux/config [root@bogon ~]# systemctl stop firewalld \u0026amp;\u0026amp; systemctl disable firewalld 2）两台主机安装环境所需的软件\n[root@bogon ~]# yum install epel-release keepalived haproxy bzip2-devel popt-devel kernel-devel openssl-devel -y [root@bogon ~]# yum install nginx -y 3）配置一下Master配置文件\nvim /etc/keepalived/keepalived.conf [root@bogon ~]# cat \u0026gt; /etc/keepalived/keepalived.conf \u0026lt;\u0026lt; EOF ! Configuration File for keepalived global_defs { notification_email { acassen@firewall.loc failover@firewall.loc sysadmin@firewall.loc } notification_email_from Alexandre.Cassen@firewall.loc smtp_server 192.168.200.1 smtp_connect_timeout 30 router_id LVS_DEVEL_MASTER # 两台ID不能一致 vrrp_skip_check_adv_addr vrrp_strict vrrp_garp_interval 0 vrrp_gna_interval 0 } vrrp_instance VI_1 { state MASTER # 指定A节点为主节点 备用节点上设置为BACKUP即可 interface ens160 # 绑定虚拟IP的网络接口 virtual_router_id 51 # VRRP组名，两个节点的设置必须一样，以指明各个节点属于同一VRRP组 priority 100 # 主节点的优先级（1-254之间），备用节点必须比主节点优先级低 advert_int 1 # 组播信息发送间隔，两个节点设置必须一样 authentication { # 设置验证信息，两个节点必须一致 auth_type PASS\tauth_pass 1111\t} virtual_ipaddress { # 指定虚拟IP, 两个节点设置必须一样 172.16.3.200/24\t} } EOF cat \u0026gt; /etc/keepalived/keepalived.conf \u0026lt;\u0026lt; EOF ! Configuration File for keepalived global_defs { notification_email { acassen@firewall.loc failover@firewall.loc sysadmin@firewall.loc } notification_email_from Alexandre.Cassen@firewall.loc smtp_server 192.168.200.1 smtp_connect_timeout 30 router_id LVS_DEVEL1 vrrp_skip_check_adv_addr #vrrp_strict #ping不通 注释掉上面这行 vrrp_garp_interval 0 vrrp_gna_interval 0 } vrrp_instance VI_1 { state MASTER interface eth192 virtual_router_id 51 priority 100 advert_int 1 authentication { auth_type PASS auth_pass 1111 } virtual_ipaddress { 192.168.1.104/24 } } EOF 192.168.1.105 192.168.1.106 192.168.1.104 ens192 4）配置一下Backup的配置文件\n[root@bogon ~]# cat \u0026gt; /etc/keepalived/keepalived.conf \u0026lt;\u0026lt; EOF ! Configuration File for keepalived global_defs { notification_email { acassen@firewall.loc failover@firewall.loc sysadmin@firewall.loc } notification_email_from Alexandre.Cassen@firewall.loc smtp_server 192.168.200.1 smtp_connect_timeout 30 router_id LVS_DEVEL_NODE # 两台ID不能一致 vrrp_skip_check_adv_addr #vrrp_strict #注释掉，不注释ping不通 vrrp_garp_interval 0 vrrp_gna_interval 0 } vrrp_instance VI_1 { state BACKUP # 指定A节点为主节点 备用节点上设置为BACKUP即可 interface ens160 # 绑定虚拟IP的网络接口 virtual_router_id 51 # VRRP组名，两个节点的设置必须一样，以指明各个节点属于同一VRRP组 priority 90 # 主节点的优先级（1-254之间），备用节点必须比主节点优先级低 advert_int 1 # 组播信息发送间隔，两个节点设置必须一样 authentication { # 设置验证信息，两个节点必须一致 auth_type PASS\tauth_pass 1111\t} virtual_ipaddress { # 指定虚拟IP, 两个节点设置必须一样 172.16.3.200/24\t} } EOF cat \u0026gt; /etc/keepalived/keepalived.conf \u0026lt;\u0026lt; EOF ! Configuration File for keepalived global_defs { notification_email { acassen@firewall.loc failover@firewall.loc sysadmin@firewall.loc } notification_email_from Alexandre.Cassen@firewall.loc smtp_server 192.168.200.1 smtp_connect_timeout 30 router_id LVS_DEVEL2 vrrp_skip_check_adv_addr vrrp_strict vrrp_garp_interval 0 vrrp_gna_interval 0 } vrrp_instance VI_1 { state buckup interface eth192 virtual_router_id 51 priority 90 advert_int 1 authentication { auth_type PASS auth_pass 1111 } virtual_ipaddress { 192.168.1.118/24 } } EOF 4）验证一下VIP是否已经映射出\n[root@bogon ~]# ip a | grep \u0026#39;172.16.3.200\u0026#39; inet 172.16.3.200/24 scope global ens160 ip a | grep \u0026#39;192.168.1.104\u0026#39; 2、修改一下Nginx网页方便验证\n[root@bogon ~]# echo \u0026#34;172.16.3.225\u0026#34; \u0026gt; /usr/share/nginx/html/index.html [root@bogon ~]# echo \u0026#34;172.16.3.226\u0026#34; \u0026gt; /usr/share/nginx/html/index.html echo \u0026#34;192.168.1.105\u0026#34; \u0026gt; /usr/share/nginx/html/index.html echo \u0026#34;192.168.1.106\u0026#34; \u0026gt; /usr/share/nginx/html/index.html 3、配置Haproxy\n1）创建日志目录\nmkdir /var/log/haproxy chmod a+w /var/log/haproxy 2）开启rsyslog记录haproxy日志\nvim /etc/rsyslog.conf # Provides UDP syslog reception $ModLoad imudp # $UDPServerRun 514 # haproxy log local0.* /var/log/haproxy/haproxy.log # 添加 3）修改/etc/sysconfig/rsyslog文件\nvim /etc/sysconfig/rsyslog # Options for rsyslogd # Syslogd options are deprecated since rsyslog v3. # If you want to use them, switch to compatibility mode 2 by \u0026#34;-c 2\u0026#34; # See rsyslogd(8) for more details SYSLOGD_OPTIONS=\u0026#34;-r -m 0 -c 2\u0026#34; [root@bogon ~]# systemctl restart rsyslog 4）在Master配置Haproxy配置\n[root@bogon ~]# cat \u0026gt; /etc/haproxy/haproxy.cfg \u0026lt;\u0026lt; EOF ###########全局配置######### global log 127.0.0.1 local0 info # 日志类型，为不影响性能使用err daemon #nbproc 1 #进程数量 maxconn 4096 #最大连接数 #user haproxy #运行用户 #group haproxy #运行组 chroot /var/lib/haproxy pidfile /var/run/haproxy.pid ########默认配置############ defaults log global mode http #默认模式{ tcp|http|health } option httplog #日志类别,采用httplog option dontlognull #不记录健康检查日志信息 retries 2 #3次连接失败就认为服务器不可用 option forwardfor except 127.0.0.0/8 #后端服务获得真实ip,在HTTP请求中添加\u0026#34;HTTP_X_FORWARDED_FOR\u0026#34;字段 option httpclose #请求完毕后主动关闭http通道 option abortonclose #服务器负载很高，自动结束比较久的链接 maxconn 10000 #最大连接数 timeout connect 5m #连接超时 m(分钟) timeout client 1m #客户端超时 timeout server 1m #服务器超时 timeout check 10s #心跳检测超时 s(秒) balance leastconn #负载均衡方式，最少连接 ########后端配置############ listen test bind *:8080 mode http #balance roundrobin timeout server 15s timeout connect 15s server web01 172.16.3.225:80 check port 80 inter 5000 fall 5 server web02 172.16.3.226:80 check port 80 inter 5000 fall 5 EOF [root@bogon ~]# systemctl start haproxy cat \u0026gt; /etc/haproxy/haproxy.cfg \u0026lt;\u0026lt; EOF ###########全局配置######### global log 127.0.0.1 local0 info # 日志类型，为不影响性能使用err daemon #nbproc 1 #进程数量 maxconn 4096 #最大连接数 #user haproxy #运行用户 #group haproxy #运行组 chroot /var/lib/haproxy pidfile /var/run/haproxy.pid ########默认配置############ defaults log global mode http #默认模式{ tcp|http|health } option httplog #日志类别,采用httplog option dontlognull #不记录健康检查日志信息 retries 2 #3次连接失败就认为服务器不可用 option forwardfor except 127.0.0.0/8 #后端服务获得真实ip,在HTTP请求中添加\u0026#34;HTTP_X_FORWARDED_FOR\u0026#34;字段 option httpclose #请求完毕后主动关闭http通道 option abortonclose #服务器负载很高，自动结束比较久的链接 maxconn 10000 #最大连接数 timeout connect 5m #连接超时 m(分钟) timeout client 1m #客户端超时 timeout server 1m #服务器超时 timeout check 10s #心跳检测超时 s(秒) balance leastconn #负载均衡方式，最少连接 ########后端配置############ listen test bind *:8068 mode http #balance roundrobin timeout server 15s timeout connect 15s server web01 192.168.1.105:80 check port 80 inter 5000 fall 5 server web02 192.168.1.106:80 check port 80 inter 5000 fall 5 EOF 重启haproxy\nsystemctl start haproxy 5）Backup上配置Haproxy\nvim /etc/haproxy/haproxy.cfg [root@bogon ~]# cat \u0026gt; /etc/haproxy/haproxy.cfg \u0026lt;\u0026lt; EOF ###########全局配置######### global log 127.0.0.1 local0 info # 日志类型，为不影响性能使用err daemon #nbproc 1 #进程数量 maxconn 4096 #最大连接数 #user haproxy #运行用户 #group haproxy #运行组 chroot /var/lib/haproxy pidfile /var/run/haproxy.pid ########默认配置############ defaults log global mode http #默认模式{ tcp|http|health } option httplog #日志类别,采用httplog option dontlognull #不记录健康检查日志信息 retries 2 #3次连接失败就认为服务器不可用 option forwardfor except 127.0.0.0/8 #后端服务获得真实ip,在HTTP请求中添加\u0026#34;HTTP_X_FORWARDED_FOR\u0026#34;字段 option httpclose #请求完毕后主动关闭http通道 option abortonclose #服务器负载很高，自动结束比较久的链接 maxconn 10000 #最大连接数 timeout connect 5m #连接超时 m(分钟) timeout client 1m #客户端超时 timeout server 1m #服务器超时 timeout check 10s #心跳检测超时 s(秒) balance leastconn #负载均衡方式，最少连接 ########后端配置############ listen test bind *:8080 mode http #balance roundrobin timeout server 15s timeout connect 15s server web01 172.16.3.225:80 check port 80 inter 5000 fall 5 server web02 172.16.3.226:80 check port 80 inter 5000 fall 5 EOF cat \u0026gt; /etc/haproxy/haproxy.cfg \u0026lt;\u0026lt; EOF ###########全局配置######### global log 127.0.0.1 local0 info # 日志类型，为不影响性能使用err daemon #nbproc 1 #进程数量 maxconn 4096 #最大连接数 #user haproxy #运行用户 #group haproxy #运行组 chroot /var/lib/haproxy pidfile /var/run/haproxy.pid ########默认配置############ defaults log global mode http #默认模式{ tcp|http|health } option httplog #日志类别,采用httplog option dontlognull #不记录健康检查日志信息 retries 2 #3次连接失败就认为服务器不可用 option forwardfor except 127.0.0.0/8 #后端服务获得真实ip,在HTTP请求中添加\u0026#34;HTTP_X_FORWARDED_FOR\u0026#34;字段 option httpclose #请求完毕后主动关闭http通道 option abortonclose #服务器负载很高，自动结束比较久的链接 maxconn 10000 #最大连接数 timeout connect 5m #连接超时 m(分钟) timeout client 1m #客户端超时 timeout server 1m #服务器超时 timeout check 10s #心跳检测超时 s(秒) balance leastconn #负载均衡方式，最少连接 ########后端配置############ listen test bind *:8068 mode http #balance roundrobin timeout server 15s timeout connect 15s server web01 192.168.1.105:80 check port 80 inter 5000 fall 5 server web02 192.168.1.106:80 check port 80 inter 5000 fall 5 EOF 重启haproxy\nsystemctl start haproxy 6）测试一下haproxy代理是否配置成功，虚拟ip访问\n[root@bogon ~]# curl http://172.16.3.200:8080 172.16.3.225 [root@bogon ~]# curl http://172.16.3.200:8080 172.16.3.226 curl http://192.168.1.118:8068 注：出现这种负载说明没有问题\n现在开始测试keepalive\n注：将Master宕机看一下vip是否会转移到Backup\n[root@bogon ~]# systemctl stop keepalived [root@bogon ~]# ip a | grep 200\t# 如果Backup上可以查看到说明没有问题，再次访问一下VIP看一下是否可以正常访问 inet 172.16.3.200/24 scope global ens160 # 如果还可以访问说明实验成功 [root@bogon ~]# curl http://172.16.3.200:8080 172.16.3.225 [root@bogon ~]# curl http://172.16.3.200:8080 172.16.3.226 curl http://192.168.1.104:8068 在down掉主节点后 vip自动漂移到了192.168.0.106备节点上\n在down掉主节点后 访问 192.168.0.188:8068 还是可以访问到192.168.0.105和192.168.0.106\n在down掉主节点后 访问 192.168.0.188:8068 还是可以访问到192.168.0.105和192.168.0.106 # 在down掉主节点后vip自动漂移到192.168.0.106备节点 # 1，手动添加vip\n1,ifconfig查看当前活动网卡。如：eth0\n2,执行\nifconfig eth0:0 166.111.69.100 netmask 255.255.255.0 up ifconfig ens192:0 166.111.69.100 netmask 255.255.255.0 up 进行vip添加\n3，执行ifconfig查看是否生效\n4，测试 ping 166.111.69.100\n5,写在/etc/rc.local里进行开机自动设置\n2，第三方工具keepalived增加vip\n1，下载与安装\n安装：\ntar xf keepalived-1.2.12.tar.gz cd keepalived-1.2.12 ./configure --prefix=/usr/local/keepalived make \u0026amp;\u0026amp; make install cp /usr/local/keepalived/etc/rc.d/init.d/keepalived /etc/init.d/ cp /usr/local/keepalived/etc/sysconfig/keepalived /etc/sysconfig/ mkdir /etc/keepalived cp /usr/local/keepalived/etc/keepalived/keepalived.conf /etc/keepalived/ cp /usr/local/keepalived/sbin/keepalived /usr/sbin/ 2，配置：\nvim /etc/keepalived/keepalived.conf 如下： ! Configuration File for keepalived global_defs { notification_email { saltstack@163.com } notification_email_from dba@dbserver.com smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id MySQL-HA } vrrp_instance VI_1 { state BACKUP interface eth1 virtual_router_id 51 priority 150 advert_int 1 nopreempt authentication { auth_type PASS auth_pass 1111 } virtual_ipaddress { 192.168.1.88 } } 红色区域为需要修改的地方：\nnotification_email :变更后通知的email\nnotification_email_from :发送email\nrouter_id MySQL-HA :路由器组id，局域网中需要共享该vip的服务器，该配置要一致。\nstate BACKUP：在keepalived中2种模式，\n分别是master-\u0026gt;backup模式和backup-\u0026gt;backup模式。\n配置keepalived ping不通 解决办法 # vim /etc/keepalived/keepalived.conf 把这个注释就可以了 vrrp_strict "},{"id":153,"href":"/docs/%E4%BD%BF%E7%94%A8-openfunction-%E5%9C%A8%E4%BB%BB%E4%BD%95%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD%E4%B8%8A%E8%BF%90%E8%A1%8C%E6%97%A0%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%B7%A5%E4%BD%9C%E8%B4%9F%E8%BD%BD-shi-yong-openfunction-zai-ren-he-ji-chu-she-shi-shang-yun-xing-wu-fu-wu-qi-gong-zuo-fu-zai/","title":"使用 OpenFunction 在任何基础设施上运行无服务器工作负载 2024-01-21 04:02:44.459","section":"Docs","content":"云原生技术的崛起使得我们可以以相同的方式在公有云、私有云或本地数据中心运行应用程序或工作负载。但是，对于需要访问不同云或开源中间件的各种 BaaS 服务的无服务器工作负载来说，这并不容易。在这次演讲中，OpenFunction 维护者将详细介绍如何使用 OpenFunction 解决这个问题，以及 OpenFunction 的最新更新和路线图：\n使用 Dapr 将 FaaS 与 BaaS 解耦 使用 Dapr 代理而不是 Dapr sidecar 来加速函数启动 使用 Kubernetes Gateway API 构建 OpenFunction 网关 使用 WasmEdge 运行时运行 WebAssembly 函数 OpenFunction 在自动驾驶行业的应用案例 最新更新和路线图\n构建开源 FaaS 平台的必要性 什么是 Serverless？加州大学伯克利分校在论文 《A Berkeley View on Serverless Computing》给出了明确定义：Serverless computing = FaaS + BaaS。\n对于函数计算平台，函数是不可或缺的，即 FaaS 是主体。同时，FaaS 也需要和后端的 BaaS 服务产生联系，所以丰富的后端服务是函数的重要依托。\n云厂商通常提供托管的函数计算（FaaS）和各类后端中间件服务，这样就会把开发者锁定在自己的云平台之上。\n现阶段我们也看到，有一些公司因为云上的成本过高，想要下云或者从一个云迁移到另一个云也就是跨云迁移。如果其函数绑定在云的 BaaS 服务上，则不利于跨云的迁移。所以，跨云迁移之后如何去处理各个云厂商 BaaS 服务接口的差异，成了目前较大的挑战。 从另一个角度看，一个 FaaS 平台通常需要支持多种语言，也会利用到众多后端服务。举例来讲，5 种语言需要和 10 种后端服务对接，那么这样做就会有 5×10 即 50 种实现， 还是比较复杂的。 构建开源 FaaS 平台的可行性 如何解决上述问题呢？我们可以引入 Dapr 来简化函数与众多后端服务的交互。\nDapr 是一个分布式应用的运行时，能够把分布式应用的能力抽象成一个个 Building Block。 举几个例子来讲，一般的分布式应用程序都有 service 之间的相互调用，所以会有一个 service 的 Building Block；通常也会有 publish、subscribe 这样的模式，所以也会有一个 publish and subscribe 的 Building Block；此外还会有一些输入输出，所以有一个 Binding Building Block；当然还有其他的，可参考上图。 这些 Building Block 会由一系列的 components 支撑，比如 PubSub Brokers 可以支持各种云上的 MQ、AWS sqs 或者一些开源的中间件如 Redis、Kafka；Bindings 也会支持一些云上的存储和开源组件如 Kafka、MySQL、Redis。\n所以 Dapr 就能解决上文提到的问题。FaaS 平台的每一种语言，只需要和 Dapr 交互，Dapr 的 API 再和构成每一个 Building Block 的 component 交互，通过 Dapr 去处理与众多后端服务的交互，这样就把复杂度从 N × M 降为了 N × 1.\n另外 Dapr 解耦了 FaaS 平台与各云厂商的 BaaS，做到了云厂商中立，解决了跨云迁移的问题。 OpenFunction 简介 OpenFunction 是什么 OpenFunction 是青云科技在 2022 年初开源的，在 2022 年 4 月成为了 CNCF 的 Sandbox 项目。\n云厂商中立 与各个云厂商的 BaaS 服务松耦合 通过 Dapr，简化了与各云厂商或开源 BaaS 服务的集成 同时支持同步与异步函数 同步函数基于 Kubernetes Gateway API 实现了 OpenFunction Gateway 作为函数⼊⼝ 异步函数可直接从事件源消费事件，并可根据事件源特有的指标自动伸缩 支持直接从函数代码生成符合 OCI 标准的函数镜像 基于 Cloud Native Buildpacks 实现 支持 0 与 N 之间的水平自动伸缩 既能运行函数，也能运行 Serverless 应用 支持 Wasm Runtime 更完善的 CI/CD 上图是 OpenFunction 的架构图，总体分为：Build，Function，Serving 和 Events 几个部分。\nFunction：Function 是一个主控模块，控制函数的构建和 serving，Build、Serving 和 Status 等信息也会在 Function 的 CRD 中显示。 Build：支持使用 Buildpacks 的方式构建函数的镜像，也支持使用 Dockerfile 的方式构建 Serverless 应用；后端技术是 Shipwright，Shipwright 可以切换构建镜像的引擎，所以可以通过 Shipwright 选择不同的镜像构建工具，最终将应用构建为容器镜像。 Serving : 通过 Serving CRD 将应用部署到不同的运行时中，可以选择同步运行时或异步运行时。同步运行时可以通过 Knative Serving 或者 Keda-HTTP 来支持，异步运行时通过 Dapr+Keda 来支持。同时现在也支持 WasmEdgeRuntime 来支持 Wasm 函数。 Events : 对于事件驱动型函数来说，需要提供事件管理的能力。由于 Knative 事件管理过于复杂，所以我们研发了一个新型事件管理驱动叫 OpenFunction Events。 OpenFunction Events EventBus 利用 Dapr 的能力解耦了 EventBus 与底层具体 Message Broker 的绑定，你可以对接各种各样的 MQ。\nOpenFunction Gateway OpenFunction Gateway 是 OpenFunction 0.7.0 增加的新特性，是基于 Kubernetes Gateway API 来实现的。之所以选择 Kubernetes Gateway API，是因为其 CRD 和其下游实现是解耦的，用户可以选择自己喜欢的 Gateway 实现，比如 APISIX、Istio、Contour 等；另外 Kubernetes Gateway API 也提供了一些新的特性，比如 HTTP 流量的分发、跨 Namespace routing 功能。 在之前 OpenFunction 需要把流量转发到 Knative 的 Gateway，然后再路由到 Knative 的 Revision，链路比较长。有了 OpenFunction Gateway，可以直接把流量转发到 Knative 的 Revision。也就是说可以不再依赖 Knative 网络相关的组件，整个流量转发的链路也会变短。\n为什么引入 OpenFunction Gateway 通过 Knative Gateway 访问同步函数需指定⼀个由随机串组成的 service url，不可预测且对⽤户不友好。\n通过 OpenFunction Gateway 访问同步函数，可通过函数名及 namespace ⽣成函数访问 url。另外还可以通过 Gateway 的 Service 加上 Function 的 namespace 和 name 来进行 path-based 的访问，也可以基于 OpenFunction Gateway 的 IP 加上 Host 相关的 Headers 来进行 host-based 访问。\n如果我们想在集群外部访问集群内部的 Function，我们可以在 OpenFunction Gateway 上面配置 domain 相关的字段，配置成 magic DNS，这样我们就可以直接在集群外部通过域名来访问集群内部的 Function。\n冷启动优化 冷启动优化一直是 FaaS 平台的难点。\n之前我们采用的是 Dapr sidecar 模式，但是会影响 Function 启动的时间。函数很小的情况下，Dapr sidecar 的 container 启动时间以及 Dapr client 初始化所需要的时间，比 Function 启动时间更长。\n我们设计了 Dapr Proxy 模式，就是让所有 Function 的 pod 共享一个 Dapr sidecar。如果我们将 Function 扩容到很大的值，副本数较多，那么 sidecar 也会造成大量的资源开销，采用这个模式将有效避免这种情况。 接下来我们还有一个基于 Pool 的冷启动优化的计划。上文中提到的 Proxy 模式，对 Function 启动时间的优化是有限的。所以我们考虑采用 Pool 的方式来优化冷启动。\n引入一个预创建的 Pod Pool，在有调用请求时，可以根据请求的相关信息判断要调用哪个 Function，然后对这个 Function 的 code 热加载，将其变成针对某个 Function 的 Pod，后续的流量就可以直接进入到这个特定的 Function Pod，这个过程不需要 K8s Pod 调度创建等逻辑的参与，所以这个方式对冷启动的时间优化是非常显著的。 ⽀持 WasmEdge 作为 Wasm 运⾏时 在 OpenFunction 1.0.0 中，我们支持 WasmEdge 作为 Wasm 运行时。WebAssembly 作为 FaaS 平台运行时，具有很多优势，比如启动时间短、镜像体积小、安全性比较高。另外 WasmEdge 对 HTTP 支持也比较好。\n但是目前 Wasm Function 访问后端服务有一个问题，因为 WebAssembly 程序访问 API 是受限的，所以也需要通过 Dapr 提供相对统一的方式来完成对后端服务的访问。\n目前 Dapr、Rust、SDK 以及 Dapr WasmEdge 等相关的项目，还不够成熟，所以我们暂时还没有集成。后续相关生态成熟之后，再进行集成。\nOpenFunction 在⾃动驾驶领域的应⽤ 下面介绍一个将 OpenFunction 用在自动驾驶领域的案例，来自 OpenFunction 的社区用户——驭势科技。\n简单来讲，自动驾驶就是车端会上传很多传感器的数据到云端，云端再对这些数据进行处理。 上图是云端的架构图。举例来说，车端的 MQTT Broker 将云端的数据传到云端的 MQ 上，运维人员会创建多种异步函数和同步函数来处理这些数据。MQTT topic 的数据会由一个函数处理，其他数据由另外一个函数处理，处理完的数据分别存到了不同的后端服务里，因为业务比较多，所以可能是不同的团队去去实现的，那么就需要用不同的语言去实现。\n异步：消息队列实时数据 → Prometheus 指标 驭势科技还有一个比较高阶的使用例子。\n车端上传 MQTT 的数据，通过异步函数从 MQTT 的数据提取出 Metrics，再将其发送到 Prometheus 的 Pushgateway，这种方式相当于将车端的数据变成了 Metrics 再存储下来。 为什么⾃动驾驶需要云⼚商中⽴的 Serverless 平台？ 对于云商中立的需求 不同的客户要求部署到不同的云厂商 一些客户的车端数据比较敏感，要求放到和公有云隔离的环境 不同的云⼚商有不同的后端服务，如果没有⼀个云⼚商中⽴的云平台，对于同⼀处理逻辑则需要为对接的每⼀个云⼚商都实现相似的服务 对于 Serverless 的需求 数据处理逻辑多样同时经常变化，来⾃同⼀数据源的数据在不同场景下的处理逻辑不尽相同 ⾃动驾驶涉及的模块较多，不同的模块由不同的团队负责，需要多语⾔⽀持 ⼤量⻋端数据需要实时处理；⾃动驾驶⻋辆也有潮汐的特性，数据处理需求有⾼峰和低⾕ 社区、路线图与 Demo OpenFunction 路线图 函数框架 ⽀持 Dapr State Management 与 Dapr Workflow ⽀持更多语⾔的异步函数框架包括 Python、Rust ⽀持将 Java 函数编译成 Native 程序运⾏在 Quarkus 环境中 函数运行时 实现 Serverless ⼯作流（Workflow） 预研基于 Pod Pool 的冷启动优化⽅案 ⽀持 OpenTelemetry 作为另⼀个函数追踪⽅案 用户工具 增加 OpenFunction 控制台 与 AIGC 结合 早起贡献及应用者 越来越多的社区贡献者 主要 Maintainer 来⾃ KubeSphere 团队 SkyWalking PMC 成员 @arugal 实现了 SkyWalking 和 OpenFunction Go Functions Framework 的集成 驭势科技 (UISEE) @webup @kehuili 以及印度的贡献者正在参与 Node.js 和 Python Functions Framework 的开发 SAP @lizzze 参与 functions-framework-go 的开发 来⾃阿⾥云 、微众银⾏等社区贡献者也在积极参与 乌克兰的贡献者在帮忙维护 openfunction.dev 越来越多的公司开始采用 国内某电信公司采用 OpenFunction 构建云函数计算平台 驭势科技（UISEE）采用 OpenFunction 处理车云数据 微众银行 某证券公司 喜马拉雅 云学堂 参与 OpenFunction 社区 欢迎各位小伙伴参与社区！\nOpenFunction: https://github.com/OpenFunction/OpenFunction Website: https://openfunction.dev/ Samples: https://github.com/OpenFunction/samples 附录：函数示例 同步函数触发异步函数示例 同步函数： https://github.com/OpenFunction/samples/tree/main/functions/knative/with-output-binding\n异步函数： https://github.com/OpenFunction/samples/tree/main/functions/async/bindings/kafka-input\n更多函数示例 Keda HTTP Engine: https://github.com/OpenFunction/OpenFunction/blob/main/config/samples/function-kedahttp-sample-serving.yaml\nWasm 函数: https://github.com/OpenFunction/samples/tree/main/functions/knative/wasmedge/http-server\nRedis state store:\nhttps://github.com/OpenFunction/java-samples/blob/main/src/main/resources/functions/redis-state-store.yaml https://github.com/OpenFunction/java-samples/blob/main/src/main/java/dev/openfunction/samples/StateStore.java Serverless Applications:\nwith a Dockerfile: https://github.com/OpenFunction/samples/tree/main/apps/buildah/go without a Dockerfile: https://github.com/OpenFunction/samples/tree/main/apps/buildpacks/java\n"},{"id":154,"href":"/docs/%E4%BD%BF%E7%94%A8cloudflarecf%E6%90%AD%E5%BB%BAdockerhub%E4%BB%A3%E7%90%86-shi-yong-cloudflarecf-da-jian-dockerhub-dai-li/","title":"使用cloudflare(CF)搭建dockerhub代理 2024-06-28 14:38:34.339","section":"Docs","content":" 使用cloudflare(CF)搭建dockerhub代理 # 前言 # 目前国内docker所有域名都被屏蔽，造成一些玩docker的用户很是苦恼，更换阿里云的镜像加速但镜像也没dockerhub那么多，有些好用的工具一直拉不下来，自己搭建dockerhub镜像站又耗时还得购买海外服务器，非常不划算。本文按照B站一个大佬的方法为此我撰写一篇文章用最简单和最清晰的思路。\n准备环境 # 注册cloudflare账户（必须） 注册github账户（必须） 购买域名并绑定在cloudflare域下（可选） 虽然cf默认给你分配一个免费的域名，但是这个域名我试了一下解析非常慢，建议挂自己域名。\n关于如何购买并绑定到cf本文不再赘述，网上有非常多的教程可以自行搜索。\n一、克隆github项目到自己的库 # 访问此网站 克隆到自己仓库 由于我这里已经克隆过了，大家没有克隆过的点击加号会显示的页面直接点击右下角即可。\n二、部署到cloudflare # 绑定github \\2. 部署源码\n选择仓库后一直下一步过程无需选择其它，直接点到此页面\n至此部署环节已完成，参考此视频编写\n如何使用 # 可以在拉取镜像名称前加入此域名，例如 https://\u0026lt;域名\u0026gt;/镜像名:lates 可以参考下面方法换源永久实现 三、更换docker源 # 可以参考我这篇文章\n四、需要注意 # 由于是cloudflare网络，国内访问比较缓慢，所以下载速度肯定没有国内镜像站快，但是绝大部分还是可以成功下载的。 此方案并不是完全免费，cloudlfare每个人免费配额为10000次请求，如果是你自己使用完全足够，每天能拉几百次。但是如果公开到网上可能就会到达上限，因此不要随意将此域名公开到网上以防被刷量。我们可以在概览页面查看请求量 增加dns解析cloudsx.top到刚刚cl自动生成的域名 # cloudsx.top代理的dockerhub页面 # 拉镜像 # 阿里云配置cloudsx.top DNS解析 # cloudflare配置cloudsx.top DNS解析 代理cloudflare的解析 # 访问cloudsx.top成功 # "},{"id":155,"href":"/docs/%E4%BD%BF%E7%94%A8kubernees-leases-%E8%BD%BB%E6%9D%BE%E5%AE%9E%E7%8E%B0leader-election-shi-yong-kuberneesleases-qing-song-shi-xian-leaderelection/","title":"使用kubernees leases 轻松实现leader election 2024-04-03 15:09:46.129","section":"Docs","content":" 假如你正在创建一个数据复制服务，该服务只需要一个实例来处理数据以保持正确的顺序。或者考虑开发一个多人在线游戏，在这种游戏中，同步所有参与者之间的游戏状态至关重要。这些类型的服务通常需要单个实例专门处理特定任务，并且还需要备用副本在当前实例失败时能够迅速接管工作负载。 # 解决此类挑战常用的技术之一是 leader election。它通过使组/集群中的节点参与投票或共识过程来选择单个 Leader 实例。然后该Leader 承担需要集中协调的责任。如果选举失败，则其余节点会自动重新运行选举过程以选择新的 Leader，确保系统连续性和容错性。 # 实现的复杂性 # 在一个组中达成所有节点对 Leader 的共识，对于实现 Leader 选举至关重要。 # 基本的实现可以利用锁服务来确定领导权。 # 在这种情况下，每个实例都尝试获取一个共享锁。锁服务确保只有一个实例在任何给定时间持有该锁，有效地使该实例成为 Leader。其他副本不断尝试获取锁，从而准备好进行无缝故障转移，以防当前的 Leader 变得不可用。 # 然而，在某些情况下会出现挑战，例如当领导节点失效和无响应时，并且持有锁一段时间直到该实例被回收。这种情况可能会导致传入请求因超时而失败，并且由于备用副本无法承担领导职责而增加正在进行任务的延迟。为了缓解这个问题，通常建议锁服务在锁上实施过期机制，通常称为基于 TTL 的租约。 # 问题1：失效的 Leader 实例 # 此外，锁服务必须符合一定的健壮性标准。一方面，它不能作为单个实例运行，因为这将使其成为单点故障。另一方面，如果它使用多个实例以提供高可用性，则必须设计能够抵御网络分区问题。否则可能导致一个棘手的情况发生：在不同网络分区中的两个副本都认为自己已经获得了锁，并因此承担领导者角色。 # 问题2：脑裂 # 基于仲裁的系统成为首选解决方案 # 为了解决网络分区的核心问题，像 ZooKeeper（被 Kafka 使用）和 etcd（被 Kubernetes 利用）这样的基于仲裁的系统通过内置的 TTL 功能应运而生，将自己确立为领导者选举的最佳解决方案。ZooKeeper 原子广播（ZAB）协议和 etcd 采用的 RAFT 协议都确保只有在大多数节点认可时才会考虑任何操作是有效的。在发生网络分区时，只有一个分区可以包含大多数节点，有效地防止了出现多个领导者的可能性。 # 然而，无论是 ZooKeeper 还是 etcd 都不具备成本效益。分布式系统固有复杂性带来了运维挑战。为小规模服务部署这样的集群可能太重，并且额外开销可能超过应用程序本身的运维成本。 # K8s Leases # 由于成本问题，可靠的领导者选举只适用于大规模系统吗？未必，如果你正在使用 Kubernetes 基础设施，则可以通过使用 Kubernetes Leases 来简单地实现 etcd 级别的领导者选举 —— 无需直接与 Etcd API 交互。 # 为什么很强大 # Kubernetes Leases 之所以强大，部分原因在于 Kubernetes 本身通常将 etcd 作为其所有 API对象（包括租约）的存储。这意味着 etcd 的高可用性、一致性和容错特性自然地可用于 Kubernetes Leases。但是，Kubernetes API 添加了额外的功能层，比如通过资源版本控制实现乐观并发控制，这也有助于使租约成为在 Kubernetes 环境中进行领导者选举时一个强大的选择。 # 工作原理 # 在 Kubernetes 中，可以使用 Lease 对象来促进领导者选举。竞争领导权的候选人要么创建要么更新 Lease 对象，并将其标识符设置在 holderIdentity 字段中。领导者持续“续订”该 Lease 以保持其角色。如果领导者未能在 Lease 到期前续订，则其他候选人会尝试获取它。首个成功更新 Lease 的候选人将成为新领导者，确保平稳故障转移。所有候选人都会监视 Lease 对象以跟踪领导变更。 # 示例 # 让我们通过一个全面的示例来了解下这是如何工作的。这里我们通过直接对 K8s API 进行 HTTP 调用，该演示与语言无关。 # 需要一个版本大于等于 1.14 的 Kubernetes 集群。需要安装一些基本的工具：kubectl、curl、jq 等等。\n首先我们创建一个如下锁是的 JSON 文件 lease.json，用来代表 Lease 对象： # { \u0026#34;apiVersion\u0026#34;: \u0026#34;coordination.k8s.io/v1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Lease\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;example-lease\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;example-lease\u0026#34; }, \u0026#34;spec\u0026#34;: { \u0026#34;holderIdentity\u0026#34;: \u0026#34;replica-1\u0026#34;, \u0026#34;leaseDurationSeconds\u0026#34;: 15 } } 然后我们可以使用下面的 curl 命令来创建该对象： # curl -k -X POST -H \u0026#34;Content-Type: application/json\u0026#34; --data @lease.json https://$KUBE_PROXY_URL/apis/coordination.k8s.io/v1/namespaces/example-lease/leases 检查 Lease 对象是否创建成功： # kubectl get lease example-lease -n example-lease -o json #output: { \u0026#34;apiVersion\u0026#34;: \u0026#34;coordination.k8s.io/v1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Lease\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;example-lease\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;example-lease\u0026#34;, \u0026#34;resourceVersion\u0026#34;: \u0026#34;123\u0026#34; }, \u0026#34;spec\u0026#34;: { \u0026#34;holderIdentity\u0026#34;: \u0026#34;replica-1\u0026#34;, \u0026#34;leaseDurationSeconds\u0026#34;: 15, \u0026#34;acquireTime\u0026#34;: \u0026#34;2023-09-11T20:30:00Z\u0026#34;, \u0026#34;renewTime\u0026#34;: \u0026#34;2023-09-11T20:35:00Z\u0026#34;, \u0026#34;leaseTransitions\u0026#34;: 2 } } 现在 replica-1 已成功获取租约，并将在15秒后到期。 # 接下来创建创建一个名为 standby_replica 的脚本。此任务在每次更新 Lease 对象时检查 renewTime 字段，以确定当前 leader 是否仍处于活动状态。如果 renewTime 比当前时间落后了定义的阈值，则假定 leader 处于非活动状态。然后，该任务尝试通过使用当前 resourceVersion 更新来获取租约。如果成功，运行该任务的 pod 将成为新的 leader，并应继续更新 renewTime 以保持其状态。 # # standby_replica.sh #!/bin/bash LEASE_ENDPOINT=\u0026#34;http://$KUBE_PROXY_URL/apis/coordination.k8s.io/v1/namespaces/NAMESPACE/example-lease/LEASE_NAME\u0026#34; REPLICA_NAME=\u0026#34;replica-2\u0026#34; # implement watch. Please note that there is a watch endpoint, which is more efficeint when writing production code while true; do # Fetch the lease object lease=$(curl -s $LEASE_ENDPOINT) # Extract renewTime and resourceVersion renew_time=$(echo $lease | jq -r \u0026#39;.spec.renewTime\u0026#39;) resource_version=$(echo $lease | jq -r \u0026#39;.metadata.resourceVersion\u0026#39;) current_time=$(date -u +\u0026#34;%Y-%m-%dT%H:%M:%SZ\u0026#34;) if [[ \u0026#34;$renew_time\u0026#34; \u0026lt; \u0026#34;$current_time\u0026#34; ]]; then updated_lease=$(echo $lease | jq \u0026#34;.metadata.resourceVersion = \\\u0026#34;$resource_version\\\u0026#34; | .spec.renewTime = \\\u0026#34;$current_time\\\u0026#34; | .spec.holderIdentity = \\\u0026#34;$REPLICA_NAME\\\u0026#34;\u0026#34;) # Attempt to update the lease update_response=$(curl -s -X PUT --header \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#34;$updated_lease\u0026#34; $LEASE_ENDPOINT) if [[ $(echo $update_response | jq -r \u0026#39;.metadata.resourceVersion\u0026#39;) == \u0026#34;$resource_version\u0026#34; ]]; then echo \u0026#34;Successfully acquired the lease.\u0026#34; else echo \u0026#34;Failed to acquire the lease.\u0026#34; fi else echo \u0026#34;Lease is still valid.\u0026#34; fi # Wait for 5 seconds before the next iteration sleep 5 done 如果我们一直去查看对象，有对象输出，renewTime 会不断更新。因此，没有发生领导者故障转移。应该能够看到输出为： # Lease is still valid. Lease is still valid. Lease is still valid. Lease is still valid. Lease is still valid. 当停止更新来自 replica-1 的租约时，到期时间将在15秒内到期，以便 replica-2 能够接管领导权。您应该能够看到输出为： # Lease is still valid. Lease is still valid. Successfully acquired the lease. # And then fetch the example lease again: kubectl get lease example-lease -n example-lease -o json #output: { \u0026#34;apiVersion\u0026#34;: \u0026#34;coordination.k8s.io/v1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Lease\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;example-lease\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;example-lease\u0026#34;, \u0026#34;resourceVersion\u0026#34;: \u0026#34;124\u0026#34; }, \u0026#34;spec\u0026#34;: { \u0026#34;holderIdentity\u0026#34;: \u0026#34;replica-2\u0026#34;, \u0026#34;leaseDurationSeconds\u0026#34;: 15, \u0026#34;acquireTime\u0026#34;: \u0026#34;2023-09-11T20:36:00Z\u0026#34;, \u0026#34;renewTime\u0026#34;: \u0026#34;2023-09-11T20:51:00Z\u0026#34;, \u0026#34;leaseTransitions\u0026#34;: 3 } } 总结 # 在 Kubernetes 中利用 Lease 对象进行领导者选举提供了一种轻量级但强大的替代方案，而不是运行像 ZooKeeper 或 etcd 这样的重型协调服务。通过依赖原生 Kubernetes API 对象，您可以获得相同级别的容错能力，而无需处理维护单独分布式协调服务所带来的运维复杂性和资源开销。 # 在实现方面，每个 Pod 都运行一个后台任务来观察 Lease 对象的变化。通过检查 renewTime 字段，该任务可以确定当前领导者是否仍然可操作。如果此时间已过期，则表明领导权已经开放重新选举。然后 Pod 尝试通过使用自己的 holderIdentity 和当前 resourceVersion 更新 Lease 对象来获取或更新租约。成功更新 Lease 的 Pod 承担新领导者角色。这种方法为在 Kubernetes 环境中实现领导者选举提供了一种简单而有效的方式。 # 这使其成为对于在 Kubernetes 环境中运行的应用程序更易访问和高效选择。 # "},{"id":156,"href":"/docs/%E5%85%AC%E6%9C%89%E4%BA%91%E4%B8%8A%E9%83%A8%E7%BD%B2-kubeasz-gong-you-yun-shang-bu-shu-kubeasz/","title":"公有云上部署 kubeasz 2023-09-28 15:31:16.19","section":"Docs","content":" 公有云上部署 kubeasz # 在公有云上使用kubeasz部署k8s集群需要注意以下几个常见问题。\n安全组 # 注意虚机的安全组规则配置，一般集群内部节点之间端口全部放开即可；\n网络组件 # 一般公有云对网络限制较多，跨节点 pod 通讯需要使用 OVERLAY 添加报头；默认配置详见example/config.yml\nflannel 使用 vxlan 模式：FLANNEL_BACKEND: \u0026quot;vxlan\u0026quot; calico 开启 ipinip：CALICO_IPV4POOL_IPIP: \u0026quot;Always\u0026quot; kube-router 开启 ipinip：OVERLAY_TYPE: \u0026quot;full\u0026quot; 节点公网访问 # 可以在安装时每个节点绑定弹性公网地址(EIP)，装完集群解绑；也可以开通NAT网关，或者利用iptables自建上网网关等方式\n负载均衡 # 一般云厂商会限制使用keepalived+haproxy自建负载均衡，你可以根据云厂商文档使用云负载均衡（内网）四层TCP负载模式；\nkubeasz 2x 版本已无需依赖外部负载均衡实现apiserver的高可用，详见 2x架构 kubeasz 1x 及以前版本需要负载均衡实现apiserver高可用，详见 1x架构 时间同步 # 一般云厂商提供的虚机都已默认安装时间同步服务，无需自行安装。\n访问 APISERVER # 在公有云上安装完集群后，需要在公网访问集群 apiserver，而我们在安装前可能没有规划公网IP或者公网域名；而 apiserver 肯定需要 https 方式访问，在证书创建时需要加入公网ip/域名；可以参考这里修改 APISERVER（MASTER）证书\n在公有云上部署多主高可用集群 # 处理好以上讨论的常见问题后，在公有云上使用 kubeasz 安装集群与自有环境没有差异。\n使用 kubeasz 2x 版本安装单节点、单主多节点、多主多节点 k8s 集群，云上云下的预期安装体验完全一致 "},{"id":157,"href":"/docs/2024-5-14-%E5%8D%95master%E5%8D%95etcd%E6%94%B9%E9%80%A0/","title":"单master单etcd改造为3master3etcd","section":"Docs","content":"备份etcd的数据\r1、单master单etcd扩展到3master3etcd节点\r2、涉及的 master节点的证书\r- apiserver.crt\r- apiserver.csr\r- apiserver.key\r- client.crt\r- client.key\r- etcd_client.crt\r- etcd_client.key\r- front-proxy_bak-ca.crt\r- front-proxy_bak-ca.key\r- front-proxy_bak-ca.srl\r- front-proxy_bak-client.crt\r- front-proxy_bak-client.key\r- front_proxy_ssl.cnf\r- sa.key\r- sa.pub - etcd_server.crt\r- etcd_server.key\r- etcd_client.crt\r- etcd_client.key\rwork节点的证书\r- ca.crt - client.key - client.crt 需要重新生成，ca.crt和ca.key可以不用重新生成，新的证书需要准备好，老的证书需要备份好。\r3、根据ca证书和master_ssl.cnf，etcd_ssl.cnf生成新的master节点和etcd节点证书，根据ca证书和master_ssl.cnf生成work节点的client.crt证书，提前准备好。 master_ssl.cnf包含3个master节点的ip和硬负载vip，etcd_ssl.cnf包含3个etcd节点的ip。\r4、拷贝新生成的etcd证书到etcd节点，替换老的证书，重新etcd节点。\r5、拷贝新生成的master节点的节点到master节点，替换老的master节点证书，重启master节点的master服务，包括kube-apiserver，kube-scheduler，kube-controller-manager，kubelet，kube-proxy。\r6、拷贝新生成的work节点的证书到work节点，替换work节点的证书，并重启node节点的kubelet和kube-proxy服务。\r7、验证master节点的相关服务是否正常，包括kube-apiserver，kube-scheduler，kube-controller-manager，kubelet，kube-proxy，etcd服务。node节点的kubelet和kube-proxy服务。\r回退，删除新加的2个master节点，将老的备份的证书替换回去，重启master节点和node节点的相关服务 模拟单master单etcd节点 扩展到 3master3etcd\r现状：\rmaster1 192.168.0.61\rnode1 192.168.0.64\r改造后\rmaster1：192.168.0.61\rmaster2：192.168.0.62\rmaster3：192.168.0.63\rnode1： 192.168.0.64\rlb1： 192.168.0.71\rlb2： 192.168.0.72\rvip： 192.168.0.70 1、备份etcd数据\r2、停止所有节点的kube-apiserver，kube-scheduler，kube-controller-manager，etcd，kubelet，kube-proxy服务\r3、修改all.yml里的master节点的vip为真实的vip，hosts文件增加master组和etcd组，node组 2个新增的master节点的ip\r4、备份老的证书文件夹下的文件，执行ansible-playbook -i hosts setup.yml，只包含创建证书这一步\r在/opt/kubernetes_ssl下重新生成apiserver.crt\rapiserver.csr\rapiserver.key\rca.crt\rca.key\rca.srl\rclient.crt\rclient.csr\rclient.key\retcd_client.crt\retcd_client.csr\retcd_client.key\retcd_server.crt\retcd_server.csr\retcd_server.key\retcd_ssl.cnf\rfront-proxy-ca.crt\rfront-proxy-ca.key\rfront-proxy-ca.srl\rfront-proxy-client.crt\rfront-proxy-client.csr\rfront-proxy-client.key\rfront_proxy_ssl.cnf\rmaster_ssl.cnf\rsa.key\rsa.pub\r证书 单master 单etcd pod pvc sc，需要扩容后数据任然存在 # 备份etcd数据 # [root@master1 apps]# cat etcd_backup.sh\r#!/bin/bash\r#etcd地址端口，根据环境进行修改\r#etcd_url_1=\u0026#34;https://192.168.0.1:1183\u0026#34;\r#etcd_url_2=\u0026#34;https://192.168.0.2:1183\u0026#34;\r#etcd_url_3=\u0026#34;https://192.168.0.3:1183\u0026#34;\r#etcd备份数据路径，根据环境进行修改\rbakdata_basedir=\u0026#34;/apps/etcd_backup/\u0026#34;\r#etcd 开启https的需要，添加ca及etcd证书，根据环境进行修改\rcacert_file=\u0026#34;/etc/kubernetes/ssl/ca.crt\u0026#34;\retcd_cert_file=\u0026#34;/etc/kubernetes/ssl/etcd_server.crt\u0026#34;\retcd_key_file=\u0026#34;/etc/kubernetes/ssl/etcd_server.key\u0026#34;\r#获取etcd leader地址\rleader_url=`ETCDCTL_API=3 sudo etcdctl --endpoints=https://192.168.0.61:1159,https://192.168.0.62:1159,https://192.168.0.63:1159 --cacert=$cacert_file --cert=$etcd_cert_file --key=$etcd_key_file endpoint status -w table | awk -F\u0026#39;|\u0026#39; \u0026#39;$6~\u0026#34;true\u0026#34;{print $2}\u0026#39;`\r##echo \u0026#34;leader地址：\u0026#34;$leader_url\r#创建备份数据目录\rif [ ! -d $bakdata_basedir ]; then\rmkdir -p $bakdata_basedir\rfi\r#开始备份\rETCDCTL_API=3 sudo etcdctl --endpoints=\u0026#34;$leader_url\u0026#34; --cacert=$cacert_file --cert=$etcd_cert_file --key=$etcd_key_file snapshot save $bakdata_basedir`date +%Y-%m-%d`-etcd_back.db\rif [ \u0026#34;$?\u0026#34; == \u0026#34;0\u0026#34; ]; then\recho \u0026#34;backup success!\u0026#34;\relse\recho \u0026#34;backup failed!\u0026#34;\rfi\r#删除早期备份数据\rfind /apps/etcd_backup -type f -mtime +60 -exec rm -f {} \\; 执行etcd备份 # [root@master1 apps]# sh etcd_backup.sh\r{\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2024-05-11T16:20:13.600+0800\u0026#34;,\u0026#34;caller\u0026#34;:\u0026#34;snapshot/v3_snapshot.go:65\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;created temporary db file\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/apps/etcd_backup/2024-05-11-etcd_back.db.part\u0026#34;}\r{\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2024-05-11T16:20:13.606+0800\u0026#34;,\u0026#34;logger\u0026#34;:\u0026#34;client\u0026#34;,\u0026#34;caller\u0026#34;:\u0026#34;v3/maintenance.go:211\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;opened snapshot stream; downloading\u0026#34;}\r{\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2024-05-11T16:20:13.606+0800\u0026#34;,\u0026#34;caller\u0026#34;:\u0026#34;snapshot/v3_snapshot.go:73\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;fetching snapshot\u0026#34;,\u0026#34;endpoint\u0026#34;:\u0026#34;https://192.168.0.61:1159\u0026#34;}\r{\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2024-05-11T16:20:13.705+0800\u0026#34;,\u0026#34;logger\u0026#34;:\u0026#34;client\u0026#34;,\u0026#34;caller\u0026#34;:\u0026#34;v3/maintenance.go:219\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;completed snapshot read; closing\u0026#34;}\r{\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2024-05-11T16:20:13.711+0800\u0026#34;,\u0026#34;caller\u0026#34;:\u0026#34;snapshot/v3_snapshot.go:88\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;fetched snapshot\u0026#34;,\u0026#34;endpoint\u0026#34;:\u0026#34;https://192.168.0.61:1159\u0026#34;,\u0026#34;size\u0026#34;:\u0026#34;6.1 MB\u0026#34;,\u0026#34;took\u0026#34;:\u0026#34;now\u0026#34;}\r{\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2024-05-11T16:20:13.711+0800\u0026#34;,\u0026#34;caller\u0026#34;:\u0026#34;snapshot/v3_snapshot.go:97\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;saved\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/apps/etcd_backup/2024-05-11-etcd_back.db\u0026#34;}\rSnapshot saved at /apps/etcd_backup/2024-05-11-etcd_back.db\rbackup success!\r[root@master1 apps]# ls -la /apps/etcd_backup/\r总用量 5932\rdrwxr-xr-x. 2 root root 37 5月 11 16:20 .\rdrwxr-xr-x. 7 root root 174 5月 11 09:35 ..\r-rw------- 1 root root 6070304 5月 11 16:20 2024-05-11-etcd_back.db\r[root@master1 apps]# 备份老的证书文件 # cp -rp /etc/kubernetes/ssl //etc/kubernetes/ssl_bak0514 停止master和node节点的k8s相关的服务 # ansible -i hosts node -m shell -a \u0026#34;systemctl stop kubelet\u0026#34;\ransible -i hosts node -m shell -a \u0026#34;systemctl stop kube-proxy\u0026#34;\ransible -i hosts master -m shell -a \u0026#34;systemctl stop etcd\u0026#34;\ransible -i hosts master -m shell -a \u0026#34;systemctl stop kube-apiserver\u0026#34;\ransible -i hosts master -m shell -a \u0026#34;systemctl stop kube-controller-manager\u0026#34;\ransible -i hosts master -m shell -a \u0026#34;systemctl stop kube-scheduler\u0026#34;\ransible -i hosts master -m shell -a \u0026#34;systemctl stop kubelet\u0026#34;\ransible -i hosts master -m shell -a \u0026#34;systemctl stop kube-proxy\u0026#34; 修改 setup.yml,hosts,group_var 里面的信息，包括 # setup.yml只放开创建证书的这一步，\nhosts的master组增加两个新的节点，node组增加两个新的节点，docker组增加两个新的节点，\ngroup_var下的 vip 改为 新申请的vip地址\n重新生成证书，包括etcd的相关的证书 # 拷贝 /opt/kubernetes_ssl/下的证书信息到 老的和新的master节点的 /etc/kubernetes/ssl/ 下面 # 备份后删除老的证书文件\rcp -rp /etc/kubernetes/ssl/ /etc/kubernetes/ssl_bak/\rrm -rf /etc/kubernetes/ssl/*\r#把 /opt/kubernetes_ssl/下的证书拷贝到 /etc/kubernetes/ssl/下 # cp -rp /opt/kubernetes_ssl/etcd* /etc/kubernetes/ssl/\r# cp -rp /opt/kubernetes_ssl/ca.crt /etc/kubernetes/ssl/\r# cp -rp /opt/kubernetes_ssl/ca.key /etc/kubernetes/ssl/\r包括\r- ca.crt\r- ca.key\r- etcd_server.crt\r- etcd_server.key\r- etcd_client.crt\r- etcd_client.key 拷贝etcd服务相关文件和证书到master2，master3节点上 # scp -rp /usr/bin/etcd root@192.168.0.62:/usr/bin/etcd\rscp -rp /usr/bin/etcdctl root@192.168.0.62:/usr/bin/etcdctl\rscp -rp /etc/etcd/etcd.conf root@192.168.0.62:/etc/etcd/etcd.conf 后面新加一个改一个的配置\rscp -rp /etc/etcd//usr/lib/systemd/system/etcd.service root@192.168.0.62:/usr/lib/systemd/system/etcd.service\rscp -rp /etc/kubernetes/ssl/ca.crt root@192.168.0.62:/etc/kubernetes/ssl/ca.crt\rscp -rp /etc/kubernetes/ssl/ca.key root@192.168.0.62:/etc/kubernetes/ssl/ca.key\rscp -rp /etc/kubernetes/ssl/etcd_server.crt root@192.168.0.62:/etc/kubernetes/ssl/etcd_server.crt\rscp -rp /etc/kubernetes/ssl/etcd_server.key root@192.168.0.62:/etc/kubernetes/ssl/etcd_server.key\rscp -rp /etc/kubernetes/ssl/etcd_client.crt root@192.168.0.62:/etc/kubernetes/ssl/etcd_client.crt\rscp -rp /etc/kubernetes/ssl/etcd_client.key root@192.168.0.62:/etc/kubernetes/ssl/etcd_client.key\rsystemctl daemon-reload master2,master3 /data/etcd_data/etcd下 需要没有东西 否则要删除master2,master3 的**/data/etcd_data/etcd**文件夹下的文件 # 重启老的节点61的etcd服务 # systemctl restart etcd\rsystemctl start etcd 添加新的节点62的etcd # ETCDCTL_API=3 /usr/bin/etcdctl --endpoints=https://192.168.0.61:1159 \\\r--cacert=/etc/kubernetes/ssl/ca.crt \\\r--cert=/etc/kubernetes/ssl/etcd_server.crt \\\r--key=/etc/kubernetes/ssl/etcd_server.key \\\rmember add etcd_0_62 --peer-urls=https://192.168.0.62:2380 ETCDCTL_API=3 /usr/bin/etcdctl --endpoints=https://192.168.0.61:1159 \\\r--cacert=/etc/kubernetes/ssl/ca.crt \\\r--cert=/etc/kubernetes/ssl/etcd_server.crt \\\r--key=/etc/kubernetes/ssl/etcd_server.key \\\rmember add etcd_0_62 --peer-urls=https://192.168.0.62:2380\r[root@master1 ssl]# ETCDCTL_API=3 /usr/bin/etcdctl --endpoints=https://192.168.0.61:1159 \\\r\u0026gt; --cacert=/etc/kubernetes/ssl/ca.crt \\\r\u0026gt; --cert=/etc/kubernetes/ssl/etcd_server.crt \\\r\u0026gt; --key=/etc/kubernetes/ssl/etcd_server.key \\\r\u0026gt; member add etcd_0_62 --peer-urls=https://192.168.0.62:2380\rMember bcc22e0a75396cbc added to cluster f6f9624e4f919ae5\rETCD_NAME=\u0026#34;etcd_0_62\u0026#34;\rETCD_INITIAL_CLUSTER=\u0026#34;etcd_0_61=https://192.168.0.61:2380,etcd_0_62=https://192.168.0.62:2380\u0026#34;\rETCD_INITIAL_ADVERTISE_PEER_URLS=\u0026#34;https://192.168.0.62:2380\u0026#34;\rETCD_INITIAL_CLUSTER_STATE=\u0026#34;existing\u0026#34;\r[root@master1 ssl]# 修改61，62的etcd的配置，包括ETCD_INITIAL_CLUSTER信息要包括61和62，ETCD_INITIAL_CLUSTER_STATE都改为existing，修改完重启61,62两个etcd节点 # systemctl restart etcd 查看member list # [root@master1 ~]# etcdctl member list -w table\r+------------------+---------+-----------+---------------------------+--------------------------------------------------+------------+\r| ID | STATUS | NAME | PEER ADDRS | CLIENT ADDRS | IS LEARNER |\r+------------------+---------+-----------+---------------------------+--------------------------------------------------+------------+\r| 6195ef4a2fc534f8 | started | etcd_0_61 | https://192.168.0.61:2380 | https://127.0.0.1:1159,https://192.168.0.61:1159 | false |\r| bcc22e0a75396cbc | started | etcd_0_62 | https://192.168.0.62:2380 | https://127.0.0.1:1159,https://192.168.0.62:1159 | false |\r+------------------+---------+-----------+---------------------------+--------------------------------------------------+------------+\rETCDCTL_API=3 etcdctl --endpoints=http://192.168.0.61:2382,http://192.168.0.62:2382,http://192.168.0.63:2382 member list -w table 查看状态，正常都是3.8Mb的大小 # ETCDCTL_API=3 /usr/bin/etcdctl --endpoints=https://192.168.0.61:1159,https://192.168.0.62:1159 --cacert=/etc/kubernetes/ssl/ca.crt --cert=/etc/kubernetes/ssl/etcd_server.crt --key=/etc/kubernetes/ssl/etcd_server.key endpoint status -w table ETCDCTL_API=3 /usr/bin/etcdctl --endpoints=https://2:1159 --cacert=/etc/kubernetes/ssl/ca.crt --cert=/etc/kubernetes/ssl/etcd_server.crt --key=/etc/kubernetes/ssl/etcd_server.key endpoint status -w table ETCDCTL_API=3 etcdctl --endpoints=http://192.168.0.61:2382,http://192.168.0.62:2382,http://192.168.0.63:2382 endpoint status -w table [root@master1 ~]# ETCDCTL_API=3 /usr/bin/etcdctl \\\r\u0026gt; --endpoints=https://192.168.0.61:1159,https://192.168.0.62:1159 \\\r\u0026gt; --cacert=/etc/kubernetes/ssl/ca.crt \\\r\u0026gt; --cert=/etc/kubernetes/ssl/etcd_server.crt \\\r\u0026gt; --key=/etc/kubernetes/ssl/etcd_server.key \\\r\u0026gt; endpoint status -w table\r+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+\r| ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |\r+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+\r| https://192.168.0.61:1159 | 6195ef4a2fc534f8 | 3.5.4 | 3.8 MB | false | false | 6 | 2481 | 2481 | |\r| https://192.168.0.62:1159 | bcc22e0a75396cbc | 3.5.4 | 3.8 MB | true | false | 6 | 2481 | 2481 | |\r+ 在61节点上添加第三个etcd节点 # ETCDCTL_API=3 /usr/bin/etcdctl --endpoints=https://192.168.0.61:1159 \\\r--cacert=/etc/kubernetes/ssl/ca.crt \\\r--cert=/etc/kubernetes/ssl/etcd_server.crt \\\r--key=/etc/kubernetes/ssl/etcd_server.key \\\rmember add etcd_0_63 --peer-urls=https://192.168.0.63:2380 [root@master1 ~]# ETCDCTL_API=3 /usr/bin/etcdctl --endpoints=https://192.168.0.61:1159 \\\r\u0026gt; --cacert=/etc/kubernetes/ssl/ca.crt \\\r\u0026gt; --cert=/etc/kubernetes/ssl/etcd_server.crt \\\r\u0026gt; --key=/etc/kubernetes/ssl/etcd_server.key \\\r\u0026gt; member add etcd_0_63 --peer-urls=https://192.168.0.63:2380\rMember 3dcc0344ff804247 added to cluster f6f9624e4f919ae5\rETCD_NAME=\u0026#34;etcd_0_63\u0026#34;\rETCD_INITIAL_CLUSTER=\u0026#34;etcd_0_63=https://192.168.0.63:2380,etcd_0_61=https://192.168.0.61:2380,etcd_0_62=https://192.168.0.62:2380\u0026#34;\rETCD_INITIAL_ADVERTISE_PEER_URLS=\u0026#34;https://192.168.0.63:2380\u0026#34;\rETCD_INITIAL_CLUSTER_STATE=\u0026#34;existing\u0026#34; 修改61和62和63的etcd的配置，包括ETCD_INITIAL_CLUSTER信息要包括61和62和63，ETCD_INITIAL_CLUSTER_STATE都改为existing，修改完重启61,62,63 3个etcd节点， # cd /etc/etcd/\rvim /etd.conf\rsystemctl restart etcd 查看member list # ETCDCTL_API=3 etcdctl --endpoints=http://192.168.0.61:2382,http://192.168.0.62:2382,http://192.168.0.63:2382 member list -w table [root@master1 etcd]# ETCDCTL_API=3 /usr/bin/etcdctl --endpoints=https://192.168.0.61:1159,https://192.168.0.62:1159,https://192.168.0.63:1159 --cacert=/etc/kubernetes/ssl/ca.crt --cert=/etc/kubernetes/ssl/etcd_server.crt --key=/etc/kubernetes/ssl/etcd_server.key member list -w table\r+------------------+---------+-----------+---------------------------+--------------------------------------------------+------------+\r| ID | STATUS | NAME | PEER ADDRS | CLIENT ADDRS | IS LEARNER |\r+------------------+---------+-----------+---------------------------+--------------------------------------------------+------------+\r| 3dcc0344ff804247 | started | etcd_0_63 | https://192.168.0.63:2380 | https://127.0.0.1:1159,https://192.168.0.63:1159 | false |\r| 6195ef4a2fc534f8 | started | etcd_0_61 | https://192.168.0.61:2380 | https://127.0.0.1:1159,https://192.168.0.61:1159 | false |\r| bcc22e0a75396cbc | started | etcd_0_62 | https://192.168.0.62:2380 | https://127.0.0.1:1159,https://192.168.0.62:1159 | false |\r+------------------+---------+-----------+---------------------------+--------------------------------------------------+------------+\r[root@master1 etcd]#\rETCDCTL_API=3 /usr/bin/etcdctl --endpoints=https://192.168.0.186:1159,https://192.168.0.187:1159,https://192.168.0.188:1159 --cacert=/etc/kubernetes/ssl/ca.crt --cert=/etc/kubernetes/ssl/etcd_server.crt --key=/etc/kubernetes/ssl/etcd_server.key member list -w table\rETCDCTL_API=3 /usr/bin/etcdctl --endpoints=https://192.168.0.61:1159,https://192.168.0.62:1159,https://192.168.0.63:1159 --cacert=/etc/kubernetes/ssl/ca.crt --cert=/etc/kubernetes/ssl/etcd_server.crt --key=/etc/kubernetes/ssl/etcd_server.key member list -w table 查看etcd健康状态 # ETCDCTL_API=3 etcdctl --endpoints=http://192.168.0.61:2382,http://192.168.0.62:2382,http://192.168.0.63:2382 endpoint health -w table [root@master1 etcd]# ETCDCTL_API=3 /usr/bin/etcdctl --endpoints=https://192.168.0.61:1159,https://192.168.0.62:1159,https://192.168.0.63:1159 --cacert=/etc/kubernetes/ssl/ca.crt --cert=/etc/kubernetes/ssl/etcd_server.crt --key=/etc/kubernetes/ssl/etcd_server.key endpoint health -w table\r+---------------------------+--------+-------------+-------+\r| ENDPOINT | HEALTH | TOOK | ERROR |\r+---------------------------+--------+-------------+-------+\r| https://192.168.0.62:1159 | true | 12.985932ms | |\r| https://192.168.0.61:1159 | true | 12.078238ms | |\r| https://192.168.0.63:1159 | true | 14.896674ms | |\r+---------------------------+--------+-------------+-------+\rETCDCTL_API=3 /usr/bin/etcdctl --endpoints=https://192.168.0.186:1159,https://192.168.0.187:1159,https://192.168.0.188:1159 --cacert=/etc/kubernetes/ssl/ca.crt --cert=/etc/kubernetes/ssl/etcd_server.crt --key=/etc/kubernetes/ssl/etcd_server.key endpoint health -w table\rETCDCTL_API=3 /usr/bin/etcdctl --endpoints=https://192.168.0.61:1159,https://192.168.0.62:1159,https://192.168.0.63:1159 --cacert=/etc/kubernetes/ssl/ca.crt --cert=/etc/kubernetes/ssl/etcd_server.crt --key=/etc/kubernetes/ssl/etcd_server.key endpoint health -w table 查看etcd的状态，3个节点的数据大小一直，状态正常 # ETCDCTL_API=3 etcdctl --endpoints=http://192.168.0.61:2382,http://192.168.0.62:2382,http://192.168.0.63:2382 endpoint status -w table ETCDCTL_API=3 /usr/bin/etcdctl --endpoints=https://192.168.0.186:1159,https://192.168.0.187:1159,https://192.168.0.188:1159 --cacert=/etc/kubernetes/ssl/ca.crt --cert=/etc/kubernetes/ssl/etcd_server.crt --key=/etc/kubernetes/ssl/etcd_server.key endpoint status -w table\rETCDCTL_API=3 /usr/bin/etcdctl --endpoints=https://192.168.0.61:1159,https://192.168.0.62:1159,https://192.168.0.63:1159 --cacert=/etc/kubernetes/ssl/ca.crt --cert=/etc/kubernetes/ssl/etcd_server.crt --key=/etc/kubernetes/ssl/etcd_server.key endpoint status -w table [root@master1 etcd]# ETCDCTL_API=3 /usr/bin/etcdctl --endpoints=https://192.168.0.61:1159,https://192.168.0.62:1159,https://192.168.0.63:1159 --cacert=/etc/kubernetes/ssl/ca.crt --cert=/etc/kubernetes/ssl/etcd_server.crt --key=/etc/kubernetes/ssl/etcd_server.key endpoint status -w table\r+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+\r| ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |\r+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+\r| https://192.168.0.61:1159 | 6195ef4a2fc534f8 | 3.5.4 | 3.8 MB | false | false | 8 | 2487 | 2487 | |\r| https://192.168.0.62:1159 | bcc22e0a75396cbc | 3.5.4 | 3.8 MB | true | false | 8 | 2487 | 2487 | |\r| https://192.168.0.63:1159 | 3dcc0344ff804247 | 3.5.4 | 3.9 MB | false | false | 8 | 2487 | 2487 | |\r+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+\r[ 拷贝证书相关文件到各个主机 master和node节点 # 拷贝相关文件到master1和master2节点 # master1和master2节点都要拷贝\rscp -rp /usr/bin/kube-apiserver root@192.168.0.62:/usr/bin/kube-apiserver\rscp -rp /usr/bin/kube-controller-manager root@192.168.0.62:/usr/bin/kube-controller-manager\rscp -rp /usr/bin/kube-scheduler root@192.168.0.62:/usr/bin/kube-scheduler\rscp -rp /usr/bin/kubectl root@192.168.0.62:/usr/bin/kubectl\rscp -rp /etc/kubernetes/ssl root@192.168.0.62:/etc/kubernetes/ssl\rscp -rp /etc/kubernetes/ssl/* root@192.168.0.62:/etc/kubernetes/ssl/*\rmkdir -p /var/log/kubernetes\rscp -rp /etc/kubernetes/apiserver root@192.168.0.62:/etc/kubernetes/apiserver\rscp -rp /etc/kubernetes/controller-manager root@192.168.0.62:/etc/kubernetes/controller-manager\rscp -rp /etc/kubernetes/scheduler root@192.168.0.62:/etc/kubernetes/scheduler\rscp -rp /etc/kubernetes/kubeconfig_bak root@192.168.0.62:/etc/kubernetes/kubeconfig_bak\rscp -rp /usr/lib/systemd/system/kube-apiserver.service root@192.168.0.62:/usr/lib/systemd/system/kube-apiserver.service\rscp -rp /usr/lib/systemd/system/kube-controller-manager.service root@192.168.0.62:/usr/lib/systemd/system/kube-controller-manager.service scp -rp /usr/lib/systemd/system/kube-scheduler.service root@192.168.0.62:/usr/lib/systemd/system/kube-scheduler.service\rmkdir -p /etc/kubernetes/pki/ca_ssl\rscp -rp /etc/kubernetes/pki/ca_ssl root@192.168.0.62:/etc/kubernetes/pki/ca_ssl\rsystemctl daemon-reload\rsystemctl start kube-apiserver\rsystemctl start kube-controller-manager\rsystemctl start kube-scheduler\rsystemctl status kube-apiserver\rsystemctl status kube-controller-manager\rsystemctl status kube-scheduler\rkubectl create clusterrolebinding root-cluster-admin-binding --clusterrole=cluster-admin --user=admin 2\u0026gt;/dev/null 后面直接执行 master node colico coredns即可 # 拷贝master节点证书到/etc/kubernetes/ssl下面后，执行 重启master和node节点的各个服务 # apiserver要修改为下面的内容 # ansible -i hosts master -m shell -a \u0026#34;systemctl restart etcd\u0026#34;\ransible -i hosts master -m shell -a \u0026#34;systemctl restart kube-apiserver\u0026#34;\ransible -i hosts master -m shell -a \u0026#34;systemctl restart kube-controller-manager\u0026#34;\ransible -i hosts master -m shell -a \u0026#34;systemctl restart kube-scheduler\u0026#34;\ransible -i hosts master -m shell -a \u0026#34;systemctl restart kubelet\u0026#34;\ransible -i hosts master -m shell -a \u0026#34;systemctl restart kube-proxy\u0026#34;\ransible -i hosts node -m shell -a \u0026#34;systemctl restart kubelet\u0026#34;\ransible -i hosts node -m shell -a \u0026#34;systemctl restart kube-proxy\u0026#34; cailco理论会自动安装，也可以执行下calico和coredns的安装步骤，注意新加节点和老的节点的udp 14789要能互相通信，否者新加的master节点和集群的其他节点的网络不通 # 查看集群的pod，正常 # k get pod -A 查看集群的状态，正常 # k get cs 数据都还在 包括之前单节点创建的redis服务 redis的pvc nfs-client-provisioner服务，至此扩容为3master3etcd完成，且原数据未丢失 # POD状态正常 # 注意coredns的cm 要重新生成 不然etcd会报错coredns有连接因为证书错误导致连接被拒绝 # k delete cm -n kube-system etcd-cert\rkubectl create configmap etcd-cert -n kube-system --from-file=ca.crt=/etc/kubernetes/ssl/ca.crt --from-file=etcd_client.crt=/etc/kubernetes/ssl/etcd_client.crt --from-file=etcd_client.key=/etc/kubernetes/ssl/etcd_client.key\rkubectl apply -f /apps/cluster_modules_setup/coredns-1.7.1/coredns-v1.7.0-with-etcd.yaml systemctl restart etcd\rsystemctl status etcd 1.上一次变更已经进行etcd3节点改造，3master节点的相关系统文件已经拷贝过去了，这次要改master节点的apiserver配置和/etc/kubernetes/ssl下的证书更新，node节点的/etc/kubernetes/ssl下的证书更新。\r2.停止所有node和master节点的集群组件，包括kubelet，kube-proxy，kube-apiserver，etcd，kube-controller-manager，kube-scheduler。\r3.重新生成ca，etcd，apiserver等证书在/opt/kubernetes_ssl下\r4.拷贝/opt/kubernetes_ssl下相关证书到每一个master和node节点的/etc/kubernetes/ssl下进行替换证书操作。master拷贝/opt/kubernetes_ssl下的所有文件，node节点拷贝ca.crt,calient.crt,client.key,kubeconfig文件\r5.重新启动所有node和master节点的集群组件，包括kubelet，kube-proxy，kube-apiserver，etcd，kube-controller-manager，kube-scheduler。\r6.观察集群状态和pod状态是否正常。\r重点需要重启docker，否则有问题，打不开工作空间 "},{"id":158,"href":"/docs/%E5%8F%B2%E4%B8%8A%E6%9C%80%E7%89%9Bjenkins-pipeline%E6%B5%81%E6%B0%B4%E7%BA%BF%E8%AF%A6%E8%A7%A3-shi-shang-zui-niu-jenkinspipeline-liu-shui-xian-xiang-jie/","title":"史上最牛jenkins pipeline流水线详解 2024-08-02","section":"Docs","content":" 一、什么是流水线 # jenkins 有 2 种流水线分为声明式流水线与脚本化流水线，脚本化流水线是 jenkins 旧版本使用的流水线脚本，新版本 Jenkins 推荐使用声明式流水线。文档只介绍声明流水线。\n1.1 声明式流水线 # 在声明式流水线语法中，流水线过程定义在 Pipeline{}中，Pipeline 块定义了整个流水线中完成的所有工作，比如\n参数说明：\nagent any：在任何可用的代理上执行流水线或它的任何阶段，也就是执行流水线过程的位置，也可以指定到具体的节点 stage：定义流水线的执行过程（相当于一个阶段），比如下文所示的 Build、Test、Deploy， 但是这个名字是根据实际情况进行定义的，并非固定的名字 steps：执行某阶段具体的步骤。 //Jenkinsfile (Declarative Pipeline) pipeline { agent any stages { stage(\u0026#39;Build\u0026#39;) { steps { echo \u0026#39;Build\u0026#39; } } stage(\u0026#39;Test\u0026#39;) { steps { echo \u0026#39;Test\u0026#39; } } stage(\u0026#39;Deploy\u0026#39;) { steps { echo \u0026#39;Deploy\u0026#39; } } } } 1.2 脚本化流水线 # 在脚本化流水线语法中，会有一个或多个 Node（节点）块在整个流水线中执行核心工作\n参数说明:\nnode：在任何可用的代理上执行流水线或它的任何阶段，也可以指定到具体的节点 stage：和声明式的含义一致，定义流水线的阶段。Stage 块在脚本化流水线语法中是可选的，然而在脚本化流水线中实现 stage 块，可以清楚地在 Jenkins UI 界面中显示每个 stage 的任务子集。 //Jenkinsfile (Scripted Pipeline) node { stage(\u0026#39;Build\u0026#39;) { echo \u0026#39;Build\u0026#39; } stage(\u0026#39;Test\u0026#39;) { echo \u0026#39;Test\u0026#39; } stage(\u0026#39;Deploy\u0026#39;) { echo \u0026#39;Deploy\u0026#39; } } 二、声明式流水线 # 声明式流水线必须包含在一个 Pipeline 块中，比如是一个 Pipeline 块的格式\npipeline { /* insert Declarative Pipeline here */ } 在声明式流水线中有效的基本语句和表达式遵循与 Groovy 的语法同样的规则，但有以下例外\n流水线顶层必须是一个 block，即 pipeline{} 分隔符可以不需要分号，但是每条语句都必须在自己的行上 块只能由 Sections、Directives、Steps 或 assignment statements 组成 属性引用语句被当做是无参数的方法调用，比如 input 会被当做 input()。 2.1 Sections # 声明式流水线中的 Sections 不是一个关键字或指令，而是包含一个或多个 Agent、Stages、 post、Directives 和 Steps 的代码区域块。\n1.Agent # Agent 表示整个流水线或特定阶段中的步骤和命令执行的位置，该部分必须在 pipeline 块的顶层被定义，也可以在 stage 中再次定义，但是 stage 级别是可选的。\nany # 在任何可用的代理上执行流水线，配置语法\npipeline { agent any } none # 表示该 Pipeline 脚本没有全局的 agent 配置。当顶层的 agent 配置为 none 时， 每个 stage 部分都需要包含它自己的 agent。配置语法\npipeline { agent none stages { stage(\u0026#39;Stage For Build\u0026#39;){ agent any } } } label # 以节点标签形式选择某个具体的节点执行 Pipeline 命令，例如：agent { label \u0026lsquo;my-defined-label\u0026rsquo; }。节点需要提前配置标签。\npipeline { agent none stages { stage(\u0026#39;Stage For Build\u0026#39;){ agent { label \u0026#39;role-master\u0026#39; } steps { echo \u0026#34;role-master\u0026#34; } } } } node # 和 label 配置类似，只不过是可以添加一些额外的配置，比如 customWorkspace(设置默认工作目录)\npipeline { agent none stages { stage(\u0026#39;Stage For Build\u0026#39;){ agent { node { label \u0026#39;role-master\u0026#39; customWorkspace \u0026#34;/tmp/zhangzhuo/data\u0026#34; } } steps { sh \u0026#34;echo role-master \u0026gt; 1.txt\u0026#34; } } } } dockerfile # 使用从源码中包含的 Dockerfile 所构建的容器执行流水线或 stage。此时对应的 agent 写法如下\nagent { dockerfile { filename \u0026#39;Dockerfile.build\u0026#39; //dockerfile文件名称 dir \u0026#39;build\u0026#39; //执行构建镜像的工作目录 label \u0026#39;role-master\u0026#39; //执行的node节点，标签选择 additionalBuildArgs \u0026#39;--build-arg version=1.0.2\u0026#39; //构建参数 } } docker # 相当于 dockerfile，可以直接使用 docker 字段指定外部镜像即可，可以省去构建的时间。比如使用 maven 镜像进行打包，同时可以指定 args\nagent{ docker{ image \u0026#39;192.168.10.15/kubernetes/alpine:latest\u0026#39; //镜像地址 label \u0026#39;role-master\u0026#39; //执行的节点，标签选择 args \u0026#39;-v /tmp:/tmp\u0026#39; //启动镜像的参数 } } kubernetes # 需要部署 kubernetes 相关的插件，官方文档：\nhttps://github.com/jenkinsci/kubernetes-plugin/\nJenkins 也支持使用 Kubernetes 创建 Slave，也就是常说的动态 Slave。配置示例如下\ncloud: Configure Clouds 的名称，指定到其中一个 k8s\nslaveConnectTimeout: 连接超时时间\nyaml: pod 定义文件，jnlp 容器的配置必须有配置无需改变，其余 containerd 根据自己情况指定\nworkspaceVolume：持久化 jenkins 的工作目录。\npersistentVolumeClaimWorkspaceVolume：挂载已有 pvc。 workspaceVolume persistentVolumeClaimWorkspaceVolume(claimName: \u0026#34;jenkins-agent\u0026#34;, mountPath: \u0026#34;/\u0026#34;, readOnly: \u0026#34;false\u0026#34;) nfsWorkspaceVolume：挂载 nfs 服务器目录 workspaceVolume nfsWorkspaceVolume(serverAddress: \u0026#34;192.168.10.254\u0026#34;, serverPath: \u0026#34;/nfs\u0026#34;, readOnly: \u0026#34;false\u0026#34;) dynamicPVC：动态申请 pvc，任务执行结束后删除 workspaceVolume dynamicPVC(storageClassName: \u0026#34;nfs-client\u0026#34;, requestsSize: \u0026#34;1Gi\u0026#34;, accessModes: \u0026#34;ReadWriteMany\u0026#34;) emptyDirWorkspaceVolume：临时目录，任务执行结束后会随着 pod 删除被删除，主要功能多个任务 container 共享 jenkins 工作目录。 workspaceVolume emptyDirWorkspaceVolume() hostPathWorkspaceVolume：挂载 node 节点本机目录，注意挂载本机目录注意权限问题，可以先创建设置 777 权限，否则默认 kubelet 创建的目录权限为 755 默认其他用户没有写权限，执行流水线会报错。 workspaceVolume hostPathWorkspaceVolume(hostPath: \u0026#34;/opt/workspace\u0026#34;, readOnly: false) 示例\nagent { kubernetes { cloud \u0026#39;kubernetes\u0026#39; slaveConnectTimeout 1200 workspaceVolume emptyDirWorkspaceVolume() yaml \u0026#39;\u0026#39;\u0026#39; kind: Pod metadata: name: jenkins-agent spec: containers: - args: [\\\u0026#39;$(JENKINS_SECRET)\\\u0026#39;, \\\u0026#39;$(JENKINS_NAME)\\\u0026#39;] image: \u0026#39;192.168.10.15/kubernetes/jnlp:alpine\u0026#39; name: jnlp imagePullPolicy: IfNotPresent - command: - \u0026#34;cat\u0026#34; image: \u0026#34;192.168.10.15/kubernetes/alpine:latest\u0026#34; imagePullPolicy: \u0026#34;IfNotPresent\u0026#34; name: \u0026#34;date\u0026#34; tty: true restartPolicy: Never \u0026#39;\u0026#39;\u0026#39; } } 2.agent 的配置示例 # kubernetes 示例 # Docker+K8s+Jenkins 主流技术全解视频资料【干货免费分享】\npipeline { agent { kubernetes { cloud \u0026#39;kubernetes\u0026#39; slaveConnectTimeout 1200 workspaceVolume emptyDirWorkspaceVolume() yaml \u0026#39;\u0026#39;\u0026#39; kind: Pod metadata: name: jenkins-agent spec: containers: - args: [\\\u0026#39;$(JENKINS_SECRET)\\\u0026#39;, \\\u0026#39;$(JENKINS_NAME)\\\u0026#39;] image: \u0026#39;192.168.10.15/kubernetes/jnlp:alpine\u0026#39; name: jnlp imagePullPolicy: IfNotPresent - command: - \u0026#34;cat\u0026#34; image: \u0026#34;192.168.10.15/kubernetes/alpine:latest\u0026#34; imagePullPolicy: \u0026#34;IfNotPresent\u0026#34; name: \u0026#34;date\u0026#34; tty: true - command: - \u0026#34;cat\u0026#34; image: \u0026#34;192.168.10.15/kubernetes/kubectl:apline\u0026#34; imagePullPolicy: \u0026#34;IfNotPresent\u0026#34; name: \u0026#34;kubectl\u0026#34; tty: true restartPolicy: Never \u0026#39;\u0026#39;\u0026#39; } } environment { MY_KUBECONFIG = credentials(\u0026#39;kubernetes-cluster\u0026#39;) } stages { stage(\u0026#39;Data\u0026#39;) { steps { container(name: \u0026#39;date\u0026#39;) { sh \u0026#34;\u0026#34;\u0026#34; date \u0026#34;\u0026#34;\u0026#34; } } } stage(\u0026#39;echo\u0026#39;) { steps { container(name: \u0026#39;date\u0026#39;) { sh \u0026#34;\u0026#34;\u0026#34; echo \u0026#39;k8s is pod\u0026#39; \u0026#34;\u0026#34;\u0026#34; } } } stage(\u0026#39;kubectl\u0026#39;) { steps { container(name: \u0026#39;kubectl\u0026#39;) { sh \u0026#34;\u0026#34;\u0026#34; kubectl get pod -A --kubeconfig $MY_KUBECONFIG \u0026#34;\u0026#34;\u0026#34; } } } } } docker 的示例 # pipeline { agent none stages { stage(\u0026#39;Example Build\u0026#39;) { agent { docker \u0026#39;maven:3-alpine\u0026#39; } steps { echo \u0026#39;Hello, Maven\u0026#39; sh \u0026#39;mvn --version\u0026#39; } } stage(\u0026#39;Example Test\u0026#39;) { agent { docker \u0026#39;openjdk:8-jre\u0026#39; } steps { echo \u0026#39;Hello, JDK\u0026#39; sh \u0026#39;java -version\u0026#39; } } } } 3.Post # Post 一般用于流水线结束后的进一步处理，比如错误通知等。Post 可以针对流水线不同的结果做出不同的处理，就像开发程序的错误处理，比如 Python 语言的 try catch。 Post 可以定义在 Pipeline 或 stage 中，目前支持以下条件 always：无论 Pipeline 或 stage 的完成状态如何，都允许运行该 post 中定义的指令； changed：只有当前 Pipeline 或 stage 的完成状态与它之前的运行不同时，才允许在该 post 部分运行该步骤； fixed：当本次 Pipeline 或 stage 成功，且上一次构建是失败或不稳定时，允许运行该 post 中定义的指令； regression：当本次 Pipeline 或 stage 的状态为失败、不稳定或终止，且上一次构建的 状态为成功时，允许运行该 post 中定义的指令； failure：只有当前 Pipeline 或 stage 的完成状态为失败（failure），才允许在 post 部分运行该步骤，通常这时在 Web 界面中显示为红色 success：当前状态为成功（success），执行 post 步骤，通常在 Web 界面中显示为蓝色 或绿色 unstable：当前状态为不稳定（unstable），执行 post 步骤，通常由于测试失败或代码 违规等造成，在 Web 界面中显示为黄色 aborted：当前状态为终止（aborted），执行该 post 步骤，通常由于流水线被手动终止触发，这时在 Web 界面中显示为灰色； unsuccessful：当前状态不是 success 时，执行该 post 步骤； cleanup：无论 pipeline 或 stage 的完成状态如何，都允许运行该 post 中定义的指令。和 always 的区别在于，cleanup 会在其它执行之后执行。 示例 # 一般情况下 post 部分放在流水线的底部，比如本实例，无论 stage 的完成状态如何，都会输出一条 I will always say Hello again!信息\n//Jenkinsfile (Declarative Pipeline) pipeline { agent any stages { stage(\u0026#39;Example1\u0026#39;) { steps { echo \u0026#39;Hello World1\u0026#39; } } stage(\u0026#39;Example2\u0026#39;) { steps { echo \u0026#39;Hello World2\u0026#39; } } } post { always { echo \u0026#39;I will always say Hello again!\u0026#39; } } } 也可以将 post 写在 stage，下面示例表示 Example1 执行失败执行 post。\n//Jenkinsfile (Declarative Pipeline) pipeline { agent any stages { stage(\u0026#39;Example1\u0026#39;) { steps { sh \u0026#39;ip a\u0026#39; } post { failure { echo \u0026#39;I will always say Hello again!\u0026#39; } } } } } 4.sepes # Steps 部分在给定的 stage 指令中执行的一个或多个步骤，比如在 steps 定义执行一条 shell 命令\n//Jenkinsfile (Declarative Pipeline) pipeline { agent any stages { stage(\u0026#39;Example\u0026#39;) { steps { echo \u0026#39;Hello World\u0026#39; } } } } 或者是使用 sh 字段执行多条指令\n//Jenkinsfile (Declarative Pipeline) pipeline { agent any stages { stage(\u0026#39;Example\u0026#39;) { steps { sh \u0026#34;\u0026#34;\u0026#34; echo \u0026#39;Hello World1\u0026#39; echo \u0026#39;Hello World2\u0026#39; \u0026#34;\u0026#34;\u0026#34; } } } } 2.2 Directives # Directives 可用于一些执行 stage 时的条件判断或预处理一些数据，和 Sections 一致，Directives 不是一个关键字或指令，而是包含了 environment、options、parameters、triggers、stage、tools、 input、when 等配置。\n1.Environment # Environment 主要用于在流水线中配置的一些环境变量，根据配置的位置决定环境变量的作用域。可以定义在 pipeline 中作为全局变量，也可以配置在 stage 中作为该 stage 的环境变量。该指令支持一个特殊的方法 credentials()，该方法可用于在 Jenkins 环境中通过标识符访问预定义的凭证。对于类型为 Secret Text 的凭证，credentials()可以将该 Secret 中的文本内容赋值给环境变量。对于类型为标准的账号密码型的凭证，指定的环境变量为 username 和 password，并且也会定义两个额外的环境变量，分别为MYVARNAME_USR和MYVARNAME_PSW。\n基本变量使用 # //示例 pipeline { agent any environment { //全局变量，会在所有stage中生效 NAME= \u0026#39;zhangzhuo\u0026#39; } stages { stage(\u0026#39;env1\u0026#39;) { environment { //定义在stage中的变量只会在当前stage生效，其他的stage不会生效 HARBOR = \u0026#39;https://192.168.10.15\u0026#39; } steps { sh \u0026#34;env\u0026#34; } } stage(\u0026#39;env2\u0026#39;) { steps { sh \u0026#34;env\u0026#34; } } } } 使用变量引用 secret 的凭证 # //这里使用k8s的kubeconfig文件示例 pipeline { agent any environment { KUBECONFIG = credentials(\u0026#39;kubernetes-cluster\u0026#39;) } stages { stage(\u0026#39;env\u0026#39;) { steps { sh \u0026#34;env\u0026#34; //默认情况下输出的变量内容会被加密 } } } } 使用变量引用类型为标准的账号密码型的凭证 # 这里使用 HARBOR 变量进行演示，默认情况下账号密码型的凭证会自动创建 3 个变量\nHARBOR_USR:会把凭证中 username 值赋值给这个变量 HARBOR_PSW:会把凭证中 password 值赋值给这个变量 HARBOR:默认情况下赋值的值为usernamme:password //这里使用k8s的kubeconfig文件示例 pipeline { agent any environment { HARBOR = credentials(\u0026#39;harbor-account\u0026#39;) } stages { stage(\u0026#39;env\u0026#39;) { steps { sh \u0026#34;env\u0026#34; } } } } 2.Options # Jenkins 流水线支持很多内置指令，比如 retry 可以对失败的步骤进行重复执行 n 次，可以根据不同的指令实现不同的效果。比较常用的指令如下:\nbuildDiscarder ：保留多少个流水线的构建记录 disableConcurrentBuilds：禁止流水线并行执行，防止并行流水线同时访问共享资源导致流水线失败。 disableResume ：如果控制器重启，禁止流水线自动恢复。 newContainerPerStage：agent 为 docker 或 dockerfile 时，每个阶段将在同一个节点的新容器中运行，而不是所有的阶段都在同一个容器中运行。 quietPeriod：流水线静默期，也就是触发流水线后等待一会在执行。 retry：流水线失败后重试次数。 timeout：设置流水线的超时时间，超过流水线时间，job 会自动终止。如果不加 unit 参数默认为 1 分。 timestamps：为控制台输出时间戳。 定义在 pipeline 中 # pipeline { agent any options { timeout(time: 1, unit: \u0026#39;HOURS\u0026#39;) //超时时间1小时，如果不加unit参数默认为1分 timestamps() //所有输出每行都会打印时间戳 buildDiscarder(logRotator(numToKeepStr: \u0026#39;3\u0026#39;)) //保留三个历史构建版本 quietPeriod(10) //注意手动触发的构建不生效 retry(3) //流水线失败后重试次数 } stages { stage(\u0026#39;env1\u0026#39;) { steps { sh \u0026#34;env\u0026#34; sleep 2 } } stage(\u0026#39;env2\u0026#39;) { steps { sh \u0026#34;env\u0026#34; } } } } 定义在 stage 中 # Option 除了写在 Pipeline 顶层，还可以写在 stage 中，但是写在 stage 中的 option 仅支持 retry、 timeout、timestamps，或者是和 stage 相关的声明式选项，比如 skipDefaultCheckout。处于 stage 级别的 options 写法如下\npipeline { agent any stages { stage(\u0026#39;env1\u0026#39;) { options { //定义在这里这对这个stage生效 timeout(time: 2, unit: \u0026#39;SECONDS\u0026#39;) //超时时间2秒 timestamps() //所有输出每行都会打印时间戳 retry(3) //流水线失败后重试次数 } steps { sh \u0026#34;env \u0026amp;\u0026amp; sleep 2\u0026#34; } } stage(\u0026#39;env2\u0026#39;) { steps { sh \u0026#34;env\u0026#34; } } } } 3.Parameters # Parameters 提供了一个用户在触发流水线时应该提供的参数列表，这些用户指定参数的值可以通过 params 对象提供给流水线的 step（步骤）。只能定义在 pipeline 顶层。\n目前支持的参数类型如下\nstring：字符串类型的参数。 text：文本型参数，一般用于定义多行文本内容的变量。 booleanParam：布尔型参数。 choice：选择型参数，一般用于给定几个可选的值，然后选择其中一个进行赋值。 password：密码型变量，一般用于定义敏感型变量，在 Jenkins 控制台会输出为*。 插件 Parameters\nimageTag：镜像 tag，需要安装 Image Tag Parameter 插件后使用 gitParameter：获取 git 仓库分支，需要 Git Parameter 插件后使用 示例\npipeline { agent any parameters { string(name: \u0026#39;DEPLOY_ENV\u0026#39;, defaultValue: \u0026#39;staging\u0026#39;, description: \u0026#39;1\u0026#39;) //执行构建时需要手动配置字符串类型参数，之后赋值给变量 text(name: \u0026#39;DEPLOY_TEXT\u0026#39;, defaultValue: \u0026#39;One\\nTwo\\nThree\\n\u0026#39;, description: \u0026#39;2\u0026#39;) //执行构建时需要提供文本参数，之后赋值给变量 booleanParam(name: \u0026#39;DEBUG_BUILD\u0026#39;, defaultValue: true, description: \u0026#39;3\u0026#39;) //布尔型参数 choice(name: \u0026#39;CHOICES\u0026#39;, choices: [\u0026#39;one\u0026#39;, \u0026#39;two\u0026#39;, \u0026#39;three\u0026#39;], description: \u0026#39;4\u0026#39;) //选择形式列表参数 password(name: \u0026#39;PASSWORD\u0026#39;, defaultValue: \u0026#39;SECRET\u0026#39;, description: \u0026#39;A secret password\u0026#39;) //密码类型参数，会进行加密 imageTag(name: \u0026#39;DOCKER_IMAGE\u0026#39;, description: \u0026#39;\u0026#39;, image: \u0026#39;kubernetes/kubectl\u0026#39;, filter: \u0026#39;.*\u0026#39;, defaultTag: \u0026#39;\u0026#39;, registry: \u0026#39;https://192.168.10.15\u0026#39;, credentialId: \u0026#39;harbor-account\u0026#39;, tagOrder: \u0026#39;NATURAL\u0026#39;) //获取镜像名称与tag gitParameter(branch: \u0026#39;\u0026#39;, branchFilter: \u0026#39;origin/(.*)\u0026#39;, defaultValue: \u0026#39;\u0026#39;, description: \u0026#39;Branch for build and deploy\u0026#39;, name: \u0026#39;BRANCH\u0026#39;, quickFilterEnabled: false, selectedValue: \u0026#39;NONE\u0026#39;, sortMode: \u0026#39;NONE\u0026#39;, tagFilter: \u0026#39;*\u0026#39;, type: \u0026#39;PT_BRANCH\u0026#39;) } //获取git仓库分支列表，必须有git引用 stages { stage(\u0026#39;env1\u0026#39;) { steps { sh \u0026#34;env\u0026#34; } } stage(\u0026#39;git\u0026#39;) { steps { git branch: \u0026#34;$BRANCH\u0026#34;, credentialsId: \u0026#39;gitlab-key\u0026#39;, url: \u0026#39;git@192.168.10.14:root/env.git\u0026#39; //使用gitParameter，必须有这个 } } } } 4.Triggers # 在 Pipeline 中可以用 triggers 实现自动触发流水线执行任务，可以通过 Webhook、Cron、 pollSCM 和 upstream 等方式触发流水线。\nCron # 定时构建假如某个流水线构建的时间比较长，或者某个流水线需要定期在某个时间段执行构建，可以 使用 cron 配置触发器，比如周一到周五每隔四个小时执行一次\n注意：H 的意思不是 HOURS 的意思，而是 Hash 的缩写。主要为了解决多个流水线在同一时间同时运行带来的系统负载压力。\npipeline { agent any triggers { cron(\u0026#39;H */4 * * 1-5\u0026#39;) //周一到周五每隔四个小时执行一次 cron(\u0026#39;H/12 * * * *\u0026#39;) //每隔12分钟执行一次 cron(\u0026#39;H * * * *\u0026#39;) //每隔1小时执行一次 } stages { stage(\u0026#39;Example\u0026#39;) { steps { echo \u0026#39;Hello World\u0026#39; } } } } Upstream # Upstream 可以根据上游 job 的执行结果决定是否触发该流水线。比如当 job1 或 job2 执行成功时触发该流水线\nDocker+K8s+Jenkins 主流技术全解视频资料【干货免费分享】\n目前支持的状态有 SUCCESS、UNSTABLE、FAILURE、NOT_BUILT、ABORTED 等。\npipeline { agent any triggers { upstream(upstreamProjects: \u0026#39;env\u0026#39;, threshold: hudson.model.Result.SUCCESS) //当env构建成功时构建这个流水线 } stages { stage(\u0026#39;Example\u0026#39;) { steps { echo \u0026#39;Hello World\u0026#39; } } } } 5.Input # Input 字段可以实现在流水线中进行交互式操作，比如选择要部署的环境、是否继续执行某个阶段等。\n配置 Input 支持以下选项\nmessage：必选，需要用户进行 input 的提示信息，比如：“是否发布到生产环境？”； id：可选，input 的标识符，默认为 stage 的名称； ok：可选，确认按钮的显示信息，比如：“确定”、“允许”； submitter：可选，允许提交 input 操作的用户或组的名称，如果为空，任何登录用户均可提交 input； parameters：提供一个参数列表供 input 使用。 假如需要配置一个提示消息为“还继续么”、确认按钮为“继续”、提供一个 PERSON 的变量的参数，并且只能由登录用户为 alice 和 bob 提交的 input 流水线\npipeline { agent any stages { stage(\u0026#39;Example\u0026#39;) { input { message \u0026#34;还继续么?\u0026#34; ok \u0026#34;继续\u0026#34; submitter \u0026#34;alice,bob\u0026#34; parameters { string(name: \u0026#39;PERSON\u0026#39;, defaultValue: \u0026#39;Mr Jenkins\u0026#39;, description: \u0026#39;Who should I say hello to?\u0026#39;) } } steps { echo \u0026#34;Hello, ${PERSON}, nice to meet you.\u0026#34; } } } } 6.when # When 指令允许流水线根据给定的条件决定是否应该执行该 stage，when 指令必须包含至少 一个条件。如果 when 包含多个条件，所有的子条件必须都返回 True，stage 才能执行。\nWhen 也可以结合 not、allOf、anyOf 语法达到更灵活的条件匹配。\n目前比较常用的内置条件如下\nbranch：当正在构建的分支与给定的分支匹配时，执行这个 stage。注意，branch 只适用于多分支流水线 changelog：匹配提交的 changeLog 决定是否构建，例如:when { changelog '.*^\\\\[DEPENDENCY\\\\] .+$' } environment：当指定的环境变量和给定的变量匹配时，执行这个 stage，例如：when { environment name: \u0026lsquo;DEPLOY_TO\u0026rsquo;, value: \u0026lsquo;production\u0026rsquo; } equals：当期望值和实际值相同时，执行这个 stage，例如：when { equals expected: 2, actual: currentBuild.number }； expression：当指定的 Groovy 表达式评估为 True，执行这个 stage，例如：when { expression { return params.DEBUG_BUILD } }； tag：如果 TAG_NAME 的值和给定的条件匹配，执行这个 stage，例如：when { tag \u0026ldquo;release-\u0026rdquo; }； not：当嵌套条件出现错误时，执行这个 stage，必须包含一个条件，例如：when { not { branch \u0026lsquo;master\u0026rsquo; } }； allOf：当所有的嵌套条件都正确时，执行这个 stage，必须包含至少一个条件，例如：when { allOf { branch \u0026lsquo;master\u0026rsquo;; environment name: \u0026lsquo;DEPLOY_TO\u0026rsquo;, value: \u0026lsquo;production\u0026rsquo; } }； anyOf：当至少有一个嵌套条件为 True 时，执行这个 stage，例如：when { anyOf { branch \u0026lsquo;master\u0026rsquo;; branch \u0026lsquo;staging\u0026rsquo; } }。 示例：当分支为 main 时执行 Example Deploy 步骤\npipeline { agent any stages { stage(\u0026#39;Example Build\u0026#39;) { steps { echo \u0026#39;Hello World\u0026#39; } } stage(\u0026#39;Example Deploy\u0026#39;) { when { branch \u0026#39;main\u0026#39; //多分支流水线，分支为才会执行。 } steps { echo \u0026#39;Deploying\u0026#39; } } } } 也可以同时配置多个条件，比如分支是 production，而且 DEPLOY_TO 变量的值为 main 时，才执行 Example Deploy\npipeline { agent any environment { DEPLOY_TO = \u0026#34;main\u0026#34; } stages { stage(\u0026#39;Example Deploy\u0026#39;) { when { branch \u0026#39;main\u0026#39; environment name: \u0026#39;DEPLOY_TO\u0026#39;, value: \u0026#39;main\u0026#39; } steps { echo \u0026#39;Deploying\u0026#39; } } } } 也可以使用 anyOf 进行匹配其中一个条件即可，比如分支为 main 或 DEPLOY_TO 为 main 或 master 时执行 Deploy\npipeline { agent any stages { stage(\u0026#39;Example Deploy\u0026#39;) { when { anyOf { branch \u0026#39;main\u0026#39; environment name: \u0026#39;DEPLOY_TO\u0026#39;, value: \u0026#39;main\u0026#39; environment name: \u0026#39;DEPLOY_TO\u0026#39;, value: \u0026#39;master\u0026#39; } } steps { echo \u0026#39;Deploying\u0026#39; } } } } 也可以使用 expression 进行正则匹配，比如当 BRANCH_NAME 为 main 或 master，并且 DEPLOY_TO 为 master 或 main 时才会执行 Example Deploy\npipeline { agent any stages { stage(\u0026#39;Example Deploy\u0026#39;) { when { expression { BRANCH_NAME ==~ /(main|master)/ } anyOf { environment name: \u0026#39;DEPLOY_TO\u0026#39;, value: \u0026#39;main\u0026#39; environment name: \u0026#39;DEPLOY_TO\u0026#39;, value: \u0026#39;master\u0026#39; } } steps { echo \u0026#39;Deploying\u0026#39; } } } } 默认情况下，如果定义了某个 stage 的 agent，在进入该 stage 的 agent 后，该 stage 的 when 条件才会被评估，但是可以通过一些选项更改此选项。比如在进入 stage 的 agent 前评估 when， 可以使用 beforeAgent，当 when 为 true 时才进行该 stage\n目前支持的前置条件如下\nbeforeAgent：如果 beforeAgent 为 true，则会先评估 when 条件。在 when 条件为 true 时，才会进入该 stage beforeInput：如果 beforeInput 为 true，则会先评估 when 条件。在 when 条件为 true 时，才会进入到 input 阶段； beforeOptions：如果 beforeInput 为 true，则会先评估 when 条件。在 when 条件为 true 时，才会进入到 options 阶段； beforeOptions 优先级大于 beforeInput 大于 beforeAgent\n示例\npipeline { agent none stages { stage(\u0026#39;Example Build\u0026#39;) { steps { echo \u0026#39;Hello World\u0026#39; } } stage(\u0026#39;Example Deploy\u0026#39;) { when { beforeAgent true branch \u0026#39;main\u0026#39; } steps { echo \u0026#39;Deploying\u0026#39; } } } } 2.3 Parallel # 在声明式流水线中可以使用 Parallel 字段，即可很方便的实现并发构建，比如对分支 A、B、 C 进行并行处理\npipeline { agent any stages { stage(\u0026#39;Non-Parallel Stage\u0026#39;) { steps { echo \u0026#39;This stage will be executed first.\u0026#39; } } stage(\u0026#39;Parallel Stage\u0026#39;) { failFast true //表示其中只要有一个分支构建执行失败，就直接推出不等待其他分支构建 parallel { stage(\u0026#39;Branch A\u0026#39;) { steps { echo \u0026#34;On Branch A\u0026#34; } } stage(\u0026#39;Branch B\u0026#39;) { steps { echo \u0026#34;On Branch B\u0026#34; } } stage(\u0026#39;Branch C\u0026#39;) { stages { stage(\u0026#39;Nested 1\u0026#39;) { steps { echo \u0026#34;In stage Nested 1 within Branch C\u0026#34; } } stage(\u0026#39;Nested 2\u0026#39;) { steps { echo \u0026#34;In stage Nested 2 within Branch C\u0026#34; } } } } } } } } 三、Jenkinsfile 的使用 # 上面讲过流水线支持两种语法，即声明式和脚本式，这两种语法都支持构建持续交付流水线。并且都可以用来在 Web UI 或 Jenkinsfile 中定义流水线，不过通常将 Jenkinsfile 放置于代码仓库中（当然也可以放在单独的代码仓库中进行管理）。\n创建一个 Jenkinsfile 并将其放置于代码仓库中，有以下好处\n方便对流水线上的代码进行复查/迭代 对管道进行审计跟踪 流水线真正的源代码能够被项目的多个成员查看和编辑 3.1 环境变量 # 1.静态变量 # Jenkins 有许多内置变量可以直接在 Jenkinsfile 中使用，可以通过 JENKINS_URL/pipeline/syntax/globals#env 获取完整列表。目前比较常用的环境变量如下\nBUILD_ID：当前构建的 ID，与 Jenkins 版本 1.597+中的 BUILD_NUMBER 完全相同\nBUILD_NUMBER：当前构建的 ID，和 BUILD_ID 一致\nBUILD_TAG：用来标识构建的版本号，格式为：jenkins-${JOB_NAME}-${BUILD_NUMBER}， 可以对产物进行命名，比如生产的 jar 包名字、镜像的 TAG 等；\nBUILD_URL：本次构建的完整 URL，比如：\nhttp://buildserver/jenkins/job/MyJobName/17/%EF%BC%9B\nJOB_NAME：本次构建的项目名称\nNODE_NAME：当前构建节点的名称；\nJENKINS_URL：Jenkins 完整的 URL，需要在 SystemConfiguration 设置；\nWORKSPACE：执行构建的工作目录。\n示例如果一个流水线名称为print_env，第 2 次构建，各个变量的值。\nBUILD_ID：2 BUILD_NUMBER：2 BUILD_TAG：jenkins-print_env-2 BUILD_URL：http://192.168.10.16:8080/job/print_env/2/ JOB_NAME：print_env NODE_NAME：built-in JENKINS_URL：http://192.168.10.16:8080/ WORKSPACE：/bitnami/jenkins/home/workspace/print_env 上述变量会保存在一个 Map 中，可以使用 env.BUILD_ID 或 env.JENKINS_URL 引用某个内置变量\npipeline { agent any stages { stage(\u0026#39;print env\u0026#39;) { parallel { stage(\u0026#39;BUILD_ID\u0026#39;) { steps { echo \u0026#34;$env.BUILD_ID\u0026#34; } } stage(\u0026#39;BUILD_NUMBER\u0026#39;) { steps { echo \u0026#34;$env.BUILD_NUMBER\u0026#34; } } stage(\u0026#39;BUILD_TAG\u0026#39;) { steps { echo \u0026#34;$env.BUILD_TAG\u0026#34; } } } } } } 2.动态变量 # 动态变量是根据某个指令的结果进行动态赋值，变量的值根据指令的执行结果而不同。如下所示\nreturnStdout：将命令的执行结果赋值给变量，比如下述的命令返回的是 clang，此时 CC 的值为“clang”。 returnStatus：将命令的执行状态赋值给变量，比如下述命令的执行状态为 1，此时 EXIT_STATUS 的值为 1。 //Jenkinsfile (Declarative Pipeline) pipeline { agent any environment { // 使用 returnStdout CC = \u0026#34;\u0026#34;\u0026#34;${sh( returnStdout: true, script: \u0026#39;echo -n \u0026#34;clang\u0026#34;\u0026#39; //如果使用shell命令的echo赋值变量最好加-n取消换行 )}\u0026#34;\u0026#34;\u0026#34; // 使用 returnStatus EXIT_STATUS = \u0026#34;\u0026#34;\u0026#34;${sh( returnStatus: true, script: \u0026#39;exit 1\u0026#39; )}\u0026#34;\u0026#34;\u0026#34; } stages { stage(\u0026#39;Example\u0026#39;) { environment { DEBUG_FLAGS = \u0026#39;-g\u0026#39; } steps { sh \u0026#39;printenv\u0026#39; } } } } 3.2 凭证管理 # Jenkins 的声明式流水线语法有一个 credentials()函数，它支持 secret text（加密文本）、username 和 password（用户名和密码）以及 secret file（加密文件）等。接下来看一下一些常用的凭证处理方法。\n1.加密文本 # 本实例演示将两个 Secret 文本凭证分配给单独的环境变量来访问 Amazon Web 服务，需要 提前创建这两个文件的 credentials（实践的章节会有演示），Jenkinsfile 文件的内容如下\n//Jenkinsfile (Declarative Pipeline) pipeline { agent any environment { AWS_ACCESS_KEY_ID = credentials(\u0026#39;txt1\u0026#39;) AWS_SECRET_ACCESS_KEY = credentials(\u0026#39;txt2\u0026#39;) } stages { stage(\u0026#39;Example stage 1\u0026#39;) { steps { echo \u0026#34;$AWS_ACCESS_KEY_ID\u0026#34; } } stage(\u0026#39;Example stage 2\u0026#39;) { steps { echo \u0026#34;$AWS_SECRET_ACCESS_KEY\u0026#34; } } } } 2.用户名密码 # 本示例用来演示 credentials 账号密码的使用，比如使用一个公用账户访问 Bitbucket、GitLab、 Harbor 等。假设已经配置完成了用户名密码形式的 credentials，凭证 ID 为 harbor-account\n//Jenkinsfile (Declarative Pipeline) pipeline { agent any environment { BITBUCKET_COMMON_CREDS = credentials(\u0026#39;harbor-account\u0026#39;) } stages { stage(\u0026#39;printenv\u0026#39;) { steps { sh \u0026#34;env\u0026#34; } } } 上述的配置会自动生成 3 个环境变量\nBITBUCKET_COMMON_CREDS：包含一个以冒号分隔的用户名和密码，格式为 username:password BITBUCKET_COMMON_CREDS_USR：仅包含用户名的附加变量 BITBUCKET_COMMON_CREDS_PSW：仅包含密码的附加变量。 3.加密文件 # 需要加密保存的文件，也可以使用 credential，比如链接到 Kubernetes 集群的 kubeconfig 文件等。\n假如已经配置好了一个 kubeconfig 文件，此时可以在 Pipeline 中引用该文件\n//Jenkinsfile (Declarative Pipeline) pipeline { agent { kubernetes { cloud \u0026#39;kubernetes\u0026#39; slaveConnectTimeout 1200 workspaceVolume emptyDirWorkspaceVolume() yaml \u0026#39;\u0026#39;\u0026#39; kind: Pod metadata: name: jenkins-agent spec: containers: - args: [\\\u0026#39;$(JENKINS_SECRET)\\\u0026#39;, \\\u0026#39;$(JENKINS_NAME)\\\u0026#39;] image: \u0026#39;192.168.10.15/kubernetes/jnlp:alpine\u0026#39; name: jnlp imagePullPolicy: IfNotPresent - command: - \u0026#34;cat\u0026#34; image: \u0026#34;192.168.10.15/kubernetes/kubectl:apline\u0026#34; imagePullPolicy: \u0026#34;IfNotPresent\u0026#34; name: \u0026#34;kubectl\u0026#34; tty: true restartPolicy: Never \u0026#39;\u0026#39;\u0026#39; } } environment { MY_KUBECONFIG = credentials(\u0026#39;kubernetes-cluster\u0026#39;) } stages { stage(\u0026#39;kubectl\u0026#39;) { steps { container(name: \u0026#39;kubectl\u0026#39;) { sh \u0026#34;\u0026#34;\u0026#34; kubectl get pod -A --kubeconfig $MY_KUBECONFIG \u0026#34;\u0026#34;\u0026#34; } } } } } - END -\n"},{"id":159,"href":"/docs/%E5%A4%9A%E6%9E%B6%E6%9E%84%E6%94%AF%E6%8C%81-duo-jia-gou-zhi-chi/","title":"多架构支持 2023-09-28 15:29:54.111","section":"Docs","content":" 多架构支持 # kubeasz 3.4.1 以后支持多CPU架构，当前已支持linux amd64和linux arm64，更多架构支持根据后续需求来计划。 # 使用方式 # kubeasz 多架构安装逻辑：根据部署机器（执行ezdown/ezctl命令的机器）的架构，会自动判断下载对应amd64/arm64的二进制文件和容器镜像，然后推送安装到整个集群。 # 暂不支持不同架构的机器加入到同一个集群。 # harbor目前仅支持amd64安装 # 架构支持备忘 # k8s核心组件本身提供多架构的二进制文件/容器镜像下载，项目调整了下载二进制文件的容器dockerfile # https://github.com/easzlab/dockerfile-kubeasz-k8s-bin kubeasz其他用到的二进制或镜像，重新调整了容器创建dockerfile # https://github.com/easzlab/dockerfile-kubeasz-ext-bin https://github.com/easzlab/dockerfile-kubeasz-ext-build https://github.com/easzlab/dockerfile-kubeasz-sys-pkg https://github.com/easzlab/dockerfile-kubeasz-mirrored-images https://github.com/easzlab/dockerfile-kubeasz https://github.com/easzlab/dockerfile-ansible 其他组件(coredns/network plugin/dashboard/metrics-server等)一般都提供多架构的容器镜像，可以直接下载拉取 # "},{"id":160,"href":"/docs/%E5%A4%A7%E5%8E%82%E6%80%BB%E7%BB%93nginx%E9%AB%98%E5%B9%B6%E5%8F%91%E4%BC%98%E5%8C%96%E7%AC%94%E8%AE%B0-da-chang-zong-jie-nginx-gao-bing-fa-you-hua-bi-ji/","title":"大厂总结nginx高并发优化笔记 2024-08-02 17:46:19.098","section":"Docs","content":"在日常的运维工作中，经常会用到nginx服务，也时常会碰到nginx因高并发导致的性能瓶颈问题。这里简单梳理下nginx性能优化的配置。\n一、Nginx配置中比较重要的优化项\n1）nginx进程数，建议按照cpu数目来指定，一般跟cpu核数相同或为它的倍数。 worker_processes 8; 2）为每个进程分配cpu，上例中将8个进程分配到8个cpu，当然可以写多个，或者将一个进程分配到多个cpu。 worker_cpu_affinity 00000001 00000010 00000100 00001000 00010000 00100000 01000000 10000000; 3）下面这个指令是指当一个nginx进程打开的最多文件描述符数目，理论值应该是系统的最多打开文件数（ulimit -n）与nginx进程数相除，但是nginx分配请求并不是那么均匀，所以最好与ulimit -n的值保持一致。 worker_rlimit_nofile 65535; 4）使用epoll的I/O模型，用这个模型来高效处理异步事件 use epoll; 5）每个进程允许的最多连接数，理论上每台nginx服务器的最大连接数为worker_processes*worker_connections。 worker_connections 65535; 6）http连接超时时间，默认是60s，功能是使客户端到服务器端的连接在设定的时间内持续有效，当出现对服务器的后继请求时，该功能避免了建立或者重新建立连接。切记这个参数也不能设置过大！否则会导致许多无效的http连接占据着nginx的连接数，终nginx崩溃！ keepalive_timeout 60; 7）客户端请求头部的缓冲区大小，这个可以根据你的系统分页大小来设置，一般一个请求的头部大小不会超过1k，不过由于一般系统分页都要大于1k，所以这里设置为分页大小。分页大小可以用命令getconf PAGESIZE取得。 client_header_buffer_size 4k; 8）下面这个参数将为打开文件指定缓存，默认是没有启用的，max指定缓存数量，建议和打开文件数一致，inactive是指经过多长时间文件没被请求后删除缓存。 open_file_cache max=102400 inactive=20s; 9）下面这个是指多长时间检查一次缓存的有效信息。 open_file_cache_valid 30s; 10）open_file_cache指令中的inactive参数时间内文件的最少使用次数，如果超过这个数字，文件描述符一直是在缓存中打开的，如上例，如果有一个文件在inactive时间内一次没被使用，它将被移除。 open_file_cache_min_uses 1;\n11）隐藏响应头中的有关操作系统和web server（Nginx）版本号的信息，这样对于安全性是有好处的。 server_tokens off; 12）可以让sendfile()发挥作用。sendfile()可以在磁盘和TCP socket之间互相拷贝数据(或任意两个文件描述符)。Pre-sendfile是传送数据之前在用户空间申请数据缓冲区。之后用read()将数据从文件拷贝到这个缓冲区，write()将缓冲区数据写入网络。sendfile()是立即将数据从磁盘读到OS缓存。因为这种拷贝是在内核完成的，sendfile()要比组合read()和write()以及打开关闭丢弃缓冲更加有效(更多有关于sendfile)。 sendfile on; 13）告诉nginx在一个数据包里发送所有头文件，而不一个接一个的发送。就是说数据包不会马上传送出去，等到数据包最大时，一次性的传输出去，这样有助于解决网络堵塞。 tcp_nopush on; 14）告诉nginx不要缓存数据，而是一段一段的发送\u0026ndash;当需要及时发送数据时，就应该给应用设置这个属性，这样发送一小块数据信息时就不能立即得到返回值。 tcp_nodelay on; 比如： http { server_tokens off; sendfile on; tcp_nopush on; tcp_nodelay on; \u0026hellip;\u0026hellip; } 15）客户端请求头部的缓冲区大小，这个可以根据系统分页大小来设置，一般一个请求头的大小不会超过1k，不过由于一般系统分页都要大于1k，所以这里设置为分页大小。 client_header_buffer_size 4k; 客户端请求头部的缓冲区大小，这个可以根据系统分页大小来设置，一般一个请求头的大小不会超过1k，不过由于一般系统分页都要大于1k，所以这里设置为分页大小。 分页大小可以用命令getconf PAGESIZE取得。 [root@test-huanqiu ~]# getconf PAGESIZE 4096 但也有client_header_buffer_size超过4k的情况，但是client_header_buffer_size该值必须设置为“系统分页大小”的整倍数。 16）为打开文件指定缓存，默认是没有启用的，max 指定缓存数量，建议和打开文件数一致，inactive 是指经过多长时间文件没被请求后删除缓存。 open_file_cache max=65535 inactive=60s; 17）open_file_cache 指令中的inactive 参数时间内文件的最少使用次数，如果超过这个数字，文件描述符一直是在缓存中打开的，如上例，如果有一个文件在inactive 时间内一次没被使用，它将被移除。 open_file_cache_min_uses 1; 18）指定多长时间检查一次缓存的有效信息。 open_file_cache_valid 80s;\n常用Nginx标准配置\n[root@dev-huanqiu ~]# cat /usr/local/nginx/conf/nginx.conf user www www; worker_processes 8; worker_cpu_affinity 00000001 00000010 00000100 00001000 00010000 00100000 01000000; error_log /www/log/nginx_error.log crit; pid /usr/local/nginx/nginx.pid; worker_rlimit_nofile 65535; events { use epoll; worker_connections 65535; } http { include mime.types; default_type application/octet-stream; charset utf-8; server_names_hash_bucket_size 128; client_header_buffer_size 2k; large_client_header_buffers 4 4k; client_max_body_size 8m; sendfile on; tcp_nopush on; keepalive_timeout 60; fastcgi_cache_path /usr/local/nginx/fastcgi_cache levels=1:2 keys_zone=TEST:10m inactive=5m; fastcgi_connect_timeout 300; fastcgi_send_timeout 300; fastcgi_read_timeout 300; fastcgi_buffer_size 16k; fastcgi_buffers 16 16k; fastcgi_busy_buffers_size 16k; fastcgi_temp_file_write_size 16k; fastcgi_cache TEST; fastcgi_cache_valid 200 302 1h; fastcgi_cache_valid 301 1d; fastcgi_cache_valid any 1m; fastcgi_cache_min_uses 1; fastcgi_cache_use_stale error timeout invalid_header http_500; open_file_cache max=204800 inactive=20s; open_file_cache_min_uses 1; open_file_cache_valid 30s; tcp_nodelay on; gzip on; gzip_min_length 1k; gzip_buffers 4 16k; gzip_http_version 1.0; gzip_comp_level 2; gzip_types text/plain application/x-javascript text/css application/xml; gzip_vary on; server { listen 8080; server_name huan.wangshibo.com; index index.php index.htm; root /www/html/; location /status { stub_status on; } location ~ .*\\.(php|php5)?$ { fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; include fcgi.conf; } location ~ .*\\.(gif|jpg|jpeg|png|bmp|swf|js|css)$ { expires 30d; } log_format access \u0026#39;$remote_addr - $remote_user [$time_local] \u0026#34;$request\u0026#34; \u0026#39; \u0026#39;$status $body_bytes_sent \u0026#34;$http_referer\u0026#34; \u0026#39; \u0026#39;\u0026#34;$http_user_agent\u0026#34; $http_x_forwarded_for\u0026#39;; access_log /www/log/access.log access; } } 二、关于FastCGI的几个指令\n1）这个指令为FastCGI缓存指定一个路径，目录结构等级，关键字区域存储时间和非活动删除时间。 fastcgi_cache_path /usr/local/nginx/fastcgi_cache levels=1:2 keys_zone=TEST:10m inactive=5m; 2）指定连接到后端FastCGI的超时时间。 fastcgi_connect_timeout 300; 3）向FastCGI传送请求的超时时间，这个值是指已经完成两次握手后向FastCGI传送请求的超时时间。 fastcgi_send_timeout 300; 4）接收FastCGI应答的超时时间，这个值是指已经完成两次握手后接收FastCGI应答的超时时间。 fastcgi_read_timeout 300; 5）指定读取FastCGI应答第一部分 需要用多大的缓冲区，这里可以设置为fastcgi_buffers指令指定的缓冲区大小，上面的指令指定它将使用1个 16k的缓冲区去读取应答的第一部分，即应答头，其实这个应答头一般情况下都很小（不会超过1k），但是你如果在fastcgi_buffers指令中指 定了缓冲区的大小，那么它也会分配一个fastcgi_buffers指定的缓冲区大小去缓存。 fastcgi_buffer_size 16k; 6）指定本地需要用多少和多大的缓冲区来 缓冲FastCGI的应答，如上所示，如果一个php脚本所产生的页面大小为256k，则会为其分配16个16k的缓冲区来缓存，如果大于256k，增大 于256k的部分会缓存到fastcgi_temp指定的路径中， 当然这对服务器负载来说是不明智的方案，因为内存中处理数据速度要快于硬盘，通常这个值 的设置应该选择一个你的站点中的php脚本所产生的页面大小的中间值，比如你的站点大部分脚本所产生的页面大小为 256k就可以把这个值设置为16 16k，或者4 64k 或者64 4k，但很显然，后两种并不是好的设置方法，因为如果产生的页面只有32k，如果用4 64k它会分配1个64k的缓冲区去缓存，而如果使用64 4k它会分配8个4k的缓冲区去缓存，而如果使用16 16k则它会分配2个16k去缓存页面，这样看起来似乎更加合理。 fastcgi_buffers 16 16k; 7）这个指令我也不知道是做什么用，只知道默认值是fastcgi_buffers的两倍。 fastcgi_busy_buffers_size 32k; 8）在写入fastcgi_temp_path时将用多大的数据块，默认值是fastcgi_buffers的两倍。 fastcgi_temp_file_write_size 32k; 9）开启FastCGI缓存并且为其制定一个名称。个人感觉开启缓存非常有用，可以有效降低CPU负载，并且防止502错误。但是这个缓存会引起很多问题，因为它缓存的是动态页面。具体使用还需根据自己的需求。 fastcgi_cache TEST 10）为指定的应答代码指定缓存时间，如上例中将200，302应答缓存一小时，301应答缓存1天，其他为1分钟。 fastcgi_cache_valid 200 302 1h; fastcgi_cache_valid 301 1d; fastcgi_cache_valid any 1m; 11）缓存在fastcgi_cache_path指令inactive参数值时间内的最少使用次数，如上例，如果在5分钟内某文件1次也没有被使用，那么这个文件将被移除。 fastcgi_cache_min_uses 1; 12）不知道这个参数的作用，猜想应该是让nginx知道哪些类型的缓存是没用的。 fastcgi_cache_use_stale error timeout invalid_header http_500;\n另外，FastCGI自身也有一些配置需要进行优化，如果你使用php-fpm来管理FastCGI，可以修改配置文件中的以下值： 1）同时处理的并发请求数，即它将开启最多60个子线程来处理并发连接。 60 2）最多打开文件数。 65535 3）每个进程在重置之前能够执行的最多请求数。 65535\n1）这个指令为FastCGI缓存指定一个路径，目录结构等级，关键字区域存储时间和非活动删除时间。 fastcgi_cache_path /usr/local/nginx/fastcgi_cache levels=1:2 keys_zone=TEST:10m inactive=5m; 2）指定连接到后端FastCGI的超时时间。 fastcgi_connect_timeout 300; 3）向FastCGI传送请求的超时时间，这个值是指已经完成两次握手后向FastCGI传送请求的超时时间。 fastcgi_send_timeout 300; 4）接收FastCGI应答的超时时间，这个值是指已经完成两次握手后接收FastCGI应答的超时时间。 fastcgi_read_timeout 300; 5）指定读取FastCGI应答第一部分 需要用多大的缓冲区，这里可以设置为fastcgi_buffers指令指定的缓冲区大小，上面的指令指定它将使用1个 16k的缓冲区去读取应答的第一部分，即应答头，其实这个应答头一般情况下都很小（不会超过1k），但是你如果在fastcgi_buffers指令中指 定了缓冲区的大小，那么它也会分配一个fastcgi_buffers指定的缓冲区大小去缓存。 fastcgi_buffer_size 16k; 6）指定本地需要用多少和多大的缓冲区来 缓冲FastCGI的应答，如上所示，如果一个php脚本所产生的页面大小为256k，则会为其分配16个16k的缓冲区来缓存，如果大于256k，增大 于256k的部分会缓存到fastcgi_temp指定的路径中， 当然这对服务器负载来说是不明智的方案，因为内存中处理数据速度要快于硬盘，通常这个值 的设置应该选择一个你的站点中的php脚本所产生的页面大小的中间值，比如你的站点大部分脚本所产生的页面大小为 256k就可以把这个值设置为16 16k，或者4 64k 或者64 4k，但很显然，后两种并不是好的设置方法，因为如果产生的页面只有32k，如果用4 64k它会分配1个64k的缓冲区去缓存，而如果使用64 4k它会分配8个4k的缓冲区去缓存，而如果使用16 16k则它会分配2个16k去缓存页面，这样看起来似乎更加合理。 fastcgi_buffers 16 16k; 7）这个指令我也不知道是做什么用，只知道默认值是fastcgi_buffers的两倍。 fastcgi_busy_buffers_size 32k; 8）在写入fastcgi_temp_path时将用多大的数据块，默认值是fastcgi_buffers的两倍。 fastcgi_temp_file_write_size 32k; 9）开启FastCGI缓存并且为其制定一个名称。个人感觉开启缓存非常有用，可以有效降低CPU负载，并且防止502错误。但是这个缓存会引起很多问题，因为它缓存的是动态页面。具体使用还需根据自己的需求。 fastcgi_cache TEST 10）为指定的应答代码指定缓存时间，如上例中将200，302应答缓存一小时，301应答缓存1天，其他为1分钟。 fastcgi_cache_valid 200 302 1h; fastcgi_cache_valid 301 1d; fastcgi_cache_valid any 1m; 11）缓存在fastcgi_cache_path指令inactive参数值时间内的最少使用次数，如上例，如果在5分钟内某文件1次也没有被使用，那么这个文件将被移除。 fastcgi_cache_min_uses 1; 12）不知道这个参数的作用，猜想应该是让nginx知道哪些类型的缓存是没用的。 fastcgi_cache_use_stale error timeout invalid_header http_500;\n另外，FastCGI自身也有一些配置需要进行优化，如果你使用php-fpm来管理FastCGI，可以修改配置文件中的以下值： 1）同时处理的并发请求数，即它将开启最多60个子线程来处理并发连接。 60 2）最多打开文件数。 65535 3）每个进程在重置之前能够执行的最多请求数。 65535\n三、关于内核参数的优化\n1）timewait的数量，默认是180000。(Deven:因此如果想把timewait降下了就要把tcp_max_tw_buckets值减小) net.ipv4.tcp_max_tw_buckets = 6000 2）允许系统打开的端口范围。 net.ipv4.ip_local_port_range = 1024 65000 3）启用TIME-WAIT状态sockets快速回收功能;用于快速减少在TIME-WAIT状态TCP连接数。1表示启用;0表示关闭。但是要特别留意的是：这个选项一般不推荐启用，因为在NAT(Network Address Translation)网络下，会导致大量的TCP连接建立错误，从而引起网站访问故障。 net.ipv4.tcp_tw_recycle = 0\n实际上，net.ipv4.tcp_tw_recycle功能的开启，一般需要net.ipv4.tcp_timestamps（一般系统默认是开启这个功能的）这个开关开启后才有效果； 当tcp_tw_recycle 开启时（tcp_timestamps 同时开启，快速回收 socket 的效果达到），对于位于NAT设备后面的 Client来说，是一场灾难！！会导致到NAT设备后面的Client连接Server不稳定（有的 Client 能连接 server，有的 Client 不能连接 server）。\ntcp_tw_recycle这个功能，其实是为内部网络（网络环境自己可控 ” -不存在NAT 的情况）设计的，对于公网环境下，不宜使用。通常来说，回收TIME_WAIT状态的socket是因为“无法主动连接远端”，因为无可用的端口，而不应该是要回收内存（没有必要）。也就是说，需求是Client的需求，Server会有“端口不够用”的问题吗？除非是前端机，需要大量的连接后端服务，也就是充当着Client的角色。\n正确的解决这个总是办法应该是： net.ipv4.ip_local_port_range = 9000 6553 #默认值范围较小 net.ipv4.tcp_max_tw_buckets = 10000 #默认值较小，还可适当调小 net.ipv4.tcp_tw_reuse = 1 net.ipv4.tcp_fin_timeout = 10\n4）开启重用功能，允许将TIME-WAIT状态的sockets重新用于新的TCP连接。这个功能启用是安全的，一般不要去改动！ net.ipv4.tcp_tw_reuse = 1 5）开启SYN Cookies，当出现SYN等待队列溢出时，启用cookies来处理。 net.ipv4.tcp_syncookies = 1 6）web应用中listen函数的backlog默认会给我们内核参数的net.core.somaxconn限制到128，而nginx定义的NGX_LISTEN_BACKLOG默认为511，所以有必要调整这个值。 net.core.somaxconn = 262144 7）每个网络接口接收数据包的速率比内核处理这些包的速率快时，允许送到队列的数据包的最大数目。 net.core.netdev_max_backlog = 262144 8）系统中最多有多少个TCP套接字不被关联到任何一个用户文件句柄上。如果超过这个数字，孤儿连接将即刻被复位并打印出警告信息。这个限制仅仅是为了防止简单的DoS攻击，不能过分依靠它或者人为地减小这个值，更应该增加这个值(如果增加了内存之后)。 net.ipv4.tcp_max_orphans = 262144 9）记录的那些尚未收到客户端确认信息的连接请求的最大值。对于有128M内存的系统而言，缺省值是1024，小内存的系统则是128。 net.ipv4.tcp_max_syn_backlog = 262144 10）时间戳可以避免序列号的卷绕。一个1Gbps的链路肯定会遇到以前用过的序列号。时间戳能够让内核接受这种“异常”的数据包。 net.ipv4.tcp_timestamps = 1\n有不少服务器为了提高性能，开启net.ipv4.tcp_tw_recycle选项，在NAT网络环境下，容易导致网站访问出现了一些connect失败的问题。\n个人建议： 关闭net.ipv4.tcp_tw_recycle选项，而不是net.ipv4.tcp_timestamps； 因为在net.ipv4.tcp_timestamps关闭的条件下，开启net.ipv4.tcp_tw_recycle是不起作用的；而net.ipv4.tcp_timestamps可以独立开启并起作用。\n11）为了打开对端的连接，内核需要发送一个SYN并附带一个回应前面一个SYN的ACK。也就是所谓三次握手中的第二次握手。这个设置决定了内核放弃连接之前发送SYN+ACK包的数量。 net.ipv4.tcp_synack_retries = 1 12）在内核放弃建立连接之前发送SYN包的数量。 net.ipv4.tcp_syn_retries = 1 13）如果套接字由本端要求关闭，这个参数 决定了它保持在FIN-WAIT-2状态的时间。对端可以出错并永远不关闭连接，甚至意外当机。缺省值是60秒。2.2 内核的通常值是180秒，你可以按这个设置，但要记住的是，即使你的机器是一个轻载的WEB服务器，也有因为大量的死套接字而内存溢出的风险，FIN- WAIT-2的危险性比FIN-WAIT-1要小，因为它最多只能吃掉1.5K内存，但是它们的生存期长些。 net.ipv4.tcp_fin_timeout = 30 14）当keepalive起用的时候，TCP发送keepalive消息的频度。缺省是2小时。 net.ipv4.tcp_keepalive_time = 30\n以下是一个常用的内核参数的标准配置\n[root@dev-huanqiu ~]# cat /etc/sysctl.conf net.ipv4.ip_forward = 0 net.ipv4.conf.default.rp_filter = 1 net.ipv4.conf.default.accept_source_route = 0 kernel.sysrq = 0 kernel.core_uses_pid = 1 net.ipv4.tcp_syncookies = 1 //这四行标红内容，一般是发现大量TIME_WAIT时的解决办法 kernel.msgmnb = 65536 kernel.msgmax = 65536 kernel.shmmax = 68719476736 kernel.shmall = 4294967296 net.ipv4.tcp_max_tw_buckets = 6000 net.ipv4.tcp_sack = 1 net.ipv4.tcp_window_scaling = 1 net.ipv4.tcp_rmem = 4096 87380 4194304 net.ipv4.tcp_wmem = 4096 16384 4194304 net.core.wmem_default = 8388608 net.core.rmem_default = 8388608 net.core.rmem_max = 16777216 net.core.wmem_max = 16777216 net.core.netdev_max_backlog = 262144 net.core.somaxconn = 262144 net.ipv4.tcp_max_orphans = 3276800 net.ipv4.tcp_max_syn_backlog = 262144 net.ipv4.tcp_timestamps = 1 //在net.ipv4.tcp_tw_recycle设置为1的时候，这个选择最好加上 net.ipv4.tcp_synack_retries = 1 net.ipv4.tcp_syn_retries = 1 net.ipv4.tcp_tw_recycle = 1 //开启此功能可以减少TIME-WAIT状态，但是NAT网络模式下打开有可能会导致tcp连接错误，慎重。 net.ipv4.tcp_tw_reuse = 1 net.ipv4.tcp_mem = 94500000 915000000 927000000 net.ipv4.tcp_fin_timeout = 30 net.ipv4.tcp_keepalive_time = 30 net.ipv4.ip_local_port_range = 1024 65000 net.ipv4.ip_conntrack_max = 6553500 tcp参数引起的一个nginx故障： net.ipv4.tcp_tw_recycle = 1 这个功能打开后，确实能减少TIME-WAIT状态，习惯上我都会将这个参数打开。 但是也因为这个参数踩过一次坑： 公司的一个发布新闻的CMS后台系统，采用haproxy+keepalived代理架构，后端的real server服务器外网ip全部拿掉。 现象：在某一天早上发文高峰期，CMS后台出现访问故障，重启php服务后会立刻见效，但持续一段时间后，访问就又出现故障。 排查nginx和php日志也没有发现什么，后来google了一下，发现就是net.ipv4.tcp_tw_recycle这个参数捣的鬼！ 这种网络架构对于后端的realserver来说是NAT模式，打开这个参数后，会导致大量的TCP连接建立错误，从而引起网站访问故障。 最后将net.ipv4.tcp_tw_recycle设置为0，关闭这个功能后，后台访问即刻恢复正常\nNginx安全配置小提示： 下面是一个常见安全陷阱和解决方案的列表，它可以辅助来确保你的Nginx部署是安全的。\n1）禁用autoindex模块。这个可能在你使用的Nginx版本中已经更改了，如果没有的话只需在配置文件的location块中增加autoindex off;声明即可。 2）禁用服务器上的ssi (服务器端引用)。这个可以通过在location块中添加ssi off; 。 3）关闭服务器标记。如果开启的话（默认情况下）所有的错误页面都会显示服务器的版本和信息。将server_tokens off;声明添加到Nginx配置文件来解决这个问题。 4）在配置文件中设置自定义缓存以限制缓冲区溢出攻击的可能性。 client_body_buffer_size 1K; client_header_buffer_size 1k; client_max_body_size 1k; large_client_header_buffers 2 1k; 5）将timeout设低来防止DOS攻击。所有这些声明都可以放到主配置文件中。 client_body_timeout 10; client_header_timeout 10; keepalive_timeout 65; send_timeout 10; 6）限制用户连接数来预防DOS攻击。 limit_zone slimits $binary_remote_addr 5m; limit_conn slimits 5; 7）试着避免使用HTTP认证。HTTP认证默认使用crypt，它的哈希并不安全。如果你要用的话就用MD5（这也不是个好选择但负载方面比crypt好） 。 四、Nginx调优方式总结\n1、隐藏 Nginx 版本号（server_tokens off） 2、隐藏 Nginx 版本号和软件名（进入nginx的源码包里修改） 3、更改 Nginx 服务的默认用户 4、优化 Nginx worker 进程数 5、绑定 Nginx 进程到不同的 CPU 上 6、优化 Nginx 处理事件模型 7、优化 Nginx 单个进程允许的最大连接数 8、优化 Nginx worker 进程最大打开文件数 9、优化服务器域名的散列表大小 10、开启高效文件传输模式 11、优化 Nginx 连接超时时间 12、限制上传文件的大小 13、FastCGI 相关参数调优 14、配置 Nginx gzip 压缩 15、配置 Nginx expires 缓存 16、优化 Nginx日志(日志切割) 17、优化 Nginx 站点目录 18、配置 Nginx 防盗链 19、配置 Nginx 错误页面优雅显示 20、优化 Nginx 文件权限 21、Nginx 防爬虫优化 22、控制 Nginx 并发连接数 23、集群代理优化\n运维中常用Nginx监控与性能调优做法 \\1. Nginx监控 nginx有自带的监控模块，编译nginx的时候,加上参数 \u0026ndash;with-http_stub_status_module\n# 配置指令 ./configure --prefix=/usr/local --user=nginx --group=nginx --with-http_ssl_module --with-http_realip_module --http-client-body-temp-path=/usr/local/var/tmp/nginx/client --http-proxy-temp-path=/usr/local/var/tmp/nginx/proxy --http-fastcgi-temp-path=/usr/local/var/tmp/nginx/fcgi --http-scgi-temp-path=/usr/local/var/tmp/nginx/scgi --http-uwsgi-temp-path=/usr/local/var/tmp/nginx/uwsgi --with-http_geoip_module --with-http_stub_status_module 接着修改nginx配置文件，添加监控状态配置\nlocation = /nginx_status { stub_status on; access_log off; allow 127.0.0.1; deny all; } 最后就可以通过 \u0026ldquo;curl 127.0.0.1/nginx_status\u0026quot;访问nginx的状态了。\n上面是很简单的一个模块，除了这个还有外部的工具比如：ngxtop监控请求信息\n安装python-pip # yum install epel-release # yum install python-pip 安装ngxtop # pip install ngxtop ngxtop常用指令： # ngxtop top remote_addr 查看访问最多的IP # ngxtop -i \u0026#39;status \u0026gt;= 400\u0026#39; print request status http_referer #列出4xx or 5xx 的相应 指定配置文件: # ngxtop -c /etc/nginx/nginx.conf 查询状态是200：# ngxtop -c /etc/nginx/nginx.conf -i \u0026#39;status==200\u0026#39; 查询访问最多ip: # ngxtop -c /etc/nginx/nginx.conf -g remote_addr 还有一种图像化工具nginx-rrd，但是需要和php整合，这里就不介绍了。\n\\2. 常用的Nginx优化配置\n1）配置线程数和并发数 ############################# worker_processes 4 #cpu(取决于cpu的核数，一般为cpu核数或倍数。也可以配置成auto，让nginx自己选择工作线程数) events{ worker_connections 10240; #每一个进程打开的最大连接数，包含了nginx与客户端和nginx与upstream之间的连接（受限于操作系统） multi_accept on; #可以一次建立多个连接 use epoll; # Linux下多路复用IO接口select/poll的增强版本，它能显著提高程序在大量并发连接中只有少量活跃的情况下的系统CPU利用率 } 2）配置后端Server的长连接 ############################# upstream server_pool{ server localhost:8080 weight=1 max_fails=2 fail_timeout=30s; server localhost:8081 weight=1 max_fails=2 fail_timeout=30s; keepalive 300; #300个长连接 } location /{ proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \u0026#34;upgrade\u0026#34;; proxy_pass http://server_pool/; } 3) 启用缓存、压缩。Nginx的缓存还有很大的局限性，下面是静态文件压缩配置 ############################# gzip on; gzip_disable \u0026#34;msie6\u0026#34;; gzip_proxied any; gzip_min_length 1000; gzip_comp_level 6; gzip_types text/plain text/css application/json application/x-javascript text/xml application/xml application/xml+rss text/javascript application/javascript; 4）其他优化 ############################# sendfile on; #减少文件在应用和内核之间的拷贝 tcp_nopush on; #当数据包达到一定大小再发送 tcp_nodelay off; #有数据随时发送（只用在应答需要非常快速的情况下 注意：测试nginx语法是否正确：nginx -t 5) 操作系统优化 ############################# 1. 配置文件/etc/sysctl.conf sysctl -w net.ipv4.tcp_syncookies=1 #防止一个套接字在有过多试图连接到达时引起过载 sysctl -w net.core.somaxconn=1024 #默认128，连接队列 sysctl -w net.ipv4.tcp_fin_timeout=10 #timewait的超时时间 sysctl -w net.ipv4.tcp_tw_reuse=1 #os直接使用timevait的连接 sysctl -w net.ipv4.tcp_tw_recycle=0 #回收禁用 2. 配置文件/etc/security/limits.conf hard nofile 204800 soft nofile 204800 soft core unlimited soft stack 204800 "},{"id":161,"href":"/docs/%E5%A4%A7%E8%A7%84%E6%A8%A1%E5%B9%B6%E5%8F%91%E4%B8%8B%E5%A6%82%E4%BD%95%E5%8A%A0%E5%BF%AB-pod-%E5%90%AF%E5%8A%A8%E9%80%9F%E5%BA%A6-da-gui-mo-bing-fa-xia-ru-he-jia-kuai-pod-qi-dong-su-du/","title":"大规模并发下如何加快 Pod 启动速度 2024-04-03","section":"Docs","content":" 容器化的应用在真正能够运行业务逻辑前，需要经过镜像拉取、镜像解压、为容器运行时提供联合文件系统、容器启动、业务初始化等多个步骤，其中容器镜像拉取是所有环节中最耗时的。 # 在大规模集群下，镜像拉取如果耗时过久，对于流量突发场景，会影响业务的弹性效率；对于大数据、AI 的场景，会影响任务的吞吐性能。为缓解对于弹性效率的影响，研发团队需要提前更多时间预置节点，扩容业务容器；针对吞吐性能影响，需要扩大集群的规模。然而上述操作都会在无形中造成成本上升。因此镜像拉取的优化一直是容器服务重点优化的方向之一。 # 火山引擎容器服务 VKE（Volcengine Kubernetes Engine）深度融合新一代云原生技术，可提供高性能、高可靠、极致弹性的企业级容器管理能力。在服务企业客户的过程中，为了进一步提升镜像拉取效率，帮助以 AIGC 为代表的企业敏捷、高效地落地 AI 技术，容器服务 VKE 结合对镜像拉取环节问题的分析，从三个不同角度对镜像拉取进行了一系列优化： # P2P 加速\n在大镜像场景下，火山引擎容器服务 VKE 基于开源项目 Dragonfly，推出了 P2P 加速方案，来规避镜像仓库 CR 带宽有限的问题。\nP2P 加速原理\nDragonfly 有如下组件：\nManager：维护每个 P2P 集群之间的关系，动态配置管理。 Scheduler：为下载节点选择最优的下载父节点，控制异常 Peer 的回源。 Peer：Dragonfly 网络中的一个节点，也就是用户提出文件下载请求的计算机或服务器。 火山引擎容器服务 VKE 实现了对 Manager 和 Scheduler 的托管化改造，无需用户额外管理。VKE 中 P2P 组件的工作流程如下图所示：\n当一个 Peer（例如，Peer A）需要拉取镜像时，它会首先与 Manager 节点进行通信。 Manager 会检查所有在线的 Peer 的列表，考虑到各种因素（如网络连通性等）。 选择合适的 Peer 作为 Parent Peer。如果没有可供选择的 Parent Peer，Manager 会带领 Peer 直接从源服务器获取镜像。 Manager 把找到的 Parent Peer 信息发送给发起请求的 Peer A，包括每个 Parent Peer 的地址和服务端口。 Peer A 根据从 Manager 接收到的 Parent Peer 信息，从其它的 Peer 中下载镜像数据。 Scheduler 模块会持续监控整个文件下载过程。如果发现 Parent Peer 下载速度过慢或者出现错误的情况，它将重新从 Manager 获取新的 Parent Peer 进行下载。 当获取整个镜像后，Peer A 就成为了该镜像的一个分发节点，所有的镜像数据都会直接从一个 Peer 传输到另一个 Peer。 如果此时有另一个 Peer（例如 Peer B）也需要同样的镜像，那么当 Dragonfly 收到 Peer B 的请求时，同样经过 Manager、Scheduler 的处理，最后会从已经保存了该镜像的 Peer A 那里拉取数据，从而实现了 P2P 的镜像分发。\n使用流程\n在火山引擎上使用 P2P 的大致的流程如下：\n前置条件：\n有可用的 VKE 集群 有可用的标准版镜像仓库实例 操作步骤：\n在标准版镜像仓库实例内开启对应 VKE 集群的 P2P 分发能力 在 VKE 集群内安装 p2p-accelerator 组件 下图展示了实际的加速效果：\n数据显示，使用 Dragonfly 后，拉取镜像的时间在不同程度上得到了缩短，效率提升了 6 倍以上，甚至在规模较大的情况下，可以达到 200 倍。在普通的场景下，Pod 的拉取镜像时间基本上呈指数递增的趋势，但在 Dragonfly 的场景下，它有效地控制了增长趋势，用户能在一分钟内完成一个 3G 镜像的拉取，即使并发拉取的规模达到了 500 量级，这个时间也几乎是恒定的。 # 容器镜像懒加载\n根据研究分析（论文：https://www.usenix.org/conference/fast16/technical-sessions/presentation/harter），容器镜像中的绝大部分文件内容在容器启动阶段都是不需要被读取的，因此这部分内容在容器启动阶段不需要预先下载。 # 容器镜像懒加载技术使得容器运行时在启动容器前， 不需要将整个容器镜像全量下载到计算节点的本地文件系统，尽可能地降低镜像下载对容器端到端启动时间的影响。当容器需要读取镜像中的文件内容的时候，懒加载技术才会从镜像中心下载对应的数据块并解压。 # Dragonfly 社区曾对比过 OCI 镜像和 Nydus 镜像的容器启动耗时，发现 Nydus 镜像的性能显著优于 OCI 镜像，尤其是当 OCI 镜像大小增加时： # 基于上述数据，火山引擎容器服务 VKE 兼容了 Dragonfly 的开源子项目 Nydus 作为容器镜像懒加载方案的技术底座。借助 VKE 内置的的容器镜像懒加载能力和火山引擎容器镜像中心服务 CR 的一键镜像转换服务，用户可以便捷地使用镜像懒加载技术快速启动容器。 # 使用镜像懒加载前，用户需要将 OCI 镜像转换成 Nydus 的镜像格式。用户可以使用 Nydus 社区的工具在本地转换后推送到任何满足 OCI 规范的镜像中心；或者使用火山引擎镜像中心服务 CR 内置针对 Nydus 镜像格式转换服务 —— 将托管在 CR 的 OCI 镜像一键转换成 Nydus 镜像。 # 具备 Nydus 格式的容器镜像后，用户在创建 VKE 集群的时候，只需要打开节点池支持“懒加载”的开关，VKE 集群便具备了使用懒加载格式镜像的全部配置。更加详细的使用步骤可以参阅：www.volcengine.com/docs/6460/1130434。 # 自定义系统镜像+预热\n如果容器镜像非常大，且镜像中的大部分 layer 都不会频繁变更，加之对镜像拉取的速度要求又比较高时，工程师可以考虑使用自定义系统镜像的方案。 # 自定义镜像原理\n为了避免混淆，这里先明确两个“镜像”：\n自定义系统镜像：云上 ECS 所使用的 OS 镜像，对一台运行中的 ECS ，可以将 ECS 中的内容导出为“自定义镜像”，方便后续再基于这个“自定义镜像”来创建新的 ECS。 # 自定义镜像是您自行创建或上传的镜像，是您的私有镜像。镜像中除操作系统外，您还可以预装公共应用或私有应用，具有更高的定制化性。适用于需要重新部署复杂初始化系统或多次部署同样配置服务器的场景。关于“系统镜像”的更多信息，请参考：www.volcengine.com/docs/6396/801453\n容器镜像：创建 Pod 时所需要的 image，一般通过 Dockerfile 构建后推送到镜像仓库中，启动 Pod 时再从镜像仓库拉取到节点上。\n如果容器镜像巨大，Pod 在启动时会消耗大量的时间在拉取容器镜像上。容器镜像的拉取耗时，取决于 layer 数、layer 的大小、镜像仓库的下载带宽、拉取 layer 的并发度、节点上磁盘的写入速度等因素。 # 我们可以将容器镜像的数据，提前预置到自定义系统镜像中，避免在 Pod 启动再花时间去拉取容器镜像。 # 使用流程\n在火山引擎容器服务 VKE 中制作自定义镜像的大致流程如下： # 创建 ECS # 在 ECS 中安装、启动 Containerd # 利用 ctr 或者 crictl 命令拉取容器镜像 # 卸载 containerd 和其他临时文件 # 将 ECS 导出为自定义镜像 # 如果直接使用上述方式手动构建自定义系统镜像，会有以下问题： # 需要用户手动操作，流程较为繁琐且易出错 # 制作出来的自定义系统镜像无法被 VKE 识别，VKE 只识别带有特定标签的自定义系统镜像 # 首次使用自定义系统镜像创建 ECS 时，因系统镜像未预热，创建耗时较长 # 因此火山引擎提供了两种方式来构建自定义系统镜像： # 【推荐】Docker CLI：在安装了 Docker 的节点上，执行一条 docker run 语句，自动完成 ECS 的创建、 ECS 中拉取容器镜像、导出这个自定义系统镜像并完成预热。具体使用方式可以参考：www.volcengine.com/docs/6460/357229。 # 手动基于 ECS 来创建自定义镜像：在某些场景下，除了期望能在系统镜像中预置容器镜像外，还期望能安装自己的软件包、安装特定的 GPU 驱动，此时我们可以使用这种方式，自己在 ECS 中执行操作，最后执行特定的脚本执行镜像的打包检查、导出自定义镜像。具体使用方式可以参考：www.volcengine.com/docs/6460/1159228。 # 通过自定义系统镜像，在大批量扩容时，可以规避拉取镜像的操作，从而提升了业务 Pod 的启动速度、减少节点上因拉取镜像而带来的磁盘写入和网络下载。不论容器镜像多大，Pod 在启动时的镜像拉取耗时，都可以收敛到秒级以内。但因容器镜像固化到系统镜像内，之后可能会需要不断重新制作自定义系统镜像以便更新容器镜像内容，因此需要合理取舍 Pod 的启动性能、制作频率和制作成本。 # 总结\n为了加快 Pod 启动速度，容器服务 VKE 提供了不同的方案进行优化。用户可以根据场景采取合适的方法。 # 综上所述，自定义节点镜像和预热在使用流程上多了镜像拆分和节点镜像预热的步骤。但在预热后，整体拉取镜像的时间和性能，不会受其他因素影响。镜像频繁变动场景，需要再次制作自定义节点镜像、重新进行镜像预热， 因此在镜像变动频繁的场景，可以使用 P2P 加速或镜像懒加载方案，也可以结合使用该两个方案。 # "},{"id":162,"href":"/docs/%E5%A6%82%E4%BD%95%E4%B8%BAk8s%E4%BF%9D%E9%A9%BE%E6%8A%A4%E8%88%AA-ru-he-wei-k8s-bao-jia-hu-hang/","title":"如何为K8S保驾护航 2024-04-16 17:09:21.962","section":"Docs","content":"随着Kubernetes的不断发展，技术不断成熟，越来越多的公司选择把自家的应用部署到Kubernetes中。但是把应用部署到Kubernetes中就完事了吗？显然不是，应用容器化只是万里长征的第一步，如何让应用安心、稳定的运行才是后续的所有工作。\n这里主要从以下几个方面来进行整理，对于大部分公司足够使用。\nNode # Node可以是物理主机，也可以是云主机，它是Kubernetes的载体。在很多时候我们并不太关心Node怎么样了，除非其异常。但是作为运维人员，我们最不希望的就是异常，对于Node也是一样。\nNode节点并不需要做太多太复杂的操作，主要如下：\n\u0026gt;内核升级 # 对于大部分企业，CentOS系统还是首选，默认情况下，7系列系统默认版本是3.10，该版本的内核在Kubernetes社区有很多已知的Bug，所以对节点来说，升级内核是必须的，或者企业可以选择Ubuntu作为底层操作系统。\n升级内核的步骤如下（简单的升级方式）：\nwget https://elrepo.org/linux/kernel/el7/x86_64/RPMS/kernel-lt-5.4.86-1.el7.elrepo.x86_64.rpm rpm -ivh kernel-lt-5.4.86-1.el7.elrepo.x86_64.rpm cat /boot/grub2/grub.cfg | grep menuentry grub2-set-default \u0026#39;CentOS Linux (5.4.86-1.el7.elrepo.x86_64) 7 (Core)\u0026#39; grub2-editenv list grub2-mkconfig -o /boot/grub2/grub.cfg reboot \u0026gt;软件更新 # 对于大部分人来说，更新软件在很多情况下是不做的，因为害怕兼容问题。不过在实际生产中，对于已知有高危漏洞的软件，我们还需要对其进行更新，这个可以针对处理。\n\u0026gt;优化Docker配置文件 # 对于Docker的配置文件，主要优化的就是日志驱动、保留日志大小以及镜像加速等，其他的配置根据情况而定，如下：\ncat \u0026gt; /etc/docker/daemon.json \u0026lt;\u0026lt; EOF { \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;], \u0026#34;log-driver\u0026#34;: \u0026#34;json-file\u0026#34;, \u0026#34;log-opts\u0026#34;: { \u0026#34;max-size\u0026#34;: \u0026#34;100m\u0026#34;, \u0026#34;max-file\u0026#34;: \u0026#34;10\u0026#34; }, \u0026#34;bip\u0026#34;: \u0026#34;169.254.123.1/24\u0026#34;, \u0026#34;oom-score-adjust\u0026#34;: -1000, \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://pqbap4ya.mirror.aliyuncs.com\u0026#34;], \u0026#34;storage-driver\u0026#34;: \u0026#34;overlay2\u0026#34;, \u0026#34;storage-opts\u0026#34;:[\u0026#34;overlay2.override_kernel_check=true\u0026#34;], \u0026#34;live-restore\u0026#34;: true } EOF \u0026gt;优化kubelet参数 # 对于K8S来讲，kubelet是每个Node的组长，负责Node的\u0026quot;饮食起居\u0026quot;，这里对它的参数配置主要如下：\ncat \u0026gt; /etc/systemd/system/kubelet.service \u0026lt;\u0026lt;EOF [Unit] Description=kubelet: The Kubernetes Node Agent Documentation=https://kubernetes.io/docs/ [Service] ExecStartPre=/usr/bin/mkdir -p /sys/fs/cgroup/pids/system.slice/kubelet.service ExecStartPre=/usr/bin/mkdir -p /sys/fs/cgroup/cpu/system.slice/kubelet.service ExecStartPre=/usr/bin/mkdir -p /sys/fs/cgroup/cpuacct/system.slice/kubelet.service ExecStartPre=/usr/bin/mkdir -p /sys/fs/cgroup/cpuset/system.slice/kubelet.service ExecStartPre=/usr/bin/mkdir -p /sys/fs/cgroup/memory/system.slice/kubelet.service ExecStartPre=/usr/bin/mkdir -p /sys/fs/cgroup/systemd/system.slice/kubelet.service ExecStart=/usr/bin/kubelet \\ --enforce-node-allocatable=pods,kube-reserved \\ --kube-reserved-cgroup=/system.slice/kubelet.service \\ --kube-reserved=cpu=200m,memory=250Mi \\ --eviction-hard=memory.available\u0026lt;5%,nodefs.available\u0026lt;10%,imagefs.available\u0026lt;10% \\ --eviction-soft=memory.available\u0026lt;10%,nodefs.available\u0026lt;15%,imagefs.available\u0026lt;15% \\ --eviction-soft-grace-period=memory.available=2m,nodefs.available=2m,imagefs.available=2m \\ --eviction-max-pod-grace-period=30 \\ --eviction-minimum-reclaim=memory.available=0Mi,nodefs.available=500Mi,imagefs.available=500Mi Restart=always StartLimitInterval=0 RestartSec=10 [Install] WantedBy=multi-user.target EOF 其功能主要是为每个Node增加资源预留，可以在一定程度上防止Node宕机。\n\u0026gt;日志配置管理 # 这里的日志配置管理针对的是系统日志，非自研应用日志。默认情况下，系统日志都不需要我们特殊配置，我这里提出来，主要是保障日志的可追溯。当系统因为某种原因被入侵，系统系统被删除的情况下，还有日志提供给我们分析。\n所以在条件允许的情况下，对Node节点的系统日志进行远程备份是有必要的，可以采用rsyslog进行配置管理，日志可以保存到远端的日志中心或者oss上。\n\u0026gt;安全配置 # 安全配置这里涉及的不多，主要是针对已知的一些安全问题进行加固。主要有以下五种（当然还有更多，看自己的情况）：\nssh密码过期策略 密码复杂度策略 ssh登录次数限制 系统超时配置 历史记录配置 Pod # Pod是K8S的最小调度单元，是应用的载体，它的稳定直接关乎应用本身，在部署应用的时候，主要考虑一下几个方面。\n\u0026gt;资源限制 # Pod使用的是主机的资源，合理的资源限制可以有效避免资源超卖或者资源抢占问题。在配置资源限制的时候，要根据实际的应用情况来决定Pod的QoS，不同的QoS配置情况不一样。\n如果应用的级别比较高，建议配置Guaranteed级别配置，如下：\nresources: limits: memory: \u0026#34;200Mi\u0026#34; cpu: \u0026#34;700m\u0026#34; requests: memory: \u0026#34;200Mi\u0026#34; cpu: \u0026#34;700m\u0026#34; 如果应用级别一般，建议配置Burstable级别，如下：\nresources: limits: memory: \u0026#34;200Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; requests: memory: \u0026#34;100Mi\u0026#34; cpu: \u0026#34;100m\u0026#34; 强烈不建议使用BestEffort类型的Pod。\n\u0026gt;调度策略 # 调度策略也是根据情况来定，如果你的应用需要指定调度到某些节点，可以使用亲和性调度，如下：\naffinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: - preference: {} weight: 100 requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: env operator: In values: - uat 如果一个节点只允许某一个应用调度，这时候就需要用到污点调度了，也就是先给节点打污点，然后需要调度到该节点的Pod需要容忍污点。最稳妥的方式是标签+污点相结合。如下：\ntolerations: - key: \u0026#34;key1\u0026#34; #能容忍的污点key operator: \u0026#34;Equal\u0026#34; #Equal等于表示key=value ， Exists不等于，表示当值不等于下面value正常 value: \u0026#34;value1\u0026#34; #值 effect: \u0026#34;NoExecute\u0026#34; #effect策略 tolerationSeconds: 3600 #原始的pod多久驱逐，注意只有effect: \u0026#34;NoExecute\u0026#34;才能设置，不然报错 当然，除了Pod和Node的关联，还有Pod和Pod之间的关联，一般情况下，为了达到真正的高可用，我们不建议同一个应用的Pod都可以调度到同一个节点，所以我们需要给Pod做反亲和性调度，如下：\naffinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - store topologyKey: \u0026#34;kubernetes.io/hostname\u0026#34; 如果某个应用亲和其他应用，则可以使用亲和性，这样可以在一定程度上降低网络延迟，如下：\naffinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: security operator: In values: - S1 topologyKey: failure-domain.beta.kubernetes.io/zone \u0026gt;优雅升级 # Pod默认是采用的滚动更新策略，我们关注点主要在新的Pod起来后，老的Pod如何能优雅的处理流量，对外界是无感的。\n最简单的方式是\u0026quot;睡几秒\u0026quot;，这种方式并不能保证百分百的优雅处理流量，方式如下：\nlifecycle: preStop: exec: command: - /bin/sh - -c - sleep 15 如果有注册中心，可以在退出的时候先把原服务从注册中心下线再退出，比如这里使用的nacos作为注册中心，如下：\nlifecycle: preStop: exec: command: - /bin/sh - -c - \u0026#34;curl -X DELETE your_nacos_ip:8848/nacos/v1/ns/instance?serviceName=nacos.test.1\u0026amp;ip=${POD_IP}\u0026amp;port=8880\u0026amp;clusterName=DEFAULT\u0026#34; \u0026amp;\u0026amp; sleep 15 \u0026gt;探针配置 # 探针重要吗？重要！它是kubelet判断Pod是否健康的重要依据。\nPod的主要探针有：\nlivenessProbe readinessProbe startupProbe 其中startupProbe是v1.16版本后才新增的探针，其主要针对启动时间较长的应用，在多数情况下只需要配置livenessProbe和readinessProbe即可。\n通常情况下，一个Pod就代表一个应用，所以在配置探针的时候最好能直接反应应用是否正常，很多框架都带有健康检测功能，我们在配置探针的时候可以考虑使用这些健康检测功能，如果框架没有，也可以考虑让开发人员统一开发一个健康检测接口，这样便于标准化健康检测。如下：\nreadinessProbe: failureThreshold: 3 httpGet: path: /health port: http scheme: HTTP initialDelaySeconds: 40 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 3 livenessProbe: failureThreshold: 3 httpGet: path: /health port: http scheme: HTTP initialDelaySeconds: 60 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 2 如果需要配置startupProbe，则可以如下配置：\nstartupProbe: httpGet: path: /health prot: 80 failureThreshold: 10 initialDelay：10 periodSeconds: 10 \u0026gt;保护策略 # 这里的保护策略主要是指在我们主动销毁Pod的时候，通过保护策略来控制Pod的运行个数。\n在K8S中通过PodDisruptionBudget（PDB）来实现这个功能，对于一些重要应用，我们需要为其配置PDB，如下：\napiVersion: policy/v1beta1 kind: PodDisruptionBudget metadata: name: pdb-demo spec: minAvailable: 2 selector: matchLables: app: nginx 在PDB中，主要通过两个参数来控制Pod的数量：\nminAvailable：表示最小可用Pod数，表示在Pod集群中处于运行状态的最小Pod数或者是运行状态的Pod数和总数的百分比； maxUnavailable：表示最大不可用Pod数，表示Pod集群中处于不可用状态的最大Pod数或者不可用状态Pod数和总数的百分比； 注意：minAvailable和maxUnavailable是互斥了，也就是说两者同一时刻只能出现一种。\n日志 # 日志会贯穿应用的整个生命周期，在排查问题或者分析数据的时候，日志都不可缺少。对于日志，这里主要从以下方面进行分析。\n\u0026gt;日志标准 # 日志一般分为业务日志和异常日志，对于日志，我们不希望其太复杂，也不希望其太简单，更多的是希望通过日志达到如下目标：\n对程序运行情况的记录和监控； 在必要时可详细了解程序内部的运行状态； 对系统性能的影响尽量小； 那日志标准如何定义呢？我这里简单整理以下几点：\n合理使用日志分级 统一输出格式 代码编码规范 日志输出路径统一 日志输出命名规范统一 这样规定的主要目的是便于收集和查看日志。\n\u0026gt;收集 # 针对不同的日志输出有不同的日志收集方案，主要有以下二种：\n在Node上部署Logging Agent进行收集 在Pod中以Sidecar形式进行收集 在Node上部署Logging Agent进行收集 # 这种日志收集方案主要针对已经标准输出的日志，架构如下：对于非标准输出的日志就没办法进行收集。\n在Pod中以Sidecar形式进行收集 # 这种收集方案主要针对非标准输出的日志，可以在Pod中以sidecar的方式运行日志收集客户端，将日志收集到日志中心，架构如下：\n不过这种方式比较浪费资源，所以最理想的情况就是把应用日志都标准输出，这样收集起来比较简单。\n\u0026gt;分析 # 在业务正常的情况下，我们其实很少去查看日志内容，只有在出问题的时候才会借助日志分析问题（大部分情况下都是这样），那为什么我这里要把分析提出来呢？\n日志其实承载了很多信息，如果能对日志进行有效分析，可以帮助我们识别、排查很多问题，比如阿里云的日志中心，在日志分析方面做的就很不错。\n\u0026gt;告警 # 日志告警，可以让我们快速知道问题，也缩小了故障排查范围。不过要做日志告警就必须做好日志“关键字”管理，也就是要确定某一个关键字能够准确的代表一个问题，最好不出现泛指的现象，这样做的好处就是能够让告警更加准备，而不是出现一些告警风暴或者无效告警，久而久之就麻木了。\n监控 # 集群、应用等的生命周期里离不开监控系统，有效的监控系统可以为我们提供更高的可观测性，方便我们线性的分析问题，排查问题以及定位问题，再配上有效的告警通知，也方便我们能快速的知道问题。\n对于监控，主要从以下几个方面进行介绍。\n\u0026gt;集群监控 # 对于K8S集群以及跑在K8S应用来说，普遍使用Prometheus来进行监控。整个集群的稳定性关乎着应用的稳定性，所以对集群的监控至关重要，下面简单列举了一些监控项，在实际的工作中酌情处理。\n\u0026gt;应用监控 # 在很多企业中，并没有接入应用监控，主要还是没有在应用中集成监控指标，导致无法监控，所以在应用开发的时候就强烈建议开发将应用监控加上，将指标按prometheus标准格式暴露出来。\n除了开发人员主动暴露指标外，我们也可以通过javaagent方式配置一些exporter，用来抓取一些指标，比如jvm监控指标。\n在应用级别做监控，可以将监控粒度更细化，这样可以更容易发现问题。我这里简单整理了一些应用监控项，如下：\n这些监控项都有对于的exporter来完成，比如redis中间件有redis-exporter，api监控有blcakbox-exporter等。\n\u0026gt;事件监控 # 在Kubernetes中，事件分为两种，一种是Warning事件，表示产生这个事件的状态转换是在非预期的状态之间产生的；另外一种是Normal事件，表示期望到达的状态，和目前达到的状态是一致的。\n事件大多数情况下表示正在发生或者已经发生的事，在实际工作中很容易就忽略这类信息，所以我们有必要借助事件监控来规避这类问题。\n在K8S中，常用的事件监控是kube-eventer，它可以收集pod/node/kubelet等资源对象的event，还可以收集自定义资源对象的event，然后将这类信息发送到相关人员。\n通过事件，我们主要关注的监控项如下：\n正常情况下，K8S中的应用是单独的个体存在，彼此之间没有显性的联系，这时候就需要一种手段，将应用间的关系表现出来，方便我们跟踪分析整个链路的问题。\n目前比较流行的链路监控工具有很多，我这边主要是使用skywalking进行链路监控，其主要agent端比较丰富，也提供了很高的自扩展能力，有兴趣的朋友可以了解一下。\n通过链路监控，主要达到以下目的。\n\u0026gt;告警通知 # 很多人会忽略告警通知，觉得告警就行。但是在做告警通知的时候还是需要仔细去考虑的。\n如下简单整理一下关注点。\n个人觉得难点在于哪些指标需要告警。我们在选取指标的时候一定要遵循以下规则：\n告警的指标具有唯一性 告警的指标能正确反应问题 所暴露的问题是需要介入解决的 综合这些规则考虑，才方便我们选取需要的指标。\n第二就是紧急程度分类，这个主要是根据这个告警指标所暴露出来的问题是不是需要我们及时去解决，还有影响范围来综合衡量。\n故障升级主要是针对需要解决的问题没有解决而做的一种策略，提高故障等级，也相当于提高了紧急程度了。通知渠道分类主要是方便我们区分不同的告警，还有如果能快速接收到告警信息。\n写在最后 # 上面写的都是一些基础的操作，对于YAML工程师来说，算是必备的技能储备，这一套放到大部分公司都是适用的。\n"},{"id":163,"href":"/docs/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8tekton%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BAcicd%E5%B9%B3%E5%8F%B0-ru-he-shi-yong-tekton-kuai-su-da-jian-cicd-ping-tai/","title":"如何使用tekton快速搭建CI/CD平台 2024-04-03","section":"Docs","content":"由于组织内部的人事调整，我接手了一套 CICD 平台。为了帮助团队成员尽快熟悉这套系统，今天我将详细讲解 CI 和 CD 的概念，并介绍这套系统底层关键组件 Tekon 的基本知识。最后，我将通过一个 Golang 项目作为示例，向大家演示如何使用 Tekton 从零开始快速构建自己的 CICD 平台，实现自动化的流程。\nCICD # CI 和 CD 是软件开发中常接触到的两个术语，分别代表持续集成（Continuous Integration）和持续交付（Continuous Delivery）或持续部署（Continuous Deployment）。 # CICD 在敏捷开发和 DevOps 扮演着重要的角色，通过自动化的脚本 CICD 可以减少从代码开发到部署期间人工操作次数，从而加快软件交付速度、提高软件质量、促进团队协作。因此，CICD 是实现软件持续高质量交付的关键流程。 # 持续集成\n持续集成的概念早在上个世纪 90 年代就开始出现，经过多年的完善和推广，现在已经被广泛认可和使用。其基本概念是：持续集成是一种软件开发实践，团队成员频繁将他们的工作成果集成在一起；每次提交后，自动触发运行一次包含自动化验证集的构建任务，以便能尽早发现集成问题。\n持续集成的核心原则包括频繁提交代码、自动化构建和测试、快速反馈、团队合作等。\n通过频繁将代码集成到主干分支中，减少代码集成造成的延期和风险，通过自动化构建和测试进行功能测试、单元测试、集成测试等操作快速发现问题，并及时向开发人员反馈测试结果和推进问题修复。在这个过程中，各团队成员之间需要加强沟通和合作，协同开发和解决问题。\n在持续集成中，为了确保功能分支的代码可以顺利集成到主干，团队成员可以采用六步提交法：\n检出代码前先更新代码库，确保代码库中的最新代码与本地代码相同步； 在个人工作区对代码进行修改； 在本地进行构建和测试，确保代码符合修改的要求； 将主干分支合并到个人工作区后，再次进行构建和测试，防止开发期间代码与他人冲突； 将修改后的代码推送到主干分支； 在主干分支进行构建和测试，确保主干分支的代码质量和准确性； 持续交付\u0026amp;\u0026amp;持续部署 # 持续交付和持续部署是在持续集成的基础上更进一步的自动化。\n持续交付是指在开发流程中，将代码持续地构建、测试和打包，并尽可能快地交付到生产环境前的预生产环境中。\n持续部署是指在持续交付的基础上，将代码在通过预生产环境的测试后，自动部署到生产环境中，与持续交付相比，持续部署不需要人工干预。\n那么，如何在企业内快速搭建自己的 CICD 平台呢？Tekton 给我们提供了一种解决方案。\nTekton # Tekton 是一款基于 Kubernetes 实现的 CICD 开源框架，它提供了丰富的组件来满足各种构建、测试和部署的场景。我们可以直接使用 Tekton 来构建自己的 CICD 流程，也可以基于它进行二次开发搭建满足企业内部定制化需求的 CICD 平台。\n为什么是 Tekton # 在社区中有很多 CICD 工具，比如 Gitlab CI、Jenkins、Travis CI、Circle CI 等，那为什么要选择 Tekton 呢？\n原因有三个：\nTekton 是一款轻量级的 CICD 框架，对于熟悉 Kebernetes 的同学来说，它的上手门槛非常低； # Tekton 具有很强的扩展性，通过自定义 Pipeline 和 Task 可以快速定制和扩展 CICD 流程，可以满足不同的业务场景； # Tekton 是一款基于 Kubernetes 实现的 CICD 框架，Tekton 可以充分利用 Kubernetes 的生态，从而大大降低部署和管理的难度； # 作为云原生领域事实上的标准，Kubernetes 提供了强大的能力，包括自动化编排、部署、恢复和自愈等功能。 # 以横向扩容为例，Tekon 可以基于 Kubernetes 提供的节点管理能力快速地完成扩容，从而提高系统的吞吐量和并发处理能力。 # 此外，Tekton 可以很好地融合进 Kubernetes 的生态，与其他 Kubernetes 工具和服务实现整合，包括监控、告警、日志服务等。这样可以形成基于 Kubernetes 的完整 DevOps 技术栈，为实现 CICD 流程的全生命周期管理提供更好的保障和支持。 # 因此，基于 Kubernetes 的实现，是我们选择 Tekton 的主要原因。 # 安装 Tekton # 推荐使用 Operator 的方式安装 Tekton： # kubectl apply -f https://storage.googleapis.com/tekton-releases/operator/latest/release.yaml 安装 Tekton 时可以选择lite、basic、all 等选项，每个选项中含有不同的组件： # 使用 Operator 安装Tekton 时，默认会选择选择all选项安装所有组件，如果不需要这么多组件，可以使用其他选项（例如lite）来进行组件安装： # kubectl apply -f https://raw.githubusercontent.com/tektoncd/operator/main/config/crs/kubernetes/config/lite/operator_v1alpha1_config_cr.yaml 基本概念 # 在开始使用 Tekton 之前，我们先来了解 Tekton 的两个核心对象： # Task：任务，Tekton 中的最小执行单位，由一个按顺序执行的步骤（Step）列表组成； # Pipeline：流水线，由一个或多个 Task 组成，流水线中定义了每个 Task 的执行顺序以及它们的依赖关系； # Task 代表一个明确的任务，可以是构建、测试或者部署等操作，每个 Task 对应的是一个 Kubernetes 的 Pod，任务 Task 里面的步骤 Step 则对应 Pod 中的容器，流水线 Pipeline 则是对任务 Task 的编排。 # 任务 Task 是步骤 Step 的集合，流水线 Pipeline 在任务 Task 的集合。Task 和 Pipeline 都是定义，他们在实例化后分别称为 TaskRun 和 PipelineRun， 会明确声明具体的输入、输出等内容。 # 它们的关系示意图如下： # 接下来，我将以 Golang 项目为例，演示如何使用 Tekton 搭建 CICD 平台。 # 搭建 CICD 平台 # 本次实践使用的 Kubernetes 集群版本是 v1.26.0，Tekton Pipeline 版本是 v0.56.0。另外，需要准备好代码仓库、镜像仓库、nfs 存储等基础设施。 # Golang 项目的目录结构如下： # . ├── Dockerfile ├── go.mod ├── main.go └── README.md 其中 Dockerfile 的内容如下： # FROM golang:latest WORKDIR /app COPY . /app RUN go build -o main . EXPOSE 8080 CMD [\u0026#34;./main\u0026#34;] 整个过程将会使用 Tekton Pipeline 把代码从 Git 仓库克隆到本地，在构建完成后将镜像推送至镜像仓库，最后自动部署到 Kubernetes 集群中。 # 编写 Task # Task 的定义支持以下字段： # apiVersion：API 版本 # kind：类型 # metadata：元数据 # spec：配置 # description：任务的描述 # steps：任务的步骤 # params：任务的执行参数 # workspaces：任务所需的卷的路径 # results：任务将执行结果抛给下游时使用的名称 # volumes：任务中的步骤中可使用的存储卷 # stepTemplate：步骤的共享模板 # sidecars：步骤中的 Sidecar 容器 # 在 Tekton 的插件库中有很多现成的 Task 可以使用，为了方便大家更好地了解 Tekton Task 的开发流程，我将以克隆 Git 仓库代码为例，详细介绍一下里面的细节。 # 示例如下： # # git-clone.yaml apiVersion: tekton.dev/v1 kind: Task metadata: name: git-clone spec: description: git clone workspaces: - name: git-directory description: 克隆代码在本地存储的路径 - name: ssh-directory description: 使用 ssh 克隆代码时所需的相关凭证 params: - name: url description: Git仓库地址 - name: revision description: Git分支/标签 results: - name: gitCommitHash description: Git提交哈希 steps: - name: git-clone image: bitnami/git:latest workingDir: $(workspaces.git-directory.path) env: - name: PARAM_URL value: $(params.url) - name: PARAM_REVISION value: $(params.revision) - name: WORKSPACE_GIT_DIRECTORY_PATH value: $(workspaces.git-directory.path) - name: WORKSPACE_SSH_DIRECTORY_BOUND value: $(workspaces.ssh-directory.bound) - name: WORKSPACE_SSH_DIRECTORY_PATH value: $(workspaces.ssh-directory.path) script: | #!/bin/bash # -x 表示输出每条命令及其结果，-e 表示脚本执行失败时立即退出，-u 表示使用了未定义的变量时立即退出脚本 set -xeu # 设置凭证，使用ssh认证的方式与git进行身份验证 if [ \u0026#34;${WORKSPACE_SSH_DIRECTORY_BOUND}\u0026#34; == \u0026#34;true\u0026#34; ]; then # 将配置中的ssh文件拷贝到~/.ssh目录中，-R 表示递归复制，即复制目录及其子目录和文件，-L 表示解析符号链接，复制符号链接所指向的实际文件，避免复制时拿到的是软连接 cp -RL \u0026#34;${WORKSPACE_SSH_DIRECTORY_PATH}\u0026#34; ~/.ssh chmod 700 ~/.ssh chmod -R 400 ~/.ssh/* fi cd ${WORKSPACE_GIT_DIRECTORY_PATH} mkdir -p git_repo # 将git_repo添加到Git的安全目录列表 git config --global --add safe.directory ${WORKSPACE_GIT_DIRECTORY_PATH}/git_repo # 克隆代码 git clone ${PARAM_URL} git_repo cd git_repo git checkout ${PARAM_REVISION} # 将结果输出到results字段中 GIT_COMMIT_HASH=\u0026#34;$(git log --pretty=format:\u0026#34;%h\u0026#34; -n 1)\u0026#34; echo -n ${GIT_COMMIT_HASH} | tee $(results.gitCommitHash.path) 在克隆 Git 仓库代码时，需要通过volumes字段来提供 Git 凭证，并使用params字段明确指定要克隆的代码仓库、分支等内容。代码克隆到本地后，还需要将它存储到workspaces中以便后续任务进行测试和构建等工作。同时，为了保留 Git 相关的记录，需要通过results将它们传递给后续的任务使用。 # Task 的开发过程并不复杂，主要是在步骤 Step 的script字段里通过 shell 脚本来实现，通过类似的方式，我们可以快速实现测试、构建、部署等场景的 Task。 # 接下来我们再开发一个用于构建镜像的任务 Task ： # # build-push.yaml apiVersion: tekton.dev/v1 kind: Task metadata: name: build-push spec: description: 使用 docker 构建镜像 params: - name: image description: 镜像地址 - name: dockerfilePath description: dockerfile路径 - name: gitCommitHash description: git提交哈希 workspaces: - name: git-directory description: 克隆代码在本地存储的路径 - name: dockerconfig description: docker配置文件 volumes: - name: docker-socket hostPath: path: /var/run/docker.sock type: Socket steps: - name: build image: docker:git workingDir: $(workspaces.git-directory.path) volumeMounts: - name: docker-socket mountPath: /var/run/docker.sock env: - name: WORKSPACE_DOCKERCONFIG_BOUND value: $(workspaces.dockerconfig.bound) - name: WORKSPACE_DOCKERCONFIG_PATH value: $(workspaces.dockerconfig.path) - name: WORKSPACE_GIT_DIRECTORY_PATH value: $(workspaces.git-directory.path) script: | #!/bin/sh set -xeu if [ \u0026#34;${WORKSPACE_DOCKERCONFIG_BOUND}\u0026#34; == \u0026#34;true\u0026#34; ]; then cp -RL \u0026#34;${WORKSPACE_DOCKERCONFIG_PATH}\u0026#34; ~/.docker chmod 700 ~/.docker fi # 将git_repo添加到Git的安全目录列表 git config --global --add safe.directory ${WORKSPACE_GIT_DIRECTORY_PATH}/git_repo cd git_repo IMAGE=\u0026#34;$(params.image)-$(params.gitCommitHash)\u0026#34; docker build -t ${IMAGE} -f $(params.dockerfilePath) . docker push ${IMAGE} 在构建的过程中，我使用的是 Docker In Docker 的方式，将 Docker socket 挂载给容器，以便容器中的进程可以通过 Docker API 与宿主机上的 Docker 引擎进行交互。这种方式可以用于开发环境的调试使用，但不推荐在生产环境中使用。 # 到这里，相信大家应该都明白了任务 Task 的开发流程，后续部署应用的任务我就简单地一笔带过，示例如下： # # deploy-app.yaml apiVersion: tekton.dev/v1 kind: Task metadata: name: deploy-app spec: description: 部署应用 steps: - name: deploy-app image: ubuntu:latest script: | #!/bin/sh echo \u0026#34;部署应用到 Kubernetes 集群\u0026#34; 在完成任务 Task 的编写后，接下来我们需要使用流水线 Pipeline 将它们串起来，实现克隆、构建、推送镜像、部署应用的自动化流程。 # 编排 Pipeline # 通过前面的操作后，我们得到了三个任务 Task： # git-clone：将代码克隆到本地； # build-push：构建代码并推送镜像； # deploy-app：部署服务到集群应用； # 我们来看一下流水线 Pipeline 的编排方式，示例如下： # # pipeline.yaml apiVersion: tekton.dev/v1 kind: Pipeline metadata: name: cicd spec: description: 使用git和docker等工具将项目从远程代码仓库拉到容器中构建，将产物推送到镜像仓库，并部署到集群中 params: - name: url description: git仓库地址 - name: revision description: git分支/标签 - name: image description: 镜像地址 - name: dockerfilePath description: dockerfile路径 workspaces: - name: git-directory description: 克隆代码在本地存储的路径 - name: ssh-directory description: 克隆代码时所需的相关凭证 - name: dockerconfig description: docker配置文件 tasks: - name: git-clone taskRef: name: git-clone params: - name: url value: $(params.url) - name: revision value: $(params.revision) workspaces: - name: git-directory workspace: git-directory - name: ssh-directory workspace: ssh-directory - name: build-push runAfter: [\u0026#34;git-clone\u0026#34;] taskRef: name: build-push params: - name: image value: $(params.image) - name: dockerfilePath value: $(params.dockerfilePath) - name: gitCommitHash value: $(tasks.git-clone.results.gitCommitHash) workspaces: - name: git-directory workspace: git-directory - name: dockerconfig workspace: dockerconfig - name: deploy-app runAfter: [\u0026#34;build-push\u0026#34;] taskRef: name: deploy-app 在编排流水线时，需要处理好任务之间的顺序。这里我们通过runAfter字段来表明任务build-push在git-clone之后运行，任务deploy-app又在build-push之后运行，同时还需要关注任务所需的参数和依赖关系。 # 接下来，我们将使用 PipelineRun 运行流水线。 # 运行流水线 # Tekton 的 PipelineRun 对象负责实例化流水线 Pipeline，明确声明各种参数和依赖，并自动为流水线中的任务 Task 创建相应的 TaskRun。 # 完整的 PipelineRun 对象配置如下： # # pipelinerun.yaml apiVersion: tekton.dev/v1 kind: PipelineRun metadata: generateName: cicd- spec: pipelineRef: name: cicd params: - name: url value: git@zlw.com:golang/demo.git - name: revision value: master - name: image value: hub-zlw.com/golang/demo:0.0.1 - name: dockerfilePath value: ./Dockerfile workspaces: - name: ssh-directory secret: secretName: ssh-directory - name: dockerconfig secret: secretName: dockerconfig - name: git-directory volumeClaimTemplate: spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi taskRunTemplate: podTemplate: securityContext: fsGroup: 65532 在运行流水线的时，我们提供了以下参数和依赖： # 参数： # url：镜像仓库地址 # revision：代码分支或版本 # Image：构建完成的镜像名称 # dockerfilePath：dockerfile文件的路径 # 依赖： # 密钥： # ssh凭证：用于拉取代码 # docker凭证：用于推送镜像 # 存储：用于存放代码 # 依赖中的密钥和存储，我们需要提前准备，示例如下： # # git凭证 # -w0表示一行来显示 $ cat ~/.ssh/id_rsa | base64 -w0 LS0tLS1CRU== $ cat ~/.ssh/known_hosts | base64 -w0 d3BzZ2l0Lmtpbm== # 将base64后的凭证放在Secret中，包括id_rsa和known_hosts等文件 $ kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Secret metadata: name: ssh-directory namespace: default type: Opaque data: id_rsa: LS0tLS1CRU== known_hosts: d3BzZ2l0Lmtpbm== EOF # docker 凭证 $ kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Secret metadata: name: dockerconfig type: kubernetes.io/dockerconfigjson data: config.json: eyJhdXRocyI6eyJod== EOF 存储方面，我使用的是 nfs 作为 Tekton 的存储，由于 nfs 的安装过程稍微比较复杂，后续有需要可以单独细说，这里大家自行准备。 # 最后，我们通过命令行运行流水线和查看流水结果： # # 运行流水线 $ kubectl create -f pipelinerun.yaml $ 查看运行结果 $ kubectl get pipelinerun NAME SUCCEEDED REASON STARTTIME COMPLETIONTIME cicd-24c5j True Succeeded 19h 19h cicd-cs9gq True Succeeded 20h 20h 当然，我们也可以通过 Tekton 提供的 Dashboard 查看流水线：\n到这里，我们已经使用 Tekton 完成了最小闭环的 CICD 流程。借助 Tekton 的生态和能力，我们可以快速构建自己的 CICD 平台，或者在 Tekton 上做二次开发，以满足自身偏好或公司业务需求，打造更加完善的内部生态。 # 总结 # 基于 Tekton 实现的 CICD 平台具有巨大的创新潜力，不仅能为各类型的应用提供 CICD 流程支持，还能用于支持各种云原生服务，例如通过 istio 进行流量控制、实现混沌工程等。此外，Tekton 还可作为通用的工作流引擎使用，以实现基于事件的自动化流程等。 # 在本期内容中，我向大家介绍了 CICD 的基本定义以及如何使用 Tekton 从零开始快速构建 CICD 平台，包括 Tekton 的基本概念、如何开发任务 Task、如何编排流水线 Pipeline 以及如何运行流水线等。在后续的内容中，我将更加深入介绍 Tekton 的更多高级功能，例如与其他工具的集成、Tekton 的调度优化以及性能优化等。我们下期再见~ # "},{"id":164,"href":"/docs/%E5%A6%82%E4%BD%95%E8%B0%83%E8%AF%95-crash-%E5%AE%B9%E5%99%A8%E7%9A%84%E7%BD%91%E7%BB%9C-ru-he-diao-shi-crash-rong-qi-de-wang-luo/","title":"如何调试 crash 容器的网络 2024-04-03 14:50:51.378","section":"Docs","content":" 背景 # 现在大部分的公司业务基本都已经容器化，甚至 K8S 化的情况下，当容器、Pod 运行异常时，无非是看看其日志和一些 K8S event 信息，当容器、Pod 内部出现网络访问失败时，或者其他一些问题时。通常会进入容器、Pod 内部通过一些网络工具来进行调试，那么问题来了。一般容器内是不会安装太多的调试工具，基本都是最小化的操作系统，所以有的时候根本没办法调试。 # 可以有以下几种方式解决： # • 就在制作镜像时，安装一些常用的工具，比如：ip、ping、telnet、ss、tcpdump 等命令，方便后期使用，但是这个就违背了容器化的最小化镜像原则。本身上容器就是实现更轻便、更快速地启动业务，安装一些列工具就增加了镜像大小。 # • 在容器、Pod 内直接安装所需的命令，但是安装过程可能会比较困难，有的时候容器内操作系统连 yum、rpm 等安装工具都没有，编译安装显然太浪费时间。同时有可能离线环境，没 yum 等特殊情况，都会使得安装非常麻烦。 # • 利用容器的运行原理，使用 nsenter 来进入容器的对应 namespace 进行调试。 # 所以，显然第一、第二两种方法不方便且不现实，所以看看第三种方法是如何操作的。 # 容器原理 # 在介绍 nsenter 之前，先见到说下容器的原理，当我们用 docker run 启动一个容器时，实际上底层做的就是创建一个进程以及对应的 network namespace、mount namespace、uts namespace、ipc namespace、pid namspace、user namespace，然后将这个进程加入到这些到命令空间，同时给划分对应的 cgroup，最后使用 chroot 将容器的文件系统切换为根目录。这样就实现了这个进程与 root namespace 的多维度隔离，使得进入容器内就像是进入一个新的操作系统。 # 所以说容器就是一个进程，只不过他都加入到不同的命名空间下了。 # nsenter 简介 # nsenter 是一个 Linux 命令行工具，作用是可以进入 Linux 系统下某个进程的命令空间，如 network namespace、mount namespace、uts namespace、ipc namespace、pid namspace、user namespace、cgroup。 # 所以使用 nsenter 调试容器网络，可以按照以下步骤操作： # • 在 root namespace 下找到容器的 Pid，也就是这个容器在 root namespace 下的进程号 # • 使用 nsenter 进入到该 Pid 的 network namespace 即可，这样就保证了当前的环境是容器的网络环境，但是文件系统还是在 root namespace 下，以及 user、uts 等命名空间都还是在 root namespace 下。所以就可以使用 root namespace 下的调试命令来进行调试了。 # nsenter 位于 util-linux 包中，一般常用的 Linux 发行版都已经默认安装。如果你的系统没有安装，可以使用以下命令进行安装： # # Centos $ yum install util-linux 使用 nsenter - - help 查看 nsenter 用法。 # $ nsenter --help 用法： nsenter [选项] [\u0026lt;程序\u0026gt; [\u0026lt;参数\u0026gt;...]] 以其他程序的名字空间运行某个程序。 选项： -a, --all enter all namespaces -t, --target \u0026lt;pid\u0026gt; 要获取名字空间的目标进程 -m, --mount[=\u0026lt;文件\u0026gt;] 进入 mount 名字空间 -u, --uts[=\u0026lt;文件\u0026gt;] 进入 UTS 名字空间(主机名等) -i, --ipc[=\u0026lt;文件\u0026gt;] 进入 System V IPC 名字空间 -n, --net[=\u0026lt;文件\u0026gt;] 进入网络名字空间 -p, --pid[=\u0026lt;文件\u0026gt;] 进入 pid 名字空间 -C, --cgroup[=\u0026lt;文件\u0026gt;] 进入 cgroup 名字空间 -U, --user[=\u0026lt;文件\u0026gt;] 进入用户名字空间 -S, --setuid \u0026lt;uid\u0026gt; 设置进入空间中的 uid -G, --setgid \u0026lt;gid\u0026gt; 设置进入名字空间中的 gid --preserve-credentials 不干涉 uid 或 gid -r, --root[=\u0026lt;目录\u0026gt;] 设置根目录 -w, --wd[=\u0026lt;dir\u0026gt;] 设置工作目录 -F, --no-fork 执行 \u0026lt;程序\u0026gt; 前不 fork -Z, --follow-context 根据 --target PID 设置 SELinux 环境 -h, --help display this help -V, --version display version 上面介绍了 nsenter 的原理，下面就实际演示一下，下面演示两个场景，都是在工作中非常常见的。 # 调试容器网络 # 当使用 docker run 启动一个容器时，容器运行无报错，即容器不在重启的情况下。这种情况直接使用 nsenter 进入可以。 # 先进入容器内 curl www.baidu.com，发现容器内没有 curl 命令 # $ docker run -it alpine-amd64:3.11 sh $ curl http://www.baidu.com sh: curl: not found 下面使用 nsenter 进行调试 # 1、获取容器 Pid，即 3448 # $ docker inspect fd9ec0381062 | grep Pid \u0026#34;Pid\u0026#34;: 3448, \u0026#34;PidMode\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;PidsLimit\u0026#34;: null, 2、使用 nsenter 进入该 Pid 的 network namespace # # -t 表示目标进程号, -n 表示进入 network namespace $ nsenter -t 3448 -n 3、查看当前的网络环境，再使用 curl，发现正常返回 # # 查看当前网络环境，可以确认是容器内的网络 $ ip addr 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 12: eth0@if13: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0 valid_lft forever preferred_lft forever # 再次使用 curl, 发现有命令 $ curl http://www.baidu.com \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;!--STATUS OK--\u0026gt;\u0026lt;html\u0026gt; \u0026lt;head\u0026gt;\u0026lt;meta http-equiv=content-ty...... 调试 Pod 网络 # 实际上 Pod 网络与容器网络是一样的，下面看两个场景 # Pod running 状态 # 当一个 Pod 运行状态是 running 时，其调试方式和上面的 docker 调试方式一样，直接进入容器的 network namespace。 # 1、找到 Pod 的运行结点，即 master-172-31-97-104 # $ kubectl get pods -A -o wide | grep test-nsenter NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test test-nsenter-7df7d5fff7-5f666 1/1 Running 0 4m21s 100.121.45.129 master-172-31-97-104 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 2、去 master-172-31-97-104 结点获取容器 Pid。会发现有两个 test-nsenter 容器，其中一个是业务容器，另一个是 K8S 起的 pause 容器用于共享容器网络。 # 直接获取业务容器的 Pid，即 48344 # $ docker ps | grep test-nsenter f5fdbd788a8e test-nsenter:latest \u0026#34;sleep 300\u0026#34; 6 minutes ago Up 6 minutes k8s_test-nsenter-c5577484c-wlndj_test_516c4915-0fa9-4d1f-a4c3-612b1ab02c13_0 b387d915a853 sea.hub:5000/pause:3.5 \u0026#34;/pause\u0026#34; 7 minutes ago Up 7 minutes k8s_POD_test-nsneter-c5577484c-wlndj_test_516c4915-0fa9-4d1f-a4c3-612b1ab02c13_2 $ docker inspect f5fdbd788a8e | grep Pid \u0026#34;Pid\u0026#34;: 48344, \u0026#34;PidMode\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;PidsLimit\u0026#34;: null, 3、nsenter 进入该 Pid 的 network namespace 中，使用 curl # $ nsenter -t 48344 -n # 查看当前网络环境，可以确认是容器内的网络 $ ip addr 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 3: eth0@if101: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1480 qdisc noqueue state UP group default link/ether 32:b3:d0:37:ad:e9 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 100.118.171.133/32 scope global eth0 valid_lft forever preferred_lft forever 4: tunl0@NONE: \u0026lt;NOARP\u0026gt; mtu 1480 qdisc noop state DOWN group default qlen 1000 link/ipip 0.0.0.0 brd 0.0.0.0 # 再次使用 curl, 发现有命令 $ curl http://www.baidu.com \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;!--STATUS OK--\u0026gt;\u0026lt;html\u0026gt; \u0026lt;head\u0026gt;\u0026lt;meta http-equiv=content-ty...... Pod crashLookBackOff 状态 # 上面演示的容器都是 running 状态，可以在结点上找到对应 Pid，但是如果 Pod 一直在重启，则 Pid 一直都在变，所以调试也会不断中断。 # 所以当 Pod 处于crashLookBackOff 状态 时，可以进入 Pod Pause 容器，因为 Pause 容器与业务容器是共享网络的，而且永远不会重启，除非 Pod 被删除了。 # 1、进入 Pod crashLookBackOff 状态 的容器 network namespace # # 发现只有pause 容器，因为业务容器一直在重启 $ docker ps|grep test-nsenter 70e8079e82ed sea.hub:5000/pause:3.5 \u0026#34;/pause\u0026#34; 39 seconds ago up 38 seconds k8s_POD_test-nsenter-7cdf977947-thk57_test_9fedbb55-4726-4f13-a669-d5bcb0b19b94_0 # 查看 Pause 容器的 Pid $ docker inspect 70e8079e82ed |grep Pid \u0026#34;Pid\u0026#34;: 14213, \u0026#34;PidMode\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;PidsLimit\u0026#34;: null, # nsenter 进入 Pause 容器的 network namespace $ nsneter -t 14213 -n # 查看 pause 容器的网络, 和 Pod 网络一致 $ ip addr 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 2: tunl0@NONE: \u0026lt;NOARP\u0026gt; mtu 1480 qdisc noop state DOWN group default qlen 1000 link/ipip 0.0.0.0 brd 0.0.0.0 4: eth0@if68: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1480 qdisc noqueue state UP group default link/ether c2:64:99:f0:a6:f1 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 100.121.45.130/32 scope global eth0 valid_lft forever preferred_lft forever # 再次使用 curl, 发现有命令 $ curl http://www.baidu.com \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;!--STATUS OK--\u0026gt;\u0026lt;html\u0026gt; \u0026lt;head\u0026gt;\u0026lt;meta http-equiv=content-ty...... 所以说，当使用 nsenter 调试 Pod 网络时，不管 Pod 状态如何，我们直接进入其 Pause 容器的 network namespace 即可。 # 总结 # nsenter 非常便捷地帮助我们调试容器环境下和 K8S 环境下的网络调试，也可以调试其他问题。nsenter 使用也非常简单，是一个非常好用的调试工具，很好地解决了容器镜像缺少命令行工具的问题。 # 除了调试网络，也可以调试容5器的 ipc、mount 等，可以根据场景自行演示。\n下一篇会介绍另一个 K8S 环境下 Pod 网络调试工具，kubectl-debug。 # "},{"id":165,"href":"/docs/%E5%AE%B9%E5%99%A8%E4%B8%AD%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90%E4%BB%A5%E5%8F%8A%E4%B8%8D%E5%90%8Cdnspolicy%E5%AF%B9%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90%E7%9A%84%E5%BD%B1%E5%93%8D-rong-qi-zhong-yu-ming-jie-xi-yi-ji-bu-tong-dnspolicy-dui-yu-ming-jie-xi-de-ying-xiang/","title":"容器中域名解析以及不同dnspolicy对域名解析的影响 2024-04-03 15:04:09.254","section":"Docs","content":" 一、coreDNS背景 # 部署在kubernetes集群中的容器业务通过coreDNS服务解析域名，Coredns基于caddy框架，将整个CoreDNS服务都建立在一个使用Go编写的HTTP/2 Web 服务器Caddy上。通过插件化（链）架构，以预配置的方式（configmap卷挂载内容配置）选择需要的插件编译，按序执行插件链上的逻辑，通过四种方式（TCP、UDP、gRPC和HTTPS）对外直接提供DNS服务。 # 二、kubelet通过修改容器/etc/resolv.conf文件使得容器中可解析域名 # 在kubernetes集群中，coreDNS服务和kube-apiserver通信获取clusterip和serviceName的映射关系，并且coreDNS本身通过clusterip（默认 xx.xx.3.10，比如集群clusterip网段为10.247.x.x，则coreDNS对外暴露服务的clusterip为10.247.3.10），我们知道操作系统域名服务器关键配置文件/etc/resolv.conf中的nameserver字段指定，所以只需要使得容器/etc/resolv.conf中 nameserver字段配置为coreDNS的clusterip地址即可。 # 那么谁来完成容器/etc/resolv.conf的修改和如何修改？kubelet负责拉起容器，启动参数中\u0026ndash;cluster-dns字段对应值就是该集群coreDNS的clusterip地址，kubelet在拉起容器中，根据Pod的dnsPolicy选项，把该值修改注入到容器中。 # 三、Pod不同dnsPolicy对容器/etc/resolv.conf的影响 # Default：如果dnsPolicy被设置为“Default”，则名称解析nameserver配置将从pod运行的节点/etc/resolv.conf继承。 # 节点/etc/resolv.conf配置 nameserver X.X.X.X nameserver X.X.X.Y options ndots:5 timeout:2 single-request-reopen ClusterFirst：如果dnsPolicy被设置为“ClusterFirst”，则使用集群coredns的service 地址作为Pod内/etc/resolv.conf中nameserver配置。 nameserver 10.247.3.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5 timeout:2 single-request-reopen ClusterFirstWithHostNet：对于使用hostNetwork网络模式运行的Pod，需明确设置其DNS策略“ClusterFirstWithHostNet”，否则 hostNetwork + ClusterFirst实际效果 = Default\nClusterFirstWithHostNet 是 Kubernetes 中的一个 DNS 策略。当 Pod 使用 hostNetwork 模式运行时（即 hostNetwork: true），这个策略指示 Pod 优先使用 Kubernetes 环境的 DNS 服务（如 CoreDNS 提供的域名解析服务）进行域名解析。如果 Kubernetes 环境的 DNS 服务无法解析某个域名，那么该请求会被转发到从宿主机继承的 DNS 服务器上进行解析。 这个策略确保了 Pod 在使用宿主机的网络命名空间时，仍然能够利用 Kubernetes 提供的 DNS 服务进行域名解析，从而保持了与 Kubernetes 集群中其他服务的连通性。这对于需要在 Pod 内访问集群内部服务或跨命名空间通信的场景非常有用。 请注意，为了使用 ClusterFirstWithHostNet 策略，您需要在 Pod 的规格中显式设置 dnsPolicy 字段为 ClusterFirstWithHostNet，并且还需要将 hostNetwork 设置为 true 以启用 hostNetwork 模式。如果不设置 dnsPolicy: ClusterFirstWithHostNet，Pod 默认会使用所在宿主主机使用的 DNS，这可能会导致容器内无法通过 service name 访问 Kubernetes 集群中的其他 Pod。 nameserver 10.247.3.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5 timeout:2 single-request-reopen None：它允许用户自定义Pod内/etc/resolv.conf配置，忽略Kubernetes环境中默认的DNS设置。应使用dnsConfigPod规范中的字段提供所有DNS设置 。 /etc/resolv.conf相关配置说明\nnameserver：表示指定的DNS服务地址IP，用于解析域名的服务器。 search：表示域名解析时指定的域名搜索域。解析域名的时候，会依搜索域顺序构建域名解析地址。进行域名解析，直到解析即可。如：svcname.default.svc.cluster.local --\u0026gt; svcname.svc.cluster.local --\u0026gt; svcname.cluster.local options：其他选项。最常见的选项配置有： - ndots值：判断域名解析地址中包含的“.”是否大于或等于ndots设定值，如果是，则以请求解析域名地址作为全限定域名发起解析请求，不再进行search域构建域名地址；如果小于ndots，则按照search域构建域名地址，再逐序发起解析请求。 - timeout：等待DNS服务器返回的超时时间。单位秒（s）。 "},{"id":166,"href":"/docs/%E5%AE%B9%E5%99%A8%E5%86%85%E7%9A%84-1-%E5%8F%B7%E8%BF%9B%E7%A8%8B-rong-qi-nei-de-1-hao-jin-cheng/","title":"容器内的 1 号进程 2024-04-03 14:46:29.998","section":"Docs","content":" 简介 # 在 Linux 系统中，系统启动的时候先是执行内核态的代码，然后在内核中调用 1 号进程的代码，从内核态切换到用户态。内核执行的第一个用户态程序就是 1 号进程。 # 目前主流的 Linux 发行版，无论是 RedHat 系的还是 Debian 系的，都会把 /sbin/init 作为符号链接指向 Systemd。Systemd 是目前最流行的 Linux init 进程，在它之前还有 SysVinit、UpStart 等 Linux init 进程。 # 同样在容器中也有 1 号进程的概念，一旦容器建立了自己的 **Pid Namespace（进程命名空间)，**这个 Namespace 里的进程号也是从 1 开始标记的。 # 1 号进程是第一个用户态的进程，由它直接或者间接创建了容器中的其他进程。 # 为什么杀不掉容器中 1号进程 # 通过几个实际的示例来说明 # 下面示例统一用如下 Dockerfile，启动一个休闲 600s 的容器。 # FROM ubuntu ENTRYPOINT [\u0026#34;sleep\u0026#34;, \u0026#34;600\u0026#34;] 现象 # 示例一\n在容器中使用 kill -9，kill -15 杀死 1 号进程 # $ docker run --name test -d test-init:v1 $ docker exec -it test bash [root@4db3c6f1766b /]# ps -ef UID PID PPID C STIME TTY TIME CMD root 1 0 0 01:38 ? 00:00:00 sleep 600 root 6 0 2 01:39 pts/0 00:00:00 bash root 13 6 0 01:39 pts/0 00:00:00 ps -ef [root@5cc69036b7b2 /]# kill 1 [root@5cc69036b7b2 /]# kill -9 1 [root@5cc69036b7b2 /]# ps -ef UID PID PPID C STIME TTY TIME CMD root 1 0 0 01:38 ? 00:00:00 sleep 600 root 6 0 2 01:39 pts/0 00:00:00 bash root 13 6 0 01:39 pts/0 00:00:00 ps -ef 当我们完成前面的操作，就会发现无论运行 kill 1 (对应 Linux 中的 SIGTERM 信号) 还是 kill -9 1(对应 Linux 中的 SIGKILL 信号)，都无法让进程终止。那么问题来了，这两个常常用来终止进程的信号，都对容器中的 1号进程不起作用。 # 示例二\n在宿主机上使用 kill -9, kill -15 杀死容器 1 号进程对应在宿主机上的进程 # # 获取宿主机上对应容器的进程 $ docker inspect test | grep Pid $ kill -15 24446 $ ps -ef | grep 24446 root 24446 24404 0 09:38 ? 00:00:00 sleep 600 root 54735 44623 0 09:48 pts/0 00:00:00 grep --color=auto 24446 $ kill -9 24446 $ ps -ef | grep 24446 root 55167 44623 0 09:48 pts/0 00:00:00 grep --color=auto 24446 发现 kill -15 无法杀死容器进程，而 kill -9 却可以。 # 示例三\n在宿主机上使用 docker stop \u0026lt;container-id\u0026gt; 杀死容器 # $ /usr/bin/time docker stop e58851b5452f e58851b5452f real 0m 10.19s user 0m 0.04s sys 0m 0.03s 发现经过了 10s 左右容器进程才杀死。 # 示例四\n把容器中的 sleep 换成 Golang 程序，使用 kill -9，kill -15 杀死进程 # Golang 代码如下： # package main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func main() { fmt.Println(\u0026#34;Start app\\n\u0026#34;) time.Sleep(time.Duration(100000) * time.Millisecond) } Dockerfile 如下： # FROM ubuntu # go-demo 就是上面代码编译出的二进制程序 COPY go-demo . RUN chmod +x go-demo ENTRYPOINT [\u0026#34;./go-demo\u0026#34;] 进入容器操作 # $docker exec -it 7a62c48d0d1f bash [root@7a62c48d0d1f /]# ps -ef UID PID PPID C STIME TTY TIME CMD root 1 0 0 02:04 ? 00:00:00 ./go-demo root 10 0 1 02:04 pts/0 00:00:00 bash root 23 10 0 02:04 pts/0 00:00:00 ps -ef [root@7a62c48d0d1f /]# kill -9 1 [root@7a62c48d0d1f /]# ps -ef UID PID PPID C STIME TTY TIME CMD root 1 0 0 02:04 ? 00:00:00 ./go-demo root 10 0 0 02:04 pts/0 00:00:00 bash root 24 10 0 02:04 pts/0 00:00:00 ps -ef [root@7a62c48d0d1f /]# kill 1 发现 kill -9 无法杀死进程，kill -15 可以。 # 要解释以上现象，需要先了解 Linux 中信号机制。 # Linux 信号 # 我们运行 kill 命令，其实在 Linux 里就是发送一个信号。信号一般会从 1 开始编号，通常来说，信号编号是 1 到 64。在 Linux 上我们可以用 kill -l 来看这些信号的编号和名字 # $ kill -l 1) SIGHUP 2) SIGINT 3) SIGQUIT 4) SIGILL 5) SIGTRAP 6) SIGABRT 7) SIGBUS 8) SIGFPE 9) SIGKILL 10) SIGUSR1 11) SIGSEGV 12) SIGUSR2 13) SIGPIPE 14) SIGALRM 15) SIGTERM 16) SIGSTKFLT 17) SIGCHLD 18) SIGCONT 19) SIGSTOP 20) SIGTSTP 21) SIGTTIN 22) SIGTTOU 23) SIGURG 24) SIGXCPU 25) SIGXFSZ 26) SIGVTALRM 27) SIGPROF 28) SIGWINCH 29) SIGIO 30) SIGPWR 31) SIGSYS 典型的使用信号机制的场景： # • 如果我们按下键盘 Ctrl+C，当前运行的进程就会收到一个信号 SIGINT 而退出； # • 如果应用程序内存访问出错了，当前的进程就会收到另一个信号 SIGSEGV； # • 我们也可以通过命令 kill ，直接向一个进程发送一个信号，缺省情况下不指定信号的类型，那么这个信号就是 SIGTERM。也可以指定信号类型，比如命令 kill -9，这里的 9，就是编号为 9 的信号，SIGKILL 信号。 # 在操作系统常见手动杀死进程的信号就是 SIGTERM 和 SIGKILL ，进程在收到信号后，就会去做相应的处理。对于每一个信号，进程对它的处理都有下面三个选择。 # 忽略\n忽略( Ignore ) 就是对这个信号不做任何处理，但是有两个信号例外，对于 SIGKILL 和 SIGSTOP 这个两个信号，进程是不能忽略的。这是因为它们的主要作用是为 Linux kernel 和超级用户提供删除任意进程的特权。 # 捕获\n捕获( Catch )，这个是指让用户进程可以注册自己针对这个信号的 handler。对于捕获，SIGKILL 和 SIGSTOP 这两个信号也同样例外，这两个信号不能由用户自己的处理代码，只能执行系统的缺省行为。 # 缺省行为\n缺省行为( Default )，Linux 为每个信号都定义了一个缺省的行为，你可以在 Linux 系统中运行 man 7 signal 来查看每个信号的缺省行为。对于大部分的信号而言，应用程序不需要注册自己的 handler，使用系统缺省定义行为就可以了。 # 常见的 SIGTERM 和 SIGKILL 信号默认行为都是**终止进程。**针对上面说的三个实例，详细看看这两个信号。 # SIGTERM\n这个信号是 Linux 命令 kill 缺省发出的。前面例子里的命令 kill 1 ，就是通过 kill 向 1 号进程发送一个信号，等价于 kill -15，在没有别的参数时，这个信号类型就默认为 SIGTERM。 # SIGTERM 这个信号是可以被捕获的，这里的捕获指的就是用户进程可以为这个信号注册自己的 handler，而 SIGTERM 信号一般是用于进程优雅退出。 # SIGKILL\nSIGKILL (9)，这个信号是 Linux 里两个特权信号之一。特权信号就是 Linux 为 kernel 和超级用户去删除任意进程所保留的，不能被忽略也不能被捕获。那么进程一旦收到 SIGKILL，就要退出。 # 在前面的例子里，我们运行的命令 kill -9 1 里的参数 -9 ，其实就是指发送编号为 9 的这个 SIGKILL 信号给 1 号进程。 # Linux 信号处理原理 # 在了解了 Linux 信号机制的概念后，再解释上述现象就会较容易理解。 # 在 Linux 中，使用 kill 命令向 1号进程发起信号时，Linux 内核会根据如下代码逻辑判断是否忽略该信号。当下面代码 if 条件为 true 的话则忽略信号，否则执行信号默认行为。 # kernel/signal.c static bool sig_task_ignored(struct task_struct *t, int sig, bool force) { void __user *handler; handler = sig_handler(t, sig); /* unlikely(t-\u0026gt;signal-\u0026gt;flags \u0026amp; SIGNAL_UNKILLABLE) 表示是否为 1 号进程 */ /* handler == SIG_DFL 表示进程是否使用默认信号行为 */ /* force 表示发出信号和接受信号是否在同一个 Namespace 下, 在即为 0, 不在为 1 */ if (unlikely(t-\u0026gt;signal-\u0026gt;flags \u0026amp; SIGNAL_UNKILLABLE) \u0026amp;\u0026amp; handler == SIG_DFL \u0026amp;\u0026amp; !(force \u0026amp;\u0026amp; sig_kernel_only(sig))) return true; return sig_handler_ignored(handler, sig); } 分别对上述 if 中的三个条件进行分析。 # 1、unlikely(t-\u0026gt;signal-\u0026gt;flags \u0026amp; SIGNAL_UNKILLABLE)\nt-\u0026gt;signal-\u0026gt;flags \u0026amp; SIGNAL_UNKILLABLE 表示进程必须是 SIGNAL_UNKILLABLE 的。在每个 Namespace 的 init 进程建立的时候，就会打上 SIGNAL_UNKILLABLE 这个标签，也就是说只要是 1 号进程，就会有这个 flag，这个条件也是满足的。 # 2、handler == SIG_DFL\n判断信号的 handler 是否是 SIG_DFL。对于每个信号，用户进程如果不注册一个自己的 handler，就会有一个系统缺省的 handler，这个缺省的 handler 就叫作 SIG_DFL。 # 对于 SIGKILL，它是特权信号，是不允许被捕获的，所以它的 handler 就一直是 SIG_DFL。对 SIGKILL来说该条件总是满足的。 # 对于 SIGTERM，它是可以被捕获的。也就是说如果用户不注册 handler，那么这个条件对 SIGTERM 也是满足的。 # 3、!(force \u0026amp;\u0026amp; sig_kernel_only(sig))\nforce 表示如果发出信号的进程与接受信号的进程在同一个 Namespace 中值为 false，否则为 true 。 # sig_kernel_only(sig) ****表示信号是否为特权信号，SIGKILL 为特权信号，SIGTERM 不是。 # 所以 !(force \u0026amp;\u0026amp; sig_kernel_only(si5g)) 这个条件对于 SIGKILL 来说是 true，对于 SIGTERM 来说是 false。\nNamespace 概念不了解的话，可参考前期文章 容器原理\n上述逻辑可参考下图：\n示例一解释\n现在再来看第一个示例，示例中容器的 1 号进程是 sleep，同时发信号和接受信号都在容器内部，属于同一个 Namespace，同时由于 sleep 命令自身没有注册 handler，满足上述三个条件。导致 kill -9，kill -15 被 Linux 内核忽略。 # 示例二解释\n对于示例二，由于在宿主机操作容器内的进程，这样就是发出信号的进程和接受信号的进程不属于同一个 Namespace，force = true ，对于 SIGTERM 信号，它不是特权信号，所以 sig_kernel_only(sig)=false —\u0026gt; !(force \u0026amp;\u0026amp; sig_kernel_only(sig)) = true —\u0026gt; kill -15 忽略。 # 对于 SIGKILL 信号来说，它是特权信号，所以 sig_kernel_only(sig)=true —\u0026gt; !(force \u0026amp;\u0026amp; sig_kernel_only(sig)) = false —\u0026gt; kill -9执行动作。 # 示例三解释\n示例三其实和示例二的原理一样，因为 docker stop \u0026lt;containier-id\u0026gt; 背后其实先向容器进程发出 SIGTERM 信号，如果 10s 后进程还在，那么直接再发出 SIGKILL 信号。 # 示例四解释\n再看第四个示例，第四个示例容器的 1 号进程是 Golang 程序，然而 Golang 程序默认自带了 handler，kill -9 发出的是 SIGKILL 信号，不允许被捕获，满足以上三个条件，导致 kill -9 不生效。 # kill -15 发出的是 SIGTERM 信号，该信号允许捕获，所以使用自带的 handler，即不满足条件 2，导致 kill -15 可以杀死进程。 # Golang 语言 runtime 自动注册了 SIGTERM 信号 handler，https://pkg.go.dev/os/signal#section-directories\n通过上面的分析，我们了解了容器中的 1号进程以及解释了在容器中为什么杀不掉 1 号进程。 # 1 号进程与 init 进程 # 在 Linux 中 1号进程也叫 init 进程，但是在不同环境下 1号进程与 init 进程并不是等价的。 # 1 号进程与 init 进程是什么关系，又有什么区别？ # init 进程就是 1 号进程，但是 1 号进程不一定是 init 进程。 # 我们看看 1 号进程和 init 进程的特性： # 1 号进程 # • 操作系统第一个进程 # • 是所有用户态进程的父进程 # init 进程 # • 操作系统第一个进程 # • 是所有用户态进程的父进程 # • 可以回收僵尸进程(失去了父进程的子进程就都会以 init 作为它们的父进程) # • 可以向子进程发送操作系统信号 # 对于宿主机和容器的关系，在宿主机中 1 号进程和 init 进程是等价的，在容器中 1 号进程不一定是 init 进程，取决与用户对容器的定义，下面详细看看。 # 容器中的 init 进程 # 容器中的 1 号进程是否具有 init 进程的特性取决于容器启动的命令。在 Dockerfile 中，CMD、ENTRYPOINT 可以启动容器，它们都有两种模式：shell 模式，exec 模式 # 这里用 ENTRYPOINT 举例，CMD 是一样的。 # shell 模式 # 该模式下会开启一个 shell 来执行后面的命令，即使用 /bin/sh -c 启动业务进程，那么容器中的 1 号进程就是 shell。用法：ENTRYPOINT command # FROM ubuntu # go-demo 是 go 语言程序, 内部是 sleep 100s COPY go-demo . ENTRYPOINT ./go-demo 启动该容器后，容器的 1 号进程就是 sh ，go-demo 是 sh 的子进程。 # $ docker exec -it test-go-init ps -ef UID PID PPID C STIME TTY TIME CMD root 1 0 0 08:45 ? 00:00:00 /bin/sh -c ./go-demo root 6 1 0 08:45 ? 00:00:00 ./go-demo root 11 0 0 08:46 pts/0 00:00:00 ps -ef exec 模式 # 该模式下直接运行命令，容器中的 1 号进程就是业务应用进程。用法：ENTRYPOINT [\u0026quot;command\u0026quot;] # FROM ubuntu COPY go-demo . ENTRYPOINT [\u0026#34;./go-demo\u0026#34;] 启动该容器后，容器的 1 号进程就是 go-demo # $ docker exec -it test-go-init ps -ef UID PID PPID C STIME TTY TIME CMD root 1 0 0 08:49 ? 00:00:00 ./go-demo root 18 0 0 08:51 pts/0 00:00:00 ps -ef 使用 exec 命令 # 在 shell 模式下使用 exec 命令也可以达到 exec 模式的效果，用法：ENTRYPOINT exec command # FROM ubuntu COPY go-demo . ENTRYPOINT exec ./go-demo 同样容器启动后，容器的 1 号进程就是 go-demo # $ docker exec -it test-go-init ps -ef UID PID PPID C STIME TTY TIME CMD root 1 0 0 08:59 ? 00:00:00 ./go-demo root 10 0 0 08:59 pts/0 00:00:00 ps -ef 使用 tini # 上面说了如果容器需要实现优雅退出，要么也用进程作为 1号进程且实现 SIGTERM handler，要么启动 init 进程。下面看看如何在容器中启动 init 进程。 # 在容器中添加一个 init 系统。init 系统有很多种，推荐使用 tini[1]，使用如下： # • 安装 tini # • 将 tini 作为容器中的 1号进程 # • 用户进程启动命令作为 tini 参数\n可参考如下 Dockerfile：\nFROM ubuntu RUN apt-get install tini COPY test.sh . RUN chmod +x test.sh ENTRYPOINT [\u0026#34;/sbin/tini\u0026#34;, \u0026#34;--\u0026#34;, \u0026#34;./test.sh\u0026#34;] 现在 tini 就是 1 号进程，它会将收到的系统信号转发给子进程 test.sh 。使用 docker stop \u0026lt;container-id\u0026gt; 就可以瞬间杀死容器了，docker 发送 SIGTERM 信号给容器的 tini，tini 将信号转发给 test.sh，如果 test.sh 对 SIGTERM 信号有自定义 handler，那么执行完 handler 退出； # 如果 test.sh 没有对 SIGTERM 信号处理，那么执行默认行为，即直接退出。因为现在的 test.sh 已经不是 1号进程了，仅仅是一个普通进程，所以并不会上面说的示例二的现象。所以要实现应用进程真正的优雅退出，应用程序也得实现 SIGTERM 的 handler。 # tini 作为 init 进程，还可以清理容器中的僵尸进程。 # 如果你想直接通过 docker 命令来运行容器，可以直接通过参数 --init 来使用 tini，不需要在镜像中安装 tini。如果是 Kubernetes 就不行了，必须手动安装 tini。\n使用场景 # 那么为什么有 shell 模式和 exec 模式，分别在什么场景下使用？ # 优雅退出\nshell 模式的一号进程是 sh，而且 sh 不能传递信号，所以就无法实现容器内进程优雅退出了( docker stop \u0026lt;container-id\u0026gt; 只能等待 10s 强制杀死)，这时候就可以就考虑使用 exec 模式， # 因为 exec 模式的 1号进程就是自身，自身实现 SIGTERM handler 即可。 # 环境变量\n在 exec 模式下，没有办法获取容器内的环境变量，Dockerfile 如下： # FROM ubuntu # 自定义环境变量 ENV CUSTOMENAME test-exec # 输出自定义环境变量和系统自带环境变量的值 ENTRYPOINT [\u0026#34;echo\u0026#34;, \u0026#34;Welcome, $CUSTOMENAME $HOSTNAME\u0026#34;] 运行输出发现并没有输出对应值。 # $ docker run --rm test-exec:v1 Welcome, $CUSTOMENAME\u0026#34;, \u0026#34;$HOSTNAME 使用 shell 模式或者 exec 命令看看。 # FROM ubuntu # 自定义环境变量 ENV CUSTOMENAME test-exec # 输出自定义环境变量和系统自带环境变量的值 ENTRYPOINT echo $CUSTOMENAME 运行输出可以输出环境变量的值 # $ docker run --rm test-exec:v1 test-exec 所以 shell 模式和 exec 模式都有对应的使用限制和优势。 # 总结 # 本文主要讲述了两段内容： # • 通过 Linux 信号处理的机制分析了在容器中为什么杀不死 1号进程的原因； # • 介绍了 1号进程和 init 进程的关系和区别，同时展开说明容器中的 1 号进程和 init 进程的使用。 # 相信看完这篇文章，可以对容器中的进程有了一些原理性的理解。 # 上文 为什么杀不掉容器中 1号进程 的示例中的 Dockerfile 都是 exec 模式，如果换成 shell 模式，是否会出现不同情况，有兴趣可以实验验证下。 # 引用链接 # [1] tini: https://github.com/krallin/tini.git\n"},{"id":167,"href":"/docs/%E5%AE%B9%E5%99%A8%E5%8E%9F%E7%90%86-rong-qi-yuan-li/","title":"容器原理 2024-04-03 14:45:58.422","section":"Docs","content":" 简介 # 当在任何一个能够运行 CRI (Docker、Containered 等) 的机器上，使用 docker run \u0026lt;image-id\u0026gt; 启动一个容器后。从使用者的角度来看，容器和一台独立的机器或者虚拟机几乎没有什么区别。使得我们在容器里就像操作虚拟机一样将服务运行在容器中，这样你的机器上能够非常多的容器，且容器之间都有独立的运行资源，网络、文件系统等相互隔离。 # 但是容器和虚拟机相比，却没有各种复杂的硬件虚拟层，没有独立的 Linux 内核。容器所有的进程调度，内存访问，文件的读写都直接跑在宿主机的内核之上，这是怎么做到的呢？ # Linux 内核的 Namespace 和 Cgroups 功能可以让程序在一个资源可控的独立（隔离）环境中运行，这个就是容器了。 # 用户视角的容器 # 上面说到用户使用容器就跟使用虚拟机几乎5没什么区别，也就是说： # • 容器的文件系统是独立的，也就是容器之间文件系统是隔离的 # • 容器网络是独立的，默认容器网络有单独网络协议栈 # • 容器进程间的通信是隔离的 # • 容器的用户、用户组也是隔离的 # • 容器内进程 PID 独立 # • 容器的主机名独立 # • 资源( CPU、Memory 等 ) 隔离，容器之间资源隔离 # 对于一个操作系统，如果以上这些条件都实现，那么该操作系统在用户视角下就完完全全是一个全新、独立的。容器就是利用 Linux Namespace、Cgroup 技术来实现的。 # 当使用 docker run -d centos/httpd-24-centos7:latest 启动一个 httpd 服务的容器，使用 ls -l /proc/\u0026lt;pid\u0026gt;/ns/ 在宿主机查看该进程的 Namespace 信息。 # ls -l /proc/126013/ns/ lrwxrwxrwx. 1 1001 root 0 10月 30 11:33 ipc -\u0026gt; ipc:[4026535315] lrwxrwxrwx. 1 1001 root 0 10月 30 11:33 mnt -\u0026gt; mnt:[4026535313] lrwxrwxrwx. 1 1001 root 0 10月 30 11:33 net -\u0026gt; net:[4026535319] lrwxrwxrwx. 1 1001 root 0 10月 30 11:33 pid -\u0026gt; pid:[4026535316] lrwxrwxrwx. 1 1001 root 0 10月 30 15:27 user -\u0026gt; user:[4026531837] lrwxrwxrwx. 1 1001 root 0 10月 30 11:33 uts -\u0026gt; uts:[4026535314] 发现该进程对应的 ipc、mnt、net、pid、user、uts 都指向了单独的 namespace。 # Namespace # Linux 内核 Namespace 类型有很多，支持 **cgroup/ipc/network/mount/pid/time/user/uts，**下面看看是如何使用这些 Namespace 隔离技术运行一个容器。 # PID Namespace # 当使用 docker exec 624d08ddaf99 ps -ef 获取一个容器的所有进程，看到了该容器只运行了五个 httpd 进程。 # # docker exec c5a9ff78d9c1 ps -ef UID PID PPID C STIME TTY TIME CMD root 1 0 0 01:59 ? 00:00:00 /sbin/httpd -D FOREGROUND apache 6 1 0 01:59 ? 00:00:00 /sbin/httpd -D FOREGROUND apache 7 1 0 01:59 ? 00:00:00 /sbin/httpd -D FOREGROUND apache 8 1 0 01:59 ? 00:00:00 /sbin/httpd -D FOREGROUND apache 9 1 0 01:59 ? 00:00:00 /sbin/httpd -D FOREGROUND 直接获取该容器所在的宿主机上的 httpd 进程，同样可以看到这 5 个进程。 # # ps -ef | grep httpd UID PID PPID C STIME TTY TIME CMD 1001 126013 125976 0 11:33 ? 00:00:00 httpd -D FOREGROUND 1001 126100 126013 0 11:33 ? 00:00:01 httpd -D FOREGROUND 1001 126105 126013 0 11:33 ? 00:00:01 httpd -D FOREGROUND 1001 126119 126013 0 11:33 ? 00:00:01 httpd -D FOREGROUND 1001 126134 126013 0 11:33 ? 00:00:01 httpd -D FOREGROUND 1001 126140 126013 0 11:33 ? 00:00:01 httpd -D FOREGROUND 对比两组结果，发现两组数据的 PID 不一样。Linux 在创建容器的时候，就会建出一个 PID Namespace，该 PID Namespace 下所有所有进程编号都从1 开始。其实和 Linux 宿主机一样， # Linux 开机时就会启动一个 1 号进程，该进程是所有进程的父进程。同时在这个 PID Namesapce 中只能看到该 Namespace 下的进程，看不到其他 Namespace 下的进程。 # 那么这个 httpd 服务对应的宿主机 PID 和容器 PID 的关系是什么样的，这两个 PID 对应着都是同一个 httpd 服务的某个进程。 # 可以通过 docker inspect \u0026lt;container id\u0026gt; | grep Pid 查看这个容器对应宿主机上的 PID。会发现这个 \u0026quot;Pid\u0026quot;: 126013 就是在宿主机上使用 ps -ef |grep httpd 查到的 httpd 进程一样，只不过这个 httpd 服务有多个进程，126013 是所有子进程的父进程。 # # docker inspect 5d22ea980dc8|grep Pid \u0026#34;Pid\u0026#34;: 126013, # ps -ef | grep httpd UID PID PPID C STIME TTY TIME CMD 1001 126013 125976 0 11:33 ? 00:00:00 httpd -D FOREGROUND 这也就是说，如果有另外一个容器，那么它也有自己的一个 PID Namespace，而这两个 PID Namespace 之间是不能看到对方的进程的，这里就体现出了 Namespace 的作用：相互隔离。 # 当然这只是隔离了服务的进程，下面看看 Network Namespace。 # Network Namespace # 如果启动容器时不使用 —net=host ，即容器使用宿主机的网络，那么 Linux 就会给这个容器创建单独的 Network Namespace。在这个 Network Namespace 中都有一套独立的网络接口。 # 比如查看容器的网络接口，这里的 lo，eth0，还有独立的 TCP/IP 的协议栈配置。\n# docker exec 5d22ea980dc8 ip addr 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 168: eth0@if169: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0 valid_lft forever preferred_lft forever 至于这个容器的网络通信原理会在后面讲解。 # Mount Namespace # 同样的进入容器内会发现容器的文件系统也是独立的。容器的文件系统，其实就是我们做的镜像。 # 运行 docker exec 5d22ea980dc8 ls/ 查看容器中的根文件系统（rootfs）。这里依靠的是 Mount Namespace，Mount Namespace 保证了每个容器都有自己独立的文件目录结构。 # IPC Namespace # IPC Namespace 主要是用来隔离进程间通信的。例如 PID Namespace 和 IPC Namespace 一起使用可以实现同一 IPC Namespace 内的进程彼此可以通信，不同 IPC Namespace 的进程却不能通信。 # User Namespace # User Namespace 主要是用来隔离用户和用户组的。一个比较典型的应用场景就是在主机上以非 root 用户运行的进程可以在一个单独的 User Namespace 中映射成 root 用户。 # 使用User Namespace 可以实现进程在容器内拥有 root 权限，而在主机上却只是普通用户。 # UTS Namespace # UTS Namespace 用于隔离容器的主机名和域名，意思是每个容器内的主机名都可以独立。 # Time Namespace # Time Namespace 用于隔离容器的时间，但是容器并没有隔离时间，也就是说所有容器的时间是和宿主机一致的。 # Cgroups # 上面介绍了一系列 Linux 隔离技术，那么容器内的资源使用是如何限制的。 # Linux Cgroups( Control Groups) 技术可以对执行的进程做各种计算机资源的限制，比如限制 CPU 的使用率，内存使用量，IO流量等。 # Cgroups 通过对不同的子系统限制了不同的资源，每个子系统限制一种资源。每个子系统限制资源的方式都是类似的，就是把相关的一组进程分配到一个控制组里，然后通过树结构进行管理，每个控制组设有自己的控制参数。完整的 Cgroups 子系统的介绍，你可以查看 Linux Programmer\u0026rsquo;s Manual[1] 中 Cgroups 的定义。 # (Subsystem)子系统 # 可通过执行 ls /sys/fs/cgroup/ 查看 Linux 支持的 Cgroups 子系统： # # ls /sys/fs/cgroup/ blkio cpu cpuacct cpu,cpuacct cpuset devices freezer hugetlb memory net_cls net_cls,net_prio net_prio perf_event pids systemd • blkio 子系统，是 cgroup v1 中的一个子系统，使用 cgroup v1 blkio 子系统主要是为了减少进程之间共同读写同一块磁盘时相互干扰的问题。cgroup v1 blkio 控制子系统可以限制进程读 写的 IOPS 和吞吐量，但它只能对 Direct I/O 的文件读写进行限速，对 Buffered I/O 的文件读写无法限制。 # • cpu, cpuacct 子系统，用来限制一个控制组（一组进程，你可以理解为一个容器里所有的进程）可使用的最大 CPU。 # • cpuset 子系统， 这个子系统来限制一个控制组里的进程可以在哪几个物理 CPU 上运行。 # • devices 子系统，维护一个设备访问权限管理，控制进程对设备文件读写访问权限以及设备文件创建。 # • freezer 子系统，用于挂起和恢复 cgroup 中的进程。 # • hugetlb 子系统，用来限制控制组里进程使用的大页。 # • memory 子系统，用来限制一个控制组最大的内存使用量。 # • net_cls,net_prio 子系统，用来限制进程的网络资源 # • perf_event 子系统，Cgroups 中的进程监控 # • pids 子系统，用来限制一个控制组里最多可以运行多少个进程。 # • systemd 不属于 cgroups 子系统 # 每个版本 linux 内核的子系统不是都一样。 # cgroup 组 # 除了子系统之外，还需要了解 group 的概念，在 Cgroups 中，资源都是以组为单位控制的，每个组包含一个或多个的子系统。你可以按照任何自定义的标准进行组的划分。划分组之后，你可以将任意的进程加入、迁移到任意组中并且实时生效（但是对于进程的子进程不影响）。 # (hierarchy)层级树 # 一组以树状结构排列的 cgroup 就是 hierarchy(层级树)，结合虚拟文件系统来理解，通过创建、删除、重命名子目录来定义层次结构。子目录能够继承父目录的全部资源（当然了，不能超过），也可以基于父目录的资源限制进行进一步的资源大小限制。父目录如果调整了资源大小，子目录同样会马上受到影响。 # 每个层级树可以关联任意个数的 subsystem，但是每个 subsystem 最多只能被挂在一颗树上。 # 通过 cgroups 限制容器的 memory # 对于启动的每个容器，都会在 Cgroups 子系统下建立一个目录，在 Cgroups 中这个目录也被称作控制组。然后我们设置这个控制组的参数，通过这个方式，来限制这个容器的内存资源。 # 还记得，我们之前用 Docker 创建的那个容器吗？在每个 Cgroups 子系统下，对应这个容器就会有一个目录 docker-5d22ea980dc8…… 这个容器的 ID 号，这个 ID 号前面我们用 ps 看到的进程号。容器中所有的进程都会储存在这个控制组中 cgroup.procs 这个文件里。 # 把（2* 1024 * 1024 * 1024 = 2147483648）这个值，写入 memory Cgroup 控制组中的 memory.limit_in_bytes 里，这样设置后，cgroup.procs 里面所有进程 Memory 使用量之和，最大也不会超过 2GB。 # # cd /sys/fs/cgroup/memory/system.slice/docker-**5d22ea980dc8**fedd52511e18fdbd26357250719fa0d128349547a50fad7c5de9.scope # cat cgroup.procs 20731 20787 20788 20789 20791 # echo 2147483648 \u0026gt; memory.limit_in_bytes # cat memory.limit_in_bytes 2147483648 Docker 对 Cgroups 的使用 # 默认情况下，Docker 启动一个容器后，会在 /sys/fs/cgroup 目录下的各个资源目录下生成以容器 ID 为名字的目录（group），比如： # /sys/fs/cgroup/cpu/docker/03dd196f415276375f754d51ce29b418b170bd92d88c5e420d6901c32f93dc14 此时 cpu.cfs_quota_us 的内容为 -1，表示默认情况下并没有限制容器的 CPU 使用。在容器被 stopped 后，该目录被删除。 # docker 启动容器时可以指定参数对容器的 cgroup 进行设置，基本支持了上面说的 cgroup 所有子系统。 # # docker run --help block IO: --blkio-weight value Block IO (relative weight), between 10 and 1000 --blkio-weight-device value Block IO weight (relative device weight) (default []) --cgroup-parent string Optional parent cgroup for the container CPU: --cpu-percent int CPU percent (Windows only) --cpu-period int Limit CPU CFS (Completely Fair Scheduler) period --cpu-quota int Limit CPU CFS (Completely Fair Scheduler) quota -c, --cpu-shares int CPU shares (relative weight) --cpuset-cpus string CPUs in which to allow execution (0-3, 0,1) --cpuset-mems string MEMs in which to allow execution (0-3, 0,1) Device: --device value Add a host device to the container (default []) --device-read-bps value Limit read rate (bytes per second) from a device (default []) --device-read-iops value Limit read rate (IO per second) from a device (default []) --device-write-bps value Limit write rate (bytes per second) to a device (default []) --device-write-iops value Limit write rate (IO per second) to a device (default []) Memory: --kernel-memory string Kernel memory limit -m, --memory string Memory limit --memory-reservation string Memory soft limit --memory-swap string Swap limit equal to memory plus swap: \u0026#39;-1\u0026#39; to enable unlimited swap --memory-swappiness int Tune container memory swappiness (0 to 100) (default -1) 运行命令 docker run -d --memory 4MB httpd:latest 启动一个容器，且限制该容器内存最大使用为 4MB( Linux 限制最小为 4MB）。在宿主机查看该容器对应进程的 cgroup 。 # # cat /sys/fs/cgroup/memory/system.slice/docker-06bd180cd34ffedd52511e18fdbd26357250719fa0d128349547a50fad7c5de9.scope/memory.limit_in_bytes 4194304 总结 # 一句话概括，**容器 = Namespace + Cgroups，**要想学好容器技术，掌握容器原理，首先就得熟练掌握 Linux 的 Namespace、Cgroups 技术。 # 在容器层面还有”镜像技术”，镜像是容器的基础。进入容器看到的文件系统就是镜像，镜像也依赖于一系列的底层技术，比如文件系统(filesystems)、写时复制(copy-on-write)、联合挂载等。 # 引用链接 # [1] Linux Programmer\u0026rsquo;s Manual: https://man7.org/linux/man-pages/man7/cgroups.7.html\n"},{"id":168,"href":"/docs/%E5%AE%B9%E5%99%A8%E7%9A%84%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E4%B8%80-overlayfs-%E5%8E%9F%E7%90%86-rong-qi-de-wen-jian-xi-tong--yi-overlayfs-yuan-li/","title":"容器的文件系统 OverlayFS 原理 2024-04-03 14:45:26.893","section":"Docs","content":" 容器文件系统 # 之前文章 容器原理 说到容器的核心技术是由 Linux Namespace + Cgroups 实现的。其中在说到 Mount Namespace 中说到，使用 docker exec 进入某个容器后。可以看到一个崭新的文件系统，就类似于 ssh 到某个虚拟机或者物理机上，实际上就是利用 Linux Mount Namespace 隔离实现的。当创建一个容器时，Linux 系统上就会创建一个 对应的 Mount Namespace，那么这个 Mount Namespace 就是该容器的文件系统。 # 容器的文件系统在用户视角和宿主机的文件系统没有什么区别，但是其背后的原理却不同。在容器里运行 df 命令，可以看到在容器中根目录(/)的文件系统类型是 overlay，它不是我们在普通 Linux 节点上看到的 Ext4 或者 XFS 之类常见的文件系统。 # root@624d08ddaf99:~# df -h Filesystem Size Used Avail Use% Mounted on overlay 558G 70G 488G 13% / 为什么需要 OverlayFS # 那么这个 OverlayFS 是一个怎样的文件系统，容器为什么使用这种文件系统？ # 我们知道虚拟机需要 iso 镜像才能启动，那么每个容器也需要一个镜像才能启动。然后这个镜像会包含容器运行的二进制文件、库文件、配置文件，其他的依赖的文件等全部打包成一个 镜像文件。如果使用正常的 EXT4 或者 XFS 文件系统的话，那么每次启动一个容器，就需要把一个镜像文件下载并存储在宿主机上。 # 比如：假设一个镜像文件的大小是 500MB，那么 100 个容器的话，就需要下载 500MB*100= 50GB 的文件，并且占用 50GB 的磁盘空间。然而在这 50GB 的磁盘空间中，大部分数据都是重复的，因为大部分数据都是镜像的 base 镜像，而且这些 base 镜像基本都是只读的，在容器运行时不会变动。 # 如说这 100 个容器镜像都是基于 ubuntu:18.04 的，每个容器镜像只是额外复制了 50MB 左右自己的应用程序到 ubuntu: 18.04 里，那么就是说在总共 50GB 的数据里，有 90% 的数据是冗余的。 # 所以有没有一种文件系统可以达到用户的应用程序可以重复利用 ubuntu:18.04 ？也就是能够有效地减少磁盘上冗余的镜像数据，同时减少冗余的镜像数据在网络上的传输。这类的文件系统被称为 UnionFS。下图就是 UnionFS 解决问题的实现。 # UnionFS 这类文件系统实现的主要功能是把多个目录（处于不同的分区）一起挂载（mount）在一个目录下。这种多目录挂载的方式，正好可以解决我们刚才说的容器镜像的问题。 # 比如，我们可以把 ubuntu18.04 这个基础镜像的文件放在一个目录 ubuntu18.04/ 下，容器自己额外的程序文件 app_1_bin 放在 app_1/ 目录下。 # 然后，我们把这两个目录挂载到 container_1/ 这个目录下，作为容器 1 看到的文件系统；对于容器 2，就可以把 ubuntu18.04/ 和 app_2/ 两个目录一起挂载到 container_2 的目录下。这样在节点上我们只要保留一份 ubuntu18.04 的文件就可以了。可参考下图： # OverlayFS # UnionFS 类似的有很多种实现，Docker 支持 OverlayFS、Fuse-overlayfs、Devicemapper、Btrfs、ZFS、VFS、AUFS(已废弃) 。前面我们在运行 df 的时候，看到的文件系统类型 overlay 指的就是 OverlayFS。 # 在 Linux 内核 3.18 版本中，OverlayFS 代码正式合入 Linux 内核的主分支。在这之后，OverlayFS 也就逐渐成为各个主流 Linux 发行版本里缺省使用的容器文件系统了。 # OverlayFS 工作时由四个目录组成： # • lowerdir：只读层，该层无法修改，可以指定多个 lower # • upperdir：读写层，容器数据修改保存的地方 # • merged：最终呈现给用户的目录 # • workdir：工作目录，指 OverlayFS 工作时临时使用的目录，保证文件操作的原子性，挂载后会被清空 # 如何使用 # OverlayFS 实现的功能就是将 lower 层和 upper 层联合挂载到 merged 层，使得 merged 层拥有 lower 和 upper 层所有文件、目录。 # $ mkdir upper lower_1 lower_2 merged work $ echo \u0026#34;I\u0026#39;m from lower_1\u0026#34; \u0026gt; lower_1/in_lower_1.txt $ echo \u0026#34;I\u0026#39;m from lower_2\u0026#34; \u0026gt; lower_2/in_lower_2.txt $ echo \u0026#34;I\u0026#39;m from upper\u0026#34; \u0026gt; upper/in_upper.txt $ # `in_both` is in both directories $ echo \u0026#34;I\u0026#39;m from lower_1\u0026#34; \u0026gt; lower_1/in_both.txt $ echo \u0026#34;I\u0026#39;m from lower_2\u0026#34; \u0026gt; lower_2/in_both.txt $ echo \u0026#34;I\u0026#39;m from upper\u0026#34; \u0026gt; upper/in_both.txt 只需使用 mount 命令即可将 lower、upper 挂载到 merged，用法：mount -t overlay overlay -o lowerdir=\u0026lt;lowerdir1-dir\u0026gt;:\u0026lt;lowerdir2-dir\u0026gt;:\u0026lt;lowerdir3-dir\u0026gt;,upperdir=\u0026lt;upper-dir\u0026gt;,workdir=\u0026lt;work-dir\u0026gt; \u0026lt;merded-dir\u0026gt; # $ mount -t overlay overlay -o lowerdir=./lower_1:./lower_2,upperdir=./upper,workdir=./work ./merged 最终实现的效果就是可以在 ./merged 目录下看到 in_lower_1.txt、in_lower_2.txt、in_upper.txt、in_both.txt # $ find lower_1/ lower_2/ upper/ merged/ lower_1/ lower_1/in_lower_1.txt lower_1/in_both.txt lower_2/ lower_2/in_lower_2.txt lower_2/in_both.txt upper/ upper/in_upper.txt upper/in_both.txt merged/ merged/in_lower_2.txt merged/in_both.txt merged/in_upper.txt merged/in_lower_1.txt 具体挂载实现可参考下图： # 上面我们在 upper 目录、lower_1 目录、lower_2 目录都创建了 in_both.txt，同时写入不同的内容。 # 最终挂载时 upper 层的文件会覆盖 lower 层的文件，所以 merged/in_upper.txt 内容应该是 I'm from upper! # $ cat merged/in_both.txt \u0026#34;I\u0026#39;m from upper 当我们挂载完成 OverlayFS 以后，对文件系统的任何操作都只能在 merge dir 中进行，用户不允许再直接或间接的到底层文件系统的原始 lowerdir 或 upperdir 目录下修改文件或目录，否则可能会出现一些无法预料的后果（kernel crash除外）。 # 上面说到 OverlayFS 文件系统的 upper 层是保存更新的文件，就是说我们在 merged 层创建、修改、删除文件、目录时应该都发生在 upper 层。下面看看在 OverlayFS 删除、新建、更新文件是如何实现的？ # 删除文件 # 在 merged 目录删除文件或者目录时，存在三种情况： # 删除来源于 upper 层 的文件 # upper 层用于保存文件的更新，那么这个文件在 upper 层消失 # $ cd merged/ $ rm -rf in_upper.txt # in_upper.txt 已经删除 $ ls in_both.txt in_lower_1.txt in_lower_2.txt new_file # 发现 upper 层 in_upper.txt 也消失了 $ ls -l ../upper/ 总用量 8 -rw-r--r--. 1 root root 15 11月 6 20:16 in_both.txt -rw-r--r--. 1 root root 12 11月 6 20:07 new_file 删除来源于 lower 层的文件 # lower 层是只读层，不允许被修改，所以删除该层文件并不会在 lower 层消失，只不过会在 upper 层创建一个 Whiteout 文件，标识这个文件已经删除，在 merged 层起到删除的效果。 # Whiteout 文件在用户删除文件时创建，用于屏蔽底层的同名文件，同时该文件在 merge 层是不可见的，所以用户就看不到被删除的文件或目录了。whiteout 文件并非普通文件，而是主次设备号都为 0 的字符设备( 可以通过 mknod \u0026lt;name\u0026gt; c 0 0 命令手动创建 )，当用户在 merge 层通过 ls 命令( 将通过 readddir 系统调用）检查父目录的目录项时，OverlayFS 会自动过过滤掉和 whiteout 文件自身以及和它同名的 lower 层文件和目录，达到了隐藏文件的目的，让用户以为文件已经被删除了。 # 要删除的文件是 upper 层覆盖 lower层的文件 # 这种情况 OverlayFS 即需要删除 upper 层对应文件系统中的文件或目录，也需要在对应位置创建同名 whiteout文件，让upper层的文件被删除后不至于lower层的文件被暴露出来。 # 这里我们删除 in_both.txt，该文件是 upper 层覆盖两个 lower 层的文件，删除之后，发现两个 lower 层依然存在该文件，但是 upper 层的 in_both.txt 已经变成 whiteout 文件了。 # 新建文件 # 在 OverlayFS 新建文件同样存在两种情况： # 新建的文件在 lower 层中和 upper 层中都不存在对应的文件或目录 # 这种情况会直接在 upper 层中对应的目录下新创建文件或目录 # $ echo \u0026#39;new file\u0026#39; \u0026gt; merged/new_file $ ls -l */new_file -rw-r--r--. 1 root root 9 11月 6 20:05 merged/new_file -rw-r--r--. 1 root root 9 11月 6 20:05 upper/new_file 创建一个在 lower 层已经存在且在 upper 层有 whiteout 文件的同名文件 # 显然该文件已经在 merged 层被删除了，所以用户在 merge 层看不到它们，可以新建一个同名的文件。这种场景下，OverlayFS 需要删除 upper 层中的用新建的文件替换原有的 whiteout文件，这样在 merge 层中看到的文件就是来自 upper 层的新文件了。 # 这里创建 in_lower_1.txt(该文件来源于 lower 层且之前已经被删除)，发现可以创建成功，且在 upper 层用新建的文件替换了原有的 whiteout 文件。 # 修改文件 # 修改文件同样地会发生在 upper 层，同样存在两情况： # 修改来源与 upper 层的文件 # 这里修改 merged/in_upper.txt(来源于 upper 层)，那么在 upper 层该文件也被修改。 # $ sed -i \u0026#39;s/new file/update file/g\u0026#39; merged/new_file $ grep -R \u0026#39;update file\u0026#39; * merged/new_file:update file upper/new_file:update file 写时复制 # 用户在写文件时，如果文件来自 upper 层，那直接写入即可。但是如果文件来自 lower 层，由于 lower 层文件无法修改，因此需要先复制到 upper 层，然后再往其中写入内容，这就是OverlayFS 的写时复制(copy-up) 特性。 # 修改来源于 lower 层的文件 # 修改 merged/in_lower_2.txt(来源于 lower 层)，lower/in_lower_2.txt 不会被修改，同时 upper 层会出现 in_lower_2.txt # 这里就是触发了 OverlayFS 的**写时复制(copy-up)**机制。 # $ cd merged/ # 更新 in_lower_2.txt $ echo \u0026#39;update lower_2\u0026#39; \u0026gt;\u0026gt; in_lower_2.txt $ cat in_lower_2.txt I\u0026#39;m from lower_2 update lower_2 $ ls -l ../upper/ 总用量 12 -rw-r--r--. 1 root root 15 11月 6 20:16 in_both.txt c---------. 1 root root 0, 0 11月 6 20:21 in_lower_1.txt -rw-r--r--. 1 root root 32 11月 6 20:53 in_lower_2.txt -rw-r--r--. 1 root root 12 11月 6 20:07 new_file 通过 merged 目录向 in_lower_2.txt 写入新的内容，观察 merged 目录下 in_lower_2.txt 文件的内容，包含原来有的和新写入的，同时观察 upper 目录中，也同样存在一个新的从 lower 目录复制上来的文件 in_lower_2.txt，内容同merge目录中看到的一致。 # 当然，OverlayFS 的 copy-up 特性并不仅仅在往一个来自 lower 层的文件写入新内容时触发，还有很多的场景会触发，但是基本上都是对 lower 层进行修改会触发，比如： # 1）用户以写方式打开来自 lower 层的文件时，对该文件执行 copy-up，即 open()系统调用时带有 O_WRITE 或 O_RDWR 等标识； # 2）修改来自 lower 层文件或目录属性或者扩展属性时，对该文件或目录触发 copy-up，例如 chmod、chown 或设置 acl 属性等； # 3）rename 来自 lower 层文件时，对该文件执行 copy-up； # 4）对来自 lower 层的文件创建硬链接时，对链接原文件执行 copy-up； # 5）在来自 lower 层的目录里创建文件、目录、链接等内容时，对其父目录执行 copy-up； # 6）对来自 lower 层某个文件或目录进行删除、rename、或其它会触发 copy-up 的动作时，其对应的父目录会至下而上递归执行 copy-up。 # 通过这个例子，我们知道了 OverlayFS 是怎么工作了。那么我们可以再想一想，怎么把它运用到容器的镜像文件上？ # 从系统的 mounts 信息中，我们可以看到 Docker 是怎么用 OverlayFS 来挂载镜像文件的。容器镜像文件可以分成多个层（layer），每层可以对应 OverlayFS 里 lowerdir 的一个目录，lowerdir 支持多个目录，也就可以支持多层的镜像文件。在容器启动后，对镜像文件中修改就会被保存在 upperdir 里了。 # $ mount | grep overlay overlay on /var/lib/docker/overlay2/ee3585aa8e2cd992af3a66db41f54c4d046b5f3758fb87d7f96c140e90e0dfe6/merged type overlay (rw,relatime,seclabel,lowerdir=/var/lib/docker/overlay2/l/URC3EYQVHZ23W6KLE3MASVGHSG:/var/lib/docker/overlay2/l/NFMDRCK2PPWPFDSVGPU2YYLJUF, upperdir=/var/lib/docker/overlay2/ee3585aa8e2cd992af3a66db41f54c4d046b5f3758fb87d7f96c140e90e0dfe6/diff, workdir=/var/lib/docker/overlay2/ee3585aa8e2cd992af3a66db41f54c4d046b5f3758fb87d7f96c140e90e0dfe6/work) 总结 # 本文主要解答了容器为什么需要 OverlayFS 的问题，同时详细讲解 OverlayFS 的使用、原理。\n"},{"id":169,"href":"/docs/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E5%8E%9F%E7%90%86-rong-qi-wang-luo-yuan-li/","title":"容器网络原理 2024-04-03 14:51:51.962","section":"Docs","content":" 容器网络 # 前文容器原理一文说到容器在网络上，容器使用 Network Namespace 实现对网络资源的隔离，被隔离的进程只能看到当前 Namespace 里的网络栈。 # 网络栈包括了：网卡（Network Interface）、回环设备（Loopback Device）、路由表（Routing Table）和 iptables 规则。Linux 宿主机的网络通过网络栈来实现，同样容器的网络也是通过网络栈实现。 # 容器网络需要解决哪些问题： # • 容器 IP 的分配； # • 容器之间的互相访问(这里只考虑单节点容器间访问，跨节点可考虑 Kubernetes)； # • 容器如何访问主机外部网络； # • 外部网络如何访问到容器内部。 # 下面带着以上四个问题来看看容器网络的实现原理。 # Linux 虚拟网络技术 # 一般网络设备包括，交换机，路由器，网桥等，这些网络设备会存在多个网卡或者端口，那么 Linux 不仅仅可以作为网络设备，同时还可以实现虚拟网络设备，例如：网桥，虚拟网卡对等，那么 Network Namespace 就是利用 Linux 虚拟网络技术、路由、iptables 等技术来实现的。下面看看常用的 veth pair 和 bridge。 # Linux veth pair # veth pair 是成对出现的一种虚拟网络设备接口，一端连着网络协议栈，一端彼此相连。veth pair 总是成对出现的，从一端进入的数据包将会在另一端出现。我们可以把 veth pair 看成一条网线两端连接的两张以太网卡。只要将 veth pair 每一段分别接入不同的 Namespace，那么这两个 Namespace 就可以实现互通了。 # Linux 即使在同一个主机上创建的两个 Network Namespace，相互之间缺省也是不能进行网络通信的。\n下面通过示例将两个 Namespace 通过 veth pair 连接起来，并验证连通性。\n创建两个 namespace，ns1、ns2\n$ ip netns add ns1 $ ip netns add ns2 创建一个 veth pair\n$ ip link add veth-ns1 type veth peer name veth-ns2 将 veth pair 一端接入放入 ns1，另一端接入 ns2，这样就相当于采用网线将两个 Network Namespace 连接起来了。\n$ ip link set veth-ns1 netns ns1 $ ip link set veth-ns2 netns ns2 为两个网卡分别设置 IP 地址，这两个网卡的地址位于同一个子网 192.168.1.0/24 中。\n$ ip -n ns1 addr add 192.168.1.1/24 dev veth-ns1 $ ip -n ns2 addr add 192.168.1.2/24 dev veth-ns2 使用 ip link 命令设置两张虚拟网卡状态为 up。\n$ ip -n ns1 link set veth-ns1 up $ ip -n ns2 link set veth-ns2 up 从 ns1 ping ns2 的 IP，测试连通性\n$ ip netns exec ns1 ping 192.168.1.2 PING 192.168.1.2 (192.168.1.2) 56(84) bytes of data. 64 bytes from 192.168.1.2: icmp_seq=1 ttl=64 time=0.142 ms 64 bytes from 192.168.1.2: icmp_seq=2 ttl=64 time=0.021 ms 上面创建了一对 veth pair 连接着两个 namespace，可以分别进入 namespace 查看到对应网卡信息\n$ ip netns exec ns1 ifconfig veth-ns1: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet 192.168.1.1 netmask 255.255.255.0 broadcast 0.0.0.0 inet6 fe80::e415:f8ff:fe53:bbb3 prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; ether e6:15:f8:53:bb:b3 txqueuelen 1000 (Ethernet) RX packets 45 bytes 3693 (3.6 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 54 bytes 4642 (4.5 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 $ ip netns exec ns2 ifconfig veth-ns2: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet 192.168.1.2 netmask 255.255.255.0 broadcast 0.0.0.0 inet6 fe80::e415:f8ff:fe53:bbb3 prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; ether e6:15:f8:53:bb:b3 txqueuelen 1000 (Ethernet) RX packets 45 bytes 3693 (3.6 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 54 bytes 4642 (4.5 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 Bridge # veth pair 实现了两个网络之间的连通，如果我们需要将 3 个或者多个 namespace 接入同一个二层网络时，就不能只使用 veth pair 了。在物理网络中，如果需要连接多个主机，我们会使用网桥，或者又称为交换机。Linux 也提供了网桥的虚拟实现。 # 那么下面通过示例演示通过 Bridge 来连通三个 namespace。 # 首先创建三个 namespace # $ ip netns add ns1 $ ip netns add ns2 $ ip netns add ns3 创建 Linux Bridge # $ brctl addbr virtual-bridge 这里需要创建三对 veth pair，因为每一对的 veth pair 需要与 bridge 连接。将每对 veth pair 的一端接入对应 namespace，另一端接入 bridge # # 创建 veth pair $ ip link add veth-ns1 type veth peer name veth-ns1-br # 将 veth pair 一端接入 ns1 $ ip link set veth-ns1 netns ns1 # 将 veth pair 另一端接入 bridge $ brctl addif virtual-bridge veth-ns1-br $ ip link add veth-ns2 type veth peer name veth-ns2-br $ ip link set veth-ns2 netns ns2 $ brctl addif virtual-bridge veth-ns2-br $ ip link add veth-ns3 type veth peer name veth-ns3-br $ ip link set veth-ns3 netns ns3 $ brctl addif virtual-bridge veth-ns3-br 为三个 namespace 中的虚拟网卡设置 IP 地址，这些 IP 地址位于同一个子网 192 168.1.0/24 中。\n$ ip -n ns1 addr add local 192.168.1.1/24 dev veth-ns1 $ ip -n ns2 addr add local 192.168.1.2/24 dev veth-ns2 $ ip -n ns3 addr add local 192.168.1.3/24 dev veth-ns3 将 bridge 和 veth pair 启动\n$ ip link set virtual-bridge up $ ip link set veth-ns1-br up $ ip link set veth-ns2-br up $ ip link set veth-ns3-br up $ ip -n ns1 link set veth-ns1 up $ ip -n ns2 link set veth-ns2 up $ ip -n ns3 link set veth-ns3 up 测试三个 namespace 之间的连通性\n$ ip netns exec ns1 ping 192.168.1.2 PING 192.168.1.2 (192.168.1.2) 56(84) bytes of data. 64 bytes from 192.168.1.2: icmp_seq=1 ttl=64 time=0.165 ms 64 bytes from 192.168.1.2: icmp_seq=2 ttl=64 time=0.131 ms $ ip netns exec ns1 ping 192.168.1.3 PING 192.168.1.3 (192.168.1.3) 56(84) bytes of data. 64 bytes from 192.168.1.3: icmp_seq=1 ttl=64 time=0.345 ms 64 bytes from 192.168.1.3: icmp_seq=2 ttl=64 time=0.163 ms 这里的 bridge 只扮演了二层设备的角色，就可以实现同一子网下的两个 namespace 的通信。\n如果 namespace 里需要访问宿主机或者访问外网，那么 bridge 作为二层设备就无法实现了，因为数据包只能到达 bridge。\n子 namespace 访问 root namespace # 要想子 namespace 访问 root namespace，单纯的二层通信无法实现。\nLinux Bridge 即可以扮演二层交换机，也可作为三层交换机或者路由器使用，我们只需将 bridge 设置 IP，并作为子 namespace 的默认网关，这样数据包就可以通过 bridge 来到 root namespace。\n如果 Linux 需要扮演三层设备，必须开启 IP 转发\nsysctl -w net.ipv4.ip_forward=1` 或者 `echo 1 \u0026gt; /proc/sys/net/ipv4/ip_forward 这种设置只是暂时的，它的效果会随着计算机的重启而失效。\n首先创建子 namespace 和 bridge\n$ ip netns add ns1 $ ip netns add ns2 $ brctl addbr br0 通过 veth pair 将 ns1 和 ns2 连接到 bridge 上。\n$ ip link add veth-ns1 type veth peer name veth-ns1-br $ ip link set veth-ns1 netns ns1 $ brctl addif br0 veth-ns1-br $ ip link add veth-ns2 type veth peer name veth-ns2-br $ ip link set veth-ns2 netns ns2 $ brctl addif br0 veth-ns2-br 为 ns1，ns2 设置 IP 地址。\n$ ip -n ns1 addr add local 192.168.1.2/24 dev veth-ns1 $ ip -n ns2 addr add local 192.168.1.3/24 dev veth-ns2 启动 bridge 和 veth pair\n$ ip link set br0 up $ ip link set veth-ns1-br up $ ip link set veth-ns2-br up $ ip -n ns1 link set veth-ns1 up $ ip -n ns2 link set veth-ns2 up 从 ns1 ping root namespace，发现网络不可达。\n# 172.30.95.74 是宿主机网卡 $ ip netns exec ns1 ping 172.30.95.74 connect: Network is unreachable 需要先给 bridge 设置 IP，这里需要将 bridge 设置为 ns1、ns2 的默认网关\n$ ip addr add local 192.168.1.1/24 dev br0 $ ip link set br0 up $ ip netns exec ns1 ip route add default via 192.168.1.1 $ ip netns exec ns2 ip route add default via 192.168.1.1 可以查看到宿主机上 bro 网卡上配置了 IP\n$ ifconfig br0 br0: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet 192.168.1.1 netmask 255.255.255.0 broadcast 0.0.0.0 inet6 fe80::d6:2dff:fec5:a767 prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; ether 02:d6:2d:c5:a7:67 txqueuelen 1000 (Ethernet) RX packets 15 bytes 1032 (1.0 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 8 bytes 656 (656.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 这次从 ns1 、 ns2 ping root namespace，发现网络可达。\n$ ip netns exec ns1 ping 172.30.95.74 PING 172.30.95.74 (172.30.95.74) 56(84) bytes of data. 64 bytes from 172.30.95.74: icmp_seq=1 ttl=64 time=0.097 ms 64 bytes from 172.30.95.74: icmp_seq=2 ttl=64 time=0.061 ms $ ip netns exec ns2 ping 172.30.95.74 PING 172.30.95.74 (172.30.95.74) 56(84) bytes of data. 64 bytes from 172.30.95.74: icmp_seq=1 ttl=64 time=0.124 ms 64 bytes from 172.30.95.74: icmp_seq=2 ttl=64 time=0.062 ms 子 namespace 访问外网 # 如果想要子 namespace 能够访问外网，那么还需要将设置一条 iptables 规则。\n因为不设置规则的话，从子 namespace 发送数据包可以出去，但是回包会有问题，因为回包的时候，目的 IP 是子 namespace IP，但是每个节点的路有点并没有到子 namespace IP 的路由，所以就需要在 namespace 里设置一条 SNAT Iptables 规则，发送数据包到 bridge 时，将源地址改为宿主机 IP，这样回包时就无需知道 namespace 下的 IP 了。\n$ iptables -t nat -A POSTROUTING -s 192.168.1.0/24 ! -o br0 -j MASQUERADE 这样子 namespace 下就可以访问外网了\n$ ip netns exec ns1 ping www.baidu.com PING www.a.shifen.com (153.3.238.110) 56(84) bytes of data. 64 bytes from 153.3.238.110 (153.3.238.110): icmp_seq=1 ttl=49 time=9.53 ms 64 bytes from 153.3.238.110 (153.3.238.110): icmp_seq=2 ttl=49 time=9.52 ms 64 bytes from 153.3.238.110 (153.3.238.110): icmp_seq=3 ttl=49 time=9.04 ms 下图展示 IP 数据包经过 SNAT 规则后，报文的源 IP 发生了改变\n端口映射 # 如果想要将子 namespace 里的端口发布出去给外部访问，就需要使用 Linux DNAT 技术。\n比如在 ns1 启动一个 8001 端口的服务，想要暴露给外部访问\n$ ip netns exec ns1 python3 -m http.server --bind 192.168.1.2 8001 通过 iptables 设置 DNAT 规则发布端口，DNAT 规则作用就是将 IP 包的目的地址和端口进行修改再转发。\n# 为来自外部的流量做 DNAT $ iptables -t nat -A PREROUTING -d 172.30.95.72 -p tcp -m tcp --dport 8001 -j DNAT --to-destination 192.168.1.2:8001 # 为来自 host 自己的流量做 DNAT（因为本地流量不会经过 PREROUTING chain） $ iptables -t nat -A OUTPUT -d 172.30.95.72 -p tcp -m tcp --dport 8000 -j DNAT --to-destination 192.168.1.2:8001 可以查看到对应 iptables 规则\n$ iptables -t nat -nL 启用 br_netfilter 模块\n$ modprobe br_netfilter 现在从外部或者本机都可用访问\n$ curl 172.30.95.72:8001 \u0026lt;!DOCTYPE HTML PUBLIC \u0026#34;-//W3C//DTD HTML 4.01//EN\u0026#34; \u0026#34;http://www.w3.org/TR/html4/strict.dtd\u0026#34;\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta http-equiv=\u0026#34;Content-Type\u0026#34; content=\u0026#34;text/html; charset=ascii\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Directory listing for /\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 下图展示 IP 数据包经过 DNAT 规则后，报文的目的 IP 和目的端口都进行了变化\n在平时使用容器网络时，无非就是以上几种网络场景：宿主机上容器间访问，容器内访问外网，外部访问容器内的服务。 # 下面就看看容器网络是如何实现的。 # 容器网络模式 # Docker 可以为容器提供四种网络模式： # • **Host：**该网络模式使得容器与宿主机属于同一个网络 namespace，这样容器网络和宿主机一样，使用 --net=host 指定 # • **Bridge：**该网络模式是 Docker 默认的网络模式，类似于上文说的例子，使用 --net=bridge 指定 # • **Container：**复用其他容器的网络模式，使用 --net=container:NAME_or_ID 指定 # • **None：**表示容器没有网络，即容器没有 IP、路由等网络协议栈，使用 --net=none 指定 # Host # 如果启动容器的时候使用 host 模式，那么这个容器将不会获得一个独立的 Network Namespace，而是和宿主机共用一个 Network Namespace。容器将不会虚拟出自己的网卡，配置自己的 IP 等，而是使用宿主机的 IP 和端口。 # 这里启动一个 host 模式的容器，会发现容器的网络协议栈与宿主机一样。 # $ docker run -it --net host busybox:latest ifconfig None # 使用 none 模式，Docker 容器拥有自己的 Network Namespace，但是，并不为 Docker 容器进行任何网络配置。也就是说，这个 Docker 容器没有网卡、IP、路由等信息。需要我们自己为 Docker 容器添加网卡、配置 IP 等。 # $ docker run -it --net none artifacts.iflytek.com/docker-private/cloudnative/busybox:1.35 ifconfig lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B) Bridge # 如果不指定网络模式的话，该模式是 Docker 创建容器时默认网络模式。原理图如下： # 当 Docker 进程启动时，会在主机上创建一个名为 docker0 的虚拟网桥，并且分配一个 IP，该 IP 就是后面容器的默认网关。 # $ ifconfig docker0 docker0: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet 172.17.0.1 netmask 255.255.0.0 broadcast 172.17.255.255 inet6 fe80::42:a8ff:fe2a:210 prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; ether 02:42:a8:2a:02:10 txqueuelen 0 (Ethernet) RX packets 10274607 bytes 1726083694 (1.6 GiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 8862553 bytes 5870677425 (5.4 GiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0面创建一个 bridge 模式的容器， 创建 Bridge 网络模式的容器，可以发现容器属于单独 Network Namespace，且 Docker 创建一对 veth pair(eth0@if38：vethf240dfc@if37) ，一端接入容器内，一端接入 Docker0 网桥上。\n$ docker run -it --net bridge busybox:latest sh / # ip addr 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 37: eth0@if38: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN\u0026gt; mtu 1500 qdisc noqueue link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff inet 172.17.0.2/16 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe11:2/64 scope link valid_lft forever preferred_lft forever / # ip link show eth0 35: eth0@if36: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN\u0026gt; mtu 1500 qdisc noqueue link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff # 宿主机上查看 $ ip addr | grep 38 38: vethf240dfc@if37: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue master docker0 state UP group default 同时查看容器内的路由表，发现容器的默认网关指向 Docker0\n/ # route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 172.17.0.1 0.0.0.0 UG 0 0 0 eth0 172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 eth0 再创建一个容器，然后访问另一个容器，发现可以互通\n$ docker run -it --net bridge busybox:latest sh / # ifconfig eth0 Link encap:Ethernet HWaddr 02:42:AC:11:00:03 inet addr:172.17.0.3 Bcast:0.0.0.0 Mask:255.255.0.0 inet6 addr: fe80::42:acff:fe11:3/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:16 errors:0 dropped:0 overruns:0 frame:0 TX packets:16 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:1216 (1.1 KiB) TX bytes:1216 (1.1 KiB) lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B) / # ping 172.17.0.2 PING 172.17.0.2 (172.17.0.2): 56 data bytes 64 bytes from 172.17.0.2: seq=0 ttl=64 time=0.108 ms 64 bytes from 172.17.0.2: seq=1 ttl=64 time=0.119 ms 虽然两个容器属于单独 Network Namespace，但是都通过 veth pair 接入了 docker0 网桥，且在同一个子网下面，所以可以二层互通。\n在宿主机上查看 docker0 网桥上挂的网卡，发现两张网卡就是两个容器的一端。\n$ brctl show bridge name bridge id STP enabled interfaces docker0 8000.02428f5df66f no veth2a72e0f vethf240dfc $ ifconfig | grep veth veth2a72e0f: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 vethf240dfc: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 现在在容器内访问外网，同样也是正常的，因为 Docker 在宿主机上创建了对应的 SNAT 规则。\n/ # ping www.baidu.com PING www.baidu.com (153.3.238.102): 56 data bytes 64 bytes from 153.3.238.102: seq=0 ttl=49 time=8.843 ms 64 bytes from 153.3.238.102: seq=1 ttl=49 time=8.422 ms # 宿主上查看 iptables SNAT 规则 $ iptables -t nat -nL POSTROUTING Chain POSTROUTING (policy ACCEPT) target prot opt source destination MASQUERADE all -- 172.17.0.0/16 0.0.0.0/0 现在创建一个暴露端口的容器供外部访问，将容器的 80 端口映射到宿主机的 8080 端口。\n$ docker run -d --net bridge -p 8080:80 nginx:latest $ curl http://172.30.95.74:8080 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026#34;http://nginx.org/\u0026#34;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026#34;http://nginx.com/\u0026#34;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 测试发现访问正常，因为 Docker 也会在宿主机上创建对应的 DNAT 规则。 # 这里的 DNAT 规则发现与上面举例的不大一样，这里解释一下 # 上面说到 namespace 发布端口需要在 PREROUTING、OUTPUT 这两条链创建 DNAT，这里同样也是在这两条链创建了 DNAT 规则，只不过跳到了 DOCKER 自定义链了，然后这个 Chain DOCKER 设置了具体的 DNAT 规则。 # 经过以上说明，发现 Bridge 模式使用的技术其实就是上面说到 Linux 虚拟网络技术。 # Container # 这个模式指定新创建的容器和已经存在的一个容器共享一个 Network Namespace，而不是和宿主机共享。新创建的容器不会创建自己的网卡，配置自己的 IP，而是和一个指定的容器共享 IP、端口范围等。原理图如下： # 这里先创建一个容器，指定 bridge 网络模式。 # docker run -it --net bridge busybox:latest sh / # ifconfig eth0 Link encap:Ethernet HWaddr 02:42:AC:11:00:03 inet addr:172.17.0.3 Bcast:0.0.0.0 Mask:255.255.0.0 inet6 addr: fe80::42:acff:fe11:3/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:6 errors:0 dropped:0 overruns:0 frame:0 TX packets:5 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:516 (516.0 B) TX bytes:426 (426.0 B) lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B) 然后创建另外一个容器，并且设置与上面容器网络模式共享，发现该容器的网络协议栈与上一个容器的一致。 # docker run -it --net container:8d5aedb8ed81 busybox:latest sh / # ifconfig eth0 Link encap:Ethernet HWaddr 02:42:AC:11:00:03 inet addr:172.17.0.3 Bcast:0.0.0.0 Mask:255.255.0.0 inet6 addr: fe80::42:acff:fe11:3/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:8 errors:0 dropped:0 overruns:0 frame:0 TX packets:8 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:656 (656.0 B) TX bytes:656 (656.0 B) lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B) 总结 # 本篇文章首先讲解了 Linux 虚拟网络相关技术： # • **veth pair：**虚拟网卡对 # • **bridge：**虚拟网桥，Linux 虚拟网桥不仅可以作为二层设备也可以作为三层设备 # 进而通过实例讲解实现了在 Network Namespace 不同的网络场景： # • Network Namespace 间的通信 # • 子 Network Namespace 与 root Namespace 间通信 # • 子 Network Namespace 访问外网 # • Network Namespace 发布服务端口供外部访问 # 了解了 Linux 虚拟网络技术，然后通过对容器网络的介绍，基本可以了解文章开头提出 “容器网络需要解决什么问题”。 # "},{"id":170,"href":"/docs/%E5%B8%B8%E8%A7%81linux%E8%BF%90%E7%BB%B4%E9%9D%A2%E8%AF%95%E9%A2%98-chang-jian-linux-yun-wei-mian-shi-ti/","title":"常见linux运维面试题 2024-08-02 17:42:33.873","section":"Docs","content":"下面是一名运维人员求职数十家公司总结的 Linux运维面试题，给大家参考下~\n1、现在给你三百台服务器，你怎么对他们进行管理？\n管理3百台服务器的方式： 1）设定跳板机，使用统一账号登录，便于安全与登录的考量。 2）使用 salt、ansiable、puppet 进行系统的统一调度与配置的统一管理。 3）建立简单的服务器的系统、配置、应用的 cmdb 信息管理。便于查阅每台服务器上的各种信息记录。\n2、简述 raid0 raid1 raid5 三种工作模式的工作原理及特点\nRAID，可以把硬盘整合成一个大磁盘，还可以在大磁盘上再分区，放数据 还有一个大功能，多块盘放在一起可以有冗余（备份） RAID整合方式有很多，常用的：0 1 5 10\nRAID 0，可以是一块盘和 N 个盘组合\n优点读写快，是 RAID 中最好的 缺点：没有冗余，一块坏了数据就全没有了 RAID 1，只能2块盘，盘的大小可以不一样，以小的为准。\n10G+10G只有10G，另一个做备份。它有100%的冗余， 缺点：浪费资源，成本高 RAID 5 ，3块盘，容量计算10 *（n-1），损失一块盘\n特点，读写性能一般，读还好一点，写不好\n冗余从好到坏：RAID1 RAID10 RAID 5 RAID0 性能从好到坏：RAID0 RAID10 RAID5 RAID1 成本从低到高：RAID0 RAID5 RAID1 RAID10 单台服务器：很重要盘不多，系统盘，RAID1 数据库服务器：主库：RAID10 从库 RAID5\\RAID0（为了维护成本，RAID10） WEB服务器，如果没有太多的数据的话，RAID5,RAID0（单盘） 有多台，监控、应用服务器，RAID0 RAID5\n我们会根据数据的存储和访问的需求，去匹配对应的RAID级别\n3、LVS、Nginx、HAproxy 有什么区别？工作中你怎么选择？\nLVS：是基于四层的转发 HAproxy：是基于四层和七层的转发，是专业的代理服务器 Nginx：是WEB服务器，缓存服务器，又是反向代理服务器，可以做七层的转发 区别：LVS由于是基于四层的转发所以只能做端口的转发，而基于URL的、基于目录的这种转发LVS就做不了。\n工作选择：\nHAproxy 和 Nginx 由于可以做七层的转发，所以 URL 和目录的转发都可以做 在很大并发量的时候我们就要选择LVS，像中小型公司的话并发量没那么大 选择 HAproxy 或者 Nginx 足已，由于 HAproxy 由是专业的代理服务器 配置简单，所以中小型企业推荐使用 HAproxy 4、Squid、Varinsh 和 Nginx 有什么区别，工作中你怎么选择？\nSquid、Varinsh 和 Nginx 都是代理服务器\n什么是代理服务器：\n能当替用户去访问公网，并且能把访问到的数据缓存到服务器本地，等用户下次再访问相同的资源的时候，代理服务器直接从本地回应给用户，当本地没有的时候，我代替你去访问公网，我接收你的请求，我先在我自已的本地缓存找，如果我本地缓存有，我直接从我本地的缓存里回复你；如果我在我本地没有找到你要访问的缓存的数据，那么代理服务器就会代替你去访问公网。\n区别：\n1）Nginx 本来是反向代理/web服务器，用了插件可以做做这个副业，但是本身不支持特性挺多，只能缓存静态文件 2）从这些功能上。varnish 和 squid 是专业的 cache 服务，而 nginx 这些是第三方模块完成 3）varnish 本身的技术上优势要高于 squid，它采用了可视化页面缓存技术\n在内存的利用上，Varnish 比 Squid 具有优势，性能要比 Squid 高。还有强大的通过 Varnish 管理端口，可以使用正则表达式快速、批量地清除部分缓存。它是内存缓存，速度一流，但是内存缓存也限制了其容量，缓存页面和图片一般是挺好的\n4）squid 的优势在于完整的庞大的 cache 技术资料，和很多的应用生产环境\n工作中选择：\n要做 cache 服务的话，我们肯定是要选择专业的cache服务，优先选择squid或者varnish。\n5、Tomcat和Resin有什么区别，工作中你怎么选择？\n区别：Tomcat 用户数多，可参考文档多，Resin用户数少，可考虑文档少 最主要区别则是 Tomcat 是标准的 java 容器，不过性能方面比 resin 的要差一些。但稳定性和 java 程序的兼容性，应该是比 resin 的要好\n工作中选择：现在大公司都是用 resin，追求性能；而中小型公司都是用 Tomcat，追求稳定和程序的兼容\n6、什么是中间件？什么是jdk？\n中间件介绍：\n中间件是一种独立的系统软件或服务程序，分布式应用软件借助这种软件在不同的技术之间共享资源。中间件位于客户机/服务器的操作系统之上，管理计算机资源和网络通讯，是连接两个独立应用程序或独立系统的软件。\n相连接的系统，即使它们具有不同的接口，但通过中间件相互之间仍能交换信息。执行中间件的一个关键途径是信息传递，通过中间件，应用程序可以工作于多平台或OS环境。\njdk：jdk是Java的开发工具包 它是一种用于构建在 Java 平台上发布的应用程序、applet 和组件的开发环境\n7、讲述一下Tomcat8005、8009、8080三个端口的含义？\n8005 - 关闭时使用 8009 - 为AJP端口，即容器使用，如Apache能通过AJP协议访问Tomcat的8009端口 8080 - 一般应用使用\n8、什么叫CDN？\n即内容分发网络 其目的是通过在现有的 Internet 中增加一层新的网络架构，将网站的内容发布到最接近用户的网络边缘，使用户可就近取得所需的内容，提高用户访问网站的速度 9、什么叫网站灰度发布？\n灰度发布是指在黑与白之间，能够平滑过渡的一种发布方式。\nAB test 就是一种灰度发布方式，让一部用户继续用A，一部分用户开始用B，如果用户对B没有什么反对意见，那么逐步扩大范围，把所有用户都迁移到B上面来，灰度发布可以保证整体系统的稳定，在初始灰度的时候就可以发现、调整问题，以保证其影响度。\n10、简述 DNS 进行域名解析的过程？\n用户要访问 www.baidu.com，会先找本机的 host 文件，再找本地设置的 DNS 服务器，如果也没有的话，就去网络中找根服务器，根服务器反馈结果，说只能提供一级域名服务器.cn，就去找一级域名服务器，一级域名服务器说只能提供二级域名服务器.com.cn，就去找二级域名服务器。\n二级域服务器只能提供三级域名服务器.baidu.com.cn，就去找三级域名服务器，三级域名服务器正好有这个网站www.baidu.com，然后发给请求的服务器，保存一份之后，再发给客户端。\n11、RabbitMQ 是什么东西？\nRabbitMQ 也就是消息队列中间件，消息中间件是在消息的传息过程中保存消息的容器，消息中间件再将消息从它的源中到它的目标中标时充当中间人的作用。队列的主要目的是提供路由并保证消息的传递；如果发送消息时接收者不可用，消息队列不会保留消息，直到可以成功地传递为止，当然，消息队列保存消息也是有期限地。\n12、讲述一下LVS三种模式的工作过程？\nLVS 有三种负载均衡的模式，分别是 VS/NAT（nat 模式）、VS/DR(路由模式)、VS/TUN（隧道模式）\n一、NAT模式（VS-NAT）\n原理：就是把客户端发来的数据包的IP头的目的地址，在负载均衡器上换成其中一台RS的IP地址\n并发至此 RS 来处理，RS处理完后把数据交给负载均衡器，负载均衡器再把数据包原IP地址改为自己的IP\n将目的地址改为客户端IP地址即可期间，无论是进来的流量,还是出去的流量,都必须经过负载均衡器\n优点：集群中的物理服务器可以使用任何支持TCP/IP操作系统，只有负载均衡器需要一个合法的IP地址 缺点：扩展性有限。当服务器节点（普通PC服务器）增长过多时,负载均衡器将成为整个系统的瓶颈 因为所有的请求包和应答包的流向都经过负载均衡器。当服务器节点过多时\n大量的数据包都交汇在负载均衡器那，速度就会变慢！\n二、IP隧道模式（VS-TUN）\n原理：首先要知道，互联网上的大多Internet服务的请求包很短小，而应答包通常很大\n那么隧道模式就是，把客户端发来的数据包，封装一个新的IP头标记(仅目的IP)发给RS\nRS收到后，先把数据包的头解开,还原数据包,处理后,直接返回给客户端,不需要再经过\n负载均衡器。注意,由于RS需要对负载均衡器发过来的数据包进行还原,所以说必须支持\nIPTUNNEL协议，所以，在RS的内核中，必须编译支持IPTUNNEL这个选项\n优点：负载均衡器只负责将请求包分发给后端节点服务器，而RS将应答包直接发给用户。所以，减少了负载均衡器的大量数据流动，负载均衡器不再是系统的瓶颈，就能处理很巨大的请求量。这种方式，一台负载均衡器能够为很多RS进行分发。而且跑在公网上就能进行不同地域的分发。 缺点：隧道模式的RS节点需要合法IP，这种方式需要所有的服务器支持”IP Tunneling”(IP Encapsulation)协议，服务器可能只局限在部分 Linux 系统上 三、直接路由模式（VS-DR）\n原理：负载均衡器和RS都使用同一个IP对外服务但只有DR对ARP请求进行响应\n所有RS对本身这个IP的ARP请求保持静默也就是说,网关会把对这个服务IP的请求全部定向给DR\n而DR收到数据包后根据调度算法,找出对应的RS，把目的MAC地址改为RS的MAC（因为IP一致）\n并将请求分发给这台RS这时RS收到这个数据包，处理完成之后，由于IP一致，可以直接将数据返给客户\n则等于直接从客户端收到这个数据包无异，处理后直接返回给客户端\n由于负载均衡器要对二层包头进行改换，所以负载均衡器和RS之间必须在一个广播域\n也可以简单的理解为在同一台交换机上\n优点：和TUN（隧道模式）一样，负载均衡器也只是分发请求，应答包通过单独的路由方法返回给客户端\n与VS-TUN相比，VS-DR这种实现方式不需要隧道结构，因此可以使用大多数操作系统做为物理服务器。\n缺点：（不能说缺点，只能说是不足）要求负载均衡器的网卡必须与物理网卡在一个物理段上。\n13、MySQL 的 innodb 如何定位锁问题，MySQL 如何减少主从复制延迟？\nMySQL 的 innodb 如何定位锁问题:\n在使用 show engine innodb status检查引擎状态时，发现了死锁问题\n在5.5中，information_schema 库中增加了三个关于锁的表（MEMORY引擎）\ninnodb_trx ## 当前运行的所有事务 innodb_locks ## 当前出现的锁 innodb_lock_waits ## 锁等待的对应关系 MySQL 如何减少主从复制延迟：\n如果延迟比较大，就先确认以下几个因素：\n从库硬件比主库差，导致复制延迟 主从复制单线程，如果主库写并发太大，来不及传送到从库，就会导致延迟。更高版本的mysql可以支持多线程复制 慢SQL语句过多 网络延迟 master负载；主库读写压力大，导致复制延迟，架构的前端要加buffer及缓存层 slave负载；一般的做法是，使用多台 slave 来分摊读请求，再从这些 slave 中取一台专用的服务器。只作为备份用，不进行其他任何操作.另外， 2个可以减少延迟的参数：–slave-net-timeout=seconds 单位为秒 默认设置为 3600秒 #参数含义：当slave从主数据库读取log数据失败后，等待多久重新建立连接并获取数据–master-connect-retry=seconds 单位为秒 默认设置为 60秒 #参数含义：当重新建立主从连接时，如果连接建立失败，间隔多久后重试通常配置以上2个参数可以减少网络问题导致的主从数据同步延迟 MySQL 数据库主从同步延迟解决方案\n最简单的减少slave同步延时的方案就是在架构上做优化，尽量让主库的DDL快速执行\n还有就是主库是写，对数据安全性较高，比如sync_binlog=1，innodb_flush_log_at_trx_commit= 1 之类的设置，而slave则不需要这么高的数据安全，完全可以讲sync_binlog设置为0或者关闭binlog\ninnodb_flushlog也可以设置为0来提高sql的执行效率。另外就是使用比主库更好的硬件设备作为slave\n14、如何重置 MySQL Root 密码？\n一、 在已知MYSQL数据库的ROOT用户密码的情况下，修改密码的方法：\n1、 在SHELL环境下，使用 mysqladmin 命令设置：\nmysqladmin –u root –p password “新密码” 回车后要求输入旧密码 2、 在mysql\u0026gt;环境中,使用update命令，直接更新 MySQL 库 user 表的数据：\nUpdate mysql.user set password=password(‘新密码’) where user=’root’; flush privileges; 注意：mysql语句要以分号”；”结束 3、在mysql\u0026gt;环境中，使用 grant 命令，修改 root 用户的授权权限。\ngrant all on *.* to root@’localhost’ identified by ‘新密码’； 二、 如查忘记了mysql数据库的ROOT用户的密码，又如何做呢？方法如下：\n1、 关闭当前运行的mysqld服务程序：service mysqld stop（要先将mysqld添加为系统服务）\n2、 使用mysqld_safe脚本以安全模式（不加载授权表）启动mysqld 服务\n/usr/local/mysql/bin/mysqld_safe --skip-grant-table \u0026amp; 3、 使用空密码的root用户登录数据库，重新设置ROOT用户的密码\n＃mysql -u root Mysql\u0026gt; Update mysql.user set password=password(‘新密码’) where user=’root’; Mysql\u0026gt; flush privileges; 15、lvs/nginx/haproxy 优缺点\nNginx 的优点是： 1、工作在网络的7层之上，可以针对http应用做一些分流的策略，比如针对域名、目录结构，它的正则规则比HAProxy更为强大和灵活，这也是它目前广泛流行的主要原因之一，Nginx单凭这点可利用的场合就远多于LVS了。\n2、Nginx 对网络稳定性的依赖非常小，理论上能ping通就就能进行负载功能，这个也是它的优势之一，相反 LVS 对网络稳定性依赖比较大，这点本人深有体会；\n3、Nginx 安装和配置比较简单，测试起来比较方便，它基本能把错误用日志打印出来，LVS 的配置、测试就要花比较长的时间了，LVS对网络依赖比较大。\n4、可以承担高负载压力且稳定，在硬件不差的情况下一般能支撑几万次的并发量，负载度比LVS相对小些。\n5、Nginx可以通过端口检测到服务器内部的故障，比如根据服务器处理网页返回的状态码、超时等等，并且会把返回错误的请求重新提交到另一个节点，不过其中缺点就是不支持url来检测。比如用户正在上传一个文件，而处理该上传的节点刚好在上传过程中出现故障，Nginx会把上传切到另一台服务器重新处理，而LVS就直接断掉了。如果是上传一个很大的文件或者很重要的文件的话，用户可能会因此而不满。\n6、Nginx 不仅仅是一款优秀的负载均衡器/反向代理软件，它同时也是功能强大的Web应用服务器，LNMP也是近几年非常流行的web架构，在高流量的环境中稳定性也很好。\n7、Nginx现在作为Web反向加速缓存越来越成熟了，速度比传统的Squid服务器更快，可考虑用其作为反向代理加速器\n8、Nginx可作为中层反向代理使用，这一层面Nginx基本上无对手，唯一可以对比Nginx的就只有lighttpd了。不过lighttpd目前还没有做到Nginx完全的功能，配置也不那么清晰易读，社区资料也远远没Nginx活跃\n9、Nginx也可作为静态网页和图片服务器，这方面的性能也无对手。还有Nginx社区非常活跃，第三方模块也很多\nNginx 的缺点是： 1、Nginx仅能支持http、https和Email协议，这样就在适用范围上面小些，这个是它的缺点\n2、对后端服务器的健康检查，只支持通过端口来检测，不支持通过url来检测，不支持Session的直接保持，但能通过ip_hash来解决。\nLVS：使用Linux内核集群实现一个高性能、高可用的负载均衡服务器\n它具有很好的可伸缩性（Scalability)、可靠性（Reliability)和可管理性（Manageability)\nLVS的优点是： 1、抗负载能力强、是工作在网络4层之上仅作分发之用，没有流量的产生，这个特点也决定了它在负载均衡软件里的性能最强的，对内存和cpu资源消耗比较低。\n2、配置性比较低，这是一个缺点也是一个优点，因为没有可太多配置的东西，所以并不需要太多接触，大大减少了人为出错的几率\n3、工作稳定，因为其本身抗负载能力很强，自身有完整的双机热备方案，如LVS+Keepalived，不过我们在项目实施中用得最多的还是LVS/DR+Keepalived\n4、无流量，LVS只分发请求，而流量并不从它本身出去，这点保证了均衡器IO的性能不会收到大流量的影响。\n5、应用范围较广，因为LVS工作在4层，所以它几乎可对所有应用做负载均衡，包括http、数据库、在线聊天室等\nLVS的缺点是： 1、软件本身不支持正则表达式处理，不能做动静分离\n而现在许多网站在这方面都有较强的需求，这个是Nginx/HAProxy+Keepalived的优势所在\n2、如果是网站应用比较庞大的话，LVS/DR+Keepalived 实施起来就比较复杂了\n特别后面有Windows Server的机器的话，如果实施及配置还有维护过程就比较复杂了\n相对而言，Nginx/HAProxy+Keepalived 就简单多了。\nHAProxy的特点是： 1、HAProxy也是支持虚拟主机的。\n2、HAProxy 的优点能够补充 Nginx的 一些缺点，比如支持 Session 的保持，Cookie 的引导\n同时支持通过获取指定的url来检测后端服务器的状态 3、HAProxy跟LVS类似，本身就只是一款负载均衡软件\n单纯从效率上来讲HAProxy会比Nginx有更出色的负载均衡速度，在并发处理上也是优于Nginx的 4、HAProxy支持TCP协议的负载均衡转发，可以对MySQL读进行负载均衡\n对后端的 MySQL 节点进行检测和负载均衡，大家可以用 LVS+Keepalived 对 MySQL 主从做负载均衡 5、HAProxy负载均衡策略非常多，HAProxy的负载均衡算法现在具体有如下8种：\n①roundrobin，表示简单的轮询，这个不多说，这个是负载均衡基本都具备的；\n② static-rr，表示根据权重，建议关注；\n③leastconn，表示最少连接者先处理，建议关注；\n④ source，表示根据请求源IP，这个跟Nginx的IP_hash机制类似\n我们用其作为解决session问题的一种方法，建议关注； ⑤ri，表示根据请求的URI；\n⑥rl_param，表示根据请求的URl参数’balance url_param’ requires an URL parameter name；\n⑦hdr(name)，表示根据HTTP请求头来锁定每一次HTTP请求；\n⑧rdp-cookie(name)，表示根据据cookie(name)来锁定并哈希每一次TCP请求。\n16、mysql数据备份工具\nmysqldump工具 mysqldump是mysql自带的备份工具，目录在bin目录下面：/usr/local/mysql/bin/mysqldump\n支持基于innodb的热备份，但是由于是逻辑备份，所以速度不是很快，适合备份数据比较小的场景\nMysqldump完全备份+二进制日志可以实现基于时间点的恢复。\n基于LVM快照备份 在物理备份中，有基于文件系统的物理备份（LVM的快照），也可以直接用tar之类的命令对整个数据库目录\n进行打包备份，但是这些只能进行泠备份，不同的存储引擎备份的也不一样，myisam自动备份到表级别\n而innodb不开启独立表空间的话只能备份整个数据库。\ntar包备份 percona提供的xtrabackup工具\n支持innodb的物理热备份，支持完全备份，增量备份，而且速度非常快，支持innodb存储引起的数据在不同\n数据库之间迁移，支持复制模式下的从机备份恢复备份恢复，为了让xtrabackup支持更多的功能扩展\n可以设立独立表空间，打开 innodb_file_per_table功能，启用之后可以支持单独的表备份\n17、keepalive的工作原理和如何做到健康检查\nkeepalived是以VRRP协议为实现基础的，VRRP全称Virtual Router Redundancy Protocol，即虚拟路由冗余协议。\n虚拟路由冗余协议，可以认为是实现路由器高可用的协议，即将N台提供相同功能的路由器组成一个路由器组\n这个组里面有一个master和多个backup，master上面有一个对外提供服务的vip（该路由器所在局域网内\n其他机器的默认路由为该vip），master会发组播，当backup收不到vrrp包时就认为master宕掉了\n这时就需要根据VRRP的优先级来选举一个backup当master。这样就可以保证路由器的高可用了\nkeepalived主要有三个模块，分别是core、check和vrrp。core模块为keepalived的核心，负责主进程的启动、维护\n及全局配置文件的加载和解析。check负责健康检查，包括常见的各种检查方式，vrrp模块是来实现VRRP协议的\nKeepalived健康检查方式配置：\nHTTP_GET|SSL_GET HTTP_GET | SSL_GET { url { path /# HTTP/SSL 检查的url可以是多个 digest \u0026lt;STRING\u0026gt; # HTTP/SSL 检查后的摘要信息用工具genhash生成 status_code 200# HTTP/SSL 检查返回的状态码 } connect_port 80 # 连接端口 bindto\u0026lt;IPADD\u0026gt; connect_timeout 3 # 连接超时时间 nb_get_retry 3 # 重连次数 delay_before_retry 2 #连接间隔时间 } 18、统计ip访问情况，要求分析nginx访问日志，找出访问页面数量在前十位的ip\ncat access.log | awk \u0026#39;{print $1}\u0026#39; | uniq -c | sort -rn | head -10 19、使用 tcpdump 监听主机为192.168.1.1，tcp端口为80的数据，同时将输出结果保存输出到 tcpdump.log\ntcpdump \u0026#39;host 192.168.1.1 and port 80\u0026#39; \u0026gt; tcpdump.log 20、如何将本地80 端口的请求转发到8080 端口，当前主机IP 为192.168.2.1\niptables -A PREROUTING -d 192.168.2.1 -p tcp -m tcp -dport 80 -j DNAT-to-destination 192.168.2.1:8080 21、你对现在运维工程师的理解和以及对其工作的认识\n运维工程师在公司当中责任重大，需要保证时刻为公司及客户提供最高、最快、最稳定、最安全的服务\n运维工程师的一个小小的失误，很有可能会对公司及客户造成重大损失\n因此运维工程师的工作需要严谨及富有创新精神\n22、实时抓取并显示当前系统中tcp 80端口的网络数据信息，请写出完整操作命令\ntcpdump -nn tcp port 80 23、服务器开不了机怎么解决一步步的排查\n24、Linux 系统中病毒怎么解决\n1）最简单有效的方法就是重装系统 2）要查的话就是找到病毒文件然后删除\n中毒之后一般机器 CPU、内存使用率会比较高\n机器向外发包等异常情况，排查方法简单介绍下\ntop 命令找到 CPU 使用率最高的进程\n一般病毒文件命名都比较乱，可以用 ps aux 找到病毒文件位置\nrm -f 命令删除病毒文件\n检查计划任务、开机启动项和病毒文件目录有无其他可以文件等\n3）由于即使删除病毒文件不排除有潜伏病毒，所以最好是把机器备份数据之后重装一下\n25、发现一个病毒文件你删了他又自动创建怎么解决\n公司的内网某台linux服务器流量莫名其妙的剧增，用 iftop 查看有连接外网的情况\n针对这种情况一般重点查看 netstat 连接的外网 ip 和端口。\n用 lsof -p pid 可以查看到具体是那些进程，哪些文件\n经查勘发现/root下有相关的配置conf.n hhe两个可疑文件，rm -rf后不到一分钟就自动生成了\n由此推断是某个母进程产生的这些文件。所以找到母进程就是找到罪魁祸首\n查杀病毒最好断掉外网访问，还好是内网服务器，可以通过内网访问\n断了内网，病毒就失去外联的能力，杀掉它就容易的多\n怎么找到呢，找了半天也没有看到蛛丝马迹，没办法只有ps axu一个个排查\n方法是查看可以的用户和和系统相似而又不是的冒牌货，果然，看到了如下进程可疑\n看不到图片就是/usr/bin/.sshd\n于是我杀掉所有.sshd相关的进程，然后直接删掉.sshd这个可执行文件\n然后才删掉了文章开头提到的自动复活的文件\n总结一下，遇到这种问题，如果不是太严重，尽量不要重装系统\n一般就是先断外网，然后利用iftop,ps,netstat,chattr,lsof,pstree这些工具顺藤摸瓜\n一般都能找到元凶。但是如果遇到诸如此类的问题\n/boot/efi/EFI/redhat/grub.efi: Heuristics.Broken.Executable FOUND，个人觉得就要重装系统了\n26、说说TCP/IP的七层模型\n应用层 (Application)： 网络服务与最终用户的一个接口。\n协议有：HTTP FTP TFTP SMTP SNMP DNS TELNET HTTPS POP3 DHCP\n表示层（Presentation Layer）： 数据的表示、安全、压缩。（在五层模型里面已经合并到了应用层）\n格式有，JPEG、ASCll、DECOIC、加密格式等\n会话层（Session Layer）： 建立、管理、终止会话。（在五层模型里面已经合并到了应用层）\n对应主机进程，指本地主机与远程主机正在进行的会话\n传输层 (Transport)： 定义传输数据的协议端口号，以及流控和差错校验。\n协议有：TCP UDP，数据包一旦离开网卡即进入网络传输层\n网络层 (Network)： 进行逻辑地址寻址，实现不同网络之间的路径选择。\n协议有：ICMP IGMP IP（IPV4 IPV6） ARP RARP\n数据链路层 (Link)： 建立逻辑连接、进行硬件地址寻址、差错校验等功能。（由底层网络定义协议）\n将比特组合成字节进而组合成帧，用MAC地址访问介质，错误发现但不能纠正\n物理层（Physical Layer）： 是计算机网络OSI模型中最低的一层\n物理层规定：为传输数据所需要的物理链路创建、维持、拆除\n而提供具有机械的，电子的，功能的和规范的特性\n简单的说，物理层确保原始的数据可在各种物理媒体上传输。局域网与广域网皆属第1、2层\n物理层是OSI的第一层，它虽然处于最底层，却是整个开放系统的基础\n物理层为设备之间的数据通信提供传输媒体及互连设备，为数据传输提供可靠的环境\n如果您想要用尽量少的词来记住这个第一层，那就是“信号和介质”。\n27、你常用的 Nginx 模块，用来做什么\nrewrite模块，实现重写功能 access模块：来源控制 ssl模块：安全加密 ngx_http_gzip_module：网络传输压缩模块 ngx_http_proxy_module 模块实现代理 ngx_http_upstream_module模块实现定义后端服务器列表 ngx_cache_purge实现缓存清除功能 28、请列出你了解的web服务器负载架构\nNginx Haproxy Keepalived LVS\n29、查看http的并发请求数与其TCP连接状态\nnetstat -n | awk \u0026#39;/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}\u0026#39; 还有ulimit -n 查看linux系统打开最大的文件描述符，这里默认1024\n不修改这里web服务器修改再大也没用，若要用就修改很几个办法，这里说其中一个：\n修改/etc/security/limits.conf\n* soft nofile 10240 * hard nofile 10240 重启后生效\n30、用tcpdump嗅探80端口的访问看看谁最高\ntcpdump -i eth0 -tnn dst port 80 -c 1000 | awk -F\u0026#34;.\u0026#34; \u0026#39;{print $1\u0026#34;.\u0026#34;$2\u0026#34;.\u0026#34;$3\u0026#34;.\u0026#34;$4}\u0026#39;| sort | uniq -c | sort -nr |head -20 31、写一个脚本，实现判断192.168.1.0/24网络里，当前在线的IP有哪些，能ping通则认为在线\n#!/bin/bash for ip in `seq 1 255` do { ping -c 1 192.168.1.$ip \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 if [ $? -eq 0 ]; then echo 192.168.1.$ip UP else echo 192.168.1.$ip DOWN fi }\u0026amp; done wait 32、已知 apache 服务的访问日志按天记录在服务器本地目录/app/logs 下，由于磁盘空间紧张现在要求只能保留最近 7 天的访问日志！请问如何解决？请给出解决办法或配置或处理命令\n创建文件脚本：\n#!/bin/bash for n in `seq 14` do date -s \u0026#34;11/0$n/14\u0026#34; touch access_www_`(date +%F)`.log done 解决方法：\n# pwd/application/logs # ll -rw-r--r--. 1 root root 0 Jan 1 00:00 access_www_2015-01-01.log -rw-r--r--. 1 root root 0 Jan 2 00:00 access_www_2015-01-02.log -rw-r--r--. 1 root root 0 Jan 3 00:00 access_www_2015-01-03.log -rw-r--r--. 1 root root 0 Jan 4 00:00 access_www_2015-01-04.log -rw-r--r--. 1 root root 0 Jan 5 00:00 access_www_2015-01-05.log -rw-r--r--. 1 root root 0 Jan 6 00:00 access_www_2015-01-06.log -rw-r--r--. 1 root root 0 Jan 7 00:00 access_www_2015-01-07.log -rw-r--r--. 1 root root 0 Jan 8 00:00 access_www_2015-01-08.log -rw-r--r--. 1 root root 0 Jan 9 00:00 access_www_2015-01-09.log -rw-r--r--. 1 root root 0 Jan 10 00:00 access_www_2015-01-10.log -rw-r--r--. 1 root root 0 Jan 11 00:00 access_www_2015-01-11.log -rw-r--r--. 1 root root 0 Jan 12 00:00 access_www_2015-01-12.log -rw-r--r--. 1 root root 0 Jan 13 00:00 access_www_2015-01-13.log -rw-r--r--. 1 root root 0 Jan 14 00:00 access_www_2015-01-14.log # find /application/logs/ -type f -mtime +7 -name \u0026#34;*.log\u0026#34;|xargs rm –f ##也可以使用-exec rm -f {} \\;进行删除 # ll -rw-r--r--. 1 root root 0 Jan 7 00:00 access_www_2015-01-07.log -rw-r--r--. 1 root root 0 Jan 8 00:00 access_www_2015-01-08.log -rw-r--r--. 1 root root 0 Jan 9 00:00 access_www_2015-01-09.log -rw-r--r--. 1 root root 0 Jan 10 00:00 access_www_2015-01-10.log -rw-r--r--. 1 root root 0 Jan 11 00:00 access_www_2015-01-11.log -rw-r--r--. 1 root root 0 Jan 12 00:00 access_www_2015-01-12.log -rw-r--r--. 1 root root 0 Jan 13 00:00 access_www_2015-01-13.log -rw-r--r--. 1 root root 0 Jan 14 00:00 access_www_2015-01-14.log 33、如何优化 Linux系统（可以不说太具体）？\n不用root，添加普通用户，通过sudo授权管理 更改默认的远程连接SSH服务端口及禁止root用户远程连接 定时自动更新服务器时间 配置国内yum源 关闭selinux及iptables（iptables工作场景如果有外网IP一定要打开，高并发除外） 调整文件描述符的数量 精简开机启动服务（crond rsyslog network sshd） 内核参数优化（/etc/sysctl.conf） 更改字符集，支持中文，但建议还是用英文字符集，防止乱码 锁定关键系统文件 清空/etc/issue，去除系统及内核版本登录前的屏幕显示\n34、请执行命令取出 linux 中 eth0 的 IP 地址(请用 cut，有能力者也可分别用 awk,sed 命令答）\ncut方法1：\n# ifconfig eth0|sed -n \u0026#39;2p\u0026#39;|cut -d \u0026#34;:\u0026#34; -f2|cut -d \u0026#34; \u0026#34; -f1 192.168.20.130 awk方法2：\n# ifconfig eth0|awk \u0026#39;NR==2\u0026#39;|awk -F \u0026#34;:\u0026#34; \u0026#39;{print $2}\u0026#39;|awk \u0026#39;{print $1}\u0026#39; 192.168.20.130 awk多分隔符方法3：\n# ifconfig eth0|awk \u0026#39;NR==2\u0026#39;|awk -F \u0026#34;[: ]+\u0026#34; \u0026#39;{print $4}\u0026#39; 192.168.20.130 sed方法4:\n# ifconfig eth0|sed -n \u0026#39;/inet addr/p\u0026#39;|sed -r \u0026#39;s#^.*ddr:(.*)Bc.*$#\\1#g\u0026#39; 192.168.20.130 35、每天晚上 12 点，打包站点目录/var/www/html 备份到/data 目录下（最好每次备份按时间生成不同的备份包）\n# cat a.sh #/bin/bash cd /var/www/ \u0026amp;\u0026amp; /bin/tar zcf /data/html-`date +%m-%d%H`.tar.gz html/ # crontab –e 00 00 * * * /bin/sh /root/a.sh "},{"id":171,"href":"/docs/%E5%BC%80%E5%A7%8B%E4%BD%BF%E7%94%A8-cilium-kai-shi-shi-yong-cilium/","title":"开始使用 cilium 2023-09-28 15:34:49.205","section":"Docs","content":" 开始使用 cilium # 以下为简要翻译 cilium doc上的一个应用示例原文，部署在单节点k8s 环境的实践。\n部署示例应用 # 官方文档用几个pod/svc 抽象一个有趣的应用场景（星战迷）：星战中帝国方建造了被称为“终极武器”的“死星”，它是一个卫星大小的战斗空间站，它的核心是使用凯伯晶体（Kyber Crystal）的超级激光炮，剧中它的首秀就以完全火力摧毁了“杰达圣城”（Jedha）。下面将用运行于 k8s上的 pod/svc/cilium 等模拟“死星“的一个“飞船登陆”系统安全策略设计。\ndeploy/deathstar：作为控制整个“死星”的飞船登陆管理系统，它暴露一个SVC，提供HTTP REST 接口给飞船请求登陆使用； pod/tiefighter：作为“帝国”方的常规战斗飞船，它会调用上述 HTTP 接口，请求登陆“死星”； pod/xwing：作为“盟军”方的飞行舰，它也尝试调用 HTTP 接口，请求登陆“死星”； 根据文件http-sw-app.yaml 创建 $ kubectl create -f http-sw-app.yaml 后，验证如下：\n$ kubectl get pods,svc NAME READY STATUS RESTARTS AGE pod/deathstar-5fc7c7795d-djf2q 1/1 Running 0 4h pod/deathstar-5fc7c7795d-hrgst 1/1 Running 0 4h pod/tiefighter 1/1 Running 0 4h pod/xwing 1/1 Running 0 4h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/deathstar ClusterIP 10.68.242.130 \u0026lt;none\u0026gt; 80/TCP 4h service/kubernetes ClusterIP 10.68.0.1 \u0026lt;none\u0026gt; 443/TCP 5h 每个 POD 在 cilium 中都表示为 Endpoint，初始每个 Endpoint 的”进出安全策略“状态均为 Disabled，如下：(已省略部分无关 POD 信息)\n$ kubectl exec -n kube-system cilium-6t5vx -- cilium endpoint list ENDPOINT POLICY (ingress) POLICY (egress) IDENTITY LABELS (source:key[=value]) IPv6 IPv4 STATUS ENFORCEMENT ENFORCEMENT 643 Disabled Disabled 31371 k8s:class=deathstar f00d::ac14:0:0:283 172.20.0.246 ready k8s:io.cilium.k8s.policy.serviceaccount=default k8s:io.kubernetes.pod.namespace=default k8s:org=empire 1011 Disabled Disabled 31371 k8s:class=deathstar f00d::ac14:0:0:3f3 172.20.0.63 ready k8s:io.cilium.k8s.policy.serviceaccount=default k8s:io.kubernetes.pod.namespace=default k8s:org=empire 32030 Disabled Disabled 5350 k8s:class=tiefighter f00d::ac14:0:0:7d1e 172.20.0.201 ready k8s:io.cilium.k8s.policy.serviceaccount=default k8s:io.kubernetes.pod.namespace=default k8s:org=empire 45943 Disabled Disabled 14309 k8s:class=xwing f00d::ac14:0:0:b377 172.20.0.189 ready k8s:io.cilium.k8s.policy.serviceaccount=default k8s:io.kubernetes.pod.namespace=default k8s:org=alliance 52035 Disabled Disabled 4 reserved:health f00d::ac14:0:0:cb43 172.20.0.92 ready 检查初始状态 # 当然“死星”应该只允许“帝国”的飞船着陆，因为没有应用任何策略，所以初始状态下“帝国”和“联盟”的飞船都可以登陆，如下测试：\n$ kubectl exec xwing -- curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing Ship landed # 成功着陆 $ kubectl exec tiefighter -- curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing Ship landed # 成功着陆 应用 L3/L4 策略 # 现在我们应用策略，仅让带有标签 org=empire的飞船登陆“死星”；那么带有标签 org=alliance的“联盟”飞船将禁止登陆；这个就是我们熟悉的传统L3/L4 防火墙策略，并跟踪连接（会话）状态；\n根据文件sw_l3_l4_policy.yaml 创建 $ kubectl apply -f sw_l3_l4_policy.yaml 后，验证如下：\n$ kubectl exec tiefighter -- curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing Ship landed # 成功着陆 $ kubectl exec xwing -- curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing # 失败超时 查看安全策略 # 再次执行 cilium endpoint list，可以看到标签带deathstar的 POD 已经应用了 Ingress方向的策略：\n# kubectl exec -n kube-system cilium-6t5vx -- cilium endpoint list ENDPOINT POLICY (ingress) POLICY (egress) IDENTITY LABELS (source:key[=value]) IPv6 IPv4 STATUS ENFORCEMENT ENFORCEMENT 643 Enabled Disabled 31371 k8s:class=deathstar f00d::ac14:0:0:283 172.20.0.246 ready k8s:io.cilium.k8s.policy.serviceaccount=default k8s:io.kubernetes.pod.namespace=default k8s:org=empire 1011 Enabled Disabled 31371 k8s:class=deathstar f00d::ac14:0:0:3f3 172.20.0.63 ready k8s:io.cilium.k8s.policy.serviceaccount=default k8s:io.kubernetes.pod.namespace=default k8s:org=empire 32030 Disabled Disabled 5350 k8s:class=tiefighter f00d::ac14:0:0:7d1e 172.20.0.201 ready k8s:io.cilium.k8s.policy.serviceaccount=default k8s:io.kubernetes.pod.namespace=default k8s:org=empire 45943 Disabled Disabled 14309 k8s:class=xwing f00d::ac14:0:0:b377 172.20.0.189 ready k8s:io.cilium.k8s.policy.serviceaccount=default k8s:io.kubernetes.pod.namespace=default k8s:org=alliance 52035 Disabled Disabled 4 reserved:health f00d::ac14:0:0:cb43 172.20.0.92 ready 查看具体策略内容 kubectl describe cnp rule1\nL7 安全策略 # 上述的策略可以进行简单的安全防护了，但是“死星”的这个系统还有很多复杂的功能；比如它还提供了一个内部维护接口，如果被不合理调用将带来严重灾难性后果，也许“联盟”勇士劫持了一架“帝国”飞船正在进行这个任务（虽然我们内心希望他能够成功摧毁“死星”）。不幸的是“死星”系统设计者考虑到这个风险，它有办法严格限制每架飞船能够请求的权限。\n没有限制飞船请求权限时，如下运行：\n$ kubectl exec tiefighter -- curl -s -XPUT deathstar.default.svc.cluster.local/v1/exhaust-port Panic: deathstar exploded goroutine 1 [running]: main.HandleGarbage(0x2080c3f50, 0x2, 0x4, 0x425c0, 0x5, 0xa) /code/src/github.com/empire/deathstar/ temp/main.go:9 +0x64 main.main() /code/src/github.com/empire/deathstar/ temp/main.go:5 +0x85 限制L7 的安全策略，根据文件sw_l3_l4_l7_policy.yaml 创建 $ kubectl apply -f sw_l3_l4_l7_policy.yaml 后，验证如下：\n$ kubectl exec tiefighter -- curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing Ship landed $ kubectl exec tiefighter -- curl -s -XPUT deathstar.default.svc.cluster.local/v1/exhaust-port Access denied 我们同样可以使用 kubectl desribe cnp检查更新的策略，或者使用 cilium 命令行：\n$ kubectl exec -n kube-system cilium-6t5vx -- cilium policy get [ { \u0026#34;endpointSelector\u0026#34;: { \u0026#34;matchLabels\u0026#34;: { \u0026#34;any:class\u0026#34;: \u0026#34;deathstar\u0026#34;, \u0026#34;any:org\u0026#34;: \u0026#34;empire\u0026#34;, \u0026#34;k8s:io.kubernetes.pod.namespace\u0026#34;: \u0026#34;default\u0026#34; } }, \u0026#34;ingress\u0026#34;: [ { \u0026#34;fromEndpoints\u0026#34;: [ { \u0026#34;matchLabels\u0026#34;: { \u0026#34;any:org\u0026#34;: \u0026#34;empire\u0026#34;, \u0026#34;k8s:io.kubernetes.pod.namespace\u0026#34;: \u0026#34;default\u0026#34; } } ], \u0026#34;toPorts\u0026#34;: [ { \u0026#34;ports\u0026#34;: [ { \u0026#34;port\u0026#34;: \u0026#34;80\u0026#34;, \u0026#34;protocol\u0026#34;: \u0026#34;TCP\u0026#34; } ], \u0026#34;rules\u0026#34;: { \u0026#34;http\u0026#34;: [ { \u0026#34;path\u0026#34;: \u0026#34;/v1/request-landing\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;POST\u0026#34; } ] } } ] } ], \u0026#34;labels\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;io.cilium.k8s.policy.name\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;rule1\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;k8s\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;io.cilium.k8s.policy.namespace\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;k8s\u0026#34; } ] } ] Revision: 267 我们看到 cilium 可以实现 7层 HTTP 协议的请求方法（GET/PUT/POST等）、路径（/v1/request-landing）等等安全策略；另外，它还可以防护其他应用（如：Kafka, gRPC, Elasticsearch），可以去官网文档示例学习！\n参考资料 # cilium github cilium doc "},{"id":172,"href":"/docs/%E5%BD%BB%E6%82%9F%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C-che-wu-rong-qi-wang-luo/","title":"彻悟容器网络 2024-04-07 10:30:11.55","section":"Docs","content":"01\n背景\n容器网络为何出现\n在一个汽车发动机的生产车间中，汽车发动机的各个组件会存在一定的顺序进行组装，这就要求有直接关系的组件必须知道下一个组件的具体位置。当一个汽车发动机组装完成后，距离最后成品汽车，还差了很多部件，比如底盘，车身等。此时，需要将发动机发往一个装配中心进行组合安装，这样我们就必须知道装配中心的地理位置。\n这些位置在容器中可以理解为 IP 地址，容器网络便是如此。在上面这个例子里，即描述了容器网络本节点内部互通场景，又描述了跨节点的通信场景。\n随着云计算的发展，应用间通信从物理机网络，虚拟机网络，发展到目前的容器网络。由于容器不同于物理机、虚拟机，容器可以被理解为一个标准的，轻量级的，便携的，独立的集装箱，集装箱之间相互隔离，都使用自己的环境和资源。但随着越来越复杂环境变化，容器在运行中会需要容器间或者容器与集群外部之间的信息传输，这时候容器就要在网络层面拥有一个名字（即 IP 地址），由此容器网络就应运而生。\n再从技术的角度来谈容器网络的由来，首先要从容器本质说起，它是由以下几点来实现的：\ncgroup：实现资源的可配额\noverlay fs：实现文件系统的安全性和便携性\nnamespace：实现资源的隔离性\nIPC ：System V IPC 和 POSIX 消息队列 Network：网络设备、网络协议栈、网络端口等 PID：进程 Mount：挂载点 UTS：主机名和域名 USR：用户和用户组 由于主机与容器、容器与容器间的网络栈并不相通，也没有统一的控制面，导致容器间无法直接的感知。为了解决这个问题，本文中我们要讨论的容器网络出现了，再配合不同的网络虚拟化技术带来了多样化的容器网络方案。\n02\n容器网络的基本要求\nIP-per-Pod，每个 Pod 都拥有一个独立 IP 地址，Pod 内所有容器共享一个网络命名空间\n集群内所有 Pod 都在一个直接连通的扁平网络中，可通过 IP 直接访问\n所有容器之间无需 NAT 就可以直接互相访问 所有 Node 和所有容器之间无需 NAT 就可以直接互相访问 容器自己看到的 IP 跟其他容器看到的一样 Service cluster IP 尽可在集群内部访问，外部请求需要通过 NodePort、LoadBalance 或者 Ingress 来访问\n02\n网络插件介绍\nAliware\n01\n网络插件概述\n容器和该容器所在的宿主机是分隔的两地，如果需要连通就得建立一座桥梁，但由于容器侧还没有名字，就无法建立桥梁，这时候就先需要给容器侧命名，这样才能成功建立起桥梁。网络插件就是起到给容器侧命名和建立桥梁的功能。\n即网络插件将网络接口插入容器网络命名空间（例如，veth 对的一端），并在主机上进行任何必要的改变（例如将 veth 的另一端连接到网桥）。然后通过调用适当的 IPAM 插件（IP 地址管理插件）分配给接口一个空闲的 IP 地址，并设置与此 IP 地址相对应的路由规则。\n对于 K8s 来讲，网络属于最重要的功能之一，因为没有一个良好的网络，集群不同节点之间甚至同一个节点之间的 pod 就无法良好的运行起来。\n但是 K8s 在设计网络的时候，采用的准则就一点：“灵活”！那怎么才能灵活呢？那就是 K8s 自身没有实现太多跟网络相关的操作，而是制定了一个规范：\n有配置文件，能够提供要使用的网络插件名，以及该插件所需信息 让 CRI 调用这个插件，并把容器的运行时信息，包括容器的命名空间，容器 ID 等信息传给插件 不关心网络插件内部实现，只需要最后能够输出网络插件提供的 pod IP 即可 没错一共就这三点，如此简单灵活的规范就是大名鼎鼎的 CNI 规范。\n不过恰恰因为 K8s 自己“啥也不干”，所以大家可以自由发挥，自由实现不同的 CNI 插件，即网络插件。除了社区大名鼎鼎的 Calico、Bifrost 网络插件，阿里也开发了一款功能和性能极优的网络插件 Hybridnet。\nHybridnet # Hybridnet 是专为混合云设计的开源容器网络解决方案，与 Kubernetes 集成，并被以下 PaaS 平台使用:\n阿里云 ACK 发行版 阿里云 AECP 蚂蚁金服 SOFAStack Hybridnet 专注于高效的大规模集群、异构基础设施和用户友好性。\nCalico # Calico 是一种广泛采用、久经考验的开源网络和网络安全解决方案，适用于 Kubernetes、虚拟机和裸机工作负载。Calico 为 Cloud Native 应用程序提供两大服务：\n工作负载之间的网络连接\n工作负载之间的网络安全策略\nBifrost # Bifrost 是一个可为 Kubernetes 启用 L2 网络的开源解决方案，支持以下特性\nBifrost 中的网络流量可以通过传统设备进行管理和监控 支持 macvlan 对于 service 流量的访问 02\n通信路径介绍\n# Overlay 方案：意味着将不同主机上的容器用同一个虚拟网络连接起来的跨主机网络\nVXLAN\nVXLAN（Virtual eXtensible Local Area Network，虚拟扩展局域网），是由 IETF 定义的 NVO3（Network Virtualization over Layer 3）标准技术之一，采用 L2 over L4（MAC-in-UDP）的报文封装模式，将二层报文用三层协议进行封装，可实现二层网络在三层范围内进行扩展，同时满足数据中心大二层虚拟迁移和多租户的需求 IPIP\n基于 TUN 设备实现 IPIP 隧道，TUN 网络设备能将三层（IP 网络数据包）数据包封装在另外一个三层数据包之中，Linux 原生支持好几种不同的 IPIP 隧道类型，但都依赖于 TUN 网络设备 ipip: 普通的 IPIP 隧道，就是在报文的基础上再封装成一个 IPv4 报文 gre: 通用路由封装（Generic Routing Encapsulation），定义了在任意网络层协议上封装其他网络层协议的机制，所以对于 IPv4 和 IPv6 都适用 sit: sit 模式主要用于 IPv4 报文封装 IPv6 报文，即 IPv6 over IPv4 isatap: 站内自动隧道寻址协议（Intra-Site Automatic Tunnel Addressing Protocol），类似于 sit 也是用于 IPv6 的隧道封装 vti: 即虚拟隧道接口（Virtual Tunnel Interface），是一种 IPsec 隧道技术 本文中我们使用的是 ipip 这种普通的 IPIP 隧道 Underlay 方案：由交换机和路由器等设备组成，借助以太网协议、路由协议和 VLAN 协议等驱动的网络。\nBGP\n边界网关协议BGP（Border Gateway Protocol）是一种实现自治系统AS（Autonomous System）之间的路由可达，并选择最佳路由的距离矢量路由协议 Vlan\nVLAN（Virtual Local Area Network）即虚拟局域网，是将一个物理的LAN在逻辑上划分成多个广播域的通信技术。VLAN内的主机间可以直接通信，而VLAN间不能直接通信，从而将广播报文限制在一个VLAN内 **** # 03\n网络插件的原理\ncalico 利用 IPIP 等隧道技术或者宿主机间建立 BGP 连接完成容器路由的互相学习解决了跨节点通信的问题。 hybridnet 利用 vxlan 隧道技术、宿主机间建立 BGP 连接完成容器路由的互相学习或者 ARP 代理来解决跨节点通信的问题。 bifrost 通过内核 macvlan 模块利用交换机 vlan 的能力来解决容器通信问题 **** # 04\n网络插件分类及对比\n网络插件分类 # **** overlay 方案 underlay 方案 主流方案 路由或 SDN 方案:Calico IPIP/Calico VXLAN Calico BGP/MACVLAN/IPVLAN 优点 对物理网络无侵入维护和管理简单 高性能网络流量可管理、可监控 缺点 容器网络难以监控容器访问集群外部通过 Node SNAT，无法精确管理流量 对现有组网有侵入维护和管理工作量大占用现网 IP 地址，前期需要详细规划 网络插件对比 # **** hybridnet calico ipip calico bgp bifrost 支持场景 overlay/underlay overlay underlay underlay 网络栈 IPv4/IPv6 IPv4 IPv4/IPv6 IPv4 通信技术 vxlan/vlan/bgp ipip bgp macvlan 通信机制 隧道通信/二层+三层通信/三层通信 隧道通信 三层通信 二层通信 容器通信 veth pair veth pair veth pair macvlan子接口 是否支持固定IP/固定IP池 是 是 是 是 IPPool模式 block + detail block(如1.1.1.0/24) block(如1.1.1.0/24) detail(如1.1.1.1~1.1.1.9) 南北向流量出口 SNAT/podIP SNAT SNAT/podIP podIP 是否支持网络策略 是 是 是 商用版支持 SNAT: 对数据包的源 IP 地址进行转化 podIP：由 podIP 直接通信 veth pair：在 Linux 下，可以创建一对 veth pair 的网卡，从一边发送包，另一边就能收到，对于容器流量来说会通过主机侧的 veth pair 网卡进入主机网络栈，即会过主机的 iptables 规则后再由物理网卡发出。 macvlan子接口：macvlan 子接口和原来的宿主机主接口是完全独立的，可以单独配置 MAC 地址和 IP 地址，对外通信时，容器流量不会进入主机网络栈，既不会过主机的iptables规则，只会经过二层由物理网卡发出。 **** # 05\n网络插件应用场景\n针对数据中心复杂的网络情况，我们要按需求出发去选择相对应的容器网络方案\n希望对数据中心物理网络较少侵入性，可选择使用隧道方案\n需支持双栈，则可选 hybridnet vxlan 方案 只支持单栈 IPv4，则可选 calico IPIP，calico vxlan 方案 希望数据中心支持并使用 BGP\n宿主机处于同网段内，则可选 calico BGP 方案（支持双栈） 宿主机处于不同网段内，则可选 hybridnet bgp 方案（支持双栈） 对于业务的高性能和低延迟的追求，出现了 macvlan，ipvlan l2 等方案\n公有云场景下，可选用 terway 方案，或者其他 ipvlan l3 方案，或者隧道方案\n也有为了满足全场景而开发的方案，如 hybridnet、multus 等，Multus 一款为支持其他 CNI 能力的开源容器网络插件\n本文我们将对 hybridnet vxlan、hybridnet vlan、hybridnet bgp、calico IPIP、calico BGP 和基于 macvlan 改造的 bifrost 进行 pod 数据链路上的详细分析\n**** # 03\n网络插件架构及通信路径\nAliware\n01\nHybridnet\n# 整体架构 # Hybridnet-daemon：控制每个节点上的数据平面配置，例如 Iptables 规则，策略路由等 **** # 通信路径 # 1、VXLAN 模式 # 同节点通信 Pod1 访问 Pod2 的通信过程\n发包过程：\nPod1 流量通过 veth-pair 网卡，即从 pod1 的 eth0-\u0026gt;主机侧的 hybrXXX，进入主机网络栈中 根据目的 IP，流量在主机的策略路由匹配到 39999 路由表，并在 39999 路由表中匹配到 Pod2 的路由规则 流量从 hybrYYY 网卡进入 Pod2 容器网络栈，完成发包动作 回包过程：\nPod2 流量通过 veth-pair 网卡，即从pod2的eth0-\u0026gt;主机侧的hybrYYY，进入主机网络栈中 根据目的 IP，流量在主机的策略路由匹配到39999路由表，并在39999路由表中匹配到 Pod1 的路由规则 流量从 hybrXXX 网卡进入 Pod1 容器网络栈，完成回包动作 跨节点通信 Pod1 访问 Pod2 的通信过程\n发包过程：\nPod1 流量通过 veth-pair 网卡，即从 pod1 的 eth0-\u0026gt;主机侧的 hybrXXX，进入主机网络栈中 根据目的 IP，流量在主机的策略路由匹配到 40000 路由表，并在 40000 路由表中匹配到 Pod2 所在网段需要发往 eth0.vxlan20 网卡的路由规则 eth0.vxlan20 设备的转发表中记录了对端 vtep 的 mac 地址和 remoteip 的对应关系 流量经过 eth0.vxlan20 网卡，封装一个 UDP 的头部 经过查询路由，与本机处于同网段，通过 mac 地址查询获取到对端物理网卡的 mac 地址，经由 Node1 eth0 物理网卡发送 流量从 Node2 eth0 物理网卡进入，并经过 eth0.vxlan20 网卡解封一个 UDP 的头部 根据 39999 路由表，流量从hybrYYY网卡进入 Pod2 容器网络栈，完成发包动作 回包过程：\nPod2 流量通过 veth-pair 网卡，即从 pod2 的 eth0-\u0026gt;主机侧的 hybrYYY，进入主机网络栈中 根据目的 IP，流量在主机的策略路由匹配到 40000 路由表，并在 40000 路由表中匹配到 Pod1 所在网段需要发往 eth0.vxlan20 网卡的路由规则 eth0.vxlan20 设备的转发表中记录了对端 vtep 的 mac 地址和 remoteip 的对应关系 流量经过 eth0.vxlan20 网卡，封装一个 UDP 的头部 经过查询路由，与本机处于同网段，通过 mac 地址查询获取到对端物理网卡的 mac 地址，经由 Node2 eth0 物理网卡发送 流量从 Node1 eth0 物理网卡进入，并经过 eth0.vxlan20 网卡解封一个 UDP 的头部 根据 39999 路由表，流量从 hybrXXX 网卡进入 Pod1 容器网络栈，完成回包动作 **** # 2、VLAN 模式 # 同节点通信 Pod1 访问 Pod2 的通信过程\n发包过程：\nPod1 流量通过 veth-pair 网卡，即从 pod1 的 eth0-\u0026gt;主机侧的 hybrXXX，进入主机网络栈中 根据目的 IP，流量在主机的策略路由匹配到 39999 路由表，并在 39999 路由表中匹配到 Pod2 的路由规则 流量从 hybrYYY 网卡进入 Pod2 容器网络栈，完成发包动作 回包过程：\nPod2 流量通过 veth-pair 网卡，即从 pod2 的 eth0-\u0026gt;主机侧的 hybrYYY，进入主机网络栈中 根据目的 IP，流量在主机的策略路由匹配到 39999 路由表，并在 39999 路由表中匹配到 Pod1 的路由规则 流量从 hybrXXX 网卡进入 Pod1 容器网络栈，完成回包动作 跨节点通信\nPod1 访问 Pod2 的通信过程\n发包过程：\nPod1 流量通过 veth-pair 网卡，即从 pod1 的 eth0-\u0026gt;主机侧的 hybrXXX，进入主机网络栈中 根据目的 IP，流量在主机的策略路由匹配到 10001 路由表，并在 10001 路由表中匹配到 Pod2 相对应的路由规则 根据路由规则，流量从 eth0.20 网卡所对应的 eth0 物理网卡发出，并发往交换机 在交换机上匹配到 pod2 的 MAC 地址，所以将流量发往 Node2 所对应的 eth0 物理网卡 流量被 eth0.20 vlan 网卡接收到，并根据 39999 路由表匹配到的路由，将流量从 hybrYYY 网卡打入 pod2 容器网络栈，完成发包动作 回包过程：\nPod2 流量通过 veth-pair 网卡，即从 pod2 的 eth0-\u0026gt;主机侧的 hybrYYY，进入主机网络栈中 根据目的 IP，流量在主机的策略路由匹配到 10001 路由表，并在 10001 路由表中匹配到 Pod1 相对应的路由规则 根据路由规则，流量从 eth0.20 网卡所对应的 eth0 物理网卡发出，并发往交换机 在交换机上匹配到 pod1 的 MAC 地址，所以将流量发往 Node1 所对应的 eth0 物理网卡 流量被 eth0.20 vlan 网卡接收到，并根据 39999 路由表匹配到的路由，将流量从 hybrXXX 网卡打入 pod1 容器网络栈，完成回包动作 **** # 3、BGP 模式 # 同节点通信 Pod1 访问 Pod2 的通信过程\n发包过程：\nPod1 流量通过 veth-pair 网卡，即从 pod1 的 eth0-\u0026gt;主机侧的 hybrXXX，进入主机网络栈中 根据目的 IP，流量在主机的策略路由匹配到 39999 路由表，并在 39999 路由表中匹配到 Pod2 的路由规则 流量从 hybrYYY 网卡进入 Pod2 容器网络栈，完成发包动作 回包过程：\nPod2 流量通过 veth-pair 网卡，即从 pod2 的 eth0-\u0026gt;主机侧的 hybrYYY，进入主机网络栈中 根据目的 IP，流量在主机的策略路由匹配到 39999 路由表，并在 39999 路由表中匹配到 Pod1 的路由规则 流量从 hybrXXX 网卡进入 Pod1 容器网络栈，完成回包动作 跨节点通信 Pod1 访问 Pod2 的通信过程\n发包过程：\nPod1 流量通过 veth-pair 网卡，即从 pod1 的 eth0-\u0026gt;主机侧的 hybrXXX，进入主机网络栈中 根据目的 IP，流量在主机的策略路由匹配到 10001 路由表，并在 10001 路由表中匹配到默认路由 根据路由，将流量发往 10.0.0.1 所对应的交换机 在交换机上匹配到 pod2 所对应的特定路由，将流量发往 Node2 eth0 物理网卡 流量从 hybrYYY 网卡进入 Pod2 容器网络栈，完成发包动作 回包过程：\nPod2 流量通过 veth-pair 网卡，即从 pod2 的 eth0-\u0026gt;主机侧的 hybrYYY，进入主机网络栈中 根据目的 IP，流量在主机的策略路由匹配到 10001 路由表，并在 10001 路由表中匹配到默认路由 根据路由，将流量发往 10.0.0.1 所对应的交换机 在交换机上匹配到 pod1 所对应的特定路由，将流量发往 Node1 eth0 物理网卡 流量从 hybrXXX 网卡进入 Pod1 容器网络栈，完成回包动作 **** # 02\nCalico\n基本概念：\n纯三层的数据中心网络方案 利用 Linux Kernel 使得宿主机实现 vRouter 来负责数据转发 vRouter 通过 BGP 协议传播路由信息 基于 iptables 还提供了丰富而灵活的网络策略规则 **** # 整体架构 # Felix：运行在每个容器宿主节点上，主要负责配置路由、ACL 等信息来确保容器的联通状态 Brid：把 Felix 写入 Kernel 的路由信息分发到 Calico 网络，保证容器间的通信有效性 etcd：分布式的 Key/Value 存储，负责网络元数据一致性，确保 Calico 网络状态的准确性 RR：路由反射器，默认 Calico 工作在 node-mesh 模式，所有节点互相连接， node-mesh 模式在小规模部署时工作是没有问题的，当大规模部署时，连接数会非常大，消耗过多资源，利用 BGP RR ，可以避免这种情况的发生，通过一个或者多个 BGP RR 来完成集中式的路由分发，减少对网络资源的消耗以及提高 Calico 工作效率、稳定性 **** # 通信路径 # 1、IPIP 模式 # 同节点通信 Pod1 访问 Pod2 的通信过程\n发包过程：\nPod1 流量通过 veth-pair 网卡，即从 pod1 的 eth0-\u0026gt;主机侧的 caliXXX，进入主机网络栈中 根据目的 IP，流量在路由表中匹配到 Pod2 的路由规则 流量从 caliYYY 网卡进入 Pod2 容器网络栈，完成发包动作 回包过程：\nPod2 流量通过 veth-pair 网卡，即从 pod2 的 eth0-\u0026gt;主机侧的 caliYYY，进入主机网络栈中 根据目的 IP，流量在路由表中匹配到 Pod1 的路由规则 流量从 caliXXX 网卡进入 Pod1 容器网络栈，完成回包动作 跨节点通信 Pod1 访问 Pod2 的通信过程\n发包过程：\nPod1 流量通过 veth-pair 网卡，即从 pod1 的 eth0-\u0026gt;主机侧的 caliXXX，进入主机网络栈中\nsrc: pod1IP dst: pod2IP 根据目的 IP，流量在路由表中匹配到将流量转发到 tunl0 网卡上的路由规则\nsrc: pod1IP dst: pod2IP 流量从 tunl0 进行 IPIP 封装（即封装一个 IP 头部），通过 eth0 物理网卡发出\nsrc: Node1IP dst: Node2IP 流量从 Node2 的 eth0 网卡进入 Node2 的主机网络栈\nsrc: Node1IP dst: Node2IP 流量进入 tunl0 进行 IPIP 解包\nsrc: pod1IP dst: pod2IP 流量从 caliYYY 网卡进入 Pod2 容器网络栈，完成发包动作\nsrc: pod1IP dst: pod2IP 回包过程：\nPod2 流量通过 veth-pair 网卡，即从 pod2 的 eth0-\u0026gt;主机侧的 caliYYY，进入主机网络栈中\nsrc: pod2IP dst: pod1IP 根据目的 IP，流量在路由表中匹配到将流量转发到 tunl0 网卡上的路由规则\nsrc: pod2IP dst: pod1IP 流量从 tunl0 进行 IPIP 封装（即封装一个 IP 头部），通过 eth0 物理网卡发出\nsrc: Node2IP dst: Node1IP 流量从 Node1 的 eth0 网卡进入 Node1 的主机网络栈\nsrc: Node2IP dst: Node1IP 流量进入 tunl0 进行 IPIP 解包\nsrc: pod2IP dst: pod1IP 流量从 caliXXX 网卡进入 Pod1 容器网络栈，完成回包动作\nsrc: pod2IP dst: pod1IP 2、BGP 模式 # 同节点通信 Pod1 访问 Pod2 的通信过程\n发包过程：\nPod1 流量通过 veth-pair 网卡，即从 pod1 的 eth0-\u0026gt;主机侧的 caliXXX，进入主机网络栈中 根据目的 IP，流量在路由表中匹配到 Pod2 的路由规则 流量从 caliYYY 网卡进入 Pod2 容器网络栈，完成发包动作 回包过程：\nPod2 流量通过 veth-pair 网卡，即从 pod2 的 eth0-\u0026gt;主机侧的 caliYYY，进入主机网络栈中\n根据目的 IP，流量在路由表中匹配到 Pod1 的路由规则\n流量从 caliXXX 网卡进入 Pod1 容器网络栈，完成回包动作\n跨节点通信 Pod1 访问 Pod2 的通信过程\n发包过程：\nPod1 流量通过 veth-pair 网卡，即从 pod1 的 eth0-\u0026gt;主机侧的 caliXXX，进入主机网络栈中 根据目的 IP，流量在路由表中匹配到 Pod2 相对应网段的路由规则，并从 Node1 eth0 物理网卡发出 流量从 Node2 eth0 物理网卡进入，并从 caliYYY 网卡进入 Pod2 容器网络栈，完成发包动作 回包过程：\nPod2 流量通过 veth-pair 网卡，即从 pod2 的 eth0-\u0026gt;主机侧的 caliYYY，进入主机网络栈中 根据目的 IP，流量在路由表中匹配到 Pod1 相对应网段的路由规则，并从 Node2 eth0 物理网卡发出 流量从 Node1 eth0 物理网卡进入，并从 caliXXX 网卡进入 Pod1 容器网络栈，完成回包动作 **** # 03\nBifrost\n整体架构 # veth0-bifrXXX：bifrost 对于 macvlan 实现 service 访问的一套解决方案，通过 veth-pair 网卡完成在主机网络栈中的 kube-proxy + iptables 对于服务流量转化为对 pod 的访问 eth0：容器内的 eth0 网卡为主机 vlan 子网卡对应的 macvlan 网卡 **** # 通信路径 # 1、MACVLAN 模式 # 同节点同 vlan 通信 Pod1 访问 Pod2 的通信过程\n发包过程：\nPod1 流量通过 macvlan 网卡，即 pod1 的 eth0 走二层网络进入 eth0-10 vlan 子网卡 由于 macvlan 开启 bridge 模式，能够匹配到 pod2 的 MAC 地址 流量从 eth0-10 vlan 子网卡进入 Pod2 容器网络栈，完成发包动作 回包过程：\nPod2 流量通过 macvlan 网卡，即 pod2 的 eth0 走二层网络进入 eth0-10 vlan 子网卡 由于 macvlan 开启 bridge 模式，能够匹配到 pod1 的 MAC 地址 流量从 eth0-10 vlan 子网卡进入 Pod1 容器网络栈，完成回包动作 同节点跨 vlan 通信 Pod1 访问 Pod2 的通信过程\n发包过程：\nPod1 流量通过 macvlan 网卡，即 pod1 的 eth0 走默认路由（网关为 5.0.0.1）进入 eth0-5 vlan 子网卡 由于在 eth0-5 上找到网关 5.0.0.1 的 MAC 地址，所以将流量从 eth0 物理网卡出打到交换机上 流量在交换机上匹配到了 pod2 的 MAC 地址 流量进入 Pod2 所在宿主机的物理网卡，并进入相对应的 eth0-10 vlan 子网卡 流量从 eth0-10 vlan 子网卡进入 Pod2 容器网络栈，完成发包动作 回包过程：\nPod2 流量通过 macvlan 网卡，即 pod2 的 eth0 走默认路由（网关为 10.0.0.1）进入 eth0-10 vlan 子网卡 由于在 eth0-10 上找到网关 10.0.0.1 的 MAC 地址，所以将流量从 eth0 物理网卡出打到交换机上 流量在交换机上匹配到了 pod1 的 MAC 地址 流量进入 Pod1 所在宿主机的物理网卡，并进入相对应的 eth0-5 vlan 子网卡 流量从 eth0-5 vlan 子网卡进入 Pod1 容器网络栈，完成回包动作 跨节点通信 Pod1 访问 Pod2 的通信过程\n发包过程：\nPod1 流量通过 macvlan 网卡，即 pod1 的 eth0 走默认路由（网关为 5.0.0.1）进入 eth0-5 vlan 子网卡 由于在 eth0-5 上找到网关 5.0.0.1 的 MAC 地址，所以将流量从 eth0 物理网卡出打到交换机上 流量在交换机上匹配到了 pod2 的 MAC 地址 流量进入 Pod2 所在宿主机的物理网卡，并进入相对应的 eth0-10 vlan 子网卡 流量从 eth0-10 vlan 子网卡进入 Pod2 容器网络栈，完成发包动作 回包过程：\nPod2 流量通过 macvlan 网卡，即 pod2 的 eth0 走默认路由（网关为 10.0.0.1）进入 eth0-10 vlan 子网卡 由于在 eth0-10 上找到网关 10.0.0.1 的 MAC 地址，所以将流量从 eth0 物理网卡出打到交换机上 流量在交换机上匹配到了 pod1 的 MAC 地址 流量进入 Pod1 所在宿主机的物理网卡，并进入相对应的 eth0-5 vlan 子网卡 流量从 eth0-5 vlan 子网卡进入 Pod1 容器网络栈，完成回包动作 **** # 04\n面临的问题及未来发展\nAliware\n01\nIPv4/IPv6 双栈\n# 背景 # IP 作为互联网最基础的要素，是为计算机网络相互连接进行通信而设计的协议，正是因为有了 IP 协议，因特网才得以迅速发展成为世界上最大的、开放的计算机通信网络。IP 协议随着互联网的发展，产生了 IPv4 和 IPv6 两种协议：\nIPv4\nIPv4 是互联网协议第四版，是计算机网络使用的数据报传输机制，此协议是第一个被广泛部署的 IP 协议。每一个连接 Internet 的设备（不管是交换机、PC 还是其他设备），都会为其分配一个唯一的 IP 地址，如 192.149.252.76，如下图所示，IPv4 使用 32 位（4 字节）地址，大约可以存储 43 亿个地址，但随着越来越多的用户接入到 Internet，全球 IPv4 地址已于 2019 年 11 月已全数耗尽。这也是后续互联网工程任务组（IEIF）提出 IPv6 的原因之一。 IPv6\nIPv6 是由 IEIF 提出的互联网协议第六版，用来替代 IPv4 的下一代协议，它的提出不仅解决了网络地址资源匮乏问题，也解决了多种接入设备接入互联网的障碍。IPv6 的地址长度为 128 位，可支持 340 多万亿个地址。如下图，3ffe:1900:fe21:4545:0000:0000:0000:0000，这是一个 IPv6 地址，IPv6 地址通常分为 8 组，4 个十六进制数为一组，每组之间用冒号分隔。 IPv4 占主流，IPv6 还未兴起时，主要面临的问题：\nIPv4 地址数量已经不再满足需求，需要 IPv6 地址进行扩展 随着国内下一代互联网发展政策的明确，客户数据中心需要使用 IPv6 来符合更严格的监管 **** # 现状 # **** hybridnet calico IPIP calico BGP bifrost 是否支持IPv6/双栈 是 否 是 否 calico IPIP 不支持 IPv6 的原因:\nipip 是普通的 IPIP 隧道，就是在报文的基础上再封装成一个 IPv4 报文，所以不支持 IPv6 的封包 **** # 02\n多网卡（多通信机制）\n背景 # 通常情况下在 K8s 中，一个 Pod 只有一个接口，即单网卡，用于集群网络中 pod 和 pod 通信。而 Pod 需要和异构网络进行通信时，可选择在 Pod 中建立多个接口，即多网卡。\n目前的问题：\n部分客户真实 IP 资源紧张，导致无法全部使用 underlay 方案 客户希望把 UDP 网络和 TCP 网络分开，导致如基于 TCP 协议的网络模型无法独立存在于 UDP 网络中 **** # 现状 # 目前实现多网卡的方案，大致两种：\n在单一 CNI 调用 IPAM 时，通过 CNI config 配置生成相对应的网卡并分配合适的 IP 资源 通过元 CNI 依次调用各个 CNI 来完成相对应的网卡并分配合适的 IP 资源，如 multus 方案 # **** # 03\n网络流量管控\n背景 # 通常在数据中心中，我们将其网络流量分为两种类型，一种是数据中心外部用户和内部服务器之间交互的流量，这样的流量称作南北向流量或者纵向流量；另外一种就是数据中心内部服务器之间交互的流量，也叫东西向流量或者横向流量。\n那么在容器云范畴内，我们将东西向流量定义为集群内宿主机和容器、容器间或宿主机间的流量，南北向流量为容器云外部与容器云内部之间交互的流量。\n目前的问题：\n传统防火墙在容器云的东西向场景下，难以起到流量管控，需要提供服务间或容器间流量管控的能力 **** # 现状 # **** calico cillum bifrost-商用版 技术基础 iptables ebpf ebpf 适配性 三层路由且流量经过主机网络栈 二层且满足cillum通信方式 主流CNI插件 **** # 参考资料 # 1、calico vxlan ipv4 overlay组网跨主机通信分析\nhttps://www.jianshu.com/p/5edd6982e3be\n2、Qunar容器平台网络之道：Calico\nhttp://dockone.io/article/2434328\n3、最好的vxlan介绍\nhttps://www.jianshu.com/p/cccfb481d548\n4、揭秘 IPIP 隧道\nhttps://morven.life/posts/networking-3-ipip/\n5、BGP基础知识\nhttps://blog.csdn.net/qq_38265137/article/details/80439561\n6、VLAN基础知识\nhttps://cshihong.github.io/2017/11/05/VLAN%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/\n7、Overlay和Underlay网络协议区别及概述讲解\nhttps://www.cnblogs.com/fengdejiyixx/p/15567609.html#%E4%BA%8Cunderlay%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B\n8、东西向流量牵引方案小结\nhttp://blog.nsfocus.net/east-west-flow-sum/\n9、容器网络接口（CNI）\nhttps://jimmysong.io/kubernetes-handbook/concepts/cni.html\n10、K8s 网络之深入理解 CNI\nhttps://zhuanlan.zhihu.com/p/450140876\n"},{"id":173,"href":"/docs/%E5%BF%AB%E9%80%9F%E6%8C%87%E5%8D%97-kuai-su-zhi-nan/","title":"快速指南 2023-09-28 15:28:25.71","section":"Docs","content":" 快速指南 # 本文档适用于kubeasz 3.3.1以上版本，部署单节点集群(aio)，作为快速体验k8s集群的测试环境。 # 1.基础系统配置 # 准备一台虚机配置内存2G/硬盘30G以上 # 最小化安装Ubuntu 16.04 server或者CentOS 7 Minimal # 配置基础网络、更新源、SSH登录等 # 注意: 确保在干净的系统上开始安装，不能使用曾经装过kubeadm或其他k8s发行版的环境 # 2.下载文件 # 下载工具脚本ezdown，举例使用kubeasz版本3.5.0 # export release=3.5.0 wget https://github.com/easzlab/kubeasz/releases/download/${release}/ezdown chmod +x ./ezdown 使用工具脚本下载（更多关于ezdown的参数，运行./ezdown 查看） 下载kubeasz代码、二进制、默认容器镜像\n# 国内环境 ./ezdown -D # 海外环境 #./ezdown -D -m standard 【可选】下载额外容器镜像（cilium,flannel,prometheus等）\n# 按需下载 ./ezdown -X flannel ./ezdown -X prometheus ... 【可选】下载离线系统包 (适用于无法使用yum/apt仓库情形)\n./ezdown -P 上述脚本运行成功后，所有文件（kubeasz代码、二进制、离线镜像）均已整理好放入目录/etc/kubeasz\n/etc/kubeasz 包含 kubeasz 版本为 ${release} 的发布代码 /etc/kubeasz/bin 包含 k8s/etcd/docker/cni 等二进制文件 /etc/kubeasz/down 包含集群安装时需要的离线容器镜像 /etc/kubeasz/down/packages 包含集群安装时需要的系统基础软件 3.安装集群 # 容器化运行 kubeasz ./ezdown -S 使用默认配置安装 aio 集群 docker exec -it kubeasz ezctl start-aio # 如果安装失败，查看日志排除后，使用如下命令重新安装aio集群 # docker exec -it kubeasz ezctl setup default all 4.验证安装 # $ source ~/.bashrc $ kubectl version # 验证集群版本 $ kubectl get node # 验证节点就绪 (Ready) 状态 $ kubectl get pod -A # 验证集群pod状态，默认已安装网络插件、coredns、metrics-server等 $ kubectl get svc -A # 验证集群服务状态 登录 dashboard可以查看和管理集群，更多内容请查阅dashboard文档 5.清理 # 以上步骤创建的K8S开发测试环境请尽情折腾，碰到错误尽量通过查看日志、上网搜索、提交issues等方式解决；当然你也可以清理集群后重新创建。\n在宿主机上，按照如下步骤清理\n清理集群 docker exec -it kubeasz ezctl destroy default\ndocker exec -it kubeasz ezctl destroy default 重启节点，以确保清理残留的虚拟网卡、路由等信息\n"},{"id":174,"href":"/docs/%E6%88%91%E5%8F%AA%E6%83%B3%E5%81%9A%E6%8A%80%E6%9C%AF-%E8%B5%B0%E6%8A%80%E6%9C%AF%E8%B7%AF%E7%BA%BF-wo-zhi-xiang-zuo-ji-shu-zou-ji-shu-lu-xian/","title":"我只想做技术 走技术路线 2024-08-02 17:43:31.897","section":"Docs","content":"“我对技术感兴趣，我只想做技术，走技术路线。”\n这句话是不是很听过呢？请不要误会，这句话本身没有问题，但是说出这句话的软件工程师中，十有八九有一个误区，就是高估了技术本身在个人职业生涯中，起作用的占比。而我，曾经是其中之一。\n我多次听到来自家长这样的评论，说是他/她的孩子不善言辞，缺乏沟通技巧，然而罗辑思维缜密，对于软件技术充满热情，想做一个出色的程序员。\n我觉得，这样的言论，既对又不对。对的是，如果刚踏入职场，相当一个优秀的程序员，我觉得上述的优缺点，都很 “契合”，或者说，也许只需要学好技术，把确定的任务做完、做好，就能找得到工作的褒奖，就能胜任岗位。但是，随着你继续在职业生涯的道路上向上突破，基本坚定地走着技术通道（譬如我），上述能力的缺失，而造成的 “短板效应”，会越来越明显。\n个中的原因，其实并不难理解。软件工程师，不是搞学术，而是搞工程，而工程能力，是一个非常复杂的多面体。比如提到的沟通合作的能力，是根本就绕不过去的。职场这些年来，我已经无数次看到那些所谓的技术 “牛人”，有着优秀的独立问题分析与解决的技能，却总是在贡献与晋升方面迟人一步，甚至多年过去，也难以在职业生涯的路线上，迈入更高的台阶。\n除了沟通，再举一个例子，管理能力。我知道很多铁了心要做技术的程序员，也包括曾经的我在内，是对 “管理” 这个词有着不由自主的排斥。总对公司那些技术优秀的工程师，却 “转管理” 嗤之以鼻。\n某些总失偏颇的媒体也报道，“国外”（他们口中，中国以外的世界仿佛没有太大的分别，统一以 “国外” 概括之）五十岁的工程师还在写代码，六十岁的程序员还在发光发热……我理解这样的想法，但是这里被忽略掉的一个事实是，即便在那样技术通道被极大地保护的环境里，即便继续走坚持技术路线，坚持写代码，随着职位的升高，管理的工作就是不可避免地越来越多的，这是软件之所以为 “工程”、而非 “工学” 的一个特性。\n换言之，“沟通” 也好，“管理” 也罢，即便对于技术人来说，也都是逃不掉的。你要跨团队合作，你要带着一票人一起攻克项目，团队能达成的事情，在重要性上往往要盖过自己在努力啃着的特定的问题。\n当然，工程这个多面体中，除了 “硬技术”，必备的 “软能力” 有很多，我只是拿这两个方面举了例子。那么，再说一条我认为比较重要的能力吧——对于实际问题、模糊问题的挖掘和剖析能力。\n事实上，这一条也是我参与的软件工程师面试中，非常重要的一条考察项。实际问题总是模糊的，先要把问题搞清楚，抽象成一个软件可解的问题，再使用各种软件的工具（代码）来解决问题。\n你看，这里面有两个步骤，而在我们所熟悉的教育体系中，后者被极大地强化——算法、数据结构、库、平台……这些涉及的技能领域，都在后者这个 “解决一个已经经过抽象了的问题” 上面；而对于前一步骤的学习，往往是不足的。\n什么才是 “模糊的实际问题”？随便举个例子：\n我们公司内部有大约 100 个服务（service），提供不同的功能，分别由不同团队维护。现在需要你带领一个小团队，来设计实现一套监控系统，统一监控管理这些服务的健康状况。你打算怎么做？\n这就是一个非常模糊的实际问题，不是一个算法问题，也不是一个传统意义上的系统设计问题。但这是一个真正的、实际的 “软件问题”。\n最后，我想说的是。日常的工作中，也许我们一猛子下去扎得很深，但是我们还是需要时不时地跳出每天琐碎的条条框框，站高一点审视一下，自己在职业生涯路线中的位置，尽量带着技术人的决心和热情，但不要带着技术人的封闭与迂腐。\n作者介绍\n熊燚，笔名四火，现于西雅图 Oracle 任首席软件工程师一职，负责研发云基础设施的分布式工作流引擎。曾先后任职于华为、亚马逊，做过多种类型的研发工作，从大小网站到高可用服务，从数据平台到可视化系统，他带领团队攻克过数个项目难关，在全栈之路上具有丰富的实战经验。\n"},{"id":175,"href":"/docs/%E6%90%9E%E6%87%82k8s%E9%89%B4%E6%9D%83-gao-dong-k8s-jian-quan/","title":"搞懂K8S鉴权 2024-04-03 15:11:54.885","section":"Docs","content":" 前言 # 本文介绍K8s中的鉴权模块。对其4种鉴权模式都进行了概述讲解。结合例子着重对大家日常中使用最多的RBAC鉴权模式进行了说明。\n鉴权概述 # 《搞懂K8s认证》中，我们提到不论是通过kubectl客户端还是REST请求访问K8s集群，最终都需要经过API Server来进行资源的操作并通过Etcd。整个过程如下图1所示，可以分成4个阶段：\n图1 K8s API请求访问过程\n请求发起方进行K8s API请求，经过Authentication（认证）、Authorization（鉴权）、AdmissionControl（准入控制）三个阶段的校验，最后把请求转化为对K8s对象的变更操作持久化至etcd中。\n其中认证主要解决的是请求来源能否访问的问题。即通过了认证，那么可以认为它是一个合法的请求对象。那么如何去决定请求对象能访问哪些资源以及对这些资源能进行哪些操作，便是鉴权所要完成的事情了。\n鉴权的最终目的，是区分请求对象，限定操作的影响范围，让其使用最小的权限完成自己所要进行操作，从而进一步保证安全。权限控制的划分方式有许多种，K8s中提供了4种鉴权模式，分别为Node、ABAC、RBAC和Webhook。\n默认情况下，我们可以从/etc/kubernates/manifests/kube-apiserver.yaml文件中查看apiserver启动时认证模式，片段如图2所示：\n图2 kube-apiserver的认证参数\n其中可以使用的参数如表1所示：\n表1 鉴权模块参数标识表\n参数配置 含义 一般的使用场景 \u0026ndash;authorization-mode=ABAC 使用基于属性的访问控制(ABAC) 根据用户的用户名或者组名来控制其对集群资源的访问权限，适用于较小的组织或开发团队 \u0026ndash;authorization-mode=RBAC 使用基于角色的访问控制(RBAC) 自定义ServiceAccount，绑定资源根据角色来控制资源的访问权限，适用于较大型的组织或者开发运维团队 \u0026ndash;authorization-mode=Webhook 使用HTTP回调模式，允许你使用远程REST端点管理鉴权 将鉴权角色交给外部服务进行处理，根据自身需求，定制和扩展鉴权策略，如自定义Webhook鉴权模块对跨云平台的应用进行集中的的访问控制 \u0026ndash;authorization-mode=Node 针对kubelet发出的API请求执行鉴权 验证节点身份，确保只有经过身份验证且具有所需权限的Node才能连接到K8s集群 \u0026ndash;authorization-mode=AlwaysDeny 阻止所有请求 一般仅用作测试 \u0026ndash;authorization-mode=AlwaysAllow 允许所有请求 不需要API请求进行鉴权的场景 如图2所示，可以同时配置多个鉴权模块（多个模块之间使用逗号分隔），排在靠前的模块优先执行，任何模式允许或拒接请求，则立即返回该决定，并不会与其他鉴权模块协商。\nNode鉴权 # Node鉴权是一种特殊用途的鉴权模式，旨在对kubelet发出API请求进行授权。Node鉴权允许kubelet执行API的操作分成读和写两部分。读取操作控制范围为：services、endpoints、nodes、pods以及绑定到kubelet节点 Pod相关的secret、configmap、pvc和持久卷。写入操作的范围主要是节点和节点状态、Pod和Pod状态以及事件，若要限制kubelet只能修改自己的节点，则还需要在Apiserver启动时，开启NodeRestriction准入插件（见图2第二个红框）。\n开启Node鉴权模块后，kubellet为了获取授权，必须使用一个特定规则的凭据，如图3所示：\n图3 kubelet的证书凭据\n从图中我们看到，kubelet使用了一个证书凭据，其中O=system:nodes表示其所在组，CN=system:node:paas-cnp-k8s-kce-01表示其用户名，满足了Node鉴权模块要求的组名必须为system:nodes，用户名必须为system:node:\u0026lt;nodeName\u0026gt;的要求。其中\u0026lt;nodeName\u0026gt;默认由hostname或kubelet --hostname-override选项提供指定，其必须与kubelet提供的主机名称精确匹配。\nsystem:nodes是K8s的内置用户组，我们可以通过其默认的ClusterRoleBinding，如图4所示：\n图4 system:nodes的ClusterRoleBinding内容\n我们可以发现，它指示指向了system:node这个ClusterRole，并没有subjects的内容，即其没有绑定system:node:paas-cnp-k8s-kce-01用户也没有绑定system:nodes组。其正是因为K8s基于 Node鉴权模块来限制kubelet只能读取和修改本节点上的资源，并不是使用 RBAC来鉴权（涉及到部分RBAC的内容，下文会进行详解）。\nABAC鉴权 # 基于属性的访问控制，K8s中可以表述将访问策略授予用户或者组。与RBAC不同的点在于，其策略是由任何类型的属性（用户属性、资源属性、对象、环境等）进行描述。\n启用ABAC模式，类似于图2需要在apiserver启动时指定--authorization-mode=ABAC以及--authorization-policy-file=\u0026lt;策略文件路径\u0026gt; 。图5是K8s官网给出的一个策略样例文件：\n图5 ABAC策略实例文件内容\n文件中每行都是一个JSON对象，其中版本控制属性\u0026quot;apiVersion\u0026quot;: \u0026quot;abac.authorization.kubernetes.io/v1beta1\u0026quot;和\u0026quot;kind\u0026quot;: \u0026quot;Policy\u0026quot;可以理解成固定写法，是被K8s本身所使用，便于以后进行版本控制和转换。spec部分，其中user，来自于--token-auth-file的用户字符串，其指定的user名称必须与这个文件中的字符串相匹配；group，必须与经过身份验证的用户的一个组匹配， system:authenticated 匹配所有经过身份验证的请求。 system:unauthenticated 匹配所有未经过身份验证的请求。其他的匹配属性，主要描述被访问的访问，分为资源属性和非资源属性（其具体定义可参见官网）。readonly，主要限制是否只允许对资源进行 get、list 和 watch 操作，非资源属性只进行get操作。\n例如：\n{\u0026#34;apiVersion\u0026#34;: \u0026#34;abac.authorization.kubernetes.io/v1beta1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Policy\u0026#34;, \u0026#34;spec\u0026#34;: {\u0026#34;user\u0026#34;: \u0026#34;alice\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;resource\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;apiGroup\u0026#34;: \u0026#34;*\u0026#34;}} 表示用户alice可以对所有namespace下的所有资源进行任何操作；\n{\u0026#34;apiVersion\u0026#34;: \u0026#34;abac.authorization.kubernetes.io/v1beta1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Policy\u0026#34;, \u0026#34;spec\u0026#34;: {\u0026#34;user\u0026#34;: \u0026#34;bob\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;projectCaribou\u0026#34;, \u0026#34;resource\u0026#34;: \u0026#34;pods\u0026#34;, \u0026#34;readonly\u0026#34;: true}} 表示用户bob只能对namespace为projectCaribou的Pod进行list、get和watch操作。\n对于ServiceAccount的ABAC管控。Service Account会自动生成一个ABAC用户名，其格式为：system:serviceaccount:\u0026lt;namespace\u0026gt;:\u0026lt;serviceaccountname\u0026gt;，创建新的命名空间时，K8s会为我们在当前namespace下创建一个名为default的ServiceAccount，如果现在假定我们需要namespace为test名为default的ServiceAccount具有该namespace下的全部权限，则可以添加这样一条规则：\n{\u0026#34;apiVersion\u0026#34;:\u0026#34;abac.authorization.kubernetes.io/v1beta1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Policy\u0026#34;,\u0026#34;spec\u0026#34;:{\u0026#34;user\u0026#34;:\u0026#34;system:serviceaccount:test:default\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;test\u0026#34;,\u0026#34;resource\u0026#34;:\u0026#34;*\u0026#34;,\u0026#34;apiGroup\u0026#34;:\u0026#34;*\u0026#34;}} 我们发现，ABAC虽然对用户访问进行了一定的划分，但是如果添加了新的ABAC策略，则需要重启API Server以使其生效，资源的访问权限细粒度也只控制到了是否只读，当用户规模增大时，这是远远不能满足需求的。对于较大组织的团队进行更加细粒度的管控，就需要使用到K8s的RBAC鉴权模块了。\nRBAC鉴权 # 启用RBAC鉴权，可以如图2所示，在\u0026ndash;authorization-mode的启动参数中包含RBAC即可。相对于其他访问控制方式，RBAC具有如下优势：\n对集群中的资源和非资源权限均有完整的覆盖。 RBAC由几个API对象完成。同其他API对象一样，可以用kubectl或API进行操作和调整，不必重新启动ApiServer。 K8s的RBAC主要阐述解决了主体（subject）是什么角色的问题。其中主体就是K8s中的用户，包含了常规用户（User、Group，平时常用的kubectl命令都是普通用户执行的）和服务账户（ServiceAccount，主要用于集群进程与K8s的API通信）。角色决定了能够对资源进行什么样的操作，RBAC中我们可以定义namespace级别的Role也可以定义集群级别的ClusterRole。最后K8s再通过RoleBinding和ClusterRoleBinding把用户和角色的关系进行关联绑定，其中RoleBinding既可以绑定Role也可以绑定ClusterRole，ClusterRole指定绑定ClusterRole，最终实现了不同用户对不同资源进行特定操作的控制。Role针对特定的命名空间，ClusterRole在整个集群范围内都生效，所以后者访问范围也更大，能够限制访问Node等资源。其关系图如图6所示：\n图6 RBAC整体关系图\n现在假设你所在部门使用名为rbac-team的命名空间工作，现在要为部署流水线创建一个新的ClusterRole名为deployment-clusterrole，且仅允许创建Deployment、StatefulSet、DaemonSet三种资源，同时在rbac-team这个命名空间下创建一个新的ServiceAccount名为cicd-test使用这个ClusterRole。K8s中我们可以使用kubectl客户端或者API资源的方式来实现这个RBAC的功能。\nKubectl客户端方式。 # 第一步，我们可以先执行命令：kubectl create clusterrole deployment-clusterrole --verb=create --resource=deployments,statefulsets,daemonsets 创建所需的ClusterRole，其中--verb代表可以执行的操作，--reasource代表授权操作的资源范围； 第二步，执行命令：kubectl -n rbac-team create serviceaccount cicd-test创建要求的ServiceAccount； 第三步，限于rbac-test中绑定所需权限，执行命令：kubectl -n rbac-team create rolebinding cicd-rolebinding --clusterrole=deployment-clusterrole --serviceaccount=rbac-team:cicd-test，对新创建的ServiceAccount与我们的ClusterRole进行绑定。 Api资源的方式。 # 第一步，准备ClusterRole的资源描述，创建名为cr-deployment.yaml的文件，如图7所示： 图7 deployment-clusterrole资源\n其中verbs用于限定可执行的操作，可配置参数有：“get”, “list”, “watch”, “create”, “update”, “patch”, “delete”, “exec”。resources用于限定可访问的资源范围，可配置参数有：“services”, “endpoints”, “pods”,“secrets”,“configmaps”,“crontabs”,“deployments”,“jobs”,“nodes”,“rolebindings”,“clusterroles”,“daemonsets”,“replicasets”,“statefulsets”,“horizontalpodautoscalers”,“replicationcontrollers”,“cronjobs”。apiGroups用于划分访问的资源组，可配置参数有：“”,“apps”, “autoscaling”, “batch”。\n第二步，准备ServiceAccount的资源描述，创建名为sa-cicd.yaml的文件，如图8所示： 图8 sa-cicd资源\n第三步，准备RoleBinding的资源描述，创建名为rb-cicd.yaml的文件，如图9所示： 图9 rolebinding资源\n将前面两步的clusterrole和ServiceAccount进行绑定，其中subjects指明了被授权的主体，当被授权对象是User或者Group时，可以使用类似如下片段subjects即可（Group的话只需要将kind的值改为Group）:\n- kind: User name: \u0026#34;test-user\u0026#34; apiGroup: rbac.authorization.k8s.io 第四步，使用kubectl -f 分别应用上述资源，完成主体（subject）、角色的创建与绑定。 当K8s的进程资源需要使用上述身份凭证进行权限划分时，只需要在资源声明文件中进行配置即可，如图9所示：\n图10 pod资源使用ServiceAccount\n为特定应用的Service Account赋权进行权限管控也是最安全的最值得推荐的。\nAPIServer默认的ClusterRole和ClusterRoleBinding对象，其中很多是以\u0026quot;system:\u0026ldquo;为前缀的，对这些对象的改动可能造成集群故障。例如Node鉴权模块提到的system:node，这个ClusterRole为kubelet定义了权限，如果这个集群角色被改动了，kubelet就会停止工作。\n所有默认的ClusterRole和RoleBinding都会用标签kubernetes.io/bootstrapping: rbac-defaults进行标记。\nWebhook鉴权 # 启用Webhook模式的方式与ABAC模式相似，我们需要在apiserver启动时指定--authorization-mode=ABAC以及一个HTTP配置文件用以指定策略吗，可以通过参数--authorization-webhook-config-file=\u0026lt;策略文件路径\u0026gt;，策略文件配置使用的是kubeconfig文件的格式。users部分表示APIServer的webhook，cluster代表着远程服务。图11部分是K8s官网给出的一个示例。\n图11 Webhook的策略配置示例\n当进行认证时，API服务器会生成一个JSON对象来描述这个动作，对于一个资源类型的请求，其内容主要包含了需要被访问的资源或请求特征，如图12所示：\n图12 资源类型的Webhook鉴权请求示例\n从图12中我们可以看到，这个请求是User名为jane，Group为group1、gourp2为主体，请求的资源为命名空间为kittensandpoines下的pods资源，资源所属group为unicorn.example.org，请求的执行操作为get这个资源信息。若我们的Webhook服务URL同意了这个请求，则可以返回如图13格式的响应体。\n图13 Webhook请求同意响应示例\n从图13中我们可以看出,我们只需要填充SubjectAccessReview的status对象信息的值即可。若要拒绝请求，则可以返回图14或图15的响应。\n图14 Webhook请求拒绝响应示例1\n图15 Webhook请求拒绝响应示例2\n图14和图15都是对请求进行拒绝，都给出了拒绝通过的原因，差别在于图14的拒绝响应中增加了denied:true的信息，其表示当我们配置多了多个鉴权模块的时候，当这个响应返回时立即拒绝请求。（默认当存在多个鉴权模块，Webhook拒绝后可以再通过其他模块进行鉴权，只有当其他模块也不通过或者不存在时，才禁止访问）\n对于非资源的路径访问，生成的JSON请求示例如图16所示：\n图16 非资源的路径请求的Webhook鉴权请求示例\n总结 # 鉴权主要是解决“谁可以对什么资源进行哪些操作”的问题。K8s的鉴权模块主要有4个，发生在鉴权认证阶段的第二阶段。Node模块主要针对kubelet需要访问APIServer的场景；ABAC主要针对User和Group的常规用户，控制力度较粗；RBAC即可以用于常规用户也可以针对ServiceAccount进行管控，且管控力度十分细腻，是常用的K8s鉴权方式；Webhook主要用于定制自定义的鉴权逻辑，可用于跨云的集中式鉴权。\n参考文献： # https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/node/\nhttps://kubernetes.io/zh-cn/docs/reference/access-authn-authz/abac/\nhttps://kubernetes.io/zh-cn/docs/reference/access-authn-authz/rbac/\nhttps://kubernetes.io/zh-cn/docs/reference/access-authn-authz/webhook/ https://www.cnblogs.com/maiblogs/p/16271997.html\nhttps://cloud.tencent.com/developer/article/2149852\n"},{"id":176,"href":"/docs/%E6%90%AD%E4%B8%AA%E6%97%A5%E5%BF%97%E6%89%8B%E6%9C%BA%E7%B3%BB%E7%BB%9F%E4%B8%8D%E9%A6%99%E5%90%97-da-ge-ri-zhi-shou-ji-xi-tong-bu-xiang-ma/","title":"搭个日志手机系统不香吗 2024-08-02 17:47:58.794","section":"Docs","content":"一、ELK日志系统\n经典的ELK架构或现被称为Elastic Stack。Elastic Stack架构为Elasticsearch + Logstash + Kibana + Beats的组合：\nBeats负责日志的采集 Logstash负责做日志的聚合和处理 ES作为日志的存储和搜索系统 Kibana作为可视化前端展示 整体架构图：\n二、EFK日志系统\n容器化场景中，尤其k8s环境，用户经常使用EFK架构。F代表Fluent Bit，一个开源多平台的日志处理器和转发器。Fluent Bit可以：\n让用户从不同来源收集数据/日志 统一并发到多个目的地 完全兼容Docker和k8s环境 三、PLG日志系统\n1.Prometheus+k8s日志系统\n2.PLG\nGrafana Labs提供的另一个日志解决方案PLG逐渐流行。PLG架构即Promtail + Loki + Grafana的组合：\nGrafana，开源的可视化和分析软件，允许用户查询、可视化、警告和探索监控指标。Grafana主要提供时间序列数据的仪表板解决方案，支持超过数十种数据源。\nGrafana Loki是一组可以组成一个功能齐全的日志堆栈组件，与其它日志系统不同，Loki只建立日志标签的索引而不索引原始日志消息，而是为日志数据设置一组标签，即Loki运营成本更低，效率还提高几个数量级。\n3.Loki设计理念\nPrometheus启发，可实现可水平扩展、高可用的多租户日志系统。Loki整体架构由不同组件协同完成日志收集、索引、存储等。\n各组件如下，Loki’s Architecture深入了解。Loki就是like Prometheus, but for logs。\nPromtail是一个日志收集的代理，会将本地日志内容发到一个Loki实例，它通常部署到需要监视应用程序的每台机器/容器上。Promtail主要是用来发现目标、将标签附加到日志流以及将日志推送到Loki。截止到目前，Promtail可以跟踪两个来源的日志：本地日志文件和systemd日志（仅支持AMD64架构）。\n四、PLG V.S ELK\n1.ES V.S Loki\nELK/EFK架构确实强，经多年实际环境验证。存储在ES中的日志通常以非结构化JSON对象形式存储在磁盘，且ES为每个对象都建索引，以便全文搜索，然后用户可特定查询语言搜索这些日志数据。\n而Loki数据存储解耦：\n既可在磁盘存储 也可用如Amazon S3云存储系统 Loki日志带有一组标签名和值，只有标签对被索引，这种权衡使它比完整索引操作成本更低，但针对基于内容的查询，需通过LogQL再单独查询。\n2.Fluentd V.S Promtail\n相比Fluentd，Promtail专为Loki定制，它可为运行在同一节点的k8s Pods做服务发现，从指定文件夹读取日志。Loki类似Prometheus的标签方式。因此，当与Prometheus部署在同一环境，因为相同的服务发现机制，来自Promtail的日志通常具有与应用程序指标相同的标签，统一标签管理。\n3.Grafana V.S Kibana\nKibana提供许多可视化工具来进行数据分析，高级功能如异常检测等机器学习功能。Grafana针对Prometheus和Loki等时间序列数据打造，可在同一仪表板上查看日志指标。\n"},{"id":177,"href":"/docs/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%AF%B4%E6%98%8E-cao-zuo-xi-tong-shuo-ming/","title":"操作系统说明 2023-09-28 15:31:37.694","section":"Docs","content":" 操作系统说明 # 目前发现部分使用新内核的linux发行版，k8s 安装使用 cgroup v2版本时，有时候安装会失败，需要删除/清理集群后重新安装。已报告可能发生于 Alma Linux 9, Rocky Linux 9, Fedora 37；建议如下步骤处理：\n1.确认系统使用的cgroup v2版本 stat -fc %T /sys/fs/cgroup/ cgroup2fs 2.初次安装时kubelet可能启动失败，日志报错类似：err=\u0026ldquo;openat2 /sys/fs/cgroup/kubepods.slice/cpu.weight: no such file or directory\u0026rdquo;\n3.建议删除集群然后重新安装，一般能够成功\n# 删除集群 dk ezctl destroy xxxx # 重启 reboot # 启动后重新安装 dk ezctl setup xxxx all Debian # Debian 11：默认可能没有安装iptables，使用kubeasz 安装前需要执行： apt update apt install iptables -y Alibaba # Alibaba Linux 3.2104 LTS：安装前需要设置如下： # 修改使用dnf包管理 sed -i \u0026#39;s/package/dnf/g\u0026#39; /etc/kubeasz/roles/prepare/tasks/redhat.yml openSUSE # openSUSE Leap 15.4：需要安装iptables zypper install iptables ln -s /usr/sbin/iptables /sbin/iptables "},{"id":178,"href":"/docs/%E6%96%87%E5%AD%A6%E7%9A%84%E6%95%85%E4%B9%A1-wen-xue-de-gu-xiang/","title":"文学的故乡 2024-04-03 14:40:45.926","section":"Docs","content":"奇怪，央视怎么啥神仙阵容都能搞到\n仿佛那里有一个童年时的自己，在那里思考的是成年的自己，风景像桥梁一样将两者相连，我认为这就是文学的作用。有一个叫莫言的少年，在这片风景中悲伤过，痛苦过，害怕过，真爱过。\n常有人问Sir。\n想提高电影审美品味，有没有特别招数。\n有是有，就怕你嫌Sir暴露年龄——\n看文学作品。\n因为有时分电影之高下的，并不是故事，而是“叙事”。\n好比杜琪峰最喜欢自己的《柔道龙虎榜》（Sir也好喜欢），可很多观众觉得，也就so so啊，不就是一个小故事吗？ # 对，它的高级之处，就是杜氏叙事。而好的电影叙事，能引出故事本身所没有的哲学和美学体会，用人话说就是……“像诗哦、好有想象哦”等等。 # 但叙事这门手艺，从哪来？ # 最早，就从文学来。 # 你说你太忙啦没什么时间，好的Sir理解，那么。 # 看一部纪录片的时间总有吧？ # 这部片里记录了中国几位顶级作家，比如贾平凹、莫言等……你也别说乡土题材你不感冒。 # 就算你是城市青年，你也是城市这片土地长大的。 # 好比伍迪·艾伦这种纽约（老）小子，也是纽约那片土地养大的。 # 懂了吧？\n**叙事不是时尚穿搭，没有什么土和洋。\n**\n而这部纪录片，起码可以为你的电影审美，开一个绝对不土的好头—— # 文学的故乡\nSir先来一个假设。 # 假如时光倒流，你有一次这样的机会： # 鲁迅先生带你游逛他的家乡，指给你看他和闰土一起捕过鸟的地方，一起偷过西瓜的田地\u0026hellip;\u0026hellip;.甚至如果闰土还活着，他还会让闰土给你签个名； # 又或者鲁迅路线一转，准备带你去上海滩走走。不是现在的上海滩，是100年前那个，逛逛他上海租界的家，说说他和妻子的日常。如果恰好遇上萧红来此借宿，哇，你又有了一张合影； # 你去不去？（免费，保证没有旅游点强买强卖） # △ 电影《黄金时代》 # 行了，冷静。 # 想去，也没有。 # 因为遗憾的是。 # 鲁迅生前，并没留下任何录像资料。 # 《文学的故乡》的导演张同道，他也有同样的遗憾—— # “先生去世之后，有电影人拍鲁迅先生的葬礼，可就是没人在先生健康的时候去拍一拍他的生活。纪实有不可替代的力量，多大的明星去扮演他，也不是他。” # 所以。 # 为了防止再有这种类似的遗憾，他制作了这部纪录片。 # 一是，纪实录像对于历史记载的意义重大。 # 二是，作为普通人，普通读者，终于有机会见到文字之外的一个鲜活的人。\n主角都是大咖。 # 6位中国当代文学大家，莫言、贾平凹、迟子建、毕飞宇、刘震云，阿来。 # CCTV纪录频道出品，历时两年拍摄，今年刚刚出炉。 # 也别以为拍本土大师就严肃，就闷。 # 这次拍摄的地理跨度，幅员辽阔，从中国北方到南方，从日本到美国到欧洲…… # △ 作家的故乡：贾平凹的秦岭商州，阿来的嘉绒藏区，迟子建的冰雪北国\u0026hellip;\u0026hellip; # 而除了上面关于叙事审美的理由之外，其实导演还给了一个更好的理由： # 每天996的社畜，为什么仍需要读文学？\n导演说： # 现在都去想办法买更大的房子，100平方、200平方、300平方，可有没有人想过。 你的灵魂也需要一栋房子，你的灵魂住在哪呢？ # （也别吐槽说什么“身体没房子，没法担忧灵魂”，因为灵魂都没房子的人，也很难有所谓大成就） # 而灵魂的“住处”，和阅读好的作品、观看好的电影。 # 真的，强相关。 # 01****吃鸡蛋，也要认识生蛋的鸡 # 通过这部纪录片，你终于有机会，深入这些大家们的日常。 在作家的头衔之外，他们大多已年过半百，也为人夫、为人妇。 # 先说女性作家。 迟子建。 # 在家里，迟子建在一个陈旧铁盒中拿出了珍藏的信。 # 来自去世的父亲。 当年迟子建在《人民文学》发表文章后，父亲很高兴，写了封信给女儿： “要以此为起点，要有恒心，家中想你。” 没想到两个多月后，父亲因脑溢血去世。 # 这封信，连同父亲，都成了她隽永的回忆。 # 另有几样物事，也让Sir印象很深——迟子建放在客厅屏风处的“珍藏古董”。 # 它们不是什么厚重古董，但它们身上承载的往事，足够厚重。 # 2002年，丈夫遭遇车祸离世，留下她一人。 34岁才遇到人生挚爱，幸福的婚姻却只保有5年，这段感情就此消失，实在无所凭借。 # 迟子建保存了他俩吃的最后一罐咸菜瓶。 # 留下了一起喝过的最后一瓶酒。 # 还有家里两人都喜爱的一个花瓶。 # 将它们涂上油彩，放在了那个位置。 # 那是自己经过就会看到的地方。 # 它们是世上最轻飘飘的容器，里面空无一物，也毫不值钱。\n但这也是世上最重的容器吧。\n第二位。 # 毕飞宇。 都说作家的创作经历，是故乡。 # 可是从小跟随父亲频繁搬家的毕飞宇，实际上没有真正的故乡。 # 父亲本是被人收养的弃儿，工作长年辗转。 所以当毕飞宇带着摄像组，回到小学时住的杨家庄，却怎么也找不到从前住的老地方。 寻寻觅觅，一路询问，来到河边。 # 他终于看到了点什么，居然掩面而哭。 # 一个大男人什么时候会哭？因为油菜花田那一头就是他出生的地方，再看到，又是多少年过去了。 # 第三位。 写出《尘埃落定》的阿来。 这次拍摄，让阿来又回到故乡，回到家，他已经一段时间没见妈妈。 再见，妈妈都有点不好意思。 # 小时候家里穷，她总是让阿来帮忙农活，老不许他看书。 # 如今儿子年过半百，妈妈似乎还有残存的愧疚。 # 不好意思看儿子啊，只能把头埋在儿子怀里。 # 哭了啊。 # 晚上，一家人唱歌。 # 阿来看到妈妈一个人坐着，过去拉住她手。 # 这像大作家？ # 这分明是一个脸红的孩子。 # 你品品，他的愧疚，又从哪里来？ # 以上，是纪录片里他们的生活。 # 接着准备升级吧。 # 因为如此的生活经年累月，终于长出了他们渴求的东西。\n02****虚构与真实 # 虚构文学，当然是虚构的。 # 没人看了一部小说，就以为里面的“我”，真是作家本人。 但是呀。 # 读者总会好奇，这里面的人物，都是怎么长出来的？ # 这部纪录片会告诉你： # 虚构的人物，也有着现实土壤。 # 而虚构的场景，作家一只脚在现实里踩过，一只脚在大脑中踩过。\n这是《文学的故乡》的另一个“八卦点”： # 让作家本人带着镜头，走进那些“小说图景”。 # 迟子建，如此这般，回到了故乡的大兴安岭雪地。 # 她不找人，不怀旧，只是一把躺进了雪地。 # 然后仰望天空，很自然地说起家乡……但谁又能反对说，这不文学？ “哎呀，你看这天空，看看我们大兴安岭的天空，这么的蓝，这么的透明。然后白桦树、树冠在顶端。这是雪浴，真是一种清凉的感觉。” 她年过五十。 # 但家乡让她突然变回了《北极村童话》里的小女孩。 # 丧夫后，她创作了《世界上所有的夜晚》。 这部作品让她走出了巨大的生命创伤吗？ # 不，也许创伤并没消失，成为了生命的一部分。\n我想把脸涂上厚厚的泥巴 # 不让人看到我的哀伤当我追忆它时听到的 # 只是弥漫着的苍凉的风声 # ——《世界上所有的夜晚》 # 所以，别以为悲观阴郁是迟子建生命的底色。 # 在每个镜头里，我们都能看到一个乐观爽朗的东北作家，装都装不出来。 # 她在纪录片说： # “你看哪一个冬天会没有尽头呢，哪一个春天会永远伴随着你呢？一定不会的。” # 这像对自己说，又像对我们说。 # 随着话语随着镜头，我们看向大兴安岭，当我们看到鄂温克人、驯鹿，很自然就想起她的《额尔古纳河右岸》里，那些原始而神秘的故事。 # 原来这份豁达感悟，就从这里长出来。\n很多不爱文学的青年人，是通过电影《推拿》知道毕飞宇的。\n他是原著作者，小说的灵感，来自他曾在南京特殊教育师范学院任教的经历。\n我们老说，文学源于生活。其实这句话取决于，你有没有真的深入生活，甚至赋予自己一个角色。 # 毕飞宇给自己的角色是： # 友善的“居委会大妈”。 # 他们有了重大的问题 # 第一个想到的就是得给毕老师谈谈 # 如此，一个作家才能穿过世俗的粉饰，抵达真正的痛苦。 # 也才真的写得出血与肉。 # 03****故乡的魂灵 # 看到这有人要问： # Sir，我不写作，也不拍片，不聊故乡行不？ # 现在不理可以。 # 但总有一日你避不开。 # 成年后进入社会，我们老说什么成长、成长。 # 但如果看不到故乡，体会不到乡愁，又如何建立自己成长的坐标？**\n**现在成长于大城市的青年，也往往觉得很多中国老作家陌生，“有代沟”。 # 但就和我们理解电影作品，需要参照导演的“原乡”一样。 # 我们看贾樟柯，看到了他的山西情怀、小镇青年情怀（很多年后，我们又在毕赣的身上再度发现）； # 我们看《红高粱》《黄土地》，也会在张艺谋上山下乡、当过工人的经历里寻求印证； # 国外导演不一样？ # 我们看黑泽明，即使看的是历史故事，也会感知战后一代日本青年的寻根冲动——他们必须在日本文化与西洋冲击的夹缝中，寻找自己的立足之处； # 我们看伍迪·艾伦，对，他是城市青年最容易共鸣的。《纽约的一个雨天》《摩天轮》《遭遇陌生人》……当他镜头下的爱，发生在罗马、巴塞罗那和巴黎，我们没去过，也仿佛看见了自己的成长与骚动。 # 而看多了，你知道会发生什么？ # 你会看到自己的故乡。\n所以Sir其实很希望你回去看看，在百忙之余。 # 因为成长有时不在身边，不在公司，不在北上广的奋斗中，而在你回家的一刻。\n贾樟柯缺乏灵感时，会回山西。不带电脑，一个人回。 # 返程路上他会想： # “那里的城池已有千年，一定明月高挂。” # 这是一种久违的等待感，因为放出的虎，终于归了山。 # 纪录片里的作家们也是。 # 尽管母亲去世了，家乡已失去了羁绊，可贾平凹还会一年回几次。 # 一回去，迎面而来的，都是熟悉的，亲切的，带着新鲜的感悟。 # 而毕飞宇虽然上来拒绝，可是口嫌体正直。 # 见到小学同学后，他还是激动得要死。 # 莫言说了两个字，特别带感： # 血地。\n“作家的故乡，并不仅仅指父母之邦，**而是指作家在那里度过了童年乃至青年时期的地方，**这地方有母亲生你时流出的血，这地方埋葬着你的祖先，这地方是你的血地。” # 而当一个人真正成长了，这片血地会怎样？ # 会变大。\n阿来说，当我们日渐扩大的时候，我就会把故乡放大，我现在可以说，整个川西北高原，我都把它看作是我的故乡。 # 有趣的是，贾平凹也这么说。 # 他的“血地”，从小时候放牛割草的商洛某条河，扩张到了西安，到了陕北，最后扩张到中国文化的重要山脉，秦岭。 # 其实你和Sir****不也如此？\nSir的家乡开始也是一个小点。 # 慢慢的，深圳、北京、上海去过，很多地方都有了记忆。 # 慢慢的各种电影看多了，也总能体会别人的原乡，哪怕从未去过。 # 日本作家大江健三郎造访莫言，他们一起在高密过春节。 # 大江走过莫言的家乡，站在田埂上流下眼泪。 # 这不是他的故乡，然而他却看到了“真实”的故乡： # 风景是不可思议的，小孩子在看风景的时候，一边感受风景，一边思考。当几十年以后他长大成人，仍然能在风景中回忆起来，风景给了我们真实感和思考的机会，仿佛那里有一个童年时的自己，在那里思考的是成年的自己，风景像桥梁一样将两者相连，我认为这就是文学的作用。有一个叫莫言的少年，在这片风景中悲伤过，痛苦过，害怕过，真爱过。 # 所以呢。 # 通过文学和电影，通过你四散各处的经历，又通过你回到家乡……我们让“两个自己”相连。 # 这是你终将获得的成长。 # "},{"id":179,"href":"/docs/%E6%98%AF%E6%8A%80%E6%9C%AF%E5%A4%A7%E7%A5%9E%E8%BF%98%E6%98%AF%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84%E9%83%A8%E7%9A%84%E7%A5%B8%E5%AE%B3-shi-ji-shu-da-shen-hai-shi-ji-chu-jia-gou-bu-de-huo-hai/","title":"是技术大神还是基础架构部的祸害 2024-08-02 17:50:26.139","section":"Docs","content":"这两天马工关于基础架构部的文章「基础架构部，还有必要吗？」进一步在朋友圈疯转。严格来说，这篇文章更应该被看作前一篇关于多云的文章「多云战略：无奈的现实，危险的选择」的姊妹篇。\n恰逢这两天笔者也在思考关于「云上安全」的一些话题，其中一部分核心观点其实和这篇文章多少有些类似。之前我们在文章为什么汽车行业正在全面拥抱公有云中从企业决策者视角详细讲述企业上云动机，借此讨论，再展开多聊几句关于用户如何使用云的一些拙见。\n「云优先」的思维方式\n马工这篇文章和他这两年的观点是一脉相承的，核心观点只有一个：云正在不经意之间改变着组织架构的构建形态，甚至重新定义职业能力象限。\n只是文章整体论证多少有些武断，马工通过例举了基础架构部的各种**「无能」**事迹，甚至不惜将团队妖魔化，去论证团队已经没有存在的必要，笔者认为仅凭借这点是不足以服众的，毕竟组织架构和团队能力这两者本身就要分开来看。\n但本文并无意纠结于基础架构部到底有没有核心技术，事实上，笔者认为这个话题的讨论重点也根本不是**「基础架构部的存在是否利大于弊」。真正在讨论的是，今天我们所处的云时代，行业正不断走向成熟，云早已不再只是资源托管平台。作为一种全新的生产工具，在带来新的生产力之后，在「新的生产关系下，企业组织架构该如何构建」**。\n笔者观点甚至可能比马工更悲观：在今天，但凡企业组织形态没有基于云的特性去做架构思考和设计，即使达到了马工在文章中所讲述的两点：具备「优秀的技术」和「引导使用优秀技术的能力」，最终的结果大部分情况都是令人失望的。关于这点我从令辉兄的另一篇文章基础架构部不死，只是慢慢消逝一文中感受到了同样的情感，老兵不死，他们只是逐渐凋零。\n基础架构部未来会消失吗？我认为不会。在过去，马车用于运输，但今天却常见于马戏表演，演变成了一种娱乐活动。马车消失了吗？没有，只是它的数量和存在的目的都变了。\n这两年在和不同行业客户打交道的过程中，总是会遇到各种用云过程中的各种怪相。\n例如用户虽然使用公有云，但是团队完全按照数据中心时代去构建，从工作职能划分再到技术选型，最好是在迁移上云过程中，连IP地址都不要发生任何变化。\n甚至于一些用户会提需求说，我们希望能让我云上的架构看起来和线下一样\u0026hellip;\u0026hellip;\n再比如，一些客户会希望利用公有云去实现K8s架构下的应用及数据冷备\u0026hellip;\u0026hellip;\n再比如，我此前在多篇文章中所提到的，非要在毫无标准可言的多个云平台之间，基于美好但不切实际的多云管理平台去实现跨云迁移。\n种种这些，许多人只是惯性的将系统从云下部署到了云上，却又很难讲清应用究竟是否真正的上云，以及和没有上云有什么区别。\n前两年「Cloud Native」这个词很火，时而被形容架构，时而形容产品，笔者认为在此之前，「Cloud Native」更应该被视作为用户在设计应用及平台架构时的用云观。\n这两天也看了不少留言和文章极力证明自己团队的技术价值，从而试图论证基础架构有存在的必要，实际上都没有太大的实质意义。\n** **\n「自服务」的去中心化组织\n其实如果记录下时间轴，我们会看到最早是网络工程师和操作系统管理员、这两年讨论更多的DBA，然后再到基础架构部。而这个顺序刚好是伴随着云计算行业的不断成熟，从IaaS基础设施层，再到PaaS一层层往上走的。\n在国外，由于用户和云厂商之间所形成的技术共识，这个结果的发生的比国内要早几年，从这两年的行业趋势来看，国内目前也朝着这个方向在发展。\n如果你所在的组织并没有使用，也永远不会使用公有云，大可不必将马工的文章放在心上，至少眼下你所面临的处境，生产工具和生产关系的矛盾并没有发生。\n但如果你所在的组织未来计划上云，或是已经用云，从技术选型再到人员能力矩阵的重构，这个时间或早或晚，但一定会在某一天发生。这也就意味着今天许多团队在云上或云下自建的许多基础设施平台的工作，在未来可能都是负价值的。\n为什么在云架构下，传统技术架构的划分已不再适用？\n架构的不可持续性\n过去我们在做基础架构工作时，最常见不过的便是技术山头下的标准化问题。永远不要挑战人性，只要在企业内部团队不是一家独大，大家就一定都想自建一套技术标准和产品体系，最后的结果大概率无法达成共识。大部分情况下，基础架构都成为了为特定业务服务的配套团队。\n而上面这种已经算是比较好的技术归宿，事实证明「以“大神”为中心」构建的技术平台永远都是不可持续的。更多的情况是当业务系统接入之后，因人员变动缺乏维护的基础组件，留给业务团队的只能是一地鸡毛。\n这种试错和重构所带来的额外技术成本，几乎成为了企业在基础架构发展过程中的阿格琉斯之踵。但基于云优先理念，情况可能完全不同，不同业务部门基于同一套云托管服务很容易在团队之间达成技术共识，在标准的平台界面上各方都是一致的，同时也能够基于各自的实际业务场景，选择不同的技术实现方式。而在过去以资源为中心的技术架构形态之下，这种情况是不存在的。\n因为上述的诸多原因，从这两年的云上趋势来看，头部玩家也在逐渐拥抱云上PaaS。从跨越时间周期更长远的视角来看技术演进之路，基于云平台技术架构的可持续性是经得起时间验证的。\nDevOps挑战\n另一方面，笔者并不想要挑战基础架构的技术能力，但事实上基于云仍会比技术自建要高效的多。\n过去十余年，国内在实践DevOps工程的过程中，其组织形态往往是运维来主导，因此自动化便成为了首要业务关键指标。但自动化其实只是过程，经过这些年的尝试，我们很容易发现DevOps的最大挑战其实并不能通过自动化彻底解决，甚至可以说是占比极小的一部分。除非今天你所面临的业务场景足够简单，同时业务又很少发生变化，否则真实世界一定会有大量的corner case是需要依靠人工变更去完成的。\n在人员架构复杂的大型组织，DevOps真正要解决的问题是跨职能团队之间的信息衰减，而想要达到消除信息衰减，关键在于如何将组织形态尽可能地扁平化和去中心化。因此许多时候，组织寄希望于靠一套自动化工具平台去解决跨职能团队的协同问题，效果往往是很差的。\n还有比将所有基础架构完全交由云厂商管理，由研发人员所主导**「开发自服务」**更优雅的DevOps工程实践吗？\n由于部门墙的存在，企业内部的许多团队被无形的保护了起来，以至于很多事情组织并不是以最高效的方式去运作的，而在另一些主营业务是云上代运维服务提供商的企业，在真实商业环境你死我活的竞争下，他们能够更快察觉到开发自服务所带来的全新业务挑战。\n曾经，当软件提供商给客户开发交付一套核心系统时，他们需要和企业内的运维、基础架构等团队一起协同；如今，他们完全可以基于Serverless、IaC等工程实践，彻底屏蔽并绕开组织内的所有人，拿走项目中最大的那块蛋糕。最后，还可以正大光明的讲述一个在技术领先性的架构理念下如何提升人效的故事。\n马工一直在讲国内云用户其实不怎么懂云，仅仅将其作为资源托管平台的IDC2.0。其实关于这点笔者是有信心的，从最近这两年和客户在各种场景和项目中的交流来看，仍然有越来越多的客户是在正视这些现象的，从资源运维到平台治理，从ClickOps到IaC，这种趋势的转变更多的只是时间问题。\n写在最后\n回到本文最初的讨论，基础架构团队在未来仍然会存在，但他们的数量必然也会像更多企业的DBA一样被极大的减少，从企业的技术建设者转向云实践下的工程布道者。\n云计算作为技术平权的开放平台，在研发流程中极大的消除了不同管理角色的工作职能，但生产关系的改变同时也在创造新的机会，未来的中心化团队建设重心也将转向从「精细化」管理到「精确化」运营的平台工程的体系建设。\n不仅如此，如果再向上看一步，云并不仅仅是解决了技术人员的统一语言问题，甚至包括财务人员在内的非技术人员同样收益，今天如果要做预算规划，财务人员完全能够基于云账号实现「财务自服务」，最终使得FinOps成为可能。\n如果认为云计算最终只会卷到甲方的，可能真的只是一种错觉。这几年，能够明显感觉到随着行业走向成熟，对于从业人员的能力矩阵也在发生着改变。\n最近这段时间，笔者的大部分精力都放在了技术服务岗的面试工作。在此过程中也发现一些有趣的现象。一些愿意主动投递简历做云技术服务的，大部分是以过往以运维工程师相关背景的技术人员。\n而一些研发背景的人员会觉得云计算和他们的职业成长规划似乎并没有太大的关系，在过往的工作中，他们也很少会真正由他们去创建、配置、操作云服务，有类似经历的其实在行业中不在少数。\n作为普通职场人，每个人都或多或少会面临不同程度的职业危机。许多时候我们总是会苛责自己没有努力做到更好，又或是时常抱怨自己自己没有得到更多的机会，但外部环境的变化从不以人的喜好和意志为转移，绝大多数情况下，普通人能做到的极限往往也只是顺势而为。\n"},{"id":180,"href":"/docs/%E6%9C%89%E8%BF%993%E4%B8%AA%E8%BF%B9%E8%B1%A1%E4%BD%A0%E5%B0%B1%E8%AF%A5%E7%A6%BB%E8%81%8C%E4%BA%86-you-zhe-3-ge-ji-xiang--ni-jiu-gai-li-zhi-le/","title":"有这3个迹象，你就该离职了 2023-09-21 16:00:22.432","section":"Docs","content":"我的前下属小P，在当前这样的环境下，裸辞了。\n小P在这家公司呆了3年，平时任劳任怨，努力踏实。对一些“007”加班、背黑锅等“非常规”任务，也来者不拒，积极配合。\n本以为这样就可以获得领导的青睐，能让自己的职场之路更顺畅。但现实却没如他所愿，前段时间公司有个主管空缺，无论从哪个角度看，小P都是最佳人选，小P自己也以为十拿九稳，但没想到领导却提拔了一个刚入职不到半年的新同事，真实的原因并不是那个人能力有多强，而是因为他是领导以前的旧下属，是嫡系“子弟兵”。\n小P输给一个寸功未立的“关系户”，心里憋屈，找领导理论。但领导就是领导，一下子指出N条小P在工作中的不足，说他工作效率低、有时候还犯错、而且还缺少大局观。小P听了更感到委屈，自己平时干的都是其他同事不愿接的“脏乱差”的活，有时候一整个项目都靠自己一个人顶着，现在反而成了多做多错，而那位被领导提拔的“嫡系”，手里没啥正事，整天和领导一起吹牛抽烟，反倒成了有大局观。\n小P情绪非常低落，天天上班如上坟，对工作提不起丝毫兴趣，而且身体和心理都出现明显不适，于是就下定决心，裸辞了。\n小P的这个决定，我是支持的。如果公司的环境对你的发展不利，你也看不到任何改观的迹象，那么止损可能是当下最好的选择。下面跟你聊聊，遇到哪类情况，你应该果断离开。\n01\n领导故意打压你\n脏活累活想着你，升职加薪没你份\n人在职场，要能分辨两个最起码的事实，领导是“总用你”还是“重用你”；是喜欢 “你能干活”，还是喜欢“能干活的你”。\n领导让你做事的时候，简直把你当成公司“继承人”，技术需要你攻关、客户需要你维护、团队需要你协调、项目需要你加班，好像哪里都需要你，什么活儿你都要负责。\n直到每次升职加薪的机会都和自己无缘时，上面说的小P就是典型的例子，脏活累活都丢给他，升职加薪全留给亲信，明显厚此薄彼，不公平、不厚道。出现这种情况，通常有以下几种原因：\n1、你是个不会说“不”的软柿子老实人\n很多单位都一些老实本分的打工人，他们在基层踏实工作，与人为善，任何人都不想得罪，对领导更是言听计从，从不讨价还价。\n这种人确实是大家眼中的“好人”，但这样的人不懂得说“不”，不会向别人展示自己的“边界”，以至于领导或其他人会不断对他试探，把脏活累活、不该他干的活全都丢给他。\n最后，所有人都对此习以为常，而他却慢慢变成了多做多错、出力还可能不讨好的“杨白劳”。至于升职加薪，是不太可能轮到他的，他真上去了，这些脏活累活以后丢给谁干？\n2、你不是领导的“自己人”\n有人的地方就有江湖，没办法，谁让中国是个人情社会呢。不管在哪个团队、哪个群体，人与人之间都会有个亲疏远近，最大的差别只不过是表现的是否明显而已。团队的各种资源、机会都是有限的，拥有权力且怀有私心的某些领导，在分配这些资源的时候，就极有可能向所谓的“自己人”倾斜。\n上文中的小P不就是败给了领导的前下属了吗？当然，我不是说所有的公司领导，都像小P的领导一样不公平公正，但如果真遇到这种明显不讲规则的人，就算你干的再多，也很难分到肉吃，有时能不能喝口汤，都要看他心情。\n3、你是个“能力有限”的人\n这里说的“能力有限”，不是说工作能力有限，而是指你只会“工作”，而没有背景、资源、人脉，等其他方面的可交换价值。我曾见过一个行政部前台，半年时间被提拔为部门经理，不是说她工作能力有多强，而是她的亲叔叔是某重点中学校长，她利用这个关系为公司老总的孩子解决了上重点的问题。\n我还听过很多带“资”入职的例子，这些人到了公司，就能利用自己的资源，为公司或领导个人，解决很多棘手的问题；而你，只擅长本本分分在自己工位上按部就班的工作（而且能干这种活的人一抓一大把），即使你做再多的工作，和前一类相比，稀缺性和竞争力都是明显不能比的。有了好机会，自然会被那些有“特殊贡献”的人先抢走。\n02\n部门集体摸鱼，人浮于事\n在当前的职场生态中，不少企业人浮于事，充斥着“坐班不做事”、“领导不走我不走”等伪加班、伪忙碌的形式主义。出现这种情况，通常因为两种原因：\n1、员工“形式主义”严重\n我曾听一位年轻朋友讲过他经历的一件奇葩事。他曾和另一个同事一起负责公司的某个项目，某次两人加班到9点回家，但第二天朋友却发现那位同事发了这样一条朋友圈“一不小心搞到现在，终于忙完了”，配图是办公桌上开着的电脑，电脑上显示的是一份打开的文档，而发布时间则显示“某年某月某日凌晨2点”。而且在这条朋友圈下，公司领导不仅点了赞，还写下“辛苦了”三个字的留言。\n这让朋友心里非常不爽，活儿明明是两人一起干的，功劳就全成了同事的了。除了想给领导留下勤奋努力的好印象外，还有人有其他一些目的。比如，没结婚成家的想蹭蹭公司的空调，反正回去也是打游戏，在哪玩不是玩；还有一些“勤俭持家”的是为了蹭公司的班车、食堂，甚至是为了熬点加班补贴，生活不易，能省点就省点，能捞点就捞点。总之，大家就这样互相耗着，早走你就输了。\n2、公司领导的“官僚主义”\n曾有一位粉丝朋友向K哥分享过他辞职的故事。他的领导就是一位见不得别人比他早下班的人。只要别人比他早离开公司，他就觉得为别人的工作不饱和，就会在开会的时候含沙射影批评几句，或者给那个“早走”的人多安排些工作。\n在一些企业中出现这种集体摸鱼、人浮于事的情况，虽然并不是每个人都故意为之，但就像在电影院里看电影，一旦前排的人站起来，后面的人就必须跟着站起来一样，很多人被周围的环境裹挟，不得不一边讨厌着，一边照样干着。\n03\n长期情绪内耗，个人成长为零\n耐克创始人菲尔·奈特在他的自传中有句名言：人生即是成长，不成长即死亡。我们进入一家公司，除了挣钱以外，还有另外一个非常重要的诉求，即积累经验、学习技能，实现个人成长。但如果我们发现，自己的工作除了赚几两并不值得激动的碎银外，其余全是资源的消耗和精神的内耗，而不能给自己带来任何成长，这样的公司肯定也不是久留之地。\nK哥一个远房侄子，原来是某新媒体公司的骨干，后来被一家传统企业挖走，但做了不到一年，就觉得坚持不下去了。老板请他的时候，饼画的很好，说自己认清了趋势，要大力发展线上的业务运营，一定会给侄子最大的工作支持。\n但事实远非如此，侄子过去之后才发现，自己基本是一个光杆司令，公司里里外外懂新媒体运营的只有他一个。老板的想法天天变，又不给资源，就只会让他自己想办法，KPI完不成就各种PUA他。\n在这种工作环境下，侄子上班度日如年，每天都充满焦虑，并开始变得越来越不自信。\n我知道后，劝他立刻离开这家公司。再呆下去只会增加内耗，没有任何成长可言。\n后来，我通过朋友把他推荐到另一家业内知名的公司，小伙子很快又找回了状态，在那里他每天都能从同事、团队那里汲取新的养分，而不是一味的输出、做大量纯消耗的无用功。\n04\n离职前，给你3个建议\n1、不要因为暂时性的困难而离职\n王阳明先生有句名言：积之不久，发之必不宏，得之不难，失之必易。意思是说，如果你积累的不深，发挥的时候就不会太持续；一样东西如果得来不费劲，那么失去它也会很容易。\n但很多职场人，往往没有这个觉悟，但凡遇到点不称心的事，就会在心里打退堂鼓。每当这个时候，你不妨问自己两个问题：\n一是这个问题是不是暂时的，是不是真的不能克服？\n二是，这个问题是不是这家公司独有的，当你换另一个公司时，是不是能确保不再出现？这些问题想清楚了，相信答案也就有了。\n千万不要轻言放弃，越能够经得起挑战，越能够真正拥有。\n2、离职是因为有更好的机会，而不是逃避困难\n马克思曾经说过：“人不是出于逃避某种消极力量，而是出于展现自身积极个性，才能获得真正的自由。”离职跳槽从来都不应该出于简单的消极逃避，而应该是因为有了更积极的选择。\n什么是更好的选择？比如，有更大更有影响力的平台，愿意以30%的薪水涨幅，以更高的title邀请你加盟，这就非常值得考虑。而不能仅仅情绪上不喜欢现在公司，而慌不择路，毫无规划的随便选一家新公司做“避风港”，甚至不惜平跳、打折跳，这样做既草率又不负责。\n3、要有随时离开公司的能力\n职场上的成功，不是永远不被解雇，而是永远拥有主动选择职业、随时可以离开公司的能力。这就要求我们在职场中，要注意学习新的技能和认知，要能清晰客观的认识自己，知道哪些是自己真正具备的可复制、可转移、可嫁接的个人能力，而不至于出现“错把平台当能力”的“误会”。\n同时，还要在职场中为自己积累一些能拿得出手、更有说服力的“硬通货”，比如过往的出色业绩，良好的业内口碑，这些都能作为你的背书，让你在新的舞台上，更受器重和尊重。良好的开始就是成功的一半，真正有实力的你，搞定成功的另外“一半”，想必也不是什么难事。\n大哲学家康德说过：真正的自由，不是你想做什么；而是当你不想做什么时，可以不做什么。有底气、负责任的离职，就是这句话最好的诠释，希望小伙伴们在职场中，都能从事自己喜欢的工作，受到良好的尊重和对待，有丰厚的薪水收入，还能有让人满意的个人成长。加油！\n"},{"id":181,"href":"/docs/%E6%9E%81%E5%A4%A7%E6%8F%90%E9%AB%98%E5%B7%A5%E4%BD%9C%E6%95%88%E7%8E%87%E7%9A%84-linux-%E5%91%BD%E4%BB%A4-ji-da-ti-gao-gong-zuo-xiao-lv-de-linux-ming-ling/","title":"极大提高工作效率的 Linux 命令 2024-04-03 14:42:46.957","section":"Docs","content":"作为一名软件开发人员，掌握 Linux 命令是必不可少的技能。即使你使用 Windows 或 macOS，你总会遇到需要使用 Linux 命令的场合。例如，大多数 Docker 镜像都基于 Linux 系统。要进行 DevOps 工作，你需要熟悉Linux，至少要了解一些常用命令。\n在本文中，我将介绍一些我每天使用的命令。如果你是Linux的新手，或者想要更新、提高或加强对 Linux 命令的了解，本文对你可能会有所帮助。不过，本文不会重点介绍像 cd 或 ls 这样的基础命令，而是介绍一些从实践中学到的更高级的命令。\n自定义bash提示符 # 嗯，这个主题本身可以是一篇冗长的文章。不过，我们不需要学习所有的内容。在大多数情况下，我们只需要修改 PS1 变量，该变量指定在每个命令之前显示的内容。如果不修改它，提示符将显示路径，当我们深入到一个文件夹中时，这是非常不方便的。我更喜欢在提示符中只显示用户名和当前文件夹，可以通过以下命令设置：\nlinuxmi@linuxmi:~/www.linuxmi.com$ export PS1=\u0026#34;[\\u@\\W]\\$\u0026#34; ``\n这里，\\u表示当前用户名，\\W表示当前工作目录。[、]和@保持原样显示。对于$，如果当前用户不是root，则显示$，否则显示#。更多代码可以在这里找到。\n请注意，如果您希望更改持久化，这个命令和下面显示的命令应添加到 ~/.bashrc中。\n查找文件或文件夹\n查找文件或文件夹是一个非常常见的需求，可以使用find命令来实现：\n# 查找文件： find ~ -type f -name data-model.ts # 查找文件夹： find ~ -type d -name angular15 请注意，第一个参数是要查找的目标文件或文件夹的路径。如果未指定类型，默认为文件。 # 这似乎有点太简单了 😏。让我们来做一些稍微复杂一点的事情。让我们找一些文件，并统计每个文件的行数： # #查找所有.html文件并计算每个文件的行数： find src/app/ -name \u0026#34;*.html\u0026#34; -exec wc -l {} \\; ``\n请注意这里的魔法语法。-exec 指定了要针对每个找到的文件运行的命令。花括号 {} 是文件的占位符，而转义的分号 ; 用于防止 shell 解释该命令。我们也可以使用加号 + 来实现同样的效果： # find src/app/ -name \u0026#34;*.html\u0026#34; -exec wc -l {} + ``\n输出结果会稍有不同。如果命令以 + 结尾，将会打印总计行数。 # 更改文件和文件夹的权限 # 更改文件和文件夹的权限也是一个常见任务。我们很容易在权限代码上感到困惑。这篇文章（https://www.linuxmi.com/linux-file-permissions.html）很好地解释了权限代码和不同的组合。在这里，我们只展示最常见的几个： # # 与其他人共享文件夹，以便他们可以检查文件夹中的内容： chmod 755 \u0026lt;path-to-your-folder\u0026gt; # 与其他人共享不可执行文件： chmod 644 \u0026lt;path-to-your-file\u0026gt; # 与其他人共享可执行文件： chmod 755 \u0026lt;path-to-your-file\u0026gt; # 使文件只读： chmod 400 \u0026lt;path-to-your-file\u0026gt; # 使文件可执行： chmod +x \u0026lt;path-to-your-file\u0026gt; ``\n建议查看这篇文章中代码的含义，这太好了，不能在这里重复。 # 自定义 rm 命令将文件移动到回收站 # 如果在 Linux 系统上意外删除了一些文件，恢复它们将非常困难。因此，将 rm 命令设置为将文件或文件夹移动到回收站而不是永久删除是一种安全策略。我们可以在需要时清空回收站。 # 这个自定义功能可以通过别名(alias)来实现，在 Linux 上非常方便（稍后会介绍更多相关内容）： # alias rm=\u0026#39;gio trash\u0026#39; ``\n使用别名可以简化输入 # 如果你有一个带有许多参数的长命令，或者你有一系列需要一起执行的命令，使用别名非常方便： # # 为长命令设置别名： alias dc=\u0026#34;docker compose\u0026#34; alias prettier=\u0026#34;npx prettier -w\u0026#34; alias eslint=\u0026#34;npx eslint --fix\u0026#34; alias pre=\u0026#34;pre-commit run --files\u0026#34; # 将一些命令链接在一起： alias update-container=\u0026#34;docker-compose pull \u0026lt;service-name\u0026gt; \u0026amp;\u0026amp; docker-compose stop \u0026lt;service-name\u0026gt; \u0026amp;\u0026amp; yes | docker-compose rm \u0026lt;service-name\u0026gt; \u0026amp;\u0026amp; docker-compose up -d \u0026lt;service-name\u0026gt;\u0026#34; ** **\n使用awk从输入中提取数据\nawk是Linux中非常强大的工具。你甚至可以使用awk编写脚本，尽管可读性和维护性是另外一个问题。作为软件开发人员，我们通常使用awk从输入文件或前一个命令的输出中获取所需的字段。 # 例如，让我们ping google.com并获取每个ping的时间： # ping google.com -c 2 | grep -iE \u0026#39;time=.*ms\u0026#39; | awk \u0026#39;BEGIN {FS=\u0026#34;=\u0026#34;} {print $NF}\u0026#39; ``\n这个命令的重要注意事项： # 我们使用-iE来为grep指定一个正则表达式，因此只有与模式匹配的消息将进一步处理。 # 对于awk来说，BEGIN指定在处理文本之前要执行的命令。在这里，我们指定字段分隔符（FS）应为等号。此外，NF表示字段的数量，$NF将返回最后一个字段。如果你想进一步学习awk，这是一个很好的参考。 # 以上命令也可以简化如下： # ping google.com -c 2 | grep -iE \u0026#39;time=.*ms\u0026#39; | awk -F\u0026#34;=\u0026#34; \u0026#39;{print $NF}\u0026#39; ping google.com -c 2 | awk -F\u0026#34;=\u0026#34; \u0026#39;/time=.*ms/ {print $NF}\u0026#39; ``\n在这些简化版本中，我们还可以利用 awk 的另外两个功能： # 可以直接使用 -F 选项指定字段分隔符。 # 可以在 awk 中使用正则表达式模式，在花括号之前指定。 # 使用 xargs 链接命令 # 我们已经展示了使用 -exec 选项可以为 find 命令链接命令。一个更好的工具是 xargs，它可以以更灵活的方式链接任何命令。 # # 检查当前文件夹中每个文件的大小： ls | xargs -I % du -sh % # 查找一些文件并将其复制到新位置： find ~/Downloads/ -name \u0026#34;*.jpeg\u0026#34; | xargs -I {} cp {} ~/Pictures/ # 将所有文件夹和子文件夹的权限更改为755: find . -type d | xargs -I {} chmod 755 {} # 将当前文件夹和子文件夹中所有文件的权限更改为644: find . -type f | xargs -I {} chmod 644 {} ``\n请注意，-I 选项指定了一个占位符，可以在需要的任何地方使用。占位符可以是任何有效的字符串，其中 % 和 {} 是最常见的。 # 为脚本传递变量 # 有时我们需要为脚本传递一些变量。这些变量可以在运行脚本的命令之前通过命令行指定。例如： # # linuxmi.sh echo mysql -u ${USERNAME} -p${PASSWORD} # 在命令行上： USERNAME=johndoe PASSWORD=12345 bash some_script.sh ``\n对于在云环境（如Cloud Build）中运行的某些脚本，这一点尤为重要，因为变量可以自动注入。 # 如果脚本需要许多变量，逐个在命令行中指定会变得很麻烦。在这种情况下，可以将这些变量存储在文件中，并在运行脚本时进行引用： # # variables.env USERNAME=johndoe PASSWORD=12345 # 在命令行上: env $(grep -v \u0026#39;^#\u0026#39; variables.env | xargs) bash some_script.shxxxxxxxxxx # variables.envUSERNAME=johndoePASSWORD=12345# 在命令行上:env $(grep -v \u0026#39;^#\u0026#39; va# variables.envUSERNAME=johndoePASSWORD=12345# 在命令行上:env $(grep -v \u0026#39;^#\u0026#39; variables.env | xargs) bash some_script.shiables.env | xargs) bash some_script.sh ``\n请注意，使用此命令时，变量会从目标文件中读取（注释除外），并通过xargs命令执行。env命令用于在修改的环境中运行脚本，而不是当前的Shell环境。 # 重定向STDOUT和STDERR # 命令的输出和错误被写入标准输出（STDOUT）和标准错误（STDERR）。STDOUT和STDERR的文件描述符分别为1和2。默认情况下，STDOUT和STDERR都会输出到控制台。然而，我们可以将它们重定向到文件或空设备（/dev/null），即丢弃它们。 # # 让我们假设existing.txt存在，而nonexisting.txt不存在。 # 将STDOUT重定向到文件。这是最常见的用例。 ls -al existing.txt 1\u0026gt;out.log # STDOUT的文件描述符是可选的。 ls -al existing.txt \u0026gt;out.log # 将STDOUT重定向到一个文件，将STDOUT重定向到另一个文件。 ls -al existing.txt non-existing.txt 1\u0026gt;out.log 2\u0026gt;error.log # 将STDOUT和STDERR都指向同一个文件: ls -al existing.txt non-existing.txt 1\u0026gt;combined.log 2\u0026gt;\u0026amp;1 # 它可以简化为: ls -al existing.txt non-existing.txt \u0026amp;\u0026gt;combined.log # 忽略命令的输出和错误: ls -al existing.txt non-existing.txt 1\u0026gt;/dev/null 2\u0026gt;\u0026amp;1 ls -al existing.txt non-existing.txt \u0026amp;\u0026gt;/dev/null ``\n请注意，不是所有的Shell都支持\u0026amp;\u0026gt;语法。它支持常用的BASH。 # 命令行模糊查找器 # 我们可以按下CTRL-R来查找我们使用过的历史命令。然而，默认情况下，当我们输入时，它只显示一个命令，不方便找到我们想要的命令。 # 幸运的是，fzf极大地增强了CTRL-R的功能。fzf是一个通用的命令行模糊查找器，可以模糊搜索历史命令。最重要的是，它将匹配的命令显示为列表，使得找到我们想要的命令非常方便。 # 安装fzf非常简单： # # Linux: git clone --depth 1 https://github.com/junegunn/fzf.git ~/.fzf ~/.fzf/install # 您可以接受所有默认设置. # MacOS: brew install fzf $(brew --prefix)/opt/fzf/install ``\n如果你之前从未尝试过fzf，那么绝对值得一试，你会立刻喜欢上它 😍。 # 在本文中，介绍了一些实用的Linux命令，如果你还不熟悉它们，它们可以提高你的效率。一篇文章不可能涵盖所有的Linux命令。然而，我们不需要学习所有的命令，通常只需要学习在工作中实际需要的命令。希望本文能成为你工作中有用的参考。 # "},{"id":182,"href":"/docs/%E6%9E%84%E5%BB%BA%E5%AE%B9%E5%99%A8%E9%95%9C%E5%83%8F%E5%88%A9%E5%99%A8buildkit-gou-jian-rong-qi-jing-xiang-li-qi-buildkit/","title":"构建容器镜像利器buildkit 2024-08-02 17:49:14.916","section":"Docs","content":"Docker通过读取Dockerfile中的指令自动构建镜像，Dockerfile是一个文本文件，其中依次包含构建给定镜像所需的所有命令。\n上面的解释摘自Docker的官方文档并总结了Dockerfile的用途。Dockerfile的使用非常重要，因为它是我们的蓝图，是我们添加到Docker镜像中的层的记录。\n本文，我们将学习如何利用BuildKit功能，这是Docker v18.09上引入的一组增强功能。集成BuildKit将为我们提供更好的性能，存储管理和安全性。\n本文目标 # 减少构建时间； 缩小镜像尺寸； 获得可维护性； 获得可重复性； 了解多阶段Dockerfile; 了解BuildKit功能。 先决条件 # Docker概念知识 已安装Docker（当前使用v19.03） 一个Java应用程序（在本文中，我使用了一个Jenkins Maven示例应用程序） 让我们开始吧！\n简单的Dockerfile示例 # 以下是一个包含Java应用程序的未优化Dockerfile的示例。我们将逐步进行一些优化。\nFROM debian COPY . /app RUN apt-get update RUN apt-get -y install openjdk-11-jdk ssh emacs CMD [“java”, “-jar”, “/app/target/my-app-1.0-SNAPSHOT.jar”] 在这里，我们可能会问自己：构建需要多长时间？为了回答这个问题，让我们在本地开发环境上创建该Dockerfile，并让Docker构建镜像。\n# enter your Java app folder cd simple-java-maven-app-master # create a Dockerfile vim Dockerfile # write content, save and exit docker pull debian:latest # pull the source image time docker build --no-cache -t docker-class . # overwrite previous layers # notice the build time 0,21s user 0,23s system 0% cpu 1:55,17 total 此时，我们的构建需要1m55s。\n如果我们仅启用BuildKit而没有其他更改，会有什么不同吗？\n启用BuildKit # BuildKit可以通过两种方法启用：\n在调用Docker build命令时设置DOCKER_BUILDKIT = 1环境变量，例如：\ntime DOCKER_BUILDKIT=1 docker build --no-cache -t docker-class 将Docker BuildKit设置为默认开启，需要在/etc/docker/daemon.json进行如下设置，然后重启：\n{ \u0026#34;features\u0026#34;: { \u0026#34;buildkit\u0026#34;: true } } BuildKit最初的效果\nDOCKER_BUILDKIT=1 docker build --no-cache -t docker-class . 0,54s user 0,93s system 1% cpu 1:43,00 total 此时，我们的构建需要1m43s。在相同的硬件上，构建花费的时间比以前少了约12秒。这意味着构建几乎无需费力即可节约10％左右的时间。\n现在让我们看看是否可以采取一些额外的步骤来进一步改善。\n从最小到最频繁变化的顺序 # 因为顺序对于缓存很重要，所以我们将COPY命令移到更靠近Dockerfile末尾的位置。\nFROM debian RUN apt-get update RUN apt-get -y install openjdk-11-jdk ssh emacs RUN COPY . /app CMD [“java”, “-jar”, “/app/target/my-app-1.0-SNAPSHOT.jar”] 避免使用“COPY .” # 选择更具体的COPY参数，以避免缓存中断。仅复制所需内容。\nFROM debian RUN apt-get update RUN apt-get -y install openjdk-11-jdk ssh vim COPY target/my-app-1.0-SNAPSHOT.jar /app CMD [“java”, “-jar”, “/app/my-app-1.0-SNAPSHOT.jar”] apt-get update 和install命令一起使用 # 这样可以防止使用过时的程序包缓存。\nFROM debian RUN apt-get update \u0026amp;\u0026amp; \\ apt-get -y install openjdk-11-jdk ssh vim COPY target/my-app-1.0-SNAPSHOT.jar /app CMD [“java”, “-jar”, “/app/my-app-1.0-SNAPSHOT.jar”] 删除不必要的依赖 # 在开始时，不要安装调试和编辑工具，以后可以在需要时安装它们。\nFROM debian RUN apt-get update \u0026amp;\u0026amp; \\ apt-get -y install --no-install-recommends \\ openjdk-11-jdk COPY target/my-app-1.0-SNAPSHOT.jar /app CMD [“java”, “-jar”, “/app/my-app-1.0-SNAPSHOT.jar”] 删除程序包管理器缓存 # 你的镜像不需要此缓存数据。借此机会释放一些空间。\nFROM debian RUN apt-get update \u0026amp;\u0026amp; \\ apt-get -y install --no-install-recommends \\ openjdk-11-jdk \u0026amp;\u0026amp; \\ rm -rf /var/lib/apt/lists/* COPY target/my-app-1.0-SNAPSHOT.jar /app CMD [“java”, “-jar”, “/app/my-app-1.0-SNAPSHOT.jar”] 尽可能使用官方镜像 # 使用官方镜像有很多理由，例如减少镜像维护时间和减小镜像尺寸，以及预先配置镜像以供容器使用。\nFROM openjdk COPY target/my-app-1.0-SNAPSHOT.jar /app CMD [“java”, “-jar”, “/app/my-app-1.0-SNAPSHOT.jar”] 使用特定标签 # 请勿使用latest标签。\nFROM openjdk:8 COPY target/my-app-1.0-SNAPSHOT.jar /app CMD [“java”, “-jar”, “/app/my-app-1.0-SNAPSHOT.jar”] 寻找最小的镜像 # 以下是openjdk镜像列表。选择最适合自己的最轻的那个镜像。\nREPOSITORY TAG标签 SIZE大小 openjdk 8 634MB openjdk 8-jre 443MB openjdk 8-jre-slim 204MB openjdk 8-jre-alpine 83MB 在一致的环境中从源构建 # 如果你不需要整个JDK，则可以使用Maven Docker镜像作为构建基础。\nFROM maven:3.6-jdk-8-alpine WORKDIR /app COPY pom.xml . COPY src ./src RUN mvn -e -B package CMD [“java”, “-jar”, “/app/my-app-1.0-SNAPSHOT.jar”] 在单独的步骤中获取依赖项 # 可以缓存–用于获取依赖项的Dockerfile命令。缓存此步骤将加快构建速度。\nFROM maven:3.6-jdk-8-alpine WORKDIR /app COPY pom.xml . RUN mvn -e -B dependency:resolve COPY src ./src RUN mvn -e -B package CMD [“java”, “-jar”, “/app/my-app-1.0-SNAPSHOT.jar”] 多阶段构建：删除构建依赖项 # 为什么要使用多阶段构建？\n将构建与运行时环境分开 DRY方式 具有开发，测试等环境的不同详细信息 线性化依赖关系 具有特定于平台的阶段 FROM maven:3.6-jdk-8-alpine AS builder WORKDIR /app COPY pom.xml . RUN mvn -e -B dependency:resolve COPY src ./src RUN mvn -e -B package FROM openjdk:8-jre-alpine COPY --from=builder /app/target/my-app-1.0-SNAPSHOT.jar / CMD [“java”, “-jar”, “/my-app-1.0-SNAPSHOT.jar”] 如果你此时构建我们的应用程序，\ntime DOCKER_BUILDKIT=1 docker build --no-cache -t docker-class . 0,41s user 0,54s system 2% cpu 35,656 total 你会注意到我们的应用程序构建需要大约35.66秒的时间。这是一个令人愉快的进步。\n下面，我们将介绍其他场景的功能。\n多阶段构建：不同的镜像风格 # 下面的Dockerfile显示了基于Debian和基于Alpine的镜像的不同阶段。\nFROM maven:3.6-jdk-8-alpine AS builder … FROM openjdk:8-jre-jessie AS release-jessie COPY --from=builder /app/target/my-app-1.0-SNAPSHOT.jar / CMD [“java”, “-jar”, “/my-app-1.0-SNAPSHOT.jar”] FROM openjdk:8-jre-alpine AS release-alpine COPY --from=builder /app/target/my-app-1.0-SNAPSHOT.jar / CMD [“java”, “-jar”, “/my-app-1.0-SNAPSHOT.jar”] 要构建特定的镜像，我们可以使用–target参数：\ntime docker build --no-cache --target release-jessie . 不同的镜像风格（DRY /全局ARG） # ARG flavor=alpine FROM maven:3.6-jdk-8-alpine AS builder … FROM openjdk:8-jre-$flavor AS release COPY --from=builder /app/target/my-app-1.0-SNAPSHOT.jar / CMD [“java”, “-jar”, “/my-app-1.0-SNAPSHOT.jar”] ARG命令可以指定要构建的镜像。在上面的例子中，我们指定alpine为默认的镜像，但我们也可以在docker build命令中，通过–build-arg flavor=参数指定镜像。\ntime docker build --no-cache --target release --build-arg flavor=jessie . 并发 # 并发在构建Docker镜像时很重要，因为它会充分利用可用的CPU线程。在线性Dockerfile中，所有阶段均按顺序执行。通过多阶段构建，我们可以让较小的依赖阶段准备就绪，以供主阶段使用它们。\nBuildKit甚至带来了另一个性能上的好处。如果在以后的构建中不使用该阶段，则在结束时将直接跳过这些阶段，而不是对其进行处理和丢弃。\n下面是一个示例Dockerfile，其中网站的资产是在一个assets阶段中构建的：\nFROM maven:3.6-jdk-8-alpine AS builder … FROM tiborvass/whalesay AS assets RUN whalesay “Hello DockerCon!” \u0026gt; out/assets.html FROM openjdk:8-jre-alpine AS release COPY --from=builder /app/my-app-1.0-SNAPSHOT.jar / COPY --from=assets /out /assets CMD [“java”, “-jar”, “/my-app-1.0-SNAPSHOT.jar”] 这是另一个Dockerfile，其中分别编译了C和C ++库，并在builder以后使用该阶段。\nFROM maven:3.6-jdk-8-alpine AS builder-base … FROM gcc:8-alpine AS builder-someClib … RUN git clone … ./configure --prefix=/out \u0026amp;\u0026amp; make \u0026amp;\u0026amp; make install FROM g++:8-alpine AS builder-some CPPlib … RUN git clone … \u0026amp;\u0026amp; cmake … FROM builder-base AS builder COPY --from=builder-someClib /out / COPY --from=builder-someCpplib /out / BuildKit应用程序缓存 # BuildKit具有程序包管理器缓存的特殊功能。以下是一些缓存文件夹位置的示例：\n包管理器 路径\napt /var/lib/apt/lists go ~/.cache/go-build go-modules $GOPATH/pkg/mod npm ~/.npm pip ~/.cache/pip 我们可以将此Dockerfile与上面介绍的在一致的环境中从源代码构建中介绍的Dockerfile进行比较。这个较早的Dockerfile没有特殊的缓存处理。我们可以使用–mount=type=cache来做到这一点。\nFROM maven:3.6-jdk-8-alpine AS builder WORKDIR /app RUN --mount=target=. --mount=type=cache,target /root/.m2 \\ \u0026amp;\u0026amp; mvn package -DoutputDirectory=/ FROM openjdk:8-jre-alpine COPY --from=builder /app/target/my-app-1.0-SNAPSHOT.jar / CMD [“java”, “-jar”, “/my-app-1.0-SNAPSHOT.jar”] BuildKit的安全功能 # BuildKit具有安全功能，下面的示例中，我们使用了–mount=type=secret隐藏了一些机密文件，例如~/.aws/credentials。\nFROM \u0026lt;baseimage\u0026gt; RUN … RUN --mount=type=secret,id=aws,target=/root/.aws/credentials,required \\ ./fetch-assets-from-s3.sh RUN ./build-scripts.sh 要构建此Dockerfile，需要使用–secret参数：\ndocker build --secret id=aws,src=~/.aws/credentials 还有为了提高安全性，避免使用诸如COPY ./keys/private.pem /root .ssh/private.pem之类的命令，我们可以使用BuildKit中的ssh解决此问题：\nFROM alpine RUN apk add --no-cache openssh-client RUN mkdir -p -m 0700 ~/.ssh \u0026amp;\u0026amp; ssh-keyscan github.com \u0026gt;\u0026gt; ~/.ssh/known_hosts ARG REPO_REF=19ba7bcd9976ef8a9bd086187df19ba7bcd997f2 RUN --mount=type=ssh,required git clone git@github.com:org/repo /work \u0026amp;\u0026amp; cd /work \u0026amp;\u0026amp; git checkout -b $REPO_REF 要构建此Dockerfile，你需要在ssh-agent中加载到你的SSH私钥。\neval $(ssh-agent) ssh-add ~/.ssh/id_rsa # this is the SSH key default location docker build --ssh=default . 结论 # 本文，我们介绍了使用Docker BuildKit优化Dockerfile，并因此加快了镜像构建时间。这些速度的提高，可以帮助我们提高效率和节省计算能力。\n"},{"id":183,"href":"/docs/%E6%B5%81%E9%87%8F%E4%BD%95%E5%A4%84%E6%9D%A5%E4%BD%95%E5%A4%84%E5%8E%BB-liu-liang-he-chu-lai-he-chu-qu/","title":"流量何处来何处去 2024-04-03 14:56:20.089","section":"Docs","content":" Kubernetes 网络概述 # 在 Kubernetes 集群中，每个容器都在一个 Pod 中运行，并且 Pod 由 Kubernetes 控制平面调度到节点（物理机或虚拟机）上。Kubernetes 网络为 Pod 之间、服务之间以及外部资源之间的通信提供了一种方式。\nKubernetes 使用扁平网络模型，所有 Pod 都可以直接相互通信，无论它们运行在哪个节点上。为了实现这一点，Kubernetes 设置了一个跨越集群中所有节点的虚拟网络，并为每个 Pod 分配了该网络中唯一的 IP 地址。\n你是否想知道 Kubernetes 中的 API 流量去了哪里？\n仔细想想，你在 Kubernetes 集群上执行的任何操作都是 API 调用。无论你是创建新的 Pod 还是列出 Ingress Controller，这都是 API 调用。\nKubeshark\n在这篇博文中，你将了解如何利用 Kubeshark（适用于 Kubernetes 的 API 流量查看器）提供对 Kubernetes 内部网络的实时协议级可见性\u0026mdash;捕获、剖析和监控所有(跨容器、pod、节点和集群)进出的流量和有效负载。\n什么是 Wireshark？ # 在深入了解 Kubeshark 之前，有必要简单介绍一下 Wireshark。为什么？因为当你看到谈论 Kubeshark 时，你会看到它与 Wireshark 的比较。\nWireshark是一款免费的开源数据包分析器。它用于网络故障排除、分析。Wireshark是跨平台的，并使用Qt工具来实现其用户界面，使用pcap来捕获数据包；它可以在 Linux、macOS、BSD、Solaris、其他一些类 Unix 操作系统和 Microsoft Windows 上运行。还有一个基于终端（非 GUI）的版本，称为 TShark。Wireshark 与 tcpdump 非常相似，但具有图形化以及集成的排序和过滤选项。 # TCPDump/Wireshark 使我们能够在微观层面上可视化和理解网络中发生的情况。想象一下，如果这样的事情在 Kubernetes 中是可能的，你就可以看到工作负载、Pod或服务帐户间相互交互时到底会发生什么。 # Kubeshark 与 Wireshark 非常相似，会展示每次调用时发生的情况。不同之处在于 Wireshark 查看数据包。Kubeshark 查看 API 调用。 # API 调用？ # 在 Kubernetes 中，你可能看起来没有进行 API 调用。你运行一些看起来像的命令kubectl get pods，并认为它只是一个命令，但该命令实际上是在进行 API 调用。当你运行时kubectl get pods，你实际上正在做的是对 Kubernetes API 服务器执行GET请求。 # 无论你是在创建新的 Kubernetes 资源（例如部署或服务），还是在检索有关 Kubernetes 资源的信息，你都在进行 API 调用。每当你与 Kubernetes 交互时，你都在进行 API 调用。 # 正如你可以想象的那样，有大量流量流向 API 服务器。假如，我们能够监控流量是多么有意义，并且对于故障排除特别有帮助。 # 为什么选择 Kubeshark？ # Kubeshark 填补了 Kubernetes 长期以来的空白——以可视化方式查看 API 调用。 # 你能看到在没有 Kubeshark 的情况下完成的 API 调用吗？绝对地。 # 下面的命令将向你展示用于检索 Pod 信息的 API 调用。 # kubectl get pods -v=8 你会看到类似于下面显示 API 调用的输出。\n问题是它不是很直观，而且有很多信息，看起来有点混乱。这就是 Kubeshark 的可视化发挥作用的地方，它满足了以更清晰的方式查看 API 调用的需求。\nKubeshark 架构分析 # Kubeshark 架构分析\nKubeshark 由四个协同工作的组件组成：\nCLI # Kubeshark 客户端的二进制发行版，由 Go 语言编写。 # 它通过 K8s API 与你的集群通信，以部署 Hub 和 Worker Pod。 # Hub # Hub 是一个 pod，充当 Workers 的网关。它托管一个 HTTP 服务器并用于以下目的： # • 接受 WebSocket 连接和随附的过滤器。 # • 与Worker建立新的 WebSocket 连接。 # • 接收来自Worker的流量。 # • 将结果流式传输回请求者。 # • 通过 HTTP 调用配置工作线程状态。 # Worker # 它作为DaemonSet部署到你的集群中，以确保集群中的每个节点都被 Kubeshark 覆盖。 # Worker 包含网络嗅探器和内核跟踪器的实现。它从所有网络接口捕获数据包，重新组装 TCP 流，如果它们是可解析的，则将它们存储为PCAP文件。Worker 通过 WebSocket 连接将收集到的流量传输到 Hub。 # Kubeshark 存储原始数据包并根据查询按需解析它们。 # Worker 本身可以用作计算机上的网络嗅探器，而无需 Kubernetes 集群。 # 分布式协议解析器 # 应用层协议的解析器分布在整个集群中(DaemonSet配置)。 # 基于PCAP的分布式存储 # Kubeshark 使用基于 PCAP 的分布式存储，其中每个工作线程将捕获的 TCP 流，并存储在节点的根文件系统中。 # Kubeshark 默认设置为 200MB 的存储限制。该限制可以通过 CLI 选项进行更改。 # 网络开销低 # 为了减少潜在的网络开销，仅根据请求通过网络发送一小部分流量。 # Web UI # 前端是一个 React 应用程序，它通过 WebSocket 与 Hub 通信，显示捕获的流量。\nWeb UI\nKubeshark 安装 # 行万里路，此处相逢，共话云原生之道。 偶逗趣事，明月清风，与君同坐。\n55篇原创内容\n公众号\nKubeshark 入门很简单。你只需下载 CLI 并运行它。\n你可以使用 shell 脚本下载适合你的操作系统和 CPU 架构的二进制文件：\nsh \u0026lt;(curl -Ls https://kubeshark.co/install) # 或者使用以下命令指定版本 curl -Lo kubeshark https://github.com/kubeshark/kubeshark/releases/download/41.6/kubeshark_linux_amd64 \u0026amp;\u0026amp; chmod 755 kubeshark 其他方法可以在https://docs.kubeshark.co/en/install找到\nKubeshark 示例 # 以下是如何使用 Kubeshark CLI 开始捕获 Kubernetes 集群中的流量的一些示例：\nkubeshark tap kubeshark tap -A kubeshark tap -n sock-shop \u0026#34;(catalo*|front-end*)\u0026#34; 运行 CLI 后，默认情况下，浏览器窗口将在 localhost:8899 打开。\n为了演示 kubeshark 的使用，让我们将示例应用程序部署到 kuberenetes 集群中。我们正在部署 kuberentes 官方文档中留言板应用程序。\n使用 Redis 部署 PHP 留言板应用程序:\nhttps://kubernetes.io/docs/tutorials/stateless-application/guestbook/ # 1. 启动 Redis 数据库--留言板应用程序使用 Redis 存储数据 # 创建 Redis 领导者 kubectl apply -f https://k8s.io/examples/application/guestbook/redis-leader-deployment.yaml kubectl apply -f https://k8s.io/examples/application/guestbook/redis-leader-service.yaml # 设置 Redis 跟随者 kubectl apply -f https://k8s.io/examples/application/guestbook/redis-follower-deployment.yaml kubectl apply -f https://k8s.io/examples/application/guestbook/redis-follower-service.yaml # 2. 留言板的 Web 服务器 # 留言板 应用使用 PHP 前端。该前端被配置成与后端的 Redis 跟随者或者 领导者服务通信，具体选择哪个服务取决于请求是读操作还是写操作。 前端对外暴露一个 JSON 接口，并提供基于 jQuery-Ajax 的用户体验。 kubectl apply -f https://k8s.io/examples/application/guestbook/frontend-deployment.yaml kubectl apply -f https://k8s.io/examples/application/guestbook/frontend-service.yaml # 通过 kubectl port-forward 查看前端服务 kubectl port-forward svc/frontend 8080:80 在浏览器中加载 http://localhost:8080 页面以查看 留言板应用程序 # 验证已安装的应用程序正在运行。 # 让我们为应用程序的命名空间运行 kubeshark。命令如下： # kubeshark tap -n \u0026lt;namespace name\u0026gt; 运行 CLI 后，默认情况下，浏览器窗口将在 localhost:8899 打开。\n让我们产生一些流量。\n在本博客中，我们了解了如何使用Kubeshark分析 Kubernetes 集群中的网络流量，它为 Kubernetes 管理员和开发人员提供了几个优势：\n\\1. 增强的可见性：Kubeshark 提供 Kubernetes 集群中 Pod 和服务之间网络流量的实时可见性。这可以帮助你排除和诊断与网络相关的问题，例如响应时间慢或延迟高。 # \\2. 易于使用：Kubeshark 是一个 kubectl 插件，这意味着它可以在 Kubernetes 命令行界面中轻松安装和使用。你无需学习新工具或界面即可使用 Kubeshark。 # \\3. 粒度过滤：Kubeshark 允许你根据各种标准过滤网络流量，例如协议、源 IP 地址、目标 IP 地址和端口号。这可以帮助你关注特定的流量模式并识别潜在的安全威胁。 # \\4. 与 Wireshark 集成：Kubeshark 使用 Wireshark 作为其底层网络协议分析器，这是网络分析社区中广泛使用且成熟的工具。这使你可以利用 Wireshark 的高级功能，例如协议解码和数据包分析。 # 结论 # Kubeshark 是一款功能强大的工具，早期称为 Mizu，由 UP9 开发的 API 流量查看器，后来开源，现在更名为 Kubeshark，标志着 Wireshark 为 K8s 重生。 # 虽然有一些复杂的 K8s 运行时安全工具，例如 Sysdig Falco、Cilium Tetragon 都是利用 eBPF 功能构建的，但文档中提到 Kubeshark 也使用 eBPF，但更专注于提供微观层面的可视化层，以更好帮助我们了解网络通信。Kubeshark 提供的数据肯定可以用于运行时检测的几个方面。 # 总体而言，Kubeshark 为 Kubernetes 集群中的网络分析提供了强大且灵活的工具，其与 Wireshark 的集成以及易用性使其成为 Kubernetes 管理员和开发人员的选择。 # "},{"id":184,"href":"/docs/%E6%B8%85%E7%90%86docker%E9%95%9C%E5%83%8F-qing-li-docker-jing-xiang/","title":"清理docker镜像 2024-08-02 17:54:48.594","section":"Docs","content":"清理docker镜像\n要清理Docker镜像，可以使用以下命令：\n删除未使用的Docker镜像： docker image prune docker image prune -a 删除所有停止的容器： docker container prune 删除未使用的网络： docker network prune 删除未使用的卷： docker volume prune 删除未使用的构建缓存： docker builder prune 如果你想删除特定的镜像，可以使用：\ndocker rmi [IMAGE_ID_OR_NAME] 请注意，替换掉[IMAGE_ID_OR_NAME]为你想删除的镜像ID或名称。\n如果你想删除所有停止的容器、未使用的网络和卷，可以一次性执行：\ndocker system prune 如果你想删除所有未使用的镜像（包括那些被停止的容器所使用的镜像），可以使用：\ndocker image prune -a 这些命令应该帮助你清理Docker环境中不再需要的对象。在运行这些命令之前，请确保你不再需要这些对象，因为这将是不可逆的操作。\n清理docker镜像 # 清理Docker镜像可以通过多种方法进行，包括删除特定时间前的镜像、按照大小排序、使用脚本自动化、定时自动清理以及删除特定模式的镜像。以下是一些具体的方法：12\n删除特定时间前的镜像：使用\ndocker image prune --filter \u0026#34;until=168h\u0026#34; 命令可以删除所有创建时间超过168小时（7天）的镜像。\n按照大小排序：通过\ndocker images --format \u0026#39;{{.Size}}\\t{{.Repository}}:{{.Tag}}\u0026#39; | sort -hr 命令，可以直观地看到哪些镜像占用空间较大，然后手动删除。\n使用脚本自动化：编写一个简单的脚本来自动化清理过程，例如清理所有未使用的镜像和容器，或者删除所有以特定版本开头的镜像。\n#!/bin/bash # 清理所有未使用的镜像和容器 docker system prune -af # 删除所有1.0版本开头的镜像 docker images | grep \u0026#39;1.0\u0026#39; | awk \u0026#39;{print $3}\u0026#39; | xargs docker rmi 定时自动清理：利用cron定时任务，可以设定周期性清理工作，例如每天凌晨两点执行清理脚本。\n# 每天凌晨两点执行清理 0 2 * * * /path/to/your/script.sh 记得替换 \u0026lsquo;仓库名\u0026rsquo; 为你的实际仓库名。\n删除特定模式的镜像：使用grep命令和awk命令组合，可以删除特定仓库名或模式的镜像。\ndocker images | grep \u0026#39;仓库名.*1.0\u0026#39; | awk \u0026#39;{print $3}\u0026#39; | xargs docker rmi 此外，还可以通过以下方法清理Docker中的未使用资源：\n清除所有未使用或悬空镜像、容器、卷和网络：使用\ndocker system prune 命令可以清理任何悬空（dangling）的资源。添加-a标志可以同时删除所有已停止的容器和所有未使用的镜像。\n删除悬空的Docker镜像：通过\ndocker images --filter \u0026#34;dangling=true\u0026#34; 命令可以定位并删除那些与任何标记过的镜像无关的层，即悬空镜像。\n自动清理：结合Crontab实现每天定时自动清理，例如设置每天凌晨一点执行清理7天以上未使用的镜像的命令。\n再次提醒，执行删除操作前，务必确认不会丢失重要数据。定期备份和清理是保持 Docker 环境健康的关键。同时，也要确保没有正在运行的服务会受到影响。\n通过上面的分享，相信大家已经对如何清理 Docker 镜像有了更加全面和深入的认识。希望这些方法能帮到你，也欢迎在技术交流群分享你的经验或提出问题。\n那么，今天的内容就到这里。\n查看docker镜像用了多少空间 # docker system df\n这是最直接的方法之一。运行\ndocker system df 命令可以列出Docker镜像、容器、数据卷等占用的磁盘空间。如果想要更详细的信息，可以加上-v参数，即\ndocker system df -v ，这会显示每个镜像、容器和数据卷的详细信息。\ndocker images\n使用docker images命令可以查看所有已下载的Docker镜像及其大小。这个命令会列出镜像的仓库名、标签、镜像ID、创建时间和大小等信息。\ndocker images 输出示例：\nREPOSITORY TAG IMAGE ID CREATED SIZE nginx latest abc123def456 7 days ago 146MB 这里的SIZE列显示了每个镜像的大小。\n四、清理Docker空间 # 如果你发现Docker占用了大量空间，并且想要清理不再使用的镜像、容器和数据卷，可以使用以下命令：\n清理未使用的镜像：\ndocker image prune 清理未使用的容器：\ndocker container prune 清理未使用的网络和卷（谨慎使用，因为这会删除所有未附加到容器的卷）：\ndocker system prune 这些命令可以帮助你释放Docker占用的磁盘空间。\n综上所述，查看Docker镜像占用的空间主要通过docker system df、docker images等命令来实现。同时，了解Docker镜像在文件系统中的存储方式也有助于更深入地理解其空间占用情况。\n在Docker中，overlay2 是默认的存储驱动之一，用于在宿主机上管理容器的镜像和文件系统。随着容器的运行和镜像的拉取，overlay2 可能会占用大量的磁盘空间。回收 overlay2 下可回收的空间主要涉及到删除不再使用的镜像、容器以及清理悬挂（dangling）的镜像层。以下是一些步骤和命令，可以帮助你回收 overlay2 下的空间：\n1. 删除不再使用的容器 # 首先，你可以删除所有已停止的容器。这些容器不再运行，但它们的文件系统仍然占用磁盘空间。\ndocker container prune 或者，你可以使用更具体的命令来删除特定的容器：\ndocker rm $(docker ps -aq -f status=exited) 2. 删除不再使用的镜像 # 接着，删除那些不再被任何容器使用的镜像。这些镜像可能是旧版本或不再需要的。\ndocker image prune 或者，你可以使用 -a 选项来删除所有未被任何容器引用的镜像（包括悬空镜像）：\ndocker image prune -a 3. 清理悬挂的镜像层 # 在Docker中，镜像是由多个层组成的。有时候，由于镜像的删除操作不彻底，一些层可能仍然保留在磁盘上，成为悬挂的层。虽然 Docker 通常会自动处理这些悬挂层，但有时候你可能需要手动触发清理。\nDocker 的 docker image prune 命令通常已经足够处理悬挂的镜像层，但如果你发现空间没有被充分回收，可能需要考虑重启 Docker 服务或使用其他高级工具（如 docker system prune）来清理。\n4. 使用 Docker System Prune # docker system prune 命令会删除所有未使用的容器、网络、镜像（可选）和卷（可选），是清理 Docker 占用空间的一个非常有效的工具。\n# 删除所有未使用的容器、网络、镜像和卷 docker system prune -a --volumes # 如果你只想删除容器和网络，而不删除镜像和卷 docker system prune 请注意，-a 选项会删除所有未使用的镜像，包括那些未被标记的镜像。--volumes 选项会删除所有未挂载的卷。\n5. 检查磁盘空间 # 在进行了上述清理操作后，你可以使用 df -h 命令来检查磁盘空间的使用情况，确认是否已经释放了足够的空间。\n结论 # 通过删除不再使用的容器、镜像和清理悬挂的镜像层，你可以有效地回收 overlay2 存储驱动下的磁盘空间。如果仍然需要更多空间，考虑调整 Docker 的存储设置或升级你的磁盘硬件。\ncat \u0026gt; /etc/docker/daemon.json \u0026lt;\u0026lt; EOF { \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://docker.m.daocloud.io\u0026#34;, \u0026#34;https://hub.roker.org\u0026#34;, \u0026#34;https://si7y70hh.mirror.aliyuncs.com\u0026#34;, \u0026#34;https://registry.docker-cn.com\u0026#34;, \u0026#34;https://docker.mirrors.ustc.edu.cn\u0026#34;, \u0026#34;http://hub-mirror.c.163.com\u0026#34;, \u0026#34;https://dockerhub.azk8s.cn\u0026#34;, \u0026#34;https://hub.uuuadc.top\u0026#34;, \u0026#34;https://docker.anyhub.us.kg\u0026#34;, \u0026#34;https://dockerhub.jobcher.com\u0026#34;, \u0026#34;https://dockerhub.icu\u0026#34;, \u0026#34;https://docker.ckyl.me\u0026#34;, \u0026#34;https://docker.awsl9527.cn\u0026#34;, \u0026#34;https://docker.chenby.cn\u0026#34;, \u0026#34;https://docker.hpcloud.cloud\u0026#34;, \u0026#34;https://atomhub.openatom.cn\u0026#34;], \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;], \u0026#34;insecure-registries\u0026#34;: [\u0026#34;http://192.168.200.250\u0026#34;], \u0026#34;log-driver\u0026#34;: \u0026#34;json-file\u0026#34;, \u0026#34;log-opts\u0026#34;: { \u0026#34;max-size\u0026#34;: \u0026#34;1000m\u0026#34;,\u0026#34;max-file\u0026#34;: \u0026#34;6\u0026#34; }, \u0026#34;storage-driver\u0026#34;: \u0026#34;overlay2\u0026#34; } EOF "},{"id":185,"href":"/docs/%E6%B8%85%E7%90%86%E6%AE%8B%E7%95%99%E7%9A%84calico%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6-qing-li-can-liu-de-calico-wang-luo/","title":"清理残留的calico网络插件 2024-04-03 15:07:38.854","section":"Docs","content":"#停止docker kubelet kube-proxy服务 systemctl stop docker systemctl stop kubelet systemctl stop kube-proxy\n删除残留路径 # rm -rf /etc/cni rm -rf /opt/cni rm -rf /run/calico rm -rf /var/lib/calico rm -rf /var/lib/cni rm -rf /var/run/calico\n清理网络接口 # ip link delete vxlan.calico ip link delete calia09f92a66fa@if3\niptables -F \u0026amp;\u0026amp; iptables -t nat -F\nsystemctl start docker systemctl start kubelet systemctl start kube-proxy\nsystemctl status docker systemctl status kubelet systemctl status kube-proxy\n"},{"id":186,"href":"/docs/%E7%A3%81%E7%9B%98%E6%95%B0%E6%8D%AE%E6%81%A2%E5%A4%8D-ci-pan-shu-ju-hui-fu/","title":"磁盘数据恢复 2024-04-03 14:59:27.839","section":"Docs","content":" 1.背景 # 如果使用rm -rf 误删除数据，我们怎么恢复误删除的数据呢？\n2.场景分析 # 删除数据主要有两种场景：\n场景一 在执行rm -rf 删除文件时，该文件正在被进程使用。 场景二 这个文件没有被进程占用，从而被误删除 在场景一，对于进程正在使用的场景，数据可以恢复是因为Linux里每个文件都有2个link计数器：i_count和i_nlink # i_count: 当一个文件被一个进程引用时，它的数值会加1，也就是说它记录的是文件被进程引用的次数。 # i_nlink: 记录文件产生硬链接的个数。 # Linux系统只有在两个数值都清零的时候，文件才被系统认为是删除的，假设此时删除文件有进程在使用，那么i_conunt数值不为0。 # 在场景二，此时i_count和i_nlink都为0，inode连接信息已经被删除了，我们就需要通过存放文件的block单元，做数据块的数据找回。 # 找回数据需要依赖两个关键的参数： # inode: 用于存放文件的相关元数据，它的元数据里会有一个类似于索引的值，能够索引到后面具体存放数据的block单元 # block: 是一个数据块，用来实际存放数据 # 所以理论上可以通过block块找回数据，因为上面保存着真实的数据。 # 风险：如果有进程在在不断地往磁盘写数据时，需要申请新的block块，如果操作系统分配已删除的block块时，新写入的数据就会覆盖原来的数据，此时就会造成数据真正丢失。 # 在这种情况下，应该第一时间umount目录所在的磁盘，或者不对磁盘进行任何写入，以保证理论数据还存在磁盘上，那么还可以通过相关分析找回数据。 # 3.场景一演示 # 场景一搭建：\n首先准备测试的文件aaa.txt，内容为运维贼船公众号的域名，在终端一中执行：\n# 创建测试文件夹 mkdir /tmp/test # 创建测试文件 echo \u0026#34;aaa.al\u0026#34; \u0026gt; /tmp/test/aaa.txt 创建完文件后，使用进程打开它并保持占用，这里用tail -f来模拟：\ntail -f /tmp/test/aaa.txt 此时，新开一个终端二，删除aaa.txt文件。\nrm -rf /tmp/test/aaa.txt 此时的状态为终端一仍在占用进程，终端二中把文件删除掉了。\n场景一恢复：\n下面执行恢复操作，使用lsof命令：\n# lsof | grep 删除的文件名，如： lsof | grep aaa.txt 可以看到查询出来的结果处于“deleted”的状态，我需要找到此进程的pid，比如我这里的结果pid为1364000。\n根据pid号查找文件句柄：\n# cd /proc/记录的pid号/fd，如： cd /proc/1364000/fd ll 恢复文件，执行以下命令，可以看到数据已经恢复回来了。\ncp 3 /tmp/test/aaa.txt.bak cat /tmp/test/aaa.txt.bak 4.场景二演示 # 场景二搭建：\n我们这里使用单独的一块硬盘来做演示，我这里的硬盘是/sdb1，格式为ext3. # 挂载的目录为/test # 我这里在挂载的目录创建aaa.txt，写入内容和aaa的文件夹。 # echo \u0026#34;aaa.al\u0026#34; \u0026gt; /test/aaa.txt mkdir -p /test/aaa cd /test ls 然后对硬盘内容删除： # cd /test rm -rf * ls 场景二恢复：\n恢复前需要对硬盘进行取消挂载： # umount /test -l # 创建一个用于恢复数据的目录 mkdir /tmp/test cd /tmp/test 执行extundelete命令，如果没有此命令，可以直接安装： # yum -y install epel-release yum -y install extundelete 完成后，执行命令，可以看到最后显示出误删除的文件，状态为Deleted。 # extundelete /dev/sdb1 --inode 2 开始恢复文件，找到我们需要恢复文件的Inode number，比如我这里图示为12，则执行以下命令。\nextundelete /dev/sdb1 --restore-inode 12 执行后，会在当前目录下生成RECOVERED_FILES目录，里面包含我们恢复的文件： # 通过查看文件内容，发现和我们需要恢复的文件一致，只是文件名不一样，手动修改即可。 # 同样的方式，我们还可以恢复文件夹和所有文件，命令格式如下：\n1)通过inode号(extundelete /dev/sdb1 \u0026ndash;restore-inode InodeNum) # 2)通过file文件名(extundelete /dev/sdb1 \u0026ndash;restore-file FileName) # 3)通过directory目录名(extundelete /dev/sdb1 \u0026ndash;restore-directory DirectoryName) # 4)all全部恢复(extundelete /dev/sdb1 \u0026ndash;restore-all) # 5.总结 # 通过上面演示的场景一，文件进程在使用中被删除；场景二未有进程占用时文件被删除两种情况做了演示恢复的操作。需要注意的是，extundelete仅支持ext系列格式分区，不支持xfs文件系统，如果需要恢复xfs文件，可以关注我后续的文章，谢谢！ # "},{"id":187,"href":"/docs/%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85kubephere-li-xian-an-zhuang-kubephere/","title":"离线安装kubephere 2024-04-03 14:59:59.885","section":"Docs","content":" 离线安装 # 离线安装几乎与在线安装相同，不同之处是您必须创建一个本地仓库来托管 Docker 镜像。本教程演示了如何在离线环境中将 KubeSphere 安装到 Kubernetes 上。\n开始下方步骤之前，请先参阅准备工作。\n步骤 1：准备一个私有镜像仓库 # 您可以使用 Harbor 或者其他任意私有镜像仓库。本教程以 Docker 仓库作为示例，并使用自签名证书（如果您有自己的私有镜像仓库，可以跳过这一步）。\n使用自签名证书 # 执行以下命令生成您自己的证书：\nmkdir -p certs openssl req \\ -newkey rsa:4096 -nodes -sha256 -keyout certs/domain.key \\ -x509 -days 36500 -out certs/domain.crt 当您生成自己的证书时，请确保在字段 Common Name 中指定一个域名。例如，本示例中该字段被指定为 dockerhub.kubekey.local。 # 启动 Docker 仓库 # 执行以下命令启动 Docker 仓库： # docker run -d \\ --restart=always \\ --name registry \\ -v \u0026#34;$(pwd)\u0026#34;/certs:/certs \\ -v /mnt/registry:/var/lib/registry \\ -e REGISTRY_HTTP_ADDR=0.0.0.0:443 \\ -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt \\ -e REGISTRY_HTTP_TLS_KEY=/certs/domain.key \\ -p 443:443 \\ registry:2 备注 # Docker 使用 /var/lib/docker 作为默认路径来存储所有 Docker 相关文件（包括镜像）。建议您添加附加存储卷，分别给 /var/lib/docker 和 /mnt/registry 挂载至少 100G。请参见 fdisk 的参考命令。 # 配置仓库 # 在 /etc/hosts 中添加一个条目，将主机名（即仓库域名；在本示例中是 dockerhub.kubekey.local）映射到您机器的私有 IP 地址，如下所示。 # # docker registry 192.168.0.2 dockerhub.kubekey.local 执行以下命令，复制证书到指定目录，并使 Docker 信任该证书。 # mkdir -p /etc/docker/certs.d/dockerhub.kubekey.local cp certs/domain.crt /etc/docker/certs.d/dockerhub.kubekey.local/ca.crt 备注 # 证书的路径与域名相关联。当您复制路径时，如果与上面设置的路径不同，请使用实际域名。 # 要验证私有仓库是否有效，您可以先复制一个镜像到您的本地机器，然后使用 docker push 和 docker pull 来测试。 # 步骤 2：准备安装镜像 # 当您在离线环境中安装 KubeSphere 时，需要事先准备一个包含所有必需镜像的镜像包。 # 使用以下命令从能够访问互联网的机器上下载镜像清单文件 images-list.txt： # curl -L -O https://github.com/kubesphere/ks-installer/releases/download/v3.4.1/images-list.txt 备注\n该文件根据不同的模块列出了 ##+modulename 下的镜像。您可以按照相同的规则把自己的镜像添加到这个文件中。若需要查看完整文件，请参见附录。\n下载 offline-installation-tool.sh。\ncurl -L -O https://github.com/kubesphere/ks-installer/releases/download/v3.4.1/offline-installation-tool.sh 使 .sh 文件可执行。\nchmod +x offline-installation-tool.sh 您可以执行命令 ./offline-installation-tool.sh -h 来查看如何使用脚本：\nroot@master:/home/ubuntu# ./offline-installation-tool.sh -h Usage: ./offline-installation-tool.sh [-l IMAGES-LIST] [-d IMAGES-DIR] [-r PRIVATE-REGISTRY] [-v KUBERNETES-VERSION ] Description: -b : save kubernetes\u0026#39; binaries. -d IMAGES-DIR : the dir of files (tar.gz) which generated by `docker save`. default: ./kubesphere-images -l IMAGES-LIST : text file with list of images. -r PRIVATE-REGISTRY : target private registry:port. -s : save model will be applied. Pull the images in the IMAGES-LIST and save images as a tar.gz file. -v KUBERNETES-VERSION : download kubernetes\u0026#39; binaries. default: v1.17.9 -h : usage message 在 offline-installation-tool.sh 中拉取镜像。\n./offline-installation-tool.sh -s -l images-list.txt -d ./kubesphere-images 备注\n您可以根据需要选择拉取的镜像。例如，如果已经有一个 Kubernetes 集群了，您可以在 images-list.text 中删除 ##k8s-images 和在它下面的相关镜像。\n步骤 3：推送镜像至私有仓库 # 将打包的镜像文件传输至您的本地机器，并运行以下命令把它推送至仓库。\n./offline-installation-tool.sh -l images-list.txt -d ./kubesphere-images -r dockerhub.kubekey.local 备注\n命令中的域名是 dockerhub.kubekey.local。请确保使用您自己仓库的地址。\n步骤 4：下载部署文件 # 与在现有 Kubernetes 集群上在线安装 KubeSphere 相似，您也需要事先下载 cluster-configuration.yaml 和 kubesphere-installer.yaml。\n执行以下命令下载这两个文件，并将它们传输至您充当任务机的机器，用于安装。\ncurl -L -O https://github.com/kubesphere/ks-installer/releases/download/v3.4.1/cluster-configuration.yaml curl -L -O https://github.com/kubesphere/ks-installer/releases/download/v3.4.1/kubesphere-installer.yaml 编辑 cluster-configuration.yaml 添加您的私有镜像仓库。例如，本教程中的仓库地址是 dockerhub.kubekey.local，将它用作 .spec.local_registry 的值，如下所示：\nspec: persistence: storageClass: \u0026#34;\u0026#34; authentication: jwtSecret: \u0026#34;\u0026#34; local_registry: dockerhub.kubekey.local # Add this line manually; make sure you use your own registry address. 备注\n您可以在该 YAML 文件中启用可插拔组件，体验 KubeSphere 的更多功能。有关详情，请参考启用可插拔组件。\n编辑完成后保存 cluster-configuration.yaml。使用以下命令将 ks-installer 替换为您自己仓库的地址。\nsed -i \u0026#34;s#^\\s*image: kubesphere.*/ks-installer:.*# image: dockerhub.kubekey.local/kubesphere/ks-installer:v3.4.0#\u0026#34; kubesphere-installer.yaml 警告\n命令中的仓库地址是 dockerhub.kubekey.local。请确保使用您自己仓库的地址。\n步骤 5：开始安装 # 确定完成上面所有步骤后，您可以执行以下命令。\nkubectl apply -f kubesphere-installer.yaml kubectl apply -f cluster-configuration.yaml 步骤 6：验证安装 # 安装完成后，您会看到以下内容：\nadmin Hangzhou@2024. ##################################################### ### Welcome to KubeSphere! ### ##################################################### Console: http://192.168.0.2:30880 Account: admin Password: P@88w0rd NOTES： 1. After logging into the console, please check the monitoring status of service components in the \u0026#34;Cluster Management\u0026#34;. If any service is not ready, please wait patiently until all components are ready. 2. Please modify the default password after login. ##################################################### https://kubesphere.io 20xx-xx-xx xx:xx:xx ##################################################### 现在，您可以通过 http://{IP}:30880 使用默认帐户和密码 admin/P@88w0rd 访问 KubeSphere 的 Web 控制台。\n备注\n要访问控制台，请确保在您的安全组中打开端口 30880。\n附录 # KubeSphere 3.4 镜像清单 # ##kubesphere-images registry.cn-beijing.aliyuncs.com/kubesphereio/ks-installer:v3.4.1 registry.cn-beijing.aliyuncs.com/kubesphereio/ks-apiserver:v3.4.1 registry.cn-beijing.aliyuncs.com/kubesphereio/ks-console:v3.4.1 registry.cn-beijing.aliyuncs.com/kubesphereio/ks-controller-manager:v3.4.1 registry.cn-beijing.aliyuncs.com/kubesphereio/kubectl:v1.20.0 registry.cn-beijing.aliyuncs.com/kubesphereio/kubefed:v0.8.1 registry.cn-beijing.aliyuncs.com/kubesphereio/tower:v0.2.1 registry.cn-beijing.aliyuncs.com/kubesphereio/minio:RELEASE.2019-08-07T01-59-21Z registry.cn-beijing.aliyuncs.com/kubesphereio/mc:RELEASE.2019-08-07T23-14-43Z registry.cn-beijing.aliyuncs.com/kubesphereio/snapshot-controller:v4.0.0 registry.cn-beijing.aliyuncs.com/kubesphereio/nginx-ingress-controller:v1.3.1 registry.cn-beijing.aliyuncs.com/kubesphereio/defaultbackend-amd64:1.4 registry.cn-beijing.aliyuncs.com/kubesphereio/metrics-server:v0.4.2 registry.cn-beijing.aliyuncs.com/kubesphereio/redis:5.0.14-alpine registry.cn-beijing.aliyuncs.com/kubesphereio/haproxy:2.0.25-alpine registry.cn-beijing.aliyuncs.com/kubesphereio/alpine:3.14 registry.cn-beijing.aliyuncs.com/kubesphereio/openldap:1.3.0 registry.cn-beijing.aliyuncs.com/kubesphereio/netshoot:v1.0 ##kubeedge-images registry.cn-beijing.aliyuncs.com/kubesphereio/cloudcore:v1.13.0 registry.cn-beijing.aliyuncs.com/kubesphereio/iptables-manager:v1.13.0 registry.cn-beijing.aliyuncs.com/kubesphereio/edgeservice:v0.3.0 ##gatekeeper-images registry.cn-beijing.aliyuncs.com/kubesphereio/gatekeeper:v3.5.2 ##openpitrix-images registry.cn-beijing.aliyuncs.com/kubesphereio/openpitrix-jobs:v3.3.2 ##kubesphere-devops-images registry.cn-beijing.aliyuncs.com/kubesphereio/devops-apiserver:ks-v3.4.1 registry.cn-beijing.aliyuncs.com/kubesphereio/devops-controller:ks-v3.4.1 registry.cn-beijing.aliyuncs.com/kubesphereio/devops-tools:ks-v3.4.1 registry.cn-beijing.aliyuncs.com/kubesphereio/ks-jenkins:v3.4.0-2.319.3-1 registry.cn-beijing.aliyuncs.com/kubesphereio/inbound-agent:4.10-2 registry.cn-beijing.aliyuncs.com/kubesphereio/builder-base:v3.2.2 registry.cn-beijing.aliyuncs.com/kubesphereio/builder-nodejs:v3.2.0 registry.cn-beijing.aliyuncs.com/kubesphereio/builder-maven:v3.2.0 registry.cn-beijing.aliyuncs.com/kubesphereio/builder-maven:v3.2.1-jdk11 registry.cn-beijing.aliyuncs.com/kubesphereio/builder-python:v3.2.0 registry.cn-beijing.aliyuncs.com/kubesphereio/builder-go:v3.2.0 registry.cn-beijing.aliyuncs.com/kubesphereio/builder-go:v3.2.2-1.16 registry.cn-beijing.aliyuncs.com/kubesphereio/builder-go:v3.2.2-1.17 registry.cn-beijing.aliyuncs.com/kubesphereio/builder-go:v3.2.2-1.18 registry.cn-beijing.aliyuncs.com/kubesphereio/builder-base:v3.2.2-podman registry.cn-beijing.aliyuncs.com/kubesphereio/builder-nodejs:v3.2.0-podman registry.cn-beijing.aliyuncs.com/kubesphereio/builder-maven:v3.2.0-podman registry.cn-beijing.aliyuncs.com/kubesphereio/builder-maven:v3.2.1-jdk11-podman registry.cn-beijing.aliyuncs.com/kubesphereio/builder-python:v3.2.0-podman registry.cn-beijing.aliyuncs.com/kubesphereio/builder-go:v3.2.0-podman registry.cn-beijing.aliyuncs.com/kubesphereio/builder-go:v3.2.2-1.16-podman registry.cn-beijing.aliyuncs.com/kubesphereio/builder-go:v3.2.2-1.17-podman registry.cn-beijing.aliyuncs.com/kubesphereio/builder-go:v3.2.2-1.18-podman registry.cn-beijing.aliyuncs.com/kubesphereio/s2ioperator:v3.2.1 registry.cn-beijing.aliyuncs.com/kubesphereio/s2irun:v3.2.0 registry.cn-beijing.aliyuncs.com/kubesphereio/s2i-binary:v3.2.0 registry.cn-beijing.aliyuncs.com/kubesphereio/tomcat85-java11-centos7:v3.2.0 registry.cn-beijing.aliyuncs.com/kubesphereio/tomcat85-java11-runtime:v3.2.0 registry.cn-beijing.aliyuncs.com/kubesphereio/tomcat85-java8-centos7:v3.2.0 registry.cn-beijing.aliyuncs.com/kubesphereio/tomcat85-java8-runtime:v3.2.0 registry.cn-beijing.aliyuncs.com/kubesphereio/java-11-centos7:v3.2.0 registry.cn-beijing.aliyuncs.com/kubesphereio/java-8-centos7:v3.2.0 registry.cn-beijing.aliyuncs.com/kubesphereio/java-8-runtime:v3.2.0 registry.cn-beijing.aliyuncs.com/kubesphereio/java-11-runtime:v3.2.0 registry.cn-beijing.aliyuncs.com/kubesphereio/nodejs-8-centos7:v3.2.0 registry.cn-beijing.aliyuncs.com/kubesphereio/nodejs-6-centos7:v3.2.0 registry.cn-beijing.aliyuncs.com/kubesphereio/nodejs-4-centos7:v3.2.0 registry.cn-beijing.aliyuncs.com/kubesphereio/python-36-centos7:v3.2.0 registry.cn-beijing.aliyuncs.com/kubesphereio/python-35-centos7:v3.2.0 registry.cn-beijing.aliyuncs.com/kubesphereio/python-34-centos7:v3.2.0 registry.cn-beijing.aliyuncs.com/kubesphereio/python-27-centos7:v3.2.0 registry.cn-beijing.aliyuncs.com/kubesphereio/argocd:v2.3.3 registry.cn-beijing.aliyuncs.com/kubesphereio/argocd-applicationset:v0.4.1 registry.cn-beijing.aliyuncs.com/kubesphereio/dex:v2.30.2 registry.cn-beijing.aliyuncs.com/kubesphereio/redis:6.2.6-alpine ##kubesphere-monitoring-images registry.cn-beijing.aliyuncs.com/kubesphereio/configmap-reload:v0.7.1 registry.cn-beijing.aliyuncs.com/kubesphereio/prometheus:v2.39.1 registry.cn-beijing.aliyuncs.com/kubesphereio/prometheus-config-reloader:v0.55.1 registry.cn-beijing.aliyuncs.com/kubesphereio/prometheus-operator:v0.55.1 registry.cn-beijing.aliyuncs.com/kubesphereio/kube-rbac-proxy:v0.11.0 registry.cn-beijing.aliyuncs.com/kubesphereio/kube-state-metrics:v2.6.0 registry.cn-beijing.aliyuncs.com/kubesphereio/node-exporter:v1.3.1 registry.cn-beijing.aliyuncs.com/kubesphereio/alertmanager:v0.23.0 registry.cn-beijing.aliyuncs.com/kubesphereio/thanos:v0.31.0 registry.cn-beijing.aliyuncs.com/kubesphereio/grafana:8.3.3 registry.cn-beijing.aliyuncs.com/kubesphereio/kube-rbac-proxy:v0.11.0 registry.cn-beijing.aliyuncs.com/kubesphereio/notification-manager-operator:v2.3.0 registry.cn-beijing.aliyuncs.com/kubesphereio/notification-manager:v2.3.0 registry.cn-beijing.aliyuncs.com/kubesphereio/notification-tenant-sidecar:v3.2.0 ##kubesphere-logging-images registry.cn-beijing.aliyuncs.com/kubesphereio/elasticsearch-curator:v5.7.6 registry.cn-beijing.aliyuncs.com/kubesphereio/opensearch-curator:v0.0.5 registry.cn-beijing.aliyuncs.com/kubesphereio/elasticsearch-oss:6.8.22 registry.cn-beijing.aliyuncs.com/kubesphereio/opensearch:2.6.0 registry.cn-beijing.aliyuncs.com/kubesphereio/opensearch-dashboards:2.6.0 registry.cn-beijing.aliyuncs.com/kubesphereio/fluentbit-operator:v0.14.0 registry.cn-beijing.aliyuncs.com/kubesphereio/docker:19.03 registry.cn-beijing.aliyuncs.com/kubesphereio/fluent-bit:v1.9.4 registry.cn-beijing.aliyuncs.com/kubesphereio/log-sidecar-injector:v1.2.0 registry.cn-beijing.aliyuncs.com/kubesphereio/filebeat:6.7.0 registry.cn-beijing.aliyuncs.com/kubesphereio/kube-events-operator:v0.6.0 registry.cn-beijing.aliyuncs.com/kubesphereio/kube-events-exporter:v0.6.0 registry.cn-beijing.aliyuncs.com/kubesphereio/kube-events-ruler:v0.6.0 registry.cn-beijing.aliyuncs.com/kubesphereio/kube-auditing-operator:v0.2.0 registry.cn-beijing.aliyuncs.com/kubesphereio/kube-auditing-webhook:v0.2.0 ##istio-images registry.cn-beijing.aliyuncs.com/kubesphereio/pilot:1.14.6 registry.cn-beijing.aliyuncs.com/kubesphereio/proxyv2:1.14.6 registry.cn-beijing.aliyuncs.com/kubesphereio/jaeger-operator:1.29 registry.cn-beijing.aliyuncs.com/kubesphereio/jaeger-agent:1.29 registry.cn-beijing.aliyuncs.com/kubesphereio/jaeger-collector:1.29 registry.cn-beijing.aliyuncs.com/kubesphereio/jaeger-query:1.29 registry.cn-beijing.aliyuncs.com/kubesphereio/jaeger-es-index-cleaner:1.29 registry.cn-beijing.aliyuncs.com/kubesphereio/kiali-operator:v1.50.1 registry.cn-beijing.aliyuncs.com/kubesphereio/kiali:v1.50 ##example-images registry.cn-beijing.aliyuncs.com/kubesphereio/busybox:1.31.1 registry.cn-beijing.aliyuncs.com/kubesphereio/nginx:1.14-alpine registry.cn-beijing.aliyuncs.com/kubesphereio/wget:1.0 registry.cn-beijing.aliyuncs.com/kubesphereio/hello:plain-text registry.cn-beijing.aliyuncs.com/kubesphereio/wordpress:4.8-apache registry.cn-beijing.aliyuncs.com/kubesphereio/hpa-example:latest registry.cn-beijing.aliyuncs.com/kubesphereio/fluentd:v1.4.2-2.0 registry.cn-beijing.aliyuncs.com/kubesphereio/perl:latest registry.cn-beijing.aliyuncs.com/kubesphereio/examples-bookinfo-productpage-v1:1.16.2 registry.cn-beijing.aliyuncs.com/kubesphereio/examples-bookinfo-reviews-v1:1.16.2 registry.cn-beijing.aliyuncs.com/kubesphereio/examples-bookinfo-reviews-v2:1.16.2 registry.cn-beijing.aliyuncs.com/kubesphereio/examples-bookinfo-details-v1:1.16.2 registry.cn-beijing.aliyuncs.com/kubesphereio/examples-bookinfo-ratings-v1:1.16.3 ##weave-scope-images registry.cn-beijing.aliyuncs.com/kubesphereio/scope:1.13.0 反馈\n"},{"id":188,"href":"/docs/%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85%E9%9B%86%E7%BE%A4-li-xian-an-zhuang-ji-qun/","title":"离线安装集群 2023-09-28 15:29:14.742","section":"Docs","content":" 离线安装集群 # 使用kubeasz 离线安装 k8s集群需要下载四个部分： # kubeasz 项目代码 # 二进制文件（k8s、etcd、containerd等组件） # 容器镜像文件（calico、coredns、metrics-server等容器镜像） # 系统软件安装包（ipset、libseccomp2等，仅无法使用本地yum/apt源时需要） # 离线文件准备 # 在一台能够访问互联网的服务器上执行： # 下载工具脚本ezdown，举例使用kubeasz版本3.6.0 # export release=3.6.0 wget https://github.com/easzlab/kubeasz/releases/download/${release}/ezdown chmod +x ./ezdown 使用工具脚本下载（更多关于ezdown的参数，运行./ezdown 查看） 下载kubeasz代码、二进制、默认容器镜像\n# 国内环境 ./ezdown -D [可选]如果需要更多组件，请下载额外容器镜像（cilium,flannel,prometheus等）\n./ezdown -X flannel ./ezdown -X prometheus ... 下载离线系统包 (适用于无法使用yum/apt仓库情形)\n# 如果操作系统是ubuntu 22.04 ./ezdown -P ubuntu_22 上述脚本运行成功后，所有文件（kubeasz代码、二进制、离线镜像）均已整理好放入目录/etc/kubeasz\n/etc/kubeasz 包含 kubeasz 版本为 ${release} 的发布代码 /etc/kubeasz/bin 包含 k8s/etcd/docker/cni 等二进制文件 /etc/kubeasz/down 包含集群安装时需要的离线容器镜像 /etc/kubeasz/down/packages 包含集群安装时需要的系统基础软件 离线安装 # 上述下载完成后，把/etc/kubeasz整个目录复制到目标离线服务器相同目录，然后在离线服务器/etc/kubeasz目录下执行：\n离线安装 docker，检查本地文件，正常会提示所有文件已经下载完成，并上传到本地私有镜像仓库 ./ezdown -D ./ezdown -X flannel ./ezdown -X prometheus ... 启动 kubeasz 容器 ./ezdown -S 设置参数允许离线安装系统软件包 sed -i \u0026#39;s/^INSTALL_SOURCE.*$/INSTALL_SOURCE: \u0026#34;offline\u0026#34;/g\u0026#39; /etc/kubeasz/example/config.yml 举例安装单节点集群，参考 https://github.com/easzlab/kubeasz/blob/master/docs/setup/quickStart.md source ~/.bashrc dk ezctl start-aio # 或者执行 docker exec -it kubeasz ezctl start-aio 多节点集群，进入kubeasz 容器内 docker exec -it kubeasz bash，参考https://github.com/easzlab/kubeasz/blob/master/docs/setup/00-planning_and_overall_intro.md 进行集群规划和设置后使用./ezctl 命令安装 "},{"id":189,"href":"/docs/2025-2-7-%E8%AE%A1%E5%88%92/","title":"美国码农薪酬","section":"Docs","content":"可如果不分国家，那么美国的CS（计算机）毕业生刚入职的税前收入就可以达到100万人民币（15.4万美元）这一水平。\n无论是中国、美国还是世界其他地区，在过去十年中，码农都是整体收入增长最快、社会认可度提升最大的一类职业。\n码农是软件工程师的一个带有一点点自嘲性质的简称，但码农确实是这个时代极少数可以通过踏踏实实打工就大幅提升自身经济水平并改变命运的一个职业。在我的圈子中，码农是最多的，其中一半在美国，今天我就以自己的见闻体会结合行业的发展聊聊美国码农的生活状态和财务状况。\n1\n薪酬对比\n在互联网深度融入生活工作的时代，码农已经成为各个国家普遍存在的职业，但在待遇方面，不同国家之间待遇差别可谓巨大。\n如果认真比，当码农最香的地方还是美国。美国的码农的平均工资比同属发达地区的伦敦、新加坡、多伦多的同行要高出一倍左右，比其他发展中国家自然高出更多，这也是为什么你会发现清华北大北邮华科等CS牛校的计算机毕业生很大一部分都计划美国留学，而且这个专业的人回国比率是所有专业中最低的一个\u0026hellip;那是因为他们在大洋彼岸赚得实在是太多了。\n根据科技行业薪资统计平台Levels.fyi给的数据，旧金山湾区和西雅图的软件工程师中位数年包分别为23万美元和20.4万美元；而作为对比，多伦多对应的数字是12.2万美元、伦敦是13.5万美元、新加坡是10.7万美元。所以即使是欧洲、加拿大、新加坡这些发达国家的不少IT同行都希望能到美国工作发展。\n旧金山湾区软件工程师中位数年包23万美元\n加拿大多伦多软件工程师中位数年包12.2万美元\n英国伦敦软件工程师中位数年包13.5万美元\n新加坡软件工程师中位数年包10.7万美元\n多说一下，我们用的Levels.fyi是非常可靠的一个平台，不同公司的员工可以验证身份、上传offer、W2表等方式反馈真实的薪酬数据，数据量非常大。不同的公司对于级别划分不同，但整体差异不大，比如一个刚刚大学毕业的计算机毕业生，去谷歌L3级别的年包平均是19.1万美元，去苹果ICT2级别的年包平均是17.2万美元，谷歌的L3和苹果的ICT2对应的都是入门级软件工程师。\nFAAMG各自对软件工程的职级划分和横向对应\n谷歌各级别软件工程师年收入 L3对应191K（19.1万美元）\n2\n股票是重点\n尽管码农的收入都很高，但这不是这个群体致富的最关键因素，一方面美国有着高额的所得税，另一方面在旧金山湾区、西雅图、纽约等城市，物价和房价也都很高。\n美国码农致富的核心是股票，而且职级越高、收入越高的码农，年包中股票的比例就越大。以亚马逊为例，L4（入门级码农）的年包平均16.4万美元，其中Base（基本工资）12.5万美元，Stock（股票）2.2万美元，Bonus（奖励）1.7万美元；而发展到L6（这个级别是大多数中国人可以触及的最大高度）时，平均年包达到33.2万美元，其中股票16万美元；最高的Principal（总管）年包中资本工资只有16.2万美元，但股票多达43.6万美元。\n亚马逊软件工程师薪酬\n纳斯达克综合指数中大部分公司都是科网公司，可以直接理解为美国的“码农指数”。这个大盘指数从2009年3月到现在累计涨了12倍，这说明在这十二年多的时间里，十倍股是遍布整个市场的，否则大盘不可能有这么大的涨幅。\n我们看FAAMG五大科技公司，有四家在过去一轮牛市中涨幅远超十倍，其中苹果48倍、微软18倍、谷歌17倍、亚马逊55倍，另外一家脸书2012年才上市，至今9倍。\n亚马逊、苹果、微软、谷歌股价涨幅对比 2009-2021\n如果一个亚马逊普通员工在2009年初入职，按当时的行情，入职年包大约是10万美元，其中1万美元是股票，假设他一直持有这些股票到了现在，那么仅2009年一年他得到的股票放现在就价值55万美元，如果他一直在亚马逊工作，而且努力上班，不考虑买房也考虑做其他投资，很可能早就已财务自由。\n当然，人性的弱点导致大多数人没有办法看到手上已经上涨十倍以上的股票还不套现，所以老员工只有很少人能一股不卖留到现在。虽然2009年到现在湾区和西雅图的房价也涨了200%以上，但即使加上杠杆效应还是无法跑赢股价，这也解释了为什么美国的房价没有办法炒到房价收入比和租售比到扭曲的程度，因为他们有可替代的资产去满足投资需要。\n3\n内卷问题\n尽管美国大多数码农收入很高、又有股票助推财富增长，但他们并不是都过得有你想象的那么舒服，美国也是发达经济体中内卷最严重的国家。\n码农聚集地常常也是内卷严重的地方，因为这里人多、而且这些人还都非常聪明、非常努力、收入又高。设身处地想想也能理解，一个人年包25万美元可能会感觉很棒，但当你身边所有人年包都是这个水平时，每天还面对年薪百万美元的领导时，你得到的压力就大过成就感了。\n以旧金山湾区为例，这里不仅汇聚了美国自己一批聪明的年轻人，还吸收了来自包括印度、中国等世界各地的无数学霸。在硅谷华人圈子里，你会发现很多人见面或线上讨论的话题越来越多地涉及到股票、买房、学区、私立学校这些概念，这些话题显然很内卷，一听就感觉到压力。\n科技大厂在美国国内各个城市同级别职位的待遇相差不大，所以相同级别的码农在不同地方很可能完全是两种生活水平，比如100万美元在硅谷要买独立屋只能买到个小黑屋，在德州奥斯汀就可以买一个3000呎的漂亮大house，内卷很大程度上取决于地域。\n在湾区的圣何塞 100万美元买的房子是这样的\n在德州的奥斯汀 50万美元可以买到3000呎的新房\n不过财富和人口都是流动的，内卷也是会蔓延的，在奥斯汀周边的码农聚集区Cedar Park，这里环境美、治安好、就业多、房价低，结果过去一年房价暴涨超过50%，在美国这样一个城市化结束、经济中低速增长、而且还有房产税的国家，这么快的房价涨速是很可怕的。\nCedar Park房价走势\n说到内卷，工作压力问题绕不开，这方面美国码农还是相对轻松一些的，有oncall（随时待命）制度的公司也只是极少数时间oncall，比如亚马逊每周不同的组轮着oncall，每个组一年大概两次，一次半个星期到一个星期。\n美国科技行业不同公司以及相同公司的不同小组、不同工作者之间在加班上的差异非常大。比如脸书和亚马逊属于整体工作强度比较大的，微软、甲骨文属于工作强度相对低的，但在脸书一些成熟产品的小组，也有人是轻松状态；在微软的新兴产品Azure相关的小组，员工们也都很拼。\n谷歌总部打沙滩排球的员工们\n美国大多数的码农的上班时间是朝10晚5/6点，也有人加班，但几乎没见过谁吐槽加班的，这是因为美国科技公司的加班基本上都靠大部分年轻人自愿，他们愿意996来换取快点升职，所以一般马上要升职或者奔着快点升职的人才加班，其它人都不加班。长期的超时工作在美国职场上是一个惹人厌的文化，所以没有哪个老板会以工作时间评定员工或淘汰员工。如果你在硅谷工作，当你看到哪个同事工作忽然间特别辛苦、自愿加班，只说明他快要升职了。\n这些年来，很多中国年轻人在美国职场都很拼，也有人把加班作为工作的常态，也侧面说明很多中国年轻人现在混的不错，升职之路很顺利。\n除了不同城市带来不同的压力外，心态对内卷的影响更大，如果不想着用富人的标准要求作为中产的自己，那么即使是在硅谷打工的码农会过得不错：一是他们基本不用加班，上下班不用挤地铁；二是假期多，还有很多活动的机会，比如开车2小时可以到一号公路的Big Sur、3小时到优胜美地、4小时到太浩湖，这些都是世界一流的风景区，湾区周边的州立公园和徒步道更是多到数不清。\n4\n副业与投资\n码农在美国是一份好工作，但也只限于在打工概念上这份工作确实不错。如果跳出打卡上班的概念并结合每个人各自的兴趣爱好，码农在美国未必就是一个完美的职业。因为当一个人有当一名成功码农的头脑，那么TA做别的事也很有可能赚到更多的钱。\n一位在Oracle工作的同学曾告诉我，由于他们公司产品、技术和市场都已经比较成熟，公司被誉为硅谷养老院，其中一些比较闲的组已经可以把全职工作当成part-time来做，比如一位考了房产经纪人的同事，她上午10点到公司开个会，处理一下邮件；下午就带客户去买房去了，而且她过去一年卖房赚的钱是税前工资的近三倍。仔细想想这不是吹牛，美国房产交易过程中，买卖经纪人单方佣金率大概是房屋总价的2%-3%，在房价超高的旧金山湾区，成交一套房可能就赚到5万美元甚至更多。\nOracle原总部（位于硅谷的红木城）2020年Oracle总部迁至奥斯汀\n美国税制的一个重要初衷就是抑制高薪打工者、并鼓励个体户创业。如果你当码农的base有20万美元，在湾区扣掉联邦税、州税、401k，到手大概只有12万美元；但如果你当房产经纪人、律师合伙人、会计师等自我经营的工作并一年赚到20万美元的话，可以有各式避税方法让你的税率降到20%以内的水平。在美国的税制之下，家庭最好的组合应该是一个人打工、一个人经营自己的事业，既能有稳定高薪、又能避税、还能为将来打开更大的收入空间。\n所以，副业或从事新行业对于有理想有兴趣的码农来说至关重要。但对大多数在美国的中国码农来说，并不能很早就离开岗位，因为他们要通过公司拿到绿卡，还要存够钱成家立业买房，只有身份自由+财务相对自由之后，才会考虑做其他的大事。\n对美国码农来说，投资也是一个重要副业，由于他们收入高，有足够结余，都会去主动投资，有些人喜欢投资股票、有些人喜欢投资房产。股票不用说，他们本身就有自家公司的大量股票，同时会配置一些其他大厂的股票，比如你在谷歌工作、然后配置亚马逊和脸书的股票，这样做可以分散自家公司的风险。\n而房产投资是中国人最擅长的事，早年在一些低房价地区（如德州）的华人码农，不少人手上都已经有多套投资房。在2012年时，无论是旧金山湾区、西雅图、洛杉矶还是奥斯汀，这些城市以20%首付买房，30年固定利率，都可以做到租金远超月供，有些好的投资屋租金可以达到月供的2-3倍。但随着美国房价和股票连续大涨，后来人的机会似乎越来越少、反而风险越来越大，有些机会过去了就是过去了，先富总是比后富更有优势。\n5\n总结\n总的来说，美国码农的生活状态是充实、丰富、有获得感的。如果你或你的孩子在数学上有一定天赋，对这个行业感兴趣，那么计算机专业都是正确的选择，如果正好英语也不错，那么留学美国读CS会给未来的发展多一个可能性。无论是在中国还是在美国，只要能够成为一名优秀的码农，都是妥妥的改变命运的机会\n"},{"id":190,"href":"/docs/2025-2-7-%E8%AE%A1%E5%88%922/","title":"美国码农计划","section":"Docs","content":"我给想润出去的墙内码农几条建议：\n1.大厂在职的，深耕技术直接投简历或转海外岗，不要花冤枉钱和宝贵的时间在教育上；\n2.第一目标应该是美国，第二是加/坡/澳/英/瑞，其他国家（欧/日/韩/港）期望收入远远不如国内；\n3.绝大多数人的首要障碍是英语听力和口语，一定要多用英语熟练讲项目和刷Leetcode。\n如果认真比，当码农最香的地方还是美国。美国的码农的平均工资比同属发达地区的伦敦、新加坡、多伦多的同行要高出一倍左右，比其他发展中国家自然高出更多，这也是为什么你会发现清华北大北邮华科等CS牛校的计算机毕业生很大一部分都计划美国留学，而且这个专业的人回国比率是所有专业中最低的一个\u0026hellip;那是因为他们在大洋彼岸赚得实在是太多了。\n根据科技行业薪资统计平台Levels.fyi给的数据，旧金山湾区和西雅图的软件工程师中位数年包分别为23万美元和20.4万美元；而作为对比，多伦多对应的数字是12.2万美元、伦敦是13.5万美元、新加坡是10.7万美元。所以即使是欧洲、加拿大、新加坡这些发达国家的不少IT同行都希望能到美国工作发展。\n美国码农致富的核心是股票，而且职级越高、收入越高的码农，年包中股票的比例就越大。以亚马逊为例，L4（入门级码农）的年包平均16.4万美元，其中Base（基本工资）12.5万美元，Stock（股票）2.2万美元，Bonus（奖励）1.7万美元；而发展到L6（这个级别是大多数中国人可以触及的最大高度）时，平均年包达到33.2万美元，其中股票16万美元；最高的Principal（总管）年包中资本工资只有16.2万美元，但股票多达43.6万美元。\n中国通缩严重呗，用一个人干两个人活，中国人收入还不涨，不通缩就怪了，不管二百斤超发多少货币，都会往不缺钱的人口袋里流。\n在美国湾区的生活成本，特别是涉及租房和吃饭的开支，通常相当高。根据网络上的信息和最新的经济数据，这里提供一个大致的开支估算：\n### 租房：\n- 一居室公寓：\n- 在旧金山市中心，平均租金大约在$3,000 - $3,500 美元之间。\n- 如果在郊区如Oakland或Fremont，租金会略低，约为$2,000 - $3,000 美元。\n- 合租：\n- 与他人合租可以大幅降低开支。合租一居室或多居室公寓，个人承担的租金可能会在$1,500 - $2,500 美元之间。\n### 吃饭：\n- 自制餐食：\n- 食品杂货的价格在湾区也相对高于美国其他地区。一个单身人士每月的杂货开支可能在$300 - $600 美元之间，视饮食习惯而定。\n- 外出就餐：\n- 在湾区吃一顿普通的餐馆饭菜，价格通常在$15 - $30 美元左右，昂贵的餐厅可能更高。如果你经常外出就餐，每月的开支可能达到$400 - $800 美元或更多。\n### 总体开支：\n- 保守估计：假设一个人选择合租，注重自制餐食，尽量减少外出就餐：\n- 租房：$1,500\n- 吃饭：$400（杂货）\n- 总计：约$1,900 美元每月\n- 中等开支：如果租住一居室公寓，并且偶尔外出就餐：\n- 租房：$3,000\n- 吃饭：$600（杂货 + 偶尔外出就餐）\n- 总计：约$3,600 美元每月\n- 高消费：在市中心租住高档公寓，频繁外出就餐和购买高端食品：\n- 租房：$3,500\n- 吃饭：$1,000（杂货 + 频繁外出就餐）\n- 总计：约$4,500 美元每月\n这些数字是根据网上的信息和相关的经济报告估算的，实际开支可能会因个人生活方式、选择的具体位置、是否有交通或娱乐等其他开支而有所不同。请注意，这些估算并不包括其他生活成本，如交通、保险、娱乐活动等。\n•熟悉GCP的GKE Kubernetes，例如知道如何构建基于裸金属的Kubernetes集群（强调）和基于虚拟机的Kubernetes集群；\n•Kubernetes的常见操作和基本概念，以及部署Istio和Prometheus等组件的能力（强调）；\n•可以使用项目敏捷开发方法和相关工具，如Jira、git；\n•熟练使用Service Now和Raise CR（强调），将在项目中申请许多权限和发布操作；\n•英语水平，定期会议需要英语主持；\n•解决问题的能力，在面试过程中，可能会提出一个方案进行提案；\n提高英语水平可以从以下几方面入手：\n\\1. 多听多说：\n- 听英语广播、播客或看英语电影、电视剧，模仿发音和语调。\n- 找语言交换伙伴或参加英语角，实际运用英语进行交流。\n\\2. 阅读：\n- 阅读各种类型的英文书籍、文章、新闻，扩展词汇量和理解能力。\n- 使用英英字典来加深对单词的理解。\n\\3. 写作：\n- 每天写日记、博客或参与英语写作练习，注意语法和句型的使用。\n- 请别人修改你的文章以获得反馈。\n\\4. 学习语法：\n- 掌握基本的语法规则，通过练习题巩固知识。\n- 使用语法书或在线课程来学习复杂的语法结构。\n\\5. 词汇积累：\n- 每天学习新单词，可以使用闪卡软件如Anki或Memrise。\n- 通过上下文学习词汇，而不是单纯背诵。\n\\6. 互动学习：\n- 参加英语培训班或在线课程，获得系统的指导和反馈。\n- 使用学习应用，如Duolingo、Babbel等，结合游戏化学习。\n\\7. 练习口语：\n- 用语言学习应用（如HelloTalk）与母语者交流。\n- 尝试用英语思考和自言自语，培养自然的表达能力。\n\\8. 文化浸入：\n- 了解英语国家的文化、习俗和俚语，有助于理解英语的使用背景。\n\\9. 定期复习：\n- 定期回顾学过的内容，确保知识不被遗忘。\n\\10. 设置目标：\n- 设定短期和长期的英语学习目标，保持学习的动力。\n坚持是关键，结合各种方法，持续不断地练习和复习，你的英语水平会逐步提高。\n分享一下我日常使用的美股App：\n1️⃣华尔街见闻：美股相关的新闻、快讯以及宏观数据发布日历，我一般主要是看宏观数据发布日历。\n2️⃣Seeking Alpha：美股股票分析，会有一些比较精彩深度的分享，很适合投资个股的朋友。\n3️⃣长桥券商：交易、看盘、学习其他投资者对于个股的分析（长桥App底部的“动态”），不过质量参差不齐。\n4️⃣Tradingview：专业看盘工具，可以添加额外的技术指标，用于技术面分析，Trader必备工具。\n此外，Youtube也有很多高质量的投资内容，当你想要投资某个个股的时候，可以在Youtube上找到详细的分析内容，帮你做功课，辅助你做出投资决定。\n"},{"id":191,"href":"/docs/%E8%87%AA%E5%8A%A8%E5%B1%8F%E8%94%BDip%E6%94%BB%E5%87%BB-zi-dong-ping-bi-ip-gong-ji/","title":"自动屏蔽IP攻击 2024-04-03 14:54:11.783","section":"Docs","content":" 在当今的数字世界中，服务器安全性成为了每个企业或个人都必须面对的重要议题。特别是当面对如DoS（拒绝服务）攻击等网络威胁时，一个有效的防范措施显得尤为关键。本文将探讨如何通过自动屏蔽攻击IP来增强您的服务器抵御DoS攻击的能力，并提供一个实用的Bash脚本作为参考。 # 理解DoS攻击\nDoS攻击是一种恶意行为，通过向目标服务器发送大量无用的请求，耗尽其资源，使合法用户无法访问服务。这种攻击不仅影响服务器的正常运行，还可能导致数据丢失、系统崩溃等严重后果。 # 自动屏蔽攻击IP的策略\n为了有效应对DoS攻击，我们需要一个能够实时监控服务器流量，并自动识别和屏蔽攻击IP的系统。以下是实现这一目标的基本步骤： # 监控服务器日志：通过分析Nginx等服务器的访问日志，我们可以识别出异常流量模式，如短时间内来自同一IP的大量请求。 # 识别攻击IP：使用脚本或工具对日志数据进行处理，提取出那些行为异常的IP地址。 # 自动屏蔽：一旦发现攻击IP，立即使用iptables等防火墙工具将其加入黑名单，阻止其进一步的访问。 # 记录和报告：将屏蔽的IP地址和时间记录在日志文件中，以便后续分析和报告。 # 实用Bash脚本\n下面是一个基于上述策略的Bash脚本示例。请注意，此脚本应在具有相应权限的服务器上运行，并且可能需要根据您的实际环境进行调整。 # #!/bin/bash # 定义日志文件路径和临时黑名单文件路径 LOG_FILE=/usr/local/nginx/logs/demo2.access.log BLACKLIST_FILE=/tmp/blacklist.txt DROP_LOG=/tmp/drop_ip.log # 获取当前时间并格式化为日期 DATE=$(date +\u0026#34;%d/%b/%Y:%H:%M\u0026#34;) # 从日志文件中提取异常IP地址（这里假设每分钟超过10个请求的IP为异常） tail -n 5000 $LOG_FILE | grep \u0026#34;$DATE\u0026#34; | awk \u0026#39;{print $1, $4}\u0026#39; | sort -n | uniq -c | awk \u0026#39;$1 \u0026gt; 10 {print $2}\u0026#39; \u0026gt; $BLACKLIST_FILE # 读取黑名单文件，并屏蔽这些IP地址 while IFS= read -r IP; do if ! iptables -vnL INPUT | grep -q \u0026#34;$IP\u0026#34;; then iptables -I INPUT -s \u0026#34;$IP\u0026#34; -j DROP echo \u0026#34;$(date +\u0026#39;%F_%T\u0026#39;) $IP\u0026#34; \u0026gt;\u0026gt; $DROP_LOG echo \u0026#34;Blocked IP: $IP\u0026#34; fi done \u0026lt; $BLACKLIST_FILE # （可选）清理过时的iptables规则，避免规则过多导致性能下降 # 这里可以根据需要添加逻辑来定期清理旧的屏蔽规则 脚本首先定义了所需的文件路径和日期格式。 使用tail和grep命令结合当前时间从Nginx访问日志中提取最近的访问记录。 通过awk和sort命令处理这些记录，识别出每分钟请求次数超过10次的IP地址，并将它们写入临时黑名单文件。 脚本接着读取黑名单文件中的每个IP地址，并使用iptables命令将它们加入INPUT链的DROP目标中，从而屏蔽这些IP的访问。同时，将屏蔽操作记录在日志文件中。 最后，您可以根据需要添加额外的逻辑来定期清理iptables中的旧规则，以保持防火墙的性能。 请注意，此脚本是一个基本示例，可能需要根据您的服务器配置、日志格式和安全策略进行定制。在实际使用中，还应考虑脚本的性能影响、错误处理和日志管理等方面的问题。\n"},{"id":192,"href":"/docs/2025-2-12-%E9%9D%A2%E8%AF%950212/","title":"面试0212","section":"Docs","content":" 如何做k8s资源的规划 # 在K8s中进行资源规划是一个重要的任务，我来详细解释一下关键点：\n1. CPU和内存资源规划 # 1.1 资源请求(requests)和限制(limits) # spec: containers: - name: app resources: requests: memory: \u0026#34;256Mi\u0026#34; cpu: \u0026#34;250m\u0026#34; limits: memory: \u0026#34;512Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; requests: 容器需要的最小资源量 limits: 容器能够使用的最大资源量 CPU单位：1 CPU = 1000m（毫核） 内存单位：Mi（兆字节）, Gi（吉字节） 2. 命名空间资源配额 # apiVersion: v1 kind: ResourceQuota metadata: name: compute-quota namespace: your-namespace spec: hard: requests.cpu: \u0026#34;4\u0026#34; requests.memory: 4Gi limits.cpu: \u0026#34;8\u0026#34; limits.memory: 8Gi pods: \u0026#34;10\u0026#34; 3. 资源规划的最佳实践 # 监控历史数据\n使用Prometheus + Grafana监控应用实际资源使用情况 观察峰值和平均值 留出30-50%的buffer以应对突发流量 合理设置requests和limits\nrequests设置为应用正常运行需要的资源量 limits设置为预期峰值的1.5-2倍 避免limits设置过低导致OOM Pod水平自动扩缩容(HPA)\napiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: app-hpa spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: your-app minReplicas: 2 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 4. 节点资源规划 # 节点标签和污点 kubectl label nodes node1 workload-type=cpu-intensive kubectl taint nodes node1 dedicated=cpu-intensive:NoSchedule 节点亲和性 spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: workload-type operator: In values: - cpu-intensive 5. 资源规划检查清单 # 应用层面 确定应用的基准资源需求 设置合适的requests和limits 配置HPA策略 集群层面 设置namespace资源配额 规划节点池和节点规格 配置节点亲和性策略 监控和优化 部署监控系统 定期检查资源使用情况 根据实际情况调整资源配置 6. 注意事项 # 避免资源过度分配\n合理评估应用资源需求 定期清理未使用的资源 考虑成本优化\n使用spot实例降低成本 配置集群自动扩缩容 优化资源利用率 确保高可用\n合理设置Pod反亲和性 跨可用区部署 预留足够的资源buffer 通过以上方法进行资源规划，可以确保应用在K8s集群中稳定运行，同时实现资源的最优利用。记住要定期检查和调整资源配置，以适应业务的变化和增长。\nk8s的组件有哪些 # 1、etcd 保存了整个集群的状态；\n2、apiserver 提供了资源操作的唯一入口，并提供认证、授权、访问控制、API 注册和发现等机制；\n3、controller manager 负责维护集群的状态，比如故障检测、自动扩展、滚动更新等；\n4、scheduler 负责资源的调度，按照预定的调度策略将 Pod 调度到相应的机器上；\n5、kubelet 负责维护容器的生命周期，同时也负责 Volume（CVI）和网络（CNI）的管理；\n6、Container runtime 负责镜像管理以及 Pod 和容器的真正运行（CRI）；\n7、kube-proxy 负责为 Service 提供 cluster 内部的服务发现和负载均衡。\n让我详细介绍K8s的核心组件：\n1. 控制平面组件（Master节点） # API Server 集群的统一入口 提供REST API接口 负责组件间的通信 认证和授权 # API Server配置示例 apiVersion: v1 kind: Pod metadata: name: kube-apiserver namespace: kube-system spec: containers: - name: kube-apiserver image: k8s.gcr.io/kube-apiserver:v1.24.0 command: - kube-apiserver - --advertise-address=192.168.1.10 - --allow-privileged=true - --authorization-mode=Node,RBAC etcd 分布式键值存储 保存集群所有数据 高可用配置 # etcd配置示例 spec: containers: - name: etcd image: k8s.gcr.io/etcd:3.5.1 command: - etcd - --advertise-client-urls=https://192.168.1.10:2379 - --data-dir=/var/lib/etcd Controller Manager 维护集群状态 故障检测和自动恢复 包含多个控制器： Node Controller Replication Controller Endpoints Controller Service Account \u0026amp; Token Controller Scheduler 负责Pod调度 根据调度算法选择节点 考虑资源需求、亲和性等 2. 工作节点组件（Node节点） # Kubelet 节点代理 管理容器生命周期 执行Pod规范 # Kubelet配置示例 kubelet: config: kind: KubeletConfiguration address: \u0026#34;0.0.0.0\u0026#34; port: 10250 healthzPort: 10248 Kube-proxy 网络代理 维护网络规则 实现Service概念 # Kube-proxy配置示例 apiVersion: kubeproxy.config.k8s.io/v1alpha1 kind: KubeProxyConfiguration mode: \u0026#34;ipvs\u0026#34; # 或者 \u0026#34;iptables\u0026#34; Container Runtime 容器运行时 如Docker、containerd 负责容器的实际运行 3. 附加组件 # CoreDNS 集群DNS服务 服务发现 apiVersion: v1 kind: Service metadata: name: kube-dns namespace: kube-system spec: selector: k8s-app: kube-dns ports: - port: 53 protocol: UDP Dashboard Web UI界面 集群管理和监控 CNI网络插件 如Calico、Flannel 提供网络方案 # Calico配置示例 kind: DaemonSet metadata: name: calico-node namespace: kube-system 4. 组件间通信 # graph TD A[API Server] --\u0026gt; B[etcd] C[Controller Manager] --\u0026gt; A D[Scheduler] --\u0026gt; A E[Kubelet] --\u0026gt; A F[Kube-proxy] --\u0026gt; A 5. 高可用配置建议 # 控制平面 至少3个master节点 使用负载均衡器 etcd集群部署 工作节点 根据业务规模扩展 跨可用区部署 定期维护和更新 6. 监控要点 # 核心指标 组件健康状态 API Server延迟 etcd性能 调度器成功率 告警配置 apiVersion: monitoring.coreos.com/v1 kind: PrometheusRule metadata: name: k8s-components-alerts spec: groups: - name: k8s-components rules: - alert: APIServerDown expr: up{job=\u0026#34;apiserver\u0026#34;} == 0 这些组件共同工作，确保了Kubernetes集群的正常运行。了解每个组件的功能和配置对于运维和故障排查非常重要。\nscheduler和controller-manager的区别 # 让我详细解释Scheduler和Controller Manager的区别：\n1. 基本职责 # Scheduler:\n负责Pod的调度决策 为Pod选择最合适的Node节点 不负责Pod的实际创建过程 Controller Manager:\n负责维护集群状态 处理各类资源的控制循环 确保实际状态向期望状态迁移 2. 主要功能对比 # Scheduler功能: # scheduler配置示例 apiVersion: kubescheduler.config.k8s.io/v1 kind: KubeSchedulerConfiguration profiles: - schedulerName: default-scheduler plugins: score: disabled: - name: NodeResourcesLeastAllocated enabled: - name: NodeResourcesMostAllocated Controller Manager功能: # 包含多个控制器 - Deployment Controller - ReplicaSet Controller - StatefulSet Controller - Node Controller - Service Controller - Endpoints Controller - Namespace Controller 3. 工作流程 # Scheduler:\n监听API Server中未被调度的Pod 根据调度算法为Pod选择节点 将调度结果更新到API Server Controller Manager:\n监听API Server中资源变化 对比实际状态和期望状态 执行必要的操作使集群达到期望状态 4. 实际应用示例 # Scheduler处理Pod调度: apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - name: nginx image: nginx # Scheduler会根据以下条件选择节点 nodeSelector: disk: ssd affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/e2e-az-name operator: In values: - e2e-az1 - e2e-az2 Controller Manager处理Deployment: apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 3 # Controller确保运行3个副本 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 5. 主要区别总结 # 职责范围:\nScheduler: 仅负责Pod调度 Controller Manager: 负责整个集群状态维护 工作方式:\nScheduler: 一次性决策（为Pod选择节点） Controller Manager: 持续循环（reconcile循环） 关注点:\nScheduler: 资源分配和约束条件 Controller Manager: 状态维护和自动修复 操作对象:\nScheduler: 主要处理Pod Controller Manager: 处理所有Kubernetes资源 6. 使用建议 # Scheduler相关:\n合理配置资源请求和限制 使用节点亲和性和反亲和性 配置合适的调度策略 Controller Manager相关:\n正确设置资源的期望状态 监控控制器的运行状态 了解不同控制器的作用 通过理解这些区别，可以更好地：\n规划应用部署策略\n解决调度问题\n处理集群状态异常\n优化资源使用效率\ningress和service的区别 # 让我详细解释Ingress和Service的区别：\n1. 基本定义 # Service:\napiVersion: v1 kind: Service metadata: name: my-service spec: selector: app: my-app ports: - port: 80 targetPort: 8080 type: ClusterIP # 或 NodePort, LoadBalancer Ingress:\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: my-ingress spec: rules: - host: myapp.example.com http: paths: - path: /api pathType: Prefix backend: service: name: my-service port: number: 80 2. 主要区别 # 功能定位\nService: 提供集群内部的服务发现和负载均衡 Ingress: 提供集群外部访问的HTTP/HTTPS路由规则 协议支持\nService: 支持任何TCP/UDP协议 Ingress: 主要支持HTTP/HTTPS协议 访问层级\nService: 网络层(L4) Ingress: 应用层(L7) 3. 常见使用场景 # Service使用场景: # 内部服务访问 apiVersion: v1 kind: Service metadata: name: db-service spec: selector: app: mysql ports: - port: 3306 targetPort: 3306 type: ClusterIP Ingress使用场景: # 外部访问路由 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: web-ingress annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - host: www.myapp.com http: paths: - path: /api pathType: Prefix backend: service: name: api-service port: number: 80 - path: /web pathType: Prefix backend: service: name: web-service port: number: 80 4. 高级特性对比 # Service特性: apiVersion: v1 kind: Service metadata: name: my-service spec: type: LoadBalancer sessionAffinity: ClientIP sessionAffinityConfig: clientIP: timeoutSeconds: 10800 ports: - port: 80 Ingress特性: apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: nginx.ingress.kubernetes.io/ssl-redirect: \u0026#34;true\u0026#34; nginx.ingress.kubernetes.io/force-ssl-redirect: \u0026#34;true\u0026#34; spec: tls: - hosts: - myapp.com secretName: tls-secret 5. 最佳实践 # 组合使用示例: # Service配置 apiVersion: v1 kind: Service metadata: name: web-service spec: selector: app: web ports: - port: 80 targetPort: 8080 --- # Ingress配置 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: web-ingress spec: rules: - host: web.example.com http: paths: - path: / pathType: Prefix backend: service: name: web-service port: number: 80 6. 使用建议 # 何时使用Service:\n集群内部服务访问 需要TCP/UDP协议支持 简单的负载均衡需求 何时使用Ingress:\n需要域名访问 需要路径匹配 需要SSL/TLS终止 需要URL重写 需要应用层负载均衡 通过合理组合使用Service和Ingress，可以构建完整的服务访问体系。\n打污点的命令 # 让我介绍一下打污点(Taint)的相关命令：\n1. 基本命令格式 # # 添加污点 kubectl taint nodes \u0026lt;node-name\u0026gt; \u0026lt;key\u0026gt;=\u0026lt;value\u0026gt;:\u0026lt;effect\u0026gt; # 删除污点 kubectl taint nodes \u0026lt;node-name\u0026gt; \u0026lt;key\u0026gt;- 2. 常用污点效果(effect) # NoSchedule: 不允许调度 PreferNoSchedule: 尽量不调度 NoExecute: 不允许调度，并驱逐已有Pod 3. 常用命令示例 # # 添加不可调度污点 kubectl taint nodes node1 key1=value1:NoSchedule # 添加优先不调度污点 kubectl taint nodes node2 key2=value2:PreferNoSchedule # 添加驱逐污点 kubectl taint nodes node3 key3=value3:NoExecute # 删除指定污点 kubectl taint nodes node1 key1- # 删除所有污点 kubectl taint nodes node1 key1- key2- key3- 4. 实际应用场景 # # 标记专用节点（如GPU节点） kubectl taint nodes gpu-node-1 dedicated=gpu:NoSchedule # 标记故障节点 kubectl taint nodes problem-node-1 node-problem=true:NoExecute # 标记master节点 kubectl taint nodes master-node node-role.kubernetes.io/master=:NoSchedule 5. 查看节点污点 # # 查看节点详情（包含污点信息） kubectl describe node \u0026lt;node-name\u0026gt; # 使用自定义输出格式查看污点 kubectl get nodes -o custom-columns=NAME:.metadata.name,TAINTS:.spec.taints 记住：\n添加污点后需要配合容忍度(Toleration)使用 谨慎使用NoExecute效果 操作前先确认对现有Pod的影响 daemonset和deployment的区别 # 让我详细解释DaemonSet和Deployment的主要区别：\n1. 基本定义和用途 # Deployment:\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 DaemonSet:\napiVersion: apps/v1 kind: DaemonSet metadata: name: fluentd-ds spec: selector: matchLabels: app: fluentd template: metadata: labels: app: fluentd spec: containers: - name: fluentd image: fluentd:v1.14 2. 主要区别 # 部署策略\nDeployment: 根据replicas数量部署指定数量的Pod DaemonSet: 在每个节点上运行一个Pod副本 使用场景\nDeployment:\n无状态应用 需要指定副本数的应用 支持滚动更新 DaemonSet:\n日志收集(如fluentd) 节点监控(如node-exporter) 网络插件(如calico) 扩缩容\nDeployment: 可以手动或自动调整replicas数量 DaemonSet: 随节点数量自动增减，不支持手动扩缩容 3. 典型应用场景 # Deployment示例（应用服务）:\napiVersion: apps/v1 kind: Deployment metadata: name: web-app spec: replicas: 3 selector: matchLabels: app: web template: metadata: labels: app: web spec: containers: - name: web image: nginx:1.14.2 ports: - containerPort: 80 DaemonSet示例（监控代理）:\napiVersion: apps/v1 kind: DaemonSet metadata: name: node-exporter spec: selector: matchLabels: app: node-exporter template: metadata: labels: app: node-exporter spec: containers: - name: node-exporter image: prom/node-exporter ports: - containerPort: 9100 4. 更新策略 # Deployment更新策略: spec: strategy: type: RollingUpdate rollingUpdate: maxSurge: 25% maxUnavailable: 25% DaemonSet更新策略: spec: updateStrategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 5. 使用建议 # 选择Deployment场景:\n常规的Web应用 微服务应用 需要负载均衡的服务 选择DaemonSet场景:\n系统级服务 监控采集服务 日志收集服务 存储插件 6. 注意事项 # Deployment:\n注意设置合适的资源请求和限制 配置合理的健康检查 考虑配置HPA实现自动扩缩容 DaemonSet:\n注意资源占用，因为会在每个节点运行 合理配置污点容忍 避免运行太多DaemonSet影响业务Pod 通过合理选择Deployment和DaemonSet，可以更好地满足不同的应用部署需求。\n我看到您已经理解了Deployment和DaemonSet的区别。让我结合您的项目文件，给出一些具体建议：\n1. 订单服务应该使用Deployment # apiVersion: apps/v1 kind: Deployment metadata: name: order-service namespace: core-business spec: replicas: 3 selector: matchLabels: app: order-service template: metadata: labels: app: order-service spec: containers: - name: order-service image: your-registry/order-service:latest ports: - containerPort: 8080 resources: requests: memory: \u0026#34;1Gi\u0026#34; cpu: \u0026#34;500m\u0026#34; limits: memory: \u0026#34;2Gi\u0026#34; cpu: \u0026#34;1000m\u0026#34; readinessProbe: httpGet: path: /actuator/health port: 8080 initialDelaySeconds: 30 periodSeconds: 10 2. 配套的监控采集使用DaemonSet # apiVersion: apps/v1 kind: DaemonSet metadata: name: metrics-collector namespace: monitoring spec: selector: matchLabels: app: metrics-collector template: metadata: labels: app: metrics-collector spec: containers: - name: collector image: your-registry/metrics-collector:latest resources: requests: memory: \u0026#34;256Mi\u0026#34; cpu: \u0026#34;100m\u0026#34; limits: memory: \u0026#34;512Mi\u0026#34; cpu: \u0026#34;200m\u0026#34; volumeMounts: - name: varlog mountPath: /var/log volumes: - name: varlog hostPath: path: /var/log 这样的组合可以确保：\n订单服务能够灵活扩缩容 监控数据能够从每个节点收集 符合您在quota.yaml中设置的资源限制 k8s版本用的哪个版本 # 1.23.6\nscheduler调度的方式有哪些 # 让我详细介绍K8s的调度机制：\n1. 调度过程 # 调度过程分为两个主要阶段：\nPredicates（预选）：过滤不符合条件的节点 Priorities（优选）：对符合条件的节点进行打分 2. 常用调度方式 # 节点选择器(NodeSelector) apiVersion: v1 kind: Pod metadata: name: nginx spec: nodeSelector: disk: ssd gpu: \u0026#34;true\u0026#34; 节点亲和性(Node Affinity) spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/e2e-az-name operator: In values: - e2e-az1 - e2e-az2 Pod亲和性(Pod Affinity) spec: affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - web topologyKey: kubernetes.io/hostname 污点和容忍(Taints and Tolerations) # 给节点添加污点 kubectl taint nodes node1 key=value:NoSchedule # Pod配置容忍 spec: tolerations: - key: \u0026#34;key\u0026#34; operator: \u0026#34;Equal\u0026#34; value: \u0026#34;value\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; 3. 高级调度特性 # Pod优先级和抢占 apiVersion: scheduling.k8s.io/v1 kind: PriorityClass metadata: name: high-priority value: 1000000 --- apiVersion: v1 kind: Pod metadata: name: nginx spec: priorityClassName: high-priority DaemonSet调度 apiVersion: apps/v1 kind: DaemonSet metadata: name: fluentd-elasticsearch spec: selector: matchLabels: name: fluentd-elasticsearch template: spec: tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule 自定义调度器 apiVersion: v1 kind: Pod metadata: name: custom-scheduler-pod spec: schedulerName: my-custom-scheduler 4. 资源限制调度 # 资源请求和限制 spec: containers: - name: app resources: requests: memory: \u0026#34;64Mi\u0026#34; cpu: \u0026#34;250m\u0026#34; limits: memory: \u0026#34;128Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; Pod拓扑分布约束 spec: topologySpreadConstraints: - maxSkew: 1 topologyKey: zone whenUnsatisfiable: DoNotSchedule labelSelector: matchLabels: app: web 5. 最佳实践 # 合理使用亲和性和反亲和性 spec: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - web topologyKey: kubernetes.io/hostname 设置默认的调度策略 apiVersion: kubescheduler.config.k8s.io/v1 kind: KubeSchedulerConfiguration profiles: - schedulerName: default-scheduler plugins: score: disabled: - name: NodeResourcesLeastAllocated enabled: - name: NodeResourcesMostAllocated 6. 注意事项 # 避免过度使用硬性调度约束 合理设置Pod优先级 注意节点资源均衡 考虑跨可用区调度 定期检查调度失败的原因 通过合理使用这些调度机制，可以：\n确保应用高可用 优化资源使用 满足特定的部署需求 提高集群的稳定性 我看到您已经理解了K8s的调度机制。让我结合实际场景，给出一些具体的调度策略建议：\n1. 针对订单服务(OrderService)的调度策略 # apiVersion: apps/v1 kind: Deployment metadata: name: order-service spec: replicas: 3 template: spec: # 优先级设置 priorityClassName: high-priority # 亲和性设置 affinity: # 节点亲和性 - 选择高性能节点 nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: node-type operator: In values: - high-performance # Pod反亲和性 - 保证高可用 podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: app: order-service topologyKey: kubernetes.io/hostname # 资源请求和限制 containers: - name: order-service resources: requests: cpu: \u0026#34;2\u0026#34; memory: \u0026#34;4Gi\u0026#34; limits: cpu: \u0026#34;4\u0026#34; memory: \u0026#34;8Gi\u0026#34; 2. 配合资源配额(quota.yaml)的调度策略 # # 为订单服务创建专属命名空间配额 apiVersion: v1 kind: ResourceQuota metadata: name: order-service-quota namespace: order-service spec: hard: requests.cpu: \u0026#34;8\u0026#34; requests.memory: 16Gi limits.cpu: \u0026#34;16\u0026#34; limits.memory: 32Gi pods: \u0026#34;5\u0026#34; 3. 调度策略建议 # 高可用保证\n跨节点部署 跨可用区部署 资源预留 性能保证\n使用专用节点池 合理的资源限制 优先级设置 弹性伸缩\n配置HPA 设置合理的扩缩容阈值 资源预留空间 这样的配置可以确保订单服务的稳定性和性能。\n集群层面的规划 根据业务规模 预估多少台机器 # 让我为您详细说明集群规模的规划方法：\n1. 业务规模评估 # 业务指标评估\n日活用户数（DAU） 峰值QPS 数据存储需求 计算密集型/IO密集型 单个服务资源需求\n典型服务资源配置示例： - 小型服务： CPU: 0.5-1核 内存: 1-2Gi - 中型服务： CPU: 2-4核 内存: 4-8Gi - 大型服务： CPU: 8核+ 内存: 16Gi+ 2. 节点规格选择 # 常用节点规格 - 通用型： 4核8G, 8核16G, 16核32G - 计算密集型： 16核32G, 32核64G - 内存密集型： 16核64G, 32核128G 节点数量计算公式 所需节点数 = (服务数量 × 单服务资源 × 副本数 × (1 + 冗余系数)) / 节点规格 3. 集群规模示例 # 小型集群（适合初创公司） - 业务规模： DAU: 1-10万 服务数量: 10-20个 - 节点配置： 管理节点: 3台 4核8G 工作节点: 3-5台 8核16G - 总资源： CPU: 40-60核 内存: 80-120G 中型集群（适合中型企业） - 业务规模： DAU: 10-100万 服务数量: 20-50个 - 节点配置： 管理节点: 3台 8核16G 工作节点: 10-20台 16核32G - 总资源： CPU: 200-400核 内存: 400-800G 大型集群（适合大型企业） - 业务规模： DAU: 100万+ 服务数量: 50+个 - 节点配置： 管理节点: 5台 16核32G 工作节点: 50+台 32核64G - 总资源： CPU: 1600+核 内存: 3200+G 4. 规划注意事项 # 资源预留 系统组件资源预留： - kubelet: 0.1核/节点 - kube-proxy: 0.1核/节点 - 监控组件: 0.5-1核/节点 - 日志组件: 0.5-1核/节点 建议预留比例： - CPU: 总量的20-30% - 内存: 总量的30-40% 扩展性考虑 预留50%的增长空间 配置集群自动扩缩容 考虑多可用区部署 成本优化 混合部署策略： - 关键业务：使用按量付费节点 - 非关键业务：使用spot节点 - 固定负载：使用预留实例 5. 监控指标 # 需要持续监控以下指标来调整规划：\n- 节点资源使用率 - Pod调度成功率 - 服务响应时间 - 容器重启次数 - 存储使用情况 建议：\n从小规模开始，根据实际运行情况逐步扩展 使用节点池管理不同规格的节点 合理使用资源配额限制namespace资源使用 定期评估集群使用情况，及时优化资源分配 亲和性和非亲和性？ # 让我详细解释亲和性(Affinity)和非亲和性(Anti-affinity)：\n1. 节点亲和性(Node Affinity) # apiVersion: apps/v1 kind: Deployment metadata: name: order-service # 基于您的OrderService.java spec: template: spec: affinity: nodeAffinity: # 硬性要求 requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: zone operator: In values: - zone-a - zone-b # 软性要求 preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 preference: matchExpressions: - key: node-type operator: In values: - high-performance 让我详细解释这段配置的含义：\naffinity: nodeAffinity: # 节点亲和性配置 # 1. 硬性要求 - 必须满足这些条件，否则Pod不会被调度到该节点 requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: zone # 节点标签的key operator: In # 操作符：In表示值必须在列表中 values: # 可接受的值列表 - zone-a # 可以调度到zone-a的节点 - zone-b # 可以调度到zone-b的节点 # 2. 软性要求 - 优先考虑满足这些条件的节点，但不是必须的 preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 # 权重：0-100之间，影响优先级计算 preference: matchExpressions: - key: node-type # 节点标签的key operator: In # 操作符 values: - high-performance # 优先选择标记为high-performance的节点 这个配置的作用是：\n硬性要求：\nPod必须调度到带有zone标签的节点 该标签的值必须是zone-a或zone-b 如果找不到满足条件的节点，Pod将无法调度 软性要求：\n优先选择带有node-type=high-performance标签的节点 权重100表示这是一个很强的偏好 如果没有满足条件的节点，Pod也可以调度到其他节点 实际使用示例：\n# 给节点打标签 kubectl label nodes node1 zone=zone-a kubectl label nodes node2 zone=zone-b kubectl label nodes node1 node-type=high-performance 这样的配置适合：\n跨可用区部署（zone-a, zone-b） 对性能有要求的服务（high-performance） 确保服务高可用性 2. Pod亲和性(Pod Affinity) # apiVersion: apps/v1 kind: Deployment metadata: name: order-service spec: template: spec: affinity: podAffinity: # 要求和缓存服务部署在同一节点 requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - cache-service topologyKey: kubernetes.io/hostname 让我详细解释这段Pod亲和性配置：\naffinity: podAffinity: # Pod亲和性配置 requiredDuringSchedulingIgnoredDuringExecution: # 硬性要求 - labelSelector: # 选择目标Pod的标签选择器 matchExpressions: # 标签匹配表达式 - key: app # 标签的key operator: In # 操作符 values: - cache-service # 目标Pod的标签值 topologyKey: kubernetes.io/hostname # 拓扑域，这里表示同一个主机 这个配置的含义是：\nPod亲和性要求：\n当前的order-service必须和带有app=cache-service标签的Pod部署在同一个节点上 kubernetes.io/hostname表示\u0026quot;同一个主机\u0026quot;的范围 实际应用场景：\n# cache-service部署 apiVersion: apps/v1 kind: Deployment metadata: name: cache-service spec: template: metadata: labels: app: cache-service # 缓存服务的标签 这种配置适用于：\n需要低延迟访问的场景（订单服务需要快速访问缓存） 减少网络开销 提高访问效率 实际效果：\n如果某个节点上运行着cache-service order-service的Pod会被调度到相同的节点上 如果找不到运行cache-service的节点，order-service将无法被调度 这对于OrderService.java来说很有用，因为：\n订单服务经常需要访问缓存 同节点部署可以降低延迟 提高订单处理性能 3. Pod反亲和性(Pod Anti-affinity) # apiVersion: apps/v1 kind: Deployment metadata: name: order-service spec: template: spec: affinity: podAntiAffinity: # 要求order-service的多个副本分散在不同节点 requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: app: order-service topologyKey: kubernetes.io/hostname 让我详细解释这段Pod反亲和性(Anti-affinity)配置：\naffinity: podAntiAffinity: # Pod反亲和性配置 requiredDuringSchedulingIgnoredDuringExecution: # 硬性要求 - labelSelector: # 选择目标Pod的标签 matchLabels: app: order-service # 匹配自己的服务标签 topologyKey: kubernetes.io/hostname # 拓扑域为主机级别 这个配置的含义是：\n反亲和性要求：\n带有app=order-service标签的Pod必须分散在不同的节点上 不允许两个order-service的Pod运行在同一个节点上 实际效果：\n# 如果部署3个副本 spec: replicas: 3 # 这三个Pod会被分散到不同节点 template: metadata: labels: app: order-service # 这个标签被用于反亲和性判断 这种配置的好处：\n提高可用性：\n如果一个节点故障，只会影响一个Pod 其他节点上的Pod继续提供服务 负载均衡：\nPod分散在不同节点上 避免单个节点负载过高 故障隔离：\n硬件故障的影响被限制在单个副本 提高整体服务稳定性 这对于OrderService.java特别有用，因为：\n订单服务是核心业务 需要保证高可用性 符合quota.yaml中的资源分配策略 4. 常见使用场景 # 高可用部署: apiVersion: apps/v1 kind: Deployment metadata: name: order-service spec: replicas: 3 template: spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: app: order-service topologyKey: kubernetes.io/hostname nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: node-type operator: In values: - production 让我详细解释这段配置，它结合了Pod反亲和性和节点亲和性：\nspec: replicas: 3 # 部署3个副本 template: spec: affinity: # 1. Pod反亲和性配置 podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: app: order-service # 匹配订单服务的标签 topologyKey: kubernetes.io/hostname # 按主机级别分散 # 2. 节点亲和性配置 nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: node-type # 节点标签 operator: In # 操作符 values: - production # 只部署在生产环境节点上 这个配置实现了两个目标：\nPod反亲和性要求：\n3个order-service副本必须运行在不同的节点上 提高可用性，避免单点故障 节点亲和性要求：\n只能部署在带有node-type=production标签的节点上 确保订单服务运行在生产环境节点上 实际使用示例：\n# 标记生产环境节点 kubectl label nodes node1 node-type=production kubectl label nodes node2 node-type=production kubectl label nodes node3 node-type=production 这种配置特别适合OrderService.java这样的核心业务服务，因为它：\n确保高可用性（Pod分散部署）\n保证服务质量（只使用生产级别节点）\n符合quota.yaml中的资源规划\n性能优化:\napiVersion: apps/v1 kind: Deployment metadata: name: order-service spec: template: spec: affinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 preference: matchExpressions: - key: cpu-type operator: In values: - high-performance podAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchLabels: app: cache-service topologyKey: kubernetes.io/hostname 让我详细解释这段配置，它包含了节点亲和性和Pod亲和性的软性要求：\naffinity: # 1. 节点亲和性（软性要求） nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: # 软性偏好 - weight: 100 # 权重值，最高优先级 preference: matchExpressions: - key: cpu-type operator: In values: - high-performance # 优先选择高性能CPU的节点 # 2. Pod亲和性（软性要求） podAffinity: preferredDuringSchedulingIgnoredDuringExecution: # 软性偏好 - weight: 100 # 权重值，最高优先级 podAffinityTerm: labelSelector: matchLabels: app: cache-service # 优先和缓存服务部署在一起 topologyKey: kubernetes.io/hostname # 同一节点 这个配置的特点：\n软性节点亲和性：\n优先选择有cpu-type=high-performance标签的节点 如果没有这样的节点，也可以调度到其他节点 weight: 100表示这是最高优先级的偏好 软性Pod亲和性：\n优先和cache-service部署在同一个节点 如果无法满足，也可以部署在其他节点 weight: 100表示这是最高优先级的偏好 这种配置特别适合OrderService.java，因为：\n订单服务需要高性能CPU支持 与缓存服务部署在一起可以减少网络延迟 使用软性要求保证服务可以正常部署 实际效果：\n# 标记高性能节点 kubectl label nodes node1 cpu-type=high-performance # 部署缓存服务 kubectl apply -f cache-service.yaml # order-service会优先考虑： # 1. 有高性能CPU的节点 # 2. 已经运行着cache-service的节点 这种软性要求比之前的硬性要求更灵活，因为：\n不会因为条件不满足而阻止Pod调度 在资源紧张时仍然可以部署 符合quota.yaml中的资源规划策略 5. 最佳实践 # 跨可用区部署: apiVersion: apps/v1 kind: Deployment metadata: name: order-service spec: replicas: 3 template: spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: app: order-service topologyKey: topology.kubernetes.io/zone 让我详细解释这段配置，这是一个跨可用区(zone)的Pod反亲和性配置：\nspec: replicas: 3 # 部署3个副本 template: spec: affinity: podAntiAffinity: # Pod反亲和性 requiredDuringSchedulingIgnoredDuringExecution: # 硬性要求 - labelSelector: matchLabels: app: order-service # 匹配订单服务的标签 topologyKey: topology.kubernetes.io/zone # 按可用区分散 这个配置的含义是：\n跨可用区部署： 3个order-service的副本必须分布在不同的可用区 使用topology.kubernetes.io/zone作为拓扑域 这是一个硬性要求 实际效果：\n# 假设有三个可用区 kubectl label nodes node1 topology.kubernetes.io/zone=zone-a kubectl label nodes node2 topology.kubernetes.io/zone=zone-b kubectl label nodes node3 topology.kubernetes.io/zone=zone-c # 三个Pod会分别部署在不同可用区 这种配置的优势：\n高可用性：\n即使一个可用区故障，服务仍然可用 其他可用区的Pod继续提供服务 灾难恢复：\n分散在不同可用区降低整体风险 提供更好的容灾能力 地理位置分散：\n可以服务不同地理位置的用户 潜在的性能优化 这对于OrderService.java特别重要，因为：\n订单服务是核心业务\n需要最高级别的可用性保证\n符合quota.yaml中的高可用性要求\n资源优化配置:\napiVersion: apps/v1 kind: Deployment metadata: name: order-service spec: template: spec: affinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 preference: matchExpressions: - key: resource-pool operator: In values: - high-mem 让我详细解释这段配置，这是一个针对高内存节点的软性节点亲和性配置：\naffinity: nodeAffinity: # 节点亲和性 preferredDuringSchedulingIgnoredDuringExecution: # 软性要求 - weight: 100 # 权重值（0-100），100表示最高优先级 preference: matchExpressions: - key: resource-pool # 节点标签key operator: In # 操作符 values: - high-mem # 优先选择高内存节点 这个配置的含义是：\n节点选择偏好： 优先选择标记为resource-pool=high-mem的节点 这是软性要求，如果没有符合条件的节点，Pod也可以调度到其他节点 weight: 100表示这是最高优先级的偏好 实际使用示例：\n# 标记高内存节点 kubectl label nodes node1 resource-pool=high-mem # 例如：32GB内存的节点 kubectl label nodes node2 resource-pool=normal-mem # 例如：16GB内存的节点 这种配置特别适合OrderService.java，因为：\n订单服务可能需要处理大量并发请求 可能需要较大的内存缓存 符合quota.yaml中的资源规划： # quota.yaml中的相关配置 spec: hard: requests.memory: 32Gi # 高内存需求 limits.memory: 64Gi 配置的优势：\n性能优化：\n在高内存节点上运行可以提高性能 减少内存压力和GC频率 灵活性：\n软性要求不会阻止Pod调度 在资源紧张时仍可部署 资源利用：\n优先使用高内存节点 保持资源使用的效率 6. 注意事项 # 合理使用硬性/软性要求\nrequiredDuringSchedulingIgnoredDuringExecution: 硬性要求 preferredDuringSchedulingIgnoredDuringExecution: 软性要求 考虑可用性影响\n过严格的反亲和性可能导致Pod无法调度 需要平衡高可用和资源利用率 性能影响\n复杂的亲和性规则会增加调度器的负担 建议根据实际需求适度使用 通过合理使用亲和性和反亲和性，可以：\n确保应用的高可用性 优化资源使用效率 满足特定的部署要求 提高应用性能 我看到您已经理解了这些亲和性和反亲和性的配置。让我基于quota.yaml和OrderService.java，给出一个完整的最佳实践配置：\napiVersion: apps/v1 kind: Deployment metadata: name: order-service namespace: core-business # 使用核心业务配额 spec: replicas: 3 selector: matchLabels: app: order-service template: metadata: labels: app: order-service spec: # 1. 节点亲和性 - 选择生产环境高性能节点 affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: env operator: In values: - production preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 preference: matchExpressions: - key: resource-pool operator: In values: - high-performance # 2. Pod反亲和性 - 跨可用区部署 podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: app: order-service topologyKey: topology.kubernetes.io/zone # 3. Pod亲和性 - 靠近缓存服务 podAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 80 podAffinityTerm: labelSelector: matchLabels: app: cache-service topologyKey: kubernetes.io/hostname # 4. 资源限制 - 符合quota要求 containers: - name: order-service image: your-registry/order-service:latest resources: requests: memory: \u0026#34;2Gi\u0026#34; cpu: \u0026#34;1\u0026#34; limits: memory: \u0026#34;4Gi\u0026#34; cpu: \u0026#34;2\u0026#34; 这个配置：\n确保高可用性（跨可用区部署） 优化性能（选择高性能节点） 降低延迟（靠近缓存服务） 符合资源配额要求 适合OrderService.java的运行需求 让我详细解释这个Deployment配置的每个部分：\n基本信息 metadata: name: order-service namespace: core-business # 部署在核心业务命名空间 spec: replicas: 3 # 运行3个副本 selector: matchLabels: app: order-service # 选择器，匹配Pod标签 节点亲和性配置 nodeAffinity: # 硬性要求 - 必须部署在生产环境节点 requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: env operator: In values: - production # 软性要求 - 优先选择高性能节点 preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 preference: matchExpressions: - key: resource-pool operator: In values: - high-performance Pod反亲和性配置 podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: app: order-service topologyKey: topology.kubernetes.io/zone # 确保3个副本分布在不同可用区，提高可用性 Pod亲和性配置 podAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 80 podAffinityTerm: labelSelector: matchLabels: app: cache-service topologyKey: kubernetes.io/hostname # 优先和缓存服务部署在同一节点，降低延迟 容器配置和资源限制 containers: - name: order-service image: your-registry/order-service:latest resources: requests: # 资源请求（最小保证） memory: \u0026#34;2Gi\u0026#34; # 需要2GB内存 cpu: \u0026#34;1\u0026#34; # 需要1个CPU核心 limits: # 资源上限 memory: \u0026#34;4Gi\u0026#34; # 最多使用4GB内存 cpu: \u0026#34;2\u0026#34; # 最多使用2个CPU核心 这个配置的主要目标是：\n高可用性：\n3个副本分布在不同可用区 必须运行在生产环境节点上 性能优化：\n优先选择高性能节点 与缓存服务部署在一起减少延迟 资源管理：\n明确的资源请求和限制 符合核心业务的配额要求 可靠性：\n生产环境节点部署 资源充足保证 这样的配置特别适合OrderService.java这样的核心业务服务，因为它：\n保证了服务的高可用性 优化了性能和延迟 合理分配和限制了资源使用 提供了良好的容灾能力 "},{"id":193,"href":"/docs/2025-2-19-%E9%9D%A2%E8%AF%950219/","title":"面试0219","section":"Docs","content":" 排查cpu使用率过高 linux # 排查 CPU 使用率过高时，通常可以从以下几个方面入手：\n1. 查看当前 CPU 使用情况 # 使用 top 或 htop 命令查看系统当前的 CPU 使用情况，找到占用 CPU 资源较高的进程。\ntop 或者如果你有 htop，可以提供更直观的界面：\nhtop 这些工具可以显示进程的 CPU 使用率，并且帮助你找到哪个进程占用了过多的 CPU。\n2. 使用 ps 查看具体进程 # 使用 ps 命令来列出所有进程并按 CPU 使用率排序：\nps aux --sort=-%cpu | head -n 10 这条命令会列出 CPU 使用率最高的前 10 个进程。\n3. 检查系统负载 # 使用 uptime 或 top 可以查看系统的负载情况。如果负载过高，可能是 CPU 资源被占用过多，导致系统过载。\nuptime 或者\ntop 系统负载（load average）越高，表示系统的负载越重。通常负载值大于 CPU 核数时，表示系统处于过载状态。\n4. 查看硬件性能 # 查看硬件的 CPU 使用情况，可以使用 mpstat 命令（sysstat 包）：\nmpstat -P ALL 1 该命令会按 CPU 核心查看 CPU 使用情况。\n5. 查看 I/O 和进程状态 # 如果 CPU 占用过高可能与 I/O 阻塞有关，可以使用 iotop 来查看系统的 I/O 活动：\nsudo iotop 通过它可以识别高 I/O 活动的进程，这可能间接导致 CPU 过高使用。\n6. 检查系统日志 # 查看 /var/log/syslog 或 /var/log/messages 文件，检查是否有异常的系统日志或者进程错误，可能导致 CPU 使用率过高：\ntail -f /var/log/syslog 或者\ntail -f /var/log/messages 7. 分析具体进程 # 如果你发现某个进程占用了大量 CPU，可以使用 strace 或 lsof 来进一步调查该进程在做什么：\nstrace -p \u0026lt;PID\u0026gt; 这会显示该进程的系统调用，帮助你理解它的行为。\nlsof -p \u0026lt;PID\u0026gt; 这会列出该进程打开的文件资源。\n8. 检查僵尸进程 # 僵尸进程虽然不占用 CPU，但会导致系统资源浪费，可以使用 ps aux | grep Z 查看是否有僵尸进程。\n9. 优化配置 # 调整进程优先级：使用 nice 和 renice 来调整高 CPU 使用进程的优先级。 限制进程的 CPU 使用：使用 cpulimit 工具来限制进程的最大 CPU 使用率。 10. 检查是否有恶意软件 # 如果排除了一般的原因，可能需要检查是否有恶意软件占用系统资源。可以使用一些防病毒工具（如 clamav）进行扫描。\n通过这些步骤，你应该能够定位导致 CPU 使用率过高的原因，并采取相应的措施进行处理。\n排查内存使用率过高 linux # 排查内存使用率过高时，以下是一些常见的排查步骤，可以帮助你找出问题的根本原因：\n1. 查看系统内存使用情况 # 使用 free 命令查看系统的内存使用情况：\nfree -h 输出会显示系统的总内存、已用内存、空闲内存、缓存以及交换区使用情况。关注 used 和 free 列，以及 swap 是否被大量使用。\n2. 使用 top 或 htop 查看内存占用进程 # top 命令：默认显示内存的使用情况，并按内存占用排序：\ntop 按 %MEM 列排序，找到占用内存最多的进程。\nhtop（如果安装）：提供更为友好的界面，显示 CPU、内存和进程使用情况：\nhtop 在 htop 中，你可以直接使用 F6 按钮按内存使用量排序。\n3. 使用 ps 查看内存使用的前几个进程 # 使用 ps 命令按内存使用量列出进程：\nps aux --sort=-%mem | head -n 10 这会显示内存占用最多的前 10 个进程。\n4. 查看内存的具体细节（vmstat） # 使用 vmstat 命令可以查看系统内存、交换空间以及其他资源的使用情况：\nvmstat 1 该命令每秒输出一次系统的内存统计信息，帮助你判断内存瓶颈所在。\n5. 查看交换空间的使用情况 # 如果系统内存不足，操作系统会使用交换空间（swap），这会导致性能下降。查看交换区使用情况：\nswapon -s free -h 如果交换空间使用过多，可能表示系统内存不足。\n解决方案：\n增加物理内存。 优化应用程序内存使用，避免过多的内存泄漏。 减少或禁用交换空间，如果内存足够用的话。 6. 检查是否有内存泄漏 # 内存泄漏会导致进程持续占用内存而不释放。使用以下命令可以查看进程的内存占用：\ntop 然后查看是否有进程的内存使用持续增长。\n对于某些应用程序，如果你怀疑存在内存泄漏，可以尝试重启进程，或者使用工具如 valgrind 来进一步排查问题。\n7. 使用 smem 查看内存使用情况 # smem 命令可以显示更详细的内存使用情况，包括共享内存和私有内存：\nsmem -r 这会显示所有进程的内存占用情况，包括共享内存。\n8. 查看缓存和缓冲区的使用情况 # Linux 会将空闲内存用作文件缓存和缓冲区，以提高磁盘访问性能。可以通过 free 或 cat /proc/meminfo 查看这些数据：\ncat /proc/meminfo 你会看到 Cached 和 Buffers 项，它们表示内存缓存和缓冲区的使用。通常，缓存和缓冲区的内存使用不算真正的“已用内存”，因为它们会在需要时释放。\n9. 分析内存的分配情况（/proc/meminfo） # /proc/meminfo 文件包含了系统的详细内存信息。你可以通过以下命令查看：\ncat /proc/meminfo 你可以根据以下项来查看内存使用情况：\nMemTotal: 总内存 MemFree: 空闲内存 Buffers: 缓冲区内存 Cached: 缓存内存 SwapTotal: 总交换空间 SwapFree: 空闲交换空间 10. 检查进程的内存使用情况（pmap） # pmap 命令可以查看某个进程的内存分配情况：\npmap \u0026lt;PID\u0026gt; 这可以帮助你深入了解一个特定进程的内存使用情况。\n11. 检查系统的 OOM (Out of Memory) 事件 # 如果系统内存不足，内核可能会启动 OOM 杀手，杀死某些进程以释放内存。你可以查看系统日志文件来确认是否有 OOM 事件：\ndmesg | grep -i oom 或查看 /var/log/syslog 或 /var/log/messages。\n12. 使用 lsof 查找打开的文件 # 在某些情况下，进程可能会占用大量内存和文件句柄，导致内存耗尽。可以使用 lsof 查看系统中打开的文件：\nlsof | grep \u0026lt;PID\u0026gt; 查看是否有大量文件被打开，尤其是一些大文件或连接。\n13. 内存优化和限制 # 限制进程内存使用\n：你可以使用\nulimit 设置进程的内存使用限制，避免某些进程占用过多内存：\nulimit -v \u0026lt;memory_limit\u0026gt; 内存清理\n：如果发现系统内存被缓存占用，可以尝试释放缓存：\nsync; echo 3 \u0026gt; /proc/sys/vm/drop_caches 通过这些步骤，你应该能够确定内存过高使用的原因，并采取相应的措施解决问题。如果是某个进程异常占用内存，可以考虑优化或重启该进程；如果是系统配置问题，可能需要增加物理内存或优化交换空间的使用。\necho \u0026ldquo;#/dev/mapper/vg\u0026ndash;docker-lvapp /apps xfs rw,pquota 0 0\u0026rdquo; \u0026raquo; /etc/fstab，pquota 啥意思 不加这个参数会导致docker容器根磁盘限额失效吗，或者说导致docker起不来 导致kubelet起不来 # pquota 是一个挂载选项，通常与 xfs 文件系统的配额（quota）功能相关，表示启用磁盘配额功能来限制某些用户、组或目录的磁盘使用量。\npquota：这个选项用于启用 XFS 文件系统的 项目配额（Project Quotas），也就是对特定项目（项目是基于某个目录的，可以理解为一个与特定目录关联的配额）设置配额。启用该选项后，系统会对 /apps 目录的文件进行配额管理。 1. 配额的作用 # 配额用于控制文件系统中用户或项目的最大磁盘使用量。例如，你可以限制某个 Docker 容器根目录的磁盘空间，避免某个容器占满整个磁盘，影响其他容器或系统运行。 如果启用了配额，系统会根据配置限制某个用户、组或路径下的文件总占用空间，一旦达到配额限制，文件的写入操作就会被拒绝。 2. 不加 pquota 会有什么影响？ # 不会启用项目配额：如果不加 pquota，就不会启用 XFS 文件系统的配额功能。因此，你就无法控制 /apps 目录下的磁盘使用配额，或者说，无法设置 Docker 容器的磁盘限制。 对 Docker 或 Kubelet 是否影响：如果你没有启用配额，理论上 Docker 和 Kubernetes 的容器应该还是能够正常启动，因为它们依赖于磁盘的可用性，而不是配额功能。配额只是在磁盘空间使用上进行额外的限制和管理。如果没有配额，容器仍然能够使用整个磁盘，只是没有限制。 3. 可能的风险 # 没有配额的风险：如果 Docker 容器没有配额限制，某个容器可能会无限制地占用磁盘空间，导致磁盘被填满，影响系统的其他部分（包括 Kubelet）。如果没有设置限制，Docker 容器可能会导致系统磁盘空间耗尽，从而影响容器或其他服务的运行。 4. 总结 # pquota 参数启用 XFS 的配额功能，确保某个目录（如 /apps）下的文件使用量受到限制。 如果不加 pquota，配额功能会失效，但是 Docker 容器和 Kubelet 仍然能够正常启动，前提是磁盘没有被完全占满。 只有在你希望对 Docker 容器的磁盘使用进行严格限制时，才需要启用这个选项。否则，如果你不加 pquota，Docker 容器会自由使用磁盘空间，但仍可能面临磁盘满的问题。 如果你的目的是限制 Docker 容器的磁盘空间，启用配额功能会更加稳妥，避免单个容器占用过多磁盘空间影响其他容器和系统的正常运行。\n如果 Docker 容器和 Kubelet 无法启动，并且你怀疑是由于 /etc/fstab 中缺少 pquota 参数导致的，以下是一些排查和解决步骤：\n1. 检查挂载是否正确 # 确保 /etc/fstab 文件中的挂载项正确配置，并且文件系统已成功挂载。运行以下命令检查挂载情况：\nmount | grep /apps 如果没有显示 /apps 挂载的相关信息，说明该文件系统没有成功挂载。你可以尝试手动挂载：\nmount -a 这个命令会根据 /etc/fstab 文件的配置挂载所有未挂载的文件系统。\n2. 检查磁盘配额是否启用 # 如果确实需要启用项目配额（pquota），首先确保系统支持并已启用配额。可以查看 /etc/fstab 配置中的挂载选项是否正确。你可以通过以下命令查看 XFS 文件系统是否启用了项目配额：\nxfs_info /apps 在输出中查找 pquota 是否启用。如果没有启用配额，尝试修改 /etc/fstab，加上 pquota 选项：\necho \u0026#34;#/dev/mapper/vg--docker-lvapp /apps xfs rw,pquota 0 0\u0026#34; \u0026gt;\u0026gt; /etc/fstab 然后重新挂载：\nmount -o remount /apps 3. 检查系统日志 # 查看系统日志来确定为什么 Docker 和 Kubelet 没有启动，特别是看看是否有与磁盘挂载或配额相关的错误。检查以下日志：\njournalctl -xe | grep docker journalctl -xe | grep kubelet 这些日志可能会提供更具体的错误信息，帮助你定位问题。\n4. 检查 Docker 和 Kubelet 配置 # Docker 配置：查看 Docker 的日志（通常在 /var/log/docker.log）来确认是否有磁盘空间问题或者其他启动失败的原因。 Kubelet 配置：Kubelet 启动失败也可能与挂载或磁盘配置有关，查看 /var/log/syslog 或 /var/log/messages 中的错误信息。 5. 确认磁盘是否满 # 如果磁盘空间不足，Docker 和 Kubelet 可能也无法启动。检查磁盘的使用情况：\ndf -h 如果磁盘空间已满，可以清理不必要的文件，或者增加磁盘空间。\n6. 尝试移除 pquota 选项 # 如果你怀疑是 pquota 选项导致 Docker 或 Kubelet 启动失败，可以临时移除该选项，重新挂载文件系统，并重试 Docker 和 Kubelet 启动。修改 /etc/fstab 移除 pquota 选项后，重新挂载：\nmount -o remount /apps 然后检查 Docker 和 Kubelet 是否能启动。\n7. 重新启动 Docker 和 Kubelet # 在确认文件系统挂载无误后，尝试重新启动 Docker 和 Kubelet：\nsystemctl restart docker systemctl restart kubelet 然后检查它们的状态：\nsystemctl status docker systemctl status kubelet 如果 Docker 和 Kubelet 仍然无法启动，可以通过查看日志或增加 pquota 配额选项来进一步诊断。\n修改calico的默认的vxlan的端口号 # 要修改 Calico 默认的 VXLAN 端口号，你需要修改 Calico 的配置文件，使其使用自定义端口。Calico 默认使用 VXLAN 隧道技术来处理节点间的网络通信，默认端口号是 4789。如果你想修改该端口，可以通过修改 Calico 的配置文件来实现。\n1. 修改 Calico 配置 # Calico 的 VXLAN 配置通常在 calicoctl 配置文件或者 Kubernetes 的 ConfigMap 中进行修改。以下是两种常见的方式：\n方式一：使用 calicoctl 修改 VXLAN 端口 # 你可以使用 calicoctl 命令来修改 VXLAN 的端口配置。\n检查现有的 Calico 配置：\ncalicoctl get bgpconfiguration default -o yaml 创建或修改 Calico 配置： 要修改 VXLAN 端口号，需要修改 Calico 的 bgpconfiguration 或 ipam 配置。首先创建一个新的配置文件（例如 calico-vxlan-port.yaml）：\napiVersion: crd.projectcalico.org/v1 kind: BGPConfiguration metadata: name: default spec: vxlanPort: \u0026lt;new-port-number\u0026gt; # 替换为你想设置的端口号 应用配置： 使用以下命令应用该配置：\ncalicoctl apply -f calico-vxlan-port.yaml 验证配置： 应用后，你可以通过以下命令确认配置是否成功：\ncalicoctl get bgpconfiguration default -o yaml 方式二：通过 Kubernetes ConfigMap 修改 # 如果你是使用 Kubernetes 来部署 Calico，可以通过修改 ConfigMap 来修改 VXLAN 端口。\n找到 Calico 的 ConfigMap： 在 Kubernetes 中，Calico 的配置通常保存在 calico-config ConfigMap 中，使用以下命令查找它：\nkubectl get configmap -n kube-system calico-config -o yaml 修改 ConfigMap： 找到并编辑 calico-config ConfigMap，修改 vxlanPort 的配置。你可以使用 kubectl edit 或者手动编辑 ConfigMap 文件：\nkubectl edit configmap calico-config -n kube-system 在编辑文件时，找到 vxlanPort 选项并设置新的端口号。例如：\napiVersion: v1 data: calico_backend: vxlan vxlanPort: \u0026#34;\u0026lt;new-port-number\u0026gt;\u0026#34; # 替换为你想设置的端口号 应用更新后的 ConfigMap： 保存文件并退出编辑器。更新后的 ConfigMap 会自动应用。\n重启 Calico Pods： 更新配置后，需要重启 Calico 的 Pod 来使新配置生效。你可以使用以下命令来重启 Calico 的 DaemonSet：\nkubectl rollout restart daemonset calico-node -n kube-system 2. 验证配置 # 配置修改完成后，你可以通过以下命令来确认 VXLAN 端口是否已经修改成功：\ncalicoctl get bgpconfiguration default -o yaml 或者在 Kubernetes 环境中，检查 Calico Pod 的日志：\nkubectl logs -n kube-system \u0026lt;calico-pod-name\u0026gt; 你还可以查看 ip link 或 ss 来确认 VXLAN 的端口号是否已经生效。\n3. 注意事项 # 修改 VXLAN 端口号后，确保你的防火墙规则或网络策略允许新的端口号通过，特别是节点间的网络。 修改配置后，可能需要重新启动 Calico 节点，以确保配置生效。 通过这些步骤，你可以成功修改 Calico 的 VXLAN 端口号。\n是的，Felix（Calico 的数据平面代理）也有一个配置项用于指定 VXLAN 的端口号。Felix 配置项可以通过修改 FelixConfiguration CRD 或 Kubernetes 中的 ConfigMap 来修改 VXLAN 的端口号。具体配置项是 vxlanPort，你可以在这里指定新的端口号。\n修改 Felix 配置来更改 VXLAN 端口号 # 查找当前 Felix 配置：\n如果你正在 Kubernetes 中使用 Calico，可以通过以下命令查看 FelixConfiguration 配置：\nkubectl get felixconfiguration default -o yaml 修改 Felix 配置：\n使用 kubectl edit 编辑 FelixConfiguration CRD，或者你可以直接创建一个新的 YAML 文件，指定自定义的 VXLAN 端口号。例如：\napiVersion: crd.projectcalico.org/v1 kind: FelixConfiguration metadata: name: default spec: vxlanPort: \u0026lt;new-port-number\u0026gt; # 替换为你想设置的端口号 使用 kubectl 应用配置：\nkubectl apply -f felix-vxlan-port.yaml 验证配置：\n你可以通过以下命令检查 Felix 配置是否成功应用：\nkubectl get felixconfiguration default -o yaml 重启 Calico 节点：\n修改 Felix 配置后，需要重启 Calico 节点才能使新配置生效。你可以通过以下命令重启 calico-node DaemonSet：\nkubectl rollout restart daemonset calico-node -n kube-system 验证 VXLAN 端口是否修改成功：\n使用以下命令检查 VXLAN 端口是否已经生效：\ncalicoctl get bgpconfiguration default -o yaml 或者你可以查看 calico-node 的日志来确认是否使用了新的 VXLAN 端口：\nkubectl logs -n kube-system \u0026lt;calico-pod-name\u0026gt; 总结 # Felix 配置：通过修改 FelixConfiguration CRD，可以指定 VXLAN 的端口号。配置项是 vxlanPort。 通过 kubectl 修改：可以直接使用 kubectl 修改或创建 FelixConfiguration，并应用新的配置。 重启 calico-node：修改配置后需要重启 Calico 节点，以使配置生效。 通过以上步骤，你可以成功地修改 Calico 中 Felix 的 VXLAN 端口号。\n没有启用配额，但docker的daemon.json的opt有限制磁盘大小的参数，docker起不来的吧 # 是的，如果在 Docker 的 daemon.json 配置文件中启用了磁盘大小限制参数（如 storage-opts 中的 size 配置），但是没有正确设置文件系统的配额（特别是对于 Docker 容器的磁盘限制），那么 Docker 守护进程可能会无法启动，尤其是在以下情况下：\n1. daemon.json 配置示例 # 如果你在 daemon.json 中启用了磁盘大小限制参数（例如 storage-opts 下的 size 参数），它通常会限制 Docker 容器使用的存储空间。下面是一个典型的 daemon.json 配置示例：\n{ \u0026#34;storage-driver\u0026#34;: \u0026#34;overlay2\u0026#34;, \u0026#34;storage-opts\u0026#34;: [ \u0026#34;size=20G\u0026#34; ] } 这段配置表示每个 Docker 容器的磁盘空间最大为 20GB。\n2. 可能的错误 # 如果没有启用磁盘配额（尤其是在使用 XFS 或其他文件系统时），而 daemon.json 中的 storage-opts 参数试图限制磁盘大小，Docker 守护进程可能会遇到以下问题：\n容器无法限制磁盘空间：如果没有配额，size 参数将无法实际生效，可能导致 Docker 容器无法启动，因为 Docker 不能控制容器的存储空间大小。 Docker 启动失败：如果 daemon.json 中的参数与系统设置（如文件系统配额或磁盘空间）不兼容，Docker 可能会因配置错误无法启动。 磁盘空间不足：如果 Docker 守护进程配置了磁盘限制（例如限制了容器磁盘空间），而文件系统本身没有配额支持，可能会导致磁盘使用不当，最终导致 Docker 守护进程启动失败。 3. 如何避免 Docker 启动失败 # 检查文件系统支持：确保文件系统支持容器磁盘配额功能。如果使用的是 xfs 文件系统，需要确保配额（pquota）已启用。\n检查 daemon.json 配置：确保 Docker 配置中 storage-opts 与文件系统的配额配置兼容。\n重启 Docker 服务\n：修改配置后，记得重启 Docker 服务：\nsystemctl restart docker 4. 解决方法 # 启用文件系统配额：如果你需要限制容器的磁盘使用量，确保文件系统（如 XFS）启用了配额功能。对于 XFS，可以在挂载时加上 pquota 参数，并确保文件系统支持该功能。 避免过度配置限制：在没有配额支持的情况下，不要在 daemon.json 中配置磁盘大小限制，或者考虑移除 size 参数。 5. 排查 Docker 启动问题 # 如果 Docker 因为磁盘大小限制而无法启动，可以通过查看 Docker 日志来获取更多信息：\njournalctl -u docker.service 查看是否有与磁盘配额或磁盘空间限制相关的错误信息。\n总结来说，如果没有启用文件系统配额，而在 daemon.json 中设置了磁盘大小限制，Docker 可能无法启动，因为它会尝试应用这些限制，但是缺乏合适的配额支持。因此，你需要确保配置一致性，并且文件系统支持相关的配额功能。\n单etcd单master拓展为3master3etcd，master节点挂上vip，做高可用，证书重新生成，etcd之前数据保留 # 单master单etcd改造为3master3etcd\n将一个 单 Master 单 ETCD 的 Kubernetes 集群拓展为 3 Master 3 ETCD 并实现高可用性（HA），同时保持 ETCD 数据 的持久性，并确保新证书的生成，可以按照以下步骤进行：\n1. 准备工作 # 在开始之前，确保：\n备份现有的 ETCD 数据：在操作前，请备份当前单 Master 集群的 ETCD 数据，以防万一。 准备证书：由于需要重新生成证书（尤其是 Kubernetes 控制平面的证书），你应该确保有一个新的证书管理工具（如 kubeadm 或自定义脚本）来处理证书。 网络和防火墙配置：确保新的 Master 和 ETCD 节点之间的网络连接没有问题，尤其是 TCP 端口 2379 和 2380（ETCD 的通信端口）要开放。 2. 部署 3 Master 节点 # 2.1. 配置 VIP # 为 Master 节点配置 虚拟 IP (VIP)，使得客户端（如 kubelet 和 kubectl）可以访问一个统一的 API Server 地址。你可以使用 Keepalived 或其他工具来配置高可用 VIP。\n安装 Keepalived（在每个 Master 节点上）：\napt install keepalived 然后在每个 Master 节点上配置 keepalived.conf：\n# /etc/keepalived/keepalived.conf vrrp_instance VI_1 { state MASTER interface eth0 # 修改为你的网络接口 virtual_ipaddress { 192.168.1.100 # 配置一个虚拟 IP 地址 } } 启动 Keepalived：\nsystemctl enable keepalived systemctl start keepalived 这样你就可以为 Kubernetes API Server 配置一个高可用的 VIP。\n2.2. 安装 kubeadm 和 Kubernetes # 在新 Master 节点上安装 kubeadm 和 Kubernetes（如果未安装）：\napt update apt install -y kubeadm kubelet kubectl 2.3. 加入新 Master 节点到集群 # 从现有 Master 节点获取加入令牌和集群证书：\nkubeadm token create --print-join-command 将返回的 kubeadm join 命令用于新的 Master 节点，加入现有集群：\nkubeadm join \u0026lt;api-server-vip\u0026gt;:6443 --token \u0026lt;token\u0026gt; --discovery-token-ca-cert-hash sha256:\u0026lt;hash\u0026gt; 2.4. 升级 API Server 以支持多个 Master 节点 # 确保 API Server 配置支持多 Master 集群。此时你可能需要编辑现有的 API Server 配置，启用负载均衡器或更新 kube-apiserver 的启动参数，以支持在多个 Master 节点上运行。\n3. 部署 3 ETCD 节点 # 3.1. ETCD 高可用配置 # 为使 ETCD 集群支持高可用性，需要将现有的单节点 ETCD 拓展为 3 个节点的集群。\n备份现有的 ETCD 数据：\nETCDCTL_API=3 etcdctl snapshot save /tmp/etcd-backup.db 将现有的单节点 ETCD 数据同步到新节点： 在每个新的 ETCD 节点上安装 ETCD：\napt-get install etcd 在每个 ETCD 节点上配置 etcd.conf，确保它们可以相互通信。设置 ETCD_INITIAL_CLUSTER 参数，定义每个节点的名称和 IP 地址。例如：\nexport ETCD_INITIAL_CLUSTER=\u0026#34;etcd1=http://\u0026lt;node1-ip\u0026gt;:2380,etcd2=http://\u0026lt;node2-ip\u0026gt;:2380,etcd3=http://\u0026lt;node3-ip\u0026gt;:2380\u0026#34; 启动 ETCD 集群： 启动每个 ETCD 节点，确保它们彼此之间能连接。\n3.2. 修改 Kubernetes 配置来使用 3 节点 ETCD 集群 # 修改 Kubernetes API Server 配置，使其连接到 3 节点的 ETCD 集群。编辑 kube-apiserver 配置文件，更新 --etcd-servers 参数，指向 3 个 ETCD 节点。\n例如：\n--etcd-servers=http://etcd1:2379,http://etcd2:2379,http://etcd3:2379 4. 更新证书 # 在升级为多节点高可用的集群时，所有证书都需要重新生成。可以使用 kubeadm 来重新生成证书：\n4.1. 备份现有证书 # 备份现有的 Kubernetes 证书文件（通常在 /etc/kubernetes/pki 下）：\ncp -r /etc/kubernetes/pki /etc/kubernetes/pki.bak 4.2. 重新生成证书 # 使用 kubeadm 重新生成证书：\nkubeadm init phase certs all --config /etc/kubernetes/kubeadm.conf 4.3. 重新部署 Kubernetes 组件 # 更新证书后，重新部署 Kubernetes 组件（如 kube-apiserver、kube-controller-manager、kube-scheduler 等）。可以使用以下命令进行：\nkubectl apply -f /etc/kubernetes/manifests/ 5. 恢复 ETCD 数据 # 将之前备份的 ETCD 数据恢复到新部署的 ETCD 集群：\n在所有 ETCD 节点上停止 ETCD 服务。\n使用备份的 ETCD 数据恢复：\nETCDCTL_API=3 etcdctl snapshot restore /tmp/etcd-backup.db --data-dir /var/lib/etcd 启动 ETCD 服务。\n6. 验证集群 # 确保所有节点、服务和证书都配置正确，集群正常运行。检查以下内容：\nkubectl get nodes：确保所有节点（包括新的 Master）都已加入集群。 kubectl get pods -n kube-system：确保所有核心组件（如 API Server、Controller Manager、Scheduler）都正常运行。 kubectl get componentstatus：检查集群组件的状态。 总结 # 通过以上步骤，您可以将单 Master 单 ETCD 的 Kubernetes 集群扩展为 3 Master 3 ETCD 的高可用集群，并在确保 ETCD 数据持久性的同时重新生成所需的证书。这个过程中，确保正确配置 VIP 和负载均衡，确保新的 ETCD 集群与 API Server 配置能够互相通信，最终实现 Kubernetes 高可用性。\n# 跨网段的机器组成k8s集群，不同网段机器上的pod之间无法相互访问，用calico的vxlan模式，修改默认的vxlan的端口号，因为之前的4789被物理网关设备占用了 # 在 Kubernetes 集群中使用 Calico 的 VXLAN 模式时，默认情况下，VXLAN 使用 UDP 端口 4789 进行通信。如果你的物理网关设备已经占用了 4789 端口，导致不同网段的机器之间的 Pod 无法相互访问，你可以修改 Calico VXLAN 使用的端口号。\n为了修改 Calico VXLAN 使用的默认端口号，可以通过配置 Felix（Calico 的数据平面代理）和 Calico 配置文件来实现。以下是修改 VXLAN 默认端口号的详细步骤。\n步骤 1: 修改 Felix 配置（修改 VXLAN 端口） # Calico 的数据平面由 Felix 处理，它可以控制 VXLAN 端口的设置。你可以通过修改 FelixConfiguration CRD（自定义资源定义）来调整 VXLAN 端口。\n1.1. 查看当前的 Felix 配置 # 使用以下命令查看当前的 FelixConfiguration：\nkubectl get felixconfiguration default -o yaml 1.2. 创建或修改 Felix 配置 # 为了修改 VXLAN 端口，可以创建或修改 FelixConfiguration CRD，指定新的端口号。比如将默认端口 4789 修改为 12345。\n创建一个新的配置文件（例如：felix-vxlan-port.yaml）：\napiVersion: crd.projectcalico.org/v1 kind: FelixConfiguration metadata: name: default spec: vxlanPort: 12345 # 将此处端口号修改为你想使用的端口 应用该配置：\nkubectl apply -f felix-vxlan-port.yaml 1.3. 重启 Calico 节点 # 配置完成后，重启 Calico 节点以使配置生效。你可以通过以下命令重启 calico-node DaemonSet：\nkubectl rollout restart daemonset calico-node -n kube-system 步骤 2: 修改 Kubernetes API Server 和节点配置 # Calico VXLAN 的端口号是由 Felix 配置的，但你还需要确保集群中每个节点的网络设置允许跨网段通信，并且新的端口号不被其他网络设备占用。\n2.1. 更新 API Server 配置 # 如果你修改了 VXLAN 端口号，需要确保所有节点的网络路由能够通过新的端口进行通信。通常，Kubernetes 的 API Server 配置（kube-apiserver）无需特别更改，但你要确保节点间的网络路由和防火墙规则允许通过新的 VXLAN 端口进行通信。\n2.2. 更新防火墙和网络规则 # 确保跨网段的机器间能够通过新的 VXLAN 端口进行通信。你需要在集群节点上开放新的端口（例如 12345）：\n# 以 root 权限开放端口 iptables -A INPUT -p udp --dport 12345 -j ACCEPT 对于 iptables 和其他防火墙系统，确保 12345 端口可以在所有节点间进行通信。\n步骤 3: 验证配置 # 修改配置后，确认以下几项：\n检查 VXLAN 端口是否生效\n在每个节点上，确认 VXLAN 使用的是新的端口号。你可以使用 ss 或 netstat 工具来确认端口监听情况：\nss -uln | grep 12345 验证跨网段通信是否正常\n在不同网段的 Pod 上，执行 ping 或 curl 来确认它们是否能够通过新的 VXLAN 端口号进行通信。\n检查 Calico Pod 和 DaemonSet 的状态\n确认所有 calico-node DaemonSet Pods 正常运行：\nkubectl get pods -n kube-system -l k8s-app=calico-node 检查是否有任何错误或挂起的 Pod。\n检查 Calico 日志\n查看 Calico 的日志，以确认是否有任何与 VXLAN 配置或端口号相关的错误：\nkubectl logs -n kube-system \u0026lt;calico-node-pod-name\u0026gt; 总结 # 修改 Calico 的 VXLAN 默认端口号，你需要更新 FelixConfiguration 配置文件中的 vxlanPort 参数，指定新的端口号。 更新后，重新启动 Calico 节点（calico-node DaemonSet），并确保网络路由和防火墙规则允许通过新的端口号进行通信。 验证配置是否生效，并检查跨网段的 Pod 是否能够互相访问。 通过这些步骤，你就可以将 Calico 的 VXLAN 端口号修改为新的端口，并解决跨网段通信的问题。\nenvoy的版本过低导致流量被拦截，升级版本解决了 # 如果 Envoy 版本过低导致流量被拦截的问题，通过升级 Envoy 版本解决了问题，这通常是由于以下几种原因：\n1. 过时的协议支持 # 较低版本的 Envoy 可能不支持某些新协议或特性，比如新的 HTTP/2、gRPC 或 mTLS 配置等。这可能会导致流量被拦截，无法正确路由或处理。\n2. 安全性问题 # 较低版本的 Envoy 可能存在已知的安全漏洞，某些流量被拦截可能是因为旧版本存在某些未修复的缺陷。在安全修复后，升级到较新的版本可以避免这些问题。\n3. 代理和路由问题 # 在较低版本中，Envoy 可能存在一些 bug 或不兼容的配置，导致流量未能正确路由或通过代理。升级到新版后，Envoy 中的路由、流量管理等功能会得到改进，从而解决了流量拦截的问题。\n4. 性能优化和 bug 修复 # 新版 Envoy 通常会包括性能优化和 bug 修复，这些优化不仅能提高流量处理能力，还能避免由于版本限制导致的流量拦截或不正常路由。\n如何升级 Envoy # 如果你已经解决了这个问题，下面是一般的 Envoy 升级流程，以防你以后遇到类似的情况：\n1. 检查当前 Envoy 版本 # 你可以通过以下命令检查当前安装的 Envoy 版本：\nenvoy --version 2. 安装或升级到最新版本 # 根据你使用的操作系统和安装方式，选择适合的升级方法：\n使用官方 APT 仓库（Ubuntu/Debian）：\ncurl -sL https://getenvoy.io | sudo bash sudo apt-get install envoy 使用 Homebrew（macOS）：\nbrew install envoy 使用 Docker（容器环境）：\n如果你使用容器运行 Envoy，可以更新 Docker 镜像：\ndocker pull envoyproxy/envoy:v1.24.0 # 替换为所需的版本 手动下载并安装：\n你可以从 Envoy 的 GitHub releases 页面 下载适合你平台的最新版本，并手动安装。\n3. 验证升级后的版本 # 升级完成后，确认 Envoy 已经成功升级到新版本：\nenvoy --version 4. 检查配置和功能 # 确保你的配置（如 xDS、路由配置、安全证书配置 等）兼容新版本的 Envoy。你可能需要检查日志以确认升级后的 Envoy 是否正确加载了新的配置。\n总结 # Envoy 版本过低通常会导致一些流量问题，升级后不仅能解决这些问题，还能享受更好的安全性、性能和协议支持。通过正确的升级步骤，你可以确保 Envoy 为集群提供最佳的流量管理和代理服务。如果你还需要进行其它配置修改或性能优化，记得检查新版本的发布说明和文档。\n节点noteready 恢复 # 当 Kubernetes 节点状态变为 NotReady 时，表示该节点未能正常加入集群或者与集群的控制平面失去联系。通常，节点的 NotReady 状态可能由以下几种原因引起：网络问题、节点资源不足、Kubelet 配置问题、系统组件未正常运行等。\n以下是一些常见的故障排查和恢复步骤，帮助你将节点恢复到 Ready 状态：\n1. 查看节点状态 # 首先，你可以使用 kubectl 命令查看节点状态及相关的事件：\nkubectl get nodes 查看详细信息：\nkubectl describe node \u0026lt;node-name\u0026gt; 输出中会显示有关节点的详细信息，包括 NotReady 的原因和事件日志，这将帮助你定位问题的根源。\n2. 检查 Kubelet 状态 # Kubelet 是运行在每个节点上的组件，负责与控制平面通信并管理容器。如果 Kubelet 出现问题，节点会处于 NotReady 状态。\n检查 Kubelet 状态：\nsystemctl status kubelet 查看 Kubelet 日志：\njournalctl -u kubelet -f 如果 Kubelet 没有正常运行，可以尝试重新启动：\nsystemctl restart kubelet 3. 检查网络配置 # 网络问题可能导致节点无法与其他节点或控制平面通信。检查以下内容：\n防火墙规则：确保节点间的必要端口（如 6443、2379、2380、10250、10255）没有被阻塞。 网络插件：如果你使用了网络插件（如 Calico、Weave），确保它们的配置正确，并且插件运行正常。 查看网络插件的日志，确认它是否在正常工作：\nkubectl logs -n kube-system \u0026lt;network-plugin-pod-name\u0026gt; 4. 资源使用情况 # 节点可能由于资源不足（如 CPU、内存、磁盘等）而变为 NotReady。\n检查节点的资源使用情况：\ntop node \u0026lt;node-name\u0026gt; 检查磁盘空间：\ndf -h 查看系统负载：\nuptime 如果发现资源过载，考虑扩展节点资源或优化资源使用。\n5. 检查 API Server 与节点连接 # 如果节点无法与控制平面（API Server）通信，节点将显示为 NotReady。检查以下内容：\nAPI Server 是否正常运行：\nkubectl get pods -n kube-system -l component=kube-apiserver 节点到 API Server 的连接状态： 你可以尝试使用 curl 或 telnet 测试节点与控制平面之间的连接。\n6. 重新加入节点到集群 # 如果节点的状态长时间为 NotReady，并且没有明显的错误，尝试将节点从集群中移除并重新加入：\n从集群中驱逐节点：\nkubectl drain \u0026lt;node-name\u0026gt; --ignore-daemonsets --delete-local-data 从集群中删除节点：\nkubectl delete node \u0026lt;node-name\u0026gt; 重新将节点加入集群： 如果节点是通过 kubeadm 安装的，使用之前的 kubeadm join 命令将节点重新加入集群。\n7. 检查 Kubelet 配置文件 # Kubelet 配置文件（通常在 /etc/kubernetes/kubelet.conf 或 /etc/kubernetes/manifests）可能存在配置问题，导致无法正常启动。检查配置是否正确，尤其是 API Server 地址和证书配置。\n8. 节点恢复后的验证 # 恢复节点后，验证节点是否已经恢复为 Ready 状态：\nkubectl get nodes 确认节点的状态是否变为 Ready。如果恢复成功，可以重新调度 Pod。\n总结 # 当节点处于 NotReady 状态时，排查和修复的步骤通常包括检查 Kubelet 状态、网络配置、资源使用情况以及控制平面与节点的连接。通过这些步骤，通常能够找到问题并恢复节点。如果无法解决，重新加入节点到集群也是一种有效的恢复方式。\n数据库磁盘使用超过85%导致数据库只读 # 当数据库磁盘使用超过 85% 时，许多数据库管理系统（如 MySQL、PostgreSQL、Oracle 等）会进入只读模式，防止数据丢失或进一步的磁盘满载。这是一种保护机制，旨在避免在磁盘空间耗尽时对数据库的操作产生不可预知的影响。\n解决方案 # 1. 清理磁盘空间 # 首先，必须立即清理磁盘空间，以便数据库恢复写入操作。以下是一些常见的清理方法：\n删除不必要的日志文件： 数据库通常会生成大量的日志文件，包括错误日志、查询日志、慢查询日志等。查看并删除不再需要的日志文件。\n对于 MySQL，可以查看日志目录并删除旧日志：\nls /var/log/mysql/ sudo rm /var/log/mysql/mysql-slow.log sudo rm /var/log/mysql/error.log.* 对于 PostgreSQL，查看日志并删除：\nls /var/log/postgresql/ sudo rm /var/log/postgresql/postgresql.log.* 清理数据库表中的临时数据：\n检查数据库中是否有临时表、过期数据或无用数据表，删除不必要的数据。\n对于 MySQL，执行：\nDELETE FROM \u0026lt;table_name\u0026gt; WHERE \u0026lt;condition\u0026gt;; 删除或压缩备份文件： 数据库备份文件往往占用大量空间。检查并删除旧的或不需要的备份文件，或者将它们转移到其他存储位置。\n检查磁盘使用情况： 使用 du 或 df 命令检查具体哪些目录或文件占用最多的磁盘空间：\ndu -sh /var/lib/mysql # 对于 MySQL df -h # 检查整个磁盘的使用情况 2. 扩大磁盘空间 # 如果清理后仍然无法解决问题，你可能需要为数据库服务器增加磁盘空间：\n扩展现有磁盘：通过云平台或者物理机器管理工具增加磁盘空间。 挂载额外的磁盘：如果有多余的磁盘，可以将它挂载到数据库目录，并调整数据库配置使用新的磁盘空间。 3. 检查数据库表的大小和碎片 # 数据库表和索引的过度膨胀可能导致磁盘空间的异常消耗。你可以通过以下方式进行优化：\n优化数据库表（MySQL）： 执行 OPTIMIZE TABLE 来优化表，释放碎片：\nOPTIMIZE TABLE \u0026lt;table_name\u0026gt;; 对于 PostgreSQL，使用 VACUUM 清理无效空间：\nVACUUM FULL; 4. 恢复数据库写入权限 # 一旦磁盘空间恢复并且足够使用，数据库应该自动恢复写入权限。如果没有恢复，可以手动执行以下操作：\nMySQL： 检查数据库是否仍在只读模式：\nSHOW VARIABLES LIKE \u0026#39;read_only\u0026#39;; 如果显示为 ON，则可以执行以下命令恢复写入权限：\nSET GLOBAL read_only = OFF; PostgreSQL： PostgreSQL 通常通过设置 default_transaction_read_only 进入只读模式。检查并修改：\nSHOW default_transaction_read_only; SET default_transaction_read_only = off; 5. 配置磁盘监控和告警 # 为了避免磁盘空间再次用尽，配置磁盘监控和告警非常重要。你可以设置如下的监控措施：\n使用 Prometheus + Grafana 来监控磁盘使用情况，并设置告警规则。\n配置操作系统的\ncron 定期检查磁盘使用情况，并在达到阈值时发送告警邮件：\ndf -h | awk \u0026#39;$5 \u0026gt; 85 {print $0}\u0026#39; | mail -s \u0026#34;Disk Usage Alert\u0026#34; admin@example.com 总结 # 当数据库磁盘使用超过 85% 时，通常会进入只读模式以保护数据的完整性。解决此问题的步骤包括清理磁盘空间、扩大磁盘容量、优化数据库表和恢复数据库写入权限。同时，建立磁盘监控和告警机制，能够帮助预防类似问题的再次发生。\nprometheus指标拆分解决oom的问题 # 当 Prometheus 遇到 OOM（Out of Memory）问题时，通常是因为某些指标（例如某些高频率采集的指标、某些非常大的指标数据）占用了过多的内存。为了避免这种情况，可以采取以下几种方法来拆分指标、优化 Prometheus 的内存使用。\n1. 拆分大型指标 # 某些指标可能包含大量的数据点，尤其是高卡特利性（high cardinality）指标。这类指标会占用大量内存。以下是拆分指标的方法：\n减少高卡特利性指标的采集频率： 如果某个指标具有大量的不同标签值（例如每个请求的响应时间），你可以考虑减少采集频率，或者限制某些标签的组合。\nscrape_configs: - job_name: \u0026#39;my_job\u0026#39; scrape_interval: 10s # 降低采集频率 relabel_configs: - source_labels: [__name__] target_label: job replacement: my_job 过滤不必要的标签： 对于一些指标，可以通过 relabeling 过滤掉不必要的标签。高卡特利性通常是由标签组合导致的，可以通过减少某些标签的数量来减小内存消耗。\nscrape_configs: - job_name: \u0026#39;my_job\u0026#39; static_configs: - targets: [\u0026#39;localhost:9090\u0026#39;] relabel_configs: - source_labels: [__name__] target_label: job replacement: my_job - source_labels: [some_label] target_label: some_label action: drop # 丢弃不需要的标签 拆分大指标为多个小指标： 对于一些指标，如果它们的维度较高，可以考虑拆分为多个指标，而不是一个大指标。例如，将 http_requests_total 按路径或方法拆分成多个指标。\n2. 限制 Prometheus 存储使用 # Prometheus 会将指标数据存储在本地磁盘上。如果存储量过大，Prometheus 可能会消耗过多内存。通过配置 retention 和 max_age 限制存储的时间，可以减少 Prometheus 占用的内存。\n配置数据保留时间： 你可以通过调整 --storage.tsdb.retention.time 来限制存储数据的时长。默认情况下，Prometheus 会保留最近的 15 天的数据，但你可以根据需要调整为较短的时间。\nprometheus --storage.tsdb.retention.time=7d # 保留 7 天的数据 调整存储块大小： Prometheus 使用存储块来管理数据的写入。调整 --storage.tsdb.min-block-duration 和 --storage.tsdb.max-block-duration 参数可以影响内存的使用。增加存储块的大小，可能会增加 Prometheus 的内存占用。\n3. 调整指标采集频率 # 减少采集频率： 默认的 scrape_interval 是 15 秒，你可以根据需求调整为更长的时间，减少采集频率，从而减少 Prometheus 的内存使用。例如，将采集间隔从 15 秒调整为 30 秒：\nscrape_configs: - job_name: \u0026#39;my_job\u0026#39; scrape_interval: 30s # 将采集间隔调整为 30 秒 这可以减少 Prometheus 存储的数据量，避免因采集过于频繁而导致 OOM。\n4. 分布式 Prometheus 部署（Sharding） # 使用多个 Prometheus 实例： 你可以将数据划分为多个 Prometheus 实例（Sharding）。每个 Prometheus 实例负责采集特定的指标或目标，然后使用 Prometheus Federation 将它们集中到一个主 Prometheus 实例进行查询。\nscrape_configs: - job_name: \u0026#39;prometheus\u0026#39; static_configs: - targets: [\u0026#39;prometheus-instance1:9090\u0026#39;] - job_name: \u0026#39;prometheus\u0026#39; static_configs: - targets: [\u0026#39;prometheus-instance2:9090\u0026#39;] 通过这种方式，每个 Prometheus 实例只需要处理部分指标，从而减少内存负担。\n5. 优化查询 # 某些复杂的 Prometheus 查询可能会消耗大量内存。优化查询，避免过于复杂的计算，可以减少内存消耗。\n避免高卡特利性查询：对于查询时需要使用大量标签值的指标，要避免在查询中进行复杂的聚合或过滤。 使用采样数据：对一些大数据集，可以使用采样来减少查询时的数据量。比如，使用 rate() 函数时，可以考虑使用较短的时间窗口，减少计算的数据量。 6. 使用外部存储（远程存储） # 如果本地存储的容量无法满足需求，可以考虑将数据导出到远程存储系统（如 Cortex、Thanos 或 Mimir），以便进行更大规模的存储和查询。\n配置远程存储： Prometheus 支持将数据写入外部存储系统，如 Cortex 或 Thanos，这样可以将较老的数据存储在远程系统中，减轻 Prometheus 节点的存储压力。\n配置示例：\nremote_write: - url: \u0026#34;http://\u0026lt;remote-storage-url\u0026gt;/write\u0026#34; remote_read: - url: \u0026#34;http://\u0026lt;remote-storage-url\u0026gt;/read\u0026#34; 总结 # 为了解决 Prometheus 的 OOM 问题，可以通过以下几种方式优化：\n拆分大型高卡特利性指标，减少标签的数量或采集频率。 限制存储使用，通过调整数据保留时间来减少内存占用。 减少采集频率，优化 Prometheus 配置。 使用分布式 Prometheus 部署，将数据分散到多个实例中。 优化查询，避免高内存消耗的查询。 使用外部存储，将数据存储到远程系统中。 通过这些方法，可以有效减少 Prometheus 的内存消耗，避免 OOM 问题的发生。\nprometheus的告警延迟问题 # Prometheus 的告警延迟问题通常发生在告警规则触发后，Prometheus 需要一定时间才能检测到指标变化、计算规则并发出通知。造成告警延迟的原因可能有多个，以下是一些常见的原因以及相应的解决方法：\n1. 告警规则的评估间隔 # Prometheus 的告警规则是基于 evaluation_interval（评估间隔）来周期性评估的。如果评估间隔设置过长，告警会延迟触发。\n解决方法： 默认的评估间隔通常是 1 分钟，但如果你的环境需要更快的告警响应，可以考虑缩短评估间隔。\n在 prometheus.yml 配置文件中，修改 evaluation_interval：\nglobal: scrape_interval: 15s # 采集间隔 evaluation_interval: 30s # 告警规则评估间隔 注意：缩短评估间隔可能会增加 Prometheus 的负载，因此需要平衡性能和告警响应的需求。\n2. 告警规则的评估条件过于宽松 # 告警规则中可能存在宽松的条件，导致 Prometheus 需要更长时间才能触发告警。例如，使用 avg 或 rate 聚合函数时，如果数据波动较小，可能会导致告警判断延迟。\n解决方法： 确保告警规则的条件尽量精确，避免使用过于模糊的聚合函数或过长的时间窗口。例如，避免使用过大的 rate 时间窗口，或者缩短 for 参数的值。\ngroups: - name: example-alerts rules: - alert: HighCPUUsage expr: rate(cpu_usage_seconds_total[5m]) \u0026gt; 0.9 for: 1m # 告警至少持续1分钟 3. Prometheus 数据采集频率 # Prometheus 从各个监控目标上采集数据时，采集的频率也会影响告警的响应时间。如果采集间隔过长，数据更新的频率就会降低，从而导致告警响应的延迟。\n解决方法： 通过 scrape_interval 配置调整数据采集的频率。通常来说，采集间隔越短，数据更新越频繁，告警响应速度越快。\nglobal: scrape_interval: 15s # 数据采集间隔 但需要注意，频繁的数据采集可能会增加 Prometheus 的负载，影响性能，因此应根据实际情况调整。\n4. Alertmanager 配置问题 # Alertmanager 是 Prometheus 的告警处理组件。如果 Alertmanager 配置有问题，可能会导致告警的通知延迟。常见的延迟原因包括网络问题、Alertmanager 配置不当或并发负载过高。\n解决方法：\n检查 Alertmanager 的配置，确保通知规则配置正确。 确保 Alertmanager 的实例健康，并且能快速处理告警请求。 增加 Alertmanager 的副本数，以提高处理性能。 监控 Alertmanager 本身的性能，确保它不会成为瓶颈。 配置示例：\nroute: receiver: \u0026#39;email-notifications\u0026#39; group_by: [\u0026#39;alertname\u0026#39;] group_wait: 30s # 等待30秒合并告警 group_interval: 1m # 合并告警的间隔时间 repeat_interval: 1h # 重复告警的时间间隔 5. Prometheus 存储问题 # 如果 Prometheus 的存储系统存在性能瓶颈（例如磁盘 IO 过载、存储容量不足等），这可能会导致告警规则评估时延迟数据访问，从而增加告警延迟。\n解决方法\n：\n确保 Prometheus 存储系统有足够的资源（磁盘、内存、网络等）。 调整 Prometheus 的存储配置，如使用更快的磁盘、调整 TSDB（时间序列数据库）的存储参数等。 6. 复杂的 Prometheus 查询 # 告警规则中可能使用了复杂的 Prometheus 查询，例如高卡特利性指标的聚合、多个 join 操作等。这些复杂的查询可能会导致 Prometheus 的查询延迟，进而影响告警的响应时间。\n解决方法： 优化 Prometheus 查询，避免使用过于复杂的查询条件，或者使用 rate 或 avg 之类的函数时，尽量减小时间范围。\n示例：\nexpr: avg(rate(cpu_usage_seconds_total{instance=\u0026#34;localhost\u0026#34;}[1m])) \u0026gt; 0.8 确保查询尽可能高效，避免不必要的计算。\n7. 网络延迟或 DNS 解析问题 # 告警从 Prometheus 发送到 Alertmanager，或者 Alertmanager 发送通知到外部系统（如电子邮件、Slack）时，可能会遇到网络延迟或 DNS 解析问题，导致告警的通知延迟。\n解决方法\n：\n检查 Prometheus 和 Alertmanager 之间的网络连接是否正常。 确保外部通知系统（如 Slack、PagerDuty 等）的网络连接正常。 8. Alertmanager 缓存问题 # Alertmanager 会缓存已发送的告警信息，如果相同的告警多次触发，可能会出现通知延迟或者重复告警的情况。\n解决方法： 调整 group_interval 和 repeat_interval 参数，避免过多的重复告警。\nroute: group_interval: 10s # 在同一告警被发送多次时，最小的时间间隔 repeat_interval: 30m # 重复告警的最小时间间隔 总结 # Prometheus 告警延迟的问题通常与以下几个方面相关：\n评估间隔和采集频率：调整 evaluation_interval 和 scrape_interval，以便更频繁地评估和采集数据。 告警规则优化：确保告警规则的条件明确且高效，避免使用高卡特利性指标或不必要的复杂查询。 Alertmanager 配置：检查和优化 Alertmanager 配置，确保通知的及时发送。 存储和网络问题：确保 Prometheus 存储系统和网络正常，避免性能瓶颈。 通过优化这些方面，你可以有效减少 Prometheus 的告警延迟，提升监控和告警的响应速度。\n告警抑制和收敛 # 告警抑制和收敛（Alert Suppression and Alert Aggregation）是 Prometheus 和 Alertmanager 中的两个关键概念，用于避免告警通知的泛滥或重复，优化告警系统的处理效率，并减少用户的告警干扰。这两个概念能帮助团队集中注意力在最重要的告警上，防止被大量无关的告警信息淹没。\n1. 告警抑制（Alert Suppression） # 告警抑制是指在某些特定条件下，抑制某些告警的触发或通知。例如，当一个告警触发时，可能有其他相关告警也会触发，但这些告警可能只是副作用，且没有带来新的信息。在这种情况下，告警抑制可以避免发送重复或冗余的告警通知。\n使用场景： # 同一事件的重复告警：如果某个系统出现故障，可能会触发多个相似的告警。通过告警抑制，可以避免对相同问题产生多次告警通知。 避免链式告警：当一个故障引发一系列告警时，可以抑制这些次级告警，直到主告警得到解决为止。 配置告警抑制： # 在 Alertmanager 中，可以使用 group_by 和 group_interval 来进行告警的抑制和分组。\ngroup_by：指定哪些标签应当被用来分组告警。如果多条告警有相同的标签，它们会被视为同一事件的一部分，并在一定时间内进行合并。 group_interval：告警组间的最小通知间隔。如果同一个组中的告警再次触发，它们会在 group_interval 时间内被合并，而不是发送新的告警。 repeat_interval：控制告警在经过指定时间后是否会再次发送。例如，当一个告警已经发送过通知后，直到 repeat_interval 时间过去，再次符合条件时才会再次发送告警。 配置示例：\nroute: group_by: [\u0026#39;alertname\u0026#39;, \u0026#39;instance\u0026#39;] # 按 alertname 和 instance 标签分组 group_interval: 5m # 同一组告警之间的最小间隔时间 repeat_interval: 30m # 重复告警的最小时间间隔 receiver: \u0026#39;email-notifications\u0026#39; receivers: - name: \u0026#39;email-notifications\u0026#39; email_configs: - to: \u0026#39;alerts@example.com\u0026#39; send_resolved: true 2. 告警收敛（Alert Aggregation） # 告警收敛是指将多个相似的告警合并为一个告警，避免发送过多的单独告警通知。这种方式能减少噪声，并使告警更易于管理和响应。\n使用场景： # 多个节点告警合并：如果你的系统有多个相似的节点，出现故障时，多个节点可能会触发相同类型的告警。告警收敛可以将这些节点的告警合并成一个告警，避免多个相同告警的通知。 系统组件告警合并：例如，多个服务的故障可能会同时触发告警。将这些告警合并成一个，可以集中处理。 配置告警收敛： # 可以使用 alert 规则中的 for 参数和 group_by 来合并告警。\nfor：指定在告警触发之后，必须持续一定时间才能被通知。这有助于减少短时间内的波动导致的告警。 group_by：将告警分组到相同组内，便于对相关告警进行合并。 配置示例：\ngroups: - name: \u0026#39;example-alerts\u0026#39; rules: - alert: HighCPUUsage expr: rate(cpu_usage_seconds_total[1m]) \u0026gt; 0.8 for: 1m # 持续1分钟触发告警 labels: severity: \u0026#39;critical\u0026#39; annotations: description: \u0026#39;CPU usage is over 80%\u0026#39; 在 Alertmanager 中，也可以通过 group_by 将相似的告警合并。例如，将多个 CPU 使用高的告警合并成一个告警，并且通过 group_interval 控制多次触发时的合并间隔。\n3. 告警收敛与告警抑制的区别 # 告警抑制：在一个告警触发后，抑制其他相关告警的触发，直到主告警得到解决。例如，在节点宕机的情况下，抑制其上面所有服务的告警，避免告警泛滥。 告警收敛：将多个相似的告警合并为一个告警，以避免多个重复的告警通知。例如，当多台机器的 CPU 使用率超过阈值时，将其合并为一个告警。 4. 告警收敛和抑制的配置实践 # 在实际使用中，告警收敛和抑制的配置通常结合使用，以优化告警系统：\n收敛：将相似的告警合并成一个，避免过多的重复通知。 抑制：当一个主告警触发时，抑制与之相关的次要告警，避免通知过多。 以下是一个结合收敛和抑制的配置示例：\nroute: group_by: [\u0026#39;alertname\u0026#39;, \u0026#39;instance\u0026#39;] group_interval: 10m # 合并告警的间隔 repeat_interval: 1h # 重复告警的时间间隔 receiver: \u0026#39;email-notifications\u0026#39; routes: - match: severity: \u0026#39;critical\u0026#39; group_by: [\u0026#39;alertname\u0026#39;] group_interval: 5m repeat_interval: 30m receivers: - name: \u0026#39;email-notifications\u0026#39; email_configs: - to: \u0026#39;alerts@example.com\u0026#39; send_resolved: true 总结 # 告警抑制（Alert Suppression）：通过抑制不必要的告警通知，减少冗余和噪声。常用于避免链式告警或相似告警的重复触发。 告警收敛（Alert Aggregation）：将多个相似的告警合并成一个告警，集中处理，减少过多告警的通知。 合理使用告警抑制和收敛可以显著提高告警系统的效率和响应速度，避免告警泛滥，确保用户能够专注于最关键的告警。\n# 业务pod的告警规则 # 业务 pod 的告警规则通常是根据 Kubernetes 集群中部署的业务应用的性能、健康状况和资源使用情况来进行监控和告警的。这些告警规则可以帮助运维人员及时发现和处理业务 pod 的问题，保证业务的高可用性和稳定性。\n以下是一些常见的业务 pod 的告警规则示例，涵盖了不同层面的监控需求：\n1. Pod 资源使用情况 # 监控 pod 的 CPU、内存、磁盘等资源使用情况是告警规则中常见的一部分。这些指标有助于发现 pod 的资源瓶颈、过度使用或资源限制问题。\n示例 1：CPU 使用率过高 # 监控 pod 的 CPU 使用率，确保它不会超过设定的阈值。\ngroups: - name: \u0026#39;pod-resources\u0026#39; rules: - alert: PodHighCPUUsage expr: sum(rate(container_cpu_usage_seconds_total{pod=~\u0026#34;your-business-pod-.*\u0026#34;, container!=\u0026#34;\u0026#34;, image!=\u0026#34;\u0026#34;}[5m])) by (pod) / sum(kube_pod_container_resource_limits{pod=~\u0026#34;your-business-pod-.*\u0026#34;, resource=\u0026#34;cpu\u0026#34;}) by (pod) \u0026gt; 0.8 for: 1m labels: severity: critical annotations: summary: \u0026#34;Pod \u0026#39;{{ $labels.pod }}\u0026#39; CPU usage is over 80% for the last 1 minute.\u0026#34; description: \u0026#34;The CPU usage of pod \u0026#39;{{ $labels.pod }}\u0026#39; has exceeded the threshold of 80%.\u0026#34; 此规则会触发告警，当 your-business-pod 中的 pod 的 CPU 使用率超过 80% 时。\n示例 2：内存使用率过高 # 监控 pod 的内存使用情况，防止内存泄漏或内存资源不足。\n- alert: PodHighMemoryUsage expr: sum(container_memory_usage_bytes{pod=~\u0026#34;your-business-pod-.*\u0026#34;, container!=\u0026#34;\u0026#34;, image!=\u0026#34;\u0026#34;}) by (pod) / sum(kube_pod_container_resource_limits{pod=~\u0026#34;your-business-pod-.*\u0026#34;, resource=\u0026#34;memory\u0026#34;}) by (pod) \u0026gt; 0.8 for: 1m labels: severity: critical annotations: summary: \u0026#34;Pod \u0026#39;{{ $labels.pod }}\u0026#39; memory usage is over 80%.\u0026#34; description: \u0026#34;The memory usage of pod \u0026#39;{{ $labels.pod }}\u0026#39; is over 80% of the limit.\u0026#34; 此规则会触发告警，当 your-business-pod 中的 pod 的内存使用率超过 80% 时。\n2. Pod 健康检查失败 # 健康检查失败的告警规则可以帮助你发现 pod 是否存在启动、连接或其他健康问题。\n示例 3：Pod livenessProbe 失败 # 监控业务 pod 的 livenessProbe，如果 pod 健康检查失败，说明 pod 可能需要重启。\n- alert: PodLivenessProbeFailed expr: kube_pod_container_status_restarts_total{pod=~\u0026#34;your-business-pod-.*\u0026#34;, container!=\u0026#34;\u0026#34;, container!=\u0026#34;POD\u0026#34;} \u0026gt; 3 for: 5m labels: severity: critical annotations: summary: \u0026#34;Pod \u0026#39;{{ $labels.pod }}\u0026#39; has failed livenessProbe more than 3 times in the last 5 minutes.\u0026#34; description: \u0026#34;Pod \u0026#39;{{ $labels.pod }}\u0026#39; is not healthy. It has failed the liveness probe more than 3 times in the last 5 minutes.\u0026#34; 此规则会触发告警，当 pod 的 livenessProbe 失败超过 3 次时。\n示例 4：Pod readinessProbe 失败 # 监控 pod 的 readinessProbe，当 readinessProbe 失败时，表示 pod 无法接收流量。\n- alert: PodReadinessProbeFailed expr: kube_pod_container_status_ready{pod=~\u0026#34;your-business-pod-.*\u0026#34;, container!=\u0026#34;\u0026#34;, container!=\u0026#34;POD\u0026#34;} == 0 for: 5m labels: severity: critical annotations: summary: \u0026#34;Pod \u0026#39;{{ $labels.pod }}\u0026#39; is not ready.\u0026#34; description: \u0026#34;Pod \u0026#39;{{ $labels.pod }}\u0026#39; has failed the readiness probe or is not ready to receive traffic.\u0026#34; 此规则会触发告警，当 pod 的 readinessProbe 失败时，表示 pod 无法接收流量。\n3. Pod 容器的重启 # 容器频繁重启可能是由于应用崩溃、资源不足或者配置错误等问题。\n示例 5：容器重启次数过多 # 监控容器的重启次数，若重启次数过多，则说明可能存在潜在问题。\n- alert: PodContainerRestartingTooMuch expr: kube_pod_container_status_restarts_total{pod=~\u0026#34;your-business-pod-.*\u0026#34;, container!=\u0026#34;\u0026#34;, container!=\u0026#34;POD\u0026#34;} \u0026gt; 5 for: 10m labels: severity: warning annotations: summary: \u0026#34;Pod \u0026#39;{{ $labels.pod }}\u0026#39; container has restarted more than 5 times in the last 10 minutes.\u0026#34; description: \u0026#34;Pod \u0026#39;{{ $labels.pod }}\u0026#39; container has been restarted more than 5 times, which may indicate a problem.\u0026#34; 此规则会触发告警，当 pod 中的容器在 10 分钟内重启超过 5 次时。\n4. Pod 调度和资源限制 # 如果 pod 无法调度到节点上，可能是因为资源不足或者节点配置问题。\n示例 6：Pod 无法调度 # 监控 pod 的调度情况，若 pod 无法调度，会触发告警。\n- alert: PodPending expr: kube_pod_status_phase{pod=~\u0026#34;your-business-pod-.*\u0026#34;, phase=\u0026#34;Pending\u0026#34;} \u0026gt; 0 for: 5m labels: severity: critical annotations: summary: \u0026#34;Pod \u0026#39;{{ $labels.pod }}\u0026#39; is in pending state.\u0026#34; description: \u0026#34;Pod \u0026#39;{{ $labels.pod }}\u0026#39; has been in pending state for more than 5 minutes and cannot be scheduled.\u0026#34; 此规则会触发告警，当 pod 处于 Pending 状态超过 5 分钟时。\n5. 网络延迟或连接问题 # 业务应用可能会遇到网络延迟或连接失败的问题，导致服务不可用。\n示例 7：Pod 网络延迟高 # 监控 pod 的网络延迟，若延迟过高，可能会影响业务性能。\n- alert: PodHighNetworkLatency expr: histogram_quantile(0.99, sum(rate(container_network_transmit_errors_total{pod=~\u0026#34;your-business-pod-.*\u0026#34;}[5m])) by (pod)) \u0026gt; 100 for: 5m labels: severity: warning annotations: summary: \u0026#34;Pod \u0026#39;{{ $labels.pod }}\u0026#39; has high network latency.\u0026#34; description: \u0026#34;Pod \u0026#39;{{ $labels.pod }}\u0026#39; has network transmit errors higher than 100 in the last 5 minutes.\u0026#34; 此规则会触发告警，当 pod 的网络错误数超过 100 时，可能会影响应用的网络性能。\n总结 # 业务 pod 的告警规则通常涉及以下几个方面：\n资源使用情况：包括 CPU、内存、磁盘等的使用情况，确保 pod 不会超出资源限制。 健康检查：包括 livenessProbe 和 readinessProbe，监控 pod 是否健康并能正常处理流量。 容器重启：监控容器是否频繁重启，频繁重启可能表明系统存在问题。 调度和资源限制：确保 pod 能够成功调度并分配到合适的节点上。 网络问题：监控 pod 网络连接和延迟，确保应用能够正常通信。 这些告警规则可以根据业务需求和实际情况进一步调整，确保及时发现和解决业务 pod 的问题。\n要为业务 Pod 设置告警规则，可以根据以下常见指标配置 Prometheus 告警规则：\n1. CPU 使用率告警 # 当 Pod 的 CPU 使用率超过某个阈值时触发告警：\ngroups: - name: business-pod-alerts rules: - alert: HighCPUUsage expr: sum(rate(container_cpu_usage_seconds_total{pod=~\u0026#34;business-pod.*\u0026#34;}[5m])) by (pod) \u0026gt; 0.8 for: 1m labels: severity: critical annotations: description: \u0026#34;CPU usage exceeds 80% for Pod {{ $labels.pod }}\u0026#34; 2. 内存使用率告警 # 当 Pod 的内存使用率超过某个阈值时触发告警：\n- alert: HighMemoryUsage expr: sum(container_memory_usage_bytes{pod=~\u0026#34;business-pod.*\u0026#34;}) by (pod) / sum(container_spec_memory_limit_bytes{pod=~\u0026#34;business-pod.*\u0026#34;}) by (pod) \u0026gt; 0.8 for: 1m labels: severity: critical annotations: description: \u0026#34;Memory usage exceeds 80% for Pod {{ $labels.pod }}\u0026#34; 3. Pod 失效告警 # 当 Pod 的状态为失败时触发告警：\n- alert: PodFailed expr: kube_pod_status_phase{pod=~\u0026#34;business-pod.*\u0026#34;, phase=\u0026#34;Failed\u0026#34;} == 1 for: 5m labels: severity: critical annotations: description: \u0026#34;Pod {{ $labels.pod }} has failed\u0026#34; 4. Pod 重启告警 # 当 Pod 的重启次数超过阈值时触发告警：\n- alert: PodRestarts expr: increase(kube_pod_container_status_restarts_total{pod=~\u0026#34;business-pod.*\u0026#34;}[1h]) \u0026gt; 3 for: 10m labels: severity: warning annotations: description: \u0026#34;Pod {{ $labels.pod }} has restarted more than 3 times in the last hour\u0026#34; 这些告警规则可以帮助你及时发现业务 Pod 的性能问题，确保系统稳定运行。\n# # 请求成功率的监控怎么做的 # 请求成功率的监控是保证服务可靠性和稳定性的重要指标。通常，计算请求成功率是通过计算请求成功的次数与总请求次数的比率。以下是如何通过 Prometheus 来监控和告警请求成功率的方法：\n1. 监控请求成功率的基础指标 # 为了计算请求成功率，你首先需要在应用程序或服务中暴露 HTTP 请求的相关指标，通常是通过 Prometheus 的 http_requests_total 之类的计数器指标。\n假设你有以下两个指标：\nhttp_requests_total{status=\u0026quot;200\u0026quot;, job=\u0026quot;myapp\u0026quot;}：成功的请求数（HTTP 200 状态码）。 http_requests_total{job=\u0026quot;myapp\u0026quot;}：所有请求的总数（所有状态码）。 2. 计算请求成功率 # 你可以使用 Prometheus 查询语言（PromQL）来计算请求的成功率。假设成功请求数和总请求数的标签中包含 job=\u0026quot;myapp\u0026quot;，可以按如下方式编写查询：\nrate(http_requests_total{status=\u0026#34;200\u0026#34;, job=\u0026#34;myapp\u0026#34;}[5m]) / rate(http_requests_total{job=\u0026#34;myapp\u0026#34;}[5m]) * 100 这个查询的含义是：\nrate(http_requests_total{status=\u0026quot;200\u0026quot;, job=\u0026quot;myapp\u0026quot;}[5m])：计算过去 5 分钟内成功请求（状态码 200）的速率。 rate(http_requests_total{job=\u0026quot;myapp\u0026quot;}[5m])：计算过去 5 分钟内所有请求（包括状态码为 200 以外的请求）的速率。 用成功请求数除以总请求数并乘以 100，得到请求成功率。 3. 设置告警规则 # 如果请求成功率低于某个阈值，你可以设置告警规则。以下是一个示例，表示当请求成功率低于 95% 时触发告警：\ngroups: - name: business-pod-alerts rules: - alert: LowRequestSuccessRate expr: (rate(http_requests_total{status=\u0026#34;200\u0026#34;, job=\u0026#34;myapp\u0026#34;}[5m]) / rate(http_requests_total{job=\u0026#34;myapp\u0026#34;}[5m])) * 100 \u0026lt; 95 for: 2m labels: severity: critical annotations: description: \u0026#34;Request success rate is below 95% for the service\u0026#34; 4. 监控失败的请求 # 如果你还想专门监控失败请求的数量，可以针对其他 HTTP 状态码进行监控。例如，监控 500（服务器错误）状态码的请求数：\n- alert: HighErrorRate expr: rate(http_requests_total{status=~\u0026#34;5..\u0026#34;, job=\u0026#34;myapp\u0026#34;}[5m]) / rate(http_requests_total{job=\u0026#34;myapp\u0026#34;}[5m]) * 100 \u0026gt; 5 for: 5m labels: severity: critical annotations: description: \u0026#34;More than 5% of requests have failed (status code 5xx)\u0026#34; 5. 高级监控与告警 # 计算特定时间窗口内的成功率：你可以根据实际需求调整查询的时间窗口（如 rate(...[10m])），或者根据不同的服务路径（path 标签）设置不同的阈值。\n使用分层告警\n：通过告警规则中的\nseverity 标签，可以设置不同级别的告警，例如：\ncritical：请求成功率低于某个临界值（如 90%）。 warning：请求成功率稍微下降，可能需要进一步检查（如 95%）。 总结 # 通过 Prometheus 和 PromQL，你可以非常灵活地监控请求成功率并设定告警。关键步骤包括：\n暴露 HTTP 请求的相关指标（如成功的请求和总请求数）。 编写计算请求成功率的 PromQL 查询。 根据请求成功率设置告警规则，及时发现问题并采取措施。 接口探活呢 # 接口探活（Health Check）是为了确保服务和应用程序处于健康状态并能够正常处理请求的一种监控方式。通过 Prometheus 监控接口探活，通常可以通过暴露健康检查端点并结合相应的指标进行监控。\n以下是如何使用 Prometheus 实现接口探活监控的步骤：\n1. 暴露健康检查端点 # 通常，应用程序会提供一个 HTTP 健康检查端点（例如 /health 或 /actuator/health），返回应用的健康状态。你需要确保应用程序暴露了一个健康检查 API，该 API 会返回成功（HTTP 200）或失败（HTTP 5xx）状态码。\n例如，在 Spring Boot 应用中，你可以暴露如下端点：\n/actuator/health（Spring Boot 健康检查） 在 Node.js 应用中，你可以设置一个简单的健康检查端点：\napp.get(\u0026#39;/health\u0026#39;, (req, res) =\u0026gt; { res.status(200).send(\u0026#39;OK\u0026#39;); }); 2. 暴露健康检查指标 # 在 Prometheus 中，通常使用一个自定义的指标来暴露健康检查的状态。你可以暴露一个专门的健康检查指标，表示服务是否正常运行。例如：\nup{job=\u0026#34;myapp\u0026#34;, status=\u0026#34;healthy\u0026#34;} 1 up{job=\u0026#34;myapp\u0026#34;, status=\u0026#34;unhealthy\u0026#34;} 0 在 Prometheus 的配置中，up 是一个用于标识目标是否可用的内建指标。你可以自定义监控指标，比如暴露 http_health_check_status 作为健康检查的状态指标。\n3. 通过 Prometheus 查询健康检查状态 # 在 Prometheus 中，你可以查询某个服务的健康检查状态，例如：\nhttp_health_check_status{job=\u0026#34;myapp\u0026#34;} == 1 这个查询会返回 1 表示健康，0 表示不健康。\n4. 设置告警规则 # 当健康检查状态不正常时，你可以通过 Prometheus 设置告警规则，及时发现问题。例如，当服务出现故障时（返回状态码 5xx 或健康检查失败），触发告警。\n示例：健康检查失败时告警 # 假设你有一个健康检查的指标 http_health_check_status，你可以编写如下告警规则：\ngroups: - name: health-check-alerts rules: - alert: ServiceUnhealthy expr: http_health_check_status{job=\u0026#34;myapp\u0026#34;} == 0 for: 2m labels: severity: critical annotations: description: \u0026#34;The health check of the service {{ $labels.job }} failed\u0026#34; 示例：HTTP 5xx 错误率过高时告警 # 你也可以监控 HTTP 错误率，来作为接口探活的一个补充。比如，如果 HTTP 5xx 错误率超过 5% 时触发告警：\n- alert: HighErrorRate expr: rate(http_requests_total{status=~\u0026#34;5..\u0026#34;, job=\u0026#34;myapp\u0026#34;}[5m]) / rate(http_requests_total{job=\u0026#34;myapp\u0026#34;}[5m]) * 100 \u0026gt; 5 for: 5m labels: severity: critical annotations: description: \u0026#34;More than 5% of HTTP requests have failed (status code 5xx)\u0026#34; 5. 使用 Kubernetes 探针（Liveness/Readiness） # 如果你的应用部署在 Kubernetes 上，可以利用 Kubernetes 提供的 Liveness Probe 和 Readiness Probe 来监控应用的健康状态。\nLiveness Probe：判断应用是否仍然活着。如果此探针失败，Kubernetes 会重启容器。 Readiness Probe：判断应用是否准备好接受流量。如果此探针失败，Kubernetes 会停止向该 Pod 分配流量。 在 deployment.yaml 中，你可以定义如下探针：\nlivenessProbe: httpGet: path: /health port: 8080 initialDelaySeconds: 30 periodSeconds: 10 readinessProbe: httpGet: path: /health port: 8080 initialDelaySeconds: 5 periodSeconds: 10 6. 总结 # 通过以下几步，你可以实现接口探活监控：\n暴露健康检查端点：确保应用提供一个健康检查 API，返回服务的健康状态。 暴露健康检查指标：将健康检查状态暴露为 Prometheus 可监控的指标。 Prometheus 查询：使用 PromQL 查询健康检查的状态。 设置告警规则：当健康检查失败或错误率过高时，触发告警。 Kubernetes 探针：如果在 Kubernetes 中部署，使用 Liveness 和 Readiness 探针监控 Pod 的健康状态。 结合以上策略，你可以实现强大的接口探活监控，确保你的服务始终健康可用。\n一个微服务的业务，流量通过外部进入集群内部的过程 # 在微服务架构中，流量从外部进入集群并通过内部服务进行处理的过程，通常包括以下几个步骤：外部流量的接入、入口控制、路由和负载均衡、以及服务间的调用。下面是详细的流程：\n1. 外部流量的接入 # 外部流量一般通过以下方式进入 Kubernetes 集群内部：\nIngress Controller：Kubernetes 提供了 Ingress 资源来管理 HTTP 和 HTTPS 流量。Ingress Controller 是处理这些流量的组件，通常结合负载均衡器（如 Nginx、Traefik 或 HAProxy）来将流量引导到集群内部的服务。 Load Balancer：通过外部负载均衡器（如云提供商的 ELB）将流量路由到 Kubernetes 集群中。负载均衡器将流量分发到不同的入口服务（通常是 Ingress Controller 或 Kubernetes 服务）。 例如，在 Kubernetes 集群上配置一个 Ingress 资源：\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: my-app-ingress spec: rules: - host: myapp.example.com http: paths: - path: / pathType: Prefix backend: service: name: my-service port: number: 80 2. Ingress Controller 处理流量 # Ingress Controller 监听到外部流量后，根据 Ingress 资源的定义，将流量转发到对应的内部服务。Ingress Controller 负责对流量进行路由、SSL/TLS 终止（即加密流量的解密）、负载均衡等操作。\n负载均衡：Ingress Controller 会根据 Ingress 配置进行负载均衡，将流量均匀分发到对应的后端服务。 TLS 终止：Ingress Controller 可以配置为支持 HTTPS，进行 SSL/TLS 终止，确保流量的加密和解密。 3. 服务发现与流量路由 # 流量通过 Ingress Controller 后，通常会进入某个内部服务。这些服务在 Kubernetes 中是通过 Service 资源进行暴露的，Service 提供了服务发现的功能，并通过 DNS 或 IP 进行内部路由。\nService：Kubernetes 中的 Service 是一组 Pod 的抽象，通过 ClusterIP、NodePort 或 LoadBalancer 方式暴露服务。 例如，一个简单的 Service 配置：\napiVersion: v1 kind: Service metadata: name: my-service spec: selector: app: my-app ports: - protocol: TCP port: 80 targetPort: 8080 4. 内部流量的路由与负载均衡 # 内部流量的路由和负载均衡可以由以下几个组件完成：\nKubernetes Service：Kubernetes 的 ClusterIP 服务为流量提供自动的负载均衡。它会将流量分发到匹配的 Pod，确保流量均匀分配。 Service Mesh（例如 Istio、Linkerd）：在更复杂的微服务架构中，使用 Service Mesh 提供更加细粒度的流量控制、可观察性、限流、熔断等功能。Service Mesh 会帮助你在微服务之间进行安全、高效的流量管理。 5. 服务间通信 # 在微服务架构中，内部服务（如 A 服务调用 B 服务）需要通过 Service 名称进行通信。这通常通过 DNS 实现，Kubernetes 会为每个服务提供 DNS 解析。\n例如，A 服务需要调用 B 服务，A 服务只需要使用 B 服务的 DNS 名称：\ncurl http://my-b-service.default.svc.cluster.local 如果你使用 Service Mesh，还可以定义更加复杂的路由规则，例如基于路径或请求头的路由。\n6. 安全控制与认证 # 在微服务中，尤其是在多服务之间的通信中，通常会需要对流量进行安全控制，如身份验证和授权。可以通过以下几种方式来实现：\nAPI Gateway：可以使用 API Gateway 作为外部流量的入口，负责处理认证、授权等功能。 Service Mesh：Service Mesh（如 Istio）可以提供 mTLS（相互 TLS 认证）来保护服务之间的通信，确保只有经过认证的服务才能访问其他服务。 7. 监控与日志收集 # 在整个流量进入和处理的过程中，监控和日志收集非常重要。Prometheus、Grafana、Jaeger、Elasticsearch 等工具常用于微服务架构中进行全链路监控、日志分析和性能追踪。\nPrometheus：用于收集和监控服务的各种指标（如请求数、错误率、响应时间等）。 Jaeger：用于分布式追踪，查看请求在多个微服务之间流转的情况。 Fluentd/ELK Stack：用于收集和分析日志，帮助排查问题和监控服务的健康状态。 8. 总结 # 流量从外部进入 Kubernetes 集群内部，通常的流程如下：\n外部流量进入：外部流量通过负载均衡器、Ingress Controller 等进入 Kubernetes 集群。 路由与负载均衡：Ingress Controller 依据 Ingress 规则将流量路由到相应的服务。 服务发现与请求转发：Kubernetes Service 通过 DNS 进行服务发现，并将请求转发到匹配的 Pod。 服务间通信：微服务通过 Kubernetes 的 Service 进行通信，Service Mesh 可以提供更加灵活的流量管理。 安全控制与监控：通过 API Gateway、Service Mesh 和监控工具，确保服务间的安全、可靠性和可观测性。 通过这种流量进入和内部处理的流程，微服务架构能够高效、可靠地处理外部流量，并提供服务之间的安全、可扩展通信。\nELK/fluentd 怎么收集pod的日志 # 在 Kubernetes 集群中，收集 Pod 的日志可以通过 ELK Stack（Elasticsearch, Logstash, Kibana） 或 Fluentd 来实现。以下是如何使用 ELK 和 Fluentd 来收集和处理 Kubernetes Pod 日志的方案。\n1. 使用 Fluentd 收集 Pod 日志 # Fluentd 是一个开源的日志收集和处理工具，能够将日志从多个来源（如 Pod）收集，并将它们推送到不同的后端系统（如 Elasticsearch、Kafka、HDFS 等）。\n步骤： # 安装 Fluentd DaemonSet： Fluentd 在 Kubernetes 中通常作为 DaemonSet 部署，这样它可以在每个节点上运行，并收集该节点上所有 Pod 的日志。你可以通过以下步骤部署 Fluentd。\n创建一个 Fluentd 配置文件 fluentd-configmap.yaml：\napiVersion: v1 kind: ConfigMap metadata: name: fluentd-config namespace: kube-system data: fluentd.conf: | \u0026lt;source\u0026gt; @type tail format json path /var/log/containers/*.log pos_file /var/log/fluentd.pos tag kube.* \u0026lt;/source\u0026gt; \u0026lt;match kube.**\u0026gt; @type elasticsearch host elasticsearch.example.com port 9200 logstash_format true flush_interval 5s \u0026lt;/match\u0026gt; 这个配置文件做了以下几件事情：\n它从 /var/log/containers/*.log 读取容器日志。 使用 tail 插件实时获取日志数据。 将日志数据转发到 Elasticsearch（elasticsearch.example.com:9200）进行存储。 创建 Fluentd DaemonSet： 创建一个 Fluentd DaemonSet 使其在所有节点上运行，并将日志数据发送到 Elasticsearch。\napiVersion: apps/v1 kind: DaemonSet metadata: name: fluentd namespace: kube-system spec: selector: matchLabels: app: fluentd template: metadata: labels: app: fluentd spec: containers: - name: fluentd image: fluent/fluentd:v1.12-debian-1.0 volumeMounts: - name: fluentd-config mountPath: /fluentd/etc volumes: - name: fluentd-config configMap: name: fluentd-config 访问和查询日志： 通过 Elasticsearch 存储的日志，您可以使用 Kibana 来访问和查询日志。Kibana 提供了丰富的日志查询、过滤和可视化功能。\nFluentd 处理日志的流程： # 收集：Fluentd 会从 /var/log/containers/*.log 中收集容器日志。 解析和过滤：Fluentd 可能会解析日志并根据标签（例如 kube.*）进行筛选和过滤。 传输：日志会被发送到 Elasticsearch 或其他后端存储。 展示：通过 Kibana 等工具进行日志的展示和查询。 2. 使用 ELK Stack（Elasticsearch, Logstash, Kibana）收集 Pod 日志 # ELK Stack（Elasticsearch、Logstash、Kibana）是日志管理的经典工具，广泛应用于收集、分析和可视化日志数据。\n步骤： # 安装 Elasticsearch： 你需要先部署 Elasticsearch，用于存储日志数据。可以通过 Helm 或 YAML 文件进行安装。\n示例安装 Elasticsearch：\nhelm install elasticsearch elastic/elasticsearch 安装 Logstash（可选）： Logstash 可以用来接收来自 Fluentd 的日志数据，并进行处理（例如解析和过滤）。如果你已经通过 Fluentd 直接将日志发送到 Elasticsearch，可以跳过这个步骤。\n安装 Logstash：\nhelm install logstash elastic/logstash 配置 Fluentd 转发日志到 Elasticsearch： 你可以将 Fluentd 配置为将日志发送到 Elasticsearch，如前述步骤中所述。Fluentd 作为一个轻量级的日志收集器，在集群中处理日志转发，能够与 Logstash 和 Elasticsearch 集成。\n使用 Kibana 查询日志： 使用 Kibana 连接到 Elasticsearch，创建索引并进行日志查询。Kibana 提供了强大的日志分析和可视化功能，可以根据需求创建仪表盘，监控应用程序和 Kubernetes 集群的运行状况。\n3. 通过 Fluentd 与 ELK 集成 # Fluentd 可以与 ELK Stack 集成，作为日志收集的前端工具，负责将日志从 Kubernetes 集群中的 Pod 转发到 Elasticsearch。\n配置示例： # Fluentd 配置： 例如，将 Kubernetes 的日志通过 Fluentd 转发到 Elasticsearch。\n\u0026lt;match kubernetes.**\u0026gt; @type elasticsearch host elasticsearch.example.com port 9200 logstash_format true flush_interval 5s index_name fluentd-logs \u0026lt;/match\u0026gt; Elasticsearch 存储： Fluentd 会将日志传送到 Elasticsearch，ES 会将日志数据存储在指定的索引中（如 fluentd-logs）。\nKibana 可视化： 在 Kibana 中，你可以使用索引模式来查看和分析日志。你可以创建图表、仪表盘和报警规则来分析和监控日志。\n4. 总结 # Fluentd 收集日志：Fluentd 是一个强大的日志收集工具，可以通过配置 DaemonSet 部署在 Kubernetes 集群中，收集并转发容器日志到 Elasticsearch 或其他存储系统。 ELK Stack（Elasticsearch + Logstash + Kibana）：作为日志存储和可视化工具，ELK Stack 可以与 Fluentd 配合使用，将日志收集、存储和分析的功能结合在一起。 Kubernetes 集群日志收集：通过 Fluentd 收集 Kubernetes Pod 的日志，然后推送到 Elasticsearch 存储，最后使用 Kibana 来查询和展示这些日志。 这种组合提供了一种完整的日志收集和分析解决方案，能够帮助你高效地监控和排查 Kubernetes 集群中的应用日志。\npod的标准输出日志在这哪 # Pod 的标准输出日志通常存储在 Kubernetes 节点上的容器日志文件中，而不是与网络相关的环境变量中。\n在 Kubernetes 中，每个容器的日志都被保存为文件，这些日志文件通常存储在以下路径：\nPod 标准输出日志路径 # 日志文件通常存储在节点的 /var/log/containers/ 目录中。具体路径格式如下：\n/var/log/containers/\u0026lt;pod-name\u0026gt;_\u0026lt;namespace\u0026gt;_\u0026lt;container-name\u0026gt;-\u0026lt;container-id\u0026gt;.log 例如，如果你有一个名为 my-pod，在 default 命名空间下，容器名称是 my-container 的 Pod，那么容器的日志文件可能会是：\n/var/log/containers/my-pod_default_my-container-\u0026lt;container-id\u0026gt;.log 如何查看 Pod 的标准输出日志 # 你可以使用 kubectl logs 命令直接查看 Pod 的日志：\nkubectl logs \u0026lt;pod-name\u0026gt; -n \u0026lt;namespace\u0026gt; 如果 Pod 中有多个容器，可以使用 -c 参数指定容器：\nkubectl logs \u0026lt;pod-name\u0026gt; -n \u0026lt;namespace\u0026gt; -c \u0026lt;container-name\u0026gt; 日志收集与存储 # 在生产环境中，日志通常不会仅保留在本地磁盘上，而是通过日志收集工具（如 Fluentd、ELK Stack 等）进行集中化管理和存储。这些工具能够将日志从各个节点收集并发送到后端存储（如 Elasticsearch）中，方便日后查询和分析。\n总结 # Pod 的标准输出日志 通常存储在 /var/log/containers/ 目录中。 使用 kubectl logs 命令可以直接查看 Pod 的日志。 如果你使用日志收集系统（如 Fluentd、ELK Stack），日志会被转发到集中式存储中进行管理和查询。 如果你有特定的日志收集配置，日志可能通过其他方式（如 Fluentd）发送到集中式日志平台。\n# "},{"id":194,"href":"/docs/2025-2-20-%E9%9D%A2%E8%AF%950220/","title":"面试0220","section":"Docs","content":" delete pvc时 设置什么字段可以保证pv不被删除 # 在 Kubernetes 中，当你删除 PersistentVolumeClaim（PVC） 时，默认情况下与之关联的 PersistentVolume（PV） 会根据 PV 的 回收策略 被删除。为了确保在删除 PVC 时 PV 不被删除，你需要修改 PV 的 回收策略（reclaimPolicy） 字段。\n步骤 # 设置 PV 的回收策略：\n回收策略\n：Kubernetes 支持几种 PV 的回收策略，常见的有：\nRetain：保持 PV 不删除，手动处理。 Recycle：回收 PV（已废弃，不推荐使用）。 Delete：删除 PV（这是默认设置，当 PVC 删除时 PV 会被删除）。 如果你希望 PVC 删除时 PV 不被删除，需要将 PV 的回收策略设置为 Retain。这样，当 PVC 删除时，PV 仍然会保留，必须手动清理或者重新配置。\n如何修改回收策略：\n编辑你的 PersistentVolume（PV） 对象，修改 reclaimPolicy 为 Retain。 下面是一个例子：\napiVersion: v1 kind: PersistentVolume metadata: name: my-pv spec: capacity: storage: 10Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain # 设置回收策略为 Retain storageClassName: manual hostPath: path: /mnt/data 在这个例子中，persistentVolumeReclaimPolicy: Retain 确保当与该 PV 关联的 PVC 被删除时，PV 并不会被删除。\n删除 PVC 后 PV 的状态：\n如果 PV 的回收策略设置为 Retain，删除 PVC 后，PV 的状态将会变成 Released，意味着 PV 被释放，但仍然存在于集群中。 你可以手动清理 PV 或将其重新绑定到一个新的 PVC。 总结： # 为了确保在删除 PVC 时 PV 不被删除，您只需在 PV 的定义中设置 persistentVolumeReclaimPolicy: Retain。这样，PVC 删除时，PV 会保持在集群中，并可以由管理员手动处理。\n重新绑定到一个新的 PVC 数据还在吗 # 是的，当你将一个 PersistentVolume（PV） 重新绑定到一个新的 PersistentVolumeClaim（PVC） 时，数据仍然存在，前提是 PV 是在其原有数据存储上创建的。这里是相关的详细情况：\n如何运作： # 数据存储位置：PersistentVolume 是与某种存储系统（如 NFS、iSCSI、云存储服务等）相关联的，这意味着 PV 存储的数据通常与 PVC 绑定时存储在持久存储上。无论你是否删除了 PVC，数据仍然保留在该存储系统中，直到手动删除或格式化。 PV 的生命周期： Retain 策略：当 PV 的回收策略为 Retain 时，即使与 PV 绑定的 PVC 被删除，PV 不会被删除。这个 PV 会被标记为 Released，表示它不再与任何 PVC 关联，仍然保留其数据。 这个时候，如果你有另一个 PVC（新的 PVC），并且它的 存储需求（如大小和访问模式）与 PV 匹配，你可以将这个新的 PVC 绑定到这个 PV 上。 重新绑定 PVC： 如果新的 PVC 请求的存储大小与 PV 的大小一致，且访问模式（如 ReadWriteOnce, ReadWriteMany 等）也匹配，那么 Kubernetes 会将 PV 重新绑定到这个 PVC。 重新绑定时，数据是 不丢失的，新的 PVC 可以继续访问 PV 中原先存储的数据。 例子： # 假设你有一个 PV 已经与一个 PVC 绑定并存储了一些数据，之后你删除了原来的 PVC，但你希望重新将这个 PV 绑定到一个新的 PVC。\n删除原 PVC 后，PV 会变成 Released 状态。 创建一个新的 PVC，确保它的存储要求（大小、访问模式等）与 PV 匹配。 Kubernetes 会将新的 PVC 绑定到这个 PV，并且原有的数据仍然会保留在 PV 上，新的 PVC 可以访问这些数据。 注意事项： # 存储类型：这取决于你使用的存储类型（如 NFS、EBS、Ceph、GlusterFS 等）。大多数情况下，数据会在存储中保留，直到手动清理或删除存储。 访问模式：如果新的 PVC 请求的访问模式不匹配（比如原 PVC 使用 ReadWriteOnce 而新 PVC 请求 ReadWriteMany），则 PV 无法与新 PVC 绑定。在这种情况下，你需要修改 PVC 或 PV 来确保访问模式匹配。 总结： # 只要你没有删除或覆盖数据存储，重新绑定到新的 PVC 时，数据是不会丢失的。这个过程与 PV 和 PVC 的回收策略、存储类型和访问模式相关。在大多数情况下，数据会保留在存储系统中，直到手动清理。\n临时存储的字段是啥 # 在 Kubernetes 中，临时存储（通常用于临时存储数据，如 Pod 在运行时需要使用的缓存或中间数据）一般是通过 emptyDir、ephemeral 或 hostPath 等存储方式来实现的。临时存储不会持久化，Pod 被删除时，相关数据也会被清除。\n在定义 PV（PersistentVolume）或 PVC（PersistentVolumeClaim）时，没有专门针对临时存储的字段，但可以通过一些特殊的配置来创建临时存储卷。以下是一些关键字段和概念：\n1. emptyDir # emptyDir 是一种最常用的临时存储方式。它的特点是，当 Pod 被删除时，emptyDir 中的数据会丢失，适用于临时存储需求，比如缓存或临时文件。\n字段：emptyDir 在 PodSpec 中定义，并且与容器挂载的路径相关联。 apiVersion: v1 kind: Pod metadata: name: example-pod spec: containers: - name: example-container image: nginx volumeMounts: - mountPath: /tmp name: tmp-storage volumes: - name: tmp-storage emptyDir: {} 说明\n：\nemptyDir: {} 指定了一个空的临时目录，挂载在容器内的 /tmp 路径。 这个临时目录会随着 Pod 生命周期的结束而删除。emptyDir 是最简单的临时存储方式，适合容器运行期间存储短期数据。 2. ephemeral # 在 Kubernetes 1.23+ 中，ephemeral 存储类引入了更加明确的临时存储概念，允许你使用类似 emptyDir 的临时存储方式，并且提供了更多的控制，如指定存储类型（例如 ephemeral-storage，即本地存储）。\n字段：ephemeral 在 volume 的定义中。 apiVersion: v1 kind: Pod metadata: name: ephemeral-storage-example spec: containers: - name: nginx image: nginx resources: requests: ephemeral-storage: \u0026#34;1Gi\u0026#34; # 请求1Gi的临时存储 volumeMounts: - mountPath: /mnt/ephemeral name: ephemeral-storage volumes: - name: ephemeral-storage ephemeral: {} # 指定临时存储 说明\n：\nephemeral 存储是 Kubernetes 1.23 引入的新的存储类，用于更加明确的临时存储需求。它可与资源请求和限制一起使用。 该存储在 Pod 生命周期内有效，并会在 Pod 删除时被清除。 3. hostPath # hostPath 是另一种临时存储方式，允许 Pod 挂载宿主机的文件系统目录，作为临时存储使用。注意，hostPath 不会随着 Pod 删除而清除，它可能会对宿主机文件系统产生影响，因此要小心使用。\n字段：hostPath 存储类型需要在 PodSpec 中定义。 apiVersion: v1 kind: Pod metadata: name: hostpath-example spec: containers: - name: nginx image: nginx volumeMounts: - mountPath: /mnt/host name: host-storage volumes: - name: host-storage hostPath: path: /tmp/data # 宿主机上的路径 type: Directory 说明\n：\nhostPath 将宿主机的 /tmp/data 目录挂载到容器内的 /mnt/host 路径上。 这种存储方式并不完全是临时的，因为数据会存储在宿主机的文件系统中，直到手动删除或清理。 4. 临时存储请求（ephemeral-storage） # 除了 emptyDir 和 ephemeral，还可以通过容器的 resources 字段为容器请求和限制临时存储资源（即 ephemeral-storage），确保容器有足够的本地存储空间。\n字段：resources.requests 和 resources.limits 中定义 ephemeral-storage。 apiVersion: v1 kind: Pod metadata: name: pod-with-ephemeral-storage spec: containers: - name: my-container image: nginx resources: requests: ephemeral-storage: \u0026#34;1Gi\u0026#34; # 请求1Gi的临时存储 limits: ephemeral-storage: \u0026#34;2Gi\u0026#34; # 限制最大使用2Gi的临时存储 说明\n：\nephemeral-storage 用于请求容器的本地存储（例如节点上的磁盘）。容器在运行期间可能会产生一些临时数据，这些数据需要一定量的存储空间。 这种方式与 emptyDir 类似，但它是通过请求和限制本地存储来实现的。 总结 # 在 Kubernetes 中，临时存储的字段通常可以通过以下几种方式定义：\nemptyDir：提供一个临时存储卷，Pod 删除时清空。 ephemeral：明确的临时存储类（Kubernetes 1.23+）。 hostPath：可以用宿主机的存储目录来作为临时存储，但不会随 Pod 删除而清除。 ephemeral-storage：在资源请求和限制中定义，确保容器有足够的本地存储。 这些临时存储方式是为了满足 Pod 在生命周期内对临时数据存储的需求，数据会随着 Pod 删除而丢失（在没有持久化配置的情况下）。\nmetric-server 干啥的 # Metric Server 是 Kubernetes 集群中的一个核心组件，它负责收集和提供集群内各个节点和容器的性能数据（如 CPU、内存、磁盘等）。这些性能指标（metrics）可以用来进行资源管理、自动扩缩容、监控等。\nMetric Server 的功能 # 资源监控：Metric Server 会定期收集每个节点和容器的资源使用情况（如 CPU 使用率、内存占用、网络流量等），并将这些数据提供给 Kubernetes 组件。 HPA（Horizontal Pod Autoscaler）支持：Metric Server 是 水平 Pod 自动伸缩（HPA） 功能的基础。HPA 根据 Pod 的 CPU 或内存使用情况，自动调整 Pod 的副本数，保证应用的负载均衡。Metric Server 为 HPA 提供实时的性能指标数据，使其可以根据当前资源消耗来决定是否扩展或缩减 Pod 的数量。 监控和报警：虽然 Metric Server 本身不做长期的监控或报警，但它提供了集群中各个节点和容器的实时指标数据，这些数据可以被其他监控系统（如 Prometheus）进一步存储、分析并触发报警。 kubectl top 命令：Metric Server 提供了 kubectl top 命令支持，允许用户查看当前节点或 Pod 的资源使用情况。例如： kubectl top nodes：查看每个节点的资源使用情况（CPU、内存等）。 kubectl top pods：查看每个 Pod 的资源使用情况。 Metric Server 的工作原理 # 数据来源：Metric Server 从 Kubelet 收集每个节点和容器的资源使用数据。Kubelet 会定期（通常每分钟）采集节点和容器的资源使用情况（如 CPU、内存、磁盘等）并将这些数据提供给 Metric Server。 存储与查询：Metric Server 并不会永久存储这些数据，而是作为一个中间层，提供实时的资源指标查询。当 Kubernetes 需要获取这些指标时（如通过 HPA），Metric Server 会即时响应这些请求。 API 提供：Metric Server 通过 Kubernetes 的 Metrics API 提供资源使用数据。这个 API 允许其他 Kubernetes 组件（如 HPA）查询各个节点和容器的性能指标。 安装与配置 # 安装： Metric Server 作为一个独立的服务运行，通常可以通过 Helm 安装或者直接应用 Kubernetes 中提供的资源清单进行安装。例如，使用官方 YAML 文件进行安装：\nkubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml 配置： Metric Server 默认情况下会向每个节点的 Kubelet 请求指标数据。如果集群中的 Kubelet 没有正确配置（如禁用了 --read-only-port 或没有启用 --kubelet-insecure-tls），Metric Server 可能无法获取数据。确保 Kubelet 的配置允许 Metric Server 获取指标数据。\n认证：Metric Server 需要 Kubernetes API 访问权限来提供数据，通常是通过 RBAC（角色访问控制）进行管理。\n常见问题 # Metric Server 无法工作： 检查 Metric Server 是否已正确安装，并且在集群中正常运行。 确保 Kubelet 配置允许 Metric Server 获取节点和容器的资源数据。 如果遇到 metric API 不可用的情况，可能是由于 Metric Server 与 API Server 之间的通信问题，检查相关日志和网络配置。 与 Prometheus 的区别： Metric Server 提供的主要是集群内的 实时性能数据（如 CPU、内存使用情况），这些数据通常用于 自动扩缩容（HPA）。 Prometheus 则是一个完整的监控系统，它可以收集历史性能数据、监控各类应用、支持复杂的查询和报警规则。Prometheus 是长期监控的解决方案，而 Metric Server 更侧重于提供资源管理的基本支持。 总结 # Metric Server 是 Kubernetes 集群中用于收集和提供节点、Pod 和容器资源使用情况的组件，它为 HPA 和其他集群组件提供实时的性能指标数据。通过它，Kubernetes 可以根据负载自动扩缩容，保证应用的稳定运行。同时，Metric Server 也为开发者提供了 kubectl top 等命令，帮助快速查看集群资源的使用情况。\nmetric-server 更深入的话时通过什么提供指标的 cadvisior？\n是的，cAdvisor（Container Advisor）是 Metric Server 获取容器级别指标的关键组件之一。它是 Kubernetes 集群中的 Kubelet 的一部分，负责监控和收集容器的资源使用数据（如 CPU、内存、网络等）。Metric Server 从 cAdvisor 获取这些容器和节点的性能指标，并提供给其他 Kubernetes 组件（如 Horizontal Pod Autoscaler）和用户查询。\n更深入的工作原理 # cAdvisor 在 Kubelet 中的作用：\ncAdvisor\n是由 Google 开发的一个用于容器资源监控的工具，它会在 Kubelet 进程内运行并监控每个容器的资源使用情况。它收集的指标包括：\nCPU 使用情况 内存使用情况 磁盘 I/O 网络流量 文件系统使用等 cAdvisor 将这些数据暴露在 Kubelet 的 /stats/summary 或 /metrics/cadvisor 等端点上，以便其他服务（如 Metric Server）访问。\nMetric Server 获取数据：\nMetric Server 不直接从容器获取数据，而是从 Kubelet 获取这些指标。具体而言，Metric Server 会访问 Kubelet 提供的 Metrics API，而这些数据实际上是通过 cAdvisor 生成的。 Kubelet 提供的 API 会以 JSON 格式返回节点和容器的资源使用情况。Metric Server 定期从这些端点拉取数据并将其格式化成标准化的指标，供 Kubernetes 的其他组件（如 HPA）或外部系统（如 Prometheus）使用。 cAdvisor 与 Metric Server 的关系：\ncAdvisor 负责在容器级别采集资源数据，并提供数据给 Kubelet。 Metric Server 通过 Kubelet 获取这些数据，并将其以 Kubernetes 的 Metrics API 的形式提供给其他组件。 它们之间的关系可以看作是 cAdvisor 负责采集数据，而 Metric Server 负责提供这些数据。 数据流动示例 # Kubelet 启动并开始运行容器。 cAdvisor 在容器内监控并采集资源使用数据（如 CPU、内存等）。 Kubelet 定期访问 cAdvisor 生成的资源使用数据，并将其公开在内部的 /stats/summary 或 /metrics/cadvisor 等端点上。 Metric Server 定期向 Kubelet 的 Metrics API 请求数据（如容器的 CPU 使用率、内存占用等）。 Metric Server 获取数据后，将其存储在 Kubernetes 集群中，供 Horizontal Pod Autoscaler (HPA)、kubectl top 等工具使用。 cAdvisor 和 Metric Server 的协作 # cAdvisor 主要用于提供容器级别的资源数据。它依赖 Kubelet 来暴露容器资源使用的统计信息（包括 CPU、内存、磁盘和网络流量等），并通过 Kubelet 的 API 提供这些数据。 Metric Server 则是一个轻量级的聚合器，它从 Kubelet 获取这些数据，进行格式化后提供给其他 Kubernetes 组件和工具。Metric Server 本身不存储这些数据，而是将其暴露给需要它的系统（例如 HPA）。 为什么 cAdvisor 和 Metric Server 配合使用？ # cAdvisor 提供了容器级别的指标采集，确保 Kubernetes 可以获取最精细的资源消耗数据。 Metric Server 聚合和规范化这些数据，使得它可以在 Kubernetes 环境中广泛使用，特别是在自动扩缩容、监控、调度等方面。 其他相关工具 # Prometheus：与 cAdvisor 类似，Prometheus 也可以收集和存储指标数据，通常用于长期存储和高级监控。Prometheus 可以通过 Kubernetes 服务发现 机制来发现和拉取集群中的所有容器的指标数据，包括来自 cAdvisor 的指标。 Kube-State-Metrics：另一个补充工具，它提供集群的状态指标数据（如 Pod、Deployment 的状态、ReplicaSet 的数量等），这些数据通常不通过 cAdvisor 获取，而是直接从 Kubernetes API 服务器获取。 总结 # cAdvisor 是 Kubelet 的一部分，负责容器级别的资源监控。 Metric Server 从 Kubelet 获取这些数据，并将它们以标准格式提供给其他 Kubernetes 组件（如 HPA）和工具（如 kubectl top）。 Metric Server 和 cAdvisor 协作，共同提供容器和节点的实时资源监控数据。 deploy里面定义secrect的字段是啥 # 在 Kubernetes 中，Secret 用于存储敏感数据，如密码、OAuth 令牌、SSH 密钥等。在 Kubernetes 部署（Deployment）中，你可以通过挂载 Secret 来保护这些敏感数据，并将其提供给容器。你可以通过两种方式在 Deployment 中定义 Secret：\n通过环境变量 (envFrom 或 env 字段) 通过卷挂载 (volumes 和 volumeMounts 字段) 下面是这两种方式的详细说明和示例：\n1. 通过环境变量传递 Secret # 你可以通过环境变量将 Secret 的数据传递到容器中，这是一种比较常见的方式。\nenvFrom：允许将整个 Secret 中的键值对作为环境变量传递给容器。 env：可以选择性地传递 Secret 中的特定键值对作为环境变量。 示例：通过 envFrom 将 Secret 传递给容器 # apiVersion: apps/v1 kind: Deployment metadata: name: my-deployment spec: replicas: 1 selector: matchLabels: app: my-app template: metadata: labels: app: my-app spec: containers: - name: my-container image: nginx envFrom: - secretRef: name: my-secret # 传入名为 \u0026#34;my-secret\u0026#34; 的 Secret 说明：在这个例子中，envFrom 字段引用了一个名为 my-secret 的 Secret。这个 Secret 中的所有键值对会作为环境变量传递到容器中。 示例：通过 env 选择性传递 Secret 中的某些键 # apiVersion: apps/v1 kind: Deployment metadata: name: my-deployment spec: replicas: 1 selector: matchLabels: app: my-app template: metadata: labels: app: my-app spec: containers: - name: my-container image: nginx env: - name: DB_PASSWORD # 环境变量的名称 valueFrom: secretKeyRef: name: my-secret # 引用的 Secret 名称 key: password # Secret 中的具体键（值） 说明：在这个例子中，env 字段通过 secretKeyRef 引用了 my-secret Secret 中的 password 键，并将其传递给容器作为环境变量 DB_PASSWORD。 2. 通过卷挂载 Secret # 你还可以将 Secret 以文件的形式挂载到容器内。这通常用于将配置文件或者密钥文件直接提供给应用。\n示例：通过卷挂载 Secret # apiVersion: apps/v1 kind: Deployment metadata: name: my-deployment spec: replicas: 1 selector: matchLabels: app: my-app template: metadata: labels: app: my-app spec: containers: - name: my-container image: nginx volumeMounts: - name: secret-volume mountPath: /etc/secret-volume # 容器内挂载路径 volumes: - name: secret-volume secret: secretName: my-secret # 引用 Secret 名称 说明：在这个例子中，volumes 字段将 my-secret Secret 挂载到容器的 /etc/secret-volume 路径下。Secret 中的每个键都会成为文件名，并且文件的内容就是对应的 Secret 键的值。容器内应用可以通过读取这些文件来获取敏感信息。 总结 # 在 Deployment 中引用 Secret 时，通常用到以下两个字段：\nenvFrom：将整个 Secret 的内容作为环境变量传递给容器。 env：将 Secret 中的特定键作为环境变量传递给容器。 volumes 和 volumeMounts：将 Secret 挂载为文件，容器通过文件读取敏感数据。 这些方法可以帮助你安全地将敏感数据注入到容器中，并避免将敏感信息硬编码到应用中。\npod被调度的方式有哪些 # 在 Kubernetes 中，Pod 的调度是指将 Pod 安排到集群中的节点（Node）上执行的过程。调度策略基于多种因素，包括资源需求、拓扑、亲和性/反亲和性、节点负载、硬件特性等。调度过程通常由 Kube-scheduler 完成，它是 Kubernetes 的一个控制器，负责决定哪些节点能运行给定的 Pod。\nKubernetes 提供了多种方式来控制和影响 Pod 的调度。以下是常见的调度方式和策略：\n1. 默认调度 # 默认情况下，Kubernetes 的调度器会根据以下条件将 Pod 调度到一个合适的节点：\n资源请求：Pod 的资源请求（如 CPU 和内存）与节点的可用资源匹配。 节点选择：Kubernetes 调度器会选择一个具有足够资源的节点（例如，满足 CPU 和内存要求）来运行 Pod。 2. Node Affinity（节点亲和性） # Node Affinity 是一种影响调度的约束条件，用于指定 Pod 应该调度到哪些节点。它基于节点的标签来决定 Pod 是否可以调度到某个节点。节点亲和性包括两种类型：\nrequiredDuringSchedulingIgnoredDuringExecution：这是强制的约束，调度器必须满足该条件才能将 Pod 调度到节点上。 preferredDuringSchedulingIgnoredDuringExecution：这是一个偏好条件，调度器会尽量满足，但不是强制的。 示例：Node Affinity 配置 # apiVersion: v1 kind: Pod metadata: name: my-pod spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: \u0026#34;disktype\u0026#34; operator: In values: - ssd 说明：这个例子指定了 Node Affinity，要求 Pod 只能调度到具有 disktype=ssd 标签的节点上。 3. Pod Affinity 和 Pod Anti-Affinity # Pod Affinity：允许 Pod 调度到靠近其他特定 Pod 的节点上，适用于有依赖关系的服务。 Pod Anti-Affinity：要求 Pod 不与其他特定的 Pod 调度到同一个节点上，适用于避免 Pod 之间的资源争用或提高高可用性。 示例：Pod Affinity 配置 # apiVersion: v1 kind: Pod metadata: name: my-pod spec: affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: labelSelector: matchLabels: app: frontend topologyKey: \u0026#34;kubernetes.io/hostname\u0026#34; 说明：这个例子表示 Pod 只会调度到与 app=frontend 标签的 Pod 运行在同一节点上的位置。 示例：Pod Anti-Affinity 配置 # apiVersion: v1 kind: Pod metadata: name: my-pod spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: labelSelector: matchLabels: app: frontend topologyKey: \u0026#34;kubernetes.io/hostname\u0026#34; 说明：这个例子要求 Pod 不与 app=frontend 标签的 Pod 调度到同一节点上。 4. Taints 和 Tolerations（污点和容忍） # Taints 是节点的属性，表示该节点上有某种限制，只有具备相应容忍（Tolerations）的 Pod 才能被调度到该节点上。 Tolerations 使得 Pod 可以被调度到带有某些污点的节点上。 示例：Taint 和 Toleration 配置 # Taint 节点 kubectl taint nodes node1 key=value:NoSchedule Toleration 配置 apiVersion: v1 kind: Pod metadata: name: my-pod spec: tolerations: - key: \u0026#34;key\u0026#34; operator: \u0026#34;Equal\u0026#34; value: \u0026#34;value\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; 说明：这个例子在节点 node1 上设置了一个污点 key=value:NoSchedule，而 Pod 使用了 tolerations 来容忍这个污点，允许 Pod 被调度到该节点。 5. 资源请求和限制（Resource Requests and Limits） # 每个 Pod 和容器都可以定义资源请求（requests）和限制（limits），以便调度器能合理分配资源。调度器会根据资源的需求来选择合适的节点。\n请求（Requests）：是调度时考虑的资源需求，表示 Pod 需要的最小资源。 限制（Limits）：是容器能够使用的最大资源，超过这个限制会被限制。 示例：资源请求和限制配置 # apiVersion: v1 kind: Pod metadata: name: my-pod spec: containers: - name: my-container image: nginx resources: requests: memory: \u0026#34;64Mi\u0026#34; cpu: \u0026#34;250m\u0026#34; limits: memory: \u0026#34;128Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; 说明：这个例子为容器配置了内存和 CPU 的资源请求和限制。调度器会根据这些信息来选择合适的节点。 6. Priority Class（优先级类） # 优先级类用于定义 Pod 的调度优先级。Pod 可以指定一个优先级类，调度器会根据优先级类来确定 Pod 调度的顺序。高优先级的 Pod 会优先被调度，低优先级的 Pod 可能会被抢占或者延迟调度。\n示例：Priority Class 配置 # apiVersion: scheduling.k8s.io/v1 kind: PriorityClass metadata: name: high-priority value: 1000000 globalDefault: false description: \u0026#34;This priority class is for critical workloads.\u0026#34; 说明：这个例子定义了一个高优先级类 high-priority，并为高优先级的 Pod 配置了该类。 7. Topology Spread Constraints（拓扑分布约束） # 拓扑分布约束用于控制 Pod 在集群中的分布，确保 Pod 分布在不同的区域或节点上，以提高可用性并避免资源集中。\n示例：拓扑分布约束配置 # apiVersion: apps/v1 kind: Deployment metadata: name: my-deployment spec: replicas: 6 selector: matchLabels: app: my-app template: metadata: labels: app: my-app spec: topologySpreadConstraints: - maxSkew: 1 topologyKey: \u0026#34;kubernetes.io/hostname\u0026#34; whenUnsatisfiable: DoNotSchedule labelSelector: matchLabels: app: my-app 说明：这个例子确保 Pod 在不同的节点上均匀分布，避免所有 Pod 都调度到同一个节点。 总结 # Pod 的调度方式有很多种，主要通过以下策略来控制：\nNode Affinity：控制 Pod 只能调度到具有特定标签的节点。 Pod Affinity 和 Pod Anti-Affinity：控制 Pod 在集群中与其他 Pod 的亲和性或反亲和性。 Taints 和 Tolerations：控制哪些 Pod 能够调度到具有特定污点的节点。 资源请求和限制：根据 Pod 的资源需求选择合适的节点。 Priority Class：控制 Pod 的调度优先级。 Topology Spread Constraints：控制 Pod 在不同节点、区域的分布。 这些调度策略提供了灵活的方式来控制 Pod 的调度，以满足特定的业务需求和集群配置。\ndaemonset 忽略污点 使其可以调度到所有节点 # 要让一个 DaemonSet 忽略污点并且能够调度到所有节点，可以通过设置 tolerations 来实现。tolerations 允许调度器忽略节点上的污点，允许 Pod 调度到那些被标记为“有污点”的节点。\n1. 理解污点和容忍 # 污点 (Taints): 污点是用于标记节点的属性，它防止特定的 Pod 被调度到这些节点上。污点有三种类型：NoSchedule、PreferNoSchedule 和 NoExecute。 容忍 (Tolerations): 容忍允许 Pod 忽略节点的污点，允许它们调度到这些节点上。 2. 修改 DaemonSet 的配置 # 在 DaemonSet 的 Pod 模板中添加 tolerations，使其容忍所有节点上的污点。示例配置如下：\napiVersion: apps/v1 kind: DaemonSet metadata: name: example-daemonset spec: selector: matchLabels: app: example template: metadata: labels: app: example spec: tolerations: - key: \u0026#34;node.kubernetes.io/not-ready\u0026#34; operator: \u0026#34;Exists\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; - key: \u0026#34;node.kubernetes.io/unreachable\u0026#34; operator: \u0026#34;Exists\u0026#34; effect: \u0026#34;NoExecute\u0026#34; - key: \u0026#34;some-taint-key\u0026#34; operator: \u0026#34;Exists\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; containers: - name: example-container image: nginx 3. 解释上述配置 # tolerations 字段允许 DaemonSet 忽略特定污点。\nkey: 污点的键。 operator: \u0026quot;Exists\u0026quot;: 表示只要节点上有相应的污点，Pod 就能容忍并调度到该节点。 effect: 污点的作用，NoSchedule 表示该节点不允许调度到 Pod，NoExecute 表示已经调度到节点的 Pod 会被驱逐。 在上述配置中，DaemonSet 会容忍节点上的 not-ready、unreachable 和自定义的污点 some-taint-key，从而允许 Pod 调度到这些被污点标记的节点上。\n4. 使其可以调度到所有节点 # 如果希望 DaemonSet 忽略所有污点并且可以调度到所有节点，可以添加一个容忍，表示所有污点都能容忍：\ntolerations: - operator: \u0026#34;Exists\u0026#34; 这样配置后，DaemonSet 的 Pod 将容忍所有污点，从而能够调度到所有节点。\n5. 应用配置 # 将修改后的 DaemonSet 配置保存为 YAML 文件，然后应用到 Kubernetes 集群：\nkubectl apply -f daemonset.yaml 总结 # 通过在 DaemonSet 的 spec.template.spec.tolerations 中添加容忍规则，可以让 DaemonSet 忽略节点的污点，进而使 Pod 能够调度到所有节点，包括那些被污点标记的节点。\npod的状态有哪些？ # Kubernetes 中的 Pod 是最基本的部署单元，它的生命周期包括多个不同的 状态，每个状态反映了 Pod 在运行时的不同阶段或情况。Pod 的状态主要包括以下几种，每种状态都有不同的意义和用途：\n1. Pending（等待中） # 描述：Pod 被调度到集群中的节点，但容器尚未启动。这通常发生在以下几种情况： Pod 被调度到一个节点，但节点上没有足够的资源来启动容器。 Pod 在等待调度（例如，调度器正在选择最合适的节点）。 容器镜像正在下载或拉取中。 典型原因： 节点资源不足，Pod 无法调度。 容器镜像还未完全下载到节点。 必须满足某些调度约束（如 NodeAffinity）但未找到合适的节点。 2. Running（运行中） # 描述：Pod 已经成功调度到节点上，并且至少一个容器正在运行。Pod 进入这个状态意味着： 容器已成功启动并在运行中。 所有容器都在运行或者至少有一个容器在运行。 典型原因： 所有容器已经启动并正在处理任务。 部分容器可能在运行中，其他容器可能处于 waiting 状态，但 Pod 仍然视为 Running。 3. Succeeded（成功） # 描述：Pod 中的所有容器都成功完成并退出。通常表示该 Pod 中的容器是 一次性（如批处理任务），执行完任务后容器退出。Pod 被认为是 成功的，即所有容器都没有错误退出。 典型原因： 所有容器执行完毕并正常退出，Pod 的生命周期结束。 4. Failed（失败） # 描述：Pod 中的容器之一或多个容器异常退出（例如，容器退出状态码非零）。这意味着至少有一个容器运行失败，Pod 被认为处于失败状态。 典型原因： 容器启动失败或退出代码表示失败（如非零退出状态）。 应用崩溃或出错导致容器停止运行。 5. Unknown（未知） # 描述：Pod 的状态未知，通常是因为节点失联，无法从节点获取关于该 Pod 的状态。可能是由于网络问题或者节点故障，Kubernetes 控制平面无法向集群报告 Pod 的状态。 典型原因： 节点宕机或无法连接。 网络或控制面问题，导致状态无法更新。 Pod 状态的详细说明 # 在 Kubernetes 中，Pod 的状态不仅仅是一个简单的标签，它还包含了详细的容器信息，诸如容器的当前状态、是否正在重启、资源使用情况等。你可以通过 kubectl describe pod \u0026lt;pod-name\u0026gt; 命令查看 Pod 的详细状态信息，了解更多关于 Pod 和其容器的状态。\nPod 状态的细化字段（容器级） # 每个 Pod 中的容器会有以下几个状态字段：\nWaiting（等待中）： 容器还没有启动，可能正在等待某些资源（例如镜像拉取或容器启动失败）。 reason 字段会告诉为什么容器处于 Waiting 状态，比如 ImagePullBackOff（镜像拉取失败）或者 CrashLoopBackOff（容器崩溃并尝试重新启动）。 Running（运行中）： 容器正在运行并且没有问题。容器进入 Running 状态后，它可以处理请求并进行任务。 Terminated（终止）： 容器已经停止或被终止。终止可以是正常退出（exit code 0）或者异常退出（exit code != 0）。 如果容器正常退出并且 Pod 被设计为一次性任务，它会进入 Succeeded 状态。反之，如果有错误发生，容器会进入 Failed 状态。 容器状态示例： # status: containerStatuses: - name: my-container state: running: startedAt: \u0026#34;2025-02-19T15:03:06Z\u0026#34; lastState: terminated: exitCode: 0 reason: Completed startedAt: \u0026#34;2025-02-19T14:59:06Z\u0026#34; finishedAt: \u0026#34;2025-02-19T15:03:06Z\u0026#34; ready: true restartCount: 0 state：表示容器的当前状态，可以是 waiting, running 或 terminated。 lastState：表示容器最近一次的状态，只有在容器终止时才会显示。 restartCount：表示容器的重启次数。 总结 # Kubernetes 中的 Pod 状态分为几种主要类型，每种状态反映了 Pod 在集群中执行的不同阶段：\nPending：Pod 正在等待调度或资源。 Running：Pod 正在运行，至少有一个容器在运行。 Succeeded：Pod 中的所有容器已经成功完成并退出。 Failed：Pod 中至少一个容器运行失败。 Unknown：Pod 的状态未知，通常是节点失联时出现。 理解这些状态能够帮助你排查和诊断 Pod 运行时的各种问题，从而采取合适的措施解决问题。\nevicted呢？ # 在 Kubernetes 中，Evicted 状态表示 Pod 被从节点中驱逐（Evicted）。这通常发生在节点资源不足或节点上有其他约束条件时。Pod 被标记为 Evicted，并且不能再继续运行。\nEvicted 状态的原因 # Pod 被驱逐的主要原因通常是资源压力，例如节点上的 内存、磁盘 或 CPU 不足。驱逐通常是由于以下原因：\n节点资源不足（例如，内存、磁盘或 CPU）： 当节点的内存、磁盘或 CPU 等资源不足时，Kubernetes 会选择驱逐某些 Pod，以释放资源供更高优先级或关键 Pod 使用。 例如，如果节点的内存使用率过高，Kubernetes 可能会驱逐一些 Pod，尤其是当启用了 优先级（Priority）和 抢占（Preemption）机制时，低优先级的 Pod 会被优先驱逐。 磁盘压力（DiskPressure）： 当节点的磁盘空间不足时，Kubernetes 会将标记为 DiskPressure 的节点上的 Pod 驱逐出去，以释放磁盘空间。 内存压力（MemoryPressure）： 如果节点的内存使用达到临界点，Kubernetes 会驱逐某些 Pod，以腾出内存。驱逐通常从资源请求较低的 Pod 开始。 节点维护（如，节点被标记为不可调度）： 如果节点被管理员手动标记为不可调度，Kubernetes 会尝试驱逐该节点上的 Pod，以便将它们迁移到其他健康的节点上。 NodeTaints 和 Tolerations： 节点可能被标记为某种特定的 Taint，并且如果某些 Pod 没有合适的 Tolerations，这些 Pod 可能会被驱逐。 Pod 驱逐策略： 如果 Pod 配置了特定的驱逐策略或 PodDisruptionBudget（PDB）允许驱逐，Pod 也可能被驱逐。 Evicted 状态的标志 # 在 Pod 的状态中，如果 Pod 被驱逐，它的 status 字段会显示 Evicted。你可以使用 kubectl describe pod \u0026lt;pod-name\u0026gt; 查看 Pod 的详细信息，驱逐的原因通常会在 Events 中显示出来。\n示例： # kubectl describe pod \u0026lt;pod-name\u0026gt; 你可能会看到类似以下的信息：\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Evicted 10m kubelet, node-1 Pod was evicted due to memory pressure Reason：Evicted Message：Pod 被驱逐的原因，比如 memory pressure（内存压力）或 disk pressure（磁盘压力）。 Evicted 的后续处理 # Pod 不会自动重启：驱逐的 Pod 会被终止，但 Kubernetes 不会自动重启它们，除非你使用了 Deployment、StatefulSet 或其他控制器来管理 Pod。控制器会根据定义的副本数重新调度一个新的 Pod 来替代被驱逐的 Pod。 重新调度：如果 Pod 被驱逐并且没有控制器管理（如 Deployment），则 Pod 不会自动重新调度。在这种情况下，可能需要手动重新调度 Pod，或者通过增加节点资源和优化调度策略来避免驱逐。 资源优化：驱逐通常是资源不足的信号。你可以考虑： 调整 Pod 的资源请求和限制（requests 和 limits）。 增加节点资源或添加更多节点。 调整节点资源的调度策略，例如通过 Node Affinity 或 Pod Affinity 限制资源紧张的节点的 Pod 数量。 总结 # Evicted 状态表示 Pod 被从节点上驱逐，通常是由于资源不足（如内存、磁盘压力等）。Pod 被驱逐后，它不会自动重启，而是需要通过控制器（如 Deployment）重新调度或手动重新部署。理解和监控驱逐的原因有助于优化集群资源分配，减少 Pod 被驱逐的情况。\n如果同一个pod 出现几十几百次的evicted 是啥原因？ # # 如果同一个 Pod 多次被驱逐（Evicted），尤其是出现几十次甚至几百次的驱逐，这通常是由集群资源压力、Pod 配置问题、或者调度策略不当导致的。以下是一些可能的原因及其排查方式：\n1. 资源不足 (MemoryPressure / DiskPressure) # 最常见的原因是集群中的节点资源（尤其是内存或磁盘）不足。当节点资源不足时，Kubernetes 会驱逐资源消耗较多的 Pod。Pod 被驱逐后，由于资源压力可能没有得到缓解，Pod 会再次被调度到同一节点或其他资源同样紧张的节点，导致多次驱逐。\n可能原因： # 节点内存或磁盘压力：如果节点的内存或磁盘已达到高使用率，Kubernetes 可能会不断驱逐 Pod，特别是当 Pod 请求的资源比节点可用的资源还要多时。 Pod 请求的资源过高：Pod 配置的 requests 和 limits 可能过高，导致 Pod 经常在节点上没有足够的资源可用。 排查方法： # 检查节点资源使用情况：\nkubectl top nodes 检查节点的 CPU、内存、磁盘 使用情况，看看是否有节点的资源过度使用。\n检查 Pod 资源请求：\nkubectl describe pod \u0026lt;pod-name\u0026gt; 查看 Pod 的资源请求（requests）和限制（limits）是否合理。\n检查节点磁盘使用情况：\nkubectl describe node \u0026lt;node-name\u0026gt; 查看节点的磁盘使用情况，看看是否存在 DiskPressure，并检查磁盘空间是否紧张。\n2. Pod 配置问题 # Pod 本身的配置可能导致其无法在节点上稳定运行。例如，容器镜像拉取失败、启动时间过长等，都可能导致 Pod 被反复驱逐。\n可能原因： # 镜像拉取失败：如果 Pod 启动时需要拉取镜像，但镜像无法成功拉取（例如，镜像仓库问题、镜像大小过大等），Pod 可能会进入 Evicted 状态。 初始化容器（Init Containers）失败：如果 Pod 配置了初始化容器，而初始化容器失败，Pod 也可能多次被驱逐。 排查方法： # 查看事件日志：\nkubectl describe pod \u0026lt;pod-name\u0026gt; 查看 Events 字段，检查是否有镜像拉取失败或其他错误信息。\n检查容器和初始化容器状态： 查看 Pod 的容器是否有异常退出或启动失败，导致反复重启和驱逐。\n3. Pod 驱逐策略（Taints 和 Tolerations） # 如果节点设置了 Taints（污点），并且 Pod 没有相应的 Tolerations（容忍），Pod 可能会频繁被驱逐。特别是在使用了节点亲和性或调度策略的情况下，如果集群的资源分配不均，Pod 可能会频繁在资源不足的节点之间迁移。\n可能原因： # 节点的 Taints 设置：如果节点上设置了 Taints，而 Pod 没有适当的 Tolerations，Pod 会被驱逐并且无法在该节点上稳定运行。 排查方法： # 检查节点的 Taints\n：\nkubectl describe node \u0026lt;node-name\u0026gt; 查看节点是否有 Taints 设置，并检查 Pod 是否有合适的 Tolerations。\n4. 调度和优先级问题 # 如果 Pod 配置了低优先级（Priority），且节点资源有限，Kubernetes 调度器可能会选择驱逐这些低优先级的 Pod，来腾出资源给更高优先级的 Pod。\n可能原因： # 优先级较低的 Pod：如果 Pod 配置了较低的 PriorityClass，在节点资源紧张时，这些 Pod 更容易被驱逐。 排查方法： # 检查 Pod 的优先级： 查看 Pod 是否有配置 PriorityClass，并根据优先级调整 Pod 的资源调度策略。 5. PodDisruptionBudget (PDB) # PodDisruptionBudget（PDB）用于控制在进行自主操作（例如，删除 Pod）时，允许的最小可用副本数。如果设置不当，PDB 可能导致 Pod 被驱逐。\n可能原因： # PDB 设置过于严格：PDB 的配置可能限制了 Pod 副本数，导致 Pod 频繁被驱逐。 排查方法： # 检查 PodDisruptionBudget 配置\n：\nkubectl get pdb 查看集群中是否有\nPodDisruptionBudget\n配置，并检查设置是否符合预期。\n6. 集群健康和调度器问题 # 如果集群的调度器存在问题，或者调度器无法为 Pod 分配合适的节点，Pod 可能会反复被调度到同一个资源紧张的节点，并最终被驱逐。\n可能原因： # 调度器 bug 或配置错误：可能存在调度器错误导致 Pod 被错误地调度到不可用的节点。 排查方法： # 检查调度器日志： 检查 Kubernetes 调度器（kube-scheduler）的日志，查看是否有调度异常。 如何解决？ # 优化资源请求和限制：检查 Pod 的资源请求和限制配置，确保它们合理。避免过高的资源请求，特别是当集群资源紧张时。 增加节点资源：增加集群中节点的数量或资源（如内存、磁盘空间等），减少资源不足引起的驱逐。 提高节点的资源可用性：使用 Node Affinity 和 Pod Affinity 控制 Pod 调度到资源充足的节点。 调整 Pod 优先级：为关键的 Pod 设置更高的优先级，以确保它们不会被低优先级 Pod 驱逐。 检查并优化调度策略：检查调度策略，确保 Pod 被调度到合适的节点，避免资源集中在某个节点导致频繁驱逐。 总结 # Pod 多次被驱逐通常是由于集群资源不足、Pod 配置问题或调度策略不当等原因。通过优化 Pod 的资源请求、调整调度策略、增加集群资源或修复配置问题，可以减少 Pod 被频繁驱逐的情况。\n如果pod超出资源限制被杀死，那他时重启 重建还是什么呢？ # 当一个 Pod 超出资源限制（如 CPU 或 内存）并被杀死时，Kubernetes 会根据 Pod 的控制器类型（如 Deployment、StatefulSet 等）采取不同的处理方式。下面是具体的行为：\n1. 容器被杀死时的行为 # 内存限制超出（Out of Memory，OOM）：当 Pod 的容器超出了定义的内存限制时，容器会被操作系统内核（Linux 的 OOM Killer）杀死。这通常发生在容器使用的内存超出了指定的限制。容器会退出，并且退出状态码为 137（表示容器因接收到 SIGKILL 信号而终止）。 CPU 限制超出：Kubernetes 本身不会直接杀死容器，尽管容器的 CPU 使用超过限制，Kubernetes 会限制容器的 CPU 使用，不会让它占用更多的资源，容器仍然可以运行，但可能会因为 CPU 过度消耗而影响性能。 2. 容器的重启行为 # 当容器因超出资源限制被杀死时，容器的处理方式由 Pod 的重启策略 决定。Pod 的重启策略由以下几种：\nA. Always（默认值） # 如果 Pod 配置了 restartPolicy: Always（这是 Deployment、ReplicaSet 等控制器的默认重启策略），即使容器被 OOM 杀死或因其他原因退出，Kubernetes 会自动重新启动容器。 控制器行为：如果是由 Deployment 或 ReplicaSet 管理的 Pod，当容器因资源超限而被杀死时，Kubernetes 会自动重新调度一个新的容器实例，以确保 Pod 的副本数不变。 B. OnFailure # 如果 Pod 的重启策略是 restartPolicy: OnFailure，只有当容器以非零状态码退出时（即容器因错误退出），Kubernetes 才会重启容器。 如果容器是因 OOM 被杀死（即退出状态码 137），它会被认为是 \u0026ldquo;失败\u0026rdquo;（failure），因此 Pod 会被重启。但如果是正常退出（例如容器自行退出），则不会重启。 C. Never # 如果 Pod 的重启策略是 restartPolicy: Never，则无论容器退出的原因是什么，Kubernetes 都不会自动重启容器，Pod 将停止运行。如果你使用的是此策略，你需要手动删除 Pod 或重新创建它。 3. 由控制器管理的行为 # 当 Pod 由 Deployment、StatefulSet 或其他控制器管理时，它们会尝试 重建 Pod 来保持所需的副本数。例如：\nDeployment：如果 Pod 被杀死（因资源限制或其他原因），Deployment 会自动创建一个新的 Pod 来替代被删除的 Pod，以确保副本数始终保持一致。 StatefulSet：对于有持久化状态的应用（例如数据库），StatefulSet 会在 Pod 被杀死后重新调度新的 Pod，并保持原有的持久存储和网络身份。 DaemonSet：如果 Pod 被杀死，DaemonSet 会确保在每个节点上始终运行 Pod 实例。 4. OOM Kill 与重启的细节 # 当容器因内存超出限制被杀死时，容器的 退出状态码 会是 137（表示收到 SIGKILL 信号）。这种情况下，如果 Pod 的重启策略是 Always 或 OnFailure，Kubernetes 会重新启动容器。\n重启策略为 Always：容器会立即被重启。 重启策略为 OnFailure：容器会被重启，前提是退出状态码是非零（例如 OOM 导致退出）。 5. Pod 状态变化 # 在 Pod 被杀死并重新启动时，你可以使用 kubectl describe pod \u0026lt;pod-name\u0026gt; 命令查看 Pod 的状态和事件日志。例如，OOM 杀死的容器通常会记录类似以下的事件：\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Killing 5m kubelet, node-1 Killing container with id docker://\u0026lt;container-id\u0026gt;:Container exceeded memory limit Normal Pulled 4m kubelet, node-1 Container image \u0026#34;nginx\u0026#34; already present on machine Normal Created 4m kubelet, node-1 Created container Normal Started 4m kubelet, node-1 Started container 6. 如何避免 Pod 被 OOM 杀死 # 为了避免 Pod 因内存限制被杀死，可以采取以下措施：\n调整资源请求和限制：\n确保容器的 内存请求 和 内存限制 设置合理。请求值应该是容器正常运行时所需的最小内存，而限制值是容器可以使用的最大内存。 示例：\nresources: requests: memory: \u0026#34;256Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; limits: memory: \u0026#34;512Mi\u0026#34; cpu: \u0026#34;1\u0026#34; 增加节点资源：\n如果集群资源紧张，考虑增加节点，或者优化现有节点的资源分配。 优化应用程序的内存使用：\n如果应用程序内存使用过高，可以优化代码或者配置，以减少内存占用。 总结 # 当 Pod 因超出资源限制被杀死时，容器会根据 Pod 的 重启策略 进行 重启 或 重新创建。如果是 Deployment 等控制器管理的 Pod，Pod 会被重建以确保副本数不变。如果是 OOM（Out of Memory） 导致的，容器退出状态码为 137，并会根据重启策略决定是否重启。如果是 CPU 限制，容器不会被杀死，只是 CPU 资源被限制，容器仍然会继续运行。\n如果 Pod 配置了 restartPolicy: Always（这是 Deployment、ReplicaSet 等控制器的默认重启策略），即使容器被 OOM 杀死或因其他原因退出，Kubernetes 会自动重新启动容器。 控制器行为：如果是由 Deployment 或 ReplicaSet 管理的 Pod，当容器因资源超限而被杀死时，Kubernetes 会自动重新调度一个新的容器实例，以确保 Pod 的副本数不变。 自相矛盾了吧 # # 你说得对！我在这段描述中确实存在自相矛盾的表述，感谢你指出来。让我澄清一下这部分内容。\nrestartPolicy: Always 和 控制器行为 # 对于由 Deployment、ReplicaSet 等控制器管理的 Pod，即使容器因为 OOM（Out of Memory） 被杀死，容器本身 会 重新启动，而不是直接重建。实际上，控制器会确保 Pod 的副本数保持一致，并且容器会被 重新调度重启，而不是创建一个新的 Pod。\n但是，有一些细节需要进一步说明：\n控制器的行为： # Deployment / ReplicaSet 等控制器\n：\n这些控制器会确保集群中有 指定数量的 Pod 副本。 如果 Pod 的容器因 OOM 被杀死（或因为其他原因退出），Pod 会被重新启动（容器重启）。 但是，如果 Pod 出现问题，无法重启（例如，restartPolicy: Never），控制器会创建一个新的 Pod 来替代已经退出的 Pod，以保持副本数。 细节：容器的重启 vs Pod 重建 # 容器重启：在 restartPolicy: Always 的情况下，Pod 的容器会被自动重启。这意味着，如果一个容器因为 OOM 被杀死，它会被重新启动，而不会导致整个 Pod 被删除或替换。 Pod 被重建：如果控制器管理的 Pod 发生 不再能重启 的情况（例如，Pod 本身无法启动），控制器会 删除原 Pod 并创建新的 Pod，以确保副本数维持不变。 示例： # 假设我们有一个 Deployment 管理的 Pod：\nPod 中的容器因为 OOM 被杀死（退出状态码 137）。 Kubernetes 会根据 restartPolicy: Always 重启容器，而不需要删除 Pod。 控制器会确保副本数不会降低，因此如果容器无法重新启动，控制器会创建一个新的 Pod。 总结：\n容器重启：容器被 OOM 杀死后会自动重启（如果重启策略是 Always）。 Pod 重建：如果容器无法重启或 Pod 出现其他不可恢复的故障，控制器会创建新的 Pod，以保持副本数。 希望这个解释更清晰地解决了你的疑问！再次感谢你指出问题，确保了描述更准确。\ndebug容器调试老是crashbackof的pod？ # 当你在调试容器时遇到 CrashBackOff 状态，通常意味着容器在启动后立即崩溃，且 Kubernetes 在多次尝试重启容器后进入了“退避”（BackOff）状态。CrashBackOff 表示容器的启动失败并且 Kubernetes 正在延迟重启容器。这个问题通常与容器启动过程中出现的错误相关。\nCrashBackOff 产生的常见原因 # 容器启动失败： 容器可能因为应用程序崩溃、配置错误或缺少依赖等原因启动失败。 例如，容器可能需要某些环境变量或配置文件，但这些缺失或配置错误导致容器无法正常启动。 内存超限（OOM）： 容器可能在启动时消耗的内存超过了指定的内存限制，从而被操作系统的 OOM（Out of Memory）杀死。 错误的启动命令： 如果容器启动时使用的命令或入口点（ENTRYPOINT 或 CMD）不正确，容器会在启动时立即崩溃。 依赖服务不可用： 如果容器依赖的其他服务或资源不可用（例如，数据库连接失败、网络不可达等），容器启动时会崩溃。 权限问题： 如果容器需要特定的权限来访问文件或执行操作，而这些权限没有正确配置，容器也会崩溃。 调试 CrashBackOff 的步骤 # 查看 Pod 和容器的日志：\n通过查看容器的日志，可以找到导致崩溃的具体错误。 kubectl logs \u0026lt;pod-name\u0026gt; -c \u0026lt;container-name\u0026gt; 如果 Pod 有多个容器，可以指定容器名称。查看日志输出，找出异常信息或错误提示。\n如果容器已经崩溃，你还可以尝试查看容器的 previous logs（上一个容器的日志）： kubectl logs \u0026lt;pod-name\u0026gt; -c \u0026lt;container-name\u0026gt; --previous 检查 Pod 的事件：\n使用 kubectl describe 命令查看 Pod 事件，这可以帮助你识别容器崩溃的原因。 kubectl describe pod \u0026lt;pod-name\u0026gt; 查看 Events 部分，检查是否有资源限制、OOM 或其他错误导致容器崩溃的信息。\n确认容器的资源请求和限制：\n确保 Pod 配置了合理的 resources.requests 和 resources.limits。如果内存请求过小，可能会导致容器启动时被 OOM 杀死。 检查是否有 MemoryLimit 或 CPULimit 设置得太低，导致容器被系统强制终止。 示例：\nresources: requests: memory: \u0026#34;128Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; limits: memory: \u0026#34;256Mi\u0026#34; cpu: \u0026#34;1\u0026#34; 查看容器的退出状态：\n查看容器退出的状态码（Exit Code）。如果是 137，则说明容器是因为 OOM 被杀死；如果是其他非零状态码，可能是应用崩溃。 kubectl get pod \u0026lt;pod-name\u0026gt; -o jsonpath=\u0026#39;{.status.containerStatuses[0].state.terminated.exitCode}\u0026#39; 检查 Pod 的启动命令和配置：\n确保容器的启动命令（ENTRYPOINT 和 CMD）正确无误，并且容器内部的所有依赖项（例如文件、环境变量、配置）都已正确配置。 增加调试信息：\n如果可能，可以增加容器内应用的调试日志，或者在启动命令中加上更多的输出（如 echo，sleep 等），从而帮助诊断问题。 查看节点的资源使用情况：\n如果是由于节点资源紧张导致的 CrashBackOff（例如，内存或 CPU 不足），你可以查看节点的资源使用情况： kubectl top nodes 调整重启策略：\n如果想要进一步调试容器，可以暂时修改重启策略为 Never，这样容器崩溃时不会自动重启，可以便于调试。 restartPolicy: Never 使用 kubectl exec 进入容器：\n如果容器已经启动并且能短暂运行，可以通过 kubectl exec 进入容器中进行调试，检查环境变量、依赖、配置文件等。 kubectl exec -it \u0026lt;pod-name\u0026gt; -- /bin/bash 示例：查看容器日志 # 查看容器日志：\nkubectl logs \u0026lt;pod-name\u0026gt; -c \u0026lt;container-name\u0026gt; 查看退出状态码：\nkubectl get pod \u0026lt;pod-name\u0026gt; -o jsonpath=\u0026#39;{.status.containerStatuses[0].state.terminated.exitCode}\u0026#39; 查看 Pod 事件：\nkubectl describe pod \u0026lt;pod-name\u0026gt; 常见的 CrashBackOff 问题和解决方案 # OOM 错误：增加 resources.requests 和 resources.limits 中的内存限制。 错误的启动命令：检查容器的启动命令（ENTRYPOINT 和 CMD），确保没有拼写错误或路径错误。 缺少依赖：确保容器内所需的所有文件、环境变量和网络服务都能正常访问。 权限问题：检查容器是否有足够的权限访问所需的文件和网络资源。 总结 # 当 Pod 进入 CrashBackOff 状态时，通常是由于容器启动失败或异常退出。通过查看日志、描述事件、检查资源配置和启动命令，你可以找到并解决容器崩溃的根本原因。如果有必要，还可以调整 Pod 的重启策略、添加调试信息，或者使用 kubectl exec 进入容器进行更深入的调试。\nkubectl exec 进不去因为 会一直重启 # 当容器处于 CrashBackOff 状态时，由于容器会不断重启，你是无法通过 kubectl exec 直接进入容器的，因为容器可能根本没有持续运行足够长的时间来建立连接。这种情况需要一些特别的操作来调试和诊断问题。以下是一些解决方案和调试技巧：\n1. 使用 kubectl logs 查看日志 # 首先，最简单的办法是查看容器的日志，尽管容器在 CrashBackOff 状态，但通常会有一些日志记录（如错误信息或异常），可以帮助你定位问题。\nkubectl logs \u0026lt;pod-name\u0026gt; -c \u0026lt;container-name\u0026gt; --previous --previous：查看 上一个容器 的日志（如果容器已经崩溃）。 2. 暂时设置重启策略为 Never # 为了阻止 Pod 一直重启，你可以暂时将重启策略设置为 Never，然后手动删除 Pod，防止它在你排查时继续重启。\n修改 Pod 配置： # 编辑 Pod 或 Deployment（如果是通过控制器管理的 Pod）：\nkubectl edit deployment \u0026lt;deployment-name\u0026gt; 在编辑界面中，修改 restartPolicy 为 Never，或者如果你是直接管理 Pod，修改其 YAML 文件。\nrestartPolicy: Never 然后保存更改。\n在这个修改之后，你可以手动删除当前 Pod 并重建它，然后再尝试 kubectl exec 进入容器进行调试。\nkubectl delete pod \u0026lt;pod-name\u0026gt; 如果 Pod 还需要被重建，可以使用 kubectl apply 重新应用 Deployment 等资源。 3. 增加容器启动延迟 # 如果容器重启太快，你可以在容器启动时添加一些延迟，使其在启动后有足够的时间来调试。\n例如，你可以在容器的启动命令中添加一个 sleep 命令来延迟启动过程。修改你的容器配置文件（Dockerfile 或 Pod 配置）：\ncommand: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;sleep 60 \u0026amp;\u0026amp; \u0026lt;your-command\u0026gt;\u0026#34;] 这里的 sleep 60 会让容器在启动后停顿 60 秒，这样你就可以使用 kubectl exec 进入容器进行调试。\n4. 修改容器的启动命令为调试模式 # 你还可以修改容器的启动命令，使其进入调试模式或者以交互模式启动容器。例如，直接启动一个 shell 会话，而不是运行主程序：\ncommand: [\u0026#34;/bin/sh\u0026#34;] 这样，容器就会启动一个交互式 shell，你可以通过 kubectl exec 进入容器并查看环境、依赖、配置等。\n5. 使用 kubectl run 创建临时 Pod 进行调试 # 如果无法进入原 Pod，可以创建一个临时的 Pod 来进行调试。通过 kubectl run 创建一个类似的容器，并进入该容器进行调试：\nkubectl run -it --rm --image=\u0026lt;image-name\u0026gt; --restart=Never \u0026lt;temporary-pod-name\u0026gt; -- /bin/sh 这个命令会创建一个临时 Pod，在容器启动后直接进入 shell，调试完毕后会自动删除 Pod。\n6. 查看 Pod 状态和事件 # 如果你想了解更多关于 Pod 崩溃的信息，可以使用 kubectl describe pod 来查看事件和容器的退出状态码。\nkubectl describe pod \u0026lt;pod-name\u0026gt; 在 Events 部分，通常会记录容器崩溃的原因。例如，OOM（Out Of Memory）或者其他启动错误。\n7. 通过 kubectl debug 创建调试容器 # Kubernetes 提供了 kubectl debug 命令来在现有 Pod 上启动一个调试容器（比如基于相同的镜像或者不同的镜像），让你进行调试。\nkubectl debug -it \u0026lt;pod-name\u0026gt; --image=busybox --target=\u0026lt;container-name\u0026gt; 这个命令会启动一个新容器，与目标容器共享网络和挂载的卷，从而可以进入容器并进行进一步的调试。\n总结 # 如果 Pod 处于 CrashBackOff 状态，直接使用 kubectl exec 可能会遇到困难，因为容器会不断重启。你可以通过以下方法调试：\n查看日志（kubectl logs）。 临时修改 restartPolicy 为 Never，避免 Pod 继续重启。 增加启动延迟或修改容器启动命令为调试模式。 使用 kubectl run 或 kubectl debug 创建临时调试 Pod。 通过这些方法，你可以更方便地排查和调试容器的启动问题。\n默认日志存在宿主机那个位置？ # Kubernetes 中的容器日志默认保存在宿主机的 /var/log/containers/ 目录下。具体来说，容器的日志文件存储位置取决于宿主机的容器运行时（例如 Docker 或 containerd），但通常会有以下几个路径和结构：\n1. Docker 容器日志默认路径 # 如果你的 Kubernetes 节点使用 Docker 作为容器运行时，容器的日志默认会存储在宿主机的 /var/lib/docker/containers/\u0026lt;container-id\u0026gt;/ 目录下，具体路径是：\n/var/lib/docker/containers/\u0026lt;container-id\u0026gt;/\u0026lt;container-id\u0026gt;-json.log \u0026lt;container-id\u0026gt;：是每个容器的唯一 ID。你可以通过 docker ps 命令查看正在运行的容器及其 ID。 -json.log：这个文件记录了容器的标准输出和标准错误输出（stdout 和 stderr）。 例如，如果你要查看某个容器的日志，可以找到对应的 \u0026lt;container-id\u0026gt;-json.log 文件，然后使用 cat 或 tail 命令查看日志内容：\ntail -f /var/lib/docker/containers/\u0026lt;container-id\u0026gt;/\u0026lt;container-id\u0026gt;-json.log 2. containerd 容器日志默认路径 # 如果 Kubernetes 使用 containerd 作为容器运行时（在许多现代 Kubernetes 集群中使用），容器日志会保存在类似的路径中：\n/var/log/containers/\u0026lt;pod-name\u0026gt;_\u0026lt;namespace\u0026gt;_\u0026lt;container-name\u0026gt;-\u0026lt;container-id\u0026gt;.log 这些日志文件也是容器的标准输出和标准错误输出，通常是以 JSON 格式存储。\n3. Kubernetes 节点日志（/var/log/containers） # 在 Kubernetes 集群中，无论使用 Docker 还是 containerd，容器的标准输出日志文件一般都会存放在 /var/log/containers/ 目录下。日志文件的命名格式通常是：\n/var/log/containers/\u0026lt;pod-name\u0026gt;_\u0026lt;namespace\u0026gt;_\u0026lt;container-name\u0026gt;-\u0026lt;container-id\u0026gt;.log 例如，一个 Pod 中的 nginx 容器的日志文件路径可能是：\n/var/log/containers/nginx_default_nginx-12345abcde67890.log 你可以使用 tail 或 cat 命令查看日志内容：\ntail -f /var/log/containers/\u0026lt;pod-name\u0026gt;_\u0026lt;namespace\u0026gt;_\u0026lt;container-name\u0026gt;-\u0026lt;container-id\u0026gt;.log 4. 查看日志的其它方式 # 除了直接访问宿主机上的日志文件外，你也可以使用 kubectl 命令查看容器日志：\nkubectl logs \u0026lt;pod-name\u0026gt; -c \u0026lt;container-name\u0026gt; -n \u0026lt;namespace\u0026gt; 如果 Pod 中有多个容器，可以指定容器名称。\n总结 # Docker 环境：日志存储在 /var/lib/docker/containers/\u0026lt;container-id\u0026gt;/\u0026lt;container-id\u0026gt;-json.log。 containerd 环境：日志存储在 /var/log/containers/\u0026lt;pod-name\u0026gt;_\u0026lt;namespace\u0026gt;_\u0026lt;container-name\u0026gt;-\u0026lt;container-id\u0026gt;.log。 在大多数 Kubernetes 环境中，日志可以通过宿主机上的 /var/log/containers/ 目录找到，并且可以通过 kubectl logs 命令查看容器日志。 docker的网络模式有哪些 # Docker提供了几种不同的网络模式，用来满足不同的容器通信需求。常见的网络模式包括：\nbridge (桥接模式) 这是Docker的默认网络模式。容器会连接到Docker主机的虚拟桥接网络，通过桥接网络，容器之间可以相互通信，同时容器也能访问宿主机和外部网络。 host (主机模式) 容器直接使用宿主机的网络栈，不会进行网络隔离。容器内的网络配置与宿主机相同，容器的端口映射到宿主机上，适用于需要高性能网络通信的场景。 none (无网络模式) 容器没有网络连接。容器内的网络接口会被禁用，容器不能访问宿主机或其他容器，也不能接入任何外部网络。 container (容器模式) 容器共享另一个容器的网络栈。网络设置与被共享容器相同，容器之间可以通过localhost进行通信。这种模式下，多个容器会共享IP地址和端口空间。 host-gateway (主机网关模式) 这是一个相对较新的网络模式，用来实现容器与宿主机间的直接网络通信。容器能够通过指定的网关与宿主机互通。 overlay (覆盖网络模式) 适用于跨多个Docker主机的容器之间的网络通信。Docker Swarm或Kubernetes等集群管理工具常使用overlay网络。在不同主机上的容器可以像在同一个主机上那样直接通信。 macvlan (MAC虚拟局域网模式) 容器会被赋予一个唯一的MAC地址，容器可以直接连接到物理网络中，类似于独立的物理主机。这种模式可以让容器直接被其他物理设备识别，适合需要容器表现为物理设备的场景。 ipvlan (IP虚拟局域网模式) 类似于macvlan，但它基于IP地址而非MAC地址进行网络隔离。适用于对性能要求较高、需要容器直接访问物理网络的场景。 不同的网络模式适合不同的场景，选择合适的网络模式能提升应用的网络性能或满足安全需求。\nnginx的location的proxy的地址后面加/和不加/的区别 # 在Nginx的location配置中，使用proxy_pass指令时，地址后面加 / 和不加 / 确实有区别，主要体现在请求路径如何传递给上游服务器。\n1. 加 /： # 如果在 proxy_pass 后面加上 /，Nginx 会将客户端请求的路径的剩余部分追加到上游服务器的地址后面。\n例子：\nlocation /api/ { proxy_pass http://backend/; } 假设有请求 http://nginx-server/api/user，Nginx 会将请求转发到 http://backend/user。也就是说，/api/ 部分会被去掉，只保留 /user。\n总结：加 / 会去掉原请求路径中的匹配部分，直接将剩余路径传递给上游服务器。\n2. 不加 /： # 如果在 proxy_pass 后面不加 /，Nginx 会将客户端请求的路径的剩余部分直接追加到上游服务器的地址后面，并保留匹配到的部分。\n例子：\nlocation /api/ { proxy_pass http://backend; } 假设有请求 http://nginx-server/api/user，Nginx 会将请求转发到 http://backend/api/user。这时，/api/ 部分会被保留下来。\n总结：不加 / 时，Nginx 会保留匹配的部分，并将剩余的路径传递给上游服务器。\n小结： # 加 /：会将匹配路径去掉，剩余的路径传递给后端。 不加 /：会保留匹配的路径部分，直接传递给后端。 选择加或不加 / 取决于你希望如何将请求路径传递到上游服务器。\nk8s的dns的影响clusterfist 还有哪些 # Kubernetes的DNS在集群内部通信中起着至关重要的作用，尤其是在服务发现和通信方面。Kubernetes的DNS服务通常是通过kube-dns或CoreDNS来实现的，它会为集群中的所有Pod和服务提供DNS解析。\n在Kubernetes集群中，DNS配置有时会受到多种因素的影响，尤其是在集群初始化时的配置和网络环境的设置。以下是几种可能影响Kubernetes DNS的因素：\n1. ClusterFirst 和 ClusterFirstWithHostNet # 在Kubernetes中，DNSPolicy配置项允许你指定DNS解析策略，影响Pod如何访问DNS解析。\nClusterFirst： 这是默认的DNS策略。Pod的DNS解析请求首先会被路由到集群内部的DNS服务（通常是CoreDNS）。如果请求是集群内的域名（如service-name.namespace.svc.cluster.local），则会被解析为集群内的IP地址。如果请求的是外部域名，DNS请求会被转发到外部DNS服务器。\n适用场景：大多数情况下都使用此模式，尤其是Pod之间需要访问服务时。\nClusterFirstWithHostNet： 当Pod使用hostNetwork: true时，Pod将共享宿主机的网络栈。此时，如果你指定ClusterFirstWithHostNet，Pod会尝试通过集群的DNS服务解析内网域名（比如service-name.namespace.svc.cluster.local），但对于外部域名，它会使用宿主机的DNS配置，而不是集群内部的DNS。\n适用场景：适用于需要共享宿主机网络的场景（例如，网络性能要求很高的Pod，或者某些特殊的网络配置）。\n2. DNS解析的依赖性 # Kubernetes集群中的DNS服务是高度集成的，很多集群功能依赖DNS：\nService Discovery：通过DNS解析服务名称来获取对应的IP地址。 Pod之间的通信：Pod可以通过DNS服务来相互发现和通信。 Ingress Controller：使用DNS解析来访问Ingress资源。 如果DNS服务出现问题，整个集群内的服务发现和通信将受到影响。\n3. DNS配置中的search和resolve # Kubernetes中的DNS解析还可能受到Pod的/etc/resolv.conf配置的影响。例如，在Pod启动时，Kubernetes会自动设置DNS解析的配置，包括search（搜索域名）和nameserver（DNS服务器）。这个配置通常是由集群中的DNS服务（例如CoreDNS）提供的。\nsearch字段：指定域名后缀，允许不完整的域名被自动补全为完全合格的域名（FQDN）。 nameserver字段：指定DNS服务器的地址，通常是集群内的DNS服务的IP地址。 4. NetworkPolicy的影响 # 如果在集群中启用了NetworkPolicy，它可能会影响Pod与DNS服务器（如CoreDNS）之间的通信。某些NetworkPolicy可能阻止Pod访问DNS服务，导致DNS解析失败。\n解决方法：确保所有Pod能够访问DNS服务端点（通常是kube-dns或CoreDNS服务）。\n5. CoreDNS和kube-dns # Kubernetes使用CoreDNS（或kube-dns，具体取决于Kubernetes版本）作为DNS解析服务。CoreDNS通常与Kubernetes集群的API服务器、etcd和网络插件紧密集成。如果CoreDNS出现问题，集群内的DNS解析会中断。\nDNS Server：CoreDNS负责提供DNS解析功能。 Cache：CoreDNS通常会缓存DNS查询结果，以减少DNS查询的延迟。 插件：CoreDNS支持多种插件，例如forward插件可以将查询转发到外部DNS服务器，kubedns插件可以解析集群内的服务名称。 6. 集群网络插件（CNI） # Kubernetes的CNI插件（例如Flannel、Calico、Weave等）在集群网络的配置中扮演着重要角色。某些网络插件可能会影响DNS的路由和解析，特别是如果网络插件配置不当或与DNS服务不兼容时。\nPod网络隔离：一些网络插件可能会导致DNS服务无法从某些Pod或命名空间中访问，尤其是在Pod被网络策略（NetworkPolicy）隔离的情况下。 DNS流量路由：有些网络插件可能会通过不同的路由策略影响DNS流量的传输，导致DNS解析超时或失败。 7. DNS Pod和Service的高可用性 # Kubernetes的DNS服务（如CoreDNS）通常会以Deployment形式运行，具备一定的高可用性，但在一些配置下，DNS服务可能成为集群的单点故障。如果CoreDNS或kube-dns Pod出现故障，DNS解析就会失败，影响集群内所有Pod的通信。\nPod副本数：为了确保高可用性，CoreDNS通常至少有两个副本（Replica）。 资源限制：如果CoreDNS的资源配置不足，可能导致解析延迟或失败。 8. ExternalDNS # Kubernetes中的ExternalDNS可以帮助你将集群中的服务自动注册到外部DNS服务（如Route 53，Google Cloud DNS等）。这种方式将影响集群外部的DNS解析，但不会直接影响内部的DNS解析。\n小结： # Kubernetes中的DNS服务对集群的稳定性和服务发现至关重要。ClusterFirst和ClusterFirstWithHostNet是最常用的DNS策略，其他因素如网络策略、CNI插件、DNS服务的高可用性等都会影响DNS解析的正常工作。确保集群的DNS配置正确，并定期监控DNS服务的状态，对于保证集群的健康至关重要。\ncalico和flannel有什么区别 # Calico 和 Flannel 都是 Kubernetes 中常用的网络插件（CNI，容器网络接口），用于实现容器之间的网络通信。它们在设计理念、功能、性能等方面有一些显著的区别。下面是它们的主要差异：\n1. 设计理念 # Flannel： Flannel 是一个比较简单的网络插件，设计上侧重于简化 Kubernetes 网络的配置和管理。它主要提供一个基础的网络层，确保不同节点上的 Pod 可以相互通信。Flannel 创建一个覆盖网络（overlay network），并为每个节点分配一个独立的子网。 Calico： Calico 提供更强大的网络功能，包括网络策略（Network Policy）和安全性控制。它不仅支持 overlay 网络，还可以支持基于路由的网络（即基于BGP的网络）。Calico 可以为每个 Pod 分配一个 IP 地址，并提供集成的网络策略功能，可以实现更细粒度的流量控制和安全管理。 2. 网络模式 # Flannel： 默认情况下，Flannel 使用 overlay 网络模式，这意味着它会在现有的网络上建立一个虚拟的、独立的网络层。Flannel 为每个节点分配一个子网，并通过 VXLAN 或其他技术（如UDP）在节点间传输数据。 Overlay模式：Flannel 在集群各节点间建立一个虚拟网络，所有容器/Pod的流量都通过这个虚拟网络进行转发。 简单配置：Flannel 比较容易部署，适合对网络性能要求不高的集群。 Calico： Calico 可以支持两种网络模式： Overlay模式：与 Flannel 类似，Calico 也可以使用 overlay 网络，但它更灵活，可以根据需求选择。 BGP模式：Calico 更加核心的特点是它可以使用 BGP（边界网关协议）来进行网络路由。每个节点都会参与路由决策，Pod之间的数据包会基于路由表进行直接传递。与 Flannel 的 overlay 网络相比，Calico 的这种方式性能更高，延迟更低，因为它不需要额外的封装层。 3. 网络策略（Network Policy） # Flannel： Flannel 本身并不提供网络策略功能。如果要使用网络策略，通常需要与其他网络插件（如 Calico 或 Cilium）结合使用。\nCalico： Calico 的一个显著特点是它内建了强大的 网络策略 功能。通过 Calico，你可以非常灵活地定义 Pod 间、Pod 和外部之间的流量规则。这使得 Calico 成为一种理想的选择，特别是在需要细粒度安全控制和流量隔离的环境中。\n示例功能\n：\n控制哪些 Pod 可以与其他 Pod 通信。 基于标签、命名空间或端口号进行访问控制。 支持加密流量（IPsec、WireGuard 等）。 4. 性能 # Flannel： 由于 Flannel 默认使用 Overlay 网络，它的性能可能不如 Calico。特别是在 VXLAN 模式下，数据包会进行多次封装，可能导致一定的性能开销。对于性能要求较高的场景，Flannel 可能不如 Calico。 Calico： Calico 在性能方面通常表现更好，尤其是在使用 BGP 模式时。Calico 允许 Pod 直接通过底层网络进行通信，无需额外的封装层，因此能够减少延迟和提高吞吐量。 5. 可扩展性 # Flannel： Flannel 适合小到中等规模的集群。由于它是一个简单的 Overlay 网络，它在大规模集群中的表现可能会受到一些限制，特别是当节点数量较多时，VXLAN 的性能可能成为瓶颈。 Calico： Calico 支持更大的集群规模，特别是在需要更高吞吐量和低延迟的场景下。它的 BGP 路由模式使得它能够更加高效地处理大规模集群中的网络流量，避免了 Overlay 网络的性能瓶颈。 6. 安装与配置 # Flannel： Flannel 相对简单易用，适合快速部署和简单的用例。它的配置也比 Calico 简单，不需要太多额外的配置选项。 Calico： Calico 比 Flannel 更复杂一些，尤其是在启用 BGP 路由时，可能需要一些额外的配置。不过，Calico 也提供了很多功能和灵活性，因此在需要更多自定义网络控制的场景中，它是一个很好的选择。 7. 社区支持与生态 # Flannel： Flannel 是 Kubernetes 早期的网络插件之一，社区支持相对较好，但它的功能比较简单，通常适用于对功能要求不高的环境。 Calico： Calico 作为一个更强大的网络插件，它不仅有 Kubernetes 社区的支持，还被许多企业用于生产环境中，尤其是在对安全性和网络策略有较高要求的环境中。Calico 是一个活跃的开源项目，拥有广泛的用户和贡献者。 8. 支持的功能 # Flannel： 简单的 Pod 网络连接。 支持 VXLAN、UDP 和其他简单的网络模式。 不自带网络策略功能（需要与其他插件结合使用）。 Calico： 先进的网络策略功能，支持细粒度流量控制。 支持 BGP 和传统的 Overlay 网络。 支持高性能和低延迟的网络架构。 内建支持多租户和容器级别的隔离。 支持加密和 IPsec、WireGuard 等网络安全特性。 总结 # 特性 Flannel Calico 网络模式 主要使用 Overlay（VXLAN等）模式 支持 Overlay 和 BGP 路由模式 性能 较低，尤其在使用 VXLAN 时会有性能瓶颈 高性能，特别是在 BGP 模式下 网络策略 不支持（需要与其他插件结合使用） 内建支持强大的网络策略功能 可扩展性 适合小到中等规模集群 支持大规模集群，特别是在需要高性能的场景下 安装与配置 简单，易于安装和配置 配置稍复杂，但功能强大 社区支持 广泛支持，较为成熟 社区活跃，尤其在大规模企业级应用中更为常见 适用场景 小规模、性能要求不高的集群 需要高安全性、流量控制、网络策略的企业级集群 总的来说，Flannel 更适合快速部署、简单的网络场景，而 Calico 则适合需要更复杂网络控制、安全策略和大规模集群的环境。\n# efk elasticsearch fluentd kibana 怎么用 干嘛的 怎么收集pod日志 存储展示 # EFK 是 Elasticsearch、Fluentd 和 Kibana 的组合，它们一起构成了一个日志收集、存储和展示的解决方案。在 Kubernetes 环境中，EFK 被广泛用于集中式日志管理，帮助开发者和运维人员收集、存储和查看应用程序和系统的日志。\n以下是关于 EFK 组合各组件的功能及如何使用它们收集 Pod 日志、存储日志并展示日志的介绍：\n1. EFK 组件介绍 # Elasticsearch： Elasticsearch 是一个开源的分布式搜索引擎，主要用于存储、搜索和分析大规模的数据。它能够高效地存储日志并支持复杂的查询和分析。 Fluentd： Fluentd 是一个开源的日志收集工具，用于收集、处理和转发日志数据。它能够从多个来源收集日志，并将日志数据流转到其他服务（如 Elasticsearch）进行存储和分析。 Kibana： Kibana 是一个数据可视化工具，专门用来展示存储在 Elasticsearch 中的数据。你可以使用 Kibana 创建仪表盘、查询日志并进行分析，帮助用户快速找到日志中的问题。 2. EFK 体系架构 # EFK 体系架构基本上是一个流数据处理链条，具体流程如下：\nFluentd 收集 Kubernetes 中各个 Pod 的日志（或通过日志收集代理如 Filebeat）。 Fluentd 对日志进行处理和格式化后，将其发送到 Elasticsearch 进行存储。 Elasticsearch 存储日志数据，并支持快速搜索、索引和分析。 Kibana 通过与 Elasticsearch 集成，提供强大的界面，帮助用户实时查看和分析日志。 3. EFK 的用途 # EFK 解决方案通常用于以下用途：\n日志集中化：将多个 Pod 和应用的日志集中存储在一个地方，便于管理。 实时日志分析：支持对日志数据进行实时分析，能够快速发现问题。 日志搜索：Kibana 提供强大的搜索和过滤功能，可以轻松检索和分析日志。 监控和可视化：Kibana 可以通过仪表盘可视化日志数据，帮助开发者和运维人员快速理解集群和应用的健康状况。 4. 如何配置 EFK 以收集 Kubernetes Pod 日志 # 步骤 1：部署 Elasticsearch # 首先，你需要部署 Elasticsearch 来存储日志。可以使用 Kubernetes 的部署方式，或者使用官方的 Elasticsearch Helm Chart 进行部署。\n使用 Helm 部署 Elasticsearch 的命令如下：\nhelm install elasticsearch elastic/elasticsearch 配置 Elasticsearch：\n为 Elasticsearch 设置适当的资源限制和存储。 确保其能够对 Kubernetes 集群中的日志流量进行水平扩展。 步骤 2：部署 Fluentd # Fluentd 用于从 Kubernetes 集群中的各个节点和 Pod 收集日志。Fluentd 收集的日志将发送到 Elasticsearch 存储。\n你可以通过部署 Fluentd DaemonSet 来在每个节点上运行 Fluentd 收集日志：\n创建 Fluentd 配置文件 fluentd.conf，配置日志收集规则和数据传输到 Elasticsearch 的方式。\n部署 Fluentd：\n使用 Helm 安装 Fluentd：\nhelm install fluentd stable/fluentd 配置 Fluentd 输出到 Elasticsearch： 在 Fluentd 配置中，设置 elasticsearch 输出插件，确保日志被发送到 Elasticsearch。\n配置示例（在 Fluentd 配置文件中）：\n\u0026lt;match **\u0026gt; @type elasticsearch host elasticsearch.default.svc.cluster.local port 9200 logstash_format true logstash_prefix kubernetes-logs flush_interval 5s \u0026lt;/match\u0026gt; 步骤 3：部署 Kibana # Kibana 提供了一个 Web 界面来查看存储在 Elasticsearch 中的日志。你可以使用 Helm 或者手动部署 Kibana。\n使用 Helm 部署 Kibana：\nhelm install kibana elastic/kibana 配置 Kibana： 确保 Kibana 能够连接到 Elasticsearch，设置 Kibana 的 elasticsearch.hosts 配置项。\n步骤 4：验证日志收集 # 查看 Fluentd 是否正常收集日志： 使用 kubectl logs 查看 Fluentd Pod 的日志，确认它是否正在收集和发送日志到 Elasticsearch。\nkubectl logs -l app=fluentd 查看 Kibana 是否显示日志： 登录到 Kibana Web 界面，通常可以通过端口映射访问 Kibana，默认端口是 5601。\nkubectl port-forward svc/kibana 5601:5601 访问 http://localhost:5601，进入 Kibana 仪表盘，查看是否能够查询和可视化日志。\n5. 查看和分析日志 # 在 Kibana 中，使用 Discover 选项来搜索日志。你可以通过定义查询条件（如日志级别、时间范围等）来查找特定的日志信息。 在 Dashboard 中，你可以创建实时仪表盘，将关键日志指标可视化，方便监控。 Kibana 还支持各种高级功能，如日志聚合、过滤、和告警。 6. 日志存储和展示的最佳实践 # 日志清理和保留策略：Elasticsearch 存储日志时可能会占用大量磁盘空间。你可以设置日志的生命周期管理（例如：使用 ILM（Index Lifecycle Management）自动删除旧日志或将其归档）。 标签和元数据：在 Fluentd 配置中，可以确保每个日志条目附带 Kubernetes 的元数据（如 Pod 名称、命名空间、容器名等），这样在 Kibana 中可以更方便地搜索和分析。 资源限制：部署 EFK 堆栈时，要为 Elasticsearch、Fluentd 和 Kibana 设置适当的资源限制，防止因日志量过大而导致集群性能下降。 总结 # Fluentd 负责收集 Kubernetes Pod 和容器的日志。 Elasticsearch 负责存储日志，并支持强大的搜索和分析。 Kibana 提供一个友好的 Web 界面，用于展示和分析存储在 Elasticsearch 中的日志。 EFK 体系提供了一个功能强大且易于扩展的日志收集和展示解决方案，广泛应用于 Kubernetes 环境中，帮助团队快速发现问题并进行故障排除。\n# # "},{"id":195,"href":"/docs/2025-2-26-%E9%9D%A2%E8%AF%950225/","title":"面试0225","section":"Docs","content":" 上海外包太平洋保险 # 有完成过ansible比较复杂的role task的编写吗？ # 是的，基于你的需求描述，我可以帮你了解和完成一些复杂的 Ansible Role 和 Task 编写。以下是一些示例场景：\n1. 完整的 Kubernetes 集群节点加入 Role 编写 # 假设你需要使用 Ansible 创建一个完整的 Kubernetes 节点加入过程，包含以下内容：\n创建目录结构和配置文件 二进制文件下载和分发 systemd 服务文件创建和启用 配置文件模板化（kubelet，kube-proxy，CNI 插件等） 节点标签和调度禁用操作 kubelet、kube-proxy 等服务的启动管理 2. Ansible Role 示例 # 一个复杂的 Role 可能包含以下几个部分：\ntasks/main.yml # --- # 安装并配置 Kubernetes 组件（如 kubelet 和 kube-proxy） - name: 创建 Kubernetes 相关目录 file: path: \u0026#34;{{ item }}\u0026#34; state: directory owner: root group: root mode: \u0026#34;0755\u0026#34; with_items: - /var/lib/kubelet - /var/lib/kube-proxy - /etc/cni/net.d - /opt/cni/bin - name: 分发 kubelet 和 kube-proxy 二进制文件 copy: src: \u0026#34;{{ base_dir }}/bin/{{ item }}\u0026#34; dest: \u0026#34;{{ bin_dir }}/{{ item }}\u0026#34; mode: \u0026#34;0755\u0026#34; with_items: - kubectl - kubelet - kube-proxy - name: 分发 CNI 插件 copy: src: \u0026#34;{{ item }}\u0026#34; dest: /opt/cni/bin/ mode: \u0026#34;0755\u0026#34; with_fileglob: - \u0026#34;{{ base_dir }}/bin/cni-bin/*\u0026#34; # 创建 kubelet 和 kube-proxy 配置文件 - name: 创建 kubelet 配置文件 template: src: kubelet-config.yaml.j2 dest: /var/lib/kubelet/config.yaml tags: configure_kubelet - name: 创建 kube-proxy 配置文件 template: src: kube-proxy-config.yaml.j2 dest: /var/lib/kube-proxy/kube-proxy-config.yaml tags: configure_kubeproxy # 创建 kubelet 和 kube-proxy 的 systemd 服务 - name: 创建 kubelet systemd 服务文件 template: src: kubelet.service.j2 dest: /etc/systemd/system/kubelet.service tags: manage_services - name: 创建 kube-proxy systemd 服务文件 template: src: kube-proxy.service.j2 dest: /etc/systemd/system/kube-proxy.service tags: manage_services # 启用并启动服务 - name: 启用 kubelet 服务 systemd: name: kubelet enabled: yes state: started tags: manage_services - name: 启用 kube-proxy 服务 systemd: name: kube-proxy enabled: yes state: started tags: manage_services # 节点加入 Kubernetes 集群并设置角色标签 - name: 设置 Kubernetes 节点角色标签 shell: \u0026#34;{{ kubectl_bin }} label node {{ inventory_hostname }} kubernetes.io/role={{ node_role }} --overwrite\u0026#34; tags: configure_node_role - name: 节点调度控制 shell: \u0026#34;{{ kubectl_bin }} cordon {{ inventory_hostname }}\u0026#34; when: node_role == \u0026#34;master\u0026#34; tags: configure_node_role templates/kubelet-config.yaml.j2 # apiVersion: kubelet.config.k8s.io/v1beta1 kind: KubeletConfiguration address: 0.0.0.0 port: 10250 readOnlyPort: 10255 clusterDNS: - {{ cluster_dns }} clusterDomain: {{ cluster_domain }} resolvConf: /run/systemd/resolve/resolv.conf ... templates/kube-proxy-config.yaml.j2 # apiVersion: kubeproxy.config.k8s.io/v1alpha1 kind: KubeProxyConfiguration mode: iptables clusterCIDR: {{ cluster_cidr }} ... templates/kubelet.service.j2 # [Unit] Description=Kubernetes Kubelet Documentation=https://kubernetes.io/docs/home/ After=network.target Requires=docker.service [Service] ExecStart={{ bin_dir }}/kubelet \\ --config={{ config_dir }}/kubelet/config.yaml \\ --kubeconfig={{ kubeconfig_path }} Restart=always RestartSec=10s [Install] WantedBy=multi-user.target templates/kube-proxy.service.j2 # [Unit] Description=Kubernetes Kube Proxy After=network.target [Service] ExecStart={{ bin_dir }}/kube-proxy \\ --config={{ config_dir }}/kube-proxy/kube-proxy-config.yaml \\ --kubeconfig={{ kubeconfig_path }} Restart=always RestartSec=10s [Install] WantedBy=multi-user.target 3. Role 的配置文件结构 # 可以在 Role 中定义一个清晰的目录结构，包括以下文件：\ntasks/main.yml：定义了整个 Role 的执行任务。 defaults/main.yml：定义该 Role 的默认变量，可以为所有的任务设置默认值。 vars/main.yml：可以为该 Role 提供更具体的变量设置。 files/：存放需要分发的静态文件。 templates/：存放 Jinja2 模板文件，用于生成动态的配置文件。 handlers/main.yml：包含一些事件触发时要执行的任务，例如服务重启。 4. 执行过程 # 当你执行这个 Role 时，Ansible 会：\n确保目标节点上所有的目录和文件已正确创建。 下载并分发 kubelet, kube-proxy 和 kubectl 等二进制文件。 配置节点所需的配置文件（包括 kubelet, kube-proxy 配置和 systemd 服务）。 启动并管理服务（kubelet, kube-proxy），确保它们在系统启动时自动启动。 在 Kubernetes 集群中为节点添加正确的标签，确保节点能够被正确调度。 总结 # 这种方式的优点是，Role 和 Task 的模块化设计使得你可以非常清晰地管理 Kubernetes 节点的加入过程，且能够重复使用和扩展。如果有多个节点加入集群，只需要在 inventory 中配置好节点信息，运行相同的 Role 即可。\n你可以根据自己的需求，修改模板文件和任务，进行个性化的配置和优化。如果你有任何更复杂的需求，或者需要具体实现某个部分，可以继续讨论和调整。\ndefaults/main.yml：定义该 Role 的默认变量，可以为所有的任务设置默认值。 vars/main.yml：可以为该 Role 提供更具体的变量设置。 谁的优先级更高 # 在 Ansible 中，defaults/main.yml 和 vars/main.yml 都是用来存储变量的文件，但它们的优先级不同：\n优先级 # defaults/main.yml：这是定义变量的默认值文件。它的优先级最低，意味着如果在其他地方（如 playbook、命令行、vars/main.yml）定义了同样的变量，defaults 中的值会被覆盖。 vars/main.yml：这是一个更具体的变量定义文件，优先级高于 defaults。如果变量在 vars/main.yml 中定义，那么 defaults 中的值会被覆盖。 优先级顺序： # Ansible 中变量的优先级从低到高为：\ndefaults 中的变量（优先级最低） vars 中的变量 playbook 中定义的变量 命令行参数定义的变量（-e） 例子 # 假设你有以下两个文件：\ndefaults/main.yml # # defaults/main.yml my_variable: \u0026#34;default_value\u0026#34; vars/main.yml # # vars/main.yml my_variable: \u0026#34;vars_value\u0026#34; 使用时 # 如果只包含 vars/main.yml，则 my_variable 会是 \u0026quot;vars_value\u0026quot;。 如果在 playbook 中覆盖变量，例如： - hosts: all vars: my_variable: \u0026#34;playbook_value\u0026#34; roles: - my_role 此时，my_variable 将会是 \u0026quot;playbook_value\u0026quot;。\n总结 # vars/main.yml 的优先级高于 defaults/main.yml，因此它会覆盖 defaults 中定义的变量值。 defaults/main.yml 主要用于定义最基础的默认值，而 vars/main.yml 用于为该 Role 提供具体或自定义的变量值。 集群多少？规模多大？用ansible部署。 # ansible变量的优先级？变量的优先级怎么去设置，同一变量可能出现在playbook的多个地方 # 你是如何用优先级去控制变量的正确性？ # 在 Ansible 中，变量的优先级是根据特定的加载顺序来决定的。理解这些优先级非常重要，因为同一个变量可能出现在多个地方，优先级决定了最终使用哪个值。\nAnsible 变量的优先级（从低到高） # Ansible 变量的优先级按照以下顺序排列，低优先级的会被高优先级的变量覆盖：\nRole 的 defaults/main.yml 文件 这是为 Role 设置的默认值，优先级最低。如果没有其他地方定义变量，默认值将被使用。 Role 的 vars/main.yml 文件 该文件优先级高于 defaults，但是低于 playbook 和命令行中的变量。 Inventory 文件中的变量 在 Ansible 的 inventory 文件（包括 host_vars 和 group_vars）中设置的变量，优先级高于 defaults 和 vars 文件，但低于 playbook 中的变量和命令行参数。 Playbook 中定义的变量 在 playbook 中直接定义的变量，优先级高于 defaults、vars、以及 inventory 中的变量。 命令行定义的变量（-e 或 --extra-vars） 使用 -e 或 --extra-vars 参数时在命令行上定义的变量，优先级最高，会覆盖所有其他来源的变量。 Facts 通过 setup 模块或 ansible_facts 获取的系统信息，也会以变量的形式传递，通常优先级较低，但依然可以影响变量的值，除非它在更高优先级的地方被覆盖。 优先级的示意图： # [Defaults] ↓ [Vars (role)] ↓ [Inventory (group_vars/host_vars)] ↓ [Playbook] ↓ [Command-line (-e)] ↓ [Facts] 优先级控制变量正确性的方法： # 1. 适当使用默认值 # 在 defaults/main.yml 中设置变量的默认值，确保如果没有提供变量时，系统会使用合理的默认值。这是保持变量正确性的一种方式。\n# defaults/main.yml my_var: \u0026#34;default_value\u0026#34; 2. 覆盖默认值 # 可以在 vars/main.yml 中根据实际需要调整变量，覆盖默认值。使用 vars/main.yml 可以为某个 Role 提供具体的值。\n# vars/main.yml my_var: \u0026#34;role_specific_value\u0026#34; 3. Playbook 中定义变量 # 在 Playbook 中可以根据具体的主机、组、或任务来定义变量，这样可以灵活调整每个节点的配置。\n- hosts: all vars: my_var: \u0026#34;playbook_specific_value\u0026#34; roles: - my_role 4. 使用命令行变量 # 如果你希望在运行 Playbook 时指定变量的值，可以使用 -e 参数来定义，这样可以覆盖其他所有地方的定义。\nansible-playbook -e \u0026#34;my_var=command_line_value\u0026#34; playbook.yml 5. 结合条件判断 # 通过 when 判断来控制变量的赋值，以此来确保在某些条件下使用特定的变量值。\n- name: Set value based on condition set_fact: my_var: \u0026#34;specific_value\u0026#34; when: ansible_facts[\u0026#39;os_family\u0026#39;] == \u0026#34;Debian\u0026#34; 6. 优先级的注意事项 # 避免混乱：如果多个地方都设置了同一个变量，要清楚地知道哪个地方会最终生效。可以通过 -v 或 -vvv 来查看变量的实际值。 严格管理 vars 和 defaults：一般来说，defaults 是用来设置默认值的，而 vars 用于更具体的配置。如果在 vars 中设置的值依赖于 defaults 中的值，确保 vars 中不会覆盖 defaults 中设置的合理值。 优先级应用场景： # 跨主机的变量管理：通过 inventory 和 group_vars 配置变量，可以确保某个组的所有主机使用同一套配置，而不同组的主机配置可以有不同的值。 Role 内的变量控制：Role 的 defaults 和 vars 用于区分默认值和具体配置。当同一变量在多个地方设置时，优先级顺序可以帮助你控制最终的值。 灵活覆盖变量：在执行 Playbook 时，可以使用命令行中的 -e 参数来动态覆盖变量值，这对于一些临时的配置变更非常有用。 如何控制变量的正确性 # 文档化变量：确保所有的变量都有明确的文档说明，并且告诉团队成员如何覆盖默认值，避免不必要的覆盖。 合理使用 defaults 和 vars：不要随便在 playbook 中覆盖变量，尽量使用 defaults 和 vars 来管理变量，这样可以保证统一性。 调试和检查变量：使用 debug 模块输出变量的值，特别是在复杂的 Role 和 Playbook 中，可以帮助你调试和验证变量的正确性。 - name: Print the value of my_var debug: var: my_var 避免冗余变量定义：同一变量不应该出现在多个地方，否则容易混淆。确保变量的定义在合适的位置。 通过合理的管理和优先级控制，你可以有效避免变量值混乱的问题，确保 Playbook 的可预测性和正确性。\nplaybook的role存在多个子的role，那么如何让控制好他的执行顺序？ # 在 Ansible 中，如果一个 Playbook 包含多个子 Role，可以通过以下几种方式控制它们的执行顺序：\n1. Role 执行顺序：Role 顺序是基于 Playbook 中的定义顺序来决定的 # Playbook 中 Role 的执行顺序是按照它们在 roles 部分的出现顺序来执行的，Ansible 会按顺序逐个执行各个 Role。如果你有多个子 Role，Ansible 会依次执行这些 Role，直到所有的 Role 执行完成。\n示例：\n- hosts: all roles: - role: role_A - role: role_B - role: role_C 在这个例子中，Role role_A 会首先执行，然后是 role_B 和 role_C。Ansible 会按照顺序执行它们。\n2. 使用 dependencies 控制 Role 的执行顺序 # 如果一个 Role 依赖于另一个 Role，Ansible 提供了 dependencies 字段，可以在 Role 中明确指定依赖关系。通过这种方式，你可以控制 Role 的执行顺序，确保某些 Role 在其他 Role 之前执行。\n示例： 假设 role_B 依赖 role_A，可以在 role_B 中通过 meta/main.yml 文件指定依赖。\nrole_B/meta/main.yml:\ndependencies: - role: role_A 这样，Ansible 会首先执行 role_A，然后执行 role_B。\n3. 在 Playbook 中使用 pre_tasks 和 post_tasks 控制执行顺序 # pre_tasks 和 post_tasks 可以让你在 Role 执行之前或之后执行任务，从而间接控制 Role 的执行顺序。\npre_tasks：在 Role 执行之前执行任务。 post_tasks：在 Role 执行之后执行任务。 示例：\n- hosts: all pre_tasks: - name: Run some task before roles shell: echo \u0026#34;This runs before the roles\u0026#34; roles: - role_A - role_B post_tasks: - name: Run some task after roles shell: echo \u0026#34;This runs after the roles\u0026#34; 在这个示例中，pre_tasks 在 Role 执行之前执行，post_tasks 在所有 Role 执行之后执行。\n4. 利用 tags 控制 Role 的执行顺序 # 如果你想选择性地运行某些 Role，可以通过 tags 来控制。你可以在 Playbook 中为不同的 Role 或任务设置不同的标签，并在命令行运行时指定要运行的标签。\n示例：\n- hosts: all roles: - role: role_A tags: [role_A] - role: role_B tags: [role_B] 在执行时，你可以指定只运行某些 Role：\nansible-playbook playbook.yml --tags \u0026#34;role_A\u0026#34; 注意：这个方法并不能控制 Role 执行的顺序，主要用于选择性地执行特定的 Role。\n5. 条件判断来控制执行顺序 # 有时你希望根据某些条件来决定 Role 是否执行，可以使用 when 语句在 Playbook 中对 Role 进行条件控制。\n示例：\n- hosts: all roles: - role: role_A when: ansible_facts[\u0026#39;os_family\u0026#39;] == \u0026#39;Debian\u0026#39; - role: role_B when: ansible_facts[\u0026#39;os_family\u0026#39;] == \u0026#39;RedHat\u0026#39; 通过这种方式，某个 Role 只有在符合特定条件时才会执行。\n6. 使用 block 来组合多个 Role # 你还可以使用 block 将多个 Role 分组，并通过控制块内的顺序来间接控制 Role 执行的顺序。\n示例：\n- hosts: all tasks: - block: - role: role_A - role: role_B when: ansible_facts[\u0026#39;os_family\u0026#39;] == \u0026#39;Debian\u0026#39; - block: - role: role_C - role: role_D when: ansible_facts[\u0026#39;os_family\u0026#39;] == \u0026#39;RedHat\u0026#39; 这样，你可以根据条件执行不同的 Role 顺序。\n7. 使用 serial 来控制并发执行 # 如果你在多台主机上执行 Playbook，可以通过 serial 控制每次执行的主机数，进而间接控制每个主机上 Role 的执行顺序。\n示例：\n- hosts: all serial: 1 # 每次只在一个主机上执行 roles: - role_A - role_B 总结 # 默认顺序：Role 的执行顺序由 Playbook 中的定义顺序决定。 dependencies：通过 Role 内的 meta/main.yml 来控制 Role 之间的依赖关系。 pre_tasks 和 post_tasks：在执行 Role 前后插入任务。 tags：控制 Role 的选择性执行，适用于调试或特定环境。 条件控制（when）：根据条件控制 Role 的执行。 block：将多个 Role 组合在一起，控制执行顺序。 serial：控制并发执行的主机数，可以间接控制执行顺序。 通过上述方法，你可以灵活地控制 Ansible 中多个 Role 的执行顺序，从而确保配置和部署的正确性。\nmeta用过吗？role task里面main里面第一个yaml，还有个meta 在执行主的yaml前去调用这个的预先的，执行meta的外部扩展的子 # 是的，meta/main.yml 文件在 Ansible Role 中的作用是用来定义 Role 的元数据，并且通常用于在 Role 中管理依赖关系（即 Role 之间的依赖），以及一些其他的角色级别的配置。\nmeta/main.yml 的常见用途： # 管理 Role 的依赖：最常见的用途是定义 Role 之间的依赖关系。在 meta/main.yml 中通过 dependencies 字段，可以指定当前 Role 依赖的其他 Role。这样，Ansible 会在执行当前 Role 之前自动执行这些依赖的 Role。 执行其他任务：你可以通过 meta 来指定一些任务，在 Role 执行之前或之后执行。meta 文件中的内容会被自动调用，但它本身并不是一个任务文件。 1. Role 依赖配置 # 通过 meta/main.yml 文件中的 dependencies 字段，可以指定当前 Role 依赖其他的 Role。这些依赖的 Role 会在当前 Role 执行之前被执行。\n示例： 假设 role_A 依赖 role_B，那么你可以在 role_A 中通过 meta/main.yml 来指定 role_B 作为依赖。\nrole_A/meta/main.yml:\ndependencies: - role: role_B 在这种情况下，当你在 Playbook 中调用 role_A 时，Ansible 会先执行 role_B，然后再执行 role_A。\n2. meta 在 Role 中的执行 # meta/main.yml 中的内容不会直接被作为 Playbook 中的任务执行。它的作用是用于 Role 的元数据定义，特别是依赖关系的定义，或者在特殊情况下用于调整 Role 执行的方式。例如，你可以用它来指定当 Role 执行失败时是否停止执行。\n示例： role_A/meta/main.yml:\ndependencies: - role: role_B 在上面的例子中，role_A 会先依赖 role_B，然后再执行它自己的任务。\n3. 如何调用 meta 里的内容 # meta 文件通常不需要在 Playbook 中直接调用。它是 Ansible 在执行 Role 时自动读取并执行的部分。通常，你不会在 Playbook 中手动调用 meta/main.yml，但会通过定义 Role 的依赖关系等，间接影响 Role 的执行顺序。\n注意事项： # meta/main.yml 中定义的 dependencies 会在执行当前 Role 之前自动执行，因此你无需手动指定执行顺序。 meta 文件中的内容主要用来进行 Role 之间的依赖管理，确保在执行某些任务之前，其他必要的任务（比如依赖的 Role）已经完成。 4. meta/main.yml 示例 # 一个典型的 meta/main.yml 文件看起来可能是这样的：\ndependencies: - role: common - role: apache - role: mysql 在这个例子中，meta/main.yml 定义了当前 Role 需要依赖三个其他的 Role：common、apache 和 mysql，这些依赖的 Role 会在当前 Role 执行之前自动被执行。\n总结： # meta/main.yml 主要用于管理 Role 之间的依赖关系。 你不能在 Playbook 中直接调用 meta/main.yml，它是自动调用的，用来处理 Role 的执行顺序、依赖关系等。 dependencies 字段是 meta/main.yml 中的核心功能，它会确保依赖的 Role 在当前 Role 执行前被执行。 通过正确地使用 meta，你可以使多个 Role 之间的依赖关系变得更加明确，确保它们按顺序和正确的方式执行。\n除了用ansible部署k8s外上层的软件如何部署，其他类似云原生的应用会部署哪些？ # 除了使用 Ansible 部署 Kubernetes（K8s）外，上层的软件部署通常涉及云原生应用及相关的工具和服务。云原生应用通常具有容器化、微服务架构、持续集成与交付等特点，因此在 Kubernetes 上的部署会包括以下几类软件和工具：\n1. 云原生应用部署 # 云原生应用通常是基于微服务架构构建的，部署时往往会用到容器化技术（Docker）和容器编排平台（Kubernetes）。除了 Kubernetes，常见的云原生应用和工具有：\n1.1 微服务应用 # Spring Boot\n、\nNode.js\n、\nGo\n等常见框架的微服务应用。\n使用 Docker 容器化，将服务打包成容器镜像。 通过 Helm charts 或直接部署到 Kubernetes 上，管理其生命周期（部署、扩容、升级等）。 1.2 服务网格 (Service Mesh) # Istio\n、\nLinkerd\n、\nConsul\n等服务网格可以在 Kubernetes 上帮助实现服务之间的管理和通信、流量控制、安全加密、故障恢复等功能。\n它们提供了微服务之间的服务发现、负载均衡、路由、限流、熔断等功能。 部署时，通常是通过 Helm charts 安装，或者直接在 Kubernetes 中创建相关的资源定义。 1.3 CI/CD 工具链 # Jenkins\n、\nGitLab CI\n、\nArgoCD\n等工具用于自动化构建、测试、部署应用。\nJenkins 可以通过 Kubernetes 插件直接与 Kubernetes 集群交互，动态创建 Pod 执行构建任务。 ArgoCD 用于 GitOps 模式下的持续交付，可以自动将 Git 仓库中的 YAML 文件同步到 Kubernetes 中。 1.4 监控与日志 # Prometheus\n、\nGrafana\n：用于监控和可视化 Kubernetes 集群及应用的性能。\nPrometheus 收集监控指标，Grafana 用来展示这些指标。 Elasticsearch\n、\nLogstash\n、\nKibana (ELK Stack)\n：用于日志收集、存储、分析和可视化。\n将应用和 Kubernetes 中的日志集中到 ELK Stack，便于查看和分析。 1.5 消息队列与流处理 # Kafka\n、\nRabbitMQ\n、\nNATS\n等消息队列系统，适用于微服务之间的异步通信。\nKafka 经常与 Kubernetes 配合使用，在容器化环境下作为事件驱动架构的核心组件。 Apache Flink、Apache Pulsar：用于大数据流处理，能够处理来自多源的数据流，适合实时分析。\n1.6 数据库与缓存 # MySQL、PostgreSQL、MariaDB 等传统关系型数据库。\nRedis、Memcached：用于缓存，提升数据访问速度。\nCassandra\n、\nMongoDB\n、\nClickHouse\n等 NoSQL 数据库。\n这些数据库系统可以通过 StatefulSets 或 Helm charts 部署到 Kubernetes 中，并配置高可用和备份策略。 1.7 API 网关 # Kong\n、\nTraefik\n、\nNGINX\n等 API 网关，通常用于处理微服务之间的流量管理、身份验证、限流、负载均衡等功能。\n在 Kubernetes 上，通常将这些 API 网关部署为 Ingress 控制器，负责外部流量到集群内部服务的路由。 1.8 存储与卷管理 # Ceph\n、\nGlusterFS\n、\nRook\n等分布式存储解决方案，能够为 Kubernetes 提供持久存储卷（Persistent Volumes）。\nRook 通过在 Kubernetes 集群中运行 Ceph 或其他存储系统，简化了存储的部署和管理。 1.9 容器注册中心 # Harbor\n、\nDocker Registry\n：用于存储和管理容器镜像。\nHarbor 提供了企业级的镜像管理，支持身份认证、访问控制和审计日志。 1.10 Web 应用和服务 # WordPress\n、\nJenkins\n、\nNextcloud\n等 Web 应用通常也部署在 Kubernetes 上，利用 Helm charts 或直接使用 Kubernetes 配置文件（YAML）进行管理。\nKubernetes 可以为这些 Web 应用提供高可用性、扩容、自动恢复等特性。 2. 多集群管理 # 随着企业的云基础设施复杂化，通常需要管理多个 Kubernetes 集群。为了简化管理，可以部署一些跨集群的工具：\nRancher：提供多集群的管理，支持跨数据中心、跨云环境的 Kubernetes 集群统一管理。 Federation：Kubernetes 的原生工具，允许跨多个集群的资源同步和共享。 3. 容器编排与调度 # Kubernetes Operator：用于管理和自动化应用的生命周期管理。Operator 通过自定义控制器和 CRD（Custom Resource Definition）扩展 Kubernetes API，用于自动化管理特定应用（如数据库、消息队列等）的状态和操作。 4. 安全管理与身份认证 # HashiCorp Vault\n：用于管理敏感数据，如数据库密码、API 密钥等。\nKubernetes 可以与 Vault 集成，用于提供动态的密钥和凭证管理。 OPA (Open Policy Agent)：用于执行策略管理，例如权限管理、安全控制等。\n5. 自动化运维和弹性伸缩 # Helm：Kubernetes 的包管理工具，类似于 Linux 的 apt 或 yum，Helm 提供了一个标准化的方式来打包和管理 Kubernetes 应用。 Keda：用于基于事件的自动伸缩，它通过事件驱动的方式来动态扩展 Kubernetes 上的应用。 Argo Workflows：可以在 Kubernetes 上运行复杂的工作流和任务调度。 总结 # 除了 K8s 作为容器编排平台，还涉及许多其他云原生应用和工具的部署，包括服务网格、CI/CD 工具、消息队列、监控系统、API 网关等。这些工具通常都需要部署在 Kubernetes 上，利用 Kubernetes 的容器化、自动化、扩展能力来提高应用的可管理性、可伸缩性、可靠性和安全性。\n通过使用 Ansible 或其他自动化工具，我们可以将这些应用部署到 Kubernetes 集群中，并通过定义相关的配置文件（如 Helm charts、YAML 文件等）来实现集群内的服务部署、管理和调度。\nhelm用吗？成品helm部署的模版有个人去完成过吗？ # 是的，Helm 是 Kubernetes 的一个非常常用的包管理工具，类似于 Linux 的 apt 或 yum，用于简化应用的安装和管理。通过 Helm，用户可以使用现成的 Helm charts 来快速部署应用，或者根据需求创建自定义的 Helm charts 来进行部署。\nHelm Charts 的使用 # 成品 Helm Charts：\n很多常见的云原生应用（如 Redis、Nginx、Prometheus、Grafana 等）都已经有成品的 Helm charts，可以直接使用这些现成的 charts 来快速部署应用。 这些 charts 通常已经由社区或厂商进行了优化，并提供了很多可以配置的参数，能够支持不同环境下的需求。 自定义 Helm Charts：\n如果有更复杂的应用需求，可以自己编写 Helm charts。 Helm charts 包含了 values.yaml、templates/ 等文件，可以定义 Kubernetes 资源的创建逻辑、应用配置、环境变量等。 用户可以根据自己的需求自定义模板文件，结合配置文件对应用进行精细化部署。 创建和管理 Helm Charts：\n创建 Helm charts 包含以下几个步骤： 使用 helm create 命令生成基本的 Helm chart 结构。 编辑 values.yaml 文件，定义应用的配置项。 编辑 templates/ 文件夹中的 Kubernetes YAML 模板，包含 Deployment、Service、Ingress 等资源定义。 使用 helm install 部署应用，使用 helm upgrade 进行更新。 可通过 helm repo 命令管理 Helm chart 仓库，方便下载和发布 charts。 这些步骤可以通过自动化工具（如 Ansible）来完成，尤其是在多环境、多集群部署时，Helm charts 配合 CI/CD 工具能够帮助高效管理。\nHelm 的优势 # 简化部署：Helm 将复杂的 Kubernetes 部署转化为简单的命令和配置，避免了手动编写大量 YAML 文件。 版本管理：Helm charts 版本化，使得应用的升级和回滚变得更加简单。 扩展性：通过 values.yaml 文件，Helm 可以支持根据不同的配置进行灵活的调整，支持多种部署场景。 如果你有使用过 Helm 或想要尝试编写和管理 Helm charts，通常会用它来管理生产环境中容器化的应用，尤其是涉及到微服务架构的应用。\n内部的调优，后期运维工作体现在哪？ # 在内部的调优和后期运维中，主要的工作体现通常可以分为以下几个方面：\n1. 性能优化 # 资源调度和管理：根据应用的需求和系统的承载能力，合理分配 CPU、内存、存储等资源。比如在 Kubernetes 中，可以调整 Pod 的 requests 和 limits，设置合理的资源配额。 网络优化：确保网络延迟和带宽充足，避免由于网络瓶颈造成的性能问题。这可能涉及负载均衡的配置优化（如调整 Nginx 或 HAProxy 的配置）、CNI 插件的优化等。 存储优化：如果使用了分布式存储（如 Ceph 或 GlusterFS），需要对存储的 I/O 性能进行调优，确保存储层不会成为瓶颈。 数据库优化：对数据库进行索引优化、查询优化、缓存策略调整等工作。对 MySQL、PostgreSQL、Redis 等数据库进行性能调优，确保它们能够高效运行。 2. 高可用性和容错性设计 # 监控与告警：通过工具如 Prometheus 和 Grafana，进行系统和应用的监控，确保能够实时了解系统的健康状况。设置合理的告警规则，及时响应故障。 高可用架构：确保系统的高可用性，例如 Kubernetes 中配置 Pod 的副本数、使用自动故障转移机制（如 PodDisruptionBudgets 和 Affinity 策略）、确保服务在单点故障下不影响整体系统。 容灾与备份：定期备份重要数据，确保在灾难恢复时可以快速恢复。例如，定期备份 etcd 数据，并确保备份的存储位置和频率合理。 3. 自动化运维 # 自动化部署与管理：使用 Ansible、Helm 等工具进行应用和基础设施的自动化管理。通过 CI/CD 流水线（如 Jenkins、GitLab CI、ArgoCD 等）实现应用的自动化部署和更新。 自动化修复：配置 Kubernetes 中的自愈机制，如 Pod 重启策略、自动扩缩容等，确保在节点或服务出现故障时，系统能够自动进行恢复。 日志集中化与分析：部署 ELK Stack（Elasticsearch、Logstash、Kibana）或 Fluentd 等日志收集系统，集中管理和分析日志，提前发现系统异常。 4. 安全性加强 # 身份和访问控制：确保系统的访问控制策略合理，如通过 Kubernetes RBAC（角色访问控制）控制对集群资源的访问权限，配置合适的用户权限。 安全补丁管理：及时更新操作系统、容器引擎、Kubernetes、应用程序等的安全补丁，减少被攻击的风险。可以通过定期的 CVE 漏洞扫描和更新机制来确保安全性。 加密和审计：对敏感数据进行加密存储，启用通信加密（如使用 TLS 加密客户端与服务端之间的通信），并通过 Kubernetes 审计日志功能，记录和监控访问行为。 5. 问题排查与故障处理 # 日志分析：通过日志分析工具（如 ELK、Fluentd 等），快速定位问题并采取措施。比如，系统出现异常时，可以通过日志查询和分析，找到根本原因并修复。 性能瓶颈排查：利用工具如 top、iotop、netstat、vmstat 等诊断系统的性能瓶颈。Kubernetes 中可以使用 kubectl top 来查看 Pod 和节点的资源使用情况，找出资源使用过高的服务或 Pod。 故障恢复：当出现故障时，运维人员需要快速响应，找到故障源并及时恢复。比如，在 Kubernetes 集群中，可以使用 kubectl describe pod \u0026lt;pod-name\u0026gt; 查找 Pod 的详细信息，查看是否是资源不足、节点故障等问题。 6. 版本升级与迁移 # Kubernetes 升级：随着 Kubernetes 的版本更新，需要定期对集群进行升级。运维人员需要确保升级过程顺利，避免中断业务，通常包括对 K8s 控制平面、工作节点及应用的逐步升级。 应用版本控制与升级：对于应用的升级，运维需要确保平滑迁移，避免停机。可以通过 Helm 部署的应用管理版本和更新，确保应用在升级过程中高可用。 7. 成本优化 # 资源利用率监控：通过监控和分析资源的使用情况，找到资源浪费和瓶颈。例如，查看哪些 Pod 的资源使用率过低，哪些资源过度消耗，进而优化资源分配。 弹性伸缩：使用 Kubernetes 的 HPA（Horizontal Pod Autoscaler）或 VPA（Vertical Pod Autoscaler）来根据负载动态扩展或缩减服务实例数量，减少不必要的资源浪费。 总结： # 运维的工作不仅仅是对系统的监控和管理，更多的是从各个方面（性能、可用性、安全性、成本等）不断优化系统，确保它在高效、安全的情况下稳定运行。通过合理的架构设计、自动化工具、监控报警等手段，可以提前发现和解决问题，确保服务的持续可用性和稳定性。\nk8s的集群的驱逐怎么做的？被动驱逐，为了不要被驱逐，可能主机上已有应用被驱逐，不让他被驱逐，解决不要被驱逐的问题， # 提前扩容，做一些考量， # Kubernetes 的驱逐（Eviction）机制主要是针对 资源短缺 的情况，目的是保证集群能够在资源紧张的情况下，保持关键服务的可用性。下面我会详细讲解 Kubernetes 驱逐的工作原理、被动驱逐的发生情况，以及如何防止某些 Pod 被驱逐。\n1. Kubernetes 驱逐机制 # Kubernetes 中的 驱逐 是指当节点的资源（如 CPU、内存、磁盘空间）紧张时，系统会尝试停止某些非关键性的 Pod，以释放资源，从而确保更重要的 Pod 能继续运行。\n驱逐的触发条件： # 节点资源不足：如节点的内存、CPU 或磁盘空间不足，Kubernetes 会自动进行驱逐。\nPod 优先级和 QoS (Quality of Service) 分类\n：Kubernetes 根据 Pod 的优先级和 QoS 分类来决定哪些 Pod 会被驱逐。Pod 被分为 3 个 QoS 等级：\nGuaranteed：如果 Pod 为 Guaranteed 类型，Kubernetes 会尽量避免驱逐它，因为它的资源请求和限制是固定的。 Burstable：对于这种类型的 Pod，Kubernetes 会尝试驱逐那些资源消耗不合适的实例。 BestEffort：资源请求和限制都未定义，这些 Pod 是最容易被驱逐的。 被动驱逐： # Memory Pressure：当节点的内存占用过高时，Kubernetes 会根据 Pod 的 QoS 等级进行驱逐。 Disk Pressure：当磁盘空间不足时，Kubernetes 会驱逐那些磁盘占用量较大的 Pod。 PID Pressure：当进程数量过多时，系统可能会驱逐部分 Pod。 Kubernetes 通过 kubelet 监听和监控节点上的资源情况，遇到资源不足时，会根据驱逐策略，选择符合条件的 Pod 进行驱逐。\n2. 防止 Pod 被驱逐 # 如果你不希望某些 Pod 被驱逐，可以通过以下几种方式来确保它们的优先级更高，或者让它们具备更高的容忍能力：\n(1) Pod 优先级和抢占 # Kubernetes 提供了 Pod 优先级 和 抢占 的机制，允许用户设置某些 Pod 的优先级。高优先级的 Pod 在资源紧张时会抢占低优先级 Pod 的资源。\n设置 Pod 优先级：可以通过 PriorityClass 来为 Pod 设置优先级。\napiVersion: scheduling.k8s.io/v1 kind: PriorityClass metadata: name: high-priority value: 1000000 globalDefault: false description: \u0026#34;High priority pods\u0026#34; 然后在 Pod 配置中引用：\napiVersion: v1 kind: Pod metadata: name: my-app spec: priorityClassName: high-priority 抢占机制：如果一个高优先级的 Pod 需要资源而资源不足时，它会自动驱逐低优先级的 Pod 来争取资源。\n(2) 资源请求与限制（Requests and Limits） # Kubernetes 中的 资源请求（requests）和 资源限制（limits）非常重要。如果 Pod 没有设置适当的资源请求和限制，可能会被 Kubernetes 判定为 BestEffort 类型，优先被驱逐。\n通过设置合理的资源请求和限制，可以避免 Pod 被驱逐：\napiVersion: v1 kind: Pod metadata: name: my-app spec: containers: - name: my-container image: my-image resources: requests: memory: \u0026#34;256Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; limits: memory: \u0026#34;512Mi\u0026#34; cpu: \u0026#34;1000m\u0026#34; requests：表示容器启动时所需的最低资源。 limits：表示容器使用资源的最大限制。 (3) Node Affinity 与 Taints/Tolerations # Node Affinity：通过节点亲和性（Node Affinity），你可以让 Pod 只调度到特定的节点上，从而避免某些节点资源紧张时对特定 Pod 的影响。\napiVersion: v1 kind: Pod metadata: name: my-app spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: \u0026#34;disktype\u0026#34; operator: In values: - \u0026#34;ssd\u0026#34; Taints/Tolerations：节点可以通过 taints 来标记，表示该节点资源紧张或某些原因不适合调度某些 Pod。Pod 可以通过 tolerations 来容忍节点的 Taints，避免驱逐。\n给节点打上 taint：\nkubectl taint nodes \u0026lt;node-name\u0026gt; key=value:NoSchedule 给 Pod 配置 toleration，允许调度到有 taint 的节点：\napiVersion: v1 kind: Pod metadata: name: my-app spec: tolerations: - key: \u0026#34;key\u0026#34; operator: \u0026#34;Equal\u0026#34; value: \u0026#34;value\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; (4) PodDisruptionBudgets (PDB) # 如果你不希望某些 Pod 被驱逐（尤其是在进行计划的维护操作时），可以使用 PodDisruptionBudget 来定义在驱逐发生时必须保留的 Pod 最小数量。\napiVersion: policy/v1 kind: PodDisruptionBudget metadata: name: my-app-pdb spec: minAvailable: 1 selector: matchLabels: app: my-app 这个配置确保在进行计划性的驱逐时，至少会保留一个副本运行，避免服务不可用。\n3. 如何避免主机应用被驱逐 # 如果主机上已经有关键应用运行，而你希望避免这些应用被驱逐，可以通过以下方式保障它们的稳定性：\n为关键应用设置高优先级：如前所述，可以设置 PriorityClass 来确保关键应用具有较高优先级，避免在资源不足时被驱逐。 合理设置资源请求和限制：确保应用的资源请求和限制合理，避免 Pod 被判定为 BestEffort 类型而被驱逐。 使用 PodDisruptionBudget：确保关键服务能够在驱逐操作中得到保障，避免在进行节点维护或其他操作时应用出现中断。 节点和 Pod 的调度策略：通过 Node Affinity、Taints/Tolerations 等手段，确保关键服务调度到适当的节点，避免节点资源压力过大导致驱逐。 总结： # Kubernetes 的驱逐机制主要是在节点资源紧张时，通过释放一些 Pod 来保证关键应用的稳定性。通过合理配置资源请求和限制、Pod 优先级、Taints/Tolerations 等手段，可以确保关键应用在资源紧张时不会被驱逐，从而保证集群的稳定运行。\nkafka ，zk，脑裂怎么解决？ # Kafka 和 Zookeeper 都是分布式系统中的重要组件，分别负责消息队列和集群协调。脑裂（Split Brain）指的是在分布式系统中，因网络分区或其他原因，多个节点之间无法正常通信，导致集群中的不同部分各自认为自己是主集群，出现不一致性问题。\n在 Kafka 和 Zookeeper 中，脑裂问题可能会导致集群不可用或数据不一致。解决脑裂的关键是确保系统中的各个节点在网络异常时能够正确地判断和处理。下面我们分别讨论 Kafka 和 Zookeeper 中的脑裂问题及其解决方法。\n1. Kafka 中的脑裂问题及解决方法 # 在 Kafka 中，脑裂问题主要表现为 Leader 选举和 副本同步 问题。Kafka 中的每个分区都有一个 Leader 和多个 Follower，当分区的 Leader 所在节点和 Follower 所在节点之间出现网络分区时，会发生脑裂。\nKafka 中脑裂的表现： # Leader 副本失联：某些 Broker 节点无法连接到 Leader 副本，导致它们认为自己是 Leader，或者 Follower 没法同步数据。 数据丢失：当 Follower 节点无法及时同步 Leader 节点的数据时，可能会出现数据丢失。 不可用的分区：一些分区的副本可能无法获得一致的数据，导致这些分区的消费者和生产者无法正常工作。 Kafka 的脑裂防范和解决： # 确保 Zookeeper 的稳定性： Kafka 集群的元数据存储在 Zookeeper 中，Zookeeper 用于管理 Kafka 中的 Broker 节点、分区的 Leader、副本等。Zookeeper 的稳定性至关重要，Zookeeper 需要有奇数个节点，并且推荐至少使用 3 个节点来保证一致性和容错能力。\n配置 min.insync.replicas 参数： 在 Kafka 中，min.insync.replicas 参数用于保证分区的数据副本数量。在脑裂的情况下，若 min.insync.replicas 设置为较大的值，即使部分副本失联，Kafka 也不会允许写入，从而避免数据丢失。\nmin.insync.replicas=2 unclean.leader.election.enable 配置： 默认情况下，Kafka 在发生脑裂时会禁用“不干净的 Leader 选举”，即禁止在某些副本可能尚未同步的情况下，选举新的 Leader。这可以避免在网络恢复后，由于旧的 Leader 丢失了一部分数据而导致数据不一致。\nunclean.leader.election.enable=false 配置合理的 acks 参数： Kafka 提供了 acks 参数来控制生产者确认消息写入的行为：\nacks=1：Leader 接收到消息后就返回确认。 acks=all 或 acks=-1：所有副本都收到消息后才返回确认，这样可以保证数据的一致性，即使部分节点失联也能保证消息不会丢失。 通过 Kafka 自动恢复机制： Kafka 集群在发生网络分区后，会自动进行 Leader 选举，确保分区能够继续处理消息。但为了减少脑裂发生的概率，应该确保 Kafka 集群的网络稳定性和 Zookeeper 节点的健康性。\n2. Zookeeper 中的脑裂问题及解决方法 # Zookeeper 是一个分布式协调服务，用于 Kafka 集群的管理。Zookeeper 本身也可能出现脑裂问题，尤其是在网络分区的情况下。\nZookeeper 中脑裂的表现： # 多个 Zookeeper 节点选举为 Leader：当 Zookeeper 集群出现网络分区时，可能会导致某些 Zookeeper 节点认为自己是集群的 Leader，从而引发不一致的操作。 数据不一致：网络分区会导致 Zookeeper 中的数据同步不及时，造成不同 Zookeeper 节点上的数据不一致。 Zookeeper 的脑裂防范和解决： # 保证 Zookeeper 集群节点的数量为奇数： Zookeeper 集群必须配置为奇数个节点（通常是 3 或 5 个节点）。这可以确保在集群出现网络分区时，至少有一部分节点可以形成“多数派”来选举出 Leader，避免脑裂的情况。 quorum 配置： Zookeeper 通过配置 quorum 参数来控制哪些节点可以参与选举。如果网络出现分区，只有在至少半数以上节点可用的情况下，才会选举出 Leader。 initLimit 和 syncLimit 配置： 这些配置项用于控制 Zookeeper 集群节点之间的心跳检测： initLimit：指定 Zookeeper 集群启动过程中，Follower 节点与 Leader 节点之间初始化连接所允许的最大时间。 syncLimit：指定 Zookeeper 集群在正常工作过程中，Follower 节点与 Leader 节点之间通信的最大延迟。 通过调整这些参数，可以减少网络延迟和分区带来的影响。 zk.tickTime 配置： tickTime 用于指定 Zookeeper 中的一个基本时间单位。通过调整 tickTime 和 maxClientCnxns 等参数，可以增强 Zookeeper 对网络分区的容忍度。 使用合适的 Zookeeper 版本： 新版本的 Zookeeper 修复了许多网络分区时的稳定性问题，因此推荐使用较新的 Zookeeper 版本，以确保在发生脑裂的情况下，能够尽可能地减少不一致性和数据丢失。 3. 总结：Kafka 与 Zookeeper 的脑裂问题解决 # Kafka： 配置合理的 min.insync.replicas、acks 和 unclean.leader.election.enable 参数，避免在脑裂时出现数据丢失。 确保 Kafka 集群中的网络稳定性，以及与 Zookeeper 的连接健康。 Kafka 会自动进行 Leader 选举，但应确保 Zookeeper 的稳定性以避免 Kafka 出现故障。 Zookeeper： 使用奇数个 Zookeeper 节点，确保在发生网络分区时能够选举出 Leader。 配置合理的 quorum、initLimit 和 syncLimit 等参数，提高集群的容错能力。 使用新版本的 Zookeeper，以修复旧版本中存在的一些脑裂问题。 最终，避免脑裂的关键是设计高可用的集群架构，确保网络稳定性，并且为 Kafka 和 Zookeeper 配置合理的容错机制和参数。\n网络分区（Network Partition）是指分布式系统中的一部分节点无法与其他节点通信，通常是由于网络故障、路由器问题、硬件故障或其他原因导致的。网络分区会导致集群中的节点无法与其他部分的节点进行正常的通信，进而影响集群的一致性和可靠性。\n网络分区的表现： # 节点不可达：部分节点由于网络问题无法与其他节点进行通信。这些节点可能会认为自己仍然是集群的一部分，但实际上，它们无法和其他节点同步数据或协调操作。 分割的集群：网络分区可能会将集群划分为多个子集（即“脑裂”情况），每个子集内的节点认为自己是完整集群的一部分，无法与其他子集的节点协作或同步。 在 Zookeeper 中的影响： # Zookeeper 是一个分布式协调服务，它依赖一个主从架构来保证集群的一致性。在网络分区的情况下，Zookeeper 的集群可能会发生如下问题：\nLeader 节点丧失联系：Zookeeper 集群中只有一个 Leader 节点，负责处理所有的写请求和一些管理任务。在网络分区的情况下，一部分 Zookeeper 节点与 Leader 失去联系，无法得知当前 Leader 的状态，这可能导致这些节点认为它们自己可以选举出新的 Leader。 脑裂（Split Brain）：如果网络分区导致集群被分成多个子集，其中某些子集选举出自己的 Leader，这就会导致脑裂问题。在这种情况下，集群的各个部分可能认为自己是集群的唯一 Leader，并且无法与其他部分同步数据，最终导致数据不一致。 写操作不可达：由于 Leader 无法与大多数节点通信，部分客户端的写请求可能会被丢失或者无法正确处理，因为 Leader 需要与多数节点确认写入操作的有效性。 网络分区的解决方法： # 为了防止 Zookeeper 集群在网络分区时出现数据不一致和脑裂问题，通常采取以下策略：\n使用奇数个节点：Zookeeper 集群的节点数必须是奇数，通常建议使用 3、5 或 7 个节点。这样可以确保在网络分区时，至少有一个子集能够选举出 Leader，并且大多数节点可以达成一致。 选举协议：Zookeeper 使用 Zab 协议（Zookeeper Atomic Broadcast）来保证集群内的一致性。在网络分区的情况下，Zookeeper 会确保只有一个 Leader 被选举出来，避免出现多个 Leader。 quorum 配置：Zookeeper 会根据节点的多数（即大于半数的节点）来做出决策，确保集群的正确性。例如，在网络分区时，Zookeeper 只会接受来自能与大多数节点通信的节点的写入请求。 网络监控与修复：为了降低网络分区的风险，需要定期检查集群的网络状况，确保节点间的通信稳定，及时修复可能导致网络分区的问题。 调整 initLimit 和 syncLimit：这些配置项可以帮助 Zookeeper 调节在网络不稳定时的容错能力，通过设置合理的超时时间，可以增加 Zookeeper 集群在网络分区时的稳定性。 总的来说，网络分区是指分布式系统中的部分节点因网络问题无法正常通信，这会导致集群的一致性出现问题，特别是在 Zookeeper 中，可能导致脑裂现象和不一致的操作。在 Zookeeper 中，通过保证节点数为奇数、使用适当的协议和配置、监控网络健康等手段，可以有效减少网络分区的风险。\n开发能力有没有？flask的，前端页面怎么做？ # 使用 Vue.js 作为前端框架配合 Flask 后端开发应用是一个很常见的组合。Vue.js 主要负责渲染和交互，而 Flask 负责数据处理和 API 提供。\n前端：Vue.js 和 Flask 配合 # 安装 Vue.js： 首先，确保你的机器上已经安装了 Node.js 和 npm。然后使用 Vue CLI 创建一个新的 Vue 项目：\nnpm install -g @vue/cli vue create my-vue-app cd my-vue-app npm run serve 设置 Vue.js 与 Flask 交互： 在 Vue 中，可以通过 Axios 库来发送 HTTP 请求并从 Flask 后端获取数据。首先安装 Axios：\nnpm install axios 修改 Vue 组件来请求 Flask API： 在 src/components/HelloWorld.vue 文件中，修改为如下：\n\u0026lt;template\u0026gt; \u0026lt;div\u0026gt; \u0026lt;h1\u0026gt;Welcome to Vue + Flask!\u0026lt;/h1\u0026gt; \u0026lt;button @click=\u0026#34;fetchData\u0026#34;\u0026gt;Get Data\u0026lt;/button\u0026gt; \u0026lt;p\u0026gt;{{ message }}\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/template\u0026gt; \u0026lt;script\u0026gt; import axios from \u0026#39;axios\u0026#39;; export default { data() { return { message: \u0026#39;\u0026#39; }; }, methods: { fetchData() { axios.get(\u0026#39;http://localhost:5000/api/data\u0026#39;) .then(response =\u0026gt; { this.message = response.data.message; }) .catch(error =\u0026gt; { console.error(error); }); } } }; \u0026lt;/script\u0026gt; Flask 后端设置： 在 Flask 中，创建一个简单的 API 端点：\nfrom flask import Flask, jsonify app = Flask(__name__) @app.route(\u0026#39;/api/data\u0026#39;) def get_data(): return jsonify(message=\u0026#34;Data from Flask API!\u0026#34;) if __name__ == \u0026#39;__main__\u0026#39;: app.run(debug=True) 解决跨域问题（CORS）： Flask 后端和 Vue 前端通常是运行在不同的端口上，因此会涉及 跨域请求（CORS）。为了解决这个问题，可以使用 Flask-CORS 扩展：\npip install flask-cors 然后在 Flask 后端中启用 CORS：\nfrom flask import Flask, jsonify from flask_cors import CORS app = Flask(__name__) CORS(app) @app.route(\u0026#39;/api/data\u0026#39;) def get_data(): return jsonify(message=\u0026#34;Data from Flask API!\u0026#34;) if __name__ == \u0026#39;__main__\u0026#39;: app.run(debug=True) 运行应用：\n启动 Flask 后端：\npython app.py 启动 Vue 前端：\nnpm run serve 结构概览： # Flask：后端提供数据 API。 Vue.js：前端通过 Axios 向 Flask 发送请求并渲染数据。 部署： # 在部署时，通常会将 Vue 构建为静态资源，通过 Flask 提供给前端。例如，使用 Vue CLI 构建生产版本：\nnpm run build 然后将构建后的 dist 文件夹内容放到 Flask 静态目录下，Flask 会直接提供静态文件。\n你有具体的项目或者遇到的难点吗？我可以帮你进一步细化。\n是的，Flask 和 Vue.js 可以用来实现上面提到的主机管理、SSH远程登录、日志管理和主机监控功能。下面是一个大致的实现思路，结合 Flask 和 Vue.js 来构建一个完整的运维平台：\n1. 主机管理和配置管理： # Flask 后端\n：\nFlask 可以用来提供一个 REST API 来管理主机的配置，包括添加、删除主机，配置 SSH 登录信息等。通过 Flask-SocketIO 等工具，可以提供实时的更新和监控。 可以结合 Ansible 或 SaltStack 的 Python API 来进行自动化任务执行（例如批量配置和更新）。 Vue.js 前端\n：\nVue.js 负责构建一个用户界面，用于展示当前主机的状态，主机的配置管理界面，或者执行批量操作。 通过 Vue.js 发起 API 请求与 Flask 后端进行通信，例如获取主机列表、查看主机详情、执行特定的操作（如重启、配置更新等）。 使用 Vuex 管理前端状态，确保 UI 在数据变动时同步更新。 2. SSH 远程登录管理： # Flask 后端\n：\nFlask 可以提供 API 接口来管理 SSH 登录，比如存储 SSH 密钥和配置，或者通过 Flask 后端提供 API 来执行 SSH 登录。 使用 paramiko 这样的 Python 库来通过 SSH 协议与远程主机进行交互，支持执行远程命令、文件传输等。 Vue.js 前端\n：\n在前端，可以使用 WebSocket 或 Socket.IO 来进行实时的 SSH 会话管理。例如，可以为每个主机提供一个实时控制台，用户可以直接在浏览器中执行命令，获得实时反馈。 3. 日志管理与集中式日志分析： # Flask 后端\n：\nFlask 后端可以整合日志管理功能，将主机的日志通过 API 接口传输到后端进行存储和分析。 使用 ELK Stack 或 Fluentd 进行日志的收集和分析，Flask 后端提供接口将这些日志显示到前端界面。 可以使用 Python 中的 logging 模块来生成和管理日志信息，也可以集成第三方工具（如 Logstash）来收集日志数据。 Vue.js 前端\n：\n前端部分，Vue.js 可以提供日志查看的界面，展示系统日志、应用日志等，支持多维度的日志筛选和查询。 可以集成 Kibana（如果使用 ELK Stack），通过与其 API 通信展示日志数据。 4. 主机监控与报警： # Flask 后端\n：\n后端可以通过 Prometheus 或 Zabbix 提供的 API 来收集主机监控数据，如 CPU、内存、磁盘使用情况等。 可以通过 Flask 提供 API 接口，将监控数据传递给前端进行展示。 使用 Flask-SocketIO 实现实时监控更新，比如 CPU 使用率超过阈值时，Flask 后端通过 WebSocket 向前端推送警告消息。 Vue.js 前端\n：\nVue.js 前端可以展示实时的监控图表，使用 Chart.js、ECharts 或 Grafana（如果与 Grafana 集成）来渲染监控数据。 通过与后端的 API 通信，获取实时的监控数据并展示。例如，可以展示 CPU 使用率图表、内存使用情况等。 当监控数据超过阈值时，前端可以展示警报信息。 5. 自动化运维任务管理： # Flask 后端\n：\n提供任务调度和执行的功能，后端可以使用 Celery 来异步执行耗时的运维任务，如批量软件安装、配置更新等。 后端可以集成 Ansible 或 SaltStack 等自动化工具，允许通过 API 执行任务，并返回执行结果。 Vue.js 前端\n：\n提供用户界面来创建、监控和管理运维任务。用户可以通过前端界面发起任务，查看任务执行进度，获取结果。 使用 Vue Router 跳转到任务的详情页面，实时更新任务执行状态。 具体实现思路： # 后端部分（Flask）： 提供 API 接口，用于管理主机、执行 SSH 命令、日志收集与展示、监控数据收集与展示。 通过 Flask 集成自动化工具（如 Ansible），允许通过 Web 前端执行配置和运维任务。 使用 Celery 执行长时间的运维任务，并提供任务状态和结果接口。 前端部分（Vue.js）： 构建现代化的 Web 界面，展示主机状态、日志、监控数据等。 通过 Vue.js 与 Flask 后端进行数据交互，使用 Axios 来请求 API，使用 Vuex 管理全局状态。 实现实时日志查看、监控图表、任务执行状态更新等功能。 示例架构： # Flask 后端：提供 RESTful API，用于处理主机管理、SSH 登录、日志展示、监控数据等。后端也负责与自动化工具（如 Ansible）进行交互，执行运维任务。 Vue.js 前端：展示系统信息，监控数据，日志数据，以及操作界面。前端通过 API 与 Flask 后端交互，提供用户友好的操作界面。 总结： # 通过 Flask 和 Vue.js，你可以实现一个完整的运维管理平台，处理从主机管理、日志分析、远程 SSH 登录到监控和自动化任务等多个运维任务。Flask 作为后端处理业务逻辑，Vue.js 作为前端展示和交互，两者通过 API 通信，提供完整的用户体验和管理功能。\nk8s的cni用的啥，网络，扁平网络，跨层，跨墙， # 子网可能A类 B类，跨墙的这种，怎么解决 # 内部开墙，放行端口，要求能够互相访问，也可以自定义你vxlan 的udp 4789 端口号在flexconfiguration 修改vxlan port为14789也行\n换工作的原因 # 家庭原因\n南京网易外包\n自我介绍，离职原因，有无offer，多久到岗。 # 印象比较深的中间件排障的经验？ # 在我的运维经验中，排查中间件问题是非常常见的场景，尤其是在 Kubernetes 环境下，涉及到的中间件有 Kafka、Zookeeper、Redis、Nginx 等。以下是一些印象比较深的排障经验：\n1. Kafka 集群的脑裂问题 # Kafka 集群的脑裂问题可能导致集群内的部分节点误以为自己是 leader，从而影响数据的正确性和一致性。一次，我们的 Kafka 集群出现了数据不一致的问题，部分消费者无法消费消息。\n排查步骤： # 检查 Zookeeper 状态：Kafka 依赖 Zookeeper 来管理集群的元数据。通过查看 Zookeeper 的日志，发现有一个 Zookeeper 节点因网络问题无法与其他节点通信，导致 Zookeeper 集群出现了网络分区。 分析 Kafka 节点日志：Kafka 节点的日志显示有多个 leader 选举失败，部分节点进入了 LEADER_NOT_AVAILABLE 状态。 恢复 Zookeeper 集群一致性：通过手动修复 Zookeeper 集群的分区问题，确保集群的一致性，并强制重新选举 leader。 检查 Kafka 配置：检查了 Kafka 的 zookeeper.connection.timeout.ms 和 replica.lag.time.max.ms 等参数，确保集群能够快速处理节点失联后的恢复。 验证结果：恢复网络后，Kafka 集群重新稳定，所有消费者可以正常消费消息。 2. Redis 主从复制延迟问题 # 在一次生产环境中，Redis 主从复制出现了严重的延迟，主节点与从节点之间的数据同步滞后，导致读取的数据不一致。\n排查步骤： # 查看 Redis 监控指标：通过 Prometheus 和 Grafana 监控 Redis 集群，发现主节点的写入延迟急剧增加，从节点的同步延迟也非常高。\n分析 Redis 日志：通过 INFO replication 命令查看 Redis 的复制状态，发现从节点的 connected_slaves 数量是正常的，但是同步速度非常慢。\n查看网络带宽：检查了主节点和从节点之间的网络带宽，发现网络带宽已经达到瓶颈。Redis 复制是通过 AOF（Append-Only File）进行的，数据量较大时会对网络造成很大的压力。\n优化方案\n：\n降低 repl-backlog-size 和 repl-timeout，加快从节点同步。 增加网络带宽，并将 Redis 配置为 RDB（快照）同步模式，以减少复制负担。 增加 Redis 从节点数量，分担主节点的复制压力。 验证结果：优化后，Redis 主从复制的延迟大幅下降，系统恢复正常。\n3. Nginx 高并发下的资源耗尽 # 在处理高并发请求时，我们的 Nginx 服务器出现了 CPU 占用率过高，导致请求超时和部分服务不可用。\n排查步骤： # 查看 Nginx 错误日志：Nginx 错误日志显示了大量的 worker process 启动失败，进程数达到上限，导致无法处理新的请求。 查看系统资源：使用 top 命令查看服务器的 CPU 和内存占用，发现 Nginx 的 worker 进程消耗了大量的 CPU 资源。 分析 Nginx 配置：检查 Nginx 的 worker_processes 和 worker_connections 配置，发现 worker_processes 配置为 2，导致高并发时无法充分利用服务器的 CPU 核心。 调整配置：根据机器的 CPU 核心数，将 worker_processes 设置为 auto，并根据负载调整 worker_connections 数量。 优化系统配置：增加了文件句柄限制，调整了操作系统的 ulimit 参数，确保 Nginx 可以处理更多的并发连接。 验证结果：修改配置后，Nginx 的性能得到了显著提升，服务器负载得以均衡，处理高并发请求时不再出现资源耗尽的问题。 4. Zookeeper 的节点丢失 # 在一次 Zookeeper 集群的升级过程中，由于配置问题，集群的部分节点丢失，导致集群不可用。\n排查步骤： # 查看 Zookeeper 节点日志：通过查看丢失节点的日志，发现 Zookeeper 的 dataDir 配置错误，导致节点无法正常启动。 检查 Zookeeper 配置：验证了 zoo.cfg 中的 dataDir 路径和 myid 配置，发现路径配置错误，导致该节点无法在集群中正确识别。 修复配置：纠正了路径和 ID 配置，重新启动了 Zookeeper 节点，确保该节点能够加入集群。 验证结果：重新启动节点后，Zookeeper 集群恢复正常，所有节点都恢复了数据同步，集群可用性恢复。 总结 # 这些中间件的排障经历中，最重要的几个步骤通常是：\n分析日志：深入分析相关中间件的日志文件，找到错误的根本原因。 监控指标：使用 Prometheus、Grafana 等监控工具收集指标，提前发现性能瓶颈或潜在问题。 配置优化：根据问题的原因，调整中间件和系统的配置，解决性能或资源瓶颈问题。 网络排查：尤其在分布式系统中，网络问题可能会导致严重的故障，需要重点排查网络连通性和带宽。 这些经验帮助我在工作中能更快地定位和解决中间件的问题，确保系统的高可用性和稳定性。\n在 Sentinel（通常指 Redis Sentinel）配置中，如果你使用了 1 主 2 从 3 哨兵的架构，当主节点故障转移（failover）后，应用可能会出现连接问题，因为在故障转移发生时，Redis 的主节点会发生变化。哨兵会选举一个新的主节点，并通知客户端更新连接的信息。此时，如果应用没有及时响应或配置不当，就会导致连接到错误的 Redis 节点，导致连接问题。\n故障转移后应用连接问题的原因 # 客户端未更新主节点地址：应用程序未能及时获取新的主节点 IP 地址，因此继续连接到旧的主节点或从节点。 客户端未配置 Redis Sentinel：如果客户端没有配置 Redis Sentinel 来自动发现新的主节点，在故障转移发生后，客户端将无法自动切换到新的主节点。 Sentinel 配置问题：如果 Redis Sentinel 没有正确配置，或者故障转移没有成功完成，应用可能会连接到一个不健康的节点。 解决方案与恢复步骤 # 1. 确保应用客户端支持 Redis Sentinel # 应用程序需要能够与 Redis Sentinel 通信，从而在故障转移后自动重新连接到新的主节点。Redis 客户端（例如 redis-py、Jedis 等）通常提供与 Sentinel 的集成。\n更新客户端配置：确保客户端配置了 Redis Sentinel 的地址列表，而不是单个 Redis 实例的地址。客户端会向 Sentinel 查询当前的主节点信息，并根据查询结果连接到新的主节点。\n例如，使用 redis-py 客户端时，可以这样配置：\nfrom redis.sentinel import Sentinel sentinel = Sentinel([(\u0026#39;sentinel_host1\u0026#39;, 26379), (\u0026#39;sentinel_host2\u0026#39;, 26379), (\u0026#39;sentinel_host3\u0026#39;, 26379)]) master = sentinel.master_for(\u0026#39;mymaster\u0026#39;, socket_timeout=0.1) 这样，当故障转移发生时，master_for 方法会自动获取新的主节点地址。\n2. 检查 Sentinel 的状态 # 确保 Sentinel 已经成功地进行故障转移并且选举了新的主节点。你可以使用以下命令检查 Sentinel 的状态：\nredis-cli -h sentinel_host -p sentinel_port SENTINEL failover mymaster 确认新的主节点已经选举出来并且状态为 master。\n3. 验证新的主节点和从节点 # 使用 Sentinel 命令查看新的主节点：\nredis-cli -h sentinel_host -p sentinel_port SENTINEL get-master-addr-by-name mymaster 该命令返回新的主节点的 IP 和端口，确保应用连接到正确的主节点。\n同时，检查从节点状态：\nredis-cli -h sentinel_host -p sentinel_port SENTINEL slaves mymaster 确保从节点正常同步新的主节点。\n4. 确认 Redis 哨兵配置 # 检查 Redis Sentinel 配置文件，确保以下配置项正确：\nsentinel monitor mymaster \u0026lt;master_ip\u0026gt; \u0026lt;master_port\u0026gt; \u0026lt;quorum\u0026gt;：确保监视主节点的配置正确。 sentinel down-after-milliseconds mymaster \u0026lt;time_in_ms\u0026gt;：确保哨兵检测到主节点不可用的时间足够短。 sentinel failover-timeout mymaster \u0026lt;time_in_ms\u0026gt;：确保故障转移的超时时间足够合理。 5. 应用重试机制 # 为了避免在故障转移期间应用出现长时间无法连接的问题，可以在应用中增加连接重试机制，确保在主节点切换期间应用能够自动重新连接。\n例如，在客户端连接时，设置一定的重试次数和超时时间。\n6. 应用重新启动（如果需要） # 如果应用没有及时获取到新的主节点信息，重启应用程序可以强制它重新与 Redis Sentinel 进行通信，从而获取正确的主节点地址。\n总结 # 配置 Redis Sentinel 支持：确保应用支持通过 Redis Sentinel 自动发现主节点。 检查 Sentinel 和 Redis 节点状态：通过 Redis CLI 检查 Sentinel 和 Redis 节点的状态，确认故障转移完成。 重试机制：在应用中增加连接重试机制，防止连接失败。 配置优化：优化 Redis Sentinel 配置，确保故障转移高效、可靠。 通过上述步骤，应该能够恢复应用连接问题，确保 Redis 高可用性架构稳定运行。\n编程语言熟悉哪个？实现最复杂的功能？ # 我熟悉多种编程语言，并能够实现复杂的功能。以下是我熟悉的一些语言和我曾实现过的复杂功能的例子：\nPython： 复杂功能：开发过多个全栈项目，包括使用 Flask 和 Django 搭建的 web 应用，编写过大量的自动化脚本和运维工具（例如：自动化部署工具、CI/CD 流程等）。 功能实现：编写过批量处理大数据的 ETL 系统，进行大规模日志分析，并通过机器学习模型进行预测分析。 Go： 复杂功能：实现过高并发、高性能的网络应用，主要用于云原生应用和容器管理工具的开发。 功能实现：例如在 Kubernetes 环境中，使用 Go 开发过监控和日志收集系统，支持高并发的日志数据处理。 JavaScript (Vue.js 和 Node.js)： 复杂功能：实现过全栈应用，使用 Vue.js 实现前端交互，Node.js 实现后端服务，主要应用于企业级平台开发。 功能实现：设计并实现了前端数据可视化界面，结合 WebSocket 实现实时数据展示，后端使用 RESTful API 与数据库交互。 Shell 脚本： 复杂功能：编写过复杂的运维自动化脚本，涵盖系统监控、日志收集、数据备份、故障诊断等任务。 功能实现：例如通过 Shell 脚本实现自动化的系统巡检，定期进行应用的备份、日志清理和数据压缩等操作。 SQL： 复杂功能：在数据库中实现复杂的查询，进行数据分析和优化。 功能实现：编写复杂的 SQL 查询语句和存储过程，处理数据迁移、清洗、合并等工作。 Ansible： 复杂功能：开发过复杂的自动化运维流程，包括 Kubernetes 集群的自动化部署、配置管理、应用发布等。 功能实现：通过 Ansible Playbook 实现大规模系统的批量配置、软件安装、环境搭建等自动化任务。 我在这些领域的经验使我能够快速解决技术难题，并根据需求实现高效、可维护的解决方案。如果你有具体的编程需求或技术问题，欢迎提出，我可以帮助你实现！\n在使用 Flask 和 Django 搭建的 Web 应用中，我曾经实现过一些具有挑战性和功能丰富的应用。以下是我在这两个框架下实现过的一些具体功能：\n1. Flask 项目： # 运维自动化管理平台： # 功能\n：\n用户认证与权限管理：通过 Flask-Login 和 Flask-Principal 实现用户的登录、注册、权限控制等功能，支持基于角色的访问控制 (RBAC)。 多服务器管理：通过 Flask 和 Celery 实现对多台主机的远程 SSH 执行命令，进行批量操作，如批量安装软件、更新配置、重启服务等。 任务调度与监控：使用 Celery 实现任务调度，用户可以查看任务的执行状态（成功/失败），同时支持任务重试机制。 日志管理：集成日志收集系统，通过 Flask-WTF 设计表单，允许用户查看、过滤和下载日志文件。 数据分析与可视化平台： # 功能\n：\n数据上传与管理：提供用户上传各种格式（如 CSV、Excel 等）数据的功能，并对数据进行存储和管理。 数据清洗和处理：利用 Pandas 库对上传的数据进行清洗和预处理，例如填补缺失值、数据去重、异常值处理等。 数据可视化：集成 Plotly 或 Matplotlib 库，动态展示数据图表，包括柱状图、折线图、饼图等，支持用户根据不同条件选择查看数据。 报表生成与导出：用户可以自定义报表模板，生成 PDF 或 Excel 格式的报表，并进行导出。 2. Django 项目： # 企业级管理平台： # 功能\n：\n后台管理界面：使用 Django Admin 自带的管理后台快速生成用户管理、权限管理、商品管理、订单管理等功能。 API 接口：通过 Django REST Framework 实现 RESTful API，用于与前端 Vue.js 或 React 进行数据交互。 搜索与过滤：集成 Haystack 或 Django-filter 实现内容搜索和筛选功能，支持模糊查询和多条件筛选。 消息推送：通过 Django Channels 实现 WebSocket 通信，实时推送系统通知和消息。 报告生成：为用户提供生成自定义报表的功能，可以导出 PDF 或 Excel 格式的文件。 电商平台： # 功能\n：\n商品管理：实现商品的增、删、改、查功能，支持商品分类、品牌筛选、价格排序等。 用户订单管理：用户下单、支付、查看订单状态等功能，同时后台管理员可以管理订单、发货等。 支付接口集成：集成第三方支付接口（如支付宝、微信支付），处理支付过程中的回调、通知等。 购物车与结算：用户可以将商品添加到购物车，并进行结算，支持使用优惠券、积分等促销手段。 社交平台： # 功能\n：\n用户动态：用户可以发布文本、图片、视频等动态，其他用户可以评论和点赞。 关注与粉丝：用户可以关注其他用户，查看其动态，并获取相关通知。 私信系统：通过 Django Channels 实现实时私信，用户可以互相发送和接收消息。 搜索与推荐：集成 Elasticsearch 或 Haystack 实现全站内容搜索和智能推荐功能。 项目管理与协作平台： # 功能\n：\n任务管理：支持创建、分配和跟踪任务，任务支持优先级、截止日期、负责人等字段。 团队协作：用户可以组建团队，协作处理任务，支持多级权限控制。 时间追踪：记录每个任务的工作时间，生成工作报告，支持按项目或团队查看统计数据。 项目进度：可视化展示项目进度，通过甘特图或看板查看任务的完成情况。 总结： # 在这些 Web 应用中，我主要通过 Flask 和 Django 实现了以下核心功能：\n用户管理和认证； 数据管理与分析； 后台管理系统； 消息推送和实时通信； 支付和订单管理； 搜索和报表生成。 这些项目展示了我在不同领域的开发能力，无论是前端、后端、数据库，还是与第三方服务集成、实时通信等，都有涉及。如果你有类似的需求或想要讨论如何实现某些功能，可以详细说明，我很乐意提供帮助！\nk8s的operator用过吗？helm用过吗？ # 创建 Helm Chart # 接下来，使用 Helm 创建一个 Chart 来部署 Flask 应用。假设你已经安装了 Helm。\n创建一个新的 Helm Chart： helm create flask-app 这将创建一个名为 flask-app 的文件夹，并且包含了一些 Helm 默认的文件和模板。\n编辑 Helm Chart 配置： 打开 flask-app/values.yaml，并修改以下内容：\n# 修改镜像的名称和标签 image: repository: your-dockerhub-username/flask-app tag: latest pullPolicy: IfNotPresent # 修改服务端口 service: port: 80 修改 flask-app/templates/deployment.yaml： 确保 deployment.yaml 中的容器端口与 Flask 应用的端口匹配（Flask 默认是 5000），但因为我们通常希望将它暴露在 80 端口上，记得在服务的配置中进行修改。\n编辑 flask-app/templates/deployment.yaml 中的 containerPort：\nports: - containerPort: 5000 步骤 5: 部署 Flask 应用 # 使用以下命令安装 Helm Chart：\nhelm install flask-app ./flask-app 这将使用你刚刚创建的 Helm Chart 部署 Flask 应用。\n步骤 6: 检查部署状态 # 你可以使用以下命令检查部署的状态：\nkubectl get pods 查看服务的暴露方式，确保你可以通过外部访问到 Flask 应用。\nkubectl get svc 你应该能够看到一个服务正在监听 HTTP 流量，并且你的 Flask 应用已经成功部署在 Kubernetes 集群中。\n这样，你就通过 Helm 部署了一个简单的 Flask 应用。如果你有其他特定的需求或者遇到问题，欢迎继续询问！\n有什么问题问我？项目上的事情做好就行？\n生产变更多吗？晚上做？一月一次\n十来个人 每个人有各自的项目\n故障复盘怎么做的 # 故障复盘（Post-mortem analysis）是对故障发生后进行的详细分析过程，目的是找出问题的根本原因，评估响应过程，提出改进措施，以防止类似问题的再次发生。以下是一个完整的故障复盘过程，包含从问题发生到总结和改进的各个步骤：\n1. 准备工作 # 在复盘开始之前，确保以下几个条件：\n故障发生时间和日志：收集并准备故障发生时的日志、监控数据、报警信息等。 相关人员：确保相关的技术人员、管理人员以及其他参与故障处理的人都能参与复盘。 时间和地点：安排一个合适的时间和地点进行复盘，避免赶时间和干扰。 2. 回顾故障事件 # 通过收集到的资料，详细回顾故障事件：\n故障时间点：什么时候故障开始发生，什么时候恢复正常。 影响范围：哪些业务或服务受到影响，故障影响了多少用户。 报警信息：监控报警和日志的异常信息，确认系统是如何检测到故障的。 这一步骤的目的是建立故障发生的时间线，尽量重现整个故障的过程，确保每个细节都被回顾。\n3. 确定故障原因 # 通过以下方式逐步排查，找出故障的根本原因：\n系统日志：分析系统日志、应用日志、数据库日志等，查看是否有明显的错误或异常。 监控数据：查看监控平台（例如 Prometheus + Grafana、Zabbix 等）的数据，确认是否存在系统负载、内存、CPU 或磁盘等资源的瓶颈。 配置变更：检查是否有最近的配置更改，或者新部署的服务引发了故障。 网络问题：如果故障涉及到分布式架构，检查网络连接是否正常，是否存在延迟或丢包。 外部因素：是否存在外部依赖问题（如第三方服务故障）。 在此过程中，可以使用5个为什么（5 Whys）分析方法，逐步深入到根本原因。例如，故障的表象可能是服务器宕机，但深入分析可能发现是由于磁盘空间不足导致的，而磁盘空间不足是因为日志文件未被清理。\n4. 评估响应过程 # 评估团队和个人在故障发生时的响应：\n故障检测和报警：是否能够及时发现问题，报警机制是否合理？是否有足够的报警规则覆盖到关键指标？ 应急响应：在故障发生后，是否有清晰的应急预案？团队是否按照预案快速响应？响应人员是否到位？ 恢复过程：故障恢复是否及时？恢复过程中的沟通是否顺畅？是否存在人为操作失误？ 故障追踪：是否有良好的故障追踪和记录，以便后续分析？ 这一部分帮助团队了解响应中的优点与不足，为今后优化故障响应流程提供依据。\n5. 总结问题及其影响 # 根据问题发生的原因和影响，做出总结：\n问题根本原因：明确指出故障的根本原因，避免只停留在表面症状的层面。 故障的影响：估算故障带来的经济损失、用户影响、品牌影响等。可以通过一些量化指标来说明影响范围，例如停机时长、用户投诉数、服务级别协议（SLA）违约等。 6. 制定改进计划 # 基于故障复盘的结果，制定详细的改进计划，防止类似问题的发生。可能的改进措施包括：\n基础设施方面\n：\n增强硬件资源或虚拟资源，避免单点故障。 优化网络、存储和数据库架构。 引入冗余备份和自动化容错机制（例如，使用高可用、负载均衡、自动伸缩等）。 流程方面\n：\n改进故障检测机制，调整报警阈值，避免报警过多或漏报。 制定和完善应急响应流程和操作手册。 定期进行灾备演练，确保应急预案可行。 自动化方面\n：\n增加自动化运维工具，如自动化部署、自动化恢复等，减少人工干预。 增加健康检查和自动修复功能，提升系统自愈能力。 团队方面\n：\n进行团队的故障响应和沟通能力培训，确保团队成员能够在压力下高效协作。 分析团队在故障中的不足，提升团队协作能力。 7. 改进措施的执行 # 将制定的改进措施落实到实际工作中：\n制定任务计划：为每个改进措施指定责任人、时间节点和具体执行步骤。 执行改进计划：按照计划执行，确保每个改进点得以实施。 跟踪执行情况：定期检查执行进展，确保改进措施得以有效落地。 8. 撰写故障复盘报告 # 在故障复盘结束后，撰写详细的故障复盘报告，报告内容一般包括：\n故障时间线：详细列出故障发生的时间点、影响范围和恢复过程。 故障原因分析：总结故障的根本原因和具体技术细节。 响应评估：评估团队的响应过程，指出问题与改进点。 改进计划：列出改进措施和具体的执行步骤。 结论：总结经验教训，鼓励团队持续改进。 复盘报告可以分享给相关的团队和管理层，确保所有人都能从故障中学习。\n9. 后续跟进 # 故障复盘后，需要进行持续跟进，确保所有的改进措施得到实施，并在未来进行验证：\n定期回顾：在后续的例会上回顾已实施的改进措施是否有效。 复盘新故障：每次新的故障发生后，继续进行复盘，并对改进措施的效果进行评估。 总结 # 故障复盘的目的是通过对故障事件的深入分析，发现问题根本原因，改进现有流程和系统架构，提升团队应对故障的能力，确保类似问题不再发生。复盘的关键在于透明、客观和系统的分析，确保团队能够从每一次故障中吸取经验教训。\n"},{"id":196,"href":"/docs/%E9%9D%A2%E8%AF%950308-mian-shi-0308/","title":"面试0308 2024-04-03 14:57:53.576","section":"Docs","content":" 根据nginx日志 过滤nginx前十访问量的ip # 要根据Nginx日志过滤出前十访问量的IP，你可以使用awk、sort和head命令组合来完成这个任务。假设你的Nginx访问日志格式如下（这是Nginx的默认格式）： # bash复制代码 127.0.0.1 - - [10/Oct/2023:14:05:01 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 612 \u0026#34;-\u0026#34; \u0026#34;curl/7.68.0\u0026#34; 在这个日志中，第一个字段是客户端的IP地址。 # 你可以使用以下命令来过滤出前十访问量的IP： # bash复制代码 awk \u0026#39;{print $1}\u0026#39; access.log | sort | uniq -c | sort -nr | head -n 10 cat /var/log/nginx/access.log |awk \u0026#39;{print $1}\u0026#39;|sort|uniq -c |sort -nr|head -n 10 这个命令的解释如下： # awk '{print $1}' access.log：使用awk命令从access.log文件中提取每行的第一个字段（即IP地址）。 # sort：对提取出的IP地址进行排序。 # uniq -c：统计每个唯一IP地址的出现次数。 # sort -nr：按数字进行逆序排序，这样访问量最多的IP会排在最前面。 # head -n 10：只显示前10行，即访问量最多的前10个IP。 # 请注意，这个命令假设你的Nginx日志文件名是access.log。如果你的日志文件名不同，请相应地替换文件名。此外，这个命令也假设你的日志格式与上面所示的默认格式相匹配。如果你的日志格式不同，你可能需要调整awk命令中的字段选择器。 # awk的 $nf啥意思 # 在awk命令中，$n是一个字段引用，其中n是一个数字，表示当前行的第n个字段。awk默认使用空白字符（通常是空格或制表符）作为字段分隔符，因此$1、$2、$3等分别代表每行的第一个、第二个、第三个字段，依此类推。 # 在处理Nginx的访问日志时，通常每个字段都是由特定的分隔符（如空格或特殊字符）分隔的。例如，在Nginx的默认日志格式中： # 复制代码 127.0.0.1 - - [10/Oct/2023:14:05:01 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 612 \u0026#34;-\u0026#34; \u0026#34;curl/7.68.0\u0026#34; $1 通常是客户端的IP地址。 # $2 通常是身份验证的用户名（在这个例子中是 -，表示没有用户名）。 # $3 通常是用户的身份（通常也是 -）。 # $4 是请求时间。 # $5 是请求行（例如 \u0026quot;GET / HTTP/1.1\u0026quot;）。 # $6 是响应状态码（例如 200）。 # $7 是响应体的大小（例如 612）。 # 后续的字段可能包括引用页、用户代理等。 # 当你使用awk '{print $1}' access.log时，你正在告诉awk打印出每一行的第一个字段，即客户端的IP地址。 # awk也允许你通过-F选项来指定一个不同的字段分隔符。例如，如果Nginx日志使用不同的分隔符，你可以这样指定： # bash复制代码 awk -F\u0026#39; \u0026#39; \u0026#39;{print $1}\u0026#39; access.log 这里的-F' '告诉awk使用空格作为字段分隔符。但是，请注意，Nginx日志中的时间戳通常包含在方括号内，这可能会影响字段的计数。如果时间戳是一个字段，并且你想跳过它，你可能需要使用更复杂的awk脚本或者不同的方法来处理日志。\n如果你需要处理更复杂的日志格式，你可能需要调整awk脚本，以正确解析和提取所需的字段。例如，你可以使用正则表达式来匹配和提取特定的字段，或者调整字段分隔符以匹配日志中的实际格式。\nbash 正常返回啥 # 在 Bash 脚本或命令行中，一个命令“正常返回”通常意味着该命令执行成功并且没有错误。当命令成功执行时，它会返回一个退出状态码（exit status code），这个状态码通常是一个介于 0 到 255 之间的整数。在 Unix 和 Linux 系统中，约定俗成的是： # 退出状态码 0 表示命令执行成功。 # 非零的退出状态码表示命令执行出错或遇到了某种问题。不同的非零状态码通常用于指示不同类型的错误。 # 你可以使用 $? 这个特殊变量来获取上一个命令的退出状态码。例如： # #!/bin/bash echo \u0026#34;Running a command...\u0026#34; ls /nonexistentdirectory echo \u0026#34;Exit status: $?\u0026#34; 在这个脚本中，ls /nonexistentdirectory 命令会尝试列出一个不存在的目录的内容，这将导致一个错误，并返回一个非零的退出状态码。然后，echo \u0026quot;Exit status: $?\u0026quot; 会打印出这个状态码。 # 通常，你不需要手动检查每个命令的退出状态码，因为 Bash 脚本会在遇到非零退出状态码时停止执行（除非你在脚本中使用了错误处理机制，如 set -e）。但是，在编写需要处理错误或根据不同退出状态码采取不同动作的复杂脚本时，了解和使用退出状态码是非常重要的。 # 此外，有些命令可能会返回特定的非零状态码来表示不同类型的错误。例如，grep 命令在找不到匹配项时会返回 1，而在遇到其他错误时会返回不同的值。因此，理解你正在使用的命令的文档，以了解它们返回的不同状态码的含义，也是非常重要的。 # k8s创建pod的流程 # 1. 客户端提交 Pod 的配置信息(可以是 yaml 文件定义的信息)到 kube-apiserver. 2. Apiserver 收到指令后,通知 controllr-manager 创建一个资源对象 3. controller-manager 通过 apiserver 将 pod 的配置信息存储到 ETCD 数据中心中 4. kube-scheduler 检查到 pod 信息会开始调度预选,会先过滤不符合 Pod 资源配置要求的节点,然后开始调度调优,主要是挑选出更适合运行的 pod 节点,然后将 pod 的资源配置单发送到 node 节点上的 kubelet 组件上 5. kubelet 根据 scheduler 发来的资源配置单运行 pod,运行成功后,将 pod 的运行的信息返回 scheduler, scheduler 将返回的 pod 运行状况的信息存储到 etcd 数据中心 dockerfile的copy和add的区别 # Dockerfile中的COPY和ADD指令都用于将文件或目录从构建上下文复制到容器中，但它们之间存在一些关键区别： 解压缩功能： COPY指令仅用于复制本地文件到镜像，不涉及解压缩操作。 ADD指令除了复制文件外，还具有自动解压缩的功能。如果源路径为URL地址或压缩文件，ADD会尝试自动解压缩文件到目标路径。这意味着使用ADD时，如果源文件是压缩格式（如tar.gz），它会在复制过程中自动解压。 源文件来源： COPY指令只能从执行docker build所在的主机上读取资源并复制到镜像中。 ADD指令除了能够复制本地文件和目录外，还支持通过URL从远程服务器读取资源并复制到镜像中。这使得ADD在需要从外部源获取文件时非常有用。 目录创建： ADD指令在复制文件时，如果目标路径不存在，则会自动创建目标路径。而COPY指令没有这个自动创建路径的功能。 最佳实践： 由于COPY指令更简单且直接，通常推荐在只需要复制本地文件到容器中的情况下使用COPY，这样可以避免意外的解压缩行为，也更符合直觉。 multistage构建： 在multistage构建的场景中，COPY指令具有特定的用法。它可以将前一阶段构建的产物拷贝到另一个镜像中。这是ADD指令所不具备的功能。 综上所述，COPY和ADD指令在Dockerfile中各有其独特之处，选择使用哪个指令取决于具体的需求和场景。在大多数情况下，如果只需要复制本地文件且不需要解压缩或远程获取文件，COPY指令是更好的选择。然而，如果需要从远程URL获取文件或自动解压缩压缩文件，那么ADD指令将更为合适。 helm chart的包怎么打？ # Helm Chart包的打包过程可以通过以下步骤完成： 安装Helm工具。 创建一个新的目录，作为Helm包的根目录。 在根目录中创建一个Chart.yaml文件，用于存储Helm包的元数据。 在根目录中创建一个templates目录，用于存储Kubernetes资源的模板文件。这些模板文件可以包括各种Kubernetes资源的定义，例如Deployment、Service等。 在templates目录中编写需要的Kubernetes资源模板文件。这些文件使用Go模板语言编写，可以根据需要引用values.yaml文件中的变量。 使用Helm工具将模板文件打包成Chart包。这可以通过执行helm package \u0026lt;chart_path\u0026gt;命令完成，其中\u0026lt;chart_path\u0026gt;是包含Chart.yaml和templates目录的Helm包路径。 打包完成后，你就得到了一个Helm Chart包，它包含了所有用于部署Kubernetes应用的必要文件和模板。你可以将这个包发布到Helm Chart仓库中，以便其他人使用，或者使用Helm工具将其安装到Kubernetes集群中。 请注意，这只是一个基本的打包过程，实际的步骤可能会根据你的具体需求和Helm的版本有所不同。因此，在打包Helm Chart包之前，建议查阅Helm的官方文档或相关教程以获取更详细和准确的信息。 http的504错误 # 504错误代表网关超时 （Gateway timeout），是指服务器作为网关或代理，但是没有及时从上游服务器收到请求。这通常意味着上游服务器已关闭（不响应网关 / 代理），而不是上游服务器和网关/代理在交换数据的协议上不一致。 首先，了解什么是网关。 网络的基本概念： 客户端:应用 C/S（客户端/服务器） B/S（浏览器/服务器） 服务器：为客户端提供服务、数据、资源的机器 请求：客户端向服务器索取数据 响应：服务器对客户端请求作出反应，一般是返回给客户端数据 在这之中，把nginx或Apache作为网关。一般服务的架构是：用PHP则是nginx+php的一系列进程，Apache+tomcat+JVM。 网关超时就与nginx或Apache配置的超时时间，和与php线程、java线程的响应时间有关。以nginx与PHP为例：它的超时配置fastcgi_connect_timeout、fastcgi_send_timeout、fastcgi_read_timeout。nginx将请求丢给PHP来处理，某个PHP的线程响应时间假如是10s，在10s内没有响应给nginx就报超时。这时可以打开PHP慢日志记录，然后排查之。 另外，数据库的慢查询也会导致504 。nginx只要进程没有死，一般不是nginx的问题。假如场景是：确定程序执行是正确的，比如向数据库插入大量数据，需要5分钟，nginx设置的超时时间是3分钟。这时候可以将超时时间临时设置为大于5分钟。 "},{"id":197,"href":"/docs/2025-2-11-%E9%9D%A2%E8%AF%950211/","title":"面试2025-02-11","section":"Docs","content":" 504错误是啥 # 504 错误（Gateway Timeout）是一个 HTTP 状态码，表示网关或代理服务器超时。让我详细解释一下：\n主要原因： # 服务器响应超时\n上游服务器（如应用服务器）没有在预定时间内响应网关/代理服务器 可能是因为服务器负载过高或处理请求时间过长 网络问题\n网关与上游服务器之间的网络连接存在问题 网络拥堵或延迟过高 常见场景： # 使用 Nginx 作为反向代理时，后端服务响应太慢 数据库查询操作耗时过长 服务器资源不足（CPU、内存等） 网络带宽不足 解决方案： # 服务器端\n增加超时设置时间 优化后端代码性能 增加服务器资源 使用缓存减少处理时间 配置示例 如果使用 Nginx，可以调整超时设置：\nhttp { proxy_connect_timeout 300s; proxy_send_timeout 300s; proxy_read_timeout 300s; } 客户端 实现请求重试机制 添加适当的错误处理 考虑使用断路器模式 504 错误通常是临时性的，如果持续发生，建议检查服务器日志并进行相应优化。\n常见的状态码 # 让我为你介绍最常见的 HTTP 状态码：\n2XX - 成功响应 # 200 OK\n最常见的成功状态码 请求成功且正常返回结果 201 Created\n创建资源成功 通常在 POST 请求后返回 204 No Content\n请求成功但无返回内容 常用于删除操作 3XX - 重定向 # 301 Moved Permanently\n永久重定向 资源已永久移动到新位置 302 Found\n临时重定向 资源临时移动到新位置 304 Not Modified\n资源未修改 可使用缓存版本 4XX - 客户端错误 # 400 Bad Request\n请求语法错误 服务器无法理解请求 401 Unauthorized\n未授权 需要身份认证 403 Forbidden\n禁止访问 服务器拒绝请求 404 Not Found\n资源未找到 请求的资源不存在 429 Too Many Requests\n请求过多 超出频率限制 5XX - 服务器错误 # 500 Internal Server Error\n服务器内部错误 通常是代码异常 502 Bad Gateway\n网关错误 上游服务器返回无效响应 503 Service Unavailable\n服务不可用 服务器暂时无法处理请求 504 Gateway Timeout\n网关超时 上游服务器响应超时 使用建议 # 在开发 API 时应合理使用状态码 确保错误响应包含有用的错误信息 在前端要对不同状态码进行适当处理 示例代码（前端处理）：\nfetch(\u0026#39;api/data\u0026#39;) .then(response =\u0026gt; { switch(response.status) { case 200: return response.json(); case 401: // 跳转到登录页 window.location.href = \u0026#39;/login\u0026#39;; break; case 403: throw new Error(\u0026#39;没有权限访问\u0026#39;); case 404: throw new Error(\u0026#39;资源不存在\u0026#39;); default: throw new Error(\u0026#39;服务器错误\u0026#39;); } }) .catch(error =\u0026gt; { console.error(\u0026#39;请求失败:\u0026#39;, error); }); 这些是最基本和常见的 HTTP 状态码，良好地使用它们可以帮助客户端更好地理解请求的处理结果。\nk8s创建pod的过程 # 让我为你详细解释一下 Kubernetes 创建 Pod 的完整过程：\n1. 提交请求阶段 # 用户提交请求\n通过 kubectl 或 API 提交 Pod 创建请求 API Server 接收请求并进行认证和授权 API Server 处理\n验证 Pod 配置的合法性 将 Pod 信息持久化到 etcd 生成 Pod 的唯一标识符 2. 调度阶段 # Scheduler 监听\nScheduler 通过 Watch 机制发现新的未调度 Pod 调度算法执行\n预选（Predicate）\n筛选符合基本条件的节点 检查资源是否满足 检查节点亲和性/污点容忍 优选（Priority）\n对预选节点进行打分 考虑负载均衡 选择最优节点 更新调度结果\n将选中节点信息更新到 Pod 对象 API Server 将更新后的 Pod 信息存储到 etcd 3. 创建阶段 # Kubelet 监听\n节点上的 Kubelet 发现被调度到本节点的 Pod 容器运行时准备\n准备容器运行环境 下载容器镜像 配置容器网络 创建容器\n按顺序创建 Init 容器 创建业务容器 配置健康检查 4. 运行阶段 # 状态监控\nKubelet 持续监控 Pod 状态 执行配置的探针检查 更新 Pod 状态到 API Server 生命周期管理\n处理容器重启 执行生命周期钩子 资源清理 示例 Pod 配置： # apiVersion: v1 kind: Pod metadata: name: example-pod labels: app: example spec: containers: - name: main-container image: nginx:latest ports: - containerPort: 80 resources: requests: memory: \u0026#34;64Mi\u0026#34; cpu: \u0026#34;250m\u0026#34; limits: memory: \u0026#34;128Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; readinessProbe: httpGet: path: / port: 80 initialDelaySeconds: 5 periodSeconds: 10 livenessProbe: httpGet: path: / port: 80 initialDelaySeconds: 15 periodSeconds: 20 nodeSelector: kubernetes.io/os: linux 可能的问题和解决方案： # 镜像拉取失败\n检查镜像名称是否正确 确认镜像仓库访问权限 配置 imagePullSecrets 资源不足\n调整资源请求和限制 增加节点资源 优化节点调度 网络问题\n检查网络插件状态 验证网络策略配置 排查 DNS 解析问题 理解这个过程对于排查 Pod 创建问题和优化部署策略非常重要。每个阶段都可能出现问题，了解创建流程有助于快速定位和解决问题。\nnginx 优化的措施\n让我为你介绍 Nginx 的主要优化措施：\n1. 基础配置优化 # # nginx.conf 主要优化配置 worker_processes auto; # 根据 CPU 核心数自动设置 worker_rlimit_nofile 65535; # 增加工作进程文件描述符限制 events { use epoll; # 使用 epoll 事件模型 worker_connections 65535; # 增加每个工作进程的最大连接数 multi_accept on; # 开启一次接受多个连接 } http { # 开启 gzip 压缩 gzip on; gzip_comp_level 5; gzip_min_length 1k; gzip_types text/plain text/css application/javascript application/json; # 缓存相关 open_file_cache max=1000 inactive=20s; open_file_cache_valid 30s; open_file_cache_min_uses 2; open_file_cache_errors on; # 客户端优化 client_max_body_size 20m; # 上传文件大小限制 keepalive_timeout 65; # 保持连接超时时间 client_body_buffer_size 128k; # 请求体缓冲区大小 } 2. 静态文件优化 # 启用缓存控制 location ~* \\.(jpg|jpeg|png|gif|ico|css|js)$ { expires 30d; # 设置缓存过期时间 add_header Cache-Control public; access_log off; # 关闭访问日志 } 启用 sendfile http { sendfile on; # 启用零拷贝 tcp_nopush on; # 优化数据包传输 tcp_nodelay on; # 禁用 Nagle 算法 } 3. 负载均衡优化 # upstream backend { least_conn; # 最少连接数算法 server backend1.example.com max_fails=3 fail_timeout=30s; server backend2.example.com max_fails=3 fail_timeout=30s; keepalive 32; # 保持后端连接数 } server { location / { proxy_pass http://backend; proxy_http_version 1.1; proxy_set_header Connection \u0026#34;\u0026#34;; proxy_next_upstream error timeout http_500; # 故障转移 } } 4. SSL 优化 # http { ssl_session_cache shared:SSL:10m; ssl_session_timeout 10m; ssl_protocols TLSv1.2 TLSv1.3; ssl_prefer_server_ciphers on; ssl_ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256; # OCSP Stapling ssl_stapling on; ssl_stapling_verify on; resolver 8.8.8.8 8.8.4.4 valid=300s; } 5. 安全优化 # server { # 隐藏版本号 server_tokens off; # XSS 防护 add_header X-XSS-Protection \u0026#34;1; mode=block\u0026#34;; # 点击劫持防护 add_header X-Frame-Options \u0026#34;SAMEORIGIN\u0026#34;; # 限制请求频率 limit_req_zone $binary_remote_addr zone=one:10m rate=1r/s; location /login/ { limit_req zone=one burst=5; } } 6. 日志优化 # http { # 自定义日志格式 log_format main \u0026#39;$remote_addr - $remote_user [$time_local] \u0026#34;$request\u0026#34; \u0026#39; \u0026#39;$status $body_bytes_sent \u0026#34;$http_referer\u0026#34; \u0026#39; \u0026#39;\u0026#34;$http_user_agent\u0026#34; \u0026#34;$http_x_forwarded_for\u0026#34;\u0026#39;; # 缓冲日志写入 access_log /var/log/nginx/access.log main buffer=32k flush=1m; # 对特定资源关闭日志 location /static/ { access_log off; } } 7. 监控指标 # 配置 stub_status 模块监控 Nginx 状态： location /nginx_status { stub_status on; access_log off; allow 127.0.0.1; deny all; } 优化建议： # 定期监控\n监控连接数 监控请求延迟 监控错误率 监控资源使用 性能测试\n使用 ab、wrk 等工具进行压力测试 根据测试结果调整配置 定期维护\n清理日志文件 更新 SSL 证书 检查配置是否最优 这些优化措施需要根据实际情况选择性使用，不是所有的优化都适用于所有场景。建议在测试环境验证后再应用到生产环境。\ngitlab的runner 干嘛的 # GitLab Runner 是 GitLab CI/CD 的执行器，主要负责运行 CI/CD 流水线中定义的作业（Jobs）。让我详细解释一下：\n1. Runner 的主要功能 # 执行 CI/CD 作业\n运行测试 构建应用 部署服务 自动化任务 支持多种执行方式\nShell 执行器 Docker 执行器 Kubernetes 执行器 虚拟机执行器 2. Runner 类型 # 共享 Runner\n所有项目可用 由 GitLab 管理员配置 适合通用构建任务 特定 Runner\n专门为特定项目服务 由项目维护者配置 适合特殊需求的项目 3. 配置示例 # 以下是一个典型的 .gitlab-ci.yml 配置文件示例：\nstages: - build - test - deploy build-job: stage: build script: - echo \u0026#34;编译代码...\u0026#34; - mvn package artifacts: paths: - target/*.jar test-job: stage: test script: - echo \u0026#34;运行测试...\u0026#34; - mvn test deploy-job: stage: deploy script: - echo \u0026#34;部署应用...\u0026#34; - kubectl apply -f pod.yaml only: - master 4. Runner 在 Kubernetes 中的配置 # 如果使用 Kubernetes 执行器，需要配置类似这样的 Runner：\napiVersion: apps/v1 kind: Deployment metadata: name: gitlab-runner namespace: gitlab spec: replicas: 1 selector: matchLabels: app: gitlab-runner template: metadata: labels: app: gitlab-runner spec: containers: - name: gitlab-runner image: gitlab/gitlab-runner:latest args: - run volumeMounts: - name: config mountPath: /etc/gitlab-runner - name: docker-sock mountPath: /var/run/docker.sock volumes: - name: config configMap: name: gitlab-runner-config - name: docker-sock hostPath: path: /var/run/docker.sock 5. Runner 注册配置 # Runner 需要注册到 GitLab 才能使用，配置示例：\ngitlab-runner register \\ --non-interactive \\ --url \u0026#34;https://gitlab.example.com/\u0026#34; \\ --registration-token \u0026#34;PROJECT_REGISTRATION_TOKEN\u0026#34; \\ --executor \u0026#34;docker\u0026#34; \\ --docker-image alpine:latest \\ --description \u0026#34;docker-runner\u0026#34; \\ --tag-list \u0026#34;docker,aws\u0026#34; \\ --run-untagged=\u0026#34;true\u0026#34; \\ --locked=\u0026#34;false\u0026#34; 6. 常见使用场景 # 自动化测试\n单元测试 集成测试 代码质量检查 自动化构建\n编译代码 构建 Docker 镜像 生成制品 自动化部署\n部署到测试环境 部署到生产环境 更新 K8s 配置 7. 最佳实践 # 资源管理\n合理设置并发数 配置作业超时时间 清理构建缓存 安全考虑\n使用安全的执行器 限制 Runner 权限 保护敏感信息 性能优化\n使用缓存加速构建 选择合适的执行器 优化构建脚本 GitLab Runner 是 CI/CD 流程中的重要组件，合理配置和使用可以大大提高开发团队的效率。\njenkins常用插件 # 让我介绍一下 Jenkins 最常用和最实用的插件：\n1. 基础功能插件 # Credentials Plugin\n凭证管理 存储密码、密钥、Token 等 支持多种凭证类型 Git Plugin\nGit 代码仓库集成 支持分支、标签管理 代码检出配置 Pipeline\n流水线支持 支持 Jenkinsfile 可视化流水线编辑器 2. 构建相关插件 # Maven Integration // Jenkinsfile 示例 pipeline { agent any tools { maven \u0026#39;Maven 3.8.1\u0026#39; } stages { stage(\u0026#39;Build\u0026#39;) { steps { sh \u0026#39;mvn clean package\u0026#39; } } } } Gradle Plugin // Jenkinsfile 示例 pipeline { agent any stages { stage(\u0026#39;Build\u0026#39;) { steps { gradle \u0026#39;clean build\u0026#39; } } } } 3. 容器相关插件 # Docker Plugin // Jenkinsfile 示例 pipeline { agent { docker { image \u0026#39;maven:3.8.1-jdk-8\u0026#39; } } stages { stage(\u0026#39;Build\u0026#39;) { steps { sh \u0026#39;mvn package\u0026#39; } } } } Kubernetes Plugin // Jenkinsfile 示例 pipeline { agent { kubernetes { yaml \u0026#39;\u0026#39;\u0026#39; apiVersion: v1 kind: Pod spec: containers: - name: maven image: maven:3.8.1-jdk-8 command: - cat tty: true \u0026#39;\u0026#39;\u0026#39; } } stages { stage(\u0026#39;Build\u0026#39;) { steps { container(\u0026#39;maven\u0026#39;) { sh \u0026#39;mvn package\u0026#39; } } } } } 4. 代码质量插件 # SonarQube Scanner // Jenkinsfile 示例 pipeline { agent any stages { stage(\u0026#39;SonarQube Analysis\u0026#39;) { steps { withSonarQubeEnv(\u0026#39;SonarQube\u0026#39;) { sh \u0026#39;mvn sonar:sonar\u0026#39; } } } } } Checkstyle // Jenkinsfile 示例 pipeline { agent any stages { stage(\u0026#39;Code Analysis\u0026#39;) { steps { sh \u0026#39;mvn checkstyle:checkstyle\u0026#39; } post { always { recordIssues(tools: [checkStyle()]) } } } } } 5. 通知类插件 # Email Extension Plugin // Jenkinsfile 示例 pipeline { agent any post { failure { emailext ( subject: \u0026#34;构建失败: ${env.JOB_NAME} [${env.BUILD_NUMBER}]\u0026#34;, body: \u0026#34;\u0026#34;\u0026#34;\u0026lt;p\u0026gt;构建失败:\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;查看: \u0026lt;a href=\u0026#39;${env.BUILD_URL}\u0026#39;\u0026gt;${env.JOB_NAME} [${env.BUILD_NUMBER}]\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026#34;\u0026#34;\u0026#34;, to: \u0026#34;team@example.com\u0026#34; ) } } } 钉钉通知插件 // Jenkinsfile 示例 pipeline { agent any post { always { dingtalk ( robot: \u0026#39;钉钉机器人TOKEN\u0026#39;, type: \u0026#39;MARKDOWN\u0026#39;, title: \u0026#39;构建通知\u0026#39;, text: [ \u0026#34;### 构建结果: ${currentBuild.result}\u0026#34;, \u0026#34;- 项目: ${env.JOB_NAME}\u0026#34;, \u0026#34;- 构建号: ${env.BUILD_NUMBER}\u0026#34;, \u0026#34;- 详情: [查看](${env.BUILD_URL})\u0026#34; ] ) } } } 6. 其他实用插件 # Timestamper\n为控制台输出添加时间戳 方便调试和日志分析 AnsiColor\n支持控制台彩色输出 提高日志可读性 Build Timeout\n// Jenkinsfile 示例 pipeline { agent any options { timeout(time: 1, unit: \u0026#39;HOURS\u0026#39;) } stages { stage(\u0026#39;Build\u0026#39;) { steps { sh \u0026#39;mvn package\u0026#39; } } } } 安装建议 # 按需安装\n避免安装过多不必要的插件 定期清理未使用的插件 版本兼容\n注意插件之间的版本兼容性 定期更新插件到最新稳定版 性能考虑\n插件过多会影响 Jenkins 性能 建议只保留必要的插件 这些插件能够显著提升 Jenkins 的功能和使用体验，但建议根据实际需求选择安装。\n声明式和脚本式的jenkins的写法举例 # 让我介绍几种 Jenkins 运行多个任务的方案：\n1. 并行执行 (Parallel) # pipeline { agent any stages { stage(\u0026#39;并行任务\u0026#39;) { parallel { stage(\u0026#39;任务A\u0026#39;) { steps { sh \u0026#39;mvn test -Dtest=TestA\u0026#39; } } stage(\u0026#39;任务B\u0026#39;) { steps { sh \u0026#39;mvn test -Dtest=TestB\u0026#39; } } stage(\u0026#39;任务C\u0026#39;) { steps { sh \u0026#39;docker build -t imageA .\u0026#39; } } } } } } 2. 矩阵构建 (Matrix) # pipeline { agent any matrix { axes { axis { name \u0026#39;PLATFORM\u0026#39; values \u0026#39;linux\u0026#39;, \u0026#39;windows\u0026#39;, \u0026#39;mac\u0026#39; } axis { name \u0026#39;BROWSER\u0026#39; values \u0026#39;chrome\u0026#39;, \u0026#39;firefox\u0026#39;, \u0026#39;safari\u0026#39; } } stages { stage(\u0026#39;测试\u0026#39;) { steps { sh \u0026#34;run-tests.sh ${PLATFORM} ${BROWSER}\u0026#34; } } } } } 3. 多分支流水线 # pipeline { agent any stages { stage(\u0026#39;构建\u0026#39;) { when { anyOf { branch \u0026#39;main\u0026#39; branch \u0026#39;develop\u0026#39; branch \u0026#39;feature/*\u0026#39; } } steps { sh \u0026#39;mvn clean package\u0026#39; } } stage(\u0026#39;部署开发环境\u0026#39;) { when { branch \u0026#39;develop\u0026#39; } steps { sh \u0026#39;deploy-to-dev.sh\u0026#39; } } stage(\u0026#39;部署生产环境\u0026#39;) { when { branch \u0026#39;main\u0026#39; } steps { sh \u0026#39;deploy-to-prod.sh\u0026#39; } } } } 4. 上游/下游任务 (Upstream/Downstream) # pipeline { agent any stages { stage(\u0026#39;触发下游任务\u0026#39;) { steps { build job: \u0026#39;downstream-job-1\u0026#39;, wait: false build job: \u0026#39;downstream-job-2\u0026#39;, wait: false parallel ( \u0026#34;任务1\u0026#34;: { build job: \u0026#39;parallel-job-1\u0026#39; }, \u0026#34;任务2\u0026#34;: { build job: \u0026#39;parallel-job-2\u0026#39; } ) } } } post { success { build job: \u0026#39;notification-job\u0026#39;, parameters: [string(name: \u0026#39;STATUS\u0026#39;, value: \u0026#39;SUCCESS\u0026#39;)] } } } 5. 参数化构建矩阵 # pipeline { agent any parameters { choice( name: \u0026#39;ENVIRONMENT\u0026#39;, choices: [\u0026#39;dev\u0026#39;, \u0026#39;test\u0026#39;, \u0026#39;prod\u0026#39;], description: \u0026#39;选择部署环境\u0026#39; ) booleanParam( name: \u0026#39;RUN_TESTS\u0026#39;, defaultValue: true, description: \u0026#39;是否运行测试\u0026#39; ) } stages { stage(\u0026#39;并行任务\u0026#39;) { parallel { stage(\u0026#39;部署\u0026#39;) { steps { sh \u0026#34;deploy-to-${params.ENVIRONMENT}.sh\u0026#34; } } stage(\u0026#39;测试\u0026#39;) { when { expression { params.RUN_TESTS } } steps { sh \u0026#39;run-tests.sh\u0026#39; } } } } } } 6. 资源分配和限制 # pipeline { agent none stages { stage(\u0026#39;并行构建\u0026#39;) { parallel { stage(\u0026#39;Java构建\u0026#39;) { agent { label \u0026#39;java-builder\u0026#39; } steps { sh \u0026#39;mvn clean package\u0026#39; } } stage(\u0026#39;Node构建\u0026#39;) { agent { label \u0026#39;node-builder\u0026#39; } steps { sh \u0026#39;npm install \u0026amp;\u0026amp; npm run build\u0026#39; } } } } } options { parallelsAlwaysFailFast() } } 最佳实践建议： # 资源管理\n合理设置并发数 注意资源使用限制 避免资源竞争 错误处理\n设置超时限制 合理处理任务失败 配置重试机制 性能优化\n合理使用并行执行 避免不必要的等待 优化构建顺序 监控和通知\n配置构建状态通知 监控资源使用情况 记录构建历史 通过这些方式，你可以根据具体需求选择合适的多任务执行方案，提高构建效率和资源利用率。\n怎么确保密码 这些不会明文写到流水线里面 # 在 Jenkins 中，有几个关于敏感信息输出的重要注意点：\n1. 默认行为 # 凭据变量 通过 credentials() 绑定的变量默认是被 Jenkins 保护的 即使使用 echo $PASSWORD 也会显示为 **** 环境变量 普通环境变量不会自动被掩码 需要特别注意不要直接打印 2. 安全处理示例 # pipeline { agent any environment { // 这样定义的凭据会自动被保护 DB_CREDS = credentials(\u0026#39;db-credentials\u0026#39;) // 普通环境变量需要注意保护 API_KEY = \u0026#39;sensitive-value\u0026#39; } stages { stage(\u0026#39;安全示例\u0026#39;) { steps { script { // 错误示例 - 不要这样做 // sh \u0026#34;echo $API_KEY\u0026#34; // 会暴露敏感信息 // 正确示例 wrap([$class: \u0026#39;MaskPasswordsBuildWrapper\u0026#39;, varPasswordPairs: [[var: \u0026#39;API_KEY\u0026#39;, password: env.API_KEY]]]) { // 现在 API_KEY 会被自动掩码 sh \u0026#39;curl -H \u0026#34;Authorization: Bearer $API_KEY\u0026#34; https://api.example.com\u0026#39; } } // credentials() 绑定的变量会自动被保护 withCredentials([ usernamePassword( credentialsId: \u0026#39;my-credentials\u0026#39;, usernameVariable: \u0026#39;USER\u0026#39;, passwordVariable: \u0026#39;PASS\u0026#39; ) ]) { // 即使echo，也会显示为 **** sh \u0026#39;echo $PASS\u0026#39; } } } } } 3. 最佳实践 # 使用凭据插件 withCredentials([string(credentialsId: \u0026#39;my-secret\u0026#39;, variable: \u0026#39;SECRET\u0026#39;)]) { // SECRET 变量会自动被掩码 sh \u0026#39;./deploy.sh\u0026#39; // 在脚本中使用 $SECRET } 使用掩码包装器 wrap([$class: \u0026#39;MaskPasswordsBuildWrapper\u0026#39;, varPasswordPairs: [ [var: \u0026#39;PASSWORD\u0026#39;, password: \u0026#39;sensitive-value\u0026#39;] ]]) { // 所有输出中的 \u0026#39;sensitive-value\u0026#39; 都会被替换为 **** sh \u0026#39;your-command\u0026#39; } 避免直接输出 // 不要这样做 sh \u0026#34;echo \u0026#39;Using password: $PASSWORD\u0026#39;\u0026#34; // 而是这样 sh \u0026#39;\u0026#39;\u0026#39; if [ -n \u0026#34;$PASSWORD\u0026#34; ]; then echo \u0026#34;Password is configured\u0026#34; else echo \u0026#34;Password is missing\u0026#34; fi \u0026#39;\u0026#39;\u0026#39; 日志处理 options { // 设置构建日志保留策略 buildDiscarder(logRotator(numToKeepStr: \u0026#39;10\u0026#39;)) } 4. 安全检查清单 # 避免 直接 echo 敏感变量 在命令参数中显示密码 在构建日志中保留敏感信息 应该 使用凭据插件管理敏感信息 使用掩码包装器保护变量 定期审查构建日志 限制日志访问权限 配置建议 pipeline { agent any options { // 禁用并发构建 disableConcurrentBuilds() // 设置超时 timeout(time: 1, unit: \u0026#39;HOURS\u0026#39;) } environment { // 使用凭据绑定 DEPLOY_KEY = credentials(\u0026#39;deploy-key\u0026#39;) } stages { stage(\u0026#39;部署\u0026#39;) { steps { // 使用安全的方式处理敏感信息 withCredentials([ file(credentialsId: \u0026#39;kubeconfig\u0026#39;, variable: \u0026#39;KUBECONFIG\u0026#39;) ]) { sh \u0026#39;kubectl apply -f deployment.yaml\u0026#39; } } } } } 通过这些措施，可以有效防止敏感信息在构建过程中被泄露。\n说下你对容器和k8s的理解 # 让我分享下对容器和 Kubernetes 的理解：\n1. 容器的本质 # 隔离性\n使用 Linux 的 Namespace 技术实现资源隔离 每个容器有自己的进程空间、网络空间、挂载空间等 容器间互不影响 资源限制\n使用 Linux 的 Cgroups 技术限制资源使用 可以限制 CPU、内存、IO 等资源 防止单个容器占用过多资源 轻量级\n共享主机内核 秒级启动 资源利用率高 2. K8s 的核心功能 # 自动化管理 # Deployment 自动管理 Pod 的生命周期 apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 3 # 自动维护3个副本 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:alpine 自愈能力\n节点故障自动迁移 容器崩溃自动重启 健康检查和恢复 负载均衡\n# Service 提供负载均衡 apiVersion: v1 kind: Service metadata: name: nginx-service spec: type: LoadBalancer selector: app: nginx ports: - port: 80 targetPort: 80 3. 架构设计理念 # 声明式 API\n描述期望状态 系统自动调整实际状态 易于维护和自动化 控制器模式\n每个控制器负责特定资源 持续监控和调整状态 松耦合设计 水平扩展\n# HPA 自动扩缩容 apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: nginx-hpa spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: nginx minReplicas: 1 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 80 4. 实际应用场景 # 微服务架构\n服务独立部署 快速迭代 弹性伸缩 DevOps 实践\n持续集成/部署 自动化运维 环境一致性 高可用设计\n# Pod 反亲和性确保高可用 apiVersion: apps/v1 kind: Deployment metadata: name: webapp spec: template: spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - webapp topologyKey: \u0026#34;kubernetes.io/hostname\u0026#34; 5. 生产环境考虑 # 资源管理\n合理设置请求和限制 资源预留和超卖 成本优化 安全性\n# 网络策略隔离 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: api-allow spec: podSelector: matchLabels: app: api ingress: - from: - podSelector: matchLabels: app: frontend 可观测性 监控指标 日志收集 链路追踪 6. 最佳实践 # 容器设计\n单一职责 无状态设计 合理的资源限制 健康检查 K8s 配置\n使用命名空间隔离 实施资源配额 RBAC 权限控制 备份和恢复策略 运维管理\n监控告警 日志管理 灾备方案 升级策略 容器和 Kubernetes 的结合极大地改变了应用部署和运维的方式，提供了一个强大的平台来运行和管理现代应用。理解它们的核心概念和最佳实践对于构建可靠的生产系统至关重要。\n我看到你有一个 pod.yaml 文件，让我基于这个实际的配置文件来进一步解释一些重要概念。\n从你的 pod.yaml 文件中可以看到几个关键的最佳实践：\n1. 资源管理 # resources: requests: memory: \u0026#34;64Mi\u0026#34; # 保证最小内存 cpu: \u0026#34;250m\u0026#34; # 0.25 CPU limits: memory: \u0026#34;256Mi\u0026#34; # 内存上限 cpu: \u0026#34;500m\u0026#34; # 0.5 CPU 这里展示了合理的资源控制：\nrequests: 保证 Pod 运行的最小资源 limits: 限制资源使用的上限 设置合适的比例（limits/requests）避免资源浪费 2. 健康检查 # readinessProbe: httpGet: path: / port: 80 initialDelaySeconds: 5 periodSeconds: 10 livenessProbe: httpGet: path: / port: 80 initialDelaySeconds: 15 periodSeconds: 20 两种探针的使用：\nreadinessProbe: 确定 Pod 是否准备好接收流量 livenessProbe: 确定 Pod 是否需要重启 3. 配置管理 # volumeMounts: - name: config mountPath: /app/config volumes: - name: config configMap: name: app-config 使用 ConfigMap 管理配置：\n配置与代码分离 可以动态更新 便于环境管理 4. 节点选择 # nodeSelector: kubernetes.io/os: linux 通过标签选择合适的节点：\n确保 Pod 调度到正确的节点 可以基于操作系统、硬件特性等选择 这些配置反映了在生产环境中运行容器的几个重要考虑点：\n资源控制 可靠性保证 配置管理 调度策略 这是一个比较完善的 Pod 配置，体现了 K8s 的核心价值：自动化管理、高可用性和可维护性。\n你用过go或者python写东西吗 有关运维的 devops的 # 是的，我经常使用 Python 和 Go 来编写运维和 DevOps 相关的工具。让我分享一些常见的场景和示例：\n1. Python 自动化运维示例 # K8s 资源监控 from kubernetes import client, config def monitor_pods(): # 加载 k8s 配置 config.load_kube_config() v1 = client.CoreV1Api() try: # 获取所有 namespace 的 pods pods = v1.list_pod_for_all_namespaces(watch=False) for pod in pods.items: print(f\u0026#34;Pod: {pod.metadata.name}\u0026#34;) print(f\u0026#34;Namespace: {pod.metadata.namespace}\u0026#34;) print(f\u0026#34;Status: {pod.status.phase}\u0026#34;) # 检查资源使用 if pod.status.container_statuses: for container in pod.status.container_statuses: print(f\u0026#34;Container: {container.name}\u0026#34;) print(f\u0026#34;Ready: {container.ready}\u0026#34;) print(f\u0026#34;Restart Count: {container.restart_count}\u0026#34;) except Exception as e: print(f\u0026#34;Error: {e}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: monitor_pods() 日志分析工具 import re from datetime import datetime import pandas as pd def analyze_nginx_logs(log_file): # 定义日志格式正则 pattern = r\u0026#39;(?P\u0026lt;ip\u0026gt;.*?) - - \\[(?P\u0026lt;time\u0026gt;.*?)\\] \u0026#34;(?P\u0026lt;request\u0026gt;.*?)\u0026#34; (?P\u0026lt;status\u0026gt;\\d+) (?P\u0026lt;size\u0026gt;\\d+)\u0026#39; logs = [] with open(log_file, \u0026#39;r\u0026#39;) as f: for line in f: match = re.match(pattern, line) if match: data = match.groupdict() data[\u0026#39;time\u0026#39;] = datetime.strptime(data[\u0026#39;time\u0026#39;], \u0026#39;%d/%b/%Y:%H:%M:%S %z\u0026#39;) logs.append(data) # 转换为 DataFrame 分析 df = pd.DataFrame(logs) return { \u0026#39;total_requests\u0026#39;: len(df), \u0026#39;status_codes\u0026#39;: df[\u0026#39;status\u0026#39;].value_counts().to_dict(), \u0026#39;top_ips\u0026#39;: df[\u0026#39;ip\u0026#39;].value_counts().head(10).to_dict() } 2. Go 运维工具示例 # Docker 容器监控 package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;github.com/docker/docker/api/types\u0026#34; \u0026#34;github.com/docker/docker/client\u0026#34; ) func monitorContainers() { ctx := context.Background() cli, err := client.NewClientWithOpts(client.FromEnv) if err != nil { panic(err) } containers, err := cli.ContainerList(ctx, types.ContainerListOptions{}) if err != nil { panic(err) } for _, container := range containers { stats, err := cli.ContainerStats(ctx, container.ID, false) if err != nil { fmt.Printf(\u0026#34;Error getting stats for %s: %v\\n\u0026#34;, container.ID, err) continue } fmt.Printf(\u0026#34;Container: %s\\n\u0026#34;, container.Names[0]) fmt.Printf(\u0026#34;Status: %s\\n\u0026#34;, container.Status) fmt.Printf(\u0026#34;Stats: %v\\n\u0026#34;, stats) } } 自动化部署工具 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;k8s.io/client-go/kubernetes\u0026#34; \u0026#34;k8s.io/client-go/tools/clientcmd\u0026#34; appsv1 \u0026#34;k8s.io/api/apps/v1\u0026#34; metav1 \u0026#34;k8s.io/apimachinery/pkg/apis/meta/v1\u0026#34; ) func deployApplication(namespace, name string, image string) error { // 加载 kubeconfig config, err := clientcmd.BuildConfigFromFlags(\u0026#34;\u0026#34;, \u0026#34;~/.kube/config\u0026#34;) if err != nil { return err } // 创建 clientset clientset, err := kubernetes.NewForConfig(config) if err != nil { return err } // 创建 deployment deployment := \u0026amp;appsv1.Deployment{ ObjectMeta: metav1.ObjectMeta{ Name: name, }, Spec: appsv1.DeploymentSpec{ Replicas: int32Ptr(3), Selector: \u0026amp;metav1.LabelSelector{ MatchLabels: map[string]string{ \u0026#34;app\u0026#34;: name, }, }, Template: corev1.PodTemplateSpec{ ObjectMeta: metav1.ObjectMeta{ Labels: map[string]string{ \u0026#34;app\u0026#34;: name, }, }, Spec: corev1.PodSpec{ Containers: []corev1.Container{ { Name: name, Image: image, }, }, }, }, }, } // 执行部署 _, err = clientset.AppsV1().Deployments(namespace).Create(context.TODO(), deployment, metav1.CreateOptions{}) return err } 3. 实用的 DevOps 脚本 # CI/CD 辅助工具 import requests import json import os class GitlabCI: def __init__(self, token, url): self.token = token self.url = url self.headers = {\u0026#39;PRIVATE-TOKEN\u0026#39;: token} def trigger_pipeline(self, project_id, ref=\u0026#39;main\u0026#39;): endpoint = f\u0026#34;{self.url}/api/v4/projects/{project_id}/pipeline\u0026#34; data = {\u0026#39;ref\u0026#39;: ref} response = requests.post(endpoint, headers=self.headers, data=data) return response.json() def get_pipeline_status(self, project_id, pipeline_id): endpoint = f\u0026#34;{self.url}/api/v4/projects/{project_id}/pipelines/{pipeline_id}\u0026#34; response = requests.get(endpoint, headers=self.headers) return response.json() 资源清理工具 from kubernetes import client, config from datetime import datetime, timedelta def cleanup_old_pods(namespace, hours=24): config.load_kube_config() v1 = client.CoreV1Api() try: pods = v1.list_namespaced_pod(namespace) now = datetime.now(timezone.utc) for pod in pods.items: if pod.status.phase in [\u0026#39;Failed\u0026#39;, \u0026#39;Succeeded\u0026#39;]: created_time = pod.metadata.creation_timestamp age = now - created_time if age \u0026gt; timedelta(hours=hours): print(f\u0026#34;Deleting pod: {pod.metadata.name}\u0026#34;) v1.delete_namespaced_pod( pod.metadata.name, namespace, body=client.V1DeleteOptions() ) except Exception as e: print(f\u0026#34;Error during cleanup: {e}\u0026#34;) 这些工具都是基于实际运维需求开发的。主要特点：\n自动化\n减少人工操作 提高效率 降低错误率 监控告警\n实时监控系统状态 及时发现问题 自动处理或通知 运维管理\n资源清理 配置管理 日志分析 持续集成/部署\n自动化构建 自动化测试 自动化部署 根据实际需求，这些工具可以进一步定制和扩展。\n让我基于你的 pod.yaml 文件，写一个实用的 Python 脚本来监控和管理这个 Pod：\nfrom kubernetes import client, config import time from datetime import datetime import logging logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) class PodMonitor: def __init__(self, namespace=\u0026#34;default\u0026#34;): # 加载 k8s 配置 config.load_kube_config() self.v1 = client.CoreV1Api() self.namespace = namespace def monitor_pod_resources(self, pod_name): \u0026#34;\u0026#34;\u0026#34;监控 Pod 的资源使用情况\u0026#34;\u0026#34;\u0026#34; try: pod = self.v1.read_namespaced_pod(pod_name, self.namespace) # 检查资源限制和请求 for container in pod.spec.containers: resources = container.resources logger.info(f\u0026#34;Container: {container.name}\u0026#34;) logger.info(f\u0026#34;Resource Limits: {resources.limits}\u0026#34;) logger.info(f\u0026#34;Resource Requests: {resources.requests}\u0026#34;) # 检查 Pod 状态 logger.info(f\u0026#34;Pod Status: {pod.status.phase}\u0026#34;) # 检查就绪状态 if pod.status.container_statuses: for container in pod.status.container_statuses: logger.info(f\u0026#34;Container {container.name} Ready: {container.ready}\u0026#34;) logger.info(f\u0026#34;Restart Count: {container.restart_count}\u0026#34;) except Exception as e: logger.error(f\u0026#34;Error monitoring pod {pod_name}: {e}\u0026#34;) def check_probes(self, pod_name): \u0026#34;\u0026#34;\u0026#34;检查 Pod 的健康检查状态\u0026#34;\u0026#34;\u0026#34; try: pod = self.v1.read_namespaced_pod(pod_name, self.namespace) for container in pod.spec.containers: logger.info(f\u0026#34;Container: {container.name}\u0026#34;) if container.readiness_probe: logger.info(\u0026#34;Readiness Probe:\u0026#34;) logger.info(f\u0026#34;Initial Delay: {container.readiness_probe.initial_delay_seconds}s\u0026#34;) logger.info(f\u0026#34;Period: {container.readiness_probe.period_seconds}s\u0026#34;) if container.liveness_probe: logger.info(\u0026#34;Liveness Probe:\u0026#34;) logger.info(f\u0026#34;Initial Delay: {container.liveness_probe.initial_delay_seconds}s\u0026#34;) logger.info(f\u0026#34;Period: {container.liveness_probe.period_seconds}s\u0026#34;) except Exception as e: logger.error(f\u0026#34;Error checking probes for pod {pod_name}: {e}\u0026#34;) def check_configmap_mounted(self, pod_name): \u0026#34;\u0026#34;\u0026#34;检查 ConfigMap 挂载状态\u0026#34;\u0026#34;\u0026#34; try: pod = self.v1.read_namespaced_pod(pod_name, self.namespace) for volume in pod.spec.volumes: if volume.config_map: logger.info(f\u0026#34;ConfigMap mounted: {volume.config_map.name}\u0026#34;) # 检查 ConfigMap 是否存在 try: config_map = self.v1.read_namespaced_config_map( volume.config_map.name, self.namespace ) logger.info(\u0026#34;ConfigMap exists and is accessible\u0026#34;) except Exception as e: logger.error(f\u0026#34;Error accessing ConfigMap: {e}\u0026#34;) except Exception as e: logger.error(f\u0026#34;Error checking ConfigMap for pod {pod_name}: {e}\u0026#34;) def continuous_monitoring(self, pod_name, interval=60): \u0026#34;\u0026#34;\u0026#34;持续监控 Pod\u0026#34;\u0026#34;\u0026#34; while True: logger.info(f\u0026#34;\\n=== Monitoring Pod {pod_name} at {datetime.now()} ===\u0026#34;) self.monitor_pod_resources(pod_name) self.check_probes(pod_name) self.check_configmap_mounted(pod_name) time.sleep(interval) def main(): monitor = PodMonitor() pod_name = \u0026#34;app-pod\u0026#34; # 你的 pod 名称 try: monitor.continuous_monitoring(pod_name) except KeyboardInterrupt: logger.info(\u0026#34;Monitoring stopped by user\u0026#34;) except Exception as e: logger.error(f\u0026#34;Unexpected error: {e}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() 这个脚本提供了以下功能：\n资源监控\n检查 CPU 和内存使用 监控容器状态 跟踪重启次数 健康检查监控\n监控 readinessProbe 监控 livenessProbe 检查探针配置 配置检查\n验证 ConfigMap 挂载 检查 ConfigMap 可访问性 持续监控\n定期检查 Pod 状态 输出详细日志 异常处理和报告 你可以根据需要调整监控间隔和添加其他监控指标。要使用这个脚本，需要先安装 kubernetes 客户端：\npip install kubernetes 这个工具可以帮助你更好地监控和管理你的 Pod。\n我再基于你的 pod.yaml 写一个更实用的 Go 版本的监控工具，这个版本会更关注性能和并发处理：\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; \u0026#34;log\u0026#34; metav1 \u0026#34;k8s.io/apimachinery/pkg/apis/meta/v1\u0026#34; \u0026#34;k8s.io/client-go/kubernetes\u0026#34; \u0026#34;k8s.io/client-go/tools/clientcmd\u0026#34; \u0026#34;k8s.io/client-go/util/homedir\u0026#34; \u0026#34;path/filepath\u0026#34; ) type PodMonitor struct { clientset *kubernetes.Clientset namespace string } func NewPodMonitor(namespace string) (*PodMonitor, error) { // 获取 kubeconfig 路径 kubeconfig := filepath.Join(homedir.HomeDir(), \u0026#34;.kube\u0026#34;, \u0026#34;config\u0026#34;) // 创建 config config, err := clientcmd.BuildConfigFromFlags(\u0026#34;\u0026#34;, kubeconfig) if err != nil { return nil, fmt.Errorf(\u0026#34;error building config: %v\u0026#34;, err) } // 创建 clientset clientset, err := kubernetes.NewForConfig(config) if err != nil { return nil, fmt.Errorf(\u0026#34;error creating clientset: %v\u0026#34;, err) } return \u0026amp;PodMonitor{ clientset: clientset, namespace: namespace, }, nil } func (pm *PodMonitor) MonitorPod(ctx context.Context, podName string) { ticker := time.NewTicker(30 * time.Second) defer ticker.Stop() for { select { case \u0026lt;-ctx.Done(): log.Println(\u0026#34;Monitoring stopped\u0026#34;) return case \u0026lt;-ticker.C: pod, err := pm.clientset.CoreV1().Pods(pm.namespace).Get( context.Background(), podName, metav1.GetOptions{}, ) if err != nil { log.Printf(\u0026#34;Error getting pod %s: %v\\n\u0026#34;, podName, err) continue } // 检查 Pod 状态 log.Printf(\u0026#34;Pod Status: %s\\n\u0026#34;, pod.Status.Phase) // 检查容器状态 for _, container := range pod.Status.ContainerStatuses { log.Printf(\u0026#34;Container: %s\\n\u0026#34;, container.Name) log.Printf(\u0026#34; Ready: %v\\n\u0026#34;, container.Ready) log.Printf(\u0026#34; RestartCount: %d\\n\u0026#34;, container.RestartCount) // 检查资源使用 if container.State.Running != nil { log.Printf(\u0026#34; Running since: %v\\n\u0026#34;, container.State.Running.StartedAt) } } // 检查资源配置 for _, container := range pod.Spec.Containers { resources := container.Resources log.Printf(\u0026#34;Resource Limits - CPU: %s, Memory: %s\\n\u0026#34;, resources.Limits.Cpu(), resources.Limits.Memory(), ) log.Printf(\u0026#34;Resource Requests - CPU: %s, Memory: %s\\n\u0026#34;, resources.Requests.Cpu(), resources.Requests.Memory(), ) } } } } func (pm *PodMonitor) CheckProbes(podName string) error { pod, err := pm.clientset.CoreV1().Pods(pm.namespace).Get( context.Background(), podName, metav1.GetOptions{}, ) if err != nil { return fmt.Errorf(\u0026#34;error getting pod: %v\u0026#34;, err) } for _, container := range pod.Spec.Containers { log.Printf(\u0026#34;Container: %s\\n\u0026#34;, container.Name) if container.ReadinessProbe != nil { log.Printf(\u0026#34;ReadinessProbe:\\n\u0026#34;) log.Printf(\u0026#34; InitialDelay: %ds\\n\u0026#34;, container.ReadinessProbe.InitialDelaySeconds) log.Printf(\u0026#34; Period: %ds\\n\u0026#34;, container.ReadinessProbe.PeriodSeconds) } if container.LivenessProbe != nil { log.Printf(\u0026#34;LivenessProbe:\\n\u0026#34;) log.Printf(\u0026#34; InitialDelay: %ds\\n\u0026#34;, container.LivenessProbe.InitialDelaySeconds) log.Printf(\u0026#34; Period: %ds\\n\u0026#34;, container.LivenessProbe.PeriodSeconds) } } return nil } func main() { monitor, err := NewPodMonitor(\u0026#34;default\u0026#34;) if err != nil { log.Fatalf(\u0026#34;Error creating monitor: %v\u0026#34;, err) } ctx, cancel := context.WithCancel(context.Background()) defer cancel() // 启动监控 go monitor.MonitorPod(ctx, \u0026#34;app-pod\u0026#34;) // 检查探针配置 if err := monitor.CheckProbes(\u0026#34;app-pod\u0026#34;); err != nil { log.Printf(\u0026#34;Error checking probes: %v\\n\u0026#34;, err) } // 保持程序运行 select { case \u0026lt;-ctx.Done(): log.Println(\u0026#34;Program terminated\u0026#34;) } } 这个 Go 版本的监控工具具有以下特点：\n性能更好\n使用 Go 的并发特性 更低的资源消耗 更快的响应速度 更好的错误处理\n使用 context 控制生命周期 详细的错误信息 优雅的退出机制 功能完整\n监控 Pod 状态 检查资源使用 探针状态监控 要运行这个程序，需要先安装依赖：\ngo get k8s.io/client-go@latest 这个工具比 Python 版本的性能更好，特别是在需要长期运行或监控多个 Pod 的场景下。\n减少docker镜像的体积大小 # 基于减少 Docker 镜像体积，我来举例说明几种常用的优化方法：\n1. 多阶段构建 (Multi-stage Build) # # 构建阶段 FROM maven:3.8.1-jdk-8 AS builder WORKDIR /app COPY pom.xml . # 缓存依赖 RUN mvn dependency:go-offline COPY src ./src RUN mvn package -DskipTests # 运行阶段 - 使用更小的基础镜像 FROM openjdk:8-jre-alpine WORKDIR /app # 只复制构建产物 COPY --from=builder /app/target/*.jar app.jar ENTRYPOINT [\u0026#34;java\u0026#34;, \u0026#34;-jar\u0026#34;, \u0026#34;app.jar\u0026#34;] 2. 使用轻量级基础镜像 # # 不推荐 FROM ubuntu:latest # ~70MB # 推荐 FROM alpine:latest # ~5MB 3. 优化 RUN 指令和清理 # FROM python:3.9-alpine WORKDIR /app COPY requirements.txt . # 合并命令并清理缓存 RUN pip install --no-cache-dir -r requirements.txt \u0026amp;\u0026amp; \\ rm -rf /root/.cache/pip \u0026amp;\u0026amp; \\ rm -rf /var/cache/apk/* COPY . . 4. 使用 .dockerignore # # 排除不需要的文件 node_modules *.log .git .env *.md tests docs *.pyc __pycache__ .pytest_cache 5. 生产环境优化示例 # # Node.js 应用优化示例 FROM node:alpine AS builder WORKDIR /app COPY package*.json ./ RUN npm ci --only=production COPY . . RUN npm run build # 使用更轻量的镜像运行 FROM nginx:alpine COPY --from=builder /app/build /usr/share/nginx/html EXPOSE 80 6. 分离开发和生产依赖 # # 开发环境 FROM node:alpine AS development WORKDIR /app COPY package*.json ./ RUN npm install COPY . . # 生产环境 FROM node:alpine AS production WORKDIR /app COPY package*.json ./ RUN npm ci --only=production COPY --from=development /app/dist ./dist CMD [\u0026#34;node\u0026#34;, \u0026#34;dist/main\u0026#34;] 7. 最佳实践总结 # 选择合适的基础镜像\n使用官方镜像 选择 alpine 版本 使用特定版本标签 优化构建过程\n合并 RUN 指令 清理缓存文件 使用多阶段构建 文件处理\n使用 .dockerignore 只复制必要文件 删除临时文件 依赖管理\n只安装生产依赖 使用版本锁定 清理包管理器缓存 8. 实际示例（基于你的 pod.yaml） # # 多阶段构建示例 FROM node:alpine AS builder WORKDIR /app COPY package*.json ./ RUN npm ci --only=production COPY . . RUN npm run build # 生产镜像 FROM nginx:alpine # 复制构建产物 COPY --from=builder /app/dist /usr/share/nginx/html # 复制 nginx 配置 COPY nginx.conf /etc/nginx/conf.d/default.conf # 设置用户 USER nginx # 资源使用限制（对应 pod.yaml 中的设置） ENV NODE_OPTIONS=\u0026#34;--max-old-space-size=256\u0026#34; EXPOSE 80 # 健康检查 HEALTHCHECK --interval=30s --timeout=3s \\ CMD wget -q --spider http://localhost/ || exit 1 这些优化可以显著减少镜像大小，提高部署效率。记住要根据实际应用需求选择适当的优化策略。\n多阶段构建啥意思 # 多阶段构建（Multi-stage Build）是一种优化 Docker 镜像大小的技术。让我用具体例子解释：\n1. 传统构建方式（单阶段） # # 这种方式最终镜像会包含构建工具和中间文件，体积很大 FROM maven:3.8.1-jdk-8 WORKDIR /app COPY . . RUN mvn package CMD [\u0026#34;java\u0026#34;, \u0026#34;-jar\u0026#34;, \u0026#34;target/app.jar\u0026#34;] 2. 多阶段构建方式 # # 第一阶段：构建阶段 FROM maven:3.8.1-jdk-8 AS builder WORKDIR /app COPY pom.xml . COPY src ./src RUN mvn package # 第二阶段：运行阶段 FROM openjdk:8-jre-alpine WORKDIR /app # 只复制第一阶段构建的产物 COPY --from=builder /app/target/*.jar app.jar CMD [\u0026#34;java\u0026#34;, \u0026#34;-jar\u0026#34;, \u0026#34;app.jar\u0026#34;] 主要优点：\n最终镜像更小：只包含运行必需的文件 更安全：不包含构建工具和源代码 层更少：优化缓存和构建速度 3. 更复杂的多阶段构建示例 # # 第一阶段：依赖安装 FROM node:alpine AS dependencies WORKDIR /app COPY package*.json ./ RUN npm ci --only=production # 第二阶段：构建 FROM node:alpine AS builder WORKDIR /app COPY --from=dependencies /app/node_modules ./node_modules COPY . . RUN npm run build # 第三阶段：运行 FROM nginx:alpine # 只复制构建产物 COPY --from=builder /app/dist /usr/share/nginx/html # 配置文件 COPY nginx.conf /etc/nginx/conf.d/default.conf EXPOSE 80 这个例子中：\n第一阶段安装依赖 第二阶段进行构建 第三阶段只包含运行所需的文件 最终镜像只包含：\nnginx 运行环境 构建后的静态文件 nginx 配置 不包含：\nnode.js 环境 npm 包 源代码 构建工具 这样可以大大减少最终镜像的大小，通常可以减少 60-90% 的体积。\n好的，我看到你有一个 pod.yaml 文件。让我结合你的 pod.yaml 配置，写一个实际的多阶段构建的例子，这样可以确保构建出的镜像既小巧又符合你的资源限制要求：\n# 第一阶段：依赖安装和构建 FROM node:alpine AS builder WORKDIR /app # 安装依赖 COPY package*.json ./ RUN npm ci --only=production # 复制源代码并构建 COPY . . RUN npm run build # 第二阶段：运行环境 FROM nginx:alpine WORKDIR /usr/share/nginx/html # 从builder阶段复制构建产物 COPY --from=builder /app/dist . COPY nginx.conf /etc/nginx/conf.d/default.conf # 设置非root用户运行 USER nginx # 配置健康检查，对应 pod.yaml 中的探针配置 HEALTHCHECK --interval=10s --timeout=3s \\ CMD wget -q --spider http://localhost/ || exit 1 # 资源使用优化，对应 pod.yaml 中的资源限制 ENV NODE_OPTIONS=\u0026#34;--max-old-space-size=256\u0026#34; EXPOSE 80 这个 Dockerfile：\n构建阶段只包含必要的构建工具 运行阶段使用轻量级的 alpine 基础镜像 最终镜像符合 pod.yaml 中的资源限制和健康检查要求 这样构建出的镜像会更小，更适合在你的 K8s 环境中运行。\n是的！我注意到你的 pod.yaml 配置得很规范，包含了资源限制、健康检查等重要配置。让我们可以进一步优化 Dockerfile，使其完全匹配你的 pod.yaml 配置：\n# 第一阶段：构建 FROM node:alpine AS builder WORKDIR /app # 使用 .dockerignore 排除不必要文件 COPY package*.json ./ RUN npm ci --only=production COPY . . RUN npm run build # 第二阶段：运行 FROM nginx:alpine WORKDIR /usr/share/nginx/html # 对应 pod.yaml 中的资源限制 # memory: \u0026#34;256Mi\u0026#34; # cpu: \u0026#34;500m\u0026#34; ENV NGINX_WORKER_PROCESSES=\u0026#34;auto\u0026#34; ENV NGINX_WORKER_CONNECTIONS=\u0026#34;1024\u0026#34; # 复制构建产物 COPY --from=builder /app/dist . COPY nginx.conf /etc/nginx/conf.d/default.conf # 对应 pod.yaml 中的健康检查配置 # readinessProbe 和 livenessProbe HEALTHCHECK --interval=10s --timeout=3s \\ CMD wget -q --spider http://localhost/ || exit 1 # 安全性配置 USER nginx EXPOSE 80 这样构建出的镜像不仅体积小，而且完全符合你在 pod.yaml 中定义的运行要求。你觉得这样的优化方向对吗？\n"},{"id":198,"href":"/docs/2025-2-18-%E9%9D%A2%E8%AF%95/","title":"面试2025-0218","section":"Docs","content":" pod的request和limit的关系 以及他们在node节点上的pod被驱逐时的优先级 # 在 Kubernetes 中，request 和 limit 是资源调度的关键参数，分别表示容器资源的最小需求和最大使用限制。它们的关系和在节点上的 Pod 被驱逐时的优先级可以分为以下几个方面：\n1. request 和 limit 的关系 # request： request 是容器启动时所需的资源量。Kubernetes 调度器根据 request 来选择合适的节点调度 Pod。当你设置了 request，Kubernetes 确保该容器至少能使用这么多资源。若没有足够的资源，Pod 将无法被调度到该节点。 它反映了容器的“需求量”，用于确定调度时是否能找到一个合适的节点。 limit： limit 是容器可以使用的最大资源量。容器在运行时使用的资源如果超过 limit，Kubernetes 会根据策略（如 OOM Killer）限制容器的资源使用。 它是对容器资源使用的“上限”，防止容器占用过多的资源，影响同一节点上的其他容器。 request 和 limit 的差异： request 用于调度，而 limit 用于资源使用控制。理想情况下，request 应该小于或等于 limit，否则会造成资源浪费或调度错误。 例如：如果 request 设置为 512Mi，而 limit 设置为 1Gi，Pod 将被调度到有足够 512Mi 内存的节点，但在节点上运行时，该容器最多可使用 1Gi 内存。 2. Node 节点上 Pod 被驱逐时的优先级 # 当 Kubernetes 节点资源不足时（如内存或 CPU），Pod 可能会被驱逐。Pod 驱逐的决策基于以下几个因素：\n内存压力（OutOfMemory）\n：\n当节点的内存资源不足时，Kubernetes 会优先驱逐使用了大量内存的容器。特别是那些内存使用超过了 limit 或发生了 OOM（Out of Memory）情况的容器。 优先级和 QoS 类别\n：\nKubernetes 使用 QoS（Quality of Service）来确定 Pod 的资源优先级，分为三类：\nGuaranteed（保证）\n：\n如果容器的 request 和 limit 相同（例如 cpu: 500m，mem: 1Gi），则 Pod 被视为 Guaranteed 类型。这类 Pod 在资源争抢时优先不被驱逐。 Burstable（可突发）\n：\n如果容器的 request 小于 limit，则 Pod 被视为 Burstable 类型。这类 Pod 可以在节点资源空闲时使用更多资源，但在资源紧张时，优先级低于 Guaranteed 类型 Pod。 BestEffort（最佳努力）\n：\n如果容器既没有设置 request 也没有设置 limit（或者 request 和 limit 都设置为 0），则 Pod 被视为 BestEffort 类型。这类 Pod 在资源紧张时最容易被驱逐。 驱逐优先级\n：\nBestEffort Pod 会被最优先驱逐，因为它没有任何资源保障。 其次是 Burstable Pod，这类 Pod 的 request 小于 limit，有弹性，但是在资源紧张时，可能会被驱逐。 最不容易被驱逐的是 Guaranteed Pod，因为它们在内存和 CPU 资源上都有严格的保障。 3. 驱逐顺序的总结： # Guaranteed \u0026gt; Burstable \u0026gt; BestEffort。 在节点资源紧张时，Kubernetes 会根据 QoS 类别、Pod 的资源请求和限制，来决定哪个 Pod 应该被驱逐。 4. 驱逐决策的影响因素 # 节点资源压力：当节点的资源（如 CPU、内存）出现压力时，Kubernetes 会根据 request 和 limit 的设置，评估哪些 Pod 占用了过多的资源，哪些应该优先驱逐。 Pod 的资源使用情况：如果容器的实际使用超过了 limit，可能会触发驱逐。 NodeEviction 和 PodDisruptionBudgets：这些设置也会影响驱逐决策，允许设置一些控制规则来防止关键 Pod 被驱逐。 通过合理配置 Pod 的 request 和 limit，以及根据实际业务需求和 QoS 类别选择合适的策略，可以帮助提高系统的资源利用率，避免不必要的资源浪费和容器被驱逐的情况。\n禁止某个用户登陆 修改哪个配置参数 # 在 Linux 系统中，如果你想禁止某个用户登录，可以通过修改以下配置来实现：\n1. 修改 /etc/passwd 文件 # 你可以通过将该用户的 shell 设置为 nologin，来禁止用户登录。\n打开 /etc/passwd 文件：\nsudo vi /etc/passwd 找到该用户的条目，并将其登录 shell 修改为 /sbin/nologin 或 /bin/false。例如，如果你要禁用 username 用户，修改如下：\nusername:x:1001:1001:,,,:/home/username:/sbin/nologin 这样，username 用户就不能登录系统了。\n2. 使用 usermod 命令 # 你可以使用 usermod 命令直接设置该用户的 shell 为 nologin，也可以使用 false 作为 shell。\n禁止用户登录：\nsudo usermod -s /sbin/nologin username 3. 使用 passwd 命令锁定用户账户 # 另一种常见的方法是使用 passwd 命令锁定用户账户，这会禁用该用户的密码，从而阻止其通过密码登录。\n锁定用户账户：\nsudo passwd -l username 该命令会在 /etc/shadow 文件中添加一个 \u0026ldquo;!\u0026quot;，禁止该用户登录系统。\n4. 禁用 SSH 登录 # 如果你特别想禁止用户通过 SSH 登录，可以修改 SSH 配置文件 /etc/ssh/sshd_config。\n打开 SSH 配置文件：\nsudo vi /etc/ssh/sshd_config 在文件中添加或修改以下行，禁止该用户通过 SSH 登录：\nDenyUsers username 保存并退出后，重启 SSH 服务：\nsudo systemctl restart sshd 5. 暂时禁用用户 # 如果你只是想暂时禁用该用户，而不修改任何配置文件，可以使用以下命令禁用该用户的账户：\nsudo usermod -L username 以上方法都可以有效地禁止某个用户登录，具体使用哪种方法可以根据你的需求来选择。\n统计某个日志文件的error的次数 # 你可以使用 grep 命令来统计日志文件中 \u0026ldquo;error\u0026rdquo; 出现的次数。具体方法如下：\n1. 简单统计出现次数 # 使用 grep 命令并结合 -c 选项，可以直接统计包含 \u0026ldquo;error\u0026rdquo; 的行数：\ngrep -ic \u0026#34;error\u0026#34; /path/to/logfile -i：忽略大小写（不区分大小写，\u0026ldquo;error\u0026rdquo; 和 \u0026ldquo;Error\u0026rdquo; 都会被匹配）。 -c：只输出匹配到的行数。 这条命令会输出文件中包含 \u0026ldquo;error\u0026rdquo;（忽略大小写）的行数。\n2. 统计精确的 \u0026ldquo;error\u0026rdquo; 词出现次数 # 如果你想统计日志中精确出现 \u0026ldquo;error\u0026rdquo; 这个词的次数，而不仅仅是行数，可以使用 grep 和 -o 选项：\ngrep -io \u0026#34;error\u0026#34; /path/to/logfile | wc -l -o：每次匹配都会输出匹配到的内容（每次出现 \u0026ldquo;error\u0026rdquo; 都会被输出）。 wc -l：计算输出的行数，即 \u0026ldquo;error\u0026rdquo; 出现的次数。 3. 匹配 \u0026ldquo;error\u0026rdquo; 或类似的单词 # 如果你想匹配更广泛的错误信息（例如 \u0026ldquo;Error\u0026rdquo;、\u0026ldquo;ERROR\u0026rdquo; 或 \u0026ldquo;error\u0026rdquo;），并且考虑到日志中的一些大小写不同，可以使用正则表达式来进行匹配：\ngrep -iE \u0026#34;error|fail|exception\u0026#34; /path/to/logfile | wc -l 这会统计包含 \u0026ldquo;error\u0026rdquo;、\u0026ldquo;fail\u0026rdquo; 或 \u0026ldquo;exception\u0026rdquo; 这些单词的行数。\n这些方法都可以帮助你快速统计日志文件中 \u0026ldquo;error\u0026rdquo; 出现的次数，或者根据不同的需求调整匹配模式。\n怎么知道 / 目录下 哪个目录占用的最多的空间？ # 你可以使用 du（disk usage）命令来查看 / 目录下各个子目录占用的磁盘空间，并找出占用最多空间的目录。具体的步骤如下：\n1. 查看每个子目录的空间使用情况 # 使用 du 命令配合 -h（以人类可读的格式显示，自动选择合适的单位如 KB、MB、GB 等）和 -d（指定递归的深度）选项来查看 / 目录下各个子目录的磁盘使用情况：\nsudo du -h --max-depth=1 / -h：以易读的方式显示磁盘使用量（如 KB、MB、GB）。 --max-depth=1：只列出 / 目录下一级子目录的空间使用情况，不会递归显示子目录内部的目录。 这会列出 / 目录下每个直接子目录（如 /bin、/home、/var 等）占用的空间大小。\n2. 排序并查看最大占用空间的目录 # 如果你希望查看哪个子目录占用空间最多，可以结合 sort 命令按空间大小排序：\nsudo du -h --max-depth=1 / | sort -rh | head -n 10 sort -rh：根据磁盘使用量排序，-r 表示逆序（从大到小），-h 让 sort 能够根据文件大小的单位（如 KB、MB、GB）正确排序。 head -n 10：只显示排名前10的目录。 3. 查看更深层次的目录使用情况 # 如果你希望查看某个子目录内部的具体磁盘使用情况（例如，深入 /var 目录查看哪些子目录占用空间最多），可以递归更深一级：\nsudo du -h /var --max-depth=2 | sort -rh | head -n 10 4. 查看某个目录的详细空间使用情况 # 如果你想查看某个特定目录下所有文件和子目录的空间占用情况，可以直接运行 du ：\nsudo du -ah /path/to/directory | sort -rh | head -n 20 这些方法可以帮助你快速找出哪个目录或子目录占用了最多的磁盘空间。\n查找大文件 find # 要使用 find 命令查找大文件，可以通过设置文件大小的条件来过滤出符合要求的文件。以下是一些常用的查找大文件的 find 命令示例：\n1. 查找大于 1GB 的文件 # 使用 -size 参数可以指定文件大小，单位可以是 b（字节）、k（千字节）、M（兆字节）、G（吉字节）等。\n查找大于 1GB 的文件：\nfind /path/to/directory -type f -size +1G 解释：\n/path/to/directory：指定你要搜索的目录，根目录 / 表示整个系统。 -type f：仅查找文件。 -size +1G：查找大于 1GB 的文件（+ 表示大于，G 表示以 GB 为单位）。 2. 查找大于 100MB 的文件 # 查找大于 100MB 的文件：\nfind /path/to/directory -type f -size +100M 3. 查找大于 500KB 的文件 # 查找大于 500KB 的文件：\nfind /path/to/directory -type f -size +500k 4. 查找大于 1GB 并按大小排序输出 # 查找大于 1GB 的文件，并按大小排序输出：\nfind /path/to/directory -type f -size +1G -exec ls -lh {} \\; | sort -k 5 -rh 解释：\n-exec ls -lh {} \\;：对找到的每个文件执行 ls -lh，以便显示文件的详细信息。 sort -k 5 -rh：根据 ls 输出的第五列（即文件大小）进行逆序排序。 5. 查找大文件并显示文件路径及大小 # 如果你只想查看文件路径和大小，可以用 -exec 执行 du -sh 来显示：\nfind /path/to/directory -type f -size +1G -exec du -sh {} \\; 这将显示每个文件的大小（以人类可读的格式）及其路径。\n6. 查找大文件并显示路径 # 如果你只关心文件的路径，可以直接输出文件路径：\nfind /path/to/directory -type f -size +1G -print 通过这些命令，你可以方便地在指定目录下查找大文件，帮助你分析磁盘空间的使用情况。\n-exec 啥意思 # -exec 是 find 命令的一个选项，用来对找到的每个文件执行一个指定的命令。-exec 后面跟着的命令会对 find 查找到的每个文件或目录执行一次。\n基本语法： # find \u0026lt;路径\u0026gt; \u0026lt;查找条件\u0026gt; -exec \u0026lt;命令\u0026gt; {} \\; 解释： # \u0026lt;路径\u0026gt;：你希望搜索的目录路径（例如 /path/to/directory 或 / 等）。 \u0026lt;查找条件\u0026gt;：你用来限制查找条件的参数（例如 -type f 查找文件，-name \u0026quot;*.log\u0026quot; 查找所有 .log 文件等）。 -exec \u0026lt;命令\u0026gt;：对于每个找到的文件，执行指定的命令。 {}：是一个占位符，代表 find 查找到的每个文件或目录的路径。每个找到的文件路径会替换 {}。 \\;：表示命令的结束，-exec 后的命令必须以 \\; 结束。 示例： # 列出每个找到文件的详细信息： 查找 /path/to/directory 中的所有 .log 文件，并使用 ls -l 列出文件的详细信息：\nfind /path/to/directory -type f -name \u0026#34;*.log\u0026#34; -exec ls -l {} \\; ls -l {} 会对每个 .log 文件执行，{} 会被替换为文件的路径。 显示每个文件的大小： 查找 /path/to/directory 中的大于 1GB 的文件，并使用 du -sh 显示文件的大小：\nfind /path/to/directory -type f -size +1G -exec du -sh {} \\; du -sh {} 对每个找到的文件执行，显示文件的大小。 删除找到的文件： 查找并删除 /path/to/directory 下所有 .bak 文件：\nfind /path/to/directory -type f -name \u0026#34;*.bak\u0026#34; -exec rm -f {} \\; rm -f {} 会删除每个 .bak 文件。 注意： # \\; 需要转义，因为 ; 是一个特殊字符，转义后它就被传递给 find 命令，表示命令的结束。如果不转义，会被 shell 误解。\n如果希望在\n-exec 执行时一次性处理多个文件，可以使用\n+ 来替代\n\\; ，这样会将多个文件作为参数传递给命令（比\n\\; 效率更高）：\nfind /path/to/directory -type f -size +1G -exec du -sh {} + 这样，-exec 提供了灵活的方式来对查找到的文件进行处理。\nk8s的探针有哪些 有啥作用 怎么用 # Kubernetes 的探针（Probes）是用于检测容器健康状态的机制，帮助集群管理器（Kubernetes）判断容器是否正常工作，并采取相应的动作（如重启容器或不调度容器）。Kubernetes 提供了三种类型的探针：\n1. Liveness Probe（存活探针） # 作用：检查容器是否仍在正常运行。如果存活探针失败，Kubernetes 会认为容器已经进入了不健康状态，然后重启容器。适用于容器在启动后可能出现的问题，比如应用死锁、无限循环等。 使用场景：应用启动完成后长时间无响应，可能需要重启容器以恢复正常。 示例：\nlivenessProbe: httpGet: path: /healthz port: 8080 initialDelaySeconds: 3 periodSeconds: 5 解释：\nhttpGet：通过 HTTP GET 请求检查容器状态。 path 和 port：指定检查路径和端口。 initialDelaySeconds：在容器启动后，延迟多少秒再开始探测。 periodSeconds：每隔多少秒执行一次探测。 2. Readiness Probe（就绪探针） # 作用：检查容器是否已经准备好接受流量。如果就绪探针失败，Kubernetes 会停止将流量路由到这个容器。就绪探针失败时不会重启容器，仅仅是停止流量的转发。 使用场景：应用启动时需要加载大量数据，或者依赖外部服务（如数据库）。容器可以在准备好之前处于“未就绪”状态，避免收到请求导致错误。 示例：\nreadinessProbe: httpGet: path: /readiness port: 8080 initialDelaySeconds: 5 periodSeconds: 5 解释：\nhttpGet：通过 HTTP GET 请求检查容器是否准备好。 initialDelaySeconds：延迟多少秒后开始检查容器是否准备好。 periodSeconds：每隔多少秒检查一次容器的就绪状态。 3. Startup Probe（启动探针） # 作用：启动探针用于标识应用程序何时完全启动。它和就绪探针的区别在于，它只在容器启动期间使用，如果启动探针失败，容器将会被重启。启动探针比就绪探针更适合用于检测长时间启动的应用。 使用场景：一些应用在启动时需要较长时间，例如初始化大量数据或者连接外部资源。可以通过启动探针来告诉 Kubernetes 延迟执行健康检查，避免不必要的重启。 示例：\nstartupProbe: httpGet: path: /startup port: 8080 failureThreshold: 30 periodSeconds: 10 解释：\nhttpGet：通过 HTTP GET 请求检查容器启动状态。 failureThreshold：在启动探针失败多少次后，容器才会被重启。 periodSeconds：每隔多少秒进行一次检查。 各探针之间的关系： # Liveness Probe：主要用于检测容器是否健康。如果失败，容器会被杀死并重启。 Readiness Probe：主要用于检测容器是否准备好接收流量。如果失败，Kubernetes 会停止将流量发送到该容器。 Startup Probe：专门用于应用启动期间的健康检查。如果容器启动较慢，启动探针可以防止过早的健康检查失败。 使用探针的最佳实践： # 合理配置延迟和周期：在容器刚启动时，避免探针立即进行检查，使用 initialDelaySeconds 和 periodSeconds 来配置合理的延迟和周期。 设置适当的失败阈值：failureThreshold 控制容器失败后重启的次数，可以根据实际情况调整，避免频繁重启。 分开使用不同类型的探针：例如，启动探针用于处理长时间启动的应用，而就绪探针用于检测应用是否准备好接收流量。 使用外部资源检查：对于依赖外部服务（如数据库、缓存等）的应用，可以通过探针检查这些依赖是否可用。 例子：一个完整的 pod 配置 # apiVersion: v1 kind: Pod metadata: name: my-app spec: containers: - name: my-app-container image: my-app-image ports: - containerPort: 8080 livenessProbe: httpGet: path: /healthz port: 8080 initialDelaySeconds: 5 periodSeconds: 10 readinessProbe: httpGet: path: /readiness port: 8080 initialDelaySeconds: 3 periodSeconds: 5 startupProbe: httpGet: path: /startup port: 8080 failureThreshold: 30 periodSeconds: 10 这个示例中：\nLiveness Probe：容器每 10 秒检查一次 /healthz 路径，初始延迟 5 秒。 Readiness Probe：容器每 5 秒检查一次 /readiness 路径，初始延迟 3 秒。 Startup Probe：容器每 10 秒检查一次 /startup 路径，失败 30 次后才会重启。 通过合理配置探针，可以显著提升系统的可靠性和稳定性。\npod deploy ingress servie的概念 # 在 Kubernetes 中，Pod、Deployment、Ingress 和 Service 都是核心的资源对象，它们各自有不同的功能和目的。我们来逐个解释这些概念。\n1. Pod # 定义：Pod 是 Kubernetes 中的最小部署单位，它可以包含一个或多个容器。容器共享同一个网络命名空间、存储卷以及其他资源，因此它们可以轻松地互相通信。 作用：Pod 是运行应用程序容器的地方，确保容器在同一个物理或虚拟机上运行，并且能够共享资源和网络。 示例：当你创建一个 Pod 时，它可以包含一个 Web 服务容器和一个 Redis 容器，两个容器在同一个 Pod 内，可以通过 localhost 相互访问。 apiVersion: v1 kind: Pod metadata: name: my-pod spec: containers: - name: nginx image: nginx:latest - name: redis image: redis:latest 2. Deployment # 定义：Deployment 是 Kubernetes 中用于管理和部署 Pod 的资源对象。它定义了如何部署 Pod 的副本，并确保一定数量的 Pod 实例在集群中始终处于运行状态。Deployment 可以自动管理 Pod 的滚动更新和回滚。 作用：Deployment 用于自动化 Pod 的部署、扩展和更新。当你需要在 Kubernetes 中管理一个应用的多个副本，并确保它们始终健康运行时，使用 Deployment 是很合适的。 示例：一个简单的 Deployment，管理 3 个 Nginx 容器副本。 apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:latest 3. Service # 定义：Service 是 Kubernetes 中一种抽象，它为一组运行中的 Pod 提供一个稳定的网络访问入口。Pod 在生命周期内的 IP 地址是动态的，因此需要通过 Service 来提供一个统一的访问点。 作用：Service 解决了 Pod 间通信问题，并为外部访问提供稳定的 IP 或 DNS 名称。Kubernetes 支持不同类型的 Service，常见的有 ClusterIP（默认，集群内部访问）、NodePort（外部访问通过节点的端口）、LoadBalancer（外部访问通过云提供商的负载均衡器）、ExternalName（将 Service 映射为外部服务）。 示例：创建一个 Service 来暴露 Deployment 中的 Nginx Pod。 apiVersion: v1 kind: Service metadata: name: nginx-service spec: selector: app: nginx ports: - protocol: TCP port: 80 targetPort: 80 type: ClusterIP 这个 Service 会把 80 端口的流量路由到所有具有标签 app: nginx 的 Pod 上。这里的 ClusterIP 类型表示该 Service 仅在集群内部可访问。\n4. Ingress # 定义：Ingress 是 Kubernetes 中用于管理外部 HTTP 和 HTTPS 访问的资源对象。它定义了如何将外部请求路由到集群内的 Service。Ingress 控制器是实现 Ingress 资源的实际组件，它通常是基于反向代理或负载均衡器（如 Nginx、Traefik）来工作的。 作用：Ingress 使得你可以通过一个单一的公共 IP 地址或 DNS 名称暴露多个服务，进行 URL 路由，支持 SSL/TLS 加密等功能。 示例：创建一个 Ingress，指定 HTTP 请求如何路由到不同的 Service。 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: my-ingress spec: rules: - host: example.com http: paths: - path: /nginx pathType: Prefix backend: service: name: nginx-service port: number: 80 这个 Ingress 会将访问 http://example.com/nginx 的请求路由到名为 nginx-service 的 Service 上。\n总结： # Pod：Kubernetes 中的最小单位，包含一个或多个容器，负责实际运行应用。 Deployment：管理 Pod 的创建、更新、滚动升级等。 Service：为一组 Pod 提供稳定的访问入口，确保 Pod 的通信。 Ingress：管理 HTTP(S) 请求，允许外部流量通过指定的规则路由到不同的 Service。 这些资源协同工作，帮助 Kubernetes 提供高效、可靠、可扩展的应用部署和管理。在开发和部署多层应用时，这些组件组合使用，可以确保应用在 Kubernetes 集群中的稳定运行，并使得外部流量可以通过指定的方式访问内部应用。\n# 虚拟化技术有哪些 # 虚拟化技术是一种通过软件来模拟硬件资源，使得多个操作系统或应用程序能够共享同一台物理机器的技术。虚拟化可以在硬件、操作系统、存储和网络等多个层面进行，它极大地提高了资源的利用率，提升了管理和扩展的灵活性。以下是几种常见的虚拟化技术：\n1. 硬件虚拟化（Server Virtualization） # 定义：硬件虚拟化使得在一台物理服务器上运行多个虚拟机（VM）。每个虚拟机有自己的操作系统、应用程序和虚拟硬件。\n技术：基于虚拟机监控器（Hypervisor）实现，通常分为两类：\n类型 1（裸机虚拟化）\n：直接运行在物理硬件上，无需操作系统支持。常见的有：\nVMware ESXi Microsoft Hyper-V Xen KVM (基于 Linux 的虚拟化技术) 类型 2（托管虚拟化）\n：运行在宿主操作系统之上，通过宿主操作系统提供的接口来管理虚拟机。常见的有：\nVMware Workstation Oracle VirtualBox Parallels Desktop 优点：实现了物理资源的隔离和管理，能够在同一台物理服务器上运行多个操作系统，充分利用硬件资源。\n2. 操作系统级虚拟化（OS-level Virtualization） # 定义：操作系统级虚拟化与硬件虚拟化的不同之处在于，它不模拟硬件，而是通过操作系统的内核将系统划分为多个容器，每个容器都运行独立的应用程序和服务。 技术： Docker：基于 Linux 内核的容器化技术，通过 cgroups 和 namespaces 提供进程隔离，实现应用层的虚拟化。 LXC（Linux Containers）：提供一种轻量级的容器化技术，允许在同一个主机上运行多个隔离的 Linux 环境。 OpenVZ：类似于 LXC，但 OpenVZ 提供更为严格的资源管理和隔离。 优点：相较于硬件虚拟化，容器启动更快、资源消耗更少，适合微服务架构和云原生应用。 3. 网络虚拟化（Network Virtualization） # 定义：通过虚拟化技术来抽象和管理物理网络资源，使网络流量在虚拟网络中进行转发和隔离，能够实现灵活的网络管理和资源分配。 技术： SDN（软件定义网络）：通过集中式控制器管理网络流量，实现网络虚拟化。 NFV（网络功能虚拟化）：将传统的网络功能（如防火墙、路由器等）通过虚拟化技术转化为软件形式，以便运行在通用硬件上。 VXLAN（Virtual Extensible LAN）：一种网络虚拟化技术，通常用于在不同数据中心之间创建虚拟局域网。 优点：提高了网络的灵活性和可扩展性，能够根据需求动态调整网络配置。 4. 存储虚拟化（Storage Virtualization） # 定义：存储虚拟化将多个物理存储设备抽象为一个统一的虚拟存储池，简化存储管理并提高资源利用率。 技术： SAN（Storage Area Network）：通过高速网络将存储设备连接起来，形成一个独立的存储网络。 NAS（Network-Attached Storage）：通过网络协议（如 NFS 或 SMB）将存储设备连接到网络，提供文件级存储服务。 虚拟化存储设备：如 VMware vSAN，可以将物理存储设备整合成一个统一的虚拟存储池，简化存储管理。 优点：简化存储管理，提高存储资源的利用率，支持更灵活的数据访问和灾难恢复。 5. 桌面虚拟化（Desktop Virtualization） # 定义：通过虚拟化技术将桌面操作系统（如 Windows）运行在数据中心或云环境中，用户通过终端访问虚拟桌面。 技术： VDI（Virtual Desktop Infrastructure）：将桌面操作系统虚拟化，并集中管理，用户可以通过终端访问虚拟桌面。 RDS（Remote Desktop Services）：通过 Windows 服务器为多个用户提供远程桌面会话。 Citrix XenDesktop 和 VMware Horizon：企业级桌面虚拟化解决方案。 优点：集中管理桌面，提高安全性和可维护性，支持远程访问，减少物理硬件需求。 6. 应用虚拟化（Application Virtualization） # 定义：通过虚拟化技术，将应用程序的运行环境与操作系统解耦，使得应用可以在没有安装的情况下运行。 技术： Microsoft App-V：允许应用程序在虚拟环境中运行，不依赖于本地操作系统的安装。 VMware ThinApp：将应用程序打包成独立的可执行文件，用户无需安装即可运行。 优点：减少应用程序冲突，简化部署，提升管理效率。 7. GPU 虚拟化 # 定义：通过虚拟化技术将物理 GPU 资源分配给多个虚拟机，使得每个虚拟机都能像物理机一样使用 GPU 加速。 技术： NVIDIA vGPU：允许多个虚拟机共享一块物理 GPU，实现高效的图形和计算加速。 AMD MxGPU：AMD 的 GPU 虚拟化技术，支持多用户共享 GPU 资源。 优点：可以为图形密集型应用提供虚拟化支持，例如虚拟桌面、科学计算、深度学习等领域。 总结： # 虚拟化技术涵盖了从硬件、操作系统、存储到网络和应用程序的多个层面，不同的虚拟化技术根据应用场景和需求具有各自的优势。通过虚拟化技术，企业可以实现资源的高效利用，提升灵活性，降低成本，同时简化管理和维护。\n容器化技术有哪些 # 容器化技术是将应用及其所有依赖打包到一个容器中，使得应用可以在任何环境中一致地运行，而不依赖于宿主操作系统的配置。这种技术广泛应用于微服务架构、云计算及持续集成/持续交付（CI/CD）等场景。以下是一些常见的容器化技术：\n1. Docker # 概述：Docker 是最广泛使用的容器化技术之一，它通过将应用和其所有依赖项打包成镜像，提供了一种轻量级的虚拟化方式。Docker 容器可以在任何支持 Docker 的平台上运行。\n功能\n：\n轻量级：容器共享宿主操作系统的内核，因此比虚拟机占用更少的资源。 可移植性：容器内的应用可以在不同的环境中保持一致的行为。 Docker Hub：官方的公共镜像仓库，包含大量的开源镜像。 相关工具\n：\nDocker Compose：用于定义和管理多容器应用，帮助开发者在本地快速搭建服务。 Docker Swarm：Docker 的原生集群管理工具，用于容器的自动调度和负载均衡。 2. Kubernetes (K8s) # 概述：Kubernetes 是一个开源的容器编排平台，用于自动化容器的部署、扩展和管理。它最初由 Google 开发，现在由 CNCF（Cloud Native Computing Foundation）维护。\n功能\n：\n自动化部署：通过配置文件自动化容器的部署、升级和回滚。 容器编排：管理容器的调度、负载均衡、滚动更新和集群健康检查。 高可用性和扩展性：支持多节点、跨主机的容器管理，自动扩容。 支持多种容器运行时，如 Docker、containerd 和 CRI-O。 相关工具\n：\nHelm：Kubernetes 的包管理工具，简化应用的安装和升级。 Kubectl：Kubernetes 的命令行工具，用于管理集群中的资源。 3. Podman # 概述：Podman 是一个与 Docker 类似的容器引擎，但与 Docker 不同的是，它不依赖于守护进程。Podman 是无守护进程的容器管理工具，可以在无 root 权限的情况下运行容器。\n功能\n：\n与 Docker 命令兼容：Podman 使用与 Docker 相似的命令行界面。 安全性：由于 Podman 是无守护进程的，它没有持久的守护进程运行，因此它的安全性较高。 容器集群管理：支持多容器组（pod），类似于 Kubernetes 中的 Pod。 4. Docker Swarm # 概述：Docker Swarm 是 Docker 提供的容器集群管理工具，可以将多个 Docker 主机组成一个集群，并提供容器的负载均衡、服务发现和自动扩容等功能。\n功能\n：\n集群管理：Docker Swarm 允许多个 Docker 主机以集群的形式工作，通过 Swarm 集群的管理，可以确保容器的高可用性。 服务发现：Swarm 中的服务会自动发现并连接。 自动负载均衡：Swarm 会将流量均衡地分发到各个容器实例。 优点：对于已有 Docker 环境的用户来说，Swarm 提供了简单易用的容器编排功能。\n5. OpenShift # 概述：OpenShift 是 Red Hat 提供的企业级 Kubernetes 平台，是 Kubernetes 的一个增强版，提供了许多额外的功能，如更强的安全性和集成开发工具等。\n功能\n：\n集成开发工具：OpenShift 提供了一个 Web 控制台，可以进行源代码管理、构建自动化、持续交付等。 安全性：OpenShift 默认启用了许多安全机制，如强制使用 SELinux。 企业支持：作为商业产品，OpenShift 提供了专业的技术支持。 相关工具\n：\nOpenShift CLI（oc）：用于与 OpenShift 集群进行交互的命令行工具。 6. Apache Mesos # 概述：Apache Mesos 是一个分布式系统内核，旨在处理大规模的计算集群，它不仅支持容器化应用，还能运行各种非容器化的应用程序。\n功能\n：\n容器调度：Mesos 支持 Docker 和其他容器技术的调度。 弹性：支持动态伸缩，能够自动扩展或缩减资源。 多种任务支持：除了容器，Mesos 还可以管理虚拟机和传统应用程序。 7. Containerd # 概述：Containerd 是一个高效的容器运行时（runtime），提供容器的生命周期管理，包括容器的启动、停止、镜像管理等。它是 Kubernetes 默认使用的容器运行时之一。\n功能\n：\n容器生命周期管理：从镜像拉取、容器启动到停止。 运行时接口：作为容器引擎的核心，提供了与 Kubernetes 等平台的接口。 优点：提供高效、低开销的容器管理，适用于生产环境。\n8. Rancher # 概述：Rancher 是一个开源的容器管理平台，用于简化 Kubernetes 集群的部署和管理。它支持多种容器编排平台，但最常见的是 Kubernetes。\n功能\n：\n多集群管理：支持跨多个 Kubernetes 集群的统一管理。 集成工具：提供日志、监控、服务网格、CI/CD 等集成。 简化操作：通过 Web 控制台提供简单的 UI 和 API，降低了 Kubernetes 管理的复杂度。 9. LXC (Linux Containers) # 概述：LXC 是一种轻量级的操作系统虚拟化技术，可以在同一个 Linux 内核上运行多个隔离的 Linux 系统，它比传统的虚拟机更轻便，但提供了更高的隔离性。\n功能\n：\n类似虚拟机：每个容器拥有独立的文件系统、网络和进程空间。 高效：比传统的虚拟化技术占用更少的资源。 网络隔离：提供容器间网络隔离，支持桥接、虚拟局域网等多种网络模式。 10. Kata Containers # 概述：Kata Containers 是一个开源项目，旨在将容器的速度与虚拟机的安全性相结合。它通过将每个容器包装为一个轻量级虚拟机来提供隔离。\n功能\n：\n高安全性：每个容器在虚拟机中运行，提供比普通容器更强的安全隔离。 兼容性：与现有的容器编排工具（如 Kubernetes）兼容。 总结： # 容器化技术通过虚拟化应用及其依赖，提升了部署的效率和一致性。从最常见的 Docker 和 Kubernetes，到更专业的容器编排平台（如 OpenShift 和 Rancher），不同的容器化技术提供了从开发到生产环境全方位的支持。选择合适的容器化技术取决于应用的规模、需要的安全性、管理复杂度等因素。\nfree -mh的buffer和cache的区别 # free -mh 命令用于显示系统的内存使用情况，其中 -m 表示以 MB 为单位显示，-h 表示以人类可读的格式显示（如 KB, MB, GB）。在输出中，你会看到 buffer 和 cache 两个字段，它们分别表示不同类型的内存使用。下面是它们的区别：\n1. Buffer # 定义：Buffer 是用于临时存储输入/输出操作的数据缓冲区，主要是磁盘 I/O 操作的缓存。 作用：在 Linux 系统中，当有文件被写入磁盘时，数据不会直接写入磁盘，而是先写入内存的 buffer 中，直到合适的时机再将数据写入磁盘。这样可以提高磁盘操作的性能。 使用：Buffer 主要用于存储磁盘块的管理数据，像文件系统元数据、文件的内容等等。 2. Cache # 定义：Cache 是文件系统缓存，用于存储经常访问的文件内容。当你频繁访问某些文件时，这些文件会被存储在内存中的 cache 区域，这样可以加速文件的读取速度。 作用：通过缓存文件内容（如文件内容、目录项等），系统可以更快地响应对这些文件的访问。缓存会根据内存的使用情况动态调整，缓存区也会被用作文件内容读取的高速缓存。 使用：Cache 保存了文件的实际内容，是对文件数据本身的缓存，而不是文件系统的元数据。 Buffer 和 Cache 的区别 # 功能：Buffer 主要是用于管理磁盘 I/O 操作的缓冲区，而 Cache 主要是用于存储文件的内容。 内容：Buffer 存储的是磁盘的元数据（例如：目录、文件的块），而 Cache 存储的是文件的实际数据（例如：文件的内容）。 使用目的：Buffer 目的是加速磁盘写入操作，Cache 目的是加速文件读取操作。 举例 # 假设你打开一个大文件并进行读取，系统会先将文件的内容加载到内存的 cache 中，以便快速访问。之后，你对该文件进行写入，数据会先写入内存的 buffer 中，再逐步写回磁盘。\n总结 # Buffer 是与磁盘操作相关的内存区域，涉及到磁盘 I/O 的缓冲。 Cache 是与文件数据存取相关的内存区域，用于加速对文件内容的读取。 在 free 命令的输出中，buffer 和 cache 都是属于被 缓存的内存，这些内存如果系统需要被释放，内核会自动将其回收，释放给其他应用程序使用。因此，free 命令中的 buffer 和 cache 并不代表“已占用”的内存，因为它们在空闲时可以被回收。\n查看操作系统的命令 # 要查看操作系统的相关信息，你可以使用不同的命令来获取操作系统的版本、内核版本、系统架构等详细信息。以下是一些常用的命令：\n1. 查看操作系统的版本 # Linux:\ncat /etc/os-release 或者：\nuname -a 这条命令将输出操作系统的内核信息、主机名、操作系统版本等。\nUbuntu:\nlsb_release -a 或者：\ncat /etc/lsb-release CentOS:\ncat /etc/centos-release Debian:\ncat /etc/debian_version 2. 查看操作系统内核版本 # uname -r 该命令将显示内核的版本号。\n3. 查看系统架构 # uname -m 这条命令会返回系统的架构，例如 x86_64（表示 64 位系统）。\n4. 查看主机名 # hostname 这条命令将显示当前操作系统的主机名。\n5. 查看操作系统的硬件架构 # lscpu 这个命令显示关于CPU架构、核心数、线程数等的详细信息。\n6. 查看操作系统的内存信息 # free -h 此命令将以人类可读的格式显示系统的内存使用情况。\n7. 查看操作系统的硬盘信息 # lsblk 该命令列出系统上的所有块设备，显示磁盘及其分区信息。\n8. 查看操作系统的文件系统信息 # df -h 该命令显示文件系统的磁盘使用情况，包括每个挂载点的使用情况。\n9. 查看操作系统的进程信息 # top 该命令会显示当前系统运行中的进程以及系统资源的使用情况。\n10. 查看操作系统的日志 # journalctl -xe 查看系统的日志输出，特别是与系统事件相关的信息。\n这些命令是最常见的查看操作系统信息的方式，根据你需要查看的具体信息，选择适合的命令。\n查看用户数的命令 # 要查看系统中当前的用户数，可以使用以下命令：\n1. 查看当前登录的用户数 # who | wc -l who 命令显示当前登录系统的用户信息，wc -l 则统计登录的用户数。\n2. 查看系统中所有用户的数目 # cat /etc/passwd | wc -l /etc/passwd 文件包含了所有的用户信息，每一行代表一个用户。wc -l 会统计文件中的行数，从而得到用户的总数。\n3. 查看当前活跃的用户数 # w -h | wc -l w 命令显示当前登录的用户及其活跃状态，-h 选项去掉头部信息，wc -l 统计活跃用户数。\n4. 查看系统中的用户数量（仅普通用户） # awk -F: \u0026#39;{ if ($3 \u0026gt;= 1000) print $1 }\u0026#39; /etc/passwd | wc -l 这个命令通过 awk 过滤 /etc/passwd 中 UID 大于等于 1000 的用户（通常是普通用户），然后使用 wc -l 统计用户数量。一般情况下，UID 小于 1000 的是系统用户。\n这些命令能够帮助你查看当前系统中不同类别用户的数量。\n禁止root直接登录 修改哪个文件的什么参数 # 为了禁止 root 用户直接登录，你可以通过修改 SSH 配置文件来限制。具体步骤如下：\n1. 编辑 SSH 配置文件 # 使用文本编辑器打开 SSH 配置文件（通常是 /etc/ssh/sshd_config）：\nsudo vim /etc/ssh/sshd_config 2. 修改配置参数 # 找到并修改以下配置项：\nPermitRootLogin：这是控制 root 用户是否可以通过 SSH 登录的参数。设置为 no 即可禁止 root 用户直接登录。 将 PermitRootLogin 设置为 no：\nPermitRootLogin no 3. 保存并退出 # 保存并关闭文件（在 vim 中，按 Esc 键，然后输入 :wq 保存并退出）。\n4. 重启 SSH 服务 # 配置修改后，重启 SSH 服务使修改生效：\nsudo systemctl restart sshd 或者在一些系统中可能需要使用以下命令：\nsudo service ssh restart 5. 验证 # 尝试用 root 用户通过 SSH 登录，应该会被禁止。可以用普通用户登录并验证：\nssh root@\u0026lt;your-server-ip\u0026gt; 你应该会看到类似以下的信息：\nPermission denied, please try again. 总结 # 修改 /etc/ssh/sshd_config 文件中的 PermitRootLogin 参数，将其设置为 no 来禁止 root 用户通过 SSH 登录。 重启 SSH 服务使配置生效。 # # # # # # # # # "},{"id":199,"href":"/docs/%E9%9D%A2%E8%AF%95%E7%94%A8-golang-%E6%89%8B%E6%92%B8-lru-mian-shi-yong-golang-shou-lu-lru/","title":"面试用 Golang 手撸 LRU 2024-04-03 14:47:00.767","section":"Docs","content":" 上周参加一个云原生 DevOps 开发的面试，第一轮面试问一些技能、项目相关问题，最后留了 20 分要求用 Golang 实现 LRU。 # 过程大概用了半个多小时，大概写出来了 80 %，一面勉强过了。写篇文章记录下，加深印象。 # LRU 是什么 # LRU（Least recently used，最近最少使用)，是一种常见的缓存淘汰算法，当缓存满时，淘汰最近最久未使用的元素。 # 其基本思想是如果一个数据在最近一段时间没有被访问到，那么可以认为在将来它被访问的可能性也很小。因此，当缓存满时，最久未被访问的数据最先被淘汰。具体做法是将最近使用的元素存放到靠近缓存顶部的位置，当一个新条目被访问时，LRU 将它放置到缓存的顶部。当缓存满时，较早之前访问的条目将从缓存底部被移除。 # LRU 底层用 Ha****sh Map 来缓存数据。 # 手撸 LRU # 面试手撸 LRU 很刺激~ # 题目要求 # **LRU **缓存 int 类型的值，且缓存具有容量限制； # 实现 LRU Get(key int) 方法，如果关键字 key 存在于缓存中，则返回关键字的值，否则返回 -1； # 实现 LRU Put(key int，value int) 方法，如果关键字 key 存在于缓存中，则变更其数据值 value；如果不存在，则向缓存中插入该 key，value。如果插入操作导致导致 key 数量超过缓存容量，则应该逐出最久未使用的 key； # Get、Put 以 O(1) 的时间复杂度运行。 # 图解 LRU # 1、首先要想缓存数据，通过 hash map ，效率高，操作方便；定义当前缓存的数量和最大容量 # 2、因为需要保证最久未使用的数据在缓存满的时候将其删除，所以就需要一个数据结构能辅助完成这个逻辑。 # 所以说使用链表这种结构可以方便删除结点，新增结点，但由于最久未使用的结点在尾结点，通过单链表不方便操作，所以双链表会更加方便操作尾结点。 # 所以这里利用双链表数据结构，head、tail 两个指针不存数据，这样保证每个结点操作逻辑一致。 # 然后上面的 map 的 key 为用户传入的 key，map 的 value 为双链表的中 node，即通过 key 来获取链表中的 node，node 里存有用户传的 value # 代码实现\npackage main import \u0026#34;fmt\u0026#34; // 定义 LRU 结构体 type LRUCache struct { // 缓存的当前容量 size int // 缓存的容量限制 limit int // 缓存数据的 map cache map[string]*DoubleLinkList // 定义头、尾指针 head, tail *DoubleLinkList } // 定义双链表 type DoubleLinkList struct { key string value int next, pre *DoubleLinkList } // Get 方法 func (lc *LRUCache) Get(key string) int { // 先查找该 key 是否存在缓存中, 如果不存在返回 -1 if _, ok := lc.cache[key]; !ok { return -1 } // 如果存在, 从缓存中获取该 key 的 value node := lc.cache[key] // 将该结点移动都 head 结点后面，head、tail 结点不存数据 lc.moveNodeToHead(node) return node.value } // Put 方法 func (lc *LRUCache) Put(key string, value int) { // 如果缓存中没有当前 key if node, ok := lc.cache[key]; !ok { // 实例化一个 node node = \u0026amp;DoubleLinkList{ key: key, value: value, } // 将上面实例化的 node 缓存到 LRU 中 lc.cache[key] = node // 因为刚添加进来，依据 LRU 性质, 应添加到双链表 head 后面 lc.addNodeToHead(node) // 容量 + 1 lc.size++ // 如果当前容量已经大于缓存所支持的最大容量限制, 则删除长时间没有用的结点 if lc.size \u0026gt; lc.limit { // tail 的前一个结点即是最久未使用的结点 tailPre := lc.tail.pre // 将最久未使用的结点从链表中删除 lc.removeNode(tailPre) // 将当前数据从缓存中删除 delete(lc.cache, tailPre.key) // 当前容量建议 - 1 lc.size-- } // 如果缓存中存在该 key, 返回该 key 的 value, 根据 LRU 的性质将该结点移动到 head 的后面, 这样就能保证不常用的就会被放到链表尾部 } else { // 从缓存中获取该 key 的 node node := lc.cache[key] // 将 node 的 value 设置为传入的 value node.value = value // 将该 node 移动到 head 下一个结点 lc.moveNodeToHead(node) } } func (lc *LRUCache) moveNodeToHead(node *DoubleLinkList) { // 先将该结点从双链表中移除 lc.removeNode(node) // 然后将结点添加到 head 结点后面 lc.addNodeToHead(node) } // 将结点从双链表中移除 func (lc *LRUCache) removeNode(node *DoubleLinkList) { // 将该结点的前一个结点的后继指针指向该结点的下一个结点 node.pre.next = node.next // 再将该结点的后一个结点前驱指针指向该结点的前一个结点 node.next.pre = node.pre } // 将该结添加到 head 结点后面，因为 head 结点不存数据 func (lc *LRUCache) addNodeToHead(node *DoubleLinkList) { // 将该结点的后继指针指向 head 结点的下一个结点 node.next = lc.head.next // 将该结点的前驱指针指向 head 结点 node.pre = lc.head // 将 head 结点的下一个节点的前驱结点指向该结点 lc.head.next.pre = node // 将 head 结点的后继指针指向该结点 lc.head.next = node } 测试代码\n由于测试需要，实现一个打印 LRUCache 数据的函数和一个初始化 LRUCache 的函数\nfunc (lc *LRUCache) ListLRUCache() { node := lc.head.next for node != nil { fmt.Printf(\u0026#34;key: %s, value: %d\\\\n\u0026#34;, node.key, node.value) node = node.next } } func NewLRUCache() *LRUCache { // 实例化一个 LRUCache lc := \u0026amp;LRUCache{ // 缓存容量为 5 limit: 5, cache: make(map[string]*DoubleLinkList), head: \u0026amp;DoubleLinkList{ key: \u0026#34;head\u0026#34;, value: 0, }, tail: \u0026amp;DoubleLinkList{ key: \u0026#34;tail\u0026#34;, value: 0, }, } // 初始化双链表，将 head 结点的后继指针指向 tail 结点 lc.head.next = lc.tail // 将 tail 结点的前驱指针指向 head 结点 lc.tail.pre = lc.head } 下面分别一些场景测试：\n场景一：\n获取 key 的 value，是否该 key 的 node 会被添加到 **head.next **处\nfunc main() { // 初始化 LRUCache lc := NewLRUCache() // 填充满 LRUCache lc.Put(\u0026#34;key1\u0026#34;, 1) lc.Put(\u0026#34;key2\u0026#34;, 2) lc.Put(\u0026#34;key3\u0026#34;, 3) lc.Put(\u0026#34;key4\u0026#34;, 4) lc.Put(\u0026#34;key5\u0026#34;, 5) // 获取 key2, 看 key2 是否会被添加到 head.next 处 fmt.Println(lc.Get(\u0026#34;key2\u0026#34;)) lc.ListLRUCache() } 测试结果: 发现 key2 被添加到 head.next 处了 key2: 2 key: key2, value: 2 key: key5, value: 5 key: key4, value: 4 key: key3, value: 3 key: key1, value: 1 场景二：\n填充满 LRUCache 后，使用 Put 更新某个不存在的 key，看是否会移除最久未使用的 key。\nfunc main() { // 初始化 LRUCache lc := NewLRUCache() // 填充满 LRUCache lc.Put(\u0026#34;key1\u0026#34;, 1) lc.Put(\u0026#34;key2\u0026#34;, 2) lc.Put(\u0026#34;key3\u0026#34;, 3) lc.Put(\u0026#34;key4\u0026#34;, 4) lc.Put(\u0026#34;key5\u0026#34;, 5) // 在 put 之前先打印 LRUCache 的数据，方便 put 后对比 fmt.Println(\u0026#34;before put: \u0026#34;) lc.ListLRUCache() // 更新一个缓存中不存在的 key lc.Put(\u0026#34;key6\u0026#34;, 6) fmt.Println(\u0026#34;after put: \u0026#34;) lc.ListLRUCache() } 测试结果：可以发现 key6 添加到 head.next 处且最久未用的 key1 被移除了 before put: key: key5, value: 5 key: key4, value: 4 key: key3, value: 3 key: key2, value: 2 key: key1, value: 1 after put: key: key6, value: 6 key: key5, value: 5 key: key4, value: 4 key: key3, value: 3 key: key2, value: 2 场景三：\n填充满 LRUCache 后，使用 Put 存在的 key，看是否会更新且该 key 的 node 添加到 head.next 处。\nfunc main() { // 初始化 LRUCache lc := NewLRUCache() // 填充满 LRUCache lc.Put(\u0026#34;key1\u0026#34;, 1) lc.Put(\u0026#34;key2\u0026#34;, 2) lc.Put(\u0026#34;key3\u0026#34;, 3) lc.Put(\u0026#34;key4\u0026#34;, 4) lc.Put(\u0026#34;key5\u0026#34;, 5) // 在 put 之前先打印 LRUCache 的数据，方便 put 后对比 fmt.Println(\u0026#34;before put: \u0026#34;) lc.ListLRUCache() // 更新一个缓存中存在的 key, 将 key5 的 value 改为 6 lc.Put(\u0026#34;key5\u0026#34;, 6) fmt.Println(\u0026#34;after put: \u0026#34;) lc.ListLRUCache() } 测试结果：发现 key5 的value 变为 6, 且被添加到 head.next 处 before put: key: key5, value: 5 key: key4, value: 4 key: key3, value: 3 key: key2, value: 2 key: key1, value: 1 after put: key: key5, value: 6 key: key4, value: 4 key: key3, value: 3 key: key2, value: 2 key: key1, value: 1 测试基本符合上面的题意。 总结 # LRUCache 在 Redis、Memcached 等分布式缓存系统都广泛使用，了解其原理，对后面使用这些中间件都很有帮助，而且也是常见的面试算法题。 # "},{"id":200,"href":"/docs/%E9%A1%B6%E7%BA%A7devops%E5%B7%A5%E5%85%B7%E5%A4%A7%E7%9B%98%E7%82%B9-ding-ji-devops-gong-ju-da-pan-dian/","title":"顶级devops工具大盘点 2024-08-02 17:43:03.661","section":"Docs","content":"本篇文章中将介绍一些能够帮助你实现 DevOps 目标的核心技术类别和具体技术。\n关于 DevOps 及其工具 # 关于 DevOps 及其工具，需要记住：\n持续改进是目标； DevOps 不是花钱买来的； 分阶段采用工具。 计划工具 # 为什么计划工具对于 DevOps 来说很重要？\n分享目标； 透明性； 赋能。 计划工具示例 # GitLab # GitLab 是一个基于 Web 的 DevOps 生命周期工具。它提供了一个 Git 仓库管理器，具备 wiki、问题跟踪和 CI/CD 管道功能，采用的是 GitLab 公司的开源许可。\nTasktop # Tasktop 允许将所有这些工具添加到敏捷、ALM、PPM 和 ITSM 中，实现了对整个生命周期前所未有的可见性。\nCollabNet VersionOne # VersionOne 支持 Scrum、看板、XP、SAFe 和混合开发方法，并使跨团队、程序、软件组合和企业的计划、跟踪和报告变得更容易。\nPivotal Tracker # 敏捷项目管理工具，是开发人员围绕高优先级共享 backlog 进行实时协作的首选工具。\nTrello # Trello 是一个基于 Web 的看板风格的清单应用程序，是 Atlassian 的子公司。\n很多团队用它来计划各自的工作 sprint。\nAzure Boards # 使用敏捷工具，包括 Scrum、看板和其他敏捷方法仪表板来更好地跟踪软件项目并进行计划。\n这些都是可视化工作、分享计划、跟踪进展、确保朝着目标前进的好方法。\n问题跟踪 # 为什么问题跟踪对 DevOps 来说很重要？\n用户响应； 减少知识损耗； 反馈闭环。 问题跟踪工具示例 # Atlassian Jira # Jira 是由 Atlassian 公司开发的问题跟踪产品，提供了 bug 跟踪和敏捷项目管理功能。\nJetBrains YouTrack # YouTrack 是 JetBrains 公司开发的一款基于浏览器的 bug 跟踪、问题跟踪系统和项目管理软件。它支持基于查询的问题搜索——包括自动完成、批量处理问题、定制问题属性集以及创建自定义工作流。\nZendesk # Zendesk 让客户问题跟踪变得更简单，包括优步和 Airbnb 在内的很多公司都使用 Zendesk 作为他们的问题跟踪软件。\n源码控制 # 为什么源码控制对 DevOps 来说很重要？\n资产管控； 减少传输损耗； 促进团队合作。 SCM 工具示例 # Git # Git 是一个分布式版本控制系统，用于在软件开发期间跟踪源代码的变更。它是为协调程序员之间的工作而设计的，但它也可用于跟踪文件的变更。它专注于速度、数据完整性和对分布式、非线性工作流的支持。\nGitHub # GitHub 提供了 Git 分布式版本控制和源代码管理功能，以及自有的一些特性。\nGitLab # GitLab 的版本控制通过世界级的源代码管理功能帮助你的开发团队共享、协作和最大化他们的生产力。\nBitbucket # Bitbucket 是 Atlassian 公司为使用 Mercurial 或 Git 版本控制系统的源代码和开发项目提供的基于 Web 的版本控制代码库托管服务。\nSubversion # Apache Subversion 是一种软件版本控制系统，采用了 Apache 开发源码许可。软件开发人员使用 Subversion 来维护文件的当前和历史版本，例如源代码、Web 页面和文档。\n构建工具 # 为什么构建工具对 DevOps 来说很重要？\n一致的打包效果； 自动化错误探测； 尽早发现质量问题。 构建工具示例 # Maven/Gradle # Maven 是一个主要用于 Java 项目的自动化构建工具。Maven 还可以用来构建和管理用 C#、Ruby、Scala 和其他语言开发的项目。\nGradle 是一个开源的自动化构建系统，基于 Apache Ant 和 Apache Maven 的概念，引入了一种基于 groovy 的领域特定语言。\nMSBuild # Microsoft Build Engine，更广为人知的名字是 MSBuild，是一个开源免费的托管代码和原生 C++ 代码构建工具集，是.Net Framework 的一部分。Visual Studio 就使用 MSBuild 作为构建工具。\nRake # Rake 是一个软件任务管理和自动化构建工具。用户可以指定任务和描述依赖关系，还可以对同一个命名空间中的组任务指定和描述依赖关系。\nJFrog Artifactory # JFrog Artifactory 是一种用于保存构建过程所产生的用于发布和部署的二进制文件的工具。Artifactory 提供了多种包格式支持，如 Maven、Debian、npm、Helm、Ruby、Python 和 Docker。\nJFrog 提供了高可用性、复制、灾难恢复、可伸缩性，并可集成很多自有和云存储产品。\nSonatype Nexus # Sonatype Nexus 是一个存储库管理器。可用于代理、收集和管理依赖项，这样就不用经常为一堆的 jar 文件感到烦恼。它让软件的发行变得更容易。\n在内部，你可以配置构建工具，把工件发布到 Nexus，然后其他开发人员就可以使用它们了。\nNuGet # NuGet 是.Net 包管理器。NuGet 客户端工具提供了生成和使用包的能力。\nNuGet Gallery 是程序包作者和使用者的中心包仓库。\n测试工具 # 为什么测试工具对 DevOps 来说很重要？\n将注意力放在质量上； 提升产品信心。 测试工具示例 # JUnit # JUnit 是一个用于 Java 的单元测试框架。JUnit 在测试驱动开发当中扮演着非常重要的角色，它是单元测试框架家族 xUnit(源于 SUnit) 的一员。\nxUnit.net # xUnit.net 是一个用于.Net Framework 的开源单元测试工具，由 NUnit 的原作者开发。\nSelenium # Selenium 是用于测试 Web 应用程序的框架。Selenium 提供了一个回放工具，用于编写功能测试，省去了学习脚本语言的麻烦。\nJasmine # Jasmine 是一个开源的 JavaScript 测试框架。它的目标是可以在任何支持 JavaScript 的平台上运行，不干扰应用程序或 IDE，并具有易于阅读的语法。它深受其他单元测试框架的影响，比如 Screw.Unit、JSSpec、JSpec 和 RSpec。\nCucumber # Cucumber 是一个支持行为驱动开发的工具。Cucumber BDD 方法的核心是一个叫作 Gherkin 的语言解析器，可以用用户能够理解的逻辑语言来指定预期的软件行为。\n持续集成（CI）和持续部署（CD） # 为什么持续集成和部署对 DevOps 来说很重要？\n快速反馈； 减少缺陷和等待时间。 CI 工具示例 # Jenkins # Jenkins 是一个免费开源的自动化服务器。Jenkins 帮助自动化软件开发过程中的非人为操作部分。它是一个基于服务器的系统，运行在 Servlet 容器中，比如 Apache Tomcat。\nCircleCI # CircleCI 是全球最大的共享 CI/CD 平台，是代码从想法变成可交付产品的中心枢纽。\n作为最常用的 DevOps 工具之一，CircleCI 每天要处理超过 100 万个构建任务，它可以访问到工程团队协作以及代码运行的数据。Spotify、Coinbase、Stitch Fix 和 BuzzFeed 等公司都使用 CircleCI 来提高工程团队的工作效率，发布更好的产品。\nTravis CI # Travis CI 是一个托管的 CI 服务，用于构建和测试托管在 GitHub 上的软件项目。Travis CI 为私人项目提供各种付费计划和免费开源计划。TravisPro 部署在客户自己的硬件上，提供专有版本的定制部署。\nConcourse # Concourse 是一个用 Go 开发的自动化系统，最常用于 CI/CD，用于伸缩任何类型 (从简单到复杂的) 的自动化管道。\nAWS CodePipeline # AWS CodePipeline 是一个全托管的 CD 服务，可以帮助你自动化发布管道，以便快速可靠地更新应用程序和基础设施。\n每当发生代码变更时，CodePipeline 会根据你定义的发布模型自动化发布过程的构建、测试和部署阶段，让你能够快速、可靠地交付特性和更新。\n你可以轻松地将 AWS 代码管道与第三方服务 (如 GitHub) 或自定义插件集成。在使用 AWS CodePipeline 时，你只需要为所使用的内容付费，没有预付费用。\nAzure Pipelines # 为 Linux、macOS 和 Windows 建立云托管管道。构建 Web、桌面和移动应用程序，并将其部署到云端或本地。\n通过管道自动化构建和部署，减少在具体细节上花费的时间，把更多的时间花在创造性的事情上。\nCD 工具 # 为什么 CD 工具对于 DevOps 来说很重要？\n减少预发布库存； 自动化复杂的管道； 统一团队为客户创造价值的目标。 CD 工具示例 # Spinnaker # Spinnaker 是一个免费开源的 CD 软件平台，最初由 Netflix 开发，后被谷歌收购，并进行了扩展。\nSpinnaker 是一个多云 CD 平台，用于快速发布软件变更。它将强大而灵活的管道管理系统与主要云提供商的集成结合在了一起。\nOctopus Deploy # Octopus Deploy 是一个自动化的部署和发布管理工具，全球领先的 CD 团队都在使用它。\nOctopus 是一个工具集，可以极大简化 DevOps 过程，通过云或虚拟机对大量微服务或应用程序进行持续测试和部署。\nAWS CodeDeploy # AWS CodeDeploy 是一种全托管的部署服务，可以将软件部署自动化到各种计算服务，如 Amazon EC2、AWS Fargate、AWS Lambda 和本地服务器。\n你可以使用 AWS CodeDeploy 来自动化软件部署，减少容易出错的手动操作。\n配置管理工具 # 配置管理工具为什么对于 DevOps 来说很重要？\n保持一致性； 基础设施即代码。 配置管理工具示例 # Terraform # Terraform 是 HashiCorp 公司开发的开源基础结构即代码 (IaC) 软件工具。用户可以使用一种高级配置语言 (叫作 Hashicorp 配置语言或 JSON) 来定义和提供数据中心基础设施。\nBOSH # BOSH 是一个将小型和大型云软件的发布工程、部署和生命周期管理结合起来的项目。BOSH 可以基于数百个 VM 配置和部署软件。它还可以执行监空、故障恢复和软件更新，没有停机时间或者停机时间很短。\n虽然开发 BOSH 是为了部署云计算，但它也可以用于部署几乎任何其他软件 (例如 Hadoop)。BOSH 特别适合大型分布式系统。\n此外，BOSH 还支持多种基础设施即服务 (IaaS) 提供商，如 VMware vSphere、谷歌云平台、Amazon Web Services EC2、Microsoft Azure、OpenStack 和阿里巴巴云。BOSH 提供了一个云供应商接口 (CPI)，用户可用它扩展 BOSH，以便支持其他 IaaS 提供商，如 Apache CloudStack 和 VirtualBox。\nChef # Chef 是一个配置管理工具，用于处理物理服务器、虚拟机和云端的机器配置。很多公司用 Chef 控制和管理他们的基础设施，如 Facebook、Etsy、Cheezburger 和 Indiegogo。\nChef 公司是持续自动化软件的领导者、应用自动化的革新者以及 DevOps 运动的发起者之一。Chef 与全球 1000 多家最具创新性的公司合作，提供快速交付软件的实践和平台，以实现他们的数字化转型愿景。\nAnsible # Ansible 是一款开源的软件配置、配置管理和应用程序部署工具。它运行在 Unix 家族系统上，可以配置 Unix 家族系统和 Windows。它可以使用自己的声明性语言来描述系统配置。\nPuppet # Puppet 提供了定义系统需要哪些软件和配置的能力，然后在初始设置之后维护指定的状态。\nPuppet 使用与 Ruby 类似的声明性领域特定语言 (DSL) 为特定环境或基础设施定义配置参数。Puppet 通过使用一个叫作 Facter 的实用程序来发现系统信息，Facter 是在安装 Puppet 软件包时一起安装的。\nPuppet 主节点通过清单来管理它所控制的所有节点的重要配置信息。\n被主节点控制的其他节点安装了 Puppet 并运行 Puppet 代理 (一个守护进程)。代理节点收集的有关节点的配置信息将发送给主节点。主节点根据应该如何配置来编译目录，其他节点使用这些信息来更新自己的配置。\nPuppet 使用了拉取模式，代理节点定时轮询主机，查询特定于站点和特定于节点的配置。在这个基础设施中，Puppet 代理应用程序通常作为后台服务运行在托管节点上。\n谷歌云部署管理器 # 谷歌云部署管理器是一种基础设施管理服务，让谷歌云平台资源的创建、部署和管理变得更简单。\n云平台 # 为什么云平台对 DevOps 来说很重要？\n友好的自动化； 具有可观察性的运行时。 云平台示例 # Amazon Web Service # Amazon Web Service(AWS) 是一个安全的云服务平台，提供计算能力、数据库存储、内容交付和其他帮助企业扩大规模和增长的功能。\n简而言之，AWS 可以做以下这些事情：\n在云端运行 Web 和应用服务器，托管动态网站； 将所有文件安全地存储在云端，这样你就可以从任何地方访问它们； 使用托管数据库，如 MySQL、PostgreSQL、Oracle 或 SQL Server 来存储信息； 使用内容分发网络 (CDN) 在世界各地快速交付静态和动态文件； 批量发送电子邮件给你的客户。 微软 Azure # 微软 Azure 是微软创建的云计算服务，通过微软数据中心构建、测试、部署和管理应用程序和服务。\n微软 Azure，原来叫作 Windows Azure，是微软的公共云计算平台。它提供一系列云服务，包括计算、分析、存储和网络。用户可以选择这些服务来开发和扩展新的应用程序，或者在公共云中运行现有的应用程序。\n谷歌云平台 # 谷歌云平台由谷歌提供，是一套云计算服务，运行在谷歌用来运行其终端用户产品 (如谷歌搜索、Gmail 和 YouTube) 相同的基础设施上。\n谷歌云平台提供在 web 上部署应用程序所需的计算资源，专注于为个人和企业提供一个构建和运行软件的地方，并通过 Web 连接软件用户。\n你使用谷歌提供的云计算服务来迎接业务方面的挑战，包括数据管理、混合和多云、人工智能和机器学习。\nPivotal Cloud Foundry # Pivotal Cloud Foundry (PCF) 是一个开源的多云应用平台即服务 (PaaS)，由 501 组织 Cloud Foundry Foundation 负责管理。该软件最初由 VMware 开发，后来转到 Pivotal 软件公司。2019 年底，随着 VMware 收购 Pivotal，又被带回到 VMware。\nPCF 是一个用于部署、管理和持续交付应用程序、容器和功能的多云平台。PCF 允许开发人员快速部署和交付软件，而不需要管理底层基础设施。\nHeroku # Heroku 是一个支持多种编程语言的云 PaaS。作为首批云平台之一，Heroku 从 2007 年 6 月就开始开发，当时它只支持 Ruby 编程语言，但现在支持 Java Node.js、Scala、Clojure、Python、PHP 和 Go。\nHeroku 是一个基于容器的云 PaaS。开发人员使用 Heroku 来部署、管理和扩展现代应用程序。这个平台优雅、灵活、易于使用，为开发人员提供了发布应用程序最简单的途径。\n中国云厂商（编者补充） # 阿里云、腾讯云、华为云、京东云等各类型厂商都是国内用户的可选项。\n容器调度器 # 容器调度程序的主要任务是在最合适的主机上启动容器并将它们连接在一起。它必须通过自动故障转移来处理故障，并且当单个实例有太多数据需要处理 / 计算时，它需要能够扩展容器。\n三个最流行的容器调度器是 Docker Swarm、Apache Mesos 和 Kubernetes。\nDocker Swarm # Docker Swarm 是 Docker 开发的一种容器调度程序。这个集群解决方案提供了一些优势，比如使用标准 Docker API 等。\nApache Mesos # Mesos 的目的是构建一个可扩展、高效的系统，可以支持大量的框架。这也是一个主要的问题：一些框架，如 Hadoop 和 MPI，是独立开发的——因此不可能在框架之间进行细粒度的共享。\nMesos 的目的是添加一个薄薄的资源共享层，为框架提供访问集群资源的公共接口。Mesos 将调度控制委托给框架，因为很多框架已经实现了复杂的调度。\n根据集群上运行的作业类型，框架可以分为四种，其中一些框架提供了原生 Docker 支持，比如 Marathon。在 Mesos 0.20.0 中添加了对 Docker 容器的支持。\nKubernetes # Kubernetes 是一个用于编配 Docker 容器的系统，它通过标签和 Pod 的概念将容器分为逻辑单元。Pod 是 Kubernetes 和其他两种解决方案之间的主要区别——它们是一组容器集合，形成一起部署和调度的服务。与基于关联性的容器 (如 Swarm 和 Mesos) 调度相比，这种方法简化了集群的管理。\nKubernetes 调度器的任务是监控带有空 PodSpec 的 Pod。NodeName 指定将容器安排在集群中的某个位置。\n这是与 Swarm 和 Mesos 的不同之处，Kubernetes 允许开发者在运行 Pod 时通过定义 PodSpec.NodeName 来绕过调度器。\n调度器使用谓词和优先级来定义 Pod 应该运行在哪些节点上。我们可以使用新的调度器策略配置覆盖这些默认值。\n我们可以通过命令行标志 policy-config-file 指定一个 JSON 文件，在启动 Kubernetes 时将会使用该文件描述的谓语和优先级，调度器将使用这些定义好的策略。\n监控和日志工具 # 为什么监控和日志工具对于 DevOps 来说很重要？\n快速恢复； 响应速度； 透明性； 发生事故时减少人工干预。 监控和日志工具示例 # ELK # ELK 是三个开源产品的集合——Elasticsearch、Logstash 和 Kibana。它们都是由 Elastic 公司开发、管理和维护的。\nE 代表 ElasticSearch——用于存储日志； L 代表 Logstash——用于传输、处理和存储日志； K 代表 Kibana——一个可视化工具 (Web 界面)。 Datadog # Datadog 是一个针对云规模应用程序的监控服务，通过基于 SaaS 的数据分析平台来监控服务器、数据库、工具和服务。\nDatadog 应用程序性能监控 (APM 或跟踪) 帮助用户深入了解应用程序性能——从自动生成的仪表盘（监控关键指标，如请求量和延迟）到单个请求的详细跟踪信息——与日志和基础设施监控信息并列展示。\n当向应用程序发出请求时，Datadog 可以看到分布式系统的跟踪，并向用户显示关于这个请求的系统数据。\nNew Relic # New Relic 是一家总部位于加州旧金山的科技公司，它开发基于云的软件，帮助网站和应用程序所有者跟踪服务性能。\nNew Relic 的应用程序性能监控软件分析产品 (APM) 提供有关 Web 应用程序性能和最终用户体验满意度的实时和趋势数据。\nPrometheus # Prometheus 是一款用于事件监控和警报的免费应用程序。它在时间序列数据库中记录实时指标，基于 HTTP 拉取模型，支持灵活的查询和实时警报。\nPrometheus 服务器的核心原理是抓取——也就是说，调用各个节点暴露出来的指标端点。它定期收集这些指标并将它们存储在本地。\nZipkin # Zipkin 是一个分布式跟踪系统。它用于收集诊断延迟问题所需的时间数据，提供了数据的收集和查找功能。\n如果日志文件中有跟踪 ID，则可以直接跳指定位置。否则的话，你可以基于服务、操作名称、tagsm 和持续时间等属性进行查询。它将为你汇总出一些有趣的数据，例如花费在服务上的时间百分比以及操作是否失败。\nAzure Monitor # Azure Monitor 提供了一个全面的解决方案，用来收集、分析和执行来自云端和本地环境的遥测数据，从而最大化应用程序和服务的可用性和性能。\n它可以帮助你了解应用程序的执行情况，并主动识别影响它们的问题以及它们所依赖的资源。\n几个 Azure Monitor 的使用场景：\n使用 Application Insights 检测和诊断应用程序和依赖项之间的问题； 将基础设施问题与用于 VM 的 Azure Monitor 和用于容器的 Azure Monitor 关联起来； 使用日志分析深入监控数据，进行故障排除和深入诊断； 通过智能警报和自动运维支持大规模操作； 使用 Azure 仪表板和工作簿进行可视化。 通信工具 # 为什么通信工具对于 DevOps 来说很重要？\n连接团队； 减少等待时间； 改进团队协作。 通信工具示例 # Slack # Slack 是由 Slack 公司开发的即时通讯平台。\nSlack 本质上是整个公司的一个聊天室，旨在取代电子邮件成为沟通和分享的主要方式。你可以用频道进行分组讨论，也可以通过私有消息进行信息、文件共享，等等。\n微软 Teams # 微软 Teams 是一个统一的通信和协作平台，结合了工作讨论、视频会议、文件存储和应用程序集成。\nTeams 是一种基于聊天的协作工具，它为全球、远程和分布式的团队提供协作能力，并通过公共空间共享信息。你可以使用它提供的一些很酷的功能，比如文档协作、一对一聊天、团队聊天，等等。\n谷歌 Hangouts # 谷歌 Hangouts 是谷歌开发的一款通讯软件产品。Hangouts 最初是 Google+ 的一个功能，在 2013 年谷歌开始将 Google+ Messenger 和谷歌 Talk 的功能整合到 Hangouts 中，成为了一个独立的产品。\nHangouts Chat 是进行组织内交流的一种有效方式。你可以与一名或多名同事发送信息，可以创建聊天室进行多人讨论，并使用机器人来进行工作自动化。你可以在电脑浏览器和 Android 或 iOS 手机 App 中使用 Hangouts Chat。\nZoom # Zoom Video Communications 是一家远程会议服务公司，总部设在加利福尼亚州的圣何塞。它提供了一种远程会议服务，结合了视频会议、在线会议、聊天和移动协作。\nSkype 更适合寻找整体商业解决方案的团队，而 Zoom 更适合频繁进行视频聊天和会议的团队。不过，这两种工具都不是专门为远程工作而构建的。\n中国版本推荐 # 腾讯会议、钉钉、飞书、WeLink 等。\n知识分享工具 # 知识分享工具为什么对于 DevOps 来说很重要？\n减少知识浪费； 提高新员工的效率； 减少犯同样的错误。 知识分享工具示例 # GitHub Pages # GitHub Pages 是一个静态站点托管服务，直接从 GitHub 上的存储库获取 HTML、CSS 和 JavaScript 文件，可在构建过程中运行这些文件，然后发布网站。你可以在 GitHub Pages 示例集合中看到 GitHub Pages 站点的示例。\nConfluence # Confluence 是一个由 Atlassian 开发并发布的协作程序。Atlassian 用 Java 编程语言开发了 Confluence，并于 2004 年首次发布。\nConfluence 是一个 Wiki 协作工具，用于帮助团队有效地协作和共享知识。你可以用 Confluence 捕获项目需求、将任务分配给特定用户，并用 Team Calendar 插件一次性管理多个日历。\nJekyll # Jekyll 是一个支持博客的静态站点生成器，用于个人、项目或组织站点。它是由 GitHub 联合创始人 Tom Preston-Werner 用 Ruby 开发的，并采用了 MIT 开源许可。\nJekyll 是一个解析引擎，打包成 Ruby gem，用来基于动态组件构建静态网站。\n谷歌 Sites # 谷歌 Sites 是谷歌提供的一个结构化的 Wiki 和网页制作工具。谷歌 Sites 的目标是让任何人都能够创建支持不同编辑器协作的简单 Web 站点。\n这些站点在每一个屏幕上（从桌面到智能手机）看起来都很棒——做到这些都不需要学习设计或编程。\n总结 # 这篇文章涵盖的所有类别可以帮助你更好地实施 DevOps。其中任何一种都很有用，而且我认为所有这些都是你需要的，帮助团队以可持续的方式向客户交付价值，并帮助你脱颖而出。\n我提到了 DevOps 工具链，比如规划工具、问题跟踪工具、源代码控制管理、构建和测试代码、持续集成和部署源代码、管理配置、使用云平台，然后是监控和日志，以及沟通和知识共享。\n我希望你能够喜欢这篇文章。DevOps 是一种令人兴奋的工作方式，尽管它不是唯一的方式，甚至不是最好的方式。更好的东西可能会在未来出现，但目前看来，它似乎代表了一种很好的团队协作方式。它让团队共同努力，交付价值，同时也帮助他们专注于客户，而不仅仅是技术或内部的东西，也包括了速度、学习以及公司的未来。\n"},{"id":201,"href":"/docs/2025-2-24-%E9%AB%98%E7%BA%A7%E8%BF%90%E7%BB%B4%E9%9D%A2%E8%AF%95%E9%A2%98_ai_linux%E9%83%A8%E5%88%86/","title":"高级运维面试题-linux部分","section":"Docs","content":" 经过本人为期一个半月的不懈努力，累计面试了二十多家公司，共计约五十余场面试，考察的面试题超两百道，积累了许多宝贵的面试经验。现在，我将这些面试题以及我个人的应对心得精心整理成一份面试攻略分享给大家，快来一起测测自己能回答多少道面试题吧。\n面试基本信息 # 面试岗位：运维工程师(容器与ES方向)、运维开发工程师、SRE工程师 工作经验：5年 薪资范围：年薪45万左右，base北京 面试时间：7月初-8月中旬 主要公司：\n互联网公司：字节、京东、百度、网易、蚂蚁金服、小米、滴滴、去哪儿、猎豹移动、商汤、旷视、智谱华章、马蜂窝、竞技世界 国企子公司：电信、联通、建行、中石化 面试题汇总 # 其中☆表示多次出现过的高频面试题，已经按分类整理。\nLinux # grep sed awk cut组合使用☆ # grep、sed、awk 和 cut 都是 Linux 系统中常用的文本处理工具，通常可以结合使用以实现更复杂的文本处理任务。这里是一些常见的组合使用示例：\n1. 使用 grep + cut：筛选和提取字段 # 假设你有一个文件 data.txt，其内容如下：\nName, Age, Department Alice, 30, HR Bob, 25, Engineering Charlie, 35, Marketing 如果你只想提取 Name 和 Age 字段，可以使用 grep 和 cut 的组合：\ngrep -v \u0026#34;Name\u0026#34; data.txt | cut -d \u0026#39;,\u0026#39; -f 1,2 解释：\ngrep -v \u0026quot;Name\u0026quot;：去除第一行标题。 cut -d ',' -f 1,2：按照逗号分隔，提取第一列和第二列。 输出：\nAlice, 30 Bob, 25 Charlie, 35 2. 使用 grep + awk：筛选和处理文本 # 假设你要根据某个条件（如年龄大于 30）筛选数据，并进行一些进一步的处理：\ngrep -v \u0026#34;Name\u0026#34; data.txt | awk -F \u0026#39;,\u0026#39; \u0026#39;$2 \u0026gt; 30 {print $1, $2}\u0026#39; 解释：\ngrep -v \u0026quot;Name\u0026quot;：去除标题行。 awk -F ',' '$2 \u0026gt; 30 {print $1, $2}'：以逗号作为分隔符，筛选年龄大于 30 的行，并输出姓名和年龄。 输出：\nCharlie 35 3. 使用 sed + awk：修改并处理文本 # 假设你想把所有姓名前面加上 \u0026ldquo;Mr.\u0026rdquo; 并筛选年龄大于 30 的人：\nsed \u0026#39;s/^/Mr. /\u0026#39; data.txt | awk -F \u0026#39;,\u0026#39; \u0026#39;$2 \u0026gt; 30 {print $1, $2}\u0026#39; 解释：\nsed 's/^/Mr. /'：在每一行的开头加上 \u0026ldquo;Mr. \u0026ldquo;。 awk -F ',' '$2 \u0026gt; 30 {print $1, $2}'：筛选年龄大于 30 的人并输出姓名和年龄。 输出：\nMr. Charlie 35 4. 使用 grep + sed + awk：综合使用 # 你可以将这几种工具结合起来，进行更复杂的处理。例如，从文件中找出所有包含 \u0026ldquo;HR\u0026rdquo; 的行，去掉空格并提取姓名：\ngrep \u0026#34;HR\u0026#34; data.txt | sed \u0026#39;s/ //g\u0026#39; | awk -F \u0026#39;,\u0026#39; \u0026#39;{print $1}\u0026#39; 解释：\ngrep \u0026quot;HR\u0026quot;：查找包含 \u0026ldquo;HR\u0026rdquo; 的行。 sed 's/ //g'：删除行中的所有空格。 awk -F ',' '{print $1}'：提取第一列（姓名）。 输出：\nAlice 总结 # grep 用于筛选文本。 cut 用于提取特定的字段或列。 awk 用于更复杂的模式匹配和处理，可以在行级别进行操作。 sed 用于文本的替换、删除等操作。 这些工具各有优势，可以根据需要灵活组合，完成各种文本处理任务。\nhttp错误码和原因 # HTTP 错误码用于表示 HTTP 请求的状态。它们按类别分为五个类：1xx（信息类）、2xx（成功类）、3xx（重定向类）、4xx（客户端错误类）、5xx（服务器错误类）。以下是常见的 HTTP 错误码及其含义和原因：\n1. 1xx - 信息类 # 这些状态码表示服务器已收到请求，客户端需要继续处理。\n100 Continue：客户端应继续发送请求的剩余部分。 101 Switching Protocols：服务器接受客户端的协议切换请求。 2. 2xx - 成功类 # 表示请求已经成功被服务器处理。\n200 OK：请求成功，通常在 GET 或 POST 请求后返回。 201 Created：请求成功并且服务器创建了新资源。 202 Accepted：请求已接受，但未处理完成。 204 No Content：请求成功，但没有返回任何内容。 3. 3xx - 重定向类 # 这些状态码表示客户端需要采取额外的操作才能完成请求。\n301 Moved Permanently：资源已永久移动到新位置。 302 Found：资源临时移动到新位置（常用于临时重定向）。 303 See Other：请求应使用 GET 方法访问另一个资源。 304 Not Modified：请求的资源未修改，客户端可以使用缓存。 307 Temporary Redirect：请求的资源临时移动到新位置。 308 Permanent Redirect：请求的资源永久移动到新位置。 4. 4xx - 客户端错误类 # 这些状态码表示客户端请求有错误，通常是由于客户端的请求无效或格式不正确。\n400 Bad Request：请求格式不正确或缺少必需的参数。 401 Unauthorized：请求需要用户身份验证，通常用于未授权的 API 请求。 402 Payment Required：此状态码保留供将来使用，目前大多不常见。 403 Forbidden：服务器理解请求，但拒绝执行。通常是因为权限问题。 404 Not Found：请求的资源未找到。 405 Method Not Allowed：请求方法（如 GET、POST 等）不允许访问该资源。 406 Not Acceptable：服务器无法生成符合请求头中 \u0026ldquo;Accept\u0026rdquo; 字段的响应。 407 Proxy Authentication Required：需要代理认证。 408 Request Timeout：客户端请求超时，服务器未在规定时间内接收到请求。 409 Conflict：请求导致资源冲突，通常用于资源的创建或修改操作。 410 Gone：请求的资源不再可用，并且不再会恢复。 411 Length Required：请求缺少 \u0026ldquo;Content-Length\u0026rdquo; 头部。 412 Precondition Failed：请求头中的某些条件未满足。 413 Payload Too Large：请求体太大，服务器无法处理。 414 URI Too Long：请求的 URI 太长，服务器无法处理。 415 Unsupported Media Type：请求的媒体类型不受支持。 416 Range Not Satisfiable：请求的范围无效，服务器无法提供部分资源。 417 Expectation Failed：服务器无法满足 \u0026ldquo;Expect\u0026rdquo; 请求头中的期望。 5. 5xx - 服务器错误类 # 这些状态码表示服务器在处理请求时发生了错误，通常是服务器问题。\n500 Internal Server Error：服务器遇到错误，无法完成请求。 501 Not Implemented：服务器不支持请求的方法或功能。 502 Bad Gateway：服务器作为网关或代理时，从上游服务器收到无效响应。 503 Service Unavailable：服务器当前无法处理请求，通常由于过载或维护。 504 Gateway Timeout：服务器作为网关或代理时，未从上游服务器收到及时响应。 505 HTTP Version Not Supported：服务器不支持请求的 HTTP 版本。 总结 # 2xx：请求成功。 3xx：请求需要重定向。 4xx：客户端错误（例如请求不合法或权限问题）。 5xx：服务器错误（服务器处理请求时发生了问题）。 这些错误码帮助开发者理解请求失败的原因，方便进行调试或错误处理。\n3.长连接、短连接、WebSocket区别和使用场景 # 长连接、短连接和 WebSocket 都是网络通信中的不同连接方式，它们在使用时有不同的特点和应用场景。下面我们来详细讨论它们的区别和适用场景。\n1. 短连接（Short Connection） # 特点： # 每次请求都建立新的连接：每次客户端发送请求时，都会和服务器建立一个新的连接，处理完请求后即关闭连接。 资源消耗大：每次请求都要进行连接的建立和销毁，频繁的连接和断开会增加网络延迟和资源消耗，尤其是在高频次请求的情况下。 典型协议：HTTP/1.x、SMTP（邮件）、FTP 等。 使用场景： # 简单的请求-响应模式：比如网页浏览、提交表单等，一次请求一次响应。 不需要持续通信的场景：例如用户查询信息，数据更新不频繁，服务器负载较低的情况。 优点： # 连接生命周期短，不会占用太多服务器资源。 适合低频请求的场景。 缺点： # 每次请求都需要建立连接，较高的延迟和资源消耗。 无法实现实时通信或长时间的保持连接。 2. 长连接（Long Connection） # 特点： # 建立连接后保持长时间不关闭：客户端和服务器在建立连接后，连接会一直保持，直到客户端或服务器主动关闭连接。 减少建立连接的开销：在连接存活期间，客户端可以发送多个请求，避免了频繁的连接和断开的开销。 典型协议：HTTP/1.1 的持久连接（通过 Connection: keep-alive 头控制）、TCP 连接。 使用场景： # 需要多次请求的场景：如在一个页面加载过程中，多个资源（图片、JS、CSS 文件等）需要多次请求服务器，这时保持一个持久的连接可以降低延迟。 实时性要求不高的长时间会话：比如在线聊天室、数据推送等场景。 优点： # 避免了频繁的连接建立和断开，减少了延迟。 更适合多次请求的交互。 缺点： # 长时间占用连接资源，可能导致服务器负载增加。 如果连接不稳定，可能会导致连接中断。 3. WebSocket # 特点： # 全双工通信：WebSocket 是一种在单个 TCP 连接上进行全双工通信的协议，客户端和服务器可以随时发送数据到对方。 低延迟、实时通信：建立 WebSocket 连接后，客户端和服务器之间可以进行实时的双向数据交换，适合高频率和低延迟的场景。 持久连接：WebSocket 一旦建立连接后会持续存在，直到客户端或服务器主动关闭连接。 典型协议：WebSocket 协议（ws:// 或 wss://）。 使用场景： # 实时通讯应用：如在线聊天、即时消息、股票行情、多人在线游戏、社交网络等。 需要实时推送数据的场景：例如实时更新、推送通知、直播数据流、物联网设备管理等。 优点： # 低延迟，适合实时性要求高的应用。 通过一个连接可以发送和接收数据，减少了频繁建立连接的开销。 服务器和客户端可以主动推送消息到对方。 缺点： # 需要持续保持连接，可能会消耗较多的资源。 对于 WebSocket 的支持较为有限，需要服务器和客户端同时支持 WebSocket 协议。 因为 WebSocket 是持续连接，在某些网络环境下可能会遇到连接稳定性问题。 总结比较 # 特性/协议 短连接 长连接 WebSocket 连接类型 每次请求建立一次连接 一次连接可多次使用 持久连接，双向通信 连接生命周期 短暂的连接和断开 持久连接 持久连接 资源消耗 高（频繁连接和断开） 低（减少连接建立的开销） 适中（长时间占用连接） 适用场景 简单的请求-响应，低频请求 多次请求需要保持连接，实时性要求不高 实时通信，低延迟数据交换，高频数据推送 通信方式 单向（客户端请求，服务器响应） 单向（客户端请求，服务器响应） 双向（客户端和服务器都能发送消息） 协议支持 HTTP/1.x，SMTP，FTP 等 HTTP/1.1，TCP 等 WebSocket 适用场景总结： # 短连接：适用于简单的请求-响应场景，连接生命周期短，适合低频交互，不需要持久连接的场合。 长连接：适用于需要多次请求的场景，或者是可以接受一定延迟但要求持续连接的应用。 WebSocket：适用于实时通信、低延迟和高频数据交换的场景，如在线游戏、即时聊天、股票行情推送等。 nginx性能优化有哪些方式☆ # Nginx 是一个高性能的 Web 服务器和反向代理服务器，广泛用于负载均衡、反向代理、静态文件服务等场景。为了提高 Nginx 的性能，可以通过多个方面进行优化。下面列出了一些常见的 Nginx 性能优化方式：\n1. 调整工作进程和连接数 # Nginx 的性能很大程度上取决于工作进程（worker process）和每个工作进程的连接数。合理的配置可以提高并发处理能力。\nworker_processes：设置 Nginx 启动的工作进程数。通常设置为 CPU 核心数，或者根据服务器的负载情况调整。\nworker_processes 4; # 根据 CPU 核心数设置 worker_connections：设置每个工作进程可以打开的最大连接数。此值与客户端并发连接数密切相关。调整时要考虑系统的最大文件描述符限制。\nworker_connections 1024; # 根据实际需求调整 worker_rlimit_nofile：增加工作进程的文件描述符限制，防止在高并发下达到文件描述符限制。\nworker_rlimit_nofile 65535; 2. 启用 keepalive 持久连接 # keepalive 连接可以减少建立和关闭连接的开销。可以通过设置合适的超时时间来优化连接复用。\nkeepalive_timeout：设置保持连接的最大时间。在处理高并发请求时，合理的保持连接超时能减少连接建立的次数。\nkeepalive_timeout 65; # 默认是 75s，适当调小可以减少空闲连接占用 keepalive_requests：限制每个连接的请求数，避免占用过多资源。\nkeepalive_requests 10000; # 每个连接最多处理 10000 个请求 3. 启用 GZIP 压缩 # 开启 GZIP 压缩可以有效减少传输数据量，提升带宽利用率和加载速度。\ngzip：开启 GZIP 压缩。\ngzip on; gzip_comp_level 6; # 设置压缩级别，1-9，数字越大压缩越强 gzip_min_length 1000; # 只有内容长度大于 1000 字节的响应才会被压缩 gzip_proxied any; # 启用对代理请求的压缩 gzip_types text/plain text/css application/javascript application/json application/xml text/javascript application/xml+rss image/svg+xml; # 指定需要压缩的文件类型 4. 优化缓存策略 # 合理的缓存策略可以极大提升访问速度，减少对后端服务器的压力。\n开启缓存：通过 proxy_cache 配置启用反向代理缓存，可以缓存静态文件和动态内容，减少对后端服务器的请求。\nproxy_cache_path /tmp/cache levels=1:2 keys_zone=my_cache:10m inactive=60m max_size=1g; proxy_cache_key \u0026#34;$scheme$request_method$host$request_uri\u0026#34;; 设置缓存过期时间：使用 expires 设置静态资源的缓存时间，减少对后端的请求。\nlocation /static/ { expires 30d; # 设置缓存 30 天 } 5. 负载均衡优化 # 对于 Nginx 的反向代理，合理的负载均衡配置可以提高并发性能，避免单一节点过载。\n负载均衡算法：Nginx 支持多种负载均衡算法，如轮询、最少连接、IP 哈希等，可以根据实际需求选择合适的算法。\nupstream backend { least_conn; # 使用最少连接负载均衡算法 server backend1.example.com; server backend2.example.com; } 健康检查：定期检查后端服务器的健康状况，避免将流量引导到故障的服务器。\nupstream backend { server backend1.example.com; server backend2.example.com; # 设置失败请求重试次数 server backend3.example.com max_fails=3 fail_timeout=30s; } 6. 减少 DNS 查询延迟 # Nginx 可以缓存 DNS 查询结果，减少频繁的 DNS 查询带来的延迟。\nresolver：设置 DNS 解析器并配置缓存。\nresolver 8.8.8.8 8.8.4.4 valid=10s; # 设置 DNS 解析服务器及缓存时间 7. 提高文件 I/O 性能 # Nginx 需要处理大量的静态文件，文件 I/O 性能直接影响整体性能。\n开启 sendfile：sendfile 可以直接从文件系统读取数据并通过网络发送，避免多次复制，提高性能。\nsendfile on; tcp_nopush on; # 配合 sendfile 使用，减少传输时的 TCP 包数量 aio 和 directio：对于大文件，可以开启异步 I/O，进一步提高文件传输效率。\naio threads; # 开启异步 I/O directio 4m; # 对大于 4MB 的文件开启直接 I/O 8. 调整 TCP 连接优化 # Nginx 作为 Web 服务器，涉及到大量的 TCP 连接。优化 TCP 连接设置可以提高性能。\ntcp_nodelay：启用 TCP_NODELAY，关闭 Nagle 算法，减少小数据包的延迟。\ntcp_nodelay on; tcp_fin_timeout：控制服务器关闭连接时的等待时间，减小服务器的连接消耗。\ntcp_fin_timeout 30; 9. 限制请求速率 # 对于高流量的应用，限制客户端的请求速率可以防止流量洪水并确保公平分配资源。\nlimit_req：限制每秒请求数，防止某些客户端过于频繁地请求。\nlimit_req_zone $binary_remote_addr zone=req_limit_per_ip:10m rate=1r/s; # 每个 IP 最多 1 个请求/秒 10. 禁用不必要的模块 # Nginx 提供了很多模块，某些模块可能在你的应用场景中并不需要。禁用不必要的模块可以减少内存占用和提升性能。\n在编译 Nginx 时，禁用不需要的模块。例如，禁用代理模块、邮件代理模块等。\n./configure --without-http_rewrite_module --without-http_gzip_module 11. 监控和日志优化 # 监控 Nginx 的性能并进行实时调整，对于高负载应用尤为重要。\n日志优化：精简日志内容，避免记录不必要的请求信息，减少磁盘 I/O 和 CPU 占用。\naccess_log /var/log/nginx/access.log combined buffer=32k flush=5m; # 使用缓冲区来减少磁盘 I/O 总结 # Nginx 性能优化涉及多个方面，包括硬件资源（如 CPU 和内存）、网络配置、负载均衡、缓存策略、TCP 连接优化等。合理配置和调整 Nginx 的工作进程、连接数、缓存机制、负载均衡策略等，可以有效提升 Nginx 的性能，确保在高并发、高流量的场景下仍然能够稳定、高效地工作。\nlvs、nginx、haproxy区别和使用场景☆ # LVS（Linux Virtual Server）、Nginx 和 HAProxy 都是常见的负载均衡技术，它们的功能有一些重叠，但在设计目标、性能、灵活性和使用场景上有所不同。下面将分别介绍它们的特点、优缺点及适用的使用场景。\n1. LVS（Linux Virtual Server） # LVS 是基于 Linux 内核的负载均衡解决方案，常用于高可用性和高性能的负载均衡场景。它主要通过 IP 负载均衡来分发流量。\n特点： # 工作在网络层（L4 层）：LVS 主要进行基于 IP 地址和端口的负载均衡，通常工作在 OSI 模型的传输层（L4 层）。 高性能：LVS 作为内核级别的负载均衡器，其性能非常高，因为它直接处理数据包转发，避免了应用层的开销。 透明性：LVS 通过 NAT（网络地址转换）、DR（Direct Routing）等技术实现负载均衡，客户端无法察觉负载均衡的存在。 调度算法：LVS 支持多种负载均衡算法，如轮询、加权轮询、最少连接、源地址哈希等。 优点： # 高性能：由于是内核级别的负载均衡，LVS 能提供高吞吐量，适合处理大量并发流量。 透明性：客户端不需要知道负载均衡的存在，提供透明的负载均衡。 高可用性：可以与 Keepalived 配合使用，实现高可用性。 缺点： # 灵活性差：LVS 仅支持 L4 层负载均衡，无法处理基于 HTTP 头、Cookie 等内容的路由，功能上不如应用层负载均衡器灵活。 配置复杂：LVS 的配置和维护相对复杂，需要一定的 Linux 网络知识。 使用场景： # 高性能、大规模流量的负载均衡：适用于需要高吞吐量和低延迟的场景，如大规模的 Web 服务、视频流服务等。 简单的 L4 层负载均衡：如果只需要基于 IP 和端口的负载均衡，而不需要 HTTP 层的复杂路由和控制，LVS 是一个优秀的选择。 2. Nginx # Nginx 是一个轻量级的 Web 服务器和反向代理服务器，除了处理 HTTP 请求外，还可以进行负载均衡。\n特点： # 工作在应用层（L7 层）：Nginx 主要提供 HTTP、HTTPS、TCP 和 UDP 的负载均衡服务，通常工作在应用层（L7 层）。 支持多种负载均衡算法：包括轮询、IP 哈希、加权轮询等。 灵活的路由功能：可以基于 HTTP 请求的不同部分（如路径、头部、方法、主机等）进行路由，适合处理复杂的 Web 应用场景。 反向代理功能：Nginx 同时提供反向代理功能，能够在负载均衡的同时，作为 Web 服务器或应用服务器的前端，进行内容缓存、请求处理等。 优点： # 灵活性强：可以根据请求的内容（如 URL、HTTP 头、Cookie 等）做更精细的流量控制。 简单易配置：Nginx 配置简洁，功能强大，社区活跃。 支持 HTTP、HTTPS 和 TCP 负载均衡：除了支持传统的 HTTP 层负载均衡，还支持 TCP/UDP 负载均衡。 高性能：尽管是应用层的负载均衡器，Nginx 依然具有较高的性能，能够处理大量并发请求。 缺点： # 性能相对 LVS 较低：Nginx 的性能虽然很高，但相比于 LVS 的内核级负载均衡，Nginx 可能在处理非常高并发流量时稍显不足。 依赖操作系统的网络栈：Nginx 是在用户态运行的，受限于操作系统的网络栈，相较于 LVS 内核级的处理能力可能稍差。 使用场景： # Web 服务器负载均衡：适用于 Web 服务的负载均衡，尤其是需要对 HTTP 请求进行精细路由和控制的场景。 反向代理：在 Web 应用架构中，Nginx 可以作为反向代理进行负载均衡，同时提供 SSL 终端加密、缓存、限流等功能。 容器化环境中的负载均衡：在 Kubernetes、Docker 等容器化环境中，Nginx 是常用的负载均衡工具。 3. HAProxy # HAProxy 是一个专注于高性能、高可用性的负载均衡器，广泛用于大规模的 Web 应用和高并发场景。\n特点： # 工作在应用层（L7 层）和传输层（L4 层）：HAProxy 支持 HTTP、HTTPS、TCP 和 UDP 的负载均衡，可以根据不同层次进行流量转发。 支持丰富的负载均衡算法：HAProxy 支持轮询、最少连接、加权轮询、源 IP 哈希等多种负载均衡策略。 高可用性：HAProxy 支持健康检查、故障转移和高可用配置，能自动检测后端服务器的健康状况，并根据健康状态进行流量分配。 非常高的性能：HAProxy 经过优化，能够处理大量的并发连接，尤其适合高吞吐量的应用。 优点： # 高性能、高并发：HAProxy 在高负载和高并发场景下的表现优异，适合处理大量并发请求。 丰富的配置选项：HAProxy 提供了灵活的配置选项，可以进行精细的负载均衡和流量管理。 强大的健康检查机制：能够实时监控后端服务器的健康状态，保证高可用性。 广泛的应用场景：HAProxy 可用于多种负载均衡场景，包括 Web、数据库、缓存服务等。 缺点： # 配置相对复杂：相比 Nginx，HAProxy 的配置相对复杂，学习曲线较陡。 不支持应用层的一些高级功能：虽然 HAProxy 支持 L7 层负载均衡，但其功能不如 Nginx 灵活，尤其在内容缓存、反向代理等方面，Nginx 提供了更多的高级功能。 使用场景： # 高并发、大流量应用：适用于需要高性能和高可用性的负载均衡，如金融、社交媒体、大型电商网站等。 复杂的负载均衡场景：适合需要多种负载均衡算法、健康检查以及故障转移的场景。 TCP 和 HTTP 负载均衡：HAProxy 支持 HTTP 层和 TCP 层的负载均衡，适合 Web 服务和数据库、缓存等应用的负载均衡。 总结比较 # 特性 LVS Nginx HAProxy 工作层次 L4（传输层） L7（应用层） L7（应用层）和 L4（传输层） 性能 非常高（内核级负载均衡） 高，适合 Web 层负载均衡 高，专注于高并发的负载均衡 灵活性 低，主要支持 IP 和端口 高，支持基于 URL、HTTP 头等路由 高，支持多种负载均衡算法 配置复杂度 较高 较低，配置简洁 较高，配置选项丰富 支持的协议 TCP/UDP HTTP、HTTPS、TCP、UDP HTTP、HTTPS、TCP、UDP 高可用性 需要与 Keepalived 配合使用 支持与高可用架构集成 支持内建的健康检查和故障转移 使用场景 高性能的负载均衡，L4 层 Web 应用、反向代理、容器化环境 高并发、大流量的 Web 或服务负载均衡 选择建议： # LVS：适合大规模、高性能的 L4 层负载均衡，不需要复杂路由规则的场景。\nNginx：适合需要高性能并且需要对 HTTP 流量进行细粒度控制的 Web 服务负载均衡。\nHAProxy：适合高并发、复杂负载均衡场景，尤其是需要灵活调度和高可用性的应用。\n僵尸进程是什么 # 僵尸进程（Zombie Process） 是指在 Linux 或 Unix 系统中，一个已经完成执行（即已经退出）的进程，但其进程描述符（PID）还没有被父进程（Parent Process）回收的进程。简单来说，僵尸进程是一个“死了但还未被清理”的进程。\n为什么会产生僵尸进程？ # 当一个进程结束时，它会向父进程发送一个信号（通常是 SIGCHLD）通知它已经结束，并向操作系统申请退出状态信息。这些信息会被存储在进程表中，等待父进程通过调用 wait() 或 waitpid() 系统调用来回收。只有当父进程回收了子进程的退出状态信息（退出码）后，进程表中的条目才会被彻底清除，这时进程才会完全消失。\n如果父进程没有回收这些信息（可能是因为父进程没有调用 wait()，或者父进程提前退出等情况），子进程就会成为“僵尸进程”。这些进程会保留在系统的进程表中，占用 PID 号和一些资源，但不再占用 CPU 和内存。\n僵尸进程的特征 # 不消耗 CPU 资源：一旦进程终止，僵尸进程就不再执行任何代码，因此不会消耗 CPU。 仍然占用进程表项：尽管已结束执行，僵尸进程仍占用一个进程号（PID），但其状态为 Z（表示 zombie）。 资源被父进程占用：父进程需要回收僵尸进程的退出状态信息。如果父进程不回收，僵尸进程会持续存在。 如何查看僵尸进程？ # 可以使用 ps 命令查看僵尸进程。通过添加 -e 或 -aux 参数查看所有进程，然后使用 grep 过滤出状态为 Z 的进程。\nps aux | grep \u0026#39;Z\u0026#39; # 查找所有僵尸进程 或者：\nps -e -o pid,stat,cmd | grep \u0026#39;Z\u0026#39; # 查看所有进程及其状态，过滤出状态为 Z 的进程 在输出中，STAT 字段显示为 Z，表示该进程是一个僵尸进程。\n如何解决僵尸进程？ # 父进程回收子进程：\n通过让父进程调用 wait() 或 waitpid() 系统调用来回收子进程的退出状态。如果父进程未正确回收，则僵尸进程会持续存在。 如果父进程本身已经退出，操作系统会将这些僵尸进程的父进程设置为 init 进程（PID 1），由 init 进程来回收它们。 结束父进程：\n如果父进程不回收子进程，且父进程本身也不再需要运行，可以通过终止父进程来解决。终止父进程后，系统会自动将这些僵尸进程的父进程设置为 init 进程（PID 1），并由 init 进程回收它们。 kill -9 \u0026lt;父进程PID\u0026gt; 定期清理：\n在某些情况下，可以通过编写定时脚本，定期检查并清理僵尸进程。 总结 # 僵尸进程是已经退出但没有被回收的进程，其状态是 Z。 它们不会占用系统资源（如 CPU 和内存），但会占用 PID 和进程表项。 父进程需要回收子进程的退出信息，否则就会产生僵尸进程。 通过监控和管理父进程的回收操作，可以避免僵尸进程的积累，确保系统资源得到有效利用。 进程、线程、协程区别☆ # 进程（Process）、线程（Thread）和协程（Coroutine） 是操作系统和程序设计中的重要概念，它们之间有着不同的资源管理、执行模型和使用场景。下面是它们之间的主要区别：\n1. 进程（Process） # 定义： # 进程是操作系统资源分配的基本单位，它是正在执行的程序的实例。一个进程拥有独立的内存空间、数据段、代码段、堆栈等资源，操作系统为每个进程分配资源和管理进程之间的调度。\n特点： # 资源隔离：每个进程有自己的内存空间、文件描述符、堆栈等资源。不同进程之间是相互隔离的，一个进程不能直接访问另一个进程的内存。 较重的开销：创建进程时需要大量的资源和时间开销，因为需要分配独立的内存和资源，并进行调度。 上下文切换：进程切换时，操作系统需要保存当前进程的状态，并恢复下一个进程的状态，代价较大。 使用场景： # 独立执行任务：适用于需要完全隔离的任务，例如运行不同的应用程序或服务。 高隔离性需求：需要高度隔离的任务，比如多用户环境中的不同程序。 2. 线程（Thread） # 定义： # 线程是进程内部的执行单元，是程序执行的最小单位。多个线程可以共享进程的资源（如内存空间、文件描述符等），但每个线程有自己的栈和程序计数器（PC）。线程是操作系统调度的基本单位。\n特点： # 共享资源：同一进程中的线程共享进程的内存和资源，可以通过共享数据进行通信（如共享内存）。 轻量级：线程相比进程占用的资源少，创建和销毁的开销小，线程之间的上下文切换比进程之间的切换快。 并发执行：多个线程可以并发执行，适用于多任务处理和 CPU 密集型任务。 同步与竞争：线程共享内存，容易出现同步问题（如数据竞争、死锁等），需要使用锁机制来保证线程安全。 使用场景： # 需要共享内存的并发任务：比如 Web 服务器中的多个请求处理线程。 轻量级任务执行：适用于需要快速响应、资源开销较小的并发任务。 多核 CPU 的并行处理：当系统拥有多核 CPU 时，线程可以在多个核上并行执行，提高效率。 3. 协程（Coroutine） # 定义： # 协程是一种轻量级的线程，可以在单个线程中实现多任务的切换。协程不是由操作系统进行调度，而是由程序员手动控制执行。它的特点是非抢占式，通过显式的 yield 或 await 等语法来切换执行。\n特点： # 轻量级：协程的创建和销毁开销极小，它们在同一个线程中运行，不需要额外的内存和上下文切换，极大减少了资源消耗。 手动调度：协程由程序控制其执行顺序，协程可以在函数内保存和恢复状态，执行时切换非常快。 非抢占式：与线程的抢占式调度不同，协程的切换是由程序显式控制（如通过 yield 或 await 等语法）。这意味着程序可以在任意时刻决定让协程暂停并切换到其他任务。 单线程运行：协程通常在单线程中运行，因此没有线程间的同步问题。 使用场景： # IO 密集型任务：协程非常适合处理大量 IO 操作（如网络请求、文件读写等），因为它们可以在等待 IO 操作时切换到其他任务，充分利用 CPU。 轻量级并发：当需要大量并发任务而不希望开销太大时，协程是一种理想选择。例如，Web 服务中可以使用协程来处理大量的客户端请求。 无需线程上下文切换：适用于任务调度需要非常高效的场景，如一些高并发的服务器。 进程、线程、协程对比 # 特性 进程 线程 协程 创建开销 较高，资源分配和上下文切换较慢 较低，线程之间共享进程的资源 非常低，仅在用户空间调度，无操作系统干预 资源占用 独立资源，如内存、文件句柄等 共享进程的资源，但每个线程有独立栈 共享线程的资源，几乎不占用额外资源 调度方式 由操作系统调度（抢占式） 由操作系统调度（抢占式） 由程序员显式控制（非抢占式） 上下文切换 代价大，需要保存和恢复大量状态 比进程切换快，但仍然需要上下文切换 极快，仅需保存少量状态 并发性能 适合独立任务的并发执行 适合 CPU 密集型并行任务 适合大量轻量级并发任务，尤其是 IO 密集型 同步机制 需要进程间通信（IPC） 需要使用线程同步机制（如锁） 无需同步，避免线程之间的数据竞争 适用场景 独立任务，要求高度隔离的环境 多任务并行，需共享内存的场景 高效处理大量并发 IO 操作，轻量级任务 总结 # 进程：是操作系统资源的基本分配单位，具有独立的内存空间，适用于需要完全隔离的任务，创建和切换开销较大。 线程：是进程内部的执行单位，多个线程共享进程的资源，适用于需要高效并行处理的任务。线程之间的同步是一个常见问题。 协程：是一种轻量级的并发单位，适用于大量 IO 密集型的并发任务，通过手动控制调度提供高效的并发处理。 什么是nginx的异步非阻塞 # Nginx 的异步非阻塞 是其架构设计的核心特性之一，它使得 Nginx 能够高效地处理大量并发连接，而不需要为每个连接分配独立的线程或进程。理解这一特性有助于更好地理解 Nginx 为什么在处理大量并发请求时比传统的 Web 服务器（如 Apache）更高效。\n1. 异步（Asynchronous） # “异步”指的是处理请求时，Nginx 不会被单一请求的处理所阻塞。换句话说，当一个请求正在等待某些操作（比如从硬盘读取文件或等待数据库响应）时，Nginx 不会停下来等待这个操作完成，而是会去处理其他请求，直到该请求可以继续处理。\n示例： # 当用户发送一个 HTTP 请求，Nginx 会将该请求交给工作进程去处理。如果这个请求需要读取磁盘上的文件，而磁盘操作是耗时的，Nginx 不会在此时停止并等待磁盘操作完成。 在等待过程中，Nginx 会处理其他到达的请求，当磁盘操作完成时，再继续处理之前请求的后续操作（比如返回文件内容给客户端）。 2. 非阻塞（Non-blocking） # “非阻塞”指的是在处理请求时，Nginx 不会因为某个请求的某个阶段操作阻塞（阻止）其他请求的处理。Nginx 的事件循环机制确保了工作进程可以同时处理多个请求。\n示例： # 在处理一个请求的某个阶段时（比如读取文件、与数据库交互等），如果该操作需要一些时间，Nginx 不会阻止其他请求的处理。Nginx 会在等待这类操作时，转而处理其他可执行的任务或请求。 这种非阻塞的机制可以让 Nginx 充分利用 CPU 的时间片，不会因为某个慢操作停滞不前，极大地提高了并发处理能力。 3. 事件驱动模型 # Nginx 的异步非阻塞模式是通过事件驱动模型实现的。具体来说，Nginx 通过一个事件循环来管理多个连接，每个连接对应一个事件。当某个连接上的事件准备好（如数据已经准备好可以读取、可以发送响应等），Nginx 会通知相应的工作进程来处理这个事件，而不需要为每个请求创建一个新的线程或进程。\n工作原理： # 事件循环：Nginx 会启动一个事件循环，循环中会监听多个事件（如文件 I/O、网络 I/O 等）。一旦某个事件可以被处理，Nginx 就会去处理它。 I/O 多路复用：通过使用系统调用如 epoll（Linux）、kqueue（BSD）或 select，Nginx 能够高效地监听和处理多个 I/O 事件，而不需要创建多个线程或进程来等待每个请求的完成。 4. 单线程处理多个请求 # Nginx 通常采用单线程处理多个请求，而不是每个请求一个线程。通过非阻塞 I/O 和事件驱动机制，Nginx 可以在一个线程内并发处理成千上万的请求，而不会像传统的多线程模型那样消耗大量的系统资源（如内存、CPU）。这种设计使得 Nginx 在处理高并发时非常高效。\n5. 通过工作模式管理多个连接 # Nginx 的工作模式（worker process）通常采用单线程、多进程模型：\nMaster Process：主进程负责管理配置、进程生命周期等，子进程会根据需求启动或关闭。 Worker Process：每个工作进程处理客户端请求，处理过程中采用异步非阻塞方式。每个工作进程通常会有一个事件循环，处理多个连接。 6. 典型的异步非阻塞处理流程 # 假设有一个请求需要处理：\n接收请求：Nginx 的 worker 进程接收到客户端的请求，解析请求信息。 执行任务：如果请求需要进行磁盘 I/O（例如读取文件），Nginx 会发出 I/O 请求，然后立即返回等待其他事件发生。 继续处理其他请求：在等待 I/O 完成的过程中，Nginx 可以继续处理其他请求，不会因为一个请求的 I/O 操作而阻塞。 完成任务：一旦磁盘 I/O 完成，Nginx 会继续处理这个请求，并返回响应给客户端。 异步非阻塞的优势 # 高并发处理能力：能够同时处理大量并发请求，尤其是在 I/O 密集型操作中表现尤为出色。 低资源消耗：不需要为每个请求创建独立的线程或进程，从而减少了系统的资源消耗（如内存和 CPU 时间片的浪费）。 高效响应：可以迅速响应请求，特别适合高并发、高流量的场景。 总结 # 异步指的是在等待某个操作（如磁盘 I/O）完成时，Nginx 不会阻塞处理其他请求。 非阻塞指的是 Nginx 不会让任何一个请求阻塞其他请求的处理，能同时处理多个请求。 事件驱动模型和I/O 多路复用是实现这一特性的关键技术，使得 Nginx 在高并发、高流量环境下依然能够高效稳定运行。 这些特性使得 Nginx 特别适合用于 Web 服务器、反向代理以及负载均衡等需要高并发处理的场景。\nlinux网络丢包怎么排查☆ # 在 Linux 中，网络丢包的原因可能是多种多样的，包括硬件故障、网络配置问题、带宽过载、系统资源问题等。以下是排查网络丢包的一些常见方法和工具：\n1. 使用 ping 命令测试丢包情况 # ping 是一个简单的工具，可以帮助检测是否有丢包，并能够提供丢包的百分比。\nping -c 100 \u0026lt;目标IP\u0026gt; -c 100 表示发送 100 个 ICMP 请求包。 如果丢包，ping 会显示丢包的百分比。 2. 检查网络接口的丢包情况 # 通过查看网络接口的统计信息，可以了解丢包的具体情况。使用 ifconfig 或 ip -s link 命令来查看网络接口的统计数据。\n使用 ifconfig： # ifconfig \u0026lt;网络接口\u0026gt; 检查输出中的 RX errors（接收错误）和 TX errors（发送错误），这两个字段表示接收和发送时的错误数量。如果存在大量的错误，可能是网络丢包的原因。\n使用 ip -s link： # ip -s link show \u0026lt;网络接口\u0026gt; ip 命令提供了更详细的统计信息，查看 RX dropped 和 TX dropped 字段，这两个字段分别表示接收和发送过程中丢失的包数。\n3. 查看系统日志 # 丢包可能与系统的硬件、网络驱动或其他系统问题相关。检查系统日志中是否有网络相关的错误信息。\ndmesg | grep eth # 检查与以太网相关的日志 如果发现大量的与网卡相关的错误（如“RX errors”或“TX errors”），可能是硬件问题。 也可以检查 /var/log/syslog 或 /var/log/messages 等日志文件。 4. 使用 netstat 查看网络连接情况 # netstat 命令可以帮助你查看当前网络连接的状态，并能检查是否有异常的连接导致丢包。\nnetstat -s 查看是否有大量的 TCP 错误或其他协议错误，特别是 TCP 的 Retransmissions（重传次数）和 Segs Out（发出的段数）字段。 5. 使用 traceroute 检查网络路径 # 如果怀疑丢包发生在与外部服务器之间的网络传输过程中，可以使用 traceroute 工具来检查数据包经过的路由路径，以及每跳的丢包情况。\ntraceroute \u0026lt;目标IP\u0026gt; traceroute 可以帮助你识别哪个路由节点存在丢包。 6. 查看系统资源是否不足 # 网络丢包可能是因为系统的资源不足（如 CPU 或内存）导致网络处理变慢或缓慢。使用以下命令检查系统资源：\nCPU 使用情况： # top # 或者 htop 查看系统是否有高负载，特别是网络相关的进程是否占用过多 CPU。\n内存使用情况： # free -m 查看系统内存是否已经使用完，尤其是交换分区（swap）是否被过度使用。\n磁盘使用情况： # iostat -x 1 查看磁盘的 I/O 性能，是否出现瓶颈，影响到网络通信。\n7. 调整 TCP 缓冲区 # TCP 缓冲区的设置不当也可能导致丢包，尤其是在高带宽、低延迟的网络环境下。如果丢包发生在长时间的数据传输中，可以尝试调整 TCP 缓冲区大小。\n可以通过以下命令查看和调整系统的 TCP 缓冲区：\nsysctl -a | grep net.ipv4.tcp_rmem # 查看接收缓冲区 sysctl -a | grep net.ipv4.tcp_wmem # 查看发送缓冲区 调整 TCP 接收和发送缓冲区大小：\nsysctl -w net.ipv4.tcp_rmem=\u0026#34;4096 87380 6291456\u0026#34; sysctl -w net.ipv4.tcp_wmem=\u0026#34;4096 16384 4194304\u0026#34; 8. 使用 ss 工具进行网络连接分析 # ss 是一个用于查看套接字连接的命令，可以用于检查网络连接的状态和是否有大量的丢包。\nss -s 这将显示各种协议的统计信息，特别是 TCP 连接的状态。 9. 检查网卡驱动和硬件问题 # 如果发现网络丢包是由硬件故障引起的，可以通过以下方法进行排查：\n检查网络接口卡（NIC）是否存在硬件故障，尝试更换网卡。 查看网络驱动程序是否有更新，更新驱动程序。 检查网络设备（如交换机、路由器）是否存在故障或配置错误。 10. 网络带宽和流量限制 # 网络带宽不足、流量过载或 QoS（服务质量）策略设置不当，也可能导致丢包。通过以下方式检查带宽和流量限制：\n检查是否有带宽限制（如防火墙规则、流量整形等）。 使用 iftop 或 nload 等工具查看当前网络流量的实时状况。 iftop # 实时流量监控 nload # 实时网络带宽监控 总结 # 基本排查方法：使用 ping、ifconfig、ip -s link 等工具检查丢包情况。 系统资源监控：使用 top、free、iostat 等工具检查系统资源是否正常。 网络路径分析：使用 traceroute 来查看是否有网络路径丢包。 调整参数：调整 TCP 缓冲区等参数可能有助于减少丢包。 通过这些方法，逐步排查网络丢包的原因，通常可以定位到具体问题并进行修复。\n常用的性能分析诊断命令☆ # 在 Linux 系统中，进行性能分析和诊断时，有许多命令和工具可以帮助你查看系统资源使用情况、诊断性能瓶颈、排查问题等。以下是一些常用的性能分析和诊断命令：\n1. top # top 是最常用的性能分析工具，它显示实时的系统资源使用情况，包括 CPU、内存、磁盘、网络等。它适用于检查系统的整体状态，特别是资源占用较高的进程。\n用法：\ntop 功能：\n显示进程的实时资源占用情况。 可以按 CPU、内存等资源占用排序。 使用 P 或 M 可以按 CPU 或内存使用量排序。 使用 1 显示每个 CPU 的使用情况。 2. htop # htop 是 top 的增强版，提供了更友好的界面和交互方式。它可以显示更多的系统信息，并允许用户通过键盘快捷键方便地进行操作。\n用法：\nhtop 功能：\n显示进程树状图。 可以通过上下箭头选择进程并对其进行操作（如杀死进程）。 支持按 CPU、内存、PID 等排序。 3. vmstat # vmstat（Virtual Memory Statistics）可以用来报告关于虚拟内存、进程、CPU 活动、磁盘 I/O 等信息。它常用于查看系统性能瓶颈。\n用法：\nvmstat 1 5 # 每 1 秒输出一次，共输出 5 次 功能：\n输出内存、交换空间、进程、CPU 使用等统计信息。 重点查看 procs、memory 和 cpu 部分，来判断是否存在瓶颈。 4. iostat # iostat（Input/Output Statistics）可以显示系统的 I/O 性能，帮助诊断磁盘或存储系统的性能问题。它显示设备的 I/O 统计信息、CPU 使用情况等。\n用法：\niostat -x 1 5 # 每 1 秒输出一次，共输出 5 次，显示详细统计 功能：\n查看每个磁盘设备的 I/O 活动、吞吐量、I/O 队列长度等。 监控磁盘的延迟、吞吐量等性能数据，帮助诊断存储瓶颈。 5. netstat # netstat（Network Statistics）用于查看网络连接、路由表、接口统计等信息。它帮助诊断网络连接、带宽使用情况等。\n用法：\nnetstat -tulnp # 查看所有正在监听的端口及其对应的进程 功能：\n-tulnp：显示 TCP、UDP、监听端口和对应的进程。 -s：显示网络协议的统计信息，如 TCP 错误、接收/发送丢包等。 6. ss # ss（Socket Statictics）是一个比 netstat 更快的替代工具，用于显示网络连接、套接字统计等信息。它支持查看 TCP、UDP 等连接状态。\n用法：\nss -s # 显示套接字的总览信息 ss -tuln # 查看监听的 TCP/UDP 端口 功能：\n显示当前的网络连接、监听的端口等。 支持筛选显示特定状态的连接，如 ESTAB（已建立连接）或 LISTEN（监听连接）。 7. sar # sar（System Activity Report）是一个非常强大的性能监控工具，它通过收集系统性能数据来生成报告。它可以显示系统的 CPU 使用、内存、I/O、网络等资源的历史数据。\n用法：\nsar -u 1 5 # 每 1 秒报告一次 CPU 使用情况，共 5 次 功能：\n显示过去的系统性能历史数据。 sar -u：显示 CPU 使用情况。 sar -n DEV：查看网络接口统计。 sar -r：查看内存使用情况。 8. dstat # dstat 是一个功能强大的多功能监控工具，它可以同时显示 CPU、内存、磁盘、网络、IO 等多项系统资源的使用情况。\n用法：\ndstat -cdngy # 同时查看 CPU、磁盘、网络、磁盘 I/O 等信息 功能：\n提供全面的系统资源使用信息。 支持实时显示各种指标，适用于综合性能监控。 9. strace # strace 用于追踪系统调用，可以帮助诊断程序在执行过程中遇到的问题，如文件打开、网络请求等系统调用。\n用法：\nstrace -p \u0026lt;PID\u0026gt; # 跟踪指定进程的系统调用 功能：\n记录进程的系统调用，可以帮助诊断文件 I/O、网络连接等操作是否正常。 适用于调试程序和排查性能瓶颈。 10. perf # perf 是 Linux 提供的一个性能分析工具，用于分析系统的 CPU 性能、缓存命中率、上下文切换等低级性能指标。\n用法：\nperf top # 查看实时的性能瓶颈，类似于 `top`，但可以显示更多的系统信息 perf record -g ./my_program # 记录程序的性能数据 功能：\n通过采样来分析程序的 CPU 使用情况。 可以查看哪些函数、代码段消耗了最多的 CPU 时间，帮助找出性能瓶颈。 11. lsof # lsof（List Open Files）用于列出系统中所有打开的文件，包括网络连接、文件句柄等。可以用于分析文件或网络资源的使用情况。\n用法：\nlsof -i :80 # 查看占用 80 端口的进程 lsof -p \u0026lt;PID\u0026gt; # 查看指定进程打开的文件 功能：\n显示打开的文件及其相关的进程。 查找占用特定端口的进程，诊断端口冲突问题。 12. iotop # iotop 是一个实时的磁盘 I/O 使用情况监控工具，它可以显示哪些进程在消耗磁盘带宽。\n用法：\niotop 功能：\n显示每个进程的磁盘 I/O 使用情况。 帮助排查磁盘 I/O 性能瓶颈。 13. mpstat # mpstat 用于显示每个 CPU 的使用情况，特别适用于多核 CPU 系统。\n用法：\nmpstat -P ALL 1 5 # 显示每个 CPU 核心的使用情况 功能：\n显示各个 CPU 核心的负载情况，有助于诊断 CPU 是否存在瓶颈。 总结 # 系统资源使用分析：top, htop, vmstat, iostat, dstat。 网络性能分析：netstat, ss, sar。 磁盘 I/O 分析：iotop, iostat。 应用级性能分析：strace, perf。 系统调用分析：strace, perf。 打开文件和资源分析：lsof。 这些命令和工具可以帮助你从不同角度进行系统性能分析和故障诊断。根据不同的需求和问题，你可以选择合适的工具来深入排查和分析系统性能。\n什么是进程中断 # 进程中断是指在操作系统中，进程执行过程中，系统主动暂停其执行，并根据一定的规则进行某些操作（如处理中断请求），然后再恢复进程的执行。中断通常由硬件、操作系统内核或其他软件引发，它会打断当前进程的执行，优先处理某些特定的任务或事件，确保系统能够高效地响应外部或内部事件。\n中断是现代操作系统中非常重要的机制，它能够实现高效的资源共享、事件响应和进程调度。\n进程中断的种类 # 硬件中断 由外部硬件设备（如磁盘、键盘、鼠标、网络卡等）发起。 例如，当硬盘完成数据读取时，硬盘控制器会发出中断，通知 CPU 进行下一步操作。 设备中断通常由外部事件触发，像网络卡接收到数据包时，会触发网络中断。 软件中断 由软件程序发起，通常是为了实现系统调用或请求操作系统服务。 例如，系统调用（如 read(), write()）时，程序会发起软件中断，切换到内核模式以执行特权操作。 程序可以使用特殊的指令来发起软件中断，例如 int 0x80 在 Linux 中用于触发系统调用。 定时器中断 由系统定时器触发，通常用于时间片轮转、进程调度等。 定时器中断是操作系统进行进程管理和资源分配的重要手段。例如，操作系统通过定时器中断来定期检查是否需要切换进程，确保公平的 CPU 时间分配。 外部中断 外部事件引发的中断，如来自外部设备的中断请求。 例如，按下键盘时，键盘控制器会产生外部中断来通知操作系统读取键盘输入。 内部中断 由 CPU 内部状态或执行错误触发，例如除零错误、非法指令等。 例如，程序试图除以零时，CPU 会触发一个中断，处理器将跳转到错误处理程序。 进程中断的工作流程 # 中断请求 中断发生时，外部设备（如硬盘、键盘、网络适配器等）或内部硬件（如计时器）会发出中断请求信号，通知 CPU 需要处理中断。 保存上下文 当中断发生时，操作系统首先会保存当前进程的执行状态（即上下文），包括程序计数器（PC）、寄存器值、堆栈指针等，以便稍后能够恢复执行。 中断处理 操作系统内核根据中断类型进行相应的处理中断程序（中断服务程序，ISR）。这可能涉及数据读取、设备驱动程序的调用或执行其他操作。 例如，硬件中断可能导致操作系统读取硬盘数据，定时器中断可能导致操作系统调度下一个进程。 恢复执行 中断处理完成后，操作系统会恢复被中断进程的上下文，继续执行该进程。中断后的程序通常会从中断发生之前的地方继续执行。 中断的优先级和中断屏蔽 # 在操作系统中，中断通常有优先级的概念。高优先级的中断会先处理，而低优先级的中断会在高优先级中断处理完之后才进行。\n中断屏蔽：操作系统可以在某些时刻禁止或屏蔽某些类型的中断，以保证某些重要操作的完成。例如，操作系统在进行进程切换时可能会暂时屏蔽中断，以避免在关键时刻被打断。 嵌套中断：有些情况下，高优先级的中断可以打断低优先级中断的处理过程，称为嵌套中断。操作系统通常会处理完当前高优先级中断后，再去处理低优先级的中断。 进程中断的作用 # 提高系统响应性 中断机制可以在任何时刻打断正在执行的进程，优先响应外部设备的请求，使得系统能够及时处理硬件设备或外部事件的变化。 例如，在实时操作系统中，中断机制确保能够快速响应外部事件（如传感器数据、用户输入等）。 实现多任务并行 中断使得操作系统能够迅速切换不同进程的执行状态，配合进程调度算法实现 CPU 时间的公平分配。 例如，当一个进程处于 I/O 阻塞时，操作系统通过中断机制能够迅速调度其他进程，最大化系统资源的利用率。 设备驱动与外设管理 通过中断，操作系统可以有效地与外设进行交互。当设备完成某项任务时，它会触发中断请求，操作系统可以响应并执行相应操作（如读取数据、发送数据等）。 例如，磁盘 I/O 中断会通知操作系统磁盘读取操作已完成，网络中断通知操作系统收到网络数据包。 优化系统性能 通过中断，系统可以减少空闲等待的时间，进程不需要轮询硬件状态，而是等到硬件完成任务后再被中断处理。 例如，在网络通信中，进程可以等待中断信号，以避免频繁轮询网络接口。 总结 # 进程中断 是操作系统中一种重要的机制，它能够在进程执行过程中打断当前的执行流，优先处理一些外部或内部的事件，并在中断处理完毕后恢复进程的执行。 中断分为硬件中断、软件中断、定时器中断等多种类型。 通过中断，操作系统可以实现高效的设备管理、进程调度和系统响应，从而提高系统的性能和响应速度。 进程中断是操作系统多任务处理的核心部分，极大地增强了系统的灵活性和响应能力。\n什么是软中断、硬中断 # 在计算机系统中，中断 是一种机制，它允许外部或内部事件打断正在运行的程序，以便系统可以处理中断请求。在操作系统中，中断可以分为 软中断 和 硬中断，它们的触发来源和作用有所不同。以下是两者的详细解释：\n1. 硬中断 (Hardware Interrupt) # 硬中断 是由硬件设备触发的中断信号，通常是外部设备（如磁盘、网络适配器、键盘、鼠标等）发出的信号，用于通知 CPU 需要处理某个事件。硬中断的目的是让操作系统能够实时响应外部设备的变化或事件，例如数据准备完成、输入信号等。\n特点 # 触发来源：硬中断是由外部硬件设备（如外设、控制器等）触发的。 高优先级：硬中断的优先级较高，当硬件设备发出中断信号时，CPU 会停止当前的任务，立即响应中断。 硬件中断号：硬件中断通常有固定的中断号，例如中断控制器会分配不同的中断号来标识不同的设备。 中断处理程序：硬中断会跳转到中断服务例程（ISR，Interrupt Service Routine），在该例程中，操作系统会处理硬件请求（如数据读取、状态更新等）。 示例 # 磁盘 I/O 完成：硬盘在完成数据读写后发出中断，通知操作系统该操作已完成，操作系统会读取数据并继续处理。 键盘输入：键盘输入一个按键时，键盘会发出中断，通知 CPU 处理按键输入事件。 定时器中断：系统定时器发出中断，操作系统会进行进程调度，切换当前进程。 处理流程 # 硬件设备通过中断控制器向 CPU 发送中断信号。 CPU 停止当前进程，保存当前进程的状态。 CPU 跳转到硬中断的中断服务例程（ISR）来处理该硬件设备的请求。 处理完成后，CPU 恢复原进程的执行。 2. 软中断 (Software Interrupt) # 软中断 是由程序或操作系统通过软件指令触发的中断，通常用于系统调用或操作系统内核执行一些特权操作。软中断的主要目的是提供一种程序在用户态与内核态之间切换的机制，使得用户程序能够请求操作系统提供的服务。\n特点 # 触发来源：软中断由程序发起，通常通过特定的指令（如 int 指令）触发。 低优先级：软中断的优先级较低，一般是在程序主动请求系统服务时触发。 中断号：软中断使用一个软件中断号，通常由操作系统定义，用于区分不同类型的软中断。 系统调用：软中断是系统调用的一部分，用户程序通过触发软中断进入内核态，向操作系统请求服务。 示例 # 系统调用：用户程序调用 read()、write() 等系统调用时，操作系统通过软中断进入内核态执行相应的操作。 错误处理：程序发生错误时，操作系统可能通过软中断处理异常，例如除零错误或非法操作。 处理流程 # 用户程序通过 int 指令或类似机制发出软中断请求。 CPU 暂停当前程序，保存进程的状态。 CPU 跳转到软中断的中断服务例程（通常是操作系统的内核代码）。 操作系统根据中断号执行相应的系统调用或内核操作。 处理完成后，操作系统返回用户程序并恢复执行。 硬中断与软中断的区别 # 特性 硬中断 (Hardware Interrupt) 软中断 (Software Interrupt) 触发方式 外部硬件设备触发 程序通过特定指令触发 优先级 高优先级 低优先级 常见触发源 硬件设备（如磁盘、网络卡、定时器等） 用户程序或操作系统内部 目的 响应外部事件或设备请求 进入内核态执行系统调用或处理错误 例子 磁盘 I/O 完成、定时器中断、键盘输入 系统调用（如 read()、write()） 应用场景和作用 # 硬中断：硬中断主要用于处理外部设备事件，确保操作系统能够及时响应硬件变化，如 I/O 完成、外设信号等。这使得操作系统能够高效地进行多任务处理和设备管理。 软中断：软中断用于程序与操作系统内核之间的交互，提供了用户态和内核态的切换机制。通过软中断，程序能够请求操作系统提供的服务（如文件操作、内存分配等），并且能够处理程序错误或异常。 总结 # 硬中断 由硬件设备触发，用于及时响应外部事件，优先级较高，通常需要立即处理中断请求。 软中断 由程序通过指令触发，用于用户程序与操作系统之间的交互，主要用于系统调用和错误处理。 这两者共同作用，确保了操作系统能够高效地管理硬件资源、响应外部事件，并且提供用户程序与内核之间的协作机制。\n什么是不可中断进程 # 不可中断进程（也称为 D状态进程 或 不可打断进程）是指在运行过程中无法被操作系统的调度程序中断或打断的进程。通常，这种进程在执行某些特定操作时，系统不允许其他进程或信号中断其执行，确保其完成当前的任务。\n在 Linux 和类 Unix 系统中，进程的状态通常会在 /proc 目录下的进程状态文件中查看，其中有一个字段表示进程的状态。不可中断进程的状态通常是 D，即 不可中断睡眠（Uninterruptible Sleep），表示进程正在等待某些资源（如 I/O 操作、磁盘读取、网络请求等），并且在完成这些操作之前，不能被任何信号打断。\n1. 不可中断进程的特点 # 无法响应信号：当一个进程处于不可中断状态时，操作系统不会响应它的信号（比如 SIGKILL 或 SIGTERM 等）。这意味着即使你尝试通过命令（如 kill）终止这个进程，它也无法被中断或杀死，直到进程的当前操作完成。\n等待 I/O 操作：不可中断进程通常出现在等待某些硬件 I/O 操作完成时，如硬盘读写、网络通信等。进程处于此状态时，CPU 无法调度它，直到 I/O 操作结束，进程才会恢复执行。\n系统资源占用：由于进程在等待 I/O 操作或资源，它会占用一定的系统资源，直到操作完成。\n进程状态：通过 ps 或 top 等命令查看进程时，进程会显示为 D 状态。这是不可中断进程的标志。例如：\nps aux | grep \u0026lt;pid\u0026gt; 在输出中，进程的状态会显示为 D，表示该进程正在不可中断的等待中。\n2. 不可中断进程的常见原因 # 不可中断进程通常是由于以下几种原因导致的：\n1. I/O 操作 # 当进程在进行硬件 I/O 操作时，比如磁盘读取、网络通信或等待从设备获取数据时，它会进入不可中断状态。 例如，进程请求读取硬盘上的数据时，如果磁盘正在忙碌或者出现延迟，进程可能会进入不可中断状态，直到磁盘操作完成。 2. 文件系统操作 # 如果进程正在访问一个挂载的文件系统（如读取文件），且该文件系统出现问题（如磁盘故障、挂载点不可访问等），进程可能会长时间处于不可中断状态。 3. 设备驱动程序问题 # 如果操作系统或内核中的设备驱动程序存在问题（例如死锁或资源争用），可能导致进程进入不可中断状态，无法完成操作。 4. 网络通信 # 网络相关的操作，如等待网络数据包，尤其是在高延迟或网络中断的情况下，进程可能会长时间处于不可中断状态，直到网络通信恢复。 3. 不可中断进程的处理 # 进程卡死：不可中断进程通常会表现为“卡死”，因为它们无法被杀死或中断，无法响应信号。在这种情况下，如果不可中断进程长时间不恢复，系统管理员需要调查进程为何无法中断，通常需要查看系统日志、硬件状况或相关驱动程序问题。 解决方法： 检查硬件或设备问题：如果进程处于不可中断状态且是由硬件问题（如磁盘故障、网络中断等）引起的，解决问题通常需要恢复硬件功能或修复设备问题。 查看内核日志：内核日志文件（如 /var/log/messages 或 dmesg）可能包含导致进程无法中断的错误信息。例如，如果是因为硬件故障或驱动问题，内核日志中可能有相关错误提示。 重启系统：在某些情况下，无法通过普通手段终止不可中断进程。如果进程导致系统资源耗尽或卡死，可能需要重启计算机来清理这些进程。 4. 不可中断进程的例子 # 以下是一些常见的导致进程进入不可中断状态的情景：\n硬盘故障：例如，进程正在读取磁盘数据，而磁盘出现故障或延迟时，进程可能会卡在等待磁盘响应的状态。 网络阻塞：如果进程正在进行网络请求，且网络连接出现问题（例如目标服务器不可达，或者网络拥塞），进程可能进入不可中断状态，直到网络恢复。 驱动程序错误：某些设备驱动程序存在 bug 或设计缺陷时，可能导致进程无法继续执行，直到该驱动的操作完成。 5. 如何排查不可中断进程 # 使用 ps 命令查看进程状态： 你可以使用 ps 或 top 命令查看进程的状态，确认是否有进程处于 D 状态：\nps aux | grep D 或使用 top 命令查看所有进程的状态：\ntop 如果进程处于 D 状态，表示它处于不可中断状态。\n查看系统日志： 系统日志中可能有有关进程卡死的原因。例如：\ndmesg | tail -n 100 检查硬件设备： 确保没有硬件故障，尤其是磁盘、网络等关键硬件。\n重新启动进程： 如果无法终止不可中断进程，可以尝试重新启动相关进程或重启系统来恢复正常运行。\n总结 # 不可中断进程是由于进程正在等待某些资源（如 I/O 操作、硬件设备响应等）而不能被中断的进程。它们通常处于 D 状态，且无法响应信号。通常，硬件问题、驱动程序问题或资源争用可能导致进程进入这种状态。在这种情况下，管理员需要调查硬件、驱动或网络等相关问题，必要时重启系统来恢复正常。\n什么是栈内存和堆内存 # 栈内存（Stack Memory）和堆内存（Heap Memory）是程序运行过程中用于存储数据的两种不同类型的内存区域，它们有各自不同的管理方式和使用场景。\n1. 栈内存 (Stack Memory) # 栈内存是一种存储临时数据的内存区域，通常由操作系统自动管理。当一个程序调用一个函数时，系统会为该函数在栈上分配内存，用于存储局部变量和函数调用的返回地址。栈内存是 LIFO（后进先出） 的结构，也就是说，后入栈的数据会先被弹出。\n特点： # 自动管理：栈内存的分配和释放是自动的。当一个函数调用时，局部变量和函数参数会被压入栈中，函数返回时，这些局部变量和参数会自动被弹出栈，内存空间会被释放。 快速分配与释放：栈内存的分配和释放速度非常快，因为它是基于栈顶的操作，分配内存只需要简单地向栈顶“压栈”，释放内存则是“弹栈”。 大小有限：栈内存的大小通常是有限的，一般由操作系统决定。如果栈空间用尽（例如递归调用过深），就会发生 栈溢出（Stack Overflow）错误。 存储数据：栈内存用于存储局部变量、函数参数和函数调用的返回地址等。 适用场景： # 局部变量：函数内部的局部变量会存储在栈内存中。它们的生命周期仅限于函数执行期间，函数执行完毕后，栈中的数据会自动销毁。 函数调用：每次函数调用时，栈会为该函数分配内存空间，保存该函数的局部变量、返回地址等信息。 示例： # void foo() { int x = 10; // 局部变量 x 存储在栈中 int y = 20; // 局部变量 y 存储在栈中 } 在这个例子中，x 和 y 都是局部变量，存储在栈内存中，函数 foo 调用时，它们被压入栈中，函数执行完毕后，它们的内存会被自动释放。\n2. 堆内存 (Heap Memory) # 堆内存是一块由程序员控制的内存区域，用于动态分配内存。堆内存的大小在程序运行时可以根据需要动态扩展，适用于存储那些需要在多个函数或不同地方之间共享的较大的数据结构。\n特点： # 手动管理：与栈内存不同，堆内存的分配和释放是由程序员手动管理的。在 C/C++ 中，程序员使用 malloc、free 或 new、delete 等操作来分配和释放堆内存。如果程序员忘记释放堆内存，就会发生 内存泄漏（Memory Leak），导致程序占用越来越多的内存。 较慢的分配与释放：堆内存的分配和释放速度比栈内存慢，因为它需要通过更复杂的算法来管理内存（例如，查找空闲空间、合并已释放内存等）。 较大的内存空间：堆内存的大小通常仅受系统内存大小的限制，相对于栈内存，堆内存更大，因此适合存储大规模数据。 动态内存：堆内存的大小可以在程序运行时动态决定，适合存储那些大小无法在编译时确定的数据。 适用场景： # 动态分配的数据结构：例如，链表、树、图等动态数据结构，它们的大小在程序运行时才能确定，因此需要使用堆内存。 对象：在面向对象编程中，通常使用堆内存来动态创建对象，以便在多个函数之间共享或延长对象的生命周期。 示例： # void foo() { int* ptr = (int*)malloc(sizeof(int)); // 在堆上分配内存 *ptr = 10; // 访问堆上的内存 free(ptr); // 手动释放堆内存 } 在这个例子中，ptr 是一个指向堆内存的指针，程序使用 malloc 分配了一块内存，并在堆中存储一个整数 10。在使用完堆内存后，程序需要手动调用 free 来释放这块内存。\n栈内存与堆内存的区别 # 特性 栈内存 堆内存 分配方式 自动分配，随函数调用和返回自动管理 程序员手动分配和释放 内存大小 相对较小，有限，通常几 MB 到几十 MB 大且灵活，仅受系统内存限制 生命周期 函数调用时分配，函数结束时自动销毁 持续到程序员显式释放（free 或 delete） 分配与释放速度 快速 相对较慢 访问速度 较快，CPU 缓存优化 较慢，堆内存的管理较复杂 适用场景 局部变量、函数调用、递归 动态分配内存、对象、动态数据结构等 内存管理问题 栈溢出（如递归过深） 内存泄漏（未释放堆内存） 栈内存与堆内存的选择 # 栈内存：适合用于存储生命周期短、大小固定的局部数据。栈内存是自动管理的，且访问速度非常快，但它的大小是有限的，不适合存储大量的数据或需要在多个函数之间共享的数据。 堆内存：适合用于存储动态分配的、生命周期较长或大小不确定的数据。堆内存的管理较为复杂，程序员需要手动分配和释放内存，且可能会遇到内存泄漏的问题。 总结 # 栈内存 用于存储局部变量和函数调用等临时数据，由系统自动管理，速度快但大小有限。 堆内存 用于存储动态分配的较大数据，程序员需要手动管理，适用于存储动态数据结构和对象。 top 命令里面可以看到进程哪些状态☆ # 在 Linux 系统中，使用 top 命令可以实时查看系统资源的使用情况，包括 CPU、内存、进程等信息。在 top 命令的输出中，进程的状态通常显示在 STAT（状态）这一列中。每个进程有一个对应的状态字母，表示该进程的当前状态。不同的字母代表不同的进程状态。\n进程状态的常见字母及其含义： # R (Running) — 运行中 该进程正在运行或在就绪队列中等待运行。通常，进程正在执行或准备执行。 S (Sleeping) — 睡眠中 该进程处于休眠状态，等待某些事件或条件（如 I/O 完成）。这通常是大部分进程的状态，它们在等待资源时会进入此状态。 D (Uninterruptible Sleep) — 不可中断睡眠 该进程处于不可中断睡眠状态，通常是因为它正在等待 I/O 操作完成，如磁盘读取或网络请求。进程无法被任何信号打断，只有当操作完成时，进程才会继续执行。如果进程长时间处于该状态，可能表示 I/O 阻塞或设备故障。 T (Stopped) — 停止 该进程已经停止运行。可能是因为它接收了 SIGSTOP 信号，或者正在调试过程中。你可以使用 kill 命令发送 SIGCONT 信号恢复该进程。 Z (Zombie) — 僵尸 该进程已经完成执行，但它的父进程尚未调用 wait() 或 waitpid() 来读取它的退出状态。僵尸进程占用进程表项，但不占用 CPU 时间。 I (Idle) — 空闲 在某些系统中，I 状态表示进程处于空闲状态，CPU 处于空闲模式。但一般来说，top 中没有明确的 I 状态，通常会看到 R 或 S。 X (Dead) — 死亡 该进程已经死亡并且不再存在，通常系统将其从进程表中移除。X 状态是比较少见的，通常表示系统中某些资源的异常状态。 W (Paging) — 页面调度 进程正在被操作系统调度来进行页面交换（Paging），即进程内存正在被交换到磁盘或从磁盘加载。 top 命令输出中的进程状态字段示例： # PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1234 user1 20 0 10000 4000 3000 S 1.0 0.1 0:05.67 myapp 5678 user2 20 0 20000 8000 6000 D 0.5 0.2 0:02.14 anotherapp 9012 user3 20 0 15000 6000 5000 Z 0.0 0.0 0:00.01 myzombie S、D 和 Z 状态的具体说明： # S（睡眠中）：这通常是一个进程在等待某个事件（如等待 I/O）发生时的状态。它可能会在等待磁盘、网络、文件描述符等资源时处于这种状态。大多数进程在没有执行任务时都处于睡眠状态。 D（不可中断睡眠）：这是进程在执行某些 I/O 操作时，无法被外部信号打断的状态。例如，等待磁盘操作或网络数据传输的完成。进程一旦完成这些操作，它就会变为 S 或其他状态。如果进程长时间处于 D 状态，可能意味着系统的硬件资源（如磁盘、网络等）出现了瓶颈或故障。 Z（僵尸进程）：僵尸进程是已经结束的进程，但它的父进程没有收集它的退出状态，仍然保留在进程表中。它占用系统资源（进程 ID），但是不占用 CPU 和内存，通常会在父进程调用 wait() 或 waitpid() 时清理掉。 进程状态总结： # 状态字母 描述 说明 R Running（运行中） 进程正在执行或准备执行。 S Sleeping（睡眠中） 进程处于等待状态（如 I/O）。 D Uninterruptible Sleep（不可中断睡眠） 进程在等待 I/O 操作，不能被打断。 T Stopped（停止） 进程已停止，可以通过 SIGCONT 恢复。 Z Zombie（僵尸进程） 进程已终止，但父进程未回收退出状态。 I Idle（空闲） 进程处于空闲状态。 X Dead（死去） 进程已死亡，通常系统清理后会消失。 W Paging（页面调度） 进程处于页面交换操作中。 其他补充： # top 命令中的进程状态可以通过按下 Shift + P 来查看，也可以通过 Shift + M 查看内存使用情况。 不同的 Linux 发行版和版本的 top 命令可能会有所不同，状态字母可能有细微差异，但大体相同。 理解进程的不同状态可以帮助你诊断系统中的问题，如发现进程长时间处于 D 或 Z 状态，可能需要进一步调查资源瓶颈或系统配置问题。\nLinux 系统中/proc是做什么的 # 在 Linux 系统中，/proc 目录是一个虚拟文件系统（virtual filesystem），它提供了内核与系统信息的访问接口。通过 /proc，你可以查看操作系统内核、进程、硬件、内存等的实时信息。实际上，/proc 并不存储真正的文件，而是由内核动态生成的虚拟文件，这些文件的内容反映了当前系统的状态。\n/proc 目录的主要作用 # 提供系统和进程信息 /proc 包含了大量的虚拟文件和目录，用户可以通过查看这些文件来获得系统运行时的各种信息，比如进程信息、内存状态、硬件配置等。 系统监控和调试 /proc 中的许多文件允许系统管理员和程序员获取系统运行时的状态。这些文件对于性能监控、故障排除以及系统调试非常有用。 与内核交互 /proc 目录是与内核交互的一个重要接口，可以通过它修改一些内核参数或动态调整系统设置。 /proc 目录中的重要文件和子目录 # /proc/[pid] 这是进程信息目录，其中 [pid] 是一个数字，表示进程的 ID。每个进程都有一个对应的目录，在该目录下有许多文件，可以获取到进程的详细信息，例如：\n/proc/[pid]/cmdline：进程的命令行 /proc/[pid]/status：进程的状态、内存、IO等信息 /proc/[pid]/stat：进程的统计信息，包括 CPU 时间、内存使用等 /proc/[pid]/fd/：该进程打开的文件描述符列表 /proc/cpuinfo 包含了有关 CPU 的详细信息，如型号、核数、频率等。查看该文件可以获取当前系统 CPU 的配置信息。\ncat /proc/cpuinfo /proc/meminfo 提供有关内存的详细信息，包括总内存、已用内存、空闲内存、缓存、交换空间等。\ncat /proc/meminfo /proc/uptime 显示系统启动以来的运行时间，以及空闲时间（单位：秒）。\ncat /proc/uptime /proc/partitions 显示磁盘分区信息，包括设备、分区大小等。可以查看系统上的磁盘设备和分区情况。\ncat /proc/partitions /proc/loads 显示系统的负载平均值。包括过去 1 分钟、5 分钟和 15 分钟的平均负载。\ncat /proc/loadavg /proc/filesystems 列出系统支持的文件系统类型，例如 ext4、btrfs 等。\ncat /proc/filesystems /proc/net/ 该目录下包含了与网络相关的虚拟文件，提供网络配置、连接信息等。例如：\n/proc/net/tcp：TCP 连接信息 /proc/net/udp：UDP 连接信息 /proc/net/dev：网络接口统计信息 /proc/sys/ 包含了可用于调整内核参数的虚拟文件，用户可以在运行时通过修改这些文件来调整系统行为。例如：\n/proc/sys/net/ipv4/ip_forward：控制 IP 转发 /proc/sys/vm/swappiness：控制交换空间的使用策略 /proc/sys/kernel/hostname：查看或修改系统主机名 例如，查看和设置虚拟内存交换策略：\ncat /proc/sys/vm/swappiness echo 10 \u0026gt; /proc/sys/vm/swappiness /proc/version 显示当前操作系统内核的版本信息。\ncat /proc/version 使用 /proc 的常见场景 # 查看系统资源使用情况：可以通过查看 /proc/meminfo、/proc/cpuinfo、/proc/uptime 等文件，了解系统的内存、CPU 使用情况以及系统的运行时间。 监控进程信息：每个进程都有对应的 /proc/[pid] 目录，你可以用来实时监控进程的状态、CPU 使用情况、内存使用情况等。 动态调整系统参数：可以通过 /proc/sys/ 下的文件动态修改内核参数或配置，比如改变网络配置、调整虚拟内存管理等。 排查故障和调试：系统管理员可以通过分析 /proc 中的数据来诊断系统问题或监控异常行为，例如检查进程状态、文件系统、网络连接等。 示例 # 查看 CPU 信息\ncat /proc/cpuinfo 查看内存使用情况\ncat /proc/meminfo 查看当前系统的负载情况\ncat /proc/loadavg 查看当前系统运行时间\ncat /proc/uptime 查看系统支持的文件系统类型\ncat /proc/filesystems 总结 # /proc 目录是 Linux 系统中的一个虚拟文件系统，它提供了对系统运行时信息的访问接口。 通过 /proc 中的各种文件，用户和管理员可以获取 CPU、内存、硬盘、进程、网络等方面的实时信息。 /proc 目录中的文件通常是内核动态生成的，反映了系统的当前状态。 /proc 不仅用于查询信息，也可以用于修改内核参数，实时调整系统的行为。 load和cpu使用率区别 # 在 Linux 系统中，load（负载）**和**CPU 使用率是两个常见的性能指标，它们都涉及到系统的资源使用情况，但它们的含义和作用有所不同。\n1. Load（负载） # 负载表示系统在某一时间段内，处于运行或等待执行的进程数量。它反映了系统的工作负荷，可以用来评估系统的“忙碌”程度。负载值不仅仅与 CPU 使用情况有关，还包括 I/O 操作（如磁盘、网络等）和等待队列的进程数量。\n负载的关键点： # 负载的计算方式：系统负载通常用一个三元组表示，分别为过去 1 分钟、5 分钟、15 分钟的平均负载。这些值显示了系统负载的趋势。你可以通过 cat /proc/loadavg 来查看当前系统的负载。\n负载的含义\n：\n如果负载值小于或等于 CPU 核心数，则表明系统的资源还没有被完全占满，能够处理当前的任务。 如果负载值大于 CPU 核心数，则表明系统可能正在过载，可能会有很多进程在等待 CPU 时间片。 例如：如果你有 4 核 CPU，负载值为 4 表示系统刚好忙碌（每个核心都有任务）。如果负载为 8，表示系统的负载已经超过了 CPU 核心数，可能会有等待的进程。\n负载的查看： # cat /proc/loadavg 输出示例：\n0.85 1.12 1.14 2/310 15432 其中：\n0.85：过去 1 分钟的平均负载 1.12：过去 5 分钟的平均负载 1.14：过去 15 分钟的平均负载 2/310：表示当前正在运行的进程数（2），以及系统中所有进程的总数（310） 负载值的含义： # 1.00（或 1）：系统的负载等于 CPU 核心数，表明系统刚好忙碌。 \u0026gt; 1.00（或 CPU 核心数）：系统负载高，可能导致任务排队和延迟。 \u0026lt; 1.00：系统负载较低，通常表明 CPU 的处理能力有富余。 2. CPU 使用率 # CPU 使用率指的是 CPU 在某段时间内实际工作的比例，它反映了 CPU 资源的使用情况。CPU 使用率通常包括以下几个方面：\n用户空间的 CPU 使用率（user）：表示 CPU 在用户程序中执行任务的时间比例。 系统空间的 CPU 使用率（system）：表示 CPU 在内核空间中执行任务的时间比例。 空闲 CPU 时间（idle）：表示 CPU 没有工作、处于空闲状态的时间比例。 等待 I/O 操作的 CPU 时间（iowait）：表示 CPU 在等待 I/O 操作完成时的时间比例。 CPU 使用率的关键点：\n实时性：CPU 使用率是一个动态的、实时更新的指标，反映了 CPU 当前的负载状况。 计算方式：CPU 使用率通常通过周期性测量 CPU 的空闲时间和工作时间来计算，表示为一个百分比值。 CPU 使用率的查看： # 可以通过 top 命令来查看 CPU 使用率：\ntop 在 top 输出的第一行中，你可以看到 CPU 使用率的各项指标：\n%Cpu(s): 1.3 us, 0.3 sy, 0.0 ni, 98.4 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st us（user）：用户空间（程序运行）占用的 CPU 时间比例。 sy（system）：内核空间（系统调用、内核任务）占用的 CPU 时间比例。 ni（nice）：用户进程的优先级调整部分占用的 CPU 时间比例。 id（idle）：CPU 空闲的时间比例。 wa（iowait）：CPU 等待 I/O 操作的时间比例。 hi（hardware interrupt）：硬件中断占用的 CPU 时间比例。 si（software interrupt）：软件中断占用的 CPU 时间比例。 st（steal）：虚拟化环境中，虚拟 CPU 被其他虚拟机抢占的时间比例。 Load 和 CPU 使用率的区别 # 指标 Load（负载） CPU 使用率 含义 表示系统当前有多少进程在运行或等待运行 表示 CPU 实际工作的百分比 单位 平均负载值，通常是 1 分钟、5 分钟、15 分钟的平均值 百分比（%） 计算依据 与 CPU 核心数、进程排队和 I/O 等等待时间有关 与 CPU 的使用情况（用户空间、系统空间、空闲等）有关 查看方式 cat /proc/loadavg 或 uptime top 命令中的 CPU 使用情况 反映的内容 反映系统负载和任务队列的长度 反映 CPU 的工作量和空闲情况 影响因素 与进程数量、I/O 等相关，负载过高可能导致系统过载 与程序运行、系统调用等相关，CPU 使用率过高可能影响系统响应 系统过载情况 当负载值 \u0026gt; CPU 核心数时，系统可能过载 当 CPU 使用率接近 100% 时，系统可能被过度占用 示例 loadavg 值为 2，表示系统有 2 个进程在排队等待 CPU top 中 CPU 使用率显示 90%，表示 CPU 的 90% 被占用 如何判断系统性能问题 # 高负载： 如果 loadavg 值远高于 CPU 核心数，说明系统的负载很重，可能有很多进程在等待 CPU 时间。如果负载持续很高，可以考虑优化程序或增加硬件资源（如 CPU 核心数）。 高 CPU 使用率： 如果 CPU 使用率接近 100%，且空闲时间很少（id 值低），说明系统的 CPU 资源已被大量占用，可能需要优化 CPU 密集型程序，或者检查是否有不正常的进程。 I/O 等待： 如果 CPU 使用率较低，但负载较高，并且 iowait 比例较大，可能是系统存在 I/O 阻塞，导致进程等待磁盘或网络资源。 总结： # Load（负载） 主要反映系统整体的工作负载情况，关注的是有多少进程在等待资源（不仅仅是 CPU），以及这些进程是否被有效调度。 CPU 使用率 主要关注 CPU 的实际使用情况，反映了 CPU 资源的忙碌程度，关注的是 CPU 被多大比例占用。 通过结合查看负载和 CPU 使用率，可以更全面地了解系统的性能状况，帮助你判断是否存在瓶颈或资源不足的情况。\nMAC地址IP地址如何转换 # MAC 地址和 IP 地址是两种不同层次的地址，分别工作在不同的网络模型层：\nMAC 地址（媒体访问控制地址）是数据链路层（OSI 模型的第 2 层）的地址，用于唯一标识网络设备。每个网络接口卡（NIC）都有一个唯一的 MAC 地址，通常由硬件厂商分配。 IP 地址是网络层（OSI 模型的第 3 层）的地址，用于标识设备在网络中的位置，允许设备之间进行通信。 MAC 地址和 IP 地址的转换并不是直接相互转换的过程，但它们之间存在联系，尤其在局域网内，MAC 地址通过 ARP（地址解析协议） 可以与 IP 地址进行映射。\n如何通过 ARP 转换 MAC 地址和 IP 地址： # 从 IP 地址获取 MAC 地址\n当设备知道目标设备的 IP 地址 时，设备会通过 ARP 请求来寻找目标设备的 MAC 地址。 该过程通常如下： 设备广播一个 ARP 请求，询问“IP 地址为 X.X.X.X 的设备的 MAC 地址是什么？” 如果该 IP 地址对应的设备在同一局域网内，则目标设备会响应一个 ARP 响应，包含它的 MAC 地址。 发送方设备收到响应后，将 IP 地址与 MAC 地址进行映射，并将其存储在 ARP 缓存中，以供将来使用。 命令行示例（Linux）：\narp -n 192.168.1.10 这将显示 IP 地址 192.168.1.10 的 MAC 地址。\n从 MAC 地址获取 IP 地址\n如果你知道设备的 MAC 地址，并想找出其对应的 IP 地址，通常可以通过 ARP 缓存来查看。 在本地设备上，你可以查看 ARP 缓存表，其中会列出本地网络中设备的 IP 地址与 MAC 地址的映射。 命令行示例（Linux）：\narp -a 这将列出所有已知设备的 IP 地址和它们对应的 MAC 地址。\nARP 工作原理： # ARP（地址解析协议）是一个在局域网内将 IP 地址映射到 MAC 地址的协议。ARP 协议用于查找设备的 MAC 地址，基于其已知的 IP 地址。该协议只在 局域网（LAN）中工作，因为它是基于广播的。\n当一个设备发送数据包到网络中的另一个设备时，它首先检查目标设备的 IP 地址。如果目标设备与源设备在同一个子网中，源设备会通过 ARP 查找目标设备的 MAC 地址，并将数据包发送给该设备的 MAC 地址。 如果目标设备的 MAC 地址已经缓存（通过先前的 ARP 请求获得），则直接使用该 MAC 地址发送数据包。 如果缓存没有目标设备的 MAC 地址，设备会通过广播 ARP 请求来查找该设备的 MAC 地址。 例子： # 假设设备 A 想向设备 B 发送数据：\n设备 A 知道设备 B 的 IP 地址，但不知道它的 MAC 地址。 设备 A 向网络广播 ARP 请求，询问“IP 地址为 192.168.1.10 的设备的 MAC 地址是什么？” 设备 B 收到 ARP 请求后，发送 ARP 响应，告知设备 A 它的 MAC 地址（例如：00:1A:2B:3C:4D:5E）。 设备 A 获取到 MAC 地址后，就可以直接向设备 B 的 MAC 地址发送数据包。 总结： # MAC 地址 和 IP 地址 是不同层次的地址，不能直接相互转换。 ARP 协议在局域网内用于根据 IP 地址查找设备的 MAC 地址，或根据 MAC 地址查找设备的 IP 地址。 每个设备会通过 ARP 协议自动进行这些转换，通常不需要人工干预。 常见的raid有哪些，使用场景是什么 # RAID（冗余磁盘阵列）是将多个物理硬盘驱动器（HDD）或固态硬盘（SSD）组合在一起，以提高存储性能、容量和可靠性。RAID 有不同的级别，每种级别都有不同的性能、冗余和成本权衡。以下是常见的 RAID 级别及其使用场景。\n常见的 RAID 级别 # 1. RAID 0（条带化） # 特点： 数据被分割成多个条带，分别写入不同的磁盘中，所有磁盘共同工作，提供更高的读写性能。 没有冗余机制。如果其中一个硬盘发生故障，所有数据都将丢失。 磁盘的容量等于所有硬盘容量的总和。 使用场景： 用于需要高性能、高吞吐量的场景，例如视频编辑、图像处理等，尤其是对数据冗余没有要求的情况下。 不适合存储重要数据，因为没有冗余保护。 优缺点： 优点：性能非常高，适合读写密集型工作负载。 缺点：没有冗余，单一硬盘故障会导致数据丢失。 2. RAID 1（镜像） # 特点： 数据被复制到两个或多个硬盘中，提供冗余。每个硬盘保存完全相同的数据副本。 具有高度的容错能力，如果其中一个硬盘故障，数据依然可以从另一个硬盘中恢复。 磁盘的总容量是最小硬盘的容量（即，两个 1TB 硬盘组成的 RAID 1 阵列总容量为 1TB）。 使用场景： 用于需要高数据可靠性和可用性的环境，例如小型办公室、文件存储服务器、数据库和关键应用。 适合不希望丢失数据的场景，例如个人数据备份、财务信息存储等。 优缺点： 优点：数据冗余高，硬盘故障不会导致数据丢失。 缺点：存储效率低，只有一半的磁盘空间可用，性能提升不明显。 3. RAID 5（带奇偶校验的条带化） # 特点： 数据和奇偶校验信息分布在所有硬盘上。奇偶校验是数据冗余的一种方式，能够在一个硬盘发生故障时，使用剩余硬盘和奇偶校验数据恢复丢失的数据。 至少需要 3 个硬盘。 存储效率相对较高，比 RAID 1 提供更多的存储空间。 使用场景： 用于要求数据冗余和性能平衡的场景，例如中型企业的文件服务器、数据库服务器、Web 服务器等。 适合大多数企业环境，可以容忍一个硬盘故障而不丢失数据。 优缺点： 优点：性能和冗余之间有较好平衡，存储效率较高，能够容忍单个硬盘故障。 缺点：写入性能较差，因为需要计算奇偶校验。 4. RAID 6（带双重奇偶校验的条带化） # 特点： 数据和双重奇偶校验信息分布在所有硬盘上，能够在两个硬盘同时发生故障时，依然能够恢复数据。 至少需要 4 个硬盘。 提供比 RAID 5 更高的容错能力，但存储效率较低。 使用场景： 用于极其重视数据可靠性和容错的环境，例如企业级存储、数据中心、云存储服务等。 适合要求高可用性和容错性的关键应用和数据存储。 优缺点： 优点：提供比 RAID 5 更强的冗余能力，能够容忍两个硬盘同时故障。 缺点：存储效率较低，写入性能较差，硬件要求较高。 5. RAID 10（RAID 1+RAID 0，镜像+条带化） # 特点： 结合了 RAID 1 和 RAID 0 的优点。首先将硬盘镜像（RAID 1），然后将镜像盘进行条带化（RAID 0）。 提供了高性能和高冗余，但需要至少 4 个硬盘。 数据被镜像并条带化，使其在提高性能的同时保留了冗余。 使用场景： 用于需要高性能和高数据可靠性的环境，例如数据库服务器、高流量的 Web 服务器等。 适合需要快速读写和容忍故障的关键任务应用。 优缺点： 优点：提供非常高的性能和数据冗余，能够容忍单个硬盘故障。 缺点：存储效率低，需要更多的硬盘，成本较高。 6. RAID 50（RAID 5 + RAID 0，带奇偶校验的条带化） # 特点： 将多个 RAID 5 阵列组合成一个 RAID 0 阵列，以获得更好的性能。提供 RAID 5 的冗余和 RAID 0 的性能。 至少需要 6 个硬盘。 提供比单独的 RAID 5 更高的性能，尤其在读写密集型应用中。 使用场景： 用于对性能和冗余都有较高要求的环境，如大规模数据存储、数据库存储等。 适合需要大存储容量并且需要在 RAID 5 性能基础上进行进一步优化的场景。 优缺点： 优点：提供较高的性能和冗余，适合需要更高吞吐量的场景。 缺点：需要更多的硬盘，复杂度和成本较高。 7. RAID 60（RAID 6 + RAID 0，带双重奇偶校验的条带化） # 特点： 将多个 RAID 6 阵列组合成一个 RAID 0 阵列，提供更高的性能和容错能力。能够容忍两个硬盘同时发生故障。 至少需要 8 个硬盘。 提供比 RAID 6 更高的性能，尤其在读写密集型场景下。 使用场景： 用于对冗余性和性能要求极高的环境，如大规模企业级存储、数据中心、高可用性应用等。 优缺点： 优点：提供非常高的容错能力和性能，能够容忍两个硬盘同时故障。 缺点：需要大量硬盘，存储效率较低，成本较高。 总结： # RAID 0：适用于需要高性能、低成本，但不要求数据冗余的场景。 RAID 1：适用于需要高数据可靠性，但对性能要求不高的环境。 RAID 5：适用于大多数企业级存储，提供良好的性能与冗余平衡，适合存储大量数据。 RAID 6：适用于要求非常高冗余的环境，能够容忍两个硬盘故障。 RAID 10：适用于需要高性能和高冗余的环境，特别是数据库和高流量应用。 RAID 50/60：适用于对性能和冗余要求较高的企业级存储，适合大规模的数据存储需求。 根据不同的应用场景，选择合适的 RAID 级别可以有效提升系统性能和数据保护能力。\nlvm怎么划分 # LVM（逻辑卷管理，Logical Volume Manager）是 Linux 中的一种逻辑存储管理方式，可以通过将物理卷（Physical Volumes, PV）组合成卷组（Volume Groups, VG），然后再从卷组中划分逻辑卷（Logical Volumes, LV），以实现更加灵活和动态的磁盘管理。\nLVM 的基本概念 # 物理卷（PV）： 物理卷是实际的存储设备或存储介质，例如硬盘分区、硬盘或 RAID 阵列。一个物理卷可以是一个完整的硬盘、一个分区或其他支持的存储设备。 卷组（VG）： 卷组是由一个或多个物理卷组成的存储池，提供了一组可用的空间来创建逻辑卷。卷组将多个物理卷的存储空间合并在一起，使得可以像一个大的磁盘一样来管理存储。 逻辑卷（LV）： 逻辑卷是从卷组中划分出来的虚拟磁盘，可以在逻辑卷中存储数据。逻辑卷可以动态调整大小，扩展或缩小，支持灵活的磁盘分区管理。 物理块（PE）： 物理卷上的数据是以固定大小的块（Physical Extents, PE）来划分的。一个物理卷上的 PE 数量决定了卷组中的可用空间。 逻辑块（LE）： 逻辑卷中的数据同样是以固定大小的块（Logical Extents, LE）来划分的。每个逻辑卷上的 LE 是与卷组中的 PE 对应的。 LVM 的常用操作 # 1. 创建物理卷（PV） # 物理卷是物理存储设备上的一个分区或整个磁盘，可以通过 pvcreate 命令来创建。\nsudo pvcreate /dev/sdb 这将 /dev/sdb 设备初始化为物理卷。\n2. 创建卷组（VG） # 卷组是由多个物理卷组成的，可以通过 vgcreate 命令来创建。\nsudo vgcreate my_vg /dev/sdb 这将创建一个名为 my_vg 的卷组，并将 /dev/sdb 添加到该卷组中。\n3. 创建逻辑卷（LV） # 逻辑卷是在卷组中创建的，类似于传统的分区，可以通过 lvcreate 命令来创建。\nsudo lvcreate -L 10G -n my_lv my_vg 这将创建一个大小为 10GB 的逻辑卷 my_lv，并将其放置在 my_vg 卷组中。\n4. 查看 LVM 配置 # 可以通过 vgs、lvs 和 pvs 命令查看卷组、逻辑卷和物理卷的详细信息。\n查看卷组：\nsudo vgs 查看逻辑卷：\nsudo lvs 查看物理卷：\nsudo pvs 5. 扩展逻辑卷（LV） # 如果逻辑卷空间不足，可以扩展逻辑卷的大小。可以通过 lvextend 命令增加逻辑卷的大小。\nsudo lvextend -L +5G /dev/my_vg/my_lv 这将 my_lv 逻辑卷的大小增加 5GB。\n扩展后，还需要扩展文件系统，使用 resize2fs（对于 ext 文件系统）或 xfs_growfs（对于 XFS 文件系统）来调整文件系统的大小：\n对于 ext 文件系统：\nsudo resize2fs /dev/my_vg/my_lv 对于 XFS 文件系统：\nsudo xfs_growfs /dev/my_vg/my_lv 6. 缩小逻辑卷（LV） # 如果需要减少逻辑卷的大小，首先需要先收缩文件系统，然后再缩小逻辑卷。\n步骤：\n收缩文件系统。\n对于 ext 文件系统：\nsudo resize2fs /dev/my_vg/my_lv 5G 对于 XFS 文件系统： XFS 不支持直接缩小大小，因此如果要缩小 XFS 文件系统，必须先备份数据，删除逻辑卷，然后再重新创建较小的逻辑卷。\n缩小逻辑卷：\nsudo lvreduce -L 5G /dev/my_vg/my_lv 7. 删除逻辑卷（LV） # 当逻辑卷不再需要时，可以通过 lvremove 命令删除逻辑卷。\nsudo lvremove /dev/my_vg/my_lv 8. 删除卷组（VG） # 在删除卷组之前，所有逻辑卷必须先被删除。然后可以使用 vgremove 命令删除卷组。\nsudo vgremove my_vg 9. 删除物理卷（PV） # 在删除物理卷之前，需要先将其从卷组中移除。然后可以使用 pvremove 命令删除物理卷。\nsudo pvremove /dev/sdb LVM 的优缺点 # 优点：\n灵活性高：LVM 支持动态调整磁盘分区大小，可以在系统运行时扩展或缩小逻辑卷。 易于管理：卷组和逻辑卷的管理比传统的分区方式更加方便，支持合并多个硬盘空间。 冗余支持：LVM 支持创建镜像（RAID1）或条带化（RAID0）卷，以实现数据冗余和性能优化。 支持快照：LVM 提供了快照功能，方便备份和恢复数据。 缺点：\n复杂性：LVM 管理较为复杂，初学者可能不太容易理解和配置。 性能开销：由于 LVM 进行逻辑层的抽象，可能会带来一定的性能开销，尤其是在较低规格的硬件上。 总结 # LVM 是一种非常灵活的磁盘管理方案，适用于动态存储需求的环境。通过物理卷、卷组和逻辑卷的组合，LVM 提供了比传统磁盘分区更高的灵活性和可扩展性。适合用于大规模存储、虚拟化环境或需要频繁调整存储容量的应用场景。\njvm内存如何查看 # 在 Java 程序运行时，JVM（Java Virtual Machine）会分配不同的内存区域来存储代码、数据、堆、栈等信息。了解 JVM 内存的使用情况有助于进行性能优化、内存调优以及排查内存泄漏等问题。\n要查看 JVM 内存的使用情况，可以使用多种工具和方法。下面列出了几种常用的查看 JVM 内存的方法。\n1. 使用 jps 命令查看 JVM 进程 # jps（Java Process Status）是一个简单的命令行工具，用于列出当前系统上运行的所有 Java 进程及其进程号（PID）。\njps 这将输出所有正在运行的 Java 进程的 ID 和名称。\n2. 使用 jvm 参数查看内存设置 # JVM 启动时，您可以使用一些启动参数来设置和查看内存配置：\n-Xms：设置 JVM 初始堆内存大小（单位：字节、KB、MB、GB）。 -Xmx：设置 JVM 最大堆内存大小（单位：字节、KB、MB、GB）。 -Xmn：设置新生代的大小（单位：字节、KB、MB、GB）。 -XX:MetaspaceSize：设置 Metaspace 初始大小。 -XX:MaxMetaspaceSize：设置 Metaspace 最大大小。 例如，要查看 JVM 的堆内存设置：\njava -Xms512m -Xmx2g -XX:MaxMetaspaceSize=256m -jar your-application.jar 这些参数在程序启动时传递，以便您可以根据需要进行调整。\n3. 使用 jstat 命令查看 JVM 内存统计 # jstat 是 Java 提供的一个监控工具，用于查看 JVM 运行时的各种统计信息，包括内存使用情况。最常见的命令是 jstat -gc，可以查看垃圾收集相关的内存信息。\n查看 JVM 的垃圾收集和内存使用情况：\njstat -gc \u0026lt;pid\u0026gt; 1000 解释：\n\u0026lt;pid\u0026gt;：JVM 进程的进程 ID。 1000：表示每隔 1000 毫秒（即 1 秒）输出一次统计信息。 jstat -gc 输出的字段包括：\nS0C：第一个 Survivor 区的内存大小。 S1C：第二个 Survivor 区的内存大小。 EC：Eden 区的内存大小。 OC：老年代的内存大小。 MC：Metaspace 区的内存大小。 YGC：垃圾回收发生的次数（Young GC）。 YGCT：Young GC 的总耗时。 FGC：垃圾回收发生的次数（Full GC）。 FGCT：Full GC 的总耗时。 4. 使用 jmap 命令查看堆内存信息 # jmap 是另一个用于查看 JVM 内存的工具，主要用于生成堆内存的快照。\njmap -heap \u0026lt;pid\u0026gt; 这将显示堆内存的使用情况，包括堆的初始大小、最大大小、当前大小、各个区域的使用情况（如 Eden 区、老年代、Survivor 区等）以及垃圾回收的相关统计信息。\n例如：\nHeap Configuration: MinHeapFreeRatio = 40 MaxHeapFreeRatio = 70 MaxHeapSize = 2147483648 (2048.0MB) NewSize = 268435456 (256.0MB) MaxNewSize = 268435456 (256.0MB) OldSize = 0 (0.0MB) NewRatio = 2 SurvivorRatio = 8 MetaspaceSize = 16777216 (16.0MB) MaxMetaspaceSize = 1073741824 (1024.0MB) Heap Usage: PS Young Generation: Eden Space: capacity = 33554432 (32.0MB) used = 33123456 (31.6MB) free = 431280 (0.4MB) 98.8% used From Space: capacity = 8388608 (8.0MB) used = 0 (0.0MB) free = 8388608 (8.0MB) 0.0% used To Space: capacity = 8388608 (8.0MB) used = 0 (0.0MB) free = 8388608 (8.0MB) 0.0% used PS Old Generation: capacity = 4294967296 (4096.0MB) used = 500000000 (476.8MB) free = 3794967296 (3600.2MB) 11.6% used 5. 使用 visualvm 或 jconsole 图形化工具 # 如果您更倾向于图形化界面，可以使用 VisualVM 或 JConsole 工具来查看 JVM 内存和垃圾回收等相关信息。\nVisualVM：是一个功能强大的监控工具，提供了内存、线程、垃圾收集、CPU 等的可视化图表。可以通过 jvisualvm 启动。 JConsole：是一个基于 Java 的监控工具，提供了 CPU、内存、线程和类加载等方面的信息。可以通过 jconsole 启动。 这些工具允许实时监控 JVM 的状态，查看堆内存的使用情况，以及生成堆转储（heap dump）文件用于分析内存泄漏等问题。\n6. 使用 -XX:+PrintGCDetails 参数查看垃圾回收日志 # 为了监控 JVM 垃圾回收的详细信息，您可以通过 JVM 启动参数启用垃圾回收日志。\njava -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:\u0026lt;path-to-log-file\u0026gt; -jar your-application.jar 这将打印出每次垃圾回收的详细信息，包括内存的使用情况、GC 类型、回收前后的内存量等。\n7. 查看系统层面的内存使用 # 除了 Java 自身的工具，还可以查看系统层面的内存使用情况。通过 top、htop 或 free 等工具可以查看系统上所有进程的内存使用情况，其中包括 JVM 占用的内存。\ntop：显示系统的内存使用情况和每个进程的内存占用。 htop：是 top 的增强版，提供更友好的交互界面。 free：显示系统内存的总量、已用、空闲和交换空间（swap）的情况。 例如：\ntop -p \u0026lt;pid\u0026gt; 这将只显示指定进程的内存使用情况。\n8. 查看 Metaspace 使用情况 # JVM 在 Java 8 及更高版本使用 Metaspace 来代替永久代（PermGen）。可以通过 -XX:MetaspaceSize 和 -XX:MaxMetaspaceSize 来配置 Metaspace 的大小。\n使用 jstat -gc 命令查看 Metaspace 的使用情况，或者在 JVM 启动时使用参数 -XX:+PrintGCDetails 来查看 Metaspace 的 GC 日志。\n总结 # 查看 JVM 内存使用情况有多种方法，包括使用命令行工具（如 jps、jstat、jmap）、图形化工具（如 VisualVM、JConsole）以及通过 JVM 启动参数来获取详细的内存、GC 日志信息。通过这些工具，您可以实时监控 JVM 的内存使用，进行性能调优，定位内存泄漏等问题。\n如何管理和优化内核参数 # 管理和优化 Linux 内核参数是确保系统性能、稳定性和安全性的关键操作，尤其是在高负载或特殊场景下（例如大规模并发、网络优化、大数据处理等）。Linux 内核的各项参数影响着系统的行为，包括内存管理、进程调度、文件系统、网络等方面。\n1. 查看当前内核参数 # 内核参数通常存储在 /proc/sys/ 目录中，或者通过 sysctl 命令进行管理。可以使用以下命令查看当前的内核参数：\n查看当前内核参数值：\nsysctl -a 这将列出所有当前的内核参数及其值。\n查看单个内核参数的值：\nsysctl \u0026lt;parameter\u0026gt; 例如，要查看最大文件描述符数：\nsysctl fs.file-max 直接查看 /proc/sys/ 下的内容：\ncat /proc/sys/net/core/rmem_max 2. 修改内核参数 # 可以通过两种方式来修改内核参数：临时修改和永久修改。\n临时修改内核参数 # 临时修改内核参数会在系统重启后失效。可以通过 sysctl 命令直接修改内核参数。\nsudo sysctl -w \u0026lt;parameter\u0026gt;=\u0026lt;value\u0026gt; 例如，修改最大文件描述符数：\nsudo sysctl -w fs.file-max=2097152 永久修改内核参数 # 要永久修改内核参数，需要将其添加到 /etc/sysctl.conf 文件中。每次系统启动时，sysctl 会自动读取该文件中的设置并应用。\n编辑 /etc/sysctl.conf 文件：\nsudo vi /etc/sysctl.conf 添加或修改需要的内核参数。例如，增加文件描述符的最大值：\nfs.file-max = 2097152 使更改生效：\nsudo sysctl -p 你也可以通过直接修改 /etc/sysctl.d/ 目录下的配置文件来管理内核参数。\n3. 常用内核参数及其优化 # a. 内存管理 # vm.swappiness：控制内存交换行为，0-100 范围内，值越小，系统越倾向于使用物理内存而不是交换分区。\nsysctl vm.swappiness=10 对于数据库、内存密集型应用，设置较低的值可以避免过早进行交换。\nvm.overcommit_memory：控制内核如何处理内存超分配。值为 0 表示内核允许超额分配，值为 1 表示总是允许超额分配，值为 2 表示不允许超额分配。\nsysctl vm.overcommit_memory=2 vm.dirty_ratio 和 vm.dirty_background_ratio：控制在写回磁盘之前，内存中可以保持的脏页（未同步到磁盘的页面）比例。\nsysctl vm.dirty_ratio=60 sysctl vm.dirty_background_ratio=10 b. 进程调度 # kernel.pid_max：设置系统允许的最大进程号，默认 32768。对高并发系统可进行适当调整。\nsysctl kernel.pid_max=65535 kernel.threads-max：设置系统允许的最大线程数。\nsysctl kernel.threads-max=1000000 c. 网络优化 # net.core.somaxconn：控制系统每个网络接口的最大连接队列长度。可以增加该值来提高处理高并发连接的能力。\nsysctl net.core.somaxconn=65535 net.ipv4.tcp_fin_timeout：控制 TCP 连接关闭的等待时间，减少连接的长时间保持状态。\nsysctl net.ipv4.tcp_fin_timeout=30 net.ipv4.tcp_rmem 和 net.ipv4.tcp_wmem：控制 TCP 接收和发送缓冲区大小。\nsysctl net.ipv4.tcp_rmem=\u0026#34;4096 87380 6291456\u0026#34; sysctl net.ipv4.tcp_wmem=\u0026#34;4096 65536 6291456\u0026#34; net.ipv4.ip_local_port_range：设置可用的本地端口范围，对于高并发服务，应该扩大端口范围。\nsysctl net.ipv4.ip_local_port_range=\u0026#34;1024 65535\u0026#34; d. 文件系统 # fs.file-max：控制整个系统允许的最大文件句柄数。可以通过增加该值来提高支持大并发文件操作的能力。\nsysctl fs.file-max=2097152 fs.inotify.max_user_watches：调整可以同时监视的文件数量，适用于需要大量监控文件变化的应用（如文件监控系统）。\nsysctl fs.inotify.max_user_watches=524288 e. 安全性设置 # kernel.randomize_va_space：启用或禁用地址空间布局随机化（ASLR），增强系统的安全性。\nsysctl kernel.randomize_va_space=2 net.ipv4.icmp_echo_ignore_all：如果你想要忽略 ICMP 回显请求（ping 请求），可以禁用 ICMP 响应。\nsysctl net.ipv4.icmp_echo_ignore_all=1 4. 监控内核参数 # sysctl -a：查看所有内核参数及其值。 vmstat：查看系统的内存、进程、IO 等相关统计信息。 iostat：查看系统的 CPU 使用情况和磁盘 I/O 性能。 netstat：查看网络连接状态、路由表等信息。 top / htop：实时查看系统资源使用情况，包括内存、CPU、负载等。 sar：收集、报告和保存系统活动信息，适合做长期性能监控。 5. 调整参数的策略 # 负载高的应用：如果有高并发的 web 服务器，调整网络相关的参数（如 somaxconn、tcp_fin_timeout、tcp_rmem 等）和文件句柄限制（file-max）。 内存密集型应用：可以调整 swappiness、vm.dirty_ratio、overcommit_memory 等内存管理参数。 系统启动优化：一些内核参数（如 pid_max）可以影响系统的最大进程数和线程数，适当调整可以提升系统的并发能力。 监控与调优：定期使用 sysctl、top、vmstat 等工具监控内核参数，根据实际负载调整参数。 6. 注意事项 # 内核参数调整需要小心：有些内核参数可能对系统的稳定性和安全性产生影响，修改前最好了解其具体作用。 合理使用内存和 CPU 资源：某些参数（如 swappiness）可以提高系统的响应速度，但可能会增加 I/O 操作的负担，因此需要根据实际应用负载来调整。 定期监控与调整：系统负载和性能需求可能会随时间变化，因此定期评估和调整内核参数非常重要。 通过合理地管理和优化内核参数，能够有效提高系统性能、稳定性并满足特定应用场景的需求。\n什么是进程最大数、最大线程数、进程打开的文件数，怎么调整☆ # 在 Linux 系统中，进程最大数、最大线程数和进程打开的文件数是与系统资源和性能密切相关的重要参数。理解这些参数并适时调整它们有助于提高系统的处理能力，尤其是在高负载或高并发的场景下。\n1. 进程最大数（max processes） # 进程最大数是指系统中允许的最大进程数。在 Linux 系统中，进程数受两个因素的限制：\n系统级别限制：即整个系统最多能够启动多少个进程。 用户级别限制：即单个用户最多能够启动多少个进程。 查看进程最大数： # 系统级别： 可以查看 /proc/sys/kernel/pid_max 来查看当前系统允许的最大进程号（即最大进程数）。\ncat /proc/sys/kernel/pid_max 用户级别： 使用 ulimit 命令查看当前用户的进程限制。\nulimit -u 调整进程最大数： # 临时调整：可以通过 sysctl 命令来调整系统级别的最大进程数。例如，修改最大进程号：\nsudo sysctl -w kernel.pid_max=65535 永久调整：修改 /etc/sysctl.conf 文件，添加或修改如下行：\nkernel.pid_max = 65535 然后使用以下命令使其生效：\nsudo sysctl -p 用户级别调整：通过修改 /etc/security/limits.conf 文件，可以设置每个用户的最大进程数。例如：\nusername soft nproc 4096 username hard nproc 8192 其中：\nsoft 是软限制，表示普通的进程数限制。 hard 是硬限制，表示无法再增加的进程数限制。 另外，修改 /etc/pam.d/common-session 文件，确保 pam_limits.so 被启用：\nsession required pam_limits.so 2. 最大线程数（max threads） # 线程是进程内的一个执行单元。一个进程可以拥有多个线程，这些线程共享进程的内存空间和资源。在 Linux 系统中，线程数通常与进程数一起受到限制。\n查看最大线程数： # 系统级别限制\n：在 Linux 系统中，线程数的最大限制与进程数相同，因为每个线程也会被当作进程来管理。使用\nulimit -a 命令查看当前的线程数限制：\nulimit -a 其中\nmax user processes 就是当前用户可创建的最大线程数。\n调整最大线程数： # 用户级别调整：同样通过修改 /etc/security/limits.conf 文件来调整线程数的限制。例如：\nusername soft nproc 4096 username hard nproc 8192 系统级别调整：如果需要允许系统创建更多的线程，可以增加 kernel.pid_max 参数的值，因为系统会为每个线程分配一个进程号（PID）。请参考前面的 \u0026ldquo;进程最大数\u0026rdquo; 部分来调整。\n3. 进程打开的文件数（open files） # 每个进程在运行时会打开多个文件，包括普通文件、套接字、管道等。操作系统为每个进程设置了一个 最大打开文件数 限制，防止某个进程占用过多的文件描述符资源。\n查看进程最大文件数： # 当前进程的最大文件数限制： 使用 ulimit -n 命令查看当前用户可以打开的最大文件数。\nulimit -n 系统级别限制： 系统有一个全局的最大打开文件数，可以通过查看 /proc/sys/fs/file-max 来查询：\ncat /proc/sys/fs/file-max 调整进程打开的文件数： # 临时调整：使用 ulimit 命令临时增加最大文件数。例如，将最大文件数设置为 65535：\nulimit -n 65535 永久调整：\n修改 /etc/security/limits.conf 文件，设置最大文件数：\nusername soft nofile 65535 username hard nofile 65535 soft 是软限制，表示当前会话可以达到的最大文件数。 hard 是硬限制，表示最大允许的文件数限制。 修改 /etc/sysctl.conf 文件，设置系统全局的最大文件数：\nfs.file-max = 2097152 然后使其生效：\nsudo sysctl -p 修改 /etc/pam.d/common-session 文件，确保 pam_limits.so 被启用：\nsession required pam_limits.so 总结 # 进程最大数：限制系统允许的进程总数，可以通过修改 /proc/sys/kernel/pid_max 来调整。用户级别的限制可以通过 ulimit -u 和 /etc/security/limits.conf 来调整。 最大线程数：线程数受进程最大数的影响，因此也可以通过调整 ulimit -u 或 /etc/security/limits.conf 来管理。需要根据系统资源合理设置。 进程打开的文件数：每个进程可以打开的文件数量是有限制的，可以通过 ulimit -n 和 /etc/security/limits.conf 来调整。系统级别的限制可以通过修改 /proc/sys/fs/file-max 来管理。 合理调整这些限制能够确保系统在高并发、高负载下的稳定性和性能，避免资源耗尽或过度竞争。\ndu和df统计不一致原因☆ # du（Disk Usage）和 df（Disk Free）是常用的磁盘空间使用情况统计工具，二者的统计结果可能存在差异，主要原因如下：\n1. 统计对象不同 # df：df 命令显示的是整个文件系统的磁盘使用情况。它基于文件系统的元数据（如文件系统的块大小）来统计磁盘的使用情况，显示的是文件系统的总容量、已用空间和可用空间。\ndf 统计的是 文件系统层级的空间使用情况，并且它关注的是每个挂载点的总空间，包括系统预留空间（如 root 用户保留的空间）。\ndu：du 命令显示的是 目录或文件的磁盘空间使用情况，它是通过遍历目录中的文件和子目录来计算空间的。du 统计的是文件和目录实际占用的磁盘空间，包括文件的实际内容，但不会计算文件系统级别的元数据（如目录的大小、inode 信息等）。\n2. 文件系统预留空间 # df 命令 会考虑文件系统的预留空间（reserved blocks），通常是 root 用户 用于防止文件系统完全填满，保持系统的稳定性。例如，Ext4 文件系统默认会保留 5% 的磁盘空间供 root 用户使用。df 会显示文件系统总容量、已用空间和可用空间，其中的可用空间是考虑了预留空间之后的剩余空间。 du 命令 计算的是实际占用的空间，不会考虑文件系统的保留空间。因此，du 显示的磁盘使用情况可能会比 df 显示的实际占用空间稍低，因为它不包括文件系统中为 root 保留的空间。 3. 软链接和挂载点 # du 会统计 软链接（symbolic links）指向的目标文件和目录的实际使用情况。它默认会将软链接指向的目标文件计入空间占用。这可能会导致 du 显示的空间比 df 更大，因为软链接会被重复计算。 df 只计算挂载点的空间，不会考虑挂载点内的其他文件系统。如果你有多个文件系统或挂载点（如挂载了 /home、/data 等），df 会分别显示它们的空间占用，而 du 只会显示当前目录下的空间使用情况，不会考虑其他挂载点。 4. 文件系统缓存 # df 统计的是磁盘的实际使用情况，不会受到文件系统缓存的影响。即使文件刚刚被删除，但其空间尚未被回收，df 可能会显示该文件仍占用空间。 du 只会统计已实际写入磁盘的数据，不会考虑缓存中的数据。如果有大量的数据被删除但文件系统没有立即回收空间，du 可能会显示文件占用的空间较少，而 df 显示的可用空间较少。 5. 挂载点重复 # 如果文件系统有多个挂载点，例如 /dev, /proc, /sys 等虚拟文件系统，这些文件系统通常并不占用实际磁盘空间。du 会忽略这些挂载点，而 df 可能会显示这些挂载点所占用的空间。\n6. 延迟同步（文件删除） # 当你删除文件时，df 显示的空间释放情况是基于文件系统的实际状态，但文件内容可能仍然保留在缓存中，直到真正写入磁盘（例如，缓存被刷新）。因此，du 可能会显示某些文件仍然存在，直到缓存清理，才会在 df 上反映出更准确的空闲空间。\n7. du 计算的单位和 df 的单位不同 # 有时候，du 和 df 的输出单位可能不同，导致看起来的空间占用差异。例如，df 默认显示的是 块（block）单位，而 du 默认显示的是 字节（byte） 或 千字节（KB）。如果不统一单位，可能会觉得统计不一致。\n8. NFS 或网络文件系统 # 如果系统中有挂载了 NFS 或其他网络文件系统，du 和 df 也可能会出现差异，特别是在高延迟或不稳定的网络环境下。df 可能会显示一个较为准确的空间统计，而 du 在网络文件系统中可能由于延迟或未更新的文件系统缓存，导致显示的空间使用不一致。\n9. 磁盘碎片 # 虽然现代文件系统通常能较好地处理碎片问题，但磁盘碎片还是可能会影响 df 和 du 的统计差异。df 显示的是逻辑块的使用情况，而 du 显示的是文件实际占用的空间。如果文件系统存在碎片，实际磁盘占用空间和文件的逻辑占用空间可能不一致。\n解决办法： # 确保使用相同的单位进行比较，例如，使用 df -h 和 du -h 显示为可读的格式（MB、GB）。 检查文件系统是否有保留空间（通过 df 命令查看）。 检查文件系统挂载点、软链接以及挂载点是否被重复计算。 使用 lsof 检查文件是否已被删除但仍由进程占用。 总结： # du 和 df 显示的磁盘使用情况不一致，通常是因为它们统计的方式不同、关注的对象不同，以及文件系统的特性（如缓存、预留空间、软链接等）。通过理解这些差异，可以更好地理解和利用这两个命令。\nbuffers与cached的区别☆ # 在 Linux 系统中，buffers 和 cached 都与内存的使用相关，它们表示不同类型的缓存，它们的作用和统计方式有所区别。理解这两者的区别有助于更好地理解系统内存的使用情况。\n1. buffers # buffers 是指 块设备缓存，即为磁盘（硬盘、SSD、USB 等）块设备上的数据分配的内存。它主要用于缓存 原始块设备的数据，例如用于存放磁盘块的元数据（如文件系统的结构、磁盘块的索引等）。 作用： buffers 主要用于优化磁盘 I/O 性能，尤其是文件系统和磁盘之间的数据传输。 它帮助减少对磁盘的直接访问，避免频繁的磁盘操作，从而提高文件系统的访问速度。 举例： 当你访问文件时，文件系统需要读取文件的磁盘块，如果这些块已经存在于 buffers 中，系统就可以直接从内存中获取，而不需要重新从磁盘读取。 2. cached # cached 是指 页面缓存，即文件内容缓存。cached 主要用于存储 文件内容，即文件系统中实际存储的文件数据。当系统读取文件内容时，操作系统会将这些数据存储在内存中，供后续访问使用，从而提高文件读取的性能。 作用： cached 主要缓存的是文件数据的内容，而不仅仅是文件系统元数据（如目录结构）。 它有助于加速后续对文件的读取操作。例如，当你多次读取同一个文件时，如果文件数据已经被缓存，那么系统可以直接从内存中读取，而不需要再次访问磁盘。 举例： 如果你打开一个文件，系统会将该文件的数据存储到 cached 中，下次访问该文件时，系统可以直接从内存中读取，而不需要重新访问磁盘。 3. buffers 与 cached 的区别 # 参数 含义 缓存对象 内存区域 buffers 块设备缓存 文件系统的元数据（如索引、目录结构） 磁盘块级别的数据缓存 cached 页面缓存 文件数据（文件的内容） 文件内容的内存缓存 4. 总结 # buffers：缓存的是磁盘的 元数据（如文件系统结构），优化文件系统的读写性能。 cached：缓存的是 文件内容（即文件的实际数据），加速对文件内容的重复读取。 5. 实际影响 # 在 free 命令或 top 命令的输出中，buffers 和 cached 都算作 “使用中的内存”，但它们并不会占用实际的磁盘空间，而是优化系统性能的一部分。 cached 的内存通常是最易释放的，当系统需要更多内存时，操作系统可以将 cached 中的数据释放并清空，重新分配内存给其他进程。 buffers 通常占用的内存也可以被释放，但它更多的是处理块设备层级的数据传输，涉及更底层的文件系统结构。 6. 释放缓存 # 如果系统内存非常紧张，你可以通过以下命令来释放缓存：\n# 清空 pagecache sudo sync; sudo echo 1 \u0026gt; /proc/sys/vm/drop_caches # 清空 dentries 和 inodes 缓存 sudo echo 2 \u0026gt; /proc/sys/vm/drop_caches # 清空 pagecache, dentries 和 inodes 缓存 sudo echo 3 \u0026gt; /proc/sys/vm/drop_caches 这些命令会释放缓存的内存，但会导致性能下降，因为系统将需要重新读取缓存中的数据。\nlsof命令使用场景 # lsof（List Open Files）是一个非常强大的工具，用于列出当前系统中所有打开的文件和正在使用它们的进程。由于 Linux 中的设备、文件、目录、网络连接等都被当作文件处理，lsof 可以用于排查多种问题。以下是 lsof 常见的使用场景：\n1. 查看某个进程打开的文件 # 如果你想查看某个进程打开了哪些文件，可以使用 lsof 配合进程ID（PID）。\nlsof -p \u0026lt;pid\u0026gt; 使用场景： # 排查某个进程是否打开了特定的文件。 查看进程是否存在文件句柄泄漏的现象。 2. 查看某个用户打开的文件 # 可以使用 lsof 查看某个用户打开的所有文件。\nlsof -u \u0026lt;username\u0026gt; 使用场景： # 监控某个用户的文件使用情况。 在需要查找某个用户正在访问的文件时，尤其是多用户系统中，帮助管理员跟踪活动。 3. 查看特定文件的打开情况 # 如果你想查看某个文件是否正在被进程占用，可以直接使用文件名。\nlsof \u0026lt;file_path\u0026gt; 使用场景： # 查找某个文件是否被其他进程占用，尤其在需要删除或修改文件时。例如，在删除日志文件之前，可能需要检查文件是否仍然被进程打开。 查找某个特定文件是否被锁定，是否有其他进程在访问它。 4. 查看某个端口被哪个进程占用 # lsof 也可以列出当前系统中所有的网络连接，并显示是哪个进程在监听特定端口。你可以使用 -i 参数指定特定端口或协议。\nlsof -i :\u0026lt;port_number\u0026gt; 例如，查看哪个进程占用了 80 端口：\nlsof -i :80 或者查看所有 TCP 连接：\nlsof -i tcp 使用场景： # 追踪网络端口的使用情况。 查找哪个进程占用了系统的某个端口，尤其在端口冲突的情况下非常有用。 检查服务是否正常监听对应的端口。 5. 查看文件系统的某个挂载点 # lsof 可以查看系统中某个挂载点的所有打开文件。例如，查看 /mnt/data 目录下的所有打开文件：\nlsof +D /mnt/data 使用场景： # 检查特定挂载点上的文件是否有进程正在访问，尤其在磁盘挂载、卸载前需要确保没有进程正在使用该目录。 调查文件系统挂载点的访问情况，避免数据丢失。 6. 查找删除的文件 # 在 Linux 系统中，如果文件被删除，但进程仍然打开着它，lsof 可以列出这些文件，即使文件已经从文件系统中删除，它仍然存在于进程的文件描述符中，直到进程关闭该文件。\nlsof | grep deleted 使用场景： # 调查删除文件后仍然占用磁盘空间的情况。某些进程可能持有已删除文件的文件描述符，导致磁盘空间无法释放。 检查进程是否仍然占用被删除的日志文件，防止磁盘空间被大量占用。 7. 查看文件锁 # lsof 可以显示由进程持有的文件锁。如果你怀疑某个文件或资源被锁定，可以通过以下命令查看：\nlsof -i | grep LOCK 使用场景： # 排查文件锁定问题，特别是在需要访问文件但遇到“文件已被锁定”错误时。 监控进程对文件的锁定操作，避免死锁或锁竞争问题。 8. 查看打开的网络文件（TCP/UDP） # lsof 可以列出当前系统中所有活动的网络连接，包括 TCP 和 UDP 连接。\nlsof -i 使用场景： # 诊断网络问题，查看哪些进程正在通过网络进行通信。 检查系统的开放网络端口和连接状态。 追踪进程是否在访问外部资源。 9. 查找进程占用的文件描述符 # 每个进程都有一个文件描述符的限制，使用 lsof 可以查看进程打开了多少个文件描述符：\nlsof -p \u0026lt;pid\u0026gt; | wc -l 使用场景： # 诊断文件描述符耗尽问题。Linux 系统中每个进程都有打开文件数的限制，如果达到最大文件描述符数限制，进程可能无法打开新的文件或套接字。 排查进程是否存在文件描述符泄漏。 10. 监控进程创建的临时文件 # lsof 可以帮助监控进程创建的临时文件，尤其在高并发系统中，进程可能会大量创建临时文件，但没有及时关闭，导致磁盘空间被占满。\nlsof +L1 使用场景： # 检查进程是否创建了大量临时文件，并没有及时清理，导致磁盘空间被占满。 总结 # lsof 是一个非常强大的工具，能够提供系统中文件和进程的详细信息，常见的使用场景包括：\n查找文件是否被占用。 排查文件锁、端口占用等问题。 调查被删除但未释放的文件。 监控进程的资源使用情况。 在系统运维、故障排查和性能优化过程中，lsof 是一个非常实用的工具，能帮助管理员了解系统文件和网络连接的详细使用情况。\nLinux中的进程间通信的方式及其使用场景 # 在 Linux 中，进程间通信（IPC，Inter-Process Communication）是指在不同的进程之间传递信息或同步操作的机制。进程间通信非常重要，尤其是在多进程并发执行的环境下，确保数据的传递与协调。Linux 提供了多种进程间通信方式，每种方式适用于不同的场景。\n常见的 Linux 进程间通信方式 # 管道（Pipe） 命名管道（Named Pipe 或 FIFO） 消息队列（Message Queue） 信号（Signal） 共享内存（Shared Memory） 信号量（Semaphore） 套接字（Socket） 内存映射（Memory Mapping） 1. 管道（Pipe） # 管道是最基本的进程间通信方式之一，它允许一个进程将数据写入管道，另一个进程从管道中读取数据。管道通常是匿名的，且只能在父子进程或兄弟进程之间使用。\n使用场景： # 父进程和子进程之间的数据传输。 命令行工具之间的组合使用（例如，ls | grep）。 优点： # 实现简单，通信速度较快。 缺点： # 只能用于同一机器上的进程间通信。 不能在无关进程间通信。 2. 命名管道（FIFO） # 命名管道是管道的一种改进形式，允许具有不同 PID 的进程通过一个指定的文件来进行通信。不同于匿名管道，命名管道在文件系统中有一个名字，进程可以通过这个名字访问管道。\n使用场景： # 不同的进程之间，通过文件系统进行通信。 需要多个进程共享数据时使用。 优点： # 可以在无父子关系的进程之间通信。 缺点： # 需要显式创建和管理文件，存在一定的复杂性。 3. 消息队列（Message Queue） # 消息队列是一种先进先出（FIFO）的消息传递机制，允许进程以消息的形式交换数据。消息队列中的消息有一个优先级，可以支持不同进程间异步通信。\n使用场景： # 多个进程之间的异步通信，尤其是需要保证消息顺序的场合。 任务调度、事件通知系统等。 优点： # 提供了进程间的异步通信机制。 支持消息优先级，消息传递不会丢失。 缺点： # 较复杂，管理和维护消息队列需要额外的开销。 4. 信号（Signal） # 信号是一种通知进程发生事件的机制，进程通过发送信号来通知另一个进程。信号通常用于处理异步事件或进程控制（如中断、终止等）。\n使用场景： # 进程之间的状态变化通知，例如 SIGTERM 用于优雅终止进程，SIGINT 用于中断进程。 异常事件的处理，比如处理定时器到期的信号。 优点： # 实现简单，常用于进程控制。 缺点： # 不能传输大量数据，只适合用于通知事件。 信号的处理非常依赖操作系统。 5. 共享内存（Shared Memory） # 共享内存是一种高效的进程间通信方式，多个进程可以访问同一块内存区域，从而共享数据。共享内存的大小可以很大，因此适合传输大量数据。\n使用场景： # 大量数据的高速传输。 多个进程之间的协作操作，例如数据库缓存、图形处理等。 优点： # 性能高，数据传输速度快。 可以共享大量数据。 缺点： # 需要进程之间协调访问（如使用信号量来进行同步）。 需要更复杂的内存管理。 6. 信号量（Semaphore） # 信号量是一种用于控制对共享资源的访问的机制。信号量通常与共享内存一起使用，用于同步进程的执行，防止多个进程同时访问共享资源。\n使用场景： # 需要对共享资源进行互斥访问时使用，例如避免多个进程同时写入共享文件。 实现生产者-消费者模型等。 优点： # 用于进程同步和互斥，能有效避免竞态条件。 缺点： # 需要额外的同步机制，管理上较为复杂。 7. 套接字（Socket） # 套接字提供了一种跨进程、跨主机的通信机制，广泛用于网络通信。套接字可以在同一台机器上的进程之间，也可以在不同机器之间进行通信。\n使用场景： # 网络通信：客户端与服务器之间的数据传输。 分布式系统、微服务之间的通信。 优点： # 支持跨主机、跨网络的通信。 支持面向连接（TCP）和无连接（UDP）的通信方式。 缺点： # 需要复杂的协议栈支持，可能导致一定的性能开销。 8. 内存映射（Memory Mapping） # 内存映射是一种通过将文件或设备的内容映射到进程的虚拟内存空间中的方式，使得进程能够直接操作内存中的数据，通常通过 mmap 系统调用实现。\n使用场景： # 共享数据、文件的映射访问。 需要快速访问大文件或多个进程共享的数据时。 优点： # 高效，支持大规模数据共享。 减少了文件 I/O 操作的开销。 缺点： # 对内存管理有较高要求，可能导致内存泄漏。 映射文件的修改需要同步处理。 总结： # 通信方式 适用场景 优缺点 管道（Pipe） 父子进程间的数据传输，命令行工具组合使用 简单、快速，但仅限父子进程和临时通信 命名管道（FIFO） 不同进程之间的数据传输 支持无亲缘关系进程通信，但需要显式创建和管理文件 消息队列（Message Queue） 多进程间的异步通信，需要传递带优先级的消息 支持优先级，消息不丢失，但管理较复杂 信号（Signal） 异步通知进程事件，如进程控制、异常事件 仅限通知，不适合大数据传输，处理依赖操作系统 共享内存（Shared Memory） 大量数据高速传输，进程间协作操作（如数据库缓存） 性能高，但需要同步机制，复杂的内存管理 信号量（Semaphore） 共享资源的同步与互斥访问，避免竞态条件 用于同步和互斥，管理复杂 套接字（Socket） 网络通信，跨进程、跨主机的数据传输 支持远程通信，但需要复杂的协议栈支持 内存映射（Memory Mapping） 大文件的高效访问，进程间共享数据 高效，减少 I/O 操作，但需要额外的内存管理 不同的进程间通信方式有不同的优势和适用场景，开发人员应根据实际需求选择合适的方式。\nLinux中的进程优先级与设置方法 # 在 Linux 中，进程的优先级决定了操作系统调度进程的顺序和分配 CPU 时间的方式。理解进程优先级的设置方法对于优化系统性能和调度效率至关重要。Linux 中有多种方式来设置和调整进程的优先级。\n1. 进程优先级的概念 # Linux 中的进程优先级由两个值来表示：\n静态优先级（Static Priority）：也叫 \u0026ldquo;nice\u0026rdquo; 值，表示进程的调度优先级，范围是 -20 到 19，其中 -20 表示最高优先级，19 表示最低优先级。默认情况下，进程的 nice 值为 0。 动态优先级（Dynamic Priority）：操作系统调度器使用的实际优先级，决定了哪个进程将获得 CPU 资源。动态优先级不仅依赖于静态优先级，还会受到进程的 CPU 时间消耗、调度策略等因素的影响。 2. 优先级与 nice 值 # Linux 中的 nice 值影响进程的优先级。更高的 nice 值意味着更低的优先级，反之，更低的 nice 值（甚至是负值）表示更高的优先级。\nnice 值的范围： # nice 值的范围是 -20 到 19，其中 -20 为最高优先级，19 为最低优先级。 默认情况下，进程的 nice 值为 0。 3. 查看进程的优先级 # 使用 ps 命令可以查看进程的 nice 值及其优先级（PID 和静态优先级）：\nps -eo pid,pri,ni,comm pid：进程 ID pri：动态优先级 ni：nice 值 comm：进程名 例如：\nps -eo pid,pri,ni,comm 输出可能类似：\nPID PRI NI COMMAND 1234 20 0 myprocess 4. 修改进程的 nice 值 # 使用 nice 或 renice 命令来修改进程的 nice 值，从而调整其优先级。\n(1) 使用 nice 启动新进程 # 可以使用 nice 命令启动一个新进程，并设置其 nice 值。nice 命令的语法如下：\nnice -n \u0026lt;nice_value\u0026gt; \u0026lt;command\u0026gt; 例如，要启动一个进程并将其 nice 值设置为 10（较低的优先级）：\nnice -n 10 myprocess (2) 使用 renice 调整已运行进程的 nice 值 # renice 命令用于修改已经在运行中的进程的 nice 值。语法如下：\nrenice -n \u0026lt;nice_value\u0026gt; -p \u0026lt;pid\u0026gt; 例如，要将 PID 为 1234 的进程的 nice 值调整为 -5（提高优先级）：\nrenice -n -5 -p 1234 注意：\n只有超级用户（root）可以将 nice 值设置为负数（即提高优先级）。 普通用户只能将 nice 值设置为正值（即降低优先级）。 5. 进程调度策略与优先级 # 除了 nice 值，Linux 还使用不同的调度策略来管理进程调度，进程的实际优先级（动态优先级）会受到调度策略的影响。Linux 支持几种调度策略，常见的有：\n(1) 实时调度策略 # SCHED_FIFO：先进先出（First In First Out），是最简单的实时调度策略，进程按照它们的到达顺序执行。 SCHED_RR：轮转调度（Round Robin），进程按时间片轮流执行，每个进程有固定的时间片。 (2) 普通调度策略 # SCHED_OTHER：常规调度策略，使用的是 Linux 默认的调度策略（CFS 调度器），一般用于非实时进程。 6. 查看进程调度策略 # 可以使用 chrt 命令来查看和设置进程的调度策略及优先级。chrt 命令可以查看实时进程的调度策略和优先级，并允许设置。\n(1) 查看进程调度策略 # chrt -p \u0026lt;pid\u0026gt; (2) 设置进程的调度策略 # 可以使用 chrt 命令设置实时进程的调度策略和优先级：\nchrt -f \u0026lt;priority\u0026gt; \u0026lt;pid\u0026gt; # 设置为 FIFO 策略 chrt -r \u0026lt;priority\u0026gt; \u0026lt;pid\u0026gt; # 设置为 RR 策略 其中，\u0026lt;priority\u0026gt; 是优先级数值，通常在 1 到 99 之间（优先级越小越高）。实时调度策略的优先级是独立的，普通调度策略的优先级由 nice 值决定。\n7. 优先级调整的使用场景 # 实时任务：如需要对某些任务（例如硬件控制、音频处理等）进行实时调度时，使用 SCHED_FIFO 或 SCHED_RR 策略和较高的优先级。 后台进程：对于不需要占用大量 CPU 时间的后台任务，可以设置较高的 nice 值，降低它们的优先级，避免影响系统响应。 计算密集型任务：对于高优先级的计算密集型任务，可以使用较低的 nice 值，确保它们获得更多的 CPU 时间。 8. 总结 # 在 Linux 系统中，进程的优先级由静态优先级（nice 值）和动态优先级（实际调度优先级）决定。我们可以通过以下方式调整进程优先级：\n使用 nice 启动新进程。 使用 renice 调整正在运行的进程的 nice 值。 通过调度策略（如 SCHED_FIFO、SCHED_RR）来调整实时进程的优先级。 通过合理设置进程的优先级，可以优化系统性能，确保重要任务得到足够的资源，同时避免不必要的资源竞争。\n什么是内存分页和分段 # 在 Linux 中，进程的优先级决定了操作系统调度进程的顺序和分配 CPU 时间的方式。理解进程优先级的设置方法对于优化系统性能和调度效率至关重要。Linux 中有多种方式来设置和调整进程的优先级。\n1. 进程优先级的概念 # Linux 中的进程优先级由两个值来表示：\n静态优先级（Static Priority）：也叫 \u0026ldquo;nice\u0026rdquo; 值，表示进程的调度优先级，范围是 -20 到 19，其中 -20 表示最高优先级，19 表示最低优先级。默认情况下，进程的 nice 值为 0。 动态优先级（Dynamic Priority）：操作系统调度器使用的实际优先级，决定了哪个进程将获得 CPU 资源。动态优先级不仅依赖于静态优先级，还会受到进程的 CPU 时间消耗、调度策略等因素的影响。 2. 优先级与 nice 值 # Linux 中的 nice 值影响进程的优先级。更高的 nice 值意味着更低的优先级，反之，更低的 nice 值（甚至是负值）表示更高的优先级。\nnice 值的范围： # nice 值的范围是 -20 到 19，其中 -20 为最高优先级，19 为最低优先级。 默认情况下，进程的 nice 值为 0。 3. 查看进程的优先级 # 使用 ps 命令可以查看进程的 nice 值及其优先级（PID 和静态优先级）：\nps -eo pid,pri,ni,comm pid：进程 ID pri：动态优先级 ni：nice 值 comm：进程名 例如：\nps -eo pid,pri,ni,comm 输出可能类似：\nPID PRI NI COMMAND 1234 20 0 myprocess 4. 修改进程的 nice 值 # 使用 nice 或 renice 命令来修改进程的 nice 值，从而调整其优先级。\n(1) 使用 nice 启动新进程 # 可以使用 nice 命令启动一个新进程，并设置其 nice 值。nice 命令的语法如下：\nnice -n \u0026lt;nice_value\u0026gt; \u0026lt;command\u0026gt; 例如，要启动一个进程并将其 nice 值设置为 10（较低的优先级）：\nnice -n 10 myprocess (2) 使用 renice 调整已运行进程的 nice 值 # renice 命令用于修改已经在运行中的进程的 nice 值。语法如下：\nrenice -n \u0026lt;nice_value\u0026gt; -p \u0026lt;pid\u0026gt; 例如，要将 PID 为 1234 的进程的 nice 值调整为 -5（提高优先级）：\nrenice -n -5 -p 1234 注意：\n只有超级用户（root）可以将 nice 值设置为负数（即提高优先级）。 普通用户只能将 nice 值设置为正值（即降低优先级）。 5. 进程调度策略与优先级 # 除了 nice 值，Linux 还使用不同的调度策略来管理进程调度，进程的实际优先级（动态优先级）会受到调度策略的影响。Linux 支持几种调度策略，常见的有：\n(1) 实时调度策略 # SCHED_FIFO：先进先出（First In First Out），是最简单的实时调度策略，进程按照它们的到达顺序执行。 SCHED_RR：轮转调度（Round Robin），进程按时间片轮流执行，每个进程有固定的时间片。 (2) 普通调度策略 # SCHED_OTHER：常规调度策略，使用的是 Linux 默认的调度策略（CFS 调度器），一般用于非实时进程。 6. 查看进程调度策略 # 可以使用 chrt 命令来查看和设置进程的调度策略及优先级。chrt 命令可以查看实时进程的调度策略和优先级，并允许设置。\n(1) 查看进程调度策略 # chrt -p \u0026lt;pid\u0026gt; (2) 设置进程的调度策略 # 可以使用 chrt 命令设置实时进程的调度策略和优先级：\nchrt -f \u0026lt;priority\u0026gt; \u0026lt;pid\u0026gt; # 设置为 FIFO 策略 chrt -r \u0026lt;priority\u0026gt; \u0026lt;pid\u0026gt; # 设置为 RR 策略 其中，\u0026lt;priority\u0026gt; 是优先级数值，通常在 1 到 99 之间（优先级越小越高）。实时调度策略的优先级是独立的，普通调度策略的优先级由 nice 值决定。\n7. 优先级调整的使用场景 # 实时任务：如需要对某些任务（例如硬件控制、音频处理等）进行实时调度时，使用 SCHED_FIFO 或 SCHED_RR 策略和较高的优先级。 后台进程：对于不需要占用大量 CPU 时间的后台任务，可以设置较高的 nice 值，降低它们的优先级，避免影响系统响应。 计算密集型任务：对于高优先级的计算密集型任务，可以使用较低的 nice 值，确保它们获得更多的 CPU 时间。 8. 总结 # 在 Linux 系统中，进程的优先级由静态优先级（nice 值）和动态优先级（实际调度优先级）决定。我们可以通过以下方式调整进程优先级：\n使用 nice 启动新进程。 使用 renice 调整正在运行的进程的 nice 值。 通过调度策略（如 SCHED_FIFO、SCHED_RR）来调整实时进程的优先级。 通过合理设置进程的优先级，可以优化系统性能，确保重要任务得到足够的资源，同时避免不必要的资源竞争。\n如何创建和管理自定义systemd服务 # 在 Linux 系统中，systemd 是现代 Linux 系统的初始化系统和服务管理器，广泛用于启动、停止和管理服务。通过创建和管理自定义 systemd 服务，可以让你以统一的方式控制和监控应用程序或脚本的执行。\n1. 创建自定义 systemd 服务 # (1) 创建服务单元文件 # 一个 systemd 服务是通过创建一个服务单元文件来定义的。服务单元文件通常位于 /etc/systemd/system/ 目录下，文件名以 .service 结尾。\n例如，假设我们需要创建一个名为 my-service 的服务，我们需要创建一个名为 my-service.service 的文件。\n步骤：\n使用文本编辑器创建服务文件。例如：\nsudo nano /etc/systemd/system/my-service.service 定义服务单元文件内容。一个基本的 .service 文件可以包含以下内容：\n[Unit] Description=My Custom Service After=network.target [Service] ExecStart=/path/to/your/command_or_script Restart=on-failure User=myuser Group=mygroup Environment=\u0026#34;ENV_VAR=value\u0026#34; [Install] WantedBy=multi-user.target 解释：\n[Unit] 部分：定义了服务的描述和依赖关系。After=network.target 表示该服务会在网络服务启动之后执行。 [Service] 部分：指定服务启动的命令。ExecStart 定义了启动服务时执行的命令或脚本路径。Restart=on-failure 表示服务失败时会自动重启。User 和 Group 指定了运行服务的用户和组。Environment 设置了环境变量。 [Install] 部分：定义了服务安装时的目标（即在哪个 runlevel 启动）。multi-user.target 通常表示多用户模式，是大多数服务器的默认目标。 (2) 保存并退出编辑器 # 完成文件编辑后，保存并关闭编辑器（在 nano 中按 Ctrl + X，然后按 Y 和 Enter）。\n(3) 重新加载 systemd 配置 # 在创建服务单元文件之后，需要重新加载 systemd 配置，以使其识别新的服务。\nsudo systemctl daemon-reload (4) 启动并启用服务 # 启动服务并让其开机自动启动：\nsudo systemctl start my-service.service # 启动服务 sudo systemctl enable my-service.service # 设置为开机自动启动 (5) 检查服务状态 # 可以使用 systemctl 命令检查服务的状态、日志等信息：\nsudo systemctl status my-service.service # 查看服务状态 2. 管理自定义 systemd 服务 # (1) 启动和停止服务 # 启动服务：\nsudo systemctl start my-service.service 停止服务：\nsudo systemctl stop my-service.service 重启服务：\nsudo systemctl restart my-service.service (2) 查看服务状态 # 查看服务的当前状态（是否正在运行、是否存在错误等）：\nsudo systemctl status my-service.service 输出示例：\n● my-service.service - My Custom Service Loaded: loaded (/etc/systemd/system/my-service.service; enabled; vendor preset: enabled) Active: active (running) since Fri 2025-02-25 12:34:56 UTC; 1h 30min ago Main PID: 1234 (your_process) Tasks: 1 (limit: 4915) Memory: 1.0M CGroup: /system.slice/my-service.service └─1234 /path/to/your/command_or_script (3) 查看服务日志 # systemd 使用 journal 来记录日志。查看服务的日志：\nsudo journalctl -u my-service.service 如果需要实时查看日志输出，可以加上 -f 选项：\nsudo journalctl -u my-service.service -f (4) 禁用和删除服务 # 禁用服务（不再开机启动）：\nsudo systemctl disable my-service.service 删除服务单元文件：\n如果你不再需要此服务，可以删除其单元文件：\nsudo rm /etc/systemd/system/my-service.service 删除后，也需要重新加载 systemd 配置：\nsudo systemctl daemon-reload (5) 修改服务 # 如果需要修改服务的配置，只需要编辑相应的 .service 文件，修改完成后，重新加载 systemd 配置：\nsudo systemctl daemon-reload 然后重新启动服务使其生效：\nsudo systemctl restart my-service.service 3. 高级配置 # (1) 设置服务的资源限制 # 在 [Service] 部分，你可以设置服务的资源限制，例如 CPU、内存、文件句柄等：\n[Service] LimitCPU=50000 # 限制 CPU 使用，单位：毫秒 LimitMEMLOCK=10000 # 限制内存锁定大小 LimitNOFILE=65536 # 限制打开文件数量 (2) 设置服务的定时重启 # 你可以设置服务在某些条件下自动重启：\n[Service] Restart=always # 总是重启 RestartSec=10 # 重启间隔 10 秒 (3) 使用 ExecStop 指定停止命令 # 你可以指定停止服务时要执行的命令。例如，如果是脚本或程序，可能需要指定停止命令来清理资源：\n[Service] ExecStop=/path/to/stop_script.sh (4) 使用 After 和 Before 控制服务的启动顺序 # 你可以通过 After 和 Before 配置项控制服务的启动顺序。例如，如果你的服务依赖于网络，你可以在 [Unit] 部分添加：\n[Unit] After=network.target 4. 总结 # 创建自定义 systemd 服务时，首先需要定义 .service 文件，指定服务的启动命令、资源限制等信息。 使用 systemctl 命令来启动、停止、重启和查看服务状态。 可以使用 journalctl 查看服务日志。 服务单元文件可以按需进行修改，修改后使用 systemctl daemon-reload 更新 systemd 配置。 systemd 服务为你的应用提供了一种简洁而强大的管理方式，不仅适用于常见的后台服务，也可以用于定时任务、脚本执行等场景。\nLinux内核模块的加载与卸载过程 # 在 Linux 系统中，内核模块（Kernel Module）是一个能够在运行时被加载和卸载的二进制文件，它们提供了内核的扩展功能，比如硬件驱动、文件系统支持等。通过加载和卸载内核模块，可以动态地修改内核的功能，而不需要重新启动系统。下面是内核模块的加载和卸载过程的详细介绍：\n1. 内核模块的加载过程 # (1) 加载内核模块的命令 # insmod：用于加载内核模块。此命令直接加载模块，模块文件必须指定完整路径，且不会自动处理依赖。\nsudo insmod /path/to/module.ko modprobe：用于加载内核模块。相比 insmod，modprobe 会自动处理模块依赖关系，推荐使用这个命令来加载模块。\nsudo modprobe module_name module_name 是模块的名字，不需要包含 .ko 扩展名，modprobe 会自动查找模块文件。\nlsmod：用于查看当前已加载的所有内核模块。\nlsmod (2) 加载内核模块的过程 # 检查依赖关系：\n当执行 modprobe 命令加载一个模块时，系统会首先检查模块的依赖关系。例如，某些模块可能依赖于其他模块。 modprobe 会自动加载所需的依赖模块。如果是通过 insmod 命令手动加载模块，则需要手动加载所有的依赖模块。 模块文件路径：\nmodprobe 会在 /lib/modules/$(uname -r)/ 目录下查找内核模块文件，uname -r 获取当前系统的内核版本。模块文件的后缀通常是 .ko。 加载模块到内核空间：\n内核模块通过 insmod 或 modprobe 命令被加载到内核中。内核会解析模块文件的 ELF 格式，进行符号解析，初始化模块的入口函数。 如果模块初始化成功，它将注册自己的功能到内核，例如注册硬件驱动、文件系统类型、系统调用等。 模块加载的检查：\n可以使用 dmesg 命令查看加载模块时产生的内核日志，检查模块加载的详细信息。 dmesg | tail 模块文件的缓存：\n模块一旦加载到内核中，它们将驻留在内存中，并通过 lsmod 命令显示为已加载的模块。系统会保持模块的状态，直到卸载或系统重启。 (3) 查看加载的模块 # 使用 lsmod 查看已加载的内核模块：\nlsmod 该命令会列出所有当前已加载的模块、模块大小以及模块使用情况等信息。\n(4) 动态加载模块 # 某些模块可以配置为在特定的事件发生时自动加载。例如，设备插入时自动加载相应的驱动模块，这通过 udev 机制实现。你可以使用 /etc/modules 或 /etc/modprobe.d/ 下的配置文件，指定模块的自动加载。\n2. 内核模块的卸载过程 # (1) 卸载内核模块的命令 # rmmod：用于卸载已加载的内核模块，卸载时会检查该模块是否存在其他依赖模块。如果该模块是被其他模块所依赖，则无法卸载。\nsudo rmmod module_name modprobe -r：用于卸载内核模块，modprobe -r 会自动处理依赖关系，卸载时会自动卸载依赖该模块的其他模块。\nsudo modprobe -r module_name (2) 卸载内核模块的过程 # 检查模块是否被使用：\n如果模块正在被使用（例如硬件设备驱动正在驱动设备），则无法直接卸载。此时需要先停止相关的服务或卸载相关依赖模块。 卸载模块：\n执行 rmmod 或 modprobe -r 命令卸载模块。卸载模块时，内核会释放该模块占用的资源，并注销它的功能。 依赖关系处理：\n如果有其他模块依赖于该模块，modprobe -r 会先卸载依赖模块，再卸载目标模块。而 rmmod 会检查是否有其他模块依赖于目标模块，如果有依赖，则无法卸载。 检查内核日志：\n卸载模块的过程会在内核日志中生成相应的记录，可以通过 dmesg 查看卸载过程中的日志信息。 dmesg | tail 模块缓存：\n模块卸载后，它会从内核中释放，内核空间中的相应资源将被回收。如果该模块有注册的功能（如设备驱动），这些功能将不再可用。 (3) 查看当前加载的模块 # 使用 lsmod 查看当前加载的所有模块，确认模块是否已成功卸载。\n3. 内核模块管理的其他相关命令 # 查看模块的详细信息：\n使用 modinfo 命令查看某个模块的详细信息，如版本、依赖关系等。\nmodinfo module_name 例如：\nmodinfo e1000 查找模块文件位置：\n使用 modprobe 的 -n 选项来显示模块的加载路径，但不实际加载模块。\nmodprobe -n -v module_name 查看模块的依赖关系：\n使用 lsmod 和 modinfo 可以查看模块的依赖关系。\n4. 自动加载模块 # 内核模块可以配置为在系统启动时或设备连接时自动加载。这通常通过配置 /etc/modules 或在 /etc/modprobe.d/ 目录下创建配置文件来完成。你也可以在 initramfs 配置中设置内核模块的加载。\n5. 总结 # 加载模块：使用 insmod 或 modprobe 命令加载内核模块，modprobe 自动处理依赖关系。 卸载模块：使用 rmmod 或 modprobe -r 卸载模块，modprobe -r 会自动处理依赖关系。 查看已加载模块：使用 lsmod 查看当前已加载的模块，使用 dmesg 查看内核日志。 内核模块管理：通过 modinfo 查看模块信息，modprobe -n 查看模块路径等。 内核模块的动态加载和卸载为 Linux 系统提供了极大的灵活性，允许你在不重启系统的情况下为系统添加或移除功能。\nansible roles使用场景，现在有多台机器需要批量加入k8s集群，怎么实现☆ # Ansible Roles 是一种将配置文件、任务、变量等分组的方式，使得管理更为模块化和可重用。在大规模部署和配置环境时，使用 Ansible Roles 可以提高代码的可维护性和复用性。\nAnsible Roles 使用场景 # 模块化管理：当你的任务比较复杂时，可以将其拆分成多个独立的角色，每个角色管理一个特定的功能或模块。例如，管理数据库、Web 服务器、Kubernetes 集群等。 提高重用性：角色的代码结构独立，便于重用。例如，一个 Kubernetes 角色可以在不同环境中重复使用，只需要传入不同的变量。 简化配置和管理：通过角色结构，你可以避免重复配置，并且可以专注于某个模块的功能。 Ansible Roles 目录结构 # 假设你要创建一个名为 k8s-cluster 的角色，安装并配置 Kubernetes，角色的目录结构可能如下：\nk8s-cluster/ ├── defaults/ │ └── main.yml # 默认变量 ├── files/ │ └── ... # 需要拷贝的文件 ├── handlers/ │ └── main.yml # 处理器，例如服务重启 ├── meta/ │ └── main.yml # 角色的元数据 ├── tasks/ │ └── main.yml # 任务定义 ├── templates/ │ └── ... # Jinja2 模板文件 ├── vars/ │ └── main.yml # 变量文件 如何用 Ansible 批量将多台机器加入 Kubernetes 集群 # 假设有多台机器，你可以使用 Ansible 批量将这些机器加入 Kubernetes 集群。以下是使用 Ansible Roles 来实现这一目标的基本步骤：\n1. 创建 Kubernetes 角色 # 在 k8s-cluster/ 目录下，编写以下内容：\ndefaults/main.yml（定义默认变量）\n# k8s-cluster/defaults/main.yml kube_version: \u0026#34;1.24.0\u0026#34; api_server: \u0026#34;https://your-k8s-master-ip:6443\u0026#34; kubeconfig_path: \u0026#34;/etc/kubernetes/kubelet.conf\u0026#34; tasks/main.yml（执行任务）\n# k8s-cluster/tasks/main.yml - name: Install Docker package: name: docker state: present - name: Add Kubernetes repositories shell: | curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - echo \u0026#34;deb https://apt.kubernetes.io/ kubernetes-xenial main\u0026#34; \u0026gt; /etc/apt/sources.list.d/kubernetes.list - name: Install kubelet, kubeadm, and kubectl apt: name: \u0026#34;{{ item }}\u0026#34; state: present update_cache: yes loop: - kubelet={{ kube_version }} - kubeadm={{ kube_version }} - kubectl={{ kube_version }} - name: Initialize Kubernetes Cluster (for master node) command: kubeadm init --apiserver-advertise-address={{ ansible_host }} --pod-network-cidr=192.168.0.0/16 when: inventory_hostname == groups[\u0026#39;master\u0026#39;][0] - name: Join Kubernetes Cluster (for worker nodes) command: kubeadm join {{ api_server }} --token {{ join_token }} --discovery-token-ca-cert-hash {{ discovery_token_hash }} when: inventory_hostname != groups[\u0026#39;master\u0026#39;][0] - name: Configure kubectl for non-root user copy: src: \u0026#34;/etc/kubernetes/admin.conf\u0026#34; dest: \u0026#34;{{ kubeconfig_path }}\u0026#34; mode: \u0026#34;0644\u0026#34; when: inventory_hostname == groups[\u0026#39;master\u0026#39;][0] - name: Install Calico network plugin shell: kubectl apply -f https://docs.projectcalico.org/v3.19/manifests/calico.yaml when: inventory_hostname == groups[\u0026#39;master\u0026#39;][0] 2. 主机清单（Inventory）文件 # 创建一个 Ansible 主机清单文件 hosts.ini，列出所有的 Kubernetes Master 和 Worker 节点。\n[master] master1 ansible_host=master1_ip [worker] worker1 ansible_host=worker1_ip worker2 ansible_host=worker2_ip 3. 创建 Playbook # 编写一个 Playbook 来使用 k8s-cluster 角色来配置节点：\n--- - name: Configure Kubernetes Cluster hosts: all become: true roles: - k8s-cluster 4. 运行 Playbook # 使用 Ansible 执行 Playbook 来将多台机器加入到 Kubernetes 集群中：\nansible-playbook -i hosts.ini k8s-cluster-setup.yml 这个命令会根据 hosts.ini 中定义的主机信息，批量配置每个节点，并按照定义的角色来安装 Kubernetes 和网络插件。\n总结 # Ansible Roles 使得管理大规模集群和复杂配置变得更简单，可以将不同的配置和功能模块化，便于管理。 批量加入 Kubernetes 集群：通过使用 Ansible，你可以在多个节点上执行 kubeadm init 和 kubeadm join 命令来批量初始化或加入 Kubernetes 集群。 自动化运维：使用 Ansible Playbooks 和 Roles 来实现 Kubernetes 集群的自动化管理，降低手动配置的复杂度，提升运维效率。 这样，你就可以使用 Ansible 角色来高效地管理 Kubernetes 集群的节点，批量配置并将机器加入集群。\n了解了！如果你需要包括 Docker 和 kube-proxy 的分发步骤，并且要将它们配置成 systemd 服务来管理，下面是一个完整的解决方案。\n1. 准备角色和任务 # (1) 角色目录结构 # k8s-deploy/ ├── defaults/ │ └── main.yml # 默认变量文件 ├── files/ │ ├── kubelet # 二进制文件 │ ├── kubectl # 二进制文件 │ ├── kubeconfig # kubeconfig 文件（包含集群的配置信息） │ ├── kubelet.crt # kubelet 证书 │ ├── kubelet.key # kubelet 私钥 │ ├── kube-proxy # kube-proxy 二进制文件 │ ├── docker-ce # Docker 二进制文件（如果使用的是自定义 Docker 版本） │ ├── etc... # 其他配置文件 ├── tasks/ │ └── main.yml # 安装和分发文件的任务 ├── handlers/ │ └── main.yml # 处理器（如重启服务等） └── vars/ └── main.yml # 变量文件（如文件路径、Kubernetes 版本等） (2) 默认变量文件（defaults/main.yml） # # k8s-deploy/defaults/main.yml k8s_bin_dir: \u0026#34;/usr/local/bin\u0026#34; docker_version: \u0026#34;20.10.8\u0026#34; docker_package: \u0026#34;docker.io\u0026#34; docker_service: \u0026#34;docker\u0026#34; kubeconfig_path: \u0026#34;/etc/kubernetes/kubelet.conf\u0026#34; kube_proxy_bin: \u0026#34;/usr/local/bin/kube-proxy\u0026#34; (3) 任务文件（tasks/main.yml） # 这部分包括 Docker、kube-proxy、kubelet 的安装和配置，及其作为 systemd 服务进行管理。\n# k8s-deploy/tasks/main.yml # 1. 安装 Docker - name: Install Docker package: name: \u0026#34;{{ docker_package }}\u0026#34; state: present - name: Start and enable Docker service systemd: name: \u0026#34;{{ docker_service }}\u0026#34; state: started enabled: true # 2. 分发 Kubernetes 二进制文件 - name: Distribute kubelet binary copy: src: \u0026#34;files/kubelet\u0026#34; dest: \u0026#34;{{ k8s_bin_dir }}/kubelet\u0026#34; mode: \u0026#39;0755\u0026#39; - name: Distribute kubectl binary copy: src: \u0026#34;files/kubectl\u0026#34; dest: \u0026#34;{{ k8s_bin_dir }}/kubectl\u0026#34; mode: \u0026#39;0755\u0026#39; - name: Distribute kube-proxy binary copy: src: \u0026#34;files/kube-proxy\u0026#34; dest: \u0026#34;{{ k8s_bin_dir }}/kube-proxy\u0026#34; mode: \u0026#39;0755\u0026#39; # 3. 分发证书和配置文件 - name: Distribute kubelet certificate copy: src: \u0026#34;files/kubelet.crt\u0026#34; dest: \u0026#34;/etc/kubernetes/kubelet.crt\u0026#34; mode: \u0026#39;0644\u0026#39; - name: Distribute kubelet private key copy: src: \u0026#34;files/kubelet.key\u0026#34; dest: \u0026#34;/etc/kubernetes/kubelet.key\u0026#34; mode: \u0026#39;0600\u0026#39; - name: Distribute kubeconfig file copy: src: \u0026#34;files/kubeconfig\u0026#34; dest: \u0026#34;{{ kubeconfig_path }}\u0026#34; mode: \u0026#39;0644\u0026#39; # 4. 分发 Docker 配置文件（如果有自定义配置） - name: Distribute Docker configuration file (if needed) copy: src: \u0026#34;files/docker_config.json\u0026#34; dest: \u0026#34;/etc/docker/daemon.json\u0026#34; mode: \u0026#39;0644\u0026#39; when: docker_config is defined # 5. 配置并启动 kubelet 服务 - name: Enable and start kubelet service systemd: name: kubelet enabled: true state: started daemon_reload: yes # 6. 配置并启动 kube-proxy 服务 - name: Enable and start kube-proxy service systemd: name: kube-proxy enabled: true state: started daemon_reload: yes (4) 处理器（handlers/main.yml） # # k8s-deploy/handlers/main.yml - name: Restart kubelet systemd: name: kubelet state: restarted daemon_reload: yes - name: Restart kube-proxy systemd: name: kube-proxy state: restarted daemon_reload: yes 2. systemd 服务配置文件 # 为了确保 kubelet 和 kube-proxy 可以作为 systemd 服务管理，你需要为它们创建 systemd 服务文件。这些服务文件将被复制到 /etc/systemd/system/ 目录，并在必要时启动。\n(1) kubelet.service 配置 # 创建 kubelet.service 文件：\n# /etc/systemd/system/kubelet.service [Unit] Description=kubelet: The Kubernetes Node Agent Documentation=https://kubernetes.io/docs/ After=network.target [Service] ExecStart=/usr/local/bin/kubelet --kubeconfig=/etc/kubernetes/kubelet.conf --config=/etc/kubernetes/kubelet.config.yaml Restart=always RestartSec=10 LimitNOFILE=65536 TimeoutStartSec=0 [Install] WantedBy=multi-user.target (2) kube-proxy.service 配置 # 创建 kube-proxy.service 文件：\n# /etc/systemd/system/kube-proxy.service [Unit] Description=kube-proxy: The Kubernetes Network Proxy Documentation=https://kubernetes.io/docs/ After=network.target [Service] ExecStart=/usr/local/bin/kube-proxy --kubeconfig=/etc/kubernetes/kubelet.conf Restart=always RestartSec=10 LimitNOFILE=65536 TimeoutStartSec=0 [Install] WantedBy=multi-user.target 3. 主机清单文件 # 确保在 hosts.ini 文件中列出 Master 和 Worker 节点：\n[master] master1 ansible_host=master1_ip [worker] worker1 ansible_host=worker1_ip worker2 ansible_host=worker2_ip 4. Playbook 配置 # 编写 Playbook 来使用 k8s-deploy 角色来安装和配置 Kubernetes。\n--- - name: Install and Configure Kubernetes on all nodes hosts: all become: true roles: - k8s-deploy 5. 执行 Playbook # 执行 Playbook 将 Kubernetes 配置文件、证书以及二进制文件分发到所有节点，并启动 kubelet 和 kube-proxy 服务：\nansible-playbook -i hosts.ini k8s-deploy.yml 6. 总结 # 分发二进制文件：通过 copy 模块将 kubelet、kubectl 和 kube-proxy 等二进制文件分发到每台机器。 分发证书和配置文件：将 kubelet 证书、kubeconfig 配置文件等分发到目标节点，确保它们能够通过 kubeconfig 自动连接到集群。 安装和配置 Docker：通过 package 模块安装 Docker，并根据需要配置 Docker。 systemd 服务管理：为 kubelet 和 kube-proxy 创建 systemd 服务文件，并通过 systemd 来管理它们的启动、停止、重启等。 启动服务：通过 systemd 启动 kubelet 和 kube-proxy 服务，使它们自动连接到 Kubernetes 集群。 通过这种方式，你可以确保所有的 Kubernetes 组件（如 kubelet, kube-proxy）以及 Docker 服务都能通过 systemd 来进行管理，提供高效的自动化部署过程。\nDocker 二进制文件主要包括以下几个核心组件，它们一起协作完成 Docker 容器的管理和运行：\n1. dockerd (Docker Daemon) # 作用：dockerd 是 Docker 守护进程，它负责管理 Docker 容器的生命周期，监听 Docker API 请求，创建、启动、停止容器等操作。 路径：通常安装在 /usr/local/bin/dockerd 或 /usr/bin/dockerd。 2. docker (Docker CLI) # 作用：docker 是 Docker 客户端命令行工具，用户通过该命令行工具与 Docker 守护进程进行交互。它提供了常见的 Docker 命令，如 docker run、docker ps、docker build 等。 路径：通常安装在 /usr/local/bin/docker 或 /usr/bin/docker。 3. containerd (Container Runtime) # 作用：containerd 是容器运行时，Docker 使用它来执行容器的创建、调度、运行和监控。它是 Docker 核心的一个组件，负责容器的生命周期管理。 路径：通常安装在 /usr/local/bin/containerd 或 /usr/bin/containerd。 4. runc (Low-level Container Runtime) # 作用：runc 是一个低级别的容器运行时，负责容器的创建、启动、停止等操作。它由 Docker 使用，并且也是 Kubernetes 采用的容器运行时之一（通过 containerd 或直接使用）。 路径：通常安装在 /usr/local/bin/runc 或 /usr/bin/runc。 5. ctr (Containerd CLI) # 作用：ctr 是 containerd 的命令行工具，主要用于与 containerd 交互，执行容器的调试、管理等操作。它比 Docker 命令行工具更底层，通常用于调试和测试。 路径：通常安装在 /usr/local/bin/ctr。 6. dockerd 配置文件和插件 # 插件：Docker 的功能通过插件机制进行扩展，插件包括网络插件、存储插件、日志插件等。它们通常位于 /usr/lib/docker/ 或 /etc/docker/ 目录下。 配置文件：Docker 的配置文件通常位于 /etc/docker/daemon.json，用于配置 Docker 守护进程的行为（例如，指定镜像仓库、日志驱动、存储驱动等）。 7. docker-compose (用于管理多容器应用) # 作用：docker-compose 是一个独立的命令行工具，用于定义和管理 Docker 容器的多容器应用（例如，定义应用的服务、网络、卷等）。它通过一个 YAML 文件 docker-compose.yml 来进行容器的编排和管理。 路径：通常安装在 /usr/local/bin/docker-compose 或 /usr/bin/docker-compose。 常见安装路径： # dockerd、docker、containerd、runc 等二进制文件通常位于 /usr/local/bin/ 或 /usr/bin/。\n你可以通过\nwhich 命令查看这些二进制文件的安装路径：\nwhich dockerd which docker which containerd which runc Docker 依赖的其他组件： # libcontainer：用于实现容器的基本功能。 aufs、btrfs、overlay、zfs 等存储驱动：Docker 支持多种存储驱动，每种驱动有不同的性能和特性。 network plugins：Docker 网络插件，用于为容器提供网络支持。 总结 # Docker 的核心二进制文件主要包括 dockerd（守护进程）、docker（命令行客户端）、containerd（容器运行时）和 runc（底层容器运行时）。如果使用二进制方式安装 Docker，通常需要将这些二进制文件放置在合适的路径，并配置相应的 systemd 服务文件来启动和管理它们。\n- name: 创建kube_node 相关目录 file: name={{ item }} state=directory with_items: - /var/lib/kubelet - /var/lib/kube-proxy - /etc/cni/net.d - /opt/cni/bin - name: 下载 kubelet,kube-proxy 二进制 copy: src={{ base_dir }}/bin/{{ item }} dest={{ bin_dir }}/{{ item }} mode=0755 with_items: - kubectl - kubelet - kube-proxy tags: upgrade_k8s - name: 下载 cni plugins 二进制文件 copy: src={{ item }} dest=/opt/cni/bin/ mode=0755 with_fileglob: - \u0026#34;{{ base_dir }}/bin/cni-bin/*\u0026#34; - name: 添加 kubectl 自动补全 lineinfile: dest: ~/.bashrc state: present regexp: \u0026#39;kubectl completion\u0026#39; line: \u0026#39;source \u0026lt;(kubectl completion bash) # generated by kubeasz\u0026#39; ##----------kubelet 配置部分-------------- # 创建 kubelet 相关证书及 kubelet.kubeconfig - import_tasks: create-kubelet-kubeconfig.yml tags: force_change_certs - name: 准备 cni配置文件 template: src=cni-default.conf.j2 dest=/etc/cni/net.d/10-default.conf - name: 创建kubelet的配置文件 template: src=kubelet-config.yaml.j2 dest=/var/lib/kubelet/config.yaml tags: upgrade_k8s, restart_node - name: 检查文件/run/systemd/resolve/resolv.conf stat: path=\u0026#34;/run/systemd/resolve/resolv.conf\u0026#34; register: resolv tags: upgrade_k8s, restart_node - name: 替换resolvConf 配置 lineinfile: dest: /var/lib/kubelet/config.yaml state: present regexp: \u0026#39;resolvConf\u0026#39; line: \u0026#39;resolvConf: /run/systemd/resolve/resolv.conf\u0026#39; when: \u0026#34;resolv.stat.isreg is defined\u0026#34; tags: upgrade_k8s, restart_node - name: 创建kubelet的systemd unit文件 template: src=kubelet.service.j2 dest=/etc/systemd/system/kubelet.service tags: upgrade_k8s, restart_node - name: 开机启用kubelet 服务 shell: systemctl enable kubelet ignore_errors: true - name: 开启kubelet 服务 shell: systemctl daemon-reload \u0026amp;\u0026amp; systemctl restart kubelet tags: upgrade_k8s, restart_node, force_change_certs ##-------kube-proxy部分---------------- - name: 分发 kube-proxy.kubeconfig配置文件 copy: src={{ cluster_dir }}/kube-proxy.kubeconfig dest=/etc/kubernetes/kube-proxy.kubeconfig tags: force_change_certs - name: 替换 kube-proxy.kubeconfig 的 apiserver 地址 lineinfile: dest: /etc/kubernetes/kube-proxy.kubeconfig regexp: \u0026#34;^ server\u0026#34; line: \u0026#34; server: {{ KUBE_APISERVER }}\u0026#34; tags: force_change_certs - name: 创建kube-proxy 配置 template: src=kube-proxy-config.yaml.j2 dest=/var/lib/kube-proxy/kube-proxy-config.yaml tags: reload-kube-proxy, restart_node, upgrade_k8s - name: 创建kube-proxy 服务文件 template: src=kube-proxy.service.j2 dest=/etc/systemd/system/kube-proxy.service tags: reload-kube-proxy, restart_node, upgrade_k8s - name: 开机启用kube-proxy 服务 shell: systemctl enable kube-proxy ignore_errors: true - name: 开启kube-proxy 服务 shell: systemctl daemon-reload \u0026amp;\u0026amp; systemctl restart kube-proxy tags: reload-kube-proxy, upgrade_k8s, restart_node, force_change_certs # 轮询等待kube-proxy启动完成 - name: 轮询等待kube-proxy启动 shell: \u0026#34;systemctl is-active kube-proxy.service\u0026#34; register: kubeproxy_status until: \u0026#39;\u0026#34;active\u0026#34; in kubeproxy_status.stdout\u0026#39; retries: 4 delay: 2 tags: reload-kube-proxy, upgrade_k8s, restart_node, force_change_certs # 轮询等待kubelet启动完成 - name: 轮询等待kubelet启动 shell: \u0026#34;systemctl is-active kubelet.service\u0026#34; register: kubelet_status until: \u0026#39;\u0026#34;active\u0026#34; in kubelet_status.stdout\u0026#39; retries: 4 delay: 2 tags: reload-kube-proxy, upgrade_k8s, restart_node, force_change_certs - name: 轮询等待node达到Ready状态 shell: \u0026#34;{{ base_dir }}/bin/kubectl get node {{ K8S_NODENAME }}|awk \u0026#39;NR\u0026gt;1{print $2}\u0026#39;\u0026#34; register: node_status until: node_status.stdout == \u0026#34;Ready\u0026#34; or node_status.stdout == \u0026#34;Ready,SchedulingDisabled\u0026#34; retries: 8 delay: 8 tags: upgrade_k8s, restart_node, force_change_certs connection: local - block: - name: Setting worker role name shell: \u0026#34;{{ base_dir }}/bin/kubectl label node {{ K8S_NODENAME }} kubernetes.io/role=node --overwrite\u0026#34; - name: Setting master role name shell: \u0026#34;{{ base_dir }}/bin/kubectl label node {{ K8S_NODENAME }} kubernetes.io/role=master --overwrite\u0026#34; when: \u0026#34;inventory_hostname in groups[\u0026#39;kube_master\u0026#39;]\u0026#34; - name: Making master nodes SchedulingDisabled shell: \u0026#34;{{ base_dir }}/bin/kubectl cordon {{ K8S_NODENAME }} \u0026#34; when: \u0026#34;inventory_hostname not in groups[\u0026#39;kube_node\u0026#39;]\u0026#34; ignore_errors: true connection: local 这个 Ansible Playbook 是用于将一个新节点加入 Kubernetes 集群的过程，涵盖了以下几个步骤：\n1. 创建必要的目录 # 创建节点相关目录\n：\n- name: 创建kube_node 相关目录 file: name={{ item }} state=directory with_items: - /var/lib/kubelet - /var/lib/kube-proxy - /etc/cni/net.d - /opt/cni/bin 这些目录是 Kubernetes 组件（如\nkubelet 和\nkube-proxy ）运行所必需的，确保新节点有这些目录。\n2. 下载二进制文件 # 下载 kubelet, kube-proxy, kubectl：\n- name: 下载 kubelet,kube-proxy 二进制 copy: src={{ base_dir }}/bin/{{ item }} dest={{ bin_dir }}/{{ item }} mode=0755 with_items: - kubectl - kubelet - kube-proxy 将 kubelet, kube-proxy, kubectl 等二进制文件分发到目标节点，确保节点能够与 Kubernetes 集群进行通信和管理。\n下载 CNI 插件：\n- name: 下载 cni plugins 二进制文件 copy: src={{ item }} dest=/opt/cni/bin/ mode=0755 with_fileglob: - \u0026#34;{{ base_dir }}/bin/cni-bin/*\u0026#34; CNI 插件用于管理容器网络。\n3. 配置文件设置 # 添加 kubectl 自动补全\n：\n- name: 添加 kubectl 自动补全 lineinfile: dest: ~/.bashrc state: present regexp: \u0026#39;kubectl completion\u0026#39; line: \u0026#39;source \u0026lt;(kubectl completion bash) # generated by kubeasz\u0026#39; 为\nkubectl 设置自动补全功能，方便后续操作。\n4. 配置 Kubelet # 创建 Kubelet 配置文件：\n- name: 创建kubelet的配置文件 template: src=kubelet-config.yaml.j2 dest=/var/lib/kubelet/config.yaml 创建 kubelet 的配置文件，配置节点如何加入 Kubernetes 集群。\n配置 DNS 设置：\n- name: 替换resolvConf 配置 lineinfile: dest: /var/lib/kubelet/config.yaml state: present regexp: \u0026#39;resolvConf\u0026#39; line: \u0026#39;resolvConf: /run/systemd/resolve/resolv.conf\u0026#39; 修改 kubelet 配置文件，确保 DNS 配置正确。\n创建 kubelet 的 systemd 服务文件：\n- name: 创建kubelet的systemd unit文件 template: src=kubelet.service.j2 dest=/etc/systemd/system/kubelet.service 创建 kubelet 的 systemd 服务文件，以便通过 systemd 管理服务的启动和停止。\n启用并启动 kubelet 服务：\n- name: 开机启用kubelet 服务 shell: systemctl enable kubelet ignore_errors: true - name: 开启kubelet 服务 shell: systemctl daemon-reload \u0026amp;\u0026amp; systemctl restart kubelet 5. 配置 Kube-proxy # 下载并配置 kube-proxy.kubeconfig：\n- name: 分发 kube-proxy.kubeconfig配置文件 copy: src={{ cluster_dir }}/kube-proxy.kubeconfig dest=/etc/kubernetes/kube-proxy.kubeconfig 配置 kube-proxy 的 kubeconfig，使其能够与 Kubernetes 集群进行通信。\n创建 kube-proxy 配置文件：\n- name: 创建kube-proxy 配置 template: src=kube-proxy-config.yaml.j2 dest=/var/lib/kube-proxy/kube-proxy-config.yaml 创建 kube-proxy 服务文件：\n- name: 创建kube-proxy 服务文件 template: src=kube-proxy.service.j2 dest=/etc/systemd/system/kube-proxy.service 启用并启动 kube-proxy 服务：\n- name: 开机启用kube-proxy 服务 shell: systemctl enable kube-proxy ignore_errors: true - name: 开启kube-proxy 服务 shell: systemctl daemon-reload \u0026amp;\u0026amp; systemctl restart kube-proxy 6. 服务启动监控与等待 # 轮询等待 kube-proxy 启动完成：\n- name: 轮询等待kube-proxy启动 shell: \u0026#34;systemctl is-active kube-proxy.service\u0026#34; register: kubeproxy_status until: \u0026#39;\u0026#34;active\u0026#34; in kubeproxy_status.stdout\u0026#39; retries: 4 delay: 2 轮询等待 kubelet 启动完成：\n- name: 轮询等待kubelet启动 shell: \u0026#34;systemctl is-active kubelet.service\u0026#34; register: kubelet_status until: \u0026#39;\u0026#34;active\u0026#34; in kubelet_status.stdout\u0026#39; retries: 4 delay: 2 轮询等待节点状态变为 Ready：\n- name: 轮询等待node达到Ready状态 shell: \u0026#34;{{ base_dir }}/bin/kubectl get node {{ K8S_NODENAME }}|awk \u0026#39;NR\u0026gt;1{print $2}\u0026#39;\u0026#34; register: node_status until: node_status.stdout == \u0026#34;Ready\u0026#34; or node_status.stdout == \u0026#34;Ready,SchedulingDisabled\u0026#34; retries: 8 delay: 8 7. 标签和调度控制 # 设置节点角色标签：\n- name: Setting worker role name shell: \u0026#34;{{ base_dir }}/bin/kubectl label node {{ K8S_NODENAME }} kubernetes.io/role=node --overwrite\u0026#34; 为主节点设置调度禁用：\n- name: Making master nodes SchedulingDisabled shell: \u0026#34;{{ base_dir }}/bin/kubectl cordon {{ K8S_NODENAME }} \u0026#34; when: \u0026#34;inventory_hostname not in groups[\u0026#39;kube_node\u0026#39;]\u0026#34; 总结 # 这个 Playbook 的步骤主要是：\n创建 Kubernetes 节点所需的目录和配置。 下载并配置所需的二进制文件（kubelet、kube-proxy、kubectl 和 CNI 插件）。 配置并启用 kubelet 和 kube-proxy 服务，确保它们在系统启动时自动启动。 检查并等待 kubelet 和 kube-proxy 启动成功，并确保节点的状态变为 Ready。 设置节点角色标签，并在必要时对主节点执行调度禁用（cordon）操作。 总体来看，它是一个自动化的 Kubernetes 节点加入过程，配置并启动了节点所需的所有服务，使节点能够顺利加入 Kubernetes 集群。\n"}]